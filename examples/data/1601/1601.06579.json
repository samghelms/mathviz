[{"file": "1601.06579.tex", "nexttext": "\nwhere $\\tau$ is some critical threshold. Intuitively, the goal of {Moran's~I}\\ is to quantify whether observations $x_i$ and $x_j$ are more similar when $w_{ij} = 1$ than when $w_{ij} = 0$.\n\n{Moran's~I}\\ is based on a hypothesized autoregressive process $X = \\rho W X + \\epsilon$, where $X$ is a vector of the linguistic observations $x_1, \\ldots x_{n}$, and $\\epsilon$ is a vector of uncorrelated noise. Since $X$ and $W$ are given, the estimation problem is to find $\\rho$ so as to minimize the magnitude of $\\epsilon$. To take a probabilistic interpretation, it is typical to assume that $\\epsilon$ consists of independent and identically-distributed (IID) normal random variables with zero mean~\\cite{ord1975estimation}. Under the null hypothesis, we would have $\\rho = 0$, indicating that there is no spatial dependence between the observations in $X$.\nBecause $\\rho$ is difficult to estimate exactly~\\cite{ord1975estimation}, {Moran's~I}\\ is used as an approximation. It is computed as,\n\\iffalse\n\n", "itemtype": "equation", "pos": 11813, "prevtext": "\n\n\\title{A Kernel Independence Test for Geographical Language Variation}\n\n\\author{Dong Nguyen}\n\\affil{University of Twente}\n\n\\author{Jacob Eisenstein}\n\\affil{Georgia Institute of Technology}\n\n\\maketitle\n\n\\begin{abstract}\nQuantifying the degree of spatial dependence for linguistic variables is a key task for analyzing dialectal variation. However, existing approaches have important drawbacks. First, they make unjustified assumptions about the nature of spatial variation: some assume that the geographical distribution of linguistic variables is Gaussian, while others assume that linguistic variation is aligned to pre-defined geopolitical units such as states or counties. Second, they are not applicable to all types of linguistic data: some approaches apply only to frequencies, others to boolean indicators of whether a linguistic variable is present. \nWe present a new method for measuring geographical language variation, which solves both of these problems. Our approach builds on reproducing kernel Hilbert space (RKHS) representations for nonparametric statistics, and takes the form of a test statistic that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real datasets: a corpus of Dutch tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in North American newspapers. Our proposed test is shown to support robust inferences across a broad range of scenarios and types of data.\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\\autoref{fig:hella} shows the geographical location of 1000 Twitter posts containing the word {\\textit{{hella}}}, an intensifier used in expressions like {\\textit{{I got hella studying to do}}} and {\\textit{{my eyes got hella big}}}~\\cite{eisenstein2014diffusion}. While the word appears in major population centers throughout the United States, the map suggests that it enjoys a particularly high level of popularity on the west coast, in the area around San Francisco. But does this represent a real geographical difference in American English, or is it the result of chance fluctuation in a finite dataset?\n\nRegional variation of language has been extensively studied in sociolinguistics and dialectology~\\cite{chambers1998dialectology,grieve2011statistical,JLG:8992970,lee1993spatial,nerbonne2013dialectometry,szmrecsanyi2012grammatical}. A common approach involves mapping the geographic distribution of a linguistic variable (e.g., the choice of {\\textit{{soda}}}, {\\textit{{pop}}}, or {\\textit{{coke}}} to refer to a soft drink) and identifying boundaries between regions based on the data. The identification of linguistic variables that exhibit regional variation is therefore the first step in many studies of regional dialects. Traditionally, this step has been based on the manual judgment of the researcher; depending on the quality of the researcher's intuitions, the most interesting or important variables might be missed. \n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=2.8in]{hella-samp1000-crop}\n\\caption{1000 geolocated tweets containing the word {\\textit{{hella}}}}\n\\label{fig:hella}\n\\end{figure}\n\nThe increasing amount of data available to study dialectal variation suggests a turn towards data-driven alternatives for variable selection. For example, researchers can mine social media data such as Twitter~\\cite{doyle2014mapping,eisenstein2010latent} or product reviews~\\cite{Hovy2015} to identify and test thousands of dialectal variables. Despite the large scale of available data, the well-known ``long tail'' phenomenon of language ensures that there will be many potential variables with low counts. A statistical metric for comparing the strength of geographical associations across potential linguistic variables would allow linguists to determine whether finite geographical samples --- such as the one shown in \\autoref{fig:hella} --- reveal a statistically meaningful association.\n\n\n\n\n\\iffalse \nLee and Kretzschmar~\\cite{lee1993spatial} used a method of point pattern analysis (PPA) to study  linguistic data. Grieve {\\textit{et al.}}~\\cite{grieve2011statistical} applied Moran's I on a  corpus of letters to the editors across the United States and their approach was adopted in various dialect studies  \\cite{Asnaghi:2014fk,JLG:8992970,szmrecsanyi2012grammatical}. Another method  is the Mantel test, which measures the element-wise correlation between two distance (or similarity) matrices. Scherrer \\cite{scherrer:2012:LINGVIS2012} used it to correlate linguistic distance with geographical distance, and Gooskens and Heeringa\n\\cite{Gooskens01112006} correlated  perceptual distance with linguistic distance. By generating suitable distance or similarity matrices, the Mantel test can also be used to test spatial dependence for linguistic variables.\n\\fi\n\n\nThe use of statistical methods to analyze spatial dependence has been only lightly studied in sociolinguistics and dialectology. Relevant methods tend to employ classical statistics such as Moran's I~\\cite[e.g.,][]{grieve2011statistical}, Point Pattern Analysis~\\cite[e.g.,][]{lee1993spatial} and the Mantel Test~\\cite[e.g.,][]{scherrer:2012:LINGVIS2012}. We review these approaches in \\autoref{sec:methods}. In general, they suffer from two main weaknesses: they make unjustified assumptions about the nature of spatial variation, and they are applicable to only some forms of linguistic data. We now review these challenges in detail.\n\n\\paragraph{Assumptions about spatial variation}\nSeveral existing methods assume that linguistic variation is aligned to pre-defined geopolitical units such as states or counties. For example, Moran's I is designed for comparing frequencies in data that has been geographically aggregated by bins, such as metropolitan statistical areas \\cite{grieve2011statistical}, cities \\cite{JLG:8992970,bo2012geolocation} and counties \\cite{szmrecsanyi2012grammatical}. Computational linguistic studies make similar assumptions, relying on politically-defined units~\\cite{Hovy2015}, geodesic grids~\\cite{wing2011simple}, KD-trees~\\cite{roller2012supervised}, Gaussians~\\cite{eisenstein2010latent}, and mixtures of Gaussians~\\cite{hong2012discovering}. All such approaches sacrifice statistical power because linguistic ``isoglosses'' (the technical term for the geographical boundaries between linguistic features) need not align with politically-defined geographical units~\\cite{nerbonne2013dialectometry}. Depending on the definition of the geographical units, they may have dramatically varying populations; in low-populations units, frequency statistics may suffer from high variance, leading to inferential errors. \n\nWhile the Mantel Test does not employ geographic binning, it is a correlation-based statistic that assumes a linear relationship between geographical similarity and linguistic similarity.\n\nThis assumption of linearity is refuted by both theoretical and empirical dialectological evidence~\\cite{nerbonne2010measuring}, and will therefore limit the statistical power of both the Mantel Test and {Moran's~I}.\n\n\\paragraph{Lack of applicability} A second problem with existing approaches is that they are not applicable to all types of linguistic data. For example, {Moran's~I}\\ is designed for datasets in which each geographical region is associated with a single \\emph{frequency} parameter, capturing the likelihood of one or another linguistic form; it is not even applicable to data in linguistic variables with three or more possible forms (e.g., {\\textit{{soda/pop/coke}}}). Conversely, in its original form, Point Pattern Analysis~\\cite{lee1993spatial} is suitable only for binary or category data.\n\n\n\n\\paragraph{Our approach}\nTo address these limitations, we propose a new test statistic that builds on a rich and growing literature on kernel-based methods for non-parametric statistics \\cite{shawe2004kernel}. In these methods, probability distributions, such as the distribution over geographical locations for each linguistic variable, are embedded in a Reproducing Kernel Hilbert space (RKHS). Specifically, we employ the Hilbert-Schmidt Independence Criterion~\\cite[HSIC;][]{gretton2005measuring}, which can be seen as capturing covariance in a high-dimensional feature space.\nHSIC can be applied to both frequency and categorical data, requires no binning and makes no parametric assumptions about the form which spatial dependence will take. Because the approach is based on kernel similarity, it can be applied to any form of linguistic data, as long as an appropriate kernel function can be constructed.\n\nTo validate this approach, we compare it against three alternative spatial statistics: Moran's I, the Mantel test, and Point Pattern Analysis (PPA). For a controlled comparison, we use synthetic data to simulate different types of regional variation, and different types\nof linguistic data. This allows us to measure the capability of each approach to recover true geo-linguistic associations, and to avoid Type I errors even in noisy and sparse data. Next, we apply these approaches to three real linguistic datasets: a corpus of Dutch tweets, a Dutch syntactic atlas and letters to the editor in North American newspapers.\n\nTo summarize, the contributions of this article are:\n\\begin{itemize}\n\\item We show how the recently-introduced HSIC test statistic can be applied to linguistic data. HSIC is a non-parametric test statistic, which can handle both frequency and categorical data, requires no discretization of geographic data, and is capable of detecting nonlinear geo-linguistic associations (\\autoref{sec:methods}).\n\\item We use synthetic data to compare the power and calibration of HSIC against three alternatives: {Moran's~I}, the Mantel Test, and Point Pattern Analysis (\\autoref{sec:synth}).\n \\item We apply these methods to analyze dialectal variation in three empirical datasets, in both English and Dutch, across a variety of registers (\\autoref{sec:real}). \n\\end{itemize}\n\n\n\\section{Methods}\n\\label{sec:methods}\nThis section describes four methods for quantifying the degree of spatial dependence in an observed signal. The first three methods are included because they are used in linguistic papers on dialect: Moran's I~\\cite{grieve2011statistical},  Point Pattern Analysis~\\cite{lee1993spatial} and the Mantel test ~\\cite{scherrer:2012:LINGVIS2012}. After describing these methods, we present the Hilbert-Schmidt Independence Criterion (HSIC), a kernel-based nonparametric statistic for measuring cross-covariance~\\cite{gretton2005measuring}. To our knowledge, this statistic has not previously been used in combination with linguistic data.\n\nWe define a consistent notation across methods. Let $x_i$ represent a scalar linguistic observation for unit $i \\in \\{1\\ldots {n}\\}$ (typically, the presence or frequency of a linguistic variable), and let $y_i$ represent a corresponding geolocation. For convenience, we define $d_{ij}$ as the spatial distance between $y_i$ and $y_j$. Suppose we have $n$ observations, so that the data $\\mathcal{D} = \\{(x_1,y_1),(x_2,y_2), \\ldots, (x_{n},y_{n})\\}$. Our goal is to test the strength of association between $X$ and $Y$, with a null hypothesis that there is no association. \n\n{}\n\\subsection{Moran's I}\nGrieve {\\textit{et al.}}~\\cite{grieve2011statistical} introduced the use of {Moran's~I}~\\cite{cliff1981spatial,moran1950} in the study of dialectal variation. To define the statistic, let $W = \\{w_{ij}\\}_{i,j \\in \\{1 \\ldots N\\}}$ represent a \\emph{spatial neighborhood matrix}, such that larger values of $w_{ij}$ indicate greater proximity, and $w_{ii} = 0$. In their application of {Moran's~I}\\ to a corpus of letters-to-the-editor, Grieve {\\textit{et al.}}\\ define $W$ as,\n\n", "index": 1, "text": "\\begin{equation}\nw_{ij} = \\begin{cases}\n1, & d_{ij} < \\tau, i\\neq j \\\\\n0, & d_{ij} \\geq \\tau, \\text{ or } i = j\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"w_{ij}=\\begin{cases}1,&amp;d_{ij}&lt;\\tau,i\\neq j\\\\&#10;0,&amp;d_{ij}\\geq\\tau,\\text{ or }i=j\\end{cases}\" display=\"block\"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>&lt;</mo><mi>\u03c4</mi></mrow><mo>,</mo><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2265</mo><mi>\u03c4</mi></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0or\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mi>j</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\n\\fi\n\n", "itemtype": "equation", "pos": 12954, "prevtext": "\nwhere $\\tau$ is some critical threshold. Intuitively, the goal of {Moran's~I}\\ is to quantify whether observations $x_i$ and $x_j$ are more similar when $w_{ij} = 1$ than when $w_{ij} = 0$.\n\n{Moran's~I}\\ is based on a hypothesized autoregressive process $X = \\rho W X + \\epsilon$, where $X$ is a vector of the linguistic observations $x_1, \\ldots x_{n}$, and $\\epsilon$ is a vector of uncorrelated noise. Since $X$ and $W$ are given, the estimation problem is to find $\\rho$ so as to minimize the magnitude of $\\epsilon$. To take a probabilistic interpretation, it is typical to assume that $\\epsilon$ consists of independent and identically-distributed (IID) normal random variables with zero mean~\\cite{ord1975estimation}. Under the null hypothesis, we would have $\\rho = 0$, indicating that there is no spatial dependence between the observations in $X$.\nBecause $\\rho$ is difficult to estimate exactly~\\cite{ord1975estimation}, {Moran's~I}\\ is used as an approximation. It is computed as,\n\\iffalse\n\n", "index": 3, "text": "\\begin{align}\nI = & \\frac{{n}}{\\sum_i^{n} \\sum_j^{n} w_{ij}} \n\\frac{\\sum_i^{n} \\sum_j^{n} w_{ij} (x_i -\\overline{x})(x_j - \\overline{x})}\n{\\sum_i^{n} (x_i - \\overline{x})^2},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I=\" display=\"inline\"><mrow><mi>I</mi><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{{n}}{\\sum_{i}^{n}\\sum_{j}^{n}w_{ij}}\\frac{\\sum_{i}^{n}\\sum_%&#10;{j}^{n}w_{ij}(x_{i}-\\overline{x})(x_{j}-\\overline{x})}{\\sum_{i}^{n}(x_{i}-%&#10;\\overline{x})^{2}},\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></msubsup><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\nwhere $\\overline{x} = \\frac{1}{{n}}\\sum_i x_i$. The ratio on the left is the inverse of the variance of $X$; the ratio on the right  corresponds to the covariance between points $i$ and $j$ that are spatially similar. Thus, the statistic rescales a spatially-reweighted covariance (the ratio on the right of \\autoref{eq:mi}) by the overall variance (the ratio on the left of \\autoref{eq:mi}), giving an estimate of the overall spatial dependence of $X$. A compact alternative notation is to rewrite the statistic in terms of the matrix of \\emph{residuals} $R = \\{r_i\\}_{i\\in 1\\ldots {n}}$, where $r_i = x_i - \\overline{x}$. This yields the form $I = \\frac{{{R}^{\\top}} W R}{{{R}^{\\top}} R}$, with ${{R}^{\\top}}$ indicating the transpose of the column vector $R$. {Moran's~I}\\ values often lie between $-1$ and $1$, but the exact range depends on the weight matrix $W$, and is theoretically unbounded~\\cite{GEAN:GEAN797}. \n\nIn hypothesis testing, our goal is to determine the $p$-value representing the likelihood that a value of {Moran's~I}\\ at least as extreme as the observed value would arise by chance under the null hypothesis. \n\nThe expected value of {Moran's~I}\\ in the case of no spatial dependence is $-\\frac{1}{n-1}$. Grieve {\\textit{et al.}}~compute $p$-values from a closed-form approximation of the variance under the null hypothesis of total randomization. A non-parametric alternative is to perform a \\emph{permutation test}, calculating the empirical $p$-value by comparing the observed test statistic against the values that arise across multiple random permutations of the original data.\n\nIn the study of dialect, $X$ typically represents the frequency or presence of some linguistic variable, such as the use of {\\textit{{soda}}} versus {\\textit{{pop}}}. We are unaware of applications of Moran's I to variables with more than two possibilities (e.g., {\\textit{{soda}}}, {\\textit{{pop}}}, {\\textit{{coke}}}), and it is not clear how this would be computed.\nA key question for the use of Moran's I is the definition of the spatial neighborhood matrix $W$. As noted above, Grieve {\\textit{et al.}}\\ set $w_{ij} = 1$ if the distance is below some threshold $\\tau$. \n\nIn Section~\\ref{sec:synth}, we use synthetic data to test the sensitivity of {Moran's~I}\\ to the value of this parameter, and evaluate heuristics that have been proposed in prior work.\n\n\\subsection{Point Pattern Analysis (PPA)}\nOne way to avoid defining a threshold for the spatial adjacency matrix $W$ is to use Delaunay triangulation, a technique for automatically producing a mesh of triangles over a set of points. A property of Delaunay triangulation is that points tend to be connected to their closest neighbors, regardless of how distant or near those neighbors are: in high-density regions, the edges will tend to be short, while in low-density regions, the edges will be long. The method is therefore arguably more suitable to data in which the density of observations is highly variable --- for example, between densely-populated cities and sparse-populated hinterlands.\n\nLee and Kretzschmar~\\cite{lee1993spatial} apply Delaunay triangulation to the analysis of a set of dialect interviews, in the framework of Point Pattern Analysis (PPA).\\footnote{Note that this procedure could in principle be used to construct $W$ for Moran's I, but we are unaware of this being tried in any previous work on dialect analysis.}\nHere, each observation is assumed to be binary, $x_i \\in \\{0,1\\}$; the Delaunay triangulation can again be represented with a matrix $W$, where $w_{ij} = 1$ whenever points $i$ and $j$ are connected in the triangulation. We can then compute the number of ``agreements'' between linked points,\n\n", "itemtype": "equation", "pos": 13145, "prevtext": "\n\\fi\n\n", "index": 5, "text": "\\begin{align}\nI = & \\frac{{n}}{\\sum_i^{n} (x_i - \\overline{x})^2}\n\\frac{\\sum_i^{n} \\sum_j^{n} w_{ij} (x_i - \\overline{x})\n(x_j - \\overline{x})}{\\sum_i^{n} \\sum_j^{n} w_{ij}},\n\\label{eq:mi}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I=\" display=\"inline\"><mrow><mi>I</mi><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{{n}}{\\sum_{i}^{n}(x_{i}-\\overline{x})^{2}}\\frac{\\sum_{i}^{n%&#10;}\\sum_{j}^{n}w_{ij}(x_{i}-\\overline{x})(x_{j}-\\overline{x})}{\\sum_{i}^{n}\\sum_%&#10;{j}^{n}w_{ij}},\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></msubsup><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></msubsup><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\nwith ${{X}^{\\top}}$ indicating the transpose of the column vector $X$. Note the similarity to the numerator of Moran's I, which can be written as ${{R}^{\\top}} W R$.\n\nTo evaluate the statistical significance of this statistic, we must also compute the \\emph{expected} number of agreements under the null hypothesis. The likelihood of any two randomly chosen points having $x_i = x_j = 1$ is $\\overline{x}^2$, and the likelihood of their having the value $x_i = x_j = 0$ is $(1-\\overline{x})^2$, where $\\overline{x}$ is again the empirical mean, $\\overline{x} = \\frac{1}{{n}} \\sum_i x_i$. Since the total number of linked points is $\\sum_{ij} w_{ij}$, the expected number of agreements is given by:\n\n", "itemtype": "equation", "pos": 17049, "prevtext": "\nwhere $\\overline{x} = \\frac{1}{{n}}\\sum_i x_i$. The ratio on the left is the inverse of the variance of $X$; the ratio on the right  corresponds to the covariance between points $i$ and $j$ that are spatially similar. Thus, the statistic rescales a spatially-reweighted covariance (the ratio on the right of \\autoref{eq:mi}) by the overall variance (the ratio on the left of \\autoref{eq:mi}), giving an estimate of the overall spatial dependence of $X$. A compact alternative notation is to rewrite the statistic in terms of the matrix of \\emph{residuals} $R = \\{r_i\\}_{i\\in 1\\ldots {n}}$, where $r_i = x_i - \\overline{x}$. This yields the form $I = \\frac{{{R}^{\\top}} W R}{{{R}^{\\top}} R}$, with ${{R}^{\\top}}$ indicating the transpose of the column vector $R$. {Moran's~I}\\ values often lie between $-1$ and $1$, but the exact range depends on the weight matrix $W$, and is theoretically unbounded~\\cite{GEAN:GEAN797}. \n\nIn hypothesis testing, our goal is to determine the $p$-value representing the likelihood that a value of {Moran's~I}\\ at least as extreme as the observed value would arise by chance under the null hypothesis. \n\nThe expected value of {Moran's~I}\\ in the case of no spatial dependence is $-\\frac{1}{n-1}$. Grieve {\\textit{et al.}}~compute $p$-values from a closed-form approximation of the variance under the null hypothesis of total randomization. A non-parametric alternative is to perform a \\emph{permutation test}, calculating the empirical $p$-value by comparing the observed test statistic against the values that arise across multiple random permutations of the original data.\n\nIn the study of dialect, $X$ typically represents the frequency or presence of some linguistic variable, such as the use of {\\textit{{soda}}} versus {\\textit{{pop}}}. We are unaware of applications of Moran's I to variables with more than two possibilities (e.g., {\\textit{{soda}}}, {\\textit{{pop}}}, {\\textit{{coke}}}), and it is not clear how this would be computed.\nA key question for the use of Moran's I is the definition of the spatial neighborhood matrix $W$. As noted above, Grieve {\\textit{et al.}}\\ set $w_{ij} = 1$ if the distance is below some threshold $\\tau$. \n\nIn Section~\\ref{sec:synth}, we use synthetic data to test the sensitivity of {Moran's~I}\\ to the value of this parameter, and evaluate heuristics that have been proposed in prior work.\n\n\\subsection{Point Pattern Analysis (PPA)}\nOne way to avoid defining a threshold for the spatial adjacency matrix $W$ is to use Delaunay triangulation, a technique for automatically producing a mesh of triangles over a set of points. A property of Delaunay triangulation is that points tend to be connected to their closest neighbors, regardless of how distant or near those neighbors are: in high-density regions, the edges will tend to be short, while in low-density regions, the edges will be long. The method is therefore arguably more suitable to data in which the density of observations is highly variable --- for example, between densely-populated cities and sparse-populated hinterlands.\n\nLee and Kretzschmar~\\cite{lee1993spatial} apply Delaunay triangulation to the analysis of a set of dialect interviews, in the framework of Point Pattern Analysis (PPA).\\footnote{Note that this procedure could in principle be used to construct $W$ for Moran's I, but we are unaware of this being tried in any previous work on dialect analysis.}\nHere, each observation is assumed to be binary, $x_i \\in \\{0,1\\}$; the Delaunay triangulation can again be represented with a matrix $W$, where $w_{ij} = 1$ whenever points $i$ and $j$ are connected in the triangulation. We can then compute the number of ``agreements'' between linked points,\n\n", "index": 7, "text": "\\begin{align}\n\\text{num-agree} = & \\sum^{n}_i \\sum^{n}_j w_{ij}(x_i x_j + (1-x_i)(1-x_j))\\\\\n= & {{X}^{\\top}} W X + {{(1-X)}^{\\top}} W (1-X),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{num-agree}=\" display=\"inline\"><mrow><mtext>num-agree</mtext><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}(x_{i}x_{j}+(1-x_{i})(1-x_{j}))\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></munderover></mstyle><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{{X}^{\\top}}WX+{{(1-X)}^{\\top}}W(1-X),\" display=\"inline\"><mrow><mrow><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": " \n\nThe variance of the number of agreements under the null hypothesis can also be obtained in closed form, enabling the computation of a $z$-statistic and $p$-value for the null hypothesis of total randomization. \n\nBecause PPA is based on counts of agreements, it requires that each $x_i$ is a categorical variable --- possibly non-binary --- rather than a frequency. In this sense, it is the inverse of Moran's I, which can be applied to frequencies, but not to non-binary variables. Thus, PPA is best suited to cases where observations correspond to individual utterances (e.g., Twitter data, dialect interviews), rather than cases where observations correspond to longer texts (e.g., newspaper corpora). \n\n\\subsection{The Mantel Test}\nMoran's I and Point Pattern Analysis are both asymmetric in their treatment of the geographical and linguistic variables --- they measure similarity on the linguistic variable between all points that are sufficiently close in space. The Mantel test takes a more symmetric approach, and can in principle be used to measure the dependence between any two arbitrary signals. Let us compute \\emph{distances} for each pair of linguistic variables, $d_x(x_i,x_j)$, and each pair of spatial locations, $d_y(y_i,y_j)$, forming a pair of distance matrices $D_x$ and $D_y$. The Mantel test then measures the element-wise correlation (usually, the Pearson correlation) between these two matrices.  Scherrer \\cite{scherrer:2012:LINGVIS2012} used the Mantel test to correlate linguistic distance with geographical distance, and Gooskens and Heeringa \\cite{Gooskens01112006} correlated perceptual distance with linguistic distance.\\footnote{The Mantel test has also been applied to non-human dialect analysis, revealing regional differences in the call structures of Amazonian parrots by computing a linguistic distance matrix $D_x$ directly from spectral measurements~\\cite{wright1996regional}.}\n\nAs usual, the goal of hypothesis testing is to determine the likelihood that the observed test statistic --- in this case, the correlation between $D_x$ and $D_y$ --- could have arisen by chance. To assess the distribution of correlations under the null hypothesis, we randomly permute the rows or columns of one of the matrices repeatedly. If the correlation between the unpermuted $D_x$ and $D_y$ is consistently higher than the correlations under permutation, then the null hypothesis is unlikely to hold.\n\nA design question for the Mantel test is the choice of the distance functions for $X$ and $Y$. For spatial locations, we compute the distance matrix based on the Euclidean distance between each pair of points. For binary or categorical linguistic data, the entries of the linguistic distances matrix are set to $0$ if $x_i = x_j$, and $1$ otherwise. For linguistic frequency data, we use the absolute difference between the frequency values.\n\n\\iffalse\nA design question for the Mantel test is the choice of the similarity functions for $X$ and $Y$. In this study, we use a kernelized variant of the Mantel Test to be consistent with our proposed method, HSIC.\\footnote{An alternative formulation of the Mantel test employs distance functions rather than similarities. We tried this in pilot studies, but did not find substantial empirical differences from the kernelized variant described here.} We compute the similarity between the spatial locations using the radial basis function (RBF) $e^{-\\gamma d^2_{i,j}}$, where $d_{i,j}$ is again the Euclidean distance between $y_i$ and $y_j$, and $\\gamma$ is a kernel bandwidth parameter that controls the rate at which the kernel function decreases with distance. In the case of categorical linguistic data, the entries of the linguistic similarity matrix $S_x$ can be set to the delta function $\\delta(x_i, x_j)$, which takes the value $1$ if $x_i = x_j$ and $0$ otherwise. In the case of frequency-based linguistic data, we again use the RBF kernel. \n\n\\fi\n\nThe Mantel test is more flexible than {Moran's~I}\\ or Point Pattern Analysis: it is applicable to binary, categorical, and frequency data, and does not require the specification of a geographical distance threshold. However, by focusing on correlations between similarities, it makes an implicit parametric assumption: in the ideal case of perfect correlation, twice as much geographical distance should imply twice as much as linguistic distance. Yet a range of dialectometric studies have found that linguistic differences increase sublinearly with geographical distance, a phenomenon that Nerbonne has dubbed ``Seguy's law''~\\cite{nerbonne2010measuring}. On this view, the parametric assumption of linear dependence between geographical and linguistic distance is incorrect. A second concern is that human settlement patterns are highly variable, so that a distance of, say, 100 kilometers may be far more linguistically meaningful in a densely-populated urban area like New England than it would be in a more sparsely-populated region like the American West. If the assumptions underlying Mantel's test --- linearity and homogeneity  --- are incorrect, then the test will be \\emph{underpowered}, failing to detect meaningful relationships in the data.\n\n{}\n\\subsection{Hilbert-Schmidt Independence Criterion (HSIC)}\nThe discussion of existing tests for spatial dependence has helped to identify some desiderata. The ideal test would be applicable across many kind of linguistic data, including both binary and categorical variables, and both frequencies and discrete observations. The Mantel test solves these problems, but makes unsupported parametric assumptions about the nature of the relationship between linguistic and spatial distance. Rather than measuring the correlation of linguistic variables with a single, thresholded distance function (as in Moran's I and PPA) or a single correlation matrix (as in the Mantel test), we might prefer to model non-linear relationships, perhaps by considering higher-order moments ($x^2, x^3, \\ldots$) or other transformations of the spatial distances.\n\nBoth of these problems can be solved through the use of Reproducing Kernel Hilbert Spaces (RKHS), a family of techniques from non-parametric statistics, capable of capturing arbitrary statistical dependencies~\\cite{gretton2005measuring}. Specifically, the Hilbert-Schmidt Independence Criterion (HSIC) provides a robust test for statistical dependence of two signals, which is simple to implement, and involves only a single tunable parameter.\n\nAt the core of RKHS-based techniques is the kernel function on pairs of instances. Let $k(x_i,x_j) : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}_+$ represent a function from pairs $(x_i, x_j)$ to non-negative real numbers; let us also assume that $k(x_i,x_j) = k(x_j,x_i)$, so that $k$ can be thought of as a measure of similarity.\nNow suppose we have a set of inputs $\\{{\\mathbf{{x}}}_i\\}_{i\\in\\{1\\ldots {n}\\}}$, and we construct a matrix $K$, such that $K_{i,j} = k(x_i,x_j)$, known as the \\emph{Gram matrix}. For appropriately chosen kernel functions,\\footnote{Specifically, we require that $K$ is symmetric, meaning that $K_{ij} = K_{ji}$ for all $i$ and $j$, and positive definite, meaning that ${{{\\mathbf{{a}}}}^{\\top}} K {\\mathbf{{a}}} > 0$ for all vectors ${\\mathbf{{a}}}$. One test for positive definiteness is that the eigenvalues of $K$ must all be positive.} \nMercer's theorem guarantees that there exists some feature function $\\phi({\\mathbf{{x}}}) : \\mathcal{X} \\to \\mathbb{R}^D$ such that $k({\\mathbf{{x}}}_i, {\\mathbf{{x}}}_j) = {{\\phi({\\mathbf{{x}}}_i)}^{\\top}} \\phi({\\mathbf{{x}}}_j)$. The dimension $D$ of the feature function may be very large, even infinite; for example, the feature function may correspond to an infinite series, $[x, x^2, x^3, \\ldots]$. Nonetheless, by working with the kernels (rather than directly with the feature functions), we can compute the inner product (and therefore the covariance) directly from the kernel function.\n\nTo make things more concrete, let us define the spatial kernel function $k_\\gamma(y_i,y_j) = e^{-\\gamma d^2_{ij}}$, where $d^2_{ij}$ is the squared Euclidean distance between $y_i$ and $y_j$, and $\\gamma$ is a parameter of the kernel function. Similarly, for linguistic frequency data, let $\\ell_\\iota(x_i,x_j) = e^{-\\iota(x_i - x_j)^2}$. This linguistic kernel function is suitable for continuous data, such as values of acoustic variables, and can also capture difference in frequencies of a binary linguistic variable. These exponentially decaying kernel functions are known as \\emph{radial basis functions} (RBFs), and are guaranteed to be symmetric and positive definite, satisfying the conditions of Mercer's theorem. The RBF kernel corresponds to a inner product between infinite-dimensional feature vectors $\\phi$~\\cite{murphy2012machine}. Thus, although the RBF kernel function is based on distances, the resulting feature map includes non-linear transformations of these distances, and therefore the approach is \\emph{not} equivalent to simply correlating linguistic and geographical distances. In the case of binary and categorical data, we use a Delta kernel, where $\\ell_\\iota(x_i,x_j)  = 1$ if $x_i = x_j$ and 0 otherwise. The Delta kernel has been used successfully in combination with HSIC for high-dimensional feature selection~\\cite{Song2012,yamada2014high}.\n\nNow, if we compute the kernel functions over all pairs of observations, we obtain the Gram matrices $K$ and $L$, where $K_{ij} = k(y_i,y_j)$ and $L_{ij} = \\ell(x_i,x_j)$, eliding the parameters $\\gamma$ and $\\iota$ for clarity. \n\n\nThe Hilbert-Schmidt Independence Criterion (HSIC) is a nonparametric measure of the dependence between $X$ and $Y$~\\cite{gretton2005measuring}, and an empirical estimator is given by,\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwith ${{X}^{\\top}}$ indicating the transpose of the column vector $X$. Note the similarity to the numerator of Moran's I, which can be written as ${{R}^{\\top}} W R$.\n\nTo evaluate the statistical significance of this statistic, we must also compute the \\emph{expected} number of agreements under the null hypothesis. The likelihood of any two randomly chosen points having $x_i = x_j = 1$ is $\\overline{x}^2$, and the likelihood of their having the value $x_i = x_j = 0$ is $(1-\\overline{x})^2$, where $\\overline{x}$ is again the empirical mean, $\\overline{x} = \\frac{1}{{n}} \\sum_i x_i$. Since the total number of linked points is $\\sum_{ij} w_{ij}$, the expected number of agreements is given by:\n\n", "index": 9, "text": "\\begin{equation}\nE[\\text{num-agree}] = (\\overline{x}^2 +  (1-\\overline{x})^2)\\sum^{n}_i\\sum^{n}_j w_{ij},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"E[\\text{num-agree}]=(\\overline{x}^{2}+(1-\\overline{x})^{2})\\sum^{n}_{i}\\sum^{n%&#10;}_{j}w_{ij},\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mtext>num-agree</mtext><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo>\u00af</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\nwhere $\\mathbf{tr}$ indicates the matrix trace, $\\mathbf{tr} A = \\sum_i A_{ii}$ and,\n\n", "itemtype": "equation", "pos": 27770, "prevtext": " \n\nThe variance of the number of agreements under the null hypothesis can also be obtained in closed form, enabling the computation of a $z$-statistic and $p$-value for the null hypothesis of total randomization. \n\nBecause PPA is based on counts of agreements, it requires that each $x_i$ is a categorical variable --- possibly non-binary --- rather than a frequency. In this sense, it is the inverse of Moran's I, which can be applied to frequencies, but not to non-binary variables. Thus, PPA is best suited to cases where observations correspond to individual utterances (e.g., Twitter data, dialect interviews), rather than cases where observations correspond to longer texts (e.g., newspaper corpora). \n\n\\subsection{The Mantel Test}\nMoran's I and Point Pattern Analysis are both asymmetric in their treatment of the geographical and linguistic variables --- they measure similarity on the linguistic variable between all points that are sufficiently close in space. The Mantel test takes a more symmetric approach, and can in principle be used to measure the dependence between any two arbitrary signals. Let us compute \\emph{distances} for each pair of linguistic variables, $d_x(x_i,x_j)$, and each pair of spatial locations, $d_y(y_i,y_j)$, forming a pair of distance matrices $D_x$ and $D_y$. The Mantel test then measures the element-wise correlation (usually, the Pearson correlation) between these two matrices.  Scherrer \\cite{scherrer:2012:LINGVIS2012} used the Mantel test to correlate linguistic distance with geographical distance, and Gooskens and Heeringa \\cite{Gooskens01112006} correlated perceptual distance with linguistic distance.\\footnote{The Mantel test has also been applied to non-human dialect analysis, revealing regional differences in the call structures of Amazonian parrots by computing a linguistic distance matrix $D_x$ directly from spectral measurements~\\cite{wright1996regional}.}\n\nAs usual, the goal of hypothesis testing is to determine the likelihood that the observed test statistic --- in this case, the correlation between $D_x$ and $D_y$ --- could have arisen by chance. To assess the distribution of correlations under the null hypothesis, we randomly permute the rows or columns of one of the matrices repeatedly. If the correlation between the unpermuted $D_x$ and $D_y$ is consistently higher than the correlations under permutation, then the null hypothesis is unlikely to hold.\n\nA design question for the Mantel test is the choice of the distance functions for $X$ and $Y$. For spatial locations, we compute the distance matrix based on the Euclidean distance between each pair of points. For binary or categorical linguistic data, the entries of the linguistic distances matrix are set to $0$ if $x_i = x_j$, and $1$ otherwise. For linguistic frequency data, we use the absolute difference between the frequency values.\n\n\\iffalse\nA design question for the Mantel test is the choice of the similarity functions for $X$ and $Y$. In this study, we use a kernelized variant of the Mantel Test to be consistent with our proposed method, HSIC.\\footnote{An alternative formulation of the Mantel test employs distance functions rather than similarities. We tried this in pilot studies, but did not find substantial empirical differences from the kernelized variant described here.} We compute the similarity between the spatial locations using the radial basis function (RBF) $e^{-\\gamma d^2_{i,j}}$, where $d_{i,j}$ is again the Euclidean distance between $y_i$ and $y_j$, and $\\gamma$ is a kernel bandwidth parameter that controls the rate at which the kernel function decreases with distance. In the case of categorical linguistic data, the entries of the linguistic similarity matrix $S_x$ can be set to the delta function $\\delta(x_i, x_j)$, which takes the value $1$ if $x_i = x_j$ and $0$ otherwise. In the case of frequency-based linguistic data, we again use the RBF kernel. \n\n\\fi\n\nThe Mantel test is more flexible than {Moran's~I}\\ or Point Pattern Analysis: it is applicable to binary, categorical, and frequency data, and does not require the specification of a geographical distance threshold. However, by focusing on correlations between similarities, it makes an implicit parametric assumption: in the ideal case of perfect correlation, twice as much geographical distance should imply twice as much as linguistic distance. Yet a range of dialectometric studies have found that linguistic differences increase sublinearly with geographical distance, a phenomenon that Nerbonne has dubbed ``Seguy's law''~\\cite{nerbonne2010measuring}. On this view, the parametric assumption of linear dependence between geographical and linguistic distance is incorrect. A second concern is that human settlement patterns are highly variable, so that a distance of, say, 100 kilometers may be far more linguistically meaningful in a densely-populated urban area like New England than it would be in a more sparsely-populated region like the American West. If the assumptions underlying Mantel's test --- linearity and homogeneity  --- are incorrect, then the test will be \\emph{underpowered}, failing to detect meaningful relationships in the data.\n\n{}\n\\subsection{Hilbert-Schmidt Independence Criterion (HSIC)}\nThe discussion of existing tests for spatial dependence has helped to identify some desiderata. The ideal test would be applicable across many kind of linguistic data, including both binary and categorical variables, and both frequencies and discrete observations. The Mantel test solves these problems, but makes unsupported parametric assumptions about the nature of the relationship between linguistic and spatial distance. Rather than measuring the correlation of linguistic variables with a single, thresholded distance function (as in Moran's I and PPA) or a single correlation matrix (as in the Mantel test), we might prefer to model non-linear relationships, perhaps by considering higher-order moments ($x^2, x^3, \\ldots$) or other transformations of the spatial distances.\n\nBoth of these problems can be solved through the use of Reproducing Kernel Hilbert Spaces (RKHS), a family of techniques from non-parametric statistics, capable of capturing arbitrary statistical dependencies~\\cite{gretton2005measuring}. Specifically, the Hilbert-Schmidt Independence Criterion (HSIC) provides a robust test for statistical dependence of two signals, which is simple to implement, and involves only a single tunable parameter.\n\nAt the core of RKHS-based techniques is the kernel function on pairs of instances. Let $k(x_i,x_j) : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}_+$ represent a function from pairs $(x_i, x_j)$ to non-negative real numbers; let us also assume that $k(x_i,x_j) = k(x_j,x_i)$, so that $k$ can be thought of as a measure of similarity.\nNow suppose we have a set of inputs $\\{{\\mathbf{{x}}}_i\\}_{i\\in\\{1\\ldots {n}\\}}$, and we construct a matrix $K$, such that $K_{i,j} = k(x_i,x_j)$, known as the \\emph{Gram matrix}. For appropriately chosen kernel functions,\\footnote{Specifically, we require that $K$ is symmetric, meaning that $K_{ij} = K_{ji}$ for all $i$ and $j$, and positive definite, meaning that ${{{\\mathbf{{a}}}}^{\\top}} K {\\mathbf{{a}}} > 0$ for all vectors ${\\mathbf{{a}}}$. One test for positive definiteness is that the eigenvalues of $K$ must all be positive.} \nMercer's theorem guarantees that there exists some feature function $\\phi({\\mathbf{{x}}}) : \\mathcal{X} \\to \\mathbb{R}^D$ such that $k({\\mathbf{{x}}}_i, {\\mathbf{{x}}}_j) = {{\\phi({\\mathbf{{x}}}_i)}^{\\top}} \\phi({\\mathbf{{x}}}_j)$. The dimension $D$ of the feature function may be very large, even infinite; for example, the feature function may correspond to an infinite series, $[x, x^2, x^3, \\ldots]$. Nonetheless, by working with the kernels (rather than directly with the feature functions), we can compute the inner product (and therefore the covariance) directly from the kernel function.\n\nTo make things more concrete, let us define the spatial kernel function $k_\\gamma(y_i,y_j) = e^{-\\gamma d^2_{ij}}$, where $d^2_{ij}$ is the squared Euclidean distance between $y_i$ and $y_j$, and $\\gamma$ is a parameter of the kernel function. Similarly, for linguistic frequency data, let $\\ell_\\iota(x_i,x_j) = e^{-\\iota(x_i - x_j)^2}$. This linguistic kernel function is suitable for continuous data, such as values of acoustic variables, and can also capture difference in frequencies of a binary linguistic variable. These exponentially decaying kernel functions are known as \\emph{radial basis functions} (RBFs), and are guaranteed to be symmetric and positive definite, satisfying the conditions of Mercer's theorem. The RBF kernel corresponds to a inner product between infinite-dimensional feature vectors $\\phi$~\\cite{murphy2012machine}. Thus, although the RBF kernel function is based on distances, the resulting feature map includes non-linear transformations of these distances, and therefore the approach is \\emph{not} equivalent to simply correlating linguistic and geographical distances. In the case of binary and categorical data, we use a Delta kernel, where $\\ell_\\iota(x_i,x_j)  = 1$ if $x_i = x_j$ and 0 otherwise. The Delta kernel has been used successfully in combination with HSIC for high-dimensional feature selection~\\cite{Song2012,yamada2014high}.\n\nNow, if we compute the kernel functions over all pairs of observations, we obtain the Gram matrices $K$ and $L$, where $K_{ij} = k(y_i,y_j)$ and $L_{ij} = \\ell(x_i,x_j)$, eliding the parameters $\\gamma$ and $\\iota$ for clarity. \n\n\nThe Hilbert-Schmidt Independence Criterion (HSIC) is a nonparametric measure of the dependence between $X$ and $Y$~\\cite{gretton2005measuring}, and an empirical estimator is given by,\n\n", "index": 11, "text": "\\begin{equation}\n{\\text{HSIC}}(x,y) = \\frac{\\mathbf{tr} KHLH}{{n}^2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\text{HSIC}}(x,y)=\\frac{\\mathbf{tr}KHLH}{{n}^{2}},\" display=\"block\"><mrow><mrow><mrow><mtext>HSIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>\ud835\udc2d\ud835\udc2b</mi><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mi>H</mi></mrow><msup><mi>n</mi><mn>2</mn></msup></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\n\n\\noindent With this definition of $H$, we have,\n\n", "itemtype": "equation", "pos": 27940, "prevtext": "\nwhere $\\mathbf{tr}$ indicates the matrix trace, $\\mathbf{tr} A = \\sum_i A_{ii}$ and,\n\n", "index": 13, "text": "\\begin{equation}\nH_{ij} = \\begin{cases}\n1 - 1/{n}, & i =j \\\\\n-1/{n}, & i \\neq j.\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"H_{ij}=\\begin{cases}1-1/{n},&amp;i=j\\\\&#10;-1/{n},&amp;i\\neq j.\\end{cases}\" display=\"block\"><mrow><msub><mi>H</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\nThese two terms can therefore be seen as mean-centered Gram matrices. By computing the trace of their matrix product, we obtain a cross-covariance between the Gram matrices. If the two data sources are independent, the expectation of this cross-covariance is zero. \n\nAn important implementation detail is that the size of each Gram matrix is the square of the number of observations, which for large data will be too expensive to compute. Following Gretton {\\textit{et al.}}\\  \\cite{gretton2005measuring}, we employ a low-rank approximation to each Gram matrix, using the incomplete Cholesky decomposition~\\cite{bach2002kernel}. Specifically, we approximate the symmetric matrices $K$ and $L$ as low-rank products, $K \\approx A A^T$ and $L \\approx B B^T$, where $A \\in \\mathbb{R}^{n \\times r_A}$ and $B \\in \\mathbb{R}^{n \\times r_B}$. The approximation quality is determined by the parameters $r_A$ and $r_B$, which are set to ensure that the  magnitudes of the residuals $K - AA^T$ and $L - BB^T$ are below a predefined threshold. HSIC may then be approximated as:\n \n\n", "itemtype": "equation", "pos": 28097, "prevtext": "\n\n\\noindent With this definition of $H$, we have,\n\n", "index": 15, "text": "\\begin{align}\n (KH)_{ij} = & k(y_i,y_j) -  \\frac{1}{{n}}\\sum_{j'}k(y_i,y_{j'})\\\\\n (LH)_{ij} = & \\ell(x_i,x_j) - \\frac{1}{{n}}\\sum_{j'}\\ell(x_i,x_{j'}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(KH)_{ij}=\" display=\"inline\"><mrow><msub><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mi>H</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle k(y_{i},y_{j})-\\frac{1}{{n}}\\sum_{j^{\\prime}}k(y_{i},y_{j^{%&#10;\\prime}})\" display=\"inline\"><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msup><mi>j</mi><mo>\u2032</mo></msup></munder></mstyle><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><msup><mi>j</mi><mo>\u2032</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(LH)_{ij}=\" display=\"inline\"><mrow><msub><mrow><mo stretchy=\"false\">(</mo><mrow><mi>L</mi><mo>\u2062</mo><mi>H</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ell(x_{i},x_{j})-\\frac{1}{{n}}\\sum_{j^{\\prime}}\\ell(x_{i},x_{j^{%&#10;\\prime}}).\" display=\"inline\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msup><mi>j</mi><mo>\u2032</mo></msup></munder></mstyle><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><msup><mi>j</mi><mo>\u2032</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06579.tex", "nexttext": "\nwhere the matrix product $HA$ can be computed without explicitly forming the ${n} \\times {n}$ matrix $H$, due to its simple structure.\n\nThe HSIC has several advantages. It can be computed for either frequency or count data, as long as an appropriate kernel function can be defined. It offers a parallel treatment of $X$ and $Y$, thereby avoiding the use of arbitrary thresholds in the construction of a spatial neighborhood matrix $W$. A potential concern is the selection of the kernel bandwidth parameters $\\gamma$ and $\\iota$. We will test the sensitivity to these parameters in the next section, and demonstrate a simple heuristic that works well in practice.\\footnote{Flaxman recently proposed a ``kernelized Mantel test'', in which correlations are taken between kernel similarities rather than distances~\\cite{flaxman2015machine}. The resulting test statistic is similar, but not identical to HSIC. Specifically, while HSIC centers the kernel matrix against the local mean kernel similarities for each point, the kernelized Mantel test centers against the global mean kernel similarity. This makes the test more sensitive to distant outliers. We implemented the kernelized Mantel test, and found its performance to be similar to the classical Mantel test, with lower statistical power than HSIC. Flaxman made similar observations in his analysis of the spatiotemporal distribution of crime events.}\n\n\\subsection{Computing Significance Tests}\nHSIC and the Mantel test are applicable to all types of data, but {Moran's~I}\\ is not applicable to categorical data and PPA is not applicable to frequency data. For all approaches, a one-tailed significance test is appropriate, since in nearly all conceivable dialectological scenarios we are testing only for the possibility of a \\emph{higher} clustering in geographical space than chance; however, the generalization to two-tailed tests is trivial. For some methods, it is possible to calculate the variance of the test statistic in closed form. However, for consistency, we employ a permutation approach to characterize the null distribution over the test statistic values. We permute the linguistic data $x$, breaking any link between geography and the language data, and then compute the test statistics for many such permutations. \n\n{}\n\\section{Synthetic Data}\n\\label{sec:synth}\nReal linguistic datasets lack ground truth about which features are geographically distinct, making it impossible to use such data to quantitatively evaluate the proposed approaches. We therefore use synthetic data to compare the power and sensitivity of the various approaches described in the previous section. Our main goals are: (1) to calibrate the $p$-values produced by each approach in the event that the null hypothesis is true, using completely randomized data; (2) to test the ability of each approach to capture spatial dependence, particularly under conditions in which the spatial dependence is obscured by noise.\n\n\\subsection{Data Generation}\n\\label{sec:data_generation}\nTo ensure the verisimilitude of our synthetic data, we target the scenario of geo-tagged tweets in the Netherlands. For each municipality $i$, we stochastically determine the number and location of the tweets as follows:\n\\begin{description}\n\\item[Number of data points] For each municipality, the number of tweets ${n}_i$ is chosen to be proportional to the population, as estimated by Statistics Netherlands (CBS). Specifically, we draw $\\tilde{{n}}_i \\sim  \\text{Poisson}(\\mu_{obs} * \\text{population}_i)$ and \nthen set ${n}_i = \\tilde{{n}_i} + 1$, ensuring that each municipality has at least one data point. The parameter $\\mu_{obs}$ controls the frequency of the linguistic variable. For example, a common orthographic variable (e.g., ``g-deletion'') might have a high value of $\\mu_{obs}$, while a rare lexical variable (e.g., {\\textit{{soda}}} versus {\\textit{{pop}}}) might have a much lower value. Note that $\\mu_{obs}$ is shared across all municipalities.\n\\item[Locations] Next, for each tweet $t$, we determine the location $y_t$ by sampling without replacement from the set of real tweet locations in municipality $i$ (the dataset is described in \\autoref{sec:twitter}). This ensures that the distribution of geo-locations in the synthetic data matches the real geographical distribution of tweets, rather than drawing from a parametric distribution which may not match the complexity of true geographical population distributions. Each location is represented as a latitude and longitude pair.\n\\end{description}\n\nFor each variable, each municipality is assigned a frequency vector $\\theta_i$, indicating the relative frequency of each variable form: e.g., 70\\% {\\textit{{soda}}}, 30\\% {\\textit{{pop}}}. We discuss methods for setting $\\theta_i$ below, which enable the simulation of a range of dialectal phenomena. \n\nWe simulate both counts data and frequency data. In counts data --- such as geotagged tweets --- the data points in each instance in municipality $i$ are drawn from a binomial or multinomial distribution with parameter $\\theta_i$. In frequency data, we observe only the relatively frequency of each variable form for each municipality. In this case, we draw the frequency from a Dirichlet distribution with expected value equal to $\\theta_i$, drawing $\\phi_t \\sim \\text{Dirichlet}(s \\theta_i)$, where the scale parameter $s$ controls the variance within each municipality.\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.5\\textwidth}\n  \\centering\n  \\includegraphics[width=0.5\\linewidth]{plots/syn_freq_linear_0_degrees_a_200.png}\n  \\caption{Angle: 0 degrees (east to west)}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=0.5\\linewidth]{plots/syn_freq_linear_120_degrees_a_200.png}\n  \\caption{Angle: 120 degrees}\n\\end{subfigure}\n\\caption{Synthetic frequency data with linear variation in two different angles} \n\\label{fig:linear_angles}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{}\n\n\n\n\n\n\\subsection{Calibration}\n\\label{sec:calibration_p_values}\nOur first use of synthetic data is to examine the $p$-values obtained from each method when the null hypothesis is true --- that is, when there is no geographical variation in the data. The $p$-value corresponds to the likelihood of seeing a test statistic at least as extreme as the observed value, under the null hypothesis. Thus, if we repeatedly generate data under the null hypothesis, a well-calibrated test will return a distribution of $p$-values that is uniform in the interval $[0,1]$: for example, we expect to observe $p < .05$ in exactly 5\\% of cases, corresponding to the allowed rate of Type I errors (incorrect rejection of the null hypothesis) at the threshold $\\alpha = 0.05$. \n\n\n\n\n\n\n\nTo measure the calibration of each of the proposed tests, we generate 1000 random datasets using the procedure described above, and then compute the $p$-values under each test. In these random datasets, the relative frequency parameters $\\theta_{i}$ are the same for all municipalities, which is the null hypothesis of complete randomization. To generate the binary and categorical data, we use $\\mu_{obs}=10^{-5}$, meaning that the expected number of observations is one per hundred thousand individuals in the municipality or province; for comparison, this corresponds roughly to the tweet frequency of the lengthened spelling {\\textit{{hellla}}} in the 2009-2012 Twitter dataset gathered by Eisenstein et al~\\cite{eisenstein2014diffusion}.\n\n\n\nTo visualize the calibration of each test, we use quantile-quantile (Q-Q) plots, comparing the obtained $p$-values with a uniform distribution. A well-calibrated test should give a straight line from the origin to $(1,1)$. \\autoref{fig:random_data} shows the Q-Q plots obtained from each method on each relevant type of data (recall that not all methods can be applied to all types of data, as described in the previous section). \n\n{\\text{HSIC}}~and {Moran's~I}\\ each have tuning parameters that control the behavior of the test: the kernel bandwidth in {\\text{HSIC}}\\, and the distance cutoff in {Moran's~I}. A simple heuristic is to use the median Euclidian distance $\\overline{d}$: in {Moran's~I}, we use $\\overline{d}$ as the distance threshold for constructing the neighborhood matrix $W$; in {\\text{HSIC}},\n\nwe use $\\frac{1}{\\overline{d}^2}$ as the kernel bandwidth parameter. \\autoref{fig:random_data} shows that by basing these parameters on the median distance between pairs of points, we get well-calibrated results. However, some prior work takes an alternative approach, sweeping over parameter values to obtain the most significant results~\\cite{grieve2011statistical}. In our experiments we sweep across the distance cutoff for {Moran's~I}, and the bandwidth for the spatial distances in HSIC. This badly distorts the calibration, particularly for {Moran's~I}, meaning that the resulting $p$-values are not reliable.  This is most severe for Moran's I on the municipality level, reaching type I error rates of $11.7\\%$ (binary data) and $14.3\\%$ (frequency data) when the significance threshold $\\alpha$ is set to $5\\%$. Given that such parameter sweeps are explicitly designed to maximize the number of positive test results --- and not the overall calibration of the test --- this is unsurprising. We therefore avoid parameter sweeps in the remainder of this article, and rely instead on median distance as a simple heuristic alternative.\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}[t]{0.31\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/random_00001}\n  \\caption{Binary data}\n\\end{subfigure}\n\\hspace{0.1in}\n\\begin{subfigure}[t]{.31\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/random_multi_00001}\n  \\caption{Categorical data}\n\\end{subfigure}\n\\hspace{0.1in}\n\\begin{subfigure}[t]{.31\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/random_freq}\n  \\caption{Frequency data\\\\}\n\\end{subfigure}\n\\caption{Quantile-quantile plots comparing the distribution of the obtained $p$-values with a uniform distribution. The y-axis is the $p$-value\nreturned by the tests. The x-axis shows the corresponding quantile\nfor a uniform distribution on the range [0,1]. The approaches that optimize the parameters, i.e. the cutoff for Moran's I (MI) and the bandwidth for HSIC (H), lead to a skewed distribution of $p$-values. } \n\\label{fig:random_data}\n\\end{figure}\n\n{}\n\\subsection{Power}\nNext, we consider synthetic data in which there is geographical variation by construction. We assess the \\emph{power} of each approach by computing the fraction of simulations for which the approaches correctly rejected the null hypothesis of no spatial dependence, given a significance threshold of $\\alpha=0.05$.\nWe again use the Netherlands as the stage for all simulations, and consider two types of geographical variation. \n\\begin{description}\n\\item[Linear variation] We generate data such that the frequency of a linguistic variant increases linearly through space, as in a dialect continuum~\\cite{heeringa2001dialect}. In most of the synthetic data experiments below, we average across a range of angles, from $0^\\circ$ to $357^\\circ$ with step sizes of $3^\\circ$, yielding 120 distinct angles in total. Each angle aligns differently with the population distribution of the Netherlands, so we also assess sensitivity of each method to the angle itself.\n\\item[Centers] Second, we consider a setting in which variation is based on one or more geographical \\emph{centers}. In this setting, all cities within some specified range of the center (or one of the centers) have some maximal frequency value $\\theta_i$; in other cities, this value decreases as distance from the nearest center grows. This corresponds to the dialectological scenario in which a variable form is centered on one specific city, as in, say, the association of the word {\\textit{{hella}}} with the San Francisco metropolitan area. We average across twenty five possible centers: the capitals of each of the twelve provinces of the Netherlands; the national capital of the Netherlands (Amsterdam); the \\emph{two} most populous cities in each of the twelve provinces. For each setting, we randomly generate synthetic data four times, resulting in a total of 100 synthetic datasets for this condition.\n\\end{description}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_linear_bandwidth_hsic_abs}\n  \\caption{HSIC: Linear variation}\n  \\label{fig:hsic-power-param-linear}\n\\end{subfigure}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_center_bandwidth_hsic_abs}\n  \\caption{HSIC: Centers}\n  \\label{fig:hsic-power-param-center}\n\\end{subfigure}\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_linear_bandwidth_morans_i_abs}\n  \\caption{{Moran's~I}: Linear variation}\n  \\label{fig:mi-power-param-linear}\n\\end{subfigure}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_center_cutoff_morans_i_abs}\n  \\caption{{Moran's~I}: Centers}\n  \\label{fig:mi-power-param-center}\n\\end{subfigure}\n\\caption{Power across different parameter settings. Higher values indicate a greater likelihood of correctly rejecting the null hypothesis.}\n\\label{fig:power-params}\n\\end{figure}\n\n\\paragraph{Parameter settings}  \n\nWe use these data generation scenarios to test the sensitivity of {\\text{HSIC}}\\ and {Moran's~I}\\ to their hyperparameters, by varying the kernel bandwidths in {\\text{HSIC}}\\  (Figures~\\ref{fig:hsic-power-param-linear} and \\ref{fig:hsic-power-param-center}) and the distance threshold in {Moran's~I}\\ (Figures~\\ref{fig:mi-power-param-linear} and \\ref{fig:mi-power-param-center}). The sensitivity of {\\text{HSIC}}\\ to the bandwidth value decreases as the number of data points increases  (as governed by $\\mu_{obs}$), especially in the case of linear variation. \nThe sensitivity of {Moran's~I}\\ to the distance cutoff value decreases with the amount of data in the case of linear variation, but in the case of center-based variation, {Moran's~I}\\ becomes \\emph{more} sensitive to this parameter as there is more data. For both methods, the same trends regarding the best performing parameters can be observed. In the case of linear variation, larger cutoffs and bandwidths perform best, but in the case of variation based on centers, smaller cutoffs and bandwidths lead to higher power. Overall, there is no single best parameter setting, but the median heuristics perform reasonably well for both types of variation.\n\n\n\n\\iffalse\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_linear_bandwidth_morans_i_abs}\n  \\caption{Linear variation}\n\\end{subfigure}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=0.74\\linewidth]{plots/syn_2cat_center_cutoff_morans_i_abs}\n  \\caption{Centers}\n\\end{subfigure}\n\n\\caption{{Moran's~I}\\ power across different distance cutoffs}\n\\label{fig:morans_i_cutoffs_abs}\n\\end{figure}\n\\fi\n\n\\paragraph{Angle of linear variation} \nWe simulate dialect continua by varying the frequency of linguistic variables linearly through space. Due to the heterogeneity of population density, different spatial angles will have very different properties: for example, one choice of angle would imply a continuum cutting through several major cities, while another choice might imply a rural-urban distinction. \\autoref{fig:2cat_angle_linear} shows the power of the methods on binary data (there are two variant forms, and each instance contains exactly one of them), in which we vary the angle of the continuum. {\\text{HSIC}}\\ is insensitive to the angle of variation, demonstrating the advantage of this kernel nonparametric method. {Moran's~I}\\ is relatively robust, while Point Pattern Analysis (PPA) performs poorly across the entire range of settings. The Mantel test is remarkably sensitive to the angle of variation, attaining nearly zero power for some scenarios of linear variation. \nThis is caused by the complex interaction between the underlying linguistic phenomenon and the east-west variation of the population density of the Netherlands. For example, when the dialect continuum is simulated at an angle of 105 degrees, the south east of the Netherlands has a higher usage of the variable, but this is only a very small region due to the shape of the country. The Mantel test apparently has great difficulty in detecting geographical variation in such cases.\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=.32\\linewidth]{plots/2cat_angle}\n  \\caption{Relationship between the statistical power of each test and the angle of linear variation across the Netherlands.}\n\\label{fig:2cat_angle_linear}\n\\end{figure}\n\n\\paragraph{Outliers} \nIn the frequency-based synthetic data, each instance uses each variable form with some continuous frequency --- this is based on the scenario of letters-to-the-editors of regional newspapers, as explored in prior work~\\cite{grieve2011statistical}. We test the robustness of each approach by introducing \\emph{outliers}: randomly selected individuals whose variable frequencies are replaced at random with extreme values of either $0$ or $1$. As shown in \\autoref{fig:synthetic_freq_noise}, {\\text{HSIC}}\\ is the most robust against outliers, while the performance of {Moran's~I}\\ is the most affected by outliers (recall that PPA applies only to discrete observations, so it cannot be compared on this measure).\n \n \n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.35\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_freq_linear_noise}\n  \\caption{Linear variation}\n\\end{subfigure}\n\\begin{subfigure}{0.35\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_freq_centers_noise.pdf}\n  \\caption{Centers}\n\\end{subfigure}\n\n\\caption{Results on synthetic frequency data ($\\sigma=0.1$) with outliers}\n\\label{fig:synthetic_freq_noise}\n\\end{figure}\n\n\\paragraph{Overall}\nWe now compare the methods by averaging across various settings simulating linear variation (Figure \\ref{fig:linear_average})\nand variation based on centers (Figure \\ref{fig:centers_average}).\nTo generate the categorical data, we vary $\\mu_{obs}$ in our experiments, with a higher $\\mu_{obs}$ resulting in more tweets and consequently less variation on the municipality level. \n\n\nAs expected, the power of the approaches increases as $\\mu_{obs}$ increases in the experiments on the categorical data, and the power of the approaches decreases as $\\sigma$ increases in the experiments on  the frequency data. The experiments on the binary and categorical data show the same trends: {\\text{HSIC}}\\ performs the best across all settings. {PPA}\\ does well when the variation is based on centers, and {Moran's~I}\\ does best when the variation is linear. {Moran's~I}\\ performs best on the frequency data, especially in the case of variation based on centers. \n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_2cat_angles}\n  \\caption{Binary data}\n\\end{subfigure}\n\\begin{subfigure}{.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_multi_angles}\n  \\caption{Categorical data (3)}\n\\end{subfigure}\n\\begin{subfigure}{.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_freq_angles}\n  \\caption{Frequency data}\n\\end{subfigure}\n\\caption{Linear variation}\n\\label{fig:linear_average}\n\\end{figure}\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_2cat_centers_prov}\n  \\caption{Binary data}\n\\end{subfigure}\n\\begin{subfigure}{.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_multi_centers_prov}\n  \\caption{Categorical data (3)}\n\\end{subfigure}\n\\begin{subfigure}{.3\\textwidth}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{plots/syn_freq_centers_prov}\n  \\caption{Frequency data}\n\\end{subfigure}\n\\caption{Centers}\n\\label{fig:centers_average}\n\\end{figure}\n\n\n\n\n\n\\subsection{Summary}\nIn this section, we evaluated each statistical test for geographical language variation on a battery of synthetic data. \n{\\text{HSIC}}\\ and the Mantel test are the only approaches applicable to all data types (binary, categorical and frequency data). Overall, {\\text{HSIC}}\\ is more effective than the Mantel test, which is much more sensitive to the specifics of the synthetic data scenario, such as the angle of the dialect continuum. {\\text{HSIC}}\\ is robust against outliers, and performs particularly well when the number of data points increases \nand it is robust against outliers. {PPA}\\ is suitable for capturing non-linear variation, but its power is low compared to other approaches in situations of linear variation. On the other hand, {Moran's~I}\\ performs well on binary data with linear variation, but its power is low in situations of variation based on centers. In our experiments on frequency data, where the other approaches also directly deal with frequency values, Moran's I performed well.\n\n{}\n\\section{Empirical Data}\n\\label{sec:real}\nWe now assess the spatial dependence of linguistic variables on three real linguistic datasets: letters to the editor (English), syntactic atlas of the Dutch dialects, and Dutch geotagged tweets. To account for multiple hypothesis testing, we use the Benjamini-Hochberg false discovery rate procedure to adjust the $p$-values~\\cite{Benjamini1995}.\n\n\\subsection{Letters to the Editor}\nIn their application of {Moran's~I}\\ to English dialects in the United\nStates, Grieve {\\textit{et al.}}\\:compile a corpus of letters-to-the-editors of\nnewspapers to measure the presence of dialect variables  in text.  To\ncompute the frequency of the lexical variables, letters are\naggregated to core-based statistical areas (CBSA), which are\ndefined by the United States to capture the geographical region around\nan urban core. The frequency of 40 manually selected lexical variables is computed for each of 206 cities. \n\nWe use the Mantel test, {\\text{HSIC}}, and {Moran's~I}\\ to assess the spatial dependence of variables in this dataset. {PPA}\\ was excluded from the analysis, because it is not suitable for frequency data. We verified our implementation of {Moran's~I}\\ by following the approach taken by Grieve {\\textit{et al.}}: we computed {Moran's~I}\\ for cutoffs in the range of 200 to 1000 miles and selected the cutoff that yielded the lowest $p$-value. The obtained cutoffs and test statistics closely followed the values reported in the analysis by Grieve {\\textit{et al.}}, with slight deviations possibly due to our use of a permutation test rather than a closed-form approximation to compute the $p$-values.\n\nAfter adjusting the $p$-values using the false discovery rate (FDR) procedure, a 500-mile cutoff results in three significant linguistic variables.\\footnote{Grieve {\\textit{et al.}}\\ report five significant variables. In our analysis, there are two variables with FDR-adjusted $p$-values of 0.0559} However, recall that the approach of selecting parameters by maximizing the number of positive test results tends to produce a large number of Type I errors. When setting the distance cutoff to the median distance between data points, none of the linguistic variables were found to have a significant geographical association. Similarly, {\\text{HSIC}}\\ and the Mantel test also found no significant associations after adjusting for multiple comparisons. Figure \\ref{lvc_thresholds} shows the proportion of significant variables according to {Moran's~I}\\ based on different thresholds. The numbers vary considerably depending on the threshold. The figure also suggests that the median distance (921 miles) may not be a suitable threshold for this dataset.\n\n\\begin{figure}\n \\includegraphics[scale=0.19]{plots/lvc_thresholds.pdf}\n \\caption{The proportion of variables detected to be significant ($p$<.05) by Moran's I\n by varying the distance threshold (without  adjusting for multiple comparisons).}\n \\label{lvc_thresholds}\n\\end{figure}\n\n\\subsection{Syntactic Atlas of the Dutch Dialects (SAND)}\nSAND \\cite{barbiers2005syntactic,barbiers2008} is an online electronic atlas that maps syntactic variation of Dutch varieties in the Netherlands, Belgium, and France.\\footnote{http://www.meertens.knaw.nl/sand/} The data was collected between the years of 2000 and 2005. SAND has been used to measure the distances between dialects, and to discover dialect regions \\cite{Spruit01112006,tks2015edisyn}.\n\nIn our experiments, we consider only locations within the Netherlands (157 locations). The number of variants per linguistic variable ranges from one (due to our restriction to the Netherlands) to eleven.\nWe do not include {Moran's~I}\\ in our experiments, since it is not applicable to linguistic variables with more than two variants. We apply the remaining methods to all linguistic variables with twenty or more data points and at least two variants, resulting in a total of 143 variables.\n\nTable \\ref{tab:sand_hsic_top} lists the 10 variables with the highest {\\text{HSIC}}\\ values. Statistical significance at a level of $\\alpha=0.05$ is detected for 65.0\\% of the linguistic variables using {\\text{HSIC}}, 78.3\\% when using {PPA}, and 52.4\\% when using the Mantel test. The three methods agree on 99 out of the 143 variables, and {\\text{HSIC}}\\ and {PPA}\\ agree on 118 variables.  \n\n\nBased on manual inspection, it seems that the non-linearity of the geographical patterns may have caused difficulties for the Mantel test. For example, \\autoref{fig:sand_mantel_example} is an example of a variable where {\\text{HSIC}}\\ and {PPA}\\ both had an adjusted $p$-value $<$ .05, but the Mantel test did not detect a significant pattern. Note the uneven dispersion of the data points and the clusters of subject relatives (purple data points) in both the north and south of the Netherlands, which does not match the linearity assumption employed by the Mantel Test.\n\n\\begin{figure}\n   \\CenterFloatBoxes\n\\begin{floatrow}\n\n\\capbtabbox{\n    \\centering\n    \\small\n\\begin{tabular}{llrrr}\n\\toprule\n\\textbf{Map id} & \\textbf{Description}\\\\\n\\toprule\n1:84b & \\specialcell[t]{Free relative, complementizer \\\\following relative pronoun} \\\\\n1:84a &  \\specialcell[t]{Short subject and object relative, \\\\complementizer following relative\\\\ pronoun}\\\\ \n1:80b &ONE pronominalisation\\\\\n1:33a & Complementizer agreement 3 plural \\\\\n1:76a & Reflexive pronouns; synthesis \\\\\n2:36b & \\specialcell[t]{ Form of the participle of \\\\the modal verb willen `want'}\\\\\n1:29a & Complementizer agreement 1 plural\\\\\n1:69a & Correlation weak reflexive pronouns\\\\\n2:30b & \\specialcell[t]{Interruption of the verbal cluster;\\\\ synthesis I}\\\\\n2:61a & Forms for iemand `somebody'\\\\\n\\hline\n\\end{tabular}\n}{\n\\caption{Highest ranked variables by HSIC. All methods had an adjusted $p$-value $<.05$ for all variables,\nexcept variable 2:30b (Mantel: $p=.118$.}\n\\label{tab:sand_hsic_top}\n}\n\\ffigbox{\n \\includegraphics[scale=0.3]{plots/sand_1_16b_nl.png}\n}{\n \\caption{SAND map 16b, book 1. \\\\Finite complementizer(s) following \\\\relative pronoun (N=112).\\\\\n  HSIC: $p$=.024; Mantel: $p$=.506;\\\\ PPA: $p$=.002.\n }.\n\n \\label{fig:sand_mantel_example}\n}\n\\end{floatrow}\n\\end{figure}\n\n\n\n\n{}\n\\subsection{Twitter}\n\\label{sec:twitter}\nOur Twitter dataset consists of 4,039,786 geotagged tweets from the Netherlands, written between January 1, 2015 and October 31, 2015. \nWe manually selected a set of linguistic variables (\\autoref{tab:results_twitter}), covering examples of lexical variation (e.g., two different words for referring to french fries),\nphonological variation (e.g., t-deletion), and syntactic variation (e.g., {\\textit{{heb gedaan}}} (`have done') vs. {\\textit{{gedaan heb}}} (`done have')). We are not aware of any previous work on dialectal variation in the Netherlands that uses spatial dependency testing on Twitter data. The number of tweets per municipality varies dramatically, and for the less frequent linguistic variables there are no tweets at all in some municipalities. In our computation of {Moran's~I}, we only include municipalities with at least one tweet.\n\n\\autoref{tab:results_twitter} shows the output of each statistical test for this data. Some of these linguistic variables\nexhibit strong spatial variation, and are identified as statistically significant by all approaches. An example is the different ways of referring to french fries ({\\textit{{friet}}} versus {\\textit{{patat}}}, Figure \\ref{fig:friet}), where the figure shows a striking difference between the south and the north of the Netherlands. \nAnother example is Figure \\ref{fig:efkes} , which shows two different ways of saying `for a little while' ({\\textit{{efkes}}} versus {\\textit{{eventjes}}}). The less common form, {\\textit{{efkes}}} is mostly used in Friesland, a province in the north of the Netherlands. \n\nExamples of linguistic variables where the approaches disagree are shown in Figure \\ref{fig:twitter_sign_disagreement}.\nThe first case (Figure \\ref{fig:doei_houdoe}) is an example of lexical variation, with two different ways of saying {\\textit{{bye}}} in the Netherlands. A commonly used form is {\\textit{{doei}}}, while {\\textit{{houdoe}}} is known to be specific to North-Brabant, a Dutch province in the south of the Netherlands. {\\text{HSIC}}\\ and {PPA}\\ both detect a significant pattern, but {Moran's~I}\\ and the Mantel test do not. The trend is less strong than in the previous examples, but the figure does suggest a higher usage of {\\textit{{houdoe}}} in the south of the Netherlands. \n\nAnother example is t-deletion for a specific phrase ({\\textit{{niet meer}}} versus {\\textit{{nie meer}}}), as shown in \\autoref{fig:niet_meer}. Previous dialect research has found that geography is the most important external factor for t-deletion in the Netherlands~\\cite{goeman1999}.\n\n\n\n\nBoth {\\text{HSIC}}\\ and {PPA}\\ report an FDR-adjusted $p<.05$, while for {Moran's~I}, \nthe geographical association does not reach the threshold of significance. \n\n\n\n\\begin{table*}\n\\footnotesize\n\\begin{tabular}{llrlllll}\n\\toprule\n\\textbf{Linguistic variables} & \\textbf{Description} & \\textbf{N} & \\textbf{Moran's I} & \\textbf{HSIC }   & \\textbf{Mantel} & \\textbf{PPA }   \\\\ \n\\midrule\nFriet / patat & french fries & 842  &  0.0004&0.0002&0.0003&0.0003\\\\\nProficiat / gefeliciteerd & congratulations & 14,474 & 0.0004&0.0002&0.0080&0.0003\\\\\nIedereen / een ieder & everyone & 13,009 & 0.8542&0.0002&0.8769&0.0432\\\\\nDoei / aju & bye & 4,427 & 0.7163&0.0050&0.2570&0.3868\\\\\nEfkes / eventjes&for a little while & 969 & 0.0036&0.0002&0.0003&0.0003\\\\\nNaar huis / naar huus& to home & 3, 942 & 0.8542&0.1090&0.1245&0.9426\\\\\nNiet meer / nie meer&not anymore & 11,596 & 0.0793&0.0002&0.5590&0.0329\\\\\nOf niet / of nie &or not & 1,882 & 0.8357&0.1010&0.4191&0.9426\\\\\n-oa- / -ao- &e.g.,  \\emph{jao} versus \\emph{joa} & 754 &  0.0004&0.0002&0.0003&0.0003\\\\\nEven weer / weer even &for a little while again & 921 &  0.0004&0.0002&0.0003&0.0003\\\\\nHave + participle & \\specialcell[t]{e.g., \\emph{heb gedaan} (`have done') \\\\vs. \\emph{gedaan heb} (`done have')} & 1,122 &0.8587&0.2849&0.6668&0.0255\\\\\nBe + participle& \\specialcell[t]{e.g., \\emph{ben geweest} (`have been') \\\\vs. \\emph{geweest ben} (`been have')} & 1,597 & 0.0793&0.2849&0.7862&0.0051\\\\\nSpijkerbroek / jeans& jeans &1,170 & 0.7796&0.0002&0.0080&0.0003\\\\\nDoei/ houdoe & bye&4,491 & 0.5016&0.0002&0.6668&0.0047\\\\\nBellen / telefoneren & to call by telephone&4,689 &0.2730&0.0003&0.9781&0.5941\\\\\n\n\\bottomrule\n\\end{tabular} \n\\caption{Twitter results. The $p$-values were calculated using 10,000 permutations and corrected for multiple comparisons.}\n\\label{tab:results_twitter}\n\\end{table*}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_friet_patat.png}\n  \\caption{French fries (\\emph{friet} versus \\emph{patat}), N=844}\n  \\label{fig:friet}\n\\end{subfigure}\n\\hspace{0.2in}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_efkes_eventjes.png}\n  \\caption{For a little while (\\emph{efkes} versus \\emph{eventjes}), N=970}\n  \\label{fig:efkes}\n\\end{subfigure}\n\n\\caption{Highly significant linguistic variables on Twitter. Grey indicates areas with no data points.  The intensity indicates the number of data points.}\n\\label{fig:twitter_sign}\n\\end{figure}\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_houdoe_doei.png}\n  \\caption{Bye ({\\textit{{doei}}} versus {\\textit{{houdoe}}}), N=4,491}\n  \\label{fig:doei_houdoe}\n\\end{subfigure}\n\\hspace{0.2in}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_niet_meer.png}\n  \\caption{t-deletion ({\\textit{{niet meer}}} vs. {\\textit{{nie meer}}}), N=11,596}\n  \\label{fig:niet_meer}\n\\end{subfigure}\n\n\\caption{Linguistic variables on Twitter where tests disagreed}\n\\label{fig:twitter_sign_disagreement}\n\\end{figure}\n\nWe also present preliminary results on using HSIC as an exploratory tool on the same Twitter corpus. To focus on active users who are most likely tweeting on a personal basis, we  exclude users with 1000 or more followers and users who have fewer than 50 tweets, resulting in  8,333 users. We exclude infrequent words (used by fewer than 100 users) and very frequent words (used by 1000 users or more), resulting in a total of 5,183 words. We represent the usage of a word by each author as a binary variable, and use {\\text{HSIC}}\\ to compute the level of spatial dependence for each word.\n\nThe top 10 words with the highest {\\text{HSIC}}\\ scores are\n{\\textit{{groningen}}} (city), {\\textit{{zwolle}}} (city), {\\textit{{eindhoven}}} (city) \n{\\textit{{arnhem}}} (city),  {\\textit{{breda}}} (city),\n{\\textit{{enschede}}} (city), {\\textit{{nijmegen}}} (city),\n{\\textit{{leiden}}} (city),\n{\\textit{{twente}}} (region) and {\\textit{{delft}}} (city). While these words do not reflect dialectal variation as it is normally construed, we expect their distribution to be heavily influenced by geography.\nManual inspection also revealed that many English words are highly ranked in the list  (e.g., {\\textit{{his, very}}}); as English speakers are more likely to visit tourist and commercial centers, it is unsurprising that these words should show a strong geographical association. The first non-topical word occurs on rank 34 in the list and is \\emph{proficiat}, which is one of the words we included in our analysis in Table \\ref{tab:results_twitter}. This replication of prior dialectological knowledge validates the usage of {\\text{HSIC}}\\ as an exploratory tool. Two other highly ranked words (ranks 60 and 71), which are less well-known, are shown in \\autoref{fig:twitter_exploratory}. {\\textit{{Joh}}} (Figure \\ref{fig:joh}) is an interjection and is used less in the southern part of the Netherlands, while\n{\\textit{{dadelijk}}} (`immediately'/`just a second',  \\autoref{fig:dadelijk}) is used more in the southern part of the Netherlands. The identification of these words speaks to the potential of {\\text{HSIC}}\\ to guide the study of dialect by revealing geographically-associated terms.\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{0.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_joh.png}\n  \\caption{Usage of \\emph{joh}}\n  \\label{fig:joh}\n\\end{subfigure}\n\\hspace{0.2in}\n\\begin{subfigure}{.4\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{plots/twitter_dadelijk.png}\n  \\caption{Usage of {\\textit{{dadelijk}}}}\n  \\label{fig:dadelijk}\n\\end{subfigure}\n\n\\caption{Linguistic features on Twitter}\n\\label{fig:twitter_exploratory}\n\\end{figure}\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe have reviewed four methods for quantifying the spatial dependence of linguistic variables: {Moran's~I}, which is perhaps the best-known in sociolinguistics and dialectology; {PPA}; the Mantel test; and the Hilbert-Schmidt Independent Criterion ({\\text{HSIC}}). Of these methods, only the Mantel test and {\\text{HSIC}}\\ can be applied to the full range of linguistic data: binary and multiway counts, as well as frequencies. The Mantel test assumes a linear relationship between geographical and linguistic distance, making it underpowered in cases where population density interacts with linguistic variation. This means that the effectiveness of the Mantel test will depend on where the variable happens to be centered, or how a dialect continuum aligns with population density; {\\text{HSIC}}\\ is more stable in the face of both of these factors. {PPA}\\ and {Moran's~I}\\ each perform well in some situations, and poorly in others. {Moran's~I}\\ is found to be relatively sensitive to the distance cutoff parameter, and {PPA}\\ struggles with dialect continuum scenarios. {\\text{HSIC}}\\ is the least sensitive to ``outlier'' individuals. Overall, we find that {\\text{HSIC}}, while not the most powerful test in every scenario, offers the broadest applicability and the least potential for catastrophic failure of any of the proposed approaches. \n\nWe then showed how to apply these tests to a diverse range of real\ndatasets: frequency observations in letters to the editor, binary observations in a dialect atlas, and binary observations in social media. We find that previous results on newspaper data were dependent on the procedure of selecting the geographical distance cutoff to maximize the number of positive test results; using all other test procedures, the significance of these results disappears. On the dialect atlas, we find that the fraction of statistically significant variables ranges from 55.2\\% to 78.3\\%  depending on the statistical approach. On the social media data, we obtain largely similar results from the four different tests, but {\\text{HSIC}}\\ detects the largest number of significant associations, identifying cases in which geography and population density were closely intertwined.\n\nMore broadly, this work proposes a somewhat unusual relationship between computation and linguistics: rather than using linguistics as guidance for developing new natural language processing algorithms or for producing new labeled training sets, we use computation as a tool for exploring and testing linguistic hypotheses in large corpora of unannotated text. The resulting method, which builds on the well-understood mathematics of kernel support vector machines, is more broadly applicable and robust than traditional parametric statistical techniques. With the rapidly increasing scale of linguistically-relevant data, we believe there will be many more opportunities to apply computation in this way.\n\n{}\n\n\n\\section*{Acknowledgments}\nThanks to Jack Grieve for sharing the corpus of dialect variables from Letters to the Editor in North American newspapers, Arthur Gretton for advice about how best to use HSIC, Erik Tjong Kim Sang for help on using the SAND data, the DB group of the University of Twente for sharing the Dutch geotagged tweets, and Leonie Cornips and Sjef Barbiers for advice on selecting the Dutch linguistic variables.\nThe first author was supported by the Netherlands Organization for Scientific Research (NWO), grant 640.005.002 (FACT). The second author was supported by National Science Foundation award RI-1452443, and by the Air Force Office of Scientific Research.\n\\bibliographystyle{plainnat}\n\\bibliography{cites}\n\n\n", "itemtype": "equation", "pos": 29329, "prevtext": "\nThese two terms can therefore be seen as mean-centered Gram matrices. By computing the trace of their matrix product, we obtain a cross-covariance between the Gram matrices. If the two data sources are independent, the expectation of this cross-covariance is zero. \n\nAn important implementation detail is that the size of each Gram matrix is the square of the number of observations, which for large data will be too expensive to compute. Following Gretton {\\textit{et al.}}\\  \\cite{gretton2005measuring}, we employ a low-rank approximation to each Gram matrix, using the incomplete Cholesky decomposition~\\cite{bach2002kernel}. Specifically, we approximate the symmetric matrices $K$ and $L$ as low-rank products, $K \\approx A A^T$ and $L \\approx B B^T$, where $A \\in \\mathbb{R}^{n \\times r_A}$ and $B \\in \\mathbb{R}^{n \\times r_B}$. The approximation quality is determined by the parameters $r_A$ and $r_B$, which are set to ensure that the  magnitudes of the residuals $K - AA^T$ and $L - BB^T$ are below a predefined threshold. HSIC may then be approximated as:\n \n\n", "index": 17, "text": "\\begin{align}\n{\\text{HSIC}}(x,y) &= \\frac{\\mathbf{tr} KHLH}{n^2},\\\\\n&\\approx  \\frac{\\mathbf{tr} (A A^T)H(B B^T)H}{n^2},\\\\\n&=  \\frac{\\mathbf{tr} (B^T (HA))(B^T(HA))^T}{n^2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\text{HSIC}}(x,y)\" display=\"inline\"><mrow><mtext>HSIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\mathbf{tr}KHLH}{n^{2}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>\ud835\udc2d\ud835\udc2b</mi><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mi>H</mi></mrow><msup><mi>n</mi><mn>2</mn></msup></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\approx\\frac{\\mathbf{tr}(AA^{T})H(BB^{T})H}{n^{2}},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2248</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>\ud835\udc2d\ud835\udc2b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>B</mi><mo>\u2062</mo><msup><mi>B</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>H</mi></mrow><msup><mi>n</mi><mn>2</mn></msup></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\mathbf{tr}(B^{T}(HA))(B^{T}(HA))^{T}}{n^{2}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>\ud835\udc2d\ud835\udc2b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>B</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>H</mi><mo>\u2062</mo><mi>A</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>B</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>H</mi><mo>\u2062</mo><mi>A</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow><msup><mi>n</mi><mn>2</mn></msup></mfrac></mstyle></mrow></math>", "type": "latex"}]