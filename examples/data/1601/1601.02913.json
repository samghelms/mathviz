[{"file": "1601.02913.tex", "nexttext": "\nwhere $C_{ij}$ is the number of co-occurrence of the $i$-th tag and the $j$-th\ntop level class. Note that in the setting of Yahoo! Challenge, the predefined to-level classes are\nalso chosen from the user-contributed tags.\n\nThe selected subclasses for class-$j$ can be then defined as the tags that have\ntheir distinctive scores above a pre-defined threshold, as shown below:\n\n", "itemtype": "equation", "pos": 7100, "prevtext": "\n\n\n\\conferenceinfo{Multimedia}{}\n\\CopyrightYear{2013}\n\\crdata{}\n\\clubpenalty=10000\n\\widowpenalty = 10000\n\n\n\\title{Learning Subclass Representations for Visually-varied Image Classification}\n\n\\author{\n\n\\alignauthor\nXinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic\\\\\n       \\affaddr{Multimedia Information Retrieval Lab, Delft University of Technology}\\\\\n       \\affaddr{Delft, The Netherlands}\\\\\n       \\email{\\{x.li-3, p.xu, y.shi, m.a.larson,a.hanjalic\\}@tudelft.nl}\n}\n\n\n\n\n\\maketitle\n\n\\begin{abstract}\n\n\nIn this paper, we present a subclass-representation approach that predicts the probability of a\nsocial image belonging to one particular class. We explore the co-occurrence of user-contributed tags to find subclasses with a strong\nconnection to the top level class. We then project each image on to the resulting subclass space to\ngenerate a subclass representation for the image. The novelty of the\napproach is that subclass representations make use of not only the content of the photos themselves, but also information on the co-occurrence of their tags, which determines membership in both subclasses and top-level classes.\nThe novelty is also that the images are classified into smaller classes, which\nhave a chance of being more visually stable and easier to model. These\nsubclasses are used as a latent space and images are represented in this space\nby their probability of relatedness to all of the subclasses. In contrast to approaches\ndirectly modeling each top-level class based on the image content, the proposed method\ncan exploit more information for visually diverse classes. The approach is\nevaluated on a set of $2$ million photos with 10 classes, released by the\nMultimedia 2013 Yahoo! Large-scale Flickr-tag Image Classification Grand\nChallenge. Experiments show that the proposed system delivers sound\nperformance for visually diverse classes compared with methods that directly\nmodel top classes.\n\\end{abstract}\n\n\n\\category{H.3}{Information Storage and Retrieval}{Content}\n\n\\keywords{Large scale image classification, Subclass representation}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\n\n\n\n\nThis paper describes the approach that we developed to address the Yahoo! Large-scale Flickr-tag Image Classification Grand Challenge. This challenge is formulated:\n\n\\begin{figure}[t]\n  \\centering\n  \\scalebox{0.45}{\\includegraphics[width=\\textwidth]{example}}\n  \\caption{Example illustration of the subclass representation}\\label{fig:examp}\n  \\vspace{-0.75cm}\n\\end{figure}\n\n\\emph{Given a training set of images together with their metadata, and a class\nlabel corresponding to each training image, to build a ranking model that for\neach class label ranks images in a test set (without metadata) as accurate as\npossible.}\n\nWe emphasize three particular characteristics of this challenge. First, the ground truth for both the training and testing set is user generate tags. As with any other tagging problem, these tags are expected to be noisy, incomplete and subjective. The ranking models should have the ability to learn useful information with difficult labels. Second, for this particular task, the 10 target tags are selected from tags most frequently used by Flickr users. The images from these classes (which we refer to as `top-level classes') show a broad variation in terms of visual representations. Thus, the ranking model should be based on a representation that can model classes with high intra-class variations. Third, the level of visual diversity varies between the given 10 target tags. For example, images related to tag ``sky'' or ``beach'' are expected to be more visually consistent than images related to ``2012'' or ``nature''. In view of this, the ranking model should have the ability to deal with different abstraction levels adaptively.\n\n\n\nIn view of the above, we propose in this paper a new method for content-based image classification/tagging based on subclass\nrepresentation. As mentioned above, the image classes in this challenge are user-generated tags. Since social images are usually annotated by more than one tag, the co-occurrence of different tags serves as a valuable information source that can be exploited for elaborating the tags (classes) which are by themselves highly visually-varied. As shown in the example in Fig.~\\ref{fig:examp}, the image class ``nature'' can cover an extremely large range of visual representations. However, the subclasses of it, such as ``flower'', ``bird'', ``forest'', are much more homogeneous in terms of the visual information. For this reason, we first discover the subclasses, which are tags frequently and exclusively co-occurring with the top-level class labels, for representing each image. Then, we train a binary classifier for each selected subclass. For a given image, the confidence scores of all subclass classifiers are concatenated to produce a high level representation.\nThus, the representation is developed to learn models of the top-level classes. This is illustrated by Fig.~\\ref{fig:scheme}.\nThe final results are predicted by the ranking model based on the the learned subclass representation.\n\nThe contribution of this paper lies in the following aspects:\n\\begin{itemize}\n\\item This method uses a co-occurrence based method to discover subclasses. Compared with semantic ontology based subclass generation methods~\\cite{DengJ2009} or using predefined concepts as subclasses~\\cite{Torresani2010}, the proposed subclass representation is expected to be more discriminative in terms of predicting the target classes.\n\\item The proposed method uses confidence score instead of binary decision of the subclass classifiers as the high level features. This strategy can develop useful representations even if the performance of subclass classifier is not reliable.\n\\end{itemize}\n\n\nThe remainder of the paper is organized as follows: in Section~\\ref{sec:system}, we present the details of the proposed method. The experimental framework and results are presented in Section~\\ref{sec:expFrame} and Section~\\ref{sec:expResult}. Then, in Section~\\ref{sec:rw} we discuss previous research contributions that are related to our approach proposed in this paper. Finally, Section~\\ref{sec:conclude} summarizes our contributions and discusses future work.\n\n\n\n\n\\section{Learning Subclass Representation}  \\label{sec:system}\n\\begin{figure}[t]\n  \\centering\n  \\scalebox{0.45}{\\includegraphics[width=\\textwidth]{scheme}}\n  \\caption{Overview of the propose approach. First subclasses are discovered, then top-level classes are predicted using subclass representations.}\\label{fig:scheme}\n\\end{figure}\n\\subsection{Mining Subclasses}\nAs discussed above, subclasses of one class (the target class) are expected to\nbe strongly connected with the target class and, moreover, relatively more\nstably reflected in visual features than the target class.\nTo define such subclasses, we exploited the tags annotating the images. We first generate a co-occurrence matrix between photos'\ntags and their top-level classes, and measure each tag's connection to one class by its\ndistinctive score, defined as:\n\n\n", "index": 1, "text": "\\begin{equation}\nS_{ij}  = \\frac{{C_{ij} }}{{\\sum\\limits_j {C_{ij} } }}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"S_{ij}=\\frac{{C_{ij}}}{{\\sum\\limits_{j}{C_{ij}}}}\" display=\"block\"><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><msub><mi>C</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><msub><mi>C</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.02913.tex", "nexttext": "\nwhere $t_i$ denotes the $i$-th tag, and $T$ denotes the set of all tags.\nNote that some tags may be only assigned to a very small number of images\nin one class. For those tags, the limited number of training images prevents\nthem from being effective subclasses. Taking this into account, we\nfurther rank all selected tags, $t_i$, in $Tsel_j $ by the number of photos in\nclass-$j$ that are tagged with $t_i$.\n\n\\subsection{Subclass Representation}\nTo generate a subclass-based representation for an image, we first use the\nimages tagged with the subclasses to train models, i.e., Support Vector Machines\n(SVMs), for classifying subclasses, and then, use the confidence scores for\npredicting each subclass as the new representation for the image. In this sense, as illustrated in Fig.~\\ref{fig:scheme},\nthe image features can be treated as the fist level representation for an image,\nwhile the confidence scores of subclasses are the high level representation. Based on the subclass representations, we further learn the model that characterize the connection between these representations and the top-level class.\n\n\\section{Experimental Framework} \\label{sec:expFrame}\n\\subsection{Dataset}\nTo verify the performance of the proposed approach, we carry out our experiments\non a dataset of photos released by the Multimedia 2013 Yahoo! Large-scale\nFlickr-tag Image Classification Grand Challenge. The dataset contains 2 million\nFlickr photos with 10 classes, i.e., 150K training and 50K test images per class. The class\nlabels are amongst the top tags annotated by the Flickr users.\n\n\nSince the release does not includes the tags associated with the photos, we re-crawled the photos' tags using the photo ID provided in the metadata.\n\nTo develop our system, the training dataset is randomly divided into three parts\nwith the ratio 4:3:1. The first is for training models for\npredicting subclasses based on image features, the second is for training models\nfor target classes based on confidence scores from the learned subclass models,\nand the last is for validation and parameter selection. The test data from the grand challenge is used to evaluate the proposed system.\n\n\\subsection{Multi-class Classification}\nTo model the connection between image feature and subclass, and the connection between subclass representation and top class, we choose an SVM-based approach.\nFor the purpose of classification for multiple classes, we apply one-against-one\ntraining strategy, which is reported to have better training time efficiency and\nprediction accuracy compared with other multi-class support vector machines,\ne.g., one-against-all~\\cite{SVM_1vs1}. To generate probability estimation from\nthe SVM model, we apply the algorithm proposed in \\cite{SVM_prob} and modified\nits original implementation in LibSVM~\\cite{LibSVM} to make it suitable\nfor distributed computing on a Hadoop-based distributed server.\n\n\\subsection{Baselines}\nWe compare our subclass-representation-based approach, denoted in the following\nas $SVM\\_SubClassProb$, to two other approaches that closely relate to our\napproach, as listed below.\n\\begin{itemize}\n  \\item $SVM\\_VisFeat$: Directly model target the 10 top-level classes based on\n  image features.\n  \\item $SVM\\_ClassProb$: Project image features on to top-level 10-classes space.\n\\end{itemize}\n\n\\section{Results} \\label{sec:expResult}\n\n\\subsection{Learning Subclass Models}\nTo learn the subclasses using the co-occurrence matrix between photos' tags and\ntheir classes, the $Thr\\_distin$ in Eq.~\\eqref{thrhod} is set to $0.6$, and then\nwe manually select the subclasses in the top $10$ tags of each class, which\nresults in a total of 54 subclasses. As some classes may contains few distinctive tags\ncompared with other classes, they may have few subclasses, i.e., class\n``travel'' only contain 1 subclass. In contrast, some classes may contain more distinctive tags, i.e., 14 subclasses for class\n``nature''. To train these subclass models for projecting images on to the subclass space, we use a maximum 10k images per subclass as training data. Note that some subclasses may contain less than 10k images. The\nperformance on the validation set, Average Precision (AP) for these subclasses models\nare illustrated in Fig.~\\ref{fig:apSubClass}.\n\\begin{figure}[htb!]\n  \\centering\n  \\scalebox{0.43}{\\includegraphics[width=\\textwidth]{subClass_AP}}\n  \\caption{AP for 54 subclasses, labels on the horizontal axis indicate subclass association with the top-level classes.}\\label{fig:apSubClass}\n\\end{figure}\n\n\\subsection{Classification Results}\nFig.\\ref{fig:MAP} illustrates the performance in terms of mean average precision\n(MAP), across all 10 top-level classes, for the proposed approach and the\nbaselines with respect to different training data scales.\nOverall, $SVM\\_SubClassProb$ performs better than $SVM\\_VisFeat$ and\n$SVM\\_ClassProb$ in different training data scale, with an averaged gain $8\\%$\nfor $SVM\\_VisFeat$ and $7\\%$ for $SVM\\_ClassProb$.\nIn addition, the $SVM\\_SubClassProb$ already achieves good performance compared to others in case of a small training data scale, e.g., 1k per class. Along with the increasing amounts of training data, the performance for $SVM\\_SubClassProb$ levels out. This is due to the fact that many subclasses do not contain enough training photos, e.g., $83\\%$ of them contains photos which are less than 10k.\n\\begin{figure}[htb!]\n  \\centering\n  \\scalebox{0.45}{\\includegraphics[width=\\textwidth]{general_MAP}}\n  \\caption{MAP with different amounts of training data.}\\label{fig:MAP}\n\\end{figure}\n\nFig.~\\ref{fig:ap10Class} further breaks down the performance in terms of AP over\ndifferent classes with respect to different training data scales. As can be\nseen, for classes ``food'', ``people'',``sky'',``nature'', $SVM\\_SubClassProb$\ngains more improvements compared to other classes. This is due to the\nfact that, for these classes, they own more subclasses compare to other classes, i.e., there are more\ndistinctive tags in these classes. Also, some classes, ``sky'',``people'',\ncontains visually highly consistent subclasses, as illustrated in\nFig.~\\ref{fig:apSubClass}, which give a strong support for top classes. This is especially obvious for class ``sky'', in which there is a subclass that has a very strong visual consistency, providing a good support for the top class. Interestingly, for the class ``food'', which owns many subclasses, and each of them has a relative low AP, however, this class still has reliable performance. We conjecture that these subclass classifiers are providing useful discriminative information in form of probabilities that they yield with respect to non-relevant subclasses.\n\n\\begin{figure}[htb!]\n  \\centering\n  \\scalebox{0.4}{\\includegraphics[width=\\textwidth]{compareLabel}}\n  \\subfigure[music] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_0_music}}}\n  \\subfigure[london] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_1_london}}}\n  \\subfigure[wedding] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_2_wedding}}}\n  \\subfigure[food] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_3_food}}}\n  \\subfigure[travel] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_4_travel}}}\n  \\subfigure[beach] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_5_beach}}}\n  \\subfigure[people] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_6_people}}}\n  \\subfigure[sky] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_7_sky}}}\n  \\subfigure[nature] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_8_nature}}}\n  \\subfigure[2012] {\\scalebox{0.23}{\\includegraphics[width=\\textwidth]{AP_9_2012}}}\n  \\vspace{-0.3cm}\n  \\caption{AP for 10 classes on different training data scale.}\\label{fig:ap10Class}\n  \\vspace{-0.4cm}\n\\end{figure}\n\n\\vspace{-0.2cm}\n\\section{Related Work} \\label{sec:rw}\n\n\nOur work is closely related to method used for predicting tags with high intra-class variation, in particular, sub-category based methods or methods based on learning high level representations are often explored. We discuss these methods here in turn.\n\n\nGenerating sub-categories has been considered as an effective method to deal with classification problems where intra-class variation is high. The ImageNet~\\cite{DengJ2009} organizes image dataset with labels corresponding to a semantic hierarchy. This method is able to build comprehensive ontology for large scale dataset. However, for a particular dataset, the sub-categories generated by data driven strategies are expected to be more discriminative. \\cite{YangWeilong2011} exploits co-watch information to learn latent sub-tags for video tag prediction. \\cite{LiL2010} proposes to discover the image hierarchy by using both visual and tag information.\nOur method generates category-specific subclasses by exploring image/tag co-occurrence, and trains classifiers for each subclass-tag. These subclasses based models are expected to be discriminative in terms of estimating the target tags, corresponding to top-level classes.\n\nLearning higher level representation is adopted when the low-level image features are not discriminative enough for the purpose of classification ~\\cite{LiL2010ObjectBank, Torresani2010}. For the supervised representation learning methods, a predefined set of models are trained based on image features. The output of these models is considered as the high level representations for predicting image categories. Recently, deep neural networks have been used on unsupervised learning image representations with large scale image dataset~\\cite{Lee2009, LeQ2012}. This representation has achieved promising results on different classification and tagging tasks~\\cite{Krizhevsky2012}. Our methods used the output of trained subclass classifiers as the high level features. This structure can be easily extend to deeper levels by finding discriminative tags for the subclasses.\n\\vspace{-0.2cm}\n\n\\section{Conclusion} \\label{sec:conclude}\nWe have presented a subclass-representation approach to the task of\nretrieving/ranking large scale social images to one particular class solely\nbased on visual content.\nThe main contribution of the approach is that by projecting the image feature\nrepresentation on to a subclass space generated by exploiting the co-occurrence\ninformation of user-contributed tags, it makes use not only of the content of the photos themselves, but also of information concerning the co-occurrence of the photo's tags with tags corresponding to top-level classes.\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n\\bibliographystyle{abbrv}\n\n\\bibliography{xinchaoBib}\n\n\n\n\n\n\n\n \n\n\n", "itemtype": "equation", "pos": 7562, "prevtext": "\nwhere $C_{ij}$ is the number of co-occurrence of the $i$-th tag and the $j$-th\ntop level class. Note that in the setting of Yahoo! Challenge, the predefined to-level classes are\nalso chosen from the user-contributed tags.\n\nThe selected subclasses for class-$j$ can be then defined as the tags that have\ntheir distinctive scores above a pre-defined threshold, as shown below:\n\n", "index": 3, "text": "\\begin{equation}\nTsel_j  = \\{ t_{i} |S_{ij}  > Thr\\_distin,t_{i}  \\in T\\}\n\\label{thrhod}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"Tsel_{j}=\\{t_{i}|S_{ij}&gt;Thr\\_distin,t_{i}\\in T\\}\" display=\"block\"><mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msub><mi>l</mi><mi>j</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>&gt;</mo><mrow><mi>T</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow><mo>,</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>\u2208</mo><mi>T</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}]