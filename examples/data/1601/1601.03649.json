[{"file": "1601.03649.tex", "nexttext": "\nwhere its resting potential is zero, and $\\tau_m$ and $R$ are model parameters. The parameter $\\tau_m$ is the membrane time constant, relating to the `leakage' of charge across the neuron's membrane when it is not at rest, and $R$ is the neuron's effective membrane resistance. The above equation analogously describes an electrical circuit containing a resistor in parallel with a capacitor, that is charged by a driving current $I_i(t)$ \\cite{Gerstner2002}. Whenever the neuron's membrane potential reaches a formal firing threshold $\\vartheta$ the neuron fires a spike, and its membrane potential is reset to a new value $u_r$.\n\nThe driving current term in Eq.~(\\ref{eq:LIF0}) can be split into two components: $I_i = I_i^{\\mathrm{syn}} + I_i^{\\mathrm{reset}}$, where $I_i^{\\mathrm{syn}}$ is the synaptic current flow into the postsynaptic neuron due to presynaptic spike arrival, and $I_i^{\\mathrm{reset}}$ is a `reset' current pulse which discharges the postsynaptic neuron immediately after it fires to enforce refractoriness. If the postsynaptic neuron receives an ordered sequence of spikes $x_j = \\{t_j^1, t_j^2, ...\\}$\nfrom each presynaptic neuron $j$, where $t_j^f$ corresponds to the $f$-th spike fired, then the \\ac{LIF} model defines the total input synaptic current by\n\n", "itemtype": "equation", "pos": 9923, "prevtext": "\n\\vspace*{0.35in}\n\n\\begin{flushleft}\n{\\Large\n\\textbf\\newline{Optimal Supervised Learning in Spiking Neural Networks for Precise Temporal Encoding}\n}\n\\newline\n\\\\\n{\\bf Brian Gardner\\textsuperscript{*},\nAndr\u00c3\u00a9 Gr\u00c3\u00bcning}\n\\\\\nDepartment of Computer Science, \nUniversity of Surrey, Guildford, GU2 7XH, U.K.\n\\\\\n* E-mail: b.gardner@surrey.ac.uk\n\\end{flushleft}\n\n\\section*{Abstract}\nPrecise spike timing as a means to encode information in neural networks is biologically supported, and is advantageous over frequency-based codes by processing input features on a much shorter time-scale. For these reasons, much recent attention has been focused on the development of supervised learning rules for spiking neural networks that utilise a temporal coding scheme. However, despite significant progress in this area, there still lack rules that are theoretically justified, and yet can be considered biologically relevant. Here we examine the general conditions under which \\emph{optimal} synaptic plasticity takes place to support the supervised learning of a precise temporal code. As part of our analysis we introduce two analytically derived learning rules, one of which relies on an instantaneous error signal to optimise synaptic weights in a network (INST rule), and the other one relying on a filtered error signal to minimise the variance of synaptic weight modifications (FILT rule). We test the optimality of the solutions provided by each rule with respect to their temporal encoding precision, and then measure the maximum number of input patterns they can learn to memorise using the precise timings of individual spikes to give an indication of their overall performance. Our results demonstrate the optimality of the FILT rule in most cases, underpinned by the rule's error-filtering mechanism which provides smooth convergence during learning. We also find the FILT rule to be most efficient at performing input pattern memorisations, and most noticeably when patterns are identified using spikes with sub-millisecond temporal precision. In comparison with existing work, we determine the performance of the FILT rule to be consistent with that of the highly efficient E-learning Chronotron rule, but with the distinct advantage that our FILT rule is also implementable as an online method for increased biological realism.\n\n\\section*{Introduction}\nIt is becoming increasingly clear that the relative timings of spikes transmitted by neurons, and not just their firing rates, is used to convey information regarding the features of input stimuli \\cite{VanRullen2005}. Spike-timing as an encoding mechanism is advantageous over rate-based codes in the sense that it is capable of tracking rapidly changing features, for example briefly presented images projected onto the retina \\cite{Gollisch2008} or tactile events signalled by the fingertip during object manipulations \\cite{Johansson2004}. It is also apparent that spikes are generated with high temporal precision, typically on the order of a few milliseconds under variable conditions \\cite{Mainen1995,Reich1997,Uzzell2004}.\n\nThe indicated importance of precise spiking as a means to process information has motivated a number of theoretical studies on learning methods for \\ac{SNN} (reviewed in \\cite{Kasinski2006,Gutig2014}). Despite this, there still lack supervised learning methods that can combine high technical efficiency with biological plausibility, and yet have a solid theoretical foundation. For example, while the recently proposed \\acf{SPAN} \\cite{Mohemmed2012} and \\acf{PSD} \\cite{Yu2013} rules have both demonstrated success in training \\acp{SNN} to form precise temporal representations of spatio-temporal spike patterns, they have lacked analytical rigour during their formulation; like many existing supervised learning methods for \\acp{SNN}, these rules have been derived from a heuristic, spike-based reinterpretation of the Widrow-Hoff learning rule, therefore making it difficult to predict the optimality of their solutions in general.\n\nThe \\acf{CHRON} \\cite{Florian2012} has emerged as a supervised learning method with stronger theoretical justification, considering that it instead works to minimise an error function based on the \\ac{VPD} \\cite{Victor1996}; the \\ac{VPD} is a metric for measuring the temporal difference between two neural spike trains, and is determined by computing the minimum cost required to transform one spike train into another via the addition, removal or temporal-shifting of individual spikes. In this study, two supervised learning rules were formulated using the \\ac{CHRON} method: the first termed E-learning, which is specifically geared towards classifying spike patterns using precisely-timed output spikes, and which provides high network capacity in terms of the number of memorised patterns. The second rule is termed I-learning, which is more biologically plausible than E-learning but comes at the cost of a reduced network memory capacity. The E-learning rule has less biological relevance than I-learning given its restriction to offline-based learning, as well as its dependence on synaptic variables that are non-local in time.\n\nA probabilistic method which optimises by gradient ascent the likelihood of generating a desired output spike train has been introduced in \\cite{Pfister2006}. This supervised method has strong theoretical justification, and importantly has been shown to give rise to synaptic weight modifications that mimic the results of experimental \\ac{STDP} protocols measuring the change in synaptic strength triggered by the relative timing differences of pre- and postsynaptic spikes \\cite{Bi1998}. Furthermore, the statistical framework in which this method has been devised is general, allowing for its extension to diverse learning paradigms such as reinforcement-based learning \\cite{Fremaux2010}, backpropagation-based learning as applied to multilayer \\acp{SNN} \\cite{Gardner2015} and recurrently connected networks \\cite{Brea2013,JimenezRezende2014}. Despite this, a potential drawback to this approach comes from its reliance on a stochastic neuron model for generating output spikes; although this model is well suited to reinforcement-based learning which relies on variable spiking for stochastic exploration \\cite{Gardner2013}, it is less well suited to the supervised learning of precisely timed output spikes where variable responses become more of a hindrance \\cite{Gardner2014a}.\n\nTo address this issues, we present two supervised learning rules, termed \\acs{INST} and \\acs{FILT}, which are initially derived based on the statistical method of \\cite{Pfister2006} but later adapted for compatibility with the deterministically spiking \\ac{LIF} neuron model; in this way, these rules are justifiable theoretically but also allow for the learning of precisely timed output spikes. Hence, we use these derived rules for demonstrative purposes to explore the general conditions under which optimal synaptic plasticity can take place in \\acp{SNN} to allow for precise temporal encoding. The two rules differ in their formulation with respect to the treatment of output spike trains: while \\ac{INST} simply relies on the \\emph{instantaneous} difference between a target and actual output spike train, \\ac{FILT} goes a step further, and convolves output spike trains with an \\emph{exponential filter} to effectively link together neighbouring target and actual spikes. By this filtering mechanism, we find the \\ac{FILT} rule is able to match the high performance of the E-learning \\ac{CHRON} rule. We also indicate the increased biological relevance of the \\ac{FILT} rule over existing spike-based supervised methods, based on this spike train filtering mechanism.\n\nThis work is organised as follows. First, the \\ac{INST} and \\ac{FILT} learning rules are formulated for \\acp{SNN} consisting of deterministic \\ac{LIF} neurons, and compared with the previously discussed \\ac{SPAN} and \\ac{PSD} rules. Next, synaptic weight changes triggered by the \\ac{INST} and \\ac{FILT} rules are analysed under various conditions, including their dependence on: the order in target and actual output spikes occur, the relative timing difference between output spikes, and output spikes that are close together in time. The proposed rules are then tested in terms of their accuracy when encoding large numbers of arbitrarily generated spike patterns using temporally-precise output spikes. For comparison purposes, results are also obtained for the technically efficient E-learning \\ac{CHRON} rule. Finally, the rules are discussed in relation to existing supervised methods, as well as their their biological significance. We also propose the rules are straightforwardly implementable in the neuromorphic hardware SpiNNaker \\cite{Furber2014}, to enable their efficient application to large network sizes.\n\n\\section*{Methods}\n\nThis section proposes two supervised learning rules for \\acp{SNN}, termed \\ac{INST} and \\ac{FILT}, that are initially formulated using the statistical approach of \\cite{Pfister2006} for analytical rigour, but later adapted for use with a deterministically spiking neuron model for the purpose of precise temporal encoding. Following this, the general task of spiking neurons trained to perform transformations between arbitrary input and output spike patterns is specified. For clarity, this section begins by relating the \\acf{LIF} neuron model to the \\acf{SRM}, which shall be used to support our subsequent analysis of supervised learning methods for \\acp{SNN}.\n\n\\subsection*{Single Neuron Model} \\label{subsec:Neuron_model}\n\nThe \\ac{LIF} neuron is a commonly used spiking neuron model, owing to its relative simplicity and analytical tractability. For these reasons, we start our analysis by considering a single postsynaptic neuron $i$ with a membrane potential $u_i(t)$ at time $t$, with its subthreshold dynamics determined by the \\ac{LIF} model:\n\n", "index": 1, "text": "\\begin{equation} \\label{eq:LIF0}\n\\tau_m \\frac{\\mathrm{d}u_i(t)}{\\mathrm{d}t} = -u_i(t) + R I_i(t) \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\tau_{m}\\frac{\\mathrm{d}u_{i}(t)}{\\mathrm{d}t}=-u_{i}(t)+RI_{i}(t)\\;,\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c4</mi><mi>m</mi></msub><mo>\u2062</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>R</mi><mo>\u2062</mo><msub><mi>I</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere the kernel $\\alpha(s)$ describes the time course of a received postsynaptic current, and $w_{ij}$ is the synaptic weight between the $j$-th presynaptic neuron that reflects the effective charge transferred due to a single spike. Furthermore, if the postsynaptic neuron responds to the spatio-temporal input pattern $\\mathbf{x} = \\{x_1, x_2, ..., x_{n_j} \\}$, due to $n_j$ presynaptic neurons, with a list of output spikes $y_i(t) = \\{t_i^1, t_i^2, ..., \\hat{t}_i < t\\}$ up to a last spike $\\hat{t}_i$ before $t$, then the reset current pulse can simply be given by\n\n", "itemtype": "equation", "pos": 11324, "prevtext": "\nwhere its resting potential is zero, and $\\tau_m$ and $R$ are model parameters. The parameter $\\tau_m$ is the membrane time constant, relating to the `leakage' of charge across the neuron's membrane when it is not at rest, and $R$ is the neuron's effective membrane resistance. The above equation analogously describes an electrical circuit containing a resistor in parallel with a capacitor, that is charged by a driving current $I_i(t)$ \\cite{Gerstner2002}. Whenever the neuron's membrane potential reaches a formal firing threshold $\\vartheta$ the neuron fires a spike, and its membrane potential is reset to a new value $u_r$.\n\nThe driving current term in Eq.~(\\ref{eq:LIF0}) can be split into two components: $I_i = I_i^{\\mathrm{syn}} + I_i^{\\mathrm{reset}}$, where $I_i^{\\mathrm{syn}}$ is the synaptic current flow into the postsynaptic neuron due to presynaptic spike arrival, and $I_i^{\\mathrm{reset}}$ is a `reset' current pulse which discharges the postsynaptic neuron immediately after it fires to enforce refractoriness. If the postsynaptic neuron receives an ordered sequence of spikes $x_j = \\{t_j^1, t_j^2, ...\\}$\nfrom each presynaptic neuron $j$, where $t_j^f$ corresponds to the $f$-th spike fired, then the \\ac{LIF} model defines the total input synaptic current by\n\n", "index": 3, "text": "\\begin{equation} \\label{eq:LIF0_current_syn}\nI_i^{\\mathrm{syn}}(t) =  \\sum_j w_{ij} \\sum_{t_j^f \\in x_j} \\alpha(t - t_j^f) \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"I_{i}^{\\mathrm{syn}}(t)=\\sum_{j}w_{ij}\\sum_{t_{j}^{f}\\in x_{j}}\\alpha(t-t_{j}^%&#10;{f})\\;,\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>I</mi><mi>i</mi><mi>syn</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $C$ is the neuron's membrane capacitance, and $\\vartheta$ and $u_r$ are the previously defined firing threshold and reset potential, respectively \\cite{Gerstner2002}. The term $\\delta$ signifies the Dirac delta-function. Hence, from combining Eqs.~(\\ref{eq:LIF0}), (\\ref{eq:LIF0_current_syn}) and (\\ref{eq:LIF0_current_reset}) and solving for the membrane potential with the initial condition $u_i(t_0) = 0$, it can be shown that:\n\n", "itemtype": "equation", "pos": 12037, "prevtext": "\nwhere the kernel $\\alpha(s)$ describes the time course of a received postsynaptic current, and $w_{ij}$ is the synaptic weight between the $j$-th presynaptic neuron that reflects the effective charge transferred due to a single spike. Furthermore, if the postsynaptic neuron responds to the spatio-temporal input pattern $\\mathbf{x} = \\{x_1, x_2, ..., x_{n_j} \\}$, due to $n_j$ presynaptic neurons, with a list of output spikes $y_i(t) = \\{t_i^1, t_i^2, ..., \\hat{t}_i < t\\}$ up to a last spike $\\hat{t}_i$ before $t$, then the reset current pulse can simply be given by\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:LIF0_current_reset}\nI_i^{\\mathrm{reset}}(t) =  -C (\\vartheta - u_r) \\sum_{t_i^f \\in y_i} \\delta(t - t_i^f) \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"I_{i}^{\\mathrm{reset}}(t)=-C(\\vartheta-u_{r})\\sum_{t_{i}^{f}\\in y_{i}}\\delta(t%&#10;-t_{i}^{f})\\;,\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>I</mi><mi>i</mi><mi>reset</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03d1</mi><mo>-</mo><msub><mi>u</mi><mi>r</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere we use the relation for the membrane time constant: $\\tau_m = R\\, C$. We approximate the time course of a postsynaptic current pulse by an exponential filter:\n\n", "itemtype": "equation", "pos": 12627, "prevtext": "\nwhere $C$ is the neuron's membrane capacitance, and $\\vartheta$ and $u_r$ are the previously defined firing threshold and reset potential, respectively \\cite{Gerstner2002}. The term $\\delta$ signifies the Dirac delta-function. Hence, from combining Eqs.~(\\ref{eq:LIF0}), (\\ref{eq:LIF0_current_syn}) and (\\ref{eq:LIF0_current_reset}) and solving for the membrane potential with the initial condition $u_i(t_0) = 0$, it can be shown that:\n\n", "index": 7, "text": "\\begin{align} \\label{eq:LIF0_solution}\nu_i(t) &= \\frac{1}{C} \\sum_j w_{ij} \\sum_{t_j^f \\in x_j} \\int_{t' = 0}^t \\alpha(t' - t_j^f) \\exp\\left(-\\frac{t - t'}{\\tau_m}\\right)\\, \\mathrm{d}{t'} \\nonumber \\\\ \n& \\quad -(\\vartheta - u_r) \\sum_{t_i^f \\in y_i} \\exp\\left(-\\frac{t - t_i^f}{\\tau_m}\\right)\\, \\Theta(t - t_i^f) \\;, \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle u_{i}(t)\" display=\"inline\"><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{C}\\sum_{j}w_{ij}\\sum_{t_{j}^{f}\\in x_{j}}\\int_{t^{%&#10;\\prime}=0}^{t}\\alpha(t^{\\prime}-t_{j}^{f})\\exp\\left(-\\frac{t-t^{\\prime}}{\\tau_%&#10;{m}}\\right)\\,\\mathrm{d}{t^{\\prime}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>C</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder></mstyle><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msup><mi>t</mi><mo>\u2032</mo></msup><mo>=</mo><mn>0</mn></mrow><mi>t</mi></msubsup></mstyle><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>t</mi><mo>\u2032</mo></msup><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo>-</mo><msup><mi>t</mi><mo>\u2032</mo></msup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>t</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-(\\vartheta-u_{r})\\sum_{t_{i}^{f}\\in y_{i}}\\exp\\left(-\\frac{%&#10;t-t_{i}^{f}}{\\tau_{m}}\\right)\\,\\Theta(t-t_{i}^{f})\\;,\" display=\"inline\"><mrow><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03d1</mi><mo>-</mo><msub><mi>u</mi><mi>r</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $q$ is the total charge transferred due to a single presynaptic spike and $\\tau_s$ is a synaptic time constant. The term $\\Theta(s)$ is the Heaviside step function, and is defined such that $\\Theta(s) = 1$ for $s > 0$ and $\\Theta(s) = 0$ otherwise. Substituting Eq.~(\\ref{eq:LIF0_alpha_kernel}) into Eq.~(\\ref{eq:LIF0_solution}) and then performing the integration yields:\n\n", "itemtype": "equation", "pos": 13122, "prevtext": "\nwhere we use the relation for the membrane time constant: $\\tau_m = R\\, C$. We approximate the time course of a postsynaptic current pulse by an exponential filter:\n\n", "index": 9, "text": "\\begin{equation} \\label{eq:LIF0_alpha_kernel}\n\\alpha(s) =  \\frac{q}{\\tau_s} \\exp\\left(-\\frac{s}{\\tau_s}\\right)\\, \\Theta(s) \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\alpha(s)=\\frac{q}{\\tau_{s}}\\exp\\left(-\\frac{s}{\\tau_{s}}\\right)\\,\\Theta(s)\\;,\" display=\"block\"><mrow><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi>q</mi><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mi>s</mi><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nWe then respectively define the \\ac{PSP} and reset kernels:\n\n", "itemtype": "equation", "pos": 13643, "prevtext": "\nwhere $q$ is the total charge transferred due to a single presynaptic spike and $\\tau_s$ is a synaptic time constant. The term $\\Theta(s)$ is the Heaviside step function, and is defined such that $\\Theta(s) = 1$ for $s > 0$ and $\\Theta(s) = 0$ otherwise. Substituting Eq.~(\\ref{eq:LIF0_alpha_kernel}) into Eq.~(\\ref{eq:LIF0_solution}) and then performing the integration yields:\n\n", "index": 11, "text": "\\begin{align} \\label{eq:LIF0_solutionB}\nu_i(t) &= \\frac{q}{C}\\, \\frac{\\tau_m}{\\tau_m - \\tau_s} \\sum_j w_{ij} \\sum_{t_j^f \\in x_j} \\left[ \\exp\\left( -\\frac{t - t_j^f}{\\tau_m} \\right) - \\exp\\left( -\\frac{t - t_j^f}{\\tau_s} \\right) \\right]\\, \\Theta(t - t_j^f) \\nonumber \\\\ \n& \\quad -(\\vartheta - u_r) \\sum_{t_i^f \\in y_i} \\exp\\left(-\\frac{t - t_i^f}{\\tau_m}\\right)\\, \\Theta(t - t_i^f) \\;. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle u_{i}(t)\" display=\"inline\"><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{q}{C}\\,\\frac{\\tau_{m}}{\\tau_{m}-\\tau_{s}}\\sum_{j}w_{ij}%&#10;\\sum_{t_{j}^{f}\\in x_{j}}\\left[\\exp\\left(-\\frac{t-t_{j}^{f}}{\\tau_{m}}\\right)-%&#10;\\exp\\left(-\\frac{t-t_{j}^{f}}{\\tau_{s}}\\right)\\right]\\,\\Theta(t-t_{j}^{f})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mi>q</mi><mi>C</mi></mfrac></mstyle></mpadded><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03c4</mi><mi>m</mi></msub><mrow><msub><mi>\u03c4</mi><mi>m</mi></msub><mo>-</mo><msub><mi>\u03c4</mi><mi>s</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder></mstyle><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder></mstyle><mrow><mrow><mo>[</mo><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace=\"4.2pt\">]</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-(\\vartheta-u_{r})\\sum_{t_{i}^{f}\\in y_{i}}\\exp\\left(-\\frac{%&#10;t-t_{i}^{f}}{\\tau_{m}}\\right)\\,\\Theta(t-t_{i}^{f})\\;.\" display=\"inline\"><mrow><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03d1</mi><mo>-</mo><msub><mi>u</mi><mi>r</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere the coefficient terms are given by $\\epsilon_0 = \\frac{q}{C}\\, \\frac{\\tau_m}{\\tau_m - \\tau_s}$ and $\\kappa_0 = -(\\vartheta - u_r)$. Hence, combining Eq.~(\\ref{eq:LIF0_solutionB}), (\\ref{eq:PSP_kernel}) and (\\ref{eq:reset_kernel}) provides the simplified \\acf{SRM} \\cite{Gerstner2002}:\n\n", "itemtype": "equation", "pos": 14102, "prevtext": "\nWe then respectively define the \\ac{PSP} and reset kernels:\n\n", "index": 13, "text": "\\begin{align}\n\\epsilon(s) &= \\epsilon_0\\, \\left[ \\exp\\left( -\\frac{s}{\\tau_m} \\right) - \\exp\\left( -\\frac{s}{\\tau_s} \\right) \\right]\\, \\Theta(s) \\quad \\mathrm{and} \\label{eq:PSP_kernel} \\\\ \n\\kappa(s) &= \\kappa_0\\, \\exp\\left(-\\frac{s}{\\tau_m}\\right)\\, \\Theta(s) \\;, \\label{eq:reset_kernel} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\epsilon(s)\" display=\"inline\"><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\left[\\exp\\left(-\\frac{s}{\\tau_{m}}\\right)-\\exp%&#10;\\left(-\\frac{s}{\\tau_{s}}\\right)\\right]\\,\\Theta(s)\\quad\\mathrm{and}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi>s</mi><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi>s</mi><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace=\"4.2pt\">]</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>and</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\kappa(s)\" display=\"inline\"><mrow><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\kappa_{0}\\,\\exp\\left(-\\frac{s}{\\tau_{m}}\\right)\\,\\Theta(s)\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>\u03ba</mi><mn>0</mn></msub></mpadded><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi>s</mi><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nsuch that the membrane potential is now expressed explicitly in terms of the presynaptic spike pattern $\\mathbf{x}$ and list of postsynaptic spikes $y_i$. The above equation is better suited to analytical treatment than its differential form given by Eq.~(\\ref{eq:LIF0}).\n\nIn our analysis we set the model parameters as follows: $\\epsilon_0 = \\SI{4}{mV}$, $\\tau_m = \\SI{10}{ms}$, $\\tau_s = \\SI{5}{ms}$, $\\vartheta = -\\SI{15}{mV}$ and $u_r = \\SI{0}{mV}$; for these choice of parameters, a single presynaptic spike evokes a \\ac{PSP} with a maximum value of \\SI{1}{mV} after a lag time close to \\SI{7}{ms}, and the postsynaptic neuron is reset to its resting value of \\SI{0}{mV} immediately after firing. Shown in Fig.~\\ref{fig1} are graphical illustrations of the postsynaptic current, \\ac{PSP} and reset kernels, as well an example of a resulting postsynaptic membrane potential as defined by Eq.~(\\ref{eq:potential}).\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig1}\n\\caption{\n{\\bf An illustration of the postsynaptic kernels used in this analysis, and an example of a resulting postsynaptic membrane potential.}\n(A) The time course of the postsynaptic current kernel $\\alpha$. (B) The \\ac{PSP} kernel $\\epsilon$. (C) The reset kernel $\\kappa$. (D) The resulting membrane potential $u_i$ as defined by Eq.~(\\ref{eq:potential}). In this example, a single presynaptic spike is received at $t_j = \\SI{0}{ms}$, and a postsynaptic spike is generated at $t_i = \\SI{4}{ms}$ from selectively tuning both the synaptic weight $w_{ij}$ and firing threshold $\\vartheta$ values. We take $C = \\SI{2.5}{nF}$ for the capacitance, such that the postsynaptic current attains a maximum value of \\SI{1}{nA}.\n}\n\\label{fig1}\n\\end{figure}\n\nWe now explore in more detail the spike generation mechanism of the postsynaptic neuron. Currently, firing events are considered to take place only when the neuron's membrane potential crosses a predefined firing threshold. Alternatively, however, we may instead consider output spikes that are generated by a stochastic process with a time-dependent firing rate $\\rho_i$, such that firing events may occur even at moments when the neuron's membrane potential is below the firing threshold. The instantaneous firing rate $\\rho_i$ is formally referred to as the stochastic intensity of the neuron, and arbitrarily depends on the distance between the neuron's membrane potential and formal firing threshold $\\vartheta$ according to\n\n", "itemtype": "equation", "pos": 14695, "prevtext": "\nwhere the coefficient terms are given by $\\epsilon_0 = \\frac{q}{C}\\, \\frac{\\tau_m}{\\tau_m - \\tau_s}$ and $\\kappa_0 = -(\\vartheta - u_r)$. Hence, combining Eq.~(\\ref{eq:LIF0_solutionB}), (\\ref{eq:PSP_kernel}) and (\\ref{eq:reset_kernel}) provides the simplified \\acf{SRM} \\cite{Gerstner2002}:\n\n", "index": 15, "text": "\\begin{equation} \\label{eq:potential}\nu_i(t|\\mathbf{x}, y_i) = \\sum_j w_{ij} \\sum_{t_j^f \\in x_j} \\epsilon (t - t_j^f) + \\sum_{t_i^f \\in y_i} \\kappa (t - t_i^f) \\;, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"u_{i}(t|\\mathbf{x},y_{i})=\\sum_{j}w_{ij}\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(t-t_%&#10;{j}^{f})+\\sum_{t_{i}^{f}\\in y_{i}}\\kappa(t-t_{i}^{f})\\;,\" display=\"block\"><mrow><msub><mi>u</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mi>\u03f5</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder><mi>\u03ba</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $u_i$ is defined by Eq.~(\\ref{eq:potential}) and $g$ is an arbitrary functional that is commonly referred to as the `escape rate' \\cite{Gerstner2002}. \n\nVarious choices exist to define the functional form of the neuron's escape rate. A common choice is to assume an exponential dependence:\n\n", "itemtype": "equation", "pos": 17329, "prevtext": "\nsuch that the membrane potential is now expressed explicitly in terms of the presynaptic spike pattern $\\mathbf{x}$ and list of postsynaptic spikes $y_i$. The above equation is better suited to analytical treatment than its differential form given by Eq.~(\\ref{eq:LIF0}).\n\nIn our analysis we set the model parameters as follows: $\\epsilon_0 = \\SI{4}{mV}$, $\\tau_m = \\SI{10}{ms}$, $\\tau_s = \\SI{5}{ms}$, $\\vartheta = -\\SI{15}{mV}$ and $u_r = \\SI{0}{mV}$; for these choice of parameters, a single presynaptic spike evokes a \\ac{PSP} with a maximum value of \\SI{1}{mV} after a lag time close to \\SI{7}{ms}, and the postsynaptic neuron is reset to its resting value of \\SI{0}{mV} immediately after firing. Shown in Fig.~\\ref{fig1} are graphical illustrations of the postsynaptic current, \\ac{PSP} and reset kernels, as well an example of a resulting postsynaptic membrane potential as defined by Eq.~(\\ref{eq:potential}).\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig1}\n\\caption{\n{\\bf An illustration of the postsynaptic kernels used in this analysis, and an example of a resulting postsynaptic membrane potential.}\n(A) The time course of the postsynaptic current kernel $\\alpha$. (B) The \\ac{PSP} kernel $\\epsilon$. (C) The reset kernel $\\kappa$. (D) The resulting membrane potential $u_i$ as defined by Eq.~(\\ref{eq:potential}). In this example, a single presynaptic spike is received at $t_j = \\SI{0}{ms}$, and a postsynaptic spike is generated at $t_i = \\SI{4}{ms}$ from selectively tuning both the synaptic weight $w_{ij}$ and firing threshold $\\vartheta$ values. We take $C = \\SI{2.5}{nF}$ for the capacitance, such that the postsynaptic current attains a maximum value of \\SI{1}{nA}.\n}\n\\label{fig1}\n\\end{figure}\n\nWe now explore in more detail the spike generation mechanism of the postsynaptic neuron. Currently, firing events are considered to take place only when the neuron's membrane potential crosses a predefined firing threshold. Alternatively, however, we may instead consider output spikes that are generated by a stochastic process with a time-dependent firing rate $\\rho_i$, such that firing events may occur even at moments when the neuron's membrane potential is below the firing threshold. The instantaneous firing rate $\\rho_i$ is formally referred to as the stochastic intensity of the neuron, and arbitrarily depends on the distance between the neuron's membrane potential and formal firing threshold $\\vartheta$ according to\n\n", "index": 17, "text": "\\begin{equation} \\label{eq:stochastic_intensity}\n\\rho_i(t) = g [ u_i(t) - \\vartheta ] \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\rho_{i}(t)=g[u_{i}(t)-\\vartheta]\\;,\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c1</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>\u03d1</mi></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $\\rho_0$ is the instantaneous firing rate at threshold $\\vartheta$, and the parameter $\\Delta u$ determines the `smoothness' of the firing rate about the threshold \\cite{Jolivet2006}. It is important to note that in taking the limit $\\Delta u \\rightarrow 0$ the deterministic \\ac{LIF} model can be recovered, the utility of which shall become apparent later.\n\n\\subsection*{Optimal Supervised Learning Method} \\label{subsec:Optimal_STDP}\n\nImplementing a stochastic model for generating postsynaptic spikes according to Eq.~(\\ref{eq:stochastic_intensity}) is advantageous, given that it allows for the determination of the likelihood of generating a desired sequence of target output spikes $y_i^{\\mathrm{ref}} = \\{\\tilde{t}_i^1, \\tilde{t}_i^2, ..., \\tilde{t}_i^{n_s} \\}$ containing $n_s$ spikes in response to an input spike pattern $\\mathbf{x}$. As shown originally by \\cite{Pfister2006}, the log-likelihood is given by\n\n", "itemtype": "equation", "pos": 17730, "prevtext": "\nwhere $u_i$ is defined by Eq.~(\\ref{eq:potential}) and $g$ is an arbitrary functional that is commonly referred to as the `escape rate' \\cite{Gerstner2002}. \n\nVarious choices exist to define the functional form of the neuron's escape rate. A common choice is to assume an exponential dependence:\n\n", "index": 19, "text": "\\begin{equation} \\label{eq:EXP_rate}\ng_i[u_i(t) - \\vartheta] = \\rho_0 \\exp \\left( \\frac{ u_i(t) - \\vartheta }{ \\Delta u } \\right) \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"g_{i}[u_{i}(t)-\\vartheta]=\\rho_{0}\\exp\\left(\\frac{u_{i}(t)-\\vartheta}{\\Delta u%&#10;}\\right)\\;,\" display=\"block\"><mrow><mrow><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>\u03d1</mi></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03c1</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mfrac><mrow><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>\u03d1</mi></mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>u</mi></mrow></mfrac><mo rspace=\"5.3pt\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $\\mathcal{Y}_i^{\\mathrm{ref}}(t) = \\sum_{\\tilde{t}_i^f \\in y_i^{\\mathrm{ref}}} \\delta(t - \\tilde{t}_i^f)$ is a target postsynaptic spike train and $T$ is the duration over which the input pattern $\\mathbf{x}$ is presented. It is emphasised, however, that the \\emph{sampled} stochastic intensity $\\rho_i(t|\\mathbf{x}, y_i)$ is used here, which depends on the actual sequence of output spikes $y_i$, and not the target output $y_i^\\mathrm{ref}$; this is motivated by a desire for biological relevance, since it is unrealistic to presume the neuron has prior knowledge of $\\rho_i(t|\\mathbf{x}, y_i^\\mathrm{ref})$ during learning. Importantly, since the neuron model is described by a linear \\ac{SRM} and the escape rate is exponential, then the log-likelihood is a concave function of its parameters \\cite{Paninski2004}. Log-concavity is ideal since it ensures no non-global local maxima exist in the likelihood, thereby allowing for computationally efficient parameter optimisation methods.\n\nIn our analysis, we seek to maximise the log-likelihood of a postsynaptic neuron generating a desired target output spike train $\\mathcal{Y}_i^{\\mathrm{ref}}$ through modifying the strengths of synaptic weights in the network. This can be achieved through the technique of gradient ascent, such that synaptic weights are modified by\n\n", "itemtype": "equation", "pos": 18805, "prevtext": "\nwhere $\\rho_0$ is the instantaneous firing rate at threshold $\\vartheta$, and the parameter $\\Delta u$ determines the `smoothness' of the firing rate about the threshold \\cite{Jolivet2006}. It is important to note that in taking the limit $\\Delta u \\rightarrow 0$ the deterministic \\ac{LIF} model can be recovered, the utility of which shall become apparent later.\n\n\\subsection*{Optimal Supervised Learning Method} \\label{subsec:Optimal_STDP}\n\nImplementing a stochastic model for generating postsynaptic spikes according to Eq.~(\\ref{eq:stochastic_intensity}) is advantageous, given that it allows for the determination of the likelihood of generating a desired sequence of target output spikes $y_i^{\\mathrm{ref}} = \\{\\tilde{t}_i^1, \\tilde{t}_i^2, ..., \\tilde{t}_i^{n_s} \\}$ containing $n_s$ spikes in response to an input spike pattern $\\mathbf{x}$. As shown originally by \\cite{Pfister2006}, the log-likelihood is given by\n\n", "index": 21, "text": "\\begin{equation} \\label{eq:log_PDF}\n\\log P( y_i^{\\mathrm{ref}}|\\mathbf{x} ) = \\int_0^T \\log \\left( \\rho_i(t|\\mathbf{x}, y_i) \\right) \\mathcal{Y}_i^{\\mathrm{ref}}(t) - \\rho_i(t|\\mathbf{x}, y_i)\\, \\mathrm{d}t \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\log P(y_{i}^{\\mathrm{ref}}|\\mathbf{x})=\\int_{0}^{T}\\log\\left(\\rho_{i}(t|%&#10;\\mathbf{x},y_{i})\\right)\\mathcal{Y}_{i}^{\\mathrm{ref}}(t)-\\rho_{i}(t|\\mathbf{x%&#10;},y_{i})\\,\\mathrm{d}t\\;,\" display=\"block\"><mrow><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mi>i</mi><mi>ref</mi></msubsup><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mi>log</mi><mrow><mo>(</mo><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mi>ref</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><mpadded width=\"+2.8pt\"><mi>t</mi></mpadded><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nTaking the derivative of Eq.~(\\ref{eq:log_PDF}) and using Eq.~(\\ref{eq:potential}) provides the gradient of the log-likelihood:\n\n", "itemtype": "equation", "pos": 20360, "prevtext": "\nwhere $\\mathcal{Y}_i^{\\mathrm{ref}}(t) = \\sum_{\\tilde{t}_i^f \\in y_i^{\\mathrm{ref}}} \\delta(t - \\tilde{t}_i^f)$ is a target postsynaptic spike train and $T$ is the duration over which the input pattern $\\mathbf{x}$ is presented. It is emphasised, however, that the \\emph{sampled} stochastic intensity $\\rho_i(t|\\mathbf{x}, y_i)$ is used here, which depends on the actual sequence of output spikes $y_i$, and not the target output $y_i^\\mathrm{ref}$; this is motivated by a desire for biological relevance, since it is unrealistic to presume the neuron has prior knowledge of $\\rho_i(t|\\mathbf{x}, y_i^\\mathrm{ref})$ during learning. Importantly, since the neuron model is described by a linear \\ac{SRM} and the escape rate is exponential, then the log-likelihood is a concave function of its parameters \\cite{Paninski2004}. Log-concavity is ideal since it ensures no non-global local maxima exist in the likelihood, thereby allowing for computationally efficient parameter optimisation methods.\n\nIn our analysis, we seek to maximise the log-likelihood of a postsynaptic neuron generating a desired target output spike train $\\mathcal{Y}_i^{\\mathrm{ref}}$ through modifying the strengths of synaptic weights in the network. This can be achieved through the technique of gradient ascent, such that synaptic weights are modified by\n\n", "index": 23, "text": "\\begin{equation} \\label{eq:grad_ascent}\n\\Delta w_{ij} \\sim \\frac{\\partial \\log P( y_i^{\\mathrm{ref}}|\\mathbf{x} )}{\\partial w_{ij}} \\;.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}\\sim\\frac{\\partial\\log P(y_{i}^{\\mathrm{ref}}|\\mathbf{x})}{%&#10;\\partial w_{ij}}\\;.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>\u223c</mo><mpadded width=\"+2.8pt\"><mfrac><mrow><mo>\u2202</mo><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mi>i</mi><mi>ref</mi></msubsup><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mfrac></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere $\\rho_i'(t|\\mathbf{x}, y_i) = \\frac{\\mathrm{d} g}{\\mathrm{d} u}|_{u = u_i(t|\\mathbf{x}, y_i)}$. Furthermore, using Eq.~(\\ref{eq:EXP_rate}) it follows that\n\n", "itemtype": "equation", "pos": 20639, "prevtext": "\nTaking the derivative of Eq.~(\\ref{eq:log_PDF}) and using Eq.~(\\ref{eq:potential}) provides the gradient of the log-likelihood:\n\n", "index": 25, "text": "\\begin{equation} \\label{eq:D_log_PDF}\n\\frac{\\partial \\log P( y_i^{\\mathrm{ref}}|\\mathbf{x} )}{\\partial w_{ij}}  = \n\\int_0^T \n\\frac{ \\rho_i'(t|\\mathbf{x}, y_i) }{ \\rho_i(t|\\mathbf{x}, y_i) } \\left[ \\mathcal{Y}_i^{\\mathrm{ref}}(t) - \\rho_i(t|\\mathbf{x}, y_i) \\right] \\sum_{t_j^f \\in x_j} \\epsilon (t - t_j^f)\\, \\mathrm{d}t \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\log P(y_{i}^{\\mathrm{ref}}|\\mathbf{x})}{\\partial w_{ij}}=\\int_{%&#10;0}^{T}\\frac{\\rho_{i}^{\\prime}(t|\\mathbf{x},y_{i})}{\\rho_{i}(t|\\mathbf{x},y_{i}%&#10;)}\\left[\\mathcal{Y}_{i}^{\\mathrm{ref}}(t)-\\rho_{i}(t|\\mathbf{x},y_{i})\\right]%&#10;\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(t-t_{j}^{f})\\,\\mathrm{d}t\\;,\" display=\"block\"><mrow><mfrac><mrow><mo>\u2202</mo><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mi>i</mi><mi>ref</mi></msubsup><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mfrac><mrow><msubsup><mi>\u03c1</mi><mi>i</mi><mo>\u2032</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mrow><mo>[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mi>ref</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mi>\u03f5</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><mpadded width=\"+2.8pt\"><mi>t</mi></mpadded><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhich, in combination with Eqs.~(\\ref{eq:grad_ascent}), (\\ref{eq:D_log_PDF}) and (\\ref{eq:D_log_PDF_frac}), provides the weight update rule:\n\n", "itemtype": "equation", "pos": 21140, "prevtext": "\nwhere $\\rho_i'(t|\\mathbf{x}, y_i) = \\frac{\\mathrm{d} g}{\\mathrm{d} u}|_{u = u_i(t|\\mathbf{x}, y_i)}$. Furthermore, using Eq.~(\\ref{eq:EXP_rate}) it follows that\n\n", "index": 27, "text": "\\begin{equation} \\label{eq:D_log_PDF_frac}\n\\frac{ \\rho_i'(t|\\mathbf{x}, y_i) }{ \\rho_i(t|\\mathbf{x}, y_i) } = \\frac{1}{\\Delta u} \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\rho_{i}^{\\prime}(t|\\mathbf{x},y_{i})}{\\rho_{i}(t|\\mathbf{x},y_{i})}=%&#10;\\frac{1}{\\Delta u}\\;,\" display=\"block\"><mrow><mrow><mfrac><mrow><msubsup><mi>\u03c1</mi><mi>i</mi><mo>\u2032</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mpadded width=\"+2.8pt\"><mfrac><mn>1</mn><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>u</mi></mrow></mfrac></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere the factor $\\frac{1}{\\Delta u}$ has been folded into a learning rate $\\eta$. The above has been derived by \\cite{Pfister2006}, and has been shown to well approximate synaptic plasticity that depends on coincident pre- and postsynaptic spike times as observed experimentally in \\cite{Bi1998}.\n\n\\subsection*{INSTantaneous-error (INST) Synaptic Plasticity Rule} \\label{subsec:INST_rule}\n\nThe weight update rule of Eq.~(\\ref{eq:w_update_stoch}) has been derived by taking a maximum-likelihood approach using a stochastic spiking neuron model, but can be adapted to the case of a deterministically firing \\ac{LIF} neuron model to allow for precise temporal encoding. By taking the limit $\\Delta u \\rightarrow 0$ for the stochastic threshold parameter in Eq.~(\\ref{eq:EXP_rate}), the stochastic intensity can assume one of two values:\n\n", "itemtype": "equation", "pos": 21429, "prevtext": "\nwhich, in combination with Eqs.~(\\ref{eq:grad_ascent}), (\\ref{eq:D_log_PDF}) and (\\ref{eq:D_log_PDF_frac}), provides the weight update rule:\n\n", "index": 29, "text": "\\begin{equation} \\label{eq:w_update_stoch}\n\\Delta w_{ij} = \\eta \\int_0^T \\left[ \\mathcal{Y}_i^{\\mathrm{ref}}(t) - \\rho_i(t|\\mathbf{x}, y_i) \\right] \\sum_{t_j^f \\in x_j} \\epsilon (t - t_j^f)\\, \\mathrm{d}t \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}=\\eta\\int_{0}^{T}\\left[\\mathcal{Y}_{i}^{\\mathrm{ref}}(t)-\\rho_{i}%&#10;(t|\\mathbf{x},y_{i})\\right]\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(t-t_{j}^{f})\\,%&#10;\\mathrm{d}t\\;,\" display=\"block\"><mrow><mi mathvariant=\"normal\">\u0394</mi><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>\u03b7</mi><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo>[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mi>ref</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msub><mi>\u03c1</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mi>\u03f5</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><mpadded width=\"+2.8pt\"><mi>t</mi></mpadded><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere the term $\\delta(t - t_i^f)$ is the Dirac delta distribution about a postsynaptic firing time $t_i^f \\in y_i$, since immediately after a spike is emitted: $u(t_i^{f+}) < \\vartheta$ as a result of the reset term in Eq.~(\\ref{eq:potential}). Hence, the firing rate can be substituted with the postsynaptic spike train $\\rho_i(t) \\rightarrow \\mathcal{Y}_i(t)$, $\\mathcal{Y}(t) = \\sum_{t^f \\in y} \\delta(t - t^f)$, to provide a deterministic adaptation of Eq.~(\\ref{eq:w_update_stoch}):\n\n", "itemtype": "equation", "pos": 22487, "prevtext": "\nwhere the factor $\\frac{1}{\\Delta u}$ has been folded into a learning rate $\\eta$. The above has been derived by \\cite{Pfister2006}, and has been shown to well approximate synaptic plasticity that depends on coincident pre- and postsynaptic spike times as observed experimentally in \\cite{Bi1998}.\n\n\\subsection*{INSTantaneous-error (INST) Synaptic Plasticity Rule} \\label{subsec:INST_rule}\n\nThe weight update rule of Eq.~(\\ref{eq:w_update_stoch}) has been derived by taking a maximum-likelihood approach using a stochastic spiking neuron model, but can be adapted to the case of a deterministically firing \\ac{LIF} neuron model to allow for precise temporal encoding. By taking the limit $\\Delta u \\rightarrow 0$ for the stochastic threshold parameter in Eq.~(\\ref{eq:EXP_rate}), the stochastic intensity can assume one of two values:\n\n", "index": 31, "text": "\\begin{equation} \\label{eq:EXP_rate_deterministic}\n\\rho[u_i(t)] = \n  \\begin{cases}\n   \\delta(t - t_i^f) & \\text{for } u_i(t_i^f) > \\vartheta \\\\\n   0 & \\text{otherwise} \\;,\n  \\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\rho[u_{i}(t)]=\\begin{cases}\\delta(t-t_{i}^{f})&amp;\\text{for }u_{i}(t_{i}^{f})&gt;%&#10;\\vartheta\\\\&#10;0&amp;\\text{otherwise}\\;,\\end{cases}\" display=\"block\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>t</mi><mi>i</mi><mi>f</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mi>\u03d1</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mpadded width=\"+2.8pt\"><mtext>otherwise</mtext></mpadded><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nthat is now a function of the difference between a target and actual postsynaptic spike train. Finally, the integration of Eq.~(\\ref{eq:w_update_INST}) can be performed to provide a batch weight update rule:\n\n", "itemtype": "equation", "pos": 23177, "prevtext": "\nwhere the term $\\delta(t - t_i^f)$ is the Dirac delta distribution about a postsynaptic firing time $t_i^f \\in y_i$, since immediately after a spike is emitted: $u(t_i^{f+}) < \\vartheta$ as a result of the reset term in Eq.~(\\ref{eq:potential}). Hence, the firing rate can be substituted with the postsynaptic spike train $\\rho_i(t) \\rightarrow \\mathcal{Y}_i(t)$, $\\mathcal{Y}(t) = \\sum_{t^f \\in y} \\delta(t - t^f)$, to provide a deterministic adaptation of Eq.~(\\ref{eq:w_update_stoch}):\n\n", "index": 33, "text": "\\begin{equation} \\label{eq:w_update_INST}\n\\lim_{\\Delta u \\rightarrow 0} \\Delta w_{ij} = \\eta \\int_0^T \\left[ \\mathcal{Y}_i^{\\mathrm{ref}}(t) - \\mathcal{Y}_i(t) \\right] \\sum_{t_j^f \\in x_j} \\epsilon (t - t_j^f)\\, \\mathrm{d}t \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\lim_{\\Delta u\\rightarrow 0}\\Delta w_{ij}=\\eta\\int_{0}^{T}\\left[\\mathcal{Y}_{i%&#10;}^{\\mathrm{ref}}(t)-\\mathcal{Y}_{i}(t)\\right]\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon%&#10;(t-t_{j}^{f})\\,\\mathrm{d}t\\;,\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>\u2192</mo><mn>0</mn></mrow></munder><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mrow><mo>[</mo><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mi>ref</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mi>t</mi></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhich we term the \\acf{INST} synaptic plasticity rule, to reflect the discontinuous nature of the postsynaptic error signal which appears as the difference between two spike trains.\n\nIt is important to note that the above is closely related to the \\ac{PSD} plasticity rule proposed by \\cite{Yu2013} and the I-learning variant of the Chronotron in \\cite{Florian2012}; weight updates for \\ac{PSD} and I-learning depend on a presynaptic term combined with an instantaneous, postsynaptic error signal, as for \\ac{INST}, but differ in terms of their functional dependence on presynaptic inputs. Specifically, both \\ac{PSD} and I-learning rely on a synaptic current term $\\alpha$, defined similarly to that in Eq.~(\\ref{eq:LIF0_alpha_kernel}), instead of the above $\\epsilon$ term as defined by Eq.~(\\ref{eq:PSP_kernel}).\nThe \\ac{INST} rule is analytically more rigorous than both \\ac{PSD} and I-learning given that optimality criterion were taken as the starting point in its formulation, with the determination that the $\\epsilon$ term should act as a presynaptic factor. By contrast, the \\ac{PSD} rule was heuristically derived by \\cite{Yu2013} when adapting the Widrow-Hoff learning rule for application in single-layer spiking networks. The I-learning rule was initially derived from minimising the \\ac{VPD} \\cite{Victor1996} with respect to synaptic weights, but the author finally assumed a weighted synaptic current to act as a presynaptic factor.\n\n\\subsection*{FILTered-error (FILT) Synaptic Plasticity Rule} \\label{subsec:FILT_rule}\n\nAs it currently stands, the rate of synaptic weight change $\\dot{w}_{ij}(t)$ resulting from Eq.~(\\ref{eq:w_update_INST}) depends on the instantaneous difference between two spike trains $\\mathcal{Y}_i^{\\mathrm{ref}}$ and $\\mathcal{Y}_i$ during learning. In other words, weight updates are only effected at the precise moments in time when target or actual output spikes are present. Although this leads to the simplified batch weight update rule of Eq.~(\\ref{eq:w_update_INST_batch}), there are two distinct disadvantages to this approach. The first concerns the convergence of actual output spikes towards matching their desired target outputs: if the instantaneous error between two spike trains is communicated to every synapse during learning, then fluctuations in the changes of synaptic weights will inevitably emerge as an undesired by-product. It then becomes problematic for the network to smoothly converge towards a stable, non-oscillating output spike train while counteracting this source of synaptic noise. Secondly, from a biological standpoint it is implausible that synaptic weights can be effected instantaneously at the precise timings of output spikes. More realistically, it can be supposed that output spikes would leave some form of synaptic trace on the order of the membrane time constant, which might act as a postsynaptic `linkage' variable for coupling together temporally contiguous, or close together, target and actual output spikes.\n\nTo minimise the variance in synaptic weights arising from a discontinuous postsynaptic error signal we convolve the target and actual output spike trains in Eq.~(\\ref{eq:w_update_INST}) with an exponential filter, thereby providing the following learning rule:\n\n", "itemtype": "equation", "pos": 23628, "prevtext": "\nthat is now a function of the difference between a target and actual postsynaptic spike train. Finally, the integration of Eq.~(\\ref{eq:w_update_INST}) can be performed to provide a batch weight update rule:\n\n", "index": 35, "text": "\\begin{equation} \\label{eq:w_update_INST_batch}\n\\Delta w_{ij}^{\\mathrm{INST}} = \\eta \\Bigg[ \\sum_{\\tilde{t}_i^g \\in y_i^{\\mathrm{ref}}} \\, \\sum_{t_j^f \\in x_j} \\epsilon (\\tilde{t}_i^g - t_j^f) - \\sum_{t_i^h \\in y_i} \\, \\sum_{t_j^f \\in x_j} \\epsilon (t_i^h - t_j^f) \\Bigg] \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}^{\\mathrm{INST}}=\\eta\\Bigg{[}\\sum_{\\tilde{t}_{i}^{g}\\in y_{i}^{%&#10;\\mathrm{ref}}}\\,\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(\\tilde{t}_{i}^{g}-t_{j}^{f})%&#10;-\\sum_{t_{i}^{h}\\in y_{i}}\\,\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(t_{i}^{h}-t_{j}^%&#10;{f})\\Bigg{]}\\;,\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>INST</mi></msubsup></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo>\u2208</mo><msubsup><mi>y</mi><mi>i</mi><mi>ref</mi></msubsup></mrow></munder></mpadded><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mpadded width=\"+1.7pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder></mpadded><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere a convolved actual output spike train is equivalent to\n\n", "itemtype": "equation", "pos": 27184, "prevtext": "\nwhich we term the \\acf{INST} synaptic plasticity rule, to reflect the discontinuous nature of the postsynaptic error signal which appears as the difference between two spike trains.\n\nIt is important to note that the above is closely related to the \\ac{PSD} plasticity rule proposed by \\cite{Yu2013} and the I-learning variant of the Chronotron in \\cite{Florian2012}; weight updates for \\ac{PSD} and I-learning depend on a presynaptic term combined with an instantaneous, postsynaptic error signal, as for \\ac{INST}, but differ in terms of their functional dependence on presynaptic inputs. Specifically, both \\ac{PSD} and I-learning rely on a synaptic current term $\\alpha$, defined similarly to that in Eq.~(\\ref{eq:LIF0_alpha_kernel}), instead of the above $\\epsilon$ term as defined by Eq.~(\\ref{eq:PSP_kernel}).\nThe \\ac{INST} rule is analytically more rigorous than both \\ac{PSD} and I-learning given that optimality criterion were taken as the starting point in its formulation, with the determination that the $\\epsilon$ term should act as a presynaptic factor. By contrast, the \\ac{PSD} rule was heuristically derived by \\cite{Yu2013} when adapting the Widrow-Hoff learning rule for application in single-layer spiking networks. The I-learning rule was initially derived from minimising the \\ac{VPD} \\cite{Victor1996} with respect to synaptic weights, but the author finally assumed a weighted synaptic current to act as a presynaptic factor.\n\n\\subsection*{FILTered-error (FILT) Synaptic Plasticity Rule} \\label{subsec:FILT_rule}\n\nAs it currently stands, the rate of synaptic weight change $\\dot{w}_{ij}(t)$ resulting from Eq.~(\\ref{eq:w_update_INST}) depends on the instantaneous difference between two spike trains $\\mathcal{Y}_i^{\\mathrm{ref}}$ and $\\mathcal{Y}_i$ during learning. In other words, weight updates are only effected at the precise moments in time when target or actual output spikes are present. Although this leads to the simplified batch weight update rule of Eq.~(\\ref{eq:w_update_INST_batch}), there are two distinct disadvantages to this approach. The first concerns the convergence of actual output spikes towards matching their desired target outputs: if the instantaneous error between two spike trains is communicated to every synapse during learning, then fluctuations in the changes of synaptic weights will inevitably emerge as an undesired by-product. It then becomes problematic for the network to smoothly converge towards a stable, non-oscillating output spike train while counteracting this source of synaptic noise. Secondly, from a biological standpoint it is implausible that synaptic weights can be effected instantaneously at the precise timings of output spikes. More realistically, it can be supposed that output spikes would leave some form of synaptic trace on the order of the membrane time constant, which might act as a postsynaptic `linkage' variable for coupling together temporally contiguous, or close together, target and actual output spikes.\n\nTo minimise the variance in synaptic weights arising from a discontinuous postsynaptic error signal we convolve the target and actual output spike trains in Eq.~(\\ref{eq:w_update_INST}) with an exponential filter, thereby providing the following learning rule:\n\n", "index": 37, "text": "\\begin{equation} \\label{eq:w_update_conv}\n\\Delta w_{ij} = \\eta \\int_0^\\infty \\left[ \\tilde{\\mathcal{Y}}_i^{\\mathrm{ref}}(t) - \\tilde{\\mathcal{Y}}_i(t) \\right] \\sum_{t_j^f \\in x_j} \\epsilon (t - t_j^f)\\, \\mathrm{d}t \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}=\\eta\\int_{0}^{\\infty}\\left[\\tilde{\\mathcal{Y}}_{i}^{\\mathrm{ref}%&#10;}(t)-\\tilde{\\mathcal{Y}}_{i}(t)\\right]\\sum_{t_{j}^{f}\\in x_{j}}\\epsilon(t-t_{j%&#10;}^{f})\\,\\mathrm{d}t\\;,\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mrow><mo>[</mo><mrow><mrow><msubsup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>ref</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mi>t</mi></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nand a similar equivalence for a target output spike train $\\tilde{\\mathcal{Y}}^{\\mathrm{ref}}_i$. The decay time constant is set to $\\tau_q = \\SI{10}{ms}$, matched to the membrane time constant $\\tau_m$, which has been indicated to give increased performance from preliminary parameter sweeps. The upper limit of $\\infty$ in Eq.~(\\ref{eq:w_update_conv}) is necessary in order to ensure convergence of the learning rule. Performing the integration of Eq.~(\\ref{eq:w_update_conv}) using the \\ac{PSP} kernel given by Eq.~(\\ref{eq:PSP_kernel}) yields the batch weight update rule:\n\n", "itemtype": "equation", "pos": 27479, "prevtext": "\nwhere a convolved actual output spike train is equivalent to\n\n", "index": 39, "text": "\\begin{equation} \\label{eq:convolved_spike_train}\n\\tilde{\\mathcal{Y}}_i(t) \\equiv \\frac{1}{\\tau_q} \\int_0^t \\mathcal{Y}_i(t') \\exp\\left( \\frac{t - t'}{\\tau_c} \\right)  \\, \\mathrm{d}t' \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\mathcal{Y}}_{i}(t)\\equiv\\frac{1}{\\tau_{q}}\\int_{0}^{t}\\mathcal{Y}_{i}(%&#10;t^{\\prime})\\exp\\left(\\frac{t-t^{\\prime}}{\\tau_{c}}\\right)\\,\\mathrm{d}t^{\\prime%&#10;}\\;,\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mrow><mfrac><mn>1</mn><msub><mi>\u03c4</mi><mi>q</mi></msub></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>t</mi></msubsup><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mfrac><mrow><mi>t</mi><mo>-</mo><msup><mi>t</mi><mo>\u2032</mo></msup></mrow><msub><mi>\u03c4</mi><mi>c</mi></msub></mfrac><mo rspace=\"4.2pt\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mo>d</mo><mpadded width=\"+2.8pt\"><msup><mi>t</mi><mo>\u2032</mo></msup></mpadded></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere the membrane and synaptic coefficient terms are $\\mathcal{C}_m = \\frac{\\tau_m}{\\tau_m + \\tau_q}$ and $\\mathcal{C}_s = \\frac{\\tau_s}{\\tau_s + \\tau_q}$, respectively. We term the above the \\acf{FILT} synaptic plasticity rule, that depends on the smoothed difference between filtered target and actual output spike trains.\n\nEq.~(\\ref{eq:w_update_FILT}) bears a similarity with the \\ac{SPAN} learning rule, in the sense that weight updates depend on convolved input and output spike trains. However, as for the \\ac{PSD} learning rule, \\ac{SPAN} was formulated from adapting the Widrow-Hoff learning rule to networks of spiking neurons, and allowed for any arbitrary choice of kernel function with which to convolve input and output spike trains. In our analysis, input spike trains are optimally convolved with the \\ac{PSP} kernel of Eq.~(\\ref{eq:PSP_kernel}), although the exponential filtering of output spike trains is arbitrary. Selecting an exponential filter simplifies the resulting learning rule however, and coincidentally provides a resemblance of \\ac{FILT} to the \\acf{vRD} as is used to measure the (dis)similarity between neuronal spike trains \\cite{Rossum2001}. Biologically speaking, such filtered traces might originate from backpropagated action potentials which travel towards the neuron's afferent synapses as a result of postsynaptic spiking.\n\n\\section*{Results}\n\n\\subsection*{Analysis of the Learning Rules} \\label{sec:Plasticity_analysis}\n\nWe first analyse the optimality of synaptic weight modifications resulting from the \\ac{INST} and \\ac{FILT} learning rules under general conditions. For ease of analysis we examine just the weight change between a single pair of pre- and postsynaptic neurons: each emitting a single spike at times $t_j$ and $t_i$, respectively. A single target output spike at time $\\tilde{t}_i$ is also imposed, which must be matched by the postsynaptic neuron. \n\nThis subsection is organised as follows. First, simplified weight update rules for \\ac{INST} and \\ac{FILT} are presented based on single pre- and postsynaptic spiking. Next, three distinct scenarios for weight changes triggered by each learning rule are examined, including: the order in which the target and actual output spikes of the postsynaptic neuron occur, the relative timing difference between the target postsynaptic spike and presynaptic spike, and finally when the target and actual output spikes of the postsynaptic neuron are in close proximity with each other (temporally contiguous).\n\n\\paragraph{Synaptic weight updates for single spikes.}\nAccording to the definition of the \\ac{INST} rule in Eq.~(\\ref{eq:w_update_INST_batch}), the synaptic weight change triggered by single spikes is given by\n\n", "itemtype": "equation", "pos": 28259, "prevtext": "\nand a similar equivalence for a target output spike train $\\tilde{\\mathcal{Y}}^{\\mathrm{ref}}_i$. The decay time constant is set to $\\tau_q = \\SI{10}{ms}$, matched to the membrane time constant $\\tau_m$, which has been indicated to give increased performance from preliminary parameter sweeps. The upper limit of $\\infty$ in Eq.~(\\ref{eq:w_update_conv}) is necessary in order to ensure convergence of the learning rule. Performing the integration of Eq.~(\\ref{eq:w_update_conv}) using the \\ac{PSP} kernel given by Eq.~(\\ref{eq:PSP_kernel}) yields the batch weight update rule:\n\n", "index": 41, "text": "\\begin{align} \\label{eq:w_update_FILT}\n\\Delta w_{ij}^{\\mathrm{FILT}} &= \\epsilon_0\\, \\eta \\Bigg[ \\sum_{\\tilde{t}_i^g \\in y_i^{\\mathrm{ref}}} \\, \\sum_{t_j^f \\in x_j} \\exp\\Big(-\\frac{\\max\\{t_j^f,\\tilde{t}_i^g\\} - \\tilde{t}_i^g}{\\tau_q}\\Big) \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{\\max\\{t_j^f,\\tilde{t}_i^g\\} - t_j^f}{\\tau_m}\\Big) \\nonumber \\\\\n& \\quad - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{\\max\\{t_j^f,\\tilde{t}_i^g\\} - t_j^f}{\\tau_s}\\Big) \\bigg)\n- \\sum_{t_i^h \\in y_i} \\, \\sum_{t_j^f \\in x_j} \\exp\\Big(-\\frac{\\max\\{t_j^f,t_i^h\\} - t_i^h}{\\tau_q}\\Big) \\nonumber \\\\\n& \\quad \\times \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{\\max\\{t_j^f,t_i^h\\} - t_j^f}{\\tau_m}\\Big) - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{\\max\\{t_j^f,t_i^h\\} - t_j^f}{\\tau_s}\\Big) \\bigg) \\Bigg] \\;,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta w_{ij}^{\\mathrm{FILT}}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>FILT</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\eta\\Bigg{[}\\sum_{\\tilde{t}_{i}^{g}\\in y_{i}^{%&#10;\\mathrm{ref}}}\\,\\sum_{t_{j}^{f}\\in x_{j}}\\exp\\Big{(}-\\frac{\\max\\{t_{j}^{f},%&#10;\\tilde{t}_{i}^{g}\\}-\\tilde{t}_{i}^{g}}{\\tau_{q}}\\Big{)}\\bigg{(}\\mathcal{C}_{m}%&#10;\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j}^{f},\\tilde{t}_{i}^{g}\\}-t_{j}^{f}}{\\tau_{m}}%&#10;\\Big{)}\" display=\"inline\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mi>\u03b7</mi><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo>\u2208</mo><msubsup><mi>y</mi><mi>i</mi><mi>ref</mi></msubsup></mrow></munder></mstyle></mpadded><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></munder></mstyle><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>q</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-\\mathcal{C}_{s}\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j}^{f},\\tilde{t%&#10;}_{i}^{g}\\}-t_{j}^{f}}{\\tau_{s}}\\Big{)}\\bigg{)}-\\sum_{t_{i}^{h}\\in y_{i}}\\,%&#10;\\sum_{t_{j}^{f}\\in x_{j}}\\exp\\Big{(}-\\frac{\\max\\{t_{j}^{f},t_{i}^{h}\\}-t_{i}^{%&#10;h}}{\\tau_{q}}\\Big{)}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mi>g</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo><mo>-</mo><mstyle displaystyle=\"true\"><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo></mstyle><mpadded width=\"+1.7pt\"><msub><mi/><mrow><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo>\u2208</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mpadded><mstyle displaystyle=\"true\"><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo></mstyle><msub><mi/><mrow><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>\u2208</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></msub><mi>exp</mi><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>q</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad\\times\\bigg{(}\\mathcal{C}_{m}\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j}%&#10;^{f},t_{i}^{h}\\}-t_{j}^{f}}{\\tau_{m}}\\Big{)}-\\mathcal{C}_{s}\\,\\exp\\Big{(}-%&#10;\\frac{\\max\\{t_{j}^{f},t_{i}^{h}\\}-t_{j}^{f}}{\\tau_{s}}\\Big{)}\\bigg{)}\\Bigg{]}\\;,\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>\u00d7</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mi>t</mi><mi>i</mi><mi>h</mi></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mi>f</mi></msubsup></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nthat is simply the difference between two \\ac{PSP} kernels. From the above, a weight change is zero when the target and actual output spikes are aligned, i.e. when $t_i = \\tilde{t}_i$, and the left or right \\ac{PSP} terms are equal to zero if their respective arguments are negatively valued.\n\nThe \\ac{FILT} batch weight update rule of Eq.~(\\ref{eq:w_update_FILT}) can be solved for single pre- and postsynaptic spikes:\n\n", "itemtype": "equation", "pos": 31743, "prevtext": "\nwhere the membrane and synaptic coefficient terms are $\\mathcal{C}_m = \\frac{\\tau_m}{\\tau_m + \\tau_q}$ and $\\mathcal{C}_s = \\frac{\\tau_s}{\\tau_s + \\tau_q}$, respectively. We term the above the \\acf{FILT} synaptic plasticity rule, that depends on the smoothed difference between filtered target and actual output spike trains.\n\nEq.~(\\ref{eq:w_update_FILT}) bears a similarity with the \\ac{SPAN} learning rule, in the sense that weight updates depend on convolved input and output spike trains. However, as for the \\ac{PSD} learning rule, \\ac{SPAN} was formulated from adapting the Widrow-Hoff learning rule to networks of spiking neurons, and allowed for any arbitrary choice of kernel function with which to convolve input and output spike trains. In our analysis, input spike trains are optimally convolved with the \\ac{PSP} kernel of Eq.~(\\ref{eq:PSP_kernel}), although the exponential filtering of output spike trains is arbitrary. Selecting an exponential filter simplifies the resulting learning rule however, and coincidentally provides a resemblance of \\ac{FILT} to the \\acf{vRD} as is used to measure the (dis)similarity between neuronal spike trains \\cite{Rossum2001}. Biologically speaking, such filtered traces might originate from backpropagated action potentials which travel towards the neuron's afferent synapses as a result of postsynaptic spiking.\n\n\\section*{Results}\n\n\\subsection*{Analysis of the Learning Rules} \\label{sec:Plasticity_analysis}\n\nWe first analyse the optimality of synaptic weight modifications resulting from the \\ac{INST} and \\ac{FILT} learning rules under general conditions. For ease of analysis we examine just the weight change between a single pair of pre- and postsynaptic neurons: each emitting a single spike at times $t_j$ and $t_i$, respectively. A single target output spike at time $\\tilde{t}_i$ is also imposed, which must be matched by the postsynaptic neuron. \n\nThis subsection is organised as follows. First, simplified weight update rules for \\ac{INST} and \\ac{FILT} are presented based on single pre- and postsynaptic spiking. Next, three distinct scenarios for weight changes triggered by each learning rule are examined, including: the order in which the target and actual output spikes of the postsynaptic neuron occur, the relative timing difference between the target postsynaptic spike and presynaptic spike, and finally when the target and actual output spikes of the postsynaptic neuron are in close proximity with each other (temporally contiguous).\n\n\\paragraph{Synaptic weight updates for single spikes.}\nAccording to the definition of the \\ac{INST} rule in Eq.~(\\ref{eq:w_update_INST_batch}), the synaptic weight change triggered by single spikes is given by\n\n", "index": 43, "text": "\\begin{equation} \\label{eq:INST_spike}\n\\Delta w_{ij}^{\\mathrm{INST}} = \\eta \\left[ \\epsilon (\\tilde{t}_i - t_j) - \\epsilon (t_i - t_j) \\right] \\;,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}^{\\mathrm{INST}}=\\eta\\left[\\epsilon(\\tilde{t}_{i}-t_{j})-\\epsilon%&#10;(t_{i}-t_{j})\\right]\\;,\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>INST</mi></msubsup></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.3pt\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhich further simplifies to\n\n", "itemtype": "equation", "pos": 32325, "prevtext": "\nthat is simply the difference between two \\ac{PSP} kernels. From the above, a weight change is zero when the target and actual output spikes are aligned, i.e. when $t_i = \\tilde{t}_i$, and the left or right \\ac{PSP} terms are equal to zero if their respective arguments are negatively valued.\n\nThe \\ac{FILT} batch weight update rule of Eq.~(\\ref{eq:w_update_FILT}) can be solved for single pre- and postsynaptic spikes:\n\n", "index": 45, "text": "\\begin{align} \\label{eq:FILT_spike}\n\\Delta w_{ij}^{\\mathrm{FILT}} &= \\epsilon_0\\, \\eta \\Bigg[ \\exp\\Big(-\\frac{\\max\\{t_j,\\tilde{t}_i\\} - \\tilde{t}_i}{\\tau_q}\\Big) \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{\\max\\{t_j,\\tilde{t}_i\\} - t_j}{\\tau_m}\\Big) \\nonumber \\\\\n& \\quad - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{\\max\\{t_j,\\tilde{t}_i\\} - t_j}{\\tau_s}\\Big) \\bigg) - \\exp\\Big(-\\frac{\\max\\{t_j,t_i\\} - t_i}{\\tau_q}\\Big) \\nonumber \\\\\n& \\quad \\times \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{\\max\\{t_j,t_i\\} - t_j}{\\tau_m}\\Big) - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{\\max\\{t_j,t_i\\} - t_j}{\\tau_s}\\Big) \\bigg) \\Bigg] \\;,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta w_{ij}^{\\mathrm{FILT}}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>FILT</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\eta\\Bigg{[}\\exp\\Big{(}-\\frac{\\max\\{t_{j},\\tilde{t%&#10;}_{i}\\}-\\tilde{t}_{i}}{\\tau_{q}}\\Big{)}\\bigg{(}\\mathcal{C}_{m}\\,\\exp\\Big{(}-%&#10;\\frac{\\max\\{t_{j},\\tilde{t}_{i}\\}-t_{j}}{\\tau_{m}}\\Big{)}\" display=\"inline\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mi>\u03b7</mi><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>q</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-\\mathcal{C}_{s}\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j},\\tilde{t}_{i%&#10;}\\}-t_{j}}{\\tau_{s}}\\Big{)}\\bigg{)}-\\exp\\Big{(}-\\frac{\\max\\{t_{j},t_{i}\\}-t_{i%&#10;}}{\\tau_{q}}\\Big{)}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo><mo>-</mo><mi>exp</mi><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>q</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad\\times\\bigg{(}\\mathcal{C}_{m}\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j}%&#10;,t_{i}\\}-t_{j}}{\\tau_{m}}\\Big{)}-\\mathcal{C}_{s}\\,\\exp\\Big{(}-\\frac{\\max\\{t_{j%&#10;},t_{i}\\}-t_{j}}{\\tau_{s}}\\Big{)}\\bigg{)}\\Bigg{]}\\;,\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>\u00d7</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhen assuming all postsynaptic spikes follow the presynaptic spike: $\\tilde{t}_i, t_i > t_j$. From the above, it can be found that weight changes are zero when target and actual postsynaptic spikes are aligned; however, unlike the \\ac{INST} rule, a negative timing of a postsynaptic spike relative to a presynaptic spike can still elicit a change in the synaptic weight.\n\nEq.~(\\ref{eq:INST_spike}) or (\\ref{eq:FILT_spike2}) influences the placement of an actual postsynaptic spike by driving an increase or decrease in a postsynaptic neuron's membrane potential close to its firing threshold, via synaptic weight modification. For our choice of \\ac{PSP} function, an increase in the synaptic weight works to shift an actual spike backwards in time, and a decrease in the synaptic weight shifts an actual spike forwards in time. Hence, by this process, the aim of a trained neuron is to find an optimal synaptic weight value which minimises the temporal difference of an actual output spike with respect to its target. \n\nIn the rest of this subsection, we start by simply examining the synaptic weight change as a function of the order in which postsynaptic spikes occur, as well its dependence on individual spikes. Next, we explore the synaptic weight change as a function of the relative timing difference between a target postsynaptic spike and input presynaptic spike, and either in the absence or presence of an actual postsynaptic spike, to establish the temporal window of each synaptic plasticity rule. Finally, we examine in detail the effect of temporally contiguous postsynaptic spikes on synaptic plasticity, and indicate the importance of the \\acp{PSP} functional form in influencing the direction of synaptic weight changes. For demonstrative purposes the learning rate of the \\ac{INST} and \\ac{FILT} rule is set to unity here, although there is no qualitative change in the results for different values.\n\n\n\\paragraph{Order of postsynaptic spikes.}\nThe panels in Fig.~\\ref{fig2} illustrate the change in the synaptic weight under \\ac{INST} and \\ac{FILT} based on the existence / order of postsynaptic spiking for values $\\tilde{t}_i, t_i > t_j$:\n\\begin{enumerate}[label=(\\Alph*)]\n\\item only an existing target spike triggers potentiation, such that the future emission of a postsynaptic spike is encouraged,\n\\item only an existing actual spike triggers depression, which acts to suppress future postsynaptic spiking,\n\\item an actual spike following its target by \\SI{5}{ms}, that is generated at \\SI{20}{ms} after stimulus onset, triggers potentiation; this acts to shift a future postsynaptic spike backwards in time towards its target,\n\\item an actual spike preceding its target by \\SI{5}{ms}, that is generated at \\SI{15}{ms} after stimulus onset, triggers depression; this acts to shift a future postsynaptic spike forwards in time towards its target.\n\\end{enumerate}\nAdditionally, the third subplot in each panel shows the time course of the \\ac{FILT} rule's synaptic error signal, that is equal to the difference between the filtered target and actual postsynaptic spike trains (see the first integrand term in Eq.~(\\ref{eq:w_update_conv})).\n\\begin{figure}[p]\n\\includegraphics[scale=1]{Figs/Fig2}\n\\caption{\n{\\bf Illustration of the \\ac{INST} and \\ac{FILT} synaptic plasticity rules for a pair of pre- and postsynaptic neurons.} For each subplot within a panel such as (A), from top to bottom: the first subplot shows the order of existing target and actual output spikes at the postsynaptic neuron, second is the \\ac{PSP} generated due to a single presynaptic spike at $t_j = \\SI{0}{ms}$, third is the filtered postsynaptic error signal for the \\ac{FILT} rule, and the final panel shows the time course of iterative weight updates for each rule. In each separate panel, either one or both of the target and actual output spikes at the postsynaptic neuron are considered. This figure is inspired from \\cite{Florian2012}.\n}\n\\label{fig2}\n\\end{figure}\n\nFrom this figure, it is clear that the direction of synaptic weight changes are the same for both learning rules, but differ in terms of their magnitude: in all cases, weight changes triggered by the \\ac{FILT} rule are weaker. The reason for this becomes apparent when taking into account the shape of the \\ac{FILT} error signal, which is multiplicatively combined with the evoked \\ac{PSP} at each point in time to \\emph{smoothly} drive synaptic weight changes, rather than the rapid changes triggered by the \\ac{INST} rule. It is emphasised that both the \\ac{INST} and \\ac{FILT} rules are implementable as online-based learning methods according to Eqs.~(\\ref{eq:w_update_INST}) and (\\ref{eq:w_update_conv}), respectively, although the \\ac{FILT} batch update rule of Eq.~(\\ref{eq:FILT_spike2}) used here turns out to be computationally more efficient.\n\nAt this point it is necessary to discuss the relationship between the timing of an actual output spike fired by a postsynaptic neuron and the shape of a \\ac{PSP} evoked by an input spike. By itself, the synapse of Fig.~\\ref{fig2} would be incapable of allowing the postsynaptic neuron to precisely fire at its desired target timing since the target coincides with the falling segment of the \\ac{PSP} curve; effectively, the postsynaptic neuron can only fire a single output spike with a lag time up to the peak value of the \\ac{PSP} kernel, since this is the only region over which the neuron's membrane potential can be adjusted to cross its firing threshold from below. Despite this, the synapse is well capable of acting in concert with other synapses that coincide with the neuron's target timing, which is essential for distributing the synaptic load of a network during learning.\n\n\n\\paragraph{Relative timing between spikes.}\nShown in Fig.~\\ref{fig3} is the synaptic weight change for each learning rule as a function of the relative timing between a target postsynaptic spike and a presynaptic spike, denoted by $t^{\\mathrm{ref}} - t^{\\mathrm{pre}}$, including for negative relative timings. The top panels correspond to the absence of an actual postsynaptic spike, and the lower panels correspond to the presence of an actual postsynaptic spike; in this example, the actual spike is held at a fixed positive timing of \\SI{20}{ms} relative to the presynaptic spike, and is denoted by $t^{\\mathrm{post}} - t^{\\mathrm{pre}}$.\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig3}\n\\caption{\n{\\bf Dependence of synaptic weight change on the relative timing difference between a target postsynaptic spike and input presynaptic spike: $t^{\\mathrm{ref}}$ and $t^{\\mathrm{pre}}$, respectively.} Columns (A) and (B) correspond to the \\ac{INST} and \\ac{FILT} learning rules, respectively. {\\it Both columns:} The top panel shows the synaptic change as a function of the relative timing difference, but in the absence of an actual postsynaptic spike. The bottom panel corresponds to an actual postsynaptic spike $t^{\\mathrm{post}}$ which follows the presynaptic spike by \\SI{20}{ms}. The downwards shift in the synaptic change reflects depression triggered by the presence of an actual postsynaptic spike.\n}\n\\label{fig3}\n\\end{figure}\n\nFrom the top panel of Fig.~\\ref{fig3}A for the \\ac{INST} rule, it is observed that the plot of the synaptic change simply follows the form of a \\ac{PSP} kernel. In this case, the synaptic change is zero for negative values of the relative timing difference, demonstrating the causality of a presynaptic spike in eliciting a desired postsynaptic spike. Interestingly, the top panel of Fig.~\\ref{fig3}B for the \\ac{FILT} rule instead demonstrates a symmetrical dependence of synaptic change on the relative timing difference, in the absence of an actual spike, which is centred just right of the origin. This contrasts with the \\ac{INST} rule, and can be explained by the \\ac{FILT} rule instead working to minimise the \\emph{smoothed} difference between a target and actual spike train, rather than just their \\emph{instantaneous} difference; in other words, even if an actual postsynaptic spike cannot technically be aligned with its target, then a close match is deemed to be sufficient under \\ac{FILT}.\n\nEach lower panel of Fig.~\\ref{fig3} effectively shifts the plot in its respective upper panel downwards, and relates to the effect of synaptic depression triggered by the presence of an actual postsynaptic spike. In each lower panel it is worth noting the synaptic change about $t^{\\mathrm{post}} - t^{\\mathrm{pre}}$, where there is a cross-over point from positive to negative values: this region has the effect of shifting actual spikes generated by the postsynaptic neuron on successive trials towards their target timings, as was discussed previously in relation to Fig.~\\ref{fig2}. It is also noted that the magnitude of synaptic change is reduced for \\ac{FILT} in comparison with \\ac{INST}, for the same reasons as discussed previously.\n\n\\paragraph{Temporally contiguous postsynaptic spikes.}\nFinally, it is important to discuss weight changes resulting from target and actual postsynaptic spikes that are close together in time, and in particular with respect to the shape of an evoked \\ac{PSP}. To this end, we instead consider a postsynaptic neuron firing an actual output spike at a time $\\tilde{t}_i + \\Delta t_i$ in response to a single presynaptic spike at time $t_j$, where $\\Delta t_i$ is a time shift relative to the neuron's target output timing of $\\tilde{t}_i$. We also assume the conditions $\\tilde{t}_i > t_j$ and $\\tilde{t}_i + \\Delta t_i > t_j$, such that a postsynaptic spike always occurs after a presynaptic spike. The resulting weight change for each learning rule is calculated and discussed below.\n\nThe \\ac{INST} rule of Eq.~(\\ref{eq:INST_spike}) can be re-expressed in terms of shifted postsynaptic spikes:\n\n", "itemtype": "equation", "pos": 32961, "prevtext": "\nwhich further simplifies to\n\n", "index": 47, "text": "\\begin{align} \\label{eq:FILT_spike2}\n\\Delta w_{ij}^{\\mathrm{FILT}} &= \\epsilon_0\\, \\eta \\Bigg[ \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_m}\\Big) - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_s}\\Big) \\bigg) \\nonumber \\\\\n& \\quad - \\bigg( \\mathcal{C}_m\\, \\exp\\Big(-\\frac{t_i - t_j}{\\tau_m}\\Big) - \\mathcal{C}_s\\, \\exp\\Big(-\\frac{t_i - t_j}{\\tau_s}\\Big) \\bigg) \\Bigg] \\;,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta w_{ij}^{\\mathrm{FILT}}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>FILT</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\eta\\Bigg{[}\\bigg{(}\\mathcal{C}_{m}\\,\\exp\\Big{(}-%&#10;\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{m}}\\Big{)}-\\mathcal{C}_{s}\\,\\exp\\Big{(}-\\frac%&#10;{\\tilde{t}_{i}-t_{j}}{\\tau_{s}}\\Big{)}\\bigg{)}\" display=\"inline\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mi>\u03b7</mi><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-\\bigg{(}\\mathcal{C}_{m}\\,\\exp\\Big{(}-\\frac{t_{i}-t_{j}}{%&#10;\\tau_{m}}\\Big{)}-\\mathcal{C}_{s}\\,\\exp\\Big{(}-\\frac{t_{i}-t_{j}}{\\tau_{s}}\\Big%&#10;{)}\\bigg{)}\\Bigg{]}\\;,\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>m</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>s</mi></msub></mpadded><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nIf the \\ac{PSP} kernel of Eq.~(\\ref{eq:PSP_kernel}) is substituted into the above equation, the \\ac{INST} rule can be more explicitly expressed:\n\n", "itemtype": "equation", "pos": 43171, "prevtext": "\nwhen assuming all postsynaptic spikes follow the presynaptic spike: $\\tilde{t}_i, t_i > t_j$. From the above, it can be found that weight changes are zero when target and actual postsynaptic spikes are aligned; however, unlike the \\ac{INST} rule, a negative timing of a postsynaptic spike relative to a presynaptic spike can still elicit a change in the synaptic weight.\n\nEq.~(\\ref{eq:INST_spike}) or (\\ref{eq:FILT_spike2}) influences the placement of an actual postsynaptic spike by driving an increase or decrease in a postsynaptic neuron's membrane potential close to its firing threshold, via synaptic weight modification. For our choice of \\ac{PSP} function, an increase in the synaptic weight works to shift an actual spike backwards in time, and a decrease in the synaptic weight shifts an actual spike forwards in time. Hence, by this process, the aim of a trained neuron is to find an optimal synaptic weight value which minimises the temporal difference of an actual output spike with respect to its target. \n\nIn the rest of this subsection, we start by simply examining the synaptic weight change as a function of the order in which postsynaptic spikes occur, as well its dependence on individual spikes. Next, we explore the synaptic weight change as a function of the relative timing difference between a target postsynaptic spike and input presynaptic spike, and either in the absence or presence of an actual postsynaptic spike, to establish the temporal window of each synaptic plasticity rule. Finally, we examine in detail the effect of temporally contiguous postsynaptic spikes on synaptic plasticity, and indicate the importance of the \\acp{PSP} functional form in influencing the direction of synaptic weight changes. For demonstrative purposes the learning rate of the \\ac{INST} and \\ac{FILT} rule is set to unity here, although there is no qualitative change in the results for different values.\n\n\n\\paragraph{Order of postsynaptic spikes.}\nThe panels in Fig.~\\ref{fig2} illustrate the change in the synaptic weight under \\ac{INST} and \\ac{FILT} based on the existence / order of postsynaptic spiking for values $\\tilde{t}_i, t_i > t_j$:\n\\begin{enumerate}[label=(\\Alph*)]\n\\item only an existing target spike triggers potentiation, such that the future emission of a postsynaptic spike is encouraged,\n\\item only an existing actual spike triggers depression, which acts to suppress future postsynaptic spiking,\n\\item an actual spike following its target by \\SI{5}{ms}, that is generated at \\SI{20}{ms} after stimulus onset, triggers potentiation; this acts to shift a future postsynaptic spike backwards in time towards its target,\n\\item an actual spike preceding its target by \\SI{5}{ms}, that is generated at \\SI{15}{ms} after stimulus onset, triggers depression; this acts to shift a future postsynaptic spike forwards in time towards its target.\n\\end{enumerate}\nAdditionally, the third subplot in each panel shows the time course of the \\ac{FILT} rule's synaptic error signal, that is equal to the difference between the filtered target and actual postsynaptic spike trains (see the first integrand term in Eq.~(\\ref{eq:w_update_conv})).\n\\begin{figure}[p]\n\\includegraphics[scale=1]{Figs/Fig2}\n\\caption{\n{\\bf Illustration of the \\ac{INST} and \\ac{FILT} synaptic plasticity rules for a pair of pre- and postsynaptic neurons.} For each subplot within a panel such as (A), from top to bottom: the first subplot shows the order of existing target and actual output spikes at the postsynaptic neuron, second is the \\ac{PSP} generated due to a single presynaptic spike at $t_j = \\SI{0}{ms}$, third is the filtered postsynaptic error signal for the \\ac{FILT} rule, and the final panel shows the time course of iterative weight updates for each rule. In each separate panel, either one or both of the target and actual output spikes at the postsynaptic neuron are considered. This figure is inspired from \\cite{Florian2012}.\n}\n\\label{fig2}\n\\end{figure}\n\nFrom this figure, it is clear that the direction of synaptic weight changes are the same for both learning rules, but differ in terms of their magnitude: in all cases, weight changes triggered by the \\ac{FILT} rule are weaker. The reason for this becomes apparent when taking into account the shape of the \\ac{FILT} error signal, which is multiplicatively combined with the evoked \\ac{PSP} at each point in time to \\emph{smoothly} drive synaptic weight changes, rather than the rapid changes triggered by the \\ac{INST} rule. It is emphasised that both the \\ac{INST} and \\ac{FILT} rules are implementable as online-based learning methods according to Eqs.~(\\ref{eq:w_update_INST}) and (\\ref{eq:w_update_conv}), respectively, although the \\ac{FILT} batch update rule of Eq.~(\\ref{eq:FILT_spike2}) used here turns out to be computationally more efficient.\n\nAt this point it is necessary to discuss the relationship between the timing of an actual output spike fired by a postsynaptic neuron and the shape of a \\ac{PSP} evoked by an input spike. By itself, the synapse of Fig.~\\ref{fig2} would be incapable of allowing the postsynaptic neuron to precisely fire at its desired target timing since the target coincides with the falling segment of the \\ac{PSP} curve; effectively, the postsynaptic neuron can only fire a single output spike with a lag time up to the peak value of the \\ac{PSP} kernel, since this is the only region over which the neuron's membrane potential can be adjusted to cross its firing threshold from below. Despite this, the synapse is well capable of acting in concert with other synapses that coincide with the neuron's target timing, which is essential for distributing the synaptic load of a network during learning.\n\n\n\\paragraph{Relative timing between spikes.}\nShown in Fig.~\\ref{fig3} is the synaptic weight change for each learning rule as a function of the relative timing between a target postsynaptic spike and a presynaptic spike, denoted by $t^{\\mathrm{ref}} - t^{\\mathrm{pre}}$, including for negative relative timings. The top panels correspond to the absence of an actual postsynaptic spike, and the lower panels correspond to the presence of an actual postsynaptic spike; in this example, the actual spike is held at a fixed positive timing of \\SI{20}{ms} relative to the presynaptic spike, and is denoted by $t^{\\mathrm{post}} - t^{\\mathrm{pre}}$.\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig3}\n\\caption{\n{\\bf Dependence of synaptic weight change on the relative timing difference between a target postsynaptic spike and input presynaptic spike: $t^{\\mathrm{ref}}$ and $t^{\\mathrm{pre}}$, respectively.} Columns (A) and (B) correspond to the \\ac{INST} and \\ac{FILT} learning rules, respectively. {\\it Both columns:} The top panel shows the synaptic change as a function of the relative timing difference, but in the absence of an actual postsynaptic spike. The bottom panel corresponds to an actual postsynaptic spike $t^{\\mathrm{post}}$ which follows the presynaptic spike by \\SI{20}{ms}. The downwards shift in the synaptic change reflects depression triggered by the presence of an actual postsynaptic spike.\n}\n\\label{fig3}\n\\end{figure}\n\nFrom the top panel of Fig.~\\ref{fig3}A for the \\ac{INST} rule, it is observed that the plot of the synaptic change simply follows the form of a \\ac{PSP} kernel. In this case, the synaptic change is zero for negative values of the relative timing difference, demonstrating the causality of a presynaptic spike in eliciting a desired postsynaptic spike. Interestingly, the top panel of Fig.~\\ref{fig3}B for the \\ac{FILT} rule instead demonstrates a symmetrical dependence of synaptic change on the relative timing difference, in the absence of an actual spike, which is centred just right of the origin. This contrasts with the \\ac{INST} rule, and can be explained by the \\ac{FILT} rule instead working to minimise the \\emph{smoothed} difference between a target and actual spike train, rather than just their \\emph{instantaneous} difference; in other words, even if an actual postsynaptic spike cannot technically be aligned with its target, then a close match is deemed to be sufficient under \\ac{FILT}.\n\nEach lower panel of Fig.~\\ref{fig3} effectively shifts the plot in its respective upper panel downwards, and relates to the effect of synaptic depression triggered by the presence of an actual postsynaptic spike. In each lower panel it is worth noting the synaptic change about $t^{\\mathrm{post}} - t^{\\mathrm{pre}}$, where there is a cross-over point from positive to negative values: this region has the effect of shifting actual spikes generated by the postsynaptic neuron on successive trials towards their target timings, as was discussed previously in relation to Fig.~\\ref{fig2}. It is also noted that the magnitude of synaptic change is reduced for \\ac{FILT} in comparison with \\ac{INST}, for the same reasons as discussed previously.\n\n\\paragraph{Temporally contiguous postsynaptic spikes.}\nFinally, it is important to discuss weight changes resulting from target and actual postsynaptic spikes that are close together in time, and in particular with respect to the shape of an evoked \\ac{PSP}. To this end, we instead consider a postsynaptic neuron firing an actual output spike at a time $\\tilde{t}_i + \\Delta t_i$ in response to a single presynaptic spike at time $t_j$, where $\\Delta t_i$ is a time shift relative to the neuron's target output timing of $\\tilde{t}_i$. We also assume the conditions $\\tilde{t}_i > t_j$ and $\\tilde{t}_i + \\Delta t_i > t_j$, such that a postsynaptic spike always occurs after a presynaptic spike. The resulting weight change for each learning rule is calculated and discussed below.\n\nThe \\ac{INST} rule of Eq.~(\\ref{eq:INST_spike}) can be re-expressed in terms of shifted postsynaptic spikes:\n\n", "index": 49, "text": "\\begin{equation} \\label{eq:INST_shifted}\n\\Delta w_{ij}^{\\mathrm{INST}} = \\eta \\left[ \\epsilon (\\tilde{t}_i - t_j) - \\epsilon (\\tilde{t}_i + \\Delta t_i - t_j) \\right] \\;.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}^{\\mathrm{INST}}=\\eta\\left[\\epsilon(\\tilde{t}_{i}-t_{j})-\\epsilon%&#10;(\\tilde{t}_{i}+\\Delta t_{i}-t_{j})\\right]\\;.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>INST</mi></msubsup></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow></mrow><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.3pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nNext, by considering small time shifts $\\Delta t_i \\ll \\tau_m, \\tau_s$ such that the actual output spike is close to its target timing, the above equation can be simplified to give the final weight update rule:\n\n", "itemtype": "equation", "pos": 43501, "prevtext": "\nIf the \\ac{PSP} kernel of Eq.~(\\ref{eq:PSP_kernel}) is substituted into the above equation, the \\ac{INST} rule can be more explicitly expressed:\n\n", "index": 51, "text": "\\begin{align} \\label{eq:INST_shifted2}\n\\Delta w_{ij}^{\\mathrm{INST}} &= \\epsilon_0\\, \\eta \\Bigg[ \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_m}\\Big) \\bigg( 1 - \\exp\\Big(-\\frac{\\Delta t_i}{\\tau_m}\\Big) \\bigg) \\nonumber \\\\\n& \\quad - \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_s}\\Big) \\bigg( 1 - \\exp\\Big(-\\frac{\\Delta t_i}{\\tau_s}\\Big) \\bigg) \\Bigg] \\;.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta w_{ij}^{\\mathrm{INST}}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>INST</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\eta\\Bigg{[}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}%&#10;{\\tau_{m}}\\Big{)}\\bigg{(}1-\\exp\\Big{(}-\\frac{\\Delta t_{i}}{\\tau_{m}}\\Big{)}%&#10;\\bigg{)}\" display=\"inline\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mi>\u03b7</mi><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mn>1</mn><mo>-</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{s}}\\Big{)}%&#10;\\bigg{(}1-\\exp\\Big{(}-\\frac{\\Delta t_{i}}{\\tau_{s}}\\Big{)}\\bigg{)}\\Bigg{]}\\;.\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mn>1</mn><mo>-</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nInterestingly, we find that small lag times $\\tilde{t}_i - t_j < s^{\\mathrm{peak}}$ \\emph{depresses} a synapse for positive time shifts ($\\Delta t_i > 0$) and \\emph{potentiates} for negative time shifts ($\\Delta t_i < 0$), where $s^{\\mathrm{peak}} = \\frac{\\tau_m \\tau_s}{\\tau_m - \\tau_s} \\log \\left( \\frac{\\tau_m}{\\tau_s} \\right)$ is the lag time at which point the \\ac{PSP} kernel assumes its maximum value; in effect, a postsynaptic spike which initially follows its target will be driven to fire even later on successive trials, and a postsynaptic spike which initially precedes its target will be driven to fire even earlier on successive trials. Clearly this is undesirable when the objective is to train a postsynaptic neuron to precisely match a target timing, and is explained by the absence of a distinct treatment for coupling together temporally contiguous target and actual postsynaptic spikes. By contrast, increased lag times $\\tilde{t}_i - t_j > s^{\\mathrm{peak}}$ \\emph{potentiates} a synapse for positive time shifts ($\\Delta t_i > 0$) and \\emph{depresses} a synapse for negative time shifts ($\\Delta t_i < 0$), as is desired.\n\nTaken together, this analysis of the \\ac{INST} rule demonstrates erroneous synaptic changes over the rising segment of the \\ac{PSP} curve, but correct synaptic changes over the falling segment of the \\ac{PSP} curve as was examined earlier (see Fig.~\\ref{fig2}). The lag time at which point the \\ac{PSP} assumes its maximum value is $s^{\\mathrm{peak}} \\approx \\SI{7}{ms}$ when using our choice of parameters (Methods), and is visualised in Fig.~\\ref{fig1}B.\n\nWith respect to the \\ac{FILT} rule, weight updates due to Eq.~(\\ref{eq:FILT_spike2}) can instead be rewritten in terms of shifted postsynaptic spikes:\n\n", "itemtype": "equation", "pos": 44069, "prevtext": "\nNext, by considering small time shifts $\\Delta t_i \\ll \\tau_m, \\tau_s$ such that the actual output spike is close to its target timing, the above equation can be simplified to give the final weight update rule:\n\n", "index": 53, "text": "\\begin{equation} \\label{eq:INST_shifted3}\n\\Delta w_{ij}^{\\mathrm{INST}} = \\epsilon_0\\, \\eta\\, \\Delta t_i \\Bigg[ \\frac{1}{\\tau_m} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_m}\\Big) - \\frac{1}{\\tau_s} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_s}\\Big) \\Bigg] \\;.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}^{\\mathrm{INST}}=\\epsilon_{0}\\,\\eta\\,\\Delta t_{i}\\Bigg{[}\\frac{1}%&#10;{\\tau_{m}}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{m}}\\Big{)}-\\frac{1}{%&#10;\\tau_{s}}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{s}}\\Big{)}\\Bigg{]}\\;.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>INST</mi></msubsup></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>\u03b7</mi></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mrow><mrow><mfrac><mn>1</mn><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo>-</mo><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo>-</mo><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nwhere we have written in full the $\\mathcal{C}_m$ and $\\mathcal{C}_s$ coefficient terms. It is confirmed that when $\\Delta t_i = 0$, i.e. when an actual postsynaptic spike is aligned with its target timing, then the synaptic weight change is zero. Also, in taking the limit $\\Delta t_i \\rightarrow \\infty$ only synaptic potentiation results, since effectively only a target postsynaptic spike is coincident with the presynaptic spike.\n\nWe next consider small time shifts $\\Delta t_i \\ll \\tau_m, \\tau_s$, such that the above equation can factorised to give the final weight update rule:\n\n", "itemtype": "equation", "pos": 46093, "prevtext": "\nInterestingly, we find that small lag times $\\tilde{t}_i - t_j < s^{\\mathrm{peak}}$ \\emph{depresses} a synapse for positive time shifts ($\\Delta t_i > 0$) and \\emph{potentiates} for negative time shifts ($\\Delta t_i < 0$), where $s^{\\mathrm{peak}} = \\frac{\\tau_m \\tau_s}{\\tau_m - \\tau_s} \\log \\left( \\frac{\\tau_m}{\\tau_s} \\right)$ is the lag time at which point the \\ac{PSP} kernel assumes its maximum value; in effect, a postsynaptic spike which initially follows its target will be driven to fire even later on successive trials, and a postsynaptic spike which initially precedes its target will be driven to fire even earlier on successive trials. Clearly this is undesirable when the objective is to train a postsynaptic neuron to precisely match a target timing, and is explained by the absence of a distinct treatment for coupling together temporally contiguous target and actual postsynaptic spikes. By contrast, increased lag times $\\tilde{t}_i - t_j > s^{\\mathrm{peak}}$ \\emph{potentiates} a synapse for positive time shifts ($\\Delta t_i > 0$) and \\emph{depresses} a synapse for negative time shifts ($\\Delta t_i < 0$), as is desired.\n\nTaken together, this analysis of the \\ac{INST} rule demonstrates erroneous synaptic changes over the rising segment of the \\ac{PSP} curve, but correct synaptic changes over the falling segment of the \\ac{PSP} curve as was examined earlier (see Fig.~\\ref{fig2}). The lag time at which point the \\ac{PSP} assumes its maximum value is $s^{\\mathrm{peak}} \\approx \\SI{7}{ms}$ when using our choice of parameters (Methods), and is visualised in Fig.~\\ref{fig1}B.\n\nWith respect to the \\ac{FILT} rule, weight updates due to Eq.~(\\ref{eq:FILT_spike2}) can instead be rewritten in terms of shifted postsynaptic spikes:\n\n", "index": 55, "text": "\\begin{align} \\label{eq:FILT_shifted}\n\\Delta w_{ij}^{\\mathrm{FILT}} &= \\epsilon_0\\, \\eta \\Bigg[ \\frac{1}{\\tau_m + \\tau_q} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_m}\\Big) \\bigg( 1 - \\exp\\Big(-\\frac{\\Delta t_i}{\\tau_m}\\Big) \\bigg) \\nonumber \\\\\n& \\quad - \\frac{1}{\\tau_s + \\tau_q} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_s}\\Big) \\bigg( 1 - \\exp\\Big(-\\frac{\\Delta t_i}{\\tau_s}\\Big) \\bigg) \\Bigg] \\;,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta w_{ij}^{\\mathrm{FILT}}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>FILT</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\epsilon_{0}\\,\\eta\\Bigg{[}\\frac{1}{\\tau_{m}+\\tau_{q}}\\exp\\Big{(}%&#10;-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{m}}\\Big{)}\\bigg{(}1-\\exp\\Big{(}-\\frac{\\Delta&#10;t%&#10;_{i}}{\\tau_{m}}\\Big{)}\\bigg{)}\" display=\"inline\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mi>\u03b7</mi><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mi>\u03c4</mi><mi>m</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mi>q</mi></msub></mrow></mfrac></mstyle><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mn>1</mn><mo>-</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad-\\frac{1}{\\tau_{s}+\\tau_{q}}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-%&#10;t_{j}}{\\tau_{s}}\\Big{)}\\bigg{(}1-\\exp\\Big{(}-\\frac{\\Delta t_{i}}{\\tau_{s}}\\Big%&#10;{)}\\bigg{)}\\Bigg{]}\\;,\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mi>\u03c4</mi><mi>s</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mi>q</mi></msub></mrow></mfrac></mstyle><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mn>1</mn><mo>-</mo><mi>exp</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03649.tex", "nexttext": "\nFrom the above, we find that small lag times $\\tilde{t}_i - t_j < s^{\\mathrm{switch}}$ depresses a synapse for positive time shifts ($\\Delta t_i > 0$) and potentiates for negative time shifts ($\\Delta t_i < 0$), where $s^{\\mathrm{switch}} = \\frac{\\tau_m \\tau_s}{\\tau_m - \\tau_s} \\log \\left( \\frac{\\tau_m + \\tau_q}{\\tau_s + \\tau_q} \\right)$ is the lag time at which point the direction of a synaptic weight change is reversed; in this way, the behaviour of synaptic weight change about $s^{\\mathrm{switch}}$ is similar to that of $s^{\\mathrm{peak}}$ for the \\ac{INST} rule. The lag time $s^{\\mathrm{switch}}$ has a functional dependence on the filter time constant $\\tau_q$, such that $\\tau_q \\in [0, \\infty)$ is mapped to a latency of $s^{\\mathrm{switch}} \\in [s^{\\mathrm{peak}}, 0)$ as illustrated in Fig.~\\ref{fig4} for $\\tau_q \\leq \\SI{40}{ms}$. As discussed previously, it is desirable that synapses are \\emph{potentiated} for actual postsynaptic spikes \\emph{following} their targets, and are depressed otherwise; hence, decreasing $s^{\\mathrm{switch}}$ with respect to its parameter $\\tau_q$ should predictably lead to increased temporal precision of the \\ac{FILT} rule.\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig4}\n\\caption{\n{\\bf The lag time $s^\\mathrm{switch}$ at which point the direction of synaptic weight change under the \\ac{FILT} rule is reversed, plotted as a function of the filter time constant $\\tau_q$.} In this figure, the values of $\\tau_m$ and $\\tau_s$ used to determine $s^\\mathrm{switch}$ are \\SI{10}{ms} and \\SI{5}{ms}, respectively. At $\\tau_q = \\SI{0}{ms}$ the lag time $s^\\mathrm{switch}$ is equivalent to $s^\\mathrm{peak}$, that is the lag time corresponding to the maximum value of the \\ac{PSP} kernel. As a reference, the value $\\tau_q = \\SI{10}{ms}$ was selected for use in our computer simulations, which was indicated to give optimal performance on preliminary runs.\n}\n\\label{fig4}\n\\end{figure}\n\n\\paragraph{Summary.}\nThis subsection has analysed the dynamics of synaptic weight modifications driven by the \\ac{INST} and \\ac{FILT} rules, based on the order, relative timing difference and temporal precision of single target and actual postsynaptic spikes. These rules are analytically rigorous and have been predicted to give rise to desirable synaptic weight changes in most cases, and in particular for the \\ac{FILT} rule, while at the same time avoiding any distinct treatment concerning the dependence of the neuron's state on its presynaptic input about its firing threshold. Previous examples highlighting the challenges faced in determining the change in a neuron's state about its firing threshold can be found in \\cite{Bohte2002,Florian2012}, where typically a linear functional dependence is assumed, but which can lead to numerical instability and adds to the computational complexity of a learning rule. By contrast, simply convolving postsynaptic spikes with an exponential filter turns out to be sufficient in ensuring the convergence towards a solution.\n\n\\subsection*{Simulations} \\label{sec:Simulations}\n\nThis subsection presents results from computer simulations testing the performance of the \\ac{INST}, \\ac{FILT} and \\acf{CHRON} learning rules. The E-learning variant of the \\ac{CHRON} rule \\cite{Florian2012} is used in our simulations, being an ideal benchmark against which our derived rules can be compared; \\ac{CHRON} is ideal since it incorporates a mechanism for linking together target and actual postsynaptic spikes, analogous to the proposed \\ac{FILT} rule, as well as allowing for a very high network capacity in terms of the maximum number of input patterns it can learn to memorise \\cite{Florian2012}. It is worth noting that these three learning rules are essentially based on distinct, fundamental spike train error measures: the \\ac{INST} rule simply based on a momentary spike count error, the \\ac{FILT} rule based on a smoothed \\acl{vRD}-like error function \\cite{Rossum2001}, and the \\ac{CHRON} rule based on an adaptation of the \\acl{VPD} measure \\cite{Victor1996}.\n\n\\paragraph{Network setup.}\nIn all simulations, the network consisted of a single postsynaptic neuron receiving input spikes from a variable number $n_j$ of presynaptic neurons. The dynamics of the postsynaptic neuron's membrane potential $u_i$ was governed according to the \\ac{SRM} model defined by Eq.~(\\ref{eq:potential}), and output spikes were instantly generated when the neuron's membrane potential reached the formal firing threshold $\\vartheta$; hence, we implemented a deterministic adaptation of the stochastic neuron model presented in Eq.~(\\ref{eq:EXP_rate}), as necessitated by the derived \\ac{INST} and \\ac{FILT} learning rules. The internal simulation time step was taken as $\\delta t = \\SI{0.1}{ms}$ for temporal precision.\n\nThe synaptic weight between each presynaptic neuron $j$ and the postsynaptic neuron $i$ was initialised randomly at the start of every simulation run, with $w_{ij}$ values uniformly distributed between 0 and $200 / n_j$; as a result, the initial firing rate of the postsynaptic neuron was driven to $\\sim \\SI{1}{Hz}$.\n\nInput patterns were conveyed to the network by the collective firing activity of presynaptic neurons, where a pattern consisted of a single, uniformly distributed spike at each neuron; the choice of single rather than multiple input spikes to form pattern representations proved to be more amenable to the subsequent analysis of gathered results. In all cases, an arbitrary realisation of each pattern was used at the start of each simulation run, which was then held fixed thereafter. By this method, a total number $p$ of unique patterns were generated. Patterns were generated with a duration $T = \\SI{200}{ms}$, that is approximately the time-scale of sensory processing in the nervous system.\n\n\\paragraph{General learning task.}\nThe postsynaptic neuron was trained to reproduce a specified target output spike train in response to each of the $p$ input patterns through synaptic weight modifications in the network, using either the \\ac{INST}, \\ac{FILT} or \\ac{CHRON} learning rules. In this way, the network learned to perform precise temporal encoding of input patterns. During training, all $p$ input patterns were sequentially presented to the network in batches, where the completion of a batch corresponded to one epoch of learning. Resulting synaptic weight changes computed for each of the individually presented input patterns (or each trial) were accumulated, and applied at the end of an epoch.\n\nThe learning rate used for each of the rules was by default $\\eta = 600 / (n_j\\; n_s\\; p)$, where $n_s$ was the number of target output spikes; any exceptions to this are specified in the main text. As shall be shown in our simulation results, it was indicated that the learning rules shared a common, optimal value for the learning rate, thereby allowing less biased comparisons to be made between them in terms of their convergence speed.\n\n\\paragraph{Performing a single input-output mapping.}\nFor demonstrative purposes, we first applied the \\ac{INST} and \\ac{FILT} learning rules to training the network to perform a mapping between a single, fixed input spike pattern and a target output spike train containing four spikes. The network contained 200 presynaptic neurons, and the target output spikes were equally spaced out with timings: \\SIlist[list-units = single]{40; 80; 120; 160}{ms}. Simulations for the learning rule were run over 200 epochs, where each epoch corresponded to the repeated presentation of the pattern. Hence, a single simulation run represented a total \\SI{40}{s} of biological time.\n\n\\begin{figure}[p]\n\\includegraphics[scale=1]{Figs/Fig5}\n\\caption{\n{\\bf Two postsynaptic neurons trained under the proposed synaptic plasticity rules, that learned to map between a single, fixed input spike pattern and a four-spike target output train.} (A) A spike raster of an arbitrarily generated input pattern, lasting \\SI{200}{ms}, where each dot represents a spike. (B) Actual output spike rasters corresponding to the \\ac{INST} rule (left) and the \\ac{FILT} rule (right) in response to the repeated presentation of the input pattern. Target output spike times are indicated by crosses. (C) The evolution of the \\ac{vRD} for each learning rule, taken as a moving average over 40 independent simulation runs. The shaded regions show the standard deviation. This example uses equally spaced target output spike timings for clarity, but the proposed rules are similarly capable of learning irregular target timings.\n}\n\\label{fig5}\n\\end{figure}\nShown in Fig.~\\ref{fig5}A is a spike raster of an arbitrarily generated input pattern, consisting of a single input spike at each presynaptic neuron. In this example, two postsynaptic neurons were tasked with transforming the input pattern into the target output spike train through synaptic weight modifications, as determined by either the \\ac{INST} or \\ac{FILT} learning rule. From the actual output spike rasters depicted in panel B, it can be seen that both postsynaptic neurons learned to rapidly match their target responses during learning. Despite this, persistent fluctuations in the timings of actual output spikes were associated with just the \\ac{INST} rule, while the \\ac{FILT} displayed stability over the remaining epochs. Finally, panel C shows the accuracy of each learning rule, given as the average \\ac{vRD} plotted as a function of the number of learning epochs. With respect to the \\ac{INST} rule, it can be seen the \\ac{vRD} failed to reach zero and was subject to a high degree of variance, as reflected by the corresponding spike raster in panel B; its final, convergent \\ac{vRD} value was \\num{0.2 \\pm 0.2}, that is an output spike timing error of around \\SI{1}{ms} with respect to its target. By contrast, the FILT rule's \\ac{vRD} value rapidly approached zero, and was subject to much less variation during the entire course of learning.\n\n\\paragraph{Synaptic weight distributions.}\nShown in Fig.~\\ref{fig6} are the distributions of synaptic weights before and after network training for the \\ac{INST} and \\ac{FILT} learning rules, corresponding to the same experiment of Fig.~\\ref{fig5}. In plotting Fig.~\\ref{fig6}, synaptic weights were sorted in chronological order with respect to their associated presynaptic firing times; for example, the height of a bar at \\SI{40}{ms} reflects the average value of a synaptic weight between a presynaptic neuron which transmitted a spike at \\SI{40}{ms}. The gold overlaid lines correspond to the previously defined target output spike timings: \\SIlist[list-units = single]{40; 80; 120; 160}{ms}.\n\\begin{figure}[p]\n\\includegraphics[scale=1]{Figs/Fig6}\n\\caption{\n{\\bf Averaged synaptic weight values before and after network training, corresponding to the same experiment of Fig.~\\ref{fig5}.} The top panel is the distribution of weights before learning, the middle panel corresponds to post training under the \\ac{INST} rule, and the bottom panel the \\ac{FILT} rule. The gold coloured vertical lines indicate the target postsynaptic firing times. Results were averaged based on 40 independent runs. This figure is inspired from \\cite{Mohemmed2012}.\n}\n\\label{fig6}\n\\end{figure}\n\nFrom this figure, the upper panel illustrates the uniform distribution of synaptic weights used to initialise of the network before any learning took place, which had the effect of driving the initial postsynaptic firing rate to $\\sim \\SI{1}{Hz}$. The middle and lower panels show the distribution of synaptic weights at the end of learning, when the \\ac{INST} and \\ac{FILT} rules were respectively applied. From these two panels, a rapid increase in the synaptic weight values preceding the target output spike timings can be seen, which then proceeded to fall off. Comparatively, the magnitude of weight change was largest for the \\ac{INST} rule, with peak values over three times that produced by \\ac{FILT}. Furthermore, only the \\ac{INST} rule resulted in negative weight values, which is especially noticeable for weights associated with input spikes immediately following the target output spike timings. In effect, these sharp depressions offset the relatively strong input drive received by the postsynaptic neuron just before the target output spike timings, which is indicative of the unstable nature of the \\ac{INST} learning rule. By contrast, the \\ac{FILT} rule led to a `smoother landscape' of synaptic weight values, following a sinusoidal-like pattern when plotted in chronological order.\n\n\\paragraph{Dependence on the learning rate.}\nIn this experiment we explored the dependence of each rule on the learning rate parameter $\\eta$ in terms of the spike-timing accuracy of a trained postsynaptic neuron. The primary objective was to establish the relative sensitivity of the rules to large values of $\\eta$, and secondly to establish a value of $\\eta$ which provided a suitable trade-off between learning speed and final convergent accuracy. Here we first include the E-learning variant of the \\ac{CHRON} rule proposed by \\cite{Florian2012}, to provide a benchmark for the \\ac{INST} and \\ac{FILT} rules. \nWith respect to the experimental setup, the network consisted of 200 presynaptic neurons and was tasked with learning to map a total of 10 different input patterns to the same, single target output spike with a timing of \\SI{100}{ms}. In this case learning took place over 500 epochs.\n\nAs shown in Fig.~\\ref{fig7} it is clear that the \\ac{INST} rule was most sensitive to changes in the learning rate, with an average \\ac{vRD} value $2.5 \\times$ that of \\ac{FILT} for the largest learning rate value $\\eta = 1$. The least sensitive rule turned out to be \\ac{CHRON}, which still managed to maintain an average \\ac{vRD} value close to zero when plotted up to the maximum value of $\\eta$. Interestingly, all three accuracy plots displayed the same general trend over the entire range of learning rates considered: there was a rapid decrease for small $\\eta$ values, followed by a plateau up to around $\\eta = 0.5$, and then a noticeable increase towards the end. \n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig7}\n\\caption{\n{\\bf The \\ac{vRD} as a function of the learning rate $\\eta$ for each learning rule.} The E-learning variant of the \\acf{CHRON} rule of \\cite{Florian2012} is included as a benchmark for the \\ac{INST} and \\ac{FILT} rules. In every instance, a network containing 200 presynaptic neurons was tasked with mapping 10 arbitrary input patterns to the same target output spike with a timing of \\SI{100}{ms}. Learning took place over 500 epochs, and results were averaged over 40 independent runs. In this case, error bars show the standard error of the mean rather than the standard deviation: the \\ac{vRD} was subject to very high variance for large $\\eta$ values, therefore we considered just its average value and not its distribution.\n}\n\\label{fig7}\n\\end{figure}\n\nTo summarise, these results support our choice of an identical learning rate for all three learning rules as used in the subsequent learning tasks of this subsection. Additional, more exhaustive parameter sweeps (not shown) conclusively demonstrated that the learning rates for all three learning rules shared the same inverse proportionality with the number of presynaptic neurons, patterns and target output spikes. This corresponded to an optimal value of $\\eta = \\num{0.3 \\pm 0.1}$ in Fig.~\\ref{fig7}.\n\n\\paragraph{Classifying spike patterns.}\nAn important characteristic of a neural network is the maximum number of patterns it can learn to reliably memorise, as well the time taken to train it. Therefore, we tested the performance of the network on a generic classification task, where input patterns belonging to different classes were identified by the precise timings of individual postsynaptic spikes. We first determine the performance of a network when trained to identify separate classes of input patterns based on the precise timing of a \\emph{single} postsynaptic spike, and then later consider identifications based on \\emph{multiple} postsynaptic spike timings. In this experiment, the network was trained under the \\ac{INST}, \\ac{FILT} or \\ac{CHRON} learning rule for comparison purposes.\n\nThe network was tasked with learning to classify $p$ arbitrarily generated input patterns into five separate classes through hetero-association: an equal number of patterns were randomly assigned to each class, and all patterns belonging to the same class were identified using a shared target output spike timing. Hence, an input pattern was considered to be correctly identified if the postsynaptic neuron responded by firing just a single output spike that fell within $\\Delta t$ of its required target timing. The value of $\\Delta t$ was varied depending on the level of temporal precision desired, with values in the range of $\\Delta t \\in (0, 5]$ ms that correspond to the typical level of spike timing precision observed in the brain \\cite{Reich1997}. For each input class a target output spike time was randomly generated according to a uniform distribution that ranged in value between 40 and \\SI{200}{ms}; the lower bound of \\SI{40}{ms} was enforced, given previous evidence indicating that smaller values are harder to reproduce by an \\ac{SNN} \\cite{Florian2012,Mohemmed2012}. To ensure input classes were uniquely identified, target output spikes were distanced from each other by a \\ac{vRD} of at least 0.5, corresponding to a minimum timing separation of \\SI{7}{ms}.\n\n\\begin{figure}[p]\n\\includegraphics[scale=0.9]{Figs/Fig8}\n\\caption{\n{\\bf The classification performance of each learning rule as a function of the number of input patterns when learning to classify $p$ patterns into five separate classes.} Each input class was identified using a single, unique target output spike timing, which the postsynaptic neuron had to learn to match to within \\SI{1}{ms}. {\\it Left:} The epoch-averaged classification performance $\\langle P_c \\rangle$ for a network containing $n_j = 200, 400$ and 600 presynaptic neurons. {\\it Right:} The corresponding number of epochs taken by the network to reach a performance level of \\SI{90}{\\%}. More than 500 epochs was considered a failure by the network to learn all the patterns at the required performance level. Results were averaged over 20 independent runs, and error bars show the standard deviation.\n}\n\\label{fig8}\n\\end{figure}\nShown in the left column of Fig.~\\ref{fig8} is the performance of a network containing either 200, 400 or 600 presynaptic neurons, as a function of the number of input patterns to be classified. In this case, we took $\\Delta t = \\SI{1}{ms}$ as the required timing precision of a postsynaptic spike with respect to its target, for each input class. To quantify the classification performance of the network, we defined a measure $\\mathcal{P}_c$ which assumed a value of \\SI{100}{\\%} in the case of a correct pattern classification, and \\SI{0}{\\%} otherwise. Hence, in order to determine the maximum number of patterns memorisable by the network, we took an epoch-averaged performance level $\\langle \\mathcal{P}_c \\rangle > \\SI{90}{\\%}$ as our cut-off point when deciding whether all of the patterns were classified with sufficient reliability; this criterion was also used to determine the minimum number of epochs taken by the network to learn all the patterns, and is plotted in the right column of this figure. Epoch values not plotted for an increased number of patterns reflected an inability of the network to learn every pattern within 500 epochs.\n\nAs expected, Fig.~\\ref{fig8} demonstrates a decrease in the classification performance as the number of input patterns presented to the network was increased, with a clear dependence on the number of presynaptic neurons contained in the network. For example, a network trained under \\ac{INST} was able to classify 15, 30 and 40 patterns at a \\SI{90}{\\%} performance level when containing 200, 400 and 600 presynaptic neurons, respectively. The number of input patterns memorised by a network can be characterised by defining a load factor $\\alpha = p / n_j$, where $p$ is the number of patterns memorised by a network containing $n_j$ presynaptic neurons \\cite{Gutig2006}. Furthermore, the \\emph{maximum} number of patterns memorisable by a network can be quantified by its memory capacity $\\alpha_m = p_m / n_j$, where $p_m$ is the maximum number of patterns memorised using $n_j$ synapses. Hence, by taking \\SI{90}{\\%} as the cut-off point for reliable pattern classifications, we found the \\ac{INST} rule had an associated memory capacity of $\\alpha_m = \\num{0.07 \\pm 0.01}$. By comparison, the memory capacities for the \\ac{FILT} and \\ac{CHRON} rules were $\\num{0.14 \\pm 0.01}$ and $\\num{0.15 \\pm 0.01}$, respectively, being around twice the capacity of that determined for \\ac{INST}. Beyond these increased memory capacity values, networks trained under \\ac{FILT} or \\ac{CHRON} were capable of performance levels very close to \\SI{100}{\\%} when classifying a relatively small number of patterns; by contrast, the maximum performance level attainable under \\ac{INST} was just over \\SI{95}{\\%}, and was subject to a relatively large variance of around \\SI{5}{\\%}. Finally, it is evident from this figure that both \\ac{FILT} and \\ac{CHRON} shared roughly the same performance levels over the entire range of input patterns and network structures considered. In terms of the time taken to train the network, both \\ac{FILT} and \\ac{CHRON} were equally fast, while \\ac{INST} was typically slower than the other rules by a factor of between three and four. This difference in the training time became more pronounced as both the number of input patterns and presynaptic neurons were increased.\n\n\\paragraph{Memory capacity.}\nWe now explore in more detail the memory capacity $\\alpha_m$ supported under each learning rule, specifically with respect to its dependence on the output spike timing precision $\\Delta t$ used to identify input patterns. In determining the memory capacity as a function of the timing precision, we used the same experimental setup as considered previously for $\\Delta t = \\SI{1}{ms}$, but extended to also consider values of $\\Delta t$ between 0.2 and \\SI{5}{ms} (equally spaced in increments of \\SI{0.2}{ms}). As before, we assumed the maximum number of patterns memorisable by the network as those that were classified with a corresponding epoch-averaged classification performance $\\langle \\mathcal{P}_c \\rangle$ of at least \\SI{90}{\\%} within 500 epochs.\n\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig9}\n\\caption{\n{\\bf The memory capacity $\\alpha_m$ of each learning rule as a function of the required output spike timing precision.} The network was trained to classify input patterns into five separate classes within 500 epochs. Memory capacity values were determined based on networks containing $n_j = 200$, 400 and 600 presynaptic neurons. Results were averaged over 20 independent runs.\n}\n\\label{fig9}\n\\end{figure}\nFrom Fig.~\\ref{fig9} it can be seen that the memory capacity provided by each learning rule increased with the value of the timing precision, which eventually levelled off for values $\\Delta t > \\SI{3}{ms}$. It is also clear that the trend for the \\ac{FILT} rule is consistent with that for \\ac{CHRON} over the entire range of timing precision values considered, while the \\ac{INST} rule gave rise to the lowest memory capacities. For values $\\Delta t < \\SI{2}{ms}$ the difference in memory capacity between \\ac{INST} and \\ac{FILT} was most pronounced, to the extent that \\ac{INST} was incapable of memorising any input patterns for $\\Delta t < \\SI{0.8}{ms}$. By contrast, \\ac{FILT} still maintained a memory capacity close to 0.07 when classifying patterns based on ultra-precise output spike timings of within \\SI{0.2}{ms}. As a validation of our method, we note that our measured memory capacity for \\ac{CHRON} at a timing precision of \\SI{1}{ms} is in close agreement with that determined originally in Fig.~9A of \\cite{Florian2012}: with a value close to 0.15 after 500 epochs of network training.\n\n\\paragraph{Multiple target output spikes.}\nFinally, we examine the performance of the learning rules when input patterns are identified by the timings of \\emph{multiple} postsynaptic spikes. In this case, the network was trained to classify 10 input patterns into five separate classes, with two patterns belonging to each class. Both patterns belonging to a class were identified by the same target output spike train; hence, a correct pattern classification was considered when the number of actual output spikes fired by the postsynaptic neuron matched the number of target output spikes, and every actual spike fell within $\\Delta t$ of its respective target. For each input class, target output spikes were randomly generated according to a uniform distribution bound between 40 and \\SI{200}{ms}, as used previously. To ensure input classes were sufficiently separated from each other, target output spike trains were distanced by a scaled \\ac{vRD} of at least $n_s / 2$, where $n_s$ was the number of spikes contained in a target train.\n\n\\begin{figure}[t]\n\\includegraphics[scale=1]{Figs/Fig10}\n\\caption{\n{\\bf The classification performance of each learning rule as a function of the number of target output spikes used to identify input patterns.} The network was tasked when classifying 10 input patterns into 5 separate classes. Correct classifications were considered when the number of actual output spikes fired by the postsynaptic neuron matched that of its target, and each actual spike fell within \\SI{1}{ms} of its corresponding target timing. In this case, a network containing 200 presynaptic neurons was trained over an extended 1000 epochs to allow for decreased learning speed, and results were averaged over 20 independent runs.\n}\n\\label{fig10}\n\\end{figure}\nShown in Fig.~\\ref{fig10} is the performance of the network trained under each learning rule when classifying input patterns based on the precise timings of between one and five target output spikes, with a timing precision $\\Delta t = \\SI{1}{ms}$. Because the learning rate was inversely proportional to the number of target spikes, we extended the maximum number of epochs to 1000 to ensure the convergence of each rule. As can be seen in this figure, the performance dropped as the number of output spikes increased, and most noticeably for the \\ac{INST} rule which returned a minimum performance value approaching \\SI{0}{\\%} when patterns were identified using five output spikes. By comparison, the \\ac{CHRON} rule gave rise to the highest performance levels over the entire range of output spikes tested, closely followed by the \\ac{FILT} rule. If we count the maximum number of output spikes learnable by the network above a \\SI{90}{\\%} performance level, we obtain one, three and four output spikes for \\ac{INST}, \\ac{FILT} and \\ac{CHRON}, respectively, where the associated number of training epochs in each instance is plotted in the right panel of the figure. From this, it is observed that \\ac{CHRON} was fastest in training the network to learn multi-spike based pattern classifications, closely followed by \\ac{FILT} and finally \\ac{INST}.\n\n\\paragraph{Summary.}\nTaken together, the experimental results of this subsection demonstrate a similarity in the performance of the \\ac{FILT} and \\ac{CHRON} rules under most circumstances, except when applied to learning multiple target output spikes for which the \\ac{CHRON} rule was best suited. The \\ac{INST} rule, however, performed worst in all cases, and in particular displayed difficulties when classifying input patterns with increasingly fine temporal precision. This disparity between \\ac{INST} and the other two rules can be explained by its lack of a distinct treatment for shifting together neighbouring target and actual output spikes, which, as predicted in the previous subsection, was found to result in oscillatory postsynaptic spiking. Hence, it is evident that incorporating a mechanism for linking together output spikes in a learning rule confers a strong advantage when temporally precise encoding of input patterns is desired.\n\nFrom the experiment concerning pattern classifications based on multiple output spike timings, it was found for each of the learning rules that the performance decreased with the number of target output spikes. This is not surprising given that the network needed to match every one of its targets with the same level of temporal precision, effectively increasing the synaptic load imposed on the network during learning. Qualitatively, these results are consistent with those found in \\cite{Florian2012} for the \\ac{CHRON} rule.\n\n\\section*{Discussion}\n\nWe have studied the conditions under which supervised synaptic plasticity can most successfully be applied to training \\acp{SNN} to learn precise temporal encoding of input patterns. For this purpose, we have analytically derived two supervised learning rules, termed \\ac{INST} and \\ac{FILT}, and analysed the optimality of their solutions on several, generic, input-output spike timing association tasks. We have also extensively tested the proposed rules' performance in terms of the maximum number of spatio-temporal input patterns that are memorisable per synapse, with patterns identified based on the precise timing of an output spike emitted by a postsynaptic neuron. This latter experiment was designed to reflect experimental observations of biological neurons utilising a latency code. In order to benchmark the performance of our proposed rules, we also implemented the previously established E-learning \\ac{CHRON} rule.\nFrom our analysis, we found \\ac{FILT} approached the high performance level of \\ac{CHRON}: relating to its ability to smoothly converge towards an optimal solution by virtue of its postsynaptic spike timing linkage mechanism. By contrast, \\ac{INST} consistently returned the lowest performance, which was underpinned by its tendency to result in oscillations of emitted postsynaptic spikes about their target timings.\n\nEssentially, weight changes driven by the \\ac{INST} and \\ac{FILT} rules depend on a combination of two activity variables: a postsynaptic error term to signal appropriate output responses, and a presynaptic eligibility term to capture the coincidence of input spikes with the output error. \\ac{INST} and \\ac{FILT} differ, however, with respect to their postsynaptic error term: while \\ac{INST} relies on the instantaneous difference between a target and actual output spike train, \\ac{FILT} instead relies on the smoothed difference between an exponentially convolved target and actual output spike train. Despite this, both rules share the same presynaptic eligibility term, that is the \\ac{PSP} evoked due to an input spike. From our formulation, the \\ac{PSP} was analytically determined as the presynaptic factor, whereas the structurally similar \\ac{SPAN} and \\ac{PSD} rules instead rely on an arbitrarily defined presynaptic kernel that is typically related to the neuron's postsynaptic current \\cite{Mohemmed2012,Yu2013}. Interestingly, in the authors' analysis of the \\ac{SPAN} rule an alpha-shaped kernel was indicated as providing the best performance during learning, which closely resembles the shape of a \\ac{PSP} curve as used here. \n\n\nIn our analysis, we determined the \\ac{FILT} rule as generally giving rise to desired weight changes, hence providing an explanation for its high performance as tested through subsequent simulations. In more detail, \\ac{FILT} operates in such a way as to remove erroneously-timed output spikes, insert output spikes at desired target timings, and shift any remaining output spikes towards their targets if they are sufficiently close together in time. These three distinct operations bear a close resemblance to those carried out by the E-learning \\ac{CHRON} rule \\cite{Florian2012}, which also happens to be a highly efficient spike-based neural classifier. The \\ac{FILT} and \\ac{CHRON} rules differ, however, in terms of their implementation: while \\ac{FILT} is implementable as an online-based learning method, \\ac{CHRON} is only implementable offline given that it depends on discrete summations over cost functions derived from the \\ac{VPD} measure. Comparatively, the \\ac{INST} rule was prone to imperfect convergence during learning, which we attributed to its inability to effectively `link together' neighbouring target and actual output spikes.\n\nComputer simulations were run to test the performance of the \\ac{INST} and \\ac{FILT} rules in terms of their temporal encoding precision, including the \\ac{CHRON} rule for comparison purposes. We found \\ac{FILT} and \\ac{CHRON} were consistent with each other performance-wise, and largely outperformed \\ac{INST}. It is worth pointing out, however, that \\ac{FILT} is more straightforward to implement than \\ac{CHRON}, since it avoids the added complexity of having to establish whether target and actual output spikes are independent of each other or not based on the \\ac{VPD} measure \\cite{Florian2012}. By comparison, \\ac{INST} is the simplest rule to implement, but comes at the cost of significantly decreased spike timing precision. With respect to the learning tasks considered, networks were trained to classify input patterns using the precise timings of output spikes; an alternative and more practically oriented method for classifying patterns might instead take the minimum distance between target and actual output spike trains in order to discriminate between different input classes, which would more effectively counteract misclassifications in the case of input noise \\cite{Gardner2014a,Gardner2015}. In this work, however, we adopted a classification method based on the precise timings of output spikes for the sake of consistency with more directly related previous studies \\cite{Florian2012,Mohemmed2012,Yu2013}, and to more thoroughly compare the relative performance of each learning rule with respect to the precision of their temporal encoding.\n\n\\subsection*{Related Work}\n\nIn our approach, we started by taking gradient ascent on an objective function for maximising the likelihood of generating desired output spike trains, based on the statistical method of \\cite{Pfister2006}; this method is well suited to our analysis, especially since it has been shown to have a unique global maximum that is obtainable using a standard gradient ascent procedure \\cite{Paninski2004}. Next, we substituted the stochastic spiking neuron model used during the derivation with a deterministic \\ac{LIF} neuron model, such that output spikes were instead restricted to being generated upon crossing a fixed firing threshold. In this way, the resulting \\ac{INST} and \\ac{FILT} rules are theoretically well-justified, and yet allow for the efficient learning of desired sequences of precisely-timed output spikes. By comparison, most previous approaches to formulating supervised learning rules for \\acp{SNN} have relied on heuristic approximations, such as adapting the Widrow-Hoff rule for use with spiking neuron models \\cite{Ponulak2010,Mohemmed2012,Yu2013}, or mapping from Perceptron to spike-based learning \\cite{Albers2013,Xu2013}. Moreover, although the well known \\acf{ReSuMe} \\cite{Ponulak2010} can more rigorously be reinterpreted as a gradient descent learning procedure \\cite{Sporea2013}, assumptions are still made regarding the functional dependence of weight changes on the relative timing differences between spikes, for the purposes of mimicking a Hebbian-like \\ac{STDP} rule \\cite{Gerstner2002}.\n\nAlthough many of the aforementioned rules have demonstrated good performance when applied to various learning tasks, the heuristics used in their formulation makes it difficult to guarantee the optimality of their solutions in general. The main intention of this work has been to address this shortcoming.\n\nIt is highlighted that the proposed \\ac{INST} and \\ac{FILT} rules are capable of learning \\emph{multiple} target output spikes; this is in an important feature of any spike-based learning rule, and makes them more biologically relevant when considering that precise spike timings used by the nervous system to convey information represent a more fundamental unit of computation than that afforded by lengthier firing rates \\cite{VanRullen2005}. Multi-spike learning rules are a natural progression from single-spike rules, such as from the original SpikeProp algorithm which is restricted to learning single-spike target outputs \\cite{Bohte2002}, and the Tempotron which is only capable of learning to either fire or not-fire an output spike \\cite{Gutig2006}.\n\n\\subsection*{Biological Plausibility}\n\nOut of the rules studied here, we believe \\ac{FILT} matches most criteria to be considered of biological relevance: first, weight updates depend on pre- and postsynaptic activity variables that are locally available at each synapse. Second, its postsynaptic error term is communicated by a smoothly decaying signal that is based on the difference between filtered target and actual output spikes, which might arise from the concentration of a neuromodulator such as calcium influenced by backpropagated action potentials \\cite{Bush2012}. Finally, it is implementable as an online learning method, which is important when considering this is how information is most likely processed by the nervous system.\n\nAs with most existing learning rules for \\acp{SNN}, the proposed rules depend on the presence of a supervisory signal to guide synaptic weight modifications. Presently, it remains unclear where such a signal might originate from, representing the largest source of uncertainty regarding their biological plausibility. A possible explanation for supervised learning might come from so termed `referent activity templates' or spike patterns generated by neural circuits existing elsewhere in the brain, which are to be mimicked by circuits of interest during learning \\cite{Knudsen1994,Miall1996}. A further possibility, and one that is gaining increasing interest, is that supervised signals might actually represent an instantiation of reinforcement learning, but operating on a much smaller time-scale. Several, biologically meaningful learning rules have been proposed based on reward-modulated synaptic plasticity \\cite{Izhikevich2007,Farries2007,Fremaux2010}, and in our previous work we have successfully demonstrated how reinforcement learning can lead to the learning of multiple, and precisely timed, output spikes \\cite{Gardner2013}.\n\n\\section*{Conclusions}\n\nIn this paper, we have addressed the scarcity of existing learning rules for networks of spiking neurons that are theoretically well-justified, and which allow for the learning of \\emph{multiple} and \\emph{precisely-timed} output spikes. In particular, we have shown our proposed \\ac{FILT} rule, that is based on exponentially filtered output spike trains, to be a highly efficient, spike-based neural classifier. Classifiers based on a temporal code are of interest since they are theoretically more capable than those using a rate-based code, for example to process information on a much more rapid time-scale. \n\n\nIn our analysis, we have restricted our attention to relatively small network sizes in testing the performance of the proposed learning rules. Our main intention though, was to explore their potential for allowing optimal weight changes, rather the scaling of their performance with an increasing number of input synapses. However, it would be of increased biological significance to test the performance of a learning method as applied to a much larger network size: containing on the order of $10^4$ synapses per neuron as is typical in the nervous system. Practically, this could well be achieved via implementation in neuromorphic hardware, such as the massively-parallel computing architecture of SpiNNaker \\cite{Furber2014}. As a starting point, the \\ac{INST} rule would be most straightforward to implement in SpiNNaker, representing an achievable, and exciting, aim for future work.\n\n\\section*{Acknowledgments}\n\nThis work was supported by the Engineering and Physical Sciences Research Council (EPSRC, grant no. EP/J500562/1) and the European Community's Seventh Framework Programme (FP7/2007-2013, grant no. 604102 (HBP) -- The Human Brain Project).\n\n\\begin{thebibliography}{10}\n\n\\bibitem{VanRullen2005}\nvan Rullen R, Guyonneau R, Thorpe SJ.\n\\newblock Spike times make sense.\n\\newblock Trends in Neurosciences. 2005;28(1):1--4.\n\n\\bibitem{Gollisch2008}\nGollisch T, Meister M.\n\\newblock Rapid neural coding in the retina with relative spike latencies.\n\\newblock Science. 2008;319(5866):1108--1111.\n\n\\bibitem{Johansson2004}\nJohansson RS, Birznieks I.\n\\newblock First spikes in ensembles of human tactile afferents code complex\n  spatial fingertip events.\n\\newblock Nature Neuroscience. 2004;7(2):170--177.\n\n\\bibitem{Mainen1995}\nMainen ZF, Sejnowski TJ.\n\\newblock Reliability of spike timing in neocortical neurons.\n\\newblock Science. 1995;268(5216):1503--1506.\n\n\\bibitem{Reich1997}\nReich DS, Victor JD, Knight BW, Ozaki T, Kaplan E.\n\\newblock Response variability and timing precision of neuronal spike trains in\n  vivo.\n\\newblock Journal of Neurophysiology. 1997;77(5):2836--2841.\n\n\\bibitem{Uzzell2004}\nUzzell V, Chichilnisky E.\n\\newblock Precision of spike trains in primate retinal ganglion cells.\n\\newblock Journal of Neurophysiology. 2004;92(2):780--789.\n\n\\bibitem{Kasinski2006}\nKasinski A, Ponulak F.\n\\newblock Comparison of supervised learning methods for spike time coding in\n  spiking neural networks.\n\\newblock Int J Appl Math Comput Sci. 2006;16(1):101--113.\n\n\\bibitem{Gutig2014}\nG{\\\"u}tig R.\n\\newblock To spike, or when to spike?\n\\newblock Current Opinion in Neurobiology. 2014;25:134--139.\n\n\\bibitem{Mohemmed2012}\nMohemmed A, Schliebs S, Matsuda S, Kasabov N.\n\\newblock {SPAN}: Spike pattern association neuron for learning spatio-temporal\n  spike patterns.\n\\newblock International Journal of Neural Systems. 2012;22(04).\n\n\\bibitem{Yu2013}\nYu Q, Tang H, Tan KC, Li H.\n\\newblock Precise-spike-driven synaptic plasticity: Learning hetero-association\n  of spatiotemporal spike patterns.\n\\newblock PloS ONE. 2013;8(11):e78318.\n\n\\bibitem{Florian2012}\nFlorian RV.\n\\newblock The Chronotron: A Neuron That Learns to Fire Temporally Precise Spike\n  Patterns.\n\\newblock PLoS ONE. 2012;7(8):e40233.\n\n\\bibitem{Victor1996}\nVictor JD, Purpura KP.\n\\newblock Nature and precision of temporal coding in visual cortex: a\n  metric-space analysis.\n\\newblock Journal of Neurophysiology. 1996;76(2):1310--1326.\n\n\\bibitem{Pfister2006}\nPfister JP, Toyoizumi T, Barber D, Gerstner W.\n\\newblock Optimal spike-timing-dependent plasticity for precise action\n  potential firing in supervised learning.\n\\newblock Neural Computation. 2006;18(6):1318--1348.\n\n\\bibitem{Bi1998}\nBi G, Poo M.\n\\newblock Synaptic modifications in cultured hippocampal neurons: dependence on\n  spike timing, synaptic strength, and postsynaptic cell type.\n\\newblock The Journal of Neuroscience. 1998;18(24):10464--10472.\n\n\\bibitem{Fremaux2010}\nFr{\\'e}maux N, Sprekeler H, Gerstner W.\n\\newblock Functional requirements for reward-modulated spike-timing-dependent\n  plasticity.\n\\newblock The Journal of Neuroscience. 2010;30(40):13326--13337.\n\n\\bibitem{Gardner2015}\nGardner B, Sporea I, Gr{\\\"u}ning A.\n\\newblock Learning Spatiotemporally Encoded Pattern Transformations in\n  Structured Spiking Neural Networks.\n\\newblock Neural Computation. 2015;27(12):2548--2586.\n\n\\bibitem{Brea2013}\nBrea J, Senn W, Pfister JP.\n\\newblock Matching recall and storage in sequence learning with spiking neural\n  networks.\n\\newblock The Journal of Neuroscience. 2013;33(23):9565--9575.\n\n\\bibitem{JimenezRezende2014}\nRezende DJ, Gerstner W.\n\\newblock Stochastic variational learning in recurrent spiking networks.\n\\newblock Frontiers in Computational Neuroscience. 2014;8.\n\n\\bibitem{Gardner2013}\nGardner B, Gr{\\\"u}ning A.\n\\newblock Learning Temporally Precise Spiking Patterns through Reward Modulated\n  Spike-Timing-Dependent Plasticity.\n\\newblock In: {Artificial Neural Networks--ICANN 2013}. Springer; 2013. p.\n  256--263.\n\n\\bibitem{Gardner2014a}\nGardner B, Gr{\\\"u}ning A.\n\\newblock Classifying patterns in a spiking neural network.\n\\newblock In: {Proceedings of the 22nd European Symposium on Artificial Neural\n  Networks (ESANN 2014)}. Springer; 2014. p. 23--28.\n\n\\bibitem{Furber2014}\nFurber SB, Galluppi F, Temple S, Plana L, et~al.\n\\newblock The {SpiNNaker} project.\n\\newblock Proceedings of the IEEE. 2014;102(5):652--665.\n\n\\bibitem{Gerstner2002}\nGerstner W, Kistler WM.\n\\newblock Spiking neuron models: Single neurons, populations, plasticity.\n\\newblock Cambridge University Press; 2002.\n\n\\bibitem{Jolivet2006}\nJolivet R, Rauch A, L{\\\"u}scher HR, Gerstner W.\n\\newblock Predicting spike timing of neocortical pyramidal neurons by simple\n  threshold models.\n\\newblock Journal of Computational Neuroscience. 2006;21(1):35--49.\n\n\\bibitem{Paninski2004}\nPaninski L.\n\\newblock Maximum likelihood estimation of cascade point-process neural\n  encoding models.\n\\newblock Network: Computation in Neural Systems. 2004;15(4):243--262.\n\n\\bibitem{Rossum2001}\nvan Rossum MC.\n\\newblock A novel spike distance.\n\\newblock Neural Computation. 2001;13(4):751--763.\n\n\\bibitem{Bohte2002}\nBohte SM, Kok JN, La~Poutre H.\n\\newblock Error-backpropagation in temporally encoded networks of spiking\n  neurons.\n\\newblock Neurocomputing. 2002;48(1):17--37.\n\n\\bibitem{Gutig2006}\nG{\\\"u}tig R, Sompolinsky H.\n\\newblock The tempotron: a neuron that learns spike timing--based decisions.\n\\newblock Nature Neuroscience. 2006;9(3):420--428.\n\n\\bibitem{Ponulak2010}\nPonulak F, Kasinski A.\n\\newblock Supervised learning in spiking neural networks with resume: Sequence\n  learning, classification, and spike shifting.\n\\newblock Neural Computation. 2010;22(2):467--510.\n\n\\bibitem{Albers2013}\nAlbers C, Westkott M, Pawelzik K.\n\\newblock Perfect Associative Learning with Spike-Timing-Dependent Plasticity.\n\\newblock In: Advances in Neural Information Processing Systems; 2013. p.\n  1709--1717.\n\n\\bibitem{Xu2013}\nXu Y, Zeng X, Han L, Yang J.\n\\newblock A supervised multi-spike learning algorithm based on gradient descent\n  for spiking neural networks.\n\\newblock Neural Networks. 2013;43:99--113.\n\n\\bibitem{Sporea2013}\nSporea I, Gr{\\\"u}ning A.\n\\newblock Supervised learning in multilayer spiking neural networks.\n\\newblock Neural Computation. 2013;25(2):473--509.\n\n\\bibitem{Bush2012}\nBush D, Jin Y.\n\\newblock Calcium control of triphasic hippocampal {STDP}.\n\\newblock Journal of Computational Neuroscience. 2012;33(3):495--514.\n\n\\bibitem{Knudsen1994}\nKnudsen EI.\n\\newblock Supervised learning in the brain.\n\\newblock Journal of Neuroscience. 1994;14(7):3985--3997.\n\n\\bibitem{Miall1996}\nMiall RC, Wolpert DM.\n\\newblock Forward models for physiological motor control.\n\\newblock Neural Networks. 1996;9(8):1265--1279.\n\n\\bibitem{Izhikevich2007}\nIzhikevich EM.\n\\newblock Solving the distal reward problem through linkage of {STDP} and\n  dopamine signaling.\n\\newblock Cerebral Cortex. 2007;17(10):2443--2452.\n\n\\bibitem{Farries2007}\nFarries MA, Fairhall AL.\n\\newblock Reinforcement Learning With Modulated Spike Timing--Dependent\n  Synaptic Plasticity.\n\\newblock Journal of Neurophysiology. 2007;98(6):3648--3665.\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 47087, "prevtext": "\nwhere we have written in full the $\\mathcal{C}_m$ and $\\mathcal{C}_s$ coefficient terms. It is confirmed that when $\\Delta t_i = 0$, i.e. when an actual postsynaptic spike is aligned with its target timing, then the synaptic weight change is zero. Also, in taking the limit $\\Delta t_i \\rightarrow \\infty$ only synaptic potentiation results, since effectively only a target postsynaptic spike is coincident with the presynaptic spike.\n\nWe next consider small time shifts $\\Delta t_i \\ll \\tau_m, \\tau_s$, such that the above equation can factorised to give the final weight update rule:\n\n", "index": 57, "text": "\\begin{equation} \\label{eq:FILT_shifted2}\n\\Delta w_{ij}^{\\mathrm{FILT}} = \\epsilon_0\\, \\eta\\, \\Delta t_i \\Bigg[ \\frac{1}{\\tau_m + \\tau_q} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_m}\\Big) - \\frac{1}{\\tau_s + \\tau_q} \\exp\\Big(-\\frac{\\tilde{t}_i - t_j}{\\tau_s}\\Big) \\Bigg] \\;.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{ij}^{\\mathrm{FILT}}=\\epsilon_{0}\\,\\eta\\,\\Delta t_{i}\\Bigg{[}\\frac{1}%&#10;{\\tau_{m}+\\tau_{q}}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{m}}\\Big{)}-%&#10;\\frac{1}{\\tau_{s}+\\tau_{q}}\\exp\\Big{(}-\\frac{\\tilde{t}_{i}-t_{j}}{\\tau_{s}}%&#10;\\Big{)}\\Bigg{]}\\;.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>FILT</mi></msubsup></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>\u03f5</mi><mn>0</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>\u03b7</mi></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mrow><mrow><mfrac><mn>1</mn><mrow><msub><mi>\u03c4</mi><mi>m</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mi>q</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo>-</mo><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>m</mi></msub></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>\u03c4</mi><mi>s</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mi>q</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo>-</mo><mfrac><mrow><msub><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><msub><mi>\u03c4</mi><mi>s</mi></msub></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"260%\" minsize=\"260%\" rspace=\"5.3pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]