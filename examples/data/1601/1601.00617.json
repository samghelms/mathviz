[{"file": "1601.00617.tex", "nexttext": "\n\n\\item {\\index{functional width}\\trmbitx functional width:}\\quad\nGiven a set ${\\ensuremath{\\EuScript{{F}}}} = \\{f_1, \\ldots, f_n\\}$ of functions each from ${\\ensuremath{\\mathbb{R}}}^d$ to ${\\ensuremath{\\mathbb{R}}}$, the width at a point $x \\in {\\ensuremath{\\mathbb{R}}}^d$ is defined $\\omega_{{\\ensuremath{\\EuScript{{F}}}}}(x) = \\max_{f_i \\in {\\ensuremath{\\EuScript{{F}}}}} f_i(x) - \\min_{f_i \\in {\\ensuremath{\\EuScript{{F}}}}} f_i(x)$.  \n\n\\item {\\index{eps-kernel for functional width}\\trmbitx ${\\varepsilon}$-kernel for functional width:}\\quad\nGiven a set ${\\ensuremath{\\EuScript{{F}}}} = \\{f_1, \\ldots, f_n\\}$ of functions each from ${\\ensuremath{\\mathbb{R}}}^d$ to ${\\ensuremath{\\mathbb{R}}}$, an ${\\varepsilon}$-kernel coreset is a subset ${\\ensuremath{\\EuScript{{G}}}} \\subset {\\ensuremath{\\EuScript{{F}}}}$ such that for all $x \\in {\\ensuremath{\\mathbb{R}}}^d$ the functional width $\\omega_{{\\ensuremath{\\EuScript{{G}}}}}(x) \\geq (1-{\\varepsilon}) \\omega_{{\\ensuremath{\\EuScript{{F}}}}}(x)$.  \n\n\\item {\\index{faithful measure}\\trmbitx faithful measure:}\\quad\nA measure $\\mu$ is faithful if these exists a constant $c$, depending on $\\mu$, such that for any point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ any ${\\varepsilon}$-kernel coreset $Q$ of $P$ is a coreset for $\\mu$ with approximation parameter $c{\\varepsilon}$.  \n\n\\item {\\index{diameter}\\trmbitx diameter:}\\quad\nThe diameter of a point set $P$ is $\\max_{p,p' \\in P} \\|p-p'\\|$.  \n\n\\item {\\index{width}\\trmbitx width:}\\quad\nThe width of a point set $P$ is $\\min_{u \\in {\\ensuremath{\\mathbb{R}}}^d, \\|u\\|=1} \\omega(P,u)$.  \n\n\n\\item {\\index{spherical shell}\\trmbitx spherical shell:}\\quad\nFor a point $c \\in {\\ensuremath{\\mathbb{R}}}^d$ and real numbers $0 \\leq r \\leq R$, it is the closed region $\\sigma(c,r,R) = \\{x \\in {\\ensuremath{\\mathbb{R}}}^d \\mid r \\leq \\|x-c\\| \\leq R\\}$ between two concentric spheres of radius $r$ and $R$ centered at $c$.  Its \\emph{width} is defined $R - r$.  \n\n\\end{gllist}\n\n\\vspace{-.5pc}\n\n\n\\Bnn{Smallest Enclosing Ball Coreset}\n\n\\noindent \nGiven a point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ of size $n$, there exists a ${\\varepsilon}$-coreset for the smallest enclosing ball problem of size $\\lceil 2/{\\varepsilon} \\rceil$ that runs in time $O(nd/{\\varepsilon} + 1/{\\varepsilon}^5)$~\\cite{BC03}.  \nPrecisely, this finds a subset $S \\subset P$ with smallest enclosing ball $B(S)$ described by center point $c$ and radius $r$; it holds that if $r$ is expanded to $(1+{\\varepsilon})r$, then the ball with the same center would contain $P$.  \n\nThe algorithm is very simple and iterative:\n At each step, maintain the center $c_i$ of the current set $S_i$, add to $S_i$ the point $p_i \\in P$ furthest from $c_i$, and finally update $S_{i+1} = S_i \\cup \\{p_i\\}$ and $c_{i+1}$ as the center of smallest enclosing ball of $S_{i+1}$.  \nClarkson~\\cite{Cla10} discusses the connection to the Frank-Wolfe~\\cite{FW56} algorithm, and the generalizations towards several sparse optimization problems relevant for machine learning, for instance support vector machines~\\cite{TKC05} and polytope distance~\\cite{GJ09}.  \n \nThese algorithms do not work in a streaming setting, as they require $\\Omega(1/{\\varepsilon})$ passes over the data, but the runtime can be improved to $O((d/{\\varepsilon} + n/{\\varepsilon}^2)\\log(n/{\\varepsilon}))$ with high probability~\\cite{CHW10}.  Another approach~\\cite{AS15} maintains a set of $O((1/{\\varepsilon}^3) \\log(1/{\\varepsilon}))$ points in a stream that handles updates in $O((d/{\\varepsilon}^2) \\log (1/{\\varepsilon}))$ time, but is not a pure coreset since in order to handle updates, it needs to maintain these points as $O((1/{\\varepsilon}^2)\\log(1/{\\varepsilon}))$ different groups.    \n\n\n\n\\Bnn{Epsilon-Kernels Coreset for Width}\n\n\\noindent \nGiven point sets $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ of size $n$, an ${\\varepsilon}$-kernel coreset for directional width exists of size $O(1/{\\varepsilon}^{(d-1)/2})$~\\cite{AHV04} and can be constructed in $O(n + 1/{\\varepsilon}^{d-(3/2)})$ time~\\cite{Cha06,YAPV04}.  These algorithms are quite different than those for MEB, and the constants have heavy dependence on $d$ (in addition to it being in the exponent of $1/{\\varepsilon}$).  They first estimate the rough shape of the points so that they can be made fat (so width and diameter are $\\Theta(1)$) through an affine transform that does not change which points form a coreset.  Then they carefully choose a small set of points in the extremal directions.  \n\nIn a stream in ${\\ensuremath{\\mathbb{R}}}^d$, the ${\\varepsilon}$-kernel coreset can be computed using $O((1/{\\varepsilon}^{(d-1)/2}) \\cdot \\log(1/{\\varepsilon}))$ space with $O(1+ (1/{\\varepsilon}^{(d-3)/2})\\log(1/{\\varepsilon}))$ update time, which can be amortized to $O(1)$ update time~\\cite{ZZ08}.  In ${\\ensuremath{\\mathbb{R}}}^2$ this can be reduced to $O(1/\\sqrt{{\\varepsilon}})$ space and $O(1)$ update time~\\cite{AY07}.  \n\nSimilar to ${\\varepsilon}$-kernels for directional width, given a set of $n$ $d$-variate linear functions ${\\ensuremath{\\EuScript{{F}}}}$ and a parameter ${\\varepsilon}$, then an ${\\varepsilon}$-kernel for functional width can be computed of size $O(1/{\\varepsilon}^{d/2})$ in time $O(n + 1/{\\varepsilon}^{d-(1/2)})$~\\cite{AHV04,Cha06}.  \n\nMany other measures can be shown to have ${\\varepsilon}$-approximate coresets by showing they are \\emph{faithful}; this includes diameter, width, minimum enclosing cylinder, and minimum enclosing box.  \nStill other problems can be given ${\\varepsilon}$-approximate coresets by linearizing the inputs so they represent a set of $n$ linear functions in higher dimensions.  Most naturally this works for creating an ${\\varepsilon}$-kernel for the width of polynomial functions.  Similar linearization is possible for a slew of other shape-fitting problems including the minimum width spherical shell problem, overviewed nicely in a survey by Agarwal, Har-Peled and Varadarajan~\\cite{AHV07}.\n\n\nThese coresets can be extended to handle a small number of outliers~\\cite{HW04,AHY08} or uncertainty in the input~\\cite{LPW14}.\nA few approaches also extend to high dimensions, such as fitting a $k$-dimensional subspace~\\cite{HV04,BHR16}.  \n\n\n\n\n\n\n\n\n\n\\A{DENSITY ESTIMATION}\n\n\\noindent \nHere we consider a point set $P \\in {\\ensuremath{\\mathbb{R}}}^d$ which represents a discrete density function.  A coreset is then a subset $Q \\subset P$ such that $Q$ represents a similar density function to $P$ under a restricted family of ways to measure the density on subsets of the domain, e.g., defined by a range space.  \n\n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{range space}\\trmbitx range space:}\\quad \nA range space $(P,{\\ensuremath{\\EuScript{{A}}}})$ consists of a ground set $P$ and a family of ranges ${\\ensuremath{\\EuScript{{R}}}}$ of subsets from $P$.  In this chapter we consider ranges which are defined geometrically, for instance when $P$ is a point set and ${\\ensuremath{\\EuScript{{R}}}}$ are all subsets defined by a ball, that is any subset of $P$ which coincides with $P \\cap B$ for any ball $B$.  \t\n\n\n\\item {\\index{epsilon-net}\\trmbitx ${\\varepsilon}$-net:}\\quad \nGiven a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$, it is a subset $Q \\subset P$ so for any $R \\in {\\ensuremath{\\EuScript{{R}}}}$ such that $|R \\cap P| \\geq {\\varepsilon} |P|$, then $R \\cap Q \\neq \\emptyset$.  \n\n\n\\item {\\index{epsilon-approximation}\\trmbitx ${\\varepsilon}$-approximation (or ${\\varepsilon}$-sample):}\\quad \nGiven a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$, it is a subset $Q \\subset P$ so for all $R \\in {\\ensuremath{\\EuScript{{R}}}}$ it implies $\\left| \\frac{|R \\cap P|}{|P|} - \\frac{|R \\cap Q|}{|Q|} \\right| \\leq {\\varepsilon}$.  \n\n\\item {\\index{VC-dimension}\\trmbitx VC-dimension:}\\quad \nFor a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$ it is the size of the largest subset $Y \\subset P$ such that for each subset $Z \\subset Y$ it holds that $Z = Y \\cap R$ for some $R \\in {\\ensuremath{\\EuScript{{R}}}}$.  \n\\end{gllist}\n\n\n\n\\Bnn{RANDOM SAMPLING BOUNDS}\n\n\\noindent \nUnlike the shape fitting coresets, these can be constructed by simply selecting a large enough random sample of $P$.  The best such size bounds typically depend on VC-dimension $\\nu$~\\cite{VC71} (or shattering dimension $\\sigma$), which for many geometrically defined ranges (e.g., by balls, halfspaces, rectangles) is $\\Theta(d)$.  \n\nA random subset $Q \\subset P$ of size $O((1/{\\varepsilon}^2)(\\nu + \\log(1/\\delta))$~\\cite{LLS01} is an ${\\varepsilon}$-approximation of any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$, with probability at least $1-\\delta$.  \nA subset $Q \\subset P$ of size $O((\\nu/{\\varepsilon})\\log(1/{\\varepsilon}\\delta))$~\\cite{HW87} is an ${\\varepsilon}$-net of any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$, with probability at least $1-\\delta$.  \n\nThese bounds are of broad interest to learning theory, because they describe how many samples are needed to learn various sorts of classifiers.   In machine learning, it is typical to assume each data point $q \\in Q$ is drawn iid from some unknown distribution, and since the above bounds have no dependence on $n$, we can replace $P$ by any probability distribution with domain ${\\ensuremath{\\mathbb{R}}}^d$.  \nConsider that each point in $Q$ has a value from $\\{-,+\\}$, and a separator range (e.g., a halfspace) should ideally have all $+$ points inside, and all $-$ points outside.  \nThen for an ${\\varepsilon}$-approximation $Q$ of a range space $(P, {\\ensuremath{\\EuScript{{A}}}})$ the range $R \\in {\\ensuremath{\\EuScript{{R}}}}$ which misclassifies the fewest points on $Q$, misclassifies at most an ${\\varepsilon}$-fraction of points in $P$ more than the optimal separator does.  \nAn ${\\varepsilon}$-net (which requires far fewer samples) can make the same claim as long as there exists a separator in ${\\ensuremath{\\EuScript{{A}}}}$ that has zero misclassified points on $P$; it was recently shown~\\cite{Han15} a weak coreset for this problem only requires $O((1/{\\varepsilon})(\\nu + \\log(1/\\delta)))$ samples.    \n\nThe typical ${\\varepsilon}$-approximation bound provides an additive error of ${\\varepsilon}$ in estimating $|R \\cap P|/|P|$ with $|R \\cap Q|/|Q|$.  One can achieve a stronger \\emph{relative $(\\rho,{\\varepsilon})$-approximation} such that \n", "itemtype": "equation", "pos": 7597, "prevtext": "\n\\setcounter{chapter}{49}\n\n\\runhds{J.M. Phillips}\n{Chapter 49: Coresets and Sketches}\n\n\\thispagestyle{plain}\n\n\\vspace{-1pc}\n\n\\noindent\\hskip-3.72pc{\\hvbxxiv 49\\hskip 27pt CORESETS and SKETCHES}\n\n\\vspace{1pc}\n\n\\noindent{\\hvxiv Jeff M. Phillips}\n\n\\vspace{3.5pc}\n\n\\textheight=50pc\n\n\\Bnn{INTRODUCTION}\n\n\\noindent \nGeometric data summarization has become an essential tool in both geometric approximation algorithms and where geometry intersects with big data problems.  In linear or near-linear time large data sets can be compressed into a summary, and then more intricate algorithms can be run on the summaries whose results approximate those of the full data set.  \nCoresets and sketches are the two most important classes of these summaries.  \n\nA \\emph{coreset} is a reduced data set which can be used as proxy for the full data set; the same algorithm can be run on the coreset as the full data set, and the result on the coreset approximates that on the full data set.  \nIt is often required or desired that the coreset is a subset of the original data set, but in some cases this is relaxed.  \nA \\emph{weighted coreset} is one where each point is assigned a weight, perhaps different than it had in the original set.   \nA \\emph{weak coreset} associated with a set of queries is one where the error guarantee holds for a query which (nearly) optimizes some criteria, but not necessarily all queries; a \\emph{strong coreset} provides error guarantees for all queries.  \n\nA \\emph{sketch} is a compressed mapping of the full data set onto a data structure which is easy to update with new or changed data, and allows certain queries whose results approximate queries on the full data set.  \nA \\emph{linear sketch} is one where the mapping is a linear function of each data point, thus making it easy for data to be added, subtracted, or modified.  \n\nThese definitions can blend together, and some summaries can be classified as either or both.  The overarching connection is that the summary size will ideally depend only on the approximation guarantee but not the size of the original data set, although in some cases logarithmic dependence is acceptable.  \n\n\nWe focus on five types of coresets and sketches: \nshape-fitting (Section 49.1), \ndensity estimation (Section 49.2),  \nhigh-dimensional vectors (Section 49.3),\nhigh-dimensional point sets / matrices (Section 49.4), and\nclustering (Section 49.5).  \nThere are many other types of coresets and sketches (e.g., for graphs~\\cite{AGM12} or Fourier transforms~\\cite{IKP14}) which we do not cover for space or because they are less geometric.  \n\n\n\n\\Bnn{COMPUTATIONAL MODELS and PRIMATIVES}\n\n\\noindent\nOften the challenge is not simply to bound the size of a coreset or sketch as a function of the error tolerance, but to also do so efficiently and in a restricted model.  So before we discuss the specifics of the summaries, it will be useful to outline some basic computational models and techniques.  \n\nThe most natural fit is a \\emph{streaming model} that allows limited space (e.g., the size of the coreset or sketch) and where the algorithm can only make a single scan over the data.  \nMore generally, data may be distributed across different locations, and it will be useful to first compress the data into a coreset or sketch, and then share only these summaries across sites.  \nSome of the summaries (e.g., linear sketches) can easily be updated with the addition of a new data point or added together.  \nThe other most common approach relies on analysis surrounding the merge-reduce paradigm~\\cite{CM96,BS80}, which may require an extra log factor in space over direct methods.  \n\nGiven two summaries $S_1$ and $S_2$ with error bounds ${\\varepsilon}_1$ and ${\\varepsilon}_2$ (specific definitions vary widely, and will be discussed subsequently), typically one can show that the \\emph{merge} (generically denoted $\\oplus$, perhaps just the union) of the two summaries $S = S_1 \\oplus S_2$ has an error bound at most ${\\varepsilon} = {\\varepsilon}_1 + {\\varepsilon}_2$.  Summaries which admit a process $\\oplus$ for which $S$ has size $\\max\\{\\textsf{size}(S_1), \\textsf{size}(S_2)\\}$ and has error bound ${\\varepsilon} = \\max\\{{\\varepsilon}_1, {\\varepsilon}_2\\}$ are said be \\emph{mergeable}~\\cite{ACHPWY13}.  \nOtherwise, if one summarizes (or \\emph{reduces}) a summary $S$ with error bound ${\\varepsilon}$ to a smaller summary $S'$ that incurs additional error ${\\varepsilon}'$ (compared to $S$), then $S'$ typically can be shown to have an error bound ${\\varepsilon} + {\\varepsilon}'$ with respect to the original data.  \n\nPutting these \\emph{merge} and \\emph{reduce} operations together, an efficient merge-reduce framework obtains a summary of size $g$ (asymptotically, perhaps up to $\\log$ factors) from a data set of size $n$, as follows.  \nFirst arbitrarily divide the data into $n/g$ subsets, each of size $g$ (assume $n/g$ is a power of $2$, otherwise pad the data with dummy points).  Then in $\\log(n/g)$ rounds until there is one remaining set, perform each of the next two steps.  First pair up all remaining sets, and merge each pair using an $\\oplus$ operator.  Second, reduce each remaining set to be a summary of size $g$.  \n\nNotice that even if the merge or reduce step requires some polynomial $m^c$ time to process $m$ data points, this is only applied to sets of size at most $2g$, hence the full runtime is dominated by the first round as $(n/g)\\cdot(2g)^c = O(n \\cdot g^{c-1})$.  \nThe log factors increases in error can be folded into the size $g$, or in many cases removed by delaying some reduce steps, and careful bookkeeping~\\cite{CM96}. \n\nIn a streaming setting this framework is adapted by mapping data points to the $n/g$ subsets in the order they arrive, and then always completing as much of the merge-reduce process as possible given the data seen.  Another $\\log (n/g)$ space factor is incurred for those many summaries which can be active at any given time.  \n\n\n\n\n\n\n\n\\A{SHAPE FITTING}\n\n\\noindent \nIn this section we will discuss problems where given an input point set $P$, the goal is to find the best fitting shape from some class to $P$.  The two central problems in this area are the minimum (or smallest) enclosing ball, which has useful solutions in high dimensions, and the ${\\varepsilon}$-kernel coreset for directional width which approximates the convex hull but also can be transformed to solve many other problems.  \n\n\n\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{minimum enclosing ball}\\trmbitx minimum enclosing ball (MEB):}\\quad\nGiven a point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$, it is the smallest ball $B$ which contains $P$.  \n\n\\item {\\index{eps-approximate minimum enclosing ball problem}\\trmbitx ${\\varepsilon}$-approximate minimum enclosing ball problem:}\\quad\nGiven a point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$, and a parameter ${\\varepsilon} >0$, the problem is to find a ball $B$ whose radius is no larger than $(1+{\\varepsilon})$ times the radius of the MEB of $P$.\n\n\n\\item {\\index{direction width}\\trmbitx directional width:}\\quad \nGiven a point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ and a unit vector $u \\in {\\ensuremath{\\mathbb{R}}}^d$, then the \\emph{directional width} of $P$ in direction $u$ is $\\omega(P,u) = \\max_{p \\in P} \\langle p, u \\rangle - \\min_{p \\in P} \\langle p, u \\rangle$.  \n\n\\item {\\index{eps-kernel coreset}\\trmbitx ${\\varepsilon}$-kernel coreset:}\\quad  \nAn \\emph{${\\varepsilon}$-kernel coreset} of a point set $P \\in {\\ensuremath{\\mathbb{R}}}^d$ is subset $Q \\subset P$ so that for all unit vectors $u \\in {\\ensuremath{\\mathbb{R}}}^d$\n", "index": 1, "text": "\n\\[\n0 \\leq \\omega(P,u) - \\omega(Q,u) \\leq {\\varepsilon} \\omega(P,u).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"0\\leq\\omega(P,u)-\\omega(Q,u)\\leq{\\varepsilon}\\omega(P,u).\" display=\"block\"><mrow><mrow><mn>0</mn><mo>\u2264</mo><mrow><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nThis requires $O((1/\\rho{\\varepsilon}^2) (\\nu \\log (1/\\rho) + \\log(1/\\delta)))$ samples~\\cite{LLS01,HS11} to succeed with probability at least $1-\\delta$.   \n\n\n\n\n\\Bnn{DISCREPANCY-BASED RESULTS}\n\n\\noindent \nThe basic idea is to build a coloring on the ground set $\\chi : X \\to \\{-1,+1\\}$ to minimize $\\sum_{x \\in R} \\chi(x)$ over all ranges.  Then we can plug this into the merge-reduce process where merging takes the union and reducing discards the points colored $-1$.  Chazelle and Matousek~\\cite{CM96} showed how slight modifications of the merge-reduce framework can remove extra log factors in the approximation.  \n\nBased on discrepancy results (see Chapters 14 and 48) we can achieve the following bounds.  These assume $d \\geq 2$ is a fixed constant, and is absorbed in $O(\\cdot)$ notation.  \nFor any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$ (a fixed constant) we can construct an ${\\varepsilon}$-approximation of size $g = O(1/{\\varepsilon}^{2 - \\nu/(\\nu+1)})$ in $O(n \\cdot g^{w-1})$ time.  \nThis is tight for range spaces ${\\ensuremath{\\EuScript{{H}}}}_d$ defined by halfspaces in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = d$.  \nFor range spaces ${\\ensuremath{\\EuScript{{B}}}}_d$ defined by balls in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = d+1$ this can be improved slightly to $g = O(1/{\\varepsilon}^{2 - \\nu/(\\nu+1)} \\sqrt{\\log (1/{\\varepsilon})})$; it is unknown if the $\\log$ factor can be removed.  \nFor range spaces ${\\ensuremath{\\EuScript{{T}}}}_d$ defined by axis-aligned rectangles in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = 2d$, this can be greatly improved to $g = O((1/{\\varepsilon}) \\log^{d+ 1/2} (1/{\\varepsilon}))$ with the best lower bound as $g = O((1/{\\varepsilon}) \\log^{d - 1} (1/{\\varepsilon}))$ for $d \\geq 2$~\\cite{Lar14,MNT15}.  \nThese colorings can be constructed adapting techniques from Bansal~\\cite{Ban10,BS13}.  Generalization to various other settings (typically following one of these patterns) can be found in books by Matousek~\\cite{Mat10} and Chazelle~\\cite{Cha01}.  \nSimilar bounds are available in a stream adapting the merge-reduce framework~\\cite{BCEG04,STZ04}.  \n\nDiscrepancy based results also exist for constructing ${\\varepsilon}$-nets.  However often the improvement over the random sampling bounds are not as dramatic; the main advantage is that the algorithms are deterministic.  \nFor halfspaces in ${\\ensuremath{\\mathbb{R}}}^3$ and balls in ${\\ensuremath{\\mathbb{R}}}^2$ we can construct ${\\varepsilon}$-nets of size $O(1/{\\varepsilon})$~\\cite{MSW90,CV07,PR08}.  \nFor axis-aligned rectangles and fat objects we can construct ${\\varepsilon}$-nets of size $O((1/{\\varepsilon}) \\log \\log (1/{\\varepsilon}))$~\\cite{AES10}.  \nPach and Tardos~\\cite{PT13} then showed these results are tight, and that similar improvements cannot exist in higher dimensions.  \n\n\n\n\n\\Bnn{GENERALIZATIONS}\n\n\\noindent \nOne can also replace the set of ranges ${\\ensuremath{\\EuScript{{R}}}}$ with a family of kernels ${\\ensuremath{\\EuScript{{K}}}} = \\{K(x,\\cdot) \\mid x \\in {\\ensuremath{\\mathbb{R}}}^d\\}$ where $K : {\\ensuremath{\\mathbb{R}}}^d \\times {\\ensuremath{\\mathbb{R}}}^d \\to [0,1]$; that is it replaces the binary $\\{0,1\\}$ notion of inclusion with a continuous map $[0,1]$.  A family of kernels ${\\ensuremath{\\EuScript{{K}}}}$ is \\emph{linked} to a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$ if for every value $\\tau \\in [0,1]$ and every kernel $K(x,\\cdot)$, the points $\\{p \\in P \\mid K(x,p) \\geq \\tau\\} = R \\cap P$ for some $R \\in {\\ensuremath{\\EuScript{{R}}}}$.  For instance, centrally symmetric, shift-invariant kernels such as Gaussians are linked to the ranges space defined by balls.  \nThen an ${\\varepsilon}$-approximation $Q$ for $(P,{\\ensuremath{\\EuScript{{R}}}})$ is also an ${\\varepsilon}$-approximation for $(P,{\\ensuremath{\\EuScript{{K}}}})$~\\cite{JKPV11}, meaning that\n", "itemtype": "equation", "pos": 18014, "prevtext": "\n\n\\item {\\index{functional width}\\trmbitx functional width:}\\quad\nGiven a set ${\\ensuremath{\\EuScript{{F}}}} = \\{f_1, \\ldots, f_n\\}$ of functions each from ${\\ensuremath{\\mathbb{R}}}^d$ to ${\\ensuremath{\\mathbb{R}}}$, the width at a point $x \\in {\\ensuremath{\\mathbb{R}}}^d$ is defined $\\omega_{{\\ensuremath{\\EuScript{{F}}}}}(x) = \\max_{f_i \\in {\\ensuremath{\\EuScript{{F}}}}} f_i(x) - \\min_{f_i \\in {\\ensuremath{\\EuScript{{F}}}}} f_i(x)$.  \n\n\\item {\\index{eps-kernel for functional width}\\trmbitx ${\\varepsilon}$-kernel for functional width:}\\quad\nGiven a set ${\\ensuremath{\\EuScript{{F}}}} = \\{f_1, \\ldots, f_n\\}$ of functions each from ${\\ensuremath{\\mathbb{R}}}^d$ to ${\\ensuremath{\\mathbb{R}}}$, an ${\\varepsilon}$-kernel coreset is a subset ${\\ensuremath{\\EuScript{{G}}}} \\subset {\\ensuremath{\\EuScript{{F}}}}$ such that for all $x \\in {\\ensuremath{\\mathbb{R}}}^d$ the functional width $\\omega_{{\\ensuremath{\\EuScript{{G}}}}}(x) \\geq (1-{\\varepsilon}) \\omega_{{\\ensuremath{\\EuScript{{F}}}}}(x)$.  \n\n\\item {\\index{faithful measure}\\trmbitx faithful measure:}\\quad\nA measure $\\mu$ is faithful if these exists a constant $c$, depending on $\\mu$, such that for any point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ any ${\\varepsilon}$-kernel coreset $Q$ of $P$ is a coreset for $\\mu$ with approximation parameter $c{\\varepsilon}$.  \n\n\\item {\\index{diameter}\\trmbitx diameter:}\\quad\nThe diameter of a point set $P$ is $\\max_{p,p' \\in P} \\|p-p'\\|$.  \n\n\\item {\\index{width}\\trmbitx width:}\\quad\nThe width of a point set $P$ is $\\min_{u \\in {\\ensuremath{\\mathbb{R}}}^d, \\|u\\|=1} \\omega(P,u)$.  \n\n\n\\item {\\index{spherical shell}\\trmbitx spherical shell:}\\quad\nFor a point $c \\in {\\ensuremath{\\mathbb{R}}}^d$ and real numbers $0 \\leq r \\leq R$, it is the closed region $\\sigma(c,r,R) = \\{x \\in {\\ensuremath{\\mathbb{R}}}^d \\mid r \\leq \\|x-c\\| \\leq R\\}$ between two concentric spheres of radius $r$ and $R$ centered at $c$.  Its \\emph{width} is defined $R - r$.  \n\n\\end{gllist}\n\n\\vspace{-.5pc}\n\n\n\\Bnn{Smallest Enclosing Ball Coreset}\n\n\\noindent \nGiven a point set $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ of size $n$, there exists a ${\\varepsilon}$-coreset for the smallest enclosing ball problem of size $\\lceil 2/{\\varepsilon} \\rceil$ that runs in time $O(nd/{\\varepsilon} + 1/{\\varepsilon}^5)$~\\cite{BC03}.  \nPrecisely, this finds a subset $S \\subset P$ with smallest enclosing ball $B(S)$ described by center point $c$ and radius $r$; it holds that if $r$ is expanded to $(1+{\\varepsilon})r$, then the ball with the same center would contain $P$.  \n\nThe algorithm is very simple and iterative:\n At each step, maintain the center $c_i$ of the current set $S_i$, add to $S_i$ the point $p_i \\in P$ furthest from $c_i$, and finally update $S_{i+1} = S_i \\cup \\{p_i\\}$ and $c_{i+1}$ as the center of smallest enclosing ball of $S_{i+1}$.  \nClarkson~\\cite{Cla10} discusses the connection to the Frank-Wolfe~\\cite{FW56} algorithm, and the generalizations towards several sparse optimization problems relevant for machine learning, for instance support vector machines~\\cite{TKC05} and polytope distance~\\cite{GJ09}.  \n \nThese algorithms do not work in a streaming setting, as they require $\\Omega(1/{\\varepsilon})$ passes over the data, but the runtime can be improved to $O((d/{\\varepsilon} + n/{\\varepsilon}^2)\\log(n/{\\varepsilon}))$ with high probability~\\cite{CHW10}.  Another approach~\\cite{AS15} maintains a set of $O((1/{\\varepsilon}^3) \\log(1/{\\varepsilon}))$ points in a stream that handles updates in $O((d/{\\varepsilon}^2) \\log (1/{\\varepsilon}))$ time, but is not a pure coreset since in order to handle updates, it needs to maintain these points as $O((1/{\\varepsilon}^2)\\log(1/{\\varepsilon}))$ different groups.    \n\n\n\n\\Bnn{Epsilon-Kernels Coreset for Width}\n\n\\noindent \nGiven point sets $P \\subset {\\ensuremath{\\mathbb{R}}}^d$ of size $n$, an ${\\varepsilon}$-kernel coreset for directional width exists of size $O(1/{\\varepsilon}^{(d-1)/2})$~\\cite{AHV04} and can be constructed in $O(n + 1/{\\varepsilon}^{d-(3/2)})$ time~\\cite{Cha06,YAPV04}.  These algorithms are quite different than those for MEB, and the constants have heavy dependence on $d$ (in addition to it being in the exponent of $1/{\\varepsilon}$).  They first estimate the rough shape of the points so that they can be made fat (so width and diameter are $\\Theta(1)$) through an affine transform that does not change which points form a coreset.  Then they carefully choose a small set of points in the extremal directions.  \n\nIn a stream in ${\\ensuremath{\\mathbb{R}}}^d$, the ${\\varepsilon}$-kernel coreset can be computed using $O((1/{\\varepsilon}^{(d-1)/2}) \\cdot \\log(1/{\\varepsilon}))$ space with $O(1+ (1/{\\varepsilon}^{(d-3)/2})\\log(1/{\\varepsilon}))$ update time, which can be amortized to $O(1)$ update time~\\cite{ZZ08}.  In ${\\ensuremath{\\mathbb{R}}}^2$ this can be reduced to $O(1/\\sqrt{{\\varepsilon}})$ space and $O(1)$ update time~\\cite{AY07}.  \n\nSimilar to ${\\varepsilon}$-kernels for directional width, given a set of $n$ $d$-variate linear functions ${\\ensuremath{\\EuScript{{F}}}}$ and a parameter ${\\varepsilon}$, then an ${\\varepsilon}$-kernel for functional width can be computed of size $O(1/{\\varepsilon}^{d/2})$ in time $O(n + 1/{\\varepsilon}^{d-(1/2)})$~\\cite{AHV04,Cha06}.  \n\nMany other measures can be shown to have ${\\varepsilon}$-approximate coresets by showing they are \\emph{faithful}; this includes diameter, width, minimum enclosing cylinder, and minimum enclosing box.  \nStill other problems can be given ${\\varepsilon}$-approximate coresets by linearizing the inputs so they represent a set of $n$ linear functions in higher dimensions.  Most naturally this works for creating an ${\\varepsilon}$-kernel for the width of polynomial functions.  Similar linearization is possible for a slew of other shape-fitting problems including the minimum width spherical shell problem, overviewed nicely in a survey by Agarwal, Har-Peled and Varadarajan~\\cite{AHV07}.\n\n\nThese coresets can be extended to handle a small number of outliers~\\cite{HW04,AHY08} or uncertainty in the input~\\cite{LPW14}.\nA few approaches also extend to high dimensions, such as fitting a $k$-dimensional subspace~\\cite{HV04,BHR16}.  \n\n\n\n\n\n\n\n\n\n\\A{DENSITY ESTIMATION}\n\n\\noindent \nHere we consider a point set $P \\in {\\ensuremath{\\mathbb{R}}}^d$ which represents a discrete density function.  A coreset is then a subset $Q \\subset P$ such that $Q$ represents a similar density function to $P$ under a restricted family of ways to measure the density on subsets of the domain, e.g., defined by a range space.  \n\n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{range space}\\trmbitx range space:}\\quad \nA range space $(P,{\\ensuremath{\\EuScript{{A}}}})$ consists of a ground set $P$ and a family of ranges ${\\ensuremath{\\EuScript{{R}}}}$ of subsets from $P$.  In this chapter we consider ranges which are defined geometrically, for instance when $P$ is a point set and ${\\ensuremath{\\EuScript{{R}}}}$ are all subsets defined by a ball, that is any subset of $P$ which coincides with $P \\cap B$ for any ball $B$.  \t\n\n\n\\item {\\index{epsilon-net}\\trmbitx ${\\varepsilon}$-net:}\\quad \nGiven a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$, it is a subset $Q \\subset P$ so for any $R \\in {\\ensuremath{\\EuScript{{R}}}}$ such that $|R \\cap P| \\geq {\\varepsilon} |P|$, then $R \\cap Q \\neq \\emptyset$.  \n\n\n\\item {\\index{epsilon-approximation}\\trmbitx ${\\varepsilon}$-approximation (or ${\\varepsilon}$-sample):}\\quad \nGiven a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$, it is a subset $Q \\subset P$ so for all $R \\in {\\ensuremath{\\EuScript{{R}}}}$ it implies $\\left| \\frac{|R \\cap P|}{|P|} - \\frac{|R \\cap Q|}{|Q|} \\right| \\leq {\\varepsilon}$.  \n\n\\item {\\index{VC-dimension}\\trmbitx VC-dimension:}\\quad \nFor a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$ it is the size of the largest subset $Y \\subset P$ such that for each subset $Z \\subset Y$ it holds that $Z = Y \\cap R$ for some $R \\in {\\ensuremath{\\EuScript{{R}}}}$.  \n\\end{gllist}\n\n\n\n\\Bnn{RANDOM SAMPLING BOUNDS}\n\n\\noindent \nUnlike the shape fitting coresets, these can be constructed by simply selecting a large enough random sample of $P$.  The best such size bounds typically depend on VC-dimension $\\nu$~\\cite{VC71} (or shattering dimension $\\sigma$), which for many geometrically defined ranges (e.g., by balls, halfspaces, rectangles) is $\\Theta(d)$.  \n\nA random subset $Q \\subset P$ of size $O((1/{\\varepsilon}^2)(\\nu + \\log(1/\\delta))$~\\cite{LLS01} is an ${\\varepsilon}$-approximation of any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$, with probability at least $1-\\delta$.  \nA subset $Q \\subset P$ of size $O((\\nu/{\\varepsilon})\\log(1/{\\varepsilon}\\delta))$~\\cite{HW87} is an ${\\varepsilon}$-net of any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$, with probability at least $1-\\delta$.  \n\nThese bounds are of broad interest to learning theory, because they describe how many samples are needed to learn various sorts of classifiers.   In machine learning, it is typical to assume each data point $q \\in Q$ is drawn iid from some unknown distribution, and since the above bounds have no dependence on $n$, we can replace $P$ by any probability distribution with domain ${\\ensuremath{\\mathbb{R}}}^d$.  \nConsider that each point in $Q$ has a value from $\\{-,+\\}$, and a separator range (e.g., a halfspace) should ideally have all $+$ points inside, and all $-$ points outside.  \nThen for an ${\\varepsilon}$-approximation $Q$ of a range space $(P, {\\ensuremath{\\EuScript{{A}}}})$ the range $R \\in {\\ensuremath{\\EuScript{{R}}}}$ which misclassifies the fewest points on $Q$, misclassifies at most an ${\\varepsilon}$-fraction of points in $P$ more than the optimal separator does.  \nAn ${\\varepsilon}$-net (which requires far fewer samples) can make the same claim as long as there exists a separator in ${\\ensuremath{\\EuScript{{A}}}}$ that has zero misclassified points on $P$; it was recently shown~\\cite{Han15} a weak coreset for this problem only requires $O((1/{\\varepsilon})(\\nu + \\log(1/\\delta)))$ samples.    \n\nThe typical ${\\varepsilon}$-approximation bound provides an additive error of ${\\varepsilon}$ in estimating $|R \\cap P|/|P|$ with $|R \\cap Q|/|Q|$.  One can achieve a stronger \\emph{relative $(\\rho,{\\varepsilon})$-approximation} such that \n", "index": 3, "text": "\n\\[\n\\max_{R \\in {\\ensuremath{\\EuScript{{R}}}}} \\left| \\frac{|R \\cap P|}{|P|} - \\frac{|R \\cap Q|}{|Q|} \\right| \\leq {\\varepsilon} \\max\\left\\{\\rho, \\frac{|R \\cap P|}{|P|}\\right\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\max_{R\\in{\\mathcal{{R}}}}\\left|\\frac{|R\\cap P|}{|P|}-\\frac{|R\\cap Q|}{|Q|}%&#10;\\right|\\leq{\\varepsilon}\\max\\left\\{\\rho,\\frac{|R\\cap P|}{|P|}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>R</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u211b</mi></mrow></munder><mo>\u2061</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2229</mo><mi>P</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mi>P</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>-</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2229</mo><mi>Q</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><mo>|</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mi>\u03c1</mi><mo>,</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2229</mo><mi>P</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mi>P</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nSurprisingly, the discrepancy-based ${\\varepsilon}$-approximations are smaller for kernels than balls, for instance requiring only $O((1/{\\varepsilon}) \\sqrt{\\log(1/{\\varepsilon})})$ points in ${\\ensuremath{\\mathbb{R}}}^2$~\\cite{Phi13}.  Similarly an $({\\varepsilon}-\\tau)$-net $Q$ for $(P,{\\ensuremath{\\EuScript{{R}}}})$ is also an $(\\tau,{\\varepsilon})$-net for $(P, {\\ensuremath{\\EuScript{{K}}}})$~\\cite{PZ15}, meaning that for all $x \\in {\\ensuremath{\\mathbb{R}}}^d$ such that $\\frac{\\sum_{p \\in P} K(x,p)}{|P|} \\geq {\\varepsilon}$ then there exists some $q \\in Q$ such at $K(x,q) \\geq \\tau$.  \n\nSimilarly we can consider a single function $f \\in {\\ensuremath{\\EuScript{{F}}}}$~\\cite{Har06} or smaller set of $k$ functions $\\{f_1, \\ldots, f_k\\} \\subset {\\ensuremath{\\EuScript{{F}}}}$~\\cite{LS10} for various well-behaved function classes ${\\ensuremath{\\EuScript{{F}}}}$, and create a weighted coreset $Q \\subset P$, with weight function $w : Q \\to {\\ensuremath{\\mathbb{R}}}$, such that $\\sum_{q \\in Q} \\min_{i \\in [k]} w(q) f_i(q)$ approximates $\\sum_{p \\in P} \\min_{i \\in [k]} f_i(p)$.  These can be extended to handle outliers and robust loss functions like Huber and Tukey~\\cite{FS12} and Gaussian mixture models~\\cite{FFK11}, and as we will see, these techniques build the foundation for coresets for clustering~\\cite{FL11}.  These approaches can probably be extended to handle some classes of kernels by using ${\\ensuremath{\\EuScript{{F}}}} = \\{f(\\cdot) = 1-K(x,\\cdot) \\mid K(x,\\cdot) \\in {\\ensuremath{\\EuScript{{K}}}}\\}$.  \n\n\n\n\\Bnn{QUANTILES SKETCH}\n\n\\noindent \nA popular sketch for the so-called quantiles problem maps to $1$-dimensional ${\\varepsilon}$-approximations.  Define the \\emph{rank} of $v$ for set $X \\in {\\ensuremath{\\mathbb{R}}}$ as $\\textsf{rank}(X,v) = |\\{x \\in X \\mid x \\leq v\\}|$.  A quantiles sketch $S$ over a data set $X$ of size $n$ allows for queries such that $|S(v) - \\textsf{rank}(X,v)| \\leq {\\varepsilon} n$ for all $v \\in {\\ensuremath{\\mathbb{R}}}$.  \nThis is equivalent to an ${\\varepsilon}$-approximation of a range space $(X,{\\ensuremath{\\EuScript{{I}}}})$ where ${\\ensuremath{\\EuScript{{I}}}}$ is defined by half-open intervals of the form $(-\\infty,a]$.  \n\nA ${\\varepsilon}$-approximation coreset of size $1/{\\varepsilon}$ can be found by simply sorting $X$ and taking evenly spaced points in that sorted ordering.  Streaming sketches are also known; most famously the Greenwald-Khanna sketch~\\cite{GK01} which takes $O((1/{\\varepsilon}) \\log ({\\varepsilon} n))$ space, where $X$ is size $n$.  Recently a streaming sketch of size $O((1/{\\varepsilon})\\log(1/{\\varepsilon}))$ was discovered~\\cite{FO15} which is optimal.  \n\n\n\n\n\n\n\\A{HIGH DIMENSIONAL VECTORS}\n\n\\noindent \nIn this section we will consider high dimensional vectors $v = (v_1, v_2, \\ldots, v_d)$.\nWhen each $v_i$ is a positive integer, we can imagine these as the counts of a labeled set (the $d$ dimensions), and a subset of these objects is a coreset which can approximate the relative frequencies.  \nHence, we can consider weighted coresets over the dimensions, and consider approximating $v$ with another vector with few non-zero coordinates.  \nEven more generally, a sketch will compactly represent another vector $u$ which behaves similarly to $v$ under various norms.   \n\n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{ell-p norm}\\trmbitx $\\ell_p$-norm:}\\quad \nFor a vector $v \\in {\\ensuremath{\\mathbb{R}}}^d$ the $\\ell_p$ norm, for $p \\in [1,\\infty)$, is defined $\\|v\\|_p = (\\sum_{i=1}^d |v_i|^p)^{1/p}$.  For $p=0$ define $\\|v\\|_0 = |\\{i \\mid v_i \\neq 0\\}|$, the number of non-zero coordinates, and for $p=\\infty$ define $\\|v\\|_\\infty = \\max_{i=1}^d |v_i|$.  \n\n\\item {\\index{k-sparse}\\trmbitx $k$-sparse:}\\quad \nA vector is $k$-sparse if $\\|v\\|_0 \\leq k$.  \n\n\\item {\\index{additive ell_p/ell_q approximation}\\trmbitx additive $\\ell_p / \\ell_q$ approximation:}\\quad\nA vector $v$ has an additive  ${\\varepsilon}$-($\\ell_p/\\ell_q)$ approximation with vector $u$ if\n$\n\\|v-u\\|_p \\leq {\\varepsilon} \\|v\\|_q.\n$\n\n\\item {\\index{k-sparse ell_p/ell_q approximation}\\trmbitx $k$-sparse $\\ell_p / \\ell_q$ approximation:}\\quad\nA vector $v$ has a $k$-sparse ${\\varepsilon}$-($\\ell_p/\\ell_q)$ approximation with vector $u$ if $u$ is $k$-sparse and \n$\n\\|v-u\\|_p \\leq {\\varepsilon} \\|v-u\\|_q.\n$\n\n\\item {\\index{frequency count}\\trmbitx frequency count:}\\quad \nFor a vector $v = (v_1, v_2, \\ldots v_d)$  the value $v_i$ is called the $i$th frequency count of $v$.\n\n\\item {\\index{frequency moment}\\trmbitx frequency moment:}\\quad \nFor a vector $v = (v_1, v_2, \\ldots v_d)$  the value $\\|v\\|_p$ is called the $p$th frequency moment of $v$.\n\\end{gllist}\n\n\n\\Bnn{FREQUENCY APPROXIMATION}\n\n\\noindent \nThere are several types of coresets and sketches for frequency counts.  \nDerived by ${\\varepsilon}$-approximation and ${\\varepsilon}$-net bounds, we can create the following coresets over dimensions.  Assume $v$ has positive integer coordinates, and each coordinate's count $v_i$ represents $v_i$ distinct objects.  Then let $S$ be a random sample of size $k$ of these objects and $u(S)$ be an approximate vector defined so $u(S)_i = (\\|v\\|_1/k) \\cdot |\\{ s \\in S \\mid s = i\\}|$.  Then with $k = O((1/{\\varepsilon}^2) \\log(1/\\delta))$ we have $\\|v - u(S)\\|_\\infty \\leq {\\varepsilon} \\|v\\|_1$ (an additive ${\\varepsilon}$-$(\\ell_\\infty/\\ell_1)$ approximation) with probability at least $1-\\delta$.  Moreover, if $k = O((1/{\\varepsilon}) \\log(1/{\\varepsilon} \\delta))$ then for all $i$ such that $v_i \\geq {\\varepsilon} \\|v\\|_1$, then $u(S)_i \\neq 0$, and we can then measure the true count to attain a weighted coreset which is again an additive ${\\varepsilon}$-$(\\ell_\\infty / \\ell_1)$ approximation.  And in fact, there can be at most $1/{\\varepsilon}$ dimensions $i$ with $v_i \\geq {\\varepsilon} \\|v\\|_1$, so there always exists a weighted coreset of size $1/{\\varepsilon}$.  \n\nSuch a weighted coreset for additive ${\\varepsilon}$-($\\ell_\\infty/\\ell_1)$ approximations that is $(1/{\\varepsilon})$-sparse can be found deterministically in a stream via the Misra-Gries sketch~\\cite{mg-fre-82} (or other rediscovered variants~\\cite{metwally06,DLM02,KSP03}).  This approach keeps $1/{\\varepsilon}$ counters with associated labels.  For a new item, if it matches a label, the counter is incremented, else if any counter is $0$ it takes over that counter/label, and otherwise, (perhaps unintuitively) all counters are decremented.  \n\nThe count-min sketch~\\cite{CM05} also provides an additive ${\\varepsilon}$-($\\ell_\\infty / \\ell_1)$ approximation with space $O((1/{\\varepsilon}) \\log (1/\\delta))$ and is successful with probability $1-\\delta$.  \nA count-sketch~\\cite{CCM02} provides an additive ${\\varepsilon}$-($\\ell_\\infty / \\ell_2)$ approximation with space $O((1/{\\varepsilon}^2) \\log (1/\\delta))$, and is successful with probability at least $1-\\delta$.  \nBoth of these linear sketches operate by using $O(\\log 1/\\delta)$ hash functions, each mapping $[d]$ to one of $O(1/{\\varepsilon})$ or $O(1/{\\varepsilon}^2)$ counters.  The counters are incremented or decremented with the value $v_i$.  Then an estimate for $v_i$ can be recovered by examining all cells where $i$ hashes; the effect of other dimensions which hash there can be shown bounded.   \n\n\n\n\\paragraph{Frequency moments.}\nAnother common task is to approximate the frequency moments $\\|v\\|_p$.  For $p =1$, this is simply the count and can be done exactly in a stream.  The AMS Sketch~\\cite{AMS99} maintains a sketch of size $O((1/{\\varepsilon}^2) \\log (1/\\delta))$ that can derive a value $\\hat F_2$ so that $| \\|v\\|_2 - \\hat F_2 | \\leq {\\varepsilon} \\|v\\|_2$ with probability at least $1-\\delta$.  \n\nThe FM Sketch~\\cite{flajolet85:_probab} (and its extensions~\\cite{AMS99,DF03}) show how to create a sketch of size $O((1/{\\varepsilon}^2) \\log (1/\\delta))$ which can derive an estimate $\\hat F_0$ so that $| \\|v\\|_0 - \\hat F_0 | \\leq {\\varepsilon} \\|v\\|_1$ with probability at least $1-\\delta$.  \nThis works when $v_i$ are positive counts, and those counts are incremented one at a time in a stream.  \nUsually these sketches and coresets have implicit assumptions that a ``word'' can fit $\\log n$ bits where the stream is of size $n$, and is sufficient for each counter.  On the other hand, these $\\ell_0$ sketches operate with bits, and only have a hidden $\\log \\log n$ factor for bits.  \n\n  \n\n\\paragraph{$k$-sparse tail approximation.}\nSome sketches can achieve $k$-sparse approximations (which are akin to coresets of size $k$) and have stronger error bounds that depend only on the ``tail'' of the matrix; this is the class of $k$-sparse ${\\varepsilon}$-($\\ell_p/\\ell_q$) approximations.  See survey by Gilbert and Indyk for more details~\\cite{GI10}.  \n\nThese bounds are typically achieved by increasing the sketch size by a factor $k$, and then the $k$-sparse vector is the top $k$ of those elements.  The main recurring argument is roughly as follows:\n  If you maintain the top $1/{\\varepsilon}$ counters, then the largest counter not maintained is of size at most ${\\varepsilon} \\|v\\|$.  Similarly, if you first remove the top $k$ counters (a set $K = \\{i_1, i_2, \\ldots, i_k\\} \\subset [d]$, let their collective norm be $\\|v_K\\|$), then maintain $1/{\\varepsilon}$ more, the largest not-maintained counter is at most ${\\varepsilon} (\\|v\\| - \\|v_K\\|)$.  \nThe goal is then to sketch a $k$-sparse vector which approximates $v_K$; for instance the Misra-Gries Sketch~\\cite{mg-fre-82} and Count-Min sketch~\\cite{CM05} achieve $k$-sparse ${\\varepsilon}$-($\\ell_1/\\ell_1$)-approximations with $O(k/{\\varepsilon})$ counters, and the Count sketch~\\cite{CCM02} achieves $k$-sparse ${\\varepsilon}$-($\\ell_2/\\ell_2)$-approximations with $O(k^2/{\\varepsilon}^2)$ counters~\\cite{BCIS10}.  \n\n\n\n \n\n\n\n\\A{HIGH DIMENSIONAL POINT SETS (MATRICES)}\n\\label{sec:subspace-matrix}\n\n\\noindent\nMatrix sketching has gained a lot of interest due to its close connection to scalability issues in machine learning and data mining.  The goal is often to replace a matrix with a small space and low-rank approximation.  \nHowever, given a $n \\times d$ matrix $A$, it can also be imagined as $n$ points each in ${\\ensuremath{\\mathbb{R}}}^d$, and the span of a rank-$k$ approximation is a $k$-dimensional subspace that approximately includes all of the points.  \n\nMany of the techniques build on approaches from vector sketching, and again, many of the sketches are naturally interpretable as weighted coresets.  Sometimes it is natural to represent the result as a reduced set of rows in a $\\ell \\times d$ matrix $B$.  Other times it is more natural to consider the dimensionality reduction problem where the goal is an $n \\times c$ matrix $S$, and sometimes you do both!  But since these problems are typically phrased in terms of matrices, the difference comes down to simply transposing the input matrix.  \n\nWe will write all results as approximating an $n \\times d$ matrix $A$ using fewer rows, for instance, with an $\\ell \\times d$ matrix $B$.  This allows one to answer questions about a unit vector $x \\in {\\ensuremath{\\mathbb{R}}}^d$.  Geometrically, we can preserve \nthe variation along direction $x$ ($\\|A x\\|^2$) \nand \nhow close is $x$ to the best rank $k$ subspace of $A$.  \n\nNotoriously this problem can be solved optimally using the numerical linear algebra technique the \\emph{singular value decomposition} in $O(nd^2)$ time.  The challenges are then to compute this more efficiently in streaming and other related settings.  \n\nWe will describe three basic approaches (row sampling, random projections, and iterative SVD variants), and then some extensions and applications~\\cite{FT15}.   The first two approaches are mainly randomized, and we will describe results with constant probability, and for the most part these bounds can be made to succeed with any probability $1-\\delta$ by increasing the size by a factor $\\log(1/\\delta)$.  \n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{matrix rank}\\trmbitx matrix rank:}\\quad\nThe \\emph{rank} of an $n \\times d$ matrix $A$, denoted $\\textsf{rank}(A)$, is the smallest $k$ such that all rows (or columns) lie in a $k$-dimensional subspace of ${\\ensuremath{\\mathbb{R}}}^d$ (or ${\\ensuremath{\\mathbb{R}}}^n$).  \n\n\\item {\\index{singular value decomposition}\\trmbitx singular value decomposition:}\\quad \nGiven an $n \\times d$ matrix $A$, the singular value decomposition is a product $U S V^T$ where $U$ and $V$ are orthogonal, and $S$ is diagonal.  $U$ is $n \\times n$, and $V$ is $d \\times d$, and $S = \\textsf{diag}(s_1, s_2, \\ldots, s_{\\min\\{n,d\\}})$ (padded with either $n-d$ rows or $d-n$ columns of all $0$s, so $S$ is $n \\times d$) where $s_1 \\geq s_2 \\geq \\ldots \\geq s_{\\min\\{n,d\\}} \\geq 0$, and $s_i = 0$ for all $i > \\textsf{rank}(A)$. \n\nThe $i$th column of $U$ (resp. column of $V$) is called the $i$th left (resp. right) \\emph{singular vector}; and $s_i$ is the $i$th \\emph{singular value}.  \n\n\\item {\\index{spectral norm}\\trmbitx spectral norm:}\\quad\nThe spectral norm of matrix $A$ is denoted $\\|A\\|_2 = \\max_x \\|A x\\|/\\|x\\|$.  \n\n\\item {\\index{Frobenius norm}\\trmbitx Frobenius norm:}\\quad\nThe Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\|a_i\\|^2}$ where $a_i$ is the $i$th row of $A$.  \n\n\\item {\\index{low rank approximation of a matrix}\\trmbitx low rank approximation of a matrix:}\\quad\nThe best rank $k$ approximation of a matrix $A$ is denoted $[A]_k$.  Let $S_k$ be the matrix $S$ (the singular values from the SVD of $A$) where the singular values $s_i$ are set to $0$ for $i > k$.  Then $[A]_k = U S_k V^T$.   Note we can also ignore the columns of $U$ and $V$ after $k$; these are implicitly set to $0$ by multiplication with $s_i = 0$.  \n\nThe $n \\times d$ matrix $[A]_k$ is optimal in that over all rank $k$ matrices $B$ it minimizes $\\|A - B\\|_2$ and $\\|A - B\\|_F$.  \n\n\\item {\\index{projection}\\trmbitx projection:}\\quad\nFor a subspace $F \\subset {\\ensuremath{\\mathbb{R}}}^d$ and point $x \\in {\\ensuremath{\\mathbb{R}}}^d$, define the projection $\\pi_F(x) = \\arg \\min_{y \\in F} \\|x-y\\|$.  For a $n \\times d$ matrix $A$, then $\\pi_F(A)$ defines the $n \\times d$ matrix where each row is individually projected on to $F$.  \n\\end{gllist}\n\n\n\n\\Bnn{ROW SUBSET SELECTION}\n\n\\noindent \nThe first approach towards these matrix sketches is to chose a careful subset of the rows (note: the literature in this area usually discusses selecting columns).  \nThe first analysis of these techniques considered sampling $\\ell = O((1/{\\varepsilon}^2) k \\log k)$ rows proportional to their squared norm as $\\ell \\times d$ matrix $B$, and showed~\\cite{FKV04,DFKVV04,drineas2006fast2} one could describe a rank-$k$ matrix $P = [\\pi_B(A)]_k$ so that \n", "itemtype": "equation", "pos": 22062, "prevtext": "\nThis requires $O((1/\\rho{\\varepsilon}^2) (\\nu \\log (1/\\rho) + \\log(1/\\delta)))$ samples~\\cite{LLS01,HS11} to succeed with probability at least $1-\\delta$.   \n\n\n\n\n\\Bnn{DISCREPANCY-BASED RESULTS}\n\n\\noindent \nThe basic idea is to build a coloring on the ground set $\\chi : X \\to \\{-1,+1\\}$ to minimize $\\sum_{x \\in R} \\chi(x)$ over all ranges.  Then we can plug this into the merge-reduce process where merging takes the union and reducing discards the points colored $-1$.  Chazelle and Matousek~\\cite{CM96} showed how slight modifications of the merge-reduce framework can remove extra log factors in the approximation.  \n\nBased on discrepancy results (see Chapters 14 and 48) we can achieve the following bounds.  These assume $d \\geq 2$ is a fixed constant, and is absorbed in $O(\\cdot)$ notation.  \nFor any range space $(P, {\\ensuremath{\\EuScript{{R}}}})$ with VC-dimension $\\nu$ (a fixed constant) we can construct an ${\\varepsilon}$-approximation of size $g = O(1/{\\varepsilon}^{2 - \\nu/(\\nu+1)})$ in $O(n \\cdot g^{w-1})$ time.  \nThis is tight for range spaces ${\\ensuremath{\\EuScript{{H}}}}_d$ defined by halfspaces in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = d$.  \nFor range spaces ${\\ensuremath{\\EuScript{{B}}}}_d$ defined by balls in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = d+1$ this can be improved slightly to $g = O(1/{\\varepsilon}^{2 - \\nu/(\\nu+1)} \\sqrt{\\log (1/{\\varepsilon})})$; it is unknown if the $\\log$ factor can be removed.  \nFor range spaces ${\\ensuremath{\\EuScript{{T}}}}_d$ defined by axis-aligned rectangles in ${\\ensuremath{\\mathbb{R}}}^d$, where $\\nu = 2d$, this can be greatly improved to $g = O((1/{\\varepsilon}) \\log^{d+ 1/2} (1/{\\varepsilon}))$ with the best lower bound as $g = O((1/{\\varepsilon}) \\log^{d - 1} (1/{\\varepsilon}))$ for $d \\geq 2$~\\cite{Lar14,MNT15}.  \nThese colorings can be constructed adapting techniques from Bansal~\\cite{Ban10,BS13}.  Generalization to various other settings (typically following one of these patterns) can be found in books by Matousek~\\cite{Mat10} and Chazelle~\\cite{Cha01}.  \nSimilar bounds are available in a stream adapting the merge-reduce framework~\\cite{BCEG04,STZ04}.  \n\nDiscrepancy based results also exist for constructing ${\\varepsilon}$-nets.  However often the improvement over the random sampling bounds are not as dramatic; the main advantage is that the algorithms are deterministic.  \nFor halfspaces in ${\\ensuremath{\\mathbb{R}}}^3$ and balls in ${\\ensuremath{\\mathbb{R}}}^2$ we can construct ${\\varepsilon}$-nets of size $O(1/{\\varepsilon})$~\\cite{MSW90,CV07,PR08}.  \nFor axis-aligned rectangles and fat objects we can construct ${\\varepsilon}$-nets of size $O((1/{\\varepsilon}) \\log \\log (1/{\\varepsilon}))$~\\cite{AES10}.  \nPach and Tardos~\\cite{PT13} then showed these results are tight, and that similar improvements cannot exist in higher dimensions.  \n\n\n\n\n\\Bnn{GENERALIZATIONS}\n\n\\noindent \nOne can also replace the set of ranges ${\\ensuremath{\\EuScript{{R}}}}$ with a family of kernels ${\\ensuremath{\\EuScript{{K}}}} = \\{K(x,\\cdot) \\mid x \\in {\\ensuremath{\\mathbb{R}}}^d\\}$ where $K : {\\ensuremath{\\mathbb{R}}}^d \\times {\\ensuremath{\\mathbb{R}}}^d \\to [0,1]$; that is it replaces the binary $\\{0,1\\}$ notion of inclusion with a continuous map $[0,1]$.  A family of kernels ${\\ensuremath{\\EuScript{{K}}}}$ is \\emph{linked} to a range space $(P,{\\ensuremath{\\EuScript{{R}}}})$ if for every value $\\tau \\in [0,1]$ and every kernel $K(x,\\cdot)$, the points $\\{p \\in P \\mid K(x,p) \\geq \\tau\\} = R \\cap P$ for some $R \\in {\\ensuremath{\\EuScript{{R}}}}$.  For instance, centrally symmetric, shift-invariant kernels such as Gaussians are linked to the ranges space defined by balls.  \nThen an ${\\varepsilon}$-approximation $Q$ for $(P,{\\ensuremath{\\EuScript{{R}}}})$ is also an ${\\varepsilon}$-approximation for $(P,{\\ensuremath{\\EuScript{{K}}}})$~\\cite{JKPV11}, meaning that\n", "index": 5, "text": "\n\\[\n\\max_{x \\in {\\ensuremath{\\mathbb{R}}}^d} \\left| \\frac{\\sum_{p \\in P} K(x,p)}{|P|} - \\frac{\\sum_{q \\in Q} K(x,q)}{|Q|} \\right| \\leq {\\varepsilon}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\max_{x\\in{\\mathbb{R}}^{d}}\\left|\\frac{\\sum_{p\\in P}K(x,p)}{|P|}-\\frac{\\sum_{q%&#10;\\in Q}K(x,q)}{|Q|}\\right|\\leq{\\varepsilon}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>x</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></munder><mo>\u2061</mo><mrow><mo>|</mo><mrow><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>\u2208</mo><mi>P</mi></mrow></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>P</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>-</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>q</mi><mo>\u2208</mo><mi>Q</mi></mrow></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><mo>|</mo></mrow></mrow><mo>\u2264</mo><mi>\u03b5</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nThis result can be extended for sampling columns in addition to rows.  \n\nThis bound was then improved by sampling proportional to the \\emph{leverage scores};  If $U_k$ is the $n \\times k$ matrix of the top $k$ left singular values of $A$, then the leverage score of row $i$ is $\\|U_k(i)\\|^2$, the norm of the $i$th row of $U_k$.  In this case $O((1/{\\varepsilon}^2) k \\log k)$ rows are needed to achieve a relative error bound~\\cite{DMM08}\n", "itemtype": "equation", "pos": 36893, "prevtext": "\nSurprisingly, the discrepancy-based ${\\varepsilon}$-approximations are smaller for kernels than balls, for instance requiring only $O((1/{\\varepsilon}) \\sqrt{\\log(1/{\\varepsilon})})$ points in ${\\ensuremath{\\mathbb{R}}}^2$~\\cite{Phi13}.  Similarly an $({\\varepsilon}-\\tau)$-net $Q$ for $(P,{\\ensuremath{\\EuScript{{R}}}})$ is also an $(\\tau,{\\varepsilon})$-net for $(P, {\\ensuremath{\\EuScript{{K}}}})$~\\cite{PZ15}, meaning that for all $x \\in {\\ensuremath{\\mathbb{R}}}^d$ such that $\\frac{\\sum_{p \\in P} K(x,p)}{|P|} \\geq {\\varepsilon}$ then there exists some $q \\in Q$ such at $K(x,q) \\geq \\tau$.  \n\nSimilarly we can consider a single function $f \\in {\\ensuremath{\\EuScript{{F}}}}$~\\cite{Har06} or smaller set of $k$ functions $\\{f_1, \\ldots, f_k\\} \\subset {\\ensuremath{\\EuScript{{F}}}}$~\\cite{LS10} for various well-behaved function classes ${\\ensuremath{\\EuScript{{F}}}}$, and create a weighted coreset $Q \\subset P$, with weight function $w : Q \\to {\\ensuremath{\\mathbb{R}}}$, such that $\\sum_{q \\in Q} \\min_{i \\in [k]} w(q) f_i(q)$ approximates $\\sum_{p \\in P} \\min_{i \\in [k]} f_i(p)$.  These can be extended to handle outliers and robust loss functions like Huber and Tukey~\\cite{FS12} and Gaussian mixture models~\\cite{FFK11}, and as we will see, these techniques build the foundation for coresets for clustering~\\cite{FL11}.  These approaches can probably be extended to handle some classes of kernels by using ${\\ensuremath{\\EuScript{{F}}}} = \\{f(\\cdot) = 1-K(x,\\cdot) \\mid K(x,\\cdot) \\in {\\ensuremath{\\EuScript{{K}}}}\\}$.  \n\n\n\n\\Bnn{QUANTILES SKETCH}\n\n\\noindent \nA popular sketch for the so-called quantiles problem maps to $1$-dimensional ${\\varepsilon}$-approximations.  Define the \\emph{rank} of $v$ for set $X \\in {\\ensuremath{\\mathbb{R}}}$ as $\\textsf{rank}(X,v) = |\\{x \\in X \\mid x \\leq v\\}|$.  A quantiles sketch $S$ over a data set $X$ of size $n$ allows for queries such that $|S(v) - \\textsf{rank}(X,v)| \\leq {\\varepsilon} n$ for all $v \\in {\\ensuremath{\\mathbb{R}}}$.  \nThis is equivalent to an ${\\varepsilon}$-approximation of a range space $(X,{\\ensuremath{\\EuScript{{I}}}})$ where ${\\ensuremath{\\EuScript{{I}}}}$ is defined by half-open intervals of the form $(-\\infty,a]$.  \n\nA ${\\varepsilon}$-approximation coreset of size $1/{\\varepsilon}$ can be found by simply sorting $X$ and taking evenly spaced points in that sorted ordering.  Streaming sketches are also known; most famously the Greenwald-Khanna sketch~\\cite{GK01} which takes $O((1/{\\varepsilon}) \\log ({\\varepsilon} n))$ space, where $X$ is size $n$.  Recently a streaming sketch of size $O((1/{\\varepsilon})\\log(1/{\\varepsilon}))$ was discovered~\\cite{FO15} which is optimal.  \n\n\n\n\n\n\n\\A{HIGH DIMENSIONAL VECTORS}\n\n\\noindent \nIn this section we will consider high dimensional vectors $v = (v_1, v_2, \\ldots, v_d)$.\nWhen each $v_i$ is a positive integer, we can imagine these as the counts of a labeled set (the $d$ dimensions), and a subset of these objects is a coreset which can approximate the relative frequencies.  \nHence, we can consider weighted coresets over the dimensions, and consider approximating $v$ with another vector with few non-zero coordinates.  \nEven more generally, a sketch will compactly represent another vector $u$ which behaves similarly to $v$ under various norms.   \n\n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{ell-p norm}\\trmbitx $\\ell_p$-norm:}\\quad \nFor a vector $v \\in {\\ensuremath{\\mathbb{R}}}^d$ the $\\ell_p$ norm, for $p \\in [1,\\infty)$, is defined $\\|v\\|_p = (\\sum_{i=1}^d |v_i|^p)^{1/p}$.  For $p=0$ define $\\|v\\|_0 = |\\{i \\mid v_i \\neq 0\\}|$, the number of non-zero coordinates, and for $p=\\infty$ define $\\|v\\|_\\infty = \\max_{i=1}^d |v_i|$.  \n\n\\item {\\index{k-sparse}\\trmbitx $k$-sparse:}\\quad \nA vector is $k$-sparse if $\\|v\\|_0 \\leq k$.  \n\n\\item {\\index{additive ell_p/ell_q approximation}\\trmbitx additive $\\ell_p / \\ell_q$ approximation:}\\quad\nA vector $v$ has an additive  ${\\varepsilon}$-($\\ell_p/\\ell_q)$ approximation with vector $u$ if\n$\n\\|v-u\\|_p \\leq {\\varepsilon} \\|v\\|_q.\n$\n\n\\item {\\index{k-sparse ell_p/ell_q approximation}\\trmbitx $k$-sparse $\\ell_p / \\ell_q$ approximation:}\\quad\nA vector $v$ has a $k$-sparse ${\\varepsilon}$-($\\ell_p/\\ell_q)$ approximation with vector $u$ if $u$ is $k$-sparse and \n$\n\\|v-u\\|_p \\leq {\\varepsilon} \\|v-u\\|_q.\n$\n\n\\item {\\index{frequency count}\\trmbitx frequency count:}\\quad \nFor a vector $v = (v_1, v_2, \\ldots v_d)$  the value $v_i$ is called the $i$th frequency count of $v$.\n\n\\item {\\index{frequency moment}\\trmbitx frequency moment:}\\quad \nFor a vector $v = (v_1, v_2, \\ldots v_d)$  the value $\\|v\\|_p$ is called the $p$th frequency moment of $v$.\n\\end{gllist}\n\n\n\\Bnn{FREQUENCY APPROXIMATION}\n\n\\noindent \nThere are several types of coresets and sketches for frequency counts.  \nDerived by ${\\varepsilon}$-approximation and ${\\varepsilon}$-net bounds, we can create the following coresets over dimensions.  Assume $v$ has positive integer coordinates, and each coordinate's count $v_i$ represents $v_i$ distinct objects.  Then let $S$ be a random sample of size $k$ of these objects and $u(S)$ be an approximate vector defined so $u(S)_i = (\\|v\\|_1/k) \\cdot |\\{ s \\in S \\mid s = i\\}|$.  Then with $k = O((1/{\\varepsilon}^2) \\log(1/\\delta))$ we have $\\|v - u(S)\\|_\\infty \\leq {\\varepsilon} \\|v\\|_1$ (an additive ${\\varepsilon}$-$(\\ell_\\infty/\\ell_1)$ approximation) with probability at least $1-\\delta$.  Moreover, if $k = O((1/{\\varepsilon}) \\log(1/{\\varepsilon} \\delta))$ then for all $i$ such that $v_i \\geq {\\varepsilon} \\|v\\|_1$, then $u(S)_i \\neq 0$, and we can then measure the true count to attain a weighted coreset which is again an additive ${\\varepsilon}$-$(\\ell_\\infty / \\ell_1)$ approximation.  And in fact, there can be at most $1/{\\varepsilon}$ dimensions $i$ with $v_i \\geq {\\varepsilon} \\|v\\|_1$, so there always exists a weighted coreset of size $1/{\\varepsilon}$.  \n\nSuch a weighted coreset for additive ${\\varepsilon}$-($\\ell_\\infty/\\ell_1)$ approximations that is $(1/{\\varepsilon})$-sparse can be found deterministically in a stream via the Misra-Gries sketch~\\cite{mg-fre-82} (or other rediscovered variants~\\cite{metwally06,DLM02,KSP03}).  This approach keeps $1/{\\varepsilon}$ counters with associated labels.  For a new item, if it matches a label, the counter is incremented, else if any counter is $0$ it takes over that counter/label, and otherwise, (perhaps unintuitively) all counters are decremented.  \n\nThe count-min sketch~\\cite{CM05} also provides an additive ${\\varepsilon}$-($\\ell_\\infty / \\ell_1)$ approximation with space $O((1/{\\varepsilon}) \\log (1/\\delta))$ and is successful with probability $1-\\delta$.  \nA count-sketch~\\cite{CCM02} provides an additive ${\\varepsilon}$-($\\ell_\\infty / \\ell_2)$ approximation with space $O((1/{\\varepsilon}^2) \\log (1/\\delta))$, and is successful with probability at least $1-\\delta$.  \nBoth of these linear sketches operate by using $O(\\log 1/\\delta)$ hash functions, each mapping $[d]$ to one of $O(1/{\\varepsilon})$ or $O(1/{\\varepsilon}^2)$ counters.  The counters are incremented or decremented with the value $v_i$.  Then an estimate for $v_i$ can be recovered by examining all cells where $i$ hashes; the effect of other dimensions which hash there can be shown bounded.   \n\n\n\n\\paragraph{Frequency moments.}\nAnother common task is to approximate the frequency moments $\\|v\\|_p$.  For $p =1$, this is simply the count and can be done exactly in a stream.  The AMS Sketch~\\cite{AMS99} maintains a sketch of size $O((1/{\\varepsilon}^2) \\log (1/\\delta))$ that can derive a value $\\hat F_2$ so that $| \\|v\\|_2 - \\hat F_2 | \\leq {\\varepsilon} \\|v\\|_2$ with probability at least $1-\\delta$.  \n\nThe FM Sketch~\\cite{flajolet85:_probab} (and its extensions~\\cite{AMS99,DF03}) show how to create a sketch of size $O((1/{\\varepsilon}^2) \\log (1/\\delta))$ which can derive an estimate $\\hat F_0$ so that $| \\|v\\|_0 - \\hat F_0 | \\leq {\\varepsilon} \\|v\\|_1$ with probability at least $1-\\delta$.  \nThis works when $v_i$ are positive counts, and those counts are incremented one at a time in a stream.  \nUsually these sketches and coresets have implicit assumptions that a ``word'' can fit $\\log n$ bits where the stream is of size $n$, and is sufficient for each counter.  On the other hand, these $\\ell_0$ sketches operate with bits, and only have a hidden $\\log \\log n$ factor for bits.  \n\n  \n\n\\paragraph{$k$-sparse tail approximation.}\nSome sketches can achieve $k$-sparse approximations (which are akin to coresets of size $k$) and have stronger error bounds that depend only on the ``tail'' of the matrix; this is the class of $k$-sparse ${\\varepsilon}$-($\\ell_p/\\ell_q$) approximations.  See survey by Gilbert and Indyk for more details~\\cite{GI10}.  \n\nThese bounds are typically achieved by increasing the sketch size by a factor $k$, and then the $k$-sparse vector is the top $k$ of those elements.  The main recurring argument is roughly as follows:\n  If you maintain the top $1/{\\varepsilon}$ counters, then the largest counter not maintained is of size at most ${\\varepsilon} \\|v\\|$.  Similarly, if you first remove the top $k$ counters (a set $K = \\{i_1, i_2, \\ldots, i_k\\} \\subset [d]$, let their collective norm be $\\|v_K\\|$), then maintain $1/{\\varepsilon}$ more, the largest not-maintained counter is at most ${\\varepsilon} (\\|v\\| - \\|v_K\\|)$.  \nThe goal is then to sketch a $k$-sparse vector which approximates $v_K$; for instance the Misra-Gries Sketch~\\cite{mg-fre-82} and Count-Min sketch~\\cite{CM05} achieve $k$-sparse ${\\varepsilon}$-($\\ell_1/\\ell_1$)-approximations with $O(k/{\\varepsilon})$ counters, and the Count sketch~\\cite{CCM02} achieves $k$-sparse ${\\varepsilon}$-($\\ell_2/\\ell_2)$-approximations with $O(k^2/{\\varepsilon}^2)$ counters~\\cite{BCIS10}.  \n\n\n\n \n\n\n\n\\A{HIGH DIMENSIONAL POINT SETS (MATRICES)}\n\\label{sec:subspace-matrix}\n\n\\noindent\nMatrix sketching has gained a lot of interest due to its close connection to scalability issues in machine learning and data mining.  The goal is often to replace a matrix with a small space and low-rank approximation.  \nHowever, given a $n \\times d$ matrix $A$, it can also be imagined as $n$ points each in ${\\ensuremath{\\mathbb{R}}}^d$, and the span of a rank-$k$ approximation is a $k$-dimensional subspace that approximately includes all of the points.  \n\nMany of the techniques build on approaches from vector sketching, and again, many of the sketches are naturally interpretable as weighted coresets.  Sometimes it is natural to represent the result as a reduced set of rows in a $\\ell \\times d$ matrix $B$.  Other times it is more natural to consider the dimensionality reduction problem where the goal is an $n \\times c$ matrix $S$, and sometimes you do both!  But since these problems are typically phrased in terms of matrices, the difference comes down to simply transposing the input matrix.  \n\nWe will write all results as approximating an $n \\times d$ matrix $A$ using fewer rows, for instance, with an $\\ell \\times d$ matrix $B$.  This allows one to answer questions about a unit vector $x \\in {\\ensuremath{\\mathbb{R}}}^d$.  Geometrically, we can preserve \nthe variation along direction $x$ ($\\|A x\\|^2$) \nand \nhow close is $x$ to the best rank $k$ subspace of $A$.  \n\nNotoriously this problem can be solved optimally using the numerical linear algebra technique the \\emph{singular value decomposition} in $O(nd^2)$ time.  The challenges are then to compute this more efficiently in streaming and other related settings.  \n\nWe will describe three basic approaches (row sampling, random projections, and iterative SVD variants), and then some extensions and applications~\\cite{FT15}.   The first two approaches are mainly randomized, and we will describe results with constant probability, and for the most part these bounds can be made to succeed with any probability $1-\\delta$ by increasing the size by a factor $\\log(1/\\delta)$.  \n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{matrix rank}\\trmbitx matrix rank:}\\quad\nThe \\emph{rank} of an $n \\times d$ matrix $A$, denoted $\\textsf{rank}(A)$, is the smallest $k$ such that all rows (or columns) lie in a $k$-dimensional subspace of ${\\ensuremath{\\mathbb{R}}}^d$ (or ${\\ensuremath{\\mathbb{R}}}^n$).  \n\n\\item {\\index{singular value decomposition}\\trmbitx singular value decomposition:}\\quad \nGiven an $n \\times d$ matrix $A$, the singular value decomposition is a product $U S V^T$ where $U$ and $V$ are orthogonal, and $S$ is diagonal.  $U$ is $n \\times n$, and $V$ is $d \\times d$, and $S = \\textsf{diag}(s_1, s_2, \\ldots, s_{\\min\\{n,d\\}})$ (padded with either $n-d$ rows or $d-n$ columns of all $0$s, so $S$ is $n \\times d$) where $s_1 \\geq s_2 \\geq \\ldots \\geq s_{\\min\\{n,d\\}} \\geq 0$, and $s_i = 0$ for all $i > \\textsf{rank}(A)$. \n\nThe $i$th column of $U$ (resp. column of $V$) is called the $i$th left (resp. right) \\emph{singular vector}; and $s_i$ is the $i$th \\emph{singular value}.  \n\n\\item {\\index{spectral norm}\\trmbitx spectral norm:}\\quad\nThe spectral norm of matrix $A$ is denoted $\\|A\\|_2 = \\max_x \\|A x\\|/\\|x\\|$.  \n\n\\item {\\index{Frobenius norm}\\trmbitx Frobenius norm:}\\quad\nThe Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\|a_i\\|^2}$ where $a_i$ is the $i$th row of $A$.  \n\n\\item {\\index{low rank approximation of a matrix}\\trmbitx low rank approximation of a matrix:}\\quad\nThe best rank $k$ approximation of a matrix $A$ is denoted $[A]_k$.  Let $S_k$ be the matrix $S$ (the singular values from the SVD of $A$) where the singular values $s_i$ are set to $0$ for $i > k$.  Then $[A]_k = U S_k V^T$.   Note we can also ignore the columns of $U$ and $V$ after $k$; these are implicitly set to $0$ by multiplication with $s_i = 0$.  \n\nThe $n \\times d$ matrix $[A]_k$ is optimal in that over all rank $k$ matrices $B$ it minimizes $\\|A - B\\|_2$ and $\\|A - B\\|_F$.  \n\n\\item {\\index{projection}\\trmbitx projection:}\\quad\nFor a subspace $F \\subset {\\ensuremath{\\mathbb{R}}}^d$ and point $x \\in {\\ensuremath{\\mathbb{R}}}^d$, define the projection $\\pi_F(x) = \\arg \\min_{y \\in F} \\|x-y\\|$.  For a $n \\times d$ matrix $A$, then $\\pi_F(A)$ defines the $n \\times d$ matrix where each row is individually projected on to $F$.  \n\\end{gllist}\n\n\n\n\\Bnn{ROW SUBSET SELECTION}\n\n\\noindent \nThe first approach towards these matrix sketches is to chose a careful subset of the rows (note: the literature in this area usually discusses selecting columns).  \nThe first analysis of these techniques considered sampling $\\ell = O((1/{\\varepsilon}^2) k \\log k)$ rows proportional to their squared norm as $\\ell \\times d$ matrix $B$, and showed~\\cite{FKV04,DFKVV04,drineas2006fast2} one could describe a rank-$k$ matrix $P = [\\pi_B(A)]_k$ so that \n", "index": 7, "text": "\n\\[\n\\|A - P\\|_F^2 \\leq \\|A - [A]_k\\|_F^2  + {\\varepsilon} \\|A\\|_F^2 \n\\hspace{.15in} \\textrm{ and }  \\hspace{.15in}\n\\|A - P\\|_2^2 \\leq \\|A - [A]_k\\|_2^2 + {\\varepsilon} \\|A\\|_F^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\|A-P\\|_{F}^{2}\\leq\\|A-[A]_{k}\\|_{F}^{2}+{\\varepsilon}\\|A\\|_{F}^{2}\\hskip 10.8%&#10;405pt\\textrm{ and }\\hskip 10.8405pt\\|A-P\\|_{2}^{2}\\leq\\|A-[A]_{k}\\|_{2}^{2}+{%&#10;\\varepsilon}\\|A\\|_{F}^{2}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><mi>P</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2264</mo><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>A</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>\u00a0and\u00a0</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><mi>P</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>A</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nThese results can be extended to sample rows and columns, generating a so-called CUR decomposition of $A$.  \nAlso, similar relative error bounds can be achieved through volume sampling~\\cite{DV06}.  \n\nAlthough this matrix approximation preserves the structure of rows of the matrix, to get the best bounds via sampling, the sampling probabilities based on leverage scores are as expensive to compute as the full SVD.  Instead one can approximate the leverage scores~\\cite{DMMW12,CLMMPS15}, for instance in a stream in $O((kd/{\\varepsilon}^2) \\log^4 n)$ bits of space~\\cite{DMMW12}.  \n\nBetter algorithms exist in a non-streaming settings~\\cite{FVR15}; these can, for instance, achieve the strong relative error bounds with only $O(k/{\\varepsilon})$ rows (and $O(k/{\\varepsilon})$ columns) and only require time $O(\\textsf{nnz}(A) \\log n + n \\textsf{poly}(\\log n, k, 1/{\\varepsilon}))$ time where $\\textsf{nnz}(A)$ is the number of non-zero entries in $A$~\\cite{BDM14,BW14}.  \n\n\n\n\\Bnn{RANDOM PROJECTIONS}\n\n\\noindent \nThe second approach to matrix sketching is based on the Johnson-Lindenstrauss (JL) Lemma~\\cite{JL84}, which says that projecting any vector $x$ (independent of its dimension, for which it will be useful here to denote as $n$) onto a random subspace $F$ of dimension $\\ell = O(1/{\\varepsilon}^2)$ preserves its norm up to $(1+{\\varepsilon})$ relative error, after rescaling: \n$(1-{\\varepsilon}) \\|x\\| \\leq \\sqrt{n/\\ell} \\|\\pi_F(x)\\| \\leq (1+{\\varepsilon}) \\|x\\|$.  \nFollow up work has shown that the projection operator $\\pi_F$ can be realized as an $\\ell \\times n$ matrix $S$ so that $(\\sqrt{n/\\ell}) \\pi_F(x) = Sx$.  \nAnd in particular, we can fill the entires of $S$ iid with Gaussian random variables~\\cite{DG03}, \n uniform $\\{-1,+1\\}$ or $\\{-1,0,+1\\}$ random variables~\\cite{Ach03}, or \n any subgaussian random variable~\\cite{Mat08}, rescaled.  Alternatively, we can make $S$ all $0$s except for one~\\cite{CW13} or a constant number~\\cite{NN13} of uniform $\\{-1,+1\\}$ random variables in each column of $S$.  The later construction essentially ``hashes'' each element of $A$ to one or a constant number of elements in $\\pi_F(x)$; basically an extension of the count-sketch~\\cite{CCM02}.  \n\nTo apply these results to matrix sketching, we simply apply the sketch matrix $S$ to $A$ instead of just a single ``dimension\" of $A$.  Then $B = SA$ is our resulting $\\ell \\times d$ matrix.  However, unlike in typical uses of the JL Lemma on a point set of size $m$, where it can be shown to preserve all distances using $\\ell = O((1/{\\varepsilon}^2) \\log m)$ target dimensions, we will strive to preserve the norm over all $d$ dimensions.  As such we use $\\ell = O(d/{\\varepsilon}^2)$ for iid JL results~\\cite{Sar06}, or $\\ell = O(d^2/{\\varepsilon}^2)$ for hashing-based approaches~\\cite{CW13,NN13}.  As was first observed by Sarlos~\\cite{Sar06} this allows one to create an oblivious subspace embedding so that for \\emph{all} $x \\in {\\ensuremath{\\mathbb{R}}}^d$ guarantees\n", "itemtype": "equation", "pos": 37514, "prevtext": "\nThis result can be extended for sampling columns in addition to rows.  \n\nThis bound was then improved by sampling proportional to the \\emph{leverage scores};  If $U_k$ is the $n \\times k$ matrix of the top $k$ left singular values of $A$, then the leverage score of row $i$ is $\\|U_k(i)\\|^2$, the norm of the $i$th row of $U_k$.  In this case $O((1/{\\varepsilon}^2) k \\log k)$ rows are needed to achieve a relative error bound~\\cite{DMM08}\n", "index": 9, "text": "\n\\[\n\\|A - \\pi_B(A)\\|_F \\leq (1+{\\varepsilon}) \\|A - [A]_k\\|_F^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\|A-\\pi_{B}(A)\\|_{F}\\leq(1+{\\varepsilon})\\|A-[A]_{k}\\|_{F}^{2}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><mrow><msub><mi>\u03c0</mi><mi>B</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nThe obliviousness of this linear projection matrix $S$ (it is created independent of $A$) is very powerful.  It means this result can be not only performed in an update-only stream, but also one that allows deletions, or arbitrary updates to an individual entry in a matrix.  Moreover, given a matrix $A$ with only $\\textsf{nnz}(A)$ non-zero entries, it can be applied in roughly $O(\\textsf{nnz}(A))$ time~\\cite{CW13,NN13}.  It also implies bounds for matrix multiplication, and as we will discuss, linear regression.  \n\n\n\\Bnn{FREQUENT DIRECTIONS}\n\n\\noindent \nThis third class of matrix sketching algorithms tries to more directly replicate those properties of the SVD, and can be deterministic.  So why not just use the SVD?  These methods, while approximate, are faster than SVD, can be used in a stream, and can sketch two disjoint parts of a matrix then combine the sketches together. \n\nThe Frequent Directions algorithm~\\cite{Lib13,FD-journal} essentially processes each row (or $O(\\ell)$ rows) of $A$ at a time, always maintaining the best rank-$\\ell$ approximation as the sketch.  But this can suffer from data drift, so crucially after each such update, it also shrinks all squared singular values of $[B]_\\ell$ by $s_{\\ell}^2$; this ensures that the additive error is never more than ${\\varepsilon} \\|A\\|_F^2$, precisely as in the Misra-Gries~\\cite{mg-fre-82} sketch for frequency approximation.  \nSetting $\\ell = k+1/{\\varepsilon}$ and $\\ell = k + k/{\\varepsilon}$, respectively, the following bounds have been shown~\\cite{GP14,FD-journal} for any unit vector $x$:\n", "itemtype": "equation", "pos": 40566, "prevtext": "\nThese results can be extended to sample rows and columns, generating a so-called CUR decomposition of $A$.  \nAlso, similar relative error bounds can be achieved through volume sampling~\\cite{DV06}.  \n\nAlthough this matrix approximation preserves the structure of rows of the matrix, to get the best bounds via sampling, the sampling probabilities based on leverage scores are as expensive to compute as the full SVD.  Instead one can approximate the leverage scores~\\cite{DMMW12,CLMMPS15}, for instance in a stream in $O((kd/{\\varepsilon}^2) \\log^4 n)$ bits of space~\\cite{DMMW12}.  \n\nBetter algorithms exist in a non-streaming settings~\\cite{FVR15}; these can, for instance, achieve the strong relative error bounds with only $O(k/{\\varepsilon})$ rows (and $O(k/{\\varepsilon})$ columns) and only require time $O(\\textsf{nnz}(A) \\log n + n \\textsf{poly}(\\log n, k, 1/{\\varepsilon}))$ time where $\\textsf{nnz}(A)$ is the number of non-zero entries in $A$~\\cite{BDM14,BW14}.  \n\n\n\n\\Bnn{RANDOM PROJECTIONS}\n\n\\noindent \nThe second approach to matrix sketching is based on the Johnson-Lindenstrauss (JL) Lemma~\\cite{JL84}, which says that projecting any vector $x$ (independent of its dimension, for which it will be useful here to denote as $n$) onto a random subspace $F$ of dimension $\\ell = O(1/{\\varepsilon}^2)$ preserves its norm up to $(1+{\\varepsilon})$ relative error, after rescaling: \n$(1-{\\varepsilon}) \\|x\\| \\leq \\sqrt{n/\\ell} \\|\\pi_F(x)\\| \\leq (1+{\\varepsilon}) \\|x\\|$.  \nFollow up work has shown that the projection operator $\\pi_F$ can be realized as an $\\ell \\times n$ matrix $S$ so that $(\\sqrt{n/\\ell}) \\pi_F(x) = Sx$.  \nAnd in particular, we can fill the entires of $S$ iid with Gaussian random variables~\\cite{DG03}, \n uniform $\\{-1,+1\\}$ or $\\{-1,0,+1\\}$ random variables~\\cite{Ach03}, or \n any subgaussian random variable~\\cite{Mat08}, rescaled.  Alternatively, we can make $S$ all $0$s except for one~\\cite{CW13} or a constant number~\\cite{NN13} of uniform $\\{-1,+1\\}$ random variables in each column of $S$.  The later construction essentially ``hashes'' each element of $A$ to one or a constant number of elements in $\\pi_F(x)$; basically an extension of the count-sketch~\\cite{CCM02}.  \n\nTo apply these results to matrix sketching, we simply apply the sketch matrix $S$ to $A$ instead of just a single ``dimension\" of $A$.  Then $B = SA$ is our resulting $\\ell \\times d$ matrix.  However, unlike in typical uses of the JL Lemma on a point set of size $m$, where it can be shown to preserve all distances using $\\ell = O((1/{\\varepsilon}^2) \\log m)$ target dimensions, we will strive to preserve the norm over all $d$ dimensions.  As such we use $\\ell = O(d/{\\varepsilon}^2)$ for iid JL results~\\cite{Sar06}, or $\\ell = O(d^2/{\\varepsilon}^2)$ for hashing-based approaches~\\cite{CW13,NN13}.  As was first observed by Sarlos~\\cite{Sar06} this allows one to create an oblivious subspace embedding so that for \\emph{all} $x \\in {\\ensuremath{\\mathbb{R}}}^d$ guarantees\n", "index": 11, "text": "\n\\[\n(1-{\\varepsilon})\\|Ax\\|_2^2 \\leq \\|SAx\\|_2^2 \\leq (1+{\\varepsilon})\\|Ax\\|_2^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"(1-{\\varepsilon})\\|Ax\\|_{2}^{2}\\leq\\|SAx\\|_{2}^{2}\\leq(1+{\\varepsilon})\\|Ax\\|_%&#10;{2}^{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>\u2264</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nOperating in a batch to process $\\Theta(\\ell)$ rows at a time, this takes $O(nd \\ell)$ time.  \nA similar approach by Feldman \\emph{et.al.}~\\cite{FSS13} provides a more general bound, and will be discussed in the context of subspace clustering next.  \n\n\n\n\n\\Bnn{LINEAR REGRESSION and ROBUST VARIANTS}\n\n\\noindent \nThe regression problem takes as input again an $n \\times d$ matrix $A$ and also an $n \\times w$ matrix $T$ (most commonly $w=1$ and $T$ is a vector); the goal is to find the $d \\times w$ matrix $X^* = \\arg\\min_X \\|AX - T\\|_F$.  \nOne can create a coreset of $\\ell$ rows (or weighted linear combination of rows): the $\\ell \\times d$ matrix $\\hat A$ and $\\ell \\times w$ matrix $\\hat T$ imply a matrix $\\hat X = \\arg\\min_X \\|\\hat A X - \\hat T\\|_2$ that satisfies \n", "itemtype": "equation", "pos": 42226, "prevtext": "\nThe obliviousness of this linear projection matrix $S$ (it is created independent of $A$) is very powerful.  It means this result can be not only performed in an update-only stream, but also one that allows deletions, or arbitrary updates to an individual entry in a matrix.  Moreover, given a matrix $A$ with only $\\textsf{nnz}(A)$ non-zero entries, it can be applied in roughly $O(\\textsf{nnz}(A))$ time~\\cite{CW13,NN13}.  It also implies bounds for matrix multiplication, and as we will discuss, linear regression.  \n\n\n\\Bnn{FREQUENT DIRECTIONS}\n\n\\noindent \nThis third class of matrix sketching algorithms tries to more directly replicate those properties of the SVD, and can be deterministic.  So why not just use the SVD?  These methods, while approximate, are faster than SVD, can be used in a stream, and can sketch two disjoint parts of a matrix then combine the sketches together. \n\nThe Frequent Directions algorithm~\\cite{Lib13,FD-journal} essentially processes each row (or $O(\\ell)$ rows) of $A$ at a time, always maintaining the best rank-$\\ell$ approximation as the sketch.  But this can suffer from data drift, so crucially after each such update, it also shrinks all squared singular values of $[B]_\\ell$ by $s_{\\ell}^2$; this ensures that the additive error is never more than ${\\varepsilon} \\|A\\|_F^2$, precisely as in the Misra-Gries~\\cite{mg-fre-82} sketch for frequency approximation.  \nSetting $\\ell = k+1/{\\varepsilon}$ and $\\ell = k + k/{\\varepsilon}$, respectively, the following bounds have been shown~\\cite{GP14,FD-journal} for any unit vector $x$:\n", "index": 13, "text": "\n\\[\n0 \\leq \\|Ax\\|^2 - \\|Bx\\|^2 \\leq {\\varepsilon} \\|A - [A]_k\\|_F^2 \n\\hspace{.15in} \\text{ and } \\hspace{.15in}\n\\|A - \\pi_{[B]_k}(A)\\|_F^2 \\leq (1+{\\varepsilon}) \\|A - [A]_k\\|_F^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"0\\leq\\|Ax\\|^{2}-\\|Bx\\|^{2}\\leq{\\varepsilon}\\|A-[A]_{k}\\|_{F}^{2}\\hskip 10.8405%&#10;pt\\text{ and }\\hskip 10.8405pt\\|A-\\pi_{[B]_{k}}(A)\\|_{F}^{2}\\leq(1+{%&#10;\\varepsilon})\\|A-[A]_{k}\\|_{F}^{2}.\" display=\"block\"><mrow><mrow><mrow><mn>0</mn><mo>\u2264</mo><mrow><msup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup><mo>-</mo><msup><mrow><mo>\u2225</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>\u2264</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mtext>\u00a0and\u00a0</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><mrow><msub><mi>\u03c0</mi><msub><mrow><mo stretchy=\"false\">[</mo><mi>B</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>A</mi><mo>-</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo stretchy=\"false\">]</mo></mrow><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nUsing the random projection techniques described above, one can sketch $\\hat{A} = SA$ and $\\hat T = SA$ with $\\ell = O(d^2/{\\varepsilon}^2)$ for hashing approaches or $\\ell = O(d/{\\varepsilon}^2)$ for iid approaches.  Moreover, Sarlos~\\cite{Sar06} observed that for the $w=1$ case, since only a single direction (the optimal one) is required to be preserved (see \\emph{weak coresets} below), one can also use just $\\ell = O(d^2/{\\varepsilon})$ rows.  \nUsing row-sampling, one can deterministically select $\\ell = O(d/{\\varepsilon}^2)$ rows~\\cite{BDM13}.\nThe above works also provide bounds for approximating the multiple-regression spectral norm $\\|AX^* - T\\|_2$.  \n\nMainly considering the single-regression problem when $w=1$ (in this case spectral and Frobenius norms bounds are equivalent $p=2$ norms), there also exists bounds for approximating $\\|A X - T\\|_p$ for $p \\in [1,\\infty)$ using random projection approaches and row sampling~\\cite{CDMMMW13,Woo14}.  The main idea is to replace iid Gaussian random variables which are $2$-stable with iid $p$-stable random variables.  These results are improved using max-norm stability results~\\cite{WZ13} embedding into $\\ell_\\infty$, or for other robust loss functions like the Huber loss~\\cite{CW15,CW15b}.  \n\n\n\n\n\n\n\n\n\\A{CLUSTERING}\n\n\\noindent\nAn assignment-based clustering of a data set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$ is defined by a set of $k$ centers $C \\subset {\\ensuremath{\\mathbb{R}}}^d$ and a function $\\phi_C : {\\ensuremath{\\mathbb{R}}}^d \\to C$, so $\\phi_C(x) = \\arg \\min_{c \\in C} \\|x-c\\|$.  The function $\\phi_C$ maps to the closest center in $C$, and it assigns each point $x \\in X$ to a center and an associated cluster.  \nIt will be useful to consider a weight $w : X \\to {\\ensuremath{\\mathbb{R}}}^+$.\nThen a clustering is evaluated by a cost function \n\n", "itemtype": "equation", "pos": 43180, "prevtext": "\nOperating in a batch to process $\\Theta(\\ell)$ rows at a time, this takes $O(nd \\ell)$ time.  \nA similar approach by Feldman \\emph{et.al.}~\\cite{FSS13} provides a more general bound, and will be discussed in the context of subspace clustering next.  \n\n\n\n\n\\Bnn{LINEAR REGRESSION and ROBUST VARIANTS}\n\n\\noindent \nThe regression problem takes as input again an $n \\times d$ matrix $A$ and also an $n \\times w$ matrix $T$ (most commonly $w=1$ and $T$ is a vector); the goal is to find the $d \\times w$ matrix $X^* = \\arg\\min_X \\|AX - T\\|_F$.  \nOne can create a coreset of $\\ell$ rows (or weighted linear combination of rows): the $\\ell \\times d$ matrix $\\hat A$ and $\\ell \\times w$ matrix $\\hat T$ imply a matrix $\\hat X = \\arg\\min_X \\|\\hat A X - \\hat T\\|_2$ that satisfies \n", "index": 15, "text": "\n\\[\n(1-{\\varepsilon}) \\|A X^* - T\\|_F^2 \\leq \\|A \\hat X - T\\|_F^2 \\leq (1+{\\varepsilon}) \\|A X^* - T\\|_F^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"(1-{\\varepsilon})\\|AX^{*}-T\\|_{F}^{2}\\leq\\|A\\hat{X}-T\\|_{F}^{2}\\leq(1+{%&#10;\\varepsilon})\\|AX^{*}-T\\|_{F}^{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo>-</mo><mi>T</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>\u2264</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>-</mo><mi>T</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo>-</mo><mi>T</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nWhen the weighting function $w(x)$ is equal for all points $x \\in X$ (say $w(x) = 1/|X|$, which we assume as default), then we may simply write ${\\ensuremath{\\textsf{cost}}}_p(X,C)$.  We also define ${\\ensuremath{\\textsf{cost}}}_\\infty(X,C) = \\max_{x \\in X} \\|x - \\phi_C(x)\\|$.  \nThese techniques extend to when the centers of the clusters are not just points, but can also be higher-dimensional subspaces.  \n \n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{k-means / k-median / k-center clustering problem}\\trmbitx $k$-means / $k$-median / $k$-center clustering problem:}\\quad \nGiven a set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, find a set $C$ of size $k$ that minimizes ${\\ensuremath{\\textsf{cost}}}_2(X,C)$ (respectively, ${\\ensuremath{\\textsf{cost}}}_1(X,C)$ and ${\\ensuremath{\\textsf{cost}}}_\\infty(X,C)$).\n\n\\item {\\index{(k,eps)-coreset for k-means / k-median / k-center}\\trmbitx $(k,{\\varepsilon})$-coreset for $k$-means / $k$-median / $k$-center:}\\quad \nGiven a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, then a subset $S \\subset X$ is a $(k,{\\varepsilon})$-coreset for $k$-means (respectively, $k$-median and $k$-center) if for all center sets $C$ of size $k$ and parameter $p=2$ (respectively $p=1$ and $p=\\infty$) that \n", "itemtype": "equation", "pos": 45119, "prevtext": "\nUsing the random projection techniques described above, one can sketch $\\hat{A} = SA$ and $\\hat T = SA$ with $\\ell = O(d^2/{\\varepsilon}^2)$ for hashing approaches or $\\ell = O(d/{\\varepsilon}^2)$ for iid approaches.  Moreover, Sarlos~\\cite{Sar06} observed that for the $w=1$ case, since only a single direction (the optimal one) is required to be preserved (see \\emph{weak coresets} below), one can also use just $\\ell = O(d^2/{\\varepsilon})$ rows.  \nUsing row-sampling, one can deterministically select $\\ell = O(d/{\\varepsilon}^2)$ rows~\\cite{BDM13}.\nThe above works also provide bounds for approximating the multiple-regression spectral norm $\\|AX^* - T\\|_2$.  \n\nMainly considering the single-regression problem when $w=1$ (in this case spectral and Frobenius norms bounds are equivalent $p=2$ norms), there also exists bounds for approximating $\\|A X - T\\|_p$ for $p \\in [1,\\infty)$ using random projection approaches and row sampling~\\cite{CDMMMW13,Woo14}.  The main idea is to replace iid Gaussian random variables which are $2$-stable with iid $p$-stable random variables.  These results are improved using max-norm stability results~\\cite{WZ13} embedding into $\\ell_\\infty$, or for other robust loss functions like the Huber loss~\\cite{CW15,CW15b}.  \n\n\n\n\n\n\n\n\n\\A{CLUSTERING}\n\n\\noindent\nAn assignment-based clustering of a data set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$ is defined by a set of $k$ centers $C \\subset {\\ensuremath{\\mathbb{R}}}^d$ and a function $\\phi_C : {\\ensuremath{\\mathbb{R}}}^d \\to C$, so $\\phi_C(x) = \\arg \\min_{c \\in C} \\|x-c\\|$.  The function $\\phi_C$ maps to the closest center in $C$, and it assigns each point $x \\in X$ to a center and an associated cluster.  \nIt will be useful to consider a weight $w : X \\to {\\ensuremath{\\mathbb{R}}}^+$.\nThen a clustering is evaluated by a cost function \n\n", "index": 17, "text": "\\[\n{\\ensuremath{\\textsf{cost}}}_p(X,w,C) = \\sum_{x \\in X} w(x) \\cdot \\|x-\\phi_C(x)\\|^p.\n \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{\\textsf{cost}}_{p}(X,w,C)=\\sum_{x\\in X}w(x)\\cdot\\|x-\\phi_{C}(x)\\|^{p}.\" display=\"block\"><mrow><mrow><mrow><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><msup><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><mrow><msub><mi>\u03d5</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>p</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\n\n\\item {\\index{projective distance}\\trmbitx projective distance:}\\quad \nConsider a set $C = (C_1, C_2, \\ldots, C_k)$ of $k$ affine subspaces of dimension $j$ in ${\\ensuremath{\\mathbb{R}}}^d$, and a power $p \\in [1,\\infty)$.  \nThen for a point $x \\in {\\ensuremath{\\mathbb{R}}}^d$ the projective distance is defined ${\\ensuremath{\\textsf{dist}}}_p(C,x) = \\min_{C_i \\in C} \\|x - \\pi_{C_i}(x)\\|^p$, recalling that $\\pi_{C_i}(x) = \\arg \\min_{y \\in C_i} \\|x-y\\|$.  \n\n\\item {\\index{projective (k,j,p)-clustering problem}\\trmbitx projective $(k,j,p)$-clustering problem:}\\quad \nGiven a set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, find a set $C$ of $k$ $j$-dimensional affine subspaces of size $k$ that minimizes ${\\ensuremath{\\textsf{cost}}}_p(X,C) = \\sum_{x \\in X} {\\ensuremath{\\textsf{dist}}}_p(C,x)$.  \n\n\\item {\\index{(k,j,eps)-coreset for projective (k,j,p)-clustering}\\trmbitx $(k,j,{\\varepsilon})$-coreset for projective $(k,j,p)$-clustering:}\\quad \nGiven a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, then a subset $S \\subset X$, a weight function $w : S \\to {\\ensuremath{\\mathbb{R}}}^+$, and a constant $\\gamma$, is a $(k,j,{\\varepsilon})$-coreset for projective $(k,j,p)$-clustering if for all $j$-dimensional center sets $C$ of size $k$ that \n", "itemtype": "equation", "pos": 46464, "prevtext": "\nWhen the weighting function $w(x)$ is equal for all points $x \\in X$ (say $w(x) = 1/|X|$, which we assume as default), then we may simply write ${\\ensuremath{\\textsf{cost}}}_p(X,C)$.  We also define ${\\ensuremath{\\textsf{cost}}}_\\infty(X,C) = \\max_{x \\in X} \\|x - \\phi_C(x)\\|$.  \nThese techniques extend to when the centers of the clusters are not just points, but can also be higher-dimensional subspaces.  \n \n\n\n\\vspace{1pc}\n\\Bnn{GLOSSARY}\n\n\\begin{gllist}\n\\item {\\index{k-means / k-median / k-center clustering problem}\\trmbitx $k$-means / $k$-median / $k$-center clustering problem:}\\quad \nGiven a set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, find a set $C$ of size $k$ that minimizes ${\\ensuremath{\\textsf{cost}}}_2(X,C)$ (respectively, ${\\ensuremath{\\textsf{cost}}}_1(X,C)$ and ${\\ensuremath{\\textsf{cost}}}_\\infty(X,C)$).\n\n\\item {\\index{(k,eps)-coreset for k-means / k-median / k-center}\\trmbitx $(k,{\\varepsilon})$-coreset for $k$-means / $k$-median / $k$-center:}\\quad \nGiven a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, then a subset $S \\subset X$ is a $(k,{\\varepsilon})$-coreset for $k$-means (respectively, $k$-median and $k$-center) if for all center sets $C$ of size $k$ and parameter $p=2$ (respectively $p=1$ and $p=\\infty$) that \n", "index": 19, "text": "\n\\[\n(1-{\\varepsilon}) {\\ensuremath{\\textsf{cost}}}_p(X,C) \\leq {\\ensuremath{\\textsf{cost}}}_p(S,C) \\leq (1+{\\varepsilon}) {\\ensuremath{\\textsf{cost}}}_p(X,C).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"(1-{\\varepsilon}){\\textsf{cost}}_{p}(X,C)\\leq{\\textsf{cost}}_{p}(S,C)\\leq(1+{%&#10;\\varepsilon}){\\textsf{cost}}_{p}(X,C).\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00617.tex", "nexttext": "\nIn many cases the constant $\\gamma$ may not be needed; it will be $0$ unless stated.  \n\n\\item {\\index{strong coreset}\\trmbitx strong coreset:}\\quad Given a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, it is a subset $S \\subset X$ that approximates the distance to any $k$-tuple of $j$-flats up to a multiplicative $(1+{\\varepsilon})$ factor.  \n\n\\item {\\index{weak coreset}\\trmbitx weak coreset:}\\quad\nGiven a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, it is a subset $S \\subset X$ such that the cost of the optimal solution (or one close to the optimal solution) of $(k,j)$-clustering on $S$, approximates the optimal solution on $X$ up to a $(1+{\\varepsilon})$ factor.  So a strong coreset is also a weak coreset, but not the other way around.  \n\n\\end{gllist}\n\n\n\n\n\\Bnn{$k$-MEANS and $k$-MEDIAN CLUSTERING CORESETS}\n\\label{sec:clustering}\n\n\\noindent \n$(k,{\\varepsilon})$-Coresets for $k$-means and for $k$-median are closely related.  The best bounds on the size of these coresets are independent of both $n$ and $d$, the number and dimension of points in the original point set $X$.  \nFeldman and Langberg~\\cite{FL11} showed how to construct a strong $(k,{\\varepsilon})$-coreset for $k$-median and $k$-means clustering of size $O(dk/{\\varepsilon}^2)$.  \nThey also show how to construct a weak $(k,{\\varepsilon})$-coreset~\\cite{FMS07} of size $O(k \\log(1/{\\varepsilon})/{\\varepsilon}^3)$.  \nThese bounds can generalize for any ${\\ensuremath{\\textsf{cost}}}_p$ for $p \\geq 1$.  However, note that for any fixed $X$ and $C$ that ${\\ensuremath{\\textsf{cost}}}_p(X,C) > {\\ensuremath{\\textsf{cost}}}_{p'}(X,C)$ for $p > p'$, hence these bounds are not meaningful for the $p = \\infty$ special case associated with the $k$-center problem.  \nUsing the Merge-Reduce framework, the weak coreset constructions can be realized in a stream using $O((k/{\\varepsilon}^3) \\log(1/{\\varepsilon}) \\log^4 n)$ space.  \n\nInterestingly, in contrast to earlier work on these problems~\\cite{HK07,HM04,Che06} which applied various forms of geometric discretization of ${\\ensuremath{\\mathbb{R}}}^d$, the above results make an explicit connection with VC-dimension-type results and density approximation~\\cite{LS10,VX12}.  The idea is each point $x \\in X$ is associated with a function $f_x(\\cdot) = {\\ensuremath{\\textsf{cost}}}_p(x,\\cdot)$, and the total cost ${\\ensuremath{\\textsf{cost}}}_p(X,\\cdot)$ is a sum of these.  \nThen the mapping of these functions onto $k$ centers corresponds to a generalized dimension, similar to the VC-dimension of a dual range space, with dimension $O(kd)$, and then standard sampling arguments can be applied.  \n\n\n\n\n\\Bnn{$k$-CENTER CLUSTERING CORESETS}\n\n\\noindent \nThe $k$-center clustering problem is harder than the $k$-means and $k$-median ones.  It is NP-hard to find a set of centers $\\tilde C$ such that ${\\ensuremath{\\textsf{cost}}}_\\infty(X,\\tilde C) \\leq (2-\\eta) {\\ensuremath{\\textsf{cost}}}_\\infty(X,C^*)$ where $C^*$ is the optimal center set and for any $\\eta >0$~\\cite{Hoc97}.  \nYet, famously the Gonzalez algorithm~\\cite{Gon85}, which always greedily chooses the point furthest away from any of the points already chosen, finds a set $\\hat C$ of size $k$, such ${\\ensuremath{\\textsf{cost}}}_\\infty(X,\\hat C) \\leq 2 \\cdot {\\ensuremath{\\textsf{cost}}}_\\infty(X,C^*)$.  This set $\\hat C$, plus the furthest point from any of these points (i.e., run the algorithm for $k+1$ steps instead of $k$) is a $(k,1)$ coreset (yielding the above stated $2$ approximation) of size $k+1$.  \nIn a streaming setting, McCutchen and Khuller~\\cite{MK08} describe a $O(k \\log k \\cdot (1/{\\varepsilon}) \\log(1/{\\varepsilon}))$ space algorithm that provides $(2+{\\varepsilon})$ approximation for the $k$-center clustering problem, and although not stated, can probably be interpreted as a streaming $(k,1+{\\varepsilon})$-coreset for $k$-center clustering.  \n\nTo get a $(k,{\\varepsilon})$-coreset, in low dimensions, one can use the result of the Gonzalez algorithm to define a grid of size $O(k/{\\varepsilon}^d)$, keeping one point from each grid cell as a coreset of the same size~\\cite{AP02}, in time $O(n + k/{\\varepsilon}^d)$~\\cite{Har04a}.  In high dimensions one can run $O(k^{O(k/{\\varepsilon})})$ sequences of $k$ parallel MEB algorithms to find a $k$-center coreset of size $O(k/{\\varepsilon})$ in $O(dnk^{O(k/{\\varepsilon})})$ time~\\cite{BC03a}.  \n\n  \n  \n\n\\Bnn{PROJECTIVE CLUSTERING CORESETS}\n\\label{sec:projection-clustering}\n\n\\noindent \nProjective clustering seeks to find a set of $k$ subspaces of dimension $j$ which approximate a large, high-dimensional data set.  This can be seen as the combination of the subspace (matrix) approximations and clustering coresets.  \n\nPerhaps surprisingly, not all shape-fitting problems admit coresets -- and in particular subspace clustering ones pose a problem.  Har-Peled showed~\\cite{Har04b} that no coreset exists for the 2-line-center clustering problem of size sublinear in the dataset.  \nThis result can be interpreted so that for $j=1$ (and extended to $j=2)$, $k=2$, and $d=3$ then there is no coreset for projective $(k,j)$-center clustering problem sublinear in $n$.  \nMoreover a result of Meggido and Tamir~\\cite{MT83} can be interpreted to say for $j \\geq 2$ and $k > \\log n$, the solution cannot be approximated in polynomial time, for any approximation factor, unless $P = NP$.  \n\nThis motivates the study of bicriteria approximations, where the solution to the projective $(j,k,p)$-clustering problem can be approximated using a solution for larger values of $j$ and/or $k$.  \nFeldman and Langberg~\\cite{FL11} describe a strong coreset for projective $(j,k)$-clustering of size  $O(djk/{\\varepsilon}^2)$ or weak coreset of size $O(kj^2 \\log(1/{\\varepsilon})/{\\varepsilon}^3)$, which approximated $k$ subspaces of dimension $j$ using $O(k \\log n)$ subspaces of dimensions $j$.  \nThis technique can yield stronger bounds in the $j=0$ and $p=\\infty$ case (the $k$-center clustering problem) where a set of $O(k \\log n)$ cluster centers can be shown to achieve error no more than the optimal set of $k$ centers: a $(k,0)$-coreset for $k$-center clustering with an extra $O(\\log n)$ factor in the number of centers.  \nOther tradeoffs are also described in their paper were the size or approximation factor varies as the required number of subspaces changes.  \nThese approaches can be made to work in a stream with an extra factor $\\log^4 n$ in space.    \n  \n  \n \n  \nFeldman, Schmidt, and Sohler~\\cite{FSS13} consider the specific case of ${\\ensuremath{\\textsf{cost}}}_2$ and crucially make use of a non-zero $\\gamma$ value in the definition of a $(k,j,{\\varepsilon})$-coreset for projective $(k,j,2)$-clustering.  \nThey show strong coresets\nof size $O(j/{\\varepsilon})$ for $k=1$ (subspace approximation), \nof size $O(k^2/{\\varepsilon}^4)$ for $j=0$ ($k$-means clustering),\nof size $\\textsf{poly}(2^k, \\log n,1/{\\varepsilon})$ if $j=1$ ($k$-lines clustering), and\nof size $\\textsf{poly}(2^{kj},1/{\\varepsilon})$ if $j,k>1$ and under the assumption that the coordinates of all points are integers between $1$ and $n^{O(1)}$.  \nThe runtime of these algorithms is dominated by computing the principle component analysis (e.g., via the singular value decomposition) which requires $O(nd \\min\\{n,d\\})$ time; this can likely be improved using approximate variants (c.f. \\cite{CEMMP15}).  \nThese constructions also extend to streaming algorithms with extra $\\log n$ factors in space.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\A{SOURCES AND RELATED MATERIAL}\n\n\n\\vspace{-1pc}\n\\Bnn{SURVEYS}\n\n\\begin{trivlist}\n\\item[]\\cite{AHV07}: A slightly dated, but excellent survey on coresets in geometry.  \n\n\\item[]\\cite{HPbook}: Book on geometric approximation that covers many of the above topics, for instance chapters 5 (${\\varepsilon}$-approximations and ${\\varepsilon}$-nets), Chapter 19 (dimensionality reduction), and Chapter 23 (${\\varepsilon}$-kernel coresets).  \n\n\\item[]\\cite{Muthu05}: On Streaming, including history, puzzles, applications, and sketching.  \n\n\\item[]\\cite{Cormode11}: Nice introduction to sketching and its variations.  \n\n\\item[]\\cite{GI10}: Survey on $k$-sparse ${\\varepsilon}$-$(\\ell_p/\\ell_q)$ approximations.\n\n\\item[]\\cite{Mah11,Woo14}: Surveys of randomized algorithms for Matrix Sketching.\n\n\\end{trivlist}\n\n\n\\Bnn{RELATED CHAPTERS}\n\n\\noindent Chapter \\phantom{1}9: Low-distortion embeddings of finite metric spaces\n\n\\noindent Chapter 14: Geometric discrepancy theory and uniform distribution \n\n\\noindent Chapter 48: Epsilon-Nets and epsilon-approximations\n\n\n\\vspace{-1pc}\n\n\\Refh\n\n\\small\n\\bibliography{chap49}\n\n", "itemtype": "equation", "pos": 47878, "prevtext": "\n\n\\item {\\index{projective distance}\\trmbitx projective distance:}\\quad \nConsider a set $C = (C_1, C_2, \\ldots, C_k)$ of $k$ affine subspaces of dimension $j$ in ${\\ensuremath{\\mathbb{R}}}^d$, and a power $p \\in [1,\\infty)$.  \nThen for a point $x \\in {\\ensuremath{\\mathbb{R}}}^d$ the projective distance is defined ${\\ensuremath{\\textsf{dist}}}_p(C,x) = \\min_{C_i \\in C} \\|x - \\pi_{C_i}(x)\\|^p$, recalling that $\\pi_{C_i}(x) = \\arg \\min_{y \\in C_i} \\|x-y\\|$.  \n\n\\item {\\index{projective (k,j,p)-clustering problem}\\trmbitx projective $(k,j,p)$-clustering problem:}\\quad \nGiven a set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, find a set $C$ of $k$ $j$-dimensional affine subspaces of size $k$ that minimizes ${\\ensuremath{\\textsf{cost}}}_p(X,C) = \\sum_{x \\in X} {\\ensuremath{\\textsf{dist}}}_p(C,x)$.  \n\n\\item {\\index{(k,j,eps)-coreset for projective (k,j,p)-clustering}\\trmbitx $(k,j,{\\varepsilon})$-coreset for projective $(k,j,p)$-clustering:}\\quad \nGiven a point set $X \\subset {\\ensuremath{\\mathbb{R}}}^d$, then a subset $S \\subset X$, a weight function $w : S \\to {\\ensuremath{\\mathbb{R}}}^+$, and a constant $\\gamma$, is a $(k,j,{\\varepsilon})$-coreset for projective $(k,j,p)$-clustering if for all $j$-dimensional center sets $C$ of size $k$ that \n", "index": 21, "text": "\n\\[\n(1-{\\varepsilon}) {\\ensuremath{\\textsf{cost}}}_p(X,C) \\leq {\\ensuremath{\\textsf{cost}}}_p(X,w,C) + \\gamma \\leq (1+{\\varepsilon}) {\\ensuremath{\\textsf{cost}}}_p(X,C).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"(1-{\\varepsilon}){\\textsf{cost}}_{p}(X,C)\\leq{\\textsf{cost}}_{p}(X,w,C)+\\gamma%&#10;\\leq(1+{\\varepsilon}){\\textsf{cost}}_{p}(X,C).\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>\u03b3</mi></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mtext>\ud835\uddbc\ud835\uddc8\ud835\uddcc\ud835\uddcd</mtext><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]