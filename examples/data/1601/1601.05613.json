[{"file": "1601.05613.tex", "nexttext": "\nThere are two methods to measure the distance in Grassmann manifold. One is to map the Grassmann points into tangent spaces where there exist measures \\cite{CetingulWrightThompsonVidal2014}\\cite{GohVidal2008}. Another method is to embed the Grassmann manifold into the symmetric matrix spaces while the  Euclidean distance is available. The latter one is easier and effective in practice, and the mapping relation can be represented as \\cite{HarandiSandersonShenLovell2013},\n\n", "itemtype": "equation", "pos": 13870, "prevtext": "\n\n\n\n\n\n\\title{Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds} \n\n\\author{Boyue~Wang, \n        Yongli~Hu~\\IEEEmembership{Member,~IEEE,} Junbin~Gao, Yanfeng~Sun~\\IEEEmembership{Member,~IEEE,} and Baocai~Yin  ~\\IEEEmembership{Member,~IEEE} \n\\IEEEcompsocitemizethanks{\n\\IEEEcompsocthanksitem  Boyue Wang, Yongli Hu, and Yanfeng Sun are with Beijing Municipal Key Lab of Multimedia and Intelligent Software Technology, College of Metropolitan Transportation, Beijing University of Technology, Beijing 100124, China. \n\nE-mail: boyue.wang@emails.bjut.edu.cn, \\{huyongli,yfsun\\}@bjut.edu.cn\n\\IEEEcompsocthanksitem Junbin Gao is with the Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney, Camperdown, NSW 2006, Australia. \\protect E-mail: junbin.gao@sydney.edu.au \n\\IEEEcompsocthanksitem Baocai Yin is with the School of Software Technology at Dalian University of Technology, Dalian 116620, China; and with Beijing Municipal Key Lab of Multimedia and Intelligent Software Technology at Beijing University of Technology, Beijing 100124, China. \\protect E-mail: ybc@bjut.edu.cn\n\n}\n}\n\n\\markboth{IEEE Transactions on ~XX,~Vol.~XX, No.~X, January~2016}\n{Wang \\MakeLowercase{\\textit{et al.}}: Kernelized LRR on Grassmann }\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\n\n\nAs a significant subspace clustering method, low rank representation (LRR) has attracted great attention in recent years. To further improve the performance of LRR and extend its applications, there are several issues to be resolved. The nuclear norm in LRR does not sufficiently use the prior knowledge of the rank which is known in many practical problems. The LRR is designed for vectorial data from linear spaces, thus not suitable for high dimensional data with intrinsic non-linear manifold structure. This paper proposes an extended LRR model for manifold-valued Grassmann data which incorporates prior knowledge by minimizing partial sum of singular values instead of the nuclear norm, namely Partial Sum minimization of Singular Values Representation (GPSSVR). The new model not only enforces the global structure of data in low rank, but also retains important information by minimizing only smaller singular values. \n\n\nTo further maintain the local structures among Grassmann points, we also integrate the Laplacian penalty with GPSSVR. An effective algorithm is proposed to solve the optimization problem based on the GPSSVR model. The proposed model and algorithms are assessed on some widely used human action video datasets and a real scenery dataset. The experimental results show that the proposed methods obviously outperform other state-of-the-art methods.\n\n\\end{abstract}\n\\begin{IEEEkeywords}\nLow Rank Representation, Nuclear norm, Subspace Clustering, Grassmann Manifold, Laplacian Matrix\n\\end{IEEEkeywords}}\n\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\n\n\n\n\n\nClustering is a fundamental problem in computer vision and machine learning and a large number of methods have been proposed to solve this problem, such as the conventional iterative methods \\cite{Tseng2000,HoYangLimLeeKriegman2003}, the statistical methods \\cite{TippingBishop1999a,GruberWeiss2004}, the factorization-based algebraic approaches \\cite{Kanatani2001,MaYangDerksenFossum2008,HongWrightHuangMa2006}, and the spectral clustering methods \\cite{LiuLinSunYuMa2013,Luxburg2007,ChenLerman2009,ElhamifarVidal2009,LiuLinYu2010,LiuYan2011,FavaroVidalRavichandran2011,LangLiuYuYan2012}. Among all the clustering methods, the Spectral Clustering (SC) algorithm is state-of-the-art with good performance in many applications \\cite{Luxburg2007,ElhamifarVidal2013} by exploring affinity information of data. In this framework, final clustering is obtained by applying a spectral method such as the Normalized Cuts (NCut) \\cite{ShiMalik2000} on the affinity matrix learned from data.  As a result, how to construct an effective affinity matrix becomes the key problem. In this paradigm, two representative methods are Sparse Subspace Clustering (SSC) \\cite{ElhamifarVidal2013} and Low Rank Representation (LRR)\\cite{LiuLinSunYuMa2013}, which are both based on the data self-expressive property. SSC uses the sparsest self representation of data produced by $l_1$ norm to construct the affinity matrix, while LRR relies on the Rank Minimization regularization, inspired by   Robust Principal Component Analysis (RPCA) \\cite{CandesLiMaWright2011}. Different from SSC, which only independently focuses on the sparsest representation for each datum and ignores the relations among object data, LRR explore the matrix rank to capture the underlying global structure hidden in data set. It has been proven that, when a data set is actually from a union of several low-dimension subspaces, LRR can reveal the this structure to facilitate subspace clustering \\cite{LiuLinSunYuMa2013}. In many clustering scenarios, LRR has obtained successful applications, such as face recognition \\cite{ZhangJiangDavis2013}, visual tracking \\cite{ZhangGhanemLiuAhuja2012} and saliency detection \\cite{LangLiuYuYan2012}.\n\n\n\n\n\\begin{figure}\n    \\begin{center}\n    \\includegraphics[width=0.5\\textwidth]{Fig1a}\n    \\end{center}\n    \\caption{(1) Image sets are represented as Grassmann points. (2) All Grassmann points are mapped into the symmetric matrices. (3) PSSVR model is formulated in symmetric matrix space and we constrain the coefficient matrix maintaining the inner structure of origin data. (4) Clustering by NCuts.}\\label{Fig1}\n\\end{figure}\n\n\nAlthough LRR shows good clustering performance, it also suffers some shortcomings. The core idea in the original LRR is based on the Rank Minimization principle which results in a non-convex problem. To provide a practical implementation for LRR, one employs the nuclear norm as a surrogate of the Rank Minimization regularization, resulting in a convex optimization problem. Cand\\'{e}s et al. \\cite{CandesTao2010} prove that under certain incoherence assumption on the singular values of the data matrix, solving the convex nuclear norm regularized problem leads to a approximation low-rank solution. \nHowever, the indirectly minimizing the rank of the coefficient matrix by nuclear norm is not a perfect approximation way. The main argument is that nuclear norm minimizes the sum of all the singular values of the affinity matrix and treats all the singular values equally to decrease the rank of the matrix. This strategy ignores the fact that different singular values of the matrix generally correspond to different importance. Additionally, in many applications, the rank of the matrix is known, for example, 3 in  photometric stereo application and 1 in background subtraction, but in the current minimization of nuclear norm, this prior information does not been well utilized.\n\n\nTo address the issues associated with the nuclear norm, researchers propose some non-convex penalty functions which are better approximation to the rank minimization \nand easier to optimize. Gu \\emph{et al}. \\cite{GuZhangZuoFeng2014} propose the weighted nuclear norm minimization method (WNNM). Jeong and Lee \\cite{JeongLee2014} use the Schatten $p$-norm of the singular values to fit the rank minimization of \\sout{the} a matrix. Most interestingly, Oh \\emph{et al}. \\cite{OhTaiBazinKimKweon2015} propose minimizing only the smallest $N-r$ singular values while keeping the largest $r$ singular values unconstrained, where $N$ is the number of singular values of the matrix and $r$ is the targeted rank of the matrix which could be usually estimated by using prior knowledge. \nActually, the larger the singular value is, the more energy the corresponding singular vector contains. So concentrating energy into several larger singular values benefits for clustering or classification via reducing the rank of the affinity matrix. Inspired by the the Partial Sum minimization of Singular Values (PSSV) method \\cite{OhTaiBazinKimKweon2015} for matrix completion, we replace the nuclear norm in LRR by the PSSV norm to construct a new clustering model, called Partial Sum Minimization of Singular Values Representation (PSSVR) model. Compared with the LRR model, PSSVR is not only able to capture global structures of the data, but also takes into account the prior knowledge of the practical applications.\n\n\n\n\n\n\n\nWe also note that LRR or other clustering methods such as the mentioned SSC is designed for vectorial data which are generated from linear spaces and that  data similarities are measured by Euclidean distance. This has limited the application of LRR in handling with very high dimensional data, due to high computational cost, such as large scale image sets and video data. Additionally, it has been proven that such high dimension data are always embedded in nonlinear low dimension manifold \\cite{WangShanChenGao2008} and are difficult to be represented by the current LRR method. In fact, with the widely using of cheaper cameras \nin many domains such as human action recognition, safety production detection and traffic jam detection, there are huge amount of video data need to be processed efficiently. However, it is impossible to deal with so many videos in real world, even if few giving labels, \nthus the unsupervised video clustering algorithms have attracted increasing interests in recent years \\cite{ShiraziHarandiSandersonAlaviLovell2012,WangHuGaoSunYin2016,TuragaVeeraraghavanSrivastavaChellappa2011} and it is urgently desired to achieve good clustering performance for the real world videos. Particularly, it is necessary to explore a proper representation of high dimensional data and incorporate their low dimensional manifold characteristics.\n\n\nIn order to explore the nonlinear manifold structure existed in the high dimension data and obtain its proper representation, many manifold learning methods are proposed, such as Locally Linear Embedding (LLE) \\cite{RoweisSaul2000}, ISOMAP \\cite{TenenbaumSilvaLangford2000}, Locally Linear Projection (LLP) \\cite{HeNiyogi2003}, Local Tangent Space Alignment (LTSA) \\cite{ZhangZha2004}. However, these methods are mainly for vectorial data usually with higher computational complexity and are not suitable to process videos from wild practical sensors. As each of those videos may contain different number of frames even without correct temporal relation, simply vectorizing them may produce vectors in different dimensions. Considering these obstacles of video vector representation, Grassmann manifold becomes a competitive tool. Grassmann manifold is widely used to represent videos in recent researches \\cite{TuragaVeeraraghavanSrivastavaChellappa2011,HarandiSandersonShiraziLovell2011,HarandiSandersonShenLovell2013},\nnot relying on the number of frames of videos. Another key reason why Grassmann manifold is popular for video representation is that Grassmann manifold can be easily embedded into a linear space --- Symmetric matrix space. Therefore, all abstract Grassmann points are embedded into the symmetric matrix space and the clustering methods can be applied in the embedded space for Grassmann manifold. Utilizing these advantages of Grassmann manifold, we represent the high dimensional videos or image sets as Grassmann manifold points for clustering.\n\n\nCombining Grassmann manifold with the above PSSVR, it leads to a new clustering method, namely Grassmann manifold PSSVR model (GPSSVR). The whole clustering procedure is illustrated in Fig. \\ref{Fig1}. The videos or image sets are firstly represented as Grassmann manifold points. All Grassmann points are then embedded into Symmetric matrix space as mentioned above, thus we naturally extend the PSSVR model onto the Grassmann manifold. GPSSVR explores the intrinsic relation hidden in non-linear high dimension data and implements clustering on manifold. \n\n\n GPSSVR mainly reveals the global structure underlying the data while the local structure information of the data is not well considered. To address this limitation, we further introduce a local structure constraint based on Laplacian matrix into our model to model the local feature of the data and get a Laplacian GPSSVR, named as LapGPSSVR.  The contributions of this paper are summarized as follows:\n\n\\begin{itemize}\n\\item Reforming the nuclear norm term in the classical LRR model to the partial sum minimization of singular values and constructing a novel self-representation based PSSV model, so-called PSSVR;\n\\item Extending the PSSVR model onto the Grassmann Manifold based on our previous works in \\cite{WangHuGaoSunYin2016}\\cite{WangHuGaoSunYin2014}; and giving a practical solution to the proposed GPSSVR model; and\n\\item Introducing a Laplacian matrix based constraint into the GPSSVR model to represent the local geometry of the data. \n\\end{itemize}\n\n\nThe rest of the paper is organized as follows. In Section \\ref{Sec:2}, we review the geometric properties of Grassmann manifold and some basic knowledge of LRR and PSSV. In Section \\ref{Sec:3}, we propose the PSSVR on Grassmann manifold and detail the solution to it. In Section \\ref{Sec:4}, a Laplacian constraint is introduced into our proposed model to maintain the local structure of data. In Section \\ref{Sec:5}, the performance of the proposed methods are evaluated on several public datasets. In the last section, we give the conclusion and elaborate the future work.\n\n\n\n\\section{Background Theory}\\label{Sec:2}\n\n\nWe review some concepts about Grassmann Manifold, Low Rank Representation and Partial Sum Minimization of Singular Values, which lead us the grounding for our proposed method.\n\n\\subsection{Grassmann Manifold}\nGrassmann manifold $\\mathcal{G}(p,d)$ \\cite{AbsilMahonySepulchre2008} consists of the set of all linear $p$-dimensional subspaces of $R^d (0<p<d)$, which stands on a space of $p\\times d$ matrices with $d$ orthogonal columns:\n", "index": 1, "text": "\n\\[\n\\mathcal{G}(p,d) = \\{X\\in R^{d\\times p} : X^T X = I_p\\}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{G}(p,d)=\\{X\\in R^{d\\times p}:X^{T}X=I_{p}\\}\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>X</mi><mo>\u2208</mo><msup><mi>R</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow><mo>:</mo><mrow><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>=</mo><msub><mi>I</mi><mi>p</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nThe embedding $\\Pi(X)$ is diffeomorphism \\cite{HelmkeHuper2007}. In this paper, we adopt the second strategy on Grassmann manifold to define the following distance inherited from the symmetric matrix space under this mapping,\n\n\n", "itemtype": "equation", "pos": 14408, "prevtext": "\nThere are two methods to measure the distance in Grassmann manifold. One is to map the Grassmann points into tangent spaces where there exist measures \\cite{CetingulWrightThompsonVidal2014}\\cite{GohVidal2008}. Another method is to embed the Grassmann manifold into the symmetric matrix spaces while the  Euclidean distance is available. The latter one is easier and effective in practice, and the mapping relation can be represented as \\cite{HarandiSandersonShenLovell2013},\n\n", "index": 3, "text": "\\begin{equation}\\label{Grassmann2Sym_mapping}\n  \\Pi : \\mathcal{G}(p,d) \\rightarrow \\text{Sym}(d), \\ \\ \\ \\Pi(X)=XX^T.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\Pi:\\mathcal{G}(p,d)\\rightarrow\\text{Sym}(d),\\ \\ \\ \\Pi(X)=XX^{T}.\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>:</mo><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2192</mo><mrow><mtext>Sym</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"17.5pt\">,</mo><mrow><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>X</mi><mo>\u2062</mo><msup><mi>X</mi><mi>T</mi></msup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nOne point on Grassmann manifold is actually an equivalent class of all the orthogonal \\sout{high }matrices in $\\mathbb{R}^{d\\times p}$, any one of which can be converted to the other by a $p\\times p$ orthogonal matrix. Thus Grassmann manifold is naturally regarded as a good representation for video clips, thus used to tackle the problem of videos matching.\n\n\\subsection{Low Rank Representation}\nGiven a set of data drawn from an unknown union of subspaces $X = [\\mathbf x_1, \\mathbf x_2, ..., \\mathbf x_m]\\in\\mathbb{R}^{d\\times m}$ where $d$ is the data dimension, the objective of subspace clustering is to assign each data sample to its underlying subspace. The basic assumption is that the data in $X$ are drawn from the union of $k$ subspaces $\\{\\mathcal{S}_i\\}^k_{i=1}$ of dimensions $\\{d_i\\}^k_{i=1}$.\n\nUnder the data self representation principle, each data point in the dataset can be written as a linear combination of other data points, i.e., $X=XZ$, where $Z\\in\\mathbb{R}^{m\\times m}$ is a matrix of similarity coefficients.\n\nThe LRR model is formulated as \\cite{LiuLinYu2010}\n\n", "itemtype": "equation", "pos": 14767, "prevtext": "\nThe embedding $\\Pi(X)$ is diffeomorphism \\cite{HelmkeHuper2007}. In this paper, we adopt the second strategy on Grassmann manifold to define the following distance inherited from the symmetric matrix space under this mapping,\n\n\n", "index": 5, "text": "\\begin{equation}\\label{Dist_Grassmann}\n d^2_g(X,Y) = \\frac12\\|\\Pi(X)-\\Pi(Y)\\|^2_F. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"d^{2}_{g}(X,Y)=\\frac{1}{2}\\|\\Pi(X)-\\Pi(Y)\\|^{2}_{F}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>d</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $E$ is the error resulting from the self representation. $F$-norm can be changed to other norms e.g. $\\ell_{1,2}$ as done in the original LRR model. When the data set does not contain many outliers, the final clustering accuracies have little differences between using $F$-norm and $\\ell_{1,2}-$norm. What is more, problem \\eqref{LRRModel} has a closed-form solution which is faster many times than the one with the $\\ell_{1,2}$-norm. LRR takes a holistic view in favor of a coefficient matrix in the lowest rank, measured by the nuclear norm $\\|\\cdot\\|_*$, which uses the sum of singular values of the matrix to approximate to the Rank Minimization regularization.\n\n\\subsection{Partial Sum Minimization of Singular Values in RPCA (PSSV)}\n\nTo recover a low rank matrix $A$ from corrupted data $X$, RPCA \\cite{WrightGaneshRaoPengMa2009} minimizes the  rank of matrix $A$ by formulating the following problem,\n", "itemtype": "equation", "pos": 15957, "prevtext": "\n\nOne point on Grassmann manifold is actually an equivalent class of all the orthogonal \\sout{high }matrices in $\\mathbb{R}^{d\\times p}$, any one of which can be converted to the other by a $p\\times p$ orthogonal matrix. Thus Grassmann manifold is naturally regarded as a good representation for video clips, thus used to tackle the problem of videos matching.\n\n\\subsection{Low Rank Representation}\nGiven a set of data drawn from an unknown union of subspaces $X = [\\mathbf x_1, \\mathbf x_2, ..., \\mathbf x_m]\\in\\mathbb{R}^{d\\times m}$ where $d$ is the data dimension, the objective of subspace clustering is to assign each data sample to its underlying subspace. The basic assumption is that the data in $X$ are drawn from the union of $k$ subspaces $\\{\\mathcal{S}_i\\}^k_{i=1}$ of dimensions $\\{d_i\\}^k_{i=1}$.\n\nUnder the data self representation principle, each data point in the dataset can be written as a linear combination of other data points, i.e., $X=XZ$, where $Z\\in\\mathbb{R}^{m\\times m}$ is a matrix of similarity coefficients.\n\nThe LRR model is formulated as \\cite{LiuLinYu2010}\n\n", "index": 7, "text": "\\begin{align}\n\\min_{Z,  E}\\| E\\|^2_{F} + \\lambda \\| Z\\|_*,  \\text{ s.t. } X = XZ+E\\label{LRRModel},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{Z,E}\\|E\\|^{2}_{F}+\\lambda\\|Z\\|_{*},\\text{ s.t. }X=XZ+E,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>Z</mi><mo>,</mo><mi>E</mi></mrow></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>E</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow></mrow><mo>,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mi>X</mi></mrow></mrow><mo>=</mo><mrow><mrow><mi>X</mi><mo>\u2062</mo><mi>Z</mi></mrow><mo>+</mo><mi>E</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $E\\in R^{d\\times m}$ is the noise which is assumed to be sparse in RPCA model. If the data is corrupted by Gaussian noise, the $l_1$ norm can be replaced by Frobenius norm. However, in many practical problems the rank can be estimated, the nuclear norm limits model performance due to over-relaxing the rank minimization constraints and ignoring the individual importance of each singular value of matrix $A$. Based on the prior knowledge about the rank of matrix $A$, Oh \\emph{et al}. \\cite{OhTaiBazinKimKweon2015} propose a new model to minimize partial sum of singular values of matrix $A$ while maintain rest singular values unconstrained, as defined by the following problem,\n\n", "itemtype": "equation", "pos": 16982, "prevtext": "\nwhere $E$ is the error resulting from the self representation. $F$-norm can be changed to other norms e.g. $\\ell_{1,2}$ as done in the original LRR model. When the data set does not contain many outliers, the final clustering accuracies have little differences between using $F$-norm and $\\ell_{1,2}-$norm. What is more, problem \\eqref{LRRModel} has a closed-form solution which is faster many times than the one with the $\\ell_{1,2}$-norm. LRR takes a holistic view in favor of a coefficient matrix in the lowest rank, measured by the nuclear norm $\\|\\cdot\\|_*$, which uses the sum of singular values of the matrix to approximate to the Rank Minimization regularization.\n\n\\subsection{Partial Sum Minimization of Singular Values in RPCA (PSSV)}\n\nTo recover a low rank matrix $A$ from corrupted data $X$, RPCA \\cite{WrightGaneshRaoPengMa2009} minimizes the  rank of matrix $A$ by formulating the following problem,\n", "index": 9, "text": "\n\\[\n\\min\\limits_{A,E}\\|A\\|_*+\\lambda\\|E\\|_0 \\ \\ \\text{s.t.} \\ \\ X=A+E,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{A,E}\\|A\\|_{*}+\\lambda\\|E\\|_{0}\\ \\ \\text{s.t.}\\ \\ X=A+E,\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>A</mi><mo>,</mo><mi>E</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>A</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>E</mi><mo>\u2225</mo></mrow><mn>0</mn></msub></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>X</mi></mrow><mo>=</mo><mrow><mi>A</mi><mo>+</mo><mi>E</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $\\|A\\|_{>r}=\\sum\\limits_{i=r+1}^{\\min(d,m)}\\sigma_i (A)$ and $\\sigma_i (A)$ represents the $i$-th singular value of the matrix $A$. Notation $r$ is the expected rank of the matrix $A$ which may be derived from the prior knowledge of defined problem.\n\n\n\\section{Partial Sum Minimization of Singular Values Representation on Grassmann Manifold (GPSSVR)}\\label{Sec:3}\nIn this section, we will propose an improved Rank Minimization approximation-based subspace clustering method, namely Partial Sum Minimization of Singular Values Representation, and extend it onto Grassmann manifold. An effective solution to the proposed model on Grassmann manifold is explored.\n\n\\subsection{PSSVR on Grassmann}\n\nFor a set of samples $X=[\\mathbf x_1, \\mathbf x_2, ..., \\mathbf x_m]\\in R^{d\\times m}$, we adopt the self-representation method same as LRR to represent the data but replace the nuclear norm of LRR with the partial sum of singular values of coefficient matrix, so we construct a Partial Sum Minimization of Singular Values Representation method (PSSVR) as follows:\n\n", "itemtype": "equation", "pos": 17743, "prevtext": "\nwhere $E\\in R^{d\\times m}$ is the noise which is assumed to be sparse in RPCA model. If the data is corrupted by Gaussian noise, the $l_1$ norm can be replaced by Frobenius norm. However, in many practical problems the rank can be estimated, the nuclear norm limits model performance due to over-relaxing the rank minimization constraints and ignoring the individual importance of each singular value of matrix $A$. Based on the prior knowledge about the rank of matrix $A$, Oh \\emph{et al}. \\cite{OhTaiBazinKimKweon2015} propose a new model to minimize partial sum of singular values of matrix $A$ while maintain rest singular values unconstrained, as defined by the following problem,\n\n", "index": 11, "text": "\\begin{align}\n\\min\\limits_{A,E} \\|A\\|_{>r} + \\lambda\\|E\\|_1, \\ \\ \\text{s.t.} \\ \\ X=A+E, \\label{PSSVModel}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{A,E}\\|A\\|_{&gt;r}+\\lambda\\|E\\|_{1},\\ \\ \\text{s.t.}\\ \\ X%&#10;=A+E,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>A</mi><mo>,</mo><mi>E</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>A</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>E</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>X</mi></mrow><mo>=</mo><mrow><mi>A</mi><mo>+</mo><mi>E</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nHere we use $\\|\\cdot\\|_F^2$ instead of $\\|\\cdot\\|_1$ in \\eqref{PSSVModel} to measure the reconstruct error $E$. By eliminating variable $E$, we can rewrite an equivalent problem as follows\n\n", "itemtype": "equation", "pos": 18927, "prevtext": "\nwhere $\\|A\\|_{>r}=\\sum\\limits_{i=r+1}^{\\min(d,m)}\\sigma_i (A)$ and $\\sigma_i (A)$ represents the $i$-th singular value of the matrix $A$. Notation $r$ is the expected rank of the matrix $A$ which may be derived from the prior knowledge of defined problem.\n\n\n\\section{Partial Sum Minimization of Singular Values Representation on Grassmann Manifold (GPSSVR)}\\label{Sec:3}\nIn this section, we will propose an improved Rank Minimization approximation-based subspace clustering method, namely Partial Sum Minimization of Singular Values Representation, and extend it onto Grassmann manifold. An effective solution to the proposed model on Grassmann manifold is explored.\n\n\\subsection{PSSVR on Grassmann}\n\nFor a set of samples $X=[\\mathbf x_1, \\mathbf x_2, ..., \\mathbf x_m]\\in R^{d\\times m}$, we adopt the self-representation method same as LRR to represent the data but replace the nuclear norm of LRR with the partial sum of singular values of coefficient matrix, so we construct a Partial Sum Minimization of Singular Values Representation method (PSSVR) as follows:\n\n", "index": 13, "text": "\\begin{equation}\\label{PSSVR-1}\n\\min\\limits_{Z} \\|Z\\|_{>r}+ \\|E\\|_F^2 \\ \\ \\text{s.t.} \\ \\ X = XZ + E.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{Z}\\|Z\\|_{&gt;r}+\\|E\\|_{F}^{2}\\ \\ \\text{s.t.}\\ \\ X=XZ+E.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><msubsup><mrow><mo>\u2225</mo><mi>E</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>X</mi></mrow><mo>=</mo><mrow><mrow><mi>X</mi><mo>\u2062</mo><mi>Z</mi></mrow><mo>+</mo><mi>E</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere the measure $\\|\\mathbf{x}_i-\\sum\\limits_{j=1}^m Z_{ij}\\mathbf{x}_j\\|_F^2$ is the Euclidean distance between the point $\\mathbf x_i$ and its linear combination of all the other data points including $\\mathbf x_i$ and $Z = [Z_{ij}]$.\n\nNow, let us consider the generalization of problem \\eqref{PSSVR-2} on Grassmann manifold. Given a set of Grassmann points $\\mathcal{X}=\\{X_1, X_2,...,X_m\\}$ where $X_i \\in \\mathcal{G}(p,d)$ and $m$ is the number of samples. Intuitively translating the PSSVR model to non-flat Grassmann manifolds results in the following formulation:\n\n", "itemtype": "equation", "pos": 19234, "prevtext": "\n\nHere we use $\\|\\cdot\\|_F^2$ instead of $\\|\\cdot\\|_1$ in \\eqref{PSSVModel} to measure the reconstruct error $E$. By eliminating variable $E$, we can rewrite an equivalent problem as follows\n\n", "index": 15, "text": "\\begin{equation}\\label{PSSVR-2}\n\\min\\limits_{Z} \\|Z\\|_{>r}+ \\sum\\limits_{i=1}^{m}\\|\\mathbf{x}_i-\\sum_{j=1}^m Z_{ij}\\mathbf{x}_j\\|_F^2,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{Z}\\|Z\\|_{&gt;r}+\\sum\\limits_{i=1}^{m}\\|\\mathbf{x}_{i}-\\sum_{j=1}^{m}%&#10;Z_{ij}\\mathbf{x}_{j}\\|_{F}^{2},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $\\left\\| X_i \\ominus (\\biguplus^m_{j=1}Z_{ij}\\odot X_j)\\right\\|_{\\mathcal{G}} $ with the operator $\\ominus$ represents the manifold distance between $X_i$ and its reconstruction $\\biguplus^N_{j=1}Z_{ij}\\odot X_j$ which denotes the combination operation on the manifold. So to establish the PSSVR model on Grassmann Manifold, one should define a proper distance and a combination operations on the manifold.\n\nFrom the geometric property of Grassmann manifold, we can use the metric of Grassmann manifold in \\eqref{Dist_Grassmann} to replace the manifold distance in \\eqref{LRRM}, i.e. $\\left\\| X_i \\ominus (\\biguplus^m_{j=1}Z_{ij}\\odot X_j)\\right\\|_{\\mathcal{G}} =d_g( X_i ,\\biguplus^m_{j=1}Z_{ij}\\odot X_j)$.\nAdditionally, from the mapping in \\eqref{Grassmann2Sym_mapping}, the mapped points are positive definite matrices in $\\text{Sym}(d)$, so they have the natural linear combination operation like that in Euclidean space. Thus we can replace the Grassmann points with its mapped points to implement the combination in \\eqref{LRRM}, i.e.\n", "itemtype": "equation", "pos": 19957, "prevtext": "\nwhere the measure $\\|\\mathbf{x}_i-\\sum\\limits_{j=1}^m Z_{ij}\\mathbf{x}_j\\|_F^2$ is the Euclidean distance between the point $\\mathbf x_i$ and its linear combination of all the other data points including $\\mathbf x_i$ and $Z = [Z_{ij}]$.\n\nNow, let us consider the generalization of problem \\eqref{PSSVR-2} on Grassmann manifold. Given a set of Grassmann points $\\mathcal{X}=\\{X_1, X_2,...,X_m\\}$ where $X_i \\in \\mathcal{G}(p,d)$ and $m$ is the number of samples. Intuitively translating the PSSVR model to non-flat Grassmann manifolds results in the following formulation:\n\n", "index": 17, "text": "\\begin{align}\n\\min_{Z}\\|Z\\|_{>r}+\\lambda\\sum^m_{i=1}\\bigg\\| X_i \\ominus (\\biguplus^m_{j=1}Z_{ij}\\odot X_j)\\bigg\\|_{\\mathcal{G}}, \\label{LRRM}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{Z}\\|Z\\|_{&gt;r}+\\lambda\\sum^{m}_{i=1}\\bigg{\\|}X_{i}\\ominus(%&#10;\\biguplus^{m}_{j=1}Z_{ij}\\odot X_{j})\\bigg{\\|}_{\\mathcal{G}},\" display=\"inline\"><mrow><mrow><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><msub><mrow><mo maxsize=\"210%\" minsize=\"210%\">\u2225</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2296</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u228e</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2299</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">\u2225</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $ \\mathcal{X} = \\{ X_1X_1^T, X_2X_2^T, ..., X_mX_m^T\\}\\subset \\text{Sym}(d)$ is a $3$-order tensor which stacks all mapped symmetric matrices along the $3$rd mode \\cite{KoldaBader2009}. Up to now, we can construct the PSSVR model on Grassmann Manifold as follows,\n\n", "itemtype": "equation", "pos": 21158, "prevtext": "\nwhere $\\left\\| X_i \\ominus (\\biguplus^m_{j=1}Z_{ij}\\odot X_j)\\right\\|_{\\mathcal{G}} $ with the operator $\\ominus$ represents the manifold distance between $X_i$ and its reconstruction $\\biguplus^N_{j=1}Z_{ij}\\odot X_j$ which denotes the combination operation on the manifold. So to establish the PSSVR model on Grassmann Manifold, one should define a proper distance and a combination operations on the manifold.\n\nFrom the geometric property of Grassmann manifold, we can use the metric of Grassmann manifold in \\eqref{Dist_Grassmann} to replace the manifold distance in \\eqref{LRRM}, i.e. $\\left\\| X_i \\ominus (\\biguplus^m_{j=1}Z_{ij}\\odot X_j)\\right\\|_{\\mathcal{G}} =d_g( X_i ,\\biguplus^m_{j=1}Z_{ij}\\odot X_j)$.\nAdditionally, from the mapping in \\eqref{Grassmann2Sym_mapping}, the mapped points are positive definite matrices in $\\text{Sym}(d)$, so they have the natural linear combination operation like that in Euclidean space. Thus we can replace the Grassmann points with its mapped points to implement the combination in \\eqref{LRRM}, i.e.\n", "index": 19, "text": "\n\\[\n\\biguplus^N_{j=1}Z_{ij}\\odot X_j =\\mathcal{X}\\times_3 Z ,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\biguplus^{N}_{j=1}Z_{ij}\\odot X_{j}=\\mathcal{X}\\times_{3}Z,\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u228e</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2299</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></mrow><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><msub><mo>\u00d7</mo><mn>3</mn></msub><mi>Z</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere the reconstruct error $\\mathcal E$ is also a $3$-order tensor and the coefficient matrix $Z\\in R^{m\\times m}$. This formula is named as GPSSVR.\n\n\\subsection{Algorithm for PSSVR on Grassmann Manifold}\n\n\nTo solve the GPSSVR problem in \\eqref{GPLRR}, we firstly simplify the representation of the tensor of reconstruct error $\\mathcal E$ to avoid the complex calculation between 3-order tensor and a matrix in \\eqref{GPLRR}.\n\nWe consider the $i$-th front slice $E_i$ of the tensor $\\mathcal E$, i.e.,\n", "itemtype": "equation", "pos": 21493, "prevtext": "\nwhere $ \\mathcal{X} = \\{ X_1X_1^T, X_2X_2^T, ..., X_mX_m^T\\}\\subset \\text{Sym}(d)$ is a $3$-order tensor which stacks all mapped symmetric matrices along the $3$rd mode \\cite{KoldaBader2009}. Up to now, we can construct the PSSVR model on Grassmann Manifold as follows,\n\n", "index": 21, "text": "\\begin{equation}\\label{GPLRR}\n\\min\\limits_{\\mathcal{E},Z} \\|Z\\|_{>r} + \\lambda\\|\\mathcal{E}\\|_F^2 \\ \\ \\  \\text{s.t.} \\ \\ \\ \\mathcal{X}=\\mathcal{X}\\times_3Z+\\mathcal{E}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{\\mathcal{E},Z}\\|Z\\|_{&gt;r}+\\lambda\\|\\mathcal{E}\\|_{F}^{2}\\ \\ \\ %&#10;\\text{s.t.}\\ \\ \\ \\mathcal{X}=\\mathcal{X}\\times_{3}Z+\\mathcal{E}.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>,</mo><mi>Z</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>=</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><msub><mo>\u00d7</mo><mn>3</mn></msub><mi>Z</mi></mrow><mo>+</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\n\nDenote\n\n", "itemtype": "equation", "pos": 22180, "prevtext": "\nwhere the reconstruct error $\\mathcal E$ is also a $3$-order tensor and the coefficient matrix $Z\\in R^{m\\times m}$. This formula is named as GPSSVR.\n\n\\subsection{Algorithm for PSSVR on Grassmann Manifold}\n\n\nTo solve the GPSSVR problem in \\eqref{GPLRR}, we firstly simplify the representation of the tensor of reconstruct error $\\mathcal E$ to avoid the complex calculation between 3-order tensor and a matrix in \\eqref{GPLRR}.\n\nWe consider the $i$-th front slice $E_i$ of the tensor $\\mathcal E$, i.e.,\n", "index": 23, "text": "\n\\[\nE_i = X_iX_i^T-\\sum\\limits_{j=1}^{m}Z_{ij}(X_jX_j^T).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"E_{i}=X_{i}X_{i}^{T}-\\sum\\limits_{j=1}^{m}Z_{ij}(X_{j}X_{j}^{T}).\" display=\"block\"><mrow><mrow><msub><mi>E</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>X</mi><mi>i</mi><mi>T</mi></msubsup></mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>X</mi><mi>j</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $\\Delta_{ij}=\\Delta_{ji}$. Hence we can define an $N \\times N$ symmetric matrix\n\n", "itemtype": "equation", "pos": 22250, "prevtext": "\n\n\nDenote\n\n", "index": 25, "text": "\\begin{equation*}\\label{Delta_ijmn}\n\\Delta_{ij} = \\text{tr}[(X_j^{T}X_i)(X_i^{T}X_j)],\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\Delta_{ij}=\\text{tr}[(X_{j}^{T}X_{i})(X_{i}^{T}X_{j})],\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>X</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>X</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>X</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nNow it is straightforward to represent the reconstruct error $\\|\\mathcal E\\|_F^2$ as\n\n", "itemtype": "equation", "pos": 22439, "prevtext": "\nwhere $\\Delta_{ij}=\\Delta_{ji}$. Hence we can define an $N \\times N$ symmetric matrix\n\n", "index": 27, "text": "\\begin{equation*}\\label{Delta_mn}\n\\Delta = (\\Delta_{ij})_{i=1,j=1}^{N}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\Delta=(\\Delta_{ij})_{i=1,j=1}^{N}.\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mrow><mi>N</mi></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nTherefore, substituting \\eqref{GPLRR_EReconstruct} into the objective function in \\eqref{GPLRR} results in an equivalent and solvable optimization model,\n\n", "itemtype": "equation", "pos": 22613, "prevtext": "\n\nNow it is straightforward to represent the reconstruct error $\\|\\mathcal E\\|_F^2$ as\n\n", "index": 29, "text": "\\begin{align}\n\\|\\mathcal{E}\\|_F^2 = \\text{tr}(\\Delta) -2\\text{tr}(Z\\Delta)+\\text{tr}(Z\\Delta Z^T). \\label{GPLRR_EReconstruct}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mathcal{E}\\|_{F}^{2}=\\text{tr}(\\Delta)-2\\text{tr}(Z\\Delta)+%&#10;\\text{tr}(Z\\Delta Z^{T}).\" display=\"inline\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><mrow><mrow><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\n\n\nTo tackle this problem, we use the alternating direction method (ADM) \\cite{LinLiuSu2011,BoydParikhChuPeleatoEckstein2011} which is widely used to solve unconstrained convex problems \\cite{LiuLinSunYuMa2013,LiuYan2012}. Firstly, we introduce an auxiliary  variable $J=Z \\in R^{m\\times m}$ to separate the terms of variable $Z$ and reformulate the optimization problem as follows,\n\n", "itemtype": "equation", "pos": 22906, "prevtext": "\n\nTherefore, substituting \\eqref{GPLRR_EReconstruct} into the objective function in \\eqref{GPLRR} results in an equivalent and solvable optimization model,\n\n", "index": 31, "text": "\\begin{align}\n\\min\\limits_{Z} - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + \\|Z\\|_{>r}. \\label{GPLRR-2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{Z}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z%&#10;\\Delta Z^{T})+\\|Z\\|_{&gt;r}.\" display=\"inline\"><mrow><mrow><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nThus the Augmented Lagrangian Multiplier (ALM) method can be applied to absorb the linear constrain into the objective function as follows,\n\n", "itemtype": "equation", "pos": 23422, "prevtext": "\n\n\n\nTo tackle this problem, we use the alternating direction method (ADM) \\cite{LinLiuSu2011,BoydParikhChuPeleatoEckstein2011} which is widely used to solve unconstrained convex problems \\cite{LiuLinSunYuMa2013,LiuYan2012}. Firstly, we introduce an auxiliary  variable $J=Z \\in R^{m\\times m}$ to separate the terms of variable $Z$ and reformulate the optimization problem as follows,\n\n", "index": 33, "text": "\\begin{equation}\n\\begin{aligned}\n\\min\\limits_{Z,J} - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + \\|J\\|_{>r} \\ \\ \\text{s.t.} \\ \\ J=Z.\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{Z,J}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z%&#10;\\Delta Z^{T})+\\|J\\|_{&gt;r}\\ \\ \\text{s.t.}\\ \\ J=Z.\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>Z</mi><mo>,</mo><mi>J</mi></mrow></munder><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>J</mi></mrow><mo>=</mo><mi>Z</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere matrix $Y$ indicates a Lagrangian Multiplier and $\\mu$ is a weight to tune the error term $\\|Z-J\\|_F^2$.\n\nThe ALM formulation \\eqref{GPLRR_ALM} can be naturally solved by  alternatively solving for $Z$, $J$ and $Y$, respectively in an iterative procedure. The pseudo code of our proposed method is summarized in Algorithm 1. Now, we will analyze how to update these variables in each iteration.\n\n\\begin{algorithm}\\label{wholeAlg}\n\\caption{ Solving the problem \\eqref{GPLRR_ALM} by ADM.}\n\\begin{algorithmic}[1]\n\\REQUIRE The Grassmann sample set $\\{X_i\\}_{i=1}^m$,$X_i\\in \\mathcal{G}(p,d)$, the cluster number $k$, the expected rank $r$, and the balancing parameters $\\lambda$. \\\\\n\\ENSURE  The GPSSVR representation $Z$ ~~\\\\\n\\STATE   Initialize:$J=Z=0,Y=0,\\mu=10^{-6},\\mu_{max}=10^{10}$ and $\\varepsilon=10^{-8}$\n\\FOR{i=1:m}\n\\FOR{j=1:m}\n\\STATE   $\\Delta_{ij}\\leftarrow \\mbox{tr}[(X_j^{T}X_i)(X_i^{T}X_j)]$;\n\\ENDFOR\n\\ENDFOR\n\\WHILE   {not converged}\n\\STATE   fix $Z$ and update $J$ by \\\\$J\\leftarrow \\min\\limits_{J}(||J||_{>r}+<Y,Z-J>+\\frac{\\mu}{2}||Z-J||_F^2) $;\n\\STATE   fix $J$ and update $Z$ by \\\\$Z = (2\\lambda\\Delta + \\mu J - Y)(2\\lambda \\Delta + \\mu I)^{-1}$ ;\n\\STATE   update the multipliers: \\\\ $Y\\leftarrow Y+\\mu(Z-J)$\n\\STATE   update the parameter $\\mu$ by $\\mu \\leftarrow \\min(\\rho\\mu,\\mu_{\\mbox{max}})$\n\\STATE   check the convergence condition: \\\\ {$\\|Z-J\\|_\\infty <\\varepsilon$}\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsubsection{Updating $J$}\nTo update $J^{k+1}$ at the $(k+1)$th iteration, we fix $Z$ and $Y$ to their $k$-th iteration values, respectively, and solve the following problem accordingly:\n\n", "itemtype": "equation", "pos": 23742, "prevtext": "\n\nThus the Augmented Lagrangian Multiplier (ALM) method can be applied to absorb the linear constrain into the objective function as follows,\n\n", "index": 35, "text": "\\begin{equation}\\label{GPLRR_ALM}\n\\begin{aligned}\nf(Z,J,Y,\\mu) &= - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T)+\\|J\\|_{>r} \\\\\n             &+ \\langle Y,Z-J \\rangle + \\frac{\\mu}{2}\\|Z-J\\|_F^2\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle f(Z,J,Y,\\mu)\" display=\"inline\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo>,</mo><mi>J</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>\u03bc</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z\\Delta Z^{T})+\\|J%&#10;\\|_{&gt;r}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\langle Y,Z-J\\rangle+\\frac{\\mu}{2}\\|Z-J\\|_{F}^{2}\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nFor the above problem \\eqref{PGLRR_subproblemJ}, a closed-form solution is suggested in \\cite{OhTaiBazinKimKweon2015} as the following theorem.\n\n\\begin{theorem}Given that $U D V^T = \\text{SVD}(Z+\\frac{Y}{\\mu})$ as defined above, the solution to \\eqref{PGLRR_subproblemJ} is given by\n", "itemtype": "equation", "pos": 25606, "prevtext": "\nwhere matrix $Y$ indicates a Lagrangian Multiplier and $\\mu$ is a weight to tune the error term $\\|Z-J\\|_F^2$.\n\nThe ALM formulation \\eqref{GPLRR_ALM} can be naturally solved by  alternatively solving for $Z$, $J$ and $Y$, respectively in an iterative procedure. The pseudo code of our proposed method is summarized in Algorithm 1. Now, we will analyze how to update these variables in each iteration.\n\n\\begin{algorithm}\\label{wholeAlg}\n\\caption{ Solving the problem \\eqref{GPLRR_ALM} by ADM.}\n\\begin{algorithmic}[1]\n\\REQUIRE The Grassmann sample set $\\{X_i\\}_{i=1}^m$,$X_i\\in \\mathcal{G}(p,d)$, the cluster number $k$, the expected rank $r$, and the balancing parameters $\\lambda$. \\\\\n\\ENSURE  The GPSSVR representation $Z$ ~~\\\\\n\\STATE   Initialize:$J=Z=0,Y=0,\\mu=10^{-6},\\mu_{max}=10^{10}$ and $\\varepsilon=10^{-8}$\n\\FOR{i=1:m}\n\\FOR{j=1:m}\n\\STATE   $\\Delta_{ij}\\leftarrow \\mbox{tr}[(X_j^{T}X_i)(X_i^{T}X_j)]$;\n\\ENDFOR\n\\ENDFOR\n\\WHILE   {not converged}\n\\STATE   fix $Z$ and update $J$ by \\\\$J\\leftarrow \\min\\limits_{J}(||J||_{>r}+<Y,Z-J>+\\frac{\\mu}{2}||Z-J||_F^2) $;\n\\STATE   fix $J$ and update $Z$ by \\\\$Z = (2\\lambda\\Delta + \\mu J - Y)(2\\lambda \\Delta + \\mu I)^{-1}$ ;\n\\STATE   update the multipliers: \\\\ $Y\\leftarrow Y+\\mu(Z-J)$\n\\STATE   update the parameter $\\mu$ by $\\mu \\leftarrow \\min(\\rho\\mu,\\mu_{\\mbox{max}})$\n\\STATE   check the convergence condition: \\\\ {$\\|Z-J\\|_\\infty <\\varepsilon$}\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsubsection{Updating $J$}\nTo update $J^{k+1}$ at the $(k+1)$th iteration, we fix $Z$ and $Y$ to their $k$-th iteration values, respectively, and solve the following problem accordingly:\n\n", "index": 37, "text": "\\begin{equation}\\label{PGLRR_subproblemJ}\n\\begin{aligned}\nJ^{k+1}  &= \\arg\\min\\limits_{J}f(Z^k,J,Y^k,\\mu^k) \\\\\n         &= \\arg\\min\\limits_{J}\\|J\\|_{>r} + \\langle Y,Z-J \\rangle + \\frac{\\mu}{2}\\|Z-J\\|_F^2\\\\\n         &= \\arg\\min\\limits_{J}(||J||_{>r}+\\frac{\\mu}{2}||J-(Z+\\frac{Y}{\\mu})||_F^2)\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle J^{k+1}\" display=\"inline\"><msup><mi>J</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}f(Z^{k},J,Y^{k},\\mu^{k})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>J</mi></munder><mo>\u2061</mo><mi>f</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>Z</mi><mi>k</mi></msup><mo>,</mo><mi>J</mi><mo>,</mo><msup><mi>Y</mi><mi>k</mi></msup><mo>,</mo><msup><mi>\u03bc</mi><mi>k</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}\\|J\\|_{&gt;r}+\\langle Y,Z-J\\rangle+\\frac{\\mu}{2}%&#10;\\|Z-J\\|_{F}^{2}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mi>J</mi></munder></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}(||J||_{&gt;r}+\\frac{\\mu}{2}||J-(Z+\\frac{Y}{\\mu}%&#10;)||_{F}^{2})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>J</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mrow><mo fence=\"true\">||</mo><mi>J</mi><mo fence=\"true\">||</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>J</mi><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mi>Y</mi><mi>\u03bc</mi></mfrac></mstyle></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $D_r$ and $D_{r^{'}}$ are diagonal matrices. $\\text{diag}(D_r)$ is the $r$ largest singular values and $\\text{diag}(D_{r^{'}})$ collects all the rest singular values from SVD. The singular value thresholding operator is defined as $\\mathcal{S}_\\tau[x] = \\text{sign}(x)\\cdot \\max(|x|-\\tau,0)$\n\\end{theorem}\n\\begin{proof}\nPlease refer to the proof of Lemma 1 in \\cite{OhTaiBazinKimKweon2015}.\n\\end{proof}\n\n\\subsubsection{Updating $Z$}\nTo update $Z^{k+1}$ at the $(k+1)$th iteration, we derive the ALM formulation \\eqref{GPLRR_ALM} with fixed $J$ and $Y$ and obtain the following form:\n\n", "itemtype": "equation", "pos": 26209, "prevtext": "\n\nFor the above problem \\eqref{PGLRR_subproblemJ}, a closed-form solution is suggested in \\cite{OhTaiBazinKimKweon2015} as the following theorem.\n\n\\begin{theorem}Given that $U D V^T = \\text{SVD}(Z+\\frac{Y}{\\mu})$ as defined above, the solution to \\eqref{PGLRR_subproblemJ} is given by\n", "index": 39, "text": "\n\\[\nJ^* = U (D_r + \\mathcal{S}_{\\mu^{-1}}D_{r^{'}}) V^T,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"J^{*}=U(D_{r}+\\mathcal{S}_{\\mu^{-1}}D_{r^{{}^{\\prime}}})V^{T},\" display=\"block\"><mrow><mrow><msup><mi>J</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>D</mi><mi>r</mi></msub><mo>+</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><msup><mi>\u03bc</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></msub><mo>\u2062</mo><msub><mi>D</mi><msup><mi>r</mi><msup><mi/><mo>\u2032</mo></msup></msup></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nThis is a quadratic optimization problem about $Z$. The closed-form solution is given by\n\n", "itemtype": "equation", "pos": 26858, "prevtext": "\nwhere $D_r$ and $D_{r^{'}}$ are diagonal matrices. $\\text{diag}(D_r)$ is the $r$ largest singular values and $\\text{diag}(D_{r^{'}})$ collects all the rest singular values from SVD. The singular value thresholding operator is defined as $\\mathcal{S}_\\tau[x] = \\text{sign}(x)\\cdot \\max(|x|-\\tau,0)$\n\\end{theorem}\n\\begin{proof}\nPlease refer to the proof of Lemma 1 in \\cite{OhTaiBazinKimKweon2015}.\n\\end{proof}\n\n\\subsubsection{Updating $Z$}\nTo update $Z^{k+1}$ at the $(k+1)$th iteration, we derive the ALM formulation \\eqref{GPLRR_ALM} with fixed $J$ and $Y$ and obtain the following form:\n\n", "index": 41, "text": "\\begin{equation}\\label{PGLRR_subproblemZ}\n\\begin{aligned}\nZ^{k+1} &= \\arg\\min_{Z}f(Z,J^k,Y^k,u^k)\\\\\n        &= \\arg\\min\\limits_{Z}- 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + \\langle Y,Z-J \\rangle \\\\\n         &\\qquad+ \\frac{\\mu}{2}\\|Z-J\\|_F^2.\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle Z^{k+1}\" display=\"inline\"><msup><mi>Z</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min_{Z}f(Z,J^{k},Y^{k},u^{k})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>\u2061</mo><mi>f</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo>,</mo><msup><mi>J</mi><mi>k</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>k</mi></msup><mo>,</mo><msup><mi>u</mi><mi>k</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{Z}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(%&#10;Z\\Delta Z^{T})+\\langle Y,Z-J\\rangle\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mi>Z</mi></munder></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad+\\frac{\\mu}{2}\\|Z-J\\|_{F}^{2}.\" display=\"inline\"><mrow><mrow><mo lspace=\"22.5pt\">+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\n\\subsubsection{Updating $Y$}\nMatrix $Y$ denotes the Lagrangian Multiplier. Once solving the former two subproblems about $J$ and $Z$ respectively in each iteration, we could update easily $Y$ by the following rule:\n\n", "itemtype": "equation", "pos": 27239, "prevtext": "\n\nThis is a quadratic optimization problem about $Z$. The closed-form solution is given by\n\n", "index": 43, "text": "\\begin{equation}\n\\begin{aligned}\nZ = (2\\lambda\\Delta + \\mu J - Y)(2\\lambda\\Delta + \\mu I)^{-1}.\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle Z=(2\\lambda\\Delta+\\mu J-Y)(2\\lambda\\Delta+\\mu I)^{-1}.\" display=\"inline\"><mrow><mrow><mi>Z</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>J</mi></mrow></mrow><mo>-</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>I</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\n\\subsubsection{Adapting Penalty Parameter $\\mu$}\nFor the penalty parameter $\\mu > 0$, we could update it by:\n", "itemtype": "equation", "pos": 27580, "prevtext": "\n\n\\subsubsection{Updating $Y$}\nMatrix $Y$ denotes the Lagrangian Multiplier. Once solving the former two subproblems about $J$ and $Z$ respectively in each iteration, we could update easily $Y$ by the following rule:\n\n", "index": 45, "text": "\\begin{equation}\n\\begin{aligned}\nY^{k+1} = Y^{k}+\\mu^{k}(Z^k-J^k).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle Y^{k+1}=Y^{k}+\\mu^{k}(Z^{k}-J^{k}).\" display=\"inline\"><mrow><mrow><msup><mi>Y</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>Y</mi><mi>k</mi></msup><mo>+</mo><mrow><msup><mi>\u03bc</mi><mi>k</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Z</mi><mi>k</mi></msup><mo>-</mo><msup><mi>J</mi><mi>k</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $\\mu_{\\max}$ is the upper bound of ${\\mu^k}$.\n\n\\subsubsection{Termination and Clustering}\nAfter obtaining the GPSSVR representation $Z^*$ by Algorithm 1, an affinity matrix can be constructed $W=\\frac{|Z^*|+|Z^{*^T}|}{2}$. \nThen, this affinity matrix can be used in a spectral clustering algorithm to get the final clustering. As a widely used spectral clustering algorithm in subspace segmentation problems, NCut is chosen in this paper. The whole clustering procedure of the proposed method is summarized in Algorithm 2.\n\n\\begin{algorithm}\\label{wholeAlg}\n\\caption{ Clustering algorithm by the PSSVR on Grassmann Manifold.}\n\\begin{algorithmic}[1]\n\\REQUIRE The videos for clustering $\\mathcal{X}$. \\\\\n\\ENSURE  The clustering results of $\\mathcal{X}$. ~~\\\\\n\\STATE   Representing $\\mathcal{X}$ as a set of Grassmann points.\\\\\n\\STATE   Mapping Grassmann points into symmetric space as \\eqref{Grassmann2Sym_mapping}. \\\\\n\\STATE   Obtaining the GPSSVR representation $Z^*$ of $\\mathcal{X}$ by Algorithm 1.\\\\\n\\STATE   Computing the affinity matrix $W=\\frac{|Z^*|+|Z^{*^T}|}{2}$. \\\\\n\\STATE   Implementing NCut(W) to get the final clustering result of $\\mathcal{X}$. \\\\\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Computational Complexity}\n\n\nThe computational complexity of Algorithm 1 could be divided into two parts: the data representation (steps 2 - 6) and the solution to the problem (steps 7 - 13).\n\nIn the data representation part, $\\Delta$ is calculated by using the trace operation, therefore, for the $m$ samples, the computational complexity of calculating $\\Delta$ should be $\\mathcal{O}(m^2)$. In the second part of the algorithm, the major computational complexity is the SVD decomposition of an $m\\times m$ matrix for updating $J$ in step 8, costing $\\mathcal{O}(m^3)$ computational time. However there is no need to calculate all the singular values due to the thresholding operation, instead calculating up to for example $4r$ first singular values by using the partial SVD like \\cite{LiuLinYu2010}.\nThus, for the $s$ iterations, the total cost of calculating the solution to the algorithm is $\\mathcal{O}(srm^2)$.\nThe overall computational complexity is $\\mathcal{O}(m^2)+\\mathcal{O}(srm^2)$.\n\n\\section{Laplacian PSSVR on Grassmann Manifold (LapGPSSVR)}\\label{Sec:4}\n\n\\subsection{Laplacian PSSVR on Grassmann Manifold}\nFor the self-representation based methods, the column of $Z$, denoted by $\\textbf z_i$, can be regarded as a new representation of data $\\textbf x_i$, and $Z_{ij}$ represents the similarity between data $\\textbf x_i$ and $\\textbf x_j$, accordingly. In the proposed method GPSSVR \\eqref{GPLRR}, the global structure is enforced by the global constraint of rank minimization of the matrix $Z$. To incorporate more local similarity information into $Z$ in our model, we consider imposing the local geometrical structures. \nFor this purpose, Laplacian matrix regularization is naturally regarded as a proper choice because it can maintain similarity between data. Thus a Laplacian Partial Sum Minimization of Singular Values Representation on Grassmann Manifold model, termed LapGRSSVR, can be formulated as\n\n", "itemtype": "equation", "pos": 27785, "prevtext": "\n\n\\subsubsection{Adapting Penalty Parameter $\\mu$}\nFor the penalty parameter $\\mu > 0$, we could update it by:\n", "index": 47, "text": "\n\\[\n\\mu^{k+1} = \\min(\\rho\\mu^k,\\mu_{\\max}),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\mu^{k+1}=\\min(\\rho\\mu^{k},\\mu_{\\max}),\" display=\"block\"><mrow><mrow><msup><mi>\u03bc</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><msup><mi>\u03bc</mi><mi>k</mi></msup></mrow><mo>,</mo><msub><mi>\u03bc</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nwhere $w_{ij}$ denotes the local similarity between Grassmann points $X_i$ and $X_j$. There are many ways to define $w_{ij}$'s. In this paper, we simply use the explicit neighborhood determined by its manifold distance measure to define all the $w_{ij}$. Let $C$ be a parameter of neighborhood size, and we define\n\n", "itemtype": "equation", "pos": 30974, "prevtext": "\nwhere $\\mu_{\\max}$ is the upper bound of ${\\mu^k}$.\n\n\\subsubsection{Termination and Clustering}\nAfter obtaining the GPSSVR representation $Z^*$ by Algorithm 1, an affinity matrix can be constructed $W=\\frac{|Z^*|+|Z^{*^T}|}{2}$. \nThen, this affinity matrix can be used in a spectral clustering algorithm to get the final clustering. As a widely used spectral clustering algorithm in subspace segmentation problems, NCut is chosen in this paper. The whole clustering procedure of the proposed method is summarized in Algorithm 2.\n\n\\begin{algorithm}\\label{wholeAlg}\n\\caption{ Clustering algorithm by the PSSVR on Grassmann Manifold.}\n\\begin{algorithmic}[1]\n\\REQUIRE The videos for clustering $\\mathcal{X}$. \\\\\n\\ENSURE  The clustering results of $\\mathcal{X}$. ~~\\\\\n\\STATE   Representing $\\mathcal{X}$ as a set of Grassmann points.\\\\\n\\STATE   Mapping Grassmann points into symmetric space as \\eqref{Grassmann2Sym_mapping}. \\\\\n\\STATE   Obtaining the GPSSVR representation $Z^*$ of $\\mathcal{X}$ by Algorithm 1.\\\\\n\\STATE   Computing the affinity matrix $W=\\frac{|Z^*|+|Z^{*^T}|}{2}$. \\\\\n\\STATE   Implementing NCut(W) to get the final clustering result of $\\mathcal{X}$. \\\\\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Computational Complexity}\n\n\nThe computational complexity of Algorithm 1 could be divided into two parts: the data representation (steps 2 - 6) and the solution to the problem (steps 7 - 13).\n\nIn the data representation part, $\\Delta$ is calculated by using the trace operation, therefore, for the $m$ samples, the computational complexity of calculating $\\Delta$ should be $\\mathcal{O}(m^2)$. In the second part of the algorithm, the major computational complexity is the SVD decomposition of an $m\\times m$ matrix for updating $J$ in step 8, costing $\\mathcal{O}(m^3)$ computational time. However there is no need to calculate all the singular values due to the thresholding operation, instead calculating up to for example $4r$ first singular values by using the partial SVD like \\cite{LiuLinYu2010}.\nThus, for the $s$ iterations, the total cost of calculating the solution to the algorithm is $\\mathcal{O}(srm^2)$.\nThe overall computational complexity is $\\mathcal{O}(m^2)+\\mathcal{O}(srm^2)$.\n\n\\section{Laplacian PSSVR on Grassmann Manifold (LapGPSSVR)}\\label{Sec:4}\n\n\\subsection{Laplacian PSSVR on Grassmann Manifold}\nFor the self-representation based methods, the column of $Z$, denoted by $\\textbf z_i$, can be regarded as a new representation of data $\\textbf x_i$, and $Z_{ij}$ represents the similarity between data $\\textbf x_i$ and $\\textbf x_j$, accordingly. In the proposed method GPSSVR \\eqref{GPLRR}, the global structure is enforced by the global constraint of rank minimization of the matrix $Z$. To incorporate more local similarity information into $Z$ in our model, we consider imposing the local geometrical structures. \nFor this purpose, Laplacian matrix regularization is naturally regarded as a proper choice because it can maintain similarity between data. Thus a Laplacian Partial Sum Minimization of Singular Values Representation on Grassmann Manifold model, termed LapGRSSVR, can be formulated as\n\n", "index": 49, "text": "\\begin{equation}\\label{LapGPlrr-1}\n\\begin{aligned}\n&\\min\\limits_{Z,\\mathcal E}\\|Z\\|_{>r} + \\lambda\\|\\mathcal E\\|_F^2 + \\beta\\sum\\limits_{i,j}\\| \\textbf z_i- \\textbf z_j\\|_2^2 w_{ij} \\\\ &\\text{s.t.} \\ \\ \\mathcal{X} = \\mathcal{X}_{\\times_3}Z + E\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{Z,\\mathcal{E}}\\|Z\\|_{&gt;r}+\\lambda\\|\\mathcal{E}\\|_{F}^%&#10;{2}+\\beta\\sum\\limits_{i,j}\\|\\textbf{z}_{i}-\\textbf{z}_{j}\\|_{2}^{2}w_{ij}\" display=\"inline\"><mrow><mrow><munder><mi>min</mi><mrow><mi>Z</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder></mstyle><mrow><msubsup><mrow><mo>\u2225</mo><mrow><msub><mtext>\ud835\udc33</mtext><mi>i</mi></msub><mo>-</mo><msub><mtext>\ud835\udc33</mtext><mi>j</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{s.t.}\\ \\ \\mathcal{X}=\\mathcal{X}_{\\times_{3}}Z+E\" display=\"inline\"><mrow><mrow><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>=</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><msub><mo>\u00d7</mo><mn>3</mn></msub></msub><mo>\u2062</mo><mi>Z</mi></mrow><mo>+</mo><mi>E</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere $\\mathcal{N}_C(X_j)$ denotes the $C$ nearest elements of $X_j$ on Grassmann manifold.\n\nBy introducing the Laplacian matrix $L$, problem \\eqref{LapGPlrr-1} can be easily re-written as its Laplacian form,\n\n\n\n", "itemtype": "equation", "pos": 31562, "prevtext": "\n\nwhere $w_{ij}$ denotes the local similarity between Grassmann points $X_i$ and $X_j$. There are many ways to define $w_{ij}$'s. In this paper, we simply use the explicit neighborhood determined by its manifold distance measure to define all the $w_{ij}$. Let $C$ be a parameter of neighborhood size, and we define\n\n", "index": 51, "text": "\\begin{equation*}\nw_{ij}=\\begin{cases}\nd_g(X_i,X_j), &X_i \\in \\mathcal{N}_C(X_j),\\\\\n0, & X_i \\notin \\mathcal{N}_C(X_j),\n\\end{cases}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"w_{ij}=\\begin{cases}d_{g}(X_{i},X_{j}),&amp;X_{i}\\in\\mathcal{N}_{C}(X_{j}),\\\\&#10;0,&amp;X_{i}\\notin\\mathcal{N}_{C}(X_{j}),\\end{cases}\" display=\"block\"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>d</mi><mi>g</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2209</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nwhere the Laplacian matrix $L \\in R^{m\\times m}$ is defined as $L=D-W$ where $W=[w_{ij}]_{i=1,j=1}^{m}$ and $D = \\text{diag}(d_{ii})$ with $d_{ii}=\\sum\\limits_j w_{ij}$.\n\n\\subsection{Algorithm for Laplacian PLRR on Grassmann Manifold}\nSimilar to deriving algorithm for GPSSVR, problem \\eqref{LapGPSSVR-2} can be easily converted to the following model:\n\n", "itemtype": "equation", "pos": 31921, "prevtext": "\nwhere $\\mathcal{N}_C(X_j)$ denotes the $C$ nearest elements of $X_j$ on Grassmann manifold.\n\nBy introducing the Laplacian matrix $L$, problem \\eqref{LapGPlrr-1} can be easily re-written as its Laplacian form,\n\n\n\n", "index": 53, "text": "\\begin{equation}\\label{LapGPSSVR-2}\n\\begin{aligned}\n\\min\\limits_{\\mathcal E,Z}\\|Z\\|_{>r} + \\lambda\\|\\mathcal E\\|_F^2 + 2\\beta \\text{tr}(ZLZ^T) \\ \\ \\text{s.t.} \\ \\ \\mathcal{X} = \\mathcal{X}_{\\times_3}Z + \\mathcal E\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{\\mathcal{E},Z}\\|Z\\|_{&gt;r}+\\lambda\\|\\mathcal{E}\\|_{F}^%&#10;{2}+2\\beta\\text{tr}(ZLZ^{T})\\ \\ \\text{s.t.}\\ \\ \\mathcal{X}=\\mathcal{X}_{\\times%&#10;_{3}}Z+\\mathcal{E}\" display=\"inline\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>,</mo><mi>Z</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>=</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><msub><mo>\u00d7</mo><mn>3</mn></msub></msub><mo>\u2062</mo><mi>Z</mi></mrow><mo>+</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nThus the alternating direction method (ADM) \\cite{LinLiuSu2011} can also be employed to solve this problem. Letting $J=Z$ to separate the variable $Z$ from the terms in the objective function, we can formulate the following problem for \\eqref{LapGPSSVR-3},\n\n", "itemtype": "equation", "pos": 32517, "prevtext": "\nwhere the Laplacian matrix $L \\in R^{m\\times m}$ is defined as $L=D-W$ where $W=[w_{ij}]_{i=1,j=1}^{m}$ and $D = \\text{diag}(d_{ii})$ with $d_{ii}=\\sum\\limits_j w_{ij}$.\n\n\\subsection{Algorithm for Laplacian PLRR on Grassmann Manifold}\nSimilar to deriving algorithm for GPSSVR, problem \\eqref{LapGPSSVR-2} can be easily converted to the following model:\n\n", "index": 55, "text": "\\begin{equation}\\label{LapGPSSVR-3}\n\\begin{aligned}\n\\min\\limits_{Z} - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + 2\\beta\\text{tr}(ZLZ^T) + \\|Z\\|_{>r}.\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{Z}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z%&#10;\\Delta Z^{T})+2\\beta\\text{tr}(ZLZ^{T})+\\|Z\\|_{&gt;r}.\" display=\"inline\"><mrow><mrow><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nSo, its Augmented Lagrangian Multiplier formulation can be defined as the following unconstrained optimization,\n\n", "itemtype": "equation", "pos": 32972, "prevtext": "\n\nThus the alternating direction method (ADM) \\cite{LinLiuSu2011} can also be employed to solve this problem. Letting $J=Z$ to separate the variable $Z$ from the terms in the objective function, we can formulate the following problem for \\eqref{LapGPSSVR-3},\n\n", "index": 57, "text": "\\begin{align}\n&\\min\\limits_{Z,J} - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + 2\\beta\\text{tr}(ZLZ^T) + \\|J\\|_{>r}, \\notag\\\\\n&\\text{s.t.} \\ \\ J=Z.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\limits_{Z,J}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z%&#10;\\Delta Z^{T})+2\\beta\\text{tr}(ZLZ^{T})+\\|J\\|_{&gt;r},\" display=\"inline\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>Z</mi><mo>,</mo><mi>J</mi></mrow></munder><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{s.t.}\\ \\ J=Z.\" display=\"inline\"><mrow><mrow><mrow><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>J</mi></mrow><mo>=</mo><mi>Z</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nThis problem can be solved by solving the two subproblems \\eqref{LapPSSVR_subproblemJ} and \\eqref{LapPSSVR_subproblemZ} below,\n\n", "itemtype": "equation", "pos": 33261, "prevtext": "\n\nSo, its Augmented Lagrangian Multiplier formulation can be defined as the following unconstrained optimization,\n\n", "index": 59, "text": "\\begin{align}\nf'(Z,J,Y,\\mu) =& - 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + 2\\beta\\text{tr}(ZLZ^T) \\notag\\\\\n &+ \\|J\\|_{>r} + \\langle Y,Z-J \\rangle + \\frac{\\mu}{2}\\|Z-J\\|_F^2.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f^{\\prime}(Z,J,Y,\\mu)=\" display=\"inline\"><mrow><mrow><msup><mi>f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo>,</mo><mi>J</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>\u03bc</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z\\Delta Z^{T})+2%&#10;\\beta\\text{tr}(ZLZ^{T})\" display=\"inline\"><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\|J\\|_{&gt;r}+\\langle Y,Z-J\\rangle+\\frac{\\mu}{2}\\|Z-J\\|_{F}^{2}.\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 33593, "prevtext": "\nThis problem can be solved by solving the two subproblems \\eqref{LapPSSVR_subproblemJ} and \\eqref{LapPSSVR_subproblemZ} below,\n\n", "index": 61, "text": "\\begin{equation}\\label{LapPSSVR_subproblemJ}\n\\begin{aligned}\nJ^{k+1}  &= \\arg\\min\\limits_{J}f'(Z^k,J,Y^k,\\mu^k) \\\\\n         &= \\arg\\min\\limits_{J}\\|J\\|_{>r} + \\langle Y,Z-J \\rangle + \\frac{\\mu}{2}\\|Z-J\\|_F^2\\\\\n         &= \\arg\\min\\limits_{J}(||J||_{>r}+\\frac{\\mu}{2}||J-(Z+\\frac{Y}{\\mu})||_F^2)\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle J^{k+1}\" display=\"inline\"><msup><mi>J</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}f^{\\prime}(Z^{k},J,Y^{k},\\mu^{k})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>J</mi></munder><mo>\u2061</mo><msup><mi>f</mi><mo>\u2032</mo></msup></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>Z</mi><mi>k</mi></msup><mo>,</mo><mi>J</mi><mo>,</mo><msup><mi>Y</mi><mi>k</mi></msup><mo>,</mo><msup><mi>\u03bc</mi><mi>k</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}\\|J\\|_{&gt;r}+\\langle Y,Z-J\\rangle+\\frac{\\mu}{2}%&#10;\\|Z-J\\|_{F}^{2}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mi>J</mi></munder></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>J</mi><mo>\u2225</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\min\\limits_{J}(||J||_{&gt;r}+\\frac{\\mu}{2}||J-(Z+\\frac{Y}{\\mu}%&#10;)||_{F}^{2})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>J</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mrow><mo fence=\"true\">||</mo><mi>J</mi><mo fence=\"true\">||</mo></mrow><mrow><mi/><mo>&gt;</mo><mi>r</mi></mrow></msub><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>J</mi><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mi>Y</mi><mi>\u03bc</mi></mfrac></mstyle></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\nBoth \\eqref{LapPSSVR_subproblemJ} and \\eqref{LapPSSVR_subproblemZ} can be solved similar to \\eqref{PGLRR_subproblemJ} and \\eqref{PGLRR_subproblemZ}, respectively. For example, the solution to \\eqref{LapPSSVR_subproblemZ} is given by\n\n", "itemtype": "equation", "pos": 33921, "prevtext": "\nand\n\n", "index": 63, "text": "\\begin{align}\nZ^{k+1} =& \\arg\\min_{Z}f'(Z,J^k,Y^k,u^k) \\notag\\\\\n        =& \\arg\\min\\limits_{Z}- 2\\lambda\\text{tr}(Z\\Delta) + \\lambda\\text{tr}(Z\\Delta Z^T) + 2\\beta \\text{tr}(ZLZ^T) \\notag\\\\\n        &+ \\langle Y,Z-J \\rangle + \\frac{\\mu}{2}\\|Z-J\\|_F^2. \\label{LapPSSVR_subproblemZ}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Z^{k+1}=\" display=\"inline\"><mrow><msup><mi>Z</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\arg\\min_{Z}f^{\\prime}(Z,J^{k},Y^{k},u^{k})\" display=\"inline\"><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>Z</mi></munder><mo>\u2061</mo><msup><mi>f</mi><mo>\u2032</mo></msup></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo>,</mo><msup><mi>J</mi><mi>k</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>k</mi></msup><mo>,</mo><msup><mi>u</mi><mi>k</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\arg\\min\\limits_{Z}-2\\lambda\\text{tr}(Z\\Delta)+\\lambda\\text{tr}(Z%&#10;\\Delta Z^{T})+2\\beta\\text{tr}(ZLZ^{T})\" display=\"inline\"><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mi>Z</mi></munder></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\langle Y,Z-J\\rangle+\\frac{\\mu}{2}\\|Z-J\\|_{F}^{2}.\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>J</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\n\\section{Experiments}\\label{Sec:5}\nIn this section, to test the effectiveness of our proposed methods, we conduct several unsupervised clustering experiments on different video datasets. The four video datasets used in our experiments are listed below:\n\n\\begin{itemize}\n  \\item \\textbf{SKIG action clips:} \\url{http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm};\n  \\item \\textbf{Ballet video clips:}  \\url{https://www.cs.sfu.ca/research/groups/VML/semilatent/};\n  \\item \\textbf{UCF sports clips:} \\url{http://crcv.ucf.edu/data/UCF_Sports_Action.php};\n  \\item \\textbf{Highway Traffic clips:} \\url{http://www.svcl.ucsd.edu/projects/traffic/}.\n\\end{itemize}\n\nTo demonstrate the performance of GPSSVR and LapGPSSVR methods, we compare them with several state-of-the-art clustering methods. Since our methods are related to LRR and manifold models, we mainly select LRR based methods or manifold based methods as baselines, which are listed below:\n\\begin{itemize}\n  \\item \\textbf{Sparse Subspace Clustering (SSC) \\cite{ElhamifarVidal2013}}: The SSC model aims to find the sparsest representation for each datum using $l_1$ regularization.\n  \\item \\textbf{Low Rank Representation (LRR) \\cite{LiuLinSunYuMa2013}}: The LRR model represents the holistic correlation among the data by the nuclear norm regularization.\n  \\item \\textbf{Low Rank Representation on Grassmann Manifold (GLRR-F) \\cite{WangHuGaoSunYin2014}}: The GLRR-F model embeds the image sets onto Grassmann manifold and extends the LRR model to Grassmann manifold space.\n  \\item \\textbf{Statistical computations on Grassmann and Stiefel manifolds (SCGSM) \\cite{TuragaVeeraraghavanSrivastavaChellappa2011}}: The SCGSM model explores statistical modeling methods that are derived from the Riemannian geometry of the manifold.\n  \\item \\textbf{Sparse Manifold Clustering and Embedding (SMCE) \\cite{ElhamifarVidalNips2011}}: The SMCE model utilizes the local manifold structure to find a small neighborhood around each data point and connects each point to its neighbors with appropriate weights.\n  \\item \\textbf{Latent Space Sparse Subspace Clustering (LS3C) \\cite{PatelNguyenVidal2013}}: The LS3C model describes a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space.\n\\end{itemize}\n\nIn all the experiments, the input raw data are image sets derived from video clips. To represent them as Grassmann points, for a video clip with $M$ frames, denoted by $\\{Y_i\\}_{i=1}^{M}$, where $Y_i$ is the $i$-th gray frame with dimension $a\\times b$,  we construct a matrix $\\mathcal{Y}=[\\text{vec}(Y_1), \\text{vec}(Y_2), ..., \\text{vec}(Y_M)]$ of size $(a\\times b)\\times M$. Thus, a Grassmann point can be generated by any orthogonalization procedure of $\\mathcal{Y}$. For convenience, we select SVD decomposition in our experiments i.e. $\\mathcal{Y}=U\\Sigma V^T$. Then we pick up the first $p$ singular-vectors of $U$ as the representation of a Grassmann point $X\\in\\mathcal{G}(p,a\\times b)$.\n\n\n\n\n\n\n\nTo execute all the comparative methods, we should formulate proper data representation for these methods as different methods demand different type of data inputs for clustering. For the baseline subspace clustering methods, LRR and SSC, they take vectors as input. They cannot be applied directly on data in form of Grassmann points. So we have to vectorize each video clip as vectorial inputs. However, the direct vectorization results in very high dimensional vectors which are hard to be handled on a normal PC. Thus we apply PCA to reduce these vectors to a low dimension which equals to the number of PCA components retaining 95\\% of its variance energy. The PCA projected vectors are be taken as inputs for both SSC and LRR.\n\nFor the manifold related methods, SCGSM directly implements clustering as it requires Grassmann point $X\\in \\mathcal{G}(p,a\\times b)$ as input. Since GPSSVR, LapGPSSVR, GLRR-F methods all embed Grassmann points into symmetric matrix space, we construct the corresponding symmetric matrix $XX^T\\in R^{(a\\times b)\\times (a\\times b)}$ as their inputs. Although SMCE and LS3C belong to manifold learning methods, they demand vectors as inputs. However, vectorizing the Grassmann point $X \\in \\mathcal{G}(p,a\\times b)$ will destroy the geometry of data, hence we vectorize the correspondent symmetric matrix $XX^T$ as their inputs. That is SMEC and LS3C take $\\text{VEC}(XX^T)$ as inputs for clustering.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nTo obtain good experimental performances, the model parameters $\\lambda$, $\\beta$, $r$ and $C$ should be assigned properly. First of all, we should give a exact estimation of the representation rank $r$. For a Grassmann point $X \\in \\mathcal{G}(p,d)$, the rank of the Grassmann point $X$, even the mapped symmetric matrix refer to \\eqref{Grassmann2Sym_mapping}, are equal to $p$.\nConsidering that a mapped symmetric matrix is linearly represented by other mapped symmetric matrices, so we may expect that the expected rank $r$ of the coefficient matrix $Z$ is relatively not larger than $p$. According to the analysis, in the experiment, we tune the expected rank $r$, around, e.g., $4$ to acquire the best experiment result. $\\lambda$ and $\\beta$ are the most important penalty parameters for balancing the reconstructed error term, low-rank term and Laplacian regularization term. Empirically, the best value of $\\lambda$ depends on the application problems and has a large range for different applications from $0.01$ to $20$. While the value of $\\beta$ is usually very small, ranging from $1.0\\times 10^{-4}$ to $1.0\\times 10^{-2}$, because the value of Laplacian regularization term is usually many times larger than two other terms. As for $C$, which defines the neighborhood size of each Grassmann point, large experimental results suggest that a size slightly larger than the average value of the numbers of data in different clusters is a good choice. We tune the model for $C$ around this initial value for different applications.\n\nIn our experiments, the performance of different algorithms is measured by the following  clustering accuracy\n", "itemtype": "equation", "pos": 34446, "prevtext": "\nBoth \\eqref{LapPSSVR_subproblemJ} and \\eqref{LapPSSVR_subproblemZ} can be solved similar to \\eqref{PGLRR_subproblemJ} and \\eqref{PGLRR_subproblemZ}, respectively. For example, the solution to \\eqref{LapPSSVR_subproblemZ} is given by\n\n", "index": 65, "text": "\\begin{align}\nZ = (2\\lambda\\Delta + \\mu J - Y)(2\\lambda\\Delta + 2\\beta L + \\mu I)^{-1}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Z=(2\\lambda\\Delta+\\mu J-Y)(2\\lambda\\Delta+2\\beta L+\\mu I)^{-1}.\" display=\"inline\"><mrow><mrow><mi>Z</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>J</mi></mrow></mrow><mo>-</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mi>L</mi></mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>I</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05613.tex", "nexttext": "\n\nAll the algorithms are coded in Matlab 2014a and implemented on an Intel Core i7-4770K 3.5GHz CPU machine with 32G RAM.\n\n\n\n\\subsection{Clustering on Human Action}\nGPSSVR model maintains the first $r$ singular values unconstrained to preserve the most dominant information as much as possible. At the same time, it minimizes the sum of the rest of singular values to seek a low rank global structure. Thus the proposed methods are generally more suitable for video clustering.\n\nIn the first experiment on human actions, we select two challenge action video datasets,  Ballet dataset and UCF sport dataset, to test the performance of the proposed method. With simple backgrounds, the Ballet dataset is an appropriate benchmark choice to verify the capacity of the proposed method for action recognition in a rather ideal condition; while the UCF sport dataset containing more variations on scenario and viewpoint can be used to examine the robustness of the proposed method in noised scenarios.\n\n\n\n\n\n\\subsubsection{Ballet Action Dataset}\nThis dataset \\cite{FathiMori2008} contains 44 video clips, collected from an instructional ballet DVD. The dataset consists of 8 complex action patterns performed by 3 subjects. The eight actions include: `left-to-right hand opening', `right-to-left hand opening', `standing hand opening', `leg swinging', `jumping', `turning', `hopping' and `standing still'. The dataset is challenging due to the significant intra-class variations in terms of speed, spatial and temporal scale, cloth texture and movement. The frame images are normalized and centered in a fixed size of $30 \\times 30$. Some frame samples of Ballet dataset are shown in Fig.~\\ref{FigE4}.\n\\begin{figure}\n    \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{E4}\n    \\end{center}\n    \\caption{Some samples from the Ballet Action dataset. Each row denotes a kind of action.}\\label{FigE4}\n\\end{figure}\n\nWe split each clip into subgroups of $M=12$ images and each subgroup is treated as an individual image set. As a result, we construct a total of $713$ image sets which are labeled as $8$ clusters. The dimension of subspace is set to $p=6$ and the Grassmann point can be represented as $X_i \\in \\mathcal{G}(6,900)$. For the setting of rank $r$, we test a number of different values from $1$ to $6$ and find the best expected rank$r$ is 3 in this experiment. The neighborhood size $C$ is tuned to $90$ according to the experimental results. \n For LRR and SSC methods, the subspace of vectors in size of $30\\times 30 \\times 12 = 10800$ are reduced to $135$ by PCA.\n\nTable \\ref{Ballettab} presents the experimental results of all the algorithms on the Ballet dataset. Although this dataset contains no complex background or illumination changes and can be regarded as a clean human action data in ideal condition, the accuracy in Table \\ref{Ballettab} are not very high as in the case of eight clusters. The reason is that some kind of actions are too similar to  clustering, e.g., 'left-to-right hand opening' and 'right-to-left hand opening'. Compared with GLRR-F method which is based on the nuclear norm regularization, the proposed methods give a higher clustering accuracy. This demonstrates the benefits of minimizing partial sum of smaller singular values and leaving the $r$ largest singular values unconstrained to preserve as much discrimination information as possible.  Of course, our proposed methods are also obviously superior to other classical methods.\n\\begin{table}\n   \\centering\n   \\begin{tabular}{|c|c|c|}\n     \\hline\n              \n              Methods,Datasets & Ballet\\\\\n              \\hline\n              GPSSVR & \\textbf{0.6059}  \\\\\n              \\hline\n              LapGPSSVR & \\textbf{0.6255}  \\\\\n              \\hline\n              GLRR-F & 0.5905 \\\\\n              \\hline\n              LRR & 0.2819    \\\\\n              \\hline\n              SSC & 0.2903    \\\\\n              \\hline\n              SCGSM & 0.5877  \\\\\n              \\hline\n              SMCE & 0.5105   \\\\\n              \\hline\n              LS3C & 0.4222   \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Subspace clustering results on the Ballet dataset.}\\label{Ballettab}\n\\end{table}\n\n\n\n\\subsubsection{UCF Sports action Dataset}\nThis dataset \\cite{RodriguezAhmedShah2008} consists of a set of actions collected from various sport matches which are typically featured on broadcast television channels. The dataset includes a total of 150 sequences. The collection has a natural pool of actions with a wide range of scenes and viewpoints, so that it is difficult for clustering. There are 10 actions in this dataset: Diving, Golf Swing, Kicking, Lifting, Riding Horse, Running, Skate Boarding, Swing-Bench, Swing-Side, and Walking. Each sequence has 22 to 144 frames. We convert these video clips into gray images and each image is resized into $30\\times 30$.\n\nWe regard each video clip as an image set, so the number of frames $M$ of each video clip is various for different video clips. There are totally $150$ image sets and $10$ clusters in this experiment. We select $p=10$ as the dimension of subspace for each Grassmann point. Therefore, a Grassmann point can be represented as $X_i \\in \\mathcal{G}(10,900)$. The expected rank $r$ is set to $4$ and the neighbor size $C$ is $12$. The PCA algorithm requires the image sets with the same number of samples, but the RGB-D sequences contains various frames from 63 to 605. Throwing away too many frames for longer sequences in the PCA algorithm is unfair for LRR and SSC methods, so we have to give up comparing with LRR and SSC in this experiment.\n\nThe experimental results are reported in Table \\ref{UCFtab}. Although this dataset has complex backgrounds, viewpoints change and scales variation and is regarded as a challenge dataset, the accuracy result seems be higher than the Ballet dataset. We conduct that the bigger movement in  sport actions helps to distinguish action clusters, resulting in higher accuracy.\n\n\\begin{figure}\n    \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{E6}\n    \\end{center}\n    \\caption{Some samples from the UCF sports dataset and each row represents a kind of action.}\\label{FigE6}\n\\end{figure}\n\\begin{table}\n   \\centering\n   \\begin{tabular}{|c|c|}\n     \\hline\n              \n              Methods,Datasets & SKIG\\\\\n              \\hline\n              GPSSVR & \\textbf{0.6800}\\\\\n              \\hline\n              LapGPSSVR & \\textbf{0.6800}\\\\\n              \\hline\n              GLRR-F & 0.6533\\\\\n              \\hline\n              SCGSM  & 0.5333\\\\\n              \\hline\n              SMCE   & 0.6200\\\\\n              \\hline\n              LS3C   & 0.4667\\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Subspace clustering results on the UCF sport dataset.}\\label{UCFtab}\n\\end{table}\n\n\\subsection{Clustering on Gesture Action}\nThe SKIG dataset \\cite{LiuShao2013} contains 1080 RGB-D sequences captured by a Kinect sensor. In this dataset, there are ten kinds of gestures of six persons: `circle', `triangle', `up-down', `right-left', `wave', `Z', `cross', `comehere', `turn-around', and `pat'. All the gestures are performed by fist, finger and elbow respectively under three backgrounds (wooden board, white plain paper and paper with characters) and two illuminations (strong light and poor light). Each RGB-D sequence contains a set of frames (63 to 605). Here the images are normalized to $24\\times 32$ with mean zero and unit variance. Fig. \\ref{FigE3} shows some samples of RGB images. In our experiments, we only use the RGB sequences in SKIG dataset.\n\\begin{figure}\n    \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{E3}\n    \\end{center}\n    \\caption{Some samples from the SKIG Action dataset and each row shows a kind of gesture.}\\label{FigE3}\n\\end{figure}\n\nSimilar to the previous experiments, each video clip is considered as an image set, thus a total of $540$ image sets are labeled as $10$ clusters. We preserve $p=10$ as the dimension of subspace, so the Grassmann point is represented as $X_i \\in \\mathcal{G}(10,768)$. In this experiment, we empirically set the expected rank and the neighbor size to $r=1$ and $C=65$, respectively. We did not conduct experiments for LRR and SSC due to the similar reason mentioned in the last experiment.\n\nTable \\ref{SKIGtab} presents all the experimental results on SKIG dataset. Compared with Human action datasets, the movement in this gesture dataset is smaller, and the illumination and background are more variate, therefore clustering task on this dataset is more challenging. Our proposed methods, especially LapGPSSVR method, have improved clustering accuracy over all other methods. Except for the discrimination information coming from the first $r$ largest singular values,  the Laplacian regularization also provides meaningful information for clustering.\n\\begin{table}\n   \\centering\n   \\begin{tabular}{|c|c|}\n     \\hline\n              \n              Methods,Datasets & SKIG\\\\\n              \\hline\n              GPSSVR & \\textbf{0.55}\\\\\n              \\hline\n              LapGPSSVR & \\textbf{0.5981}\\\\\n              \\hline\n              GLRR-F & 0.5056\\\\\n              \\hline\n              SCGSM  & 0.3704\\\\\n              \\hline\n              SMCE   & 0.4611\\\\\n              \\hline\n              LS3C   & 0.4148\\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Subspace clustering results on the SKIG dataset.}\\label{SKIGtab}\n\\end{table}\n\n\\subsection{Clustering on Natural Scene}\n\nIn this experiment, we wish to inspect the proposed methods on practical applications in more complex conditions, such as Traffic Dataset. The traffic dataset \\cite{ChanVasconcelos2008} used in this experiment contains 253 video sequences of highway traffic captured under various weather conditions, such as sunny, cloudy and rainy. These sequences are labeled with three traffic levels: light, medium and heavy. There are 44 clips at heavy level, 45 clips at medium level and 164 clips at light level. Each video sequence has 42 to 52 frames. The video sequences are converted to gray images and each image is normalized to size  $24 \\times 24$ with mean zero and unit variance. Some samples of the Highway traffic dataset are shown in Fig.~\\ref{FigE5}.\n\\begin{figure}\n    \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{E5}\n    \\end{center}\n    \\caption{Some samples from the Highway traffic dataset. First row illustrates heavy traffic level, second row illustrates medium traffic level and last row illustrates light traffic level.}\\label{FigE5}\n\\end{figure}\n\nThe model parameter setting is described as follows. Each video clip is regarded as an image set and we generate a total of $253$ image sets labeled with 3 clusters. The Grassmann points in this experiment are chosen as $p=10$ dimension subspaces, i.e., $X_i\\in \\mathcal{G}(10,576)$. We choose the expected rank and the neighbor size as $r=2$ and $C=61$. For LRR and SSC methods, we vectorize the first $42$ frames in each clip (discarding the rest frames in the clip) and then use PCA algorithm to reduce the dimension $24\\times 24 \\times 42 = 24192$ to $147$.\n\nTable \\ref{Traffictab} presents the clustering result of all the algorithms. Obviously, our proposed methods get the highest accuracy $89.33\\%$ which almost reaches the highest classification accuracy 92.18\\% in \\cite{SankaranarayananTuragaBaraniukChellappa2010}. For the real traffic applications, we can use the proposed methods to learn the different thresholds for the traffic jam levels on specific roads based on the historical traffic data, which is considered more accurate classify the traffic jam levels on individual road than using the empirical uniform thresholds for all roads. So, this method is meaningful for some practical applications.\n\n\\begin{table}\n   \\centering\n   \\begin{tabular}{|c|c|}\n     \\hline\n              \n              Methods, Datasets & Traffic\\\\\n              \\hline\n              GPSSVR &  \\textbf{0.8617}\\\\\n              \\hline\n              LapGPSSVR &  \\textbf{0.8933}\\\\\n              \\hline\n              GLRR-F & 0.8498 \\\\\n              \\hline\n              LRR & 0.6838 \\\\\n              \\hline\n              SSC & 0.6285 \\\\\n              \\hline\n              SCGSM & 0.6443 \\\\\n              \\hline\n              SMCE & 0.5613 \\\\\n              \\hline\n              LS3C & 0.6364 \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Subspace clustering results on the Traffic dataset.}\\label{Traffictab}\n\\end{table}\n\n\\section{Conclusion}\\label{Sec:6}\nIn this paper, we have proposed a novel PSSVR model on Grassmann manifold by embedding the manifold onto the space of symmetric matrices. Compared with the nuclear norm used in the LRR method, PSSV is proved a better approximation to the rank minimization problem, which is beneficial in exploring the global structure of data. We also provide efficient algorithms for the proposed methods. The computational complexity of the proposed GPSSVR method is presented, which proves that our algorithms are effective. In addition, to maintain the local structure hidden in data, we introduce sout{the} a Laplacian regularization into our model. Several public video datasets are used to evaluate the performance of the proposed methods. The experimental results show that the proposed methods outperform the state-of-the-art clustering methods.\n\n\\section*{Acknowledgements}\nThe research project is supported by the Australian Research Council (ARC) through the grant DP140102270 and also partially supported by National Natural Science Foundation of China under Grant No. 61390510, 61133003, 61370119, 61171169, 61227004, Beijing Natural Science Foundation No. 4132013 and Funding Project for Academic Human Resources Development in Institutions of Higher Learning Under the Jurisdiction of Beijing Municipality(PHR(IHLB)).\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{reference_boyue}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{boyuewang}}]\n{Boyue Wang} received the B.Sc. degree from Hebei University of Technology,\nTianjin, China, in 2012. he is currently pursuing the\nPh.D. degree in the Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology,\nBeijing University of Technology, Beijing.\nHis current research interests include computer\nvision, pattern recognition, manifold learning and kernel methods.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{huyongli}}]\n{Yongli Hu} received his Ph.D. degree from Beijing University of Technology in 2005. He is a professor in College of Metropolitan Transportation at Beijing University of Technology. He is\na researcher at the Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology.\nHis research interests include computer graphics, pattern recognition and multimedia technology.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{JunbinGao}}]\n{Junbin Gao} graduated from Huazhong University of Science and Technology (HUST),\nChina in 1982 with BSc. degree in Computational Mathematics and\nobtained PhD from Dalian University of Technology, China in 1991. He is a Professor of Big Data Analytics in the University of Sydney Business School at the University of Sydney and was a Professor in Computer Science\nin the School of Computing and Mathematics at Charles Sturt\nUniversity, Australia. He was a senior lecturer, a lecturer in Computer Science from 2001 to 2005 at\nUniversity of New England, Australia. From 1982 to 2001 he was an\nassociate lecturer, lecturer, associate professor and professor in\nDepartment of Mathematics at HUST. His main research interests\ninclude machine learning, data analytics, Bayesian learning and\ninference, and image analysis.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{YanfengSun}}]\n{Yanfeng Sun} received her Ph.D. degree from Dalian University of Technology in 1993. She is a professor in College of Metropolitan Transportation at Beijing University of Technology. She is\na researcher at the Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology. She is the membership of China Computer Federation.\n Her research interests are multi-functional perception and image processing.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{BaocaiYin}}]\n{Baocai Yin} received his Ph.D. degree from Dalian University of Technology in 1993.\nHe is a Professor in School of Software Technology, Dalian University of Technology. He is\na researcher at the Beijing Municipal Key Laboratory of Multimedia and Intelligent Software Technology.\nHe is a member of China Computer Federation. His\nresearch interests cover multimedia, multifunctional perception, virtual reality and computer graphics.\n\\end{IEEEbiography}\n\n\n", "itemtype": "equation", "pos": 40657, "prevtext": "\n\n\\section{Experiments}\\label{Sec:5}\nIn this section, to test the effectiveness of our proposed methods, we conduct several unsupervised clustering experiments on different video datasets. The four video datasets used in our experiments are listed below:\n\n\\begin{itemize}\n  \\item \\textbf{SKIG action clips:} \\url{http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm};\n  \\item \\textbf{Ballet video clips:}  \\url{https://www.cs.sfu.ca/research/groups/VML/semilatent/};\n  \\item \\textbf{UCF sports clips:} \\url{http://crcv.ucf.edu/data/UCF_Sports_Action.php};\n  \\item \\textbf{Highway Traffic clips:} \\url{http://www.svcl.ucsd.edu/projects/traffic/}.\n\\end{itemize}\n\nTo demonstrate the performance of GPSSVR and LapGPSSVR methods, we compare them with several state-of-the-art clustering methods. Since our methods are related to LRR and manifold models, we mainly select LRR based methods or manifold based methods as baselines, which are listed below:\n\\begin{itemize}\n  \\item \\textbf{Sparse Subspace Clustering (SSC) \\cite{ElhamifarVidal2013}}: The SSC model aims to find the sparsest representation for each datum using $l_1$ regularization.\n  \\item \\textbf{Low Rank Representation (LRR) \\cite{LiuLinSunYuMa2013}}: The LRR model represents the holistic correlation among the data by the nuclear norm regularization.\n  \\item \\textbf{Low Rank Representation on Grassmann Manifold (GLRR-F) \\cite{WangHuGaoSunYin2014}}: The GLRR-F model embeds the image sets onto Grassmann manifold and extends the LRR model to Grassmann manifold space.\n  \\item \\textbf{Statistical computations on Grassmann and Stiefel manifolds (SCGSM) \\cite{TuragaVeeraraghavanSrivastavaChellappa2011}}: The SCGSM model explores statistical modeling methods that are derived from the Riemannian geometry of the manifold.\n  \\item \\textbf{Sparse Manifold Clustering and Embedding (SMCE) \\cite{ElhamifarVidalNips2011}}: The SMCE model utilizes the local manifold structure to find a small neighborhood around each data point and connects each point to its neighbors with appropriate weights.\n  \\item \\textbf{Latent Space Sparse Subspace Clustering (LS3C) \\cite{PatelNguyenVidal2013}}: The LS3C model describes a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space.\n\\end{itemize}\n\nIn all the experiments, the input raw data are image sets derived from video clips. To represent them as Grassmann points, for a video clip with $M$ frames, denoted by $\\{Y_i\\}_{i=1}^{M}$, where $Y_i$ is the $i$-th gray frame with dimension $a\\times b$,  we construct a matrix $\\mathcal{Y}=[\\text{vec}(Y_1), \\text{vec}(Y_2), ..., \\text{vec}(Y_M)]$ of size $(a\\times b)\\times M$. Thus, a Grassmann point can be generated by any orthogonalization procedure of $\\mathcal{Y}$. For convenience, we select SVD decomposition in our experiments i.e. $\\mathcal{Y}=U\\Sigma V^T$. Then we pick up the first $p$ singular-vectors of $U$ as the representation of a Grassmann point $X\\in\\mathcal{G}(p,a\\times b)$.\n\n\n\n\n\n\n\nTo execute all the comparative methods, we should formulate proper data representation for these methods as different methods demand different type of data inputs for clustering. For the baseline subspace clustering methods, LRR and SSC, they take vectors as input. They cannot be applied directly on data in form of Grassmann points. So we have to vectorize each video clip as vectorial inputs. However, the direct vectorization results in very high dimensional vectors which are hard to be handled on a normal PC. Thus we apply PCA to reduce these vectors to a low dimension which equals to the number of PCA components retaining 95\\% of its variance energy. The PCA projected vectors are be taken as inputs for both SSC and LRR.\n\nFor the manifold related methods, SCGSM directly implements clustering as it requires Grassmann point $X\\in \\mathcal{G}(p,a\\times b)$ as input. Since GPSSVR, LapGPSSVR, GLRR-F methods all embed Grassmann points into symmetric matrix space, we construct the corresponding symmetric matrix $XX^T\\in R^{(a\\times b)\\times (a\\times b)}$ as their inputs. Although SMCE and LS3C belong to manifold learning methods, they demand vectors as inputs. However, vectorizing the Grassmann point $X \\in \\mathcal{G}(p,a\\times b)$ will destroy the geometry of data, hence we vectorize the correspondent symmetric matrix $XX^T$ as their inputs. That is SMEC and LS3C take $\\text{VEC}(XX^T)$ as inputs for clustering.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nTo obtain good experimental performances, the model parameters $\\lambda$, $\\beta$, $r$ and $C$ should be assigned properly. First of all, we should give a exact estimation of the representation rank $r$. For a Grassmann point $X \\in \\mathcal{G}(p,d)$, the rank of the Grassmann point $X$, even the mapped symmetric matrix refer to \\eqref{Grassmann2Sym_mapping}, are equal to $p$.\nConsidering that a mapped symmetric matrix is linearly represented by other mapped symmetric matrices, so we may expect that the expected rank $r$ of the coefficient matrix $Z$ is relatively not larger than $p$. According to the analysis, in the experiment, we tune the expected rank $r$, around, e.g., $4$ to acquire the best experiment result. $\\lambda$ and $\\beta$ are the most important penalty parameters for balancing the reconstructed error term, low-rank term and Laplacian regularization term. Empirically, the best value of $\\lambda$ depends on the application problems and has a large range for different applications from $0.01$ to $20$. While the value of $\\beta$ is usually very small, ranging from $1.0\\times 10^{-4}$ to $1.0\\times 10^{-2}$, because the value of Laplacian regularization term is usually many times larger than two other terms. As for $C$, which defines the neighborhood size of each Grassmann point, large experimental results suggest that a size slightly larger than the average value of the numbers of data in different clusters is a good choice. We tune the model for $C$ around this initial value for different applications.\n\nIn our experiments, the performance of different algorithms is measured by the following  clustering accuracy\n", "index": 67, "text": "\n\\[\n\\text{Accuracy} = \\frac{\\text{number of correctly classified points}}{\\text{total number of points}}\\times 100\\%.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\text{Accuracy}=\\frac{\\text{number of correctly classified points}}{\\text{%&#10;total number of points}}\\times 100\\%.\" display=\"block\"><mrow><mrow><mtext>Accuracy</mtext><mo>=</mo><mrow><mfrac><mtext>number of correctly classified points</mtext><mtext>total number of points</mtext></mfrac><mo>\u00d7</mo><mrow><mn>100</mn><mo lspace=\"0pt\" rspace=\"3.5pt\">%</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]