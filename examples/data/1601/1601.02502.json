[{"file": "1601.02502.tex", "nexttext": "\nwhere $C$ is the set of sentences constituting the training corpus, and $s[w-l:w+l]$ is a word window on the sentence $s$ centered around $w$.\nFor the sake of simplicity this equation does not include the ``negative-sampling'' term, see \\cite{mikolov2013efficient} for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip-gram can be seen as a materialization of the distributional hypothesis \\cite{harris1968mathematical}: ``Words used in similar contexts have similar meanings''.\nWe will now see how to extend this idea to cross-lingual contexts.\n\n\n\n\n\\subsection{Trans-gram}\nIn this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages.\n\n\\begin{figure*}[t]\n\\begin{adjustbox}{max width=\\textwidth}\n\\begin{tikzpicture}\n[\nsum/.style={ellipse, draw=blue!80, fill=white!0, very thick, minimum width=1cm},\nwordSkip/.style={rectangle, draw=forestgreen!80, fill=white!0, very thick, minimum width=1cm},\nsentSkip/.style={rectangle, draw=blue!80, fill=white!0, very thick,  minimum width=1cm, xshift=-6mm},\narrowSkip/.style={draw=blue},\nwordTrans/.style={rectangle, draw=forestgreen!80, fill=white!0, very thick, minimum width=1cm},\nsentTrans/.style={draw=red!80, fill=white!0, very thick,  minimum width=1cm, xshift=-6mm},\narrowTrans/.style={draw=red}\n]\n\n\n\\node[sentSkip]  (sEn)    {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{cat}}}}}}}}}_{\\operatorname{{}}} }$ \\;\\textunderscore\\; ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{on}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{e}}} }$};\n\n\n\\node[wordSkip]  (satEn)  [above = of sEn]    {${ \\vec{\\vphantom{l}\\operatorname{{sits}}}_{\\operatorname{{e}}} }$};\n\\draw[arrowSkip] (sEn.north west)    --  (satEn.south west);\n\\draw[arrowSkip] (sEn.north east)    --  (satEn.south east);\n\\path (sEn.north) -- node (JEn) {$J_{e}$} (satEn.south);\n\n\n\n\\node[sentTrans]  (sFrEn)   [right = of sEn]   {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{cat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sits}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{on}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{mat}}}}}}}}}_{\\operatorname{{e}}} }$};\n\n\\node[wordTrans]  (satFrEn) [above = of sFrEn]      {${ \\vec{\\vphantom{l}\\operatorname{{assis}}}_{\\operatorname{{f}}} }$};\n\\draw[arrowTrans] (sFrEn.north west)    --  (satFrEn.south west);\n\\draw[arrowTrans] (sFrEn.north east)    --  (satFrEn.south east);\n\\path (sFrEn.north) -- node (JFrEn) {$\\Omega_{f,e}$} (satFrEn.south);\n\n\\node[sentTrans]  (sEnFr)   [right = of sFrEn]   {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{chat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{est}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{assis}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sur}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{tapis}}}}}}}}}_{\\operatorname{{f}}} }$};\n\n\\node[wordTrans]  (satEnFr) [above = of sEnFr]   {${ \\vec{\\vphantom{l}\\operatorname{{sits}}}_{\\operatorname{{e}}} }$};\n\\draw[arrowTrans] (sEnFr.north west)    --  (satEnFr.south west);\n\\draw[arrowTrans] (sEnFr.north east)    --  (satEnFr.south east);\n\\path (sEnFr.north) -- node (JEnFr) {$\\Omega_{e,f}$} (satEnFr.south);\n\n\\node[sentSkip]  (sFr)   [right = of sEnFr] {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{chat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{est}}}}}}}}}_{\\operatorname{{}}} }$ \\;\\textunderscore\\; ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sur}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{f}}} }$};\n\n\\node[wordSkip]  (satFr) [above = of sFr]   {${ \\vec{\\vphantom{l}\\operatorname{{assis}}}_{\\operatorname{{f}}} }$};\n\\draw[arrowSkip] (sFr.north west) --  (satFr.south west);\n\\draw[arrowSkip] (sFr.north east) --  (satFr.south east);\n\\path (sFr.north) -- node (JFr) {$J_f$} (satFr.south);\n\n\\path (satFrEn.base) -- node (b) {} (satEnFr.base);\n\\node[sum]  (sum) [above = of b]   {$\\sum$};\n\\draw[->] (satEn.north) .. controls +(up:5mm) ..  (sum.west);\n\\draw[->] (satFrEn.north) .. controls +(up:2mm) ..  (sum.south west);\n\\draw[->] (satEnFr.north) .. controls +(up:2mm) ..  (sum.south east);\n\\draw[->] (satFr.north) .. controls +(up:5mm) ..  (sum.east);\n\n\\end{tikzpicture}\n\\end{adjustbox}\n\n\\caption{\n\\fontsize{8}{4} The four partial objectives contributing to the alignment of English and French: a Skip-gram objective per language ($J_e$ and $J_f$) over a window surrounding a target word (blue) and two Trans-gram objectives ($\\Omega_{e,f}$ and $\\Omega_{f,e}$) over the whole sentence aligned with the sentence from which the target word is extracted (red).\n\n}\n\\label{trans_gram}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\nOur method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA \\cite{gouws2014bilbowa}, we use Skip-gram as a mono-lingual loss.\nAssuming we are trying to learn aligned word vectors for languages $e$ (e.g. English) and $f$ (e.g. French), we note $J_e$ and $J_f$ the two mono-lingual losses.\n\nIn BilBOWA, the cross-lingual loss function is a distance between bag-of-words representations of two aligned sentences.\nBut as \\cite{levy2014neural} showed that the Skip-gram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skip-gram than BilBOWA.\n\n\n\n\n\n\n\n\nTherefore, we introduce a new task, Trans-gram, similar to Skip-gram.\nEach English sentence $s_e$ in our aligned corpus $A_{e, f}$ is aligned with a French sentence $s_f$.\nIn Skip-gram, the context picked for a target word $w_e$ in a sentence $s_e$ is the set of words $c_e$ appearing in the window centered around $w_e$: $s_e[w_e-l:w_e+l]$.\nIn Trans-gram, the context picked for a target word $w_e$ in a sentence $s_e$ will be all the words $c_f$ appearing in $s_f$.\nThe loss can thus be written as:\n\n", "itemtype": "equation", "pos": 5554, "prevtext": "\n\\maketitle\n\\begin{abstract}\nWe introduce \\emph{Trans-gram}, a simple\nand computationally-efficient method to simultaneously learn and\nalign word-embeddings for a variety of languages, using only\nmonolingual data and a smaller set of sentence-aligned data.\nWe use our new method to compute aligned word-embeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language.\nWe also achieve state of the art results on standard cross-lingual text classification and word translation tasks.\n\\end{abstract}\n\n\\section{Introduction}\n\nWord-embeddings are a representation of words with fixed-sized vectors.\nIt is a distributed representation \\cite{hinton1984distributed} in the\nsense that there is not necessarily a one-to-one correspondence\nbetween vector dimensions and linguistic properties.\nThe linguistic properties are distributed along the dimensions of the space.\n\nA popular method to compute word-embeddings is the Skip-gram model \\cite{mikolov2013efficient}. This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. \nThis allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral authors came up with different methods to align word-embeddings across two languages \\cite{klementiev2012inducing,mikolov2013exploiting,lauly2014autoencoder,gouws2014bilbowa}. \n\n\nIn this article, we introduce a new method called Trans-gram, which learns word embeddings aligned across many languages, in a simple and efficient fashion, using only sentence alignments rather than word alignments.\nWe compare our method with previous approaches on a cross-lingual document classification task and on a word translation task and obtain state of the art results on these tasks.\n\nAdditionally, word-embeddings for twenty-one languages are learned simultaneously - to our knowledge - for the first time, in less than two and a half hours.\nFurthermore, we illustrate some interesting properties that are captured such as cross-lingual analogies, e.g \n${ \\vec{\\vphantom{l}\\operatorname{{rey}}}_{\\operatorname{{es}}} } - { \\vec{\\vphantom{l}\\operatorname{{Mann}}}_{\\operatorname{{de}}} } + { \\vec{\\vphantom{l}\\operatorname{{femme}}}_{\\operatorname{{fr}}} } \\approx { \\vec{\\vphantom{l}\\operatorname{{regina}}}_{\\operatorname{{it}}} }$\nwhich can be used for disambiguation.\n\n\\section{Review of Previous Work}\n\nA number of methods have been explored to train and align bilingual word-embeddings.\nThese methods pursue two objectives: \nfirst, similar representations (i.e. spatially close) must be assigned to similar words (i.e. ``semantically close'') within each language - this is the \\textbf{mono-lingual objective};\nsecond, similar representations must be assigned to similar words across languages - this is the \\textbf{cross-lingual objective}. \n\nThe simplest approach consists in separating the mono-lingual optimization task from the cross-lingual optimization task.\nThis is for example the case in  \\cite{mikolov2013exploiting}.\nThe idea is to separately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages.\nThis method was further extended by \\cite{faruqui2014improving}.\nEven though those algorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists.\nMoreover, they require word alignments which are a rare and expensive resource. \n\nAnother approach consists in focusing entirely on the cross-lingual objective.\nThis was explored in \\cite{hermann2013multilingual,lauly2014autoencoder} where every couple of aligned sentences is transformed into two fixed-size vectors.\nThen, the model minimizes the Euclidean distance between both vectors.\nThis idea allows processing corpus aligned at sentence-level rather than word-level.\nHowever, it does not leverage the abundance of existing mono-lingual corpora .\n\nA popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simultaneously.\nThis is mostly done by minimizing the sum of mono-lingual loss functions for each language and the cross-lingual loss function.\n\\cite{klementiev2012inducing} proved this approach to be useful by obtaining state-of-the-art results on several tasks.\n\\cite{gouws2014bilbowa} extends their work with a more computationally-efficient implementation.\n\n\n\\section{From Skip-Gram to Trans-Gram}\n\n\\subsection{Skip-gram}\n\nWe briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skip-gram allows to train word embeddings for a language using mono-lingual data.\nThis method uses a dual representation for words.\nEach word $w$ has two embeddings: a target vector, $\\vec{w}$ ($\\in \\mathbb{R}^{D}$), and a context vector, ${\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{w}}}}}}}$ ($\\in \\mathbb{R}^{D}$).\nThe algorithm tries to estimate the probability of a word $w$ to appear in the context of a word $c$.\nMore precisely we are learning the embeddings $\\vec{w}$, ${\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{c}}}}}}}$ so that:\n$\\sigma(\\vec{w} \\cdot {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{c}}}}}}}) = P(w|c)$ where $\\sigma$ is the sigmoid function.\n\nA simplified version of the loss function minimized by Skip-gram is the following:\n\n", "index": 1, "text": "\\begin{equation}\n    J = \\displaystyle \n        \\sum_{s \\in C}{\n            \\sum_{w \\in s}{\n                \\sum_{c \\in s[w-l:w+l]}{\n                    - \\log \\sigma (\\vec{w} \\cdot {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{c}}}}}}})\n                }\n            }\n        }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"J=\\displaystyle\\sum_{s\\in C}{\\sum_{w\\in s}{\\sum_{c\\in s[w-l:w+l]}{-\\log\\sigma(%&#10;\\vec{w}\\cdot{\\reflectbox{$\\vec{\\reflectbox{${c}$}}$}})}}}\" display=\"block\"><mrow><mi>J</mi><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mi>C</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>w</mi><mo>\u2208</mo><mi>s</mi></mrow></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>c</mi><mo>\u2208</mo><mi>s</mi><mrow><mo stretchy=\"false\">[</mo><mi>w</mi><mo>-</mo><mi>l</mi><mo>:</mo><mi>w</mi><mo>+</mo><mi>l</mi><mo stretchy=\"false\">]</mo></mrow></mrow></munder></mrow></mrow><mo>-</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>\u22c5</mo><mpadded depth=\"2.0pt\" height=\"7.0pt\" width=\"7.5pt\"><mover accent=\"true\"><mpadded depth=\"2.0pt\" height=\"7.0pt\" width=\"7.5pt\"><mi>c</mi></mpadded><mo stretchy=\"false\">\u2192</mo></mover></mpadded></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02502.tex", "nexttext": "\n\nThis loss isn't symmetric with respect to the languages. We, therefore, use two cross-lingual objectives: $\\Omega_{e,f}$ aligning $e$'s target vectors and $f$'s context vectors and $\\Omega_{f,e}$ aligning $f$'s target vectors and $e$'s context vectors.\nBy comparison BilBOWA only aligns $e$'s target vectors and $f$'s target vectors.\nThe figure \\ref{trans_gram} illustrates the four objectives.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that we make the assumption that the meaning of a word is uniformly distributed in the whole sentence.\nThis assumption, although a naive one, gave us in practice excellent results.\nAlso our method uses only sentence-aligned corpus and not word-aligned corpus which are rarer.\n\nTo add a third language $i$ (e.g. Italian), we just have to add 3 new objectives ($J_i$, $\\Omega_{e,i}$ and $\\Omega_{i,e}$) to the global loss. If available we could also add $\\Omega_{f,i}$ or $\\Omega_{i,f}$ but in our case we only used corpora aligned with English.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Implementation}\n\nIn our experiments, we used the Europarl \\cite{europarlv7} aligned corpora.\nEuroparl-v7 has two peculiarities: firstly, the corpora are aligned at sentence-level;\nsecondly each pair of languages contains English as\none of its members: for instance, there is no French/Italian pair.\nIn other words, English is used as a pivot language.\nNo bi-lingual lexicons nor other bi-lingual datasets aligned at the\nword level were used.\n\nUsing only the Europarl-v7 texts as both monolingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We also computed 300 dimensions vectors using the Wikipedia extracts provided by \\cite{al2013polyglot} as monolingual data for each language. The training time was 21 hours.\n\n\n\n\n\n\\section{Experiments}\n\n\\subsection{Reuters Cross-lingual Document Classification}\nWe used a subset of the English and German sections of the Reuters RCV1/RCV2 corpora  \\cite{reuters2004} (10000 documents each), as in \\cite{klementiev2012inducing}, and we replicated the experimental setting.\nIn the English dataset, there are four topics: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We used these topics as our labels and we only selected documents labeled with a single topic.\nWe trained our classifier on the articles of one language, where each document was represented using an IDF weighted sum of the vectors of its words, we then tested it on the articles of the other language.\nThe classifier used was an averaged perceptron, and we used the implementation from \\cite{klementiev2012inducing}\\footnote{Thanks to S. Gouws for providing this implementation}.\nThe word vectors were computed on the Europarl-v7 parallel corpus with size 40 like other methods. For this task only the target vectors where used.\n\nWe report the percentage precision obtained with our method, in comparison with other methods, in Table \\ref{results_classification}. The table also include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel corpus and the Wikipedia as mono-lingual corpus.\nThe previous state of the art results were detained \\cite{gouws2014bilbowa} with BilBOWA and \\cite{lauly2014autoencoder} with their Bilingual Auto-encoder model.\nThis model learns word embeddings during a translation task that uses an encoder-decoder approach.\nWe also report the scores from Klementiev et al. who introduced the task and the BiCVM model scores from \\cite{hermann2013multilingual}.\n\nThe results show an overall significant improvement over the other methods, with the added advantage of being computationally efficient.\n\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{l|c|c|c}\n\\hline\n\\textbf{Method}  &  \\textbf{En $\\rightarrow$ De} &  \\textbf{De $\\rightarrow$ En} &  \\textbf{Speed-up in training time} \\\\\n\\hline\nKlementiev et al. & 77.6\\% & 71.1\\% & $\\times 1$ \\\\\nBilingual Auto-encoder & \\textbf{91.8\\%} & 72.8\\% & $\\times 3$ \\\\\nBiCVM & 83.7\\% & 71.4\\% & $\\times 320$ \\\\\nBilBOWA             & 86,5\\% & 75\\% & $\\times 800$  \\\\\n\\hline\n\\textbf{Trans-gram} & 87,8\\% & \\textbf{78,7\\%} & \\textbf{$\\times 600$}\\\\\n\\textbf{Trans-gram (size 300 vectors EP+WIKI)} & 91,1\\% & \\textbf{78,4\\%}\\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison of Trans-gram with various methods for Reuters English/German classification}\n\\label{results_classification}\n\n\\begin{tabular}{l||c|c||c|c}\n\\multicolumn{5}{c}{\\vspace{0.1cm}}\\\\\n\\hline\n\\textbf{Method}  &  \n\\textbf{En $\\rightarrow$ Es P@1} &  \\textbf{En $\\rightarrow$ Es P@5} & \n\\textbf{Es $\\rightarrow$ En P@1} & \\textbf{Es $\\rightarrow$ En P@5} \\\\ \n\\hline\nEdit distance       & 13\\% & 18\\% & 24\\% & 27\\% \\\\\nBing                & 55\\% &      & 71\\% &      \\\\\nTranslation Matrix  & 33\\% & 35\\% & 51\\% & 52\\% \\\\\nBilBOWA             & 39\\% & 44\\% & \\textbf{51\\%} & 55\\% \\\\\n\\hline\n\\textbf{Trans-gram} & \\textbf{45\\%} & \\textbf{61\\%} & 47\\% & \\textbf{62\\%} \\\\\n\\hline\n\\end{tabular}\n\\caption{Results on the translation task}\n\\label{results_tr}\n\\end{table*}\n\n\\subsection{P@k Word Translation}\nNext we evaluated our method on a word translation task, introduced in \\cite{mikolov2013exploiting} and used in \\cite{gouws2014bilbowa}. The words were extracted from the publicly available WMT11\\footnote{http://www.statmt.org/wmt11/} corpus. The experiments were done for two sets of translation: English to Spanish and Spanish to English.\n\\cite{mikolov2013exploiting} extracted the top $6K$ most frequent words and translated them with Google Translate.\nThey used the top $5K$ pairs to train a translation matrix, and evaluated their method on the remaining $1K$.\nAs our English and Spanish vectors are already aligned we don't need the $5K$ training pairs and use only the $1K$ test pairs.\n\nThe reported score, the translation precision $P@k$, is the fraction of test-pairs where the target translation (Google Translate) is one of the $k$ translations proposed by our model. \nFor a given English word, $w$, our model takes its target vectors $\\vec{w}$ and proposes the $k$ closest Spanish word using the co-similarity of their vectors to $\\vec{w}$.\nWe compare ourselves to the ``translation matrix'' method and to the BilBowa aligned vectors.\nWe also report the scores obtained by a trivial algorithm that uses edit-distance to determine the closest translation and by the Bing Translator service.\n\n\n\n\n\n\n\n\\section{Interesting properties}\n\n\\subsection{Cross-lingual disambiguation}\n\nWe now present the task of cross-lingual disambiguation as an example of possible uses of aligned multilingual vectors. The goal of this task is  to find a suitable representation of each sense of a given polysemous word. The idea of our method is to look for a language in which the undesired senses are represented by unambiguous words and then to perform some arithmetic operation. \n\n\n\nLet's illustrate the process with a concrete example: consider the French word ``train'', ${{\\operatorname{{train}}}_{\\operatorname{{fr}}}}$. The three closest Polish words to ${ \\vec{\\vphantom{l}\\operatorname{{train}}}_{\\operatorname{{fr}}} }$ translate in English into ``now'', ``a train'' and ``when''. This seems a poor matching. In fact, ${{\\operatorname{{train}}}_{\\operatorname{{fr}}}}$ is polysemous.\nIt can name a line of railroad cars, but it is also used to form progressive tenses. The French ``Il est \\emph{en train de} manger'' translates into ``he is eat\\emph{ing}'', or in Italian ``\\emph{sta} mangiando''.\n\nAs the Italian word ``sta'' is used to form progressive tenses, it's a good candidate to disambiguate ${{\\operatorname{{train}}}_{\\operatorname{{fr}}}}$.\nLet's introduce the vector $\\vec{v} = { \\vec{\\vphantom{l}\\operatorname{{train}}}_{\\operatorname{{fr}}} } - { \\vec{\\vphantom{l}\\operatorname{{sta}}}_{\\operatorname{{it}}} }$.\nNow the three polish words closest to $\\vec{v}$ translate in English into ``a train'', ``a train'' and ``railroad''.\nTherefore $\\vec{v}$ is a better representation for the railroad sense of ${{\\operatorname{{train}}}_{\\operatorname{{fr}}}}$.\n\n\\subsection{Transfer of linguistic features}\n\nAnother interesting property of the vectors generated by Trans-gram is the transfer of linguistic features through a pivot language that does not possess these features.\n\n\n\nLet's illustrate this by focusing on Latin languages, which possess some features that English does not, like rich conjugations.\nFor example, in French and Italian the infinitives of ``eat'' are ${{\\operatorname{{manger}}}_{\\operatorname{{fr}}}}$ and ${{\\operatorname{{mangiare}}}_{\\operatorname{{it}}}}$, and the first plural persons are ${{\\operatorname{{mangeons}}}_{\\operatorname{{fr}}}}$ and ${{\\operatorname{{mangiamo}}}_{\\operatorname{{it}}}}$. Actually in our models we observe the following alignments:\n${ \\vec{\\vphantom{l}\\operatorname{{manger}}}_{\\operatorname{{fr}}} }   \\approx { \\vec{\\vphantom{l}\\operatorname{{mangiare}}}_{\\operatorname{{it}}} }$ and\n${ \\vec{\\vphantom{l}\\operatorname{{mangeons}}}_{\\operatorname{{fr}}} } \\approx { \\vec{\\vphantom{l}\\operatorname{{mangiamo}}}_{\\operatorname{{it}}} }$.\n\nIt is thus remarkable to see that features not present in English match in languages aligned through English as the only pivot language.\nWe also found similar transfers for the genders of adjectives and are currently studying other similar properties captured by Trans-gram.\n\n\\section{Conclusion}\nIn this paper we provided the following contributions:\nTrans-gram, a new method to compute cross-lingual word-embeddings in a single word space;\nstate of the art results on cross-lingual NLP tasks;\na sketch of a cross-lingual calculus to help disambiguate polysemous words;\nthe exhibition of linguistic features transfers through a pivot-language not possessing those features.\n\nWe are still exploring promising properties of the generated vectors and their applications in other NLP tasks (Sentiment Analysis, NER...).\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{acl}\n\\bibliography{main}\n\n\n\n", "itemtype": "equation", "pos": 13147, "prevtext": "\nwhere $C$ is the set of sentences constituting the training corpus, and $s[w-l:w+l]$ is a word window on the sentence $s$ centered around $w$.\nFor the sake of simplicity this equation does not include the ``negative-sampling'' term, see \\cite{mikolov2013efficient} for more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip-gram can be seen as a materialization of the distributional hypothesis \\cite{harris1968mathematical}: ``Words used in similar contexts have similar meanings''.\nWe will now see how to extend this idea to cross-lingual contexts.\n\n\n\n\n\\subsection{Trans-gram}\nIn this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages.\n\n\\begin{figure*}[t]\n\\begin{adjustbox}{max width=\\textwidth}\n\\begin{tikzpicture}\n[\nsum/.style={ellipse, draw=blue!80, fill=white!0, very thick, minimum width=1cm},\nwordSkip/.style={rectangle, draw=forestgreen!80, fill=white!0, very thick, minimum width=1cm},\nsentSkip/.style={rectangle, draw=blue!80, fill=white!0, very thick,  minimum width=1cm, xshift=-6mm},\narrowSkip/.style={draw=blue},\nwordTrans/.style={rectangle, draw=forestgreen!80, fill=white!0, very thick, minimum width=1cm},\nsentTrans/.style={draw=red!80, fill=white!0, very thick,  minimum width=1cm, xshift=-6mm},\narrowTrans/.style={draw=red}\n]\n\n\n\\node[sentSkip]  (sEn)    {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{cat}}}}}}}}}_{\\operatorname{{}}} }$ \\;\\textunderscore\\; ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{on}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{e}}} }$};\n\n\n\\node[wordSkip]  (satEn)  [above = of sEn]    {${ \\vec{\\vphantom{l}\\operatorname{{sits}}}_{\\operatorname{{e}}} }$};\n\\draw[arrowSkip] (sEn.north west)    --  (satEn.south west);\n\\draw[arrowSkip] (sEn.north east)    --  (satEn.south east);\n\\path (sEn.north) -- node (JEn) {$J_{e}$} (satEn.south);\n\n\n\n\\node[sentTrans]  (sFrEn)   [right = of sEn]   {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{cat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sits}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{on}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{the}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{mat}}}}}}}}}_{\\operatorname{{e}}} }$};\n\n\\node[wordTrans]  (satFrEn) [above = of sFrEn]      {${ \\vec{\\vphantom{l}\\operatorname{{assis}}}_{\\operatorname{{f}}} }$};\n\\draw[arrowTrans] (sFrEn.north west)    --  (satFrEn.south west);\n\\draw[arrowTrans] (sFrEn.north east)    --  (satFrEn.south east);\n\\path (sFrEn.north) -- node (JFrEn) {$\\Omega_{f,e}$} (satFrEn.south);\n\n\\node[sentTrans]  (sEnFr)   [right = of sFrEn]   {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{chat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{est}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{assis}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sur}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{tapis}}}}}}}}}_{\\operatorname{{f}}} }$};\n\n\\node[wordTrans]  (satEnFr) [above = of sEnFr]   {${ \\vec{\\vphantom{l}\\operatorname{{sits}}}_{\\operatorname{{e}}} }$};\n\\draw[arrowTrans] (sEnFr.north west)    --  (satEnFr.south west);\n\\draw[arrowTrans] (sEnFr.north east)    --  (satEnFr.south east);\n\\path (sEnFr.north) -- node (JEnFr) {$\\Omega_{e,f}$} (satEnFr.south);\n\n\\node[sentSkip]  (sFr)   [right = of sEnFr] {${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{chat}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{est}}}}}}}}}_{\\operatorname{{}}} }$ \\;\\textunderscore\\; ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{sur}}}}}}}}}_{\\operatorname{{}}} }$ ${ {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{\\vphantom{l}\\operatorname{{le}}}}}}}}}_{\\operatorname{{f}}} }$};\n\n\\node[wordSkip]  (satFr) [above = of sFr]   {${ \\vec{\\vphantom{l}\\operatorname{{assis}}}_{\\operatorname{{f}}} }$};\n\\draw[arrowSkip] (sFr.north west) --  (satFr.south west);\n\\draw[arrowSkip] (sFr.north east) --  (satFr.south east);\n\\path (sFr.north) -- node (JFr) {$J_f$} (satFr.south);\n\n\\path (satFrEn.base) -- node (b) {} (satEnFr.base);\n\\node[sum]  (sum) [above = of b]   {$\\sum$};\n\\draw[->] (satEn.north) .. controls +(up:5mm) ..  (sum.west);\n\\draw[->] (satFrEn.north) .. controls +(up:2mm) ..  (sum.south west);\n\\draw[->] (satEnFr.north) .. controls +(up:2mm) ..  (sum.south east);\n\\draw[->] (satFr.north) .. controls +(up:5mm) ..  (sum.east);\n\n\\end{tikzpicture}\n\\end{adjustbox}\n\n\\caption{\n\\fontsize{8}{4} The four partial objectives contributing to the alignment of English and French: a Skip-gram objective per language ($J_e$ and $J_f$) over a window surrounding a target word (blue) and two Trans-gram objectives ($\\Omega_{e,f}$ and $\\Omega_{f,e}$) over the whole sentence aligned with the sentence from which the target word is extracted (red).\n\n}\n\\label{trans_gram}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\nOur method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA \\cite{gouws2014bilbowa}, we use Skip-gram as a mono-lingual loss.\nAssuming we are trying to learn aligned word vectors for languages $e$ (e.g. English) and $f$ (e.g. French), we note $J_e$ and $J_f$ the two mono-lingual losses.\n\nIn BilBOWA, the cross-lingual loss function is a distance between bag-of-words representations of two aligned sentences.\nBut as \\cite{levy2014neural} showed that the Skip-gram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skip-gram than BilBOWA.\n\n\n\n\n\n\n\n\nTherefore, we introduce a new task, Trans-gram, similar to Skip-gram.\nEach English sentence $s_e$ in our aligned corpus $A_{e, f}$ is aligned with a French sentence $s_f$.\nIn Skip-gram, the context picked for a target word $w_e$ in a sentence $s_e$ is the set of words $c_e$ appearing in the window centered around $w_e$: $s_e[w_e-l:w_e+l]$.\nIn Trans-gram, the context picked for a target word $w_e$ in a sentence $s_e$ will be all the words $c_f$ appearing in $s_f$.\nThe loss can thus be written as:\n\n", "index": 3, "text": "\\begin{equation}\n    \\Omega_{e,f} = \\displaystyle \n        \\sum_{ (s_e, s_f) \\in A_{e, f} }{\n            \\sum_{w_e \\in s_e}{\n                \\sum_{c_f \\in s_f}{\n                    - \\log \\sigma (\\vec{w_e} \\cdot {\\reflectbox{\\ensuremath{\\vec{\\reflectbox{\\ensuremath{{c_f}}}}}}})\n                }\n            }\n        }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\Omega_{e,f}=\\displaystyle\\sum_{(s_{e},s_{f})\\in A_{e,f}}{\\sum_{w_{e}\\in s_{e}%&#10;}{\\sum_{c_{f}\\in s_{f}}{-\\log\\sigma(\\vec{w_{e}}\\cdot{\\reflectbox{$\\vec{%&#10;\\reflectbox{${c_{f}}$}}$}})}}}\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mrow><mi>e</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>e</mi></msub><mo>,</mo><msub><mi>s</mi><mi>f</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>A</mi><mrow><mi>e</mi><mo>,</mo><mi>f</mi></mrow></msub></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>e</mi></msub><mo>\u2208</mo><msub><mi>s</mi><mi>e</mi></msub></mrow></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>c</mi><mi>f</mi></msub><mo>\u2208</mo><msub><mi>s</mi><mi>f</mi></msub></mrow></munder></mrow></mrow><mo>-</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><msub><mi>w</mi><mi>e</mi></msub><mo stretchy=\"false\">\u2192</mo></mover><mo>\u22c5</mo><mpadded depth=\"2.0pt\" height=\"7.0pt\" width=\"7.5pt\"><mover accent=\"true\"><mpadded depth=\"3.1pt\" height=\"7.0pt\" width=\"11.7pt\"><msub><mi>c</mi><mi>f</mi></msub></mpadded><mo stretchy=\"false\">\u2192</mo></mover></mpadded></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]