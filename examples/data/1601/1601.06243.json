[{"file": "1601.06243.tex", "nexttext": "\nwhere the tensor $\\mathcal{I}$ donates the observed LR image, $D$ is a downsampling operator, $S$ is a blurring operator, $\\mathcal{X}$ is the HR image to be reconstructed and $\\mathbf{e}$ represents the observation noise. Since this is an ill-posed problem, some regularization terms of $\\mathcal{X}$ based on prior knowledge, denoted by $\\mathfrak{R}(\\mathcal{X})$, can be introduced to regularize the solution to refine the solution space: $\\hat{\\mathcal{X}}=\\arg\\min_{\\mathcal{X}}\\{\\|DS\\mathcal{X}-\\mathcal{I}\\|^2+\\lambda\\mathfrak{R}(\\mathcal{X})\\}$, where $\\lambda$ is a scalar parameter to make a trade-off between the fidelity term and the regularization term.\n\\subsection{3D TV regularization}\nTotal variation (TV) \\cite{TV1} is often used to preserve local spatial consistency in image recovery and suppress image noise. Considering the fact that an HR hyperspecctral image to be reconstructed is treated as a tensor, and its local spatial-and-spectral consistency, or say, smoothness ccharacterized by 3D total variation, which is expressed as \n$TV(\\mathcal{X})=\\sum_{ijk} |x_{ijk}- x_{ij,k-1}| + |x_{ijk} - x_{i,j-1,k}| + |x_{ijk} - x_{i-1,j,k}|,$ where $x_{ijk}$ is the $(i,j,k)$-th entry of tensor $\\mathcal{X}$.\n\n\\subsection{Low-rank regularization}\nThe spatial-and-spectral correlation of a hyperspectral image implies that\neach unfolded matrix, if a hyperspectral image represented as a tensor, is low rank.\nHence, following the work~\\cite{ten2}, low-rank property of a three-order tensor can be measured by a weighted sum of three ranks:\n\n", "itemtype": "equation", "pos": 3861, "prevtext": "\n\n\n\\maketitle\n\n\\begin{abstract}\n\n\nIn this paper, we propose a novel approach to hyperspectral image super-resolution by modeling the global spatial-and-spectral correlation and local smoothness properties over\nhyperspectral images. Specifically, we utilize the tensor nuclear norm and tensor folded-concave penalty functions to describe the global spatial-and-spectral correlation hidden in hyperspectral images, and 3D total variation (TV) to characterize the local spatial-and-spectral smoothness across all  hyperspectral bands. Then, we develop an efficient algorithm for solving the resulting optimization problem by combing the local linear approximation (LLA) strategy and alternative direction method of multipliers (ADMM).\nExperimental results on one hyperspectral image dataset illustrate the merits of the proposed approach .\n\\end{abstract}\n\n\\begin{keywords}\nHyperspectral images, Super-resolution reconstruction, nuclear norm, Folded-concave penalty, 3D total variation.\n\\end{keywords}\n\n\\section{Introduction}\n\n\n\n\n\nHyperspectral images (HSIs) are recordings of reflectance of light of some real world scenes or objects including hundreds of spectral bands ranging from ultraviolet to infrared wavelength~\\cite{Intr1,Intr2}.\nThe abundant spectral bands of HSIs provide fine spectral feature differences between various materials of interest and enable many computer vision tasks more successfully achievable.\nHowever, due to the constraints of imaging hardware, signal to noise ratio (SNR) and time constraints, the acquired hyperspectral images unfortunately have low spatial resolution, which cannot give any active help for high precision processing requirements in many fields including mineralogy, manufacturing, medical diagnostics, and surveillance. \nHence, the task of reconstructing a hyperspectral image of high resolution (HR) from an observed low resolution (LR) hyperspectral image or sequence is a valuable research issue.\n\n\n\n\n\n\nThe problem of  hyperspectral image super-resolution (HSSR) can be solved by designing various traditional signal processing techniques, including the works \\cite{SR3, SR4, TV1}. \n\n\n\n\n\n\n\n\nIn the recent years, applying prior information of HR auxiliary images into the process of HSSR has been becoming more and more popular~\\cite{spatial,Review}. However, such HR images are not always easy to get due to the limitations of remote sensing system. Therefore, super-resolution of single HSI cube has atracted increased interest in many practical scenarios .\n\nIn this paper, we consider a single HSI cube as a tensor with three modes (width, height, and band)\nand then discover the hidden spatial-and-spectral structures using tensor modelling for enhancing its spatial resolution.\nSpecifically, the spectral bands of a HSI have strong correlations and each band  if considered as a matrix has relatively strong correlation; this spatial-and-spectral correlation can be modelled\nby a low-rank tensor penalty. Additionally, for each voxel, from the spatial viewpoint its intensity seems to\nalmost equal to those in its neighbourhood, and the same from the spectral viewpoint;\nwe then describe this local spatial-and-spectral smoothness property using 3D total variation.\nAs such, the HSSR task resorts to solving an optimization problem, which can be efficiently solved by combing LLA strategy and ADMM. \n\n\\section{HSSR via total variation and low-rank regularizations}\nIn this section, we first introduce the observation model. Then, we utilize 3D TV to describe \nlocal smoothness of a hyperspectral image, and adopt a tensor folded-concave penalty  to characterize global correlation of a hyperspectral image.\nFinally, a novel regularization model is derived for the HSSR task.\n\\subsection{Observation model}\nThe low spatial resolution hyperspectral image can be generated by the following observation model:\n\n", "index": 1, "text": "$$\\mathcal{I}=DS\\mathcal{X}+\\mathbf{e},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{I}=DS\\mathcal{X}+\\mathbf{e},\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi><mo>=</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>+</mo><mi>\ud835\udc1e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere $\\alpha_i\\geqslant 0$ and satisfies $\\sum_{i}^{3}\\alpha_i=1$.\nSince the optimization problem with rank constraint (1) is intractable, and matrix nuclear norm is exploited as a tight convex\nsurrogate of the matrix rank~\\cite{FC2} , one can replace the rank function \\eqref{ten_rank} with  the following tensor nuclear norm:\n\n", "itemtype": "equation", "pos": 5458, "prevtext": "\nwhere the tensor $\\mathcal{I}$ donates the observed LR image, $D$ is a downsampling operator, $S$ is a blurring operator, $\\mathcal{X}$ is the HR image to be reconstructed and $\\mathbf{e}$ represents the observation noise. Since this is an ill-posed problem, some regularization terms of $\\mathcal{X}$ based on prior knowledge, denoted by $\\mathfrak{R}(\\mathcal{X})$, can be introduced to regularize the solution to refine the solution space: $\\hat{\\mathcal{X}}=\\arg\\min_{\\mathcal{X}}\\{\\|DS\\mathcal{X}-\\mathcal{I}\\|^2+\\lambda\\mathfrak{R}(\\mathcal{X})\\}$, where $\\lambda$ is a scalar parameter to make a trade-off between the fidelity term and the regularization term.\n\\subsection{3D TV regularization}\nTotal variation (TV) \\cite{TV1} is often used to preserve local spatial consistency in image recovery and suppress image noise. Considering the fact that an HR hyperspecctral image to be reconstructed is treated as a tensor, and its local spatial-and-spectral consistency, or say, smoothness ccharacterized by 3D total variation, which is expressed as \n$TV(\\mathcal{X})=\\sum_{ijk} |x_{ijk}- x_{ij,k-1}| + |x_{ijk} - x_{i,j-1,k}| + |x_{ijk} - x_{i-1,j,k}|,$ where $x_{ijk}$ is the $(i,j,k)$-th entry of tensor $\\mathcal{X}$.\n\n\\subsection{Low-rank regularization}\nThe spatial-and-spectral correlation of a hyperspectral image implies that\neach unfolded matrix, if a hyperspectral image represented as a tensor, is low rank.\nHence, following the work~\\cite{ten2}, low-rank property of a three-order tensor can be measured by a weighted sum of three ranks:\n\n", "index": 3, "text": "\\begin{equation}\n\\label{ten_rank}\n\\text{Rank}(\\mathcal{X})=\\sum_{i}^{3}\\alpha_i \\text{Rank}(\\mathcal{X}_{(i)}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\text{Rank}(\\mathcal{X})=\\sum_{i}^{3}\\alpha_{i}\\text{Rank}(\\mathcal{X}_{(i)}),\" display=\"block\"><mrow><mrow><mrow><mtext>Rank</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi><mn>3</mn></munderover><mrow><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>\u2062</mo><mtext>Rank</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere $\\|\\mathbf{Z}\\|_{*}:=\\sum_{k=1}^{\\min(m,n)}\\sigma_k(\\mathbf{Z})$ denotes the nuclear norm of matrix $\\mathbf{Z}$ of size $m\\times n$, and $\\mathcal{X}_{(i)}$ is the $i$-th unfolded matrix of tensor $\\mathcal{X}$~\\cite{ten2}. \n\n\nAlthough the convex nuclear norm  \\eqref{ten_norm} performs well in various tensor recovery problems, studies such as \\cite{FC2} have shown that the nuclear norm over-penalizes large singular values, and thus leads to the modeling bias in low rank structure estimation. Folded-concave penalty~\\cite{FC3} can be used to remedy this modeling bias, as shown in some works \\cite{FC3, FC}. Thus, we shall utilize one of the folded penalties, the minmax concave plus (MCP) penalty, of the form:\n\n", "itemtype": "equation", "pos": 5914, "prevtext": "\nwhere $\\alpha_i\\geqslant 0$ and satisfies $\\sum_{i}^{3}\\alpha_i=1$.\nSince the optimization problem with rank constraint (1) is intractable, and matrix nuclear norm is exploited as a tight convex\nsurrogate of the matrix rank~\\cite{FC2} , one can replace the rank function \\eqref{ten_rank} with  the following tensor nuclear norm:\n\n", "index": 5, "text": "\\begin{equation}\n\\label{ten_norm}\n\\|\\mathcal{X}\\|_{*}=\\sum_{i}^{3}\\alpha_i \\|\\mathcal{X}_{(i)}\\|_{*},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\|\\mathcal{X}\\|_{*}=\\sum_{i}^{3}\\alpha_{i}\\|\\mathcal{X}_{(i)}\\|_{*},\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi><mn>3</mn></munderover><mrow><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nFollowing \\cite{FC}, the folded-concave norm of a matrix $X$ is defined as $\\|X\\|_{P_\\lambda}:=\\sum_{j=1}^{r}P_{\\lambda}\\big(\\sigma_j(X)\\big)$\\footnote{Note that $\\|X\\|_{P_{\\lambda}}$ is nonconvex with respect to $X$.}, where $\\sigma_j(X)$ is the $j$-th singular value of $X$ and $r$ is the rank. As such, the tensor MCP penalty is defined by applying the MCP penalty function to each unfolded matrix $\\mathcal{X}_{(i)}$:\n\n", "itemtype": "equation", "pos": 6754, "prevtext": "\nwhere $\\|\\mathbf{Z}\\|_{*}:=\\sum_{k=1}^{\\min(m,n)}\\sigma_k(\\mathbf{Z})$ denotes the nuclear norm of matrix $\\mathbf{Z}$ of size $m\\times n$, and $\\mathcal{X}_{(i)}$ is the $i$-th unfolded matrix of tensor $\\mathcal{X}$~\\cite{ten2}. \n\n\nAlthough the convex nuclear norm  \\eqref{ten_norm} performs well in various tensor recovery problems, studies such as \\cite{FC2} have shown that the nuclear norm over-penalizes large singular values, and thus leads to the modeling bias in low rank structure estimation. Folded-concave penalty~\\cite{FC3} can be used to remedy this modeling bias, as shown in some works \\cite{FC3, FC}. Thus, we shall utilize one of the folded penalties, the minmax concave plus (MCP) penalty, of the form:\n\n", "index": 7, "text": "\\begin{equation}\nP_{\\lambda}=\n   \\begin{cases}\n   a\\lambda^2/2 &\\mbox{if $|t|\\geqslant a\\lambda$}\\\\\n   \\lambda|t|-\\frac{t^2}{2a} &\\mbox{otherwise}.\n   \\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"P_{\\lambda}=\\begin{cases}a\\lambda^{2}/2&amp;\\mbox{if $|t|\\geqslant a\\lambda$}\\\\&#10;\\lambda|t|-\\frac{t^{2}}{2a}&amp;\\mbox{otherwise}.\\end{cases}\" display=\"block\"><mrow><msub><mi>P</mi><mi>\u03bb</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>a</mi><mo>\u2062</mo><msup><mi>\u03bb</mi><mn>2</mn></msup></mrow><mo>/</mo><mn>2</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2a7e</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>-</mo><mstyle displaystyle=\"false\"><mfrac><msup><mi>t</mi><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><mi>a</mi></mrow></mfrac></mstyle></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\n\\subsection{Proposed model}\n\nBased on the previous discussions, we now derive the following regularization model for the HSSR task:\n\n", "itemtype": "equation", "pos": 7354, "prevtext": "\nFollowing \\cite{FC}, the folded-concave norm of a matrix $X$ is defined as $\\|X\\|_{P_\\lambda}:=\\sum_{j=1}^{r}P_{\\lambda}\\big(\\sigma_j(X)\\big)$\\footnote{Note that $\\|X\\|_{P_{\\lambda}}$ is nonconvex with respect to $X$.}, where $\\sigma_j(X)$ is the $j$-th singular value of $X$ and $r$ is the rank. As such, the tensor MCP penalty is defined by applying the MCP penalty function to each unfolded matrix $\\mathcal{X}_{(i)}$:\n\n", "index": 9, "text": "\\begin{equation}\n\\label{ten_mcp}\n\\|\\mathcal{X}\\|_{P_{\\lambda}}=\\sum_{i}^{N}\\alpha_i\\|\\mathcal{X}_{(i)}\\|_{P_{\\lambda}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\|\\mathcal{X}\\|_{P_{\\lambda}}=\\sum_{i}^{N}\\alpha_{i}\\|\\mathcal{X}_{(i)}\\|_{P_{%&#10;\\lambda}}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>\u2225</mo></mrow><msub><mi>P</mi><mi>\u03bb</mi></msub></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi><mi>N</mi></munderover><mrow><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2225</mo></mrow><msub><mi>P</mi><mi>\u03bb</mi></msub></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere the scalars $\\lambda_1$ and $\\lambda_2$ are regularization parameters, and $\\mathcal{L}(\\mathcal{X}_{(i)})$ is the low-rank measure function (1) or (4) for $\\mathcal{X}$. \n\\section{Optimization Algorithm}\n\\label{sec:pagestyle}\nWe first rewrite (5) as the following equivalent form by introducing $N$ auxiliary variable $\\{\\mathcal{M}_{i}\\}_{i=1}^N$:\n\n", "itemtype": "equation", "pos": 7621, "prevtext": "\n\\subsection{Proposed model}\n\nBased on the previous discussions, we now derive the following regularization model for the HSSR task:\n\n", "index": 11, "text": "\\begin{equation}\n\\label{ten_rec}\n\\min_{\\mathcal{X}}\\|DS\\mathcal{X}-\\mathcal{I}\\|^2_F+\\lambda_1TV(\\mathcal{X})+\\lambda_2\\mathcal{L}(\\mathcal{X}_{(i)}), \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathcal{X}}\\|DS\\mathcal{X}-\\mathcal{I}\\|^{2}_{F}+\\lambda_{1}TV(\\mathcal%&#10;{X})+\\lambda_{2}\\mathcal{L}(\\mathcal{X}_{(i)}),\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>-</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\n\nBased on ADMM~\\cite{ADMM}, the augmented Lagrangian function is written as follows: \n\n", "itemtype": "equation", "pos": 8144, "prevtext": "\nwhere the scalars $\\lambda_1$ and $\\lambda_2$ are regularization parameters, and $\\mathcal{L}(\\mathcal{X}_{(i)})$ is the low-rank measure function (1) or (4) for $\\mathcal{X}$. \n\\section{Optimization Algorithm}\n\\label{sec:pagestyle}\nWe first rewrite (5) as the following equivalent form by introducing $N$ auxiliary variable $\\{\\mathcal{M}_{i}\\}_{i=1}^N$:\n\n", "index": 13, "text": "\\begin{equation}\n\\begin{split}\n  &\\min_{\\mathcal{X},\\{\\mathcal{M}_i\\}_{i=1}^N}~~\\|DS\\mathcal{X}-\\mathcal{I}\\|^2_F+\\lambda_1TV(\\mathcal{X})+\\lambda_2\\mathcal{L}(\\mathcal{M}_i)\\\\\n  &    s.t~~\\mathcal{X}_{(i)}=\\mathcal{M}_{i(i)},i=1,2,...,N\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\min_{\\mathcal{X},\\{\\mathcal{M}_{i}\\}_{i=1}^{N}}~{}%&#10;~{}\\|DS\\mathcal{X}-\\mathcal{I}\\|^{2}_{F}+\\lambda_{1}TV(\\mathcal{X})+\\lambda_{2%&#10;}\\mathcal{L}(\\mathcal{M}_{i})\\\\&#10;&amp;\\displaystyle s.t~{}~{}\\mathcal{X}_{(i)}=\\mathcal{M}_{i(i)},i=1,2,...,N\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+6.6pt\"><munder><mi>min</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>,</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>-</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>s</mi><mo>.</mo><mrow><mrow><mrow><mpadded width=\"+6.6pt\"><mi>t</mi></mpadded><mo>\u2062</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><mo>=</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi></mrow></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere $\\{\\mathcal{Y}_i\\}_{i=1}^N$ are Lagrangian parameters. We shall break (7) into three subproblems and iteratively update each variable through fixing the other ones. Let $k$ denotes the $k$th iteration step:\n\n\\textbf{Subproblem 1: }\n\n", "itemtype": "equation", "pos": 8495, "prevtext": "\n\nBased on ADMM~\\cite{ADMM}, the augmented Lagrangian function is written as follows: \n\n", "index": 15, "text": "\\begin{equation}\n\\begin{split}\n   &L(\\mathcal{X},\\mathcal{Y}_i,\\mathcal{M}_i)=\\|DS\\mathcal{X}-\\mathcal{I}\\|^2_F+\\lambda_1TV(\\mathcal{X})\\\\\n   &+\\sum_{i=1}^{N}\\lambda_2\\mathcal{L}(\\mathcal{M}_{i(i)})+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}_{(i)}+\\frac{\\mathcal{Y}_{i(i)}}{\\rho}\\|^2_F, \n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle L(\\mathcal{X},\\mathcal{Y}_{i},\\mathcal{M}_{i})=\\|%&#10;DS\\mathcal{X}-\\mathcal{I}\\|^{2}_{F}+\\lambda_{1}TV(\\mathcal{X})\\\\&#10;&amp;\\displaystyle+\\sum_{i=1}^{N}\\lambda_{2}\\mathcal{L}(\\mathcal{M}_{i(i)})+\\sum^{%&#10;N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}_{(i)}+\\frac{\\mathcal{Y}%&#10;_{i(i)}}{\\rho}\\|^{2}_{F},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi>L</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>-</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>-</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><mo>+</mo><mfrac><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mi>\u03c1</mi></mfrac></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nThe well-known gradient  method can be easily applied to solve this subproblem.\n\n\\textbf{Subproblem 2: }\n\n", "itemtype": "equation", "pos": 9066, "prevtext": "\nwhere $\\{\\mathcal{Y}_i\\}_{i=1}^N$ are Lagrangian parameters. We shall break (7) into three subproblems and iteratively update each variable through fixing the other ones. Let $k$ denotes the $k$th iteration step:\n\n\\textbf{Subproblem 1: }\n\n", "index": 17, "text": "\\begin{equation}\n\\begin{split}\n    & \\mathcal{X}^{(k+1)}=arg\\min_\\mathcal{X}~~\\|DS\\mathcal{X}-\\mathcal{I}\\|^2_F+\\lambda_1TV(\\mathcal{X}) \\\\\n    &+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}^k-\\mathcal{X}_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^k}{\\rho}\\|^2_F\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathcal{X}^{(k+1)}=arg\\min_{\\mathcal{X}}~{}~{}\\|DS%&#10;\\mathcal{X}-\\mathcal{I}\\|^{2}_{F}+\\lambda_{1}TV(\\mathcal{X})\\\\&#10;&amp;\\displaystyle+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}^{k}-\\mathcal{X%&#10;}_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^{k}}{\\rho}\\|^{2}_{F}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mpadded width=\"+6.6pt\"><munder><mi>min</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>-</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></msubsup><mo>-</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><mo>+</mo><mfrac><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></msubsup><mi>\u03c1</mi></mfrac></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nThe solution of this subproblem depends on the choice of the low rank term $\\mathcal{L}(\\mathcal{X})$. We first consider the case of nuclear norm, i.e., \n\n", "itemtype": "equation", "pos": 9453, "prevtext": "\nThe well-known gradient  method can be easily applied to solve this subproblem.\n\n\\textbf{Subproblem 2: }\n\n", "index": 19, "text": "\\begin{equation}\n\\begin{split}\n    & \\{\\mathcal{M}_i^{(k+1)}\\}_{i=1}^N= arg\\min_{\\{\\mathcal{M}_i\\}_{i=1}^N}\\sum_{i=1}^{N}\\lambda_2\\mathcal{L}(\\mathcal{M}_{i(i)})\\\\\n    &+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^k_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^k}{\\rho}\\|^2_F\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\{\\mathcal{M}_{i}^{(k+1)}\\}_{i=1}^{N}=arg\\min_{\\{%&#10;\\mathcal{M}_{i}\\}_{i=1}^{N}}\\sum_{i=1}^{N}\\lambda_{2}\\mathcal{L}(\\mathcal{M}_{%&#10;i(i)})\\\\&#10;&amp;\\displaystyle+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^{k%&#10;}_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^{k}}{\\rho}\\|^{2}_{F}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><munder><mi>min</mi><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>-</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi></msubsup></mrow><mo>+</mo><mfrac><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></msubsup><mi>\u03c1</mi></mfrac></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nAccording to~\\cite{ten2}, its close-form solution is expressed as\n\n", "itemtype": "equation", "pos": 9913, "prevtext": "\nThe solution of this subproblem depends on the choice of the low rank term $\\mathcal{L}(\\mathcal{X})$. We first consider the case of nuclear norm, i.e., \n\n", "index": 21, "text": "\\begin{equation}\n\\sum_{i=1}^{N}\\lambda_2\\alpha_i\\|\\mathcal{M}_{i(i)}\\|_*+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^k_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^k}{\\rho}\\|^2_F\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{N}\\lambda_{2}\\alpha_{i}\\|\\mathcal{M}_{i(i)}\\|_{*}+\\sum^{N}_{i=1}%&#10;\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^{k}_{(i)}+\\frac{\\mathcal{Y}_{i(%&#10;i)}^{k}}{\\rho}\\|^{2}_{F}\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>-</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi></msubsup></mrow><mo>+</mo><mfrac><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></msubsup><mi>\u03c1</mi></mfrac></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nFor a given matrix $X$, the singular value shrinkage operator $S_{\\tau}(X)$ is defined by $S_{\\tau}(X):=U_XD_{\\tau}(\\Sigma_X)V_X^T$, where $X=U_X\\sigma_XV_X^T$ is the singular value decomposition of $X$ and $[D_{\\tau}(A)]_{ij}=sgn(A_{ij})(|A_{ij}|-\\tau)_+$.\n\nWhile for the MCP case, we adopt the same idea of~\\cite{FC3, FC} to solve the resulting nonconvex problem. More precisely, we use the local linear approximation (LLA) algorithm to transform the MCP penalization problem into a series of weighted nuclear norm penalization problem. Then the resulting optimization problem can be solved as well. More precisely, the \\textbf{subproblem 2} can be written as\n\n", "itemtype": "equation", "pos": 10176, "prevtext": "\nAccording to~\\cite{ten2}, its close-form solution is expressed as\n\n", "index": 23, "text": "\\begin{equation}\n\\mathcal{M}_i=fold_i[S_{\\lambda_2\\alpha_i/\\rho}(\\mathcal{X}_{(i)}^{(k+1)})-\\mathcal{Y}_{i(i)}^{(k)}]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{M}_{i}=fold_{i}[S_{\\lambda_{2}\\alpha_{i}/\\rho}(\\mathcal{X}_{(i)}^{(k+%&#10;1)})-\\mathcal{Y}_{i(i)}^{(k)}]\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><msub><mi>d</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msub><mi>S</mi><mrow><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow><mo>/</mo><mi>\u03c1</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere $Q_{P_\\lambda}(\\sigma(X|X_{(i)}^{k}))$ is the locally linear approximation of $\\|X\\|_{P_\\lambda}$ when $X^{k}$ is given. Then the solution of this optimization problem is $\\mathcal{M}_{i(i)}=S_{\\alpha_i/\\rho,W_i}(\\mathcal{X}_{(i)}-\\frac{\\mathcal{Y}_{i(i)}}{\\rho})$ and the weight matrix $W_i$ is given by $W_i=Diag((\\lambda-(\\sigma(\\mathcal{X}_{(i)})/a))_+)$ for some fixed $a>1$.\n\n\\textbf{Subproblem 3: }\n\n", "itemtype": "equation", "pos": 10971, "prevtext": "\nFor a given matrix $X$, the singular value shrinkage operator $S_{\\tau}(X)$ is defined by $S_{\\tau}(X):=U_XD_{\\tau}(\\Sigma_X)V_X^T$, where $X=U_X\\sigma_XV_X^T$ is the singular value decomposition of $X$ and $[D_{\\tau}(A)]_{ij}=sgn(A_{ij})(|A_{ij}|-\\tau)_+$.\n\nWhile for the MCP case, we adopt the same idea of~\\cite{FC3, FC} to solve the resulting nonconvex problem. More precisely, we use the local linear approximation (LLA) algorithm to transform the MCP penalization problem into a series of weighted nuclear norm penalization problem. Then the resulting optimization problem can be solved as well. More precisely, the \\textbf{subproblem 2} can be written as\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{split}\n    & \\{\\mathcal{M}_i^{(k+1)}\\}_{i=1}^N= arg\\min_{\\{\\mathcal{M}_i\\}_{i=1}^N}\\sum_{i=1}^{N}\\lambda_2\\alpha_iQ_{P_\\lambda}(\\sigma(\\mathcal{M}_{i(i)})|\\sigma(\\mathcal{X}^{k}))\\\\\n    &+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^k_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^k}{\\rho}\\|^2_F, \n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\{\\mathcal{M}_{i}^{(k+1)}\\}_{i=1}^{N}=arg\\min_{\\{%&#10;\\mathcal{M}_{i}\\}_{i=1}^{N}}\\sum_{i=1}^{N}\\lambda_{2}\\alpha_{i}Q_{P_{\\lambda}}%&#10;(\\sigma(\\mathcal{M}_{i(i)})|\\sigma(\\mathcal{X}^{k}))\\\\&#10;&amp;\\displaystyle+\\sum^{N}_{i=1}\\frac{\\rho}{2}\\|\\mathcal{M}_{i(i)}-\\mathcal{X}^{k%&#10;}_{(i)}+\\frac{\\mathcal{Y}_{i(i)}^{k}}{\\rho}\\|^{2}_{F},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>min</mi><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></munder><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>\u03bb</mi><mn>2</mn></msub><msub><mi>\u03b1</mi><mi>i</mi></msub><msub><mi>Q</mi><msub><mi>P</mi><mi>\u03bb</mi></msub></msub><mrow><mo stretchy=\"false\">(</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mi>k</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>-</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi></msubsup></mrow><mo>+</mo><mfrac><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></msubsup><mi>\u03c1</mi></mfrac></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06243.tex", "nexttext": "\nwhere $\\rho$ is a parameter associated with convergence rate with fixed value, i.e., 1.05. \n\\section{Experimental study}\n\nWe now test the proposed method on a HSI dataset. The reference image without noisy bands is a $256\\times256\\times146$ hyperspectral image acquired over Moffett field, CA, in 1994 (AVIRIS). The blurring kernel is Gaussian kernel and the LR image is generated by downsampling the original HR image with a factor of 2, i.e.,  the LR image is of size $128\\times128\\times146$.\n\nWe compare our method with three other popular methods, including\nthe bicubic method described in \\cite{Bi}, NARM proposed in \\cite{Na} and Sparse Representation method by Yang et al.~\\cite{yang}. The reconstructed results of the test HSI for a specific band 100 are shown in Fig.1.\n\\begin{figure*}[htb]\n\\centering\n\\includegraphics[width=500pt]{GTr.pdf}\n\\caption{Visual comparison of different super-resolution reconstruction methods.}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne can observe  that the Bicubic\ninterpolation blurs the image and the high-frequency spatial details are lost. The other methods provide better reconstruction visual effects. \nAdditionally, our proposed method shown in Fig.1(e) and (f) outperforms the other ones. It is also interesting to note that the folded-concave penalization, i.e.,  the MCP,  outperforms other competing methods.\n\nTo further evaluate the quality of the proposed reconstruction strategy, several image quality measures have been employed, including peak-signal to noise ratio (PSNR), spectral angle mapper (SAM), and relative dimensionless global error in synthesis (ERGAS). It is known that the larger the PSNR, the better the image quality is; the lower the SAM and ERGAS value are, the smaller spectral distortion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[h]\n\n\n\\caption{Quantitative Measures for Different SRR Methods}\n\\label{table:para}\n\\centering\n\\begin{tabular}{|c||c||c||c|}\n\\hline\nQuantitative Measures              &PSNR     &SAM      &ERGAS   \\\\\n\\hline\nBicubic                          &33.0236  &0.1248   &126.0507\\\\\n\\hline\nNRAM                               &33.1197  &0.1297   &124.3686  \\\\\n\\hline\nSparse Representation              &35.7409  &0.1651   &117.4637\\\\\n\\hline\nNuclear Norm Penalty               &\\textit{36.9567 } &\\textit{0.0843}   &\\textit{95.0166} \\\\\n\\hline\nMCP Penalty             &\\textbf{37.8732}  &\\textbf{0.0720}   &\\textbf{88.5562}\\\\\n\\hline\n\\end{tabular}\n\\end{table}\nIt can be seen from Table 1 that the proposed method with nuclear norm and folded-concave penalties outperforms other competing ones. Again, the MCP penalization provides best reconstruction results, which illustrates the advantage of folded-concave penalty over convex nuclear norm penalty. \n\n\\section{Conclusion}\nIn this paper, we propose a novel method for hyperspectral  image super-resolution  by tensor\nstructural modelling. The proposed method considers the global correlation and local smoothness of a hyperspectral image by combining low-rank and total variation regularizations imposed on a tensor. Experimental results reveal that the proposed methods outperform other compared methods, and especially folded concave penalization is superior over the nuclear norm penalization for the HSSR task.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEbib}\n\\bibliography{igarss}\n\n\n", "itemtype": "equation", "pos": 11733, "prevtext": "\nwhere $Q_{P_\\lambda}(\\sigma(X|X_{(i)}^{k}))$ is the locally linear approximation of $\\|X\\|_{P_\\lambda}$ when $X^{k}$ is given. Then the solution of this optimization problem is $\\mathcal{M}_{i(i)}=S_{\\alpha_i/\\rho,W_i}(\\mathcal{X}_{(i)}-\\frac{\\mathcal{Y}_{i(i)}}{\\rho})$ and the weight matrix $W_i$ is given by $W_i=Diag((\\lambda-(\\sigma(\\mathcal{X}_{(i)})/a))_+)$ for some fixed $a>1$.\n\n\\textbf{Subproblem 3: }\n\n", "index": 27, "text": "\\begin{equation}\n  \\mathcal{Y}_i^{(k+1)}=\\mathcal{Y}_i^{(k)}+\\rho(\\mathcal{M}_i^{(k+1)}-\\mathcal{X}^{(k+1)}), \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{Y}_{i}^{(k+1)}=\\mathcal{Y}_{i}^{(k)}+\\rho(\\mathcal{M}_{i}^{(k+1)}-%&#10;\\mathcal{X}^{(k+1)}),\" display=\"block\"><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]