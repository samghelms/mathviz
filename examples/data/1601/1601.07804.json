[{"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{x}\\in\\mathbb{R}^{N}\\ (N=\\prod_{i}N_{i})$ denotes the\nvectorized signal, $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{M\\times N}\\ (M<N)$\nis the sensing matrix, $\\mathbf{y}\\in\\mathbb{R}^{M}$ represents the\nmeasurement vector and $\\mathbf{e}\\in\\mathbb{R}^{M}$ is a noise term.\nThe vectorized signal is assumed to be sparse in some sparsifying\nbasis $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{N\\times\\hat{N}}\\ (N\\leq\\hat{N})$,\ni.e., \n\n", "itemtype": "equation", "pos": 11543, "prevtext": "\n\n\\title{Joint Sensing Matrix and Sparsifying Dictionary Optimization for\nTensor Compressive Sensing}\n\n\n\\author{Xin Ding, \\textit{Student Member, IEEE}, Wei Chen, \\textit{Member,\nIEEE}, and Ian J. Wassell \\thanks{Xin Ding and Ian J. Wassell are with the Computer Lab, University\nof Cambridge, UK (e-mail: xd225, ijw24@cam.ac.uk).}\\thanks{Wei Chen is with the State Key Laboratory of Rail Traffic Control\nand Safety, Beijing Jiaotong University, China, and also with the\nComputer Lab, University of Cambridge, UK (e-mail: wc253@cam.ac.uk).}}\n\\maketitle\n\\begin{abstract}\nTensor Compressive Sensing (TCS) is a multidimensional framework of\nCompressive Sensing (CS), and it is advantageous in terms of reducing\nthe amount of storage, easing hardware implementations and preserving\nmultidimensional structures of signals in comparison to a conventional\nCS system. In a TCS system, instead of using a random sensing matrix\nand a predefined dictionary, the average-case performance can be further\nimproved by employing an optimized multidimensional sensing matrix\nand a learned multilinear sparsifying dictionary. In this paper, we\npropose a joint optimization approach of the sensing matrix and dictionary\nfor a TCS system. For the sensing matrix design in TCS, an extended\nseparable approach with a closed form solution and a novel iterative\nnon-separable method are proposed when the multilinear dictionary\nis fixed. In addition, a multidimensional dictionary learning method\nthat takes advantages of the multidimensional structure is derived,\nand the influence of sensing matrices is taken into account in the\nlearning process. A joint optimization is achieved via alternately\niterating the optimization of the sensing matrix and dictionary. Numerical\nexperiments using both synthetic data and real images demonstrate\nthe superiority of the proposed approaches.\\end{abstract}\n\n\\begin{IEEEkeywords}\nMultidimensional system, compressive sensing, tensor compressive sensing,\ndictionary learning, sensing matrix optimization.\n\\end{IEEEkeywords}\n\n\n\\section{Introduction}\n\nThe traditional signal acquisition-and-compression paradigm removes\nthe signal redundancy and preserves the essential contents of signals\nto achieve savings on storage and transmission, where the minimum\nsampling ratio is restricted by the Shannon-Nyquist Theorem at the\nsignal sampling stage. The wasteful process of sensing-then-compressing\nis replaced by directly acquiring the compressed version of signals\nin Compressive Sensing (CS) \\cite{Candes2006,Donoho2006,Candes2008},\na new sampling paradigm that leverages the fact that most signals\nhave sparse representations (i.e., there are only a few non-zero coefficients)\nin some suitable basis. Successful reconstruction of such signals\nis guaranteed for a sufficient number of randomly taken samples that\nare far fewer in number than that required in the Shannon-Nyquist\nTheorem. Therefore CS is very attractive for applications such as\nmedical imaging and wireless sensor networks where data acquisition\nis expensive \\cite{Lustig2007,Chen2011(2)}.\n\nAchieving successful CS reconstruction has been characterized by a\nnumber of properties, e.g., the Restricted Isometry Property (RIP)\n\\cite{Candes2006}, the mutual coherence \\cite{donoho2006stable}\nand the null space property \\cite{Donoho2006}. These properties have\nbeen used to provide sufficient conditions on sensing matrices and\nto quantify the worst-case reconstruction performance \\cite{Candes2008a,donoho2006stable,Donoho2006}.\nRandom matrices such as Gaussian or Bernoulli matrices have been shown\nto fulfill these conditions, and hence are widely used as the sensing\nmatrix in CS applications. In view of the fact that the mainstream\nview in the signal processing community considers the average-case\nperformance rather than the worst-case performance, later on, it is\nshown that the average-case reconstruction performance can be further\nenhanced by optimizing the sensing matrix according to the aforementioned\nconditions, e.g., \\cite{Elad2007,Xu2010,Chen2013,Li2013,Cleju2014}.\nOn the other hand, instead of using a fixed signal-sparsifying basis,\ne.g., a Discrete Wavelet Transform (DWT), one can further enhance\nCS performance by employing a basis which is learned from a training\ndata set to abstract the basic atoms that compose the signal ensemble.\nThe process of learning such a basis is referred to as ``sparsifying\ndictionary learning'' and it has been widely investigated in the\nliterature \\cite{Engan2000,Aharon2006,Tovsic2011,Sahoo2013,Dai2012}.\nIn addition, by further exploiting the interaction between the sensing\nmatrix and the sparsifying dictionary, joint optimization of the two\nhas also been considered in \\cite{Duarte2009,Chen2013dictionary,Bai2015}.\n\nHowever, in the process of sensing and reconstruction, the conventional\nCS framework considers vectorized signals, and multidimensional signals\nare mapped in a vector format in a CS system. At the sensing node,\nsuch a vectorization requires the hardware to be capable of simultaneously\nmultiplexing along all data dimensions, which is hard to achieve especially\nwhen one of the dimensions is along a timeline. Secondly, a real-world\nvectorized signal requires an enormous sensing matrix that has as\nmany columns as the number of signal elements. Consequently such an\napproach imposes large demands on the storage and processing power.\nIn addition, the vectorization also results in a loss of structure\nalong the various dimensions, the presence of which is beneficial\nfor developing efficient reconstruction algorithms. For these reasons,\napplying conventional CS to applications that involve multidimensional\nsignals is challenging.\n\nExtending CS to multidimensional signals has attracted growing interests\nover the past few years. Most of the related work in the literature\nfocuses on CS for 2D signals (i.e., matrices), e.g., matrix completion\n\\cite{Recht2010,Candes2012:matrixCom}, and the reconstruction of\nsparse and low rank matrices \\cite{Golbabaee2012,Chartrand2012,Otazo2015}.\nIn \\cite{Duarte2012}, Kronecker product matrices are proposed for\nuse in CS systems, which makes it possible to partition the sensing\nprocess along signal dimensions and paves the way to developing CS\nfor tensors, i.e., signals with two or more dimensions. Tensor CS\n(TCS) has been studied in \\cite{Sidiropoulos2012,Friendland2014,Caiafa2013,Caiafa:2013},\nwhere the main focus is on algorithm development for reconstruction.\nTo the best of our knowledge, there is no prior work concerning the\nenhancement of TCS via optimizing the sensing matrices at various\ndimensions in a tensor. In addition, although dictionary learning\ntechniques have been considered for tensors \\cite{Seibert2014,Roemer2014,Peng2014},\nit is still not clear how to conduct tensor dictionary learning to\nincorporate the influence of sensing matrices in TCS.\n\nIn this paper, we investigate joint sensing matrix design and dictionary\nlearning for TCS systems. Unlike the optimization for a conventional\nCS system where a single sensing matrix and a sparsifying basis for\nvectorized signals are obtained, we produce a multiplicity of them\nfunctioning along various tensor dimensions, thereby maintaining the\nadvantages of TCS. The contributions of this work are as follows: \n\\begin{itemize}\n\\item We are the first to consider the optimization of a multidimensional\nsensing matrix and dictionary for a TCS system and a joint optimization\nof the two is designed, which also includes particular cases of optimizing\nthe sensing matrix for a given multilinear dictionary and learning\nthe dictionary for a given multidimensional sensing matrix.\n\\item We propose a separable approach for sensing matrix design by extending\nthe existing work for conventional CS. In this approach, the optimization\nis proved to be separable, i.e., the sensing matrix along each dimension\ncan be independently optimized, and the approach has closed form solution.\n\\item We put forth a non-separable method for sensing matrix design using\na combination of the state-of-art measures for sensing matrix optimization.\nThis approach leads to the best reconstruction performance in our\ncomparison, but it is iterative and hence needs more computing power\nto implement.\n\\item We propose a multidimensional dictionary learning approach that couples\nthe optimization of the multidimensional sensing matrix. This approach\nextends KSVD \\cite{Aharon2006} and coupled-KSVD \\cite{Duarte2009}\nto take full advantages of the multidimensional structure in tensors\nwith a reduced number of iterations required for the update of dictionary\natoms. \n\\end{itemize}\nThe proposed approaches are demonstrated to enhance the performance\nof existing TCS systems via the use of extensive simulations using\nboth synthetic data and real images. \n\nThe remainder of this paper is organized as follows. Section \\ref{sec:CS and TCS}\nformulates CS and TCS, and introduces the related theory. Section\n\\ref{sec:PhiDesign} reviews the sensing matrix design approaches\nfor CS and presents the proposed methods for TCS sensing matrix design.\nIn Section \\ref{sec:PsiDesign}, the related dictionary learning techniques\nare reviewed, followed by the elaboration of the proposed multidimensional\ndictionary learning approach and the joint optimization algorithm\nis presented. Experimental results are given in Section \\ref{sec:simulations}\nand Section \\ref{sec:conclusions} concludes the paper.\n\n\n\\subsection{Multilinear Algebra and Notations}\n\nBoldface lower-case letters, boldface upper-case letters and non-boldface\nletters denote vectors, matrices and scalars, respectively. A mode-$n$\ntensor is an $n$-dimensional array $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times...\\times N_{n}}$.\nThe mode-$i$ vectors of a tensor are determined by fixing every index\nexcept the one in the mode $i$ and the slices of a tensor are its\ntwo dimensional sections determined by fixing all but two indices.\nBy arranging all the mode-$i$ vectors as columns of a matrix, the\nmode-$i$ unfolding matrix $\\mathbf{X}_{(i)}\\in\\mathbb{R}^{N_{i}\\times N_{1}...N_{i-1}N_{i+1}...N_{n}}$\nis obtained. The mode-$k$ tensor by matrix product is defined as:\n$\\underline{\\mathbf{Z}}=\\underline{\\mathbf{X}}\\times_{k}\\mathbf{A},$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{J\\times N_{k}}$, $\\underline{\\mathbf{Z}}\\in\\mathbb{R}^{N_{1}\\times...\\times N_{k-1}\\times J\\times N_{k+1}\\times...\\times N_{n}}$\nand it is calculated by: $\\underline{\\mathbf{Z}}=fold_{i}(\\mathbf{A}\\mathbf{X}_{(i)})$,\nwhere $fold_{i}(\\cdot)$ means folding up a matrix along mode $i$\nto a tensor. The matrix Kronecker product and vector outer product\nare denoted by $\\mathbf{A}\\otimes\\mathbf{B}$ and $\\mathbf{a}\\circ\\mathbf{b}$,\nrespectively. The $l_{p}$ norm of a vector is defined as: $||\\mathbf{x}||_{p}=(\\sum_{\\mathit{i}=1}^{\\mathit{n}}|\\mathit{x_{i}}|^{\\mathit{p}})^{\\frac{1}{\\mathit{p}}}$.\nFor vectors, matrices and tensors, the $l_{0}$ norm is given by the\nnumber of nonzero entries. $\\mathbf{I}_{N}$ denotes the $N\\times N$\nidentity matrix. The operator $(\\cdot)^{-1}$, $(\\cdot)^{T}$ and\n$tr(\\cdot)$ represent matrix inverse, matrix transpose and the trace\nof a matrix, respectively. The number of elements for a vector, matrix\nor tensor is denoted by $len(\\cdot)$.\n\n\n\\section{Compressive Sensing (CS) and Tensor Compressive Sensing (TCS)}\n\n\\label{sec:CS and TCS}\n\n\n\\subsection{Sensing Model}\n\nConsider a multidimensional signal $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times...\\times N_{n}}$.\nConventional CS takes measurements from its vectorized version via:\n\n", "index": 1, "text": "\\begin{equation}\n\\mathbf{y}=\\boldsymbol{\\Phi}\\mathbf{x}+\\mathbf{e},\\label{eq:CS sensing}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}=\\boldsymbol{\\Phi}\\mathbf{x}+\\mathbf{e},\" display=\"block\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>+</mo><mi>\ud835\udc1e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{s}\\in\\mathbb{R}^{\\hat{N}}$ is the sparse representation\nof $\\mathbf{x}$ and it has only $K\\ (K\\ll\\hat{N})$ non-zero coefficients.\nThus the sensing model can be rewritten as:\n\n", "itemtype": "equation", "pos": 12070, "prevtext": "\nwhere $\\mathbf{x}\\in\\mathbb{R}^{N}\\ (N=\\prod_{i}N_{i})$ denotes the\nvectorized signal, $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{M\\times N}\\ (M<N)$\nis the sensing matrix, $\\mathbf{y}\\in\\mathbb{R}^{M}$ represents the\nmeasurement vector and $\\mathbf{e}\\in\\mathbb{R}^{M}$ is a noise term.\nThe vectorized signal is assumed to be sparse in some sparsifying\nbasis $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{N\\times\\hat{N}}\\ (N\\leq\\hat{N})$,\ni.e., \n\n", "index": 3, "text": "\\begin{equation}\n\\mathbf{x}=\\boldsymbol{\\Psi}\\mathbf{s},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{x}=\\boldsymbol{\\Psi}\\mathbf{s},\" display=\"block\"><mrow><mrow><mi>\ud835\udc31</mi><mo>=</mo><mrow><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\ud835\udc2c</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{A}=\\boldsymbol{\\Phi\\Psi}\\in\\mathbb{R}^{M\\times\\hat{N}}$\nis the equivalent sensing matrix.\n\nEven though CS has been successfully applied to practical sensing\nsystems \\cite{Duarte2008,Marcia2009,Majidzadeh2010}, the sensing\nmodel has a few drawbacks when it comes to tensor signals. First of\nall, the multidimensional structure presented in the original signal\n$\\underline{\\mathbf{X}}$ is omitted due to the vectorization, which\nloses information that can lead to efficient reconstruction algorithms.\nBesides, as stated by (\\ref{eq:CS sensing}), the sensing system is\nrequired to operate along all dimensions of the signal simultaneously,\nwhich is difficult to achieve in practice. Furthermore, the size of\n$\\boldsymbol{\\Phi}$ associated with the vectorized signal becomes\ntoo large to be practical for applications involving multidimensional\nsignals. \n\nTCS tackles these problems by utilizing separable sensing operators\nalong tensor modes and its sensing model is: \n\n", "itemtype": "equation", "pos": 12331, "prevtext": "\nwhere $\\mathbf{s}\\in\\mathbb{R}^{\\hat{N}}$ is the sparse representation\nof $\\mathbf{x}$ and it has only $K\\ (K\\ll\\hat{N})$ non-zero coefficients.\nThus the sensing model can be rewritten as:\n\n", "index": 5, "text": "\\begin{equation}\n\\mathbf{y}=\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\mathbf{s}+\\mathbf{e}=\\mathbf{A}\\mathbf{s}+\\mathbf{e},\\label{eq:CS eqSensing}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}=\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\mathbf{s}+\\mathbf{e}=\\mathbf{A}%&#10;\\mathbf{s}+\\mathbf{e},\" display=\"block\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\ud835\udc2c</mi></mrow><mo>+</mo><mi>\ud835\udc1e</mi></mrow><mo>=</mo><mrow><mi>\ud835\udc00\ud835\udc2c</mi><mo>+</mo><mi>\ud835\udc1e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\underline{\\mathbf{Y}}\\in\\mathbb{R}^{M_{1}\\times...M_{n}}$\nrepresents the measurement, $\\underline{\\mathbf{E}}\\in\\mathbb{R}^{M_{1}\\times...M_{n}}$\ndenotes the noise term, $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,...,n)$\nare sensing matrices and $M_{i}<N_{i}$. The multidimensional signal\nis assumed to be sparse in a separable sparsifying basis $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{N_{i}\\times\\hat{N}_{i}}\\ (i=1,...,n)$,\ni.e.,\n\n", "itemtype": "equation", "pos": 13467, "prevtext": "\nwhere $\\mathbf{A}=\\boldsymbol{\\Phi\\Psi}\\in\\mathbb{R}^{M\\times\\hat{N}}$\nis the equivalent sensing matrix.\n\nEven though CS has been successfully applied to practical sensing\nsystems \\cite{Duarte2008,Marcia2009,Majidzadeh2010}, the sensing\nmodel has a few drawbacks when it comes to tensor signals. First of\nall, the multidimensional structure presented in the original signal\n$\\underline{\\mathbf{X}}$ is omitted due to the vectorization, which\nloses information that can lead to efficient reconstruction algorithms.\nBesides, as stated by (\\ref{eq:CS sensing}), the sensing system is\nrequired to operate along all dimensions of the signal simultaneously,\nwhich is difficult to achieve in practice. Furthermore, the size of\n$\\boldsymbol{\\Phi}$ associated with the vectorized signal becomes\ntoo large to be practical for applications involving multidimensional\nsignals. \n\nTCS tackles these problems by utilizing separable sensing operators\nalong tensor modes and its sensing model is: \n\n", "index": 7, "text": "\\begin{equation}\n\\underline{\\mathbf{Y}}=\\underline{\\mathbf{X}}\\times_{1}\\boldsymbol{\\Phi}_{1}\\times_{2}\\boldsymbol{\\Phi}_{2}...\\times_{n}\\boldsymbol{\\Phi}_{n}+\\underline{\\mathbf{E}},\\label{eq:TCS sensing}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\underline{\\mathbf{Y}}=\\underline{\\mathbf{X}}\\times_{1}\\boldsymbol{\\Phi}_{1}%&#10;\\times_{2}\\boldsymbol{\\Phi}_{2}...\\times_{n}\\boldsymbol{\\Phi}_{n}+\\underline{%&#10;\\mathbf{E}},\" display=\"block\"><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mo>=</mo><mrow><mrow><mrow><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><msub><mo>\u00d7</mo><mi>n</mi></msub><msub><mi>\ud835\udebd</mi><mi>n</mi></msub></mrow><mo>+</mo><munder accentunder=\"true\"><mi>\ud835\udc04</mi><mo>\u00af</mo></munder></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{\\hat{N}_{1}\\times...\\hat{N}_{n}}$\nis the sparse representation that has only $K\\ (K\\ll\\prod_{i}\\hat{N}_{i})$\nnon-zero coefficients. The equivalent sensing model can then be written\nas: \n\n", "itemtype": "equation", "pos": 14142, "prevtext": "\nwhere $\\underline{\\mathbf{Y}}\\in\\mathbb{R}^{M_{1}\\times...M_{n}}$\nrepresents the measurement, $\\underline{\\mathbf{E}}\\in\\mathbb{R}^{M_{1}\\times...M_{n}}$\ndenotes the noise term, $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,...,n)$\nare sensing matrices and $M_{i}<N_{i}$. The multidimensional signal\nis assumed to be sparse in a separable sparsifying basis $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{N_{i}\\times\\hat{N}_{i}}\\ (i=1,...,n)$,\ni.e.,\n\n", "index": 9, "text": "\\begin{equation}\n\\underline{\\mathbf{X}}=\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}...\\times_{n}\\boldsymbol{\\Psi}_{n},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\underline{\\mathbf{X}}=\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}%&#10;\\times_{2}\\boldsymbol{\\Psi}_{2}...\\times_{n}\\boldsymbol{\\Psi}_{n},\" display=\"block\"><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><mo>=</mo><mrow><mrow><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udebf</mi><mn>2</mn></msub></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><msub><mo>\u00d7</mo><mi>n</mi></msub><msub><mi>\ud835\udebf</mi><mi>n</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{A}_{i}=\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\ (i=1,...,n)$\nare the equivalent sensing matrices.\n\nUsing the TCS sensing model in (\\ref{eq:TCS sensing}), the sensing\nprocedure in (\\ref{eq:CS sensing}) is partitioned into a few processes\nhaving smaller sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,...,n)$\nand yet it maintains the multidimensional structure of the original\nsignal $\\underline{\\mathbf{X}}$. It is also useful to mention that\nthe TCS model in (\\ref{eq:TCS eqSensing}) is equivalent to:\n\n", "itemtype": "equation", "pos": 14547, "prevtext": "\nwhere $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{\\hat{N}_{1}\\times...\\hat{N}_{n}}$\nis the sparse representation that has only $K\\ (K\\ll\\prod_{i}\\hat{N}_{i})$\nnon-zero coefficients. The equivalent sensing model can then be written\nas: \n\n", "index": 11, "text": "\\begin{equation}\n\\underline{\\mathbf{Y}}=\\underline{\\mathbf{S}}\\times_{1}\\mathbf{A}_{1}\\times_{2}\\mathbf{A}_{2}...\\times_{n}\\mathbf{A}_{n},\\label{eq:TCS eqSensing}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\underline{\\mathbf{Y}}=\\underline{\\mathbf{S}}\\times_{1}\\mathbf{A}_{1}\\times_{2%&#10;}\\mathbf{A}_{2}...\\times_{n}\\mathbf{A}_{n},\" display=\"block\"><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mo>=</mo><mrow><mrow><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udc00</mi><mn>1</mn></msub></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udc00</mi><mn>2</mn></msub></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><msub><mo>\u00d7</mo><mi>n</mi></msub><msub><mi>\ud835\udc00</mi><mi>n</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nas derived in \\cite{Caiafa2013}. By denoting $\\overline{\\mathbf{A}}=\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{1}$,\nit becomes a conventional CS model akin to (\\ref{eq:CS eqSensing}),\nexcept that the sensing matrix in (\\ref{eq:CSvsTCS}) has a multilinear\nstructure.\n\n\n\\subsection{Signal Reconstruction }\n\n\\label{sub:SigReconModel}\n\nIn conventional CS, the problem of reconstructing $\\mathbf{s}$ from\nthe measurement vector $\\mathbf{y}$ captured using (\\ref{eq:CS eqSensing})\nis modeled as a $l_{0}$ minimization problem as follows: \n\n", "itemtype": "equation", "pos": 15277, "prevtext": "\nwhere $\\mathbf{A}_{i}=\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\ (i=1,...,n)$\nare the equivalent sensing matrices.\n\nUsing the TCS sensing model in (\\ref{eq:TCS sensing}), the sensing\nprocedure in (\\ref{eq:CS sensing}) is partitioned into a few processes\nhaving smaller sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,...,n)$\nand yet it maintains the multidimensional structure of the original\nsignal $\\underline{\\mathbf{X}}$. It is also useful to mention that\nthe TCS model in (\\ref{eq:TCS eqSensing}) is equivalent to:\n\n", "index": 13, "text": "\\begin{equation}\n\\mathbf{y}=(\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{1})\\mathbf{s},\\label{eq:CSvsTCS}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}=(\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{%&#10;1})\\mathbf{s},\" display=\"block\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc00</mi><mi>n</mi></msub><mo>\u2297</mo><msub><mi>\ud835\udc00</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2297</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2297</mo><msub><mi>\ud835\udc00</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc2c</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\varepsilon$ is a tolerance parameter. Many algorithms have\nbeen developed to solve this problem, including Basis Pursuit (BP)\n\\cite{Candes2005,Candes2006,Candes2008,Donoho2006}, i.e., conducting\nconvex optimization by relaxing the $l_{0}$ norm in (\\ref{eq:CS_l0_min})\nas the $l_{1}$ norm, and greedy algorithms such as Orthogonal Matching\nPursuit (OMP) \\cite{Tropp2007} and Iterative Hard Thresholding (IHT)\n\\cite{Blumensath2009}. The reconstruction performance of the $l_{1}$\nminimization approach has been studied in \\cite{Candes2005,Candes2008a},\nwhere the well known Restricted Isometry Property (RIP) was introduced\nto provide a sufficient condition for successful signal recovery.\n\n\\textit{\\textcolor{black}{Definition 1: }}\\textcolor{black}{A matrix\n$\\mathbf{A}$ satisfies the RIP of order $K$ with a the Restricted\nIsometry Constant (RIC) ${\\color{black}\\delta}_{{\\color{black}K}}$\nbeing the smallest number such that\n\n", "itemtype": "equation", "pos": 15976, "prevtext": "\nas derived in \\cite{Caiafa2013}. By denoting $\\overline{\\mathbf{A}}=\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{1}$,\nit becomes a conventional CS model akin to (\\ref{eq:CS eqSensing}),\nexcept that the sensing matrix in (\\ref{eq:CSvsTCS}) has a multilinear\nstructure.\n\n\n\\subsection{Signal Reconstruction }\n\n\\label{sub:SigReconModel}\n\nIn conventional CS, the problem of reconstructing $\\mathbf{s}$ from\nthe measurement vector $\\mathbf{y}$ captured using (\\ref{eq:CS eqSensing})\nis modeled as a $l_{0}$ minimization problem as follows: \n\n", "index": 15, "text": "\\begin{equation}\n\\underset{\\mathbf{s}}{min}\\ ||\\mathbf{s}||_{0},\\ s.t.\\ ||\\mathbf{y}-\\mathbf{As}||\\leq\\varepsilon,\\label{eq:CS_l0_min}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\mathbf{s}}{min}\\ ||\\mathbf{s}||_{0},\\ s.t.\\ ||\\mathbf{y}-\\mathbf{As%&#10;}||\\leq\\varepsilon,\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>\ud835\udc2c</mo></munder></mpadded><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc2c</mi><mo fence=\"true\">||</mo></mrow><mn>0</mn></msub></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc32</mi><mo>-</mo><mi>\ud835\udc00\ud835\udc2c</mi></mrow><mo fence=\"true\">||</mo></mrow><mo>\u2264</mo><mi>\u03b5</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nholds for all} \\textcolor{black}{$\\mathbf{s}$ with $||\\mathbf{s}||_{0}\\leq K$.\n}\\hfill{}$\\blacksquare$\n\n\\textit{\\textcolor{black}{Theorem 1:}}\\textcolor{black}{{} Assume that\n${\\color{black}\\delta_{2K}<\\sqrt{2}-1}$ and $||\\mathbf{e}||_{2}\\leq\\varepsilon$.\nThen the solution ${\\color{black}\\mathbf{\\hat{s}}}$ to (\\ref{eq:CS_l0_min})\nobeys\n\n", "itemtype": "equation", "pos": 17060, "prevtext": "\nwhere $\\varepsilon$ is a tolerance parameter. Many algorithms have\nbeen developed to solve this problem, including Basis Pursuit (BP)\n\\cite{Candes2005,Candes2006,Candes2008,Donoho2006}, i.e., conducting\nconvex optimization by relaxing the $l_{0}$ norm in (\\ref{eq:CS_l0_min})\nas the $l_{1}$ norm, and greedy algorithms such as Orthogonal Matching\nPursuit (OMP) \\cite{Tropp2007} and Iterative Hard Thresholding (IHT)\n\\cite{Blumensath2009}. The reconstruction performance of the $l_{1}$\nminimization approach has been studied in \\cite{Candes2005,Candes2008a},\nwhere the well known Restricted Isometry Property (RIP) was introduced\nto provide a sufficient condition for successful signal recovery.\n\n\\textit{\\textcolor{black}{Definition 1: }}\\textcolor{black}{A matrix\n$\\mathbf{A}$ satisfies the RIP of order $K$ with a the Restricted\nIsometry Constant (RIC) ${\\color{black}\\delta}_{{\\color{black}K}}$\nbeing the smallest number such that\n\n", "index": 17, "text": "\\begin{equation}\n(1-\\delta_{K})||\\mathbf{s||_{\\mathrm{2}}^{\\mathrm{2}}}\\leq||\\mathbf{As||_{\\mathrm{2}}^{\\mathrm{2}}}\\leq(1+\\delta_{K})||\\mathbf{s||_{\\mathrm{2}}^{\\mathrm{2}}}\\label{eq:RIP}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"(1-\\delta_{K})||\\mathbf{s||_{\\mathrm{2}}^{\\mathrm{2}}}\\leq||\\mathbf{As||_{%&#10;\\mathrm{2}}^{\\mathrm{2}}}\\leq(1+\\delta_{K})||\\mathbf{s||_{\\mathrm{2}}^{\\mathrm%&#10;{2}}}\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03b4</mi><mi>K</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc2c</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>\u2264</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc00\ud835\udc2c</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><msub><mi>\u03b4</mi><mi>K</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc2c</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere ${\\color{black}\\mathit{C_{0}=\\frac{\\mathrm{2+(2\\sqrt{2}-2)\\delta_{2\\mathit{K}}}}{\\mathrm{1-(\\sqrt{2}+1)\\delta_{2\\mathit{K}}}}}}$,\n${\\color{black}\\mathit{C_{1}=\\frac{4\\sqrt{\\mathrm{1+\\delta_{2\\mathit{K}}}}}{\\mathrm{1-(\\sqrt{2}+1)\\delta_{2\\mathit{K}}}}}}$,\n${\\color{black}\\delta_{2\\mathit{K}}}$ is the RIC of matrix ${\\color{black}\\mathbf{A}}$,\n$\\mathbf{s}_{K}$ is an approximation of ${\\color{black}\\mathbf{s}}$\nwith all but the ${\\color{black}\\mathit{K}}$ largest entries set\nto zero.}\\hfill{}$\\blacksquare$\n\nThe previous theorem states that for the noiseless case, any sparse\nsignal with fewer than $K$ non-zero coefficients can be exactly recovered\nif the RIC of the equivalent sensing matrix satisfies \\textcolor{black}{${\\color{black}\\delta_{2K}<\\sqrt{2}-1}$;\nwhile for the noisy case and the not exactly sparse case, the reconstructed\nsignal is still a good approximation of the original signal under\nthe same condition. The theoretical guarantees of successful reconstruction\nfor the greedy approaches have also been investigated in \\cite{Tropp2007,Blumensath2009}. }\n\n\\textcolor{black}{The RIP essentially measures the quality of the\nequivalent sensing matrix $\\mathbf{A}$, which closely relates to\nthe design of $\\boldsymbol{\\Phi}$ and $\\boldsymbol{\\Psi}$. However,\nsince the RIP is not tractable, another measure is often used for\nCS projection design, i.e., the mutual coherence of $\\mathbf{A}$\n\\cite{donoho2006stable} and it is defined by: \n\n", "itemtype": "equation", "pos": 17602, "prevtext": "\nholds for all} \\textcolor{black}{$\\mathbf{s}$ with $||\\mathbf{s}||_{0}\\leq K$.\n}\\hfill{}$\\blacksquare$\n\n\\textit{\\textcolor{black}{Theorem 1:}}\\textcolor{black}{{} Assume that\n${\\color{black}\\delta_{2K}<\\sqrt{2}-1}$ and $||\\mathbf{e}||_{2}\\leq\\varepsilon$.\nThen the solution ${\\color{black}\\mathbf{\\hat{s}}}$ to (\\ref{eq:CS_l0_min})\nobeys\n\n", "index": 19, "text": "\\begin{equation}\n||{\\color{black}\\mathbf{\\hat{s}-s\\mathrm{||_{2}\\leq\\mathit{C_{0}}\\mathbf{\\mathit{K}\\mathrm{^{-1/2}}}||\\mathbf{s}}-\\mathrm{\\mathbf{s}}_{\\mathit{K}}}}||_{1}+\\mathit{C_{1}}\\varepsilon\\label{eq:error bound}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"||{\\color[rgb]{0,0,0}\\mathbf{\\hat{s}-s\\mathrm{||_{2}\\leq\\mathit{C_{0}}\\mathbf{%&#10;\\mathit{K}\\mathrm{{}^{-1/2}}}||\\mathbf{s}}-\\mathrm{\\mathbf{s}}_{\\mathit{K}}}}|%&#10;|_{1}+\\mathit{C_{1}}\\varepsilon\" display=\"block\"><mrow><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi mathcolor=\"#000000\">\ud835\udc2c</mi><mo mathcolor=\"#000000\" stretchy=\"false\">^</mo></mover><mo mathcolor=\"#000000\">-</mo><mi mathcolor=\"#000000\">\ud835\udc2c</mi><mo mathcolor=\"#000000\" stretchy=\"false\">|</mo><msub><mo mathcolor=\"#000000\" stretchy=\"false\">|</mo><mn mathcolor=\"#000000\">2</mn></msub><mo mathcolor=\"#000000\">\u2264</mo><msub><mi mathcolor=\"#000000\">C</mi><mn mathcolor=\"#000000\"/></msub><mi mathcolor=\"#000000\">K</mi><mmultiscripts><mo mathcolor=\"#000000\" stretchy=\"false\">|</mo><mprescripts/><none/><mrow><mo mathcolor=\"#000000\">-</mo><mrow><mn mathcolor=\"#000000\">1</mn><mo mathcolor=\"#000000\">/</mo><mn mathcolor=\"#000000\">2</mn></mrow></mrow></mmultiscripts><mo mathcolor=\"#000000\" stretchy=\"false\">|</mo><mi mathcolor=\"#000000\">\ud835\udc2c</mi><mo mathcolor=\"#000000\">-</mo><msub><mi mathcolor=\"#000000\">\ud835\udc2c</mi><mi mathcolor=\"#000000\">K</mi></msub><mo stretchy=\"false\">|</mo><msub><mo stretchy=\"false\">|</mo><mn>1</mn></msub><mo>+</mo><msub><mi>C</mi><mn mathvariant=\"italic\">1</mn></msub><mi>\u03b5</mi></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\n}where $\\mathbf{a}_{i}$ denotes the $i$th column of $\\mathbf{A}$.\nIt has been shown that the reconstruction error of the $l_{1}$ minimization\nproblem is bounded if $\\mu(\\mathbf{A})<1/(4K-1)$. Based on the concept\nof mutual coherence, optimal projection design approaches are derived,\ne.g., in \\cite{Elad2007,Duarte2009,Xu2010}.\n\nWhen it comes to TCS, the reconstruction approaches for CS can still\nbe utilized owing to the relationship in (\\ref{eq:CSvsTCS}). However,\nfor the algorithms where explicit usage of $\\overline{\\mathbf{A}}$\nis required, e.g, OMP, the implementation is restricted by the large\ndimension of $\\overline{\\mathbf{A}}$. By extending the CS reconstruction\napproaches to utilize tensor-based operations, TCS reconstruction\nalgorithms employing only small matrices $\\mathbf{A}_{i}\\ (i=1,...,n)$\nhave been developed in \\cite{Caiafa2013,Caiafa:2013,Rivenson2009a,Rivenson2009b}.\nThese methods maintain the theoretical guarantees of conventional\nCS when $\\overline{\\mathbf{A}}$ obeys the condition on the RIC or\nthe mutual coherence, but reduce the computational complexity and\nrelax the storage memory requirement.\n\nEven so, the conditions on $\\overline{\\mathbf{A}}$ are not intuitive\nfor a practical TCS system, which explicitly utilizes multiple separable\nsensing matrices $\\mathbf{A}_{i}\\ (i=1,...,n)$ instead of a single\nmatrix $\\overline{\\mathbf{A}}$. Fortunately, the authors of \\cite{Duarte2012}\nhave derived the following relationships to clarify the corresponding\nconditions on $\\mathbf{A}_{i}\\ (i=1,...,n)$.\n\n\\textit{\\textcolor{black}{Theorem 2: }}\\textcolor{black}{Let }$\\mathbf{A}_{i}\\ (i=1,...,n)$\nbe matrices with RICs $\\delta_{K}(\\mathbf{A}_{1})$, ..., $\\delta_{K}(\\mathbf{A}_{n})$,\nrespectively, and their mutual coherence are $\\mu(\\mathbf{A}_{1})$,\n..., $\\mu(\\mathbf{A}_{n})$. Then for the matrix $\\overline{\\mathbf{A}}=\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{1}$,\nwe have \n\n", "itemtype": "equation", "pos": 19295, "prevtext": "\nwhere ${\\color{black}\\mathit{C_{0}=\\frac{\\mathrm{2+(2\\sqrt{2}-2)\\delta_{2\\mathit{K}}}}{\\mathrm{1-(\\sqrt{2}+1)\\delta_{2\\mathit{K}}}}}}$,\n${\\color{black}\\mathit{C_{1}=\\frac{4\\sqrt{\\mathrm{1+\\delta_{2\\mathit{K}}}}}{\\mathrm{1-(\\sqrt{2}+1)\\delta_{2\\mathit{K}}}}}}$,\n${\\color{black}\\delta_{2\\mathit{K}}}$ is the RIC of matrix ${\\color{black}\\mathbf{A}}$,\n$\\mathbf{s}_{K}$ is an approximation of ${\\color{black}\\mathbf{s}}$\nwith all but the ${\\color{black}\\mathit{K}}$ largest entries set\nto zero.}\\hfill{}$\\blacksquare$\n\nThe previous theorem states that for the noiseless case, any sparse\nsignal with fewer than $K$ non-zero coefficients can be exactly recovered\nif the RIC of the equivalent sensing matrix satisfies \\textcolor{black}{${\\color{black}\\delta_{2K}<\\sqrt{2}-1}$;\nwhile for the noisy case and the not exactly sparse case, the reconstructed\nsignal is still a good approximation of the original signal under\nthe same condition. The theoretical guarantees of successful reconstruction\nfor the greedy approaches have also been investigated in \\cite{Tropp2007,Blumensath2009}. }\n\n\\textcolor{black}{The RIP essentially measures the quality of the\nequivalent sensing matrix $\\mathbf{A}$, which closely relates to\nthe design of $\\boldsymbol{\\Phi}$ and $\\boldsymbol{\\Psi}$. However,\nsince the RIP is not tractable, another measure is often used for\nCS projection design, i.e., the mutual coherence of $\\mathbf{A}$\n\\cite{donoho2006stable} and it is defined by: \n\n", "index": 21, "text": "\\begin{equation}\n\\mu(\\mathbf{A})=\\underset{1\\leq i,\\ j\\leq\\hat{N},\\ i\\neq j}{max}|\\mathbf{a}_{i}^{T}\\mathbf{a}_{j}|,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mu(\\mathbf{A})=\\underset{1\\leq i,\\ j\\leq\\hat{N},\\ i\\neq j}{max}|\\mathbf{a}_{i%&#10;}^{T}\\mathbf{a}_{j}|,\" display=\"block\"><mrow><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mrow><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mi>j</mi><mo>\u2264</mo><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow></mrow></munder><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mi>\ud835\udc1a</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\n \\hfill{}$\\blacksquare$\n\nIn \\cite{Duarte2012}, these relationships are then utilized to derive\nthe reconstruction error bounds for a TCS system.\n\n\n\\section{Optimized Multilinear Projections for TCS}\n\n\\label{sec:PhiDesign}\n\nIn this section, we show how to optimize the multilinear sensing matrix\nwhen the dictionaries $\\boldsymbol{\\Psi}_{i}\\ (i=1,...,n)$ for each\ndimension are fixed. We first introduce the related design approaches\nfor CS, then present the proposed methods for TCS, including a separable\nand a non-separable design approach.\n\n\n\\subsection{Sensing Matrix Design for CS }\n\n\\label{sub:CS sensing design}\n\nWe observe that the sufficient conditions on the RIC or the mutual\ncoherence for successful CS reconstruction, as reviewed in Section\n\\ref{sub:SigReconModel}, only describe the worst case bound, which\nmeans that the average recovery performance is not reflected. In fact,\nthe most challenging part of CS sensing matrix design lies in deriving\na measure that can directly reveal the expected-case reconstruction\naccuracy. \n\nIn \\cite{Elad2007}, Elad \\textit{et al.} proposed the notion of averaged\nmutual coherence, based on which an iterative algorithm is derived\nfor optimal sensing matrix design. This approach aims to minimize\nthe largest absolute values of the off-diagonal entries in the Gram\nmatrix of $\\mathbf{A}$, i.e., $\\mathbf{G}_{\\mathbf{A}}=\\mathbf{A}^{T}\\mathbf{A}$.\nIt has been shown to outperform a random Gaussian sensing matrix in\nterms of reconstruction accuracy, but is time-consuming to construct\nand can ruin the worst case guarantees by inducing large off-diagonal\nvalues that are not in the original Gram matrix. In order to make\nany subset of columns in $\\mathbf{A}$ as orthogonal as possible,\nSapiro\\textit{ et al.} proposed in \\cite{Duarte2009} to make $\\mathbf{G}_{\\mathbf{A}}$\nas close as possible to an identity matrix, i.e., $\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\approx\\mathbf{I}_{\\hat{N}}$.\nIt is then approximated by minimizing $||\\boldsymbol{\\Lambda}-\\boldsymbol{\\Lambda\\Gamma}^{T}\\boldsymbol{\\Gamma\\Lambda}||_{F}^{2}$,\nwhere $\\boldsymbol{\\Gamma}$ comes from the eigen-decomposition of\n$\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}$, i.e., $\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{T}$,\nand $\\boldsymbol{\\Gamma}=\\boldsymbol{\\Phi}\\mathbf{V}$. This approach\nis also iterative, but outperforms Elad's method. Considering the\nfact that $\\mathbf{A}$ has minimum coherence when the magnitudes\nof all the off-diagonal entries of $\\mathbf{G}_{\\mathbf{A}}$ are\nequal, Xu \\textit{et al.} proposed an Equiangular Tight Frame (ETF)\nbased method in \\cite{Xu2010}. The problem is modeled as: $min_{\\mathbf{G}_{t}\\in\\mathcal{H}}||\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}-\\mathbf{G}_{t}||_{F}^{2}$,\nwhere $\\mathbf{G}_{t}$ is the target Gram matrix and $\\mathcal{H}$\nis the set of the ETF Gram matrices. Improved performance has been\nobserved for the obtained sensing matrix.\n\nMore recently, based on the same idea as Sapiro, the problem of \n\n", "itemtype": "equation", "pos": 21361, "prevtext": "\n}where $\\mathbf{a}_{i}$ denotes the $i$th column of $\\mathbf{A}$.\nIt has been shown that the reconstruction error of the $l_{1}$ minimization\nproblem is bounded if $\\mu(\\mathbf{A})<1/(4K-1)$. Based on the concept\nof mutual coherence, optimal projection design approaches are derived,\ne.g., in \\cite{Elad2007,Duarte2009,Xu2010}.\n\nWhen it comes to TCS, the reconstruction approaches for CS can still\nbe utilized owing to the relationship in (\\ref{eq:CSvsTCS}). However,\nfor the algorithms where explicit usage of $\\overline{\\mathbf{A}}$\nis required, e.g, OMP, the implementation is restricted by the large\ndimension of $\\overline{\\mathbf{A}}$. By extending the CS reconstruction\napproaches to utilize tensor-based operations, TCS reconstruction\nalgorithms employing only small matrices $\\mathbf{A}_{i}\\ (i=1,...,n)$\nhave been developed in \\cite{Caiafa2013,Caiafa:2013,Rivenson2009a,Rivenson2009b}.\nThese methods maintain the theoretical guarantees of conventional\nCS when $\\overline{\\mathbf{A}}$ obeys the condition on the RIC or\nthe mutual coherence, but reduce the computational complexity and\nrelax the storage memory requirement.\n\nEven so, the conditions on $\\overline{\\mathbf{A}}$ are not intuitive\nfor a practical TCS system, which explicitly utilizes multiple separable\nsensing matrices $\\mathbf{A}_{i}\\ (i=1,...,n)$ instead of a single\nmatrix $\\overline{\\mathbf{A}}$. Fortunately, the authors of \\cite{Duarte2012}\nhave derived the following relationships to clarify the corresponding\nconditions on $\\mathbf{A}_{i}\\ (i=1,...,n)$.\n\n\\textit{\\textcolor{black}{Theorem 2: }}\\textcolor{black}{Let }$\\mathbf{A}_{i}\\ (i=1,...,n)$\nbe matrices with RICs $\\delta_{K}(\\mathbf{A}_{1})$, ..., $\\delta_{K}(\\mathbf{A}_{n})$,\nrespectively, and their mutual coherence are $\\mu(\\mathbf{A}_{1})$,\n..., $\\mu(\\mathbf{A}_{n})$. Then for the matrix $\\overline{\\mathbf{A}}=\\mathbf{A}_{n}\\otimes\\mathbf{A}_{n-1}\\otimes...\\otimes\\mathbf{A}_{1}$,\nwe have \n\n", "index": 23, "text": "\\begin{align}\n\\mu(\\overline{\\mathbf{A}}) & =\\prod_{i=1}^{n}\\mu(\\mathbf{A}_{i}),\\\\\n\\delta_{K}(\\overline{\\mathbf{A}}) & \\leq\\prod_{i=1}^{n}(1+\\delta_{K}(\\mathbf{A}_{i}))-1.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu(\\overline{\\mathbf{A}})\" display=\"inline\"><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\prod_{i=1}^{n}\\mu(\\mathbf{A}_{i}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc00</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\delta_{K}(\\overline{\\mathbf{A}})\" display=\"inline\"><mrow><msub><mi>\u03b4</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\prod_{i=1}^{n}(1+\\delta_{K}(\\mathbf{A}_{i}))-1.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>\u03b4</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc00</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nhas been considered and an analytical solution has been derived in\n\\cite{Li2013}. Meanwhile, in \\cite{Chen2013,Chen2012}, it has been\nshown that in order to achieve good expected-case Mean Squared Error\n(MSE) performance, the equivalent sensing matrix ought to be close\nto a Parseval tight frame, thus leading to the following design approach:\n\n", "itemtype": "equation", "pos": 24631, "prevtext": "\n \\hfill{}$\\blacksquare$\n\nIn \\cite{Duarte2012}, these relationships are then utilized to derive\nthe reconstruction error bounds for a TCS system.\n\n\n\\section{Optimized Multilinear Projections for TCS}\n\n\\label{sec:PhiDesign}\n\nIn this section, we show how to optimize the multilinear sensing matrix\nwhen the dictionaries $\\boldsymbol{\\Psi}_{i}\\ (i=1,...,n)$ for each\ndimension are fixed. We first introduce the related design approaches\nfor CS, then present the proposed methods for TCS, including a separable\nand a non-separable design approach.\n\n\n\\subsection{Sensing Matrix Design for CS }\n\n\\label{sub:CS sensing design}\n\nWe observe that the sufficient conditions on the RIC or the mutual\ncoherence for successful CS reconstruction, as reviewed in Section\n\\ref{sub:SigReconModel}, only describe the worst case bound, which\nmeans that the average recovery performance is not reflected. In fact,\nthe most challenging part of CS sensing matrix design lies in deriving\na measure that can directly reveal the expected-case reconstruction\naccuracy. \n\nIn \\cite{Elad2007}, Elad \\textit{et al.} proposed the notion of averaged\nmutual coherence, based on which an iterative algorithm is derived\nfor optimal sensing matrix design. This approach aims to minimize\nthe largest absolute values of the off-diagonal entries in the Gram\nmatrix of $\\mathbf{A}$, i.e., $\\mathbf{G}_{\\mathbf{A}}=\\mathbf{A}^{T}\\mathbf{A}$.\nIt has been shown to outperform a random Gaussian sensing matrix in\nterms of reconstruction accuracy, but is time-consuming to construct\nand can ruin the worst case guarantees by inducing large off-diagonal\nvalues that are not in the original Gram matrix. In order to make\nany subset of columns in $\\mathbf{A}$ as orthogonal as possible,\nSapiro\\textit{ et al.} proposed in \\cite{Duarte2009} to make $\\mathbf{G}_{\\mathbf{A}}$\nas close as possible to an identity matrix, i.e., $\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\approx\\mathbf{I}_{\\hat{N}}$.\nIt is then approximated by minimizing $||\\boldsymbol{\\Lambda}-\\boldsymbol{\\Lambda\\Gamma}^{T}\\boldsymbol{\\Gamma\\Lambda}||_{F}^{2}$,\nwhere $\\boldsymbol{\\Gamma}$ comes from the eigen-decomposition of\n$\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}$, i.e., $\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{T}$,\nand $\\boldsymbol{\\Gamma}=\\boldsymbol{\\Phi}\\mathbf{V}$. This approach\nis also iterative, but outperforms Elad's method. Considering the\nfact that $\\mathbf{A}$ has minimum coherence when the magnitudes\nof all the off-diagonal entries of $\\mathbf{G}_{\\mathbf{A}}$ are\nequal, Xu \\textit{et al.} proposed an Equiangular Tight Frame (ETF)\nbased method in \\cite{Xu2010}. The problem is modeled as: $min_{\\mathbf{G}_{t}\\in\\mathcal{H}}||\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}-\\mathbf{G}_{t}||_{F}^{2}$,\nwhere $\\mathbf{G}_{t}$ is the target Gram matrix and $\\mathcal{H}$\nis the set of the ETF Gram matrices. Improved performance has been\nobserved for the obtained sensing matrix.\n\nMore recently, based on the same idea as Sapiro, the problem of \n\n", "index": 25, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}}{min}\\ ||\\mathbf{I}_{\\hat{N}}-\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}\\label{eq: Li's design}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}}{min}\\ ||\\mathbf{I}_{\\hat{N}}-\\boldsymbol{\\Psi}^{T%&#10;}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}\" display=\"block\"><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>\ud835\udebd</mo></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover></msub><mo>-</mo><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>\ud835\udebd</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udebf</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $||\\boldsymbol{\\Phi}||_{F}^{2}$ is the sensing cost that also\naffects the reconstruction accuracy (as verified in \\cite{Chen2013,Chen2012}).\nA closed form solution to this problem was also obtained in \\cite{Chen2013,Chen2012}.\nThese approaches have further improved the average reconstruction\nperformance for a CS system that is able to employ the optimized sensing\nmatrix.\n\nOn the other hand, using the model of Xu's method \\cite{Xu2010},\nCleju \\cite{Cleju2014} proposed to take $\\mathbf{G}_{t}=\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}$\nso that the equivalent sensing matrix has similar properties to those\nof $\\boldsymbol{\\Psi}$; and Bai \\textit{et al. }\\cite{Bai2015} proposed\ncombining the ETF Grams and that proposed by Cleju to solve: $min_{\\mathbf{G}_{t}\\in\\mathcal{H}}(1-\\beta)||\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}+\\beta||\\mathbf{G}_{t}-\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}$,\nwhere $\\beta$ is a trade-off parameter. Promising results of these\nmethods are demonstrated.\n\n\n\\subsection{Multidimensional Sensing Matrix Design for TCS}\n\n\\label{sub:SensingDesign}\n\nIn contrast to the aforementioned methods, we consider optimization\nof the sensing matrix for TCS. Compared to the design process in conventional\nCS, the main distinction for the TCS is that we would like to optimize\nmultiple separable sensing matrices $\\boldsymbol{\\Phi}_{i}\\ (i=1,...,n)$,\nrather than a single matrix $\\boldsymbol{\\Phi}$. In this section,\nin addition to extending the approaches in (\\ref{eq: Li's design})\nand (\\ref{eq:Chen's design}) to the TCS case, we also propose a new\napproach for TCS sensing matrices design by combining the state-of-art\nideas in \\cite{Chen2013,Cleju2014,Bai2015}. To simplify our exposition,\nwe elaborate our methods in the following sections for the case of\n$n=2$, i.e., the tensor signal becomes a matrix, but note that the\nmethods can be straightforwardly extended to an $n$ mode tensor case\n($n>2$).\n\nAs reviewed in Section \\ref{sub:SigReconModel}, the performance of\nexisting TCS reconstruction algorithms relies on the quality of $\\overline{\\mathbf{A}}$,\nwhere $\\overline{\\mathbf{A}}=\\mathbf{A}_{2}\\otimes\\mathbf{A}_{1}$\nwhen $n=2$. Therefore, when the multilinear dictionary $\\overline{\\boldsymbol{\\Psi}}=\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1}$\nis given, one can optimize $\\overline{\\boldsymbol{\\Phi}}$ (where\n$\\overline{\\boldsymbol{\\Phi}}=\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$)\nusing the methods for CS as introduced in Section \\ref{sub:CS sensing design}. \n\nHowever, when implementing a TCS system, it is still necessary to\nobtain the separable matrices, i.e., $\\boldsymbol{\\Phi}_{1}$ and\n$\\boldsymbol{\\Phi}_{2}$. One intuitive solution is to design $\\overline{\\boldsymbol{\\Phi}}$\nusing the aforementioned approaches for CS and then to decompose $\\overline{\\boldsymbol{\\Phi}}$\nby solving the following problem:\n\n", "itemtype": "equation", "pos": 25175, "prevtext": "\nhas been considered and an analytical solution has been derived in\n\\cite{Li2013}. Meanwhile, in \\cite{Chen2013,Chen2012}, it has been\nshown that in order to achieve good expected-case Mean Squared Error\n(MSE) performance, the equivalent sensing matrix ought to be close\nto a Parseval tight frame, thus leading to the following design approach:\n\n", "index": 27, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}}{min}\\ ||\\boldsymbol{\\Phi}||_{F}^{2},\\ s.t.\\ \\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}=\\mathbf{I}_{M},\\label{eq:Chen's design}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}}{min}\\ ||\\boldsymbol{\\Phi}||_{F}^{2},\\ s.t.\\ %&#10;\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}=%&#10;\\mathbf{I}_{M},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>\ud835\udebd</mo></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\ud835\udebd</mi><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udebf</mi><mo>\u2062</mo><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>\ud835\udebd</mi><mi>T</mi></msup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>M</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhich has been studied as a Nearest Kronecker Product (NKP) problem\nin \\cite{Van2000}. But this is not a feasible solution for TCS sensing\nmatrix design. First of all, $\\overline{\\boldsymbol{\\Phi}}$ can only\nbe exactly decomposed as $\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$\nwhen a certain permutation of $\\overline{\\boldsymbol{\\Phi}}$ has\nrank 1 \\cite{Van2000}, which is not the case for most sensing strategies.\nWhen the term in (\\ref{eq:NKP}) is minimized to a non-zero value,\nthe solution $\\hat{\\boldsymbol{\\Phi}}_{1},\\ \\hat{\\boldsymbol{\\Phi}}_{2}$\nleads to a sensing matrix $\\hat{\\boldsymbol{\\Phi}}_{2}\\otimes\\hat{\\boldsymbol{\\Phi}}_{1}$,\nwhich may not satisfy the condition of the sensing matrix $\\overline{\\boldsymbol{\\Phi}}$\nfor good CS recovery (e.g., the requirement on the mutual coherence),\nthereby ruining the reconstruction guarantees. Secondly, to solve\n(\\ref{eq:NKP}), explicit storage of $\\overline{\\boldsymbol{\\Phi}}$\nis necessary, which is restrictive for high dimensional problems.\nIn addition, when the number of tensor modes increases, the problem\nbecomes more complex to solve.\n\nTherefore, we aim to optimize $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\ndirectly without knowing $\\overline{\\boldsymbol{\\Phi}}$. Extending\n(\\ref{eq: Li's design}) and (\\ref{eq:Chen's design}), we first propose\na method that is shown to be separable as independent sub-design-problems.\nThen a non-separable design approach is presented and a gradient based\nalgorithm is derived.\n\n\n\\subsubsection{A Separable Design Approach}\n\nThe proposed separable design approach (Approach I) is as follows:\n\n", "itemtype": "equation", "pos": 28395, "prevtext": "\nwhere $||\\boldsymbol{\\Phi}||_{F}^{2}$ is the sensing cost that also\naffects the reconstruction accuracy (as verified in \\cite{Chen2013,Chen2012}).\nA closed form solution to this problem was also obtained in \\cite{Chen2013,Chen2012}.\nThese approaches have further improved the average reconstruction\nperformance for a CS system that is able to employ the optimized sensing\nmatrix.\n\nOn the other hand, using the model of Xu's method \\cite{Xu2010},\nCleju \\cite{Cleju2014} proposed to take $\\mathbf{G}_{t}=\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}$\nso that the equivalent sensing matrix has similar properties to those\nof $\\boldsymbol{\\Psi}$; and Bai \\textit{et al. }\\cite{Bai2015} proposed\ncombining the ETF Grams and that proposed by Cleju to solve: $min_{\\mathbf{G}_{t}\\in\\mathcal{H}}(1-\\beta)||\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Psi}-\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}+\\beta||\\mathbf{G}_{t}-\\boldsymbol{\\Psi}^{T}\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}\\boldsymbol{\\Psi}||_{F}^{2}$,\nwhere $\\beta$ is a trade-off parameter. Promising results of these\nmethods are demonstrated.\n\n\n\\subsection{Multidimensional Sensing Matrix Design for TCS}\n\n\\label{sub:SensingDesign}\n\nIn contrast to the aforementioned methods, we consider optimization\nof the sensing matrix for TCS. Compared to the design process in conventional\nCS, the main distinction for the TCS is that we would like to optimize\nmultiple separable sensing matrices $\\boldsymbol{\\Phi}_{i}\\ (i=1,...,n)$,\nrather than a single matrix $\\boldsymbol{\\Phi}$. In this section,\nin addition to extending the approaches in (\\ref{eq: Li's design})\nand (\\ref{eq:Chen's design}) to the TCS case, we also propose a new\napproach for TCS sensing matrices design by combining the state-of-art\nideas in \\cite{Chen2013,Cleju2014,Bai2015}. To simplify our exposition,\nwe elaborate our methods in the following sections for the case of\n$n=2$, i.e., the tensor signal becomes a matrix, but note that the\nmethods can be straightforwardly extended to an $n$ mode tensor case\n($n>2$).\n\nAs reviewed in Section \\ref{sub:SigReconModel}, the performance of\nexisting TCS reconstruction algorithms relies on the quality of $\\overline{\\mathbf{A}}$,\nwhere $\\overline{\\mathbf{A}}=\\mathbf{A}_{2}\\otimes\\mathbf{A}_{1}$\nwhen $n=2$. Therefore, when the multilinear dictionary $\\overline{\\boldsymbol{\\Psi}}=\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1}$\nis given, one can optimize $\\overline{\\boldsymbol{\\Phi}}$ (where\n$\\overline{\\boldsymbol{\\Phi}}=\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$)\nusing the methods for CS as introduced in Section \\ref{sub:CS sensing design}. \n\nHowever, when implementing a TCS system, it is still necessary to\nobtain the separable matrices, i.e., $\\boldsymbol{\\Phi}_{1}$ and\n$\\boldsymbol{\\Phi}_{2}$. One intuitive solution is to design $\\overline{\\boldsymbol{\\Phi}}$\nusing the aforementioned approaches for CS and then to decompose $\\overline{\\boldsymbol{\\Phi}}$\nby solving the following problem:\n\n", "index": 29, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||\\overline{\\boldsymbol{\\Phi}}-\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2},\\label{eq:NKP}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||\\overline{%&#10;\\boldsymbol{\\Phi}}-\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2},\" display=\"block\"><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebd</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo>-</mo><mrow><msub><mi>\ud835\udebd</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nand it is an extension of (\\ref{eq: Li's design}) to the case when\na multilinear sensing matrix is employed. The solution of (\\ref{eq:Approach I})\nis presented in Theorem 3 and Approach I is also summarized in Algorithm\n1.\n\n\\textit{Theorem 3:} Assume for $i=1,\\ 2,$ $\\bar{N}_{i}=rank(\\boldsymbol{\\Psi}_{i})$,\n$\\boldsymbol{\\Psi}_{i}=\\mathbf{U}_{\\boldsymbol{\\Psi}_{i}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\mathbf{V}_{\\boldsymbol{\\Psi}_{i}}^{T}$ is an SVD of $\\boldsymbol{\\Psi}_{i}$ and $\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}}\\in\\mathbb{R}^{\\bar{N}_{i}\\times\\bar{N}_{i}}$.\nLet $\\hat{\\boldsymbol{\\Phi}}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,\\ 2)$\nbe matrices with $rank(\\hat{\\boldsymbol{\\Phi}}_{i})=M_{i}$ and $M_{i}\\leq\\bar{N}_{i}$\nis assumed. Then \n\\begin{itemize}\n\\item the following equation is a solution to (\\ref{eq:Approach I}): \n\n", "itemtype": "equation", "pos": 30208, "prevtext": "\nwhich has been studied as a Nearest Kronecker Product (NKP) problem\nin \\cite{Van2000}. But this is not a feasible solution for TCS sensing\nmatrix design. First of all, $\\overline{\\boldsymbol{\\Phi}}$ can only\nbe exactly decomposed as $\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$\nwhen a certain permutation of $\\overline{\\boldsymbol{\\Phi}}$ has\nrank 1 \\cite{Van2000}, which is not the case for most sensing strategies.\nWhen the term in (\\ref{eq:NKP}) is minimized to a non-zero value,\nthe solution $\\hat{\\boldsymbol{\\Phi}}_{1},\\ \\hat{\\boldsymbol{\\Phi}}_{2}$\nleads to a sensing matrix $\\hat{\\boldsymbol{\\Phi}}_{2}\\otimes\\hat{\\boldsymbol{\\Phi}}_{1}$,\nwhich may not satisfy the condition of the sensing matrix $\\overline{\\boldsymbol{\\Phi}}$\nfor good CS recovery (e.g., the requirement on the mutual coherence),\nthereby ruining the reconstruction guarantees. Secondly, to solve\n(\\ref{eq:NKP}), explicit storage of $\\overline{\\boldsymbol{\\Phi}}$\nis necessary, which is restrictive for high dimensional problems.\nIn addition, when the number of tensor modes increases, the problem\nbecomes more complex to solve.\n\nTherefore, we aim to optimize $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\ndirectly without knowing $\\overline{\\boldsymbol{\\Phi}}$. Extending\n(\\ref{eq: Li's design}) and (\\ref{eq:Chen's design}), we first propose\na method that is shown to be separable as independent sub-design-problems.\nThen a non-separable design approach is presented and a gradient based\nalgorithm is derived.\n\n\n\\subsubsection{A Separable Design Approach}\n\nThe proposed separable design approach (Approach I) is as follows:\n\n", "index": 31, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\boldsymbol{\\Psi}_{2}^{T}\\otimes\\boldsymbol{\\Psi}_{1}^{T})(\\boldsymbol{\\Phi}_{2}^{T}\\otimes\\boldsymbol{\\Phi}_{1}^{T})(\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1})(\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1})||_{F}^{2},\\label{eq:Approach I}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||\\mathbf{I}_{%&#10;\\hat{N}_{1}\\hat{N}_{2}}-(\\boldsymbol{\\Psi}_{2}^{T}\\otimes\\boldsymbol{\\Psi}_{1}%&#10;^{T})(\\boldsymbol{\\Phi}_{2}^{T}\\otimes\\boldsymbol{\\Phi}_{1}^{T})(\\boldsymbol{%&#10;\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1})(\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol%&#10;{\\Psi}_{1})||_{F}^{2},\" display=\"block\"><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebd</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></msub><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udebf</mi><mn>2</mn><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mi>\ud835\udebf</mi><mn>1</mn><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udebd</mi><mn>2</mn><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mi>\ud835\udebd</mi><mn>1</mn><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udebd</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udebf</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $i=1,\\ 2$, $\\mathbf{U}\\in\\mathbb{R}^{M_{i}\\times M_{i}}$ and\n$\\mathbf{V}\\in\\mathbb{R}^{\\bar{N}_{i}\\times\\bar{N}_{i}}$ are arbitrary\northonormal matrices; \n\\item the resulting equivalent sensing matrices $\\hat{\\mathbf{A}}_{i}=\\hat{\\boldsymbol{\\Phi}}_{i}\\boldsymbol{\\Psi}_{i}\\ (i=1,\\ 2)$\nare Parseval tight frames, i.e., $||\\hat{\\mathbf{A}}_{i}^{T}\\mathbf{z}||_{2}=||\\mathbf{z}||_{2}$,\nwhere $\\mathbf{z}\\in\\mathbb{R}^{\\hat{N}_{i}}$ is an arbitrary vector.\n\\item the minimum of (\\ref{eq:Approach I}) is $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$; \n\\item separately solving the sub-problems\n\n", "itemtype": "equation", "pos": 31525, "prevtext": "\nand it is an extension of (\\ref{eq: Li's design}) to the case when\na multilinear sensing matrix is employed. The solution of (\\ref{eq:Approach I})\nis presented in Theorem 3 and Approach I is also summarized in Algorithm\n1.\n\n\\textit{Theorem 3:} Assume for $i=1,\\ 2,$ $\\bar{N}_{i}=rank(\\boldsymbol{\\Psi}_{i})$,\n$\\boldsymbol{\\Psi}_{i}=\\mathbf{U}_{\\boldsymbol{\\Psi}_{i}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\mathbf{V}_{\\boldsymbol{\\Psi}_{i}}^{T}$ is an SVD of $\\boldsymbol{\\Psi}_{i}$ and $\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}}\\in\\mathbb{R}^{\\bar{N}_{i}\\times\\bar{N}_{i}}$.\nLet $\\hat{\\boldsymbol{\\Phi}}_{i}\\in\\mathbb{R}^{M_{i}\\times N_{i}}\\ (i=1,\\ 2)$\nbe matrices with $rank(\\hat{\\boldsymbol{\\Phi}}_{i})=M_{i}$ and $M_{i}\\leq\\bar{N}_{i}$\nis assumed. Then \n\\begin{itemize}\n\\item the following equation is a solution to (\\ref{eq:Approach I}): \n\n", "index": 33, "text": "\\begin{equation}\n\\hat{\\boldsymbol{\\Phi}}_{i}=\\mathbf{U}\\left[\\begin{array}{cc}\n\\mathbf{I}_{M_{i}} & \\mathbf{0}\\end{array}\\right]\\left[\\begin{array}{cc}\n\\mathbf{V}^{T}\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}}^{-1} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\mathbf{U}_{\\boldsymbol{\\Psi}_{i}}^{T},\\label{eq:solutionApproI}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\boldsymbol{\\Phi}}_{i}=\\mathbf{U}\\left[\\begin{array}[]{cc}\\mathbf{I}_{M_{%&#10;i}}&amp;\\mathbf{0}\\end{array}\\right]\\left[\\begin{array}[]{cc}\\mathbf{V}^{T}%&#10;\\boldsymbol{\\Lambda}_{\\boldsymbol{\\Psi}_{i}}^{-1}&amp;\\mathbf{0}\\\\&#10;\\mathbf{0}&amp;\\mathbf{0}\\end{array}\\right]\\mathbf{U}_{\\boldsymbol{\\Psi}_{i}}^{T},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\ud835\udebd</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><mi>\ud835\udc14</mi><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udc08</mi><msub><mi>M</mi><mi>i</mi></msub></msub></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udc15</mi><mi>T</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udeb2</mi><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc14</mi><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mi>T</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nfor $i=1,\\ 2$ leads to the same solutions as (\\ref{eq:solutionApproI})\nand the resulting objective in (\\ref{eq:Approach I}) has the same\nminimum, i.e., $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$.\\hfill{}$\\blacksquare$\n\\end{itemize}\n\\textit{Proof:} The proof is given in Appendix \\ref{sec:AppendixI}.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Design Approach I}}\n\n\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}$ $(i=1,\\ 2)$. \n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$.\n\n1:\\textbf{ for} $i=1,\\ 2$ \\textbf{do}\n\n2: \\hspace{1em}Calculate optimized $\\hat{\\boldsymbol{\\Phi}}_{i}$\nusing (\\ref{eq:solutionApproI});\n\n3: \\textbf{end}\n\n4:\\textbf{ }Normalization for $i=1,\\ 2$: $\\hat{\\boldsymbol{\\Phi}}_{i}=\\sqrt{N_{i}}\\hat{\\boldsymbol{\\Phi}}_{i}/||\\hat{\\boldsymbol{\\Phi}}_{i}||_{F}$.\n\\end{algorithm}\n\n\nClearly, Approach I is separable, which means that we can independently\ndesign each $\\boldsymbol{\\Phi}_{i}$ according to the corresponding\nsparsifying dictionary $\\boldsymbol{\\Psi}_{i}$ in mode $i$. This\nobservation stays consistent when we consider the situation in an\nalternative way. Applying the method in (\\ref{eq: Li's design}) to\nacquire the optimal $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\nindependently, we are actually trying to make any subset of columns\nin $\\mathbf{A}_{1}$ and $\\mathbf{A}_{2}$, respectively, as orthogonal\nas possible. As a result, the matrix $\\overline{\\mathbf{A}}=\\mathbf{A}_{2}\\otimes\\mathbf{A}_{1}$\nthat is obtained will also be as orthogonal as possible. This follows\nfrom the fact that for any two columns of $\\overline{\\mathbf{A}}$,\nwe have \n\n", "itemtype": "equation", "pos": 32464, "prevtext": "\nwhere $i=1,\\ 2$, $\\mathbf{U}\\in\\mathbb{R}^{M_{i}\\times M_{i}}$ and\n$\\mathbf{V}\\in\\mathbb{R}^{\\bar{N}_{i}\\times\\bar{N}_{i}}$ are arbitrary\northonormal matrices; \n\\item the resulting equivalent sensing matrices $\\hat{\\mathbf{A}}_{i}=\\hat{\\boldsymbol{\\Phi}}_{i}\\boldsymbol{\\Psi}_{i}\\ (i=1,\\ 2)$\nare Parseval tight frames, i.e., $||\\hat{\\mathbf{A}}_{i}^{T}\\mathbf{z}||_{2}=||\\mathbf{z}||_{2}$,\nwhere $\\mathbf{z}\\in\\mathbb{R}^{\\hat{N}_{i}}$ is an arbitrary vector.\n\\item the minimum of (\\ref{eq:Approach I}) is $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$; \n\\item separately solving the sub-problems\n\n", "index": 35, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}_{i}}{min}\\ ||\\mathbf{I}_{\\hat{N}_{i}}-\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Phi}_{i}^{T}\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}||_{F}^{2}\\label{eq:sub_prob_I}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}_{i}}{min}\\ ||\\mathbf{I}_{\\hat{N}_{i}}-\\boldsymbol{%&#10;\\Psi}_{i}^{T}\\boldsymbol{\\Phi}_{i}^{T}\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{%&#10;i}||_{F}^{2}\" display=\"block\"><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udebd</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udebd</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{\\overline{a}}$, $\\mathbf{a}_{1}$ and $\\mathbf{a}_{2}$\ndenote the column of $\\overline{\\mathbf{A}}$, $\\mathbf{A}_{1}$ and\n$\\mathbf{A}_{2}$, respectively, and $p,\\ q,\\ l,\\ s,\\ c,\\ d$ are\nthe column indices. \n\nUsing the second statement of Theorem 3, we can derive the following\ncorollary.\n\n\\textit{Corollary 1:} The solution in (\\ref{eq:solutionApproI}) also\nsolves the following problems for $i=1,\\ 2$: \n\n", "itemtype": "equation", "pos": 34267, "prevtext": "\nfor $i=1,\\ 2$ leads to the same solutions as (\\ref{eq:solutionApproI})\nand the resulting objective in (\\ref{eq:Approach I}) has the same\nminimum, i.e., $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$.\\hfill{}$\\blacksquare$\n\\end{itemize}\n\\textit{Proof:} The proof is given in Appendix \\ref{sec:AppendixI}.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Design Approach I}}\n\n\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}$ $(i=1,\\ 2)$. \n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$.\n\n1:\\textbf{ for} $i=1,\\ 2$ \\textbf{do}\n\n2: \\hspace{1em}Calculate optimized $\\hat{\\boldsymbol{\\Phi}}_{i}$\nusing (\\ref{eq:solutionApproI});\n\n3: \\textbf{end}\n\n4:\\textbf{ }Normalization for $i=1,\\ 2$: $\\hat{\\boldsymbol{\\Phi}}_{i}=\\sqrt{N_{i}}\\hat{\\boldsymbol{\\Phi}}_{i}/||\\hat{\\boldsymbol{\\Phi}}_{i}||_{F}$.\n\\end{algorithm}\n\n\nClearly, Approach I is separable, which means that we can independently\ndesign each $\\boldsymbol{\\Phi}_{i}$ according to the corresponding\nsparsifying dictionary $\\boldsymbol{\\Psi}_{i}$ in mode $i$. This\nobservation stays consistent when we consider the situation in an\nalternative way. Applying the method in (\\ref{eq: Li's design}) to\nacquire the optimal $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\nindependently, we are actually trying to make any subset of columns\nin $\\mathbf{A}_{1}$ and $\\mathbf{A}_{2}$, respectively, as orthogonal\nas possible. As a result, the matrix $\\overline{\\mathbf{A}}=\\mathbf{A}_{2}\\otimes\\mathbf{A}_{1}$\nthat is obtained will also be as orthogonal as possible. This follows\nfrom the fact that for any two columns of $\\overline{\\mathbf{A}}$,\nwe have \n\n", "index": 37, "text": "\\begin{align}\n|\\mathbf{\\overline{a}}_{p}^{T}\\mathbf{\\overline{a}}_{q}| & =|[(\\mathbf{a}_{2})_{l}^{T}\\otimes(\\mathbf{a}_{1})_{s}^{T}][(\\mathbf{a}_{2})_{c}\\otimes(\\mathbf{a}_{1})_{d}]|\\nonumber \\\\\n & =|[(\\mathbf{a}_{2})_{l}^{T}(\\mathbf{a}_{2})_{c}][(\\mathbf{a}_{1})_{s}^{T}(\\mathbf{a}_{1})_{d}]|,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|\\mathbf{\\overline{a}}_{p}^{T}\\mathbf{\\overline{a}}_{q}|\" display=\"inline\"><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo>\u00af</mo></mover><mi>p</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo>\u00af</mo></mover><mi>q</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=|[(\\mathbf{a}_{2})_{l}^{T}\\otimes(\\mathbf{a}_{1})_{s}^{T}][(%&#10;\\mathbf{a}_{2})_{c}\\otimes(\\mathbf{a}_{1})_{d}]|\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>l</mi><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>s</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>c</mi></msub><mo>\u2297</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>d</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=|[(\\mathbf{a}_{2})_{l}^{T}(\\mathbf{a}_{2})_{c}][(\\mathbf{a}_{1})%&#10;_{s}^{T}(\\mathbf{a}_{1})_{d}]|,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>l</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>c</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>s</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>d</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhich represent the separable sub-problems of the following design\napproach:\n\n", "itemtype": "equation", "pos": 34992, "prevtext": "\nwhere $\\mathbf{\\overline{a}}$, $\\mathbf{a}_{1}$ and $\\mathbf{a}_{2}$\ndenote the column of $\\overline{\\mathbf{A}}$, $\\mathbf{A}_{1}$ and\n$\\mathbf{A}_{2}$, respectively, and $p,\\ q,\\ l,\\ s,\\ c,\\ d$ are\nthe column indices. \n\nUsing the second statement of Theorem 3, we can derive the following\ncorollary.\n\n\\textit{Corollary 1:} The solution in (\\ref{eq:solutionApproI}) also\nsolves the following problems for $i=1,\\ 2$: \n\n", "index": 39, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Phi}_{i}}{min}\\ ||\\boldsymbol{\\Phi}_{i}||_{F}^{2},\\ s.t.\\ \\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Phi}_{i}^{T}=\\mathbf{I}_{M_{i}},\\label{eq:subII}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Phi}_{i}}{min}\\ ||\\boldsymbol{\\Phi}_{i}||_{F}^{2},\\ s.t%&#10;.\\ \\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\boldsymbol{\\Psi}_{i}^{T}%&#10;\\boldsymbol{\\Phi}_{i}^{T}=\\mathbf{I}_{M_{i}},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udebd</mi><mi>i</mi></msub><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><msub><mi>\ud835\udebd</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udebd</mi><mi>i</mi><mi>T</mi></msubsup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><msub><mi>M</mi><mi>i</mi></msub></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nand it is in fact a multidimensional extension of the CS sensing matrix\ndesign approach proposed in \\cite{Chen2013}. \\hfill{}$\\blacksquare$\n\n\\textit{Proof:} Since the equivalent sensing matrices designed using\nApproach I are Parseval tight frames, it follows from the derivation\nin \\cite{Chen2013} that the sub-problems in (\\ref{eq:subII}) have\nthe same solution as in (\\ref{eq:solutionApproI}). The problem in\n(\\ref{eq:Approach II}) can be proved separable simply by revealing\nthe fact that $||\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2}=||\\boldsymbol{\\Phi}_{2}||_{F}^{2}||\\boldsymbol{\\Phi}_{1}||_{F}^{2}$,\nand when $\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Phi}_{i}^{T}=\\mathbf{I}_{M_{i}}$\nis satisfied for both $i=1$ and 2, the constraint in (\\ref{eq:Approach II})\nis also satisfied. \\hfill{}$\\blacksquare$\n\nBy decomposing the original problems into independent sub-problems,\nthe sensing matrices can be designed in parallel and the problem becomes\neasier to solve. However, the CS sensing matrix design approaches\nare not always separable after being extended to the multidimensional\ncase, because a variety of different criteria can be used for sensing\nmatrix design as reviewed in Section \\ref{sub:CS sensing design},\nand in many cases the decomposition is not provable. We will propose\na non-separable approach in the following section.\n\n\n\\subsubsection{A Non-separable Design Approach}\n\nTaking into account: i) the impact of sensing cost on reconstruction\nperformance \\cite{Chen2013}; ii) the benefit of making the equivalent\nsensing matrix so that it has similar properties to those of the sparsifying\ndictionary \\cite{Cleju2014}; and iii) the conventional requirement\non the mutual coherence, we put forth the following Design Approach\nII: \n\n", "itemtype": "equation", "pos": 35311, "prevtext": "\nwhich represent the separable sub-problems of the following design\napproach:\n\n", "index": 41, "text": "\\begin{gather}\n\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2},\\label{eq:Approach II}\\\\\ns.t.\\ (\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1})(\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1})(\\boldsymbol{\\Psi}_{2}^{T}\\otimes\\boldsymbol{\\Psi}_{1}^{T})(\\boldsymbol{\\Phi}_{2}^{T}\\otimes\\boldsymbol{\\Phi}_{1}^{T})=\\mathbf{I}_{M_{1}M_{2}},\\nonumber \n\\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\ ||%&#10;\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2},\" display=\"block\"><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebd</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udebd</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle s.t.\\ (\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1})(%&#10;\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1})(\\boldsymbol{\\Psi}_{2}^{T}%&#10;\\otimes\\boldsymbol{\\Psi}_{1}^{T})(\\boldsymbol{\\Phi}_{2}^{T}\\otimes\\boldsymbol{%&#10;\\Phi}_{1}^{T})=\\mathbf{I}_{M_{1}M_{2}},\" display=\"block\"><mrow><mrow><mi>s</mi><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udebd</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udebf</mi><mn>2</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udebf</mi><mn>2</mn><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mi>\ud835\udebf</mi><mn>1</mn><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udebd</mi><mn>2</mn><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mi>\ud835\udebd</mi><mn>1</mn><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mrow><msub><mi>M</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>M</mi><mn>2</mn></msub></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\overline{\\boldsymbol{\\Psi}}=\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1}$,\n$\\overline{\\boldsymbol{\\Phi}}=\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$,\n$\\alpha$ and $\\beta$ are tuning parameters. As investigated in \\cite{Chen2013}\nand \\cite{Bai2015}, $\\alpha\\geq0$ controls the sensing energy; while\n$\\beta\\in[0,\\ 1]$ balances the impact of the first and third terms\nto achieve optimal performance under different conditions of the measurement\nnoise. The choice of these parameters will be investigated in Section\n\\ref{sub:Simulation_Phi}.\n\nTo solve (\\ref{eq:Approach III}), we adopt a coordinate descent method.\nDenoting the objective as $f(\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2})$,\nwe first compute its gradient with respect to $\\boldsymbol{\\Phi}_{1}$\nand $\\boldsymbol{\\Phi}_{2}$, respectively, and the result is as follows:\n\n\n", "itemtype": "equation", "pos": 37555, "prevtext": "\nand it is in fact a multidimensional extension of the CS sensing matrix\ndesign approach proposed in \\cite{Chen2013}. \\hfill{}$\\blacksquare$\n\n\\textit{Proof:} Since the equivalent sensing matrices designed using\nApproach I are Parseval tight frames, it follows from the derivation\nin \\cite{Chen2013} that the sub-problems in (\\ref{eq:subII}) have\nthe same solution as in (\\ref{eq:solutionApproI}). The problem in\n(\\ref{eq:Approach II}) can be proved separable simply by revealing\nthe fact that $||\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}||_{F}^{2}=||\\boldsymbol{\\Phi}_{2}||_{F}^{2}||\\boldsymbol{\\Phi}_{1}||_{F}^{2}$,\nand when $\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Phi}_{i}^{T}=\\mathbf{I}_{M_{i}}$\nis satisfied for both $i=1$ and 2, the constraint in (\\ref{eq:Approach II})\nis also satisfied. \\hfill{}$\\blacksquare$\n\nBy decomposing the original problems into independent sub-problems,\nthe sensing matrices can be designed in parallel and the problem becomes\neasier to solve. However, the CS sensing matrix design approaches\nare not always separable after being extended to the multidimensional\ncase, because a variety of different criteria can be used for sensing\nmatrix design as reviewed in Section \\ref{sub:CS sensing design},\nand in many cases the decomposition is not provable. We will propose\na non-separable approach in the following section.\n\n\n\\subsubsection{A Non-separable Design Approach}\n\nTaking into account: i) the impact of sensing cost on reconstruction\nperformance \\cite{Chen2013}; ii) the benefit of making the equivalent\nsensing matrix so that it has similar properties to those of the sparsifying\ndictionary \\cite{Cleju2014}; and iii) the conventional requirement\non the mutual coherence, we put forth the following Design Approach\nII: \n\n", "index": 43, "text": "\\begin{align}\n\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\\  & (1-\\beta)||(\\overline{\\boldsymbol{\\Psi}})^{T}\\overline{\\boldsymbol{\\Psi}}-(\\overline{\\boldsymbol{\\Psi}})^{T}(\\overline{\\boldsymbol{\\Phi}})^{T}\\overline{\\boldsymbol{\\Phi}}\\overline{\\boldsymbol{\\Psi}}||_{F}^{2}\\nonumber \\\\\n & +\\alpha||\\overline{\\boldsymbol{\\Phi}}||_{F}^{2}+\\beta||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\overline{\\boldsymbol{\\Psi}})^{T}(\\overline{\\boldsymbol{\\Phi}})^{T}\\overline{\\boldsymbol{\\Phi}}\\overline{\\boldsymbol{\\Psi}}||_{F}^{2},\\label{eq:Approach III}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2}}{min}\" display=\"inline\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebd</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle(1-\\beta)||(\\overline{\\boldsymbol{\\Psi}})^{T}\\overline{%&#10;\\boldsymbol{\\Psi}}-(\\overline{\\boldsymbol{\\Psi}})^{T}(\\overline{\\boldsymbol{%&#10;\\Phi}})^{T}\\overline{\\boldsymbol{\\Phi}}\\overline{\\boldsymbol{\\Psi}}||_{F}^{2}\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover></mrow><mo>-</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\alpha||\\overline{\\boldsymbol{\\Phi}}||_{F}^{2}+\\beta||\\mathbf{I}%&#10;_{\\hat{N}_{1}\\hat{N}_{2}}-(\\overline{\\boldsymbol{\\Psi}})^{T}(\\overline{%&#10;\\boldsymbol{\\Phi}})^{T}\\overline{\\boldsymbol{\\Phi}}\\overline{\\boldsymbol{\\Psi}%&#10;}||_{F}^{2},\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></msub><mo>-</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udebd</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udebf</mi><mo>\u00af</mo></mover></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $i,\\ j\\in\\{1,\\ 2\\}$ and $j\\neq i$, $\\mathbf{G}_{\\mathbf{A}_{i}}=\\mathbf{A}_{i}^{T}\\mathbf{A}_{i}$\nand $\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}=\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Psi}_{i}$. \n\nFor generality, we also provide the result for the $n>2$ case as\nfollows: \n\\begin{eqnarray}\n\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}} & = & 4\\omega_{i}(\\mathbf{A}_{i}\\mathbf{G}_{\\mathbf{A}_{i}}\\boldsymbol{\\Psi}_{i}^{T})-4\\beta\\theta_{i}(\\mathbf{A}_{i}\\boldsymbol{\\Psi}_{i}^{T})\\nonumber \\\\\n & + & 2\\alpha\\tau_{i}\\boldsymbol{\\Phi}_{i}+(4\\beta-4)\\rho_{i}(\\mathbf{A}_{i}\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}\\boldsymbol{\\Psi}_{i}^{T}),\n\\end{eqnarray}\nwhere $i,\\ j\\in\\{1,...,n\\}$ and $j\\neq i$, $\\omega_{i}=\\prod_{j}||\\mathbf{G}_{\\mathbf{A}_{j}}||_{F}^{2}$,\n$\\theta_{i}=\\prod_{j}||\\mathbf{A}_{j}||_{F}^{2}$, $\\tau_{i}=\\prod_{j}||\\boldsymbol{\\Phi}_{j}||_{F}^{2}$,\n$\\rho_{i}=\\prod_{j}||\\boldsymbol{\\Psi}_{j}\\mathbf{A}_{j}^{T}||_{F}^{2}$. \n\nWith the gradient obtained, we can solve (\\ref{eq:Approach III})\nby alternatively updating $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\nas follows: \n\n", "itemtype": "equation", "pos": 38970, "prevtext": "\nwhere $\\overline{\\boldsymbol{\\Psi}}=\\boldsymbol{\\Psi}_{2}\\otimes\\boldsymbol{\\Psi}_{1}$,\n$\\overline{\\boldsymbol{\\Phi}}=\\boldsymbol{\\Phi}_{2}\\otimes\\boldsymbol{\\Phi}_{1}$,\n$\\alpha$ and $\\beta$ are tuning parameters. As investigated in \\cite{Chen2013}\nand \\cite{Bai2015}, $\\alpha\\geq0$ controls the sensing energy; while\n$\\beta\\in[0,\\ 1]$ balances the impact of the first and third terms\nto achieve optimal performance under different conditions of the measurement\nnoise. The choice of these parameters will be investigated in Section\n\\ref{sub:Simulation_Phi}.\n\nTo solve (\\ref{eq:Approach III}), we adopt a coordinate descent method.\nDenoting the objective as $f(\\boldsymbol{\\Phi}_{1},\\boldsymbol{\\Phi}_{2})$,\nwe first compute its gradient with respect to $\\boldsymbol{\\Phi}_{1}$\nand $\\boldsymbol{\\Phi}_{2}$, respectively, and the result is as follows:\n\n\n", "index": 45, "text": "\\begin{align}\n\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}} & =4||\\mathbf{G}_{\\mathbf{A}_{j}}||_{F}^{2}(\\mathbf{A}_{i}\\mathbf{G}_{\\mathbf{A}_{i}}\\boldsymbol{\\Psi}_{i}^{T})-4\\beta||\\mathbf{A}_{j}||_{F}^{2}(\\mathbf{A}_{i}\\boldsymbol{\\Psi}_{i}^{T})\\nonumber \\\\\n & +2\\alpha||\\boldsymbol{\\Phi}_{j}||_{F}^{2}\\boldsymbol{\\Phi}_{i}+4(\\beta-1)||\\boldsymbol{\\Psi}_{j}\\mathbf{A}_{j}^{T}||_{F}^{2}(\\mathbf{A}_{i}\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}\\boldsymbol{\\Psi}_{i}^{T}),\\label{eq:gradient_2D}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>f</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=4||\\mathbf{G}_{\\mathbf{A}_{j}}||_{F}^{2}(\\mathbf{A}_{i}\\mathbf{G%&#10;}_{\\mathbf{A}_{i}}\\boldsymbol{\\Psi}_{i}^{T})-4\\beta||\\mathbf{A}_{j}||_{F}^{2}(%&#10;\\mathbf{A}_{i}\\boldsymbol{\\Psi}_{i}^{T})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mn>4</mn><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc06</mi><msub><mi>\ud835\udc00</mi><mi>j</mi></msub></msub><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc00</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><msub><mi>\ud835\udc00</mi><mi>i</mi></msub></msub><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>4</mn><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc00</mi><mi>j</mi></msub><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc00</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+2\\alpha||\\boldsymbol{\\Phi}_{j}||_{F}^{2}\\boldsymbol{\\Phi}_{i}+4(%&#10;\\beta-1)||\\boldsymbol{\\Psi}_{j}\\mathbf{A}_{j}^{T}||_{F}^{2}(\\mathbf{A}_{i}%&#10;\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}\\boldsymbol{\\Psi}_{i}^{T}),\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b1</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udebd</mi><mi>j</mi></msub><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mrow><mn>4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udebf</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc00</mi><mi>j</mi><mi>T</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc00</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><msub><mi>\ud835\udebf</mi><mi>i</mi></msub></msub><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\eta>0$ is a step size parameter. The algorithm for solving\n(\\ref{eq:Approach III}) is summarized in Algorithm 2.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Design Approach II}}\n\n\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}$ $(i=1,\\ 2)$, $\\boldsymbol{\\Phi}_{i}^{(0)}$\n$(i=1,\\ 2)$, $\\alpha$, $\\beta$, $\\eta$, $t=0$.\n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$.\n\n1:\\textbf{ Repeat}\n\n2: \\hspace{1em}\\textbf{for} $i=1,\\ 2$ \\textbf{do}\n\n3:\\hspace{2em} $\\boldsymbol{\\Phi}_{i}^{(t+1)}=\\boldsymbol{\\Phi}_{i}^{(t)}-\\eta\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}},$\nwhere $\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}}$ is given\nby (\\ref{eq:gradient_2D});\n\n4:\\hspace{1em} \\textbf{end}\n\n5:\\hspace{1em} $t=t+1$;\n\n6:\\textbf{ Until} a stopping criteria is met.\n\n7:\\textbf{ }Normalization for $i=1,\\ 2$: $\\hat{\\boldsymbol{\\Phi}}_{i}=\\sqrt{N_{i}}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}$.\n\\end{algorithm}\n\n\nTill now, we have considered optimizing the multidimensional sensing\nmatrix when the sparsifying dictionaries for each tensor mode are\ngiven. For the purpose of joint optimization, we will proceed to optimize\nthe dictionaries by coupling fixed sensing matrices. The joint optimization\nwill eventually be achieved by alternatively optimizing the sensing\nmatrices and the sparsifying dictionaries. \n\n\n\\section{Jointly Learning The Multidimensional Dictionary and Sensing Matrix}\n\n\\label{sec:PsiDesign}\n\nIn this section, we first propose a sensing-matrix-coupled method\nfor multidimensional sparsifying dictionary learning. Then it is combined\nwith the previously introduced optimization approach for a multilinear\nsensing matrix to yield a joint optimization algorithm. In the spirit\nof the coupled KSVD method \\cite{Duarte2009}, our approach for dictionary\nlearning can be viewed as a sensing-matrix-coupled version of a tensor\nKSVD algorithm. We start by briefly introducing the coupled KSVD method.\n\n\n\\subsection{Coupled KSVD}\n\n\\label{sub:cKSVD and KHOSVD}\n\nThe Coupled KSVD (cKSVD) \\cite{Duarte2009} is a dictionary learning\napproach for vectorized signals. Let $\\mathbf{X}=\\left[\\begin{array}{ccc}\n\\mathbf{x}_{1} & ... & \\mathbf{x}_{T}\\end{array}\\right]$ be a $N\\times T$ matrix containing a training sequence of $T$ signals\n$\\mathbf{x}_{1},...,\\mathbf{x}_{T}$. The cKSVD aims to solve the\nfollowing problem, i.e., to learn a dictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{N\\times\\hat{N}}$\nfrom $\\mathbf{X}$:\n\n", "itemtype": "equation", "pos": 40557, "prevtext": "\nwhere $i,\\ j\\in\\{1,\\ 2\\}$ and $j\\neq i$, $\\mathbf{G}_{\\mathbf{A}_{i}}=\\mathbf{A}_{i}^{T}\\mathbf{A}_{i}$\nand $\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}=\\boldsymbol{\\Psi}_{i}^{T}\\boldsymbol{\\Psi}_{i}$. \n\nFor generality, we also provide the result for the $n>2$ case as\nfollows: \n\\begin{eqnarray}\n\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}} & = & 4\\omega_{i}(\\mathbf{A}_{i}\\mathbf{G}_{\\mathbf{A}_{i}}\\boldsymbol{\\Psi}_{i}^{T})-4\\beta\\theta_{i}(\\mathbf{A}_{i}\\boldsymbol{\\Psi}_{i}^{T})\\nonumber \\\\\n & + & 2\\alpha\\tau_{i}\\boldsymbol{\\Phi}_{i}+(4\\beta-4)\\rho_{i}(\\mathbf{A}_{i}\\mathbf{G}_{\\boldsymbol{\\Psi}_{i}}\\boldsymbol{\\Psi}_{i}^{T}),\n\\end{eqnarray}\nwhere $i,\\ j\\in\\{1,...,n\\}$ and $j\\neq i$, $\\omega_{i}=\\prod_{j}||\\mathbf{G}_{\\mathbf{A}_{j}}||_{F}^{2}$,\n$\\theta_{i}=\\prod_{j}||\\mathbf{A}_{j}||_{F}^{2}$, $\\tau_{i}=\\prod_{j}||\\boldsymbol{\\Phi}_{j}||_{F}^{2}$,\n$\\rho_{i}=\\prod_{j}||\\boldsymbol{\\Psi}_{j}\\mathbf{A}_{j}^{T}||_{F}^{2}$. \n\nWith the gradient obtained, we can solve (\\ref{eq:Approach III})\nby alternatively updating $\\boldsymbol{\\Phi}_{1}$ and $\\boldsymbol{\\Phi}_{2}$\nas follows: \n\n", "index": 47, "text": "\\begin{equation}\n\\boldsymbol{\\Phi}_{i}^{(t+1)}=\\boldsymbol{\\Phi}_{i}^{(t)}-\\eta\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\Phi}_{i}^{(t+1)}=\\boldsymbol{\\Phi}_{i}^{(t)}-\\eta\\frac{\\partial f%&#10;}{\\partial\\boldsymbol{\\Phi}_{i}},\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udebd</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>\ud835\udebd</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>f</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{S}=\\left[\\begin{array}{ccc}\n\\mathbf{s}_{1} & ... & \\mathbf{s}_{T}\\end{array}\\right]$ is the sparse representation with size $\\hat{N}\\times T$, $\\gamma>0$\nis a tuning parameter and $\\mathbf{Y}\\in\\mathbb{R}^{M\\times T}$ contains\nthe measurement vectors taken by the sensing matrix $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{M\\times N}$,\ni.e., $\\mathbf{Y}=\\left[\\begin{array}{ccc}\n\\mathbf{y}_{1} & ... & \\mathbf{y}_{T}\\end{array}\\right]$ and $\\mathbf{Y}=\\boldsymbol{\\Phi}\\mathbf{X}+\\mathbf{E}$ with $\\mathbf{E}\\in\\mathbb{R}^{M\\times T}$\nrepresenting the noise. Then the problem in (\\ref{eq:coupled KSVD})\nis reformatted as:\n\n", "itemtype": "equation", "pos": 43134, "prevtext": "\nwhere $\\eta>0$ is a step size parameter. The algorithm for solving\n(\\ref{eq:Approach III}) is summarized in Algorithm 2.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Design Approach II}}\n\n\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}$ $(i=1,\\ 2)$, $\\boldsymbol{\\Phi}_{i}^{(0)}$\n$(i=1,\\ 2)$, $\\alpha$, $\\beta$, $\\eta$, $t=0$.\n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$.\n\n1:\\textbf{ Repeat}\n\n2: \\hspace{1em}\\textbf{for} $i=1,\\ 2$ \\textbf{do}\n\n3:\\hspace{2em} $\\boldsymbol{\\Phi}_{i}^{(t+1)}=\\boldsymbol{\\Phi}_{i}^{(t)}-\\eta\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}},$\nwhere $\\frac{\\partial f}{\\partial\\boldsymbol{\\Phi}_{i}}$ is given\nby (\\ref{eq:gradient_2D});\n\n4:\\hspace{1em} \\textbf{end}\n\n5:\\hspace{1em} $t=t+1$;\n\n6:\\textbf{ Until} a stopping criteria is met.\n\n7:\\textbf{ }Normalization for $i=1,\\ 2$: $\\hat{\\boldsymbol{\\Phi}}_{i}=\\sqrt{N_{i}}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}$.\n\\end{algorithm}\n\n\nTill now, we have considered optimizing the multidimensional sensing\nmatrix when the sparsifying dictionaries for each tensor mode are\ngiven. For the purpose of joint optimization, we will proceed to optimize\nthe dictionaries by coupling fixed sensing matrices. The joint optimization\nwill eventually be achieved by alternatively optimizing the sensing\nmatrices and the sparsifying dictionaries. \n\n\n\\section{Jointly Learning The Multidimensional Dictionary and Sensing Matrix}\n\n\\label{sec:PsiDesign}\n\nIn this section, we first propose a sensing-matrix-coupled method\nfor multidimensional sparsifying dictionary learning. Then it is combined\nwith the previously introduced optimization approach for a multilinear\nsensing matrix to yield a joint optimization algorithm. In the spirit\nof the coupled KSVD method \\cite{Duarte2009}, our approach for dictionary\nlearning can be viewed as a sensing-matrix-coupled version of a tensor\nKSVD algorithm. We start by briefly introducing the coupled KSVD method.\n\n\n\\subsection{Coupled KSVD}\n\n\\label{sub:cKSVD and KHOSVD}\n\nThe Coupled KSVD (cKSVD) \\cite{Duarte2009} is a dictionary learning\napproach for vectorized signals. Let $\\mathbf{X}=\\left[\\begin{array}{ccc}\n\\mathbf{x}_{1} & ... & \\mathbf{x}_{T}\\end{array}\\right]$ be a $N\\times T$ matrix containing a training sequence of $T$ signals\n$\\mathbf{x}_{1},...,\\mathbf{x}_{T}$. The cKSVD aims to solve the\nfollowing problem, i.e., to learn a dictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{N\\times\\hat{N}}$\nfrom $\\mathbf{X}$:\n\n", "index": 49, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ \\gamma||\\mathbf{X}-\\boldsymbol{\\Psi}\\mathbf{S}||_{F}^{2}+||\\mathbf{Y}-\\boldsymbol{\\Phi\\Psi}\\mathbf{S}||_{F}^{2},\\ s.t.\\ \\forall i,\\ ||\\mathbf{s}_{i}||_{0}\\leq K,\\label{eq:coupled KSVD}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ \\gamma||\\mathbf{X}-\\boldsymbol{%&#10;\\Psi}\\mathbf{S}||_{F}^{2}+||\\mathbf{Y}-\\boldsymbol{\\Phi\\Psi}\\mathbf{S}||_{F}^{%&#10;2},\\ s.t.\\ \\forall i,\\ ||\\mathbf{s}_{i}||_{0}\\leq K,\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><mi>\ud835\udebf</mi><mo>,</mo><mi>\ud835\udc12</mi></mrow></munder></mpadded><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mrow><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\ud835\udc12</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc18</mi><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\ud835\udc12</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo rspace=\"7.5pt\">,</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc2c</mi><mi>i</mi></msub><mo fence=\"true\">||</mo></mrow><mn>0</mn></msub></mrow><mo>\u2264</mo><mi>K</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\mathbf{Z}=\\left[\\begin{array}{cc}\n\\gamma\\mathbf{X}^{T} & \\mathbf{Y}^{T}\\end{array}\\right]^{T}$, $\\mathbf{D}=\\left[\\begin{array}{cc}\n\\gamma\\mathbf{I}_{N} & \\boldsymbol{\\Phi}^{T}\\end{array}\\right]^{T}\\boldsymbol{\\Psi}$. The problem can then be solved following the conventional KSVD algorithm\n\\cite{Aharon2006} and conducting proper normalization. \n\nSpecifically, with an initial arbitrary $\\boldsymbol{\\Psi}$, it first\nrecovers $\\mathbf{S}$ using some available algorithms, e.g., OMP.\nThen the objective in (\\ref{eq:eqCoupled KSVD}) is rewritten as:\n\n", "itemtype": "equation", "pos": 44023, "prevtext": "\nwhere $\\mathbf{S}=\\left[\\begin{array}{ccc}\n\\mathbf{s}_{1} & ... & \\mathbf{s}_{T}\\end{array}\\right]$ is the sparse representation with size $\\hat{N}\\times T$, $\\gamma>0$\nis a tuning parameter and $\\mathbf{Y}\\in\\mathbb{R}^{M\\times T}$ contains\nthe measurement vectors taken by the sensing matrix $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{M\\times N}$,\ni.e., $\\mathbf{Y}=\\left[\\begin{array}{ccc}\n\\mathbf{y}_{1} & ... & \\mathbf{y}_{T}\\end{array}\\right]$ and $\\mathbf{Y}=\\boldsymbol{\\Phi}\\mathbf{X}+\\mathbf{E}$ with $\\mathbf{E}\\in\\mathbb{R}^{M\\times T}$\nrepresenting the noise. Then the problem in (\\ref{eq:coupled KSVD})\nis reformatted as:\n\n", "index": 51, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ ||\\mathbf{Z}-\\mathbf{D}\\mathbf{S}||_{F}^{2},\\ s.t.\\ \\forall i,\\ ||\\mathbf{s}_{i}||_{0}\\leq K,\\label{eq:eqCoupled KSVD}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ ||\\mathbf{Z}-\\mathbf{D}\\mathbf{S%&#10;}||_{F}^{2},\\ s.t.\\ \\forall i,\\ ||\\mathbf{s}_{i}||_{0}\\leq K,\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><mi>\ud835\udebf</mi><mo>,</mo><mi>\ud835\udc12</mi></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc19</mi><mo>-</mo><mi>\ud835\udc03\ud835\udc12</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo rspace=\"7.5pt\">,</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc2c</mi><mi>i</mi></msub><mo fence=\"true\">||</mo></mrow><mn>0</mn></msub></mrow><mo>\u2264</mo><mi>K</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $p$ is the index of the current atom we aim to update, $\\tilde{\\mathbf{s}}_{p}^{T}$\nis the row of $\\mathbf{S}$ where the zeros have been removed, $\\mathbf{R}_{p}=\\mathbf{Z}-\\sum_{q\\neq p}\\mathbf{d}_{q}\\mathbf{s}_{q}^{T}$\nand $\\tilde{\\mathbf{R}}_{p}$ denotes the columns of $\\mathbf{R}_{p}$\ncorresponding to $\\tilde{\\mathbf{s}}_{p}^{T}$. Let $\\tilde{\\mathbf{R}}_{p}=\\mathbf{U}_{\\mathbf{R}}\\boldsymbol{\\Lambda}_{\\mathbf{R}}\\mathbf{V}_{\\mathbf{R}}^{T}$\nbe a SVD of $\\tilde{\\mathbf{R}}_{p}$, then the highest component\nof the coupled error $\\tilde{\\mathbf{R}}_{p}$ can be eliminated by\ndefining: \n\n", "itemtype": "equation", "pos": 44777, "prevtext": "\nwhere $\\mathbf{Z}=\\left[\\begin{array}{cc}\n\\gamma\\mathbf{X}^{T} & \\mathbf{Y}^{T}\\end{array}\\right]^{T}$, $\\mathbf{D}=\\left[\\begin{array}{cc}\n\\gamma\\mathbf{I}_{N} & \\boldsymbol{\\Phi}^{T}\\end{array}\\right]^{T}\\boldsymbol{\\Psi}$. The problem can then be solved following the conventional KSVD algorithm\n\\cite{Aharon2006} and conducting proper normalization. \n\nSpecifically, with an initial arbitrary $\\boldsymbol{\\Psi}$, it first\nrecovers $\\mathbf{S}$ using some available algorithms, e.g., OMP.\nThen the objective in (\\ref{eq:eqCoupled KSVD}) is rewritten as:\n\n", "index": 53, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ ||\\tilde{\\mathbf{R}}_{p}-\\mathbf{d}_{p}\\tilde{\\mathbf{s}}_{p}^{T}||_{F}^{2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi},\\mathbf{S}}{min}\\ ||\\tilde{\\mathbf{R}}_{p}-\\mathbf%&#10;{d}_{p}\\tilde{\\mathbf{s}}_{p}^{T}||_{F}^{2},\" display=\"block\"><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><mi>\ud835\udebf</mi><mo>,</mo><mi>\ud835\udc12</mi></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc11</mi><mo stretchy=\"false\">~</mo></mover><mi>p</mi></msub><mo>-</mo><mrow><msub><mi>\ud835\udc1d</mi><mi>p</mi></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc2c</mi><mo stretchy=\"false\">~</mo></mover><mi>p</mi><mi>T</mi></msubsup></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\lambda_{\\mathbf{R}}^{1}$ is the largest singular value of\n$\\tilde{\\mathbf{R}}_{p}$ and $\\mathbf{u}_{\\mathbf{R}}^{1}$, $\\mathbf{v}_{\\mathbf{R}}^{1}$\nare the corresponding left and right singular vectors. The update\ncolumn $p$ of $\\boldsymbol{\\Psi}$ is obtained after normalization:\n$\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}=\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}/||\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}||_{2}$.\nThe above process is then iterated to update every atom of $\\boldsymbol{\\Psi}$. \n\nClearly the sensing matrix has been taken into account during the\ndictionary learning process, which has been shown to be beneficial\nfor CS reconstruction performance \\cite{Duarte2009}. In order to\nlearn multidimensional separable dictionaries for high dimensional\nsignals, and to achieve joint optimization of the multidimensional\ndictionary and sensing matrix, we will derive a coupled-KSVD algorithm\nfor a tensor, i.e., cTKSVD, in the following section. Again for simplicity\nwe will still describe the main flow for 2-D signals, i.e., $n=2$.\n\n\n\\subsection{The cTKSVD Approach}\n\nConsider a training sequence of 2-D signals $\\mathbf{X}_{1},...,\\mathbf{X}_{T}$,\nwe obtain a tensor $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times N_{2}\\times T}$\nby stacking them along the third dimension. Denoting the stack of\nthe sparse representations $\\mathbf{S}_{i}\\in\\mathbb{R}^{\\hat{N}_{1}\\times\\hat{N}_{2}},\\ (i=1,...,T)$\nby $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{\\hat{N}_{1}\\times\\hat{N}_{2}\\times T}$,\nwe propose the following optimization problem to learn the multidimensional\ndictionary:\n\n", "itemtype": "equation", "pos": 45531, "prevtext": "\nwhere $p$ is the index of the current atom we aim to update, $\\tilde{\\mathbf{s}}_{p}^{T}$\nis the row of $\\mathbf{S}$ where the zeros have been removed, $\\mathbf{R}_{p}=\\mathbf{Z}-\\sum_{q\\neq p}\\mathbf{d}_{q}\\mathbf{s}_{q}^{T}$\nand $\\tilde{\\mathbf{R}}_{p}$ denotes the columns of $\\mathbf{R}_{p}$\ncorresponding to $\\tilde{\\mathbf{s}}_{p}^{T}$. Let $\\tilde{\\mathbf{R}}_{p}=\\mathbf{U}_{\\mathbf{R}}\\boldsymbol{\\Lambda}_{\\mathbf{R}}\\mathbf{V}_{\\mathbf{R}}^{T}$\nbe a SVD of $\\tilde{\\mathbf{R}}_{p}$, then the highest component\nof the coupled error $\\tilde{\\mathbf{R}}_{p}$ can be eliminated by\ndefining: \n\n", "index": 55, "text": "\\begin{align}\n\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p} & =(\\gamma^{2}\\mathbf{I}_{N}+\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi})^{-1}\\left[\\begin{array}{cc}\n\\gamma\\mathbf{I}_{N} & \\boldsymbol{\\Phi}^{T}\\end{array}\\right]\\mathbf{u}_{\\mathbf{R}}^{1},\\\\\n\\tilde{\\mathbf{s}}_{p} & =||\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}||_{2}\\lambda_{\\mathbf{R}}^{1}\\mathbf{v}_{\\mathbf{R}}^{1},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}\" display=\"inline\"><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mi>p</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\gamma^{2}\\mathbf{I}_{N}+\\boldsymbol{\\Phi}^{T}\\boldsymbol{\\Phi}%&#10;)^{-1}\\left[\\begin{array}[]{cc}\\gamma\\mathbf{I}_{N}&amp;\\boldsymbol{\\Phi}^{T}\\end{%&#10;array}\\right]\\mathbf{u}_{\\mathbf{R}}^{1},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>N</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udebd</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udebd</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>N</mi></msub></mrow></mtd><mtd columnalign=\"center\"><msup><mi>\ud835\udebd</mi><mi>T</mi></msup></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc2e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{\\mathbf{s}}_{p}\" display=\"inline\"><msub><mover accent=\"true\"><mi>\ud835\udc2c</mi><mo stretchy=\"false\">~</mo></mover><mi>p</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=||\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}||_{2}\\lambda_{\\mathbf{R}}%&#10;^{1}\\mathbf{v}_{\\mathbf{R}}^{1},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mrow><mo fence=\"true\">||</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mi>p</mi></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>\u03bb</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc2f</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nin which \n\n", "itemtype": "equation", "pos": 47484, "prevtext": "\nwhere $\\lambda_{\\mathbf{R}}^{1}$ is the largest singular value of\n$\\tilde{\\mathbf{R}}_{p}$ and $\\mathbf{u}_{\\mathbf{R}}^{1}$, $\\mathbf{v}_{\\mathbf{R}}^{1}$\nare the corresponding left and right singular vectors. The update\ncolumn $p$ of $\\boldsymbol{\\Psi}$ is obtained after normalization:\n$\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}=\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}/||\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{p}||_{2}$.\nThe above process is then iterated to update every atom of $\\boldsymbol{\\Psi}$. \n\nClearly the sensing matrix has been taken into account during the\ndictionary learning process, which has been shown to be beneficial\nfor CS reconstruction performance \\cite{Duarte2009}. In order to\nlearn multidimensional separable dictionaries for high dimensional\nsignals, and to achieve joint optimization of the multidimensional\ndictionary and sensing matrix, we will derive a coupled-KSVD algorithm\nfor a tensor, i.e., cTKSVD, in the following section. Again for simplicity\nwe will still describe the main flow for 2-D signals, i.e., $n=2$.\n\n\n\\subsection{The cTKSVD Approach}\n\nConsider a training sequence of 2-D signals $\\mathbf{X}_{1},...,\\mathbf{X}_{T}$,\nwe obtain a tensor $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times N_{2}\\times T}$\nby stacking them along the third dimension. Denoting the stack of\nthe sparse representations $\\mathbf{S}_{i}\\in\\mathbb{R}^{\\hat{N}_{1}\\times\\hat{N}_{2}},\\ (i=1,...,T)$\nby $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{\\hat{N}_{1}\\times\\hat{N}_{2}\\times T}$,\nwe propose the following optimization problem to learn the multidimensional\ndictionary:\n\n", "index": 57, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\mathbf{\\underline{S}}}{min}\\ ||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{D}_{1}\\times_{2}\\mathbf{D}_{2}||_{F}^{2},\\ s.t.,\\ \\forall i,\\ ||\\mathbf{S}_{i}||_{0}\\leq K,\\label{eq:Coupled K-HOSVD}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\mathbf{\\underline{S}}}{%&#10;min}\\ ||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{D}_{1}%&#10;\\times_{2}\\mathbf{D}_{2}||_{F}^{2},\\ s.t.,\\ \\forall i,\\ ||\\mathbf{S}_{i}||_{0}%&#10;\\leq K,\" display=\"block\"><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebf</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebf</mi><mn>2</mn></msub><mo>,</mo><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder></mrow></munder></mpadded><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><munder accentunder=\"true\"><mi>\ud835\udc19</mi><mo>\u00af</mo></munder><mo>-</mo><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udc03</mi><mn>1</mn></msub><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udc03</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mo stretchy=\"false\">|</mo><mi>F</mi><mn>2</mn></msubsup><mo rspace=\"7.5pt\">,</mo><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo><mo rspace=\"7.5pt\">,</mo><mo>\u2200</mo><mi>i</mi><mo rspace=\"7.5pt\">,</mo><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc12</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mo stretchy=\"false\">|</mo><mn>0</mn></msub><mo>\u2264</mo><mi>K</mi><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nand $\\gamma>0$ is a tuning parameter.\n\nThe problem in (\\ref{eq:Coupled K-HOSVD}) aims to minimize the representation\nerror $||\\underline{\\mathbf{X}}-\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}||_{F}^{2}$\nand the overall projection error $||\\underline{\\mathbf{Y}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{A}_{1}\\times_{2}\\mathbf{A}_{2}||_{F}^{2}$\nwith constraints on the sparsity of each slice of the tensor. In addition,\nit also takes into account the projection errors induced by $\\boldsymbol{\\Phi}_{1}$\nand $\\boldsymbol{\\Phi}_{2}$ individually. \n\nUsing an available sparse reconstruction algorithm for the TCS, e.g.,\nTensor OMP (TOMP) \\cite{Caiafa:2013}, and initial dictionaries $\\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$,\nthe sparse representation $\\underline{\\mathbf{S}}$ can be estimated\nfirst. Then we update the multilinear dictionary alternately. We first\nupdate the atoms of $\\boldsymbol{\\Psi}_{1}$ with $\\boldsymbol{\\Psi}_{2}$\nfixed. The objective in (\\ref{eq:Coupled K-HOSVD}) is rewritten as:\n\n", "itemtype": "equation", "pos": 47793, "prevtext": "\nin which \n\n", "index": 59, "text": "\\begin{align}\n\\underline{\\mathbf{Z}} & =\\left[\\begin{array}{cc}\n\\gamma^{2}\\underline{\\mathbf{X}} & \\gamma\\underline{\\mathbf{Y}}_{2}\\\\\n\\gamma\\underline{\\mathbf{Y}}_{1} & \\underline{\\mathbf{Y}}\n\\end{array}\\right],\\ \\underline{\\mathbf{Y}}_{i}=\\underline{\\mathbf{X}}\\times_{i}\\boldsymbol{\\Phi}_{i}+\\underline{\\mathbf{E}}_{i},\\\\\n\\mathbf{D}_{1} & =\\left[\\begin{array}{c}\n\\gamma\\mathbf{I}_{\\hat{N}_{1}}\\\\\n\\boldsymbol{\\Phi}_{1}\n\\end{array}\\right]\\boldsymbol{\\Psi}_{1},\\ \\mathbf{D}_{2}=\\left[\\begin{array}{c}\n\\gamma\\mathbf{I}_{\\hat{N}_{2}}\\\\\n\\boldsymbol{\\Phi}_{2}\n\\end{array}\\right]\\boldsymbol{\\Psi}_{2},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underline{\\mathbf{Z}}\" display=\"inline\"><munder accentunder=\"true\"><mi>\ud835\udc19</mi><mo>\u00af</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left[\\begin{array}[]{cc}\\gamma^{2}\\underline{\\mathbf{X}}&amp;\\gamma%&#10;\\underline{\\mathbf{Y}}_{2}\\\\&#10;\\gamma\\underline{\\mathbf{Y}}_{1}&amp;\\underline{\\mathbf{Y}}\\end{array}\\right],\\ %&#10;\\underline{\\mathbf{Y}}_{i}=\\underline{\\mathbf{X}}\\times_{i}\\boldsymbol{\\Phi}_{%&#10;i}+\\underline{\\mathbf{E}}_{i},\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder></mrow></mtd><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mi>i</mi></msub><mo>=</mo><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mi>i</mi></msub><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc04</mi><mo>\u00af</mo></munder><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{D}_{1}\" display=\"inline\"><msub><mi>\ud835\udc03</mi><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left[\\begin{array}[]{c}\\gamma\\mathbf{I}_{\\hat{N}_{1}}\\\\&#10;\\boldsymbol{\\Phi}_{1}\\end{array}\\right]\\boldsymbol{\\Psi}_{1},\\ \\mathbf{D}_{2}=%&#10;\\left[\\begin{array}[]{c}\\gamma\\mathbf{I}_{\\hat{N}_{2}}\\\\&#10;\\boldsymbol{\\Phi}_{2}\\end{array}\\right]\\boldsymbol{\\Psi}_{2},\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><mi>\ud835\udc03</mi><mn>2</mn></msub><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mn>2</mn></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\underline{\\mathbf{R}}_{p_{1}}=\\underline{\\mathbf{Z}}-\\sum_{q_{1}\\neq p_{1}}\\sum_{q_{2}}(\\mathbf{d}_{1})_{q_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(q_{1}-1)\\hat{N}_{2}+q_{2}}$;\n$p_{1}$ is the index of the atom for the current update and $q_{1},\\ q_{2}$\ndenote the indices of the remaining atoms of $\\boldsymbol{\\Psi}_{1}$\nand all the atoms of $\\boldsymbol{\\Psi}_{2}$, respectively; $\\mathbf{d}_{1}$,\n$\\mathbf{d}_{2}$ are columns of $\\mathbf{D}_{1}$, $\\mathbf{D}_{2}$;\n$\\mathbf{s}$ is the mode-3 vector of $\\underline{\\mathbf{S}}$. Then\nto satisfy the sparsity constraint in (\\ref{eq:Coupled K-HOSVD}),\nwe only keep the non-zero entries of $\\mathbf{s}_{(p_{1}-1)\\hat{N}_{2}+q_{2}}$\nand the corresponding subset of $\\underline{\\mathbf{R}}_{p_{1}}$\nto obtain: \n\n", "itemtype": "equation", "pos": 49452, "prevtext": "\nand $\\gamma>0$ is a tuning parameter.\n\nThe problem in (\\ref{eq:Coupled K-HOSVD}) aims to minimize the representation\nerror $||\\underline{\\mathbf{X}}-\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}||_{F}^{2}$\nand the overall projection error $||\\underline{\\mathbf{Y}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{A}_{1}\\times_{2}\\mathbf{A}_{2}||_{F}^{2}$\nwith constraints on the sparsity of each slice of the tensor. In addition,\nit also takes into account the projection errors induced by $\\boldsymbol{\\Phi}_{1}$\nand $\\boldsymbol{\\Phi}_{2}$ individually. \n\nUsing an available sparse reconstruction algorithm for the TCS, e.g.,\nTensor OMP (TOMP) \\cite{Caiafa:2013}, and initial dictionaries $\\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$,\nthe sparse representation $\\underline{\\mathbf{S}}$ can be estimated\nfirst. Then we update the multilinear dictionary alternately. We first\nupdate the atoms of $\\boldsymbol{\\Psi}_{1}$ with $\\boldsymbol{\\Psi}_{2}$\nfixed. The objective in (\\ref{eq:Coupled K-HOSVD}) is rewritten as:\n\n", "index": 61, "text": "\\begin{equation}\n||\\underline{\\mathbf{R}}_{p_{1}}-\\sum_{q_{2}}(\\mathbf{d}_{1})_{p_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(p_{1}-1)\\hat{N}_{2}+q_{2}}||_{F}^{2},\\label{eq:ErrorKHOSVD}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"||\\underline{\\mathbf{R}}_{p_{1}}-\\sum_{q_{2}}(\\mathbf{d}_{1})_{p_{1}}\\circ(%&#10;\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(p_{1}-1)\\hat{N}_{2}+q_{2}}||_{F}^{2},\" display=\"block\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><munder accentunder=\"true\"><mi>\ud835\udc11</mi><mo>\u00af</mo></munder><msub><mi>p</mi><mn>1</mn></msub></msub><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msub><mi>q</mi><mn>2</mn></msub></munder><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><mo>\u2218</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>q</mi><mn>2</mn></msub></msub><mo>\u2218</mo><msub><mi>\ud835\udc2c</mi><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mo>+</mo><msub><mi>q</mi><mn>2</mn></msub></mrow></msub></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\n\n\nAssuming that after carrying out a Higher Order SVD (HOSVD) \\cite{De2000}\nfor $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$, the largest singular\nvalue is $\\lambda_{\\mathbf{R}}^{1}$ and the corresponding singular\nvectors are $\\mathbf{u}_{\\mathbf{R}}^{1}$, $\\mathbf{v}_{\\mathbf{R}}^{1}$\nand $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$, we eliminate the largest\nerror by: \n\n", "itemtype": "equation", "pos": 50436, "prevtext": "\nwhere $\\underline{\\mathbf{R}}_{p_{1}}=\\underline{\\mathbf{Z}}-\\sum_{q_{1}\\neq p_{1}}\\sum_{q_{2}}(\\mathbf{d}_{1})_{q_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(q_{1}-1)\\hat{N}_{2}+q_{2}}$;\n$p_{1}$ is the index of the atom for the current update and $q_{1},\\ q_{2}$\ndenote the indices of the remaining atoms of $\\boldsymbol{\\Psi}_{1}$\nand all the atoms of $\\boldsymbol{\\Psi}_{2}$, respectively; $\\mathbf{d}_{1}$,\n$\\mathbf{d}_{2}$ are columns of $\\mathbf{D}_{1}$, $\\mathbf{D}_{2}$;\n$\\mathbf{s}$ is the mode-3 vector of $\\underline{\\mathbf{S}}$. Then\nto satisfy the sparsity constraint in (\\ref{eq:Coupled K-HOSVD}),\nwe only keep the non-zero entries of $\\mathbf{s}_{(p_{1}-1)\\hat{N}_{2}+q_{2}}$\nand the corresponding subset of $\\underline{\\mathbf{R}}_{p_{1}}$\nto obtain: \n\n", "index": 63, "text": "\\begin{equation}\n||\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}-\\sum_{q_{2}}(\\mathbf{d}_{1})_{p_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{\\tilde{s}}_{(p_{1}-1)\\hat{N}_{2}+q_{2}}||_{F}^{2}.\\label{eq:ErrorKHOSVD2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"||\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}-\\sum_{q_{2}}(\\mathbf{d}_{1})_{p_{1}}%&#10;\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{\\tilde{s}}_{(p_{1}-1)\\hat{N}_{2}+q_{%&#10;2}}||_{F}^{2}.\" display=\"block\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><munderover accent=\"true\" accentunder=\"true\"><mi>\ud835\udc11</mi><mo mathsize=\"142%\" stretchy=\"false\">\u00af</mo><mo stretchy=\"false\">~</mo></munderover><msub><mi>p</mi><mn>1</mn></msub></msub><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msub><mi>q</mi><mn>2</mn></msub></munder><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><mo>\u2218</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>q</mi><mn>2</mn></msub></msub><mo>\u2218</mo><msub><mover accent=\"true\"><mi>\ud835\udc2c</mi><mo stretchy=\"false\">~</mo></mover><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mo>+</mo><msub><mi>q</mi><mn>2</mn></msub></mrow></msub></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\tilde{\\mathbf{S}}_{p_{1},:,:}$ denotes the horizontal slice\nof $\\underline{\\mathbf{S}}$ at index $p_{1}$ that contains only\nnon-zero mode-2 vectors. The atom of $\\boldsymbol{\\Psi}_{1}$ is then\ncalculated using the pseudo-inverse as: \n\n", "itemtype": "equation", "pos": 51024, "prevtext": "\n\n\nAssuming that after carrying out a Higher Order SVD (HOSVD) \\cite{De2000}\nfor $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$, the largest singular\nvalue is $\\lambda_{\\mathbf{R}}^{1}$ and the corresponding singular\nvectors are $\\mathbf{u}_{\\mathbf{R}}^{1}$, $\\mathbf{v}_{\\mathbf{R}}^{1}$\nand $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$, we eliminate the largest\nerror by: \n\n", "index": 65, "text": "\\begin{equation}\n(\\hat{\\mathbf{d}}_{1})_{p_{1}}=\\mathbf{u}_{\\mathbf{R}}^{1},\\ \\mathbf{D}_{2}\\tilde{\\mathbf{S}}_{p_{1},:,:}=\\mathbf{v}_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}),\\label{eq:update_Middle}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m1\" class=\"ltx_Math\" alttext=\"(\\hat{\\mathbf{d}}_{1})_{p_{1}}=\\mathbf{u}_{\\mathbf{R}}^{1},\\ \\mathbf{D}_{2}%&#10;\\tilde{\\mathbf{S}}_{p_{1},:,:}=\\mathbf{v}_{\\mathbf{R}}^{1}\\circ(\\lambda_{%&#10;\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}),\" display=\"block\"><mrow><mrow><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><mo>=</mo><msubsup><mi>\ud835\udc2e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><msub><mi>\ud835\udc03</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">~</mo></mover><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mo>:</mo><mo>,</mo><mo>:</mo></mrow></msub></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc2f</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2218</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bb</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udf4e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nThe current update is then obtained after normalization: \n\n", "itemtype": "equation", "pos": 51524, "prevtext": "\nwhere $\\tilde{\\mathbf{S}}_{p_{1},:,:}$ denotes the horizontal slice\nof $\\underline{\\mathbf{S}}$ at index $p_{1}$ that contains only\nnon-zero mode-2 vectors. The atom of $\\boldsymbol{\\Psi}_{1}$ is then\ncalculated using the pseudo-inverse as: \n\n", "index": 67, "text": "\\begin{align}\n(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}} & =(\\gamma^{2}\\mathbf{I}_{N_{1}}+\\boldsymbol{\\Phi}_{1}^{T}\\boldsymbol{\\Phi}_{1})^{-1}\\left[\\begin{array}{cc}\n\\gamma\\mathbf{I}_{N_{1}} & \\boldsymbol{\\Phi}_{1}^{T}\\end{array}\\right]\\mathbf{u}_{\\mathbf{R}}^{1}.\\label{eq:updatePsi}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\gamma^{2}\\mathbf{I}_{N_{1}}+\\boldsymbol{\\Phi}_{1}^{T}%&#10;\\boldsymbol{\\Phi}_{1})^{-1}\\left[\\begin{array}[]{cc}\\gamma\\mathbf{I}_{N_{1}}&amp;%&#10;\\boldsymbol{\\Phi}_{1}^{T}\\end{array}\\right]\\mathbf{u}_{\\mathbf{R}}^{1}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mi>N</mi><mn>1</mn></msub></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udebd</mi><mn>1</mn><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mi>N</mi><mn>1</mn></msub></msub></mrow></mtd><mtd columnalign=\"center\"><msubsup><mi>\ud835\udebd</mi><mn>1</mn><mi>T</mi></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc2e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\n Since $\\mathbf{D}_{2}$ and the support indices of each mode-2 vector\nin $\\tilde{\\mathbf{S}}_{p_{1},:,:}$ are known, the updated coefficients\n$\\tilde{\\mathbf{S}}_{p_{1},:,:}$ can be easily calculated by the\nLeast Square (LS) solution. The above process is repeated for all\nthe atoms to update the dictionary $\\boldsymbol{\\Psi}_{1}$.\n\nThe next step is to update $\\boldsymbol{\\Psi}_{2}$ with the obtained\n$\\boldsymbol{\\Psi}_{1}$ fixed. It follows a similar procedure to\nthat described previously. Specifically, the objective in (\\ref{eq:Coupled K-HOSVD})\nis rewritten as: \n\n", "itemtype": "equation", "pos": 51883, "prevtext": "\nThe current update is then obtained after normalization: \n\n", "index": 69, "text": "\\begin{align}\n(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}} & =\\frac{(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}}{||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}||_{2}},\\\\\n\\mathbf{D}_{2}\\tilde{\\mathbf{S}}_{p_{1},:,:} & =||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}||_{2}\\mathbf{v}_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}).\\label{eq:UpdateS}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E38.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E38.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}}{||(\\mathbf%&#10;{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}||_{2}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><msub><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{D}_{2}\\tilde{\\mathbf{S}}_{p_{1},:,:}\" display=\"inline\"><mrow><msub><mi>\ud835\udc03</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">~</mo></mover><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mo>:</mo><mo>,</mo><mo>:</mo></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1})_{p_{1}}||_{2}\\mathbf{v}%&#10;_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}%&#10;}^{1}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msub><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>1</mn></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc2f</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo>\u2218</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bb</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udf4e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nwhere $\\tilde{\\mathbf{s}}$ is the mode-3 vector with only non-zero\nentries, $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$ is the corresponding\nsubset of $\\underline{\\mathbf{R}}_{p_{2}}$, $\\underline{\\mathbf{R}}_{p_{2}}=\\underline{\\mathbf{Z}}-\\sum_{q_{1}}\\sum_{q_{2}\\neq p_{2}}(\\mathbf{d}_{1})_{q_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(q_{1}-1)\\hat{N}_{2}+q_{2}}$\nand $p_{2}$ is the index of the atom for current update. A HOSVD\nis carried out for $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$ and the\nupdate steps corresponding to (\\ref{eq:update_Middle}) - (\\ref{eq:UpdateS})\nnow become: \n\n", "itemtype": "equation", "pos": 52859, "prevtext": "\n Since $\\mathbf{D}_{2}$ and the support indices of each mode-2 vector\nin $\\tilde{\\mathbf{S}}_{p_{1},:,:}$ are known, the updated coefficients\n$\\tilde{\\mathbf{S}}_{p_{1},:,:}$ can be easily calculated by the\nLeast Square (LS) solution. The above process is repeated for all\nthe atoms to update the dictionary $\\boldsymbol{\\Psi}_{1}$.\n\nThe next step is to update $\\boldsymbol{\\Psi}_{2}$ with the obtained\n$\\boldsymbol{\\Psi}_{1}$ fixed. It follows a similar procedure to\nthat described previously. Specifically, the objective in (\\ref{eq:Coupled K-HOSVD})\nis rewritten as: \n\n", "index": 71, "text": "\\begin{equation}\n||\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}-\\sum_{q_{1}}(\\mathbf{d}_{1})_{q_{1}}\\circ(\\mathbf{d}_{2})_{p_{2}}\\circ\\mathbf{\\tilde{s}}_{(q_{1}-1)\\hat{N}_{2}+p_{2}}||_{F}^{2},\\label{eq:ErrorKHOSVD_Psi2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E40.m1\" class=\"ltx_Math\" alttext=\"||\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}-\\sum_{q_{1}}(\\mathbf{d}_{1})_{q_{1}}%&#10;\\circ(\\mathbf{d}_{2})_{p_{2}}\\circ\\mathbf{\\tilde{s}}_{(q_{1}-1)\\hat{N}_{2}+p_{%&#10;2}}||_{F}^{2},\" display=\"block\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><munderover accent=\"true\" accentunder=\"true\"><mi>\ud835\udc11</mi><mo mathsize=\"142%\" stretchy=\"false\">\u00af</mo><mo stretchy=\"false\">~</mo></munderover><msub><mi>p</mi><mn>2</mn></msub></msub><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msub><mi>q</mi><mn>1</mn></msub></munder><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>q</mi><mn>1</mn></msub></msub><mo>\u2218</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub><mo>\u2218</mo><msub><mover accent=\"true\"><mi>\ud835\udc2c</mi><mo stretchy=\"false\">~</mo></mover><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub></mrow></msub></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nin which $\\tilde{\\mathbf{S}}_{:,p_{2},:}$ represents the lateral\nslice at index $p_{2}$ and its updated elements can also be calculated\nusing LS. The dictionary $\\boldsymbol{\\Psi}_{2}$ is then updated\niteratively. The whole process of updating $\\underline{\\mathbf{S}},\\ \\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$\nis repeated to obtain the final solution of (\\ref{eq:Coupled K-HOSVD}).\n\nThe uncoupled version of the proposed cTKSVD method (denoted by TKSVD)\ncan be easily obtained by modifying the problem in (\\ref{eq:Coupled K-HOSVD})\nto: \n\n", "itemtype": "equation", "pos": 53680, "prevtext": "\nwhere $\\tilde{\\mathbf{s}}$ is the mode-3 vector with only non-zero\nentries, $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$ is the corresponding\nsubset of $\\underline{\\mathbf{R}}_{p_{2}}$, $\\underline{\\mathbf{R}}_{p_{2}}=\\underline{\\mathbf{Z}}-\\sum_{q_{1}}\\sum_{q_{2}\\neq p_{2}}(\\mathbf{d}_{1})_{q_{1}}\\circ(\\mathbf{d}_{2})_{q_{2}}\\circ\\mathbf{s}_{(q_{1}-1)\\hat{N}_{2}+q_{2}}$\nand $p_{2}$ is the index of the atom for current update. A HOSVD\nis carried out for $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$ and the\nupdate steps corresponding to (\\ref{eq:update_Middle}) - (\\ref{eq:UpdateS})\nnow become: \n\n", "index": 73, "text": "\\begin{align}\n(\\hat{\\mathbf{d}}_{2})_{p_{2}} & =\\mathbf{v}_{\\mathbf{R}}^{1},\\ \\mathbf{D}_{1}\\tilde{\\mathbf{S}}_{:,p_{2},:}=\\mathbf{u}_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}),\\\\\n(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}} & =(\\gamma^{2}\\mathbf{I}_{N_{2}}+\\boldsymbol{\\Phi}_{2}^{T}\\boldsymbol{\\Phi}_{2})^{-1}\\left[\\begin{array}{cc}\n\\gamma\\mathbf{I}_{N_{2}} & \\boldsymbol{\\Phi}_{2}^{T}\\end{array}\\right]\\mathbf{v}_{\\mathbf{R}}^{1},\\label{eq:UpdatePsi2}\\\\\n(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}} & =(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}/||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}||_{2},\\\\\n\\mathbf{D}_{1}\\tilde{\\mathbf{S}}_{:,p_{2},:} & =||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}||_{2}\\mathbf{u}_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}),\\label{eq:UpdateS2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E41.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\hat{\\mathbf{d}}_{2})_{p_{2}}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E41.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathbf{v}_{\\mathbf{R}}^{1},\\ \\mathbf{D}_{1}\\tilde{\\mathbf{S}}_{%&#10;:,p_{2},:}=\\mathbf{u}_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}%&#10;\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}),\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><msubsup><mi>\ud835\udc2f</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><msub><mi>\ud835\udc03</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">~</mo></mover><mrow><mo>:</mo><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mo>:</mo></mrow></msub></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc2e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2218</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bb</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udf4e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E42.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E42.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\gamma^{2}\\mathbf{I}_{N_{2}}+\\boldsymbol{\\Phi}_{2}^{T}%&#10;\\boldsymbol{\\Phi}_{2})^{-1}\\left[\\begin{array}[]{cc}\\gamma\\mathbf{I}_{N_{2}}&amp;%&#10;\\boldsymbol{\\Phi}_{2}^{T}\\end{array}\\right]\\mathbf{v}_{\\mathbf{R}}^{1},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mi>N</mi><mn>2</mn></msub></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udebd</mi><mn>2</mn><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mi>N</mi><mn>2</mn></msub></msub></mrow></mtd><mtd columnalign=\"center\"><msubsup><mi>\ud835\udebd</mi><mn>2</mn><mi>T</mi></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc2f</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E43.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E43.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}/||(\\mathbf{\\hat{%&#10;\\boldsymbol{\\psi}}}_{2})_{p_{2}}||_{2},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub><mo>/</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E44.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{D}_{1}\\tilde{\\mathbf{S}}_{:,p_{2},:}\" display=\"inline\"><mrow><msub><mi>\ud835\udc03</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">~</mo></mover><mrow><mo>:</mo><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mo>:</mo></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E44.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=||(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2})_{p_{2}}||_{2}\\mathbf{u}%&#10;_{\\mathbf{R}}^{1}\\circ(\\lambda_{\\mathbf{R}}^{1}\\boldsymbol{\\omega}_{\\mathbf{R}%&#10;}^{1}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msub><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udf4d</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mn>2</mn></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc2e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo>\u2218</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bb</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udf4e</mi><mi>\ud835\udc11</mi><mn>1</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nand it can be solved following the same procedures as described previously\nfor cTKSVD except that the steps of pseudo-inverse and normalization\nare no longer needed. \n\nThe proposed cTKSVD for multidimensional dictionary learning is different\nto the KHOSVD method \\cite{Roemer2014}, i.e., another tensor-based\ndictionary learning approach obtained by extending the KSVD method.\nThe learning process of KHOSVD follows the same train of thought as\nwith the conventional KSVD method, except that to eliminate the largest\nerror in each iteration, a HOSVD \\cite{De2000}, i.e., SVD for tensors,\nis employed. However, the process of KHOSVD does not take full advantage\nof the multilinear structure and involves duplicated updating of the\natoms, which leads to a slow convergence speed. The proposed cTKSVD\napproach is distinct from KHOSVD in the following respects. First,\nduring the update of each atom, a slice of the coefficient is updated\naccordingly in cTKSVD; while only a vector is updated in KHOSVD. Therefore,\nin cTKSVD, each iteration of the outer loop contains $\\hat{N}_{1}+\\hat{N}_{2}$\ninner iterations, which is $\\hat{N}_{1}\\hat{N}_{2}$ for KHOSVD (and\nfor KSVD). It means that cTKSVD requires HOSVD to be executed $\\hat{N}_{1}\\hat{N}_{2}-\\hat{N}_{1}-\\hat{N}_{2}$\nfewer times than for the KHOSVD method and hence reduces the complexity.\nIn addition, KHOSVD does not take into account the influence from\nthe sensing matrix. The benefit of coupling of the sensing matrices\nin cTKSVD will be shown by simulations in Section \\ref{sub:Simulation_Psi}.\n\nHere, we also provide the problem formulation when one needs to learn\n3-D sparsifying dictionaries. The cTKSVD for cases where $n>2$ can\nbe modeled following a similar strategy. For a training sequence consisting\nof $T$ stacked 3-D signals $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times N_{2}\\times N_{3}\\times T}$,\nwe learn the dictionaries by solving: \n\n", "itemtype": "equation", "pos": 55107, "prevtext": "\nin which $\\tilde{\\mathbf{S}}_{:,p_{2},:}$ represents the lateral\nslice at index $p_{2}$ and its updated elements can also be calculated\nusing LS. The dictionary $\\boldsymbol{\\Psi}_{2}$ is then updated\niteratively. The whole process of updating $\\underline{\\mathbf{S}},\\ \\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$\nis repeated to obtain the final solution of (\\ref{eq:Coupled K-HOSVD}).\n\nThe uncoupled version of the proposed cTKSVD method (denoted by TKSVD)\ncan be easily obtained by modifying the problem in (\\ref{eq:Coupled K-HOSVD})\nto: \n\n", "index": 75, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\mathbf{\\underline{S}}}{min}\\ ||\\underline{\\mathbf{X}}-\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}||_{F}^{2},\\ s.t.\\ \\forall i,\\ ||\\mathbf{S}_{i}||_{0}\\leq K,\\label{eq:KHOSVD obj}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E45.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\mathbf{\\underline{S}}}{%&#10;min}\\ ||\\underline{\\mathbf{X}}-\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{%&#10;\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}||_{F}^{2},\\ s.t.\\ \\forall i,\\ ||%&#10;\\mathbf{S}_{i}||_{0}\\leq K,\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebf</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebf</mi><mn>2</mn></msub><mo>,</mo><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><mo>-</mo><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udebf</mi><mn>2</mn></msub></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo rspace=\"7.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo rspace=\"7.5pt\">,</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc12</mi><mi>i</mi></msub><mo fence=\"true\">||</mo></mrow><mn>0</mn></msub></mrow><mo>\u2264</mo><mi>K</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nin which \n\n", "itemtype": "equation", "pos": 57325, "prevtext": "\nand it can be solved following the same procedures as described previously\nfor cTKSVD except that the steps of pseudo-inverse and normalization\nare no longer needed. \n\nThe proposed cTKSVD for multidimensional dictionary learning is different\nto the KHOSVD method \\cite{Roemer2014}, i.e., another tensor-based\ndictionary learning approach obtained by extending the KSVD method.\nThe learning process of KHOSVD follows the same train of thought as\nwith the conventional KSVD method, except that to eliminate the largest\nerror in each iteration, a HOSVD \\cite{De2000}, i.e., SVD for tensors,\nis employed. However, the process of KHOSVD does not take full advantage\nof the multilinear structure and involves duplicated updating of the\natoms, which leads to a slow convergence speed. The proposed cTKSVD\napproach is distinct from KHOSVD in the following respects. First,\nduring the update of each atom, a slice of the coefficient is updated\naccordingly in cTKSVD; while only a vector is updated in KHOSVD. Therefore,\nin cTKSVD, each iteration of the outer loop contains $\\hat{N}_{1}+\\hat{N}_{2}$\ninner iterations, which is $\\hat{N}_{1}\\hat{N}_{2}$ for KHOSVD (and\nfor KSVD). It means that cTKSVD requires HOSVD to be executed $\\hat{N}_{1}\\hat{N}_{2}-\\hat{N}_{1}-\\hat{N}_{2}$\nfewer times than for the KHOSVD method and hence reduces the complexity.\nIn addition, KHOSVD does not take into account the influence from\nthe sensing matrix. The benefit of coupling of the sensing matrices\nin cTKSVD will be shown by simulations in Section \\ref{sub:Simulation_Psi}.\n\nHere, we also provide the problem formulation when one needs to learn\n3-D sparsifying dictionaries. The cTKSVD for cases where $n>2$ can\nbe modeled following a similar strategy. For a training sequence consisting\nof $T$ stacked 3-D signals $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_{1}\\times N_{2}\\times N_{3}\\times T}$,\nwe learn the dictionaries by solving: \n\n", "index": 77, "text": "\\begin{equation}\n\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\boldsymbol{\\Psi}_{3},\\mathbf{\\underline{S}}}{min}\\ ||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{D}_{1}\\times_{2}\\mathbf{D}_{2}\\times_{3}\\mathbf{D}_{3}||_{F}^{2},\\ s.t.,\\ \\forall i,\\ ||\\mathbf{\\underline{S}}_{i}||_{0}\\leq K,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E46.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\boldsymbol{\\Psi}_{1},\\boldsymbol{\\Psi}_{2},\\boldsymbol{\\Psi}_{3},%&#10;\\mathbf{\\underline{S}}}{min}\\ ||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}%&#10;\\times_{1}\\mathbf{D}_{1}\\times_{2}\\mathbf{D}_{2}\\times_{3}\\mathbf{D}_{3}||_{F}%&#10;^{2},\\ s.t.,\\ \\forall i,\\ ||\\mathbf{\\underline{S}}_{i}||_{0}\\leq K,\" display=\"block\"><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><msub><mi>\ud835\udebf</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udebf</mi><mn>2</mn></msub><mo>,</mo><msub><mi>\ud835\udebf</mi><mn>3</mn></msub><mo>,</mo><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder></mrow></munder></mpadded><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><munder accentunder=\"true\"><mi>\ud835\udc19</mi><mo>\u00af</mo></munder><mo>-</mo><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>\ud835\udc03</mi><mn>1</mn></msub><msub><mo>\u00d7</mo><mn>2</mn></msub><msub><mi>\ud835\udc03</mi><mn>2</mn></msub><msub><mo>\u00d7</mo><mn>3</mn></msub><msub><mi>\ud835\udc03</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mo stretchy=\"false\">|</mo><mi>F</mi><mn>2</mn></msubsup><mo rspace=\"7.5pt\">,</mo><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo><mo rspace=\"7.5pt\">,</mo><mo>\u2200</mo><mi>i</mi><mo rspace=\"7.5pt\">,</mo><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc12</mi><mo>\u00af</mo></munder><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mo stretchy=\"false\">|</mo><mn>0</mn></msub><mo>\u2264</mo><mi>K</mi><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nand if we denote the operator ``$\\nearrow_{3}$'' as stacking tensors\nalong their third mode, then in the above formulation of $\\underline{\\mathbf{Z}}$,\n\n", "itemtype": "equation", "pos": 57666, "prevtext": "\nin which \n\n", "index": 79, "text": "\\begin{align}\n\\underline{\\mathbf{Z}} & =\\left[\\begin{array}{cc}\n\\gamma^{2}\\underline{\\mathbf{G}}_{1} & \\gamma\\underline{\\mathbf{G}}_{2}\\\\\n\\gamma\\underline{\\mathbf{G}}_{3} & \\underline{\\mathbf{G}}_{4}\n\\end{array}\\right],\\ \\mathbf{D}_{1}=\\left[\\begin{array}{c}\n\\gamma\\mathbf{I}_{\\hat{N}_{1}}\\\\\n\\boldsymbol{\\Phi}_{1}\n\\end{array}\\right]\\boldsymbol{\\Psi}_{1},\\\\\n\\mathbf{D}_{2} & =\\left[\\begin{array}{c}\n\\gamma\\mathbf{I}_{\\hat{N}_{2}}\\\\\n\\boldsymbol{\\Phi}_{2}\n\\end{array}\\right]\\boldsymbol{\\Psi}_{2},\\ \\mathbf{D}_{3}=\\left[\\begin{array}{c}\n\\gamma\\mathbf{I}_{\\hat{N}_{3}}\\\\\n\\boldsymbol{\\Phi}_{3}\n\\end{array}\\right]\\boldsymbol{\\Psi}_{3},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E47.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underline{\\mathbf{Z}}\" display=\"inline\"><munder accentunder=\"true\"><mi>\ud835\udc19</mi><mo>\u00af</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E47.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left[\\begin{array}[]{cc}\\gamma^{2}\\underline{\\mathbf{G}}_{1}&amp;%&#10;\\gamma\\underline{\\mathbf{G}}_{2}\\\\&#10;\\gamma\\underline{\\mathbf{G}}_{3}&amp;\\underline{\\mathbf{G}}_{4}\\end{array}\\right],%&#10;\\ \\mathbf{D}_{1}=\\left[\\begin{array}[]{c}\\gamma\\mathbf{I}_{\\hat{N}_{1}}\\\\&#10;\\boldsymbol{\\Phi}_{1}\\end{array}\\right]\\boldsymbol{\\Psi}_{1},\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>3</mn></msub></mrow></mtd><mtd columnalign=\"center\"><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>4</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><mi>\ud835\udc03</mi><mn>1</mn></msub><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udebd</mi><mn>1</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mn>1</mn></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E48.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{D}_{2}\" display=\"inline\"><msub><mi>\ud835\udc03</mi><mn>2</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E48.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left[\\begin{array}[]{c}\\gamma\\mathbf{I}_{\\hat{N}_{2}}\\\\&#10;\\boldsymbol{\\Phi}_{2}\\end{array}\\right]\\boldsymbol{\\Psi}_{2},\\ \\mathbf{D}_{3}=%&#10;\\left[\\begin{array}[]{c}\\gamma\\mathbf{I}_{\\hat{N}_{3}}\\\\&#10;\\boldsymbol{\\Phi}_{3}\\end{array}\\right]\\boldsymbol{\\Psi}_{3},\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udebd</mi><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mn>2</mn></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><mi>\ud835\udc03</mi><mn>3</mn></msub><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>3</mn></msub></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udebd</mi><mn>3</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mn>3</mn></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nThe problem can then be solved following similar steps to those introduced\nearlier in this section.\n\nWe have now derived the method of learning the sparsifying dictionaries\nwhen the multilinear sensing matrix is fixed. Combining this approach\nwith the methods of optimizing the sensing matrices elaborated in\nSection \\ref{sub:SensingDesign}, we can then jointly optimize $\\boldsymbol{\\Phi}_{1},\\ \\boldsymbol{\\Phi}_{2}$\nand $\\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$ by alternating\nbetween them. The overall procedure is summarized in Algorithm 3.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Joint Optimization} }\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}^{(0)}$ $(i=1,\\ 2)$, $\\boldsymbol{\\Phi}_{i}^{(0)}$\n$(i=1,\\ 2)$, $\\underline{\\mathbf{X}}$, $\\alpha$, $\\beta$, $\\eta$,\n$\\gamma$, \n\n\\hspace{3.1em}$iter=0$.\n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$, $\\hat{\\boldsymbol{\\Psi}}_{i}$\n$(i=1,\\ 2)$.\n\n1:\\textbf{ Repeat until convergence:}\n\n2:\\hspace{1.3em}For $\\hat{\\boldsymbol{\\Psi}}_{i}^{(iter)}$ $(i=1,\\ 2)$\nfixed, optimize $\\hat{\\boldsymbol{\\Phi}}_{i}^{(iter+1)}$ $(i=$\n\n\\hspace{2.1em}$1,\\ 2)$ using one of the approaches given in Section\n\\ref{sub:SensingDesign};\n\n3:\\hspace{1.3em}For $\\hat{\\boldsymbol{\\Psi}}_{i}^{(iter)}$ , $\\hat{\\boldsymbol{\\Phi}}_{i}^{(iter+1)}$\n$(i=1,\\ 2)$ fixed, solve (\\ref{eq:Coupled K-HOSVD}) using \n\n\\hspace{2.1em}TOMP to obtain $\\underline{\\hat{\\mathbf{S}}}$;\n\n4:\\hspace{1.3em}\\textbf{For $p_{1}=1$ }to\\textbf{ $\\hat{N}_{1}$}\n\n5:\\hspace{2.3em}Compute $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$\nusing (\\ref{eq:Coupled K-HOSVD}) - (\\ref{eq:ErrorKHOSVD});\n\n6:\\hspace{2.3em}Do HOSVD to $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$\nto obtain $\\lambda_{\\mathbf{R}}^{1}$, $\\mathbf{u}_{\\mathbf{R}}^{1}$,\n$\\mathbf{v}_{\\mathbf{R}}^{1}$ and $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$;\n\n7:\\hspace{2.3em}Update $(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1}^{(iter+1)})_{p_{1}}$,\n$\\mathbf{D}_{2}\\mathbf{\\tilde{S}}_{p_{1,:,:}}$ using (\\ref{eq:updatePsi})\n- (\\ref{eq:UpdateS}) and \n\n\\hspace{3.1em}calculate $\\mathbf{\\tilde{S}}_{p_{1,:,:}}$ by LS;\n\n8:\\hspace{1.3em}\\textbf{end}\n\n9:\\hspace{1.3em}\\textbf{For $p_{2}=1$ }to\\textbf{ $\\hat{N}_{2}$}\n\n10:\\hspace{1.8em}Compute $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$\nusing (\\ref{eq:Coupled K-HOSVD}) and (\\ref{eq:ErrorKHOSVD_Psi2});\n\n11:\\hspace{1.8em}Do HOSVD to $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$\nto obtain $\\lambda_{\\mathbf{R}}^{1}$, $\\mathbf{u}_{\\mathbf{R}}^{1}$,\n$\\mathbf{v}_{\\mathbf{R}}^{1}$ and $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$;\n\n12:\\hspace{1.8em}Update $(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2}^{(iter+1)})_{p_{2}}$,\n$\\mathbf{D}_{1}\\mathbf{\\tilde{S}}_{:,p_{2},:}$ using (\\ref{eq:UpdatePsi2})\n- (\\ref{eq:UpdateS2}) and \n\n\\hspace{3.1em}calculate $\\mathbf{\\tilde{S}}_{:,p_{2},:}$ by LS;\n\n13:\\hspace{0.8em}\\textbf{end}\n\n14:\\hspace{0.8em}$iter=iter+1$; \n\\end{algorithm}\n\n\n\n\\section{Experimental Results}\n\n\\label{sec:simulations}\n\nIn this section, we evaluate the proposed approaches via simulations\nusing both synthetic data and real images. We first test the sensing\nmatrix design approaches proposed in Section \\ref{sub:SensingDesign}\nwith the sparsifying dictionaries being given. Then the cTKSVD approach\nis evaluated when the sensing matrices are fixed. Finally the experiments\nfor the joint optimization of the two are presented. \n\n\n\\subsection{Optimal Multidimensional Sensing Matrix}\n\n\\label{sub:Simulation_Phi}\n\nThis section is intended to examine the proposed separable approach\nI and non-separable approach II for multidimensional sensing matrix\ndesign. Before doing so, we first test the tuning parameters for Approach\nII, i.e., the non-separable design approach presented in Section \\ref{sub:SensingDesign}-2.\nAs detailed in Section \\ref{sub:SensingDesign}-1, Approach I has\na closed form solution and there are no tuning parameters involved. \n\nWe evaluate the Mean Squared Error (MSE) performance of different\nsensing matrices generated using Approach II with various parameters\nand the results are reported by averaging over 500 trials. A random\n2D signal $\\mathbf{S}\\in\\mathbb{R}^{64\\times64}$ with sparsity $K=80$\nis generated, where the randomly placed non-zero elements follow an\ni.i.d zero-mean unit-variance Gaussian distribution. Both the dictionaries\n$\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{64\\times256}\\ (i=1,\\ 2)$ and\nthe initial sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{40\\times64}\\ (i=1,\\ 2)$\nare generated randomly with i.i.d zero-mean unit-variance Gaussian\ndistributions, and the dictionaries are then column normalized while\nthe sensing matrices are normalized by: $\\boldsymbol{\\Phi}_{i}=\\sqrt{64}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}.$\nWhen taking measurements, random additive Gaussian noise with variance\n$\\sigma^{2}$ is induced. A constant step size $\\eta=1e-7$ is used\nfor Approach II and the BP solver SPGL1 \\cite{Berg2008} is employed\nfor reconstructions. \n\\begin{figure}\n\\centering{}\\includegraphics{figures/Phi_parameters}\\caption{MSE performance of sensing matrices generated by Approach II with\ndifferent values of $\\alpha$ and $\\beta$. (a) $\\sigma^{2}=0,\\ \\alpha=1;$\n(b) $\\sigma^{2}=0,\\ \\beta=0.8;$ (c) $\\sigma^{2}=10^{-2},\\ \\alpha=1;$\n(d) $\\sigma^{2}=10^{-2},\\ \\beta=0.2.$\\label{fig:Phi_parameters}}\n\\end{figure}\n\n\nFig. \\ref{fig:Phi_parameters} illustrates the results for the parameter\ntests. In Fig. \\ref{fig:Phi_parameters} (a) and (c), the parameter\n$\\beta$ is evaluated for the noiseless ($\\sigma^{2}=0$) and high\nnoise ($\\sigma^{2}=10^{-2}$) cases, respectively, when $\\alpha=1$.\nFrom both (a) and (c), we can see that when $\\beta=0$ or 1, the MSE\nis larger than that for the other values, which means that both terms\nof Approach II that are controlled by $\\beta$ are essential for obtaining\noptimal sensing matrices. In addition, we can see that when $\\beta$\nbecomes larger in the range of $[0.1,\\ 0.9]$, the MSE decreases slightly\nin (a), but increases slightly in (b). This indicates the choice of\n$\\beta$ under different conditions of sensing noise, which is consistent\nwith that observed in \\cite{Bai2015}. Thus in the remaining experiments,\nwe take $\\beta=0.8$ when sensing noise is low and $\\beta=0.2$ when\nthe noise is high. Fig. \\ref{fig:Phi_parameters} (b) and (d) demonstrate\nthe MSE results for the tests of parameter $\\alpha$. It is observed\nthat $\\alpha=1$ is optimal for the noiseless case while it becomes\n$\\alpha=0.6$ when high noise exists. Therefore a larger $\\alpha$\nis preferred when low noise is involved, which needs to be reduced\naccordingly when the noise becomes higher.\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dM_BP_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dM_OMP_new}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different sensing matrices for (a) the BP, (b)\nthe OMP when $M_{i}\\ (i=1,\\ 2)$ varies. ($K=80,\\ N_{1}=N_{2}=64,\\ \\hat{N}_{1}=\\hat{N}_{2}=256$\nand $\\sigma^{2}=10^{-4}$) \\label{fig:Phi_M}}\n\\end{figure}\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dK_BP_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dK_OMP_new}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different sensing matrices for (a) the BP, (b)\nthe OMP when $K$ varies. ($M_{1}=M_{2}=40,\\ N_{1}=N_{2}=64,\\ \\hat{N}_{1}=\\hat{N}_{2}=256$\nand $\\sigma^{2}=10^{-4}$) \\label{fig:Phi_K}}\n\\end{figure}\n\n\nWe then proceed to examine the performance of both the proposed approaches.\nAs this is the first work to optimize the multidimensional sensing\nmatrix, we take the i.i.d Gaussian sensing matrices that are commonly\nused in CS problems for comparison. Besides, since Sapiro's approach\n\\cite{Duarte2009} has the same spirit to that of Approach I (as reviewed\nin Section \\ref{sub:CS sensing design}), it can be easily extended\nto the multidimensional case, i.e., individually generating $\\boldsymbol{\\Phi}_{i}\\ (i=1,\\ 2)$\nusing the approach in \\cite{Duarte2009}. We hence also include it\nin the comparisons and denote it by Separable Sapiro's approach (SS).\nThe previously described synthetic data is generated for the experiments\nand both BP and OMP are investigated for the reconstruction.\n\nDifferent sensing matrices are first evaluated using BP and OMP when\nthe number of measurements varies. A small amount of noise ($\\sigma^{2}=10^{-4}$)\nis added when taking measurements and the parameters are chosen as:\n$\\alpha=1,\\ \\beta=0.8$. From Fig. \\ref{fig:Phi_M}, it can be observed\nthat both the proposed approaches perform much better than the Gaussian\nsensing matrices, among which Approach II has better performance.\nIn general, the SS method performs worse than Approach I, although\nthe difference is not obvious at some points. Note that SS is an iterative\nmethod while Approach I is non-iterative. \n\nThe proposed approaches are again observed to be superior to the other\nmethods when the number of measurements is fixed but the signal sparsity\n$K$ is varied, as shown in Fig. \\ref{fig:Phi_K}. Compared to Approach\nI, Approach II exhibits better performance, but at the cost of higher\ncomputational complexity and the proper choice of the parameters. \n\n\n\\subsection{Optimal Multidimensional Dictionary with the Sensing Matrices Coupled}\n\n\\label{sub:Simulation_Psi}\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_parameters_M7}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_parameters_M3}\n\\par\\end{centering}\n\n}\\caption{Convergence behavior of cTKSVD with different values of $\\gamma$\ncompared to that of cKSVD with its optimal parameter setting when\n(a) $M_{1}=M_{2}=7;$ (b) $M_{1}=M_{2}=3$. \\label{fig:Psi_parameters}}\n\\end{figure}\n\nIn this section, we evaluate the proposed cTKSVD method with a given\nmultidimensional sensing matrix. A training sequence of 5000 2D signals\n($T=5000$) is generated, i.e., $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{18\\times18\\times5000}$,\nwhere each signal has $K=4\\ (2\\times2)$ randomly placed non-zero\nelements that follow an i.i.d zero-mean unit-variance Gaussian distribution.\nThe dictionaries $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{10\\times18}\\ (i=1,\\ 2)$\nare also drawn from i.i.d Gaussian distributions, followed by normalization\nsuch that they have unit-norm columns. The time-domain training signals\n$\\underline{\\mathbf{X}}\\in\\mathbb{R}^{10\\times10\\times5000}$ are\nthen formed by: $\\underline{\\mathbf{X}}=\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}$.\nThe test data of size $10\\times10\\times5000$ is generated following\nthe same procedure. Random Gaussian noise with variance $\\sigma^{2}$\nis added to both the training and test data. Two i.i.d random Gaussian\nmatrices are employed as the sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times10}\\ (i=1,\\ 2)$,\nnormalized by: $\\boldsymbol{\\Phi}_{i}=\\sqrt{10}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}.$\nTOMP \\cite{Caiafa2013} is utilized in both the training stage and\nthe reconstructions of the test stage for tensor-based approaches\nand OMP is employed for the vector-based approaches.\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_dN3}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_dSigma}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different dictionaries when (a) $T$ varies ($\\sigma^{2}=0$),\n(b) $\\sigma^{2}$ varies ($T=5000$). ($K=4,\\ M_{1}=M_{2}=7,\\ N_{1}=N_{2}=10,\\ \\hat{N}_{1}=\\hat{N}_{2}=18$)\n\\label{fig:Psi_T_sigma}}\n\\end{figure}\n\nWe first investigate the convergence behavior of the cTKSVD approach\nand examine the choice of the parameter $\\gamma$. We define the Average\nRepresentation Error (ARE) \\cite{Aharon2006,Chen2013dictionary} of\ncTKSVD as: $\\sqrt{||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{D}_{1}\\times_{2}\\mathbf{D}_{2}||_{F}^{2}/len(\\underline{\\mathbf{Z}})},$\nwhere $\\underline{\\mathbf{Z}}$, $\\mathbf{D}_{1}$ and $\\mathbf{D}_{2}$\nhave the same definitions as in (\\ref{eq:Coupled K-HOSVD}). Fig.\n\\ref{fig:Psi_parameters} shows the AREs of cTKSVD at different numbers\nof iterations for different values of $\\gamma$. The cKSVD method\n\\cite{Duarte2009} (reviewed in Section \\ref{sub:CS sensing design})\nis also tested and only the results of the optimal $\\gamma$ are displayed\nin Fig. \\ref{fig:Psi_parameters}. Note that cKSVD learns a single\ndictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{100\\times324}$, rather\nthan the separable multilinear dictionaries $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{10\\times18}\\ (i=1,\\ 2)$.\nThe ARE of cKSVD is thus modified accordingly as: $\\sqrt{||\\mathbf{Z}-\\mathbf{D}\\mathbf{S}||_{F}^{2}/len(\\mathbf{Z})}$,\nin which the symbols follow the definitions in (\\ref{eq:eqCoupled KSVD}).\nFrom Fig. \\ref{fig:Psi_parameters}, it can be seen that cTKSVD exhibits\nstable convergence behavior with different parameters. It converges\nto a lowest ARE with $\\gamma=1/64$ when $M_{i}=7$ and the optimal\n$\\gamma$ is $1/128$ when $M_{i}=3$. The reconstruction MSE values\nare also shown in the legend, which are similar to each other but\nreveal the same optimal choice of $\\gamma$ as described. Thus the\noptimal $\\gamma$ is lower when the number of measurements decreases,\nwhich is consistent with the observation in \\cite{Duarte2009}. In\nboth experiments, cTKSVD with the optimal $\\gamma$ outperforms cKSVD\nin terms of ARE and MSE.\n\nThen the MSE performance of dictionaries learned by cTKSVD is compared\nwith that of cKSVD \\cite{Duarte2009} and KHOSVD \\cite{Roemer2014}\nwhen the number of training sequences $T$ and the noise variance\n$\\sigma^{2}$ vary. We use $\\gamma=1/64$ for cTKSVD and $\\gamma=1/32$\nfor cKSVD. To see the benefit of coupling sensing matrices, we also\nevaluate the uncoupled version of the proposed approach, i.e., TKSVD,\nin the experiments. The results can be found in Fig. \\ref{fig:Psi_T_sigma}.\nIt is observable that cTKSVD outperforms all the other methods in\nterms of the reconstruction MSE. The sensing-matrix-coupled approaches\n(cKSVD and cTKSVD) are superior to the uncoupled approaches (TKSVD\nand KHOSVD). The TKSVD method leads to smaller MSE compared to KHOSVD,\nas it fully exploits the multidimensional structure. In addition,\nsince cKSVD is not an approach that explicitly considers a multidimensional\ndictionary, it requires longer training sequences to learn the multilinear\nstructure from the vectorized data. As seen in Fig. \\ref{fig:Psi_T_sigma}\n(a), to achieve a MSE of 0.02, cTKSVD only needs 2000 training data;\nwhile approximately 6000 is required for the cKSVD approach. For the\nsame reason, the performance of cKSVD degrades dramatically when the\ntraining data is less than 1000.\n\n\n\\subsection{TCS with Jointly Optimized Sensing Matrix and Dictionary}\n\n\\begin{figure}\n\\centering{}\\includegraphics{figures/Comb_converg_new}\\caption{Convergence behavior of various joint optimization methods. ($T=5000,\\ K=4,\\ M_{1}=M_{2}=6,\\ N_{1}=N_{2}=8,\\ \\hat{N}_{1}=\\hat{N}_{2}=16,\\ \\sigma^{2}=0$)\n\\label{fig:Comb_converg}}\n\\end{figure}\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Comb_dM_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Comb_dSigma_new}\n\\par\\end{centering}\n\n}\\caption{PSNR performance of different methods when (a) $M_{i}\\ (i=1,\\ 2)$\nvaries ($\\sigma^{2}=0$), (b) $\\sigma^{2}$ varies ($M_{1}=M_{2}=6$).\n($T=5000,\\ K=4,\\ M_{1}=M_{2}=6,\\ N_{1}=N_{2}=8,\\ \\hat{N}_{1}=\\hat{N}_{2}=16$)\n\\label{fig:comb_M_sigma}}\n\\end{figure}\n\n\nNow we examine the performance of the proposed joint optimization\napproach in Algorithm 3. The training data consists of 5000 $8\\times8$\npatches obtained by randomly extracting 25 patches from each of the\n200 images in a training set from the Berkeley segmentation dataset\n\\cite{Martin2001}. The test data is obtained by extracting non-overlapping\n$8\\times8$ patches from the other 100 images in the dataset. A 2D\nDiscrete Cosine Transform (DCT) is employed to initialize the dictionaries\n$\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{8\\times16}\\ (i=1,\\ 2)$ and i.i.d\nGaussian matrices are used as the initial sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times8}\\ (i=1,\\ 2)$.\nRandom Gaussian noise with variance $\\sigma^{2}$ is added to the\nmeasurements at the test stage. We employ TOMP for reconstruction\nand the Peak Signal to Noise Ratio (PSNR) is used as the evaluation\ncriteria.\n\nIn the first experiment, we examine the convergence behavior of Algorithm\n3 when the proposed approach I and II are utilized for the sensing\nmatrix optimization step (respectively denote by I + cTKSVD and II\n+ cTKSVD). We take $M_{1}=M_{2}=6$ and no noise is added to the measurements\nat the test stage, i.e., $\\sigma^{2}=0$. By conducting the simulations\nperformed previously to obtain the results in Fig. \\ref{fig:Phi_parameters}\nand \\ref{fig:Psi_parameters}, the parameters are chosen as: $\\alpha=3,\\ \\beta=0.8,\\ \\gamma=1/8$.\nThe step size for II + cTKSVD is set as: $\\eta=1e-5$. The PSNR performance\nfor different numbers of iterations is illustrated in Fig. \\ref{fig:Comb_converg}.\nSince Sapiro's approach in \\cite{Duarte2009} also jointly optimizes\nthe sensing matrix and dictionary, we include it in this figure (denoted\nby Sapiro's + cKSVD). The parameter $\\gamma$ is optimal at $1/2$\nfor cKSVD under our settings. However, note that Sapiro's approach\nis only for vectorized signals in the conventional CS problem, i.e.,\na single sensing matrix $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{36\\times64}$\nand a dictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{64\\times256}$ are\nobtained. It is not suitable for a practical TCS system, where separable\nmultidimensional sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{6\\times8}\\ (i=1,\\ 2)$\nare required. Even so, from Fig. \\ref{fig:Comb_converg}, we can see\nthe proposed approaches outperform Sapiro's approach. All the methods\nconverge in less than 10 iterations, among which II + cTKSVD leads\nto the highest PSNR value.\n\nThen the proposed approaches are compared with various other approaches\nwhen the number of measurements ($M_{i}\\ (i=1,\\ 2)$) and the noise\nvariance ($\\sigma^{2}$) vary. Specifically, using the notation employed\npreviously and by denoting the method of combining sensing matrix\ndesign with that of the dictionary learning using a ``+'', the methods\nfor comparison are: II + TKSVD, Gaussian + cTKSVD, Sapiro's + cKSVD\nand SS + KHOSVD. In these approaches, II + TKSVD and SS + KHOSVD are\nuncoupled methods; Gaussian + cTKSVD does not involve sensing matrix\noptimization; Sapiro's + cKSVD is for conventional CS system only. \n\nThe results are shown in Fig. \\ref{fig:comb_M_sigma}. We can see\nthat the proposed approaches obtain higher PSNR values than all of\nthe other methods and II + cTKSVD performs best. To see the gain of\ncoupling sensing matrices during dictionary learning and optimizing\nthe sensing matrices, respectively, we compare II + cTKSVD with II\n+ TKSVD and Gaussian + cTKSVD. For instance, when $M_{i}=5,\\ \\sigma^{2}=0$,\nII + cTKSVD has a gain of about 3dB over II + TKSVD and nearly 9dB\nover Gaussian + cTKSVD. Although Sapiro's + cKSVD has a similar performance\nto ours at some specific settings, it is not for a TCS system that\nrequires multiple separable sensing matrices. Examples of reconstructed\nimages using these methods are demonstrated in Fig. \\ref{fig:gallary}\nand \\ref{fig:gallary-1} with the corresponding PSNR values listed.\nAll of the conducted simulations verify that the proposed methods\nof multidimensional sensing matrix and dictionary optimization improve\nthe performance of a TCS system.\n\n\\begin{figure}\n\\centering{}\\includegraphics{figures/gallary_M6_new}\\caption{Reconstruction example when $M_{1}=M_{2}=6$. The images from left\nto right, top to bottom and their PSNR (dB) values are: II+cTKSVD\n(35.41), I+cTKSVD (34.97), Sapiro's+cKSVD (33.64), II+TKSVD (33.57),\nSS+KHOSVD (28.62), Gaussian+cTKSVD (28.05). \\label{fig:gallary}}\n\\end{figure}\n\\begin{figure}\n\\centering{}\\includegraphics{figures/gallary_M4_new}\\caption{Reconstruction example when $M_{1}=M_{2}=4$. The images from left\nto right, top to bottom and their PSNR (dB) values are: II+cTKSVD\n(29.91), I+cTKSVD (29.45), Sapiro's+cKSVD (28.72), II+TKSVD (26.60),\nSS+KHOSVD (22.62), Gaussian+cTKSVD (21.94). \\label{fig:gallary-1}}\n\\end{figure}\n\n\n\n\\section{Conclusions}\n\n\\label{sec:conclusions}\n\nIn this paper, we propose to jointly optimize the multidimensional\nsensing matrix and dictionary for TCS systems. To obtain the optimized\nsensing matrices, a separable approach with closed form solutions\nhas been presented and a joint iterative approach with novel design\nmeasures has also been proposed. The iterative approach certainly\nhas higher complexity, but also exhibits better performance. An approach\nto learning the multidimensional dictionary has been designed, which\nexplicitly takes the multidimensional structure into account and removes\nthe redundant updates in the existing multilinear approaches in the\nliterature. Further gain is obtained by coupling the multidimensional\nsensing matrix while learning the dictionary. The performance advantage\nof the proposed approaches has been demonstrated by experiments using\nboth synthetic data and real images.\n\n\n\\appendices{}\n\n\n\\section{Proof of Theorem 3}\n\n\\label{sec:AppendixI}\n\nAssume $\\mathbf{A}_{i}=\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}=\\mathbf{U}_{\\mathbf{A}_{i}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{i}} & \\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{i}}^{T}$ is an SVD of $\\mathbf{A}_{i}$ for $i=1,\\ 2$ and $rank(\\mathbf{A}_{i})=M_{i}$.\nThen the objective we want to minimize in (\\ref{eq:Approach I}) can\nbe rewritten as:\n\n\\textit{\\footnotesize{}\n", "itemtype": "equation", "pos": 58459, "prevtext": "\nand if we denote the operator ``$\\nearrow_{3}$'' as stacking tensors\nalong their third mode, then in the above formulation of $\\underline{\\mathbf{Z}}$,\n\n", "index": 81, "text": "\\begin{align}\n\\underline{\\mathbf{G}}_{1} & =(\\gamma\\underline{\\mathbf{X}})\\nearrow_{3}(\\underline{\\mathbf{Y}}_{3}),\\ \\underline{\\mathbf{G}}_{2}=(\\gamma\\underline{\\mathbf{Y}}_{2})\\nearrow_{3}(\\underline{\\mathbf{Y}}_{23}),\\nonumber \\\\\n\\underline{\\mathbf{G}}_{3} & =(\\gamma\\underline{\\mathbf{Y}}_{1})\\nearrow_{3}(\\underline{\\mathbf{Y}}_{13}),\\ \\underline{\\mathbf{G}}_{4}=(\\gamma\\underline{\\mathbf{Y}})\\nearrow_{3}(\\underline{\\mathbf{Y}}_{12}),\\nonumber \\\\\n\\underline{\\mathbf{Y}}_{i} & =\\underline{\\mathbf{X}}\\times_{i}\\boldsymbol{\\Phi}_{i}+\\underline{\\mathbf{E}}_{i},\\ \\underline{\\mathbf{Y}}_{ij}=\\underline{\\mathbf{X}}\\times_{i}\\boldsymbol{\\Phi}_{i}\\times_{j}\\boldsymbol{\\Phi}_{j}+\\underline{\\mathbf{E}}_{ij}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underline{\\mathbf{G}}_{1}\" display=\"inline\"><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\gamma\\underline{\\mathbf{X}})\\nearrow_{3}(\\underline{\\mathbf{Y}%&#10;}_{3}),\\ \\underline{\\mathbf{G}}_{2}=(\\gamma\\underline{\\mathbf{Y}}_{2})\\nearrow%&#10;_{3}(\\underline{\\mathbf{Y}}_{23}),\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mo>\u2197</mo><mn>3</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>3</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mo>\u2197</mo><mn>3</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>23</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underline{\\mathbf{G}}_{3}\" display=\"inline\"><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>3</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\gamma\\underline{\\mathbf{Y}}_{1})\\nearrow_{3}(\\underline{%&#10;\\mathbf{Y}}_{13}),\\ \\underline{\\mathbf{G}}_{4}=(\\gamma\\underline{\\mathbf{Y}})%&#10;\\nearrow_{3}(\\underline{\\mathbf{Y}}_{12}),\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mo>\u2197</mo><mn>3</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>13</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><munder accentunder=\"true\"><mi>\ud835\udc06</mi><mo>\u00af</mo></munder><mn>4</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mo>\u2197</mo><mn>3</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mn>12</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E49.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underline{\\mathbf{Y}}_{i}\" display=\"inline\"><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E49.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underline{\\mathbf{X}}\\times_{i}\\boldsymbol{\\Phi}_{i}+\\underline%&#10;{\\mathbf{E}}_{i},\\ \\underline{\\mathbf{Y}}_{ij}=\\underline{\\mathbf{X}}\\times_{i%&#10;}\\boldsymbol{\\Phi}_{i}\\times_{j}\\boldsymbol{\\Phi}_{j}+\\underline{\\mathbf{E}}_{%&#10;ij}.\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mi>i</mi></msub><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc04</mi><mo>\u00af</mo></munder><mi>i</mi></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><munder accentunder=\"true\"><mi>\ud835\udc18</mi><mo>\u00af</mo></munder><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mrow><mrow><munder accentunder=\"true\"><mi>\ud835\udc17</mi><mo>\u00af</mo></munder><msub><mo>\u00d7</mo><mi>i</mi></msub><msub><mi>\ud835\udebd</mi><mi>i</mi></msub></mrow><msub><mo>\u00d7</mo><mi>j</mi></msub><msub><mi>\ud835\udebd</mi><mi>j</mi></msub></mrow><mo>+</mo><msub><munder accentunder=\"true\"><mi>\ud835\udc04</mi><mo>\u00af</mo></munder><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\n}Denote $\\boldsymbol{\\Sigma}=\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{2}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\otimes\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{1}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]=diag(\\boldsymbol{\\nu}_{\\mathbf{A}_{2}}\\otimes\\boldsymbol{\\nu}_{\\mathbf{A}_{1}}),$ $\\boldsymbol{\\nu}_{\\mathbf{A}_{i}}=diag(\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{i}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right])$, then we have \n\n", "itemtype": "equation", "pos": 81075, "prevtext": "\nThe problem can then be solved following similar steps to those introduced\nearlier in this section.\n\nWe have now derived the method of learning the sparsifying dictionaries\nwhen the multilinear sensing matrix is fixed. Combining this approach\nwith the methods of optimizing the sensing matrices elaborated in\nSection \\ref{sub:SensingDesign}, we can then jointly optimize $\\boldsymbol{\\Phi}_{1},\\ \\boldsymbol{\\Phi}_{2}$\nand $\\boldsymbol{\\Psi}_{1},\\ \\boldsymbol{\\Psi}_{2}$ by alternating\nbetween them. The overall procedure is summarized in Algorithm 3.\n\n\\begin{algorithm}[h]\n\\caption{\\textbf{Joint Optimization} }\n\\textbf{Input: }$\\boldsymbol{\\Psi}_{i}^{(0)}$ $(i=1,\\ 2)$, $\\boldsymbol{\\Phi}_{i}^{(0)}$\n$(i=1,\\ 2)$, $\\underline{\\mathbf{X}}$, $\\alpha$, $\\beta$, $\\eta$,\n$\\gamma$, \n\n\\hspace{3.1em}$iter=0$.\n\n\\textbf{Output:} $\\hat{\\boldsymbol{\\Phi}}_{i}$ $(i=1,\\ 2)$, $\\hat{\\boldsymbol{\\Psi}}_{i}$\n$(i=1,\\ 2)$.\n\n1:\\textbf{ Repeat until convergence:}\n\n2:\\hspace{1.3em}For $\\hat{\\boldsymbol{\\Psi}}_{i}^{(iter)}$ $(i=1,\\ 2)$\nfixed, optimize $\\hat{\\boldsymbol{\\Phi}}_{i}^{(iter+1)}$ $(i=$\n\n\\hspace{2.1em}$1,\\ 2)$ using one of the approaches given in Section\n\\ref{sub:SensingDesign};\n\n3:\\hspace{1.3em}For $\\hat{\\boldsymbol{\\Psi}}_{i}^{(iter)}$ , $\\hat{\\boldsymbol{\\Phi}}_{i}^{(iter+1)}$\n$(i=1,\\ 2)$ fixed, solve (\\ref{eq:Coupled K-HOSVD}) using \n\n\\hspace{2.1em}TOMP to obtain $\\underline{\\hat{\\mathbf{S}}}$;\n\n4:\\hspace{1.3em}\\textbf{For $p_{1}=1$ }to\\textbf{ $\\hat{N}_{1}$}\n\n5:\\hspace{2.3em}Compute $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$\nusing (\\ref{eq:Coupled K-HOSVD}) - (\\ref{eq:ErrorKHOSVD});\n\n6:\\hspace{2.3em}Do HOSVD to $\\tilde{\\underline{\\mathbf{R}}}_{p_{1}}$\nto obtain $\\lambda_{\\mathbf{R}}^{1}$, $\\mathbf{u}_{\\mathbf{R}}^{1}$,\n$\\mathbf{v}_{\\mathbf{R}}^{1}$ and $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$;\n\n7:\\hspace{2.3em}Update $(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{1}^{(iter+1)})_{p_{1}}$,\n$\\mathbf{D}_{2}\\mathbf{\\tilde{S}}_{p_{1,:,:}}$ using (\\ref{eq:updatePsi})\n- (\\ref{eq:UpdateS}) and \n\n\\hspace{3.1em}calculate $\\mathbf{\\tilde{S}}_{p_{1,:,:}}$ by LS;\n\n8:\\hspace{1.3em}\\textbf{end}\n\n9:\\hspace{1.3em}\\textbf{For $p_{2}=1$ }to\\textbf{ $\\hat{N}_{2}$}\n\n10:\\hspace{1.8em}Compute $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$\nusing (\\ref{eq:Coupled K-HOSVD}) and (\\ref{eq:ErrorKHOSVD_Psi2});\n\n11:\\hspace{1.8em}Do HOSVD to $\\tilde{\\underline{\\mathbf{R}}}_{p_{2}}$\nto obtain $\\lambda_{\\mathbf{R}}^{1}$, $\\mathbf{u}_{\\mathbf{R}}^{1}$,\n$\\mathbf{v}_{\\mathbf{R}}^{1}$ and $\\boldsymbol{\\omega}_{\\mathbf{R}}^{1}$;\n\n12:\\hspace{1.8em}Update $(\\mathbf{\\hat{\\boldsymbol{\\psi}}}_{2}^{(iter+1)})_{p_{2}}$,\n$\\mathbf{D}_{1}\\mathbf{\\tilde{S}}_{:,p_{2},:}$ using (\\ref{eq:UpdatePsi2})\n- (\\ref{eq:UpdateS2}) and \n\n\\hspace{3.1em}calculate $\\mathbf{\\tilde{S}}_{:,p_{2},:}$ by LS;\n\n13:\\hspace{0.8em}\\textbf{end}\n\n14:\\hspace{0.8em}$iter=iter+1$; \n\\end{algorithm}\n\n\n\n\\section{Experimental Results}\n\n\\label{sec:simulations}\n\nIn this section, we evaluate the proposed approaches via simulations\nusing both synthetic data and real images. We first test the sensing\nmatrix design approaches proposed in Section \\ref{sub:SensingDesign}\nwith the sparsifying dictionaries being given. Then the cTKSVD approach\nis evaluated when the sensing matrices are fixed. Finally the experiments\nfor the joint optimization of the two are presented. \n\n\n\\subsection{Optimal Multidimensional Sensing Matrix}\n\n\\label{sub:Simulation_Phi}\n\nThis section is intended to examine the proposed separable approach\nI and non-separable approach II for multidimensional sensing matrix\ndesign. Before doing so, we first test the tuning parameters for Approach\nII, i.e., the non-separable design approach presented in Section \\ref{sub:SensingDesign}-2.\nAs detailed in Section \\ref{sub:SensingDesign}-1, Approach I has\na closed form solution and there are no tuning parameters involved. \n\nWe evaluate the Mean Squared Error (MSE) performance of different\nsensing matrices generated using Approach II with various parameters\nand the results are reported by averaging over 500 trials. A random\n2D signal $\\mathbf{S}\\in\\mathbb{R}^{64\\times64}$ with sparsity $K=80$\nis generated, where the randomly placed non-zero elements follow an\ni.i.d zero-mean unit-variance Gaussian distribution. Both the dictionaries\n$\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{64\\times256}\\ (i=1,\\ 2)$ and\nthe initial sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{40\\times64}\\ (i=1,\\ 2)$\nare generated randomly with i.i.d zero-mean unit-variance Gaussian\ndistributions, and the dictionaries are then column normalized while\nthe sensing matrices are normalized by: $\\boldsymbol{\\Phi}_{i}=\\sqrt{64}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}.$\nWhen taking measurements, random additive Gaussian noise with variance\n$\\sigma^{2}$ is induced. A constant step size $\\eta=1e-7$ is used\nfor Approach II and the BP solver SPGL1 \\cite{Berg2008} is employed\nfor reconstructions. \n\\begin{figure}\n\\centering{}\\includegraphics{figures/Phi_parameters}\\caption{MSE performance of sensing matrices generated by Approach II with\ndifferent values of $\\alpha$ and $\\beta$. (a) $\\sigma^{2}=0,\\ \\alpha=1;$\n(b) $\\sigma^{2}=0,\\ \\beta=0.8;$ (c) $\\sigma^{2}=10^{-2},\\ \\alpha=1;$\n(d) $\\sigma^{2}=10^{-2},\\ \\beta=0.2.$\\label{fig:Phi_parameters}}\n\\end{figure}\n\n\nFig. \\ref{fig:Phi_parameters} illustrates the results for the parameter\ntests. In Fig. \\ref{fig:Phi_parameters} (a) and (c), the parameter\n$\\beta$ is evaluated for the noiseless ($\\sigma^{2}=0$) and high\nnoise ($\\sigma^{2}=10^{-2}$) cases, respectively, when $\\alpha=1$.\nFrom both (a) and (c), we can see that when $\\beta=0$ or 1, the MSE\nis larger than that for the other values, which means that both terms\nof Approach II that are controlled by $\\beta$ are essential for obtaining\noptimal sensing matrices. In addition, we can see that when $\\beta$\nbecomes larger in the range of $[0.1,\\ 0.9]$, the MSE decreases slightly\nin (a), but increases slightly in (b). This indicates the choice of\n$\\beta$ under different conditions of sensing noise, which is consistent\nwith that observed in \\cite{Bai2015}. Thus in the remaining experiments,\nwe take $\\beta=0.8$ when sensing noise is low and $\\beta=0.2$ when\nthe noise is high. Fig. \\ref{fig:Phi_parameters} (b) and (d) demonstrate\nthe MSE results for the tests of parameter $\\alpha$. It is observed\nthat $\\alpha=1$ is optimal for the noiseless case while it becomes\n$\\alpha=0.6$ when high noise exists. Therefore a larger $\\alpha$\nis preferred when low noise is involved, which needs to be reduced\naccordingly when the noise becomes higher.\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dM_BP_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dM_OMP_new}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different sensing matrices for (a) the BP, (b)\nthe OMP when $M_{i}\\ (i=1,\\ 2)$ varies. ($K=80,\\ N_{1}=N_{2}=64,\\ \\hat{N}_{1}=\\hat{N}_{2}=256$\nand $\\sigma^{2}=10^{-4}$) \\label{fig:Phi_M}}\n\\end{figure}\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dK_BP_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Phi_dK_OMP_new}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different sensing matrices for (a) the BP, (b)\nthe OMP when $K$ varies. ($M_{1}=M_{2}=40,\\ N_{1}=N_{2}=64,\\ \\hat{N}_{1}=\\hat{N}_{2}=256$\nand $\\sigma^{2}=10^{-4}$) \\label{fig:Phi_K}}\n\\end{figure}\n\n\nWe then proceed to examine the performance of both the proposed approaches.\nAs this is the first work to optimize the multidimensional sensing\nmatrix, we take the i.i.d Gaussian sensing matrices that are commonly\nused in CS problems for comparison. Besides, since Sapiro's approach\n\\cite{Duarte2009} has the same spirit to that of Approach I (as reviewed\nin Section \\ref{sub:CS sensing design}), it can be easily extended\nto the multidimensional case, i.e., individually generating $\\boldsymbol{\\Phi}_{i}\\ (i=1,\\ 2)$\nusing the approach in \\cite{Duarte2009}. We hence also include it\nin the comparisons and denote it by Separable Sapiro's approach (SS).\nThe previously described synthetic data is generated for the experiments\nand both BP and OMP are investigated for the reconstruction.\n\nDifferent sensing matrices are first evaluated using BP and OMP when\nthe number of measurements varies. A small amount of noise ($\\sigma^{2}=10^{-4}$)\nis added when taking measurements and the parameters are chosen as:\n$\\alpha=1,\\ \\beta=0.8$. From Fig. \\ref{fig:Phi_M}, it can be observed\nthat both the proposed approaches perform much better than the Gaussian\nsensing matrices, among which Approach II has better performance.\nIn general, the SS method performs worse than Approach I, although\nthe difference is not obvious at some points. Note that SS is an iterative\nmethod while Approach I is non-iterative. \n\nThe proposed approaches are again observed to be superior to the other\nmethods when the number of measurements is fixed but the signal sparsity\n$K$ is varied, as shown in Fig. \\ref{fig:Phi_K}. Compared to Approach\nI, Approach II exhibits better performance, but at the cost of higher\ncomputational complexity and the proper choice of the parameters. \n\n\n\\subsection{Optimal Multidimensional Dictionary with the Sensing Matrices Coupled}\n\n\\label{sub:Simulation_Psi}\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_parameters_M7}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_parameters_M3}\n\\par\\end{centering}\n\n}\\caption{Convergence behavior of cTKSVD with different values of $\\gamma$\ncompared to that of cKSVD with its optimal parameter setting when\n(a) $M_{1}=M_{2}=7;$ (b) $M_{1}=M_{2}=3$. \\label{fig:Psi_parameters}}\n\\end{figure}\n\nIn this section, we evaluate the proposed cTKSVD method with a given\nmultidimensional sensing matrix. A training sequence of 5000 2D signals\n($T=5000$) is generated, i.e., $\\underline{\\mathbf{S}}\\in\\mathbb{R}^{18\\times18\\times5000}$,\nwhere each signal has $K=4\\ (2\\times2)$ randomly placed non-zero\nelements that follow an i.i.d zero-mean unit-variance Gaussian distribution.\nThe dictionaries $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{10\\times18}\\ (i=1,\\ 2)$\nare also drawn from i.i.d Gaussian distributions, followed by normalization\nsuch that they have unit-norm columns. The time-domain training signals\n$\\underline{\\mathbf{X}}\\in\\mathbb{R}^{10\\times10\\times5000}$ are\nthen formed by: $\\underline{\\mathbf{X}}=\\underline{\\mathbf{S}}\\times_{1}\\boldsymbol{\\Psi}_{1}\\times_{2}\\boldsymbol{\\Psi}_{2}$.\nThe test data of size $10\\times10\\times5000$ is generated following\nthe same procedure. Random Gaussian noise with variance $\\sigma^{2}$\nis added to both the training and test data. Two i.i.d random Gaussian\nmatrices are employed as the sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times10}\\ (i=1,\\ 2)$,\nnormalized by: $\\boldsymbol{\\Phi}_{i}=\\sqrt{10}\\boldsymbol{\\Phi}_{i}/||\\boldsymbol{\\Phi}_{i}||_{F}.$\nTOMP \\cite{Caiafa2013} is utilized in both the training stage and\nthe reconstructions of the test stage for tensor-based approaches\nand OMP is employed for the vector-based approaches.\n\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_dN3}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Dic_dSigma}\n\\par\\end{centering}\n\n}\\caption{MSE performance of different dictionaries when (a) $T$ varies ($\\sigma^{2}=0$),\n(b) $\\sigma^{2}$ varies ($T=5000$). ($K=4,\\ M_{1}=M_{2}=7,\\ N_{1}=N_{2}=10,\\ \\hat{N}_{1}=\\hat{N}_{2}=18$)\n\\label{fig:Psi_T_sigma}}\n\\end{figure}\n\nWe first investigate the convergence behavior of the cTKSVD approach\nand examine the choice of the parameter $\\gamma$. We define the Average\nRepresentation Error (ARE) \\cite{Aharon2006,Chen2013dictionary} of\ncTKSVD as: $\\sqrt{||\\underline{\\mathbf{Z}}-\\underline{\\mathbf{S}}\\times_{1}\\mathbf{D}_{1}\\times_{2}\\mathbf{D}_{2}||_{F}^{2}/len(\\underline{\\mathbf{Z}})},$\nwhere $\\underline{\\mathbf{Z}}$, $\\mathbf{D}_{1}$ and $\\mathbf{D}_{2}$\nhave the same definitions as in (\\ref{eq:Coupled K-HOSVD}). Fig.\n\\ref{fig:Psi_parameters} shows the AREs of cTKSVD at different numbers\nof iterations for different values of $\\gamma$. The cKSVD method\n\\cite{Duarte2009} (reviewed in Section \\ref{sub:CS sensing design})\nis also tested and only the results of the optimal $\\gamma$ are displayed\nin Fig. \\ref{fig:Psi_parameters}. Note that cKSVD learns a single\ndictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{100\\times324}$, rather\nthan the separable multilinear dictionaries $\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{10\\times18}\\ (i=1,\\ 2)$.\nThe ARE of cKSVD is thus modified accordingly as: $\\sqrt{||\\mathbf{Z}-\\mathbf{D}\\mathbf{S}||_{F}^{2}/len(\\mathbf{Z})}$,\nin which the symbols follow the definitions in (\\ref{eq:eqCoupled KSVD}).\nFrom Fig. \\ref{fig:Psi_parameters}, it can be seen that cTKSVD exhibits\nstable convergence behavior with different parameters. It converges\nto a lowest ARE with $\\gamma=1/64$ when $M_{i}=7$ and the optimal\n$\\gamma$ is $1/128$ when $M_{i}=3$. The reconstruction MSE values\nare also shown in the legend, which are similar to each other but\nreveal the same optimal choice of $\\gamma$ as described. Thus the\noptimal $\\gamma$ is lower when the number of measurements decreases,\nwhich is consistent with the observation in \\cite{Duarte2009}. In\nboth experiments, cTKSVD with the optimal $\\gamma$ outperforms cKSVD\nin terms of ARE and MSE.\n\nThen the MSE performance of dictionaries learned by cTKSVD is compared\nwith that of cKSVD \\cite{Duarte2009} and KHOSVD \\cite{Roemer2014}\nwhen the number of training sequences $T$ and the noise variance\n$\\sigma^{2}$ vary. We use $\\gamma=1/64$ for cTKSVD and $\\gamma=1/32$\nfor cKSVD. To see the benefit of coupling sensing matrices, we also\nevaluate the uncoupled version of the proposed approach, i.e., TKSVD,\nin the experiments. The results can be found in Fig. \\ref{fig:Psi_T_sigma}.\nIt is observable that cTKSVD outperforms all the other methods in\nterms of the reconstruction MSE. The sensing-matrix-coupled approaches\n(cKSVD and cTKSVD) are superior to the uncoupled approaches (TKSVD\nand KHOSVD). The TKSVD method leads to smaller MSE compared to KHOSVD,\nas it fully exploits the multidimensional structure. In addition,\nsince cKSVD is not an approach that explicitly considers a multidimensional\ndictionary, it requires longer training sequences to learn the multilinear\nstructure from the vectorized data. As seen in Fig. \\ref{fig:Psi_T_sigma}\n(a), to achieve a MSE of 0.02, cTKSVD only needs 2000 training data;\nwhile approximately 6000 is required for the cKSVD approach. For the\nsame reason, the performance of cKSVD degrades dramatically when the\ntraining data is less than 1000.\n\n\n\\subsection{TCS with Jointly Optimized Sensing Matrix and Dictionary}\n\n\\begin{figure}\n\\centering{}\\includegraphics{figures/Comb_converg_new}\\caption{Convergence behavior of various joint optimization methods. ($T=5000,\\ K=4,\\ M_{1}=M_{2}=6,\\ N_{1}=N_{2}=8,\\ \\hat{N}_{1}=\\hat{N}_{2}=16,\\ \\sigma^{2}=0$)\n\\label{fig:Comb_converg}}\n\\end{figure}\n\\begin{figure}\n\\begin{centering}\n\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Comb_dM_new}\n\\par\\end{centering}\n\n}\n\\par\\end{centering}\n\n\\centering{}\\subfloat[]{\\begin{centering}\n\\includegraphics{figures/Comb_dSigma_new}\n\\par\\end{centering}\n\n}\\caption{PSNR performance of different methods when (a) $M_{i}\\ (i=1,\\ 2)$\nvaries ($\\sigma^{2}=0$), (b) $\\sigma^{2}$ varies ($M_{1}=M_{2}=6$).\n($T=5000,\\ K=4,\\ M_{1}=M_{2}=6,\\ N_{1}=N_{2}=8,\\ \\hat{N}_{1}=\\hat{N}_{2}=16$)\n\\label{fig:comb_M_sigma}}\n\\end{figure}\n\n\nNow we examine the performance of the proposed joint optimization\napproach in Algorithm 3. The training data consists of 5000 $8\\times8$\npatches obtained by randomly extracting 25 patches from each of the\n200 images in a training set from the Berkeley segmentation dataset\n\\cite{Martin2001}. The test data is obtained by extracting non-overlapping\n$8\\times8$ patches from the other 100 images in the dataset. A 2D\nDiscrete Cosine Transform (DCT) is employed to initialize the dictionaries\n$\\boldsymbol{\\Psi}_{i}\\in\\mathbb{R}^{8\\times16}\\ (i=1,\\ 2)$ and i.i.d\nGaussian matrices are used as the initial sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{M_{i}\\times8}\\ (i=1,\\ 2)$.\nRandom Gaussian noise with variance $\\sigma^{2}$ is added to the\nmeasurements at the test stage. We employ TOMP for reconstruction\nand the Peak Signal to Noise Ratio (PSNR) is used as the evaluation\ncriteria.\n\nIn the first experiment, we examine the convergence behavior of Algorithm\n3 when the proposed approach I and II are utilized for the sensing\nmatrix optimization step (respectively denote by I + cTKSVD and II\n+ cTKSVD). We take $M_{1}=M_{2}=6$ and no noise is added to the measurements\nat the test stage, i.e., $\\sigma^{2}=0$. By conducting the simulations\nperformed previously to obtain the results in Fig. \\ref{fig:Phi_parameters}\nand \\ref{fig:Psi_parameters}, the parameters are chosen as: $\\alpha=3,\\ \\beta=0.8,\\ \\gamma=1/8$.\nThe step size for II + cTKSVD is set as: $\\eta=1e-5$. The PSNR performance\nfor different numbers of iterations is illustrated in Fig. \\ref{fig:Comb_converg}.\nSince Sapiro's approach in \\cite{Duarte2009} also jointly optimizes\nthe sensing matrix and dictionary, we include it in this figure (denoted\nby Sapiro's + cKSVD). The parameter $\\gamma$ is optimal at $1/2$\nfor cKSVD under our settings. However, note that Sapiro's approach\nis only for vectorized signals in the conventional CS problem, i.e.,\na single sensing matrix $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{36\\times64}$\nand a dictionary $\\boldsymbol{\\Psi}\\in\\mathbb{R}^{64\\times256}$ are\nobtained. It is not suitable for a practical TCS system, where separable\nmultidimensional sensing matrices $\\boldsymbol{\\Phi}_{i}\\in\\mathbb{R}^{6\\times8}\\ (i=1,\\ 2)$\nare required. Even so, from Fig. \\ref{fig:Comb_converg}, we can see\nthe proposed approaches outperform Sapiro's approach. All the methods\nconverge in less than 10 iterations, among which II + cTKSVD leads\nto the highest PSNR value.\n\nThen the proposed approaches are compared with various other approaches\nwhen the number of measurements ($M_{i}\\ (i=1,\\ 2)$) and the noise\nvariance ($\\sigma^{2}$) vary. Specifically, using the notation employed\npreviously and by denoting the method of combining sensing matrix\ndesign with that of the dictionary learning using a ``+'', the methods\nfor comparison are: II + TKSVD, Gaussian + cTKSVD, Sapiro's + cKSVD\nand SS + KHOSVD. In these approaches, II + TKSVD and SS + KHOSVD are\nuncoupled methods; Gaussian + cTKSVD does not involve sensing matrix\noptimization; Sapiro's + cKSVD is for conventional CS system only. \n\nThe results are shown in Fig. \\ref{fig:comb_M_sigma}. We can see\nthat the proposed approaches obtain higher PSNR values than all of\nthe other methods and II + cTKSVD performs best. To see the gain of\ncoupling sensing matrices during dictionary learning and optimizing\nthe sensing matrices, respectively, we compare II + cTKSVD with II\n+ TKSVD and Gaussian + cTKSVD. For instance, when $M_{i}=5,\\ \\sigma^{2}=0$,\nII + cTKSVD has a gain of about 3dB over II + TKSVD and nearly 9dB\nover Gaussian + cTKSVD. Although Sapiro's + cKSVD has a similar performance\nto ours at some specific settings, it is not for a TCS system that\nrequires multiple separable sensing matrices. Examples of reconstructed\nimages using these methods are demonstrated in Fig. \\ref{fig:gallary}\nand \\ref{fig:gallary-1} with the corresponding PSNR values listed.\nAll of the conducted simulations verify that the proposed methods\nof multidimensional sensing matrix and dictionary optimization improve\nthe performance of a TCS system.\n\n\\begin{figure}\n\\centering{}\\includegraphics{figures/gallary_M6_new}\\caption{Reconstruction example when $M_{1}=M_{2}=6$. The images from left\nto right, top to bottom and their PSNR (dB) values are: II+cTKSVD\n(35.41), I+cTKSVD (34.97), Sapiro's+cKSVD (33.64), II+TKSVD (33.57),\nSS+KHOSVD (28.62), Gaussian+cTKSVD (28.05). \\label{fig:gallary}}\n\\end{figure}\n\\begin{figure}\n\\centering{}\\includegraphics{figures/gallary_M4_new}\\caption{Reconstruction example when $M_{1}=M_{2}=4$. The images from left\nto right, top to bottom and their PSNR (dB) values are: II+cTKSVD\n(29.91), I+cTKSVD (29.45), Sapiro's+cKSVD (28.72), II+TKSVD (26.60),\nSS+KHOSVD (22.62), Gaussian+cTKSVD (21.94). \\label{fig:gallary-1}}\n\\end{figure}\n\n\n\n\\section{Conclusions}\n\n\\label{sec:conclusions}\n\nIn this paper, we propose to jointly optimize the multidimensional\nsensing matrix and dictionary for TCS systems. To obtain the optimized\nsensing matrices, a separable approach with closed form solutions\nhas been presented and a joint iterative approach with novel design\nmeasures has also been proposed. The iterative approach certainly\nhas higher complexity, but also exhibits better performance. An approach\nto learning the multidimensional dictionary has been designed, which\nexplicitly takes the multidimensional structure into account and removes\nthe redundant updates in the existing multilinear approaches in the\nliterature. Further gain is obtained by coupling the multidimensional\nsensing matrix while learning the dictionary. The performance advantage\nof the proposed approaches has been demonstrated by experiments using\nboth synthetic data and real images.\n\n\n\\appendices{}\n\n\n\\section{Proof of Theorem 3}\n\n\\label{sec:AppendixI}\n\nAssume $\\mathbf{A}_{i}=\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}=\\mathbf{U}_{\\mathbf{A}_{i}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{i}} & \\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{i}}^{T}$ is an SVD of $\\mathbf{A}_{i}$ for $i=1,\\ 2$ and $rank(\\mathbf{A}_{i})=M_{i}$.\nThen the objective we want to minimize in (\\ref{eq:Approach I}) can\nbe rewritten as:\n\n\\textit{\\footnotesize{}\n", "index": 83, "text": "\n\\[\n\\left|\\left|\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\mathbf{V}_{\\mathbf{A}_{2}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{2}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{2}}^{T})\\otimes(\\mathbf{V}_{\\mathbf{A}_{1}}\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{1}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{1}}^{T})\\right|\\right|{}_{F}^{2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\left|\\left|\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\mathbf{V}_{\\mathbf{A}_{2}}%&#10;\\left[\\begin{array}[]{cc}\\boldsymbol{\\Lambda}_{\\mathbf{A}_{2}}^{2}&amp;\\mathbf{0}%&#10;\\\\&#10;\\mathbf{0}&amp;\\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{2}}^{T})\\otimes%&#10;(\\mathbf{V}_{\\mathbf{A}_{1}}\\left[\\begin{array}[]{cc}\\boldsymbol{\\Lambda}_{%&#10;\\mathbf{A}_{1}}^{2}&amp;\\mathbf{0}\\\\&#10;\\mathbf{0}&amp;\\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{1}}^{T})\\right|%&#10;\\right|{}_{F}^{2}.\" display=\"block\"><mrow><mo>|</mo><mo>|</mo><msub><mi>\ud835\udc08</mi><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></msub><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>2</mn></msub></msub><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeb2</mi><msub><mi>\ud835\udc00</mi><mn>2</mn></msub><mn>2</mn></msubsup></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>2</mn></msub><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2297</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>1</mn></msub></msub><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeb2</mi><msub><mi>\ud835\udc00</mi><mn>1</mn></msub><mn>2</mn></msubsup></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>1</mn></msub><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>|</mo><mo>|</mo><mmultiscripts><mo>.</mo><mprescripts/><mi>F</mi><mn>2</mn></mmultiscripts></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nLet $\\boldsymbol{\\nu}_{\\mathbf{A}_{i}}=[(v_{i})_{1},...,(v_{i})_{M_{i}},\\mathbf{0}]^{T}$,\nthen the sub-vector of the diagonal of $\\boldsymbol{\\Sigma}$ containing\nits non-zero values is: \\textit{\\small{}$\\hat{\\boldsymbol{\\nu}}=[(v_{2})_{1}(v_{1})_{1},...,(v_{2})_{1}(v_{1})_{M_{1}},...,(v_{2})_{M_{2}}(v_{1})_{1},...,(v_{2})_{M_{2}}(v_{1})_{M_{1}}]^{T}$.}\nThus (\\ref{eq:F_norm}) becomes: \n\n", "itemtype": "equation", "pos": 82075, "prevtext": "\n}Denote $\\boldsymbol{\\Sigma}=\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{2}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]\\otimes\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{1}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right]=diag(\\boldsymbol{\\nu}_{\\mathbf{A}_{2}}\\otimes\\boldsymbol{\\nu}_{\\mathbf{A}_{1}}),$ $\\boldsymbol{\\nu}_{\\mathbf{A}_{i}}=diag(\\left[\\begin{array}{cc}\n\\boldsymbol{\\Lambda}_{\\mathbf{A}_{i}}^{2} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right])$, then we have \n\n", "index": 85, "text": "\\begin{equation}\n||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\mathbf{V}_{\\mathbf{A}_{2}}\\otimes\\mathbf{V}_{\\mathbf{A}_{1}})\\boldsymbol{\\Sigma}(\\mathbf{V}_{\\mathbf{A}_{2}}^{T}\\otimes\\mathbf{V}_{\\mathbf{A}_{1}}^{T})||_{F}^{2}.\\label{eq:F_norm}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E50.m1\" class=\"ltx_Math\" alttext=\"||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-(\\mathbf{V}_{\\mathbf{A}_{2}}\\otimes%&#10;\\mathbf{V}_{\\mathbf{A}_{1}})\\boldsymbol{\\Sigma}(\\mathbf{V}_{\\mathbf{A}_{2}}^{T%&#10;}\\otimes\\mathbf{V}_{\\mathbf{A}_{1}}^{T})||_{F}^{2}.\" display=\"block\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></msub><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>2</mn></msub></msub><mo>\u2297</mo><msub><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>1</mn></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udeba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>2</mn></msub><mi>T</mi></msubsup><mo>\u2297</mo><msubsup><mi>\ud835\udc15</mi><msub><mi>\ud835\udc00</mi><mn>1</mn></msub><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07804.tex", "nexttext": "\nTherefore we can obtain that the minimum value of (\\ref{eq:Approach I})\nis $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$, and that it is achieved when\nthe entries of $\\hat{\\boldsymbol{\\nu}}$ are all unity. \n\nClearly $\\boldsymbol{\\Lambda}_{\\mathbf{A}_{i}}$=$\\mathbf{I}_{M_{i}}$\nfor $i=1,\\ 2$ is a solution, i.e., $\\mathbf{A}_{i}=\\mathbf{U}_{\\mathbf{A}_{i}}\\left[\\begin{array}{cc}\n\\mathbf{I}_{M_{i}} & \\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{i}}^{T}$ with $\\mathbf{U}_{\\mathbf{A}_{i}}\\in\\mathbb{R}^{M_{i}\\times M_{i}}$\nand $\\mathbf{V}_{\\mathbf{A}_{i}}\\in\\mathbb{R}^{\\hat{N}_{i}\\times\\hat{N}_{i}}$\nbeing arbitrary orthonormal matrices. Then we would like to find $\\boldsymbol{\\Phi}_{i}\\ (i=1,\\ 2)$\nsuch that $\\boldsymbol{\\Phi}_{i}\\boldsymbol{\\Psi}_{i}=\\mathbf{U}_{\\mathbf{A}_{i}}\\left[\\begin{array}{cc}\n\\mathbf{I}_{M_{i}} & \\mathbf{0}\\end{array}\\right]\\mathbf{V}_{\\mathbf{A}_{i}}^{T}$. Following the derivation of Theorem 2 in \\cite{Li2013}, the solution\nin (\\ref{eq:solutionApproI}) can be found. \n\nWith this solution, for an arbitrary vector $\\mathbf{z}\\in\\mathbb{R}^{\\hat{N}_{i}}$,\nwe have $||\\mathbf{A}_{i}^{T}\\mathbf{z}||_{2}^{2}=tr(\\mathbf{z}^{T}\\mathbf{A}_{i}\\mathbf{A}_{i}^{T}\\mathbf{z})=tr(\\mathbf{z}^{T}\\mathbf{z})=||\\mathbf{z}||_{2}^{2}$,\nwhich indicates that the resulting equivalent sensing matrices $\\mathbf{A}_{i}\\ (i=1,\\ 2)$\nare Parseval tight frames. In addition, we observe that the solution\nin (\\ref{eq:solutionApproI}) can be obtained by separately solving\nthe sub-problems in (\\ref{eq:sub_prob_I}), of which the solutions\nhave been derived in \\cite{Li2013}. By substituting the solutions\nof the sub-problems into (\\ref{eq:Approach I}), we can conclude the\nminimum remains as $\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}$. \n\n\\bibliographystyle{IEEEbib}\n\\bibliography{ProjectDesign,fyr,tensor}\n\n\n", "itemtype": "equation", "pos": 82715, "prevtext": "\nLet $\\boldsymbol{\\nu}_{\\mathbf{A}_{i}}=[(v_{i})_{1},...,(v_{i})_{M_{i}},\\mathbf{0}]^{T}$,\nthen the sub-vector of the diagonal of $\\boldsymbol{\\Sigma}$ containing\nits non-zero values is: \\textit{\\small{}$\\hat{\\boldsymbol{\\nu}}=[(v_{2})_{1}(v_{1})_{1},...,(v_{2})_{1}(v_{1})_{M_{1}},...,(v_{2})_{M_{2}}(v_{1})_{1},...,(v_{2})_{M_{2}}(v_{1})_{M_{1}}]^{T}$.}\nThus (\\ref{eq:F_norm}) becomes: \n\n", "index": 87, "text": "\\begin{equation}\n||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-\\boldsymbol{\\Sigma}||_{F}^{2}=\\hat{N}_{1}\\hat{N}_{2}-M_{1}M_{2}+\\sum_{p=1}^{M_{2}}\\sum_{q=1}^{M_{1}}(1-(v_{2})_{p}(v)_{q})^{2}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E51.m1\" class=\"ltx_Math\" alttext=\"||\\mathbf{I}_{\\hat{N}_{1}\\hat{N}_{2}}-\\boldsymbol{\\Sigma}||_{F}^{2}=\\hat{N}_{1%&#10;}\\hat{N}_{2}-M_{1}M_{2}+\\sum_{p=1}^{M_{2}}\\sum_{q=1}^{M_{1}}(1-(v_{2})_{p}(v)_%&#10;{q})^{2}.\" display=\"block\"><mrow><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc08</mi><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></msub><mo>-</mo><mi>\ud835\udeba</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>N</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mo>-</mo><mrow><msub><mi>M</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>M</mi><mn>2</mn></msub></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mn>2</mn></msub></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mn>1</mn></msub></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>p</mi></msub><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mi>q</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]