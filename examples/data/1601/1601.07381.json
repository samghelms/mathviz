[{"file": "1601.07381.tex", "nexttext": "\n\nActivation functions $f(\\cdot)$ and $g(\\cdot)$, both applied component-wise, are typically implemented as a sigmoidal and identity function, respectively.\nThe former provides the activation column vector of the reservoir neurons, the latter the column vector network output.\nTo increase the overall stability, it is possible \\cite{jaeger2002adaptive} to add a small, uniformly distributed noise term to the state update, before computing the non-linear transformation $f(\\cdot)$. However, in this work we do not consider any noise component, implementing hence a fully deterministic state update mechanism.\nThe output weight matrices $\\mathbf{W}_r^o \\in \\mathbb{R}^{N_r \\times N_o}$ and $\\mathbf{W}_i^o \\in \\mathbb{R}^{N_i \\times N_o}$, which connect, respectively, reservoir and input to the output, represent the readout of the network. The standard training procedure for such matrices requires solving a straightforward regularized least-square problem \\cite{jaeger2001echo}.\n\nEven though the three weight matrices $\\mathbf{W}_{r}^{r}$, $\\mathbf{W}_{o}^{r}$, and $\\mathbf{W}_{i}^{r}$ are generated randomly, they can be suitably designed and scaled to obtain desired properties.\nFor instance, $\\mathbf{W}_{r}^{o}$ can scale with multiplicative constant $\\omega_{o}$; in this work, we set $\\omega_{o} = 0$, which has the effect of removing the output feedback connection.\n$W_i^r$ is controlled by scalar parameter $\\omega_i$.\nSince the gain of the sigmoid non-linearity in the neurons is largest around the origin, the scaling coefficient $\\omega_{i}$ of $\\mathbf{W}_{i}^{r}$ determines the amount of non-linearity introduced by the processing units. \nIn particular, inputs far from zero tend to drive the activation of the neurons towards saturation where they show more non-linearity.\nFinally, the spectral radius of $\\mathbf{W}_r^r$, denoted as $\\rho(\\mathbf{W}_{r}^{r})$ or $\\rho$, controls important properties as discussed in the sequel.\n\n\nAn ESN is typically designed so that the influence of past inputs on the network gradually fades away and the initial state of the reservoir is eventually washed out.\nThis is granted by the Echo State Property (ESP), which ensures that, given any input sequence taken from a compact set, future trajectories of any two different initial states become indistinguishable.\nESP was originally investigated in \\cite{jaeger2001echo} and successively in \\cite{yildiz2012re}; we refer the interested reader to \\cite{manjunath2013echo} for a more recent definition, where also the influence of input is explicitly accounted for.\nIn ESNs with no output feedback, as in our case, the state update of equation (\\ref{eq:esn_state}) reduces to\n\n\n", "itemtype": "equation", "pos": 12759, "prevtext": "\n\n\\maketitle\n\n\n\n\\begin{abstract}\nIn this paper, we elaborate over the well-known interpretability issue in echo state networks.\nThe idea is to investigate the dynamics of reservoir neurons with time-series analysis techniques taken from research on complex systems. Notably, we analyze time-series of neuron activations with Recurrence Plots (RPs) and Recurrence Quantification Analysis (RQA), which permit to visualize and characterize high-dimensional dynamical systems.\nWe show that this approach is useful in a number of ways. \nFirst, the two-dimensional representation offered by RPs provides a way for visualizing the high-dimensional dynamics of a reservoir. Our results suggest that, if the network is stable, reservoir and input denote similar line patterns in the respective RPs. Conversely, the more unstable the ESN, the more the RP of the reservoir presents instability patterns.\nAs a second result, we show that the $\\mathrm{L_{max}}$ measure is highly correlated with the well-established maximal local Lyapunov exponent.\nThis suggests that complexity measures based on RP diagonal lines distribution provide a valuable tool to quantify the degree of network stability.\nFinally, our analysis shows that all RQA measures fluctuate on the proximity of the so-called edge of stability, where an ESN typically achieves maximum computational capability.\nWe verify that the determination of the edge of stability provided by such RQA measures is more accurate than the one offered by the maximal local Lyapunov exponent.\nTherefore, we claim that RPs and RQA-based analyses can be used as valuable tools to design an effective network given a specific problem.\\\\\n\\keywords{Echo state network; Recurrence plot; Recurrence quantification analysis; Time-series; Dynamics.}\n\\end{abstract}\n\n\n\n\\section{Introduction}\n\n\nSince the very first recurrent neural network architectures, attempts have been made to describe and understand the internal dynamics of the system -- see e.g. \\cite{1000129} and references therein. Nowadays, such efforts found renewed interest by those researchers trying to ``open the black-box'' \\cite{schiller2005analyzing,marichal2015analysis,sussillo2013opening,4435137}. This comes natural as recent advances in various fields, such as neurosciences and biophysical systems modeling, demand understanding of the inner mechanism that drives the inductive inference in order to produce novel scientific results \\cite{sussillo2014neural}.\n\n\nReservoir computing is a class of state-space models characterized by a fixed state transition structure, the \\textit{reservoir}, and a trainable, memory-less \\textit{readout} layer \\cite{verstraeten2007experimental,lukovsevivcius2009reservoir,dambre2012information}. A reservoir must be sufficiently complex to capture all salient features of the inputs, behaving as a time dependent, non-linear kernel function, which maps the inputs into a higher dimensional space.\nThe reservoir is typically generated with a pseudo-random procedure, not rarely driven by a set of rules-of-thumb and a trial-and-error approach.\nA popular reservoir computing architecture is the Echo State Network (ESN) \\cite{jaeger2002adaptive}, a recurrent neural network with a non-trainable, sparse recurrent reservoir and an adaptable (usually) linear readout mapping the reservoir to the output.\nESN reservoir characterization and design attracted significant research efforts in the last decade \\cite{strauss2012design,gallicchio2011architectural,ma2014direct}.\nThis is mostly due to the puzzling behavior of the reservoir, which, although randomly initialized, has shown to be effective in modeling non-linear dynamical systems of various nature \\cite{scardapane2015,bianchi2015prediction,liu2012data,7286732,6480841,Skowronski2007414}.\nThis also motivated researchers to better understand, and hence control, the dynamics of reservoirs given the fact they are driven by input signals.\nThe many existing approaches to reservoir design can be roughly partitioned as either supervised or unsupervised \\cite{lukovsevivcius2009reservoir}.\nIn the former case, the output of the system is taken into account to analyze, and then design accordingly, the reservoir.\nIn the latter case, the reservoir is designed by considering algebraic/topological properties of the weight matrix \\cite{xue2007decoupled,boccato2014self,6105577} or criteria based on statistics of the neuron activations \\cite{ozturk2007analysis,schrauwen2008improving}.\nRecently, it was shown that topologies designed according to deterministic criteria \\cite{rodan2012simple,rodan2011minimum} produced state-of-the-art results in all major benchmarks, having a more contractive dynamics with respect to randomly generated reservoirs.\nAnother interesting reservoir design was proposed in \\cite{appeltant2011information}. The authors showed that, for certain tasks, a large (randomly connected) reservoir could be replaced by a single non-linear neuron with delayed feedback.\nIt is worth citing that also the nature of the neuron activation function (spiking vs analog) has been studied in this context \\cite{busing2010connectivity}, suggesting that networks operating with spiking neurons achieve inferior performance at the current state of research, possibly due to the adopted complex pre- and post-processing strategies.\nInput-dependent measures to characterize the reservoir dynamics have been proposed in \\cite{verstraeten2009quantification}, which considered the temporal profile of the Jacobian of the reservoir, rather than static quantities such as the spectral radius.\nAnother input-dependent method for describing the dynamic profile of the reservoir is proposed in \\cite{boedecker2012information}. The authors demonstrated that both transfer entropy (the predictive information provided by a source about a destination that was not already contained in the destination history) and active information storage (the amount of past information that is relevant to predict the next state) calculated on the reservoir neurons are maximized right on the transition to an unstable regime.\n\n\nThe use of Poincar\\'{e} recurrence of states provides fundamental information for the analysis of autonomous dynamical systems \\cite{marwan2007recurrence}.\nThis follows from Poincar\\'{e}'s theorem, which guarantees that the states of a dynamic system must recur during its evolution.\nIn other terms, the system trajectory in phase space must return in an arbitrarily small neighbourhood of any of the previously visited states with probability one.\nRecurrences contain all relevant information regarding a system behavior in phase space and can be linked also with dynamical invariants (e.g., metric entropy) and features related to stability.\nHowever, especially for high-dimensional complex systems, the recurrence time, which is the time elapsed between recurring states, is difficult to calculate even when assuming full analytical knowledge of the system.\n\nRecurrence Plots (RPs) \\cite{marwan2007recurrence,marwan2011avoid,eroglu2014entropy,marwan2013recurrence}, together with the computation of dynamical invariants and heuristic complexity measures called Recurrence Quantification Analysis (RQA), offer a simple yet effective tool to analyze such recurrences starting from a time-series derived from the system under analysis.\nAn RP is a visual representation of recurrence time, which also provides information about the duration of the recurrence by inspecting line patterns \\cite{marwan2005line}.\nRPs are constructed by considering a suitable distance in the phase (equivalently, state) space and a threshold used to determine the recurrence/similarity of states during the evolution of the system.\nRecently, RPs have been extended for studying heterogeneous recurrences \\cite{yang2014heterogeneous}, i.e., qualitatively different types of recurrences in phase space. It is worth citing also the rapidly developing field of recurrence networks \\cite{donner2010recurrence}, whose main goal is to exploit complex network methods to analyze time-series related to dynamical systems.\n\n\nIn this paper, we address the interpretability issue of ESNs by analyzing the dynamics of the activations of the reservoir neurons with RPs and RQA complexity measures. To the best of our knowledge, recurrence analysis has never been used to investigate reservoirs or ESNs.\nRPs and RQA-based techniques allow the designer to visualize and characterize (high-dimensional) dynamical systems starting from a matrix encoding the recurrences of the system states over time.\nWe show that RPs and RQA-based analyses allow to deduce important and consistent conclusions about the behavior of the network. Therefore, we suggest that they can be used as a valuable analysis tool to design a network for the problem at hand.\nThe novelty content of what proposed can be summarized as:\n\\begin{enumerate}\n\t\\item We offer a method to visualize high-dimensional dynamics of input-driven ESNs through a two-dimensional representation encoding recurrences of states. Our results show that, if the network is stable, line patterns of RPs related to reservoir dynamics and those of the input signal are similar. This similarity is lost as soon as the network becomes unstable;\n\t\\item We comment that the degree of instability of the network can be quantified by means of a measure derived from the RP. This measure is named $\\mathrm{L_{max}}$ and is based on the diagonal lines of the RP only (hence it is computationally cost effective). To support this claim, we show that such a measure is correlated with the well-established maximal local Lyapunov exponent over the entire range of the control parameters;\n\t\\item We show that RQA measures can help the application designer to tune important network parameters (e.g., spectral radius and input scaling), in order to push the ESN in a critical state, where computational capability (defined in terms of prediction performance and short-term memory capacity) is maximized. Such a critical state (called edge of stability) depends on the dynamics of the network which, in turn, is influenced by the input signal. Therefore, this cannot be achieved by considering only static measures, e.g., taking into account only the reservoir topology. On the other hand, as we will show here, the proposed method is sensitive also to the nature of the input signal driving the network.\n\\end{enumerate}\n\n\nThis paper is structured as follows.\nIn Sec. \\ref{sec:esn_reservoir_dynamics} we introduce a typical ESN model and discuss some important figures of merit that have been proposed in the literature to characterize a reservoir.\nIn Sec. \\ref{sec:rp_esn}, we present the contribution of our work, i.e., we link recurrence analysis with ESNs and discuss how this new tool can be used in practice to design a network exposing both stability and forecasting accuracy properties.\nIn Sec. \\ref{sec:exp}, we introduce experiments to support our claims. \nFinally, Sec. \\ref{sec:conclusions} offers concluding remarks and future directions.\n\n\n\n\\section{Dynamics of the ESN reservoir}\n\\label{sec:esn_reservoir_dynamics}\n\nA schematic representation of an ESN is shown in Fig. \\ref{fig:esn}.\nAn ESN consists of a reservoir of $N_r$ nodes characterized by a non-linear transfer function $f(\\cdot)$, typically implemented as a hyperbolic tangent. At time instant $k$, the network is driven by the input signal $\\mathbf{x}[k]\\in \\mathbb{R}^{N_i}$ and produces the output $\\mathbf{y}[k] \\in \\mathbb{R}^{N_o}$, being $N_i$ and $N_o$ the dimensionalities of input and output, respectively. The weight matrices $\\mathbf{W}_r^r \\in \\mathbb{R}^{N_r \\times N_r}$ (reservoir internal connections), $\\mathbf{W}_i^r \\in \\mathbb{R}^{N_i \\times N_r}$ (input-to-reservoir connections), and $\\mathbf{W}_o^r \\in \\mathbb{R}^{N_o \\times N_r}$ (output-to-reservoir feedback connections) contain real values in the $[-1, 1]$ interval drawn from a uniform distribution (another common choice is to draw them from a Gaussian distribution).\n\n\\begin{figure}[htp!]\n    \\centering\n    \\includegraphics[scale=1, keepaspectratio]{./ESN}\n    \\caption{The ESN architecture. The circles represent input $\\mathbf{x}$, state, $\\mathbf{h}$, and output, $\\mathbf{y}$, respectively. Solid squares $\\mathbf{W}_{r}^{o}$ and $\\mathbf{W}_{i}^{o}$, are the trainable matrices, respectively, of the readout, while dashed squares, $\\mathbf{W}_{r}^{r}$, $\\mathbf{W}_{o}^{r}$, and $\\mathbf{W}_{i}^{r}$, are randomly initialized matrices. The polygon represents the non-linear transformation performed by neurons and $\\text{z}^{\\text{-1}}$ is the backshift/lag operator.}\n    \\label{fig:esn}\n\\end{figure}\n\n\nAn ESN is then a discrete-time non-linear system with feedback, whose dynamics are described as:\n\n\n", "index": 1, "text": "\\begin{align}\n\\label{eq:esn_state}\n\\mathbf{h}[k] & = f\\left( \\mathbf{W}_{r}^{r} \\mathbf{h}[k-1] + \\mathbf{W}_{i}^{r} \\mathbf{x}[k] + \\mathbf{W}_{o}^{r} \\mathbf{y}[k-1] \\right); \\\\\n\\nonumber\\mathbf{y}[k] & = g\\left( \\mathbf{W}_{r}^{o} \\mathbf{h}[k] + \\mathbf{W}_{i}^{o} \\mathbf{x}[k] \\right).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{h}[k]\" display=\"inline\"><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=f\\left(\\mathbf{W}_{r}^{r}\\mathbf{h}[k-1]+\\mathbf{W}_{i}^{r}%&#10;\\mathbf{x}[k]+\\mathbf{W}_{o}^{r}\\mathbf{y}[k-1]\\right);\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\ud835\udc16</mi><mi>r</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc16</mi><mi>i</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc16</mi><mi>o</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{y}[k]\" display=\"inline\"><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=g\\left(\\mathbf{W}_{r}^{o}\\mathbf{h}[k]+\\mathbf{W}_{i}^{o}\\mathbf%&#10;{x}[k]\\right).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\ud835\udc16</mi><mi>r</mi><mi>o</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc16</mi><mi>i</mi><mi>o</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nIn order to study the stability of the network, we analyze the Jacobian of the state update (\\ref{eq:esn_state_nooutputfeedback}) of the reservoir and generate a derived measure, the maximal local Lyapunov exponent ($\\lambda$). Such a quantity is used to approximate (for an autonomous system) the separation rate in phase space of trajectories having very similar initial states.\n$\\lambda$ was proposed to characterize a reservoir, and demonstrated its efficacy in designing a suitable network configuration in several applications \\cite{verstraeten2006reservoir,verstraeten2009quantification,verstraeten2007experimental}.\n$\\lambda$ is calculated by considering the Jacobian at time $k$, which can be conveniently expressed if neurons are implemented with a \\textit{tanh} activation function as\n\n\n", "itemtype": "equation", "pos": 15744, "prevtext": "\n\nActivation functions $f(\\cdot)$ and $g(\\cdot)$, both applied component-wise, are typically implemented as a sigmoidal and identity function, respectively.\nThe former provides the activation column vector of the reservoir neurons, the latter the column vector network output.\nTo increase the overall stability, it is possible \\cite{jaeger2002adaptive} to add a small, uniformly distributed noise term to the state update, before computing the non-linear transformation $f(\\cdot)$. However, in this work we do not consider any noise component, implementing hence a fully deterministic state update mechanism.\nThe output weight matrices $\\mathbf{W}_r^o \\in \\mathbb{R}^{N_r \\times N_o}$ and $\\mathbf{W}_i^o \\in \\mathbb{R}^{N_i \\times N_o}$, which connect, respectively, reservoir and input to the output, represent the readout of the network. The standard training procedure for such matrices requires solving a straightforward regularized least-square problem \\cite{jaeger2001echo}.\n\nEven though the three weight matrices $\\mathbf{W}_{r}^{r}$, $\\mathbf{W}_{o}^{r}$, and $\\mathbf{W}_{i}^{r}$ are generated randomly, they can be suitably designed and scaled to obtain desired properties.\nFor instance, $\\mathbf{W}_{r}^{o}$ can scale with multiplicative constant $\\omega_{o}$; in this work, we set $\\omega_{o} = 0$, which has the effect of removing the output feedback connection.\n$W_i^r$ is controlled by scalar parameter $\\omega_i$.\nSince the gain of the sigmoid non-linearity in the neurons is largest around the origin, the scaling coefficient $\\omega_{i}$ of $\\mathbf{W}_{i}^{r}$ determines the amount of non-linearity introduced by the processing units. \nIn particular, inputs far from zero tend to drive the activation of the neurons towards saturation where they show more non-linearity.\nFinally, the spectral radius of $\\mathbf{W}_r^r$, denoted as $\\rho(\\mathbf{W}_{r}^{r})$ or $\\rho$, controls important properties as discussed in the sequel.\n\n\nAn ESN is typically designed so that the influence of past inputs on the network gradually fades away and the initial state of the reservoir is eventually washed out.\nThis is granted by the Echo State Property (ESP), which ensures that, given any input sequence taken from a compact set, future trajectories of any two different initial states become indistinguishable.\nESP was originally investigated in \\cite{jaeger2001echo} and successively in \\cite{yildiz2012re}; we refer the interested reader to \\cite{manjunath2013echo} for a more recent definition, where also the influence of input is explicitly accounted for.\nIn ESNs with no output feedback, as in our case, the state update of equation (\\ref{eq:esn_state}) reduces to\n\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:esn_state_nooutputfeedback}\n\\mathbf{h}[k] = f(\\mathbf{W}_{r}^{r}\\mathbf{h}[k-1] + \\mathbf{W}_{i}^{r}\\mathbf{x}[k]).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{h}[k]=f(\\mathbf{W}_{r}^{r}\\mathbf{h}[k-1]+\\mathbf{W}_{i}^{r}\\mathbf{x}%&#10;[k]).\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>\ud835\udc16</mi><mi>r</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc16</mi><mi>i</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $h_l[k]$ is the activation of the $l$-th neuron, with $l=1, 2, ..., N_r$.\n$\\lambda$ is then computed as\n\n", "itemtype": "equation", "pos": 16700, "prevtext": "\n\nIn order to study the stability of the network, we analyze the Jacobian of the state update (\\ref{eq:esn_state_nooutputfeedback}) of the reservoir and generate a derived measure, the maximal local Lyapunov exponent ($\\lambda$). Such a quantity is used to approximate (for an autonomous system) the separation rate in phase space of trajectories having very similar initial states.\n$\\lambda$ was proposed to characterize a reservoir, and demonstrated its efficacy in designing a suitable network configuration in several applications \\cite{verstraeten2006reservoir,verstraeten2009quantification,verstraeten2007experimental}.\n$\\lambda$ is calculated by considering the Jacobian at time $k$, which can be conveniently expressed if neurons are implemented with a \\textit{tanh} activation function as\n\n\n", "index": 5, "text": "\\begin{align}\n\\label{eq:jacob}\n&\\mathbf{J}(h[k]) = \\\\\n&\\nonumber\\left[\\begin{array}{cccc}\n1 - (h_1[k])^2 & 0 & \\ldots & 0 \\\\\n0 & 1 - (h_2[k])^2 & \\ldots & 0 \\\\\n\\vdots & \\vdots  & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & 1 - (h_{N_r}[k])^2 \\\\\n\\end{array}\\right]\n\\mathbf{W}_{r}^{r} \\ ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{J}(h[k])=\" display=\"inline\"><mrow><mrow><mi>\ud835\udc09</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left[\\begin{array}[]{cccc}1-(h_{1}[k])^{2}&amp;0&amp;\\ldots&amp;0\\\\&#10;0&amp;1-(h_{2}[k])^{2}&amp;\\ldots&amp;0\\\\&#10;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\&#10;0&amp;0&amp;\\ldots&amp;1-(h_{N_{r}}[k])^{2}\\\\&#10;\\end{array}\\right]\\mathbf{W}_{r}^{r}\\ ,\" display=\"inline\"><mrow><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22f1</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>h</mi><msub><mi>N</mi><mi>r</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mpadded width=\"+5pt\"><msubsup><mi>\ud835\udc16</mi><mi>r</mi><mi>r</mi></msubsup></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $r_n[k]$ is the module of $n$-th eigenvalue of $\\mathbf{J}(h[k])$ and $K$ is the total number of time-steps in the considered trajectory.\nIn autonomous systems, $\\lambda>0$ indicates that the dynamics is chaotic.\nLocal, first-order approximations provided by Eq. \\ref{eq:jacob} are useful also for studying the stability of a (simplified) reservoir operating around the zero state, $\\mathbf{0}$.\nIn fact, implementing $f(\\cdot)$ as a \\textit{tanh} assures $f(\\mathbf{0})=\\mathbf{0}$, i.e., $\\mathbf{0}$ is a fixed point of the ESN dynamics.\nTherefore, by linearizing (\\ref{eq:esn_state_nooutputfeedback}) around $\\mathbf{0}$ and assuming a zero-input, we obtain from (\\ref{eq:jacob})\n\n\n", "itemtype": "equation", "pos": 17102, "prevtext": "\n\nwhere $h_l[k]$ is the activation of the $l$-th neuron, with $l=1, 2, ..., N_r$.\n$\\lambda$ is then computed as\n\n", "index": 7, "text": "\\begin{equation}\n\\label{eq:MLLE}\n\\lambda = \\max_{n=1, ..., N_r} \\frac{1}{K} \\sum_{k=1}^{K} \\log \\left( r_n[k] \\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\lambda=\\max_{n=1,...,N_{r}}\\frac{1}{K}\\sum_{k=1}^{K}\\log\\left(r_{n}[k]\\right),\" display=\"block\"><mrow><mrow><mi>\u03bb</mi><mo>=</mo><mrow><mrow><munder><mi>max</mi><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>N</mi><mi>r</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><mi>K</mi></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><msub><mi>r</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\n\nTraditional linear stability analysis techniques can be applied to (\\ref{eq:lin_dyn_state}).\nIn particular, if $\\rho(\\mathbf{W}_{r}^{r}) < 1$ then the dynamic around the zero state is stable.\nIn the more general case, the non-linearity of the sigmoid functions in (\\ref{eq:esn_state_nooutputfeedback}) forces the norm of the state vector of the reservoir to remain bounded.\n\nThis means that actually it is possible to find reservoirs (\\ref{eq:esn_state_nooutputfeedback}) having $\\rho(\\mathbf{W}_{r}^{r}) > 1$ while still possessing the ESP with stability when driven by a non-trivial input signal.\n\n\n\nIn the more realistic and useful scenario where the input driving the network is a generic (non-zero) signal, a sufficient condition for the ESP is met if $\\mathbf{W}_{r}^{r}$ is diagonally Schur-stable, i.e., if there exists a positive definite diagonal matrix, $\\mathbf{P}$, such that $(\\mathbf{W}_{r}^{r})^{T} \\mathbf{P} \\mathbf{W}_{r}^{r} - \\mathbf{P}$ is negative definite \\cite{yildiz2012re}.\n\nHowever, this recipe is fairly restrictive in practice. In many cases, such a sufficient condition might generate reservoirs that are not rich enough in terms of provided dynamics, since the use of a conservative scaling factor might compromise the amount of memory in the network and thus the ability to accurately model a given problem.\nTherefore, for most practical purposes, the necessary condition $\\rho(\\mathbf{W}_{r}^{r}) < 1$ is considered ``sufficient in practice'', because if the spectral radius is less than 1, the state update map is contractive with high probability, regardless of the input and given a sufficiently large reservoir \\cite{6105577}.\n\n\nA reservoir should be implemented by considering two conflicting functionalities: offering a vanishing memory of past inputs but also performing a nonlinear transformations of the input, which, however, reduces the memory of the network.\nThe number of reservoir neurons and the bounds on $\\rho$ can be used for a na\\\"{\\i}ve quantification of the computational capability of a reservoir \\cite{yildiz2012re}. However, those are static measures that only consider the algebraic properties of the weight matrix, $\\mathbf{W}_{r}^{r}$, without taking into account other factors, such as the input scaling $\\omega_{i}$ and the particular properties of the given input signals.\nMoreover, it is still not clear, in a mathematical sense, how these stability bounds relate to the actual ESN dynamics when processing non-trivial input signals \\cite{manjunath2013echo}. \nIn this context, the idea of pushing the system toward the so-called ``edge of stability'' (also called edge of criticality) has been explored. \\cite{langton1990computation, bertschinger2004real,legenstein2007edge} have shown that several dynamical systems, among which randomly connected recurrent neural networks, achieved the highest computational capabilities when moving toward the unstable (sometime even chaotic) regime, where the ESP is lost and the system enters into an oscillatory mode.\nThis justifies the use of spectral radii above the unity in some practical applications.\nTypically, the stable--unstable transition is detected numerically by considering the $\\lambda$ index (\\ref{eq:MLLE}).\n\nIt is important to remark that, although the critical state might maximize the computational capability, there are also tasks that either require very little in terms of computation or reservoirs with fast reaction times and fast locking into attractor states (e.g., multistable switching circuits \\cite{jaeger2001short}). Such a behavior is achieved in a regime far from the edge of stability. For these reasons, it is known that systems do not necessarily need to move toward the edge of stability to solve a generic computational tasks \\cite{jaeger2001echo}.\n\nFurther descriptors used for characterizing the dynamics of a reservoir are based on information-theoretic quantities, such as the (average) transfer entropy and the active information storage one \\cite{boedecker2012information}. The authors have shown that such quantities peak right when $\\lambda>0$.\nAlso the minimal singular value \\cite{verstraeten2009quantification} of the Jacobian was demonstrated to be an accurate predictor of ESN performance, providing more consistent information than both $\\lambda$ and $\\rho$.\n\nComplementary to $\\lambda$, the minimal singular value, and the aforementioned information-theoretic descriptors, in this study we propose to use RPs and the related RQA measures for characterizing the dynamics of an ESN reservoir when driven by an input signal.\n\n\n\n\\section{Analyzing ESN dynamics by investigating recurrences of neuron activations}\n\\label{sec:rp_esn}\n\nIn this section, we discuss how the input-driven dynamics of an ESN reservoir (\\ref{eq:esn_state_nooutputfeedback}) can be fruitfully analyzed by means of RP-based techniques.\n\nThe sequence of ESN states can be seen as a multi-variate time-series $\\mathbf{h}$ consisting of $N_r$ state variables coming from the reservoir neuron activations.\nAn RP is constructed by calculating an $K\\times K$ binary matrix $\\mathbf{R}$, where the generic element $R_{ij}$ is defined as\n\n\n", "itemtype": "equation", "pos": 17928, "prevtext": "\n\nwhere $r_n[k]$ is the module of $n$-th eigenvalue of $\\mathbf{J}(h[k])$ and $K$ is the total number of time-steps in the considered trajectory.\nIn autonomous systems, $\\lambda>0$ indicates that the dynamics is chaotic.\nLocal, first-order approximations provided by Eq. \\ref{eq:jacob} are useful also for studying the stability of a (simplified) reservoir operating around the zero state, $\\mathbf{0}$.\nIn fact, implementing $f(\\cdot)$ as a \\textit{tanh} assures $f(\\mathbf{0})=\\mathbf{0}$, i.e., $\\mathbf{0}$ is a fixed point of the ESN dynamics.\nTherefore, by linearizing (\\ref{eq:esn_state_nooutputfeedback}) around $\\mathbf{0}$ and assuming a zero-input, we obtain from (\\ref{eq:jacob})\n\n\n", "index": 9, "text": "\\begin{equation}\n\\label{eq:lin_dyn_state}\n\\mathbf{h}[k] = \\mathbf{J}(\\mathbf{0})\\mathbf{h}[k-1] = \\mathbf{W}_{r}^{r}\\mathbf{h}[k-1].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{h}[k]=\\mathbf{J}(\\mathbf{0})\\mathbf{h}[k-1]=\\mathbf{W}_{r}^{r}\\mathbf{%&#10;h}[k-1].\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udc09</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn/><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc16</mi><mi>r</mi><mi>r</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $K$ is the length of the time-series and $\\Theta(\\cdot)$ the Heaviside function: $\\Theta(x)=0$ if $x<0$; $\\Theta(x)=1$ otherwise.\n$\\tau_{\\mathrm{RP}}>0$ is a user-defined threshold used to identify recurrences.\n$\\tau_{\\mathrm{RP}}$ can be defined in different ways, but typically it represents a percentage of the average or the maximum phase space distance between the states.\nThe selection of an ``optimal'' threshold $\\tau_{\\mathrm{RP}}$ is a problem-dependent issue.\nHowever, we comment that, usually, $\\tau_{\\mathrm{RP}}$ has an impact only on the quantitative information derived from an RP but does not affect the general properties of the system as seen through a recurrence analysis.\n$\\mathbf{R}$ is constructed by considering also a nonnegative dissimilarity measure $d(\\cdot, \\cdot)$, which evaluates the distance between states.\nTypical examples include the Euclidean, Manhattan, or max-norm distances.\nSeveral further options have been exploited in the literature for $\\tau_{\\mathrm{RP}}$ and $d(\\cdot, \\cdot)$; we refer the reader to \\cite{marwan2007recurrence} for further detailed discussions.\nFig. \\ref{fig:RP_build} depicts the algorithmic steps requested to generate an RP on ESN states.\n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=\\columnwidth]{./RP_build}\n\\caption{When $\\mathbf{x}[k]$ is fed as input to the $N_r$ neurons of the ESN reservoir, the internal state is updated to $\\mathbf{h}[k] = [ h_1[k], h_2[k], \\ldots ,h_{N_r}[k] ]^T$, where $h_n[k]$ is the output of the $n$-th neuron. Once the time-series $\\mathbf{h}$ is generated, the RP is constructed by using a threshold $\\tau_{\\mathrm{RP}}$ and a dissimilarity measure $d(\\cdot, \\cdot)$. If $d(\\mathbf{h}[k], \\mathbf{h}[i]) \\leq \\tau_{\\mathrm{RP}}$, the cell of the RP in position $(k,i)$ is colored in black, otherwise it is left white. The elements in gray highlight the operations performed at time-step $k$.}\n\\label{fig:RP_build}\n\\end{figure}\n\n\nDepending on the properties of the analyzed time-series, different line patterns emerge in an RP \\cite{marwan2005line}.\nBesides providing an immediate visualization of the system properties, starting from $\\mathbf{R}$, it is possible to compute also several complexity measures, those associated with an RQA.\nSuch measures are defined by the distribution of both vertical/horizontal and diagonal line structures present in the RP and provide a numerical characterization of the properties associated with the underlying dynamics.\n\nIn Sec. \\ref{sec:rqa}, we provide details about the RQA measures considered in this study.\nIn Sec. \\ref{sec:design_stable_effective_net}, we discuss how a network designer should take into account the two important issues of stability and computational capability by using the tools here introduced.\nFinally, in Sec. \\ref{sec:visual_id} we explain how to interpret the RP and related RQA measures relative to ESN state sequences driven by different classes of inputs.\n\n\n\\subsection{RQA complexity measures}\n\\label{sec:rqa}\n\nWe assume here to have generated $\\mathbf{R}$ according to (\\ref{eq:RM}) on a time-series of length $K$.\nMany of the following measures are based on the histograms $P(l)$ and $P(v)$, counting, respectively, the number of diagonal and vertical lines having specific lengths $l$ and $v$,\n\n\n", "itemtype": "equation", "pos": 23238, "prevtext": "\n\n\nTraditional linear stability analysis techniques can be applied to (\\ref{eq:lin_dyn_state}).\nIn particular, if $\\rho(\\mathbf{W}_{r}^{r}) < 1$ then the dynamic around the zero state is stable.\nIn the more general case, the non-linearity of the sigmoid functions in (\\ref{eq:esn_state_nooutputfeedback}) forces the norm of the state vector of the reservoir to remain bounded.\n\nThis means that actually it is possible to find reservoirs (\\ref{eq:esn_state_nooutputfeedback}) having $\\rho(\\mathbf{W}_{r}^{r}) > 1$ while still possessing the ESP with stability when driven by a non-trivial input signal.\n\n\n\nIn the more realistic and useful scenario where the input driving the network is a generic (non-zero) signal, a sufficient condition for the ESP is met if $\\mathbf{W}_{r}^{r}$ is diagonally Schur-stable, i.e., if there exists a positive definite diagonal matrix, $\\mathbf{P}$, such that $(\\mathbf{W}_{r}^{r})^{T} \\mathbf{P} \\mathbf{W}_{r}^{r} - \\mathbf{P}$ is negative definite \\cite{yildiz2012re}.\n\nHowever, this recipe is fairly restrictive in practice. In many cases, such a sufficient condition might generate reservoirs that are not rich enough in terms of provided dynamics, since the use of a conservative scaling factor might compromise the amount of memory in the network and thus the ability to accurately model a given problem.\nTherefore, for most practical purposes, the necessary condition $\\rho(\\mathbf{W}_{r}^{r}) < 1$ is considered ``sufficient in practice'', because if the spectral radius is less than 1, the state update map is contractive with high probability, regardless of the input and given a sufficiently large reservoir \\cite{6105577}.\n\n\nA reservoir should be implemented by considering two conflicting functionalities: offering a vanishing memory of past inputs but also performing a nonlinear transformations of the input, which, however, reduces the memory of the network.\nThe number of reservoir neurons and the bounds on $\\rho$ can be used for a na\\\"{\\i}ve quantification of the computational capability of a reservoir \\cite{yildiz2012re}. However, those are static measures that only consider the algebraic properties of the weight matrix, $\\mathbf{W}_{r}^{r}$, without taking into account other factors, such as the input scaling $\\omega_{i}$ and the particular properties of the given input signals.\nMoreover, it is still not clear, in a mathematical sense, how these stability bounds relate to the actual ESN dynamics when processing non-trivial input signals \\cite{manjunath2013echo}. \nIn this context, the idea of pushing the system toward the so-called ``edge of stability'' (also called edge of criticality) has been explored. \\cite{langton1990computation, bertschinger2004real,legenstein2007edge} have shown that several dynamical systems, among which randomly connected recurrent neural networks, achieved the highest computational capabilities when moving toward the unstable (sometime even chaotic) regime, where the ESP is lost and the system enters into an oscillatory mode.\nThis justifies the use of spectral radii above the unity in some practical applications.\nTypically, the stable--unstable transition is detected numerically by considering the $\\lambda$ index (\\ref{eq:MLLE}).\n\nIt is important to remark that, although the critical state might maximize the computational capability, there are also tasks that either require very little in terms of computation or reservoirs with fast reaction times and fast locking into attractor states (e.g., multistable switching circuits \\cite{jaeger2001short}). Such a behavior is achieved in a regime far from the edge of stability. For these reasons, it is known that systems do not necessarily need to move toward the edge of stability to solve a generic computational tasks \\cite{jaeger2001echo}.\n\nFurther descriptors used for characterizing the dynamics of a reservoir are based on information-theoretic quantities, such as the (average) transfer entropy and the active information storage one \\cite{boedecker2012information}. The authors have shown that such quantities peak right when $\\lambda>0$.\nAlso the minimal singular value \\cite{verstraeten2009quantification} of the Jacobian was demonstrated to be an accurate predictor of ESN performance, providing more consistent information than both $\\lambda$ and $\\rho$.\n\nComplementary to $\\lambda$, the minimal singular value, and the aforementioned information-theoretic descriptors, in this study we propose to use RPs and the related RQA measures for characterizing the dynamics of an ESN reservoir when driven by an input signal.\n\n\n\n\\section{Analyzing ESN dynamics by investigating recurrences of neuron activations}\n\\label{sec:rp_esn}\n\nIn this section, we discuss how the input-driven dynamics of an ESN reservoir (\\ref{eq:esn_state_nooutputfeedback}) can be fruitfully analyzed by means of RP-based techniques.\n\nThe sequence of ESN states can be seen as a multi-variate time-series $\\mathbf{h}$ consisting of $N_r$ state variables coming from the reservoir neuron activations.\nAn RP is constructed by calculating an $K\\times K$ binary matrix $\\mathbf{R}$, where the generic element $R_{ij}$ is defined as\n\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:RM}\nR_{ij} = \\Theta(\\tau_{\\mathrm{RP}} - d(\\mathbf{h}[i], \\mathbf{h}[j])), \\ \\ 1\\leq i,j \\leq K\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"R_{ij}=\\Theta(\\tau_{\\mathrm{RP}}-d(\\mathbf{h}[i],\\mathbf{h}[j])),\\ \\ 1\\leq i,j\\leq&#10;K\" display=\"block\"><mrow><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c4</mi><mi>RP</mi></msub><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mn>\u2005\u20051</mn><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>\u2264</mo><mi>K</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nSuch RQA measures are standard in recurrence analysis and cover different aspects of the dynamics (e.g., features related to the stability and complexity of the system evolution).\nAbbreviations are kept consistent with \\cite{marwan2007recurrence}.\\\\\n\n\\textbf{RR.}\nThe recurrence rate is a measure of the density of recurrences in $\\mathbf{R}$,\n\n\n", "itemtype": "equation", "pos": 26666, "prevtext": "\n\nwhere $K$ is the length of the time-series and $\\Theta(\\cdot)$ the Heaviside function: $\\Theta(x)=0$ if $x<0$; $\\Theta(x)=1$ otherwise.\n$\\tau_{\\mathrm{RP}}>0$ is a user-defined threshold used to identify recurrences.\n$\\tau_{\\mathrm{RP}}$ can be defined in different ways, but typically it represents a percentage of the average or the maximum phase space distance between the states.\nThe selection of an ``optimal'' threshold $\\tau_{\\mathrm{RP}}$ is a problem-dependent issue.\nHowever, we comment that, usually, $\\tau_{\\mathrm{RP}}$ has an impact only on the quantitative information derived from an RP but does not affect the general properties of the system as seen through a recurrence analysis.\n$\\mathbf{R}$ is constructed by considering also a nonnegative dissimilarity measure $d(\\cdot, \\cdot)$, which evaluates the distance between states.\nTypical examples include the Euclidean, Manhattan, or max-norm distances.\nSeveral further options have been exploited in the literature for $\\tau_{\\mathrm{RP}}$ and $d(\\cdot, \\cdot)$; we refer the reader to \\cite{marwan2007recurrence} for further detailed discussions.\nFig. \\ref{fig:RP_build} depicts the algorithmic steps requested to generate an RP on ESN states.\n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=\\columnwidth]{./RP_build}\n\\caption{When $\\mathbf{x}[k]$ is fed as input to the $N_r$ neurons of the ESN reservoir, the internal state is updated to $\\mathbf{h}[k] = [ h_1[k], h_2[k], \\ldots ,h_{N_r}[k] ]^T$, where $h_n[k]$ is the output of the $n$-th neuron. Once the time-series $\\mathbf{h}$ is generated, the RP is constructed by using a threshold $\\tau_{\\mathrm{RP}}$ and a dissimilarity measure $d(\\cdot, \\cdot)$. If $d(\\mathbf{h}[k], \\mathbf{h}[i]) \\leq \\tau_{\\mathrm{RP}}$, the cell of the RP in position $(k,i)$ is colored in black, otherwise it is left white. The elements in gray highlight the operations performed at time-step $k$.}\n\\label{fig:RP_build}\n\\end{figure}\n\n\nDepending on the properties of the analyzed time-series, different line patterns emerge in an RP \\cite{marwan2005line}.\nBesides providing an immediate visualization of the system properties, starting from $\\mathbf{R}$, it is possible to compute also several complexity measures, those associated with an RQA.\nSuch measures are defined by the distribution of both vertical/horizontal and diagonal line structures present in the RP and provide a numerical characterization of the properties associated with the underlying dynamics.\n\nIn Sec. \\ref{sec:rqa}, we provide details about the RQA measures considered in this study.\nIn Sec. \\ref{sec:design_stable_effective_net}, we discuss how a network designer should take into account the two important issues of stability and computational capability by using the tools here introduced.\nFinally, in Sec. \\ref{sec:visual_id} we explain how to interpret the RP and related RQA measures relative to ESN state sequences driven by different classes of inputs.\n\n\n\\subsection{RQA complexity measures}\n\\label{sec:rqa}\n\nWe assume here to have generated $\\mathbf{R}$ according to (\\ref{eq:RM}) on a time-series of length $K$.\nMany of the following measures are based on the histograms $P(l)$ and $P(v)$, counting, respectively, the number of diagonal and vertical lines having specific lengths $l$ and $v$,\n\n\n", "index": 13, "text": "\\begin{align}\n\\nonumber P(l)&=\\sum_{i,j=1}^{K} (1-R_{i-1,j-1})(1-R_{i+l,j+l})\\prod_{k=0}^{l-1} R_{i+k,j+k}; \\\\\n\\nonumber P(v)&=\\sum_{i,j=1}^{K} (1-R_{i,j})(1-R_{i,j+v})\\prod_{k=0}^{v-1} R_{i,j+k}. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(l)\" display=\"inline\"><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i,j=1}^{K}(1-R_{i-1,j-1})(1-R_{i+l,j+l})\\prod_{k=0}^{l-1}R%&#10;_{i+k,j+k};\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>R</mi><mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>R</mi><mrow><mrow><mi>i</mi><mo>+</mo><mi>l</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mi>l</mi></mrow></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><msub><mi>R</mi><mrow><mrow><mi>i</mi><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mi>k</mi></mrow></mrow></msub></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(v)\" display=\"inline\"><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i,j=1}^{K}(1-R_{i,j})(1-R_{i,j+v})\\prod_{k=0}^{v-1}R_{i,j+%&#10;k}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mi>v</mi></mrow></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>v</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mi>k</mi></mrow></mrow></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nGiven Eq. \\ref{eq:RM}, RR corresponds to the correlation sum, an important concept used in chaos theory.\nRR could help also with the selection of $\\tau_{\\mathrm{RP}}$ when performing multiple tests on different conditions, e.g., by preserving the rate.\\\\\n\n\\textbf{DET.}\nA measure of the determinism level of the system, based on the percentage of diagonal lines of minimum length $l_{\\mathrm{min}}$,\n\n\n", "itemtype": "equation", "pos": 27222, "prevtext": "\n\nSuch RQA measures are standard in recurrence analysis and cover different aspects of the dynamics (e.g., features related to the stability and complexity of the system evolution).\nAbbreviations are kept consistent with \\cite{marwan2007recurrence}.\\\\\n\n\\textbf{RR.}\nThe recurrence rate is a measure of the density of recurrences in $\\mathbf{R}$,\n\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eq:RR}\n\\mathrm{RR}=\\frac{1}{K^2}\\sum_{i,j=1}^{K} R_{ij}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{RR}=\\frac{1}{K^{2}}\\sum_{i,j=1}^{K}R_{ij}.\" display=\"block\"><mrow><mrow><mi>RR</mi><mo>=</mo><mrow><mfrac><mn>1</mn><msup><mi>K</mi><mn>2</mn></msup></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nA periodic system would have $\\mathrm{DET}$ close to unity and close to zero for a signal with no time-dependency.\nThe threshold $l_{\\mathrm{min}}$ can be used also to discriminate different forms of determinism, such as periodic and chaotic motions. In fact, it is known that in chaotic dynamics diagonal lines are very short -- due to exponential divergence in phase space.\\\\\n\n$\\textbf{L}_{\\textbf{max}}.$\nMaximum diagonal line length,\n\n\n", "itemtype": "equation", "pos": 27720, "prevtext": "\n\nGiven Eq. \\ref{eq:RM}, RR corresponds to the correlation sum, an important concept used in chaos theory.\nRR could help also with the selection of $\\tau_{\\mathrm{RP}}$ when performing multiple tests on different conditions, e.g., by preserving the rate.\\\\\n\n\\textbf{DET.}\nA measure of the determinism level of the system, based on the percentage of diagonal lines of minimum length $l_{\\mathrm{min}}$,\n\n\n", "index": 17, "text": "\\begin{equation}\n\\label{eq:DET}\n\\mathrm{DET}=\\frac{\\sum_{l=l_{\\mathrm{min}}}^{K} lP(l)}{\\sum_{l=1}^{K} lP(l)} \\in[0, 1].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{DET}=\\frac{\\sum_{l=l_{\\mathrm{min}}}^{K}lP(l)}{\\sum_{l=1}^{K}lP(l)}\\in%&#10;[0,1].\" display=\"block\"><mrow><mrow><mi>DET</mi><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><msub><mi>l</mi><mi>min</mi></msub></mrow><mi>K</mi></msubsup><mrow><mi>l</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>l</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\nwhere $1\\leq\\mathrm{L_{max}}\\leq\\sqrt{2}K$.\nThis measure, although heuristic in principle, is related to the mean exponential divergence in phase space.\nA (heuristic) measure of divergence can be obtained as\n\n\n", "itemtype": "equation", "pos": 28296, "prevtext": "\n\nA periodic system would have $\\mathrm{DET}$ close to unity and close to zero for a signal with no time-dependency.\nThe threshold $l_{\\mathrm{min}}$ can be used also to discriminate different forms of determinism, such as periodic and chaotic motions. In fact, it is known that in chaotic dynamics diagonal lines are very short -- due to exponential divergence in phase space.\\\\\n\n$\\textbf{L}_{\\textbf{max}}.$\nMaximum diagonal line length,\n\n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq:lmax}\n\\mathrm{L_{max}}=\\max\\{l_i\\}_{i=1}^{N_l},\\ \\ N_l=\\sum_{l\\geq l_{\\mathrm{min}}} P(l),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{L_{max}}=\\max\\{l_{i}\\}_{i=1}^{N_{l}},\\ \\ N_{l}=\\sum_{l\\geq l_{\\mathrm{%&#10;min}}}P(l),\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">L</mi><mi>max</mi></msub><mo>=</mo><mi>max</mi><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>l</mi></msub></msubsup><mo rspace=\"12.5pt\">,</mo><msub><mi>N</mi><mi>l</mi></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2265</mo><msub><mi>l</mi><mi>min</mi></msub></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nChaotic systems do not present long diagonal lines, since trajectories diverge exponentially fast. As formally discussed in \\cite{marwan2007recurrence}, it is possible to relate $\\mathrm{L_{max}}$ with the correlation entropy of the system -- that is a dynamical invariant measure. Notably, correlation entropy can be estimated by using the diagonal lines distribution.\nThe correlation entropy is a lower-bound for the sum of the positive Lyapunov exponents, providing thus a formal connection with the analysis of sensitivity on initial conditions.\\\\\n\n\\textbf{LAM.}\nLaminarity is a descriptor of the presence of laminar phases in the system,\n\n\n", "itemtype": "equation", "pos": 28638, "prevtext": "\nwhere $1\\leq\\mathrm{L_{max}}\\leq\\sqrt{2}K$.\nThis measure, although heuristic in principle, is related to the mean exponential divergence in phase space.\nA (heuristic) measure of divergence can be obtained as\n\n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:div}\n\\mathrm{DIV}=1/\\mathrm{L_{max}}\\in(0, 1].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{DIV}=1/\\mathrm{L_{max}}\\in(0,1].\" display=\"block\"><mrow><mrow><mi>DIV</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><msub><mi mathvariant=\"normal\">L</mi><mi>max</mi></msub></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $v_{\\mathrm{min}}$ is a threshold for the minimal vertical line length to be considered.\nLaminar phases denote states of the system that do not change or change very slowly for a number of consecutive time-steps.\\\\\n\n\\textbf{ENTR.}\nA complexity measure of an RP with respect to the diagonal lines distribution,\n\n\n", "itemtype": "equation", "pos": 29372, "prevtext": "\n\nChaotic systems do not present long diagonal lines, since trajectories diverge exponentially fast. As formally discussed in \\cite{marwan2007recurrence}, it is possible to relate $\\mathrm{L_{max}}$ with the correlation entropy of the system -- that is a dynamical invariant measure. Notably, correlation entropy can be estimated by using the diagonal lines distribution.\nThe correlation entropy is a lower-bound for the sum of the positive Lyapunov exponents, providing thus a formal connection with the analysis of sensitivity on initial conditions.\\\\\n\n\\textbf{LAM.}\nLaminarity is a descriptor of the presence of laminar phases in the system,\n\n\n", "index": 23, "text": "\\begin{equation}\n\\label{eq:lam}\n\\mathrm{LAM}=\\frac{\\sum_{v=v_{\\mathrm{min}}}^{K} vP(v)}{\\sum_{v=1}^{K} vP(v)}\\in[0, 1],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{LAM}=\\frac{\\sum_{v=v_{\\mathrm{min}}}^{K}vP(v)}{\\sum_{v=1}^{K}vP(v)}\\in%&#10;[0,1],\" display=\"block\"><mrow><mrow><mi>LAM</mi><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>=</mo><msub><mi>v</mi><mi>min</mi></msub></mrow><mi>K</mi></msubsup><mrow><mi>v</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>v</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $p(l)=P(l)/N_l$. As usual, $0\\leq\\mathrm{ENTR}\\leq\\log(K)$.\nSignals with no time-dependence present $\\mathrm{ENTR}\\simeq 0$, i.e., the diagonal lines distribution is fully concentrated on very short lines (e.g., single dots).\nConversely, such a measure assumes high values when the diagonal lines distribution become heterogeneous.\\\\\n\n\\textbf{SWRP.}\nEntropy of a weighted RP. Such a measure has been recently introduced in \\cite{eroglu2014entropy} and provides an alternative to ENTR (\\ref{eq:ENTR}). It quantifies the complexity of scalar distributions of \\textit{strengths} for each time-step, providing a solution for the border effects that might appear in the computation of ENTR.\nThe computation of SWRP considers the similarity matrix $\\mathbf{\\tilde{S}}$, whose elements are defined as $\\tilde{S}_{ij} = \\exp(-d(\\mathbf{h}[i], \\mathbf{h}[j]))$, where $d(\\cdot, \\cdot)$ is the same as in Eq. (\\ref{eq:RM}).\nThe results do not depend on $\\tau_{\\mathrm{RP}}$ and are not influenced by the length $K$ of the time-series.\nThe strengths are used to measure the heterogeneity of the density of a given point in phase space.\nThe $i$-th strength is defined as: $s_i = \\sum_{j=1}^K \\tilde{S}_{ij}$.\nThe distribution of the strengths is represented with histograms using a user-defined number $B$ of bins.\nSWRP measure is then defined as\n\n\n", "itemtype": "equation", "pos": 29825, "prevtext": "\n\nwhere $v_{\\mathrm{min}}$ is a threshold for the minimal vertical line length to be considered.\nLaminar phases denote states of the system that do not change or change very slowly for a number of consecutive time-steps.\\\\\n\n\\textbf{ENTR.}\nA complexity measure of an RP with respect to the diagonal lines distribution,\n\n\n", "index": 25, "text": "\\begin{equation}\n\\label{eq:ENTR}\n\\mathrm{ENTR}=-\\sum_{l=l_{\\mathrm{min}}}^{K} p(l) \\ln(p(l)),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{ENTR}=-\\sum_{l=l_{\\mathrm{min}}}^{K}p(l)\\ln(p(l)),\" display=\"block\"><mrow><mrow><mi>ENTR</mi><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><msub><mi>l</mi><mi>min</mi></msub></mrow><mi>K</mi></munderover><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nwhere $P(b)$ is the number of elements in the $b$-th bin and $p(b) = P(b)/K$ is the respective probability; as usual, $0\\leq\\mathrm{SWRP}\\leq\\log(B)$.\n\n\n\\subsection{Design of a stable and effective network}\n\\label{sec:design_stable_effective_net}\n\nRecurrence analysis can be used to (i) guarantee stability for a given configuration and (ii) tune the network, so that the highest computational capability can be achieved.\nThese aspects are treated in the sequel.\\\\\n\n\\noindent\\textit{\\textbf{Stability issue}}\\\\\nConstructing an RP from $\\mathbf{h}$ allows to investigate the network stability issue.\nAs mentioned before, the degree of stability can be calculated with $\\mathrm{L_\\mathrm{max}}$ (\\ref{eq:lmax}): the higher $\\mathrm{L_\\mathrm{max}}$, the more stable the system.\nIn addition to $\\mathrm{L_\\mathrm{max}}$, the designer can have an immediate visual interpretation of stability by inspecting RP: short and erratic diagonal lines in RPs denote instability/chaoticity, while long diagonal lines denote regularity (e.g., a periodic motion). We show in the experimental section that $\\mathrm{L_\\mathrm{max}}$ is anticorrelated with $\\lambda$ and hence it can be considered as a reliable indicator for the (input-dependent) degree of network stability.\nIn the experiments we highlight also that, if the network is stable, the dynamics of the input and reservoir produce very similar line patterns in the respective RPs.\nIf the network is unstable, then the aforementioned similarity is lost.\nTherefore, RPs can be used by the designer as visual tools to analyze the response of the network to a specific input.\\\\\n\n\\noindent\\textit{\\textbf{Computational capability issue}}\\\\\nA widespread criterion to determine the (signal-dependent) edge of stability relies on the sign of $\\lambda$, which becomes positive when the system enters into a (globally) unstable regime.\nWe propose, instead, to use information derived from RR (\\ref{eq:RR}), DET (\\ref{eq:DET}), LAM (\\ref{eq:lam}), ENTR (\\ref{eq:ENTR}), and SWRP (\\ref{eq:swrp}) for this purpose. We observed that, when such indexes start to fluctuate, the network achieves high prediction accuracy. This provides a guideline to the network designer for setting critical parameters, such as $\\rho$ and $\\omega_i$, in presence of input.\nRQA measures, when evaluated on ESNs initialized with different weight matrices, assume very different values given $\\rho$ and $\\omega_i$ when the network in unstable.\nAccordingly, it emerges that the standard deviation for such RQA measures is very high in such unstable regimes. We propose to identify the configurations of $\\rho$ and $\\omega_i$ which bring the system to the edge of stability with those for which we observe a sudden increment in the standard deviation.\nLet $P$ and $\\Omega$ be the domains for $\\rho$ and $\\omega_i$, respectively.\nIn this study, we calculate such configurations by checking when the standard deviations of RQA measures assume values grater than their mean values computed over $P\\times\\Omega$. \nFor a given RQA measure, say $q$, we identify the edge of stability as a set of pairs $\\{ (\\rho^{1}, \\omega_i^{1} ), (\\rho^{2}, \\omega_i^{2}), ..., (\\rho^{|\\Omega|}, \\omega_i^{|\\Omega|}) \\} \\subset P \\times \\Omega$.\nFor every $\\omega_i^j \\in \\Omega$, we select the largest $\\rho^j\\in P$ such that\n\n\n", "itemtype": "equation", "pos": 31277, "prevtext": "\n\nwhere $p(l)=P(l)/N_l$. As usual, $0\\leq\\mathrm{ENTR}\\leq\\log(K)$.\nSignals with no time-dependence present $\\mathrm{ENTR}\\simeq 0$, i.e., the diagonal lines distribution is fully concentrated on very short lines (e.g., single dots).\nConversely, such a measure assumes high values when the diagonal lines distribution become heterogeneous.\\\\\n\n\\textbf{SWRP.}\nEntropy of a weighted RP. Such a measure has been recently introduced in \\cite{eroglu2014entropy} and provides an alternative to ENTR (\\ref{eq:ENTR}). It quantifies the complexity of scalar distributions of \\textit{strengths} for each time-step, providing a solution for the border effects that might appear in the computation of ENTR.\nThe computation of SWRP considers the similarity matrix $\\mathbf{\\tilde{S}}$, whose elements are defined as $\\tilde{S}_{ij} = \\exp(-d(\\mathbf{h}[i], \\mathbf{h}[j]))$, where $d(\\cdot, \\cdot)$ is the same as in Eq. (\\ref{eq:RM}).\nThe results do not depend on $\\tau_{\\mathrm{RP}}$ and are not influenced by the length $K$ of the time-series.\nThe strengths are used to measure the heterogeneity of the density of a given point in phase space.\nThe $i$-th strength is defined as: $s_i = \\sum_{j=1}^K \\tilde{S}_{ij}$.\nThe distribution of the strengths is represented with histograms using a user-defined number $B$ of bins.\nSWRP measure is then defined as\n\n\n", "index": 27, "text": "\\begin{equation}\n\\label{eq:swrp}\n\\mathrm{SWRP} = - \\sum_{b=1}^B p(b) \\ln p(b),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{SWRP}=-\\sum_{b=1}^{B}p(b)\\ln p(b),\" display=\"block\"><mrow><mrow><mi>SWRP</mi><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nit is satisfied. $\\sigma_q(\\rho^j, \\omega_i^j)$ is the standard deviation of $q$ for the configuration $(\\rho^j, \\omega_i^j)$ and $\\bar{\\sigma}_q$ is the average value computed over $P \\times \\Omega$.\nFig. \\ref{fig:std_flucts} shows an illustrative example.\nWe stress that other criteria based on the fluctuations of RQA measures could be conceived.\nHowever, as we will show in the experiments, such a criterion offers a more accurate description for the edge of stability than $\\lambda$.\n\n\n\\begin{figure}[ht!]\n\\centering\n\n    \\subfigure[]{\n    \\includegraphics[width=0.49\\columnwidth]{./sin_RRfluct}\n    \\label{fig:sin_fluct}}\\hspace{-1.6em}\n    ~\n\t\t\\subfigure[]{\n    \\includegraphics[width=0.49\\columnwidth]{./mg_RRfluct}\n    \\label{fig:mg_fluct}}\n\t\t\n\\caption{Standard deviations of two RQA measures for different values of $\\rho$ and $\\omega_i$. The edge of stability is identified in correspondence of the parameters configurations for which the standard deviation increases abruptly (shown as a red line), assuming values grater than its mean value.}\n\\label{fig:std_flucts}\n\\end{figure}\n\n\n\n\\subsection{Visualization and classification of reservoir dynamics}\n\\label{sec:visual_id}\n\nIn the following, we show how RPs permit to visualize, and hence classify, the reservoir dynamics when fed with inputs possessing important characteristics.\nIn doing so, we assume given a stable ESN described by (\\ref{eq:esn_state_nooutputfeedback}); RPs are constructed following the procedure depicted in Fig. \\ref{fig:RP_build}.\nAlthough many classes of important signals/systems exist (with related sub-classes) \\cite{marwan2007recurrence}, here we focus on the ability to discriminate between important classes for the input signals: (i) with/without time-dependence; (ii) periodic/non-periodic motions; (iii) laminar behaviours; (iv) chaotic dynamics; finally, (v) non-stationary processes.\n\n\n\\begin{figure}[htp!]\n\\centering\n\t\\subfigure[Gaussian white noise; $\\tau_{\\mathrm{RP}} = 0.4$]{\n\t\\includegraphics[width=0.23\\textwidth]{./WN_SR099_T01}\n\t\\label{fig:WN_SR01_T04}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[Periodic; $\\tau_{\\mathrm{RP}} = 0.2$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM383_SR09_T03}\n\t\\label{fig:LM3.83_SR09_T03}}\n\t\n    \\subfigure[LM: laminar states; $\\tau_{\\mathrm{RP}} = 0.5$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM3679_SR09_T05}\n\t\\label{fig:LM3.679_SR09_T05}}\\hspace{-1.2em}\n\t~  \t\t\n\t\\subfigure[LM: chaos; $\\tau_{\\mathrm{RP}} = 0.2$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM399_SR09_T03}\n\t\\label{fig:LM3.99_SR09_T03}}\n\t\n\t\\subfigure[Brownian motion; $\\tau_{\\mathrm{RP}} = 0.2$]{\n    \\includegraphics[width=0.22\\textwidth]{./fBmP_SR09_T02}\n    \\label{fig:fBmP_SR0.9_T02}}\n    ~\n\t\\subfigure[Drift; $\\tau_{\\mathrm{RP}} = 0.2$]{\n    \\includegraphics[width=0.22\\textwidth]{./sin_drift}\n    \\label{fig:sin_drift}}\n\t\t\t\t\n\\caption{RPs generated by state sequences $\\mathbf{h}$ of ESN fed with input signals associated with the considered classes. Both axes represent time.}\n\\label{fig:RP_examples}\n\\end{figure}\n\nWe refer to examples in order to observe the different behaviours shown by an RP in correspondence with the aforementioned classes.\nWe recall that RPs offer visual information and the validity of the following comments are general and not application-specific.\nIn addition, we stress that RP-based analyses are applicable also to time-series with few hundreds of records.\\\\\n\n\n\\noindent\\textit{\\textbf{Time-dependency}}\\\\\nA uniformly distributed RP is a clear sign of the absence of a time-dependency in the time-series (e.g., an uncorrelated signal).\nIt is possible to rely on specific RQA measures in order to numerically investigate the presence of time-dependency.\nA signature for this is observed by checking the outcomes of DET (\\ref{eq:DET}), ENTR (\\ref{eq:ENTR}), and SWRP (\\ref{eq:swrp}).\nAll three indexes would yield very low values (close to zero) when there is no time-dependence in the signal.\nA periodic signal offers a counter-example having instead a strong time-dependency; DET would be very high for a periodic signal, yet ENTR and SWRP would still be low.\nIn fact, ENTR and SWRP are conceived to highlight the complexity aspects: signals with low complexity include those with no temporal structure.\nAs an example, in Fig. \\ref{fig:WN_SR01_T04} we examine the RP generated by feeding the ESN with white Gaussian noise, a typical example of signal with no time-dependency.\nWe observe a uniform RP for the reservoir states, which is peculiar for all signals composed by realizations of statistically independent variables.\\\\\n\n\\noindent\\textit{\\textbf{Periodicity}}\\\\\nEvery periodic system would induce long diagonal lines and the vertical spacing provides the characteristic period of the oscillation.\nA periodic system is typically accompanied by high values for DET (\\ref{eq:DET}) and $\\mathrm{L_{max}}$ (\\ref{eq:lmax}). In addition, as stressed before, it has low complexity as expressed by ENTR and SWRP.\nIn Fig. \\ref{fig:LM3.83_SR09_T03}, we show an example of periodic motion generated with a sinusoid having a single dominating frequency. The regularity of the diagonal lines can be immediately recognized from the figure.\\\\\n\n\n\\noindent\\textit{\\textbf{Laminarity}}\\\\\nA system presents laminar phases if its state does not change or change very slowly over a number of successive time-steps.\nLaminar phases can be visually recognized in an RP by the presence of (fairly) large black rectangles.\nEvery system possessing laminar phases is characterized by high values for LAM (\\ref{eq:lam}).\nIn order to provide an example (but results are  independently valid), we exploit the logistic map (LM),\n\n", "itemtype": "equation", "pos": 34687, "prevtext": "\n\nwhere $P(b)$ is the number of elements in the $b$-th bin and $p(b) = P(b)/K$ is the respective probability; as usual, $0\\leq\\mathrm{SWRP}\\leq\\log(B)$.\n\n\n\\subsection{Design of a stable and effective network}\n\\label{sec:design_stable_effective_net}\n\nRecurrence analysis can be used to (i) guarantee stability for a given configuration and (ii) tune the network, so that the highest computational capability can be achieved.\nThese aspects are treated in the sequel.\\\\\n\n\\noindent\\textit{\\textbf{Stability issue}}\\\\\nConstructing an RP from $\\mathbf{h}$ allows to investigate the network stability issue.\nAs mentioned before, the degree of stability can be calculated with $\\mathrm{L_\\mathrm{max}}$ (\\ref{eq:lmax}): the higher $\\mathrm{L_\\mathrm{max}}$, the more stable the system.\nIn addition to $\\mathrm{L_\\mathrm{max}}$, the designer can have an immediate visual interpretation of stability by inspecting RP: short and erratic diagonal lines in RPs denote instability/chaoticity, while long diagonal lines denote regularity (e.g., a periodic motion). We show in the experimental section that $\\mathrm{L_\\mathrm{max}}$ is anticorrelated with $\\lambda$ and hence it can be considered as a reliable indicator for the (input-dependent) degree of network stability.\nIn the experiments we highlight also that, if the network is stable, the dynamics of the input and reservoir produce very similar line patterns in the respective RPs.\nIf the network is unstable, then the aforementioned similarity is lost.\nTherefore, RPs can be used by the designer as visual tools to analyze the response of the network to a specific input.\\\\\n\n\\noindent\\textit{\\textbf{Computational capability issue}}\\\\\nA widespread criterion to determine the (signal-dependent) edge of stability relies on the sign of $\\lambda$, which becomes positive when the system enters into a (globally) unstable regime.\nWe propose, instead, to use information derived from RR (\\ref{eq:RR}), DET (\\ref{eq:DET}), LAM (\\ref{eq:lam}), ENTR (\\ref{eq:ENTR}), and SWRP (\\ref{eq:swrp}) for this purpose. We observed that, when such indexes start to fluctuate, the network achieves high prediction accuracy. This provides a guideline to the network designer for setting critical parameters, such as $\\rho$ and $\\omega_i$, in presence of input.\nRQA measures, when evaluated on ESNs initialized with different weight matrices, assume very different values given $\\rho$ and $\\omega_i$ when the network in unstable.\nAccordingly, it emerges that the standard deviation for such RQA measures is very high in such unstable regimes. We propose to identify the configurations of $\\rho$ and $\\omega_i$ which bring the system to the edge of stability with those for which we observe a sudden increment in the standard deviation.\nLet $P$ and $\\Omega$ be the domains for $\\rho$ and $\\omega_i$, respectively.\nIn this study, we calculate such configurations by checking when the standard deviations of RQA measures assume values grater than their mean values computed over $P\\times\\Omega$. \nFor a given RQA measure, say $q$, we identify the edge of stability as a set of pairs $\\{ (\\rho^{1}, \\omega_i^{1} ), (\\rho^{2}, \\omega_i^{2}), ..., (\\rho^{|\\Omega|}, \\omega_i^{|\\Omega|}) \\} \\subset P \\times \\Omega$.\nFor every $\\omega_i^j \\in \\Omega$, we select the largest $\\rho^j\\in P$ such that\n\n\n", "index": 29, "text": "\\begin{equation}\n\\label{eq:edge_criterion}\n\\sigma_q(\\rho^j, \\omega_i^j) \\leq \\bar{\\sigma}_q,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{q}(\\rho^{j},\\omega_{i}^{j})\\leq\\bar{\\sigma}_{q},\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c3</mi><mi>q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c1</mi><mi>j</mi></msup><mo>,</mo><msubsup><mi>\u03c9</mi><mi>i</mi><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><msub><mover accent=\"true\"><mi>\u03c3</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>q</mi></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\nwhere usually $\\tau_{\\mathrm{LM}}\\in(0,4]$; here LM (\\ref{eq:LM}) is used with initial condition $\\mathbf{x}[0] = 0.5$.\nIn Fig. \\ref{fig:LM3.679_SR09_T05} we show the RP when $\\tau_{\\mathrm{LM}} = 3.679$. The system exhibits chaos-chaos transitions in that configuration. In fact, the related RP is compatible with the one of a (mildly) chaotic system, showing also the presence of laminar phases visualized as large black rectangles.\\\\\n\n\n\\noindent\\textit{\\textbf{Chaoticity}}\\\\\nRPs offer a particularly useful visual tool in the case of chaotic dynamics. In fact, such dynamics can be recognized by the presence of erratic and very short diagonal lines. As a consequence, RR (\\ref{eq:RR}) would be very low. ENTR and SWRP are two measures of complexity that are useful in characterizing also the degree of chaoticity: the higher the measures, the more chaotic/complex the system.\nChaos is characterized by trajectories diverging exponentially fast. This can be quantified with $\\mathrm{L_{max}}$ (\\ref{eq:lmax}) and DIV (\\ref{eq:div}), whose values would be respectively very low and close to one for systems with a high degree of chaoticity.\n\nAs an example, we consider a chaotic system obtained through (\\ref{eq:LM}) with $\\tau_{\\mathrm{LM}} = 4$.\nThe reservoir dynamics, as shown in the RP in Fig. \\ref{fig:LM3.99_SR09_T03}, denotes fully developed chaos, as indicated by the presence of short and erratic diagonal lines only.\\\\\n\n\n\\noindent\\textit{\\textbf{Non-stationarity}}\\\\\nPeculiar line patterns observed for all nonstationary signals include large white areas with irregular patterns denoting abrupt changes in the dynamics.\nDrift is another typical form of nonstationarity, which is visually recognized in an RP by the fading of recurrences in the upper-left and lower-right corners.\nIn Fig. \\ref{fig:fBmP_SR0.9_T02}, we show an example by feeding the ESN with a well-known nonstationary signal: Brownian motion, a random walk resulting in a nonstationary stochastic process; whose increments correspond to Gaussian white noise, a stationary process. In Fig. \\ref{fig:sin_drift}, we show an example of drift by adding a linear trend to a sinusoid.\nNonstationarity can be numerically detected by considering an RQA measure called TREND (not used in our study) and by analyzing the variation of RQA measures when time-delay is applied to the signal (see \\cite{marwan2007recurrence} for technical details).\n\n\n\n\n\\section{Experiments}\n\\label{sec:exp}\n\nIn this section, we consider two input signals: a sinusoid and the Mackey-Glass time-series, which are often considered as benchmark in the ESN literature. We chosen these two signals also because they exemplify a very regular and a mildly chaotic system, respectively.\nWe perform two experiments. In the first one -- Sec. \\ref{sec:exp1} -- we use RPs to visualize the dynamics of reservoirs when driven by a given input signal. When the reservoir operates in a stable regime, RPs of reservoir and input show similar line patterns.\n\nIn the second experiment -- Sec. \\ref{sec:exp2} -- we use RPs as an analysis tool to tune two critical parameters, the spectral radius ($\\rho$) and the input scaling ($\\omega_i$), in an ESN driven by input signal. We study the derived RQA measures for different settings of $\\rho$ and $\\omega_i$, with the aim to (i) assess the network stability and (ii) determine a suitable setting for these two parameters, where the predictive power of the network is maximized.\n\nIn both experiments, we consider an ESN with no output feedback ($\\omega_o = 0$), configured with a standard setting: uniformly distributed weights in $[-1, 1]$ for $\\mathrm{W}_i^r$ and $\\mathrm{W}_r^r$, percentage of non-zero connections in $\\mathrm{W}_r^r$ of 25\\%. The readout is trained by setting the regularization parameter in the linear regression to $0.1$. According to the standard drop-out procedure, we discarded the first 100 elements of $\\mathbf{h}$ in order to get rid of the ESN transient states. In the first experiment, we set the number of reservoir neurons to $N_r = 50$. In preliminary tests, we noted that the number of reservoir neurons, even if very small, does not affect the results obtained for the analysis of the recurrences.\nTherefore, we do not provide here a study in these terms.\nWe used the Manhattan distance for evaluating the dissimilarity in the phase space,\n\n\n", "itemtype": "equation", "pos": 40435, "prevtext": "\n\nit is satisfied. $\\sigma_q(\\rho^j, \\omega_i^j)$ is the standard deviation of $q$ for the configuration $(\\rho^j, \\omega_i^j)$ and $\\bar{\\sigma}_q$ is the average value computed over $P \\times \\Omega$.\nFig. \\ref{fig:std_flucts} shows an illustrative example.\nWe stress that other criteria based on the fluctuations of RQA measures could be conceived.\nHowever, as we will show in the experiments, such a criterion offers a more accurate description for the edge of stability than $\\lambda$.\n\n\n\\begin{figure}[ht!]\n\\centering\n\n    \\subfigure[]{\n    \\includegraphics[width=0.49\\columnwidth]{./sin_RRfluct}\n    \\label{fig:sin_fluct}}\\hspace{-1.6em}\n    ~\n\t\t\\subfigure[]{\n    \\includegraphics[width=0.49\\columnwidth]{./mg_RRfluct}\n    \\label{fig:mg_fluct}}\n\t\t\n\\caption{Standard deviations of two RQA measures for different values of $\\rho$ and $\\omega_i$. The edge of stability is identified in correspondence of the parameters configurations for which the standard deviation increases abruptly (shown as a red line), assuming values grater than its mean value.}\n\\label{fig:std_flucts}\n\\end{figure}\n\n\n\n\\subsection{Visualization and classification of reservoir dynamics}\n\\label{sec:visual_id}\n\nIn the following, we show how RPs permit to visualize, and hence classify, the reservoir dynamics when fed with inputs possessing important characteristics.\nIn doing so, we assume given a stable ESN described by (\\ref{eq:esn_state_nooutputfeedback}); RPs are constructed following the procedure depicted in Fig. \\ref{fig:RP_build}.\nAlthough many classes of important signals/systems exist (with related sub-classes) \\cite{marwan2007recurrence}, here we focus on the ability to discriminate between important classes for the input signals: (i) with/without time-dependence; (ii) periodic/non-periodic motions; (iii) laminar behaviours; (iv) chaotic dynamics; finally, (v) non-stationary processes.\n\n\n\\begin{figure}[htp!]\n\\centering\n\t\\subfigure[Gaussian white noise; $\\tau_{\\mathrm{RP}} = 0.4$]{\n\t\\includegraphics[width=0.23\\textwidth]{./WN_SR099_T01}\n\t\\label{fig:WN_SR01_T04}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[Periodic; $\\tau_{\\mathrm{RP}} = 0.2$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM383_SR09_T03}\n\t\\label{fig:LM3.83_SR09_T03}}\n\t\n    \\subfigure[LM: laminar states; $\\tau_{\\mathrm{RP}} = 0.5$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM3679_SR09_T05}\n\t\\label{fig:LM3.679_SR09_T05}}\\hspace{-1.2em}\n\t~  \t\t\n\t\\subfigure[LM: chaos; $\\tau_{\\mathrm{RP}} = 0.2$]{\n\t\\includegraphics[width=0.23\\textwidth]{./LM399_SR09_T03}\n\t\\label{fig:LM3.99_SR09_T03}}\n\t\n\t\\subfigure[Brownian motion; $\\tau_{\\mathrm{RP}} = 0.2$]{\n    \\includegraphics[width=0.22\\textwidth]{./fBmP_SR09_T02}\n    \\label{fig:fBmP_SR0.9_T02}}\n    ~\n\t\\subfigure[Drift; $\\tau_{\\mathrm{RP}} = 0.2$]{\n    \\includegraphics[width=0.22\\textwidth]{./sin_drift}\n    \\label{fig:sin_drift}}\n\t\t\t\t\n\\caption{RPs generated by state sequences $\\mathbf{h}$ of ESN fed with input signals associated with the considered classes. Both axes represent time.}\n\\label{fig:RP_examples}\n\\end{figure}\n\nWe refer to examples in order to observe the different behaviours shown by an RP in correspondence with the aforementioned classes.\nWe recall that RPs offer visual information and the validity of the following comments are general and not application-specific.\nIn addition, we stress that RP-based analyses are applicable also to time-series with few hundreds of records.\\\\\n\n\n\\noindent\\textit{\\textbf{Time-dependency}}\\\\\nA uniformly distributed RP is a clear sign of the absence of a time-dependency in the time-series (e.g., an uncorrelated signal).\nIt is possible to rely on specific RQA measures in order to numerically investigate the presence of time-dependency.\nA signature for this is observed by checking the outcomes of DET (\\ref{eq:DET}), ENTR (\\ref{eq:ENTR}), and SWRP (\\ref{eq:swrp}).\nAll three indexes would yield very low values (close to zero) when there is no time-dependence in the signal.\nA periodic signal offers a counter-example having instead a strong time-dependency; DET would be very high for a periodic signal, yet ENTR and SWRP would still be low.\nIn fact, ENTR and SWRP are conceived to highlight the complexity aspects: signals with low complexity include those with no temporal structure.\nAs an example, in Fig. \\ref{fig:WN_SR01_T04} we examine the RP generated by feeding the ESN with white Gaussian noise, a typical example of signal with no time-dependency.\nWe observe a uniform RP for the reservoir states, which is peculiar for all signals composed by realizations of statistically independent variables.\\\\\n\n\\noindent\\textit{\\textbf{Periodicity}}\\\\\nEvery periodic system would induce long diagonal lines and the vertical spacing provides the characteristic period of the oscillation.\nA periodic system is typically accompanied by high values for DET (\\ref{eq:DET}) and $\\mathrm{L_{max}}$ (\\ref{eq:lmax}). In addition, as stressed before, it has low complexity as expressed by ENTR and SWRP.\nIn Fig. \\ref{fig:LM3.83_SR09_T03}, we show an example of periodic motion generated with a sinusoid having a single dominating frequency. The regularity of the diagonal lines can be immediately recognized from the figure.\\\\\n\n\n\\noindent\\textit{\\textbf{Laminarity}}\\\\\nA system presents laminar phases if its state does not change or change very slowly over a number of successive time-steps.\nLaminar phases can be visually recognized in an RP by the presence of (fairly) large black rectangles.\nEvery system possessing laminar phases is characterized by high values for LAM (\\ref{eq:lam}).\nIn order to provide an example (but results are  independently valid), we exploit the logistic map (LM),\n\n", "index": 31, "text": "\\begin{equation}\n\\label{eq:LM}\n\\mathbf{x}[n+1] = \\tau_{\\mathrm{LM}}\\mathbf{x}[n](1-\\mathbf{x}[n]),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{x}[n+1]=\\tau_{\\mathrm{LM}}\\mathbf{x}[n](1-\\mathbf{x}[n]),\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03c4</mi><mi>LM</mi></msub><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nThe threshold $\\tau_{\\mathrm{RP}}$ has been calculated by using a percentage of the average dissimilarity value between the states in $\\mathbf{h}$. For simplifying the notation, we directly indicate the percentage referring to $\\tau_{\\mathrm{RP}}$, e.g., $\\tau_{\\mathrm{RP}}=0.1$ indicates 10\\% of the average distance between all states.\nSometimes, to better visualize the RPs, we increase $\\tau_{\\mathrm{RP}}$ beyond values typically used in the literature.\nHowever, as stated before, our main aim here is to show the agreement between RQA measures and the stability--instability transition together with the corresponding accuracy of the network, rather than providing an \\textit{exact} quantification of its characteristics.\n\nOur results are easily reproducible by using the ESN\\footnote{http://www.reservoir-computing.org/node/129} and RP\\footnote{http://www.recurrence-plot.tk/} toolboxes available online.\n\n\n\n\\subsection{Visualization of ESN dynamics}\n\\label{sec:exp1}\n\n\nThe first signal is a sinusoid defined as $\\mathbf{x}[k]= \\sin(\\psi k)$, where $k=1, 2, ..., 5000$ and $\\psi = 3/50$.\n\n\n\\begin{figure}[h!]\n\\centering\n\n\t\\subfigure[RP of the input signal]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_input_T02}\n\t\\label{fig:sin_input_T02}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[$\\rho = 0.99$, $\\tau_{\\mathrm{RP}} = 0.1$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR099_T01}\n\t\\label{fig:sin_SR099_T01}}\n\t\t\t\t\n\t\\subfigure[$\\rho = 1.5$, $\\tau_{\\mathrm{RP}} = 0.8$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR15_T08}\n\t\\label{fig:sin_SR15_T08}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[$\\rho = 2$, $\\tau_{\\mathrm{RP}} = 0.8$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR2_T08}\n\t\\label{fig:sin_SR2_T08}}\n\t\t\t\t\n\\caption{RP for the input signal and the sequence of states of the reservoir. When $\\rho=0.99$ the activations are compatible with the input dynamics. When $\\rho$ exceeds one, the activations denote instability.}\n\\label{fig:sin}\n\\end{figure}\n\nIn Fig. \\ref{fig:sin} we show the RPs relative to the input signal and the reservoir states; input signal has been embedded into a 2-dimensional phase space.\nIn Fig. \\ref{fig:sin_SR099_T01} we used a conservative setting for $\\rho$. The resulting RP is compatible with a periodic system having a single frequency. This suggests that, although the (randomly initialized) reservoir performs a non-linear mapping, its resulting dynamics preserves the one of the input system.\nIn fact, the vertical spacing between the diagonal lines is almost the same, which is 98 in Fig. \\ref{fig:sin_input_T02} and 101 in Fig. \\ref{fig:sin_SR099_T01}.\nThis small difference is due to a slight discrepancy in the period of the two systems (i.e., input and reservoir), whose cause was formally explained in \\cite{manjunath2013echo}. There the authors demonstrate that when an ESN is driven by a periodic signal with period $\\vartheta$, the induced network dynamics asymptotically becomes periodic as well, with period $r \\vartheta, r>0$. \nOn the other hand, when $\\rho$ is pushed beyond one, the RPs are comparable with the ones of unstable/chaotic systems with a degree of instability monotonically related to the value of $\\rho$. Please note that, while in Figs. \\ref{fig:sin_input_T02} and \\ref{fig:sin_SR099_T01} we used $\\tau_{\\mathrm{RP}} = 0.2$ and $\\tau_{\\mathrm{RP}} = 0.1$, respectively, in Figs. \\ref{fig:sin_SR15_T08} and \\ref{fig:sin_SR2_T08} we used a large threshold, $\\tau_{\\mathrm{RP}} = 0.8$, to improve readability of the plots.\n\n\nThe second input signal is given by a time-series generated from the Mackey-Glass (MG) system,\n\n\n", "itemtype": "equation", "pos": 44904, "prevtext": "\nwhere usually $\\tau_{\\mathrm{LM}}\\in(0,4]$; here LM (\\ref{eq:LM}) is used with initial condition $\\mathbf{x}[0] = 0.5$.\nIn Fig. \\ref{fig:LM3.679_SR09_T05} we show the RP when $\\tau_{\\mathrm{LM}} = 3.679$. The system exhibits chaos-chaos transitions in that configuration. In fact, the related RP is compatible with the one of a (mildly) chaotic system, showing also the presence of laminar phases visualized as large black rectangles.\\\\\n\n\n\\noindent\\textit{\\textbf{Chaoticity}}\\\\\nRPs offer a particularly useful visual tool in the case of chaotic dynamics. In fact, such dynamics can be recognized by the presence of erratic and very short diagonal lines. As a consequence, RR (\\ref{eq:RR}) would be very low. ENTR and SWRP are two measures of complexity that are useful in characterizing also the degree of chaoticity: the higher the measures, the more chaotic/complex the system.\nChaos is characterized by trajectories diverging exponentially fast. This can be quantified with $\\mathrm{L_{max}}$ (\\ref{eq:lmax}) and DIV (\\ref{eq:div}), whose values would be respectively very low and close to one for systems with a high degree of chaoticity.\n\nAs an example, we consider a chaotic system obtained through (\\ref{eq:LM}) with $\\tau_{\\mathrm{LM}} = 4$.\nThe reservoir dynamics, as shown in the RP in Fig. \\ref{fig:LM3.99_SR09_T03}, denotes fully developed chaos, as indicated by the presence of short and erratic diagonal lines only.\\\\\n\n\n\\noindent\\textit{\\textbf{Non-stationarity}}\\\\\nPeculiar line patterns observed for all nonstationary signals include large white areas with irregular patterns denoting abrupt changes in the dynamics.\nDrift is another typical form of nonstationarity, which is visually recognized in an RP by the fading of recurrences in the upper-left and lower-right corners.\nIn Fig. \\ref{fig:fBmP_SR0.9_T02}, we show an example by feeding the ESN with a well-known nonstationary signal: Brownian motion, a random walk resulting in a nonstationary stochastic process; whose increments correspond to Gaussian white noise, a stationary process. In Fig. \\ref{fig:sin_drift}, we show an example of drift by adding a linear trend to a sinusoid.\nNonstationarity can be numerically detected by considering an RQA measure called TREND (not used in our study) and by analyzing the variation of RQA measures when time-delay is applied to the signal (see \\cite{marwan2007recurrence} for technical details).\n\n\n\n\n\\section{Experiments}\n\\label{sec:exp}\n\nIn this section, we consider two input signals: a sinusoid and the Mackey-Glass time-series, which are often considered as benchmark in the ESN literature. We chosen these two signals also because they exemplify a very regular and a mildly chaotic system, respectively.\nWe perform two experiments. In the first one -- Sec. \\ref{sec:exp1} -- we use RPs to visualize the dynamics of reservoirs when driven by a given input signal. When the reservoir operates in a stable regime, RPs of reservoir and input show similar line patterns.\n\nIn the second experiment -- Sec. \\ref{sec:exp2} -- we use RPs as an analysis tool to tune two critical parameters, the spectral radius ($\\rho$) and the input scaling ($\\omega_i$), in an ESN driven by input signal. We study the derived RQA measures for different settings of $\\rho$ and $\\omega_i$, with the aim to (i) assess the network stability and (ii) determine a suitable setting for these two parameters, where the predictive power of the network is maximized.\n\nIn both experiments, we consider an ESN with no output feedback ($\\omega_o = 0$), configured with a standard setting: uniformly distributed weights in $[-1, 1]$ for $\\mathrm{W}_i^r$ and $\\mathrm{W}_r^r$, percentage of non-zero connections in $\\mathrm{W}_r^r$ of 25\\%. The readout is trained by setting the regularization parameter in the linear regression to $0.1$. According to the standard drop-out procedure, we discarded the first 100 elements of $\\mathbf{h}$ in order to get rid of the ESN transient states. In the first experiment, we set the number of reservoir neurons to $N_r = 50$. In preliminary tests, we noted that the number of reservoir neurons, even if very small, does not affect the results obtained for the analysis of the recurrences.\nTherefore, we do not provide here a study in these terms.\nWe used the Manhattan distance for evaluating the dissimilarity in the phase space,\n\n\n", "index": 33, "text": "\\begin{equation}\n\\label{eq:h_diss}\nd(\\mathbf{h}[j], \\mathbf{h}[i])  = \\sum_{n=1}^{N_r} | h_n[j] - h_n[i] |.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"d(\\mathbf{h}[j],\\mathbf{h}[i])=\\sum_{n=1}^{N_{r}}|h_{n}[j]-h_{n}[i]|.\" display=\"block\"><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo><mrow><mi>\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>r</mi></msub></munderover><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>h</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>h</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\nWe obtained a time-series of 150000 time-steps using $\\tau_{\\mathrm{MG}} = 17, \\alpha = 0.2, \\beta = 0.1$, initial condition $x(0)=1.2$, and 0.1 as integration step for (\\ref{eq:MG}).\nIn Fig. \\ref{fig:mg}, we report RPs of the input signal and those related to the three different settings of the ESN with an increasing value of $\\rho$.\nAs in the previous case, it is worth stressing the similarity between RPs on input in Fig. \\ref{fig:MG_input_t05} and reservoir in Fig. \\ref{fig:MG_SR09_t05}.\nAs the reservoir is pushed toward instability (by increasing $\\rho$), it is possible to observe the usual incremental transition toward a chaotic regime in the related RPs.\nIn Figs. \\ref{fig:MG_SR15_t05} and \\ref{fig:MG_SR2_t05} $\\rho$ is set to 1.5 and 2, respectively. In both cases we observe typical line patterns of unstable systems.\nHowever, we note that when $\\rho=2$ the system becomes fully chaotic (e.g., see \\ref{fig:LM3.99_SR09_T03} for a visual comparison), with very short and erratic diagonal lines.\n\n\n\\begin{figure}[ht!]\n\\centering\n\n    \\subfigure[RP of the input signal]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_input_t05}\n    \\label{fig:MG_input_t05}}\\hspace{-1.2em}\n    ~\n\t\\subfigure[$\\rho = 0.9$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR09_t05}\n    \\label{fig:MG_SR09_t05}}\n\t\t\n    \\subfigure[$\\rho  = 1.5$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR15_t05}\n    \\label{fig:MG_SR15_t05}}\\hspace{-1.2em}\n    ~\n    \\subfigure[$\\rho  = 2$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR2_t05}\n    \\label{fig:MG_SR2_t05}}\n\t\t\n\\caption{RPs of the MG time-series and of the state of the reservoir.}\n\\label{fig:mg}\n\\end{figure}\n\n\n\n\n\\subsection{Characterization of reservoirs with RQA}\n\\label{sec:exp2}\n\nHere we evaluate RQA measures with respect to the accuracy achieved by ESN relative to the forecast of the sinusoidal and MG time-series.\nFor this purpose, we consider also $\\lambda$ (\\ref{eq:MLLE}), which, as previously discussed, is a well-known tool for determining the stability of a dynamic system.\nWe evaluate how $\\lambda$ and RQA measures vary as the values of $\\rho$ and $\\omega_i$ are modified.\nFirst, we compare $\\lambda$ with $\\mathrm{L_{max}}$, the value of the longest diagonal line in an RP -- see Eq. \\ref{eq:lmax} and related discussion. $\\mathrm{L_{max}}$ is a global indicator of stability that we show here to be highly correlated with $\\lambda$.\nSuccessively, we consider the condition $\\lambda>0$ as a discriminator to determine the (input-dependent) edge of stability of the network.\nWe then compare such information with those provided by the criterion proposed in Eq. \\ref{eq:edge_criterion}, which is based on fluctuations of RQA complexity measures.\nTo demonstrate the validity of the proposed method, we show that the network configurations determining the edge of stability by using Eq. \\ref{eq:edge_criterion} produce results that are significantly better (in a statistical sense) than those derived with the $\\lambda>0$ criterion.\n\nThe error measure that we adopt for evaluating the prediction is the Normalized Root Mean Squared Error (NRMSE) function,\n\n\n", "itemtype": "equation", "pos": 48608, "prevtext": "\n\nThe threshold $\\tau_{\\mathrm{RP}}$ has been calculated by using a percentage of the average dissimilarity value between the states in $\\mathbf{h}$. For simplifying the notation, we directly indicate the percentage referring to $\\tau_{\\mathrm{RP}}$, e.g., $\\tau_{\\mathrm{RP}}=0.1$ indicates 10\\% of the average distance between all states.\nSometimes, to better visualize the RPs, we increase $\\tau_{\\mathrm{RP}}$ beyond values typically used in the literature.\nHowever, as stated before, our main aim here is to show the agreement between RQA measures and the stability--instability transition together with the corresponding accuracy of the network, rather than providing an \\textit{exact} quantification of its characteristics.\n\nOur results are easily reproducible by using the ESN\\footnote{http://www.reservoir-computing.org/node/129} and RP\\footnote{http://www.recurrence-plot.tk/} toolboxes available online.\n\n\n\n\\subsection{Visualization of ESN dynamics}\n\\label{sec:exp1}\n\n\nThe first signal is a sinusoid defined as $\\mathbf{x}[k]= \\sin(\\psi k)$, where $k=1, 2, ..., 5000$ and $\\psi = 3/50$.\n\n\n\\begin{figure}[h!]\n\\centering\n\n\t\\subfigure[RP of the input signal]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_input_T02}\n\t\\label{fig:sin_input_T02}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[$\\rho = 0.99$, $\\tau_{\\mathrm{RP}} = 0.1$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR099_T01}\n\t\\label{fig:sin_SR099_T01}}\n\t\t\t\t\n\t\\subfigure[$\\rho = 1.5$, $\\tau_{\\mathrm{RP}} = 0.8$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR15_T08}\n\t\\label{fig:sin_SR15_T08}}\\hspace{-1.2em}\n\t~\n\t\\subfigure[$\\rho = 2$, $\\tau_{\\mathrm{RP}} = 0.8$]{\n\t\\includegraphics[width=0.23\\textwidth]{./sin_SR2_T08}\n\t\\label{fig:sin_SR2_T08}}\n\t\t\t\t\n\\caption{RP for the input signal and the sequence of states of the reservoir. When $\\rho=0.99$ the activations are compatible with the input dynamics. When $\\rho$ exceeds one, the activations denote instability.}\n\\label{fig:sin}\n\\end{figure}\n\nIn Fig. \\ref{fig:sin} we show the RPs relative to the input signal and the reservoir states; input signal has been embedded into a 2-dimensional phase space.\nIn Fig. \\ref{fig:sin_SR099_T01} we used a conservative setting for $\\rho$. The resulting RP is compatible with a periodic system having a single frequency. This suggests that, although the (randomly initialized) reservoir performs a non-linear mapping, its resulting dynamics preserves the one of the input system.\nIn fact, the vertical spacing between the diagonal lines is almost the same, which is 98 in Fig. \\ref{fig:sin_input_T02} and 101 in Fig. \\ref{fig:sin_SR099_T01}.\nThis small difference is due to a slight discrepancy in the period of the two systems (i.e., input and reservoir), whose cause was formally explained in \\cite{manjunath2013echo}. There the authors demonstrate that when an ESN is driven by a periodic signal with period $\\vartheta$, the induced network dynamics asymptotically becomes periodic as well, with period $r \\vartheta, r>0$. \nOn the other hand, when $\\rho$ is pushed beyond one, the RPs are comparable with the ones of unstable/chaotic systems with a degree of instability monotonically related to the value of $\\rho$. Please note that, while in Figs. \\ref{fig:sin_input_T02} and \\ref{fig:sin_SR099_T01} we used $\\tau_{\\mathrm{RP}} = 0.2$ and $\\tau_{\\mathrm{RP}} = 0.1$, respectively, in Figs. \\ref{fig:sin_SR15_T08} and \\ref{fig:sin_SR2_T08} we used a large threshold, $\\tau_{\\mathrm{RP}} = 0.8$, to improve readability of the plots.\n\n\nThe second input signal is given by a time-series generated from the Mackey-Glass (MG) system,\n\n\n", "index": 35, "text": "\\begin{equation}\n\\label{eq:MG}\n\\frac{dx}{dt} = \\frac{\\alpha x(t-\\tau_{\\mathrm{MG}})}{1+ x(t-\\tau_{\\mathrm{MG}})^{10}} - \\beta x(t).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\frac{dx}{dt}=\\frac{\\alpha x(t-\\tau_{\\mathrm{MG}})}{1+x(t-\\tau_{\\mathrm{MG}})^%&#10;{10}}-\\beta x(t).\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>x</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mfrac><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msub><mi>\u03c4</mi><mi>MG</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi>x</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msub><mi>\u03c4</mi><mi>MG</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>10</mn></msup></mrow></mrow></mfrac><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07381.tex", "nexttext": "\n\nbeing $\\hat{y}_i$ the $i$-th ESN prediction and $y_i$ the ground-truth. The prediction accuracy, $\\gamma$, is defined as $\\gamma=1-\\mathrm{NRMSE}$.\n\nIn order to obtain more interpretable RQA values varying in the unit interval, we normalize all RQA measures by using a unity-based normalization separately for each RQA measure, which considers every setting of $\\rho$ and $\\omega_i$ for each signal.\nIt is important to note that this operation neglects the possibility to perform a more quantitative analysis -- which we remind it is not the scope of this paper. However, it allows to immediately recognize areas of interest and transitions in the related plots. We always use 50 bins for computing the SWRP measure (\\ref{eq:swrp}).\nAll results are always reported as average of 15 different simulations with independent initialization of $\\mathbf{W}_i^r$ and $\\mathbf{W}_r^r$ in the ESN.\nFor this second experiment, since the number of reservoir neurons has an impact on the prediction performance, we adopted two different configurations for $N_r$.\\\\\n\n\n\n\\textit{\\textbf{Sinusoidal signal.}}\nThe ESN is trained to perform a 25-step ahead prediction and it is tested on a time-series of 1500 time-steps; we used a reservoir size $N_r = 75$.\nWe test the network performance by varying $\\rho \\in P = [0.01, 2]$ and $\\omega_i \\in \\Omega = [0.01, 1]$, both discretized with resolution 0.1.\n\nIn Fig. \\ref{fig:sin_RQA}, we show the RQA measures calculated by varying $\\rho$ and $\\omega_i$.\nThe ESN, when $\\rho \\leq 1$, shows high levels of regularity and stability, as expressed by RR, DET, LAM, and $\\mathrm{L_{max}}$.\nNotably, when $\\rho \\simeq 1$, we note that $\\mathrm{L_{max}}$ starts to decrease, suggesting that we are approaching the unstable phase.\nENTR and SWRP behave similarly, although they are maximized for different configurations of $\\rho$ and $\\omega_i$.\nIn fact, ENTR assumes the relative maximum value for larger input scaling when $\\rho\\simeq 1$. On the other hand, SWRP assumes high values for smaller values of $\\omega_i$ and $\\rho$.\n\nFig. \\ref{fig:sin_RQA_2d} shows a 2D section of Fig. \\ref{fig:sin_RQA}, obtained by selecting a specific input scaling $\\omega_i = 0.8$.\nIn this way, we can visually assess the agreement between $\\mathrm{L_{max}}$ and $\\lambda$, and also the increasing RQA variability (i.e., standard deviation) on the edge of stability.\nIn Fig. \\ref{fig:sin_MLLE_Lmax_DIV}, we reported $\\lambda$, $\\mathrm{L_{max}}$, and DIV (\\ref{eq:div}) as we vary $\\rho$. In order to improve the visualization, $\\lambda$ has been rescaled by dividing it for its maximum (absolute) value.\n$\\lambda$ and $\\mathrm{L_{max}}$ are anticorrelated with (Pearson) correlation equal to $-0.78$: the value of $\\mathrm{L_{max}}$ decreases as $\\rho$ increases, while $\\lambda$, as expected, increases with $\\rho$.\nAdditionally, we can observe that there exists a positive correlation ($0.79$) between $\\lambda$ and DIV.\nWe stress that the agreement between $\\lambda$ and $\\mathrm{L_{max}}$ is consistent for the entire range of $\\omega_i$, as represented in Fig. \\ref{fig:sin_MLLE_vs_Lmax} and quantified in Tab. \\ref{tab:corrStability}, confirming that statistics of the RP diagonal lines offer consistent and solid complexity measures characterizing the network stability.\nPlease, notice that the correlations in Tab. \\ref{tab:corrStability} are computed on two matrices in $\\mathbb{R}^{|P|} \\times \\mathbb{R}^{|\\Omega|}$. Since the matrix elements are not interrelated, such matrices can be represented as vectors having dimension $|P|\\cdot|\\Omega|$. The correlation is then computed on such vectors.  \n\nLet us comment the results for RR, DET, LAM, ENTR, and SWRP with respect to determination of the edge of stability.\nAll these RQA measures denote low fluctuations until when $\\lambda$ becomes positive -- see related panels in Fig. \\ref{fig:sin_RQA_2d} -- which is achieved for $\\rho\\simeq1.2$.\nFull details for $\\gamma$ are shown in Fig. \\ref{fig:sin_NRMSE}, while in Fig. \\ref{fig:sin_Edge} we show different lines in the $\\rho$--$\\omega_i$ plane, each one denoting a specific determination of the edge of stability (by using either $\\lambda$ and the RQA measures). In the same plot, with a red line we indicate the configurations where the best performance are achieved.\nFirst, we observe that, in general, for low values of $\\omega_i$ we obtain a slightly inferior prediction accuracy. In addition, the gray area in Fig. \\ref{fig:sin_NRMSE} is larger for higher values of $\\omega_i$ and it includes also several configurations with $\\rho=1.2$. This is justified by the fact that high-amplitude signals tend to saturate the nonlinear activation functions and cause the poles to shrink toward the origin. This results in a system with a larger stability margin and a more contractive dynamics, hence justifying the use of larger values of $\\rho$.\nTo quantify the accuracy in identifying the edge of stability, we report in Tab. \\ref{tab:distancesEdge} the average distances of the edges defined trough RQA measures with respect to the maximum values of $\\gamma$ (red line in Fig. \\ref{fig:sin_Edge}). Our results show that, in many cases, RQA measures determine a more accurate edge of stability with respect to $\\lambda$.\nNotably, RR produces differences with $\\gamma$ that are significantly ($p<0.0004$) smaller than those of $\\lambda$; the same holds for DET ($p<0.0168$), LAM ($p<0.0050$), ENTR ($p<0.0011$), but not for SWRP, whose outcome is significantly worse than $\\lambda$ ($p<0.0078$). Such results have been obtained by using the \\textit{t}-test with 0.05 as significance threshold.\nSuch important differences might be related to the low sensitivity of $\\lambda$ to variations of $\\omega_i$, as can be noticed by looking at Fig. \\ref{fig:sin_MLLE}.\nAdditionally, we can observe that the smooth variation of $\\lambda$ follows very closely the values of $\\rho$. \n\nLet us discuss the results in terms of correlation between RQA measures and $\\gamma$; see Fig. \\ref{fig:sin_RQA_vs_NRMSE} for a graphical comparison.\nIn Tab. \\ref{tab:corrPrediction} we provide the correlation values.\nFor the sinusoidal input, correlations of RQA measures with $\\gamma$ are not very satisfactory, with the only exception of SWRP.\nHowever, we note that the estimated mutual information (normalized in the unit interval) indicates in general better nonlinear dependencies.\\\\\n\n\n\n\\textit{\\textbf{MG time-series.}}\nThe prediction of the MG time-series is a common benchmark where ESNs achieved good performance in terms of prediction accuracy \\cite{shi2007support,jaeger2004harnessing}. For this test, we train the ESN to perform a 20-step ahead prediction on a test time-series of 2000 time-steps; we set the size of the reservoir to $N_r = 100$; intervals $P$ and $\\Omega$ are defined as before.\n\nIn Fig. \\ref{fig:mg_RQA}, we show the outcomes for six RQA measures as both $\\rho$ and $\\omega_i$ change.\nWith respect to the sinusoidal input, we immediately observe a higher sensitivity with respect to $\\omega_i$.\nThis can be explained by the need to use a higher degree of non-linearity for dealing with the forecast of the MG time-series.\nThe dynamics of the reservoir appear as highly deterministic with laminar phases (as expressed by DET and LAM, respectively) for the entire range of $\\omega_i$.\nWe note a consistent maximization for SWRP and ENTR, which assume high values for a large set of configurations that include also values for $\\rho\\simeq 2$ when $\\omega_i > 0.5$.\nFor what concerns RR, instead, we notice that it is maximized only for very small values of $\\rho$ and high values of $\\omega_i$, suggesting that density of recurrences (i.e., RR) is very sensitive to $\\rho$ for the MG time-series.\n\nIn Fig. \\ref{fig:mg_RQA_2d}, we show the mean and standard deviation of the RQA values when $\\omega_i = 0.5$ is fixed. This is performed, as before, in order to assess the network stability and locate the edge of stability in terms of increase in the fluctuations of the RQA measures.\nIn Fig. \\ref{fig:mg_MLLE_Lmax_DIV} we show the mean value of $\\lambda$, which has been graphically rescaled also in this case, $\\mathrm{L_{max}}$, and DIV as we vary $\\rho$.\nAs for the sinusoidal input, $\\lambda$ and $\\mathrm{L_{max}}$ show a good anticorrelation, with a value of $-0.64$. Analogously, $\\lambda$ and DIV are correlated with a slightly lower value of $0.60$. The agreement between $\\mathrm{L_{max}}$ and $\\lambda$ is confirmed for all values of $\\omega_i$, as depicted in Fig. \\ref{fig:mg_MLLE_vs_Lmax} and numerically quantified in the second row of Tab. \\ref{tab:corrStability}.\n\nLet us now take into account the issue of determining the edge of stability.\nIn Fig. \\ref{fig:mg_NRMSE}, we notice that high prediction accuracy $\\gamma$ is obtained for configurations located in the center of the plot toward the right-hand side, corresponding to $\\rho \\simeq 1.5$ and $\\omega_i\\geq0.8$. For instance, when $\\omega_i=0.5$ the best accuracy is obtained for $1\\leq\\rho\\leq1.3$.\nWhen considering the specific case of $\\omega_i = 0.5$, $\\lambda$ becomes greater than 0 when $\\rho\\geq0.7$, as shown in Fig. \\ref{fig:mg_MLLE_Lmax_DIV}.\nHowever, we note that all RQA measures start to fluctuate for much higher values of $\\rho$, close to 1.5.\nThis is graphically represented in Fig. \\ref{fig:mg_Edge}, where we reported the determination of the edge of stability according to both conditions based on $\\lambda>0$ and RQA measure fluctuations.\nAs clearly shown by the plot, $\\lambda$ significantly underestimates the location of the edge of stability for almost all values of $\\omega_i>0.2$.\nOn the other hand, we note that all RQA measures show a very good correlation with the red line denoting the best performance, $\\gamma$, even though they slightly overestimate the edge.\nThis comparison is made formal in Tab. \\ref{tab:distancesEdge}, where we show that all RQA measures denote smaller distances with $\\gamma$ than $\\lambda$, with differences that are statistically significant.\nNotably, \\textit{p}-values are as follows: RR ($p<0.0001$), DET ($p<0.0005$), LAM ($0.0001$), ENTR ($p<0.0003$), and SWRP ($p<0.0003$).\nTherefore, in the case of the MG time-series, $\\lambda$ and RQA measures provide two completely different information regarding the edge of stability: the information provided by RQA measures is significantly more accurate.\nThis important result, as in the previous case, might be related to the low sensitivity of $\\lambda$ to the input scaling; see Fig. \\ref{fig:mg_MLLE}.\n\nLet us now take into account in Fig. \\ref{fig:mg_RQA_vs_NRMSE} the prediction accuracy with respect to the RQA measures.\nAs shown in Tab. \\ref{tab:corrPrediction}, we notice a good linear agreement between SWRP and $\\gamma$, denoting a correlation of $0.68$. Also ENTR achieves a good correlation of $0.62$, as in fact ENTR and SWRP behave similarly for the MG time-series (see Fig. \\ref{fig:mg_RQA}).\nNonlinear dependency, as indicated by mutual information, shows high values for RR, ENTR, and SWRP.\n\n\\bgroup\n\\setlength\\tabcolsep{1em} \n\\begin{table}[th!]\\scriptsize\n\\caption{Correlations between $\\lambda$, DIV, and $\\mathrm{L_{max}}$.}\n\\begin{center}\n\\vspace{-0.3cm}\n\\begin{tabular}{lll}\n\\hline\n\\multicolumn{1}{l}{} & $\\mathbf{\\lambda} \\backslash \\mathrm{L_{max}}$ & $\\mathbf{\\lambda} \\backslash \\text{DIV}$ \\\\\n\\hline\n\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{Sin}} & -0.74 & 0.53 \\\\ \n\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{MG}} & -0.65 & 0.57 \\\\\n\\hline\n\\end{tabular}\n\\label{tab:corrStability}\n\\end{center}\n\\end{table}\n\\egroup\n\n\\bgroup\n\\setlength\\tabcolsep{0.5em} \n\\begin{table}[th!]\\scriptsize\n\\caption{Average distances between maximum of $\\gamma$ with both $\\lambda$ and RQA measures (to determine the edge of stability). Results in bold indicates statistically significant results with respect to $\\lambda$.}\n\\begin{center}\n\\vspace{-0.3cm}\n\\begin{tabular}{lllllll}\n\\hline\n\\multicolumn{1}{l}{}  & $\\mathbf{\\lambda}$ & \\textbf{RR} & \\textbf{DET} & \\textbf{LAM} & \\textbf{ENTR} & \\textbf{SWRP}  \\\\\n\\hline\n\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{Sin}} & 1.7$\\pm$0.95 & \\textbf{0.7}$\\pm$\\textbf{0.67} & \\textbf{1.0}$\\pm$\\textbf{0.82} & \\textbf{0.9}$\\pm$\\textbf{0.74} & \\textbf{0.8}$\\pm$\\textbf{0.63} & 2.5$\\pm$0.85 \\\\ \n\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{MG}}  & 7.3$\\pm$4.78 & \\textbf{2.0}$\\pm$\\textbf{0.94} & \\textbf{3.1}$\\pm$\\textbf{1.29} & \\textbf{2.6}$\\pm$\\textbf{1.35} & \\textbf{2.9}$\\pm$\\textbf{1.29} & \\textbf{3.0}$\\pm$\\textbf{0.94} \\\\\n\\hline\n\\end{tabular}\n\\label{tab:distancesEdge}\n\\end{center}\n\\end{table}\n\\egroup\n\n\\bgroup\n\\setlength\\tabcolsep{1em} \n\\begin{table}[th!]\\scriptsize\n\\caption{Dependency between $\\gamma$ and RQA measures. First row shows correlations; second one estimated mutual information.}\n\\begin{center}\n\\vspace{-0.3cm}\n\\begin{tabular}{llllll}\n\\hline\n& \\textbf{RR} & \\textbf{DET} & \\textbf{LAM} & \\textbf{ENTR} & \\textbf{SWRP}  \\\\\n\\hline\n\\multirow{2}{*}{\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{Sin}}} & 0.35 & 0.42 & 0.40 & 0.43 & 0.55 \\\\\n& 0.66 & 0.69 & 0.67 & 0.68 & 0.81 \\\\\n\\hline\n\\multirow{2}{*}{\\centering\\rotatebox{90}{\\hspace{-0.7em} \\textbf{MG}}} & 0.57 & 0.40 & 0.38 & 0.62 & 0.68 \\\\\n& 0.77 & 0.55 & 0.54 & 0.78 & 0.80 \\\\\n\\hline\n\\end{tabular}\n\\label{tab:corrPrediction}\n\\end{center}\n\\end{table}\n\\egroup\n\n\n\n\\begin{figure}[!htp]\n\\centering\n\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.29\\columnwidth]{./sin_RQA_rr}\n        }\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.29\\columnwidth]{./sin_RQA_det}\n\t\t\t\t}\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[$\\mathrm{L_{max}}$]{\n\t\t\t\t\\includegraphics[width=0.29\\columnwidth]{./sin_RQA_lmax}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure{\n        \\includegraphics[width=1.3em,height=7.5em]{./colorbar}\n        }\t\n\t\t\t\t\n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.29\\columnwidth]{./sin_RQA_lam}\n        }\\hspace{-1em}\n\t\t\t\t~\t\t\t\n\t\t\t\t\\subfigure[ENTR]{\n        \\includegraphics[width=0.29\\columnwidth]{./sin_RQA_entr}\n\t\t\t\t}\\hspace{-1em}\n        ~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.29\\columnwidth]{./sin_RQA_swrp}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure{\n        \\includegraphics[width=1.3em,height=7.5em]{./colorbar}\n        }\t\t\t\t\t\t\t\t\n\t\t\t\t\n\\caption{RQA measures for sinusoid while changing $\\rho$ (vertical axis) and $\\omega_i$ (horizontal axis).}\n\\label{fig:sin_RQA}\n\\end{figure}\n\n\n\\begin{figure}[!htp]\n\\centering\n\t\t\t\t\\subfigure[$\\mathrm{L_{max}}$, $\\lambda$ and DIV]{\n        \\includegraphics[width=0.2\\textwidth]{./sin_MLLE_Lmax_DIV}\n\t\t\t\t\\label{fig:sin_MLLE_Lmax_DIV}\n        }\n\t\t\t\t~\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.22\\textwidth]{./sin_rr}\n        }\n\n\t\t\t\t\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.22\\textwidth]{./sin_det}\n\t\t\t\t}\t\t\t\t\n\t\t\t\t~\n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.22\\textwidth]{./sin_lam}\n        }\n\n\n        \\subfigure[ENTR]{\n        \\includegraphics[width=0.22\\textwidth]{./sin_entr}\n        }\n        ~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.22\\textwidth]{./sin_swrp}\n        }\n\t\t\t\t\n\\caption{RQA measures of sinusoidal input for a fixed input scaling, $\\omega_i=0.8$. The mean value of each measure is drawn with a solid black line and the standard deviation with gray dashed lines. All RQA measures are stable around their mean values until $\\rho$ is pushed beyond 1. Fig. \\ref{fig:sin_MLLE_Lmax_DIV} depicts the rescaled, mean value of $\\lambda$ (gray solid line), the mean value of $\\mathrm{L_{max}}$ (solid black line), and the mean value of DIV (dashed black line).}\n\\label{fig:sin_RQA_2d}\n\\end{figure}\n\n\n\\begin{figure}[htp!]\n\\centering\n\n\t\\subfigure[$\\lambda$ (light gray) vs $\\mathrm{L_{max}}$ (dark gray)]{\n\t\\includegraphics[width=0.48\\columnwidth]{./sin_LLE_vs_Lmax}\n\t\\label{fig:sin_MLLE_vs_Lmax}}\\hspace{-1.4em}\n\t~\n\t\\subfigure[$\\gamma$]{\n\t\\includegraphics[width=0.48\\columnwidth]{./sin_NRMSE}\n\t\\label{fig:sin_NRMSE}}\n\t\t\t\t\n\t\\subfigure[Determination of the edge of stability]{\n\t\\includegraphics[width=0.48\\columnwidth]{./sin_Edge}\n\t\\label{fig:sin_Edge}}\\hspace{-1.4em}\n\t~\n\t\\subfigure[$\\lambda$]{\n\t\\includegraphics[width=0.48\\columnwidth]{./sin_LLE}\n\t\\label{fig:sin_MLLE}}\n\t\t\t\t\n\\caption{Sinusoidal input. In Fig. \\ref{fig:sin_MLLE_vs_Lmax} $\\lambda$ (light gray surface) assumes positive values in correspondence of the gap of $\\mathrm{L_{max}}$ (dark gray surface), when its value drops to near-zero values. Prediction performance $\\gamma$, Fig. \\ref{fig:sin_NRMSE}, determination of the edge of stability, Fig. \\ref{fig:sin_Edge}, and $\\lambda$, Fig. \\ref{fig:sin_MLLE}, calculated for different values of $\\rho$ and $\\omega_i$. The gray area in Fig. \\ref{fig:sin_Edge} shows configurations with high accuracy and the red line the maximum values of $\\gamma$.}\n\\label{fig:sin_performance}\n\\end{figure}\n\n\n\\begin{figure}[ht!]\n\\centering\n\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.32\\columnwidth]{./sin_RQA_rr2}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.32\\columnwidth]{./sin_RQA_det2}\n\t\t\t\t}\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.32\\columnwidth]{./sin_RQA_lam2}\n        }\\hspace{-1.2em}\n\t\t\t\t\n\t\t\t\t\\subfigure[ENTR]{\n        \\includegraphics[width=0.32\\columnwidth]{./sin_RQA_entr2}\n\t\t\t\t}\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.32\\columnwidth]{./sin_RQA_swrp2}\n        }\n\t\t\t\t\n\\caption{Sinusoidal input. RQA (dark gray) vs $\\gamma$ (light gray). RQA measures calculated using $\\tau_{\\mathrm{RP}} = 0.3$.}\n\\label{fig:sin_RQA_vs_NRMSE}\n\\end{figure}\n\n\n\n\n\n\n\\begin{figure}[ht!]\n\\centering\n\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_rr}\n        }\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_det}\n\t\t\t\t}\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[$\\mathrm{L_{max}}$]{\n\t\t    \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_lmax}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure{\n        \\includegraphics[width=1.3em,height=7.5em]{./colorbar}\n        }\n\t\t\t\t\n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_lam}\n        }\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[ENTR]{\n        \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_entr}\n\t\t\t\t}\\hspace{-1em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.29\\columnwidth]{./mg_RQA_swrp}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure{\n        \\includegraphics[width=1.3em,height=7.5em]{./colorbar}\n        }\n\t\t\t\t\n\\caption{RQA measures of MG while changing $\\rho$ (vertical axis) and $\\omega_i$ (horizontal axis).}\n\\label{fig:mg_RQA}\n\\end{figure}\n\n\n\\begin{figure}[!ht]\n\\centering\n\n\t\t\t\t\\subfigure[$\\mathrm{L_{max}}$, $\\lambda$ and DIV]{\n        \\includegraphics[width=0.2\\textwidth]{./mg_MLLE_Lmax_DIV}\n\t\t\t\t\\label{fig:mg_MLLE_Lmax_DIV}\n        }\n\t\t\t\t~\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.22\\textwidth]{./mg_rr}\n        }\n\t\t\t\t\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.22\\textwidth]{./mg_det}\n\t\t\t\t}\n        ~        \n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.22\\textwidth]{./mg_lam}\n        }\n\t\t\t\t\n\t\t\t\t\\subfigure[ENTR]{\n        \\includegraphics[width=0.22\\textwidth]{./mg_entr}\n\t\t\t\t}\n\t\t\t\t~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.22\\textwidth]{./mg_swrp}\n        }\n\t\t\t\t\t\n\\caption{RQA measures for MG by considering a fixed input scaling, $\\omega_i=0.5$. Solid black lines represent the mean value of each measure; gray dashed lines the standard deviation. The standard deviation of every RQA increases significantly as soon as the system enters into unstable regime. In Fig. \\ref{fig:mg_MLLE_Lmax_DIV}, we report the rescaled, mean value of $\\lambda$ (gray solid line), the mean value of $\\mathrm{L_{max}}$ (solid black line), and the mean value of DIV (dashed black line).}\n\\label{fig:mg_RQA_2d}\n\\end{figure}\n\n\n\\begin{figure}[htp!]\n\\centering\n\n\t\\subfigure[$\\lambda$ (light gray) vs $\\mathrm{L_{max}}$ (dark gray)]{\n    \\includegraphics[width=0.48\\columnwidth]{./mg_LLE_vs_Lmax}\n    \\label{fig:mg_MLLE_vs_Lmax}}\\hspace{-1.4em}\n    ~\n    \\subfigure[$\\gamma$]{\n    \\includegraphics[width=0.48\\columnwidth]{./mg_NRMSE}\n    \\label{fig:mg_NRMSE}}\n    \n\t\t\\subfigure[Determination of the edge of stability]{\n\t\t\\includegraphics[width=0.48\\columnwidth]{./mg_Edge}\n\t\t\\label{fig:mg_Edge}}\\hspace{-1.4em}\n\t\t~    \n    \\subfigure[$\\lambda$]{\n    \\includegraphics[width=0.48\\columnwidth]{./mg_LLE}\n    \\label{fig:mg_MLLE}}\n\t\t\n\\caption{In Fig. \\ref{fig:mg_MLLE_vs_Lmax}, $\\lambda$ (light gray surface) becomes positive when the value of $\\mathrm{L_{max}}$ (dark gray surface) is drastically reduced to a value close to zero. ESN performance $\\gamma$, Fig. \\ref{fig:mg_NRMSE}, determination of the edge of stability, Fig. \\ref{fig:mg_Edge}, and $\\lambda$, Fig. \\ref{fig:mg_MLLE}, for MG time-series when considering different values of $\\rho$ and $\\omega_i$. The gray area in Fig. \\ref{fig:mg_Edge} shows configurations with high accuracy and the red line represents the maximum values of $\\gamma$.}\n\\label{fig:mg_performance}\n\\end{figure}\n\n\n\\begin{figure}[ht!]\n\\centering\n\n\t\t\t\t\\subfigure[RR]{\n        \\includegraphics[width=0.32\\columnwidth]{./mg_RQA_rr2}\n        }\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[DET]{\n        \\includegraphics[width=0.32\\columnwidth]{./mg_RQA_det2}\n\t\t\t\t}\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[LAM]{\n        \\includegraphics[width=0.32\\columnwidth]{./mg_RQA_lam2}\n        }\\hspace{-1.2em}\n\t\t\t\t\n\t\t\t\t\\subfigure[ENTR]{\n        \\includegraphics[width=0.32\\columnwidth]{./mg_RQA_entr2}\n\t\t\t\t}\\hspace{-1.2em}\n\t\t\t\t~\n\t\t\t\t\\subfigure[SWRP]{\n        \\includegraphics[width=0.32\\columnwidth]{./mg_RQA_swrp2}\n        }\n\t\t\t\t\n\\caption{MG time-series. RQA (dark gray) and $\\gamma$ (light gray) for different values of $\\rho$ and $\\omega_i$. RQA measures calculated using $\\tau_{\\mathrm{RP}} = 0.4$.}\n\\label{fig:mg_RQA_vs_NRMSE}\n\\end{figure}\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nIn this paper, we have investigated the possibility to analyze the dynamics of echo state networks by means of recurrence plots and related RQA complexity measures.\nBeing a non-autonomous system, its dynamical properties depend not only on the spectral radius of the reservoir matrix, but also on the particular input driving the network and the related scaling coefficient.\nWe have shown how RPs can be used for reliably analyzing the internal state evolution of the network when qualitatively different signals are presented as input.\nOur results indicates that the recurrences elaborated from the reservoir dynamics clearly denoted an instability level monotonically related to the instability of the network.\nThis coherent behavior suggests that RPs could be used as a visual tool to design a network.\n\nOur results highlighted a strong statistical agreement between the maximal local Lyapunov exponent and $\\mathrm{L_{max}}$, an RQA complexity measure based on the diagonal lines in a recurrence plot. This fact could be exploited to define novel, input-dependent stability criteria for reservoirs based on statistics of state recurrences.\nWe have also examined prediction problems on two signals: a sinusoidal waveform and the Mackey-Glass time-series.\nThe smooth variation of $\\lambda$ was mainly driven by $\\rho$ but not very sensitive to the different values used for $\\omega_i$.\nThis fact prevented to accurately identify the input-dependent edge of stability of the network, i.e., the region of the control parameter space where the network departs from a stable regime to reach and operate in an unstable, yet computationally effective phase.\nTo this end, we have designed a criterion based on the fluctuations of RQA measures. We observed that, when the network approached the edge of stability, all RQA measures abruptly changed their average value showing also a rapid increase of fluctuations.\nWe demonstrated in the experiments that the proposed criterion based on RQA measures was capable to identify the edge of stability with a significantly better precision than $\\lambda$.\n\nThe results presented in this paper suggest to allocate future research effort in a number of ways.\nFirst, it would be interesting to study reservoirs initialized according to different (either statistical or deterministic) rules. In addition, it could be interesting to derive a connection between the network short-term memory capability and RQA measures.\nIt is worth considering invariant measures elaborated from RPs: the correlation entropy \\cite{thiel2004estimation} provides a natural lower-bound to the sum of positive Lyapunov exponents.\nIt might be interesting to study the so-called joint recurrences, calculated between the input system and the reservoir. This might be useful to discover (hidden) mechanisms of synchronization.\nFinally, we suggest that such RP-based analyses could be extended to fully trainable recurrent neural networks. The recurrent layer would not be modeled as a multi-dimenional time-series, but instead as a time-varying network. To this end, we will search for an interplay between graph matching \\cite{gm_survey} and recurrence analysis in order to study the dynamics of neural networks with trainable and/or variable size recurrent layers.\n\n\n\\clearpage\n\\bibliographystyle{abbrvnat}\n\\bibliography{Bibliography}\n\n\n", "itemtype": "equation", "pos": 51967, "prevtext": "\nWe obtained a time-series of 150000 time-steps using $\\tau_{\\mathrm{MG}} = 17, \\alpha = 0.2, \\beta = 0.1$, initial condition $x(0)=1.2$, and 0.1 as integration step for (\\ref{eq:MG}).\nIn Fig. \\ref{fig:mg}, we report RPs of the input signal and those related to the three different settings of the ESN with an increasing value of $\\rho$.\nAs in the previous case, it is worth stressing the similarity between RPs on input in Fig. \\ref{fig:MG_input_t05} and reservoir in Fig. \\ref{fig:MG_SR09_t05}.\nAs the reservoir is pushed toward instability (by increasing $\\rho$), it is possible to observe the usual incremental transition toward a chaotic regime in the related RPs.\nIn Figs. \\ref{fig:MG_SR15_t05} and \\ref{fig:MG_SR2_t05} $\\rho$ is set to 1.5 and 2, respectively. In both cases we observe typical line patterns of unstable systems.\nHowever, we note that when $\\rho=2$ the system becomes fully chaotic (e.g., see \\ref{fig:LM3.99_SR09_T03} for a visual comparison), with very short and erratic diagonal lines.\n\n\n\\begin{figure}[ht!]\n\\centering\n\n    \\subfigure[RP of the input signal]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_input_t05}\n    \\label{fig:MG_input_t05}}\\hspace{-1.2em}\n    ~\n\t\\subfigure[$\\rho = 0.9$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR09_t05}\n    \\label{fig:MG_SR09_t05}}\n\t\t\n    \\subfigure[$\\rho  = 1.5$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR15_t05}\n    \\label{fig:MG_SR15_t05}}\\hspace{-1.2em}\n    ~\n    \\subfigure[$\\rho  = 2$, $\\tau_{\\mathrm{RP}} = 0.5$]{\n    \\includegraphics[width=0.23\\textwidth]{./MG_SR2_t05}\n    \\label{fig:MG_SR2_t05}}\n\t\t\n\\caption{RPs of the MG time-series and of the state of the reservoir.}\n\\label{fig:mg}\n\\end{figure}\n\n\n\n\n\\subsection{Characterization of reservoirs with RQA}\n\\label{sec:exp2}\n\nHere we evaluate RQA measures with respect to the accuracy achieved by ESN relative to the forecast of the sinusoidal and MG time-series.\nFor this purpose, we consider also $\\lambda$ (\\ref{eq:MLLE}), which, as previously discussed, is a well-known tool for determining the stability of a dynamic system.\nWe evaluate how $\\lambda$ and RQA measures vary as the values of $\\rho$ and $\\omega_i$ are modified.\nFirst, we compare $\\lambda$ with $\\mathrm{L_{max}}$, the value of the longest diagonal line in an RP -- see Eq. \\ref{eq:lmax} and related discussion. $\\mathrm{L_{max}}$ is a global indicator of stability that we show here to be highly correlated with $\\lambda$.\nSuccessively, we consider the condition $\\lambda>0$ as a discriminator to determine the (input-dependent) edge of stability of the network.\nWe then compare such information with those provided by the criterion proposed in Eq. \\ref{eq:edge_criterion}, which is based on fluctuations of RQA complexity measures.\nTo demonstrate the validity of the proposed method, we show that the network configurations determining the edge of stability by using Eq. \\ref{eq:edge_criterion} produce results that are significantly better (in a statistical sense) than those derived with the $\\lambda>0$ criterion.\n\nThe error measure that we adopt for evaluating the prediction is the Normalized Root Mean Squared Error (NRMSE) function,\n\n\n", "index": 37, "text": "\\begin{equation}\n\\label{eq:nrmse}\n\\textrm{NRMSE} = \\frac{\\sqrt{\\frac{1}{K} \\sum_{i=1}^K (\\hat{y}_i - y_i )^2 }}{y_{\\text{max}} - y_{\\text{min}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\textrm{NRMSE}=\\frac{\\sqrt{\\frac{1}{K}\\sum_{i=1}^{K}(\\hat{y}_{i}-y_{i})^{2}}}{%&#10;y_{\\text{max}}-y_{\\text{min}}},\" display=\"block\"><mrow><mrow><mtext>NRMSE</mtext><mo>=</mo><mfrac><msqrt><mrow><mfrac><mn>1</mn><mi>K</mi></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt><mrow><msub><mi>y</mi><mtext>max</mtext></msub><mo>-</mo><msub><mi>y</mi><mtext>min</mtext></msub></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}]