[{"file": "1601.03610.tex", "nexttext": "\nwhere $|G|$ denotes the size of $G$.\nScientists need to assess the degree of similarity between the matched treated and control units. Similarity relation ($\\approx$) can be assessed by estimating the standardized mean difference for each confounding variable between matched treated and control units, or by applying graphical methods such as quantile-quantile plots, cumulative distribution functions plots, etc. \\cite{austin2009relative, austin2008goodness, austin2008assessing, harder2010propensity}. If sufficient balance has not been achieved, the applied matching method needs to be revised.\n\n\\subsection{Directed Acyclic Graphs}\n\nPearl \\cite{pearl2010introduction, pearl2000causality, pearl1995causal} proposed the use of directed acyclic graphs (DAGs) for representing causal relationships. In causal graphs, nodes represent the variables of the experiment. If $\\mathbf{P}$ a set of the direct predecessors (\\textit{parents}) of a node $Y$, a direct arrow from a node $Q$ (can represent $X$ or one of the variables of set $\\mathbf{Z}$) to $Y$ will exist only if $Y \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q | \\mathbf{P} \\textbackslash\\{Q\\}$. A direct arrow from a node $Q$ to a node $Y$ represents a causal relationship between the two variables (i.e. $Q$ causes $Y$). \n\nPearl also introduces a graphical criterion for defining a \\textit{sufficient set} of variables that need to be controlled in order to achieve conditional ignorability when testing the causal impact of a variable $X$ on a variable $Y$ (\\textit{back-door criterion}). According to this rule, a subset $\\mathbf{H}$ of  variables  is \\textit{sufficient} if no element of $\\mathbf{H}$ is a descendant of $X$ and the elements of $\\mathbf{H}$ block all paths from $X$ to $Y$ that end with an arrow to $X$ (\\textit{back-door paths}). The intuition behind this criterion is that back-door paths from $X$ to $Y$ represent spurious associations and therefore need to be excluded in order to obtain unbiased estimation of the causal effect of $X$ on $Y$. \n\nThe graph can be used to derive a system of linear equations where each variable is regressed on all its direct predecessors. The graph can be created either by utilizing prior knowledge about the structure of the model or by guessing a model and assessing its validity using observational data (e.g. by fitting the  data to the system of linear equations derived from the graph). However, this approach suffers from the same limitations as regression based ones.  \n\n\\subsection{Causality on Time-series}\n\nCausality studies on time-series have been largely based on Granger causality \\cite{granger1969investigating}. The Granger causality test examines if past values of one variable are useful in prediction of future values of another variable. In detail, a time-series $X$ Granger causes a time-series $Y$ if modeling $Y$ by regressing it on past values of both $Y$ and $X$ results in reduced residual noise compared to a simple autoregressive model. However, Granger causality tests cannot prove real causality since they do not satisfy the conditional ignorability assumption, i.e., the values of both treatment variable $X$ and control variable $Y$ may be driven by a third variable. Moreover, it considers only linear relationships. Granger causality has been extended to handle multivariate cases \\cite{barrett2010multivariate} as well as non-linear cases \\cite{zhang2012kernel, nawrath2010distinguishing}. However, as it was mentioned also at the introduction, inaccuracies on model specification may result in misleading conclusions. In \\cite{peters2013causal} authors propose an additional model check procedure after fitting a model in order to reduce the amount of false positive causality results. \n\nModel-free approaches for causal inference in time-series have also been proposed. Schreiber introduced \\textit{transfer entropy} \\cite{schreiber2000measuring} in order to examine whether the uncertainty about a time-series $Y$ is reduced by knowing past values of a time-series $X$. Transfer entropy is a model-free equivalent of bivariate Granger causality. This framework has been extended by Runge \\cite{pompe2011momentary} in order to handle also multivariate cases. In detail, Runge proposed the use of directed acyclic graphs for time-series data. The nodes of the graph correspond to the time-series of the study as well as their lagged versions. Consider $Q^l$, the lagged version of a time-series $Q$, and let $P$ denote the parents of a node $Y$.\nAn arrow from $Q^l$ to $Y$ will exist only if $Y \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q^l | \\mathbf{P} \\textbackslash\\{Q^l\\}$. The graph is built by performing conditional independence tests using conditional mutual information. However, performing conditional independence tests on continuous data is particularly challenging especially for high-dimensional data \\cite{zhang2012kernel}.\n\n\n\n\n\\section{Proposed Mechanism}\n\\label{sec:mechanism}\n\nGiven the previously discussed limitations on existing methodologies for causality discovery in time-series, we propose a novel framework that enables causal inference in time-series  data that is based on matching design and therefore does not require specification of a model class. The proposed framework also requires only few conditional independence tests, thus it can handle more effectively high-dimensional data. Denote by $Y$ and $X$ the time-series that represent the effect and the cause, respectively and by $\\mathbf{Z}$ a set of time-series representing other characteristics relevant for the study. Let us also denote by $Y^l$, $X^l$ and $\\mathbf{Z}^l$ the $l$-lagged versions of the time series $Y$, $X$ and $\\mathbf{Z}$, respectively (i.e., if $Y(u)$ the $u$-th sample of $Y$, $Y(u) = Y^l(u+l)$). We define a maximum lag value $L$ and a set of time-series $\\mathbf{S}$ =\\{$Y$, $Y^1$, ..., $Y^L$, $X$, $X^1$, ..., $X^L$, $\\mathbf{Z}$, $\\mathbf{Z}^1$, ..., $\\mathbf{Z}^L$\\}. In order to build a graph, we examine the dependencies between the variables $X$, $Y$ and all the other variables of the set $\\mathbf{S}$. According to Pearl's framework, a line from a variable $Q_1$ to a variable $Q_2$ is added only if $Q_2$ is dependent on $Q_1$, conditional on all direct predecessors of $Q_2$. Here, we relax this condition as follows:\n\nA line from a lagged node $Q_1^l$ (including lag 0) to a non-lagged node $Q_2$ exists if:\n\n\\begin{itemize}\n\\item $Q_1^l$ precedes temporally $Q_2$ i.e.$\\forall u$,  the $u^{th}$ sample of $Q_1^l$ precedes temporally the $u^{th}$ sample of $Q_2$.\nIn our case the time series are sampled on daily basis (index $u$), but for lag zero, within the same day $u$ the series $Q_1$ can be sampled before $Q_2$.\n\\item $Q_2 \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q_1^l | \\mathbf{P}\\cap (Q_1, Q_1^1, ..., Q_1^m)\\textbackslash\\{Q_1^l\\}$, where $\\mathbf{P}$ is a set of the direct predecessors of $Q_2$ and $m<l$.\n\\end{itemize} \n\nThus, in our framework, a direct edge between two nodes does not necessarily imply a causal relationship. By relaxing Pearl's rule, we reduce the number of conditional independence tests that are required. As discussed later, applying this relaxed rule for creating the graph does not impact the validity of our methodology. In this study we use vanishing correlation as a signature of independence. Mutual information could also be used in order to test independence. \n\n\\begin{algorithm}[htp]\n \\KwData{The set of time-series $\\mathbf{S}$}\n \\KwResult{ The set of confounding variables $\\mathbf{H}$ }\n\\SetKwFunction{algo}{algo}\\SetKwFunction{predecessors}{predecessors}\n$\\mathbf{P1}:=\\predecessors{$\\mathbf{S}$, $Y$}$\\;\n$\\mathbf{P2}:=\\predecessors{$\\mathbf{S}$, $X$}$\\;\n$\\mathbf{H}:= \\mathbf{P1}\\cap \\mathbf{P2}$\\;\n\n  \\SetKwFunction{myproc}{Procedure}{}{}\n\\tcc{this procedure returns a set $\\mathbf{P}$ of the direct predecessors of node $Q$. $\\mathbf{P}$ is a subset of $\\mathbf{S}$.}\n  \\myproc{\\predecessors{$\\mathbf{S}$, $Q$}}{\n$\\mathbf{P}:=\\{\\}$\\; \n\\For{i=0 to L}{\n  \\For{all $S^0 \\in \\mathbf{S}$}{\n\t $\\mathbf{B}:=(S^0, S^1, ..., S^L) \\cap \\mathbf{P}$\\;\n\t \\If{({$Q|\\mathbf{B} \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} S^i$} \\textbf{and} {$S^i(u)$ precedes $Q(u)$})}{\n        $\\mathbf{P}:=\\mathbf{P} \\cup S^i $\\;\n   }\t\n  }\n }\nreturn $\\mathbf{P}$\\;}\n \\caption{Defining the set of confounding variables.}\n\\label{Alg:predecessors}\n\\end{algorithm}\n\nThe units of the study correspond to daily stamps of the set of time-series $\\mathbf{S}$. Thus, a unit $u$ will be characterized by a set of values $\\mathbf{S}(u)$ =\\{$Y(u)$, $Y^1(u)$, ..., $Y^L(u)$, $X(u)$, $X^1(u)$, ..., $X^L(u)$, $\\mathbf{Z}(u)$, $\\mathbf{Z}^1(u)$, ..., $\\mathbf{Z}^L(u)$\\}. In what follows we will discuss how our methodology addresses the three general assumptions of causality studies discussed in Section \\ref{sec:background}.\n\n\\textit{Conditional Ignorability Assumption:} We apply the Algorithm \\ref{Alg:predecessors} in order to find the set of time-series $\\mathbf{H}$ that need to be controlled in order to satisfy the conditional ignorability assumption. According to our method, the resulted set contains all the direct predecessors that nodes $X$ and $Y$ have in common. Thus, all the variables that are correlated both with $X$ and $Y$ time-series are included; hence, the set $\\mathbf{H}$ is sufficient. However $\\mathbf{H}$ may include also redundant time-series i.e. some of the time-series included at $\\mathbf{H}$ may not correlate with $X$ or $Y$ conditional to a subset of $\\mathbf{H}$. In causality studies based on regression, including redundant predictors on the model could result in overfitting and would jeopardize the validity of the conclusions. Moreover, the application of methods based on conditional independence tests using information theoretic approaches would be challenged by the inclusion of redundant covariates since it would require conditioning on large sets of variables. In contrast, studies based on matching are less affected by the inclusion of redundant confounding variables (spurious correlations). Several methods that enable matching on a large number of confounding variables have been proposed \\cite{diamond2013genetic, sekhon2009opiates, rubin1997estimating}. In addition, scientists are able to apply balance diagnostic tests in order to assess if any confounding bias has been adequately eliminated; consequently, false conclusions due to confounding bias can be diminished. Following the matching design, the set of time-series $\\mathbf{H}$ is controlled by creating a set of pairs of units $G$ where each treated unit $u$ is matched with one or more untreated units $v$ such that $\\mathbf{H}(u)  \\approx \\mathbf{H}(v)$. \n\n\n\\textit{Stable Unit Treatment Value Assumption:} Denote by $\\mathbf{P}$ the set of time-series that are direct predecessors of the effect variable $Y$. Assuming $X \\in \\mathbf{P}$ (if not, $X$ is independent of $Y$ and therefore there is no causation), the  assumption is violated if $X^l \\in \\mathbf{P}$ and $X^l \\not\\in \\mathbf{H}$, for $ l > 0$. Since units correspond to daily samples, $X^l \\in \\mathbf{P}$ implies that the outcome $Y(u)$ of a unit $u$ depends on the treatment $X(u-l)$ received by a unit $u-l$. In order to satisfy the  assumption, we modify the $\\mathbf{H}$ set as follows:\n\n", "itemtype": "equation", "pos": 15824, "prevtext": "\n\\title{Model-free Causality Detection: An Application to Social Media and Financial Data}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\numberofauthors{3} \n\n\n\n\\author{\n\n\n\n\n\n\n\n\n\n\n\n\\alignauthor\nFani Tsapeli \\\\\n       \\affaddr{School of Computer Science}\\\\\n       \\affaddr{University of Birmingham}\\\\\n       \\affaddr{Edgbaston B15 2TT Birmingham, UK}\\\\\n       \\email{t.tsapeli@cs.bham.ac.uk}\n\n\\alignauthor\nMirco Musolesi \\\\\n       \\affaddr{Department of Geography}\\\\\n       \\affaddr{University College London}\\\\\n       \\affaddr{Gower Street WC1E 6BT London, UK}\\\\\n       \\email{m.musolesi@ucl.ac.uk}\n\n\\alignauthor\nPeter Tino \\\\\n       \\affaddr{School of Computer Science}\\\\\n      \\affaddr{University of Birmingham}\\\\\n      \\affaddr{Edgbaston B15 2TT Birmingham, UK}\\\\\n      \\email{p.tino@cs.bham.ac.uk}\n}\n\\maketitle\n\\begin{abstract}\nAccording to behavioral finance, stock market returns are influenced by emotional, social and psychological factors. Several recent works support this theory by providing evidence of correlation between stock market prices and collective sentiment indexes measured using social media data. However, a pure correlation analysis is not sufficient to prove that stock market returns are influenced by such emotional factors since both stock market prices and collective sentiment may be driven by a third unmeasured factor. Controlling for factors that could influence the study by applying multivariate regression models is challenging given the complexity of stock market data. False assumptions about the linearity or non-linearity of the model and inaccuracies on model specification may result in misleading conclusions.\n\nIn this work, we propose a novel framework for causal inference that does not require any assumption about the statistical relationships among the variables of the study and can effectively control a large number of factors. We apply our method in order to estimate the causal impact that information posted in social media may have on stock market returns of four big companies. Our results indicate that social media data not only correlate with stock market returns but also influence them.\n\n\\end{abstract}\n\n\n\\category{G.3}{Probability and Statistics}{Time-series Analysis}\n\n\\terms{Causal Inference, Time-series analysis}\n\n\\keywords{Causality, social media, stock market, sentiment tracking, time-series} \n\n\n\\section{Introduction}\n\nWe are living in the era of social media, \nusing tools such as Facebook, Twitter and blogs to communicate with our friends, to share our experiences and to express our opinion and emotions. Recently, mining and analyzing this kind of data has emerged as an area of great interest for both the industrial and academic communities. Several studies have examined the ability of social media to serve as crowd-sensing platforms. For example, authors in~\\cite{asur2010predicting} demonstrate that social media can monitor the popularity of products or services and predict their future revenues. Evidence has been found that social media can be used to predict election results \\cite{tumasjan2010predicting} or even stock market prices \\cite{bollen2011twitter}.\n\nMost studies so far have focused on using social media data as early indicators of real-world events. But to what extent do opinions expressed through social media actually have a \\textit{causal} influence on the examined events? For example, are stock market prices influenced by the opinions and sentiments that are reported in social media, or is it the case that stock market prices and sentiments are driven only by other (e.g. financial) factors? Would the results have been different if we could manipulate social media data? In order to answer such questions a causality study is required. \n\nSome recent studies have examined the ability of social media to influence real-world events by applying randomized control trials. For example, authors in~\\cite{Bond2012} examine the effect of political mobilization messages by using Facebook to deliver such messages to a randomly selected population; the effect of the messages is measured by comparing the real-world voting activity of this group with the voting activity of a control group. Similarly, in \\cite{Muchnik2013} authors use randomized trials in order to examine the social influence of aggregated opinions posted in a social news website. Randomized control trials are a reliable technique for conducting causal inference studies. However, their applicability is limited since they require scientists to gather data using experimental procedures and do not allow the exploitation of the large amount of observational data. In many cases, it is not feasible to apply experimental designs or it is considered unethical. \n\nIn this work, we study the causal impact of social, psychological and emotional factors on stock market prices of big companies using observational data collected through Twitter. Twitter enables us to capture people sentiments and opinions about traded assets and their reactions on related news and events. Previous works have demonstrated that social media data correlate with stock market prices~\\cite{preis2010complex, bollen2011twitter, zhang2011predicting, zhang2012predicting}. These studies were predominantly based on correlation or Granger causality analysis. Granger causality tests the ability of a time-series to predict values of another one. However, it cannot be used to discover real causality. A positive result on a Granger causality test does not necessarily imply that there is a causal link between the examined time-series since both the examined time-series may be influenced by a third variable (\\textit{confounding bias}). Multivariate regression techniques can be applied in order to control for confounding bias. Some studies attempt to improve the accuracy of stock market prediction models by applying multivariate regression \\cite{mao2011predicting}. However, the focus of these works is on prediction rather than on causal inference. Applying regression models for causal inference suffers from two main limitations. First, stock market prices can be influenced by a large number of factors such as stock market prices of other companies \\cite{kenett2010dominating, chi2010network}, foreign currency exchange rates and commodity prices. Such factors may also influence people sentiments. Consequently, to eliminate any confounding bias one is required to include a large number of predictors in the regression model. Estimation of regression coefficients in a model with a large number of predictors can be challenging.  When data dimensionality is comparable to the sample size, noise may dominate the 'true' signal, rendering the study infeasible~\\cite{fan2014challenges}. Second, inaccuracies in model specification, estimation or selection may result in invalid causal conclusions.\n\nGiven the limitations of model-based methods, we propose a novel framework for causal inference in time-series that is based on \\textit{matching design}~\\cite{william2002experimental, MatchingSurvey}. This technique attempts to eliminate confounding bias by creating pairs of \\textit{similar} treated and untreated objects, i.e. objects with similar values on baseline characteristics that could influence the causality study. Thus, the effect of an event is estimated by comparing each object exposed to an event with a \\textit{similar} object that has not been exposed. Matching design bypasses the limitations of regression-based methods since it does not require specification of a model class. However, it cannot be applied in time-series since it assumes that the objects of the study are realizations of i.i.d variables. We reformulate the concept of matching design to make it suitable for causal inference on time-series data.  In our case the time-series collection includes  \\textit{treatment} time-series $X$, \\textit{response} time-series $Y$ and a set of time-series $\\mathbf{Z}$ which contain characteristics relevant to the study. We consider time-instants of the whole time-series collection as the objects of the study, i.e. the values of $X, Y$ and $\\mathbf{Z}$ at time instant $u$ constitute the $u$-th object (sampled at time $u$).\nWe assess the causal impact of a time-series $X$ on $Y$ by comparing different objects on $Y$ after controlling for characteristics captured in $\\mathbf{Z}$. As explained in section \\ref{sec:mechanism}, our methodology assures that the objects are uncorrelated, which is a weaker version of the independence assumption requirement of the matching design. We apply our framework in order to estimate the causal impact that the sentiment of information posted in social media may have on traded assets. In detail, we estimate a daily sentiment index (\\textit{treatment} time-series) based on information posted in Twitter and we assess its impact on daily stock market closing prices (\\textit{response} time-series) of four big technological companies after controlling for other factors that may influence the study, such as the performance of other big companies.\n\nIn summary, the contribution of this work is twofold:\n\n\\begin{enumerate}\n\\item We propose a causal inference framework for time-series that can be applied to high-dimensional data without imposing any restriction on the model describing the associations among the data. We demonstrate, using synthetic data, that our methodology is more effective on detecting true causality compared to other methods that have been applied so far, for causal inference in time-series. \n\n\\item We apply our method in order to quantify the causal impact of emotional and psychological factors, captured by social media, on stock market prices of specific companies. To the best of our knowledge, this is the first study that measures the causal influence of such factors on finance. \n\\end{enumerate}\n\nThe rest of this paper is organized as follows. In Section \\ref{sec:background} we discuss the main methodologies that are used for causal inference. In Section \\ref{sec:mechanism} we present the proposed framework. In Section \\ref{sec:evaluation} we apply our method in order to assess the causal impact of information posted in Twitter on stock market prices of specific companies. Moreover, we evaluate our approach on synthetic data, in conjunction with other methods that have been previously applied for causal inference in time-series. Finally, in Section \\ref{sec:related-work} we discuss some relevant works which attempt to uncover the relationship between social media data and stock market movement. Section \\ref{sec:conclusions} concludes the paper by summarizing our contributions.\n\n\\section{Background on Causal Inference}\n\\label{sec:background}\n\n\nCausal analysis attempts to understand whether differences on a specific characteristic $Y$ within a population of \\textit{units} are caused by a factor $X$. $Y$ is called \\textit{response}, \\textit{effect} or \\textit{outcome} variable and $X$ \\textit{treatment} variable or \\textit{cause}.  \\textit{Units} are the basic objects of the study and they may correspond to humans, animals or any kind of experimental objects. \n\n\\subsection{Potential Outcome Framework}\n\nThe key concept on causation theory is that given a unit $u$, the value of the corresponding response variable  $Y(u)$ can be manipulated by changing the value of the treatment variable $X(u)$~\\cite{Holland86, morgan2014counterfactuals}.  In this paper, we will consider $X(u)$ as a binary treatment variable. Hence, there will be two treatments: $x_1$ for treated units and $x_0$ for untreated. We denote by $Y_1(u)$ the value of $Y$ when $X(u)=x_1$ and $Y_0(u)$ when $X(u)=x_0$. In order to test the effect of $x_1$ on unit $u$, we need to estimate the quantity $Y_1(u) - Y_0(u)$. The fundamental problem of causal inference is that \\textit{it is impossible to observe both $Y_1(u)$ and $Y_0(u)$ on the same unit $u$ and, therefore, it is impossible to measure the real causal effect of $x_1$ on the unit.}~\\cite{Holland86, morgan2014counterfactuals}. Thus, the average effect of a treatment $X$ is estimated by comparing a population of objects that received the treatment $x_1$ with a population that received the treatment $x_0$ and evaluating the corresponding average values of the effect variable $Y$. We denote by  $Y_1$ and $Y_0$ random variables representing the outcome variable when the treatments $x_1$ and $x_0$ are applied, respectively. We also define a (random) variable $DY = Y_1 - Y_0$. Then, the average treatment effect (ATE) of $x_1$  is estimated as the expected value $E\\{DY\\}$. \n \nThe average treatment effect can be estimated only if the following three assumptions are satisfied:\n\n\\begin{enumerate}\n\\item the effect differences are i.i.d. realizations of  $DY$.\n\\item the observed outcome in one unit is independent from the treatment received by any other unit (\\textit{Stable Unit Treatment Value Assumption - SUTVA}).\n\\item the assignment of units to treatments is independent from the outcome (\\textit{ignorability}). Considering two groups of units $U$, $V$ such that the treatment $x_1$ is applied on each unit $u \\in U$ and $x_0$ on each unit $v \\in V$, ignorability can be formally expressed as $P(X(u)) = P(X(v))$, $\\forall u \\in U$, $v \\in V$. The assumption of ignorability requires that all the units have equal probability to be assigned to a treatment. If this assumption does not hold, the units that received a treatment $x_1$ may systematically differ from units that did not receive such a treatment. In such a case  the average value of the outcome variable of the treated units  could be different from that of other units, even if the treatment had not been received at all. \n\\end{enumerate}\n\nIn experimental studies, ignorability can be achieved by randomly assigning units to treatments. However, in observational studies this is not feasible. Instead, the average treatment effect can be estimated by relaxing ignorability to \\textit{conditional ignorability}. According to conditional ignorability assumption, the treatment assignment is independent from the outcome, conditional on a set of confounding  variables $\\mathbf{H}$. Variables $\\mathbf{H}$ represent baseline characteristics of the units that are considered relevant for the study (e.g. in a medical study that examines the impact of a drug, baseline characteristics could be the previous health condition of the units (in this case patients), their age etc.). Thus, conditional ignorability is expressed as $P(X(u)|\\mathbf{H}(u)) = P(X(v)|\\mathbf{H}(v))$, $\\forall u \\in U$, $v \\in V$. The variables that must be included in the set $\\mathbf{H}$ in order to achieve conditional ignorability are also called \\textit{confounding variables}.\n\nThere are two main methodologies that are applied in order to achieve conditional ignorability: \\textit{regression} and \\textit{matching}~\\cite{william2002experimental}. Regression expresses the outcome variable $Y$ as a function of the treatment variable $X$ and the set of variables $\\mathbf{H}$~\\cite{freedman1997association, imbens2004nonparametric}. Linear models are usually applied. Methods based on regression require  scientists to correctly specify a regression model. These models are affected by the typical problems of model-based approaches to causality detection.\n\nMatching comprises a more flexible methodology for causal inference in observational data since it does not require the specification of a model~\\cite{MatchingSurvey}. Conditional ignorability is achieved by creating sub-population within which the values of the confounding variables $\\mathbf{H}$ are the same or similar. Thus, considering a set $G$ of pairs of treated and untreated (\\textit{control}) units $(u, v)$ such that $\\mathbf{H}(u)  \\approx \\mathbf{H}(v)$, we can estimate the average treatment effect (ATE) as \n\n", "index": 1, "text": "\\begin{equation}\n\\widehat{E}\\{DY\\} = \\frac{\\sum_{(u, v) \\in G} Y(u) - Y(v)}{|G|},\n\\label{eq:ATE}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\widehat{E}\\{DY\\}=\\frac{\\sum_{(u,v)\\in G}Y(u)-Y(v)}{|G|},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>E</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>Y</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>G</mi></mrow></msub><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>G</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\nsatisfying $Y(u) {\\protect\\mathpalette{\\protect\\independenT}{\\perp}} \\mathbf{X}(v) | \\mathbf{H}(u)$, $\\forall u \\neq v$.\n\n\n\\textit{i.i.d. Assumption:}  Denote by $Y_1$  the value of the outcome variable for units that have received a treatment and with $Y_0$ for untreated units (assuming $X$ is a binary treatment). The average causal effect is estimated as $\\widehat{E}\\{Y_1 - Y_0 | \\mathbf{H}\\}$. In order to enable statistical inference, the variable $\\Delta Y := Y_1 - Y_0 | \\mathbf{H}$ needs to be i.i.d.. If $\\mathbf{P}$ the set of direct predecessors of $Y$, the outcome value $Y(u)$ of each unit $u$ will depend on the outcome value $Y(u-l)$ of unit $u-l$ if there is a time-series $Y^l \\in \\mathbf{P}$. In case that $Y^l \\notin \\mathbf{H}$, the i.i.d. assumption would be violated. In order to satisfy this assumption, we modify the set of time-series $\\mathbf{H}$ as follows:\n\n\n", "itemtype": "equation", "pos": 27255, "prevtext": "\nwhere $|G|$ denotes the size of $G$.\nScientists need to assess the degree of similarity between the matched treated and control units. Similarity relation ($\\approx$) can be assessed by estimating the standardized mean difference for each confounding variable between matched treated and control units, or by applying graphical methods such as quantile-quantile plots, cumulative distribution functions plots, etc. \\cite{austin2009relative, austin2008goodness, austin2008assessing, harder2010propensity}. If sufficient balance has not been achieved, the applied matching method needs to be revised.\n\n\\subsection{Directed Acyclic Graphs}\n\nPearl \\cite{pearl2010introduction, pearl2000causality, pearl1995causal} proposed the use of directed acyclic graphs (DAGs) for representing causal relationships. In causal graphs, nodes represent the variables of the experiment. If $\\mathbf{P}$ a set of the direct predecessors (\\textit{parents}) of a node $Y$, a direct arrow from a node $Q$ (can represent $X$ or one of the variables of set $\\mathbf{Z}$) to $Y$ will exist only if $Y \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q | \\mathbf{P} \\textbackslash\\{Q\\}$. A direct arrow from a node $Q$ to a node $Y$ represents a causal relationship between the two variables (i.e. $Q$ causes $Y$). \n\nPearl also introduces a graphical criterion for defining a \\textit{sufficient set} of variables that need to be controlled in order to achieve conditional ignorability when testing the causal impact of a variable $X$ on a variable $Y$ (\\textit{back-door criterion}). According to this rule, a subset $\\mathbf{H}$ of  variables  is \\textit{sufficient} if no element of $\\mathbf{H}$ is a descendant of $X$ and the elements of $\\mathbf{H}$ block all paths from $X$ to $Y$ that end with an arrow to $X$ (\\textit{back-door paths}). The intuition behind this criterion is that back-door paths from $X$ to $Y$ represent spurious associations and therefore need to be excluded in order to obtain unbiased estimation of the causal effect of $X$ on $Y$. \n\nThe graph can be used to derive a system of linear equations where each variable is regressed on all its direct predecessors. The graph can be created either by utilizing prior knowledge about the structure of the model or by guessing a model and assessing its validity using observational data (e.g. by fitting the  data to the system of linear equations derived from the graph). However, this approach suffers from the same limitations as regression based ones.  \n\n\\subsection{Causality on Time-series}\n\nCausality studies on time-series have been largely based on Granger causality \\cite{granger1969investigating}. The Granger causality test examines if past values of one variable are useful in prediction of future values of another variable. In detail, a time-series $X$ Granger causes a time-series $Y$ if modeling $Y$ by regressing it on past values of both $Y$ and $X$ results in reduced residual noise compared to a simple autoregressive model. However, Granger causality tests cannot prove real causality since they do not satisfy the conditional ignorability assumption, i.e., the values of both treatment variable $X$ and control variable $Y$ may be driven by a third variable. Moreover, it considers only linear relationships. Granger causality has been extended to handle multivariate cases \\cite{barrett2010multivariate} as well as non-linear cases \\cite{zhang2012kernel, nawrath2010distinguishing}. However, as it was mentioned also at the introduction, inaccuracies on model specification may result in misleading conclusions. In \\cite{peters2013causal} authors propose an additional model check procedure after fitting a model in order to reduce the amount of false positive causality results. \n\nModel-free approaches for causal inference in time-series have also been proposed. Schreiber introduced \\textit{transfer entropy} \\cite{schreiber2000measuring} in order to examine whether the uncertainty about a time-series $Y$ is reduced by knowing past values of a time-series $X$. Transfer entropy is a model-free equivalent of bivariate Granger causality. This framework has been extended by Runge \\cite{pompe2011momentary} in order to handle also multivariate cases. In detail, Runge proposed the use of directed acyclic graphs for time-series data. The nodes of the graph correspond to the time-series of the study as well as their lagged versions. Consider $Q^l$, the lagged version of a time-series $Q$, and let $P$ denote the parents of a node $Y$.\nAn arrow from $Q^l$ to $Y$ will exist only if $Y \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q^l | \\mathbf{P} \\textbackslash\\{Q^l\\}$. The graph is built by performing conditional independence tests using conditional mutual information. However, performing conditional independence tests on continuous data is particularly challenging especially for high-dimensional data \\cite{zhang2012kernel}.\n\n\n\n\n\\section{Proposed Mechanism}\n\\label{sec:mechanism}\n\nGiven the previously discussed limitations on existing methodologies for causality discovery in time-series, we propose a novel framework that enables causal inference in time-series  data that is based on matching design and therefore does not require specification of a model class. The proposed framework also requires only few conditional independence tests, thus it can handle more effectively high-dimensional data. Denote by $Y$ and $X$ the time-series that represent the effect and the cause, respectively and by $\\mathbf{Z}$ a set of time-series representing other characteristics relevant for the study. Let us also denote by $Y^l$, $X^l$ and $\\mathbf{Z}^l$ the $l$-lagged versions of the time series $Y$, $X$ and $\\mathbf{Z}$, respectively (i.e., if $Y(u)$ the $u$-th sample of $Y$, $Y(u) = Y^l(u+l)$). We define a maximum lag value $L$ and a set of time-series $\\mathbf{S}$ =\\{$Y$, $Y^1$, ..., $Y^L$, $X$, $X^1$, ..., $X^L$, $\\mathbf{Z}$, $\\mathbf{Z}^1$, ..., $\\mathbf{Z}^L$\\}. In order to build a graph, we examine the dependencies between the variables $X$, $Y$ and all the other variables of the set $\\mathbf{S}$. According to Pearl's framework, a line from a variable $Q_1$ to a variable $Q_2$ is added only if $Q_2$ is dependent on $Q_1$, conditional on all direct predecessors of $Q_2$. Here, we relax this condition as follows:\n\nA line from a lagged node $Q_1^l$ (including lag 0) to a non-lagged node $Q_2$ exists if:\n\n\\begin{itemize}\n\\item $Q_1^l$ precedes temporally $Q_2$ i.e.$\\forall u$,  the $u^{th}$ sample of $Q_1^l$ precedes temporally the $u^{th}$ sample of $Q_2$.\nIn our case the time series are sampled on daily basis (index $u$), but for lag zero, within the same day $u$ the series $Q_1$ can be sampled before $Q_2$.\n\\item $Q_2 \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} Q_1^l | \\mathbf{P}\\cap (Q_1, Q_1^1, ..., Q_1^m)\\textbackslash\\{Q_1^l\\}$, where $\\mathbf{P}$ is a set of the direct predecessors of $Q_2$ and $m<l$.\n\\end{itemize} \n\nThus, in our framework, a direct edge between two nodes does not necessarily imply a causal relationship. By relaxing Pearl's rule, we reduce the number of conditional independence tests that are required. As discussed later, applying this relaxed rule for creating the graph does not impact the validity of our methodology. In this study we use vanishing correlation as a signature of independence. Mutual information could also be used in order to test independence. \n\n\\begin{algorithm}[htp]\n \\KwData{The set of time-series $\\mathbf{S}$}\n \\KwResult{ The set of confounding variables $\\mathbf{H}$ }\n\\SetKwFunction{algo}{algo}\\SetKwFunction{predecessors}{predecessors}\n$\\mathbf{P1}:=\\predecessors{$\\mathbf{S}$, $Y$}$\\;\n$\\mathbf{P2}:=\\predecessors{$\\mathbf{S}$, $X$}$\\;\n$\\mathbf{H}:= \\mathbf{P1}\\cap \\mathbf{P2}$\\;\n\n  \\SetKwFunction{myproc}{Procedure}{}{}\n\\tcc{this procedure returns a set $\\mathbf{P}$ of the direct predecessors of node $Q$. $\\mathbf{P}$ is a subset of $\\mathbf{S}$.}\n  \\myproc{\\predecessors{$\\mathbf{S}$, $Q$}}{\n$\\mathbf{P}:=\\{\\}$\\; \n\\For{i=0 to L}{\n  \\For{all $S^0 \\in \\mathbf{S}$}{\n\t $\\mathbf{B}:=(S^0, S^1, ..., S^L) \\cap \\mathbf{P}$\\;\n\t \\If{({$Q|\\mathbf{B} \\not{\\protect\\mathpalette{\\protect\\independenT}{\\perp}} S^i$} \\textbf{and} {$S^i(u)$ precedes $Q(u)$})}{\n        $\\mathbf{P}:=\\mathbf{P} \\cup S^i $\\;\n   }\t\n  }\n }\nreturn $\\mathbf{P}$\\;}\n \\caption{Defining the set of confounding variables.}\n\\label{Alg:predecessors}\n\\end{algorithm}\n\nThe units of the study correspond to daily stamps of the set of time-series $\\mathbf{S}$. Thus, a unit $u$ will be characterized by a set of values $\\mathbf{S}(u)$ =\\{$Y(u)$, $Y^1(u)$, ..., $Y^L(u)$, $X(u)$, $X^1(u)$, ..., $X^L(u)$, $\\mathbf{Z}(u)$, $\\mathbf{Z}^1(u)$, ..., $\\mathbf{Z}^L(u)$\\}. In what follows we will discuss how our methodology addresses the three general assumptions of causality studies discussed in Section \\ref{sec:background}.\n\n\\textit{Conditional Ignorability Assumption:} We apply the Algorithm \\ref{Alg:predecessors} in order to find the set of time-series $\\mathbf{H}$ that need to be controlled in order to satisfy the conditional ignorability assumption. According to our method, the resulted set contains all the direct predecessors that nodes $X$ and $Y$ have in common. Thus, all the variables that are correlated both with $X$ and $Y$ time-series are included; hence, the set $\\mathbf{H}$ is sufficient. However $\\mathbf{H}$ may include also redundant time-series i.e. some of the time-series included at $\\mathbf{H}$ may not correlate with $X$ or $Y$ conditional to a subset of $\\mathbf{H}$. In causality studies based on regression, including redundant predictors on the model could result in overfitting and would jeopardize the validity of the conclusions. Moreover, the application of methods based on conditional independence tests using information theoretic approaches would be challenged by the inclusion of redundant covariates since it would require conditioning on large sets of variables. In contrast, studies based on matching are less affected by the inclusion of redundant confounding variables (spurious correlations). Several methods that enable matching on a large number of confounding variables have been proposed \\cite{diamond2013genetic, sekhon2009opiates, rubin1997estimating}. In addition, scientists are able to apply balance diagnostic tests in order to assess if any confounding bias has been adequately eliminated; consequently, false conclusions due to confounding bias can be diminished. Following the matching design, the set of time-series $\\mathbf{H}$ is controlled by creating a set of pairs of units $G$ where each treated unit $u$ is matched with one or more untreated units $v$ such that $\\mathbf{H}(u)  \\approx \\mathbf{H}(v)$. \n\n\n\\textit{Stable Unit Treatment Value Assumption:} Denote by $\\mathbf{P}$ the set of time-series that are direct predecessors of the effect variable $Y$. Assuming $X \\in \\mathbf{P}$ (if not, $X$ is independent of $Y$ and therefore there is no causation), the  assumption is violated if $X^l \\in \\mathbf{P}$ and $X^l \\not\\in \\mathbf{H}$, for $ l > 0$. Since units correspond to daily samples, $X^l \\in \\mathbf{P}$ implies that the outcome $Y(u)$ of a unit $u$ depends on the treatment $X(u-l)$ received by a unit $u-l$. In order to satisfy the  assumption, we modify the $\\mathbf{H}$ set as follows:\n\n", "index": 3, "text": "\\begin{equation}\n\\mathbf{H} := ((X^1, ..., X^L) \\cap \\mathbf{P}) \\cup \\mathbf{H}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{H}:=((X^{1},...,X^{L})\\cap\\mathbf{P})\\cup\\mathbf{H},\" display=\"block\"><mrow><mrow><mi>\ud835\udc07</mi><mo>:=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msup><mi>X</mi><mi>L</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><mi>\ud835\udc0f</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u222a</mo><mi>\ud835\udc07</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nCausal inference will be performed by matching on the modified set of time-series $\\mathbf{H}$ thus, the variable $\\Delta Y := Y_1 - Y_0 | \\mathbf{H}$ will be i.i.d.. \n\n\\section{Evaluation}\n\\label{sec:evaluation}\n\\subsection{Causal Effect of Social Media on Stock Markets}\n\\label{sec:finance}\n\n\\subsubsection{Dataset Description}\nWe apply our method in order to investigate whether information about specific companies and people reactions influence stock market prices. We gather this information by analyzing relevant tweets. Twitter enables us to capture people opinions about the target companies, their optimism/pessimism about stock market movements and their reaction to news such as quarterly results announcements or new product launches. Our study considers the daily closing prices of four big tech companies based on USA: Apple, Microsoft, Amazon and Yahoo!. We estimate a daily sentiment index for each of these companies by analyzing the sentiment of related tweets. Our study is based on data gathered for four years, from January 2011 to December 2014. We examine whether the sentiment of tweets that are posted before stock market closing time influences the closing prices of the target stocks. In order to eliminate any confounding bias we need to control for factors that may affect both humans sentiment and the target stock prices. Potential influential factor on stocks daily closing prices are their opening prices and their performance during the previous days. Several works have also demonstrated that the performance of other big companies (either local or overseas companies) could influence some stocks (see for example \\cite{kenett2010dominating,chi2010network}). Foreign currency exchange rates may also cause money flows to overseas markets and consequently influence stocks prices. Finally, commodities prices could affect the earnings of companies and, therefore, their stocks prices. More specifically, our study involves the following time-series:\n\n \\textbf{The \\textit{response} time-series $\\mathbf{Y}$.} The difference on the closing prices of the target stocks between two consecutive days. The $u^{th}$ sample of the time-series corresponds to the closing value of the $u^{th}$ day minus the closing value of the previous day. \n\n \\textbf{The \\textit{treatment} time-series $\\mathbf{X}$.}  A daily sentiment index that is estimated using tweets related to the target stocks that are posted up to 24 hours before the closing time of the corresponding stock market. In order to assure that the values of the treatment variable are driven by information that has been available before the closing time of the target stocks, we omit from the study tweets posted up to one hour before the closing time. Tweets are filtered using the name of the company and the stock symbol as keywords.\n\n \\textbf{The set of time-series $\\mathbf{Z}$.} \n We consider the following time-series which might influence our case study:\n\\begin{enumerate}\n\\item \\textbf{The difference between the opening and closing prices of two consecutive days.} This time-series is an indicator of the activity of the target stocks at the start of the trading day. \n\n\\item \\textbf{The stock market prices of several major companies around the world.} In our study we include all the components of the most important stock market indexes such as NASDAQ-100, Dow-30, Nikkei 225, DAX and FTSE. The study could be influenced only by factors that precede temporally both the treatment and effect variables. Thus, we use the difference between the opening and closing prices of two consecutive days for stocks that are traded in the USA exchange markets. The closing time of companies traded at the overseas markets precedes the closing time of the USA stock exchange market, thus the time-series for all the overseas companies stocks correspond to the difference on the closing prices between two consecutive days. Although the values of the treatment variable are driven by tweets that are posted both before and after the corresponding values of the time-series that we use to describe the performance of big companies, for convenience, we consider that the $u^{th}$ sample of the treatment time-series occurs one hour before the USA stock exchange market closing time at day $u$.  Thus, the $u^{th}$ sample of any of the time-series that are used to describe the performance of either a USA-based company or an overseas company temporally precedes the $u^{th}$ sample of the treatment time-series.\n\n\\item \\textbf{The daily opening values of foreign currency exchange rates minus the previous day opening values.} We include the exchange rates between dollar and British pound, Euro, Australian dollar, Japanese Yen, Swiss Franc and Chinese Yen. \n\n\\item \\textbf{The difference between the opening values of commodities for consecutive days.} We include the following commodities: gold, silver, copper, gas and oil. \n\\end{enumerate}\n\n\n\\subsubsection{Daily Sentiment Index Estimation}\n\nWe classify each tweet as negative, neutral or positive using the SentiStrength classifier~\\cite{thelwall2013heart}. SentiStrength estimates the sentiment of a sentence using a list of terms where each term is assigned a weight indicating its positivity or negativity. We updated the list of terms in order to include terms that are commonly used in finance. \n\nIn order to account for the classification error, we estimate a probability distribution function of the daily sentiment instead of a single metric. Let us define a set of three objects $S = \\{positive, neutral, negative\\}$. Each object $i \\in S$ denotes a classification category. Let us also define a random variable $V_i$ as follows:\n\n\n", "itemtype": "equation", "pos": 28241, "prevtext": "\nsatisfying $Y(u) {\\protect\\mathpalette{\\protect\\independenT}{\\perp}} \\mathbf{X}(v) | \\mathbf{H}(u)$, $\\forall u \\neq v$.\n\n\n\\textit{i.i.d. Assumption:}  Denote by $Y_1$  the value of the outcome variable for units that have received a treatment and with $Y_0$ for untreated units (assuming $X$ is a binary treatment). The average causal effect is estimated as $\\widehat{E}\\{Y_1 - Y_0 | \\mathbf{H}\\}$. In order to enable statistical inference, the variable $\\Delta Y := Y_1 - Y_0 | \\mathbf{H}$ needs to be i.i.d.. If $\\mathbf{P}$ the set of direct predecessors of $Y$, the outcome value $Y(u)$ of each unit $u$ will depend on the outcome value $Y(u-l)$ of unit $u-l$ if there is a time-series $Y^l \\in \\mathbf{P}$. In case that $Y^l \\notin \\mathbf{H}$, the i.i.d. assumption would be violated. In order to satisfy this assumption, we modify the set of time-series $\\mathbf{H}$ as follows:\n\n\n", "index": 5, "text": "\\begin{equation}\n\\mathbf{H} := ((Y^1, ..., Y^L) \\cap \\mathbf{P}) \\cup \\mathbf{H}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{H}:=((Y^{1},...,Y^{L})\\cap\\mathbf{P})\\cup\\mathbf{H}\" display=\"block\"><mrow><mi>\ud835\udc07</mi><mo>:=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mi>Y</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msup><mi>Y</mi><mi>L</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><mi>\ud835\udc0f</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u222a</mo><mi>\ud835\udc07</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nWe derive the probability distribution functions of each random variable $V_i$, with $i \\in S$, based on the classification performance results. We evaluate the performance of the classifier by manually classifying 1200 randomly selected tweets (200 tweets for each one of the four examined companies). The probability distribution functions are presented in Table \\ref{tab:accuracy}.\n\n\\begin{table}[t]\n\\centering\n\\caption{Accuracy of the text classification for each classification category.}\n\\begin{tabular}{ |c|c|c|c| }\n  \\hline \n  {}&{$P(V_i = 0)$}&{$P(V_i = 1)$}&{$P(V_i = 2)$}\\\\\n  \\hline \n  {i = positive}&{0.05}&{0.27}&{0.68}\\\\\n  \\hline \n  {i = neutral}&{0.03}&{0.91}&{0.06}\\\\\n  \\hline \n  {i = negative}&{0.65}&{0.29}&{0.06}\\\\\n  \\hline \n\\end{tabular}\n\\label{tab:accuracy}\n\\end{table}\n\nLet us define with $N_i$ the number of tweets posted within a day that are classified in category $i$. We define a random variable $\\mathcal{V}_u$ which corresponds to the sentiment of a day $u$ as follows:\n\n\n", "itemtype": "equation", "pos": 34017, "prevtext": "\n\nCausal inference will be performed by matching on the modified set of time-series $\\mathbf{H}$ thus, the variable $\\Delta Y := Y_1 - Y_0 | \\mathbf{H}$ will be i.i.d.. \n\n\\section{Evaluation}\n\\label{sec:evaluation}\n\\subsection{Causal Effect of Social Media on Stock Markets}\n\\label{sec:finance}\n\n\\subsubsection{Dataset Description}\nWe apply our method in order to investigate whether information about specific companies and people reactions influence stock market prices. We gather this information by analyzing relevant tweets. Twitter enables us to capture people opinions about the target companies, their optimism/pessimism about stock market movements and their reaction to news such as quarterly results announcements or new product launches. Our study considers the daily closing prices of four big tech companies based on USA: Apple, Microsoft, Amazon and Yahoo!. We estimate a daily sentiment index for each of these companies by analyzing the sentiment of related tweets. Our study is based on data gathered for four years, from January 2011 to December 2014. We examine whether the sentiment of tweets that are posted before stock market closing time influences the closing prices of the target stocks. In order to eliminate any confounding bias we need to control for factors that may affect both humans sentiment and the target stock prices. Potential influential factor on stocks daily closing prices are their opening prices and their performance during the previous days. Several works have also demonstrated that the performance of other big companies (either local or overseas companies) could influence some stocks (see for example \\cite{kenett2010dominating,chi2010network}). Foreign currency exchange rates may also cause money flows to overseas markets and consequently influence stocks prices. Finally, commodities prices could affect the earnings of companies and, therefore, their stocks prices. More specifically, our study involves the following time-series:\n\n \\textbf{The \\textit{response} time-series $\\mathbf{Y}$.} The difference on the closing prices of the target stocks between two consecutive days. The $u^{th}$ sample of the time-series corresponds to the closing value of the $u^{th}$ day minus the closing value of the previous day. \n\n \\textbf{The \\textit{treatment} time-series $\\mathbf{X}$.}  A daily sentiment index that is estimated using tweets related to the target stocks that are posted up to 24 hours before the closing time of the corresponding stock market. In order to assure that the values of the treatment variable are driven by information that has been available before the closing time of the target stocks, we omit from the study tweets posted up to one hour before the closing time. Tweets are filtered using the name of the company and the stock symbol as keywords.\n\n \\textbf{The set of time-series $\\mathbf{Z}$.} \n We consider the following time-series which might influence our case study:\n\\begin{enumerate}\n\\item \\textbf{The difference between the opening and closing prices of two consecutive days.} This time-series is an indicator of the activity of the target stocks at the start of the trading day. \n\n\\item \\textbf{The stock market prices of several major companies around the world.} In our study we include all the components of the most important stock market indexes such as NASDAQ-100, Dow-30, Nikkei 225, DAX and FTSE. The study could be influenced only by factors that precede temporally both the treatment and effect variables. Thus, we use the difference between the opening and closing prices of two consecutive days for stocks that are traded in the USA exchange markets. The closing time of companies traded at the overseas markets precedes the closing time of the USA stock exchange market, thus the time-series for all the overseas companies stocks correspond to the difference on the closing prices between two consecutive days. Although the values of the treatment variable are driven by tweets that are posted both before and after the corresponding values of the time-series that we use to describe the performance of big companies, for convenience, we consider that the $u^{th}$ sample of the treatment time-series occurs one hour before the USA stock exchange market closing time at day $u$.  Thus, the $u^{th}$ sample of any of the time-series that are used to describe the performance of either a USA-based company or an overseas company temporally precedes the $u^{th}$ sample of the treatment time-series.\n\n\\item \\textbf{The daily opening values of foreign currency exchange rates minus the previous day opening values.} We include the exchange rates between dollar and British pound, Euro, Australian dollar, Japanese Yen, Swiss Franc and Chinese Yen. \n\n\\item \\textbf{The difference between the opening values of commodities for consecutive days.} We include the following commodities: gold, silver, copper, gas and oil. \n\\end{enumerate}\n\n\n\\subsubsection{Daily Sentiment Index Estimation}\n\nWe classify each tweet as negative, neutral or positive using the SentiStrength classifier~\\cite{thelwall2013heart}. SentiStrength estimates the sentiment of a sentence using a list of terms where each term is assigned a weight indicating its positivity or negativity. We updated the list of terms in order to include terms that are commonly used in finance. \n\nIn order to account for the classification error, we estimate a probability distribution function of the daily sentiment instead of a single metric. Let us define a set of three objects $S = \\{positive, neutral, negative\\}$. Each object $i \\in S$ denotes a classification category. Let us also define a random variable $V_i$ as follows:\n\n\n", "index": 7, "text": "\\begin{equation}\nV_i = \n\\left\\{\n  \\begin{array}{rcr}\n    0 & \\mbox{if a negative tweet is classified in class i}\\\\\n    1 & \\mbox{if a neutral tweet is classified in class i}\\\\\n    2 & \\mbox{if a positive tweet is classified in class i}\\\\\n\\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"V_{i}=\\left\\{\\begin{array}[]{rcr}0&amp;\\mbox{if a negative tweet is classified in %&#10;class i}\\\\&#10;1&amp;\\mbox{if a neutral tweet is classified in class i}\\\\&#10;2&amp;\\mbox{if a positive tweet is classified in class i}\\\\&#10;\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>V</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mtext>if a negative tweet is classified in class i</mtext></mtd><mtd/></mtr><mtr><mtd columnalign=\"right\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mtext>if a neutral tweet is classified in class i</mtext></mtd><mtd/></mtr><mtr><mtd columnalign=\"right\"><mn>2</mn></mtd><mtd columnalign=\"center\"><mtext>if a positive tweet is classified in class i</mtext></mtd><mtd/></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nSince $2$ is the maximum value of $V_i$, $\\mathcal{V}_u \\in \\{0, 1, ..., 2 \\cdot \\sum_i N_i \\}$. We estimate the probability distribution of $\\mathcal{V}_u$ by deriving the probability-generating function under the assumption that the real sentiment of a tweet is independent to the sentiment of any other tweet given the observed classification of the tweet. The probability-generating function of $\\mathcal{V}_u$ is expressed as follows:\n\n\n", "itemtype": "equation", "pos": 35291, "prevtext": "\n\nWe derive the probability distribution functions of each random variable $V_i$, with $i \\in S$, based on the classification performance results. We evaluate the performance of the classifier by manually classifying 1200 randomly selected tweets (200 tweets for each one of the four examined companies). The probability distribution functions are presented in Table \\ref{tab:accuracy}.\n\n\\begin{table}[t]\n\\centering\n\\caption{Accuracy of the text classification for each classification category.}\n\\begin{tabular}{ |c|c|c|c| }\n  \\hline \n  {}&{$P(V_i = 0)$}&{$P(V_i = 1)$}&{$P(V_i = 2)$}\\\\\n  \\hline \n  {i = positive}&{0.05}&{0.27}&{0.68}\\\\\n  \\hline \n  {i = neutral}&{0.03}&{0.91}&{0.06}\\\\\n  \\hline \n  {i = negative}&{0.65}&{0.29}&{0.06}\\\\\n  \\hline \n\\end{tabular}\n\\label{tab:accuracy}\n\\end{table}\n\nLet us define with $N_i$ the number of tweets posted within a day that are classified in category $i$. We define a random variable $\\mathcal{V}_u$ which corresponds to the sentiment of a day $u$ as follows:\n\n\n", "index": 9, "text": "\\begin{equation}\n\\mathcal{V}_u = \\sum_{i\\in S} N_i \\cdot V_i \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{V}_{u}=\\sum_{i\\in S}N_{i}\\cdot V_{i}\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb1</mi><mi>u</mi></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><msub><mi>N</mi><mi>i</mi></msub><mo>\u22c5</mo><msub><mi>V</mi><mi>i</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nThe probability distribution function of $\\mathcal{V}_u$ is estimated by taking the derivatives of $G_{\\mathcal{V}_u}$. If $\\mathcal{N}_u$ the number of tweets posted a day $u$, then, $\\mathcal{V}_u \\in \\{0, 1, ..., \\mathcal{N}_u \\cdot M\\}$ and the probability that the general sentiment of a day $u$ is positive is given by the probability $P_{pos}(u) = P(\\mathcal{V}_u>\\frac{\\mathcal{N}_u \\cdot M}{2})$. \n\n\\subsubsection{Results}\n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.47\\textwidth]{probabilityPosGivenSentimentAll20.eps}\n\n  \n\\caption{Probability distribution function of having a positive movement on the traded assets prices conditional to the sentiment of the tweets.}\n\\label{fig:probPosMovement}\n\\end{figure}\n\n\n\\begin{figure*}\n  \\centering\n                \\includegraphics[width=0.75\\textwidth]{merged.eps}\n\\captionsetup{justification=centering}\n\\caption{Correlation between the confounding variables and the treatment and effect time-series.}\n\\label{fig:correlations}\n\\end{figure*}\n\nWe define the treatment time-series $X(u)$ equal to the probability $P_{pos}(u)$. We include to our study all the previously mentioned variables. The maximum lag $L$ is set to 1. For each of the four target stocks, we applied Algorithm \\ref{Alg:predecessors} in order to find the set of time-series $\\mathbf{H}$ that needs to be controlled. We consider a correlation to be statistically significant if the corresponding p-value is smaller than 0.05. We used Spearman's rank correlation in order to capture potentially non-linear relationships among the examined variables. We found that stock movements are significantly correlated with the sentiment of tweets posted within the same day. Our findings come in agreement with results of other studies \\cite{bollen2011twitter, preis2010complex, zhang2011predicting}. We also found that stock prices are independent to past tweets sentiment conditional to more recent tweets. This indicates that any effect of tweets on stock prices is instant rather than long-term. Finally, according to our results, the daily movement of the traded assets for the target companies does not correlate with past days movements. This finding is consistent with the weak-form efficient market hypothesis according to which, it is not feasible to predict stock market movements by applying technical analysis. In Table~\\ref{tab:correlation} we present the correlation coefficient of the effect variable $Y$ with the treatment variable $X$ and the 1-lagged variables $X^1$ and $Y^1$ for each one of the four examined companies. In Figure \\ref{fig:probPosMovement} we present the empirical probability distribution function of having a positive movement on the traded assets prices conditional to the sentiment of the tweets $P(Y>0 | X)$. The probability distribution function is estimated using data collectively for the four examined companies. Our results indicate that the probability of having a positive movement on the stock market does not increase linearly with the daily tweets positivity index. Stock market movement is quite uncertain when the positivity index of the tweets ranges between 0.35 and 0.65, while the probability of having a positive movement is increasing for positivity index larger than 0.65. Moreover, we notice a relatively high probability of having a positive movement in days with sentiment positivity index lower than 0.1. Considering that daily tweets sentiment capture the current and past stock market trends, this could be attributed to the fact  that investors may consider that it is a good time to invest money when assets prices are low; consequently, this could give lead to an increase of stock market prices.\n\n\\begin{table}[th]\n\\centering\n\\caption{Correlation of $Y$ with $X$, $X^1$ and $Y^1$.}\n\\begin{tabular}{ |c|c|c|c|c|}\n  \\hline \n  {}&{AAPL}&{MSFT}&{AMZN}&{YHOO}\\\\\n  \\hline \n  {$X$}&{0.393}&{0.155}&{0.237}&{0.273}\\\\\n  \\hline\n {$X^{1}$}&{ 0.032}&{0.036}&{0.012}&{0.046}\\\\\n\\hline\n {$Y^{1}$}&{0.009}&{ -0.003}&{-0.037}&{0.031}\\\\\n  \\hline  \n\\end{tabular}\n\\label{tab:correlation}\n\\end{table}\n\n\nMoreover, we find that both the effect and the treatment variables correlate with the most recent stock prices of several local and overseas companies. The daily movements of the target stocks correlate with US dollar exchange rates; however, currency exchange rates do not have any impact on the treatment variable. In Table \\ref{tab:number-of-confounders} we present the number of variables from each category that will be included in the set $\\mathbf{H}$ for the four target companies and in Figure \\ref{fig:correlations}, we present the correlation coefficients of the treatment and effect time-series with all variables in set $\\mathbf{H}$. For all the examined stocks, the strongest confounder is their opening prices. \n\n\n\\begin{table}[th]\n\\centering\n\\caption{Number of variables that are included in the set $\\mathbf{H}$ for each of the four examined companies.}\n\\begin{tabular}{ |c|c|c|c|c|}\n  \\hline \n  {}&{AAPL}&{MSFT}&{AMZN}&{YHOO}\\\\\n  \\hline \n  {Nasdaq-100 Comp.}&{6}&{21}&{33}&{7}\\\\\n  \\hline\n {Nikkei Comp.}&{1}&{3}&{1}&{13}\\\\\n  \\hline \n{DAX Comp.}&{18}&{2}&{7}&{10}\\\\\n  \\hline \n{FTSE Comp.}&{10}&{3}&{12}&{26}\\\\\n  \\hline \n{Dow-30 Comp.}&{7}&{3}&{9}&{2}\\\\\n  \\hline \n{FOREX}&{0}&{0}&{0}&{0}\\\\\n  \\hline  \n{Commodities}&{0}&{0}&{0}&{0}\\\\\n  \\hline  \n\n\\end{tabular}\n\\label{tab:number-of-confounders}\n\\end{table}\n\nWe create a binary treatment variable $\\tilde{X}$ by applying thresholds on $X$. More specifically, a unit $u$, which describes the $u^{th}$ day of the study is considered to be treated (i.e. $\\tilde{X}=1$) if $X(u) \\geq P_{thresh}^1$ and untreated (i.e. $\\tilde{X}=0$) if $X(u) < P_{thresh}^0$. We conduct our study for three different pairs of thresholds. In detail, we consider a pair of thresholds $T1$, where thresholds $P_{thresh}^1$ and $P_{thresh}^0$ are set to the $50^{th}$ percentile of $X$, a pair of thresholds $T2$ where $P_{thresh}^1$ is set to the $60^{th}$ percentile of $X$ and $P_{thresh}^0$ to the $40^{th}$ percentile of $X$ and finally a pair $T3$ where $P_{thresh}^1$ and $P_{thresh}^0$ are set to the $70^{th}$ and $30^{th}$ percentiles respectively. By increasing the value of $P_{thresh}^1$ and decreasing the value of $P_{thresh}^0$ we eliminate from our study days in which the estimated tweets polarity is uncertain either due to measurement error or because the overall sentiment that is expressed during these days is considered to be neutral. Although discretization of a continuous variable results in information loss which may jeopardize, in some cases, the reliability of the causal inference, we enhance the validity of our conclusions by considering different threshold values. \n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.47\\textwidth]{causalityBar.eps}\n\\caption{Normalized ATE for the three threshold pairs.}\n\\label{fig:ATE}\n\\end{figure}\nIn order to eliminate the effect of the confounding variables we need to match treated and control units with similar values on their set of confounding variables. We create optimal pairs of treated and untreated units by applying Genetic Matching algorithm \\cite{diamond2013genetic}. Genetic matching is a multivariate matching method which applies an evolutionary search algorithm in order to find optimal matches which minimize a loss function. We use as a loss function the average standardized mean difference between the treated and control units for all the confounding variables $H_i \\in \\mathbf{H}$ which is defined as follows:\n\n\n", "itemtype": "equation", "pos": 35810, "prevtext": "\n\nSince $2$ is the maximum value of $V_i$, $\\mathcal{V}_u \\in \\{0, 1, ..., 2 \\cdot \\sum_i N_i \\}$. We estimate the probability distribution of $\\mathcal{V}_u$ by deriving the probability-generating function under the assumption that the real sentiment of a tweet is independent to the sentiment of any other tweet given the observed classification of the tweet. The probability-generating function of $\\mathcal{V}_u$ is expressed as follows:\n\n\n", "index": 11, "text": "\\begin{equation}\nG_{\\mathcal{V}_u} = \\prod_{i\\in S} (G_{V_i}(z))^{N_i} = \\prod_{i\\in S} (\\sum_{x=0}^{2} p(V_i = x)\\cdot z^x)^{N_i}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"G_{\\mathcal{V}_{u}}=\\prod_{i\\in S}(G_{V_{i}}(z))^{N_{i}}=\\prod_{i\\in S}(\\sum_{%&#10;x=0}^{2}p(V_{i}=x)\\cdot z^{x})^{N_{i}}\" display=\"block\"><mrow><msub><mi>G</mi><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb1</mi><mi>u</mi></msub></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>G</mi><msub><mi>V</mi><mi>i</mi></msub></msub><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mi>N</mi><mi>i</mi></msub></msup><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><msup><mrow><mo stretchy=\"false\">(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>V</mi><mi>i</mi></msub><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><msup><mi>z</mi><mi>x</mi></msup><mo stretchy=\"false\">)</mo></mrow><msub><mi>N</mi><mi>i</mi></msub></msup></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nWe check if sufficient balance between treated and untreated subjects has been achieved by checking the standardized mean difference for each confounding variable. The remaining bias from a confounding variable is considered to be insignificant if the standardized mean difference is smaller than $0.1$ \\cite{austin2009relative, austin2008goodness}. \n\n\\begin{figure*}[t!]\n\\centering\n       \\subfloat[Amazon.]\n                {\\includegraphics[width=0.4\\textwidth]{QQamazon-eps-converted-to.pdf}\n                \\label{fig:amazon}}\n~\n        \\subfloat[Apple.]\n                {\\includegraphics[width=0.4\\textwidth]{QQapple-eps-converted-to.pdf}\n                \\label{fig:apple}}\n\\\\\n        \\subfloat[Microsoft.]\n                {\\includegraphics[width=0.4\\textwidth]{QQmsft-eps-converted-to.pdf}\n                \\label{fig:msft}}\n~\n        \\subfloat[Yahoo!.]\n                {\\includegraphics[width=0.4\\textwidth]{QQyahoo.eps}\n                \\label{fig:yahoo}}\n\\caption{Percentiles of treated units versus percentiles of matched control units.}\n\\label{fig:qqplot}\n\\end{figure*}\n\n\nWe examine the causal effect of the sentiment of tweets on the target stocks for the three pairs of thresholds. We apply Equation \\ref{eq:ATE} in order to estimate the average treatment effect (ATE). Under the assumption that the examined treatment has no impact on the effect variable, the ATE would be equal to 0. We use a t-test to assess how significant is the difference of the observed ATE value from 0. In Figure \\ref{fig:ATE}, we present the average treatment effect normalized by the variance of the effect variable $Y$ along with the 95\\% confidence interval values. According to our results, the effect of the tweets sentiment on the stocks prices of all the examined stocks is statistically significant. We also observe that the causal impact is stronger for larger values of the $P_{thresh}^1$ and smaller $P_{thresh}^0$ threshold values, i.e. the observed difference on the effect variable between the treatment and control groups is larger when we consider only days for which there is less uncertainty on the estimated tweets polarity. For Apple, it was not possible to create balanced treated and control groups for the thresholds pair $T3$. This is due to the fact that the opening prices of the AAPL stocks are very strongly correlated with both the effect and treatment variables and, therefore, there were not enough treatment and control units with similar values on their confounding variables. Since any causal conclusions are not reliable when the treated and control groups are not balanced, we do not present results for Apple for this pair of threshold values. Finally, in Figure~\\ref{fig:qqplot} we compare the distributions of the effect variable $Y$ for the treated and control units by plotting their percentiles against each other. Under the hypothesis that the treatment variable has no effect on variable $Y$, the plot should follow approximately the line $y=x$. However, most of the points of the plot lie below the reference line $y=x$, indicating that the majority of the percentiles of variable $Y$ for the treated units are larger than the corresponding percentiles for the control units. \n\n\n\\subsection{Evaluation with Synthetic Data}\n\\label{sec:synthetic}\nIn order to demonstrate the potential of our approach we assess its effectiveness in detecting causal relationships on linear and non-linear synthetic data. We also compare our approach with a multivariate Granger causality model and with an information theoretic approach based on Runge's framework \\cite{pompe2011momentary} and we demonstrate that our method is more efficient on avoiding false causal conclusions. We denote with $X(u)$ and $Y(u)$ the treatment and outcome time-series respectively and with $\\mathbf{Z}$ a set of $M$ confounding variables. The relationships among them are described by the following model:\n\n\n", "itemtype": "equation", "pos": 43476, "prevtext": "\n\nThe probability distribution function of $\\mathcal{V}_u$ is estimated by taking the derivatives of $G_{\\mathcal{V}_u}$. If $\\mathcal{N}_u$ the number of tweets posted a day $u$, then, $\\mathcal{V}_u \\in \\{0, 1, ..., \\mathcal{N}_u \\cdot M\\}$ and the probability that the general sentiment of a day $u$ is positive is given by the probability $P_{pos}(u) = P(\\mathcal{V}_u>\\frac{\\mathcal{N}_u \\cdot M}{2})$. \n\n\\subsubsection{Results}\n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.47\\textwidth]{probabilityPosGivenSentimentAll20.eps}\n\n  \n\\caption{Probability distribution function of having a positive movement on the traded assets prices conditional to the sentiment of the tweets.}\n\\label{fig:probPosMovement}\n\\end{figure}\n\n\n\\begin{figure*}\n  \\centering\n                \\includegraphics[width=0.75\\textwidth]{merged.eps}\n\\captionsetup{justification=centering}\n\\caption{Correlation between the confounding variables and the treatment and effect time-series.}\n\\label{fig:correlations}\n\\end{figure*}\n\nWe define the treatment time-series $X(u)$ equal to the probability $P_{pos}(u)$. We include to our study all the previously mentioned variables. The maximum lag $L$ is set to 1. For each of the four target stocks, we applied Algorithm \\ref{Alg:predecessors} in order to find the set of time-series $\\mathbf{H}$ that needs to be controlled. We consider a correlation to be statistically significant if the corresponding p-value is smaller than 0.05. We used Spearman's rank correlation in order to capture potentially non-linear relationships among the examined variables. We found that stock movements are significantly correlated with the sentiment of tweets posted within the same day. Our findings come in agreement with results of other studies \\cite{bollen2011twitter, preis2010complex, zhang2011predicting}. We also found that stock prices are independent to past tweets sentiment conditional to more recent tweets. This indicates that any effect of tweets on stock prices is instant rather than long-term. Finally, according to our results, the daily movement of the traded assets for the target companies does not correlate with past days movements. This finding is consistent with the weak-form efficient market hypothesis according to which, it is not feasible to predict stock market movements by applying technical analysis. In Table~\\ref{tab:correlation} we present the correlation coefficient of the effect variable $Y$ with the treatment variable $X$ and the 1-lagged variables $X^1$ and $Y^1$ for each one of the four examined companies. In Figure \\ref{fig:probPosMovement} we present the empirical probability distribution function of having a positive movement on the traded assets prices conditional to the sentiment of the tweets $P(Y>0 | X)$. The probability distribution function is estimated using data collectively for the four examined companies. Our results indicate that the probability of having a positive movement on the stock market does not increase linearly with the daily tweets positivity index. Stock market movement is quite uncertain when the positivity index of the tweets ranges between 0.35 and 0.65, while the probability of having a positive movement is increasing for positivity index larger than 0.65. Moreover, we notice a relatively high probability of having a positive movement in days with sentiment positivity index lower than 0.1. Considering that daily tweets sentiment capture the current and past stock market trends, this could be attributed to the fact  that investors may consider that it is a good time to invest money when assets prices are low; consequently, this could give lead to an increase of stock market prices.\n\n\\begin{table}[th]\n\\centering\n\\caption{Correlation of $Y$ with $X$, $X^1$ and $Y^1$.}\n\\begin{tabular}{ |c|c|c|c|c|}\n  \\hline \n  {}&{AAPL}&{MSFT}&{AMZN}&{YHOO}\\\\\n  \\hline \n  {$X$}&{0.393}&{0.155}&{0.237}&{0.273}\\\\\n  \\hline\n {$X^{1}$}&{ 0.032}&{0.036}&{0.012}&{0.046}\\\\\n\\hline\n {$Y^{1}$}&{0.009}&{ -0.003}&{-0.037}&{0.031}\\\\\n  \\hline  \n\\end{tabular}\n\\label{tab:correlation}\n\\end{table}\n\n\nMoreover, we find that both the effect and the treatment variables correlate with the most recent stock prices of several local and overseas companies. The daily movements of the target stocks correlate with US dollar exchange rates; however, currency exchange rates do not have any impact on the treatment variable. In Table \\ref{tab:number-of-confounders} we present the number of variables from each category that will be included in the set $\\mathbf{H}$ for the four target companies and in Figure \\ref{fig:correlations}, we present the correlation coefficients of the treatment and effect time-series with all variables in set $\\mathbf{H}$. For all the examined stocks, the strongest confounder is their opening prices. \n\n\n\\begin{table}[th]\n\\centering\n\\caption{Number of variables that are included in the set $\\mathbf{H}$ for each of the four examined companies.}\n\\begin{tabular}{ |c|c|c|c|c|}\n  \\hline \n  {}&{AAPL}&{MSFT}&{AMZN}&{YHOO}\\\\\n  \\hline \n  {Nasdaq-100 Comp.}&{6}&{21}&{33}&{7}\\\\\n  \\hline\n {Nikkei Comp.}&{1}&{3}&{1}&{13}\\\\\n  \\hline \n{DAX Comp.}&{18}&{2}&{7}&{10}\\\\\n  \\hline \n{FTSE Comp.}&{10}&{3}&{12}&{26}\\\\\n  \\hline \n{Dow-30 Comp.}&{7}&{3}&{9}&{2}\\\\\n  \\hline \n{FOREX}&{0}&{0}&{0}&{0}\\\\\n  \\hline  \n{Commodities}&{0}&{0}&{0}&{0}\\\\\n  \\hline  \n\n\\end{tabular}\n\\label{tab:number-of-confounders}\n\\end{table}\n\nWe create a binary treatment variable $\\tilde{X}$ by applying thresholds on $X$. More specifically, a unit $u$, which describes the $u^{th}$ day of the study is considered to be treated (i.e. $\\tilde{X}=1$) if $X(u) \\geq P_{thresh}^1$ and untreated (i.e. $\\tilde{X}=0$) if $X(u) < P_{thresh}^0$. We conduct our study for three different pairs of thresholds. In detail, we consider a pair of thresholds $T1$, where thresholds $P_{thresh}^1$ and $P_{thresh}^0$ are set to the $50^{th}$ percentile of $X$, a pair of thresholds $T2$ where $P_{thresh}^1$ is set to the $60^{th}$ percentile of $X$ and $P_{thresh}^0$ to the $40^{th}$ percentile of $X$ and finally a pair $T3$ where $P_{thresh}^1$ and $P_{thresh}^0$ are set to the $70^{th}$ and $30^{th}$ percentiles respectively. By increasing the value of $P_{thresh}^1$ and decreasing the value of $P_{thresh}^0$ we eliminate from our study days in which the estimated tweets polarity is uncertain either due to measurement error or because the overall sentiment that is expressed during these days is considered to be neutral. Although discretization of a continuous variable results in information loss which may jeopardize, in some cases, the reliability of the causal inference, we enhance the validity of our conclusions by considering different threshold values. \n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.47\\textwidth]{causalityBar.eps}\n\\caption{Normalized ATE for the three threshold pairs.}\n\\label{fig:ATE}\n\\end{figure}\nIn order to eliminate the effect of the confounding variables we need to match treated and control units with similar values on their set of confounding variables. We create optimal pairs of treated and untreated units by applying Genetic Matching algorithm \\cite{diamond2013genetic}. Genetic matching is a multivariate matching method which applies an evolutionary search algorithm in order to find optimal matches which minimize a loss function. We use as a loss function the average standardized mean difference between the treated and control units for all the confounding variables $H_i \\in \\mathbf{H}$ which is defined as follows:\n\n\n", "index": 13, "text": "\\begin{equation}\nSMD_{H} = \\sum_{H_i \\in \\mathbf{H}} \\frac{\\sum_{(u, v) \\in G} |H_i(u) - H_i(v)|}{|G| \\cdot  \\sigma_{H_i}}/|\\mathbf{H}|\n\\label{eq:std-mean}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"SMD_{H}=\\sum_{H_{i}\\in\\mathbf{H}}\\frac{\\sum_{(u,v)\\in G}|H_{i}(u)-H_{i}(v)|}{|%&#10;G|\\cdot\\sigma_{H_{i}}}/|\\mathbf{H}|\" display=\"block\"><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><msub><mi>D</mi><mi>H</mi></msub></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>\u2208</mo><mi>\ud835\udc07</mi></mrow></munder><mrow><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>G</mi></mrow></msub><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mi>G</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u22c5</mo><msub><mi>\u03c3</mi><msub><mi>H</mi><mi>i</mi></msub></msub></mrow></mfrac><mo>/</mo><mrow><mo stretchy=\"false\">|</mo><mi>\ud835\udc07</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\\begin{eqnarray}\nY(u) &=& h_{yy}(Y(u-1)) + f_{yz}(\\mathbf{Z}(u)) \n \\nonumber \\\\\n&& + f_{yx}(X(u))  + \\epsilon_y(u)\n\\end{eqnarray}\n\n", "itemtype": "equation", "pos": 47554, "prevtext": "\n\nWe check if sufficient balance between treated and untreated subjects has been achieved by checking the standardized mean difference for each confounding variable. The remaining bias from a confounding variable is considered to be insignificant if the standardized mean difference is smaller than $0.1$ \\cite{austin2009relative, austin2008goodness}. \n\n\\begin{figure*}[t!]\n\\centering\n       \\subfloat[Amazon.]\n                {\\includegraphics[width=0.4\\textwidth]{QQamazon-eps-converted-to.pdf}\n                \\label{fig:amazon}}\n~\n        \\subfloat[Apple.]\n                {\\includegraphics[width=0.4\\textwidth]{QQapple-eps-converted-to.pdf}\n                \\label{fig:apple}}\n\\\\\n        \\subfloat[Microsoft.]\n                {\\includegraphics[width=0.4\\textwidth]{QQmsft-eps-converted-to.pdf}\n                \\label{fig:msft}}\n~\n        \\subfloat[Yahoo!.]\n                {\\includegraphics[width=0.4\\textwidth]{QQyahoo.eps}\n                \\label{fig:yahoo}}\n\\caption{Percentiles of treated units versus percentiles of matched control units.}\n\\label{fig:qqplot}\n\\end{figure*}\n\n\nWe examine the causal effect of the sentiment of tweets on the target stocks for the three pairs of thresholds. We apply Equation \\ref{eq:ATE} in order to estimate the average treatment effect (ATE). Under the assumption that the examined treatment has no impact on the effect variable, the ATE would be equal to 0. We use a t-test to assess how significant is the difference of the observed ATE value from 0. In Figure \\ref{fig:ATE}, we present the average treatment effect normalized by the variance of the effect variable $Y$ along with the 95\\% confidence interval values. According to our results, the effect of the tweets sentiment on the stocks prices of all the examined stocks is statistically significant. We also observe that the causal impact is stronger for larger values of the $P_{thresh}^1$ and smaller $P_{thresh}^0$ threshold values, i.e. the observed difference on the effect variable between the treatment and control groups is larger when we consider only days for which there is less uncertainty on the estimated tweets polarity. For Apple, it was not possible to create balanced treated and control groups for the thresholds pair $T3$. This is due to the fact that the opening prices of the AAPL stocks are very strongly correlated with both the effect and treatment variables and, therefore, there were not enough treatment and control units with similar values on their confounding variables. Since any causal conclusions are not reliable when the treated and control groups are not balanced, we do not present results for Apple for this pair of threshold values. Finally, in Figure~\\ref{fig:qqplot} we compare the distributions of the effect variable $Y$ for the treated and control units by plotting their percentiles against each other. Under the hypothesis that the treatment variable has no effect on variable $Y$, the plot should follow approximately the line $y=x$. However, most of the points of the plot lie below the reference line $y=x$, indicating that the majority of the percentiles of variable $Y$ for the treated units are larger than the corresponding percentiles for the control units. \n\n\n\\subsection{Evaluation with Synthetic Data}\n\\label{sec:synthetic}\nIn order to demonstrate the potential of our approach we assess its effectiveness in detecting causal relationships on linear and non-linear synthetic data. We also compare our approach with a multivariate Granger causality model and with an information theoretic approach based on Runge's framework \\cite{pompe2011momentary} and we demonstrate that our method is more efficient on avoiding false causal conclusions. We denote with $X(u)$ and $Y(u)$ the treatment and outcome time-series respectively and with $\\mathbf{Z}$ a set of $M$ confounding variables. The relationships among them are described by the following model:\n\n\n", "index": 15, "text": "\\begin{equation}\nX(u) = h_{xx}(X(u-1)) + f_{xz}(\\mathbf{Z}(u)) + \\epsilon_x(u)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"X(u)=h_{xx}(X(u-1))+f_{xz}(\\mathbf{Z}(u))+\\epsilon_{x}(u)\" display=\"block\"><mrow><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>h</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>f</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>z</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc19</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03f5</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\nwhere $\\epsilon_x(u)$, $\\epsilon_y(u)$ and $\\epsilon_{z_i}(u)$ are i.i.d. Gaussian noise variables with zero mean and std. dev. equal to $20 + 2\\cdot M$, $10 + 2\\cdot M$ and $10$, respectively.\n\n\nWe consider the following four cases: \n\n\\textbf{Case 1.} The model is linear. Thus, $f_{xz}(\\mathbf{Z}(u)) = \\sum_i \\alpha_{xz,i} \\cdot Z_i(u) $, $h_{xx}(X(u-1)) = \\alpha_{xx} \\cdot X(u-1)$\n$h_{yy}(Y(u-1)) = \\alpha_{yy} \\cdot Y(u-1)$, $f_{yz}(\\mathbf{Z}(u)) = \\sum_i \\alpha_{yz,i} \\cdot Z_i(u)$, $f_{yx}(X(u)) = \\alpha_{yx} \\cdot X(u)$, $h_{z_i}(Z_i(u-1)) = \\alpha_{z_i} \\cdot Z(u-1)$.\n\n\\textbf{Case 2.} We apply the linear model of Case 1, but we set $f_{yx}(X(u)) = 0$. In this case the treatment time-series $X(u)$ does not have any causal impact on the outcome time-series. \n\n\\textbf{Case 3.} The associations of the confounding variables with the treatment and effect variables are non-linear. In particular,  \n\n", "itemtype": "equation", "pos": 47778, "prevtext": "\n\\begin{eqnarray}\nY(u) &=& h_{yy}(Y(u-1)) + f_{yz}(\\mathbf{Z}(u)) \n \\nonumber \\\\\n&& + f_{yx}(X(u))  + \\epsilon_y(u)\n\\end{eqnarray}\n\n", "index": 17, "text": "\\begin{equation}\nZ_i(u) = h_{z_i}(Z_i(u-1)) + \\epsilon_{z_i}(u), \\forall Z_i \\in \\mathbf{Z},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"Z_{i}(u)=h_{z_{i}}(Z_{i}(u-1))+\\epsilon_{z_{i}}(u),\\forall Z_{i}\\in\\mathbf{Z},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>h</mi><msub><mi>z</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03f5</mi><msub><mi>z</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mo>\u2200</mo><msub><mi>Z</mi><mi>i</mi></msub></mrow><mo>\u2208</mo><mi>\ud835\udc19</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 48798, "prevtext": "\nwhere $\\epsilon_x(u)$, $\\epsilon_y(u)$ and $\\epsilon_{z_i}(u)$ are i.i.d. Gaussian noise variables with zero mean and std. dev. equal to $20 + 2\\cdot M$, $10 + 2\\cdot M$ and $10$, respectively.\n\n\nWe consider the following four cases: \n\n\\textbf{Case 1.} The model is linear. Thus, $f_{xz}(\\mathbf{Z}(u)) = \\sum_i \\alpha_{xz,i} \\cdot Z_i(u) $, $h_{xx}(X(u-1)) = \\alpha_{xx} \\cdot X(u-1)$\n$h_{yy}(Y(u-1)) = \\alpha_{yy} \\cdot Y(u-1)$, $f_{yz}(\\mathbf{Z}(u)) = \\sum_i \\alpha_{yz,i} \\cdot Z_i(u)$, $f_{yx}(X(u)) = \\alpha_{yx} \\cdot X(u)$, $h_{z_i}(Z_i(u-1)) = \\alpha_{z_i} \\cdot Z(u-1)$.\n\n\\textbf{Case 2.} We apply the linear model of Case 1, but we set $f_{yx}(X(u)) = 0$. In this case the treatment time-series $X(u)$ does not have any causal impact on the outcome time-series. \n\n\\textbf{Case 3.} The associations of the confounding variables with the treatment and effect variables are non-linear. In particular,  \n\n", "index": 19, "text": "\\begin{equation*}\nf_{xz}(\\mathbf{Z}(u)) = \\sum_i {\\alpha_{xz,i} \\cdot (Z_i(u))^2}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"f_{xz}(\\mathbf{Z}(u))=\\sum_{i}{\\alpha_{xz,i}\\cdot(Z_{i}(u))^{2}}\" display=\"block\"><mrow><mrow><msub><mi>f</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>z</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc19</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><msub><mi>\u03b1</mi><mrow><mrow><mi>x</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>\u22c5</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\nWe use the linear equations of Case 1 for the rest of the functions. \n\n\\textbf{Case 4.} We use the non-linear model of Case 3, but we set $f_{yx}(X(u)) = 0$. In this case, the multivariate linear Granger causality approach may return positive causality result, even though the treatment time-series $X(u)$ does not have any causal impact on the outcome time-series. \n\nWe assume that time-series $X$ is sampled before $Y$ and that all time-series $\\mathbf{Z}$ are sampled before $X$. A unit $u$ of the study corresponds to 'day' index $u$:  $S(u):=$ $(X(u)$, $Y(u)$, $\\mathbf{Z}(u)$, $X^1(u)$, $Y^1(u)$, $\\mathbf{Z}^1(u))$. We apply the following three methodologies on the synthetic data generated using the models above in order to assess the causal impact of variable $X$ on $Y$:\n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.4\\textwidth]{Drawing4.pdf}\n\\caption{Resulting graph after applying Algorithm \\ref{Alg:predecessors} on the synthetic data when $M=1$ (i.e. there is only one confounding variable). Graph depicts the direct predecessors of nodes $X$ and $Y$. The set of nodes $\\mathbf{H}$ will contain the direct predecessors that nodes $X$ and $Y$ have in common. In the four examined cases $X$ correlates with $Y$, though in Case 2 and Case 4, this is a spurious correlation due to the set of confounding variables $\\mathbf{Z}$. There is also a spurious correlation of node $X$ with node $Y^1$. $X$ and $Y$ are independent to $\\mathbf{Z}^1$ conditional to $\\mathbf{Z}$ and $Y$ is independent to $X^1$ conditional to $X$.}\n\\label{fig:exampleGraph}\n\\end{figure}\n\n\\textbf{Multivariate Granger Causality (MGC).} We apply stepwise regression in order to fit our data to the following model:\n\n", "itemtype": "equation", "pos": 48896, "prevtext": "\n\n", "index": 21, "text": "\\begin{equation*}\nf_{yz}(\\mathbf{Z}(u)) = \\sum_i {\\alpha_{yz,i} \\cdot (Z_i(u))^2} \n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"f_{yz}(\\mathbf{Z}(u))=\\sum_{i}{\\alpha_{yz,i}\\cdot(Z_{i}(u))^{2}}\" display=\"block\"><mrow><mrow><msub><mi>f</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>z</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc19</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><msub><mi>\u03b1</mi><mrow><mrow><mi>y</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>\u22c5</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\nWe conclude that $X$ causes $Y$ if $X$ or any lagged version of $X$ is included in the regression model. \n\n\\textbf{Conditional Mutual Information Tests (CMI).} Following Runge's approach \\cite{pompe2011momentary} a causal graph is created by performing conditional independence tests using conditional mutual information as described at section \\ref{sec:background}.3. We learn the causal graph by applying PC algorithm \\cite{spirtes1995learning}. \n\n\\textbf{Matching Design for Time-series (MDT).} Following the proposed approach, we apply Algorithm \\ref{Alg:predecessors} in order to find the set of variables $\\mathbf{H}$ that needs to be controlled in order to achieve conditional ignorability. $\\mathbf{H}$ includes any $Z_i \\in \\mathbf{Z}$ that correlates both with $X$ and $Y$.  Moreover, we satisfy the i.i.d assumption by including in $\\mathbf{H}$ the time-series $Y^1$. In order to create groups of treated and untreated units we first transform the time series $X$ into a binary stream $\\tilde X$: $\\tilde X(u)=0$, if $X(u) < \\mu_X$;  $\\tilde X(u)=1$, otherwise, where $\\mu_X$ is the mean of $X$ ($u$ is considered treated if $X(u)>\\mu_X$). Then, we create pairs of treated and untreated units by applying Genetic Matching algorithm \\cite{diamond2013genetic} using as loss function the average standardized mean difference between the treated and control units for all the confounding variables \\ref{eq:std-mean}. Finally, the average treatment is estimated using equation (\\ref{eq:ATE}) and a t-test is used to examine whether the observed $ATE$ is statistically significant from 0. \n\n\n\n\\begin{figure*}\n\\centering\n       \\subfloat[Linear Case.]\n                {\\includegraphics[width=0.49\\textwidth]{linear.eps}\n                \\label{fig:linear}}\n~\n        \\subfloat[Non-linear Case.]\n                {\\includegraphics[width=0.49\\textwidth]{noLinear.eps}\n                \\label{fig:noLinear}}\n\n\\caption{Comparison of the MDT, CMI and MGC causality detection methods on synthetic data.}\n\\label{fig:synthetic-res}\n\\end{figure*}\n\n\nWe generate $100$ samples for each time-series. We vary the number of confounding variables M that are included at set $\\mathbf{Z}$ from 10 to 50. In detail, we evaluate the three methodologies for $M = \\{10, 20, 30, 40, 50\\}$. For each $M$ value, we repeat our study for $30$ randomly selected sets of model coefficients ($\\alpha$s). All model coefficients are randomly generated from uniform distribution  on $[-4,4]$ for the linear cases and on $[-1, 1]$ for the non-linear cases. Finally, for each one of the $30$ sets of model coefficients we repeat each study for $100$ different noise realizations. For the $n^{th}$ noise realization, we define:\n\n\n", "itemtype": "equation", "pos": 50700, "prevtext": "\nWe use the linear equations of Case 1 for the rest of the functions. \n\n\\textbf{Case 4.} We use the non-linear model of Case 3, but we set $f_{yx}(X(u)) = 0$. In this case, the multivariate linear Granger causality approach may return positive causality result, even though the treatment time-series $X(u)$ does not have any causal impact on the outcome time-series. \n\nWe assume that time-series $X$ is sampled before $Y$ and that all time-series $\\mathbf{Z}$ are sampled before $X$. A unit $u$ of the study corresponds to 'day' index $u$:  $S(u):=$ $(X(u)$, $Y(u)$, $\\mathbf{Z}(u)$, $X^1(u)$, $Y^1(u)$, $\\mathbf{Z}^1(u))$. We apply the following three methodologies on the synthetic data generated using the models above in order to assess the causal impact of variable $X$ on $Y$:\n\n\\begin{figure}\n  \\centering\n\\includegraphics[width=0.4\\textwidth]{Drawing4.pdf}\n\\caption{Resulting graph after applying Algorithm \\ref{Alg:predecessors} on the synthetic data when $M=1$ (i.e. there is only one confounding variable). Graph depicts the direct predecessors of nodes $X$ and $Y$. The set of nodes $\\mathbf{H}$ will contain the direct predecessors that nodes $X$ and $Y$ have in common. In the four examined cases $X$ correlates with $Y$, though in Case 2 and Case 4, this is a spurious correlation due to the set of confounding variables $\\mathbf{Z}$. There is also a spurious correlation of node $X$ with node $Y^1$. $X$ and $Y$ are independent to $\\mathbf{Z}^1$ conditional to $\\mathbf{Z}$ and $Y$ is independent to $X^1$ conditional to $X$.}\n\\label{fig:exampleGraph}\n\\end{figure}\n\n\\textbf{Multivariate Granger Causality (MGC).} We apply stepwise regression in order to fit our data to the following model:\n\n", "index": 23, "text": "\\begin{equation}\nY(t) = a_1 \\cdot Y(t-1) + \\sum_{l=0}^1 b_l \\cdot X(t-l) + \\sum_{l=0}^1  \\mathbf{c}_l \\cdot \\mathbf{Z}(t-l)  + \\delta + \\epsilon(t)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"Y(t)=a_{1}\\cdot Y(t-1)+\\sum_{l=0}^{1}b_{l}\\cdot X(t-l)+\\sum_{l=0}^{1}\\mathbf{c%&#10;}_{l}\\cdot\\mathbf{Z}(t-l)+\\delta+\\epsilon(t)\" display=\"block\"><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>\u22c5</mo><mi>Y</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></munderover><mrow><mrow><msub><mi>b</mi><mi>l</mi></msub><mo>\u22c5</mo><mi>X</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mi>l</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></munderover><mrow><mrow><msub><mi>\ud835\udc1c</mi><mi>l</mi></msub><mo>\u22c5</mo><mi>\ud835\udc19</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mi>l</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mi>\u03b4</mi><mo>+</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03610.tex", "nexttext": "\n\nFor the $k^{th}$ set of model coefficients we also define $A_k = \\sum_{n=1}^{100} S_n$. In Case 1 and Case 3, $A_k$ denotes the number of times that a causal relationship from $X$ to $Y$ is successfully inferred (\\textit{true positive}) for the $k^{th}$ set of model coefficients and different noise realizations, while in Case 2 and 4 it denotes the number of times that a causal relationship is falsely inferred (\\textit{false positive}). In Figure \\ref{fig:synthetic-res} we present the mean value of $A_k$, $\\mu_A$ along with the standard error of the mean. According to our results, the proposed causal inference technique reduces significantly the number of false positive causality conclusions while it is slightly less successful on detecting real causality for $M=10$. Multivariate Granger causality achieves almost 100\\% accuracy on true causality detection both for the linear (Case 1) and non-linear (Case 3) cases. However, it performs poorly in terms of avoiding false positive conclusions. The performance of all the examined methods improves for larger $M$ values (apart from multivariate Granger causality on the linear cases). This is due to the fact that, by adding more variables on the set $\\mathbf{Z}$, the dependence of $Y$ and $X$ with each individual $Z_i \\in \\mathbf{Z}$ is weaker; consequently, although $M$ covariates are used to generate $X$ and $Y$ time-series, for large $M$ values, only a subset of them has significant effect on them. Thus, cancelling out the effect of $\\mathbf{Z}$ is easier. \n\n\n\n\\section{Related Work}\n\\label{sec:related-work}\n\nSeveral works have previously examined the potential of information extracted from social media, search engine query data or other web-related information to predict stock market returns. For example, in \\cite{zhang2011predicting} the authors demonstrate that the level of optimism/pessimism, which is estimated using Twitter data, correlates with stock market movement. \nOther projects have been focussed on the possible use of sentiment analysis based on Twitter data for the prediction of traded assets prices by applying a bivariate Granger causality analysis~\\cite{bollen2011twitter, zhang2012predicting}.\nSimilarly, in \\cite{zheludev2014can} information theoretic methods are used to investigate whether sentiment analysis of social media can provide statistically significant information for the prediction of stock markets.\nIn \\cite{preis2010complex, preis2013quantifying} the authors have also demonstrated that search engine query data correlate with stock market movements. In \\cite{moat2013quantifying} the authors propose a trading strategy that utilizes information about Wikipedia views. They demonstrate that their trading strategy outperforms random strategy. However, all the above mentioned studies are based on bivariate models. Although their results indicate that social media and other web sources may carry useful information for stock market prediction, by using these techniques it is not possible to figure out if other factors are influencing the observed trends. \n\nTrading strategies that utilize both technical analysis and sentiment analysis are discussed in \\cite{schumaker2009textual, deng2011combining}. However, these works are based on regression analysis, thus they suffer from the limitations that have been previously discussed. Moreover, all the studies so far focus mainly on prediction of stock market movement. Although they provide insights about the influence that emotional and social factors may have on stock market, they do not investigate the presence of causality. To the best of our knowledge, this is the first work that attempts to measure the causal effect of such factors on stock markets. \n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nIn this study, for the first time we have attempted to quantify the causal impact of social and emotional factors, captured by social media, on daily stock market returns of individual companies (i.e., not just a mere correlation between the two). We have proposed a novel framework for causal analysis in time-series which does not require any assumptions about the statistical relationships among the variables of the study, i.e., it is model-free. Our evaluation on synthetic data demonstrates that our method is more effective on inferring true causality and avoiding false positive conclusions compared to other methods that have been previously used for causal inference in time-series. Our approach can incorporate a large number of factors and, therefore, can effectively handle complex data such as financial data. Indeed, causality studies that are based on observational data rather than experimental procedures could be biased in case of missing confounding variables. However, conducting experimental studies is not feasible in most cases. In this work we have minimized the risk of biased conclusions due to unmeasured confounding variables by including in our study a large number of factors.\nWe have estimated a sentiment index indicating the probability that the general sentiment of a day, based on tweets posted for a target company, is positive. Our results show that Twitter data polarity does indeed have a causal impact on the stock market prices of the examined companies. Hence, we believe social media data could represent a valuable source of information for understanding the dynamics of stock market movements. \n\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem{asur2010predicting}\nS.~Asur and B.~A. Huberman.\n\\newblock Predicting the future with social media.\n\\newblock In {\\em Proceedings of the IEEE/WIC/ACM International Conference on\n  Web Intelligence and Intelligent Agent Technology (WI-IAT'10)}, pages\n  492--499. IEEE, 2010.\n\n\\bibitem{austin2008assessing}\nP.~C. Austin.\n\\newblock Assessing balance in measured baseline covariates when using\n  many-to-one matching on the propensity-score.\n\\newblock {\\em Pharmacoepidemiology and Drug Safety}, 17(12):1218--1225, 2008.\n\n\\bibitem{austin2008goodness}\nP.~C. Austin.\n\\newblock Goodness-of-fit diagnostics for the propensity score model when\n  estimating treatment effects using covariate adjustment with the propensity\n  score.\n\\newblock {\\em Pharmacoepidemiology and Drug Safety}, 17(12):1202--1217, 2008.\n\n\\bibitem{austin2009relative}\nP.~C. Austin.\n\\newblock The relative ability of different propensity score methods to balance\n  measured covariates between treated and untreated subjects in observational\n  studies.\n\\newblock {\\em Medical Decision Making}, 29(6):661--677, 2009.\n\n\\bibitem{barrett2010multivariate}\nA.~B. Barrett, L.~Barnett, and A.~K. Seth.\n\\newblock Multivariate granger causality and generalized variance.\n\\newblock {\\em Physical Review E}, 81(4):041907, 2010.\n\n\\bibitem{bollen2011twitter}\nJ.~Bollen, H.~Mao, and X.~Zeng.\n\\newblock Twitter mood predicts the stock market.\n\\newblock {\\em Journal of Computational Science}, 2(1):1--8, 2011.\n\n\\bibitem{Bond2012}\nR.~M. Bond et~al.\n\\newblock A 61-million-person experiment in social influence and political\n  mobilization.\n\\newblock {\\em Nature}, 489(7415):295--298, 2012.\n\n\\bibitem{chi2010network}\nK.~T. Chi, J.~Liu, and F.~C. Lau.\n\\newblock A network perspective of the stock market.\n\\newblock {\\em Journal of Empirical Finance}, 17(4):659--667, 2010.\n\n\\bibitem{deng2011combining}\nS.~Deng, T.~Mitsubuchi, K.~Shioda, T.~Shimada, and A.~Sakurai.\n\\newblock Combining technical analysis with sentiment analysis for stock price\n  prediction.\n\\newblock In {\\em Proceedings of IEEE Ninth International Conference on\n  Dependable, Autonomic and Secure Computing (DASC'11)}, pages 800--807. IEEE,\n  2011.\n\n\\bibitem{diamond2013genetic}\nA.~Diamond and J.~S. Sekhon.\n\\newblock Genetic matching for estimating causal effects: A general\n  multivariate matching method for achieving balance in observational studies.\n\\newblock {\\em Review of Economics and Statistics}, 95(3):932--945, 2013.\n\n\\bibitem{fan2014challenges}\nJ.~Fan, F.~Han, and H.~Liu.\n\\newblock Challenges of big data analysis.\n\\newblock {\\em National Science Review}, 1(2):293--314, 2014.\n\n\\bibitem{freedman1997association}\nD.~Freedman.\n\\newblock From association to causation via regression.\n\\newblock {\\em Advances in Applied Mathematics}, 18(1):59--110, 1997.\n\n\\bibitem{granger1969investigating}\nC.~W. Granger.\n\\newblock Investigating causal relations by econometric models and\n  cross-spectral methods.\n\\newblock {\\em Econometrica: Journal of the Econometric Society}, pages\n  424--438, 1969.\n\n\\bibitem{harder2010propensity}\nV.~S. Harder, E.~A. Stuart, and J.~C. Anthony.\n\\newblock Propensity score techniques and the assessment of measured covariate\n  balance to test causal associations in psychological research.\n\\newblock {\\em Psychological methods}, 15(3):234, 2010.\n\n\\bibitem{Holland86}\nP.~W. Holland.\n\\newblock Statistics and causal inference.\n\\newblock {\\em Journal of the American Statistical Association},\n  81(396):945--960, 1986.\n\n\\bibitem{imbens2004nonparametric}\nG.~W. Imbens.\n\\newblock Nonparametric estimation of average treatment effects under\n  exogeneity: A review.\n\\newblock {\\em Review of Economics and Statistics}, 86(1):4--29, 2004.\n\n\\bibitem{kenett2010dominating}\nD.~Y. Kenett, M.~Tumminello, A.~Madi, G.~Gur-Gershgoren, R.~N. Mantegna, and\n  E.~Ben-Jacob.\n\\newblock Dominating clasp of the financial sector revealed by partial\n  correlation analysis of the stock market.\n\\newblock {\\em PloS one}, 5(12):e15032, 2010.\n\n\\bibitem{mao2011predicting}\nH.~Mao, S.~Counts, and J.~Bollen.\n\\newblock Predicting financial markets: Comparing survey, news, twitter and\n  search engine data.\n\\newblock {\\em arXiv preprint arXiv:1112.1051}, 2011.\n\n\\bibitem{moat2013quantifying}\nH.~S. Moat, C.~Curme, A.~Avakian, D.~Y. Kenett, H.~E. Stanley, and T.~Preis.\n\\newblock Quantifying wikipedia usage patterns before stock market moves.\n\\newblock {\\em Scientific Reports}, 3, 2013.\n\n\\bibitem{morgan2014counterfactuals}\nS.~L. Morgan and C.~Winship.\n\\newblock {\\em Counterfactuals and causal inference}.\n\\newblock Cambridge University Press, 2014.\n\n\\bibitem{Muchnik2013}\nL.~Muchnik, S.~Aral, and S.~Taylor.\n\\newblock Social influence bias: A randomized experiment.\n\\newblock {\\em Science}, 341(6146):647--651, 2013.\n\n\\bibitem{nawrath2010distinguishing}\nJ.~Nawrath, M.~C. Romano, M.~Thiel, I.~Z. Kiss, M.~Wickramasinghe, J.~Timmer,\n  J.~Kurths, and B.~Schelter.\n\\newblock Distinguishing direct from indirect interactions in oscillatory\n  networks with multiple time scales.\n\\newblock {\\em Physical Review Letters}, 104(3):038701, 2010.\n\n\\bibitem{pearl1995causal}\nJ.~Pearl.\n\\newblock Causal diagrams for empirical research.\n\\newblock {\\em Biometrika}, 82(4):669--688, 1995.\n\n\\bibitem{pearl2000causality}\nJ.~Pearl.\n\\newblock {\\em Causality: models, reasoning and inference}.\n\\newblock Cambridge University Press, 2009.\n\n\\bibitem{pearl2010introduction}\nJ.~Pearl.\n\\newblock An introduction to causal inference.\n\\newblock {\\em The International Journal of Biostatistics}, 6(2), 2010.\n\n\\bibitem{peters2013causal}\nJ.~Peters, D.~Janzing, and B.~Sch{\\\"o}lkopf.\n\\newblock Causal inference on time series using restricted structural equation\n  models.\n\\newblock In {\\em Advances in Neural Information Processing Systems}, pages\n  154--162, 2013.\n\n\\bibitem{pompe2011momentary}\nB.~Pompe and J.~Runge.\n\\newblock Momentary information transfer as a coupling measure of time series.\n\\newblock {\\em Physical Review E}, 83(5):051122, 2011.\n\n\\bibitem{preis2013quantifying}\nT.~Preis, H.~S. Moat, and H.~E. Stanley.\n\\newblock {Quantifying trading behavior in financial markets using Google\n  Trends}.\n\\newblock {\\em Scientific Reports}, 3, 2013.\n\n\\bibitem{preis2010complex}\nT.~Preis, D.~Reith, and H.~E. Stanley.\n\\newblock Complex dynamics of our economic life on different scales: insights\n  from search engine query data.\n\\newblock {\\em Philosophical Transactions of the Royal Society A: Mathematical,\n  Physical and Engineering Sciences}, 368(1933):5707--5719, 2010.\n\n\\bibitem{rubin1997estimating}\nD.~B. Rubin.\n\\newblock Estimating causal effects from large data sets using propensity\n  scores.\n\\newblock {\\em Annals of Internal Medicine}, 127(8\\_Part\\_2):757--763, 1997.\n\n\\bibitem{schreiber2000measuring}\nT.~Schreiber.\n\\newblock Measuring information transfer.\n\\newblock {\\em Physical Review Letters}, 85(2):461, 2000.\n\n\\bibitem{schumaker2009textual}\nR.~P. Schumaker and H.~Chen.\n\\newblock Textual analysis of stock market prediction using breaking financial\n  news: The azfin text system.\n\\newblock {\\em ACM Transactions on Information Systems (TOIS)}, 27(2):12, 2009.\n\n\\bibitem{sekhon2009opiates}\nJ.~S. Sekhon.\n\\newblock Opiates for the matches: Matching methods for causal inference.\n\\newblock {\\em Annual Review of Political Science}, 12:487--508, 2009.\n\n\\bibitem{william2002experimental}\nW.~R. Shadish, T.~D. Cook, and D.~T. Campbell.\n\\newblock {\\em Experimental and quasi-experimental designs for generalized\n  causal inference}.\n\\newblock Wadsworth Cengage learning, 2002.\n\n\\bibitem{spirtes1995learning}\nP.~Spirtes and C.~Meek.\n\\newblock Learning bayesian networks with discrete variables from data.\n\\newblock In {\\em Proceedings of the First International Conference on\n  Knowledge Discovery and Data Mining (KDD'95)}, volume~1, pages 294--299,\n  1995.\n\n\\bibitem{MatchingSurvey}\nE.~A. Stuart.\n\\newblock Matching methods for causal inference: A review and a look forward.\n\\newblock {\\em Statistical Science: a Review Journal of the Institute of\n  Mathematical Statistics}, 25(1):1, 2010.\n\n\\bibitem{thelwall2013heart}\nM.~Thelwall.\n\\newblock Heart and soul: Sentiment strength detection in the social web with\n  sentistrength.\n\\newblock {\\em Cyberemotions}, pages 1--14, 2013.\n\n\\bibitem{tumasjan2010predicting}\nA.~Tumasjan, T.~O. Sprenger, P.~G. Sandner, and I.~M. Welpe.\n\\newblock {Predicting Elections with Twitter: What 140 Characters Reveal about\n  Political Sentiment}.\n\\newblock In {\\em Proceedings of the 4th International Conference on Weblogs\n  and Social Media (ICWSM'10)}, volume~10, pages 178--185, 2010.\n\n\\bibitem{zhang2012kernel}\nK.~Zhang, J.~Peters, D.~Janzing, and B.~Sch{\\\"o}lkopf.\n\\newblock Kernel-based conditional independence test and application in causal\n  discovery.\n\\newblock {\\em arXiv preprint arXiv:1202.3775}, 2012.\n\n\\bibitem{zhang2011predicting}\nX.~Zhang, H.~Fuehres, and P.~A. Gloor.\n\\newblock {Predicting stock market indicators through Twitter {\"I hope it is\n  not as bad as I fear\"}}.\n\\newblock In {\\em Proceedings of the 2nd Collaborative Innovation Networks\n  Conference}, volume~26, pages 55--62. Elsevier, 2011.\n\n\\bibitem{zhang2012predicting}\nX.~Zhang, H.~Fuehres, and P.~A. Gloor.\n\\newblock {Predicting asset value through {Twitter} buzz}.\n\\newblock In {\\em Advances in Collective Intelligence 2011}, pages 23--34.\n  Springer, 2012.\n\n\\bibitem{zheludev2014can}\nI.~Zheludev, R.~Smith, and T.~Aste.\n\\newblock When can social media lead financial markets?\n\\newblock {\\em Scientific Reports}, 4, 2014.\n\n\\end{thebibliography}\n\n\n\\balancecolumns\n\n\n", "itemtype": "equation", "pos": 53558, "prevtext": "\nWe conclude that $X$ causes $Y$ if $X$ or any lagged version of $X$ is included in the regression model. \n\n\\textbf{Conditional Mutual Information Tests (CMI).} Following Runge's approach \\cite{pompe2011momentary} a causal graph is created by performing conditional independence tests using conditional mutual information as described at section \\ref{sec:background}.3. We learn the causal graph by applying PC algorithm \\cite{spirtes1995learning}. \n\n\\textbf{Matching Design for Time-series (MDT).} Following the proposed approach, we apply Algorithm \\ref{Alg:predecessors} in order to find the set of variables $\\mathbf{H}$ that needs to be controlled in order to achieve conditional ignorability. $\\mathbf{H}$ includes any $Z_i \\in \\mathbf{Z}$ that correlates both with $X$ and $Y$.  Moreover, we satisfy the i.i.d assumption by including in $\\mathbf{H}$ the time-series $Y^1$. In order to create groups of treated and untreated units we first transform the time series $X$ into a binary stream $\\tilde X$: $\\tilde X(u)=0$, if $X(u) < \\mu_X$;  $\\tilde X(u)=1$, otherwise, where $\\mu_X$ is the mean of $X$ ($u$ is considered treated if $X(u)>\\mu_X$). Then, we create pairs of treated and untreated units by applying Genetic Matching algorithm \\cite{diamond2013genetic} using as loss function the average standardized mean difference between the treated and control units for all the confounding variables \\ref{eq:std-mean}. Finally, the average treatment is estimated using equation (\\ref{eq:ATE}) and a t-test is used to examine whether the observed $ATE$ is statistically significant from 0. \n\n\n\n\\begin{figure*}\n\\centering\n       \\subfloat[Linear Case.]\n                {\\includegraphics[width=0.49\\textwidth]{linear.eps}\n                \\label{fig:linear}}\n~\n        \\subfloat[Non-linear Case.]\n                {\\includegraphics[width=0.49\\textwidth]{noLinear.eps}\n                \\label{fig:noLinear}}\n\n\\caption{Comparison of the MDT, CMI and MGC causality detection methods on synthetic data.}\n\\label{fig:synthetic-res}\n\\end{figure*}\n\n\nWe generate $100$ samples for each time-series. We vary the number of confounding variables M that are included at set $\\mathbf{Z}$ from 10 to 50. In detail, we evaluate the three methodologies for $M = \\{10, 20, 30, 40, 50\\}$. For each $M$ value, we repeat our study for $30$ randomly selected sets of model coefficients ($\\alpha$s). All model coefficients are randomly generated from uniform distribution  on $[-4,4]$ for the linear cases and on $[-1, 1]$ for the non-linear cases. Finally, for each one of the $30$ sets of model coefficients we repeat each study for $100$ different noise realizations. For the $n^{th}$ noise realization, we define:\n\n\n", "index": 25, "text": "\\begin{equation}\nS_n = \n\\left\\{\n  \\begin{array}{rcr}\n\t1 & \\mbox{if $X$ was detected as cause of $Y$}\\\\\n    0 & \\mbox{otherwise}\\\\\n    \n\\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"S_{n}=\\left\\{\\begin{array}[]{rcr}1&amp;\\mbox{if $X$ was detected as cause of $Y$}%&#10;\\\\&#10;0&amp;\\mbox{otherwise}\\\\&#10;\\par&#10;\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>S</mi><mi>n</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mrow><mtext>if\u00a0</mtext><mi>X</mi><mtext>\u00a0was detected as cause of\u00a0</mtext><mi>Y</mi></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"right\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mtext>otherwise</mtext></mtd><mtd/></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}]