[{"file": "1601.06502.tex", "nexttext": "\nIndeed, this is precisely the \\emph{randomize-then-combine paradigm}\nproposed by Bellare and Micciancio~\\cite{bellare1997new} for incremental\nhashing of messages, which is readily (in fact, more naturally than to\nmessage hashing) applied by Clarke~{et al.}~\\cite{clarke2003incremental} to multiset hashing.  Our goal is to minimize the computational cost for computing $H$ and the representation size for elements of $G$ while achieving a given level of collision resistance.\n\n\\subsubsection*{Collision resistance}\n\nA \\emph{collision} for a hash function $H$ is a pair $x, x'$ such that $x \\not= x'$ but $H(x) = H(x')$.  For any group-homomorphic hash function $H$ from a group $(X, +)$ to $(G, +)$, a collision can equivalently be defined as a value $x \\in \\ker H \\setminus \\Set{0_X}$.   By the birthday bound that applies to any hash function, a collision can be found with at most expected $O(\\sqrt{{\\lvert {G} \\rvert}})$ hash computations; we can hope to design a multiset hash function for which expected time $\\Omega(\\sqrt{{\\lvert {G} \\rvert}})$ is also a lower bound.\\footnote{In this and the other collision bounds that follow, it is assumed that the expectations are taken over a random choice of hash function $H$ and group $(G, +_G)$ from some hash function family (distribution) $\\mathcal{H}$.}\n\nA \\emph{preimage attack} seeks to invert the hash function, namely to find a value $x$ such that $H(x) = y$, for a random element $y$ in the image of $H$.  We can hope to design a multiset hash function for which the expected time complexity of the best preimage attack is also equal to the generic upper bound $\\Omega({\\lvert {G} \\rvert})$.  Note that for a homomorphic hash function we do not consider preimage attacks on the identity element $0_G$, since its preimage is fixed.\n\nA \\emph{second preimage attack} seeks to find a value $x'$ such that $H(x') = H(x)$, for some known value $x$.  Since a second preimage implies a collision, the time complexity of a second preimage attack is lower bounded by the time complexity of the best  collision attack, ideally $\\Omega(\\sqrt{{\\lvert {G} \\rvert}})$.  For a general, non-homomorphic hash function,  we can hope that the best attack has expected $\\Omega({\\lvert {G} \\rvert})$ time complexity.  For any homomorphic hash function, however, the group structure implies that a second preimage attack is no harder than a collision attack (with expected time complexity upper-bounded by $O(\\sqrt{{\\lvert {G} \\rvert}})$).\n\n\n\\section{Generic multiset hash families}\n\nA random oracle $\\hat{H} \\colon A \\rightarrow G$ clearly achieves the\noptimal preimage resistance of $\\Theta({\\lvert {G} \\rvert})$ and the optimal\ncollision resistance of $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$, in the sense that at\nleast this many oracle queries are needed to compute preimages and\ncollisions respectively.\n\nIt does not follow, however, that the associated multiset hash function\n$H = H_G\\colon {\\mathbb{Z}}^{(A)} \\rightarrow G$ has the same security level; for\nexample, if we choose $G={\\mathbb{Z}}_2^n$, then $O(n)$ oracle queries, instead of\n$\\Omega(2^n)$, are enough to find arbitrary preimages in polynomial time\nby solving a simple $n\\times n$ linear system over ${\\mathbb{Z}}_2$. However,\nBellare and Micciancio~\\cite{bellare1997new} have shown (in the set hash setting, but\nthis generalizes naturally to multisets) how to obtain a security\nreduction for $H_G$ based on a computational hardness assumption on the\ngroup $G$. For concrete choices of $G$, that hardness assumption is\nrelated to standard number theoretic problems, such as the discrete\nlogarithm problem or modular knapsacks.\n\nWhen $G={\\mathbb{Z}}_p^\\times$, the resulting multiset hash function $H_G$ is\nessentially {\\texttt{{MSet-Mu-Hash}}}{}~\\cite{clarke2003incremental}, the multiset\nvariant of {\\texttt{{MuHash}}}{}~\\cite{bellare1997new}. When $G={\\mathbb{Z}}_m^n$, we\nessentially obtain {\\texttt{{MSet-VAdd-Hash}}}{}~\\cite{clarke2003incremental}, the\nmultiset variant of {\\texttt{{LtHash}}}{} (for $n > 1$) or {\\texttt{{AdHash}}}{} (for $n =\n1$)~\\cite{bellare1997new}. These functions all have security reductions\nin the framework sketched above.\n\nIt is relatively easy to find plausible concrete instantiations of the\nrandom oracle $\\hat H$ to a group like ${\\mathbb{Z}}_2^n$, but for more general groups, this is\nusually more complicated, and as a result it is often convenient to\nreplace $\\hat H$ by a \\emph{pseudo-random oracle}, i.e.\\ a construction\nthat is indifferentiable from a random oracle in the sense of\nMaurer~{et al.}~\\cite{maurer2004indifferentiability}. Typically, we can take\n$\\hat H$ of the form $\\hat H(a) = f\\big(h(a)\\big)$ where $h\\colon A\\to X$\nis a random oracle to some intermediate set $X$ (such as bit strings, so\nthat we can plausibly instantiate it with standard hash function\nconstructions like SHA-2\\footnote{We will assume that elements of $A$ can\nbe readily encoded as octet strings.}) and $f\\colon X \\rightarrow G$ is\nan \\emph{admissible encoding\nfunction}~\\cite{boneh2001identity,brier2010efficient} that has the\nproperty of mapping the uniform distribution over $X$ to a distribution\nindistinguishable from uniform over $G$.\n\n\\subsubsection*{Security bounds}\n\\label{sec:generalized-multiset-hash-security-bounds}\n\n{\\texttt{{AdHash}}}{} is appealing for its simplicity, but is far from optimal in terms of hash code size.  In the \\emph{set} hashing setting (i.e.\\ $M(a) \\in \\Set{0,1}$), the best known attack is the generalized birthday attack~\\cite{wagner2002generalized}; under the assumption that this attack is optimal, the group ${\\mathbb{Z}}_{2^n}$ corresponds to a security level of roughly $2 \\sqrt{n}$ bits.  In the \\emph{multiset} hashing setting, {\\texttt{{AdHash}}}{} is completely impractical due to the extremely large hash code sizes $n$ required to defeat lattice reduction attacks described in \\cref{sec:adhash-attack}.\n\nThere are reductions from computing discrete logarithms in a group $G$ to\nfinding collisions in the corresponding random oracle multiset hash\nfunction\n$H_{G}$~\\cite{impagliazzo1996efficient,bellare1997new,clarke2003incremental}.\nThese reductions can be used to prove a collision resistance property for\nthe generic multiset hash family over any group in which computing\ndiscrete logarithms is hard, such as ${\\mathbb{Z}}_p^\\times$.  However, because\ndiscrete logarithms in ${\\mathbb{Z}}_p^\\times$  can be solved by e.g.\\ the Number\nField Sieve with (heuristic) subexponential time complexity $L_p\\big[1/3,\n\\sqrt[3]{64/9}\\big]$~\\cite[p.~128]{menezes2010handbook}, it is usually\nestimated that we need to choose $p$ of around $3200$ bits for $128$-bit\nsecurity (see for example the evaluation of the ECRYPT~II report on key\nsizes~\\cite{smart2010ecryptkeysizes}).  In contrast, in a generic group, discrete logarithms\ncannot be computed faster than expected time $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$,\nwhich is also the optimal collision resistance.\n\n\n\n \n\n\n\n\n\\section{Elliptic Curve Multiset Hash}\n\\label{sec:elliptic-curve-multiset-hash}\n\nFor properly chosen elliptic curves over finite fields,\nthere are no known algorithms for solving the discrete logarithm problem in the elliptic curve group faster than in a generic group, i.e.\\ expected time $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$.  Therefore, there is a clear possibility for using an elliptic curve group to obtain a given level of collision resistance with a much lower group size than with {\\texttt{{MSet-Mu-Hash}}}{}.\n\n\n\n\n\nApplying the generic multiset hash construction to elliptic curve groups\npresents a problem, however: while it is easy to define a very efficient\nadmissible encoding from $\\{0,1\\}^k$ to ${\\mathbb{Z}}_p^\\times$ for sufficiently\nlarge $k$, an admissible encoding to an elliptic curve group is not so\neasily defined.  While constructions for admissible encoding functions\nhave been demonstrated~\\cite{brier2010efficient}, their computational\ncost is higher than we would like.\n\n\\subsection{Generalized discrete logarithm security reduction}\n\nIn fact, we can significantly relax the requirement on the encoding\nfunction $f$ and still obtain a very tight reduction, due to random\nself-reducibility of the discrete logarithm problem.  Our relaxed\nrequirement is related to the definition of $\\alpha$-weak encodings by\nBrier~{et al.}~\\cite{brier2010efficient}, and is satisfied in practice by a large class of encoding functions~\\cite{brier2010efficient}.\n\\begin{definition}\n  A function $f \\colon S \\rightarrow R$ between finite sets is said to be an $(\\alpha, \\beta)$-weak encoding, for integer $\\alpha \\geq 1$ and real value $\\beta \\geq 1$, if it satisfies the following properties:\n\\begin{enumerate}\n\\item \\emph{Samplable}: there is an efficient randomized algorithm for computing ${\\lvert {f^{-1}(r)} \\rvert}$ and sampling uniformly from $f^{-1}(r)$ for any $r \\in R$.\n\\item ${\\lvert {f^{-1}(r)} \\rvert} \\leq \\alpha$ for all $r \\in R$.\n\\item ${\\rm I\\kern-.3em E}_r[ {\\lvert { f^{-1}(r) } \\rvert} / \\alpha ] \\geq 1 / \\beta$.\n\\end{enumerate}\n\\end{definition}\nAn $(\\alpha, \\beta)$-weak encoding function $f$ allows us to efficiently\nsample $s \\in S$ uniformly at random using $\\beta$ uniform samples $r \\in\nR$ in expectation, with the property that $f(s) = r$ for any accepted\nsample $s$ obtained from $r$.\\footnote{Under the definition of Brier~{et\nal.}~\\cite{brier2010efficient}, an $\\alpha$-weak encoding $f$ is an $(\\alpha {\\lvert {S} \\rvert} /  {\\lvert {R} \\rvert}, \\alpha^2 {\\lvert {S} \\rvert} / {\\lvert {R} \\rvert})$-weak encoding.  Our definition allows for a tighter bound to be given in \\cref{thm:dlpreduction}.}\n\n\\begin{definition}\n\\label{def:alpha-beta-weak-multiset-hash-family}\nLet $f \\colon X \\rightarrow G$ be an $(\\alpha, \\beta)$-weak encoding from\n$X$ to the abelian group $G$. Assume that $G$ admits as a direct factor a cyclic\nsubgroup $\\langle g \\rangle$ of prime order\n$\\rho$, and that we can efficiently sample from the\ncomplement group $\\overline{\\langle g \\rangle}$ in the direct factor\ndecomposition $H=\\langle g\\rangle \\oplus \\overline{\\langle g \\rangle}$. Given a random oracle\n$h\\colon A\\to X$, we denote by $\\hat H_f$ the function $A\\to G$ given by $\\hat\nH_f(a) = f\\big(h(a)\\big)$, and by $H_f\\colon {\\mathbb{Z}}^{(A)}\\to G$ the\nassociated multiset hash function.\n\\end{definition}\n\nThe following theorem shows that finding a collision in $H_f$\nwith multiplicities up to $\\rho - 1$ is as hard as computing discrete\nlogarithms to the base $g$, up to a small factor that depends on $\\beta$.\nNote that $H_f$ does not depend on the choice of subgroup\n$\\langle g \\rangle$, but the strongest security result is obtained by\nchoosing the largest prime-order subgroup.  The requirement of efficient\nsamplability of $\\overline{\\langle g \\rangle}$ is easily satisfied in\npractice, since efficiency concerns regarding representation size dictate\nthat $\\overline{\\langle g \\rangle}$ be as small as possible (usually\nhaving at most $8$ elements, and most of the time only $1$ or $2$).\n\n\\begin{restatable}{theorem}{dlpreduction}\n\\label{thm:dlpreduction}\nLet $H_f$ be a multiset hash function as in\n\\cref{def:alpha-beta-weak-multiset-hash-family}.  Given an algorithm\n$\\mathcal{C}$ with access to the underlying random oracle $h$ that finds\na non-empty multiset $M \\in \\ker H_f$ with $\\lvert M \\rvert_\\infty <\n\\rho$, in expected time $t'$ with probability $\\epsilon'$\nusing $q$ queries to $h$, discrete logarithms to the\nbase $g$ can be computed with probability $\\epsilon = \\epsilon'/2$ in\nexpected time $t + T_1 + q T_2 + q \\beta T_3 + L T_4$, where $L \\geq\n\\lvert M \\rvert_0$ is a bound on the length of the output of\n$\\mathcal{C}$; $T_1, \\dotsc, T_4$ denote the time required for a constant\nnumber of group operations, and are given in the proof.\n\\end{restatable}\n\\begin{proof}\nSee Appendix~\\ref{sec:proofthmdlpreduction}.\n\\end{proof}\n\nConcretely, if $G = E({\\mathbb{F}}_{p^m})$ is the group of ${\\mathbb{F}}_{p^m}$-rational\npoints on a suitable elliptic curve $E$ chosen to avoid any discrete\nlogarithm weaknesses, with a subgroup $\\langle g \\rangle$ of prime order\n$\\rho \\geq {\\lvert {G} \\rvert}/4$, and $f$ is an $(\\alpha, \\beta)$-weak encoding\nfunction with small constant $\\beta$, then $H_f$ has collision\nresistance roughly $p^{m/2}/2$.  Since an element of $E({\\mathbb{F}}_{p^m})$ can be\nrepresented using $\\lceil \\log_2 p^m \\rceil$ bits, the collision\nresistance of $H_f$ is essentially optimal (to within a few\nbits).\n\n\n\\subsection{Shallue-van de Woestijne (SW) encoding in characteristic 2}\n\nThe Shallue-van de Woestijne (SW) algorithm for characteristic 2 fields~\\cite{shallue2006construction} can be used to map any point $w \\in {\\mathbb{F}}_{2^m}$ to a pair $(x,y) \\in \\left({\\mathbb{F}}_{2^n}\\right)^2$ satisfying an arbitrary elliptic curve equation\n\n", "itemtype": "equation", "pos": 14541, "prevtext": "\n\n\\title{Elliptic Curve Multiset Hash}\n\n\\ifcompj\n\\author{Jeremy Maitin-Shepard}\n\\email{jbms@cs.berkeley.edu}\n\\affiliation{UC Berkeley, USA}\n\n\\author{\\\\Mehdi Tibouchi}\n\\affiliation{NTT Secure Platform Laboratories, Japan}\n\n\\author{Diego F.\\ Aranha}\n\\affiliation{University of Campinas, Brazil}\n\n\\shortauthors{J.~Maitin-Shepard, M.~Tibouchi, D.F.~Aranha}\n\n\\keywords{Homomorphic Hashing; Elliptic Curves; Efficient\nImplementation; GLS254; PCLMULQDQ}\n\\else\n\\author{Jeremy Maitin-Shepard\\\\\nUC Berkeley\\\\\n{\\tt\\small jbms@cs.berkeley.edu}\n\\and\nMehdi Tibouchi\\\\\nNTT Secure Platform Laboratories\\\\\n{\\tt\\small tibouchi.mehdi@lab.ntt.co.jp}\n\\and\nDiego F.\\ Aranha\\\\\nInstitute of Computing, University of Campinas\\\\\n{\\tt\\small dfaranha@ic.unicamp.br}}\n\\date{}\n\\newtheorem{definition}{Definition}\n\\newtheorem{theorem}{Theorem}\n\\maketitle\n\\fi\n\n\n\\begin{abstract}\nA homomorphic, or incremental, multiset hash function, associates a hash value to\narbitrary collections of objects (with possible repetitions) in such a\nway that the hash of the union of two collections is easy to compute from\nthe hashes of the two collections themselves: it is simply their sum\nunder a suitable group operation. In particular, hash values of large\ncollections can be computed incrementally and/or in parallel. Homomorphic\nhashing is thus a very useful primitive with applications ranging from\ndatabase integrity verification to streaming set/multiset comparison and\nnetwork coding.\n\n\\medskip\nUnfortunately, constructions of homomorphic hash functions in the\nliterature are hampered by two main drawbacks: they tend to be much\nlonger than usual hash functions at the same security level (e.g.{} to\nachieve a collision resistance of $2^{128}$, they are several thousand\nbits long, as opposed to $256$ bits for usual hash functions), and they\nare also quite slow.\n\n\\medskip\nIn this paper, we introduce the Elliptic Curve Multiset Hash (ECMH),\nwhich combines a usual bit string-valued hash function like BLAKE2 with\nan efficient\nencoding into binary elliptic curves to overcome both difficulties. On\nthe one hand, the size of ECMH digests is essentially optimal: $2m$-bit\nhash values provide $O(2^m)$ collision resistance. On the other hand, we\ndemonstrate a highly-efficient software implementation of ECMH, which our thorough empirical evaluation shows to be capable of processing over 3 million set elements per second on a \\SI{4}{GHz} Intel\nHaswell machine at the 128-bit security level---many times\nfaster than previous practical methods.\n\n\\medskip\nWhile incremental hashing based on elliptic curves has been considered previously~\\cite{brown2008encrypted}, the proposed method was less efficient, susceptible to timing attacks, and potentially patent-encumbered~\\cite{brown2007method}, and no practical implementation was demonstrated.\n\\ifcompj\\else\\par\n\\bigskip\\noindent\n\\textbf{Keywords:} homomorphic hashing, elliptic curves, efficient\nimplementation, GLS254, PCLMULQDQ.\\fi\n\\end{abstract}\n\n\\ifcompj\\maketitle\\fi\n\n\\section{Introduction}\n\n\\subsubsection*{Homomorphic hashing}\nA \\emph{multiset} is a generalization of a set in which each element has\nan associated integer \\emph{multiplicity}. Given a possibly infinite set\n$A$, a set (resp.{} multiset) homomorphic hash function on $A$ maps\nfinite subsets of $A$ (resp.{} finitely-supported multisets on $A$) to\nfixed-length hash values, allowing incremental updates: when new elements\nare added to the (multi)set, the hash value of the modified (multi)set\ncan be computed in time proportional to the degree of modification.\n\nThe incremental update property makes homomorphic hashing a very useful\nand versatile primitive. It has found applications in many areas of\ncomputer security and algorithmics, including network\ncoding~\\cite{DBLP:conf/infocom/GkantsidisR06} and verifiable peer-to-peer\ncontent distribution~\\cite{DBLP:conf/sp/KrohnFM04}, secure Internet\nrouting~\\cite{DBLP:conf/nsdi/SubramanianRSSK04}, Byzantine fault\ntolerance~\\cite{DBLP:conf/osdi/CastroL99,DBLP:journals/tocs/CastroL02},\n\\emph{streaming} set and multiset equality\ncomparison~\\cite{cathalo2009comparing}, and various aspects of database\nsecurity, such as access pattern privacy~\\cite{DBLP:conf/ccs/2008} and\nintegrity protection~\\cite{clarke2003incremental}.\n\nThis latter use case provides a simple example of how the primitive is\nused in practice: one can use homomorphic hashing to verify the integrity\nof a database with a transaction log, by computing a hash value for each\ntransaction in such a way that the hash of the complete\ndatabase state is equal to the (appropriately-defined) sum of the hashes\nof all transactions. Another observation~\\cite{bellare1997new} is\nthat homomorphic hashing can be used for incremental and parallel hashing\nof lists, arrays, strings and other similar data structures: for example,\nthe list $(b_1, \\ldots, b_n)$ can be represented as the set\n$\\Set*{(1,b_1), \\ldots, (n,b_n)}$, and it suffices to apply the\nhomomorphic hash function to that set.\n\n\\subsubsection*{Constructing homomorphic hash functions}\nA framework for constructing provably secure homomorphic hash functions\n(in some suitably\nidealized model, such as the random oracle model) was introduced by\nBellare and Micciancio~\\cite{bellare1997new}, and later extended to the multiset hash\nsetting by Clarke~{et al.}~\\cite{clarke2003incremental}, and revisited by\nCathalo~{et al.}~\\cite{cathalo2009comparing}.\n\nRoughly speaking, the framework of Bellare and Micciancio can be described as\nfollows. To construct a (multi)set homomorphic hash function on $A$, one\ncan start with a usual hash function $\\hat H$ from $A$ to some additive\ngroup $G$, and extend it to finite subsets of $A$ (resp.{} multisets on\n$A$) by setting $H(\\Set*{a_1,\\dots,a_n}) = \\hat H(a_1)+\\cdots+\\hat\nH(a_n)$ (resp.{} $H(\\Set*{a_1^{m_1},\\dots,a_n^{m_n}}) = m_1\\cdot \\hat\nH(a_1)+\\cdots+ m_n\\cdot\\hat H(a_n)$, where $m_i$ is the multiplicity of\n$a_i$).  And in fact, it is clear that all possible homomorphic hash\nfunctions arise in that way.  Note that as in Clarke~{et\nal.}~\\cite{clarke2003incremental}, and unlike the original framework of\nBellare and Miciancio~\\cite{bellare1997new}, there is no block index $i$ included in the hash $\\hat{H}(a_i)$ of each element $a_i$, because we are hashing unordered sets/multisets, rather than ordered sequences of blocks.\n\nAssume that the underlying hash function $\\hat H$ is ideal (i.e.{} it\nbehaves like a random oracle). Then we can ask when the corresponding\nhomomorphic hash function $H$ is secure (collision resistant, say). This\ntranslates to a knapsack-like number-theoretic assumption on the group\n$G$, which Bellare and Micciancio show holds, for example, when the\ndiscrete logarithm problem is hard in $G$.\n\nConcretely, Bellare and Micciancio and the authors of subsequent works\npropose a number of possible instantiations for $H$ which\nessentially amount to choosing $G = {\\mathbb{Z}}_p^\\times$ or $G = {\\mathbb{Z}}_m^n$ for\nsuitable parameters $p,m,n$. These concrete instantiations yield simple\nimplementations, but they all suffer from suboptimal output size (they\nrequire outputs of several thousand bits to achieve collision resistance\nat the $128$ security level), and their efficiency is generally\nunsatisfactory. Essentially all practical applications of homomorphic\nhashing in the security literature seem to focus on the case $G =\n{\\mathbb{Z}}_p^\\times$, called {\\texttt{{MuHash}}}.\n\n\\subsubsection*{Our contributions}\nWithin Bellare and Micciancio's framework, constructing a homomorphic\nhash function amounts to choosing a group $G$ where the appropriate\nnumber-theoretic assumption holds, together with a hash function to $G$\nwhose behavior is close enough to ideal for the security proof to go\nthrough.\n\nIn this paper, we propose a novel concrete construction of a multiset\nhash function by choosing $G$ as the group of points of a binary elliptic\ncurves, and picking the hash function following the approach of\nBrier~{et al.}~\\cite{brier2010efficient} (which we improve upon slightly)\napplied to the binary curve variant of Shallue and van de Woestijne's\nencoding function~\\cite{shallue2006construction}. We also describe a\nsoftware implementation of our proposal (building upon the work of\nAranha~{et al.}~\\cite{aranha2014binary} for binary curve hashing, and using\nBLAKE2~\\cite{aumasson2013blake2} as the actual underlying hash function)\nand provide extensive performance results showing that our function\noutperforms existing methods by a large margin on modern CPU\narchitectures (especially those supporting carry-less multiplication).\nFurthermore, choosing an elliptic curve (with small cofactor) for the\ngroup of hash values solves the ``output size'' problem of homomorphic\nhashing outright: $O(2^n)$ collision security is achieved with roughly\n$2n$-bit long digests. Yet, they do not seem to have been used in\nconcrete implementations of homomorphic hashing so far\\footnote{One can mention EECH~\\cite{brown2008encrypted} as\n    relevant related work that also uses binary curves for hashing, but\n    the authors didn't consider \\emph{homomorphic} hashing at all, and\ntheir functions seems poorly suited for that goal. See\nSection~\\ref{sec:discussion} for a more detailed discussion.}. One can\nwonder why; the most\nlikely explanation is that usual methods for hashing to elliptic\ncurves are far too inefficient to make curves attractive from a\nperformance standpoint: almost all such methods require at least one full\nsize exponentiation in the base field of the curve, which will be much\nmore costly by itself than the single multiplication (in a much larger\nfield) required by {\\texttt{{MuHash}}}---even on curves over fast prime fields at the\n128-bit security level~\\cite{aranha2014binary}, such an encoding function is over $3$ times slower than {\\texttt{{MuHash}}}{} at equivalent\nsecurity on Haswell, and over $20$ times slower than our construction.  Only by using binary curves and relatively\nsophisticated implementation techniques do we avoid that stumbling block\nand prove that elliptic curves can be competitive. As a result,\nwe achieve a processing speed of over 3 million set elements per second on a\n\\SI{4}{GHz} Intel Haswell CPU at the $128$-bit security level.\nSpeedups are expected with the release of Intel Broadwell processors and\nits improved implementation of carry-less multiplication.\n\n\\subsubsection*{Are binary elliptic curves safe?}\n\nRecently, new developments have been announced regarding the asymptotic\ncomplexity of the discrete logarithm problem on binary elliptic curves,\nparticularly by Semaev~\\cite{semaev2015}. These results are somewhat\ncontroversial, since they are based on heuristic assumptions that\nprevailing evidence suggests are unlikely to\nhold~\\cite{kosters2015,DBLP:conf/crypto/HuangKY15}, and\ntheir storage requirements appear to make them purely theoretical\nanyway~\\cite{galbraith2015ellipticnews}.\n\nHowever, if Semaev's claims of an $L[1/2]$ attack turn out to be correct,\nthe asymptotic security of binary elliptic curve-based ECMH would be\nreduced. The \\emph{concrete} security of our construction, on the other\nhand, would be completely unaffected on curves of up to 300+ bits (and in\nparticular at the 128-bit security level on GLS254), since the claimed\nattack is worse than generic attacks on such curves. Moreover, even if\n\\emph{actually practical} $L[1/2]$ attacks were to be found, ECMH on\nbinary curves is likely to remain attractive, since it mainly competes\nagainst {\\texttt{{MuHash}}}, which is vulnerable to an $L[1/3]$ subexponential\nattack.\n\nFor all these reasons, we believe that ECMH on binary elliptic curves is\na safe choice for security-minded practitioners, and that the switch from\n{\\texttt{{MuHash}}}{} to ECMH is entirely justified in view of the considerable\nperformance gain (which lets designers choose a higher security margin\nand still come out far ahead).\n\n\\section{Homomorphic Multiset Hash Function}\n\nFormally, we define a multiset $M \\in {\\mathbb{Z}}^{(A)}$ as a function with finite\nsupport mapping a base set $A$ to the integers ${\\mathbb{Z}}$.  As an extension of\nthe usual definition in which multiplicities are restricted to ${\\mathbb{Z}}_{\\geq\n0}$, we allow negative multiplicities as well.  We will implicitly consider\nsubsets $S \\subseteq A$ to be multisets in ${\\mathbb{Z}}^{(A)}$.\n\nClarke~{et al.}~\\cite{clarke2003incremental} introduce a definition of a multiset hash function that efficiently supports incrementally adding (multisets of) elements.  We give a simpler (but nearly equivalent\\footnote{We give a proof of equivalence (under a mild assumption) in~\\cref{sec:clarke-equivalence}.}) definition that makes the connection to homomorphic hash functions~\\cite{krohn2004fly} explicit:\n\n\\begin{definition}\n  Let $A$ be a set and let $(G, +_G)$ be a finite group.  A function $H\n\\colon {\\mathbb{Z}}^{(A)} \\rightarrow G$ that maps multisets over the base set $A$\nto a point in $G$ is said to be a \\emph{homomorphic multiset hash\nfunction} if $H$ is a group homomorphism from the pointwise-additive\ngroup of functions $({\\mathbb{Z}}^{(A)}, +)$ to $(G, +_G)$; equivalently,\n  $H(M_1 + M_2) = H(M_1) +_G H(M_2)$ for all $M_1, M_2 \\in {\\mathbb{Z}}^{(A)}$.  We define $\\hat{H} \\colon A \\rightarrow G$ by $\\hat{H}(a) = H(\\Set{a})$.\n  \\label{def:homomorphic-multiset-hash}\n\\end{definition}\n\nThis definition minimally captures an intuitive notion of a multiset hash\nfunction that supports incrementally adding and removing (multisets of)\nelements.  These incremental updates are efficient assuming that addition\nand negation in $G$ can be performed efficiently and $H(M)$ can be\ncomputed efficiently (e.g.\\ in time linear in the representation length\nof the non-zero values of $M$).  Note that since pointwise addition in\n${\\mathbb{Z}}^{(A)}$ is commutative, the relevant subgroup $H({\\mathbb{Z}}^{(A)}) \\leq G$ is necessarily commutative, and therefore without loss of generality we can assume that $G$ is commutative.  It may seem that it is too strong of an assumption to require a group structure on $G$, or equivalently, that (multisets of) elements can be removed as well as added.  In fact, provided that $+_G$ is lossless, in that $a +_G b = a +_G c$ implies $b = c$, there is no loss of generality.  We show in \\cref{sec:monoid-to-group} that we can construct a group that supports (efficient) incremental removals based only on (efficient) incremental additions.\n\nSince the set of singleton subsets of $A$ generates the group $({\\mathbb{Z}}^{(A)}, +)$, $H$ can conversely be uniquely defined by $\\hat{H}$:\n", "index": 1, "text": "\n\\[ H(M) = \\sum_{a \\in A} M(a) \\cdot \\hat{H}(a). \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"H(M)=\\sum_{a\\in A}M(a)\\cdot\\hat{H}(a).\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>a</mi><mo>\u2208</mo><mi>A</mi></mrow></munder><mrow><mrow><mrow><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nwhere $a, b \\in {\\mathbb{F}}_{2^m}$.  It constructs three values of $x$ from $w$ with the property that at least one necessarily has a corresponding value $y$ satisfying \\cref{eq:elliptic-curve}.  In addition to the usual arithmetic operations over ${\\mathbb{F}}_{2^m}$, its definition depends on three \\emph{linear} maps:\n\\begin{enumerate}\n\\item the \\emph{trace} function $\\mathrm{Tr} \\colon {\\mathbb{F}}_{2^m} \\rightarrow {\\mathbb{F}}_2$ defined by $\\mathrm{Tr}(x) = \\sum_{i=0}^{m-1} x^{2^i}$;~\\cite[p.~130]{hankerson2004guide}\n\\item a \\emph{quadratic solver} function $\\mathrm{QS} \\colon \\Set{ x \\in {\\mathbb{F}}_{2^m} \\given \\mathrm{Tr}(x) = 0 } \\rightarrow {\\mathbb{F}}_{2^m}$ that satisfies $\\mathrm{QS}(x)^2 + \\mathrm{QS}(x) = x$ and $\\mathrm{QS}(0) = 0$;\n\\item $\\mathrm{coeff}_0 \\colon {\\mathbb{F}}_{2^m} \\rightarrow {\\mathbb{F}}_2$ where $\\mathrm{coeff}_0(x)$ is the zeroth coefficient of any (fixed) polynomial representation of $x$.\n\\end{enumerate}\n\nAn optimized version of the algorithm that requires only a single field inversion~\\cite{aranha2014binary} is shown as \\cref{alg:sw-encode}.  The algorithm is parameterized by a value $t \\in {\\mathbb{F}}_{2^m}$ satisfying $t^4 + t \\not= 0$; for fields of degree $m > 4$, we can choose $t = z$ where $z$ is the indeterminate in the polynomial representation of ${\\mathbb{F}}_{2^m}$.  The result $(x, \\lambda) = (x, x + y / x)$ is represented in $\\lambda$-affine coordinates~\\cite{oliveira2014two} for efficiency.\\footnote{There is exactly one point with $x = 0$ satisfying \\cref{eq:elliptic-curve}: $(x = 0, y = \\sqrt{b})$.  When using $\\lambda$-affine coordinates, this value must be represented specially.}  The addition to $\\lambda$ of $\\mathrm{coeff}_0(w)$ in \\cref{alg:sw-encode:coeff-line} is not part of the original SW algorithm; this trivial addition serves to halve the number of collisions at essentially no extra cost.\n\n\\begin{algorithm*}\n  \\caption{Optimized Shallue--van de Woestijne encoding in characteristic 2~\\cite{aranha2014binary}}\n  \\label{alg:sw-encode}\n  \\begin{algorithmic}[1]\n    \\Require $t \\in {\\mathbb{F}}_{2^m}$ such that $t \\cdot (t + 1) \\cdot (t^2 + t + 1) = t^4 + t \\not= 0$\n    \\item[\\textbf{Precompute:}] $t_1 = \\frac{t}{t^2 + t + 1}$, $t_2 = \\frac{1 + t}{t^2 + t + 1}, t_3 = \\frac{t \\cdot (1 + t)}{t^2 + t + 1}; t_j^{-1} = 1 / t_j$ for $j = 1, 2, 3$\n    \\Function{SWChar2}{$w \\in {\\mathbb{F}}_{2^m}$ ; $a, b \\in {\\mathbb{F}}_{2^m}$}\n      \\State $c\\gets w^2 + w + a$\n      \\If{$c = 0$} \\Comment{This condition may hold only if $\\mathrm{Tr}(a) = 0$}\n        \\State \\Return $(x = 0, y = \\sqrt{b})$ \\Comment{This is the single point satisfying \\cref{eq:elliptic-curve} with $x = 0$}\n      \\EndIf\n      \\State $c^{-1} \\gets 1 / c$\n      \\For{$j = 1$ to $3$}\n        \\State $x \\gets t_j \\cdot c$\n        \\State $x^{-1} \\gets t_j^{-1} \\cdot c^{-1}$\n        \\State $h_j \\gets (x^{-1})^2 \\cdot b + x + a$\n        \\If{$\\mathrm{Tr}(h_j) = 0$} \\Comment{This condition necessarily holds for at least one $j$}\n          \\State $\\lambda \\gets \\mathrm{QS}(h_j) + x + \\mathrm{coeff}_0(w)$  \\Comment{$c$ does not depend on $\\mathrm{coeff}_0(w)$}\n          \\label{alg:sw-encode:coeff-line}\n          \\State \\Return $(x, \\lambda)$ \\Comment{$y = (\\lambda + x) \\cdot x = \\mathrm{QS}(h_j) \\cdot x$}\n        \\EndIf\n      \\EndFor\n    \\EndFunction\n  \\end{algorithmic}\n\\end{algorithm*}\n\nIt is clear from the definition that the number of preimages of any point\n$(x, \\lambda)$ under \\textproc{SWChar2} is at most $\\alpha = 3$, since $c\n\\in \\Set{ t_j^{-1} \\cdot x \\given j = 1, 2, 3 }$ and $w$ is uniquely\ndetermined from $c$, $x$, and $\\lambda$ by\n\n", "itemtype": "equation", "pos": 27332, "prevtext": "\nIndeed, this is precisely the \\emph{randomize-then-combine paradigm}\nproposed by Bellare and Micciancio~\\cite{bellare1997new} for incremental\nhashing of messages, which is readily (in fact, more naturally than to\nmessage hashing) applied by Clarke~{et al.}~\\cite{clarke2003incremental} to multiset hashing.  Our goal is to minimize the computational cost for computing $H$ and the representation size for elements of $G$ while achieving a given level of collision resistance.\n\n\\subsubsection*{Collision resistance}\n\nA \\emph{collision} for a hash function $H$ is a pair $x, x'$ such that $x \\not= x'$ but $H(x) = H(x')$.  For any group-homomorphic hash function $H$ from a group $(X, +)$ to $(G, +)$, a collision can equivalently be defined as a value $x \\in \\ker H \\setminus \\Set{0_X}$.   By the birthday bound that applies to any hash function, a collision can be found with at most expected $O(\\sqrt{{\\lvert {G} \\rvert}})$ hash computations; we can hope to design a multiset hash function for which expected time $\\Omega(\\sqrt{{\\lvert {G} \\rvert}})$ is also a lower bound.\\footnote{In this and the other collision bounds that follow, it is assumed that the expectations are taken over a random choice of hash function $H$ and group $(G, +_G)$ from some hash function family (distribution) $\\mathcal{H}$.}\n\nA \\emph{preimage attack} seeks to invert the hash function, namely to find a value $x$ such that $H(x) = y$, for a random element $y$ in the image of $H$.  We can hope to design a multiset hash function for which the expected time complexity of the best preimage attack is also equal to the generic upper bound $\\Omega({\\lvert {G} \\rvert})$.  Note that for a homomorphic hash function we do not consider preimage attacks on the identity element $0_G$, since its preimage is fixed.\n\nA \\emph{second preimage attack} seeks to find a value $x'$ such that $H(x') = H(x)$, for some known value $x$.  Since a second preimage implies a collision, the time complexity of a second preimage attack is lower bounded by the time complexity of the best  collision attack, ideally $\\Omega(\\sqrt{{\\lvert {G} \\rvert}})$.  For a general, non-homomorphic hash function,  we can hope that the best attack has expected $\\Omega({\\lvert {G} \\rvert})$ time complexity.  For any homomorphic hash function, however, the group structure implies that a second preimage attack is no harder than a collision attack (with expected time complexity upper-bounded by $O(\\sqrt{{\\lvert {G} \\rvert}})$).\n\n\n\\section{Generic multiset hash families}\n\nA random oracle $\\hat{H} \\colon A \\rightarrow G$ clearly achieves the\noptimal preimage resistance of $\\Theta({\\lvert {G} \\rvert})$ and the optimal\ncollision resistance of $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$, in the sense that at\nleast this many oracle queries are needed to compute preimages and\ncollisions respectively.\n\nIt does not follow, however, that the associated multiset hash function\n$H = H_G\\colon {\\mathbb{Z}}^{(A)} \\rightarrow G$ has the same security level; for\nexample, if we choose $G={\\mathbb{Z}}_2^n$, then $O(n)$ oracle queries, instead of\n$\\Omega(2^n)$, are enough to find arbitrary preimages in polynomial time\nby solving a simple $n\\times n$ linear system over ${\\mathbb{Z}}_2$. However,\nBellare and Micciancio~\\cite{bellare1997new} have shown (in the set hash setting, but\nthis generalizes naturally to multisets) how to obtain a security\nreduction for $H_G$ based on a computational hardness assumption on the\ngroup $G$. For concrete choices of $G$, that hardness assumption is\nrelated to standard number theoretic problems, such as the discrete\nlogarithm problem or modular knapsacks.\n\nWhen $G={\\mathbb{Z}}_p^\\times$, the resulting multiset hash function $H_G$ is\nessentially {\\texttt{{MSet-Mu-Hash}}}{}~\\cite{clarke2003incremental}, the multiset\nvariant of {\\texttt{{MuHash}}}{}~\\cite{bellare1997new}. When $G={\\mathbb{Z}}_m^n$, we\nessentially obtain {\\texttt{{MSet-VAdd-Hash}}}{}~\\cite{clarke2003incremental}, the\nmultiset variant of {\\texttt{{LtHash}}}{} (for $n > 1$) or {\\texttt{{AdHash}}}{} (for $n =\n1$)~\\cite{bellare1997new}. These functions all have security reductions\nin the framework sketched above.\n\nIt is relatively easy to find plausible concrete instantiations of the\nrandom oracle $\\hat H$ to a group like ${\\mathbb{Z}}_2^n$, but for more general groups, this is\nusually more complicated, and as a result it is often convenient to\nreplace $\\hat H$ by a \\emph{pseudo-random oracle}, i.e.\\ a construction\nthat is indifferentiable from a random oracle in the sense of\nMaurer~{et al.}~\\cite{maurer2004indifferentiability}. Typically, we can take\n$\\hat H$ of the form $\\hat H(a) = f\\big(h(a)\\big)$ where $h\\colon A\\to X$\nis a random oracle to some intermediate set $X$ (such as bit strings, so\nthat we can plausibly instantiate it with standard hash function\nconstructions like SHA-2\\footnote{We will assume that elements of $A$ can\nbe readily encoded as octet strings.}) and $f\\colon X \\rightarrow G$ is\nan \\emph{admissible encoding\nfunction}~\\cite{boneh2001identity,brier2010efficient} that has the\nproperty of mapping the uniform distribution over $X$ to a distribution\nindistinguishable from uniform over $G$.\n\n\\subsubsection*{Security bounds}\n\\label{sec:generalized-multiset-hash-security-bounds}\n\n{\\texttt{{AdHash}}}{} is appealing for its simplicity, but is far from optimal in terms of hash code size.  In the \\emph{set} hashing setting (i.e.\\ $M(a) \\in \\Set{0,1}$), the best known attack is the generalized birthday attack~\\cite{wagner2002generalized}; under the assumption that this attack is optimal, the group ${\\mathbb{Z}}_{2^n}$ corresponds to a security level of roughly $2 \\sqrt{n}$ bits.  In the \\emph{multiset} hashing setting, {\\texttt{{AdHash}}}{} is completely impractical due to the extremely large hash code sizes $n$ required to defeat lattice reduction attacks described in \\cref{sec:adhash-attack}.\n\nThere are reductions from computing discrete logarithms in a group $G$ to\nfinding collisions in the corresponding random oracle multiset hash\nfunction\n$H_{G}$~\\cite{impagliazzo1996efficient,bellare1997new,clarke2003incremental}.\nThese reductions can be used to prove a collision resistance property for\nthe generic multiset hash family over any group in which computing\ndiscrete logarithms is hard, such as ${\\mathbb{Z}}_p^\\times$.  However, because\ndiscrete logarithms in ${\\mathbb{Z}}_p^\\times$  can be solved by e.g.\\ the Number\nField Sieve with (heuristic) subexponential time complexity $L_p\\big[1/3,\n\\sqrt[3]{64/9}\\big]$~\\cite[p.~128]{menezes2010handbook}, it is usually\nestimated that we need to choose $p$ of around $3200$ bits for $128$-bit\nsecurity (see for example the evaluation of the ECRYPT~II report on key\nsizes~\\cite{smart2010ecryptkeysizes}).  In contrast, in a generic group, discrete logarithms\ncannot be computed faster than expected time $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$,\nwhich is also the optimal collision resistance.\n\n\n\n \n\n\n\n\n\\section{Elliptic Curve Multiset Hash}\n\\label{sec:elliptic-curve-multiset-hash}\n\nFor properly chosen elliptic curves over finite fields,\nthere are no known algorithms for solving the discrete logarithm problem in the elliptic curve group faster than in a generic group, i.e.\\ expected time $\\Theta(\\sqrt{{\\lvert {G} \\rvert}})$.  Therefore, there is a clear possibility for using an elliptic curve group to obtain a given level of collision resistance with a much lower group size than with {\\texttt{{MSet-Mu-Hash}}}{}.\n\n\n\n\n\nApplying the generic multiset hash construction to elliptic curve groups\npresents a problem, however: while it is easy to define a very efficient\nadmissible encoding from $\\{0,1\\}^k$ to ${\\mathbb{Z}}_p^\\times$ for sufficiently\nlarge $k$, an admissible encoding to an elliptic curve group is not so\neasily defined.  While constructions for admissible encoding functions\nhave been demonstrated~\\cite{brier2010efficient}, their computational\ncost is higher than we would like.\n\n\\subsection{Generalized discrete logarithm security reduction}\n\nIn fact, we can significantly relax the requirement on the encoding\nfunction $f$ and still obtain a very tight reduction, due to random\nself-reducibility of the discrete logarithm problem.  Our relaxed\nrequirement is related to the definition of $\\alpha$-weak encodings by\nBrier~{et al.}~\\cite{brier2010efficient}, and is satisfied in practice by a large class of encoding functions~\\cite{brier2010efficient}.\n\\begin{definition}\n  A function $f \\colon S \\rightarrow R$ between finite sets is said to be an $(\\alpha, \\beta)$-weak encoding, for integer $\\alpha \\geq 1$ and real value $\\beta \\geq 1$, if it satisfies the following properties:\n\\begin{enumerate}\n\\item \\emph{Samplable}: there is an efficient randomized algorithm for computing ${\\lvert {f^{-1}(r)} \\rvert}$ and sampling uniformly from $f^{-1}(r)$ for any $r \\in R$.\n\\item ${\\lvert {f^{-1}(r)} \\rvert} \\leq \\alpha$ for all $r \\in R$.\n\\item ${\\rm I\\kern-.3em E}_r[ {\\lvert { f^{-1}(r) } \\rvert} / \\alpha ] \\geq 1 / \\beta$.\n\\end{enumerate}\n\\end{definition}\nAn $(\\alpha, \\beta)$-weak encoding function $f$ allows us to efficiently\nsample $s \\in S$ uniformly at random using $\\beta$ uniform samples $r \\in\nR$ in expectation, with the property that $f(s) = r$ for any accepted\nsample $s$ obtained from $r$.\\footnote{Under the definition of Brier~{et\nal.}~\\cite{brier2010efficient}, an $\\alpha$-weak encoding $f$ is an $(\\alpha {\\lvert {S} \\rvert} /  {\\lvert {R} \\rvert}, \\alpha^2 {\\lvert {S} \\rvert} / {\\lvert {R} \\rvert})$-weak encoding.  Our definition allows for a tighter bound to be given in \\cref{thm:dlpreduction}.}\n\n\\begin{definition}\n\\label{def:alpha-beta-weak-multiset-hash-family}\nLet $f \\colon X \\rightarrow G$ be an $(\\alpha, \\beta)$-weak encoding from\n$X$ to the abelian group $G$. Assume that $G$ admits as a direct factor a cyclic\nsubgroup $\\langle g \\rangle$ of prime order\n$\\rho$, and that we can efficiently sample from the\ncomplement group $\\overline{\\langle g \\rangle}$ in the direct factor\ndecomposition $H=\\langle g\\rangle \\oplus \\overline{\\langle g \\rangle}$. Given a random oracle\n$h\\colon A\\to X$, we denote by $\\hat H_f$ the function $A\\to G$ given by $\\hat\nH_f(a) = f\\big(h(a)\\big)$, and by $H_f\\colon {\\mathbb{Z}}^{(A)}\\to G$ the\nassociated multiset hash function.\n\\end{definition}\n\nThe following theorem shows that finding a collision in $H_f$\nwith multiplicities up to $\\rho - 1$ is as hard as computing discrete\nlogarithms to the base $g$, up to a small factor that depends on $\\beta$.\nNote that $H_f$ does not depend on the choice of subgroup\n$\\langle g \\rangle$, but the strongest security result is obtained by\nchoosing the largest prime-order subgroup.  The requirement of efficient\nsamplability of $\\overline{\\langle g \\rangle}$ is easily satisfied in\npractice, since efficiency concerns regarding representation size dictate\nthat $\\overline{\\langle g \\rangle}$ be as small as possible (usually\nhaving at most $8$ elements, and most of the time only $1$ or $2$).\n\n\\begin{restatable}{theorem}{dlpreduction}\n\\label{thm:dlpreduction}\nLet $H_f$ be a multiset hash function as in\n\\cref{def:alpha-beta-weak-multiset-hash-family}.  Given an algorithm\n$\\mathcal{C}$ with access to the underlying random oracle $h$ that finds\na non-empty multiset $M \\in \\ker H_f$ with $\\lvert M \\rvert_\\infty <\n\\rho$, in expected time $t'$ with probability $\\epsilon'$\nusing $q$ queries to $h$, discrete logarithms to the\nbase $g$ can be computed with probability $\\epsilon = \\epsilon'/2$ in\nexpected time $t + T_1 + q T_2 + q \\beta T_3 + L T_4$, where $L \\geq\n\\lvert M \\rvert_0$ is a bound on the length of the output of\n$\\mathcal{C}$; $T_1, \\dotsc, T_4$ denote the time required for a constant\nnumber of group operations, and are given in the proof.\n\\end{restatable}\n\\begin{proof}\nSee Appendix~\\ref{sec:proofthmdlpreduction}.\n\\end{proof}\n\nConcretely, if $G = E({\\mathbb{F}}_{p^m})$ is the group of ${\\mathbb{F}}_{p^m}$-rational\npoints on a suitable elliptic curve $E$ chosen to avoid any discrete\nlogarithm weaknesses, with a subgroup $\\langle g \\rangle$ of prime order\n$\\rho \\geq {\\lvert {G} \\rvert}/4$, and $f$ is an $(\\alpha, \\beta)$-weak encoding\nfunction with small constant $\\beta$, then $H_f$ has collision\nresistance roughly $p^{m/2}/2$.  Since an element of $E({\\mathbb{F}}_{p^m})$ can be\nrepresented using $\\lceil \\log_2 p^m \\rceil$ bits, the collision\nresistance of $H_f$ is essentially optimal (to within a few\nbits).\n\n\n\\subsection{Shallue-van de Woestijne (SW) encoding in characteristic 2}\n\nThe Shallue-van de Woestijne (SW) algorithm for characteristic 2 fields~\\cite{shallue2006construction} can be used to map any point $w \\in {\\mathbb{F}}_{2^m}$ to a pair $(x,y) \\in \\left({\\mathbb{F}}_{2^n}\\right)^2$ satisfying an arbitrary elliptic curve equation\n\n", "index": 3, "text": "\\begin{equation}\n  E_{a,b} \\colon y^2 + x \\cdot y = x^3 + a \\cdot x^2 + b,\n  \\label{eq:elliptic-curve}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"E_{a,b}\\colon y^{2}+x\\cdot y=x^{3}+a\\cdot x^{2}+b,\" display=\"block\"><mrow><mrow><msub><mi>E</mi><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>:</mo><mrow><mrow><msup><mi>y</mi><mn>2</mn></msup><mo>+</mo><mrow><mi>x</mi><mo>\u22c5</mo><mi>y</mi></mrow></mrow><mo>=</mo><mrow><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mrow><mi>a</mi><mo>\u22c5</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><mo>+</mo><mi>b</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\nThe preimage set for any point $(x, \\lambda)$ can be efficiently computed\nby these same formulas.  Furthermore, Aranha~{et al.}~\\cite{aranha2014binary}\nshow that the proportion of curve points with $k$ preimages under $\\textproc{SWChar2}$ for $k =\n0,1,2,3$ is $9/32$, $15/32$, $7/32$, and $1/32$, respectively, up to an\nerror term of $O(2^{-n/2})$.  It follows that\n${\\rm I\\kern-.3em E}_P[{\\lvert {\\textproc{SWChar2}^{-1}(P)} \\rvert} / \\alpha] = 1/3 \\pm\nO(2^{-n/2})$, and therefore, \\textsc{SWChar2} is an $(\\alpha,\n\\beta)$-weak encoding with $\\beta = 3 + O(2^{-n/2})$.\n\n\\subsection{Hash function definition}\n\nBased on this encoding function, we define the \\emph{elliptic curve multiset hash\n(ECMH)}: given a binary elliptic curve group $E_{a,b}({\\mathbb{F}}_{2^m})$ and an\nintermediate hash function $h \\colon A \\to {\\mathbb{Z}}_{2^m}$ (modeled as a random\noracle), we define $\\mathrm{ECMH}_{a,b,h}(x) = \\textsc{SWChar2}(h(x); a, b)$.  Commonly used elliptic curves over ${\\mathbb{F}}_{2^m}$, including the NIST-recommended ones,\n\nhave a generator of prime order $\\rho > 2^{m-2}$ with an easily determined complement group of size $h = {\\lvert {\\overline{\\langle g \\rangle}} \\rvert} \\leq 4$.  Thus, the samplability requirement on $\\overline{\\langle g \\rangle}$ is easily satisfied in practice.  Hence, by \\cref{thm:dlpreduction}, finding a collision in ECMH is as hard (up to a small constant factor) as computing discrete logarithms to the base $g$, which we assume to be $O(2^{m/2})$.\n\nSimilar suitable encoding algorithms exist for elliptic curves over fields of characteristic $p > 2$~\\cite{shallue2006construction,brier2010efficient}, and could also be used to define an elliptic curve multiset hash.  However, the use of a characteristic 2 field eliminates the need for an expensive field exponentiation in order to solve a quadratic equation, which would otherwise dominate the computation time, and on modern CPUs that support fast carry-less multiplication, fast implementations of all other required field operations are also possible for characteristic $2$~\\cite{taverne2011jcen}.\n\n\\subsection{Compressed representation of curve points}\n\\label{sec:compressed-point-representation}\n\nThe group of ${\\mathbb{F}}_{2^m}$-rational points on an elliptic curve $E_{a,b}$ has order ${\\lvert {E_{a,b}({\\mathbb{F}}_{2^m})} \\rvert} \\approx 2^m$.  Each point is naturally represented as a pair $(x,y) \\in {\\mathbb{F}}_{2^m}^2$ (or $(x, \\lambda) \\in {\\mathbb{F}}_{2^m}^2$), but there is a well-known method for encoding a point using just $m + 1$ bits: given $x$ there are at most two possible values for $y$ (or $\\lambda$) if $(x,y)$ (or $(x,\\lambda)$) satisfy $E_{a,b}$, and they can be recovered efficiently using a small number of field operations.  Thus, a point can be encoded by its $x$ value and a single additional bit to disambiguate the two possible points.  The elliptic curve group identity element (the point at infinity) can be encoded specially without increasing the representation size, by using a bit sequence that would not otherwise encode a valid point.\n\n\n\\section{Implementation}\n\nWe developed an optimized implementation of elliptic curve multiset hash (ECMH) as an open-source C++ library~\\cite{maitinshepard2015ecmhLibrary},\n\nwith support for all NIST-recommended binary elliptic curves~\\cite{fips186-4} and the record-breaking GLS254 curve~\\cite{oliveira2014two}, as well as several other SEC~2-recommended curves~\\cite{sec2-1}.  Using a combination of C++ templates and code generation, we were able to write generic code to support many different configurations without sacrificing runtime performance; only for modular reduction was a custom implementation required for each supported field.  We incorporated existing fast x86/x86-64 polynomial multiplication, squaring, and modular reduction routines for ${\\mathbb{F}}_{2^{163}}$, ${\\mathbb{F}}_{2^{193}}$, ${\\mathbb{F}}_{2^{233}}$, ${\\mathbb{F}}_{2^{239}}$, ${\\mathbb{F}}_{2^{283}}$, ${\\mathbb{F}}_{2^{409}}$, ${\\mathbb{F}}_{2^{571}}$~\\cite{bluhm2013fast} and for ${\\mathbb{F}}_{2^{127}}$~\\cite{oliveira2014two}.\n\nWe implemented field inversion using a polynomial-basis Itoh--Tsujii inversion method making use of multi-squaring tables~\\cite{guajardo2002itoh,taverne2011jcen,oliveira2014two,aranha2014binary}.  We generated field inversion routines for each field degree automatically based on an A\\textsuperscript{*} search procedure for computing the optimal Itoh--Tsujii addition chain and set of multi-squaring tables, based on a machine-specific cost model estimated from field operation performance measurements~\\cite{cryptoeprint:2015:028}.\n\nWe also developed optimized implementations of the {\\texttt{{MSet-Mu-Hash}}}{} and\n{\\texttt{{MSet-Add-Hash}}}{} hash functions, based on the modular arithmetic functions in the OpenSSL library version 1.0.1i, for the purpose of comparison.\n\n\\subsection{Intermediate hash function}\n\nECMH requires an intermediate hash function $h \\colon A \\to {\\mathbb{Z}}_{2^m}$.\nUnder our assumption that the base set $A$ is the set of octet strings,\nwe simply require a standard cryptographic hash function (modeled as a\nrandom oracle) with output size $m$.  Given the inherent property of any homomorphic hash function that a single collision leads to arbitrary second preimages, we advise using a keyed hash function when possible to minimize risk.\n\nAny standard hash function with fixed output size greater than $m$ bits can simply be truncated to $m$ bits.  Standard expansion techniques can be used to efficiently generate an arbitrary length $m$ output from a hash function with fixed output size $b < m$.  Sponge constructions, such as Keccak~\\cite{bertoni2009keccak}, are particularly convenient since they support arbitrary output sizes.\n\nBoth {\\texttt{{AdHash}}}{} and {\\texttt{{MuHash}}}{} similarly require intermediate hash functions, but with much larger output sizes $m$ for equivalent security levels.\n\nWe designed our implementation to support arbitrary hash functions, but for our performance evaluation, we selected BLAKE2~\\cite{aumasson2013blake2} because of its state-of-the-art performance.  For $m \\leq 256$, we used the BLAKE2s variant (256-bit output), truncating the output to $m$ bits.  For $256 < m \\leq 512$, we used the BLAKE2b variant (512-bit output) with truncation.  For $m > 512$, we used BLAKE2b repeatedly to generate sufficient output, in such a way that the underlying compression function is called a minimum number of times.\n\n\\subsection{Linear field operations}\n\\label{sec:impl:linear-field-operations}\n\nSeveral key operations for ${\\mathbb{F}}_{2^m}$, such as squaring, multi-squaring ($x \\mapsto x^{2^i}$), square root, and half-trace, are linear in the coefficients.  For multi-squaring (useful for inversion) and half-trace, an implementation based on a lookup table can be significantly faster than direct computation~\\cite{bos2010ecc2k,taverne2011jcen,oliveira2014two,aranha2014binary}.  The coefficients are split into $\\lceil m / \\beta \\rceil$ blocks of $\\beta$ bits, and a separate table of $2^{\\beta}$ entries is precomputed for each block position, using a total of ${s_{{m,\\beta}}} = \\lceil m / \\beta \\rceil \\cdot 2^{\\beta} \\cdot \\lceil m / W \\rceil \\cdot W / 8$ bytes of memory, where $W$ is the word size in bits.  The linear transform can then be computed from the precomputed tables with $k = \\lceil m / \\beta \\cdot \\lceil m / W \\rceil$ memory accesses and $k - 1$ XOR operations.\n\n\\subsection{Blinding for side-channel resistance}\n\nThe fastest implementation of ECMH is susceptible to timing and cache side-channel attacks, due to the use of lookup tables (for inversion and QS), and the use of branching (for \\textsc{SWChar2}).  A branch-free implementation of \\textsc{SWChar2} adds only a few additional multiplications and squarings.  Lookup tables are unavoidable for good performance, but we can blind inversion at a cost of just two multiplications and generation of one random field element.  We likewise can blind QS at a cost of 1 squaring, 2 additions, and generation of one random field element, as well as a few bit operations to ensure the random element is in the image of QS.  In this way we can fully protect against timing and cache side-channel attacks at only a small additional cost.\n\n\\subsection{Quadratic extension field}\n\nFor even $m$, representing ${\\mathbb{F}}_{2^m}$ as a quadratic extension of ${\\mathbb{F}}_{2^{m/2}}$ results in significantly faster field operations relative to an odd-degree field of roughly the same size: inversion in the extension field requires only one inversion in the base field (effectively reducing the memory and computation costs by nearly a factor of 4 for a table-based multi-squaring implementation), and half-trace requires only 2 half-trace computations in the base field (reducing, for a table-based implementation, the computation cost by a factor of 2 and the memory requirement by a factor of 4)~\\cite{oliveira2014two}.  We use this representation to support the GLS254 elliptic curve over ${\\mathbb{F}}_{2^{254}}$~\\cite{oliveira2014two}.\n\n\n\n\\subsection{In-memory representation of elliptic curve points}\n\nAlthough an element in the elliptic curve group of points $E_{a,b}({\\mathbb{F}}_{2^m})$ can be represented directly using the standard affine $(x,y)$-representation or the $\\lambda$-affine $(x,\\lambda)$ representation, and more compactly using just $m+1$ bits as described in \\cref{sec:compressed-point-representation}, we can more efficiently perform group operations using the $\\lambda$-projective representation $(\\tilde{x},\\tilde{\\lambda},z)$ corresponding to the $\\lambda$-affine representation $(x = \\tilde{x}/z, \\lambda = \\tilde{\\lambda}/z)$: This representation allows point addition and point doubling to be performed without any field inversions~\\cite{oliveira2014two}.\n\n\n\n\\subsection{Batch \\textsc{SWChar2} computation}\n\\label{sec:batch-swchar2}\nA large fraction of the computational cost of our elliptic curve multiset hash construction is due to the single field inversion required by the \\textsc{SWChar2} encoding function.  Using Montgomery's trick, $n$ independent elements can be inverted simultaneously at the cost of just 1 field inversion and $3(n-1)$ field multiplications~\\cite{shacham2001improving}.  Since field inversion is much more than 3 times as expensive as field multiplication, this provides significant computational savings.\n\n\n\n\n\\subsection{Montgomery domain for {\\texttt{{MSet-Mu-Hash}}}{}}\n\\label{sec:muhash-montgomery}\nA key cost in a na\\\"{\\i}ve implementation of {\\texttt{{MSet-Mu-Hash}}}{} is the reduction modulo $p$ required by multiplication in ${\\mathbb{Z}}_p^\\times$.  To avoid this cost, we can use the \\emph{Montgomery reduction}~\\cite{montgomery1985modular} defined by\n\\ifcompj\n\n", "itemtype": "equation", "pos": 31065, "prevtext": "\nwhere $a, b \\in {\\mathbb{F}}_{2^m}$.  It constructs three values of $x$ from $w$ with the property that at least one necessarily has a corresponding value $y$ satisfying \\cref{eq:elliptic-curve}.  In addition to the usual arithmetic operations over ${\\mathbb{F}}_{2^m}$, its definition depends on three \\emph{linear} maps:\n\\begin{enumerate}\n\\item the \\emph{trace} function $\\mathrm{Tr} \\colon {\\mathbb{F}}_{2^m} \\rightarrow {\\mathbb{F}}_2$ defined by $\\mathrm{Tr}(x) = \\sum_{i=0}^{m-1} x^{2^i}$;~\\cite[p.~130]{hankerson2004guide}\n\\item a \\emph{quadratic solver} function $\\mathrm{QS} \\colon \\Set{ x \\in {\\mathbb{F}}_{2^m} \\given \\mathrm{Tr}(x) = 0 } \\rightarrow {\\mathbb{F}}_{2^m}$ that satisfies $\\mathrm{QS}(x)^2 + \\mathrm{QS}(x) = x$ and $\\mathrm{QS}(0) = 0$;\n\\item $\\mathrm{coeff}_0 \\colon {\\mathbb{F}}_{2^m} \\rightarrow {\\mathbb{F}}_2$ where $\\mathrm{coeff}_0(x)$ is the zeroth coefficient of any (fixed) polynomial representation of $x$.\n\\end{enumerate}\n\nAn optimized version of the algorithm that requires only a single field inversion~\\cite{aranha2014binary} is shown as \\cref{alg:sw-encode}.  The algorithm is parameterized by a value $t \\in {\\mathbb{F}}_{2^m}$ satisfying $t^4 + t \\not= 0$; for fields of degree $m > 4$, we can choose $t = z$ where $z$ is the indeterminate in the polynomial representation of ${\\mathbb{F}}_{2^m}$.  The result $(x, \\lambda) = (x, x + y / x)$ is represented in $\\lambda$-affine coordinates~\\cite{oliveira2014two} for efficiency.\\footnote{There is exactly one point with $x = 0$ satisfying \\cref{eq:elliptic-curve}: $(x = 0, y = \\sqrt{b})$.  When using $\\lambda$-affine coordinates, this value must be represented specially.}  The addition to $\\lambda$ of $\\mathrm{coeff}_0(w)$ in \\cref{alg:sw-encode:coeff-line} is not part of the original SW algorithm; this trivial addition serves to halve the number of collisions at essentially no extra cost.\n\n\\begin{algorithm*}\n  \\caption{Optimized Shallue--van de Woestijne encoding in characteristic 2~\\cite{aranha2014binary}}\n  \\label{alg:sw-encode}\n  \\begin{algorithmic}[1]\n    \\Require $t \\in {\\mathbb{F}}_{2^m}$ such that $t \\cdot (t + 1) \\cdot (t^2 + t + 1) = t^4 + t \\not= 0$\n    \\item[\\textbf{Precompute:}] $t_1 = \\frac{t}{t^2 + t + 1}$, $t_2 = \\frac{1 + t}{t^2 + t + 1}, t_3 = \\frac{t \\cdot (1 + t)}{t^2 + t + 1}; t_j^{-1} = 1 / t_j$ for $j = 1, 2, 3$\n    \\Function{SWChar2}{$w \\in {\\mathbb{F}}_{2^m}$ ; $a, b \\in {\\mathbb{F}}_{2^m}$}\n      \\State $c\\gets w^2 + w + a$\n      \\If{$c = 0$} \\Comment{This condition may hold only if $\\mathrm{Tr}(a) = 0$}\n        \\State \\Return $(x = 0, y = \\sqrt{b})$ \\Comment{This is the single point satisfying \\cref{eq:elliptic-curve} with $x = 0$}\n      \\EndIf\n      \\State $c^{-1} \\gets 1 / c$\n      \\For{$j = 1$ to $3$}\n        \\State $x \\gets t_j \\cdot c$\n        \\State $x^{-1} \\gets t_j^{-1} \\cdot c^{-1}$\n        \\State $h_j \\gets (x^{-1})^2 \\cdot b + x + a$\n        \\If{$\\mathrm{Tr}(h_j) = 0$} \\Comment{This condition necessarily holds for at least one $j$}\n          \\State $\\lambda \\gets \\mathrm{QS}(h_j) + x + \\mathrm{coeff}_0(w)$  \\Comment{$c$ does not depend on $\\mathrm{coeff}_0(w)$}\n          \\label{alg:sw-encode:coeff-line}\n          \\State \\Return $(x, \\lambda)$ \\Comment{$y = (\\lambda + x) \\cdot x = \\mathrm{QS}(h_j) \\cdot x$}\n        \\EndIf\n      \\EndFor\n    \\EndFunction\n  \\end{algorithmic}\n\\end{algorithm*}\n\nIt is clear from the definition that the number of preimages of any point\n$(x, \\lambda)$ under \\textproc{SWChar2} is at most $\\alpha = 3$, since $c\n\\in \\Set{ t_j^{-1} \\cdot x \\given j = 1, 2, 3 }$ and $w$ is uniquely\ndetermined from $c$, $x$, and $\\lambda$ by\n\n", "index": 5, "text": "\\begin{align*}\n  w &\\in \\Set{ \\mathrm{QS}(c - a), \\mathrm{QS}(c - a) + 1 }, \\\\\n  \\mathrm{coeff}_0(w) &= \\lambda + x + \\mathrm{QS}(x^{-2} \\cdot b + x + a).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w\" display=\"inline\"><mi>w</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\in\\Set{\\mathrm{QS}(c-a),\\mathrm{QS}(c-a)+1},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2208</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Set</mtext></merror><mo>\u2062</mo><mi>QS</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>QS</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{coeff}_{0}(w)\" display=\"inline\"><mrow><msub><mi>coeff</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\lambda+x+\\mathrm{QS}(x^{-2}\\cdot b+x+a).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\u03bb</mi><mo>+</mo><mi>x</mi><mo>+</mo><mrow><mi>QS</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>x</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo>\u22c5</mo><mi>b</mi></mrow><mo>+</mo><mi>x</mi><mo>+</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\\else\n", "itemtype": "equation", "pos": 42005, "prevtext": "\n\nThe preimage set for any point $(x, \\lambda)$ can be efficiently computed\nby these same formulas.  Furthermore, Aranha~{et al.}~\\cite{aranha2014binary}\nshow that the proportion of curve points with $k$ preimages under $\\textproc{SWChar2}$ for $k =\n0,1,2,3$ is $9/32$, $15/32$, $7/32$, and $1/32$, respectively, up to an\nerror term of $O(2^{-n/2})$.  It follows that\n${\\rm I\\kern-.3em E}_P[{\\lvert {\\textproc{SWChar2}^{-1}(P)} \\rvert} / \\alpha] = 1/3 \\pm\nO(2^{-n/2})$, and therefore, \\textsc{SWChar2} is an $(\\alpha,\n\\beta)$-weak encoding with $\\beta = 3 + O(2^{-n/2})$.\n\n\\subsection{Hash function definition}\n\nBased on this encoding function, we define the \\emph{elliptic curve multiset hash\n(ECMH)}: given a binary elliptic curve group $E_{a,b}({\\mathbb{F}}_{2^m})$ and an\nintermediate hash function $h \\colon A \\to {\\mathbb{Z}}_{2^m}$ (modeled as a random\noracle), we define $\\mathrm{ECMH}_{a,b,h}(x) = \\textsc{SWChar2}(h(x); a, b)$.  Commonly used elliptic curves over ${\\mathbb{F}}_{2^m}$, including the NIST-recommended ones,\n\nhave a generator of prime order $\\rho > 2^{m-2}$ with an easily determined complement group of size $h = {\\lvert {\\overline{\\langle g \\rangle}} \\rvert} \\leq 4$.  Thus, the samplability requirement on $\\overline{\\langle g \\rangle}$ is easily satisfied in practice.  Hence, by \\cref{thm:dlpreduction}, finding a collision in ECMH is as hard (up to a small constant factor) as computing discrete logarithms to the base $g$, which we assume to be $O(2^{m/2})$.\n\nSimilar suitable encoding algorithms exist for elliptic curves over fields of characteristic $p > 2$~\\cite{shallue2006construction,brier2010efficient}, and could also be used to define an elliptic curve multiset hash.  However, the use of a characteristic 2 field eliminates the need for an expensive field exponentiation in order to solve a quadratic equation, which would otherwise dominate the computation time, and on modern CPUs that support fast carry-less multiplication, fast implementations of all other required field operations are also possible for characteristic $2$~\\cite{taverne2011jcen}.\n\n\\subsection{Compressed representation of curve points}\n\\label{sec:compressed-point-representation}\n\nThe group of ${\\mathbb{F}}_{2^m}$-rational points on an elliptic curve $E_{a,b}$ has order ${\\lvert {E_{a,b}({\\mathbb{F}}_{2^m})} \\rvert} \\approx 2^m$.  Each point is naturally represented as a pair $(x,y) \\in {\\mathbb{F}}_{2^m}^2$ (or $(x, \\lambda) \\in {\\mathbb{F}}_{2^m}^2$), but there is a well-known method for encoding a point using just $m + 1$ bits: given $x$ there are at most two possible values for $y$ (or $\\lambda$) if $(x,y)$ (or $(x,\\lambda)$) satisfy $E_{a,b}$, and they can be recovered efficiently using a small number of field operations.  Thus, a point can be encoded by its $x$ value and a single additional bit to disambiguate the two possible points.  The elliptic curve group identity element (the point at infinity) can be encoded specially without increasing the representation size, by using a bit sequence that would not otherwise encode a valid point.\n\n\n\\section{Implementation}\n\nWe developed an optimized implementation of elliptic curve multiset hash (ECMH) as an open-source C++ library~\\cite{maitinshepard2015ecmhLibrary},\n\nwith support for all NIST-recommended binary elliptic curves~\\cite{fips186-4} and the record-breaking GLS254 curve~\\cite{oliveira2014two}, as well as several other SEC~2-recommended curves~\\cite{sec2-1}.  Using a combination of C++ templates and code generation, we were able to write generic code to support many different configurations without sacrificing runtime performance; only for modular reduction was a custom implementation required for each supported field.  We incorporated existing fast x86/x86-64 polynomial multiplication, squaring, and modular reduction routines for ${\\mathbb{F}}_{2^{163}}$, ${\\mathbb{F}}_{2^{193}}$, ${\\mathbb{F}}_{2^{233}}$, ${\\mathbb{F}}_{2^{239}}$, ${\\mathbb{F}}_{2^{283}}$, ${\\mathbb{F}}_{2^{409}}$, ${\\mathbb{F}}_{2^{571}}$~\\cite{bluhm2013fast} and for ${\\mathbb{F}}_{2^{127}}$~\\cite{oliveira2014two}.\n\nWe implemented field inversion using a polynomial-basis Itoh--Tsujii inversion method making use of multi-squaring tables~\\cite{guajardo2002itoh,taverne2011jcen,oliveira2014two,aranha2014binary}.  We generated field inversion routines for each field degree automatically based on an A\\textsuperscript{*} search procedure for computing the optimal Itoh--Tsujii addition chain and set of multi-squaring tables, based on a machine-specific cost model estimated from field operation performance measurements~\\cite{cryptoeprint:2015:028}.\n\nWe also developed optimized implementations of the {\\texttt{{MSet-Mu-Hash}}}{} and\n{\\texttt{{MSet-Add-Hash}}}{} hash functions, based on the modular arithmetic functions in the OpenSSL library version 1.0.1i, for the purpose of comparison.\n\n\\subsection{Intermediate hash function}\n\nECMH requires an intermediate hash function $h \\colon A \\to {\\mathbb{Z}}_{2^m}$.\nUnder our assumption that the base set $A$ is the set of octet strings,\nwe simply require a standard cryptographic hash function (modeled as a\nrandom oracle) with output size $m$.  Given the inherent property of any homomorphic hash function that a single collision leads to arbitrary second preimages, we advise using a keyed hash function when possible to minimize risk.\n\nAny standard hash function with fixed output size greater than $m$ bits can simply be truncated to $m$ bits.  Standard expansion techniques can be used to efficiently generate an arbitrary length $m$ output from a hash function with fixed output size $b < m$.  Sponge constructions, such as Keccak~\\cite{bertoni2009keccak}, are particularly convenient since they support arbitrary output sizes.\n\nBoth {\\texttt{{AdHash}}}{} and {\\texttt{{MuHash}}}{} similarly require intermediate hash functions, but with much larger output sizes $m$ for equivalent security levels.\n\nWe designed our implementation to support arbitrary hash functions, but for our performance evaluation, we selected BLAKE2~\\cite{aumasson2013blake2} because of its state-of-the-art performance.  For $m \\leq 256$, we used the BLAKE2s variant (256-bit output), truncating the output to $m$ bits.  For $256 < m \\leq 512$, we used the BLAKE2b variant (512-bit output) with truncation.  For $m > 512$, we used BLAKE2b repeatedly to generate sufficient output, in such a way that the underlying compression function is called a minimum number of times.\n\n\\subsection{Linear field operations}\n\\label{sec:impl:linear-field-operations}\n\nSeveral key operations for ${\\mathbb{F}}_{2^m}$, such as squaring, multi-squaring ($x \\mapsto x^{2^i}$), square root, and half-trace, are linear in the coefficients.  For multi-squaring (useful for inversion) and half-trace, an implementation based on a lookup table can be significantly faster than direct computation~\\cite{bos2010ecc2k,taverne2011jcen,oliveira2014two,aranha2014binary}.  The coefficients are split into $\\lceil m / \\beta \\rceil$ blocks of $\\beta$ bits, and a separate table of $2^{\\beta}$ entries is precomputed for each block position, using a total of ${s_{{m,\\beta}}} = \\lceil m / \\beta \\rceil \\cdot 2^{\\beta} \\cdot \\lceil m / W \\rceil \\cdot W / 8$ bytes of memory, where $W$ is the word size in bits.  The linear transform can then be computed from the precomputed tables with $k = \\lceil m / \\beta \\cdot \\lceil m / W \\rceil$ memory accesses and $k - 1$ XOR operations.\n\n\\subsection{Blinding for side-channel resistance}\n\nThe fastest implementation of ECMH is susceptible to timing and cache side-channel attacks, due to the use of lookup tables (for inversion and QS), and the use of branching (for \\textsc{SWChar2}).  A branch-free implementation of \\textsc{SWChar2} adds only a few additional multiplications and squarings.  Lookup tables are unavoidable for good performance, but we can blind inversion at a cost of just two multiplications and generation of one random field element.  We likewise can blind QS at a cost of 1 squaring, 2 additions, and generation of one random field element, as well as a few bit operations to ensure the random element is in the image of QS.  In this way we can fully protect against timing and cache side-channel attacks at only a small additional cost.\n\n\\subsection{Quadratic extension field}\n\nFor even $m$, representing ${\\mathbb{F}}_{2^m}$ as a quadratic extension of ${\\mathbb{F}}_{2^{m/2}}$ results in significantly faster field operations relative to an odd-degree field of roughly the same size: inversion in the extension field requires only one inversion in the base field (effectively reducing the memory and computation costs by nearly a factor of 4 for a table-based multi-squaring implementation), and half-trace requires only 2 half-trace computations in the base field (reducing, for a table-based implementation, the computation cost by a factor of 2 and the memory requirement by a factor of 4)~\\cite{oliveira2014two}.  We use this representation to support the GLS254 elliptic curve over ${\\mathbb{F}}_{2^{254}}$~\\cite{oliveira2014two}.\n\n\n\n\\subsection{In-memory representation of elliptic curve points}\n\nAlthough an element in the elliptic curve group of points $E_{a,b}({\\mathbb{F}}_{2^m})$ can be represented directly using the standard affine $(x,y)$-representation or the $\\lambda$-affine $(x,\\lambda)$ representation, and more compactly using just $m+1$ bits as described in \\cref{sec:compressed-point-representation}, we can more efficiently perform group operations using the $\\lambda$-projective representation $(\\tilde{x},\\tilde{\\lambda},z)$ corresponding to the $\\lambda$-affine representation $(x = \\tilde{x}/z, \\lambda = \\tilde{\\lambda}/z)$: This representation allows point addition and point doubling to be performed without any field inversions~\\cite{oliveira2014two}.\n\n\n\n\\subsection{Batch \\textsc{SWChar2} computation}\n\\label{sec:batch-swchar2}\nA large fraction of the computational cost of our elliptic curve multiset hash construction is due to the single field inversion required by the \\textsc{SWChar2} encoding function.  Using Montgomery's trick, $n$ independent elements can be inverted simultaneously at the cost of just 1 field inversion and $3(n-1)$ field multiplications~\\cite{shacham2001improving}.  Since field inversion is much more than 3 times as expensive as field multiplication, this provides significant computational savings.\n\n\n\n\n\\subsection{Montgomery domain for {\\texttt{{MSet-Mu-Hash}}}{}}\n\\label{sec:muhash-montgomery}\nA key cost in a na\\\"{\\i}ve implementation of {\\texttt{{MSet-Mu-Hash}}}{} is the reduction modulo $p$ required by multiplication in ${\\mathbb{Z}}_p^\\times$.  To avoid this cost, we can use the \\emph{Montgomery reduction}~\\cite{montgomery1985modular} defined by\n\\ifcompj\n\n", "index": 7, "text": "\\begin{multline*}\n  \\mathrm{Redc}(t ; p, r) = t \\cdot r^{-1} \\bmod p, \\\\ \\text{$0 \\leq t < p \\cdot r$, $r > p$, $\\gcd(r,p) = 1$.}\n\\end{multline*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"p4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{Redc}(t;p,r)=t\\cdot r^{-1}\\bmod p,\\\\&#10;\\displaystyle\\text{$0\\leq t&lt;p\\cdot r$, $r&gt;p$, $\\gcd(r,p)=1$.}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>;</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>t</mi><mo>\u22c5</mo><msup><mi>r</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mi>p</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mn>0</mn><mo>\u2264</mo><mi>t</mi><mo>&lt;</mo><mrow><mi>p</mi><mo>\u22c5</mo><mi>r</mi></mrow></mrow><mtext>,\u00a0</mtext><mrow><mi>r</mi><mo>&gt;</mo><mi>p</mi></mrow><mtext>,\u00a0</mtext><mrow><mrow><mi>gcd</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mtext>.</mtext></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\\fi\nIf $r$ is chosen to be a power of $2$, or a power of $2^w$, where $w$ is the word size, then the computational cost of $\\mathrm{Redc}$ is significantly lower than a reduction $\\bmod p$.\n\nWe represent an element $x \\in {\\mathbb{Z}}_p^\\times$ as a triplet $(w, y, z) \\in {\\mathbb{Z}}_{p-1} \\times {\\mathbb{Z}}_P^\\times \\times {\\mathbb{Z}}_p^\\times$ corresponding to $y / z \\cdot r^w \\bmod p$, where $r$ is the Montgomery reduction constant.  Multiplication under this representation is defined by\n\n", "itemtype": "equation", "pos": 42156, "prevtext": "\n\\else\n", "index": 9, "text": "\n\\[\n  \\mathrm{Redc}(t ; p, r) = t \\cdot r^{-1} \\bmod p, \\qquad \\text{$0 \\leq t < p \\cdot r$, $r > p$, $\\gcd(r,p) = 1$.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{Redc}(t;p,r)=t\\cdot r^{-1}\\bmod p,\\qquad\\text{$0\\leq t&lt;p\\cdot r$, $r&gt;p%&#10;$, $\\gcd(r,p)=1$.}\" display=\"block\"><mrow><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>;</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>t</mi><mo>\u22c5</mo><msup><mi>r</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mi>p</mi></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mn>0</mn><mo>\u2264</mo><mi>t</mi><mo>&lt;</mo><mrow><mi>p</mi><mo>\u22c5</mo><mi>r</mi></mrow></mrow><mtext>,\u00a0</mtext><mrow><mi>r</mi><mo>&gt;</mo><mi>p</mi></mrow><mtext>,\u00a0</mtext><mrow><mrow><mi>gcd</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\\iffalse\n\\begin{alignat*}{13}\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, y_2,\\, &&z_2 &&\\big) &&= \\big(w_1 + w_2, \\,&&\\mathrm{Redc}(y_1 \\cdot y_2),\\,&&\\mathrm{Redc}(z_1 \\cdot z_2)&&\\big); \\\\\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, y_2,\\, &&1&&\\big) &&= \\big(w_1 + w_2 + 1,\\,&&\\mathrm{Redc}(y_1 \\cdot y_2),\\,&&z_1&&\\big); \\\\\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, 1,\\, &&z_2&&\\big) &&= \\big(w_1 + w_2 - 1,\\,&&y_1,\\,&&\\mathrm{Redc}(z_1 \\cdot z_2)&&\\big).\n\\end{alignat*}\n\\fi\n\n\\section{Performance measurement}\n\nAs our test platforms we used an Intel Westmere i7-970 \\SI{3.2}{\\giga\\hertz} CPU (with \\SI{12}{MiB} L3 cache) and an Intel Haswell i7-4790K \\SI{4.0}{GHz} CPU (with \\SI{8}{MiB} L3 cache).  Both of these processors support the {\\texttt{{PCLMULQDQ}}} instruction for carry-less multiplication, Westmere being the first Intel architecture to support it; on the much more recent Haswell architecture, where this instruction has significantly lower cost, alternative modular reduction routines based on it are used for ${\\mathbb{F}}_{2^{{163}}}$, ${\\mathbb{F}}_{2^{{283}}}$, and ${\\mathbb{F}}_{2^{{571}}}$ for a modest gain in performance~\\cite{bluhm2013fast}.    Our implementation used a word size of $W = 128$ bits and a block size of $B = 8$ bits for all half trace and multi-squaring tables.  All code was compiled separately for each architecture using version 3.5 of the Clang compiler at the highest optimization level.\n\n\\subsection{Robust operation timing}\n\\label{sec:execution-time-measurement}\n\nWe measured the execution time of all operations in CPU cycles, using the combination of {\\texttt{{RDTSC}}}, {\\texttt{{RDTSCP}}}, and {\\texttt{{CPUID}}} instructions recommended by Intel~\\cite{paoloni2010benchmark}.  To improve accuracy and reduce variance, we disabled TurboBoost, frequency scaling, and HyperThreading, and ensured that a single non-boot CPU core was used for all benchmarks on each machine.  For each operation, we estimated the benchmarking overhead and subtracted it from the measured number of cycles.  Additionally, we automatically determined a per-measurement repeat count for each operation that ensured the benchmarking overhead was less than 10\\%.\n\nThe execution time was computed as the median of the cycle measurements; the number of cycle measurements for each operation from which the median was computed was at least 1000 and chosen automatically to ensure a sufficiently small 99\\% confidence interval on the median estimate (less than the larger of $\\sfrac{1}{1000}$ of the estimated median or $\\sfrac{1}{10}$ of a cycle).  For consistency, we ensured warm-cache conditions for all estimates by discarding the first 2000 measurements.\n\n\\subsection{Consistent measurement of memory-dependent operations}\n\\label{sec:memory-dependent-measurement}\nFor operations with data-dependent memory accesses, such as table-based multi-squaring, half trace computation, and the higher-level operations based on these primitives, we measured the aggregate execution time for a set of inputs guaranteed to induce a uniform memory access pattern (and then divided by the number of inputs), in order to obtain worst-case warm-cache estimates.  Failure to do so results in a large underestimate of execution time.\n\nWe also observed the performance characteristics of table operations to be significantly affected by the size of the virtual memory pages backing the tables; in particular, on the x86-64 test machines, both the base level performance and the scaling of execution times with increasing table size were significantly better with \\SI{2}{MiB} (huge) pages than with \\SI{4}{KiB} pages, due to the cost of translation lookaside buffer (TLB) misses.  The Linux transparent huge page support (introduced in Linux version 2.6.38) results in some, but not all, memory regions being backed automatically by huge pages, depending on a number of factors including region alignment and physical memory fragmentation; when not taken into account, this significantly reduced the reliability of our performance measurements.  For consistent performance, we therefore ensured that all lookup tables were backed by huge pages.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Results}\n\\label{sec:performance-results}\n\nIn order to obtain performance results for a full range of security levels, we evaluated the performance of ECMH using each of the following eight elliptic curves: sect163k1~\\cite{sec2-1} (NIST K-163~\\cite{fips186-4}), sect193r1~\\cite{sec2-1}, sect233k1~\\cite{sec2-1} (NIST K-233~\\cite{fips186-4}), sect239k1~\\cite{sec2-1}, GLS254~\\cite{oliveira2014two}, sect283k1~\\cite{sec2-1} (NIST K-283~\\cite{fips186-4}), sect409k1~\\cite{sec2-1} (NIST K-409~\\cite{fips186-4}), and sect571k1~\\cite{sec2-1} (NIST K-571~\\cite{fips186-4}).\n\n\\begin{figure}\n  \\centering\n  \\includegraphics{hash_size_plot.pdf}\n  \\caption{Security level attained as a function of hash code size for Elliptic Curve Multiset Hash (ECMH), {\\texttt{{AdHash}}}{} (restricted to \\emph{set} hashing), and {\\texttt{{MuHash}}}{}.  For {\\texttt{{MuHash}}}{}, the \\emph{multiset} hashing security level was determined based on the conjectured time-complexity $L_p\\left[1/3, \\sqrt[3]{64/9}\\right]$ of the Number Field Sieve for solving discrete logarithms in ${\\mathbb{Z}}_p^\\times$~\\cite[p.~128]{menezes2010handbook}.  For {\\texttt{{AdHash}}}{}, we determined a \\emph{set} hashing security level of $2 \\sqrt{n}$ corresponded to groups ${\\mathbb{Z}}_{2^n}$ based on the assumption that the generalized birthday attack~\\cite{wagner2002generalized} is optimal.}\n  \\label{fig:hash_code_size}\n\\end{figure}\n\nBased on \\cref{thm:dlpreduction} and the assumed hardness of the Elliptic Curve Discrete Logarithm Problem, the ECMH using an elliptic curve group of order $\\rho$ has collision resistance of $O(\\sqrt{\\rho})$, corresponding to a security level $\\log_2 \\rho$ bits.  We also evaluated {\\texttt{{MuHash}}}{} and {\\texttt{{AdHash}}}{} (for set hashing only) using group sizes corresponding to the same range of security levels.  The correspondence between security level and hash code size under each method is shown in \\cref{fig:hash_code_size}.\n\n\\begin{figure*}\n  \\centering\n  \\includegraphics{multiset_hash_cycle_plot.pdf}\n  \n  \\label{fig:multiset_hash_cycles}\n\\end{figure*}\n\nFor each multiset hash $H$, we measured the computational cost of incremental hash code updates corresponding to a sequence of incremental additions or removals of multiset elements, i.e.\\ incrementing or decrementing by 1 the multiplicity of each element in the sequence.  Larger changes in multiplicity can also be handled efficiently by scalar multiplication in the group, but we expect incremental additions and removals to be the most common case.  We used a sequence of 1024 randomly generated 32-byte strings;\\footnote{As ECMH depends on lookup tables with a block size of $B = 8$, 1024 random elements ensures high coverage of the tables and a random access pattern, in order to correctly estimate execution time, as described in \\cref{sec:memory-dependent-measurement}.} longer strings would simply impose an additional cost independent of $H$.\n\nThe average cost per element reflects the cost of the intermediate hash function based on BLAKE2, the cost of encoding the expanded bit sequence as a group element, and the cost of one group operation to add the encoded element to a running total.  In the case of ECMH, the encoding is \\textsc{SWChar2} and the group operation is implemented as the mixed addition of a $\\lambda$-affine and a $\\lambda$-projective point; batch ECMH effectively replaces 1 field inversion by 3 multiplications, as described in \\cref{sec:batch-swchar2}.  In the case of {\\texttt{{AdHash}}}{}, the encoding is trivial and the group operation is simply integer addition; batch computation would offer no advantage.  For {\\texttt{{MuHash}}}{}, the encoding requires a comparison and at most one subtraction, and the group operation requires just a single Montgomery multiplication, as described in \\cref{sec:muhash-montgomery}; batch computation would offer no advantage over the Montgomery representation already used.\n\nThe results are shown in \\cref{fig:multiset_hash_cycles} and in \\cref{tab:multiset_hash_cycles}.  Only element addition performance is shown, as due to the representations used, element removal performance is nearly identical.  Timings for point encoding, compression, and decompression are given in \\cref{tab:elliptic_curve_operation2_cycles}.  Base field operation timings are given in \\cref{tab:field_operation_cycles}, and a comparison of curve operation performance under $\\lambda$-affine and $\\lambda$-projective point representations is given in \\cref{tab:elliptic_curve_operation_cycles}.\n\n\n\\begin{table*}[p]\n  \\caption{Comparison of multiset hashing performance, as in \\cref{fig:multiset_hash_cycles}.  Note that the {\\texttt{{AdHash}}}{} performance applies only to \\emph{set} hashing.}\n  \\centerfloat\n  \\footnotesize\n\n\\begin{tabular}{llrc@{/}c@{/}c@{/}cccrc@{/}c@{/}c@{/}ccc}\n\n& & & \\multicolumn{6}{c}{Westmere cycles} & & \\multicolumn{6}{c}{Haswell cycles}\\\\\n\\cmidrule(r){4-9}\\cmidrule(r){11-16}& &  & \\multicolumn{4}{c}{ECMH} &  &  &  & \\multicolumn{4}{c}{ECMH} &  & \\\\\n\n$n$&Curve &  & single & blind & batch & blind & {\\texttt{{MuHash}}}{} & {\\texttt{{AdHash}}}{} &  & single & blind & batch & blind & {\\texttt{{MuHash}}}{} & {\\texttt{{AdHash}}}{}\\\\\n\\midrule\n81&sect163k1& & 3601&4436&2023&2418 & 3939 & 2998& & 2199&2556&1133&1349 & 2208 & 2186\\\\\n96&sect193r1& & 4326&5444&2595&3198 & 7384 & 3687& & 2342&2755&1287&1580 & 3967 & 2674\\\\\n116&sect233k1& & 4667&5726&2444&2933 & 13414 & 5160& & 2605&2968&1209&1495 & 7074 & 3708\\\\\n119&sect239k1& & 5183&6361&2630&3164 & 16532 & 5117& & 3061&3474&1422&1700 & 8537 & 3684\\\\\n127&GLS254& & 2835&3872&2307&2882 & 20631 & 5920& & 1592&1973&1184&1426 & 10472 & 4239\\\\\n141&sect283k1& & 7524&9271&3513&4254 & 33472 & 7286& & 3828&4291&1733&2024 & 17251 & 5178\\\\\n204&sect409k1& & 12621&16696&5686&6878 & 176767 & 14997& & 5788&6897&2473&2948 & 84027 & 10632\\\\\n285&sect571k1& & 23654&29628&9206&10746 & 890172 & 28485& & 11745&16664&4188&4940 & 467938 & 20152\\\\\n\n\n\\end{tabular}\n  \\label{tab:multiset_hash_cycles}\n\\end{table*}\n\n\n\\begin{table*}[p]\n  \\caption{Performance of elliptic curve point encoding, compression (to a minimal-length bit string), and decompression (from said bit string).  Compression (comp.) and decompression (dec.) use $\\lambda$-projective coordinates.  Batch encoding is with a batch size of $256$.}\n  \\centering\n  \\footnotesize\n\n\\begin{tabular}{lrc@{/}c@{/}c@{/}cccrc@{/}c@{/}c@{/}ccc}\n\n & & \\multicolumn{6}{c}{Westmere cycles} & & \\multicolumn{6}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-8}\\cmidrule(r){10-15} &  & \\multicolumn{4}{c}{\\textsc{SWChar2}} &  &  &  & \\multicolumn{4}{c}{\\textsc{SWChar2}} &  & \\\\\n\nCurve &  & single & blind & batch & blind & Comp. & Dec. &  & single & blind & batch & blind & Comp. & Dec.\\\\\n\\midrule\nsect163k1& & 2629&3248&853&1234 & 2222 & 2268& & 1500&1854&432&641 & 1370 & 1402\\\\\nsect193r1& & 3217&4073&1227&1821 & 2541 & 2669& & 1603&2014&548&823 & 1370 & 1443\\\\\nsect233k1& & 3577&4340&1076&1537 & 3080 & 3154& & 1852&2214&510&719 & 1673 & 1732\\\\\nsect239k1& & 4033&4880&1158&1670 & 3468 & 3569& & 2227&2616&570&817 & 2024 & 2045\\\\\nGLS254& & 1671&2551&978&1566 & 1166 & 1245& & 874&1280&437&709 & 665 & 716\\\\\nsect283k1& & 5738&6865&1587&2253 & 4772 & 5134& & 2822&3278&698&993 & 2320 & 2624\\\\\nsect409k1& & 8612&10523&2707&3720 & 7497 & 7563& & 4395&5172&1157&1573 & 3883 & 4074\\\\\nsect571k1& & 17968&21721&4301&5867 & 15174 & 16337& & 8987&13329&1867&2573 & 7132 & 8448\\\\\n\n\n\\end{tabular}\n  \\label{tab:elliptic_curve_operation2_cycles}\n\\end{table*}\n\n\\clearpage\n\n\\begin{table*}[p]\n  \\caption{Field operation performance for ${\\mathbb{F}}_{2^m}$.  Batch inversion is with a batch size of $256$.}\n  \\centering\n  \\footnotesize\n\\begin{tabular}{lrccc@{/}c@{/}c@{/}cc@{/}crccc@{/}c@{/}c@{/}cc@{/}c}\n\n & & \\multicolumn{8}{c}{Westmere cycles} & & \\multicolumn{8}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-10}\\cmidrule(r){12-19} &  &  &  & \\multicolumn{4}{c}{Invert} & \\multicolumn{2}{c}{QS} &  &  &  & \\multicolumn{4}{c}{Invert} & \\multicolumn{2}{c}{QS}\\\\\n\n$m$ &  & Mul. & Sq. & single & blind & batch & blind & var. & blind &  & Mul. & Sq. & single & blind & batch & blind & var. & blind\\\\\n\\midrule\n127& & 44 & 11 & 721&904&124&124 & 41&131& & 23 & 9 & 435&534&59&59 & 21&82\\\\\n163& & 84 & 32 & 1807&2074&266&269 & 115&227& & 43 & 24 & 1159&1309&127&127 & 67&152\\\\\n193& & 113 & 26 & 2210&2533&343&344 & 149&264& & 46 & 20 & 1129&1308&128&128 & 79&155\\\\\n233& & 109 & 30 & 2745&3092&341&342 & 191&313& & 48 & 24 & 1439&1583&131&131 & 95&178\\\\\n239& & 119 & 34 & 3139&3492&372&376 & 187&313& & 51 & 31 & 1755&1933&160&159 & 93&183\\\\\n254& & 99 & 17 & 868&1211&310&313 & 88&245& & 38 & 15 & 514&664&111&116 & 62&158\\\\\n283& & 148 & 36 & 4438&4869&473&474 & 420&560& & 55 & 28 & 2222&2423&175&179 & 227&296\\\\\n409& & 274 & 35 & 7899&8688&884&867 & 807&959& & 93 & 30 & 3500&3805&291&296 & 404&478\\\\\n571& & 431 & 65 & 16217&17808&1308&1323 & 1457&1692& & 168 & 38 & 6873&7611&464&489 & 737&842\\\\\n\n\n\\end{tabular}\n  \\label{tab:field_operation_cycles}\n\\end{table*}\n\n\\begin{table*}[p]\n  \\caption{Performance of elliptic curve group operations using $\\lambda$-affine and $\\lambda$-projective point representations.  The result of point addition or doubling is always represented in $\\lambda$-projective coordinates.}\n  \\centering\n  \\footnotesize\n\\begin{tabular}{lrc@{/}c@{/}cc@{/}cc@{/}crc@{/}c@{/}cc@{/}cc@{/}c}\n\n & & \\multicolumn{7}{c}{Westmere cycles} & & \\multicolumn{7}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-9}\\cmidrule(r){11-17} &  & \\multicolumn{3}{c}{Add} & \\multicolumn{2}{c}{Double} & \\multicolumn{2}{c}{Negate} &  & \\multicolumn{3}{c}{Add} & \\multicolumn{2}{c}{Double} & \\multicolumn{2}{c}{Negate}\\\\\n\nCurve &  & aff. & mix. & full & aff. & proj. & aff. & proj. &  & aff. & mix. & full & aff. & proj. & aff. & proj.\\\\\n\\midrule\nsect163k1& & 500&748&1016 & 192&472 & 12&18& & 213&305&408 & 105&188 & 6&9\\\\\nsect193r1& & 604&952&1268 & 199&640 & 12&18& & 236&370&468 & 105&281 & 7&9\\\\\nsect233k1& & 624&936&1276 & 202&540 & 12&18& & 235&341&462 & 107&224 & 7&10\\\\\nsect239k1& & 684&1032&1388 & 244&620 & 12&18& & 308&453&595 & 144&311 & 6&9\\\\\nGLS254& & 572&856&1168 & 162&488 & 9&14& & 219&308&435 & 78&196 & 5&9\\\\\nsect283k1& & 864&1308&1792 & 280&768 & 15&23& & 329&489&655 & 138&302 & 12&19\\\\\nsect409k1& & 1496&2320&3144 & 416&1280 & 18&29& & 542&788&1075 & 194&506 & 12&22\\\\\nsect571k1& & 2276&3516&4772 & 644&1956 & 21&35& & 859&1253&1688 & 296&740 & 15&22\\\\\n\n\n\\end{tabular}\n  \\label{tab:elliptic_curve_operation_cycles}\n\\end{table*}\n\n\n\n\\clearpage\n\n\\section{Discussion}\n\\label{sec:discussion}\n\n\nElliptic curve multiset hash significantly outperforms the existing methods of\n{\\texttt{{MuHash}}}{} and {\\texttt{{AdHash}}}{}, particularly in the batch setting, while requiring\nsignificantly smaller hash codes at all security levels.  In fact, the hash code\nsize is essentially optimal.  Because the single field inversion required by the\nencoding function \\textsc{SWChar2} accounts for a large fraction of the\ncomputational cost, particularly with larger field degrees, the use of\nMontgomery's trick in the batch setting significantly reduces the computational\ncost.  The lower computational cost at the 127-bit security level is due to the\nefficiency of the GLS254 curve implementation; the quadratic extension field\nrepresentation of ${\\mathbb{F}}_{2^{{254}}}$ employed, and the close match of the degree to the\nword size $W = 128$, significantly reducing the cost of field operations.\nQuadratic extension field representations for other fields, such as ${\\mathbb{F}}_{2^{{502}}}$,\ncould potentially be used to obtain similar performance improvements at other\nsecurity levels. Furthermore, our choice of parameters follows the trend of\nincreasing native support to binary field arithmetic in Desktop\nprocessors and will likely benefit from improvements to the carry-less\nmultiplication instruction in the recently released Broadwell processor family.\n\nOur work is very related to the Encrypted Elliptic Curve Hash\n(EECH)~\\cite{brown2008encrypted}.  That construction also encodes\nseparate bit strings as points on a binary elliptic curve and then\ncombines those points using point addition.  Like our approach, it relies on the property of binary elliptic curves that curve points can be decoded from a non-redundant representation without expensive field exponentiations, using instead a precomputed lookup table for half-trace, and notes that better performance may be obtained using batch inversion and hardware support for carry-less multiplication.\n\nThe full EECH construction is proposed as an\nincremental hash for bit strings (the message is split into fixed-size\nblocks, and each block, concatenated with the block index, is encoded as\nan elliptic curve point).  In contrast to our elliptic curve multiset hash, it is\nspecifically designed to \\emph{avoid} reliance on an underlying random\noracle, relying instead on redundancy/padding in the point encoding\nfunction for collision resistance.\n\nWhile the \\emph{full} construction is not well-suited to homomorphic multiset hashing\\footnote{Using an elliptic curve over ${\\mathbb{F}}_{2^{{m}}}$, under the EECH construction at most $b$ bits of input data can be encoded per point to retain collision resistance of $2^{m - b}$.  Optimal collision resistance of $2^m$ for the representation size requires that $b \\leq m / 2$.  Each multiset element $a \\in A$ (assumed to be a bit string) must therefore be split into one or more blocks of $b$ bits, each encoded as a separate elliptic curve point.  For elements longer than $b$ bits, this is likely to be significantly more expensive than hashing $a$ with a fast hash function like BLAKE2 and then encoding the result into a single elliptic curve point.  EECH also offers no preimage resistance by default.  There is a proposed pairing-based variant PEECH that relies on an elliptic curve pairing to define a homomorphic one-way function.  This provides preimage resistance at the cost of significantly higher computational cost and representation size.}, we can make the fairer comparison between our ECMH construction and a straightforward randomize-then-combine-style~\\cite{bellare1997new} construction over binary elliptic curve groups using the implementation techniques proposed for EECH.  Such a construction was neither explicitly proposed nor implemented, and there was no prior evidence that it would be practical performance wise.  Our work goes significantly beyond this:\n\\begin{itemize}\n\\item We provide a thorough empirical analysis of performance, and demonstrate for the first time that an elliptic curve-based multiset hash actually significantly exceeds the performance of {\\texttt{{AdHash}}}{} and {\\texttt{{MuHash}}}{}.\n\\item We demonstrate that a fully blinded implementation is possible at only a minor performance penalty.  We also demonstrate batch variants of both the regular and fully-blinded implementations that are significantly faster.  In contrast, the try-and-increment encoding method proposed for EECH has no guaranteed time bound, making it unavoidably susceptible to timing attacks, and less amenable to speedup by batch inversion.\n\\item Our security proof is based on existing techniques~\\cite{bellare1997new,brier2010efficient} but the security bound we obtain is novel in several ways:\n  \\begin{itemize}\n  \\item The hash function $\\hat{H}$ into the elliptic curve group need not be indistinguishable from a random oracle, but is instead permitted to satisfy the weaker property of being an $(\\alpha, \\beta)$-weak encoding, which significantly reduces the computational cost.\n  \\item The hash function $\\hat{H}$ can map to the full elliptic curve group, rather than only a cyclic subgroup, as is required by EECH.  This allows for a simpler implementation that does not rely on patent-encumbered techniques~\\cite{brown2007method} for efficiently testing for subgroup membership.\n  \\end{itemize}\n\\end{itemize}\n\nIt was originally suggested~\\cite{bellare1997new} that while finding collisions in {\\texttt{{MuHash}}}{} is provably as hard as the Discrete Logarithm Problem (DLP), the converse is not necessarily true: it may be that {\\texttt{{MuHash}}}{} is still collision resistant even if discrete logarithms can be computed efficiently.  In fact, though, by computing discrete logarithms, finding a collision in {\\texttt{{MuHash}}}{} can be reduced to finding a collision in {\\texttt{{AdHash}}}{}.  It would therefore be susceptible to a generalized birthday attack~\\cite{wagner2002generalized} in the set hashing setting or to lattice reduction attacks in the multiset hashing setting.  The same reduction applies to our elliptic curve multiset hash, and is even more effective because of the smaller group order.\n\n\n\n\n\\begin{thebibliography}{99}\n\n\\bibitem{brown2008encrypted}\nBrown, D. R.~L. (2008) The encrypted elliptic curve hash.\n\\newblock {\\em IACR Cryptology ePrint Archive}, {\\bf  2008}, 12.\n\n\\bibitem{brown2007method}\nBrown, D. and Yamada, A. (2007).\n\\newblock Method and apparatus for performing validation of elliptic curve\n  public keys.\n\n\\bibitem{DBLP:conf/infocom/GkantsidisR06}\nGkantsidis, C. and Rodriguez, P. (2006) Cooperative security for network coding\n  file distribution.\n\\newblock {\\em INFOCOM}. {IEEE}.\n\n\\bibitem{DBLP:conf/sp/KrohnFM04}\nKrohn, M.~N., Freedman, M.~J., and Mazi{\\`{e}}res, D. (2004) On-the-fly\n  verification of rateless erasure codes for efficient content distribution.\n\\newblock {\\em IEEE S{\\&}P},  pp. 226--240. {IEEE} Computer Society.\n\n\\bibitem{DBLP:conf/nsdi/SubramanianRSSK04}\nSubramanian, L., Roth, V., Stoica, I., Shenker, S., and Katz, R.~H. (2004)\n  Listen and whisper: Security mechanisms for {BGP}.\n\\newblock In Morris, R. and Savage, S. (eds.), {\\em USENIX NSDI},  pp.\n  127--140. {USENIX}.\n\n\\bibitem{DBLP:conf/osdi/CastroL99}\nCastro, M. and Liskov, B. (1999) Practical byzantine fault tolerance.\n\\newblock In Seltzer, M.~I. and Leach, P.~J. (eds.), {\\em USENIX OSDI},  pp.\n  173--186. {USENIX} Association.\n\n\\bibitem{DBLP:journals/tocs/CastroL02}\nCastro, M. and Liskov, B. (2002) Practical byzantine fault tolerance and\n  proactive recovery.\n\\newblock {\\em {ACM} Trans. Comput. Syst.}, {\\bf  20}, 398--461.\n\n\\bibitem{cathalo2009comparing}\nCathalo, J., Naccache, D., and Quisquater, J.-J. (2009) Comparing with {RSA}.\n\\newblock {\\em IMACC},  pp. 326--335. Springer.\n\n\\bibitem{DBLP:conf/ccs/2008}\nNing, P., Syverson, P.~F., and Jha, S. (eds.) (2008) {\\em Proceedings of the\n  2008 {ACM} Conference on Computer and Communications Security, {CCS} 2008,\n  Alexandria, Virginia, USA, October 27-31, 2008}. {ACM}.\n\n\\bibitem{clarke2003incremental}\nClarke, D., Devadas, S., Van~Dijk, M., Gassend, B., and Suh, G.~E. (2003)\n  Incremental multiset hash functions and their application to memory integrity\n  checking.\n\\newblock {\\em ASIACRYPT},  pp. 188--207. Springer.\n\n\\bibitem{bellare1997new}\nBellare, M. and Micciancio, D. (1997) A new paradigm for collision-free\n  hashing: Incrementality at reduced cost.\n\\newblock {\\em EUROCRYPT},  pp. 163--192. Springer.\n\n\\bibitem{brier2010efficient}\nBrier, E., Coron, J.-S., Icart, T., Madore, D., Randriam, H., and Tibouchi, M.\n  (2010) Efficient indifferentiable hashing into ordinary elliptic curves.\n\\newblock {\\em CRYPTO},  pp. 237--254. Springer.\n\n\\bibitem{shallue2006construction}\nShallue, A. and van~de Woestijne, C.~E. (2006) Construction of rational points\n  on elliptic curves over finite fields.\n\\newblock {\\em ANTS},  pp. 510--524. Springer.\n\n\\bibitem{aranha2014binary}\nAranha, D.~F., Fouque, P.-A., Qian, C., Tibouchi, M., and Zapalowicz, J.-C.\n  (2014) {Binary Elligator Squared}.\n\\newblock {\\em SAC},  pp. 20--37. Springer.\n\n\\bibitem{aumasson2013blake2}\nAumasson, J.-P., Neves, S., Wilcox-O\u00e2\u0080\u0099Hearn, Z., and Winnerlein, C. (2013)\n  {BLAKE2}: simpler, smaller, fast as {MD5}.\n\\newblock {\\em ACNS},  pp. 119--135. Springer.\n\n\\bibitem{semaev2015}\nSemaev, I. (2015).\n\\newblock New algorithm for the discrete logarithm problem on elliptic curves.\n\\newblock Cryptology ePrint Archive, Report 2015/310.\n\\newblock \\url{http://eprint.iacr.org/}.\n\n\\bibitem{kosters2015}\nKosters, M. and Yeo, S.~L. (2015).\n\\newblock Notes on summation polynomials.\n\\newblock arXiv:1503.08001.\n\n\\bibitem{DBLP:conf/crypto/HuangKY15}\nHuang, M.~A., Kosters, M., and Yeo, S.~L. (2015) Last fall degree, hfe, and\n  weil descent attacks on {ECDLP}.\n\\newblock In Gennaro, R. and Robshaw, M. (eds.), {\\em Advances in Cryptology -\n  {CRYPTO} 2015 - 35th Annual Cryptology Conference, Santa Barbara, CA, USA,\n  August 16-20, 2015, Proceedings, Part {I}},  Lecture Notes in Computer\n  Science, {\\bf9215},  pp. 581--600. Springer.\n\n\\bibitem{galbraith2015ellipticnews}\nGalbraith, S. (2015).\n\\newblock Elliptic curve discrete logarithm problem in characteristic two.\n\\newblock\n  \\url{https://ellipticnews.wordpress.com/2015/04/13/elliptic-curve-discrete-logarithm-problem-in-characteristic-two/}.\n\n\\bibitem{krohn2004fly}\nKrohn, M.~N., Freedman, M.~J., and Mazieres, D. (2004) On-the-fly verification\n  of rateless erasure codes for efficient content distribution.\n\\newblock {\\em IEEE S\\&P},  pp. 226--240. IEEE.\n\n\\bibitem{maurer2004indifferentiability}\nMaurer, U.~M., Renner, R., and Holenstein, C. (2004) Indifferentiability,\n  impossibility results on reductions, and applications to the random oracle\n  methodology.\n\\newblock In Naor, M. (ed.), {\\em TCC},  Lecture Notes in Computer Science,\n  {\\bf2951},  pp. 21--39. Springer.\n\n\\bibitem{boneh2001identity}\nBoneh, D. and Franklin, M. (2001) Identity-based encryption from the {W}eil\n  pairing.\n\\newblock {\\em CRYPTO},  pp. 213--229. Springer.\n\n\\bibitem{wagner2002generalized}\nWagner, D. (2002) A generalized birthday problem.\n\\newblock {\\em CRYPTO},  pp. 288--304. Springer.\n\n\\bibitem{impagliazzo1996efficient}\nImpagliazzo, R. and Naor, M. (1996) Efficient cryptographic schemes provably as\n  secure as subset sum.\n\\newblock {\\em Journal of Cryptology}, {\\bf  9}, 199--216.\n\n\\bibitem{menezes2010handbook}\nMenezes, A.~J., Van~Oorschot, P.~C., and Vanstone, S.~A. (2010) {\\em Handbook\n  of applied cryptography}. CRC press.\n\n\\bibitem{smart2010ecryptkeysizes}\nSmart, N.~P. et al. (2010) {ECRYPT~II} yearly report on algorithms and key\n  lengths. Technical report. European Network of Excellence in Cryptology II.\n\\newblock \\url{http://www.ecrypt.eu.org/documents/D.SPA.13.pdf}.\n\n\\bibitem{hankerson2004guide}\nHankerson, D., Vanstone, S., and Menezes, A.~J. (2004) {\\em Guide to elliptic\n  curve cryptography}. Springer.\n\n\\bibitem{oliveira2014two}\nOliveira, T., L{\\'o}pez, J., Aranha, D.~F., and Rodr{\\'\\i}guez-Henr{\\'\\i}quez,\n  F. (2014) Two is the fastest prime: lambda coordinates for binary elliptic\n  curves.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  4}, 3--17.\n\n\\bibitem{taverne2011jcen}\nTaverne, J., Faz-Hern\\'andez, A., Aranha, D.~F., Rodr\u00c3\u00adguez-Henr\u00c3\u00adquez, F.,\n  Hankerson, D., and L\u00c3\u00b3pez, J. (2011) Speeding scalar multiplication over\n  binary elliptic curves using the new carry-less multiplication instruction.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  1}, 187--199.\n\n\\bibitem{maitinshepard2015ecmhLibrary}\nMaitin-Shepard, J.\n\\newblock {C++} {Elliptic} {Curve} {Multiset} {Hash} library.\n\\newblock \\url{http://jeremyms.com/ecmh}.\n\n\\bibitem{fips186-4}\n{National Institute of Standards and Technology} (2013) {FIPS 186-4}: {Digital\n  Signature Standard (DSS)}, {Federal Information Processing Standard (FIPS)},\n  publication 186-4. Technical report. Department of Commerce,  Gaithersburg,\n  MD, USA.\n\n\\bibitem{sec2-1}\nResearch, C. (2000) {\\em {{SEC 2}: {Recommended Elliptic Curve Domain\n  Parameters}}}. Standards for Efficient Cryptography.\n\\newblock Version 1.0.\n\n\\bibitem{bluhm2013fast}\nBluhm, M. and Gueron, S. (2015) Fast software implementation of binary elliptic\n  curve cryptography.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  5}, 215--226.\n\n\\bibitem{guajardo2002itoh}\nGuajardo, J. and Paar, C. (2002) Itoh--{T}sujii inversion in standard basis and\n  its application in cryptography and codes.\n\\newblock {\\em Designs, Codes and Cryptography}, {\\bf  25}, 207--216.\n\n\\bibitem{cryptoeprint:2015:028}\nMaitin-Shepard, J. (2015).\n\\newblock Optimal software-implemented {Itoh--Tsujii} inversion for\n  $\\mathrm{GF}(2^m)$.\n\\newblock Cryptology ePrint Archive, Report 2015/028.\n\\newblock \\url{http://eprint.iacr.org/}.\n\n\\bibitem{bertoni2009keccak}\nBertoni, G., Daemen, J., Peeters, M., and Van~Assche, G. (2009) Keccak sponge\n  function family main document.\n\\newblock {\\em Submission to NIST (Round 2)}, {\\bf  3}.\n\n\\bibitem{bos2010ecc2k}\nBos, J.~W., Kleinjung, T., Niederhagen, R., and Schwabe, P. (2010) {ECC2K}-130\n  on cell {CPUs}.\n\\newblock {\\em AFRICACRYPT},  pp. 225--242. Springer.\n\n\\bibitem{shacham2001improving}\nShacham, H. and Boneh, D. (2001) Improving {SSL} handshake performance via\n  batching.\n\\newblock {\\em CT-RSA},  pp. 28--43. Springer.\n\n\\bibitem{montgomery1985modular}\nMontgomery, P.~L. (1985) Modular multiplication without trial division.\n\\newblock {\\em Mathematics of Computation}, {\\bf  44}, 519--521.\n\n\\bibitem{paoloni2010benchmark}\nPaoloni, G. (2010) How to benchmark code execution times on {Intel} {IA-32} and\n  {IA-64} instruction set architectures. Technical report.\n\n\\bibitem{DBLP:conf/eurocrypt/GamaN08}\nGama, N. and Nguyen, P.~Q. (2008) Predicting lattice reduction.\n\\newblock In Smart, N.~P. (ed.), {\\em EUROCRYPT},  Lecture Notes in Computer\n  Science, {\\bf4965},  pp. 31--51. Springer.\n\n\\bibitem{chen2011bkz}\nChen, Y. and Nguyen, P.~Q. (2011) {BKZ} 2.0: Better lattice security estimates.\n\\newblock In Lee, D.~H. and Wang, X. (eds.), {\\em ASIACRYPT},  Lecture Notes in\n  Computer Science, {\\bf7073},  pp. 1--20. Springer.\n\n\\bibitem{DBLP:conf/ima/PolS13}\nvan~de Pol, J. and Smart, N.~P. (2013) Estimating key sizes for high\n  dimensional lattice-based systems.\n\\newblock In Stam, M. (ed.), {\\em IMACC},  Lecture Notes in Computer Science,\n  {\\bf8308},  pp. 290--303. Springer.\n\n\\bibitem{lindner2014latticechallenge}\nLindner, R. et al.\n\\newblock {TU Darmstadt} lattice challenge: Hall of fame.\n\\newblock \\url{http://www.latticechallenge.org/halloffame.php}, accessed 17\n  October 2014.\n\n\\end{thebibliography}\n\\appendix\n\n\\section{Security reduction based on ($\\alpha, \\beta$)-weak encodings}\n\\label{sec:proofthmdlpreduction}\n\nWe prove \\cref{thm:dlpreduction}, which reduces solving discrete logarithms to finding collisions in a homomorphic multiset hash function based on an $(\\alpha, \\beta)$-weak encoding.\n\n\\dlpreduction*\n\\begin{proof}\n  Let a $Q \\in \\langle P \\rangle$, for which we wish to find $n \\in {\\mathbb{Z}}$ such that $n \\cdot P = Q$, be given.  We simulate each successive distinct query $h(a_i)$ to the random oracle $h$ for $i = 1, \\ldots, k$ using the following algorithm:\n  \\begin{enumerate}\n  \\item Sample uniformly at random $r_i \\in {\\mathbb{Z}}_\\rho$, $d_i \\in \\Set{0,1}$, $J_i \\in \\overline{\\langle P \\rangle}$, $j \\in {\\mathbb{Z}}_{\\lceil \\alpha \\rceil}$.\n  \\item Compute $Q_i = r_i Q + d_i P + J_i$.  Note that since $\\langle P \\rangle$ has prime order, $Q$ is a generator of $\\langle P \\rangle$, and therefore $Q_i$ is distributed uniformly in $G$.\n  \\item If $j < {\\lvert {f^{-1}(Q_i)} \\rvert}$, sample $x_i$ from $f^{-1}(Q_i)$ uniformly at random.  Otherwise, resample $r_i, d_i, J_i$, and $j$.\n  \\item Return $x_i$.  Note that $x_i$ is uniformly distributed in $X$, and the expected number of sampling attempts is $\\alpha / \\beta$.\n  \\end{enumerate}\n\n  Under the simulated $h$, $\\mathcal{C}$ finds a non-empty $M \\in \\ker H$ in expected time $t$ with success probability $\\epsilon$.  Consider the case that a collision is found.  (Otherwise, we fail to compute the discrete logarithm.)  Without loss of generality, we can assume $M$ is non-zero only for values $a_i$ on which $h$ was queried.  Thus, we have\n  \n", "itemtype": "equation", "pos": 42778, "prevtext": "\n\\fi\nIf $r$ is chosen to be a power of $2$, or a power of $2^w$, where $w$ is the word size, then the computational cost of $\\mathrm{Redc}$ is significantly lower than a reduction $\\bmod p$.\n\nWe represent an element $x \\in {\\mathbb{Z}}_p^\\times$ as a triplet $(w, y, z) \\in {\\mathbb{Z}}_{p-1} \\times {\\mathbb{Z}}_P^\\times \\times {\\mathbb{Z}}_p^\\times$ corresponding to $y / z \\cdot r^w \\bmod p$, where $r$ is the Montgomery reduction constant.  Multiplication under this representation is defined by\n\n", "index": 11, "text": "\\begin{align*}\n(w_1,y_1,z_1)\\cdot(w_2,y_2,z_2) &=\n  (w_1+w_2, \\mathrm{Redc}(y_1y_2), \\mathrm{Redc}(z_1z_2)); \\\\\n(w_1,y_1,z_1)\\cdot(w_2,y_2,1)   &=\n  (w_1+w_2+1, \\mathrm{Redc}(y_1y_2), z_1); \\\\\n(w_1,y_1,z_1)\\cdot(w_2,1,z_2) &=\n  (w_1+w_2-1, y_1, \\mathrm{Redc}(z_1z_2)).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(w_{1},y_{1},z_{1})\\cdot(w_{2},y_{2},z_{2})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(w_{1}+w_{2},\\mathrm{Redc}(y_{1}y_{2}),\\mathrm{Redc}(z_{1}z_{2}));\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub></mrow><mo>,</mo><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>y</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>z</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(w_{1},y_{1},z_{1})\\cdot(w_{2},y_{2},1)\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(w_{1}+w_{2}+1,\\mathrm{Redc}(y_{1}y_{2}),z_{1});\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>y</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(w_{1},y_{1},z_{1})\\cdot(w_{2},1,z_{2})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>,</mo><mn>1</mn><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(w_{1}+w_{2}-1,y_{1},\\mathrm{Redc}(z_{1}z_{2})).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mrow><mi>Redc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>z</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n  which implies\n  \n", "itemtype": "equation", "pos": 74914, "prevtext": "\n\\iffalse\n\\begin{alignat*}{13}\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, y_2,\\, &&z_2 &&\\big) &&= \\big(w_1 + w_2, \\,&&\\mathrm{Redc}(y_1 \\cdot y_2),\\,&&\\mathrm{Redc}(z_1 \\cdot z_2)&&\\big); \\\\\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, y_2,\\, &&1&&\\big) &&= \\big(w_1 + w_2 + 1,\\,&&\\mathrm{Redc}(y_1 \\cdot y_2),\\,&&z_1&&\\big); \\\\\n  &\\big(w_1, y_1, z_1\\big) \\cdot \\big(w_2, 1,\\, &&z_2&&\\big) &&= \\big(w_1 + w_2 - 1,\\,&&y_1,\\,&&\\mathrm{Redc}(z_1 \\cdot z_2)&&\\big).\n\\end{alignat*}\n\\fi\n\n\\section{Performance measurement}\n\nAs our test platforms we used an Intel Westmere i7-970 \\SI{3.2}{\\giga\\hertz} CPU (with \\SI{12}{MiB} L3 cache) and an Intel Haswell i7-4790K \\SI{4.0}{GHz} CPU (with \\SI{8}{MiB} L3 cache).  Both of these processors support the {\\texttt{{PCLMULQDQ}}} instruction for carry-less multiplication, Westmere being the first Intel architecture to support it; on the much more recent Haswell architecture, where this instruction has significantly lower cost, alternative modular reduction routines based on it are used for ${\\mathbb{F}}_{2^{{163}}}$, ${\\mathbb{F}}_{2^{{283}}}$, and ${\\mathbb{F}}_{2^{{571}}}$ for a modest gain in performance~\\cite{bluhm2013fast}.    Our implementation used a word size of $W = 128$ bits and a block size of $B = 8$ bits for all half trace and multi-squaring tables.  All code was compiled separately for each architecture using version 3.5 of the Clang compiler at the highest optimization level.\n\n\\subsection{Robust operation timing}\n\\label{sec:execution-time-measurement}\n\nWe measured the execution time of all operations in CPU cycles, using the combination of {\\texttt{{RDTSC}}}, {\\texttt{{RDTSCP}}}, and {\\texttt{{CPUID}}} instructions recommended by Intel~\\cite{paoloni2010benchmark}.  To improve accuracy and reduce variance, we disabled TurboBoost, frequency scaling, and HyperThreading, and ensured that a single non-boot CPU core was used for all benchmarks on each machine.  For each operation, we estimated the benchmarking overhead and subtracted it from the measured number of cycles.  Additionally, we automatically determined a per-measurement repeat count for each operation that ensured the benchmarking overhead was less than 10\\%.\n\nThe execution time was computed as the median of the cycle measurements; the number of cycle measurements for each operation from which the median was computed was at least 1000 and chosen automatically to ensure a sufficiently small 99\\% confidence interval on the median estimate (less than the larger of $\\sfrac{1}{1000}$ of the estimated median or $\\sfrac{1}{10}$ of a cycle).  For consistency, we ensured warm-cache conditions for all estimates by discarding the first 2000 measurements.\n\n\\subsection{Consistent measurement of memory-dependent operations}\n\\label{sec:memory-dependent-measurement}\nFor operations with data-dependent memory accesses, such as table-based multi-squaring, half trace computation, and the higher-level operations based on these primitives, we measured the aggregate execution time for a set of inputs guaranteed to induce a uniform memory access pattern (and then divided by the number of inputs), in order to obtain worst-case warm-cache estimates.  Failure to do so results in a large underestimate of execution time.\n\nWe also observed the performance characteristics of table operations to be significantly affected by the size of the virtual memory pages backing the tables; in particular, on the x86-64 test machines, both the base level performance and the scaling of execution times with increasing table size were significantly better with \\SI{2}{MiB} (huge) pages than with \\SI{4}{KiB} pages, due to the cost of translation lookaside buffer (TLB) misses.  The Linux transparent huge page support (introduced in Linux version 2.6.38) results in some, but not all, memory regions being backed automatically by huge pages, depending on a number of factors including region alignment and physical memory fragmentation; when not taken into account, this significantly reduced the reliability of our performance measurements.  For consistent performance, we therefore ensured that all lookup tables were backed by huge pages.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Results}\n\\label{sec:performance-results}\n\nIn order to obtain performance results for a full range of security levels, we evaluated the performance of ECMH using each of the following eight elliptic curves: sect163k1~\\cite{sec2-1} (NIST K-163~\\cite{fips186-4}), sect193r1~\\cite{sec2-1}, sect233k1~\\cite{sec2-1} (NIST K-233~\\cite{fips186-4}), sect239k1~\\cite{sec2-1}, GLS254~\\cite{oliveira2014two}, sect283k1~\\cite{sec2-1} (NIST K-283~\\cite{fips186-4}), sect409k1~\\cite{sec2-1} (NIST K-409~\\cite{fips186-4}), and sect571k1~\\cite{sec2-1} (NIST K-571~\\cite{fips186-4}).\n\n\\begin{figure}\n  \\centering\n  \\includegraphics{hash_size_plot.pdf}\n  \\caption{Security level attained as a function of hash code size for Elliptic Curve Multiset Hash (ECMH), {\\texttt{{AdHash}}}{} (restricted to \\emph{set} hashing), and {\\texttt{{MuHash}}}{}.  For {\\texttt{{MuHash}}}{}, the \\emph{multiset} hashing security level was determined based on the conjectured time-complexity $L_p\\left[1/3, \\sqrt[3]{64/9}\\right]$ of the Number Field Sieve for solving discrete logarithms in ${\\mathbb{Z}}_p^\\times$~\\cite[p.~128]{menezes2010handbook}.  For {\\texttt{{AdHash}}}{}, we determined a \\emph{set} hashing security level of $2 \\sqrt{n}$ corresponded to groups ${\\mathbb{Z}}_{2^n}$ based on the assumption that the generalized birthday attack~\\cite{wagner2002generalized} is optimal.}\n  \\label{fig:hash_code_size}\n\\end{figure}\n\nBased on \\cref{thm:dlpreduction} and the assumed hardness of the Elliptic Curve Discrete Logarithm Problem, the ECMH using an elliptic curve group of order $\\rho$ has collision resistance of $O(\\sqrt{\\rho})$, corresponding to a security level $\\log_2 \\rho$ bits.  We also evaluated {\\texttt{{MuHash}}}{} and {\\texttt{{AdHash}}}{} (for set hashing only) using group sizes corresponding to the same range of security levels.  The correspondence between security level and hash code size under each method is shown in \\cref{fig:hash_code_size}.\n\n\\begin{figure*}\n  \\centering\n  \\includegraphics{multiset_hash_cycle_plot.pdf}\n  \n  \\label{fig:multiset_hash_cycles}\n\\end{figure*}\n\nFor each multiset hash $H$, we measured the computational cost of incremental hash code updates corresponding to a sequence of incremental additions or removals of multiset elements, i.e.\\ incrementing or decrementing by 1 the multiplicity of each element in the sequence.  Larger changes in multiplicity can also be handled efficiently by scalar multiplication in the group, but we expect incremental additions and removals to be the most common case.  We used a sequence of 1024 randomly generated 32-byte strings;\\footnote{As ECMH depends on lookup tables with a block size of $B = 8$, 1024 random elements ensures high coverage of the tables and a random access pattern, in order to correctly estimate execution time, as described in \\cref{sec:memory-dependent-measurement}.} longer strings would simply impose an additional cost independent of $H$.\n\nThe average cost per element reflects the cost of the intermediate hash function based on BLAKE2, the cost of encoding the expanded bit sequence as a group element, and the cost of one group operation to add the encoded element to a running total.  In the case of ECMH, the encoding is \\textsc{SWChar2} and the group operation is implemented as the mixed addition of a $\\lambda$-affine and a $\\lambda$-projective point; batch ECMH effectively replaces 1 field inversion by 3 multiplications, as described in \\cref{sec:batch-swchar2}.  In the case of {\\texttt{{AdHash}}}{}, the encoding is trivial and the group operation is simply integer addition; batch computation would offer no advantage.  For {\\texttt{{MuHash}}}{}, the encoding requires a comparison and at most one subtraction, and the group operation requires just a single Montgomery multiplication, as described in \\cref{sec:muhash-montgomery}; batch computation would offer no advantage over the Montgomery representation already used.\n\nThe results are shown in \\cref{fig:multiset_hash_cycles} and in \\cref{tab:multiset_hash_cycles}.  Only element addition performance is shown, as due to the representations used, element removal performance is nearly identical.  Timings for point encoding, compression, and decompression are given in \\cref{tab:elliptic_curve_operation2_cycles}.  Base field operation timings are given in \\cref{tab:field_operation_cycles}, and a comparison of curve operation performance under $\\lambda$-affine and $\\lambda$-projective point representations is given in \\cref{tab:elliptic_curve_operation_cycles}.\n\n\n\\begin{table*}[p]\n  \\caption{Comparison of multiset hashing performance, as in \\cref{fig:multiset_hash_cycles}.  Note that the {\\texttt{{AdHash}}}{} performance applies only to \\emph{set} hashing.}\n  \\centerfloat\n  \\footnotesize\n\n\\begin{tabular}{llrc@{/}c@{/}c@{/}cccrc@{/}c@{/}c@{/}ccc}\n\n& & & \\multicolumn{6}{c}{Westmere cycles} & & \\multicolumn{6}{c}{Haswell cycles}\\\\\n\\cmidrule(r){4-9}\\cmidrule(r){11-16}& &  & \\multicolumn{4}{c}{ECMH} &  &  &  & \\multicolumn{4}{c}{ECMH} &  & \\\\\n\n$n$&Curve &  & single & blind & batch & blind & {\\texttt{{MuHash}}}{} & {\\texttt{{AdHash}}}{} &  & single & blind & batch & blind & {\\texttt{{MuHash}}}{} & {\\texttt{{AdHash}}}{}\\\\\n\\midrule\n81&sect163k1& & 3601&4436&2023&2418 & 3939 & 2998& & 2199&2556&1133&1349 & 2208 & 2186\\\\\n96&sect193r1& & 4326&5444&2595&3198 & 7384 & 3687& & 2342&2755&1287&1580 & 3967 & 2674\\\\\n116&sect233k1& & 4667&5726&2444&2933 & 13414 & 5160& & 2605&2968&1209&1495 & 7074 & 3708\\\\\n119&sect239k1& & 5183&6361&2630&3164 & 16532 & 5117& & 3061&3474&1422&1700 & 8537 & 3684\\\\\n127&GLS254& & 2835&3872&2307&2882 & 20631 & 5920& & 1592&1973&1184&1426 & 10472 & 4239\\\\\n141&sect283k1& & 7524&9271&3513&4254 & 33472 & 7286& & 3828&4291&1733&2024 & 17251 & 5178\\\\\n204&sect409k1& & 12621&16696&5686&6878 & 176767 & 14997& & 5788&6897&2473&2948 & 84027 & 10632\\\\\n285&sect571k1& & 23654&29628&9206&10746 & 890172 & 28485& & 11745&16664&4188&4940 & 467938 & 20152\\\\\n\n\n\\end{tabular}\n  \\label{tab:multiset_hash_cycles}\n\\end{table*}\n\n\n\\begin{table*}[p]\n  \\caption{Performance of elliptic curve point encoding, compression (to a minimal-length bit string), and decompression (from said bit string).  Compression (comp.) and decompression (dec.) use $\\lambda$-projective coordinates.  Batch encoding is with a batch size of $256$.}\n  \\centering\n  \\footnotesize\n\n\\begin{tabular}{lrc@{/}c@{/}c@{/}cccrc@{/}c@{/}c@{/}ccc}\n\n & & \\multicolumn{6}{c}{Westmere cycles} & & \\multicolumn{6}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-8}\\cmidrule(r){10-15} &  & \\multicolumn{4}{c}{\\textsc{SWChar2}} &  &  &  & \\multicolumn{4}{c}{\\textsc{SWChar2}} &  & \\\\\n\nCurve &  & single & blind & batch & blind & Comp. & Dec. &  & single & blind & batch & blind & Comp. & Dec.\\\\\n\\midrule\nsect163k1& & 2629&3248&853&1234 & 2222 & 2268& & 1500&1854&432&641 & 1370 & 1402\\\\\nsect193r1& & 3217&4073&1227&1821 & 2541 & 2669& & 1603&2014&548&823 & 1370 & 1443\\\\\nsect233k1& & 3577&4340&1076&1537 & 3080 & 3154& & 1852&2214&510&719 & 1673 & 1732\\\\\nsect239k1& & 4033&4880&1158&1670 & 3468 & 3569& & 2227&2616&570&817 & 2024 & 2045\\\\\nGLS254& & 1671&2551&978&1566 & 1166 & 1245& & 874&1280&437&709 & 665 & 716\\\\\nsect283k1& & 5738&6865&1587&2253 & 4772 & 5134& & 2822&3278&698&993 & 2320 & 2624\\\\\nsect409k1& & 8612&10523&2707&3720 & 7497 & 7563& & 4395&5172&1157&1573 & 3883 & 4074\\\\\nsect571k1& & 17968&21721&4301&5867 & 15174 & 16337& & 8987&13329&1867&2573 & 7132 & 8448\\\\\n\n\n\\end{tabular}\n  \\label{tab:elliptic_curve_operation2_cycles}\n\\end{table*}\n\n\\clearpage\n\n\\begin{table*}[p]\n  \\caption{Field operation performance for ${\\mathbb{F}}_{2^m}$.  Batch inversion is with a batch size of $256$.}\n  \\centering\n  \\footnotesize\n\\begin{tabular}{lrccc@{/}c@{/}c@{/}cc@{/}crccc@{/}c@{/}c@{/}cc@{/}c}\n\n & & \\multicolumn{8}{c}{Westmere cycles} & & \\multicolumn{8}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-10}\\cmidrule(r){12-19} &  &  &  & \\multicolumn{4}{c}{Invert} & \\multicolumn{2}{c}{QS} &  &  &  & \\multicolumn{4}{c}{Invert} & \\multicolumn{2}{c}{QS}\\\\\n\n$m$ &  & Mul. & Sq. & single & blind & batch & blind & var. & blind &  & Mul. & Sq. & single & blind & batch & blind & var. & blind\\\\\n\\midrule\n127& & 44 & 11 & 721&904&124&124 & 41&131& & 23 & 9 & 435&534&59&59 & 21&82\\\\\n163& & 84 & 32 & 1807&2074&266&269 & 115&227& & 43 & 24 & 1159&1309&127&127 & 67&152\\\\\n193& & 113 & 26 & 2210&2533&343&344 & 149&264& & 46 & 20 & 1129&1308&128&128 & 79&155\\\\\n233& & 109 & 30 & 2745&3092&341&342 & 191&313& & 48 & 24 & 1439&1583&131&131 & 95&178\\\\\n239& & 119 & 34 & 3139&3492&372&376 & 187&313& & 51 & 31 & 1755&1933&160&159 & 93&183\\\\\n254& & 99 & 17 & 868&1211&310&313 & 88&245& & 38 & 15 & 514&664&111&116 & 62&158\\\\\n283& & 148 & 36 & 4438&4869&473&474 & 420&560& & 55 & 28 & 2222&2423&175&179 & 227&296\\\\\n409& & 274 & 35 & 7899&8688&884&867 & 807&959& & 93 & 30 & 3500&3805&291&296 & 404&478\\\\\n571& & 431 & 65 & 16217&17808&1308&1323 & 1457&1692& & 168 & 38 & 6873&7611&464&489 & 737&842\\\\\n\n\n\\end{tabular}\n  \\label{tab:field_operation_cycles}\n\\end{table*}\n\n\\begin{table*}[p]\n  \\caption{Performance of elliptic curve group operations using $\\lambda$-affine and $\\lambda$-projective point representations.  The result of point addition or doubling is always represented in $\\lambda$-projective coordinates.}\n  \\centering\n  \\footnotesize\n\\begin{tabular}{lrc@{/}c@{/}cc@{/}cc@{/}crc@{/}c@{/}cc@{/}cc@{/}c}\n\n & & \\multicolumn{7}{c}{Westmere cycles} & & \\multicolumn{7}{c}{Haswell cycles}\\\\\n\\cmidrule(r){3-9}\\cmidrule(r){11-17} &  & \\multicolumn{3}{c}{Add} & \\multicolumn{2}{c}{Double} & \\multicolumn{2}{c}{Negate} &  & \\multicolumn{3}{c}{Add} & \\multicolumn{2}{c}{Double} & \\multicolumn{2}{c}{Negate}\\\\\n\nCurve &  & aff. & mix. & full & aff. & proj. & aff. & proj. &  & aff. & mix. & full & aff. & proj. & aff. & proj.\\\\\n\\midrule\nsect163k1& & 500&748&1016 & 192&472 & 12&18& & 213&305&408 & 105&188 & 6&9\\\\\nsect193r1& & 604&952&1268 & 199&640 & 12&18& & 236&370&468 & 105&281 & 7&9\\\\\nsect233k1& & 624&936&1276 & 202&540 & 12&18& & 235&341&462 & 107&224 & 7&10\\\\\nsect239k1& & 684&1032&1388 & 244&620 & 12&18& & 308&453&595 & 144&311 & 6&9\\\\\nGLS254& & 572&856&1168 & 162&488 & 9&14& & 219&308&435 & 78&196 & 5&9\\\\\nsect283k1& & 864&1308&1792 & 280&768 & 15&23& & 329&489&655 & 138&302 & 12&19\\\\\nsect409k1& & 1496&2320&3144 & 416&1280 & 18&29& & 542&788&1075 & 194&506 & 12&22\\\\\nsect571k1& & 2276&3516&4772 & 644&1956 & 21&35& & 859&1253&1688 & 296&740 & 15&22\\\\\n\n\n\\end{tabular}\n  \\label{tab:elliptic_curve_operation_cycles}\n\\end{table*}\n\n\n\n\\clearpage\n\n\\section{Discussion}\n\\label{sec:discussion}\n\n\nElliptic curve multiset hash significantly outperforms the existing methods of\n{\\texttt{{MuHash}}}{} and {\\texttt{{AdHash}}}{}, particularly in the batch setting, while requiring\nsignificantly smaller hash codes at all security levels.  In fact, the hash code\nsize is essentially optimal.  Because the single field inversion required by the\nencoding function \\textsc{SWChar2} accounts for a large fraction of the\ncomputational cost, particularly with larger field degrees, the use of\nMontgomery's trick in the batch setting significantly reduces the computational\ncost.  The lower computational cost at the 127-bit security level is due to the\nefficiency of the GLS254 curve implementation; the quadratic extension field\nrepresentation of ${\\mathbb{F}}_{2^{{254}}}$ employed, and the close match of the degree to the\nword size $W = 128$, significantly reducing the cost of field operations.\nQuadratic extension field representations for other fields, such as ${\\mathbb{F}}_{2^{{502}}}$,\ncould potentially be used to obtain similar performance improvements at other\nsecurity levels. Furthermore, our choice of parameters follows the trend of\nincreasing native support to binary field arithmetic in Desktop\nprocessors and will likely benefit from improvements to the carry-less\nmultiplication instruction in the recently released Broadwell processor family.\n\nOur work is very related to the Encrypted Elliptic Curve Hash\n(EECH)~\\cite{brown2008encrypted}.  That construction also encodes\nseparate bit strings as points on a binary elliptic curve and then\ncombines those points using point addition.  Like our approach, it relies on the property of binary elliptic curves that curve points can be decoded from a non-redundant representation without expensive field exponentiations, using instead a precomputed lookup table for half-trace, and notes that better performance may be obtained using batch inversion and hardware support for carry-less multiplication.\n\nThe full EECH construction is proposed as an\nincremental hash for bit strings (the message is split into fixed-size\nblocks, and each block, concatenated with the block index, is encoded as\nan elliptic curve point).  In contrast to our elliptic curve multiset hash, it is\nspecifically designed to \\emph{avoid} reliance on an underlying random\noracle, relying instead on redundancy/padding in the point encoding\nfunction for collision resistance.\n\nWhile the \\emph{full} construction is not well-suited to homomorphic multiset hashing\\footnote{Using an elliptic curve over ${\\mathbb{F}}_{2^{{m}}}$, under the EECH construction at most $b$ bits of input data can be encoded per point to retain collision resistance of $2^{m - b}$.  Optimal collision resistance of $2^m$ for the representation size requires that $b \\leq m / 2$.  Each multiset element $a \\in A$ (assumed to be a bit string) must therefore be split into one or more blocks of $b$ bits, each encoded as a separate elliptic curve point.  For elements longer than $b$ bits, this is likely to be significantly more expensive than hashing $a$ with a fast hash function like BLAKE2 and then encoding the result into a single elliptic curve point.  EECH also offers no preimage resistance by default.  There is a proposed pairing-based variant PEECH that relies on an elliptic curve pairing to define a homomorphic one-way function.  This provides preimage resistance at the cost of significantly higher computational cost and representation size.}, we can make the fairer comparison between our ECMH construction and a straightforward randomize-then-combine-style~\\cite{bellare1997new} construction over binary elliptic curve groups using the implementation techniques proposed for EECH.  Such a construction was neither explicitly proposed nor implemented, and there was no prior evidence that it would be practical performance wise.  Our work goes significantly beyond this:\n\\begin{itemize}\n\\item We provide a thorough empirical analysis of performance, and demonstrate for the first time that an elliptic curve-based multiset hash actually significantly exceeds the performance of {\\texttt{{AdHash}}}{} and {\\texttt{{MuHash}}}{}.\n\\item We demonstrate that a fully blinded implementation is possible at only a minor performance penalty.  We also demonstrate batch variants of both the regular and fully-blinded implementations that are significantly faster.  In contrast, the try-and-increment encoding method proposed for EECH has no guaranteed time bound, making it unavoidably susceptible to timing attacks, and less amenable to speedup by batch inversion.\n\\item Our security proof is based on existing techniques~\\cite{bellare1997new,brier2010efficient} but the security bound we obtain is novel in several ways:\n  \\begin{itemize}\n  \\item The hash function $\\hat{H}$ into the elliptic curve group need not be indistinguishable from a random oracle, but is instead permitted to satisfy the weaker property of being an $(\\alpha, \\beta)$-weak encoding, which significantly reduces the computational cost.\n  \\item The hash function $\\hat{H}$ can map to the full elliptic curve group, rather than only a cyclic subgroup, as is required by EECH.  This allows for a simpler implementation that does not rely on patent-encumbered techniques~\\cite{brown2007method} for efficiently testing for subgroup membership.\n  \\end{itemize}\n\\end{itemize}\n\nIt was originally suggested~\\cite{bellare1997new} that while finding collisions in {\\texttt{{MuHash}}}{} is provably as hard as the Discrete Logarithm Problem (DLP), the converse is not necessarily true: it may be that {\\texttt{{MuHash}}}{} is still collision resistant even if discrete logarithms can be computed efficiently.  In fact, though, by computing discrete logarithms, finding a collision in {\\texttt{{MuHash}}}{} can be reduced to finding a collision in {\\texttt{{AdHash}}}{}.  It would therefore be susceptible to a generalized birthday attack~\\cite{wagner2002generalized} in the set hashing setting or to lattice reduction attacks in the multiset hashing setting.  The same reduction applies to our elliptic curve multiset hash, and is even more effective because of the smaller group order.\n\n\n\n\n\\begin{thebibliography}{99}\n\n\\bibitem{brown2008encrypted}\nBrown, D. R.~L. (2008) The encrypted elliptic curve hash.\n\\newblock {\\em IACR Cryptology ePrint Archive}, {\\bf  2008}, 12.\n\n\\bibitem{brown2007method}\nBrown, D. and Yamada, A. (2007).\n\\newblock Method and apparatus for performing validation of elliptic curve\n  public keys.\n\n\\bibitem{DBLP:conf/infocom/GkantsidisR06}\nGkantsidis, C. and Rodriguez, P. (2006) Cooperative security for network coding\n  file distribution.\n\\newblock {\\em INFOCOM}. {IEEE}.\n\n\\bibitem{DBLP:conf/sp/KrohnFM04}\nKrohn, M.~N., Freedman, M.~J., and Mazi{\\`{e}}res, D. (2004) On-the-fly\n  verification of rateless erasure codes for efficient content distribution.\n\\newblock {\\em IEEE S{\\&}P},  pp. 226--240. {IEEE} Computer Society.\n\n\\bibitem{DBLP:conf/nsdi/SubramanianRSSK04}\nSubramanian, L., Roth, V., Stoica, I., Shenker, S., and Katz, R.~H. (2004)\n  Listen and whisper: Security mechanisms for {BGP}.\n\\newblock In Morris, R. and Savage, S. (eds.), {\\em USENIX NSDI},  pp.\n  127--140. {USENIX}.\n\n\\bibitem{DBLP:conf/osdi/CastroL99}\nCastro, M. and Liskov, B. (1999) Practical byzantine fault tolerance.\n\\newblock In Seltzer, M.~I. and Leach, P.~J. (eds.), {\\em USENIX OSDI},  pp.\n  173--186. {USENIX} Association.\n\n\\bibitem{DBLP:journals/tocs/CastroL02}\nCastro, M. and Liskov, B. (2002) Practical byzantine fault tolerance and\n  proactive recovery.\n\\newblock {\\em {ACM} Trans. Comput. Syst.}, {\\bf  20}, 398--461.\n\n\\bibitem{cathalo2009comparing}\nCathalo, J., Naccache, D., and Quisquater, J.-J. (2009) Comparing with {RSA}.\n\\newblock {\\em IMACC},  pp. 326--335. Springer.\n\n\\bibitem{DBLP:conf/ccs/2008}\nNing, P., Syverson, P.~F., and Jha, S. (eds.) (2008) {\\em Proceedings of the\n  2008 {ACM} Conference on Computer and Communications Security, {CCS} 2008,\n  Alexandria, Virginia, USA, October 27-31, 2008}. {ACM}.\n\n\\bibitem{clarke2003incremental}\nClarke, D., Devadas, S., Van~Dijk, M., Gassend, B., and Suh, G.~E. (2003)\n  Incremental multiset hash functions and their application to memory integrity\n  checking.\n\\newblock {\\em ASIACRYPT},  pp. 188--207. Springer.\n\n\\bibitem{bellare1997new}\nBellare, M. and Micciancio, D. (1997) A new paradigm for collision-free\n  hashing: Incrementality at reduced cost.\n\\newblock {\\em EUROCRYPT},  pp. 163--192. Springer.\n\n\\bibitem{brier2010efficient}\nBrier, E., Coron, J.-S., Icart, T., Madore, D., Randriam, H., and Tibouchi, M.\n  (2010) Efficient indifferentiable hashing into ordinary elliptic curves.\n\\newblock {\\em CRYPTO},  pp. 237--254. Springer.\n\n\\bibitem{shallue2006construction}\nShallue, A. and van~de Woestijne, C.~E. (2006) Construction of rational points\n  on elliptic curves over finite fields.\n\\newblock {\\em ANTS},  pp. 510--524. Springer.\n\n\\bibitem{aranha2014binary}\nAranha, D.~F., Fouque, P.-A., Qian, C., Tibouchi, M., and Zapalowicz, J.-C.\n  (2014) {Binary Elligator Squared}.\n\\newblock {\\em SAC},  pp. 20--37. Springer.\n\n\\bibitem{aumasson2013blake2}\nAumasson, J.-P., Neves, S., Wilcox-O\u00e2\u0080\u0099Hearn, Z., and Winnerlein, C. (2013)\n  {BLAKE2}: simpler, smaller, fast as {MD5}.\n\\newblock {\\em ACNS},  pp. 119--135. Springer.\n\n\\bibitem{semaev2015}\nSemaev, I. (2015).\n\\newblock New algorithm for the discrete logarithm problem on elliptic curves.\n\\newblock Cryptology ePrint Archive, Report 2015/310.\n\\newblock \\url{http://eprint.iacr.org/}.\n\n\\bibitem{kosters2015}\nKosters, M. and Yeo, S.~L. (2015).\n\\newblock Notes on summation polynomials.\n\\newblock arXiv:1503.08001.\n\n\\bibitem{DBLP:conf/crypto/HuangKY15}\nHuang, M.~A., Kosters, M., and Yeo, S.~L. (2015) Last fall degree, hfe, and\n  weil descent attacks on {ECDLP}.\n\\newblock In Gennaro, R. and Robshaw, M. (eds.), {\\em Advances in Cryptology -\n  {CRYPTO} 2015 - 35th Annual Cryptology Conference, Santa Barbara, CA, USA,\n  August 16-20, 2015, Proceedings, Part {I}},  Lecture Notes in Computer\n  Science, {\\bf9215},  pp. 581--600. Springer.\n\n\\bibitem{galbraith2015ellipticnews}\nGalbraith, S. (2015).\n\\newblock Elliptic curve discrete logarithm problem in characteristic two.\n\\newblock\n  \\url{https://ellipticnews.wordpress.com/2015/04/13/elliptic-curve-discrete-logarithm-problem-in-characteristic-two/}.\n\n\\bibitem{krohn2004fly}\nKrohn, M.~N., Freedman, M.~J., and Mazieres, D. (2004) On-the-fly verification\n  of rateless erasure codes for efficient content distribution.\n\\newblock {\\em IEEE S\\&P},  pp. 226--240. IEEE.\n\n\\bibitem{maurer2004indifferentiability}\nMaurer, U.~M., Renner, R., and Holenstein, C. (2004) Indifferentiability,\n  impossibility results on reductions, and applications to the random oracle\n  methodology.\n\\newblock In Naor, M. (ed.), {\\em TCC},  Lecture Notes in Computer Science,\n  {\\bf2951},  pp. 21--39. Springer.\n\n\\bibitem{boneh2001identity}\nBoneh, D. and Franklin, M. (2001) Identity-based encryption from the {W}eil\n  pairing.\n\\newblock {\\em CRYPTO},  pp. 213--229. Springer.\n\n\\bibitem{wagner2002generalized}\nWagner, D. (2002) A generalized birthday problem.\n\\newblock {\\em CRYPTO},  pp. 288--304. Springer.\n\n\\bibitem{impagliazzo1996efficient}\nImpagliazzo, R. and Naor, M. (1996) Efficient cryptographic schemes provably as\n  secure as subset sum.\n\\newblock {\\em Journal of Cryptology}, {\\bf  9}, 199--216.\n\n\\bibitem{menezes2010handbook}\nMenezes, A.~J., Van~Oorschot, P.~C., and Vanstone, S.~A. (2010) {\\em Handbook\n  of applied cryptography}. CRC press.\n\n\\bibitem{smart2010ecryptkeysizes}\nSmart, N.~P. et al. (2010) {ECRYPT~II} yearly report on algorithms and key\n  lengths. Technical report. European Network of Excellence in Cryptology II.\n\\newblock \\url{http://www.ecrypt.eu.org/documents/D.SPA.13.pdf}.\n\n\\bibitem{hankerson2004guide}\nHankerson, D., Vanstone, S., and Menezes, A.~J. (2004) {\\em Guide to elliptic\n  curve cryptography}. Springer.\n\n\\bibitem{oliveira2014two}\nOliveira, T., L{\\'o}pez, J., Aranha, D.~F., and Rodr{\\'\\i}guez-Henr{\\'\\i}quez,\n  F. (2014) Two is the fastest prime: lambda coordinates for binary elliptic\n  curves.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  4}, 3--17.\n\n\\bibitem{taverne2011jcen}\nTaverne, J., Faz-Hern\\'andez, A., Aranha, D.~F., Rodr\u00c3\u00adguez-Henr\u00c3\u00adquez, F.,\n  Hankerson, D., and L\u00c3\u00b3pez, J. (2011) Speeding scalar multiplication over\n  binary elliptic curves using the new carry-less multiplication instruction.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  1}, 187--199.\n\n\\bibitem{maitinshepard2015ecmhLibrary}\nMaitin-Shepard, J.\n\\newblock {C++} {Elliptic} {Curve} {Multiset} {Hash} library.\n\\newblock \\url{http://jeremyms.com/ecmh}.\n\n\\bibitem{fips186-4}\n{National Institute of Standards and Technology} (2013) {FIPS 186-4}: {Digital\n  Signature Standard (DSS)}, {Federal Information Processing Standard (FIPS)},\n  publication 186-4. Technical report. Department of Commerce,  Gaithersburg,\n  MD, USA.\n\n\\bibitem{sec2-1}\nResearch, C. (2000) {\\em {{SEC 2}: {Recommended Elliptic Curve Domain\n  Parameters}}}. Standards for Efficient Cryptography.\n\\newblock Version 1.0.\n\n\\bibitem{bluhm2013fast}\nBluhm, M. and Gueron, S. (2015) Fast software implementation of binary elliptic\n  curve cryptography.\n\\newblock {\\em Journal of Cryptographic Engineering}, {\\bf  5}, 215--226.\n\n\\bibitem{guajardo2002itoh}\nGuajardo, J. and Paar, C. (2002) Itoh--{T}sujii inversion in standard basis and\n  its application in cryptography and codes.\n\\newblock {\\em Designs, Codes and Cryptography}, {\\bf  25}, 207--216.\n\n\\bibitem{cryptoeprint:2015:028}\nMaitin-Shepard, J. (2015).\n\\newblock Optimal software-implemented {Itoh--Tsujii} inversion for\n  $\\mathrm{GF}(2^m)$.\n\\newblock Cryptology ePrint Archive, Report 2015/028.\n\\newblock \\url{http://eprint.iacr.org/}.\n\n\\bibitem{bertoni2009keccak}\nBertoni, G., Daemen, J., Peeters, M., and Van~Assche, G. (2009) Keccak sponge\n  function family main document.\n\\newblock {\\em Submission to NIST (Round 2)}, {\\bf  3}.\n\n\\bibitem{bos2010ecc2k}\nBos, J.~W., Kleinjung, T., Niederhagen, R., and Schwabe, P. (2010) {ECC2K}-130\n  on cell {CPUs}.\n\\newblock {\\em AFRICACRYPT},  pp. 225--242. Springer.\n\n\\bibitem{shacham2001improving}\nShacham, H. and Boneh, D. (2001) Improving {SSL} handshake performance via\n  batching.\n\\newblock {\\em CT-RSA},  pp. 28--43. Springer.\n\n\\bibitem{montgomery1985modular}\nMontgomery, P.~L. (1985) Modular multiplication without trial division.\n\\newblock {\\em Mathematics of Computation}, {\\bf  44}, 519--521.\n\n\\bibitem{paoloni2010benchmark}\nPaoloni, G. (2010) How to benchmark code execution times on {Intel} {IA-32} and\n  {IA-64} instruction set architectures. Technical report.\n\n\\bibitem{DBLP:conf/eurocrypt/GamaN08}\nGama, N. and Nguyen, P.~Q. (2008) Predicting lattice reduction.\n\\newblock In Smart, N.~P. (ed.), {\\em EUROCRYPT},  Lecture Notes in Computer\n  Science, {\\bf4965},  pp. 31--51. Springer.\n\n\\bibitem{chen2011bkz}\nChen, Y. and Nguyen, P.~Q. (2011) {BKZ} 2.0: Better lattice security estimates.\n\\newblock In Lee, D.~H. and Wang, X. (eds.), {\\em ASIACRYPT},  Lecture Notes in\n  Computer Science, {\\bf7073},  pp. 1--20. Springer.\n\n\\bibitem{DBLP:conf/ima/PolS13}\nvan~de Pol, J. and Smart, N.~P. (2013) Estimating key sizes for high\n  dimensional lattice-based systems.\n\\newblock In Stam, M. (ed.), {\\em IMACC},  Lecture Notes in Computer Science,\n  {\\bf8308},  pp. 290--303. Springer.\n\n\\bibitem{lindner2014latticechallenge}\nLindner, R. et al.\n\\newblock {TU Darmstadt} lattice challenge: Hall of fame.\n\\newblock \\url{http://www.latticechallenge.org/halloffame.php}, accessed 17\n  October 2014.\n\n\\end{thebibliography}\n\\appendix\n\n\\section{Security reduction based on ($\\alpha, \\beta$)-weak encodings}\n\\label{sec:proofthmdlpreduction}\n\nWe prove \\cref{thm:dlpreduction}, which reduces solving discrete logarithms to finding collisions in a homomorphic multiset hash function based on an $(\\alpha, \\beta)$-weak encoding.\n\n\\dlpreduction*\n\\begin{proof}\n  Let a $Q \\in \\langle P \\rangle$, for which we wish to find $n \\in {\\mathbb{Z}}$ such that $n \\cdot P = Q$, be given.  We simulate each successive distinct query $h(a_i)$ to the random oracle $h$ for $i = 1, \\ldots, k$ using the following algorithm:\n  \\begin{enumerate}\n  \\item Sample uniformly at random $r_i \\in {\\mathbb{Z}}_\\rho$, $d_i \\in \\Set{0,1}$, $J_i \\in \\overline{\\langle P \\rangle}$, $j \\in {\\mathbb{Z}}_{\\lceil \\alpha \\rceil}$.\n  \\item Compute $Q_i = r_i Q + d_i P + J_i$.  Note that since $\\langle P \\rangle$ has prime order, $Q$ is a generator of $\\langle P \\rangle$, and therefore $Q_i$ is distributed uniformly in $G$.\n  \\item If $j < {\\lvert {f^{-1}(Q_i)} \\rvert}$, sample $x_i$ from $f^{-1}(Q_i)$ uniformly at random.  Otherwise, resample $r_i, d_i, J_i$, and $j$.\n  \\item Return $x_i$.  Note that $x_i$ is uniformly distributed in $X$, and the expected number of sampling attempts is $\\alpha / \\beta$.\n  \\end{enumerate}\n\n  Under the simulated $h$, $\\mathcal{C}$ finds a non-empty $M \\in \\ker H$ in expected time $t$ with success probability $\\epsilon$.  Consider the case that a collision is found.  (Otherwise, we fail to compute the discrete logarithm.)  Without loss of generality, we can assume $M$ is non-zero only for values $a_i$ on which $h$ was queried.  Thus, we have\n  \n", "index": 13, "text": "\\begin{align*}\n    0_G &= \\sum_{i=1}^k M(a_i) \\cdot h(a_i)\n        = \\sum_{i=1}^k M(a_i) \\cdot Q_i \\\\\n        &= \\sum_{i=1}^k M(a_i) \\cdot \\left[ r_i \\cdot Q + d_i \\cdot P + J_i \\right],\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 0_{G}\" display=\"inline\"><msub><mn>0</mn><mi>G</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i=1}^{k}M(a_{i})\\cdot h(a_{i})=\\sum_{i=1}^{k}M(a_{i})\\cdot&#10;Q%&#10;_{i}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mrow><mrow><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>h</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mrow><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><msub><mi>Q</mi><mi>i</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i=1}^{k}M(a_{i})\\cdot\\left[r_{i}\\cdot Q+d_{i}\\cdot P+J_{i}%&#10;\\right],\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mrow><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mrow><mo>[</mo><mrow><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>\u22c5</mo><mi>Q</mi></mrow><mo>+</mo><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>\u22c5</mo><mi>P</mi></mrow><mo>+</mo><msub><mi>J</mi><mi>i</mi></msub></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n  where\n  \n", "itemtype": "equation", "pos": 75134, "prevtext": "\n  which implies\n  \n", "index": 15, "text": "\\begin{equation}\n    \\label{eq:dlpreduction:eq-full-with-subs}\n    r \\cdot Q + \\sum_{i=1}^k J_i \\cdot M(a_i) = - d \\cdot P,\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"r\\cdot Q+\\sum_{i=1}^{k}J_{i}\\cdot M(a_{i})=-d\\cdot P,\" display=\"block\"><mrow><mrow><mrow><mrow><mi>r</mi><mo>\u22c5</mo><mi>Q</mi></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mrow><msub><mi>J</mi><mi>i</mi></msub><mo>\u22c5</mo><mi>M</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u22c5</mo><mi>P</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n  Since $r \\cdot Q \\in \\langle P \\rangle$ and $-d \\cdot P \\in \\langle P \\rangle$, it follows that $\\sum_{i=1}^k J_i \\cdot M(a_i) = O_G$ in \\cref{eq:dlpreduction:eq-full-with-subs}; we therefore have $r \\cdot Q = - d \\cdot P$.  Since $M$ is non-empty, there exists a value $i$ such that $M(a_i) \\not= 0$.  Consider that the distribution of $d_i$ conditioned on $Q_1, \\ldots, Q_k$ is still uniform in $\\Set{0,1}$, and therefore $\\Pr(d_i = 0) = 1/2$, and hence, $\\Pr(d = 0) \\leq 1/2$.  If $d \\not= 0$, then $r \\not= 0$, and therefore $\\Pr(r \\not= 0) \\geq 1/2$.\n\n  If $r = 0$, we fail to compute the discrete logarithm.  Otherwise, $r$ has an inverse $r^{-1}$ in ${\\mathbb{Z}}_\\rho^\\times$ and we have $Q = r^{-1} r Q = - r^{-1} d P$.  Thus, $n = - r^{-1} d$ is a solution to the discrete logarithm problem.  Since we only fail if $\\mathcal{C}$ fails or $r = 0$, we find a solution with probability at least $\\epsilon/2$.\n\nEach query $a_i$ to the simulated random oracle requires a table lookup to check if $a_i$ has been queried previously.  If it has not, we must repeatedly sample $r_i$, $d_i$, $J_i$ and $j$ and compute $Q_i = r_i Q + d_i P + J_i$ in time\n\\ifcompj\n\n", "itemtype": "equation", "pos": 75285, "prevtext": "\n  where\n  \n", "index": 17, "text": "\\begin{align*}\n    r &= \\sum_{i=1}^k r_i \\cdot M(a_i) \\bmod \\rho, &&\n    d = \\sum_{i=1}^k d_i \\cdot M(a_i) \\bmod \\rho.\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle r\" display=\"inline\"><mi>r</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i=1}^{k}r_{i}\\cdot M(a_{i})\\bmod\\rho,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>\u22c5</mo><mi>M</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mi>\u03c1</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle d=\\sum_{i=1}^{k}d_{i}\\cdot M(a_{i})\\bmod\\rho.\" display=\"inline\"><mrow><mrow><mi>d</mi><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>\u22c5</mo><mi>M</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mi>\u03c1</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\\else\n", "itemtype": "equation", "pos": 76584, "prevtext": "\n  Since $r \\cdot Q \\in \\langle P \\rangle$ and $-d \\cdot P \\in \\langle P \\rangle$, it follows that $\\sum_{i=1}^k J_i \\cdot M(a_i) = O_G$ in \\cref{eq:dlpreduction:eq-full-with-subs}; we therefore have $r \\cdot Q = - d \\cdot P$.  Since $M$ is non-empty, there exists a value $i$ such that $M(a_i) \\not= 0$.  Consider that the distribution of $d_i$ conditioned on $Q_1, \\ldots, Q_k$ is still uniform in $\\Set{0,1}$, and therefore $\\Pr(d_i = 0) = 1/2$, and hence, $\\Pr(d = 0) \\leq 1/2$.  If $d \\not= 0$, then $r \\not= 0$, and therefore $\\Pr(r \\not= 0) \\geq 1/2$.\n\n  If $r = 0$, we fail to compute the discrete logarithm.  Otherwise, $r$ has an inverse $r^{-1}$ in ${\\mathbb{Z}}_\\rho^\\times$ and we have $Q = r^{-1} r Q = - r^{-1} d P$.  Thus, $n = - r^{-1} d$ is a solution to the discrete logarithm problem.  Since we only fail if $\\mathcal{C}$ fails or $r = 0$, we find a solution with probability at least $\\epsilon/2$.\n\nEach query $a_i$ to the simulated random oracle requires a table lookup to check if $a_i$ has been queried previously.  If it has not, we must repeatedly sample $r_i$, $d_i$, $J_i$ and $j$ and compute $Q_i = r_i Q + d_i P + J_i$ in time\n\\ifcompj\n\n", "index": 19, "text": "\\begin{multline*}\nT_3 = T_{\\mathrm{samp}}({\\mathbb{Z}}_\\rho) + T_\\mathrm{samp}({\\mathbb{Z}}_2) +\\\\ T_\\mathrm{samp}(\\overline{\\langle g \\rangle}) + T_{\\exp(G)} + 2 T_\\mathrm{mult}(G),\n\\end{multline*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"p10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T_{3}=T_{\\mathrm{samp}}({\\mathbb{Z}}_{\\rho})+T_{\\mathrm{samp}}({%&#10;\\mathbb{Z}}_{2})+\\\\&#10;\\displaystyle T_{\\mathrm{samp}}(\\overline{\\langle g\\rangle})+T_{\\exp(G)}+2T_{%&#10;\\mathrm{mult}}(G),\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>T</mi><mn>3</mn></msub><mo>=</mo><mrow><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mrow><mo stretchy=\"false\">\u27e8</mo><mi>g</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>T</mi><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>T</mi><mi>mult</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nuntil $j < \\lvert f^{-1}(Q_i) \\rvert$, which requires $\\beta$ attempts in expectation, since $f$ is an $(\\alpha, \\beta)$-weak encoding.  We then sample $x_i \\in f^{-1}(Q_i)$.  Thus, each of the $q$ queries to the random oracle require expected time $\\beta T_3 + T_2$, where\n", "itemtype": "equation", "pos": 76788, "prevtext": "\n\\else\n", "index": 21, "text": "\n\\[\nT_3 = T_{\\mathrm{samp}}({\\mathbb{Z}}_\\rho) + T_\\mathrm{samp}({\\mathbb{Z}}_2) + T_\\mathrm{samp}(\\overline{\\langle g \\rangle}) + T_{\\exp(G)} + 2 T_\\mathrm{mult}(G),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"T_{3}=T_{\\mathrm{samp}}({\\mathbb{Z}}_{\\rho})+T_{\\mathrm{samp}}({\\mathbb{Z}}_{2%&#10;})+T_{\\mathrm{samp}}(\\overline{\\langle g\\rangle})+T_{\\exp(G)}+2T_{\\mathrm{mult%&#10;}}(G),\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mn>3</mn></msub><mo>=</mo><mrow><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mrow><mo stretchy=\"false\">\u27e8</mo><mi>g</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>T</mi><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>T</mi><mi>mult</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\n\nWe can compute $r$ and $d$ as a sum of $L$ terms in time $L \\cdot T_4$, where\n", "itemtype": "equation", "pos": 77231, "prevtext": "\nuntil $j < \\lvert f^{-1}(Q_i) \\rvert$, which requires $\\beta$ attempts in expectation, since $f$ is an $(\\alpha, \\beta)$-weak encoding.  We then sample $x_i \\in f^{-1}(Q_i)$.  Thus, each of the $q$ queries to the random oracle require expected time $\\beta T_3 + T_2$, where\n", "index": 23, "text": "\n\\[ T_2 = T_\\mathrm{lookup} + T_\\mathrm{samp}(f^{-1}). \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"T_{2}=T_{\\mathrm{lookup}}+T_{\\mathrm{samp}}(f^{-1}).\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>T</mi><mi>lookup</mi></msub><mo>+</mo><mrow><msub><mi>T</mi><mi>samp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nFinally, we can compute $n$ from $r$ and $d$ in time\n", "itemtype": "equation", "pos": 77367, "prevtext": "\n\nWe can compute $r$ and $d$ as a sum of $L$ terms in time $L \\cdot T_4$, where\n", "index": 25, "text": "\n\\[ T_4 = T_\\mathrm{lookup} + 2 T_\\mathrm{add}({\\mathbb{Z}}_\\rho) + T_\\mathrm{mult}({\\mathbb{Z}}_\\rho). \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"T_{4}=T_{\\mathrm{lookup}}+2T_{\\mathrm{add}}({\\mathbb{Z}}_{\\rho})+T_{\\mathrm{%&#10;mult}}({\\mathbb{Z}}_{\\rho}).\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mn>4</mn></msub><mo>=</mo><mrow><msub><mi>T</mi><mi>lookup</mi></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>T</mi><mi>add</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>mult</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nThus, the total expected time is $t + T_1 + q T_2 + q \\beta T_3 + L T_4$.\n\\end{proof}\n\n\n\n\n\n\n\\section{Security analysis of $\\mbox{{\\texttt{{AdHash}}}{}}$ in the multiset setting}\n\\label{sec:adhash-attack}\n\nThe best known attack on Bellare and Micciancio's incremental hash\nfunction {\\texttt{{AdHash}}}{} when it is used to hash \\emph{sets} is Wagner's\ngeneralized birthday attack~\\cite{wagner2002generalized}. However,\nwhen the function is used for \\emph{multiset} hashing, as proposed by\nClarke~{et al.}~\\cite[Theorem 6]{clarke2003incremental}, its security is\nmuch weaker. Indeed, finding a multiset collision on {\\texttt{{AdHash}}}{} with\n$q$ random oracle queries is equivalent to finding a vector\n$(a_1,\\dots,a_q)\\in{\\mathbb{Z}}^q$ of polynomial norm such that:\n", "itemtype": "equation", "pos": 77526, "prevtext": "\nFinally, we can compute $n$ from $r$ and $d$ in time\n", "index": 27, "text": "\n\\[ T_1 = T_\\mathrm{inv}({\\mathbb{Z}}_\\rho) + T_\\mathrm{mult}({\\mathbb{Z}}_\\rho) + T_\\mathrm{negate}(Z_\\rho). \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"T_{1}=T_{\\mathrm{inv}}({\\mathbb{Z}}_{\\rho})+T_{\\mathrm{mult}}({\\mathbb{Z}}_{%&#10;\\rho})+T_{\\mathrm{negate}}(Z_{\\rho}).\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>=</mo><mrow><mrow><msub><mi>T</mi><mi>inv</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>mult</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u2124</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>negate</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nwhere the $h_i$'s are the hash values returned by the oracle, and $M$ is\nthe {\\texttt{{AdHash}}}{} modulus. In other words, the problem is to find a short\nvector in the full rank lattice $L\\subset{\\mathbb{Z}}^q$ of vectors orthogonal to\n$(h_1,\\dots,h_q)$ modulo $M$.\n\nThe volume ${\\operatorname{vol}}(L) = [{\\mathbb{Z}}^q:L]$ of $L$ is clearly at most $M$, since $L$\nis the kernel of a homomorphism to ${\\mathbb{Z}}/M{\\mathbb{Z}}$. Therefore, a lattice\nreduction algorithm with Hermite factor constant $c$\n(see~\\cite{DBLP:conf/eurocrypt/GamaN08}) is expected to find a vector in\n$L$ of Euclidean norm at most $c^q\\cdot M^{1/q}$. By choosing $q =\n\\sqrt{\\frac{\\log M}{\\log c}}$, we obtain a multiset collision of size\nroughly $2\\sqrt{\\log_2 M\\cdot\\log_2 c}$ bits. For $k$ bits of security\nagainst this multiset collision attack, it is thus necessary to choose:\n", "itemtype": "equation", "pos": 78404, "prevtext": "\nThus, the total expected time is $t + T_1 + q T_2 + q \\beta T_3 + L T_4$.\n\\end{proof}\n\n\n\n\n\n\n\\section{Security analysis of $\\mbox{{\\texttt{{AdHash}}}{}}$ in the multiset setting}\n\\label{sec:adhash-attack}\n\nThe best known attack on Bellare and Micciancio's incremental hash\nfunction {\\texttt{{AdHash}}}{} when it is used to hash \\emph{sets} is Wagner's\ngeneralized birthday attack~\\cite{wagner2002generalized}. However,\nwhen the function is used for \\emph{multiset} hashing, as proposed by\nClarke~{et al.}~\\cite[Theorem 6]{clarke2003incremental}, its security is\nmuch weaker. Indeed, finding a multiset collision on {\\texttt{{AdHash}}}{} with\n$q$ random oracle queries is equivalent to finding a vector\n$(a_1,\\dots,a_q)\\in{\\mathbb{Z}}^q$ of polynomial norm such that:\n", "index": 29, "text": "\n\\[ \\sum_{i=1}^q a_i h_i \\equiv 0 \\pmod M, \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{q}a_{i}h_{i}\\equiv 0\\pmod{M},\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mrow><mo>\u2261</mo><mrow><mn>0</mn><mspace width=\"veryverythickmathspace\"/><mrow><mo lspace=\"8.1pt\" stretchy=\"false\">(</mo><mrow><mo movablelimits=\"false\">mod</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06502.tex", "nexttext": "\nThis is similar to Wagner's attack in the sense that the size of $M$\nshould be at least quadratic in the security parameter, but the constant\nis typically much larger. Over a large range of lattice dimensions,\na security level of $k=128$ bits corresponds to a Hermite factor constant\n$c\\approx1.007$~\\cite{chen2011bkz,DBLP:conf/ima/PolS13}.\nHence, a conservative choice of $M$ should be at least 400,000-bit long,\nwhich is obviously impractical. Even $k=80$ corresponds to\n$c\\approx1.008$ and requires $M$ to be chosen larger than 100,000 bits.\n\nAt any rate, recommended sizes for the set-hash setting are highly\ninsecure in the multiset hash setting. Consider a modulus $M$ of $1600$\nbits, appropriate for $80$-bit security in the set-hash setting. Simply\ndoing $q=230$ oracle queries and easily reducing the corresponding\nlattice with LLL (not even BKZ!), which has a Hermite factor constant\n$c\\approx1.021$, yields a multiset collision of weight about\n$c^{230}\\cdot 2^{1600/230}\\leq 15000$ (less that $14$-bit long).\nSimilarly, given a $4096$-bit modulus $M$ (as used for $128$-bit security\nin the set-hash setting), doing $q=500$ queries and reduction the\ncorresponding $500$-dimensional lattice with BKZ-28\\footnote{This is by\nno means a large computational effort even by academic standards. Recent\nacademic lattice reduction records target lattices of dimension $>800$\nusing BKZ with block size $90$ and\nup~\\cite{chen2011bkz,lindner2014latticechallenge}.}, which has a Hermite\nfactor constant $c\\approx1.011$~\\cite{DBLP:conf/eurocrypt/GamaN08},\nyields a multiset collision of weight about $c^{500}\\cdot 2^{4096/500}\\leq\n70000$ (less than $17$-bit long).\n\n\\section{Group structure implied by incremental additions}\n\\label{sec:monoid-to-group}\n\nConsider a more limited definition of an incremental multiset hash function, under which only incremental additions (and non-negative multiplicities) are supported:\n\\begin{definition}\n  Let $A$ be a set, and let $T$ be a finite set with an associative\noperation $+_T \\colon T \\times T \\rightarrow T$.  A function $H \\colon\n{\\mathbb{Z}}_{\\geq 0}^A \\rightarrow T$ is a \\emph{monoid-homomorphic multiset hash\nfunction} if $H(M_1 + M_2) = H(M_1) +_T H(M_2)$ for all $M_1, M_2 \\in\n{\\mathbb{Z}}^{(A)}$.\n\\end{definition}\n\nNote that $(H({\\mathbb{Z}}_{\\geq 0}^A), +_T)$ is necessarily a commutative monoid under this definition.  Thus, without loss of generality, we can assume that $(T, +_T)$ is a commutative monoid.\n\n\\begin{theorem}\n  If we make the additional assumption that $(T, +_T)$ has the\ncancellation property, i.e.\\ $a + b = a + c$ implies $b = c$ for all $a,\nb, c \\in T$, then we can construct a (group-)homomorphic multiset hash\nfunction $H'$ from ${\\mathbb{Z}}^{(A)}$ into a group $G$ that embeds $T$.  Furthermore, this construction has only a constant factor time and space overhead of $2$.\n\\end{theorem}\n\\begin{proof}\n  Since $T$ is a finite, commutative monoid with the cancellation property, there must exist an inverse for every element, and therefore $T$ is a group.\n  However, to ensure that the inverse can be computed efficiently, we use the Grothendieck construction in which we represent the positive and negative parts by separate elements of $T$.\n\n  Let $G$ be the quotient set $T \\times T /\\!\\!\\!\\equiv_G$, where the equivalence relation $\\equiv_G$ is given by $(a_+, a_-) \\equiv_G (b_+, b_-)$ if, and only if, $a_+ + b_- = a_- + b_+$, for all $a_+, a_-, b_+, b_- \\in T$.  We define the addition operation $[(a_+, a_-)] +_G [(b_+, b_-)] = [(a_+ +_T b_+, a_- +_T b_-)]$.  Note that $+_G$ respects $\\equiv_G$, and the inverse is given by $-[(a_+, a_-)] = [(a_-, a_+)]$.\n\n  We define the hash function $H' \\colon {\\mathbb{Z}}^{(A)} \\rightarrow G$ by $H'(M) = [(H(\\max(M,0)), H(\\max(-M,0)))]$.\n  Since $\\max(-M_1,0) + \\max(-M_2,0) + \\max(M_1 + M_2,0) = \\max(M_1,0) +\n\\max(M_2,0) + \\max(-(M_1 + M_2),0)$ for all $M_1, M_2 \\in {\\mathbb{Z}}^{(A)}$, we have\n$H'(M_1 + M_2) = [(H(\\max(M_1 + M_2,0)), H(\\max(-(M_1 + M_2))))] \n               = [(H(\\max(M_1,0)), H(\\max(-M_1,0)))] + [(H(\\max(M_2,0)), H(\\max(-M_2,0)))]\n               = H'(M_1) + H'(M_2)$.\n\n  Finally, we can embed $T$ in $G$ using that map $\\phi(a) = [(a,H({\\varnothing}))]$ for all $a \\in T$.  It follows directly from the definition of $\\equiv_G$ and $+_G$ that $\\phi$ is an injective homomorphism.  Note that the representation size for an element of $G$ is twice the representation size of an element of $T$, and $H'$ and $+_G$ require two invocations of $H$ and $+_T$, respectively.\n\\end{proof}\n\n\n\\section{Equivalence of incremental multiset hash function definitions}\n\\label{sec:clarke-equivalence}\n\n\\Cref{def:homomorphic-multiset-hash} is based on the definition of an\nincremental multiset hash function given by Clarke~{et al.}~\\cite{clarke2003incremental}, which we restate as follows:\n\n\\begin{definition}\n  \\label{def:clarke-multiset-hash}\n  Let $\\mathcal{H}^r : A^{{\\mathbb{Z}}_{\\geq 0}} \\rightarrow T$ and $+_\\mathcal{H}^r \\colon T \\times T \\rightarrow T$ be probabilistic algorithms using randomness $r \\in R$, where $T$ is a finite set, and let $\\equiv_\\mathcal{H}$ be an equivalence relation over $T$.  The triple $(\\mathcal{H}, +_\\mathcal{H}, \\equiv_\\mathcal{H})$ is a \\emph{multiset hash function} if it satisfies the following properties:\n  \\begin{enumerate}\n  \\item $\\mathcal{H}^{r_1}(M) \\equiv_\\mathcal{H} \\mathcal{H}^{r_2}(M)$,\nfor all $M \\in {\\mathbb{Z}}^{(A)}$, $r_1, r_2 \\in R$;\n  \\item $+_\\mathcal{H}$ respects the equivalence relation $\\equiv_{\\mathcal{H}}$;\n  \\item $s_1 +_\\mathcal{H}^{r_2} s_2 \\equiv_\\mathcal{H} s_3$ if\n$\\mathcal{H}^{r_3}(M_1) \\equiv_\\mathcal{H} s_1$, $\\mathcal{H}^{r_4}(M_2)\n\\equiv_\\mathcal{H} s_2$, and $\\mathcal{H}^{r_1}(M_1 + M_2) = s_3$ for all\n$M_1, M_2 \\in {\\mathbb{Z}}^{(A)}$, $s_1, s_2, s_3 \\in T$, $r_1, r_2, r_3, r_4 \\in R$.\n  \\end{enumerate}\n\\end{definition}\n\nThis differs from our definition of a monoid-homomorphic multiset hash function (\\cref{sec:monoid-to-group}) only in that it allows for randomness in the hash function and in the addition operation $+_T$.  Note that this randomness is for a fixed hash function, and is independent of the randomness in choosing the hash function from a hash function family.  The multiset hash function {\\texttt{{MSet-Add-Hash}}}{}~\\cite{clarke2003incremental} relies on this randomness for security.  In fact, though, the randomness is not integral to the hashing operation itself, but rather is used as a nonce in encrypting the hash code, which we view as an orthogonal operation.\\footnote{Note also that {\\texttt{{MSet-Add-Hash}}}{} is secure as a keyed hash function but not (under the same assumptions) as a public hash function.}  Therefore, we dispense with this randomness in our definition.\n\nAs explained in \\cref{sec:monoid-to-group}, if we assume that $(T, +_T)$ has the cancellation property, i.e.\\ that $+_T$ does not itself introduce any additional collisions, then a simple construction produces a (group-)homomorphic multiset hash function from any multiset hash function satisfying \\cref{def:clarke-multiset-hash}.\n\n\n\n", "itemtype": "equation", "pos": 79308, "prevtext": "\nwhere the $h_i$'s are the hash values returned by the oracle, and $M$ is\nthe {\\texttt{{AdHash}}}{} modulus. In other words, the problem is to find a short\nvector in the full rank lattice $L\\subset{\\mathbb{Z}}^q$ of vectors orthogonal to\n$(h_1,\\dots,h_q)$ modulo $M$.\n\nThe volume ${\\operatorname{vol}}(L) = [{\\mathbb{Z}}^q:L]$ of $L$ is clearly at most $M$, since $L$\nis the kernel of a homomorphism to ${\\mathbb{Z}}/M{\\mathbb{Z}}$. Therefore, a lattice\nreduction algorithm with Hermite factor constant $c$\n(see~\\cite{DBLP:conf/eurocrypt/GamaN08}) is expected to find a vector in\n$L$ of Euclidean norm at most $c^q\\cdot M^{1/q}$. By choosing $q =\n\\sqrt{\\frac{\\log M}{\\log c}}$, we obtain a multiset collision of size\nroughly $2\\sqrt{\\log_2 M\\cdot\\log_2 c}$ bits. For $k$ bits of security\nagainst this multiset collision attack, it is thus necessary to choose:\n", "index": 31, "text": "\n\\[ \\log_2 M \\geq \\frac{k^2}{4\\log_2 c}. \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\log_{2}M\\geq\\frac{k^{2}}{4\\log_{2}c}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mi>M</mi></mrow><mo>\u2265</mo><mfrac><msup><mi>k</mi><mn>2</mn></msup><mrow><mn>4</mn><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mi>c</mi></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}]