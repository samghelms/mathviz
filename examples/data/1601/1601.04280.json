[{"file": "1601.04280.tex", "nexttext": "\nwhere $\\Delta_r\\stackrel{\\Delta}{=}\\sqrt{\\sum_{i=r+1}^{\\min (m,n)}\n\\sigma_i^2}$, $r<k$.\nThen, the performance of the algorithm is compared with the current\nstate-of-the-art methods that compute low-rank matrix\napproximations. The presented algorithm is parallelizable and can be\nfully implemented on a GPU.\n\nThe paper is organized as follows: Section \\ref{sec:preliminaries}\nreviews some mathematical results that are needed for the\ndevelopment of the sparse randomized LU algorithm. Section\n\\ref{sec:sparserandlu} presents the sparse randomized LU algorithm\nand the error bound resulted from the approximation. Section\n\\ref{sec:numerical_results} presents numerical results for the\napproximation  error and for the running time of the sparse\nrandomized LU with comparison to other algorithms.\n\n\\section{Preliminaries}\n\\label{sec:preliminaries} This section presents the mathematical\nbackground needed in the rest of the paper.  More specifically, we\nreview the properties of the Sub-sampled Random Fourier Transform\n(SRFT) matrices and the sparse embedding matrices. Throughout the\npaper, $\\Vert \\cdot \\Vert_F$ denotes the Frobenius norm, $\\Vert\n\\cdot \\Vert_2$ denotes the spectral norm when the argument is a\nmatrix or the $l_2$ (Euclidean) norm for vector arguments. $M_{m\n\\times n}$ is the set of $m \\times n$ matrices, $\\sigma_i(\\cdot)$ is\nthe $i$th largest singular value of a matrix, and\n$\\Delta_k(\\cdot)=\\sqrt{\\sum_{i=k+1}^{\\min(m,n)}\\sigma_i^2}$ ,\n$k=0,\\ldots,\\min(m,n)-1$.\n\n\\subsection{The SRFT matrix} \\label{subsec:srft}\nThe SRFT matrix, which is presented in \\cite{AC,WLRT}, is a random matrix\ndenoted by $\\Pi$. It is decomposed into $\\Pi = DFS$ where $D$ is an\n$n \\times n$ diagonal matrix whose entries are i.i.d. random\nvariables drawn from a uniform distribution on the unit circle in\n$\\mathbb{C}$, $F$ is an $n\\times n$ discrete Fourier transform such\nthat $F_{jk}= \\frac{1}{\\sqrt{n}}e^{-2\\pi i\n    (j-1)(k-1)/n}$, $j,k = 1, \\ldots, n $ and $S$ is an $n\\times l$ matrix whose entries are all\nzeros except for a single randomly placed 1 in each column.\n\nLemma \\ref{lem:SRFTmult} shows that matrix multiplication by an SRFT\nmatrix can be done faster in comparison to an arbitrary matrix.\n\\begin{lemma}[\\cite{WLRT}] \\label{lem:SRFTmult}\n    For any $m \\times n$ matrix $A$, let $\\Pi$ be the $n \\times l$  SRFT matrix. Then, $Y=A\\Pi$ can be computed in $\\mathcal{O}(mn\\log l)$  floating point operations.\n\\end{lemma}\n\n\\begin{theorem}[Follows from \n    Theorem 1.3 in \\cite{tropp2011improved} ] \\label{thm:FJLT_SRFT}\n    For any $U \\in M_{n\\times r}$ with orthogonal columns, if $\\Pi \\in M_{k \\times n}$,\n    is a randomly chosen SRFT matrix, where $r,k$ and $n$ satisfy\n    $4\\left[ \\sqrt{r} + \\sqrt{8\\log(rn)}\\right]^2 \\log r \\leq k \\leq n$.  Then, with probability of at least $1-\\mathcal{O}(r^{-1})$, the largest and the smallest singular values of $\\Pi U $ are in $[0.40,1.48]$.\n\\end{theorem}\n\n\\subsection{Sparse Embedding Matrices} \\label{subsec:SEM}\n\n\nFor a parameter $t \\in \\mathbb{N}$, consider the random linear map $S = \\Phi D$, where $S \\in M_{k \\times n}$, such that for $h : \\{1,\\ldots, n\\} \\to \\{1,\\ldots, k\\}$,a random map such that for each $i \\in \\{1,\\ldots, n\\}$, $h(i) = t'$ for $t' \\in \\{1,\\ldots, k\\}$ with probability $1/t$, we have\n\\begin{enumerate}\n    \\item $\\Phi \\in \\{0,1\\}^{k \\times n}$ is a $k \\times n$ ($k \\le n$) binary matrix with nonzero entries $\\Phi_{h(i),i} = 1$ and all the remaining entries equal to $0$. In other words, $\\Phi$ is a matrix with a single 1 in each row.\n    \\item $D$ is an $n\\times n$ random diagonal matrix where each diagonal entry  is independently chosen to be $+1$ or $-1$ with equal probability.\n\\end{enumerate}\n A matrix $S$ that satisfies 1 \\& 2 is referred to as a sparse embedding matrix (SEM).\n\\begin{lemma}\n    Let $S \\in M_{k\\times n}$ be an SEM matrix. Then, $\\|S\\|_F = \\sqrt{n}$.\n\\end{lemma}\n\\begin{theorem}\\label{thm:sing_vals_S}\n    The largest singular value of a $k\\times n$ SEM is bounded, with high probability, by $C(n,k) = \\sqrt{\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}}$ for large enough $n$.\n\\end{theorem}\nThe proof of Theorem \\ref{thm:sing_vals_S} uses Lemma \\ref{lem:norm_of_Pi}:\n\\begin{lemma}\\label{lem:norm_of_Pi}\n     The operator norm of an SEM $S \\in M_{k\\times n}$ is the square root of the maximal number of non-zeros in a row in $S$.\n\\end{lemma}\n\\begin{proof}\n    Assume, without loss of generality, that there are $\\kappa_i$, $i=1,\\ldots ,k$ non-zeros in each row,  $\\kappa_1 \\geq \\ldots \\geq \\kappa_k$.\n    Denote the set of non-zero indeces in the $i$th row by $K_i$ ($|K_i| = \\kappa_i$, $i = 1, \\ldots, k$).\n    Since there is only one non-zero in each column, $\\sum_{i=1}^k \\kappa_i = n$.\n    There is a vector $v$ of unit length such that $\\|Sv\\| =\\sqrt{\\kappa_1} $.\n    Let  $v = (v_1, \\ldots, v_n)^T$ be such that $\\sum_{i=1}^k v_i^2 = 1$. Then\n    \n", "itemtype": "equation", "pos": 4520, "prevtext": "\n\n\\title{Randomized LU Decomposition Using Sparse Projections}\n\n\n\\author\n{Yariv Aizenbud${^1}$~~Gil Shabat${^2}$~~Amir Averbuch${^2}$\\\\\n${^1}$School of Applied Mathematics, Tel Aviv University, Israel\\\\\n${^2}$School of Computer Science, Tel Aviv University, Israel\n}\n\n\\maketitle\n\n\\begin{abstract}\n    A fast algorithm for the approximation of a low rank LU decomposition is presented. In order to achieve a low complexity, the algorithm uses sparse random projections combined with FFT-based random projections. The asymptotic approximation error of the algorithm is analyzed and a theoretical error bound is presented. Finally, numerical examples illustrate that for a similar approximation error, the sparse LU algorithm is faster than recent state-of-the-art methods. The algorithm is completely parallelizable that enables to run on a GPU. The performance is tested on a GPU card, showing a significant improvement in the running time in comparison to sequential execution.\n\\end{abstract}\n\n\\smallskip\n\\noindent \\textbf{Keywords.} LU decomposition, random matrices, sparse matrices, sparse Johnson-Lindenstrauss transform.\n\n\\section{Introduction}\nLow-rank matrix  approximations are  a key component for efficient\nprocessing, manipulating and analysis of big datasets. Often, data\nmatrices can be very large and yet have many redundancies and\ndependencies between rows and columns that result in being a low-rank matrix. Finding a low-rank approximation of a matrix enables us to\nprocess the entire matrix by using only a small set of vectors.\nApplications that utilize low-rank matrix approximations include\ndata compression, noise filtering, principle component analysis\nand kernel methods, to name some. Although a low-rank matrix\napproximation can be computed using well-known matrix decomposition\nmethods, such as singular value decomposition (SVD) or rank\nrevealing QR (RRQR), very often this is impractical due to high\ncomputational load. Therefore, there is an ongoing interest in the development of fast algorithms\nfor computing low-rank matrix approximations. Randomized algorithms\nfor low rank matrix approximations include SVD\n\\cite{halko2011finding,WLRT,randecomp}, LU\n\\cite{shabat2013randomized}, CUR\n\\cite{drineas2006fast,drineas2008relative}, principal component\nanalysis (PCA) \\cite{szlam2014implementation,halko2011algorithm}, to\nname some. Randomized algorithms have gained an increasing\npopularity because of their abilities to perform matrix computations\nfaster and on larger data sets than classical algorithms such as\n\\cite{golub2012matrix}. \n\nSparse random projections have been studied\nfor dimensionality reduction as a sparse variant of the\nJohnson-Lindenstrauss (JL) transform. A nearly tight lower bound for\nseveral dimensionality reduction linear maps for a predetermined \nsparsity is given in \\cite{nelson2013sparsity,nelson2014lower}.\n\nAlgorithms, which utilize sparse random projections for SVD and\nregression computations, are given in\n\\cite{clarkson2013low,kane2014sparser,nelson2013osnap,achlioptas2007fast}. Algorithms that are based on sparse dimensionality reduction transforms benefit from the fact that their projection step is more computationally efficient than those that use dense matrices in their projection step.\nWhile the complexity of the algorithms, which use a structured JL transform such as FFT-based random projections \\cite{WLRT}, does not change when applied to sparse matrices, algorithms that are based on sparse random projections are accelerated when applied to sparse matrices.\n\nIn this paper, the randomized LU algorithms\n\\cite{shabat2013randomized} are extended by utilizing sparse random projections. We introduce an LU decomposition algorithm that uses sparse random projections combined with the fast Johnson-Lindenstrauss (FJL) transform. FJL transforms are based on the fast Fourier transform (FFT) \\cite{AC} and are also used in\n\\cite{WLRT}. This combination of sparse JL with FJL was introduced in \\cite{clarkson2013low} to produce faster\nalgorithms. The algorithm presented in this paper is shown to be significantly\nfaster for a low-rank matrix decomposition than the algorithms mentioned above.\nIn addition, a detailed theoretical analysis is presented for the\nderived error bounds of the algorithm.\n\nFor a given matrix $A$ of size $m \\times n$, the algorithm computes\nthe lower and upper triangular matrices $L$ and $U$ of sizes $m\n\\times k$ and $k \\times n$, respectively, and permutation matrices\n$P$ and $Q$ such that with high probability\n\n", "index": 1, "text": "\\begin{equation}\n\\|LU - PAQ\\|_F \\leq \\mathcal{O}(\\Delta_r)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\|LU-PAQ\\|_{F}\\leq\\mathcal{O}(\\Delta_{r})\" display=\"block\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>L</mi><mo>\u2062</mo><mi>U</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>Q</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    Since $\\max\\limits_{\\sum\\limits_{K_j} v_i^2=\\alpha_j} \\sum v_i$ is achieved when $v_i = \\sqrt{\\frac{\\alpha_j}{\\kappa_j}}$ for all $ i \\in K_j$, then we have\n    \n", "itemtype": "equation", "pos": 9455, "prevtext": "\nwhere $\\Delta_r\\stackrel{\\Delta}{=}\\sqrt{\\sum_{i=r+1}^{\\min (m,n)}\n\\sigma_i^2}$, $r<k$.\nThen, the performance of the algorithm is compared with the current\nstate-of-the-art methods that compute low-rank matrix\napproximations. The presented algorithm is parallelizable and can be\nfully implemented on a GPU.\n\nThe paper is organized as follows: Section \\ref{sec:preliminaries}\nreviews some mathematical results that are needed for the\ndevelopment of the sparse randomized LU algorithm. Section\n\\ref{sec:sparserandlu} presents the sparse randomized LU algorithm\nand the error bound resulted from the approximation. Section\n\\ref{sec:numerical_results} presents numerical results for the\napproximation  error and for the running time of the sparse\nrandomized LU with comparison to other algorithms.\n\n\\section{Preliminaries}\n\\label{sec:preliminaries} This section presents the mathematical\nbackground needed in the rest of the paper.  More specifically, we\nreview the properties of the Sub-sampled Random Fourier Transform\n(SRFT) matrices and the sparse embedding matrices. Throughout the\npaper, $\\Vert \\cdot \\Vert_F$ denotes the Frobenius norm, $\\Vert\n\\cdot \\Vert_2$ denotes the spectral norm when the argument is a\nmatrix or the $l_2$ (Euclidean) norm for vector arguments. $M_{m\n\\times n}$ is the set of $m \\times n$ matrices, $\\sigma_i(\\cdot)$ is\nthe $i$th largest singular value of a matrix, and\n$\\Delta_k(\\cdot)=\\sqrt{\\sum_{i=k+1}^{\\min(m,n)}\\sigma_i^2}$ ,\n$k=0,\\ldots,\\min(m,n)-1$.\n\n\\subsection{The SRFT matrix} \\label{subsec:srft}\nThe SRFT matrix, which is presented in \\cite{AC,WLRT}, is a random matrix\ndenoted by $\\Pi$. It is decomposed into $\\Pi = DFS$ where $D$ is an\n$n \\times n$ diagonal matrix whose entries are i.i.d. random\nvariables drawn from a uniform distribution on the unit circle in\n$\\mathbb{C}$, $F$ is an $n\\times n$ discrete Fourier transform such\nthat $F_{jk}= \\frac{1}{\\sqrt{n}}e^{-2\\pi i\n    (j-1)(k-1)/n}$, $j,k = 1, \\ldots, n $ and $S$ is an $n\\times l$ matrix whose entries are all\nzeros except for a single randomly placed 1 in each column.\n\nLemma \\ref{lem:SRFTmult} shows that matrix multiplication by an SRFT\nmatrix can be done faster in comparison to an arbitrary matrix.\n\\begin{lemma}[\\cite{WLRT}] \\label{lem:SRFTmult}\n    For any $m \\times n$ matrix $A$, let $\\Pi$ be the $n \\times l$  SRFT matrix. Then, $Y=A\\Pi$ can be computed in $\\mathcal{O}(mn\\log l)$  floating point operations.\n\\end{lemma}\n\n\\begin{theorem}[Follows from \n    Theorem 1.3 in \\cite{tropp2011improved} ] \\label{thm:FJLT_SRFT}\n    For any $U \\in M_{n\\times r}$ with orthogonal columns, if $\\Pi \\in M_{k \\times n}$,\n    is a randomly chosen SRFT matrix, where $r,k$ and $n$ satisfy\n    $4\\left[ \\sqrt{r} + \\sqrt{8\\log(rn)}\\right]^2 \\log r \\leq k \\leq n$.  Then, with probability of at least $1-\\mathcal{O}(r^{-1})$, the largest and the smallest singular values of $\\Pi U $ are in $[0.40,1.48]$.\n\\end{theorem}\n\n\\subsection{Sparse Embedding Matrices} \\label{subsec:SEM}\n\n\nFor a parameter $t \\in \\mathbb{N}$, consider the random linear map $S = \\Phi D$, where $S \\in M_{k \\times n}$, such that for $h : \\{1,\\ldots, n\\} \\to \\{1,\\ldots, k\\}$,a random map such that for each $i \\in \\{1,\\ldots, n\\}$, $h(i) = t'$ for $t' \\in \\{1,\\ldots, k\\}$ with probability $1/t$, we have\n\\begin{enumerate}\n    \\item $\\Phi \\in \\{0,1\\}^{k \\times n}$ is a $k \\times n$ ($k \\le n$) binary matrix with nonzero entries $\\Phi_{h(i),i} = 1$ and all the remaining entries equal to $0$. In other words, $\\Phi$ is a matrix with a single 1 in each row.\n    \\item $D$ is an $n\\times n$ random diagonal matrix where each diagonal entry  is independently chosen to be $+1$ or $-1$ with equal probability.\n\\end{enumerate}\n A matrix $S$ that satisfies 1 \\& 2 is referred to as a sparse embedding matrix (SEM).\n\\begin{lemma}\n    Let $S \\in M_{k\\times n}$ be an SEM matrix. Then, $\\|S\\|_F = \\sqrt{n}$.\n\\end{lemma}\n\\begin{theorem}\\label{thm:sing_vals_S}\n    The largest singular value of a $k\\times n$ SEM is bounded, with high probability, by $C(n,k) = \\sqrt{\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}}$ for large enough $n$.\n\\end{theorem}\nThe proof of Theorem \\ref{thm:sing_vals_S} uses Lemma \\ref{lem:norm_of_Pi}:\n\\begin{lemma}\\label{lem:norm_of_Pi}\n     The operator norm of an SEM $S \\in M_{k\\times n}$ is the square root of the maximal number of non-zeros in a row in $S$.\n\\end{lemma}\n\\begin{proof}\n    Assume, without loss of generality, that there are $\\kappa_i$, $i=1,\\ldots ,k$ non-zeros in each row,  $\\kappa_1 \\geq \\ldots \\geq \\kappa_k$.\n    Denote the set of non-zero indeces in the $i$th row by $K_i$ ($|K_i| = \\kappa_i$, $i = 1, \\ldots, k$).\n    Since there is only one non-zero in each column, $\\sum_{i=1}^k \\kappa_i = n$.\n    There is a vector $v$ of unit length such that $\\|Sv\\| =\\sqrt{\\kappa_1} $.\n    Let  $v = (v_1, \\ldots, v_n)^T$ be such that $\\sum_{i=1}^k v_i^2 = 1$. Then\n    \n", "index": 3, "text": "$$\n    \\|Sv\\|_2^2 = \\sum\\limits_{j=1}^k\\left( \\sum\\limits_{i \\in K_j}v_{i}\\right) ^2.\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\|Sv\\|_{2}^{2}=\\sum\\limits_{j=1}^{k}\\left(\\sum\\limits_{i\\in K_{j}}v_{i}\\right)%&#10;^{2}.\" display=\"block\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msup><mrow><mo>(</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>K</mi><mi>j</mi></msub></mrow></munder><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    Since $\\sum \\alpha_j = 1 $ it follows that\n    $\n        \\|Sv\\|^2 \\leq \\kappa_1.\n    $\n    Thus,\n    $\n    \\|Sv\\| \\leq \\sqrt{\\kappa_1}.\n    $\n\\end{proof}\n\\begin{rem}\n\tIn a similar way, one can show that all the singular values of $S$ are of the form $\\sqrt{\\kappa_i}$.\n\\end{rem}\n\n\\begin{proof}[Proof of Theorem \\ref{thm:sing_vals_S}]\n    By Lemma \\ref{lem:norm_of_Pi}, the norm of $S$ is the square root of the maximal number of non-zeros (${\\operatorname{nnz}}$) in a row.\n    The maximal ${\\operatorname{nnz}}$  in each row is distributed as the maximum of $n$ balls thrown into $k$ urns.\n    By Theorem 1 in \\cite{raab1998balls}, the probability of the norm to be more than $\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}$ is $o(1)$.\n    Thus, the norm is bounded, with high probability, by $\\sqrt{\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}}$ for sufficiently large $n$\n\\end{proof}\n\n\\begin{theorem}[Appears \n    as Theorem 3 in \\cite{nelson2013osnap} ] \\label{thm:FJLT_sp}\n    For any $U \\in M_{m\\times r}$ with orthogonal columns, if $S \\in M_{l \\times m}$ where $l \\geq \\delta ^{-1}(r^2+r)/(2\\varepsilon - \\varepsilon^2)^2$ is a randomly chosen SEM,  then with probability of at least $1-\\delta$, the largest and smallest singular values of $SU $ are in the interval $[1-\\varepsilon, 1+\\varepsilon]$.\n\n\\end{theorem}\n\n\n\n\\begin{corollary} \\label{cor:sing_SU}\n    Let $\\Omega = \\Pi S$, where $\\Pi \\in M_{k \\times l}$ is as in Theorem \\ref{thm:FJLT_SRFT} and $S \\in M_{l \\times m}$ as in Theorem \\ref{thm:FJLT_sp}. Then,\n    for any $U \\in M_{m\\times r}$, which has orthogonal columns with high probability,\n    $\\|\\Omega U\\|_2 \\leq 1.48(1+\\varepsilon)$ and $\\|(\\Omega U)^{-1}\\|_2 \\leq 0.4\\frac{1}{(1-\\varepsilon)}$.\n\\end{corollary}\n\n\n\\begin{theorem}[Appears as Lemma 46 in \\cite{clarkson2013low}] \\label{thm:affine_sparse_embed}\n    Let $A \\in M_{m \\times d}$ be of rank $r$, $B \\in M_{m \\times d'}$,\n    and $c = d + d'$. For SEM $S \\in M_{l \\times m}$ and SRFT matrix $\\Pi \\in M_{k \\times l}$, there exist $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$\n    and $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$ such that for $\\Omega =\\Pi S$, $\\tilde{X} = {\\operatornamewithlimits{\\text{argmin}}}_X \\|\\Omega(AX - B)\\|_F$ satisfies $\\|A \\tilde{X} - B\\|_F \\leq (1 +\n    \\varepsilon) \\min_X \\|AX - B\\|_F$ with a fixed non-zero probability. The operator $\\Omega$ can be applied in $O({\\operatorname{nnz}}(A) + {\\operatorname{nnz}}(B) + lc \\log l)$ operations.\n\\end{theorem}\n\nAn improved bound appears in \\cite{nelson2013osnap} and is shown to be near optimal in \\cite{nelson2013sparsity}.\n\n\\section{Sparse Randomized LU}\n\\label{sec:sparserandlu} Similarly to the work presented in\n\\cite{randecomp, halko2011finding}, the key idea in the current\nalgorithm is that the image of $AS$ for a randomly chosen SEM $S$ is\n``close'' to the image of $A$ up to an error of order $\\Delta_r$. It\nis shown in \\cite{halko2011finding} that for each $r$ there is $k>r$\nsuch that if $S$ is a random matrix of size $n \\times k$ generated\nfrom the set of Gaussian i.i.d. matrices, or from SRFT matrices,\nthen with high probability the image of $AS$ is close to the image\nof $A$. More rigorously, if we denote by $Q$ an $n \\times k$ matrix\nwith orthonormal columns that has the same image as $AS$, which is\ncalculated by the QR algorithm, then $\\|A-QQ^*A\\|_F \\le\n\\mathcal{O}(\\Delta_r)$. We show in Theorem \\ref{thm:A_QQA} that this\nis also true for the set of random SEM:\n\\begin{theorem} \\label{thm:A_QQA}\n    Let $A$ be an $m \\times n$ matrix. Assume that $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$, $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$,\n    $\\Pi \\in M_{k \\times l}$ is an SRFT matrix and $S \\in M_{l \\times n}$ is an SEM. Let $\\Omega = \\Pi S$ and the QR decomposition of $A\\Omega^*$ is denoted by $QR$. Then, $\\|A-QQ^*A\\|_F<(1+\\varepsilon)\\Delta_r(A)$.\n\\end{theorem}\nThe proof Theorem \\ref{thm:A_QQA} uses ideas similar to some in \\cite{clarkson2013low}.\n\\begin{proof}\n    First, we show that $\\min\\limits_{{\\operatorname{rank}} X=r} \\|QX - A\\|_F \\leq (1 + \\varepsilon) \\Delta_r$. Assume $A_r$ is the best rank $r$ approximation of $A$. Then, directly from this assumption, it follows that $\n    \\min\\limits_Y\\|YA_r - A\\|_F = \\|A_r - A\\|_F = \\Delta_r\n    $.\n    From Theorem \\ref{thm:affine_sparse_embed} follows that if\n    $\\tilde{Y} = {\\operatornamewithlimits{\\text{argmin}}} \\|(YA_r-A)\\Omega^*\\|_F$, then\n    \n", "itemtype": "equation", "pos": 9713, "prevtext": "\n    Since $\\max\\limits_{\\sum\\limits_{K_j} v_i^2=\\alpha_j} \\sum v_i$ is achieved when $v_i = \\sqrt{\\frac{\\alpha_j}{\\kappa_j}}$ for all $ i \\in K_j$, then we have\n    \n", "index": 5, "text": "$$\n        \\|Sv\\|^2 \\leq \\sum\\limits_{j=1}^k\\left( \\kappa_j  \\sqrt{\\frac{\\alpha_j}{\\kappa_j}} \\right) ^2 = \\sum\\limits_{j=1}^k\\left(  \\sqrt{\\kappa_j \\alpha_j} \\right) ^2 = \\sum\\limits_{j=1}^k \\kappa_j \\alpha_j.\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\|Sv\\|^{2}\\leq\\sum\\limits_{j=1}^{k}\\left(\\kappa_{j}\\sqrt{\\frac{\\alpha_{j}}{%&#10;\\kappa_{j}}}\\right)^{2}=\\sum\\limits_{j=1}^{k}\\left(\\sqrt{\\kappa_{j}\\alpha_{j}}%&#10;\\right)^{2}=\\sum\\limits_{j=1}^{k}\\kappa_{j}\\alpha_{j}.\" display=\"block\"><mrow><mrow><msup><mrow><mo>\u2225</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>\u03ba</mi><mi>j</mi></msub><mo>\u2062</mo><msqrt><mfrac><msub><mi>\u03b1</mi><mi>j</mi></msub><msub><mi>\u03ba</mi><mi>j</mi></msub></mfrac></msqrt></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msup><mrow><mo>(</mo><msqrt><mrow><msub><mi>\u03ba</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03b1</mi><mi>j</mi></msub></mrow></msqrt><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03ba</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03b1</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    Note that\n    \n", "itemtype": "equation", "pos": 14347, "prevtext": "\n    Since $\\sum \\alpha_j = 1 $ it follows that\n    $\n        \\|Sv\\|^2 \\leq \\kappa_1.\n    $\n    Thus,\n    $\n    \\|Sv\\| \\leq \\sqrt{\\kappa_1}.\n    $\n\\end{proof}\n\\begin{rem}\n\tIn a similar way, one can show that all the singular values of $S$ are of the form $\\sqrt{\\kappa_i}$.\n\\end{rem}\n\n\\begin{proof}[Proof of Theorem \\ref{thm:sing_vals_S}]\n    By Lemma \\ref{lem:norm_of_Pi}, the norm of $S$ is the square root of the maximal number of non-zeros (${\\operatorname{nnz}}$) in a row.\n    The maximal ${\\operatorname{nnz}}$  in each row is distributed as the maximum of $n$ balls thrown into $k$ urns.\n    By Theorem 1 in \\cite{raab1998balls}, the probability of the norm to be more than $\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}$ is $o(1)$.\n    Thus, the norm is bounded, with high probability, by $\\sqrt{\\frac{n}{k} + \\sqrt{2\\frac{n}{k} \\log k}}$ for sufficiently large $n$\n\\end{proof}\n\n\\begin{theorem}[Appears \n    as Theorem 3 in \\cite{nelson2013osnap} ] \\label{thm:FJLT_sp}\n    For any $U \\in M_{m\\times r}$ with orthogonal columns, if $S \\in M_{l \\times m}$ where $l \\geq \\delta ^{-1}(r^2+r)/(2\\varepsilon - \\varepsilon^2)^2$ is a randomly chosen SEM,  then with probability of at least $1-\\delta$, the largest and smallest singular values of $SU $ are in the interval $[1-\\varepsilon, 1+\\varepsilon]$.\n\n\\end{theorem}\n\n\n\n\\begin{corollary} \\label{cor:sing_SU}\n    Let $\\Omega = \\Pi S$, where $\\Pi \\in M_{k \\times l}$ is as in Theorem \\ref{thm:FJLT_SRFT} and $S \\in M_{l \\times m}$ as in Theorem \\ref{thm:FJLT_sp}. Then,\n    for any $U \\in M_{m\\times r}$, which has orthogonal columns with high probability,\n    $\\|\\Omega U\\|_2 \\leq 1.48(1+\\varepsilon)$ and $\\|(\\Omega U)^{-1}\\|_2 \\leq 0.4\\frac{1}{(1-\\varepsilon)}$.\n\\end{corollary}\n\n\n\\begin{theorem}[Appears as Lemma 46 in \\cite{clarkson2013low}] \\label{thm:affine_sparse_embed}\n    Let $A \\in M_{m \\times d}$ be of rank $r$, $B \\in M_{m \\times d'}$,\n    and $c = d + d'$. For SEM $S \\in M_{l \\times m}$ and SRFT matrix $\\Pi \\in M_{k \\times l}$, there exist $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$\n    and $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$ such that for $\\Omega =\\Pi S$, $\\tilde{X} = {\\operatornamewithlimits{\\text{argmin}}}_X \\|\\Omega(AX - B)\\|_F$ satisfies $\\|A \\tilde{X} - B\\|_F \\leq (1 +\n    \\varepsilon) \\min_X \\|AX - B\\|_F$ with a fixed non-zero probability. The operator $\\Omega$ can be applied in $O({\\operatorname{nnz}}(A) + {\\operatorname{nnz}}(B) + lc \\log l)$ operations.\n\\end{theorem}\n\nAn improved bound appears in \\cite{nelson2013osnap} and is shown to be near optimal in \\cite{nelson2013sparsity}.\n\n\\section{Sparse Randomized LU}\n\\label{sec:sparserandlu} Similarly to the work presented in\n\\cite{randecomp, halko2011finding}, the key idea in the current\nalgorithm is that the image of $AS$ for a randomly chosen SEM $S$ is\n``close'' to the image of $A$ up to an error of order $\\Delta_r$. It\nis shown in \\cite{halko2011finding} that for each $r$ there is $k>r$\nsuch that if $S$ is a random matrix of size $n \\times k$ generated\nfrom the set of Gaussian i.i.d. matrices, or from SRFT matrices,\nthen with high probability the image of $AS$ is close to the image\nof $A$. More rigorously, if we denote by $Q$ an $n \\times k$ matrix\nwith orthonormal columns that has the same image as $AS$, which is\ncalculated by the QR algorithm, then $\\|A-QQ^*A\\|_F \\le\n\\mathcal{O}(\\Delta_r)$. We show in Theorem \\ref{thm:A_QQA} that this\nis also true for the set of random SEM:\n\\begin{theorem} \\label{thm:A_QQA}\n    Let $A$ be an $m \\times n$ matrix. Assume that $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$, $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$,\n    $\\Pi \\in M_{k \\times l}$ is an SRFT matrix and $S \\in M_{l \\times n}$ is an SEM. Let $\\Omega = \\Pi S$ and the QR decomposition of $A\\Omega^*$ is denoted by $QR$. Then, $\\|A-QQ^*A\\|_F<(1+\\varepsilon)\\Delta_r(A)$.\n\\end{theorem}\nThe proof Theorem \\ref{thm:A_QQA} uses ideas similar to some in \\cite{clarkson2013low}.\n\\begin{proof}\n    First, we show that $\\min\\limits_{{\\operatorname{rank}} X=r} \\|QX - A\\|_F \\leq (1 + \\varepsilon) \\Delta_r$. Assume $A_r$ is the best rank $r$ approximation of $A$. Then, directly from this assumption, it follows that $\n    \\min\\limits_Y\\|YA_r - A\\|_F = \\|A_r - A\\|_F = \\Delta_r\n    $.\n    From Theorem \\ref{thm:affine_sparse_embed} follows that if\n    $\\tilde{Y} = {\\operatornamewithlimits{\\text{argmin}}} \\|(YA_r-A)\\Omega^*\\|_F$, then\n    \n", "index": 7, "text": "$$\n    \\|\\tilde{Y}A_r-A\\|_F \\leq  (1 + \\varepsilon)  \\min\\limits_Y\\|YA_r - A\\|_F =  (1 + \\varepsilon) \\Delta_r.\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\|\\tilde{Y}A_{r}-A\\|_{F}\\leq(1+\\varepsilon)\\min\\limits_{Y}\\|YA_{r}-A\\|_{F}=(1+%&#10;\\varepsilon)\\Delta_{r}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mover accent=\"true\"><mi>Y</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msub><mi>A</mi><mi>r</mi></msub></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><munder><mi>min</mi><mi>Y</mi></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>Y</mi><mo>\u2062</mo><msub><mi>A</mi><mi>r</mi></msub></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    Thus,\n    \n", "itemtype": "equation", "pos": 14484, "prevtext": "\n    Note that\n    \n", "index": 9, "text": "$$\n    {\\operatornamewithlimits{\\text{argmin}}} \\|(YA_r-A)\\Omega^*\\|_F = {\\operatornamewithlimits{\\text{argmin}}} \\|YA_r\\Omega^*-A\\Omega^*\\|_F = A\\Omega^*(A_r\\Omega^*)^\\dagger.\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{\\operatornamewithlimits{\\text{argmin}}}\\|(YA_{r}-A)\\Omega^{*}\\|_{F}={%&#10;\\operatornamewithlimits{\\text{argmin}}}\\|YA_{r}\\Omega^{*}-A\\Omega^{*}\\|_{F}=A%&#10;\\Omega^{*}(A_{r}\\Omega^{*})^{\\dagger}.\" display=\"block\"><mrow><mrow><mrow><mtext>argmin</mtext><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>Y</mi><mo>\u2062</mo><msub><mi>A</mi><mi>r</mi></msub></mrow><mo>-</mo><mi>A</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>=</mo><mrow><mtext>argmin</mtext><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>Y</mi><mo>\u2062</mo><msub><mi>A</mi><mi>r</mi></msub><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup></mrow><mo>-</mo><mrow><mi>A</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>=</mo><mrow><mi>A</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>A</mi><mi>r</mi></msub><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    From Eq. \\eqref{eq:ARsomthing_A} it follows that\n    \n", "itemtype": "equation", "pos": 14682, "prevtext": "\n    Thus,\n    \n", "index": 11, "text": "\\begin{equation}\\label{eq:ARsomthing_A}\n    \\|A\\Omega^*(A_r\\Omega^*)^\\dagger A_r-A\\|_F \\leq (1 + \\varepsilon) \\Delta_r.\n    \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\|A\\Omega^{*}(A_{r}\\Omega^{*})^{\\dagger}A_{r}-A\\|_{F}\\leq(1+\\varepsilon)\\Delta%&#10;_{r}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>A</mi><mi>r</mi></msub><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup><mo>\u2062</mo><msub><mi>A</mi><mi>r</mi></msub></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    By using the fact that\n    \n", "itemtype": "equation", "pos": 14878, "prevtext": "\n    From Eq. \\eqref{eq:ARsomthing_A} it follows that\n    \n", "index": 13, "text": "$$\n    \\min\\limits_{{\\operatorname{rank}} X=r} \\|A\\Omega^*X - A\\|_F \\leq (1 + \\varepsilon)\\Delta_r \\mbox{, where } X \\in M_{k \\times n}.\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{{\\operatorname{rank}}X=r}\\|A\\Omega^{*}X-A\\|_{F}\\leq(1+\\varepsilon%&#10;)\\Delta_{r}\\mbox{, where }X\\in M_{k\\times n}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mrow><mo>rank</mo><mo>\u2061</mo><mi>X</mi></mrow><mo>=</mo><mi>r</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub><mo>\u2062</mo><mtext>, where\u00a0</mtext><mo>\u2062</mo><mi>X</mi></mrow><mo>\u2208</mo><msub><mi>M</mi><mrow><mi>k</mi><mo>\u00d7</mo><mi>n</mi></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n    we get\n    \n", "itemtype": "equation", "pos": 15053, "prevtext": "\n    By using the fact that\n    \n", "index": 15, "text": "$$\n    \\min\\limits_{X s.t. {\\operatorname{rank}} X=r} \\|QX - A\\|_F \\leq \\min\\limits_{X s.t. {\\operatorname{rank}} X=r}\\|A\\Omega^*X - A\\|_F\n    $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{Xs.t.{\\operatorname{rank}}X=r}\\|QX-A\\|_{F}\\leq\\min\\limits_{Xs.t.{%&#10;\\operatorname{rank}}X=r}\\|A\\Omega^{*}X-A\\|_{F}\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mrow><mi>X</mi><mo>\u2062</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo>.</mo><mrow><mrow><mo>rank</mo><mo>\u2061</mo><mi>X</mi></mrow><mo>=</mo><mi>r</mi></mrow></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>Q</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>\u2264</mo><mrow><munder><mi>min</mi><mrow><mrow><mi>X</mi><mo>\u2062</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo>.</mo><mrow><mrow><mo>rank</mo><mo>\u2061</mo><mi>X</mi></mrow><mo>=</mo><mi>r</mi></mrow></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mo>*</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n\n    It follows that $\\|QQ^*A - A\\|_F \\leq \\min\\limits_{X s.t. {\\operatorname{rank}} X=r} \\|QX - A\\|_F $, which concludes the proof.\n\\end{proof}\n\nTheorem \\ref{thm:A_QQA} shows that $QQ^*A$ approximates  $A$ well.\nSince $Q$ and $Q^*A$ are relatively small matrices and since $Q$\nhas orthogonal columns, then the SVD computation of $Q^*A$ is faster than\nthe SVD computation of $A$. Unfortunately, $Q^*$ is a dense\nmatrix, then the multiplication $Q^*A$ is computationally expensive. We\nnow show how to replace the computation of $Q^*A$ with a\nmultiplication of $A$ by a sparse matrix without affecting the\naccuracy too much.\n\\begin{corollary}\\label{col:A_LLA}\n    Let $A$ be a $m \\times n$ matrix. Assume  $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$, $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$,\n    $\\Pi \\in M_{k \\times l}$ is an SRFT matrix and an SEM $S \\in M_{l \\times n}$. Denote $\\Omega = \\Pi S$ and the pivoted LU decomposition of $A\\Omega^*$ is denoted by $PA\\Omega^* = LU$. Then $\\|PA-LL^\\dagger PA\\|_F<(1+\\varepsilon)\\Delta_r(A)$.\n\\end{corollary}\n\\begin{proof}\n    The proof is the same as that of Theorem \\ref{thm:A_QQA}. The reason that the same proof works is that ${\\rm Im} \\, L = {\\rm Im} \\, Q$.\n\\end{proof}\n\n\n\\begin{algorithm}[H]\n    \\caption{Sparse Randomized LU Decomposition}\n    \\textbf{Input:} $A$ matrix of size $m \\times n$ to decompose; approximation rank $r < n$; $k_1 < l_1 < k_2 < l_2$ number of columns to use in the projections and the size of output matrices.\\\\\n    \\textbf{Output:} Matrices $P,Q,L,U$ such that $\\Vert PAQ-LU\\Vert_F \\le \\mathcal{O}(\\Delta_r(A))$, where $P$ and $Q$\n    are orthogonal permutation matrices, $L$ and $U$ are lower and upper triangular matrices, respectively.\n    \\begin{algorithmic}[1]\n        \\STATE Create a random SEM $S_1\\in M_{l_1 \\times n}$ and an SRFT matrix $\\Pi_1\\in M_{k_1 \\times l_1}$. Let $\\Omega_1 = \\Pi_1 S_1$ be of size $k_1 \\times n$.\n        \\STATE Compute $B = A\\Omega_1^*$ ($B\\in M_{m\\times k_1}$).\n        \\STATE Compute the LU decomposition of $B$: $PB = L_1U_1$, where $L_1\\in M_{m\\times k_1}$ is a lower triangular matrix and   $U_1\\in M_{k_1\\times k_1}$ is an upper triangular matrix.\n        \\STATE Create a random SEM $S_2$ of size $l_2 \\times m$ and an SRFT matrix $\\Pi_2\\in M_{k_2 \\times l_2}$. Let $\\Omega_2 = \\Pi_2 S_2$ be of size $k_2 \\times m$.\n        \\STATE Compute $\\Omega_2 L_1$ and $(\\Omega_2 L_1)^\\dagger$.\n        \\STATE Compute the LU decomposition with right partial pivoting of $(\\Omega_2 L_1)^\\dagger \\Omega_2 P A$ such that $(\\Omega_2 L_1)^\\dagger \\Omega_2 P AQ =\n        \\tilde{L}U$.\n        \\STATE $L \\gets L_1\\tilde{L}$.\n        \\STATE Return $L,U,P,Q$ \n    \\end{algorithmic}\n    \\label{alg:sparse_randomized_LU_2}\n\\end{algorithm}\n\n\n\\begin{theorem}[Correctness of the algorithm]\n    Let $A$ be an $m \\times n$ matrix. The sparse randomized LU decomposition of $A$ uses the integers\n     $k_1 = \\mathcal{O}(r\\log(r)), k_2 = \\mathcal{O}(r), l_1 = \\mathcal{O}(r^2\\log^6(r)), l_2 = \\mathcal{O}(r^2)$.\n      Application of Algorithm \\ref{alg:sparse_randomized_LU_2} gives $PAQ \\approx LU$, where $P$ and $Q$ are permutation matrices, and $L$ and $U$ are lower and upper triangular matrices,\n      respectively. Then, the approximation error from the application of the sparse randomized LU decomposition is bounded by $\\|LU - PAQ\\|_F \\leq \\mathcal{O}(\\Delta_r)$ with high probability.\n\\end{theorem}\n\n\\begin{proof}\nChoose $0 < \\varepsilon < 1$ ($\\varepsilon$ affects the error of the\ndecomposition) and $0 < \\delta < 1$ ($\\delta$ affects the\nprobability that the decomposition is accurate). According to\nAlgorithm \\ref{alg:sparse_randomized_LU_2}, $\\Omega_1=\\Pi_1 S_1 \\in\nM_{k_1\\times n}$ where $\\Pi_1 \\in M_{k_1 \\times l_1}$ is an SRFT\nmatrix and $S_1$ is a random SEM. The pivoted LU decomposition of\n$B$ is given by $PB=L_1U_1$. Let $ k_1 =\n\\mathcal{O}(r\\varepsilon^{-1}\\log(r/\\varepsilon))$ and $l_1 =\n\\mathcal{O}(r^2\\log^6(r/\\varepsilon)+ r\\varepsilon^{-1})$.  Then\nfrom Corollary \\ref{col:A_LLA} it follows that $\\|PA-L_1L_1^\\dagger\nPA\\|_F<(1+\\varepsilon)\\Delta_r$. Let\n\n", "itemtype": "equation", "pos": 15214, "prevtext": "\n    we get\n    \n", "index": 17, "text": "\\begin{equation}\\label{eq:minQX_A}\n    \\min\\limits_{X s.t. {\\operatorname{rank}} X=r} \\|QX - A\\|_F \\leq (1 + \\varepsilon) \\Delta_r.\n    \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\min\\limits_{Xs.t.{\\operatorname{rank}}X=r}\\|QX-A\\|_{F}\\leq(1+\\varepsilon)%&#10;\\Delta_{r}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mrow><mi>X</mi><mo>\u2062</mo><mi>s</mi></mrow><mo>.</mo><mi>t</mi><mo>.</mo><mrow><mrow><mo>rank</mo><mo>\u2061</mo><mi>X</mi></mrow><mo>=</mo><mi>r</mi></mrow></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>Q</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>-</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nThen, by Corollary \\ref{cor:sing_SU}, with high probability,\n$\\Omega_2 L_1$ is left invertible. Thus,\n\n", "itemtype": "equation", "pos": 19455, "prevtext": "\n\n    It follows that $\\|QQ^*A - A\\|_F \\leq \\min\\limits_{X s.t. {\\operatorname{rank}} X=r} \\|QX - A\\|_F $, which concludes the proof.\n\\end{proof}\n\nTheorem \\ref{thm:A_QQA} shows that $QQ^*A$ approximates  $A$ well.\nSince $Q$ and $Q^*A$ are relatively small matrices and since $Q$\nhas orthogonal columns, then the SVD computation of $Q^*A$ is faster than\nthe SVD computation of $A$. Unfortunately, $Q^*$ is a dense\nmatrix, then the multiplication $Q^*A$ is computationally expensive. We\nnow show how to replace the computation of $Q^*A$ with a\nmultiplication of $A$ by a sparse matrix without affecting the\naccuracy too much.\n\\begin{corollary}\\label{col:A_LLA}\n    Let $A$ be a $m \\times n$ matrix. Assume  $l = O(r^2 \\log^6(r/\\varepsilon) + r\\varepsilon^{-1})$, $k = O(r\\varepsilon^{-1} \\log(r/\\varepsilon))$,\n    $\\Pi \\in M_{k \\times l}$ is an SRFT matrix and an SEM $S \\in M_{l \\times n}$. Denote $\\Omega = \\Pi S$ and the pivoted LU decomposition of $A\\Omega^*$ is denoted by $PA\\Omega^* = LU$. Then $\\|PA-LL^\\dagger PA\\|_F<(1+\\varepsilon)\\Delta_r(A)$.\n\\end{corollary}\n\\begin{proof}\n    The proof is the same as that of Theorem \\ref{thm:A_QQA}. The reason that the same proof works is that ${\\rm Im} \\, L = {\\rm Im} \\, Q$.\n\\end{proof}\n\n\n\\begin{algorithm}[H]\n    \\caption{Sparse Randomized LU Decomposition}\n    \\textbf{Input:} $A$ matrix of size $m \\times n$ to decompose; approximation rank $r < n$; $k_1 < l_1 < k_2 < l_2$ number of columns to use in the projections and the size of output matrices.\\\\\n    \\textbf{Output:} Matrices $P,Q,L,U$ such that $\\Vert PAQ-LU\\Vert_F \\le \\mathcal{O}(\\Delta_r(A))$, where $P$ and $Q$\n    are orthogonal permutation matrices, $L$ and $U$ are lower and upper triangular matrices, respectively.\n    \\begin{algorithmic}[1]\n        \\STATE Create a random SEM $S_1\\in M_{l_1 \\times n}$ and an SRFT matrix $\\Pi_1\\in M_{k_1 \\times l_1}$. Let $\\Omega_1 = \\Pi_1 S_1$ be of size $k_1 \\times n$.\n        \\STATE Compute $B = A\\Omega_1^*$ ($B\\in M_{m\\times k_1}$).\n        \\STATE Compute the LU decomposition of $B$: $PB = L_1U_1$, where $L_1\\in M_{m\\times k_1}$ is a lower triangular matrix and   $U_1\\in M_{k_1\\times k_1}$ is an upper triangular matrix.\n        \\STATE Create a random SEM $S_2$ of size $l_2 \\times m$ and an SRFT matrix $\\Pi_2\\in M_{k_2 \\times l_2}$. Let $\\Omega_2 = \\Pi_2 S_2$ be of size $k_2 \\times m$.\n        \\STATE Compute $\\Omega_2 L_1$ and $(\\Omega_2 L_1)^\\dagger$.\n        \\STATE Compute the LU decomposition with right partial pivoting of $(\\Omega_2 L_1)^\\dagger \\Omega_2 P A$ such that $(\\Omega_2 L_1)^\\dagger \\Omega_2 P AQ =\n        \\tilde{L}U$.\n        \\STATE $L \\gets L_1\\tilde{L}$.\n        \\STATE Return $L,U,P,Q$ \n    \\end{algorithmic}\n    \\label{alg:sparse_randomized_LU_2}\n\\end{algorithm}\n\n\n\\begin{theorem}[Correctness of the algorithm]\n    Let $A$ be an $m \\times n$ matrix. The sparse randomized LU decomposition of $A$ uses the integers\n     $k_1 = \\mathcal{O}(r\\log(r)), k_2 = \\mathcal{O}(r), l_1 = \\mathcal{O}(r^2\\log^6(r)), l_2 = \\mathcal{O}(r^2)$.\n      Application of Algorithm \\ref{alg:sparse_randomized_LU_2} gives $PAQ \\approx LU$, where $P$ and $Q$ are permutation matrices, and $L$ and $U$ are lower and upper triangular matrices,\n      respectively. Then, the approximation error from the application of the sparse randomized LU decomposition is bounded by $\\|LU - PAQ\\|_F \\leq \\mathcal{O}(\\Delta_r)$ with high probability.\n\\end{theorem}\n\n\\begin{proof}\nChoose $0 < \\varepsilon < 1$ ($\\varepsilon$ affects the error of the\ndecomposition) and $0 < \\delta < 1$ ($\\delta$ affects the\nprobability that the decomposition is accurate). According to\nAlgorithm \\ref{alg:sparse_randomized_LU_2}, $\\Omega_1=\\Pi_1 S_1 \\in\nM_{k_1\\times n}$ where $\\Pi_1 \\in M_{k_1 \\times l_1}$ is an SRFT\nmatrix and $S_1$ is a random SEM. The pivoted LU decomposition of\n$B$ is given by $PB=L_1U_1$. Let $ k_1 =\n\\mathcal{O}(r\\varepsilon^{-1}\\log(r/\\varepsilon))$ and $l_1 =\n\\mathcal{O}(r^2\\log^6(r/\\varepsilon)+ r\\varepsilon^{-1})$.  Then\nfrom Corollary \\ref{col:A_LLA} it follows that $\\|PA-L_1L_1^\\dagger\nPA\\|_F<(1+\\varepsilon)\\Delta_r$. Let\n\n", "index": 19, "text": "$$\nl_2 \\geq \\delta^{-1}(r^2+r)/(2\\varepsilon-\\varepsilon^2)^2, k_2 \\geq 4\\left[\\sqrt{r} + \\sqrt{8\\log(rl_2)}\\right]^2 \\log r.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"l_{2}\\geq\\delta^{-1}(r^{2}+r)/(2\\varepsilon-\\varepsilon^{2})^{2},k_{2}\\geq 4%&#10;\\left[\\sqrt{r}+\\sqrt{8\\log(rl_{2})}\\right]^{2}\\log r.\" display=\"block\"><mrow><mrow><mrow><msub><mi>l</mi><mn>2</mn></msub><mo>\u2265</mo><mrow><mrow><msup><mi>\u03b4</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><mi>r</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b5</mi></mrow><mo>-</mo><msup><mi>\u03b5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>\u2265</mo><mrow><mn>4</mn><mo>\u2062</mo><msup><mrow><mo>[</mo><mrow><msqrt><mi>r</mi></msqrt><mo>+</mo><msqrt><mrow><mn>8</mn><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>r</mi><mo>\u2062</mo><msub><mi>l</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msqrt></mrow><mo>]</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>r</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nNext, we bound $\\|L_1(\\Omega_2 L_1)^\\dagger (\\Omega_2 L_1)L_1^\\dagger P A - P A\\|_F$ by the following:\n\n\n", "itemtype": "equation", "pos": 19686, "prevtext": "\nThen, by Corollary \\ref{cor:sing_SU}, with high probability,\n$\\Omega_2 L_1$ is left invertible. Thus,\n\n", "index": 21, "text": "$$\\|L_1L_1^\\dagger PA - PA\\|_F = \\|L_1(\\Omega_2 L_1)^\\dagger (\\Omega_2 L_1)L_1^\\dagger P A - P A\\|_F. $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\|L_{1}L_{1}^{\\dagger}PA-PA\\|_{F}=\\|L_{1}(\\Omega_{2}L_{1})^{\\dagger}(\\Omega_{2%&#10;}L_{1})L_{1}^{\\dagger}PA-PA\\|_{F}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nLet $L_1 = U\\Sigma V^*$ be the SVD of $L_1$, where $U\\in M_{m\\times\nk_1}$,  $\\Sigma \\in M_{k_1\\times k_1}$, and $V \\in M_{k_1\\times\nk_1}$. Then\n\n", "itemtype": "equation", "pos": 19895, "prevtext": "\nNext, we bound $\\|L_1(\\Omega_2 L_1)^\\dagger (\\Omega_2 L_1)L_1^\\dagger P A - P A\\|_F$ by the following:\n\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:bound1proof}\n    \\begin{split}\n        \\|L_1& (\\Omega_2 L_1)^{-1}\\Omega_2 PA- PA\\|_F= \\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 PA - L_1(\\Omega_2 L_1)^{-1}(\\Omega_2 L_1)L_1^\\dagger PA \\\\\n        &+L_1(\\Omega_2 L_1)^{-1}(\\Omega_2 L_1)L_1^\\dagger PA - PA\\|_F\\\\\n        &=\\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 (PA -L_1L_1^\\dagger PA) + L_1L_1^\\dagger PA - PA\\|_F \\\\\n        & \\leq\\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 (PA-L_1L_1^\\dagger PA)\\|_F  +  \\|L_1L_1^\\dagger PA - PA\\|_F \\\\\n        & \\leq \\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 \\|_2\\|PA-L_1L_1^\\dagger PA\\|_F +\\|L_1L_1^\\dagger PA - PA\\|_F \\\\\n        &=(\\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 \\|_2 + 1)\\|PA-L_1L_1^\\dagger PA\\|_F.\n     \\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\|L_{1}&amp;\\displaystyle(\\Omega_{2}L_{1})^{-1}\\Omega_{2%&#10;}PA-PA\\|_{F}=\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}PA-L_{1}(\\Omega_{2}L_{1})^%&#10;{-1}(\\Omega_{2}L_{1})L_{1}^{\\dagger}PA\\\\&#10;&amp;\\displaystyle+L_{1}(\\Omega_{2}L_{1})^{-1}(\\Omega_{2}L_{1})L_{1}^{\\dagger}PA-%&#10;PA\\|_{F}\\\\&#10;&amp;\\displaystyle=\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}(PA-L_{1}L_{1}^{\\dagger}%&#10;PA)+L_{1}L_{1}^{\\dagger}PA-PA\\|_{F}\\\\&#10;&amp;\\displaystyle\\leq\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}(PA-L_{1}L_{1}^{%&#10;\\dagger}PA)\\|_{F}+\\|L_{1}L_{1}^{\\dagger}PA-PA\\|_{F}\\\\&#10;&amp;\\displaystyle\\leq\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}\\|_{2}\\|PA-L_{1}L_{1}%&#10;^{\\dagger}PA\\|_{F}+\\|L_{1}L_{1}^{\\dagger}PA-PA\\|_{F}\\\\&#10;&amp;\\displaystyle=(\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}\\|_{2}+1)\\|PA-L_{1}L_{1%&#10;}^{\\dagger}PA\\|_{F}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mo>\u2225</mo><msub><mi>L</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mi>P</mi><mi>A</mi><mo>-</mo><mi>P</mi><mi>A</mi><msub><mo>\u2225</mo><mi>F</mi></msub><mo>=</mo><mo>\u2225</mo><msub><mi>L</mi><mn>1</mn></msub><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mi>P</mi><mi>A</mi><mo>-</mo><msub><mi>L</mi><mn>1</mn></msub><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><msub><mi>L</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mi>P</mi><mi>A</mi></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mo>+</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>-</mo><msub><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo fence=\"true\">\u2225</mo></mrow><mi>F</mi></msub></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>\u2264</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nBy combining Eqs. \\eqref{eq:bound1proof} and \\eqref{eq:bound2proof}\nwith Corollary \\ref{cor:sing_SU} and Theorem \\ref{thm:sing_vals_S},\nwe get\n\n", "itemtype": "equation", "pos": 20749, "prevtext": "\nLet $L_1 = U\\Sigma V^*$ be the SVD of $L_1$, where $U\\in M_{m\\times\nk_1}$,  $\\Sigma \\in M_{k_1\\times k_1}$, and $V \\in M_{k_1\\times\nk_1}$. Then\n\n", "index": 25, "text": "\\begin{equation}\\label{eq:bound2proof}\n\\begin{array}{lll}\n    \\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 \\|_2 & = & \\|U \\Sigma V^*(\\Omega_2  U \\Sigma V^*)^{-1}\\Omega_2 \\|_2 \\\\\n    & = & \\|U \\Sigma V^*(\\Sigma V^*)^{-1}(\\Omega_2 U )^{-1}\\Omega_2 \\|_2 \\\\\n    & = & \\|U (\\Omega_2 U )^{-1}\\Omega_2\\|_2 \\\\\n    & = & \\| (\\Omega_2 U )^{-1}\\Omega_2\\|_2 \\\\\n    & \\leq & \\| (\\Omega_2 U )^{-1}\\|_2 \\|\\Omega_2\\|_{2} .\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{lll}\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}\\|_{2}&amp;=&amp;\\|U\\Sigma V%&#10;^{*}(\\Omega_{2}U\\Sigma V^{*})^{-1}\\Omega_{2}\\|_{2}\\\\&#10;&amp;=&amp;\\|U\\Sigma V^{*}(\\Sigma V^{*})^{-1}(\\Omega_{2}U)^{-1}\\Omega_{2}\\|_{2}\\\\&#10;&amp;=&amp;\\|U(\\Omega_{2}U)^{-1}\\Omega_{2}\\|_{2}\\\\&#10;&amp;=&amp;\\|(\\Omega_{2}U)^{-1}\\Omega_{2}\\|_{2}\\\\&#10;&amp;\\leq&amp;\\|(\\Omega_{2}U)^{-1}\\|_{2}\\|\\Omega_{2}\\|_{2}.\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd><mtd columnalign=\"left\"><mo>=</mo></mtd><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><mi>V</mi><mo>*</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><mi>V</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mo>=</mo></mtd><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><mi>V</mi><mo>*</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><mi>V</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>U</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mo>=</mo></mtd><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><mi>U</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>U</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mo>=</mo></mtd><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>U</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mo>\u2264</mo></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>U</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nFrom Algorithm \\ref{alg:sparse_randomized_LU_2}, obtain\n\n", "itemtype": "equation", "pos": 21314, "prevtext": "\nBy combining Eqs. \\eqref{eq:bound1proof} and \\eqref{eq:bound2proof}\nwith Corollary \\ref{cor:sing_SU} and Theorem \\ref{thm:sing_vals_S},\nwe get\n\n", "index": 27, "text": "$$\n\\begin{array}{lll}\n\\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 PA- PA\\|_F & \\leq  &(\\frac{C(n,k_2)}{0.4(1-\\varepsilon)} + 1)\\|PA-L_1L_1^\\dagger PA\\|_F \\\\ &\\leq &1.48(1+\\varepsilon)\\left(\\frac{C(n,k_2)}{0.4(1-\\varepsilon)} + 1\\right) \\Delta_r.\n\n\\end{array}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{lll}\\|L_{1}(\\Omega_{2}L_{1})^{-1}\\Omega_{2}PA-PA\\|_{F}&amp;\\leq&amp;(%&#10;\\frac{C(n,k_{2})}{0.4(1-\\varepsilon)}+1)\\|PA-L_{1}L_{1}^{\\dagger}PA\\|_{F}\\\\&#10;&amp;\\leq&amp;1.48(1+\\varepsilon)\\left(\\frac{C(n,k_{2})}{0.4(1-\\varepsilon)}+1\\right)%&#10;\\Delta_{r}.\\par&#10;\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mtd><mtd columnalign=\"left\"><mo>\u2264</mo></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mfrac><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>0.4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>L</mi><mn>1</mn><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mo>\u2264</mo></mtd><mtd columnalign=\"left\"><mrow><mrow><mn>1.48</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>0.4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\\subsection{Algorithm Complexity}\n\nDenote $m,n,r,k_1, k_2, l_1, l_2$ as in Algorithm \\ref{alg:sparse_randomized_LU_2}. Assume, without loss of generality, that $m \\geq n$. Then\n\\begin{enumerate}\n    \\item $\\Omega_1$ construction takes  $\\mathcal{O}(n + l_1 k_1)$ operations.\n    \\item $B=A\\Omega_1^*$ computation takes $\\mathcal{O}(mn+ml_1\\log(k_1))$ operations.\n    \\item Computation of the pivoted LU decomposition of $B$ takes $\\mathcal{O}(mk_1^2)$ operations.\n    \\item $\\Omega_2$ construction takes  $\\mathcal{O}(m + l_2 k_2)$ operations.\n    \\item $\\Omega_2 L_1$ and $(\\Omega_2 L_1)^{\\dagger}$ computation takes $\\mathcal{O}(m k_1 + k_1 l_2 \\log(k_2))$ and $\\mathcal{O}(k_2k_1^2)$ operations respectively.\n    \\item $(\\Omega_2 L_1)^{\\dagger}\\Omega_2 PA $ computation takes $\\mathcal{O}(mn+nl_2\\log(k_2)+k_2k_1n)$ operations.\n    \\item LU decomposition of  $(\\Omega_2 L_1)^{\\dagger}\\Omega_2 PA$ takes $\\mathcal{O}(k_2^2n)$ operations.\n    \\item $L = L_1\\tilde{L}$ computation takes $\\mathcal{O}(mk_1^2)$ operations.\n\\end{enumerate}\nThis sums up to a total complexity of\n\n", "itemtype": "equation", "pos": 21621, "prevtext": "\nFrom Algorithm \\ref{alg:sparse_randomized_LU_2}, obtain\n\n", "index": 29, "text": "$$\\|LU - PAQ\\|_F =\\|L_1 \\tilde{L}U Q^* - PA\\|_F =\\|L_1(\\Omega_2 L_1)^{-1}\\Omega_2 PA - PA\\|_F \\leq \\mathcal{O}(\\Delta_r),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\|LU-PAQ\\|_{F}=\\|L_{1}\\tilde{L}UQ^{*}-PA\\|_{F}=\\|L_{1}(\\Omega_{2}L_{1})^{-1}%&#10;\\Omega_{2}PA-PA\\|_{F}\\leq\\mathcal{O}(\\Delta_{r}),\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>L</mi><mo>\u2062</mo><mi>U</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>Q</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><msup><mi>Q</mi><mo>*</mo></msup></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msub><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\nand the complexity of the decomposition of a sparse matrix $A$ is \n\n", "itemtype": "equation", "pos": 22861, "prevtext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\\subsection{Algorithm Complexity}\n\nDenote $m,n,r,k_1, k_2, l_1, l_2$ as in Algorithm \\ref{alg:sparse_randomized_LU_2}. Assume, without loss of generality, that $m \\geq n$. Then\n\\begin{enumerate}\n    \\item $\\Omega_1$ construction takes  $\\mathcal{O}(n + l_1 k_1)$ operations.\n    \\item $B=A\\Omega_1^*$ computation takes $\\mathcal{O}(mn+ml_1\\log(k_1))$ operations.\n    \\item Computation of the pivoted LU decomposition of $B$ takes $\\mathcal{O}(mk_1^2)$ operations.\n    \\item $\\Omega_2$ construction takes  $\\mathcal{O}(m + l_2 k_2)$ operations.\n    \\item $\\Omega_2 L_1$ and $(\\Omega_2 L_1)^{\\dagger}$ computation takes $\\mathcal{O}(m k_1 + k_1 l_2 \\log(k_2))$ and $\\mathcal{O}(k_2k_1^2)$ operations respectively.\n    \\item $(\\Omega_2 L_1)^{\\dagger}\\Omega_2 PA $ computation takes $\\mathcal{O}(mn+nl_2\\log(k_2)+k_2k_1n)$ operations.\n    \\item LU decomposition of  $(\\Omega_2 L_1)^{\\dagger}\\Omega_2 PA$ takes $\\mathcal{O}(k_2^2n)$ operations.\n    \\item $L = L_1\\tilde{L}$ computation takes $\\mathcal{O}(mk_1^2)$ operations.\n\\end{enumerate}\nThis sums up to a total complexity of\n\n", "index": 31, "text": "$$\n\\mathcal{O}\\left(mn  +  mk_1^2  +  nk_2^2  +  m l_1\\log(k_1)  +  nl_2\\log(k_2)  +  k_1l_2\\log(k_2)  \\right),\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{O}\\left(mn+mk_{1}^{2}+nk_{2}^{2}+ml_{1}\\log(k_{1})+nl_{2}\\log(k_{2})+%&#10;k_{1}l_{2}\\log(k_{2})\\right),\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>+</mo><mrow><mi>m</mi><mo>\u2062</mo><msubsup><mi>k</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msubsup><mi>k</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>m</mi><mo>\u2062</mo><msub><mi>l</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msub><mi>l</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>l</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04280.tex", "nexttext": "\n\n\\section{Numerical Results}\n\\label{sec:numerical_results} In this section, the performance of\nthe algorithm is evaluated. The algorithm is implemented in MATLAB\nusing complex matrices. The Sub-sampled Randomized Hadamard Transform (SRHT) \\cite{tropp2011improved} is used with real matrices instead of using the SRFT\nmatrix to achieve an efficient computation.\n\n\\subsection{Numerical rank growth}\nIn this experiment, we consider a matrix of size $n = 5000$ where\nits  numerical rank changes between 50 to 900, i.e., the first $r$\nsingular values are 1 and the other are exponentially decaying from\n$e^{-10}$ to $e^{-200}$. As shown in Figure \\ref{fig:const_n__step_sigma}, Algorithm \\ref{alg:sparse_randomized_LU_2} results in an approximation of\nthe same order as the numerical rank, up to a small error.\n\n\n    \\begin{figure}[H]\n        \\makebox[\\textwidth][c]{\n\t        \\begin{subfigure}[b]{0.6\\textwidth}\n\t            \\includegraphics[width=1\\textwidth]{sparse_LU_test__const_n__step_sigma__n_5000__sig_exp_-10_-100__ranks_50_50_900_time.png}\n\t            \\caption{}\n\t            \\label{fig:const_n__step_sigma_time}\n\t        \\end{subfigure}\n\t        ~\n\t        \\begin{subfigure}[b]{0.6\\textwidth}\n\t            \\includegraphics[width=1\\textwidth]{sparse_LU_test__const_n__step_sigma__n_5000__sig_exp_-10_-100__ranks_50_50_900_err.png}\n\t            \\caption{}\n\t            \\label{fig:const_n__step_sigma_err}\n\t        \\end{subfigure}\n\t    }\n\n        \\caption{Results from the approximation of a matrix of size $5000 \\times 5000$ with different numerical ranks.\n         The numerical rank is shown on the x-axis. (a) the y-axis denotes the time  each algorithm takes. (b) the y-axis denotes the error of each algorithm. }\n        \\label{fig:const_n__step_sigma}\n    \\end{figure}\n\n\\subsection{Improving the accuracy for a fixed matrix}\nIn this experiment, we consider a matrix of size $n = 5000$ with singular values that decay exponentially from 1 to $e^{-100}$.\n We compute the $r$-th rank approximation by increasing $r$.\n\n    \\begin{figure}[H]\n        \\makebox[\\textwidth][c]{\n\t\t    \\begin{subfigure}[b]{0.6\\textwidth}\n\t\t\t    \\includegraphics[width=1\\textwidth]{sparse_LU_test__const_n__const_sigma__n_5000__sig_exp_0_-100__ranks_100_50_900_time.png}\n\t            \\caption{}\n\t            \\label{fig:const_n__const_sigma_time}\n\t        \\end{subfigure}\n\t        ~\n\t        \\begin{subfigure}[b]{0.6\\textwidth}\n\t\t\t    \\includegraphics[width=1\\textwidth]{sparse_LU_test__const_n__const_sigma__n_5000__sig_exp_0_-100__ranks_100_50_900_err.png}\n\t            \\caption{}\n\t            \\label{fig:const_n__const_sigma_err}\n\t        \\end{subfigure}\n\t    }\n        \\caption{Results from the  approximation a matrix of size $5000 \\times 5000$ with exponentially decaying singular values.\n        The approximation rank is shown on the x-axis. (a) the y-axis denotes the time each algorithm takes. (b) the y-axis denotes the error of each algorithm.}\n        \\label{fig:const_n__const_sigma}\n    \n    \\end{figure}\n\n\\subsection{Running on GPU}\nThe Sparse randomized LU decomposition (Algorithm \\ref{alg:sparse_randomized_LU_2}) can be\nfully parallelized to run efficiently on a GPU card and on a\ndistributed computing system such as Hadoop or Spark. In the following test, a $5000 \\times 5000$\nrandom matrix was processed in double precision on a GPU card using\nthe MATLAB's GPU interface. MATLAB 2015a enables us to apply \ncertain sparse matrices operations to the GPU. GTX Titan Black GPU card was used. Figure \\ref{fig:gpu_vs_cpu} compares the running\ntime between GPU and CPU.\n\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.75\\textwidth]{sparse_LU_GPU_vs_CPU.png}\n    \\caption{Running time on GPU vs. CPU of the randomized sparse LU decomposition (Alg. \\ref{alg:sparse_randomized_LU_2})}\n    \\label{fig:gpu_vs_cpu}\n\\end{figure}\n\n\n\\section*{Conclusion}\n\\label{sec:conclusion} In this paper, the Sparse--Randomized--LU\nalgorithm is presented. This algorithm utilizes sparse random\nprojections that are combined with FFT--based projections for\ncomputing low rank LU matrix decompositions. The proposed technique\nwas analyzed theoretically to achieve asymptotic bounds. The\nconducted numerical experiments  compare the performance of the\nalgorithm to other  algorithms such as sparse SVD and fast\nrandomized LU.\n\\section*{Acknowledgment}\nThis research was partially supported by the Israeli Ministry of\nScience \\& Technology (Grants No. 3-9096, 3-10898), US-Israel\nBinational Science Foundation (BSF 2012282), Blavatnik Computer\nScience Research Fund and Blavatink ICRC Funds.\n\n\n\\bibliographystyle{siam}\n\n\\bibliography{SparseRandomizedLU}\n\n", "itemtype": "equation", "pos": 23043, "prevtext": "\nand the complexity of the decomposition of a sparse matrix $A$ is \n\n", "index": 33, "text": "$$\n\\mathcal{O}\\left({\\operatorname{nnz}}(A)  +  mk_1^2  +  nk_2^2  +  m l_1\\log(k_1)  +  nl_2\\log(k_2)  +  k_1l_2\\log(k_2)  \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{O}\\left({\\operatorname{nnz}}(A)+mk_{1}^{2}+nk_{2}^{2}+ml_{1}\\log(k_{1%&#10;})+nl_{2}\\log(k_{2})+k_{1}l_{2}\\log(k_{2})\\right).\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>nnz</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>m</mi><mo>\u2062</mo><msubsup><mi>k</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msubsup><mi>k</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>m</mi><mo>\u2062</mo><msub><mi>l</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msub><mi>l</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>l</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]