[{"file": "1601.05384.tex", "nexttext": "\n\\noindent where $\\mbox{weight}_i$ and $\\mbox{cost}_i$ are the\nweight and cost of the $i$th task, respectively, and \n$\\mbox{unlocks}_i$ is the set of tasks that the $i$th task\nunlocks.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=TaskWeight.eps,height=0.4\\textwidth}}\n    \\caption{Computation of the task weight.\n      In this task graph, the height of each task corresponds to its\n      computational {\\em cost}.\n      The {\\em weight} of each task, e.g.~the topmost task,\n      is computed from its cost plus\n      the maximum dependent task's weight.\n      This corresponds to the computatoinal cost of the critical\n      path following the task's dependencies.}\n    \\label{fig:TaskWeight}\n\\end{figure}\n\n\n\\subsection{Resources}\n\nResources consist of the following data structure:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct resource {\n  struct resource *parent;\n  volatile int lock, hold;\n  int owner;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\nThe {\\tt parent} field, which points to another resource, is\nused to create hierarchical resources, i.e.~resources\nthat are themselves subsets of larger resources.\nThis can be useful, e.g.~in the context of particle simulations\ndescribed in the next section, where particles are sorted\ninto hierarchical cells which are used at different levels.\nThe {\\tt owner} field is the ID of the queue to which this\nresource has been preferentially assigned.\n\nThe {\\tt lock} field is either {\\tt 0} or {\\tt 1} and indicates\nwhether this resource is currently in use, i.e.~{\\em locked}.\nTo avoid race conditions, this value should be tested\nand set using atomic instructions only.\nThe {\\tt hold} field is a counter indicating how many\nsub-resources of the current resource are locked.\nIf a resource's hold counter is not zero, then it is\n{\\em held} and cannot be locked.\nLikewise, if a resource is locked, it cannot be held\n(see {[1]}{Resources}).\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=Resources.eps,width=0.7\\textwidth}}\n    \\caption{A hierarchy of cells (left) and the hierarchy of\n        corresponding hierarchical resources at each level.\n        Each square on the right represents a single resource, and\n        arrows indicate the resource's parent.\n        Resources colored red are locked, resources colored orange\n        are held, where the number in the square indicates the\n        value of the hold counter.}\n    \\label{fig:Resources}\n\\end{figure}\n\nIncrementing the hold counter of the resource can be implemented\nas follows:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid resource_hold(struct resource *r) {\n  if (atomic_cas(&r->lock, 0, 1) != 0) return 0;\n  atomic_inc(&r->hold);\n  r->lock = 0;\n  return 1;\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where {\\tt atomic\\_cas(val,old,new)} is an atomic\ncompare-and-swap operation that sets {\\tt val} to {\\tt new}\nif it is currently equal to {\\tt old}.\nSimilarly, {\\tt atomic\\_inc(val)} increments {\\tt val} by one\natomically.\nThe resource's {\\tt lock} is used to check if the resource\nis already locked (line 2), and it is held while {\\tt hold}\nis incremented, to avoid overlapping hold/lock operations.\nIf the resource can be locked, the hold counter is incremented\natomically (line~3), and the lock is released (line~4),\nreturning {\\tt 1} or {\\tt 0} if the resource could be held\nor not, respectively.\n\nThe locking procedure itself is implemented as follows:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid resource_lock(struct resource *r) {\n  struct resource *up, *top;\n  if (r->hold && atomic_cas(&r->lock, 0, 1) != 0)\n    return 0;\n  if (r->hold) {\n    r->lock = 0;\n    return 0;\n  }\n  for (up = r->parent; up != NULL; up = up->parent)\n    if (!resource_hold(up)) break;\n  if ((top = up) != NULL) {\n    for (up = r->parent; up != top, up = up->parent)\n      atomic_dec(&up->hold);\n    r->lock = 0;\n    return 0;\n  } else\n    return 1;\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where in line~3 the resource is first locked if it\nis not held.\nDue to a possible race condition when holding the resource,\nthe {\\tt hold} counter must be checked again once the resource\nhas been locked (line~5).\nIn lines~9--10 the hold counters of the hierarchical parents\nare incremented using the procedure described earlier.\nIf this process fails at any point (line~11), the\npreviously set hold counters are decremented (line~13)\nand the lock is released (line~14).\nThe procedure then returns {\\tt 1} or {\\tt 0} if the resource\ncould be locked or not, respectively.\n\nFinally, unlocking a resource is relatively straight-forward,\ni.e.~the lock is set to zero and the hold counters up the\nresource hierarchy are decremented.\n\n\n\\subsection{Queues}\n\nThe main job of the task queues is, given a set of ready tasks,\nto find the task with maximum weight, i.e.~the task along the\nlongest critical path, whose resources can all\nbe locked, and to do so as efficiently as possible.\n\nOne possible strategy would be to maintain an array of tasks\nsorted by their weights, and to traverse that list in descending\norder, trying to lock the resources of each task, until\na lockable task is found, or returning a failure otherwise.\nAlthough this would return the best possible task, it\nrequires maintaining a sorted array in which inserting\nor removing an entry is in {[1]}{n} for $n$ elements.\nUsing an unsorted array would require only {[1]}{1} operations for\ninsertion and deletion, but is undesirable as it completely\nignores the task weights.\n\nAs a compromise, the queue stores the tasks in an array\norganized as a max-heap, i.e.~where the $k$th entry is ``larger''\nthan both the $2k+1$st and the $2k+2$nd entry,\nwith the task with maximum weight\nin the first position.\nMaintaining this heap structure requires {[1]}{\\log n}\noperations for both insertion and deletion, i.e. for the\nbubble-up and trickle-down operations respectively.\n\nUnfortunately, there is no way of efficiently traversing all\nthe elements in such a heap in decreasing order.\nThe array of tasks is therefore traversed as if it were sorted,\nreturning the first task that can be locked.\nAlthough the first task in the array will be the task with\nmaximum weight, the following tasks are only loosely ordered,\nwhere the $k$th of $n$ tasks has a larger weight than at least\n$\\lfloor n/k\\rfloor -1$ other tasks.\nAlthough this is not a particularly good lower bound, it turns\nout to be quite sufficient in practice.\n\nThe data structure for the queue is defined as follows:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct queue {\n  struct task **tasks;\n  int count, lock;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where {\\tt tasks} is an array of {\\tt count} pointers\nto the tasks in max-heap order, and {\\tt lock} is used to\nguarantee exclusive access to the queue.\n\nInserting a task in the queue is relatively straight-forward:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid queue_put(struct queue *q, struct task *t) {\n  while (atomic_cas(q->lock, 0, 1) != 0) {}\n  q->tasks[q->count++] = t;\n  bubble-up the q->count - 1st entry of q->tasks.\n  q->lock = 0;\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where the loop in line~2 spins until an exclusive\nlock on the queue can be obtained.\nThe task is added to the end of the heap array (line~3)\nand the heap order is fixed (line~4).\nBefore returning, the lock on the queue is released (line~5).\n\nObtaining a task from the queue can be implemented as follows:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct task *queue_get(struct queue *q) {\n  struct task *res = NULL;\n  int j, k;\n  while (atomic_cas(q->lock, 0, 1) != 0) {}\n  for (k = 0; k < q->count; k++) {\n    for (j = 0; j < q->tasks[k]->nr_locks; j++)\n      if (!resource_lock(q->tasks[k]->lock[j])) break;\n    if (j < q->tasks[k]->nr_locks)\n      for (j = j - 1; j >= 0; j--)\n        resource_unlock(q->tasks[k]->lock[j]);\n    else\n      break;\n  }\n  if (k < q->count) {\n    res = q->tasks[k];\n    q->tasks[k] = q->tasks[--q->count];\n    trickle-down the kth entry of q->tasks.\n  }\n  q->lock = 0;\n  return res;\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where, as with the queue insertion, the queue is first\nlocked for exclusive access (line~4).\nThe array of task pointers is then traversed (line~5),\nlocking the resources of each task (lines~6--7).\nIf any of these locks fail (line~8), the locks that were obtained\nare released (lines~9--10), otherwise, the traversal is aborted\n(line~12).\nIf all the locks on a task could be obtained (line~14), the\ntask pointer is replaced by the last pointer in the heap (line~16)\nand the heap order is restored (line~17).\nFinally, the queue lock is released (line~19) and the locked task,\nor {\\tt NULL} if no lockable task could be found, is returned.\n\nNote that this approach of sequentially locking multiple resources\nis prone to the so-called ``dining philosophers'' problem, i.e.~if\ntwo tasks attempt, simultaneously, to lock the resources $A$ and $B$;\nand $B$ and $A$, respectively, via separate queues, their respective calls\nto {\\tt queue\\_get} will potentially fail perpetually.\nThis type of deadlock, however, is easily avoided by sorting the\nresources in each task according to some global criteria, e.g.~the\nresource ID or the address in memory of the resource.\n\nNote also that protecting the entire queue with a mutex\nis not particularly scalable, and several authors, e.g.~\\cite{ref:Sundell2003},\nhave presented concurrent data structures that avoid this type\nof locking.\nHowever, since we normally use one queue per computational thread,\ncontention will only happens due to work-stealing, i.e.~when\nanother idle computational thread tries to poach tasks.\nSince this happens only rarely, we opt for the simpler locking approach.\nThis decision is backed by the results in Section~5.\n\n\\subsection{Scheduler}\n\nThe scheduler object is used as the main interface to the\nQuickSched task scheduler, and as such contains the instances\nof the other three object types:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct qsched {\n  struct task *tasks;\n  struct queue *queues;\n  struct resource *res;\n  int nr_tasks, nr_queues, nr_resources;\n  volatile int waiting;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where the only additional field {\\tt waiting} is\nused to keep track of the number of tasks that have not been\nexecuted.\nNote that for brevity, and to avoid conflicts with the naming\nschemes of other standard libraries, the type name {\\tt qsched}\nis used for the scheduler data type.\n\nThe tasks are executed as follows:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid qsched_run(qsched *s, void (*fun)(int, void *)) {\n  qsched_start(s);\n  #pragma omp parallel\n  {\n    int qid = omp_get_thread_num() \n    struct task *t;\n    while ((t = qsched_gettask(s, qid)) != NULL) {\n      fun(t->type, t->data);\n      qsched_done(s, t);\n    }\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where {\\tt qsched\\_start} initializes the tasks and\nfills the queues (line~1).\nFor simplicity, OpenMP \\citep{ref:Dagum1998}, which is available\nfor most compilers, is used to create a parallel section\nin which the code between lines~4 and~11 is executed\nconcurrently.\nA version using {\\tt pthreads} \\citep{ref:Pthreads1995}\ndirectly\\footnote{In most environments, OpenMP is implemented\non top of {\\tt pthreads}, e.g. the {\\tt gcc} compiler's libgomp.}\nis also available.\nThe parallel section consists of a loop (lines~7--10) in\nwhich a task is acquired via {\\tt qsched\\_gettask}\nand its type and data are passed to a user-supplied\n{\\em execution function} {\\tt fun}.\nOnce the task has been executed, it is returned to the\nscheduler via the function {\\tt qsched\\_done}, i.e.~to\nunlock its resources and unlock dependent tasks.\nThe loop terminates when the scheduler runs out of tasks,\ni.e.~when {\\tt qsched\\_gettask} returns {\\tt NULL}, and\nthe function exits once all the threads have exited their\nloops.\n\nAt the start of a parallel computation, {\\tt qsched\\_start}\nidentifies the tasks that have no dependencies and sends them\nto queues via the function {\\tt qsched\\_enqueue} which\ntries to identify the best\nqueue for a given task by looking at which queues last used\nthe resources used and locked by the task, e.g.:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid qsched_enqueue(qsched *s, struct task *t) {\n  int best = 0, score[s->nr_queues];\n  for (int k = 0; k < s->nr_queues; k++)\n    score[k] = 0;\n  for (int k = 0; k < t->nr_locks; k++) {\n    int qid = t->locks[k]->owner;\n    if (++score[qid] > score[best]) best = qid;\n  }\n  for (int k = 0; k < t->nr_uses; k++) {\n    int qid = t->uses[k]->owner;\n    if (++score[qid] > score[best]) best = qid;\n  }\n  queue_put(&s->queues[best], t);\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where the array {\\tt score} keeps a count of the\ntask resources ``owned'', or last used, by each queue.\nThe task is then sent to the queue with the highest such score\n(line~13).\n\nThe function {\\tt qsched\\_gettask} fetches a task from\none of the queues:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct task *qsched_gettask(qsched *s, int qid) {\n  struct task *res = NULL;\n  int k;\n  while (s->waiting) {\n    if ((res = queue_get(s->queues[qid])) == NULL) {\n      loop over all other queues in random order with index k\n        if ((res = queue_get(s->queues[k])) != NULL)\n          break;\n    } else\n      break;\n  }\n  if (res != NULL && s->reown) {\n    for (k = 0; k < res->nr_locks; k++)\n      res->locks[k]->owner = qid;\n    for (k = 0; k < res->nr_uses; k++)\n      res->uses[k]->owner = qid;\n  }\n  return res;\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where the parameter {\\tt qid} is the index of the\npreferred queue.\nIf the queue is empty, or all of the tasks in that queue had\nunresolved conflicts, the scheduler uses a variant of the\n{\\em work stealing} described in \\cite{ref:Blumofe1999},\ni.e.~it loops over all other queues\nin a random order (line~6) and tries to get a task from them\n(line~7).\nIf a task could be obtained from any queue and task re-owning\nis enabled (line~12),\nthe resources it locks and uses are marked as now being owned\nby the preferred queue (lines~13--16).\nFinally, the task, or {\\tt NULL} if no task could be obtained,\nis returned.\n\nThe final step in a task's life cycle is, on completion,\nto unlock the resources and tasks which depend on it.\nThis is handled by the function {\\tt qsched\\_done}, which\ncalls {\\tt qsched\\_enqueue} on any tasks for which the\nwait counter is decremented to zero.\nOnce all the dependent tasks have been unlocked, the\n{\\tt waiting} counter of the scheduler is decremented.\n\n\n\\section{Validation}\n\nThis section presents two test cases that show\nhow QuickSched can be used in real-world applications, and\nprovides benchmarks to assess its efficiency and scalability.\nThe first test is the tiled QR decomposition originally\ndescribed in \\cite{ref:Buttari2009}, which has been used as a benchmark\nby other authors \\citep{ref:Agullo2009b,ref:Badia2009,ref:Bosilca2012}.\nThis example only requires dependencies and is presented \nas a point of comparison to existing task-based parallel\nprogramming infrastructures.\nThe second example is a Barnes-Hut tree-code, a problem\nsimilar to the Fast Multipole Method described in both\n\\cite{ref:Ltaief2012} and \\cite{ref:Agullo2013}.\nThis example shows how conflicts, modeled\nvia hierarchical resources, can be useful in modelling and executing\na problem efficiently.\n\nThe source code of both examples is distributed with the\nQuickSched library, along with scripts to run the benchmarks\nand generate the plots used in the following.\nAll examples were compiled with gcc v.\\,5.2.0 using the\n{\\tt -O2 -march=native} flags and run on\na 64-core AMD Opteron 6376 machine at 2.67\\,GHz.\n\n\n\\subsection{Task-Based QR Decomposition}\n\n\\cite{ref:Buttari2009} introduced the concept of using task-based\nparallelism for tile-based algorithms in numerical linear algebra,\npresenting parallel codes for the Cholesky, LU, and QR\ndecompositions.\nThese algorithms are now part of the PLASMA and MAGMA\nlibraries for parallel linear algebra \\citep{ref:Agullo2009}.\nThe former uses the QUARK task scheduler, which was originally\ndesigned for this specific task, while the latter currently uses\nthe StarPU task scheduler \\citep{ref:Agullo2011}.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=QR.eps,width=0.9\\textwidth}}\n    \\caption{Task-based QR decomposition of a matrix consisting\n        of $4\\times 4$ tiles.\n        Each circle represents a tile, and its color represents\n        the type of task on that tile at that level.\n        Empty circles have no task associated with them.\n        The arrows represent dependencies at each level, and\n        tasks at each level also implicitly depend on the\n        task at the same location in the previous level.}\n    \\label{fig:QR}\n\\end{figure}\n\nThe tiled QR factorization is based on four basic tasks,\nor kernels, as shown in {[1]}{QR}.\nFor a matrix consisting of $N\\times N$ tiles, $N$ passes,\nor levels, are computed, each computing a column and row of the QR\ndecomposition.\nThe tasks can be defined in terms of the tuples $(i,j,k)$,\nwhere $i$ and $j$ are the row and column of the tile, respectively,\nand $k$ is its level:\n\n\\begin{center}\n    \\begin{tabular}{llll}\n        Task & where & depends on task(s) & locks tile(s) \\\\\n        \\hline\n        \\epsfig{file=TaskRed.eps,height=9pt} DGEQRF & $i=j=k$ & $(i,j,k-1)$ & $(i,j)$ \\\\\n        \\epsfig{file=TaskGreen.eps,height=9pt} DLARFT & $i=k$, $j>k$ & $(i,j,k-1)$, $(k,k,k)$ & $(i,j)$ \\\\\n        \\epsfig{file=TaskBlue.eps,height=9pt} DTSQRF & $i>k$, $j=k$ & $(i,j,k-1)$, $(i-1,j,k)$ & $(i,j)$, $(j,j)$ \\\\\n        \\epsfig{file=TaskYellow.eps,height=9pt} DSSRFT & $i>k$, $j>k$ & $(i,j,k-1)$, $(i-1,j,k)$, $(i,k,k)$ & $(i,j)$ \\\\\n        \\hline\n    \\end{tabular}\n\\end{center}\n\n\\noindent and where the task names are the BLAS-like operation\nperformed on the given tiles.\nEvery task depends on the task at the same position and the\nprevious level, i.e.~the task $(i,j,k)$ always depends on\n$(i,j,k-1)$ for $k>1$.\nEach task also modifies its own tile $(i,j)$, and the DTSQRF\ntask additionally modifies the lower triangular part of the $(j,j)$th tile.\n\nAlthough the tile-based QR decomposition requires only dependencies,\ni.e.~no additional conflicts are needed to avoid concurrent access to\nthe matrix tiles, we still model each tile as a separate resource\nin QuickSched such that the scheduler can preferrentially assign\ntasks using the same tiles to the same thread.\nThe resources/tiles are initially assigned to the queues in column-major\norder, i.e.~the first $\\lfloor n_\\mathsf{tiles}/n_\\mathsf{queues}\\rfloor$\nare assigned to the first queue, and so on.\n\nThe QR decomposition was computed for a $2048\\times 2048$\nrandom matrix using tiles of size $64\\times 64$ floats using QuickSched\nas described above.\nThe task costs were initialized to the asymptotic cost of the underlying\noperations.\nFor this matrix, a total of 11\\,440 tasks with 21\\,824 dependencies,\nas well as 1\\,024 resources with 21\\,856 locks and 11\\,408 uses\nwere generated.\n\nFor these tests, {\\tt pthread} parallelism and resource re-owning\nwere used with one queue per core.\nThe QR decomposition was computed 10 times for each number of\ncores, and the average thereof taken for the scaling and\nefficiency results in {[1]}{QRResults}.\nThe timings are for {\\tt qsched\\_run}, including the cost of\n{\\tt qsched\\_start}, which does not run in parallel.\nSetting up the scheduler, tasks, and resources took, in all\ncases, an average of 7.2\\,ms, i.e.~at most 3\\% of the total\ncomputational cost.\n\nThe same decomposition was implemented using OmpSs v.\\,1.99.0,\ncalling the kernels directly using {\\tt \\#pragma omp task}\nannotations with the respective dependencies, and\nthe runtime parameters\n\\begin{quote}\n  \\tt --disable-yield --schedule=socket --cores-per-socket=16 \\\\--num-sockets=4\n\\end{quote}\n\\noindent Several different schedulers and parameterizations\nwere discussed with the authors of OmpSs and tested, with\nthe above settings producing the best results.\n\nThe scaling and efficiency relative to QuickSched are \nshown in {[1]}{QRResults}.\nThe difference in timings is the result of the different\ntask scheduling policies, as well as a smaller lag between the\nindividual tasks, as shown in {[1]}{QRTasks},\nwhich shows the assignment of the different tasks to cores for the\n64-core run.\nThe most visible difference between both schedulers is that\nthe DGEQRF tasks (in red) are scheduled as soon as they\nbecome available in QuickSched, thus preventing bottlenecks\nnear the end of the computation.\n\nSince in QuickSched the entire task structure is known explicitly\nin advance, the scheduler ``knows'' that the DGEQRF tasks all\nlie on the longest critical path and therefore executes them as\nsoon as possible.\nOmpSs does not exploit this knowledge, resulting in the less efficient\nscheduling seen in {[1]}{QRTasks}.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=QR_scaling.eps,width=\\textwidth}}\n    \\caption{Strong scaling and parallel efficiency of the tiled QR decomposition\n        computed over a $2048\\times 2048$ matrix with tiles of size\n        $64\\times 64$.\n        The QR decomposition with QuickSched takes 233\\,ms,\n        achieving 73\\% parallel efficiency, over all 64 cores.\n        The scaling and efficiency for OmpSs are computed relative to QuickSched.\n        }\n    \\label{fig:QRResults}\n\\end{figure}\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=tasks_qr.eps,width=\\textwidth}}\n    \\centerline{\\epsfig{file=tasks_qr_ompss.eps,width=\\textwidth}}\n    \\caption{Task scheduling in QuickSched (above) and OmpSs (below)\n        for a $2048\\times 2048$ matrix on 64 cores.\n        The task colors correspond to those in {[1]}{QR}.}\n    \\label{fig:QRTasks}\n\\end{figure}\n\n\n\\subsection{Task-Based Barnes-Hut N-Body Solver}\n\nThe Barnes-Hut tree-code \\citep{ref:Barnes1986}\nis an algorithm to approximate the\nsolution of an $N$-body problem, i.e.~computing all the\npairwise interactions between a set of $N$ particles,\nin {[1]}{N\\log N} operations, as opposed to in {[1]}{N^2} for the\nnaive direct computation.\nThe algorithm is based on a recursive octree decomposition:\nStarting from a cubic cell containing all the particles,\nthe cell is recursively bisected along all three spatial dimensions,\nresulting in eight sub-cells, until the number of particles\nper cell is smaller than some limit $n_\\mathsf{max}$.\nThe particle interactions can then be formulated recursively:\nGiven a particle and a set of particles in a cell,\nif the particle and cell\nare sufficiently well separated, the particle-cell interactions\nare approximated by interacting the particle with the cell's\ncenter of mass.\nIf the particle and the cell are too close, and the cell\nhas sub-cells, i.e.~it contained more than $n_\\mathsf{max}$\nparticles and was split in the recursive octree decomposition,\nthen the particle is interacted with each of the sub-cells\nrecursively.\nFinally, if the cell is not split, i.e.~it is a leaf cell\nin the octree, then the particle is interacted with all\nparticles in the cell, except for the particle itself if\nit is in the same cell.\nThis operation is performed for each particle, starting\nwith the root-level cell containing all the particles.\n\nIn our implementation, the particle data is sorted hierarchically,\nfollowing the octree structure.\nUnlike in many codes, where the leaves store an array of\npointers to the underlying particles, which are not necessarily\ncontiguous in memory, the cells, at all\nlevels, store only a pointer to the first of their own particles,\nand the total number of particles.\nThe current approach, illustrated in {[1]}{CellParts} is not\nonly more compact, it also allows a direct and more cache-efficient access\nto the list of particles for any inner node of the tree.\nThe cost of sorting the particles, with a recursive\npartitioning similar to QuickSort \\citep{ref:Hoare1962},\nis in {[1]}{N\\log N}.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=CellParts.eps,width=0.9\\textwidth}}\n    \\caption{Hierarchical ordering of the particle data structures\n    (right) according to their cell (left).\n    Each cell has a pointer to the first of its particles (same color\n    as cells) in the same global parts array.}\n    \\label{fig:CellParts}\n\\end{figure}\n\nThe task-based implementation consists of three\ntypes of tasks:\n\\begin{itemize}\n    \\item {\\em Self}-interactions, in which all particles\n        in a single cell interact with all other particles in the\n        same cell,\n    \\item {\\em Particle-particle} pair interactions, in which\n        all particles in a cell interact with all\n        particles in another cell,\n    \\item {\\em Particle-cell} pair interactions, in which\n        all particles in one cell are interacted with the\n        center of mass of all other cells in the tree.\n\\end{itemize}\n\nThese tasks can be created recursively over the cell hierarchy\nas shown in the function {\\tt make\\_tasks} in {[1]}{MakeTasks}.\nThe function is called on the root cell with the root cell\nand {\\tt NULL} as its two cell parameters.\nThe function recurses as follows (line numbers refer to {[1]}{MakeTasks}:\n\\begin{itemize}\n    \\item If called with a single, split cell (lines~6--7),\n        recurse over all the cell's sub-cells, and all\n        pairs of the cell's sub-cells (lines~8--11),\n    \\item If called with a single unsplit cell (line~13),\n        create a self-interaction task (line~14) as well as a particle-cell\n        task on that cell (line~18),\n    \\item If called with two neighbouring cells and both cells\n        are split (line~22),\n        recurse over all pairs of sub-cells spanning\n        both cells (lines~24--26), and\n    \\item If called with two neighbouring cells\n        and at least one of the cells is not split, create\n        a particle-particle pair task over both cells (line~29),\n    \\item If called with two non-neighbouring cells,\n        do nothing, as these interactions\n        will be computed by the particle-cell task.\n\\end{itemize}\n\\noindent Every interaction task additionally locks\nthe cells on which it operates (lines~17, 20, and 32--33).\nIn order to prevent generating\na large number of very small tasks, the task generation only recurses\nif the cells contain more than a minimum number $n_\\mathsf{task}$\nof particles each (lines~7 and~23).\n\nAs shown in {[1]}{BHTasks}, the particle-self and particle-particle pair\ninteraction tasks are implemented\nby computing the interactions between all particle pairs spanning\nboth cells in a double for-loop (lines~9--11 and~22-24 therein).\nSome extra logic (lines~2--7 and~15--20) is added to deal with\nsplit cells that did not contain enough particles to warrant the\ngeneration of finer tasks.\nThe particle-cell interactions for each leaf node are computed by\ntraversing the tree recursively starting from the root node and:\n\\begin{itemize}\n  \\item If called with a node that is a hierarchical parent of\n    the leaf node, or with a node that is a direct neighbour of\n    a hierarchical parent of the leaf node, recurse over the\n    node's sub-cells (lines~29--32),\n  \\item Otherwise, compute the interaction between the leaf node's\n    center of mass and all the particles in the leaf node (lines~33--35).\n\\end{itemize}\n\nThis task decomposition differs from the traditional tree-walk\nin the Barnes-Hut algorithm in that the particle-cell interactions\nare grouped per leaf, with each leaf doing its own tree walk,\nas opposed to doing a tree walk for each individual particle.\nThis approach was chosen to maximize the memory locality\nof the particle-cell calculation, as the particles in the leaf,\nwhich are traversed for each particle-cell interaction, are\ncontiguous in memory, and are thus more likely to remain in the\nlowest-level cache during the entire tree-walk.\n\nThis Barnes-Hut tree-code was used to approximate the gravitational\nN-Body problem for 1\\,000\\,000 particles with uniformly random coordinates\nin $[0,1]^3$.\nThe parameters $n_\\mathsf{max}=100$ and $n_\\mathsf{task}=5000$\nwere used to generate the tasks.\nUsing the above scheme generated 97\\,553 tasks, of which\n512 self-interaction tasks, 5\\,068 particle-particle interaction\ntask, and 32\\,768 particle-cell interaction tasks.\nA total of 43\\,416 locks on 37\\,449 resources were generated.\nSetting up the tasks took, on average XXX\\,ms, i.e.~at most\nXXX\\% of the total computation time.\nStoring the tasks, resources, and dependencies required XXX\\,MB,\nwhich is only XX\\% of the XXX\\,MB required to store the particle\ndata.\n\nFor these tests, {\\tt pthread}s parallelism was used and resource\nre-owning was switched off.\nResource ownership was attributed by dividing the global\n{\\tt parts} array by the number of queues and assigning each cell's\n{\\tt res} to the fraction of the {\\tt parts} array to which\nthe first of its own {\\tt parts} belong.\nThe interactions were computed 10 times for each number of\ncores, and the average thereof taken for the scaling and\nefficiency results in {[1]}{BHResults}.\nThe timings are for {\\tt qsched\\_run}, including the cost of\n{\\tt qsched\\_start}, which does not run in parallel.\nSetting up the scheduler, tasks, and resources took, in all\ncases, an average of 51.3\\,ms.\n\nFor comparison, the same computations were run using the popular\nastrophysics simulation software Gadget-2 \\citep{ref:Springel2005},\nusing a traditional Barnes-Hut implementation based on octrees\nand distributed-memory parallelism based on domain decompositions\nand MPI \\citep{ref:Snir1998}.\nTo achieve the same accuracy, an opening angle of 0.5 was used.\nOn a single core, the task-based tree traversal is already 1.9$\\times$\nfaster than Gadget-2, due to the cache efficiency of the task-based\ncomputations, which, by design, maximize the amount of computation\nper memory access.\nAt 59 cores, where Gadget-2 performs best, the task-based tree traversal is\n2.51$\\times$ faster, and at the full 64 cores it is 4$\\times$ faster,\ndue to the better strong scaling of the task-based approach as opposed\nto the MPI-based parallelism in Gadget-2.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=BH_scaling.eps,width=\\textwidth}}\n    \\caption{Strong scaling and parallel efficiency of the Barnes-Hut tree-code\n        computed over 1\\,000\\,000 particles.\n        Solving the N-Body problem takes 323\\,ms, achieving 75\\% parallel\n        efficiency over all 64 cores.\n        For comparison, timings are shown for the same computation using\n        the popular astrophysics code Gadget-2.\n        The scaling for Gadget-2 (left) is shown relative to the performance of\n        QuickSched, whereas the parallel efficiency (right) is computed relative\n        to Gadget-2 on a single core.\n        }\n    \\label{fig:BHResults}\n\\end{figure}\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=tasks_bh_dynamic_64.eps,width=\\textwidth}}\n    \\caption{Task scheduling of the Barnes-Hut tree-code on 64 cores.\n      The red tasks correspond to particle self-interactions, the green\n      tasks to the particle-particle pair interactions, and the blue\n      tasks to the particle-cell interactions.}\n    \\label{fig:BHTasksPlot}\n\\end{figure}\n\nUnlike the QR decomposition, the results scale well only to\n32 cores, achieving 90\\% parallel efficiency, and then\nlevel off for increasing numbers of cores.\nThis, however, is not a problem of the task-based parallel\nalgorithm, or of QuickSched, but of the memory bandwidth\nof the underlying hardware.\n{[1]}{BHTimes} shows the accumulated cost of each task type and of \nQuickSched over the number of cores.\nAt 64 cores, the scheduler overheads account for only $\\sim 1$\\% of\nthe total computational cost, whereas,\nas of 32 cores, the cost of both pair types grow by up to\n40\\%.\nThis is due to the cache hierarchy of the AMD Opteron 6376 in which\npairs of cores share a comon 2\\,MB L2 cache.\nWhen using half the cores or less, each core has its L@ cache to\nitself, whereas beyond 32 cores they are shared, resulting in more\nfrequent cache misses.\nThis cen be seen when comparing the costs of the particle-particle\ninteraction and particle-cell interaction tasks: while the former grow by\nroughly 30\\%, the latter grow by only 10\\% as they do much more\ncomputation per memory access.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=BH_times.eps,width=0.8\\textwidth}}\n    \\caption{Accumulated cost of each task type and of the overheads\n        associated with {\\tt qsched\\_gettask}, summed over all cores.\n        As of 32 cores, the cost of both pair interaction task\n        types grow by up to 30\\%.\n        The cost of the particle-cell interactions, which entail significantly more\n        computation per memory access, grow only by at most 10\\%.\n        The scheduler overheads, i.e.~{\\tt qsched\\_gettask},\n        make up less than 1\\% of the total time.}\n    \\label{fig:BHTimes}\n\\end{figure}\n\n\n\\section{Discussion and Conclusions}\n\nThe task scheduler described in the previous sections, QuickSched,\ndiffers from existing task-based programming schemes\nin a number of ways.\nThe most obvious such difference is the addition of {\\em conflicts},\nmodeled using exclusive locks on hierarchical resources.\nThis extension to the standard dependencies-only model\nof task-based parallelism allows for more complex task relations,\nsuch as in the Barnes-Hut tree-code described earlier.\n\nAnother significant difference is that the tasks, their\ndependencies, and their conflicts must be described\n{\\em explicitly} before the parallel computation starts.\nThis as opposed to implicit dependencies generated\nby task spawning, e.g.~as in Cilk, or to extracting the\ndependencies from the task parameters, e.g.~in QUARK or OmpSs.\nExplicitly defining dependencies has the advantage that \nmore elaborate dependency structures can be generated.\nFurthermore, knowing the structure of the entire task\ngraph from the start of the computation provides valuable\ninformation when scheduling the tasks, e.g.~using the \ncritical path along the dependencies to compute the\ntask weight.\n\nFinally, as opposed to the most other task-based\nprogramming environments which rely on compiler extensions\nand/or code pre-processors, QuickSched operates as a regular\nC-language library, based on standard parallel functionality\nprovided by OpenMP and/or {\\tt pthreads}.\nThis ensures a maximum amount of portability on existing\nand future architectures.\nThe interfaces are also kept as simple\nas possible in order to reduce the burden on the programmer\nwhen implementing task-based codes.\n\nThe QuickSched library itself is remarkably simple, consisting of\nless than 3\\,000 lines of code, including comments.\nBoth examples, which are distributed with QuickSched,\nrequire less than 1\\,000 lines of code each.\nFor a more complex, large-scale example of a task-based computation\nbased on the same algorithms, we refer to \\cite{ref:Gonnet2014},\nfor which the scheduler was originally designed, and from\nwhich QuickSched was back-ported.\n\nIn both examples, QuickSched performs extremely well, even\non a large number of shared-memory cores.\nThis performance is due, on the one hand, to the\ndivision of labor between the scheduler and the queues,\nand on the other hand due to the simple yet efficient\nalgorithms for task selection and resource locking.\nThe task weighting based on the length of the critical\npath of the dependencies delivers, in the examples shown,\ngood parallel efficiency.\n\nThere are several possible improvements to QuickSched which\nhave not been addressed in this paper.\nThe most obvious of which are the following:\n\\begin{itemize}\n    \\item {\\em Priorities}: The current implementation of\n        QuickSched does not take the resource locks into\n        account when selecting tasks in the queues, e.g.~it\n        may be advantageous, in some settings, to avoid tasks\n        which are involved in too many potential conflicts\n        and would therefore restrict the maximum degree of\n        parallelism when scheduled.\n    \\item {\\em Work-stealing}: During work-stealing, the\n        queues are probed in a random order although\n        the total relative cost of the tasks in the queue,\n        as well as the length of their critical paths are\n        known,\n    \\item {\\em Costs}: The size of the resources used by\n        a task are currently not taken into account when\n        assigning it to the queues in {\\tt qsched\\_enqueue},\n        or when approximating the cost of a task.\n\\end{itemize}\n\nQuickSched is distributed under the GNU Lesser General Public Licence\nv\\,3.0 and is available for download via\n\\url{https://sourceforge.net/projects/quicksched/files/}.\n\n\n\n\\section*{Acknowledgments}\nThe authors would like to thank Tom Theuns and Richard Bowers of the\nInstitute for Computational Cosmology at Durham University for the\nhelpful discussions.\nThis work was supported by a Durham University Seedcorn Grant\nnumber 21.12.080130 from\nwhich the hardware used in the experiments was purchased.\n\n\n\n\n\\bibliography{quicksched}\n\n\n\n\\appendix\n\\section{User Interface}\n\nThis section describes the QuickSched interface functions and how they\nare called.\n\nAs mentioned previously, the {\\tt qsched} object is the main\ninterface to the task scheduler.\nAs such, it provides functionality for task and resource\ncreation, for assigning resources to tasks, either as locks\nor uses, and for assigning dependencies between tasks.\nThe tasks and resources themselves are opaque to the\nuser, and handles of the types {\\tt qsched\\_task\\_t}\nand {\\tt qsched\\_res\\_t} are used instead.\n\nThe main functions for setting up the scheduler are:\n\\begin{itemize}\n    \\item {\\tt void qsched\\_init(struct qsched *s, int nr\\_queues, int flags)} \\\\\n        Initializes a {\\tt qsched} object with the given number of queues.\n        The {\\tt flags} parameter can be set to any bitwise or combination\n        of {\\tt qsched\\_flag\\_none},\n        {\\tt qsched\\_flag\\_yield}, and {\\tt qsched\\_flag\\_pthread},\n        which are described further below.\n        This function must be called before any of the other\n        functions are used.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_free(struct qsched *s)} \\\\\n        Releases all the memory and other resources allocated by the\n        given {\\tt qsched} object.\n        After this function has been called, the {\\tt qsched} will\n        need to be re-initialized for reuse.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_reset(struct qsched *s)} \\\\\n        Clears the tasks and resources in the given {\\tt qsched},\n        but does not release the allocated memory or change\n        the number of queues.\n        \\vspace{1mm}\n    \\item {\\tt qsched\\_task\\_t qsched\\_addtask(struct qsched *s, int type, unsigned int flags, void *data, int data\\_size, int cost)} \\\\\n        Creates a new task within the given {\\tt qsched} and returns\n        its handle.\n        The {\\tt type} and {\\tt data} field are copied into the {\\tt qsched}\n        and passed to the execution function when the {\\tt qsched} is run.\n        The parameter flags is either {\\tt task\\_flag\\_none} or\n        {\\tt task\\_flag\\_virtual}.\n        Tasks marked as virtual do not have any action associated with them,\n        e.g. they are used only to group or otherwise dependencies, and\n        are not passed to the execution function in {\\tt qsched\\_run}.\n        \\vspace{1mm}\n    \\item {\\tt qsched\\_res\\_t qsched\\_addres(struct qsched *s, int owner, qsched\\_res\\_t parent)} \\\\\n        Creates a new resource within the given {\\tt qsched} and returns\n        its handle.\n        The owner field is the initial queue ID to which this resource\n        should be assigned, or {\\tt qsched\\_owner\\_none}.\n        The {\\tt parent} field is the handle of the hierarchical parent of\n        the new resource or {\\tt qsched\\_res\\_none} if the resource\n        has no hierarchical parent.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_addlock(struct qsched *s, qsched\\_task\\_t t, qsched\\_res\\_t res)} \\\\\n        Append the resource {\\tt res} to the task {\\tt t}'s list of\n        locks.\n        The task {\\tt t} will then conflict with any other task that\n        also locks the resource {\\tt res}, its hierarchical parents, or\n        any resource hierarchically below it.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_adduse(struct qsched *s, qsched\\_task\\_t t, qsched\\_res\\_t res)} \\\\\n        Similar to {\\tt qsched\\_addlock}, yet the resource is only used and\n        is not part of a conflict.\n        This information is used when assigning tasks to specific queues.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_addunlock(struct qsched *s, qsched\\_task\\_t ta, qsched\\_task\\_t tb)} \\\\\n        Appends the task {\\tt tb} to the list of tasks that the task {\\tt ta}\n        unlocks.\n        The task {\\tt tb} then depends on the task {\\tt ta}.\n        \\vspace{1mm}\n    \\item {\\tt void qsched\\_run(struct qsched *s, int nr\\_threads, qsched\\_funtype fun)} \\\\\n        Executes the tasks in the given {\\tt qsched} using {\\tt nr\\_threads}\n        threads via the execution function {\\tt fun}, as described in the\n        previous section.\n        Once a {\\tt qsched} has been initialized and filled with\n        tasks and resources, it can be run more than once.\n        \\vspace{1mm}\n\\end{itemize}\n\nThe library can be compiled to use OpenMP and/or the\n{\\tt pthreads} library.\nOpenMP is the default, but calling {\\tt qsched\\_init} with\neither the {\\tt qsched\\_flag\\_yield}\nor the {\\tt qsched\\_flag\\_pthread} switches to using {\\tt pthreads},\nif available, for the parallel loop.\n\nOpenMP has the advantage of being available for most compilers\nand also potentially providing some extra platform-specific\nscheduling features, e.g.~optimal thread location and/or affinity,\nand of integrating seamlessly with other parallel parts of the\nuser application.\nThe disadvantage of using OpenMP is that it does not provide\nany mechanism for yielding a thread if no tasks are available,\ni.e. the main loop in {\\tt qsched\\_gettask}, described in the\nprevious section, will spin until a task becomes available.\nThis may be a problem if other parts of the user application\nare running concurrently in the background outside of QuickSched.\nCalling {\\tt qsched\\_init} with the {\\tt qsched\\_flag\\_yield}\nforces the use of {\\tt pthreads} and uses conditional variables\nto wait for a new task to be enqueued if obtaining a task\nfrom any of the queues fails.\nThis relinquishes the waiting computational thread for other\nprocesses.\n\n\n\\section{Tiled QR Implementation}\n\n\\begin{figure}\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}[basicstyle=\\scriptsize\\tt]\nenum {tDGEQRF, tDLARFT, tDTSQRF, tDSSRFT};\nvoid make_tasks(struct qsched *s, int m, int n) {\n  int i, j, k, data[3];\n  qsched_task_t tid[m * n], tid_new;\n  qsched_res_t rid[m * n];\n  for (k = 0; k < m * n; k++) {\n    tid[k] = qsched_task_none;\n    rid[k] = qsched_addres(s, qsched_res_none);\n  }\n  for (k = 0; k < m && k < n; k++) {\n    /* DGEQRF task at (k,k). */\n    data[0] = k; data[1] = k; data[2] = k;\n    tid_new = qsched_addtask(s, tDGEQRF, qsched_flags_none, data,\n                             sizeof(int) * 3, 2);\n    qsched_addlock(s, tid_new, rid[k * m + k])\n    if (tid[k * m + k] != qsched_task_none)\n      qsched_addunlock(s, tid[k * m + k], tid_new);\n    tid[k * m + k] = tid_new;\n    for (j = k + 1; j < n; j++) {\n      /* DLARFT task at (k,j). */\n      data[0] = k; data[1] = j; data[2] = k;\n      tid_new = qsched_addtask(s, tDLARFT, qsched_flags_none, data,\n                               sizeof(int) * 3, 3);\n      qsched_addlock(s, tid_new, rid[j * m + k])\n      qsched_addunlock(s, tid[k * m + k], tid_new);\n      if (tid[j * m + k] != qsched_task_none)\n        qsched_addunlock(s, tid[j * m + k], tid_new);\n      tid[j * m + k] = tid_new;\n    }\n    for (i = k + 1; i < m; i++) {\n      /* DTSQRF task at (i,k). */\n      data[0] = i; data[1] = k; data[2] = k;\n      tid_new = qsched_addtask(s, tDTSQRF, qsched_flags_none, data,\n                               sizeof(int) * 3, 3);\n      qsched_addlock(s, tid_new, rid[k * m + i])\n      qsched_addlock(s, tid_new, rid[k * m + k])\n      qsched_addunlock(s, tid[k * m + k], tid_new);\n      if (tid[k * m + i] != qsched_task_none)\n        qsched_addunlock(s, tid[k * m + i], tid_new);\n      tid[k * m + i] = tid_new;\n      for (j = k + 1; j < n; j++) {\n        /* DSSRFT task at (i,j). */\n        data[0] = i; data[1] = j; data[2] = k;\n        tid_new = qsched_addtask(s, tDSSRFT, qsched_flags_none, data,\n                                 sizeof(int) * 3, 5);\n        qsched_addlock(s, tid_new, rid[j * m + i])\n        qsched_addunlock(s, tid[j * m + k], tid_new);\n        qsched_addunlock(s, tid[k * m + i], tid_new);\n        if (tid[j * m + i] != qsched_task_none)\n          qsched_addunlock(s, tid[j * m + i], tid_new);\n        tid[j * m + i] = tid_new;\n      }\n    }\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\caption{Example code to generate the tasks for the tiled\n    QR decomposition.}\n\\label{fig:CodeQR}\n\\end{figure}\n\nSetting up the dependencies and locks for a matrix of\n$m\\times n$ tiles is implemented as shown in {[1]}{CodeQR},\nwhere the $m\\times n$ matrix {\\tt tid} stores the handles\nof the last task at position $(i,j)$ and is initialized with\nempty tasks (line~7).\nSimilarly, {\\tt rid} stores the handles of the resources for each\ntile of the matrix, which are allocated in line~8.\n\nThe following loops mirror the task generation described in\nAlgorithm~2 of \\citep{ref:Buttari2009}.\nFor each level {\\tt k} (line~10), a DGEQRF task is created\nfor tile $(k,k)$ (lines~13--14).\nA lock is added for the newly created task on the\nresource associated with the $(k,k)$th tile (line~15).\nIf a task exists at that position at the previous level\n(line~16), a dependency is added between the old task and\nthe new (line~17), and the new task is stored in {\\tt tid}\n(line~18).\nThe remaining tasks are generated in the same way, with\ntheir respective locks and dependencies.\n\nThe execution function for these tasks simply calls the appropriate\nkernels on the matrix tiles given by the task data:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid exec_fun(int type, void *data) {\n  int *idata = (int *)data;\n  int i = idata[0], j = idata[1], k = idata[2];\n  switch (type) {\n    case tDGEQRF:\n      DGEQRF(A[i, j], ...);\n      break;\n    case tDLARFT:\n      DLARFT(A[i, j], A[i, i], ...);\n      break;\n    case tDTSQRF:\n      DTSQRF(A[i, j], A[j, j], ...);\n      break;\n    case tDSSRFT:\n      DSSRFT(A[i, j], A[i, k], A[k, j], ...);\n      break;\n    default:\n      error(\"Unknown task type.\");\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where {\\tt A} is the matrix over which the QR\ndecomposition is executed.\n\n\n\\section{Barnes-Hut N-body Solver Implementation}\n\nThe cells themselves are implemented using the following \ndata structure:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct cell {\n  double loc[3], h[3], com[3], mass;\n  int split, count;\n  struct part *parts;\n  struct cell *progeny[8];\n  qsched_res_t res;\n  qsched_task_t task_com;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where {\\tt loc} and {\\tt h} are the location\nand size of the cell, respectively.\nThe {\\tt com} and {\\tt mass} fields represent the cell's\ncenter of mass, which will be used in the particle-cell interactions.\nThe {\\tt res} filed is the hierarchical resource representing\nthe cell's particles, and it is the parent resource of the cell\nprogeny's {\\tt res}.\nSimilarly, the {\\tt task\\_com} is a task handle to\ncompute the center of mass of the cell's particles, and \nit depends on the {\\tt task\\_com} of all the progeny if\nthe cell is split.\n{\\tt parts} is a pointer to an array of {\\tt count} \nparticle structures, which contain all the particle\ndata of the form:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct part {\n  double x[3], a[3], mass;\n  int id;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent i.e.~the particle position, acceleration, mass,\nand ID, respectively.\n\n\\begin{figure}\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}[basicstyle=\\scriptsize\\tt]\nvoid comp_self(struct cell *c) {\n  if (c->split) {\n    for (int j = 0; j < 8; j++) {\n      comp_self(c->progeny[j]);\n      for (int k = j + 1; k < 8; k++)\n        comp_pair(c->progeny[j], c->progeny[k]);\n    }\n  } else {\n    for (int j = 0; j < c->count; j++)\n      for (int k = j + 1; k < c->count; k++)\n        interact c->parts[j] and c->parts[k].\n  }\n}\n\nvoid comp_pair(struct cell *ci, struct cell *cj) {\n  if (ci and cj are not neighbours)\n    return;\n  if (ci->split && cj->split) {\n    for (int j = 0; j < 8; j++)\n      for (int k = 0; k < 8; k++)\n        comp_pair(ci->progeny[j], cj->progeny[k]);\n  } else {\n    for (int j = 0; j < ci->count; j++)\n      for (int k = 0; k < cj->count; k++)\n        interact ci->parts[j] and cj->parts[k].\n  }\n}\n\nvoid comp_pair_cp(struct cell *leaf, struct cell *c) {\n  if (c is a parent of leaf ||\n      c is a neighbour of a parent of leaf) {\n    for (int k = 0; k < 8; k++)\n     comp_pair_cp(leaf, c->progeny[k]);\n  } else if (leaf and c are not direct neighbours) {\n    for (int k = 0; k < leaf->count; k++)\n      interact leaf->parts[k] and c center of mass.\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n    \\caption{Task functions for the Barnes-Hut tree-code.}\n    \\label{fig:BHTasks}\n\\end{figure}\n\n\\begin{figure}\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}[basicstyle=\\scriptsize\\tt]\nenum { tSELF, tPAIR_PP, tPAIR_PC };\nvoid make_tasks(struct qsched *s, struct cell *ci, struct cell *cj) {\n  int j, k;\n  qsched_task_t tid;\n  struct cell *data[2];\n  if (cj == NULL) {\n    if (ci->split && ci->count > n_task) {\n      for (j = 0; j < 8; j++) {\n        make_tasks(s, ci->progeny[j], NULL);\n        for (k = j + 1; k < 8; k++)\n          make_tasks(s, ci->progeny[j], ci->progeny[k]);\n      }\n    } else {\n      tid = qsched_addtask(s, tSELF, qsched_flags_none, &ci,\n                           sizeof(struct cell *),\n                           ci->count * ci->count);\n      qsched_addlock(s, tid, ci->res);\n      tid = qsched_addtask(s, tPAIR_PC, qsched_flags_none, &ci,\n                           sizeof(struct cell *), ci->count);\n      qsched_addlock(s, tid, ci->res);\n    }\n  } else if (ci->split && cj->split &&\n             ci->count * cj->count > n_task * n_task) {\n    for (j = 0; j < 8; j++)\n      for (k = 0; k < 8; k++)\n        make_tasks(s, ci->progeny[j], cj->progeny[k]);\n  } else {\n    data[0] = ci; data[1] = cj;\n    tid = qsched_addtask(s, tPAIR_PP, qsched_flags_none, data,\n                         sizeof(struct cell *) * 2,\n                         ci->count * cj->count);\n    qsched_addlock(s, tid, ci->res);\n    qsched_addlock(s, tid, cj->res);\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n    \\caption{C-like pseudo-code for recursive task creation\n        for the Barnes-Hut tree-code.}\n    \\label{fig:MakeTasks}\n\\end{figure}\n\nThe functions for the task themselves are relatively\nstraight-forward and shown in {[1]}{BHTasks}, and the\nexecution function can be written as:\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nvoid exec_fun(int type, void *data) {\n  struct cell **cells = (struct cell **)data;\n  switch (type) {\n    case tSELF:\n      comp_self(cells[0]);\n      break;\n    case tPAIR_PP:\n      comp_pair(cells[0], cells[1]);\n      break;\n    case tPAIR_PC:\n      comp_pair_pc(cells[0], root);\n      break;\n    case tCOM:\n      comp_com(cells[0]);\n      break;\n    default:\n      error(\"Unknown task type.\");\n  }\n}\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\n\n\n", "itemtype": "equation", "pos": 20088, "prevtext": "\n\n\\flushbottom\n\\maketitle\n\\thispagestyle{empty}\n\n\n\\section{Introduction}\n\nTask-based parallelism is a conceptually simple paradigm for\nshared-memory parallelism in which a computation is broken-down\ninto a set of inter-dependent tasks which are executed\nconcurrently.\nTask dependencies are used to model the flow of data between\ntasks, e.g.~if task $B$ requires some data generated by task $A$,\nthen task $B$ {\\em depends} on task $A$ and cannot be executed\nbefore task $A$ has completed.\nThe tasks and their dependencies can be seen as the nodes and edges,\nrespectively, of a Directed Acyclic Graph (DAG) which can be\ntraversed in topological order, executing the tasks at the nodes\non the way down.\n\nOnce modelled in such a way, the computation is somewhat trivial\nto parallelize:\ngiven a set of inter-dependent tasks and a set of computational\nthreads, each thread repeatedly selects a task with no\nunsatisfied dependencies from the DAG and executes it.\nIf no tasks are available, the thread waits until any other\nthread finishes executing a task, thus potentially releasing\nnew tasks, or until all tasks in the DAG have been executed.\nNote that although the parallel execution\nitself is trivial, it does not always guaranteed to be efficient.\nSeveral factors may limit the maximum degree of parallelism, e.g.~the\nstructure of the task dependency DAG itself, or the order in which\navailable tasks are executed.\n\n{[1]}{Tasks} shows such a DAG for a set of tasks with\narrows indicating the direction of the dependencies, i.e.~an\narrow from task $A$ to task $B$ indicates that task $B$ depends\non task $A$.\nIn a parallel setting, tasks $A$, $G$, and $J$ can be\nexecuted concurrently.\nOnce task $G$ has completed, tasks $F$ and $H$ become available\nand can be executed by any other computational thread.\n\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=Tasks.eps,width=0.5\\textwidth}}\n    \\caption{A set of tasks (circles) and their dependencies (arrows).\n        The arrows indicate the direction of the dependency, i.e.~an\n        arrow from task $A$ to task $B$ indicates that task $B$ depends\n        on task $A$.\n        Tasks $A$, $G$, and $J$ have no unsatisfied dependencies and\n        can therefore be executed.\n        Once task $G$ has completed, tasks $F$ and $H$ become available,\n        and task $E$ only becomes available once both tasks $D$ and $F$\n        have completed.}\n    \\label{fig:Tasks}\n\\end{figure}\n\nOne of the first implementations of a task-based parallel programming\nsystems is Cilk \\citep{ref:Blumofe1995}, an extension to the C\nprogramming language which allows function calls to be ``spawned''\nas new tasks.\nDependencies are enforced by the {\\tt sync} keyword, which\nforces a thread to wait for all the tasks that it spawned\nto complete.\n\nIn SMP superscalar \\citep{ref:Perez2008}, StarPU \\citep{ref:Augonnet2011},\nQUARK \\citep{ref:Yarkhan2011}, and KAAPI \\citep{ref:Gautier2007}\nthe programmer specifies\nwhat shared data each task will access, and how that data will\nbe accessed, e.g.~read, write, or read-write access.\nThe dependencies between tasks are then generated\nautomatically by the runtime system, assuming that the\ndata must be accessed and updated in the order in which\nthe tasks are generated.\nStarPU also provides an interface for specifying additional\ndependencies explicitly.\nIntel's Threading Building Blocks (TBB)\n\\citep{ref:Reinders2010}\nprovide task-based parallelism using C++ templates in which\ndependencies are handled either by explicitly waiting\nfor spawned tasks, or by explicitly manipulating\ntask reference counters.\n\nFinally, the very popular OpenMP standard provides some basic support\nfor spawning tasks, similar to Cilk, as of version 3.0\n\\citep{ref:OpenMP2008}.\nOmpSs \\citep{ref:Duran2011} extends this scheme with automatic\ndependency generation as in SMP superscalar, of which it\nis a direct descendant, along with\nthe ability to explicitly wait on certain tasks.\n\nIn all of these systems, the tasks are only aware of a single\ntype of relationship between each other, i.e. dependencies, which\nspecify a strict ordering between two tasks.\nIn many cases, however, the task ordering need not necessarily\nbe this strict.\nConsider the case of two tasks that update some shared resource\nin an order-independent way, e.g. when accumulating a result in\na shared variable, or exclusively writing to an output file.\nIn order to avoid concurrent access to that resource, it is\nimperative that the execution of both tasks does not overlap,\nyet the order in which the tasks are executed is irrelevant.\nIn the following, such a relationship will be referred to\nas a {\\em conflict} between two tasks.\n{[1]}{TaskConflicts} shows a task graph extended by conflicting tasks\njoined by thick dashed lines.\nNone of tasks $F$, $H$, and $I$ can be executed concurrently,\ni.e. they must be serialized, yet in no particular order.\n\nIn dependency-only systems, such conflicts can be modelled\nwith dependencies, which enforce a pre-determined arbitrary\nordering on conflicting tasks.\nThis artificial restriction on the order\nin which tasks can be scheduled can, however, severely limit the\nparallelizability of a computation, especially in the presence\nof multiple conflicts per task.\nBoth \\cite{ref:Ltaief2012} and \\cite{ref:Agullo2013} note\nthis problem in their respective implementations of the Fast Multipole\nMethod (FMM), in which forces computed in different tasks are\naccumulated on a set of particles.\n\nSeveral libraries provide some mechanism to model such\nconflicts, either directly or indirectly.\nIn the QUARK scheduler, conflicts can be modeled by explicitly\nmarking dependencies as concurrent.\nKAAPI and OmpSS, on the other hand, allow marking access to\ncertain variables as reductions, yet only for basic operations,\ne.g.~summation or maximum/minimum.\n\nThis paper presents QuickSched, a framework for task-based\nparallel programming with constraints, which aims to achieve\nthe following goals:\n\\begin{itemize}\n    \\item {\\em Correctness}: All constraints, i.e.~dependencies and\n        conflicts, must be correctly enforced,\n    \\item {\\em Speed}: The overheads associated with task management\n        should be as small as possible,\n    \\item {\\em Memory/cache efficiency}: Tasks accessing similar\n        sets of data should be preferentially executed on the\n        same core to preserve memory/cache locality as far as possible, and\n    \\item {\\em Parallel efficiency}: The order in which the tasks\n        are executed should be chosen such\n        that sufficient work is available for all computational\n        threads at all times.\n\\end{itemize}\n\\noindent \nSection~2 describes the main design considerations, Section~3 the\nunderlying algorithms and data structures, and\nSection~4 their specific implementation in QuickSched.\nSection~5 presents two test-cases:\n\\begin{enumerate}\n    \\item The tiled QR\n    decomposition described in \\cite{ref:Buttari2009} and for\n    which the QUARK scheduler was originally developed, and\n    \\item A task-based Barnes-Hut tree-code to compute the\n    gravitational N-body problem similar to the FMM codes\n    of \\cite{ref:Ltaief2012} and \\cite{ref:Agullo2013},\n\\end{enumerate}\nThese real-world examples show how QuickSched can be used in practice,\nand can be used to assess its efficiency.\nSection~6 concludes with some general observations and future work\ndirections.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=TaskConflicts.eps,width=0.5\\textwidth}}\n    \\caption{Task graph with conflicts (thick dashed lines).\n        If two or more tasks are joined by a conflict, they cannot be\n        executed concurrently, i.e. tasks $B$ and $D$ cannot be run an\n        the same time.\n        Tasks belonging to different conflicting sets, e.g. tasks $B$\n        and $F$, or tasks $F$ and $J$, however, can be executed\n        concurrently.}\n    \\label{fig:TaskConflicts}\n\\end{figure}\n\n\n\\section{Design Considerations}\n\nFrom a programmer's perspective, there are two main paradigms for generating\ntask dependencies:\n\\begin{itemize}\n  \\item Implicitly via spawning and waiting, e.g.~as is done in Cilk\n    and OpenMP~3.0, or\n  \\item Automatic extraction from data dependencies, e.g.~as is done in\n    StarPU, QUARK, and OmpSs.\n\\end{itemize}\n\nThe first scheme, spawning and waiting, is arguably the simplest to\nuse.\nFor simple depedency structures in which each task depends on only a\nsingle task, i.e.~if the task DAG is a tree, each task {\\em spawns}, or\ncreates, its dependent tasks after its completion (see {[1]}{Spawn}a).\nHence for the tasks $A$--$E$ in {[1]}{Tasks}, task $A$ would spawn\ntasks $B$ and $D$, task $B$ would spawn task $C$, and task $D$ would\nspawn task $E$.\nIf a task has more than one dependency, e.g. tasks $D$--$F$ in {[1]}{Tasks},\nthen the task generation order is reversed: Task $E$ is executed first,\nand first spawns tasks $D$ and $F$, and waits for both their completion\nbefore doing its own computations (see {[1]}{Spawn}b).\n\nAlthough simple to use, this implicit dependency management\nlimits the types of DAGs that can be represented, e.g.~for\nall the tasks in {[1]}{Tasks}, using such a spawning and waiting model\nwould create implicit dependencies between the lowest-level\ntasks $C$, $E$, and $K$.\nThe main thread would spawn tasks $A$, $G$ and $J$, $A$ spawns $B$ and $D$,\n$G$ spawns $F$, $H$, and then $I$, $B$ spawns $C$.\nThe main thread then has to wait for $A$, $G$, and $J$,\nand thus implicitly all their spawned tasks, before executing\n$E$ and $K$.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=Spawn.eps,width=0.9\\textwidth}}\n    \\caption{Two different task graphs and how they can be implemented\n      using spawning and waiting.\n      For the task graph on the left, each task spawns its dependent\n      task or tasks. For the task graph on the right, in which task $C$\n      has multiple dependencies, tasks $A$ and $B$ must be spawned and waited\n      for by the calling thread before task $C$ can be spawned.}\n    \\label{fig:Spawn}\n\\end{figure}\n\nAutomatic dependency extraction, on the other hand,\nworks by enforcing dependencies\nbetween tasks that access the same data.\nThese data dependencies are provided explicitly by the programmer, e.g.~by\ndescribing which parameters to a task are input, output, and input/output.\nThe dependencies are enforced in the order in which the tasks are \ncreated.\nThis approach usually relies on compiler extensions, e.g.~{\\tt pragma}s\nin C/C++, or a system of function call wrappers, to describe the task parameters\nand their intent.\n\nThis approach allows programmers to specify rich dependency hierarchies\nwith very little effort, i.e.~without having to explicitly think about\ndependencies at all, yet they still only allow for one type of relationship,\ni.e.~dependencies, and lack the ability to deal with conflicts as\ndescribed in the previous section.\nThey may also not be able to understand more complex memory access patterns,\ne.g.~for two tasks modifying the upper and lower triangular parts of a matrix,\nwhich access the same block of memory, but never actually generate any\nconcurrency issues.\n\n\nHere, QuickSched takes a drastically different approach by requiring\nthe programmer to create the complete task graph and its dependencies\nexplicitly before execution.\nThis approach has two main advantages:\n\\begin{itemize}\n  \\item It gives the user maximum flexibility with regards to the\n    structure of the task graph generated,\n  \\item Knowing the complete structure of the task graph before execution\n    allows the task scheduler to make more informed decisions\n    as to how the tasks should be prioritized.\n\\end{itemize}\n\n\\noindent The obvious disadvantage is the burden of producing a correct\ntask graph is placed on the programmer.\nAlthough some problems such as cyclic dependencies can be detected\nautomatically, there is no automatic way to detect whether the\ndependencies actualy reflect the intent of the programmer.\nDue to this added complexity, we consider QuickSched to be\na tool not designed for casual parallel programmers, but for \nthose interested in investing a bit more programming effort to achieve\nbetter performance.\n\nAs opposed to the dependencies,\nconflicts between tasks or groups of tasks are not specified directly,\nbut are instead modeled as exclusive locks on a shared resource\nwhich have to be obtained by a task before it can execute.\nThus, in {[1]}{TaskConflicts}, before executing, task $F$ has\nto obtain an exclusive lock on the resource associated with\nthe conflict between tasks $F$, $H$, and $I$.\nWhile task $F$ is being executed, neither $H$ nor $I$ can \nlock the same resource, and therefore will not execute until\ntask $F$ is done and the lock has been released.\n\nAs with all other task-based libraries, the partitioning of the\ncomputation into tasks is also left entirely to the programmer.\nIn theory, any program can be automatically converted to a task-based\nrepresentation since each statement in the program code\ncan be considered a single task, with dependencies to the\nstatements/tasks that produce the input values.\nThis set of basic tasks could, again in theory, be reduced by merging\ntasks that share dependencies and/or resources using a variety\nof graph algorithms.\nThe decomposition of a computation into tasks, however, usually\ninvolves re-thinking the underlying algorithms such that they\nbest fit the task-based paradigm, e.g.~as in the examples in the\nfollowing sections, or as in \\cite{ref:Gonnet2014,ref:Buttari2009,ref:Ltaief2012}.\nThis process requires careful evaluation of the underlying\ncomputation, and is probably best\nnot left as an automatic transformation of an otherwise serial code.\n\nFinally, the task granularity is an important issue: if the task\ndecomposition is too coarse, then good parallelism\nand load-balancing will be difficult to achieve.\nConverseley, if the tasks are too small, the costs of selecting and\nscheduling tasks, which is usually constant per task, will\nquickly destroy any performance gains from parallelism.\nStarting from a per-statement set of tasks, it is therefore\nreasonable to group them by their dependencies and shared resources.\n\nIn the examples presented herein, we have chosen our task decomposition\nand granularity such that\n\\begin{itemize}\n  \\item Each task maximizes the ratio of computation to data required,\n  \\item The resources required for each task fit comfortably in the\n    lowest-level caches of the underlying system.\n\\end{itemize}\n\\noindent The first critera is biased towards bigger tasks, while the\nsecond limits their size.\nThe parameters controlling the size of the tasks in the examples, \ni.e.~the tile size in the QR decomposition and the limits $n_\\mathsf{max}$\nand $n_\\mathsf{task}$ were determined empirically and only optimized\nto the closest power of two or rough power of ten, respectively.\nFurther tuning these parameters could very likely lead to further\nperformance gains, but such an effort would go beyond the scope,\nand point, of this paper.\n\n\n\\section{Data Structures and Algorithms}\n\nThe QuickSched task scheduler consists of four main\nobjects types: {\\em task}, {\\em resource}, {\\em scheduler},\nand {\\em queue}.\nThe task and resource objects are used\nto model the computation, i.e. the work that is to be done\nand the data on which it will be done, respectively.\nThe scheduler and queue objects manage\nhow the work is done, i.e. which tasks get scheduled\nwhere and when, respectively.\n\n\\begin{figure}\n    \\centerline{\\epsfig{file=QSched.eps,width=0.8\\textwidth}}\n    \\caption{Schematic of the QuickSched task scheduler.\n        The tasks (circles) are stored in the scheduler (left).\n        Once a task's dependencies have been resolved, the task\n        is moved to one of the task queues.\n        Tasks that are not involved in any active conflicts\n        are then taken from the queues by the different\n        computational threads and executed.\n        After execution, their dependent tasks are unlocked\n        in the scheduler (dashed arrow).}\n    \\label{fig:QSched}\n\\end{figure}\n\nThe division of labor between the scheduler and\nthe queue objects is illustrated in {[1]}{QSched}.\nThe scheduler holds the tasks and is in charge\nof managing {\\em dependencies}.\nOnce a task has no unresolved dependencies, it is passed\non to a queue object.\nThe queue object, on the other hand, is in charge\nof managing {\\em conflicts}.\nComputational threads can query a queue and will\nreceive only tasks for which all conflicts have been\nresolved, i.e. for which all necessary resources could be \nexclusively locked.\n\nThere is also a division of responsibilities regarding\n{\\em efficiency} between the scheduler and the queue\nobjects.\nThe tasks in each queue are grouped according to the resources\nthey use, i.e. all the tasks in the same queue use a\nsimilar set of resources.\nThe underlying assumption is that each computational\nthread will preferentially access the same queue for tasks.\nIf the tasks in the queue share the same set of resources,\nit increases the probability of said resources already\nbeing present in the thread's cache, thus increasing\n{\\em cache efficiency}.\nThe scheduler is in charge of selecting the most appropriate\nqueue for each task, based on information stored in each task\non which resources are used.\nGiven a set of tasks with similar resources for which all\ndependencies are resolved, it is up to the queue to decide which\ntasks to prioritize.\n\nThe following subsections describe these four object types\nin detail, as well as their operations.\n\n\n\\subsection{Tasks}\n\\label{sec:tasks}\n\nA task consists of the following data structure, in C-like pseudo-code:\n\n\\begin{center}\\begin{minipage}{0.9\\textwidth}\n    \\begin{lstlisting}\nstruct task {\n  int type, wait;\n  void *data;\n  struct task **unlocks;\n  struct resource **locks, **uses;\n  int size_data, nr_unlocks, nr_locks, nr_uses;\n  int cost, weight;\n};\n    \\end{lstlisting}\n\\end{minipage}\\end{center}\n\\noindent where the {\\tt data}, {\\tt unlocks}, {\\tt locks},\nand {\\tt uses} arrays are pointers to the contents of other\narrays, i.e.~they are not allocated individually.\n\n{\\em What} the task does is determined by the {\\tt type}\nfield, e.g.~which can be mapped to any particular function,\nand the {\\tt data} pointer which points to an array of\n{\\tt size\\_data} bytes containing data specific to the task,\ne.g.~the parameters for a specific function call.\nBoth fields are application-specific and therefore not\nimportant for the scheduler itself.\n\nThe {\\tt unlocks} field points to the first element of\nan array of {\\tt nr\\_unlocks} pointers to other tasks.\nThese pointers represent the dependencies in reverse:\nif task $B$ depends on task $A$, then task $A$ {\\em unlocks}\ntask $B$.\nThe unlocks therefore follow the direction of the arrows\nin {[2]}{Tasks}{TaskConflicts}.\nConversely, {\\tt wait} is the number of unresolved dependencies\nassociated with this task, i.e.~the number of unexecuted tasks\nthat unlock this task.\nThe wait-counters can\nbe set by initializing all the task wait counters to zero and then\nincrementing the wait counter of each unlock-task of each task.\n\nThe {\\tt locks} field of each task points to the first element of\nan array of {\\tt nr\\_locks} pointers to {\\em resources}\nfor which exclusive locks must be obtained for the task\nto execute.\nEach locked resource represents a task conflict.\nSimilarly, {\\tt uses} points to the first element of\nan array of {\\tt nr\\_uses} pointers to resources which\nwill be used, but need not be locked.\n\nFinally, {\\tt cost} and {\\tt weight} are measures\nfor the relative computational cost of this task, and the\nrelative cost of the critical path following the\ndependencies of this task, respectively, i.e.~the task's cost\nplus the maximum dependent task's weight (see {[1]}{TaskWeight}).\nThe task cost can be either a rough estimate provided by the user,\nor the actual cost of the same task last time it was executed.\nThe task weights are computed by traversing\nthe tasks DAG in reverse topological order following their dependencies,\ne.g.~as per \\cite{ref:Kahn1962} in $\\mathcal O(n)$ for $n$ tasks,\nand computing each task's weight, e.g.\n\n", "index": 1, "text": "\\begin{equation*}\n  \\mbox{weight}_i = \\mbox{cost}_i + \\max_{j \\in \\mbox{\\small unlocks}_i}\\left\\{\\mbox{weight}_j\\right\\}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mbox{weight}_{i}=\\mbox{cost}_{i}+\\max_{j\\in\\mbox{\\small unlocks}_{i}}\\left\\{%&#10;\\mbox{weight}_{j}\\right\\}.\" display=\"block\"><mrow><mrow><msub><mtext>weight</mtext><mi>i</mi></msub><mo>=</mo><mrow><msub><mtext>cost</mtext><mi>i</mi></msub><mo>+</mo><mrow><munder><mi>max</mi><mrow><mi>j</mi><mo>\u2208</mo><msub><mtext mathsize=\"128%\">unlocks</mtext><mi>i</mi></msub></mrow></munder><mo>\u2061</mo><mrow><mo>{</mo><msub><mtext>weight</mtext><mi>j</mi></msub><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]