[{"file": "1601.00389.tex", "nexttext": "\nwhere ${\\mathcal{B}} \\in {\\mathbb{R}}^{p \\times k}, k \\ll p$. The random vectors $\\zeta \\in {\\mathbb{R}}^k, \\epsilon \\in {\\mathbb{R}}^p$ are independent of each other, and they are normally distributed as\\footnote{The mean vector does not play a significant role in our development, and therefore we consider zero-mean random variables throughout this paper.} $\\zeta \\sim \\mathcal{N}(0, \\Sigma_\\zeta), \\epsilon \\sim \\mathcal{N}(0,\\Sigma_\\epsilon)$, with $\\Sigma_\\zeta \\succ 0, \\Sigma_\\epsilon \\succ 0$ and $\\Sigma_\\epsilon$ being diagonal. Here the random vector $\\zeta$ represents a small number of unobserved, latent variables that impact all the observed variables $y$, and the matrix ${\\mathcal{B}}$ specifies the effect that the latent variables have on the observed variables. However, the latent variables $\\zeta$ themselves do not have any interpretable meaning, and they are essentially a mathematical abstraction employed to fit a concisely parameterized model to the conditional distribution of $y | \\zeta$ (which represents the remaining uncertainty in $y$ after accounting for the effects of the latent variables $\\zeta$) -- this conditional distribution is succinctly described as it is specified by a model consisting of independent variables (as the covariance of the Gaussian random vector $\\epsilon$ is diagonal).\n\nA natural approach to attributing semantic information to the latent variables $\\zeta$ in a factor model is to obtain measurements of some additional plausibly useful covariates $x \\in {\\mathbb{R}}^q$ (the choice of these variables is domain-specific), and to link these to the variables $\\zeta$.  However, defining and specifying such a link in a precise manner is challenging.  Indeed, a fundamental difficulty that arises in establishing this association is that the variables $\\zeta$ in the factor model \\eqref{eqn:factormodel} are not identifiable. In particular, for any non-singular matrix $\\mathcal{W} \\in {\\mathbb{R}}^{k \\times k}$, we have that ${\\mathcal{B}} \\zeta = ({\\mathcal{B}} \\mathcal{W}^{-1}) (\\mathcal{W} \\zeta)$. In this paper, we describe a systematic and computationally tractable methodology based on convex optimization that integrates factor analysis and the task of interpreting the latent variables. Our convex relaxation approach generalizes the \\emph{minimum-trace factor analysis} technique, which has received much attention in the mathematical programming community over the years \\cite{Ledermann,Shapiro1,S2,S3,Saunderson}.\n\n\n\\subsection{A Composite Factor Model}\n\\label{secion:composite}\nWe begin by making the observation that the column space of ${\\mathcal{B}}$ -- which specifies the $k$-dimensional component of $y$ that is influenced by the latent variables $\\zeta$ -- is invariant under transformations of the form ${\\mathcal{B}} \\rightarrow {\\mathcal{B}} \\mathcal{W}^{-1}$ for non-singular matrices $\\mathcal{W} \\in {\\mathbb{R}}^{k \\times k}$. Consequently, we approach the problem of associating the covariates $x$ to the latent variables $\\zeta$ by linking the effects of $x$ on $y$ to the column space of ${\\mathcal{B}}$. Conceptually, we seek a decomposition of the column space of ${\\mathcal{B}}$ into transverse subspaces ${\\mathfrak{H}}_x, {\\mathfrak{H}}_u \\subset {\\mathbb{R}}^p, ~ {\\mathfrak{H}}_x \\cap {\\mathfrak{H}}_u = \\{0\\}$ so that ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}) \\approx {\\mathfrak{H}}_x \\oplus {\\mathfrak{H}}_u$ -- the subspace ${\\mathfrak{H}}_x$ specifies those components of $y$ that are influenced by the latent variables $\\zeta$ and are also affected by the covariates $x$, and the subspace ${\\mathfrak{H}}_u$ represents any unobserved residual effects on $y$ due to $\\zeta$ that are not captured by $x$. To identify such a decomposition of the column space of ${\\mathcal{B}}$, our objective is to split the term ${\\mathcal{B}} \\zeta$ in the factor model~\\eqref{eqn:factormodel} as\n\n", "itemtype": "equation", "pos": 3299, "prevtext": "\n\\maketitle\n\\begin{abstract}\nLatent or unobserved phenomena pose a significant difficulty\nin data analysis as they induce complicated and confounding\ndependencies among a collection of observed variables.  Factor\nanalysis is a prominent multivariate statistical modeling approach\nthat addresses this challenge by identifying the effects of (a small\nnumber of) latent variables on a set of observed variables.  However,\nthe latent variables in a factor model are purely mathematical objects\nthat are derived from the observed phenomena, and they do not have any\ninterpretation associated to them.  A natural approach for attributing\nsemantic information to the latent variables in a factor model is to\nobtain measurements of some additional plausibly useful covariates\nthat may be related to the original set of observed variables, and to\nassociate these auxiliary covariates to the latent variables.  In this\npaper, we describe a systematic approach for identifying such\nassociations.  Our method is based on solving computationally\ntractable convex optimization problems, and it can be viewed as a\ngeneralization of the minimum-trace factor analysis procedure for\nfitting factor models via convex optimization.  We analyze the\ntheoretical consistency of our approach in a high-dimensional setting\nas well as its utility in practice via experimental demonstrations\nwith real data.\\end{abstract}\n\n\\section{Introduction} \\label{sec:intro}\n\n\nA central goal in data analysis is to identify concisely described models that characterize the statistical dependencies among a collection of variables. Such concisely parametrized models avoid problems associated with overfitting, and they are often useful in providing meaningful interpretations of the relationships inherent in the underlying variables. Latent or unobserved phenomena complicate the task of determining concisely parametrized models as they induce confounding dependencies among the observed variables that are not easily or succinctly described. Consequently, significant efforts over many decades have been directed towards the problem of accounting for the effects of latent phenomena in statistical modeling. A common shortcoming of approaches to latent-variable modeling is that the latent variables are typically mathematical constructs that are derived from the originally observed data, and these variables do not directly have semantic information linked to them. Discovering interpretable meaning underlying latent variables would clearly impact a range of contemporary problem domains throughout science and technology. For example, in data-driven approaches to scientific discovery, the association of semantics to latent variables would lead to the identification of new phenomena that are relevant to a scientific process, or would guide data-gathering exercises by providing choices of variables for which to obtain new measurements.\n\nIn this paper, we focus for the sake of concreteness on the challenge of interpreting the latent variables in a factor model \\cite{Spearman}. \\emph{Factor analysis} is perhaps the most widely used latent-variable modeling technique in practice. The objective with this method is to fit observations of a collection of random variables $y \\in {\\mathbb{R}}^p$ to the following linear model:\n\n", "index": 1, "text": "\\begin{equation}\ny = {\\mathcal{B}} \\zeta + \\epsilon,\n\\label{eqn:factormodel}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"y={\\mathcal{B}}\\zeta+\\epsilon,\" display=\"block\"><mrow><mrow><mi>y</mi><mo>=</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo>\u2062</mo><mi>\u03b6</mi></mrow><mo>+</mo><mi>\u03f5</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwhere the column space of ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ is the subspace ${\\mathfrak{H}}_x$ and the column space of ${\\mathcal{B}}_u \\in {\\mathbb{R}}^{p \\times \\dim({\\mathfrak{H}}_u)}$ is the subspace ${\\mathfrak{H}}_u$, i.e., $\\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{A}})) \\allowbreak + \\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}_u)) \\allowbreak = \\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}))$ and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{A}}) \\allowbreak \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}_u) \\allowbreak = \\{0\\}$. Since the number of latent variables $\\zeta$ in the factor model~\\eqref{eqn:factormodel} is typically much smaller than $p$, the dimension of the column space of ${\\mathcal{A}}$ is also much smaller than $p$; as a result, if the dimension $q$ of the additional covariates $x$ is large, the matrix ${\\mathcal{A}}$ has small rank. Hence, the matrix ${\\mathcal{A}}$ plays two important roles: its column space (in ${\\mathbb{R}}^p$) identifies those components of the subspace ${\\mathcal{B}}$ that are influenced by the covariates $x$, and its rowspace (in ${\\mathbb{R}}^q$) specifies those components of (a potentially large number of) the covariates $x$ that influence $y$. Thus, \\emph{the projection of the covariates $x$ onto the rowspace of ${\\mathcal{A}}$ represents the interpretable component of the latent variables $\\zeta$.} The term ${\\mathcal{B}}_u \\zeta_u$ in~\\eqref{eqn:mid} represents, in some sense, the effects of those phenomena that continue to remain unobserved despite the incorporation of the covariates $x$.\n\nMotivated by this discussion, we fit observations of $(y,x) \\in {\\mathbb{R}}^p \\times{\\mathbb{R}}^q$ to the following \\emph{composite factor model} that incorporates the effects of the covariates $x$ as well as of additional unobserved latent phenomena on $y$:\n\n", "itemtype": "equation", "pos": 7291, "prevtext": "\nwhere ${\\mathcal{B}} \\in {\\mathbb{R}}^{p \\times k}, k \\ll p$. The random vectors $\\zeta \\in {\\mathbb{R}}^k, \\epsilon \\in {\\mathbb{R}}^p$ are independent of each other, and they are normally distributed as\\footnote{The mean vector does not play a significant role in our development, and therefore we consider zero-mean random variables throughout this paper.} $\\zeta \\sim \\mathcal{N}(0, \\Sigma_\\zeta), \\epsilon \\sim \\mathcal{N}(0,\\Sigma_\\epsilon)$, with $\\Sigma_\\zeta \\succ 0, \\Sigma_\\epsilon \\succ 0$ and $\\Sigma_\\epsilon$ being diagonal. Here the random vector $\\zeta$ represents a small number of unobserved, latent variables that impact all the observed variables $y$, and the matrix ${\\mathcal{B}}$ specifies the effect that the latent variables have on the observed variables. However, the latent variables $\\zeta$ themselves do not have any interpretable meaning, and they are essentially a mathematical abstraction employed to fit a concisely parameterized model to the conditional distribution of $y | \\zeta$ (which represents the remaining uncertainty in $y$ after accounting for the effects of the latent variables $\\zeta$) -- this conditional distribution is succinctly described as it is specified by a model consisting of independent variables (as the covariance of the Gaussian random vector $\\epsilon$ is diagonal).\n\nA natural approach to attributing semantic information to the latent variables $\\zeta$ in a factor model is to obtain measurements of some additional plausibly useful covariates $x \\in {\\mathbb{R}}^q$ (the choice of these variables is domain-specific), and to link these to the variables $\\zeta$.  However, defining and specifying such a link in a precise manner is challenging.  Indeed, a fundamental difficulty that arises in establishing this association is that the variables $\\zeta$ in the factor model \\eqref{eqn:factormodel} are not identifiable. In particular, for any non-singular matrix $\\mathcal{W} \\in {\\mathbb{R}}^{k \\times k}$, we have that ${\\mathcal{B}} \\zeta = ({\\mathcal{B}} \\mathcal{W}^{-1}) (\\mathcal{W} \\zeta)$. In this paper, we describe a systematic and computationally tractable methodology based on convex optimization that integrates factor analysis and the task of interpreting the latent variables. Our convex relaxation approach generalizes the \\emph{minimum-trace factor analysis} technique, which has received much attention in the mathematical programming community over the years \\cite{Ledermann,Shapiro1,S2,S3,Saunderson}.\n\n\n\\subsection{A Composite Factor Model}\n\\label{secion:composite}\nWe begin by making the observation that the column space of ${\\mathcal{B}}$ -- which specifies the $k$-dimensional component of $y$ that is influenced by the latent variables $\\zeta$ -- is invariant under transformations of the form ${\\mathcal{B}} \\rightarrow {\\mathcal{B}} \\mathcal{W}^{-1}$ for non-singular matrices $\\mathcal{W} \\in {\\mathbb{R}}^{k \\times k}$. Consequently, we approach the problem of associating the covariates $x$ to the latent variables $\\zeta$ by linking the effects of $x$ on $y$ to the column space of ${\\mathcal{B}}$. Conceptually, we seek a decomposition of the column space of ${\\mathcal{B}}$ into transverse subspaces ${\\mathfrak{H}}_x, {\\mathfrak{H}}_u \\subset {\\mathbb{R}}^p, ~ {\\mathfrak{H}}_x \\cap {\\mathfrak{H}}_u = \\{0\\}$ so that ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}) \\approx {\\mathfrak{H}}_x \\oplus {\\mathfrak{H}}_u$ -- the subspace ${\\mathfrak{H}}_x$ specifies those components of $y$ that are influenced by the latent variables $\\zeta$ and are also affected by the covariates $x$, and the subspace ${\\mathfrak{H}}_u$ represents any unobserved residual effects on $y$ due to $\\zeta$ that are not captured by $x$. To identify such a decomposition of the column space of ${\\mathcal{B}}$, our objective is to split the term ${\\mathcal{B}} \\zeta$ in the factor model~\\eqref{eqn:factormodel} as\n\n", "index": 3, "text": "\\begin{equation}\n{\\mathcal{B}} \\zeta \\approx {\\mathcal{A}} x + {\\mathcal{B}}_u \\zeta_u,\n\\label{eqn:mid}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{B}}\\zeta\\approx{\\mathcal{A}}x+{\\mathcal{B}}_{u}\\zeta_{u},\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo>\u2062</mo><mi>\u03b6</mi></mrow><mo>\u2248</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>u</mi></msub><mo>\u2062</mo><msub><mi>\u03b6</mi><mi>u</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwhere ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ with $\\mathrm{rank}({\\mathcal{A}}) \\ll \\min\\{p,q\\}$, ${\\mathcal{B}}_u \\in {\\mathbb{R}}^{p \\times k_u}$ with $k_u \\ll p$, and the variables $\\zeta_u,\\bar{\\epsilon}$ are independent of each other (and of $x$) and normally distributed as $\\zeta_u \\sim \\mathcal{N}(0,\\Sigma_{\\zeta_u}), \\bar{\\epsilon} \\sim \\mathcal{N}(0,\\Sigma_{\\bar{\\epsilon}})$, with $\\Sigma_{\\zeta_u} \\succ 0, \\Sigma_{\\bar{\\epsilon}} \\succ 0$ and $\\Sigma_{\\bar{\\epsilon}}$ being a diagonal matrix. The matrix ${\\mathcal{A}}$ may also be viewed as the map specifying the best linear estimate of $y$ based on $x$. In other words, the goal is to identify a low-rank matrix ${\\mathcal{A}}$ such that the conditional distribution of $y | x$ (and equivalently of $y | {\\mathcal{A}} x$) is specified by a standard factor model of the form~\\eqref{eqn:composite}.\n\n\\subsection{Composite Factor Modeling via Convex Optimization}\n\\label{section:compositeF}\n\nNext we describe techniques to fit observations of $y \\in {\\mathbb{R}}^p$ and of $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$) to the models~\\eqref{eqn:factormodel} and~\\eqref{eqn:composite} respectively. These methods are key subroutines in our algorithmic approach for associating semantics to the latent variables in a factor model (see Section~\\ref{section:experiments} for a high-level discussion of our approach and Section~\\ref{section:experiments} for a more detailed experimental demonstration). Fitting observations of $y \\in {\\mathbb{R}}^p$ (or of $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$) to the factor model~\\eqref{eqn:factormodel} (or to \\eqref{eqn:composite}) is accomplished by identifying a Gaussian model over $y$ (or over $(y,x)$) with the covariance matrix of the model satisfying certain algebraic properties.  For background on multivariate Gaussian statistical models, we refer the reader to \\cite{Kay}).\n\n\n\nThe covariance matrix of $y$ in the factor model is decomposable as the sum of a low-rank matrix ${\\mathcal{B}} \\Sigma_\\zeta {\\mathcal{B}}'$ (corresponding to the $k \\ll p$ latent variables $\\zeta$) and a diagonal matrix $\\Sigma_\\epsilon$. Based on this algebraic structure, a natural approach to factor modeling is to find the smallest rank (positive semidefinite) matrix such that the difference between this matrix and the empirical covariance of the observations of $y$ is close to being a diagonal matrix (according to some measure of closeness, such as in the Frobenius norm). This problem is computationally intractable to solve in general due to the rank minimization objective \\cite{Nataranjan}. As a result, a common heuristic is to replace the matrix rank by the trace functional, which results in the minimum trace factor analysis problem \\cite{Ledermann,Shapiro1,S2,S3}; this problem is convex and it can be solved efficiently. The use of the trace of a positive semidefinite matrix as a surrogate for the matrix rank goes back many decades, and this topic has received much renewed interest over the past several years \\cite{Meshabi,Fazel,Recht,Candes}.\n\nIn attempting to generalize the minimum-trace factor analysis approach to the composite factor model, one encounters a difficulty that arises due to the parametrization of the underlying Gaussian model in terms of covariance matrices. Specifically, with the additional covariates $x \\in {\\mathbb{R}}^q$ in the composite model \\eqref{eqn:composite}, our objective is to identify a Gaussian model over $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ with the joint covariance $\\Sigma = \\begin{pmatrix} \\Sigma_y ~ \\Sigma_{yx} \\\\ \\Sigma'_{yx} ~ \\Sigma_x \\end{pmatrix} \\in \\mathbb{S}^{p + q}$ satisfying certain structural properties. One of these properties is that the conditional distribution of $y | x$ is specified by a factor model, which implies that the conditional covariance of $y | x$ must be decomposable as the sum of a low-rank matrix and a diagonal matrix. However, this conditional covariance is given by the Schur complement $\\Sigma_y - \\Sigma_{yx} \\Sigma_x^{-1} \\Sigma'_{yx}$, and specifying a constraint on the conditional covariance matrix in terms of the joint covariance matrix $\\Sigma$ presents an obstacle to obtaining computationally tractable optimization formulations.\n\nA more convenient approach to parameterizing conditional distributions in Gaussian models is to consider models specified in terms of inverse covariance matrices, which are also called \\emph{precision matrices}. Specifically, the algebraic properties that we desire in the joint covariance matrix $\\Sigma$ of $(y,x)$ in a composite factor model can also be stated in terms of the joint precision matrix $\\Theta = \\Sigma^{-1}$ via conditions on the submatrices of $\\Theta = \\begin{pmatrix} \\Theta_y ~ \\Theta_{yx} \\\\ \\Theta'_{yx} ~ \\Theta_x \\end{pmatrix}$. First, the precision matrix of the conditional distribution of $y | x$ is specified by the submatrix $\\Theta_y$; as the covariance matrix of the conditional distribution of $y | x$ is the sum of a diagonal matrix and a low-rank matrix, the Woodbury matrix identity implies that the submatrix $\\Theta_y$ is the difference of a diagonal matrix and a low-rank matrix. Second, the rank of the submatrix $\\Theta_{yx} \\in {\\mathbb{R}}^{p \\times q}$ is equal to the rank of ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ in non-degenerate models (i.e., if $\\Sigma \\succ 0$) because the relation between ${\\mathcal{A}}$ and $\\Theta$ is given by ${\\mathcal{A}} = -[\\Theta_y]^{-1} \\Theta_{yx}$. Based on this algebraic structure desired in $\\Theta$, we propose the following natural convex relaxation for fitting a collection of observations $\\{(y^{(i)},x^{(i)})\\} \\subset {\\mathbb{R}}^{p + q}$ to the composite model~\\eqref{eqn:composite}:\n\\begin{eqnarray}\n(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y) = \\arg\\min_{\\substack{\\Theta \\in {\\mathbb{S}}^{p+q}, ~\\Theta \\succ 0 \\\\ D_y,L_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n) + \\lambda_n [\\gamma\\|\\Theta_{yx}\\|_{\\star} + \\mathrm{trace}(L_y)] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_{y} = D_y - L_y, ~ L_y \\succeq 0, D_y ~\\mathrm{is~diagonal} &\n\\label{eqn:main}\n\\end{eqnarray}\nThe term $\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n)$ is the Gaussian log-likelihood function that enforces fidelity to the data, and it is given as follows (up to some additive and multiplicative terms):\n\n", "itemtype": "equation", "pos": 9312, "prevtext": "\nwhere the column space of ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ is the subspace ${\\mathfrak{H}}_x$ and the column space of ${\\mathcal{B}}_u \\in {\\mathbb{R}}^{p \\times \\dim({\\mathfrak{H}}_u)}$ is the subspace ${\\mathfrak{H}}_u$, i.e., $\\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{A}})) \\allowbreak + \\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}_u)) \\allowbreak = \\dim({\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}))$ and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{A}}) \\allowbreak \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}({\\mathcal{B}}_u) \\allowbreak = \\{0\\}$. Since the number of latent variables $\\zeta$ in the factor model~\\eqref{eqn:factormodel} is typically much smaller than $p$, the dimension of the column space of ${\\mathcal{A}}$ is also much smaller than $p$; as a result, if the dimension $q$ of the additional covariates $x$ is large, the matrix ${\\mathcal{A}}$ has small rank. Hence, the matrix ${\\mathcal{A}}$ plays two important roles: its column space (in ${\\mathbb{R}}^p$) identifies those components of the subspace ${\\mathcal{B}}$ that are influenced by the covariates $x$, and its rowspace (in ${\\mathbb{R}}^q$) specifies those components of (a potentially large number of) the covariates $x$ that influence $y$. Thus, \\emph{the projection of the covariates $x$ onto the rowspace of ${\\mathcal{A}}$ represents the interpretable component of the latent variables $\\zeta$.} The term ${\\mathcal{B}}_u \\zeta_u$ in~\\eqref{eqn:mid} represents, in some sense, the effects of those phenomena that continue to remain unobserved despite the incorporation of the covariates $x$.\n\nMotivated by this discussion, we fit observations of $(y,x) \\in {\\mathbb{R}}^p \\times{\\mathbb{R}}^q$ to the following \\emph{composite factor model} that incorporates the effects of the covariates $x$ as well as of additional unobserved latent phenomena on $y$:\n\n", "index": 5, "text": "\\begin{equation}\ny = {\\mathcal{A}} x + {\\mathcal{B}}_u \\zeta_u + \\bar{\\epsilon}\n\\label{eqn:composite}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"y={\\mathcal{A}}x+{\\mathcal{B}}_{u}\\zeta_{u}+\\bar{\\epsilon}\" display=\"block\"><mrow><mi>y</mi><mo>=</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>u</mi></msub><mo>\u2062</mo><msub><mi>\u03b6</mi><mi>u</mi></msub></mrow><mo>+</mo><mover accent=\"true\"><mi>\u03f5</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nThis function is concave as a function of the joint precision matrix\\footnote{An additional virtue of parametrizing our problem in terms of precision matrices rather than in terms of covariance matrices is that the log-likelihood function in Gaussian models is not concave over the cone of positive semidefinite matrices when viewed as a function of the covariance matrix.} $\\Theta$. The matrices $D_y, L_y$ represent the diagonal and low-rank components of $\\Theta_y$. As with the idea behind minimum-trace factor analysis, the role of the trace norm penalty on $L_y$ is to induce low-rank structure in this matrix. Based on a more recent line of work originating with the thesis of Fazel \\cite{Fazel,Recht,Candes}, the nuclear norm penalty $\\|\\Theta_{yx}\\|_\\star$ on the submatrix $\\Theta_{yx}$ (which is in general a non-square matrix) is useful for promoting low-rank structure in that submatrix of $\\Theta$. The parameter $\\gamma$ provides a tradeoff between the observed/interpretable and the unobserved parts of the composite factor model \\eqref{eqn:composite}, and the parameter $\\lambda_n$ provides a tradeoff between the fidelity of the model to the data and the overall complexity of the model (the total number of observed and unobserved components in the composite model \\eqref{eqn:composite}). In summary, for $\\lambda_n, \\gamma \\geq 0$ the regularized maximum-likelihood problem \\eqref{eqn:main} is a convex program. From the optimal solution $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ of \\eqref{eqn:main}, we can obtain estimates for the parameters of the composite factor model \\eqref{eqn:composite} as follows:\n\n", "itemtype": "equation", "pos": 15783, "prevtext": "\nwhere ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ with $\\mathrm{rank}({\\mathcal{A}}) \\ll \\min\\{p,q\\}$, ${\\mathcal{B}}_u \\in {\\mathbb{R}}^{p \\times k_u}$ with $k_u \\ll p$, and the variables $\\zeta_u,\\bar{\\epsilon}$ are independent of each other (and of $x$) and normally distributed as $\\zeta_u \\sim \\mathcal{N}(0,\\Sigma_{\\zeta_u}), \\bar{\\epsilon} \\sim \\mathcal{N}(0,\\Sigma_{\\bar{\\epsilon}})$, with $\\Sigma_{\\zeta_u} \\succ 0, \\Sigma_{\\bar{\\epsilon}} \\succ 0$ and $\\Sigma_{\\bar{\\epsilon}}$ being a diagonal matrix. The matrix ${\\mathcal{A}}$ may also be viewed as the map specifying the best linear estimate of $y$ based on $x$. In other words, the goal is to identify a low-rank matrix ${\\mathcal{A}}$ such that the conditional distribution of $y | x$ (and equivalently of $y | {\\mathcal{A}} x$) is specified by a standard factor model of the form~\\eqref{eqn:composite}.\n\n\\subsection{Composite Factor Modeling via Convex Optimization}\n\\label{section:compositeF}\n\nNext we describe techniques to fit observations of $y \\in {\\mathbb{R}}^p$ and of $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$) to the models~\\eqref{eqn:factormodel} and~\\eqref{eqn:composite} respectively. These methods are key subroutines in our algorithmic approach for associating semantics to the latent variables in a factor model (see Section~\\ref{section:experiments} for a high-level discussion of our approach and Section~\\ref{section:experiments} for a more detailed experimental demonstration). Fitting observations of $y \\in {\\mathbb{R}}^p$ (or of $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$) to the factor model~\\eqref{eqn:factormodel} (or to \\eqref{eqn:composite}) is accomplished by identifying a Gaussian model over $y$ (or over $(y,x)$) with the covariance matrix of the model satisfying certain algebraic properties.  For background on multivariate Gaussian statistical models, we refer the reader to \\cite{Kay}).\n\n\n\nThe covariance matrix of $y$ in the factor model is decomposable as the sum of a low-rank matrix ${\\mathcal{B}} \\Sigma_\\zeta {\\mathcal{B}}'$ (corresponding to the $k \\ll p$ latent variables $\\zeta$) and a diagonal matrix $\\Sigma_\\epsilon$. Based on this algebraic structure, a natural approach to factor modeling is to find the smallest rank (positive semidefinite) matrix such that the difference between this matrix and the empirical covariance of the observations of $y$ is close to being a diagonal matrix (according to some measure of closeness, such as in the Frobenius norm). This problem is computationally intractable to solve in general due to the rank minimization objective \\cite{Nataranjan}. As a result, a common heuristic is to replace the matrix rank by the trace functional, which results in the minimum trace factor analysis problem \\cite{Ledermann,Shapiro1,S2,S3}; this problem is convex and it can be solved efficiently. The use of the trace of a positive semidefinite matrix as a surrogate for the matrix rank goes back many decades, and this topic has received much renewed interest over the past several years \\cite{Meshabi,Fazel,Recht,Candes}.\n\nIn attempting to generalize the minimum-trace factor analysis approach to the composite factor model, one encounters a difficulty that arises due to the parametrization of the underlying Gaussian model in terms of covariance matrices. Specifically, with the additional covariates $x \\in {\\mathbb{R}}^q$ in the composite model \\eqref{eqn:composite}, our objective is to identify a Gaussian model over $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ with the joint covariance $\\Sigma = \\begin{pmatrix} \\Sigma_y ~ \\Sigma_{yx} \\\\ \\Sigma'_{yx} ~ \\Sigma_x \\end{pmatrix} \\in \\mathbb{S}^{p + q}$ satisfying certain structural properties. One of these properties is that the conditional distribution of $y | x$ is specified by a factor model, which implies that the conditional covariance of $y | x$ must be decomposable as the sum of a low-rank matrix and a diagonal matrix. However, this conditional covariance is given by the Schur complement $\\Sigma_y - \\Sigma_{yx} \\Sigma_x^{-1} \\Sigma'_{yx}$, and specifying a constraint on the conditional covariance matrix in terms of the joint covariance matrix $\\Sigma$ presents an obstacle to obtaining computationally tractable optimization formulations.\n\nA more convenient approach to parameterizing conditional distributions in Gaussian models is to consider models specified in terms of inverse covariance matrices, which are also called \\emph{precision matrices}. Specifically, the algebraic properties that we desire in the joint covariance matrix $\\Sigma$ of $(y,x)$ in a composite factor model can also be stated in terms of the joint precision matrix $\\Theta = \\Sigma^{-1}$ via conditions on the submatrices of $\\Theta = \\begin{pmatrix} \\Theta_y ~ \\Theta_{yx} \\\\ \\Theta'_{yx} ~ \\Theta_x \\end{pmatrix}$. First, the precision matrix of the conditional distribution of $y | x$ is specified by the submatrix $\\Theta_y$; as the covariance matrix of the conditional distribution of $y | x$ is the sum of a diagonal matrix and a low-rank matrix, the Woodbury matrix identity implies that the submatrix $\\Theta_y$ is the difference of a diagonal matrix and a low-rank matrix. Second, the rank of the submatrix $\\Theta_{yx} \\in {\\mathbb{R}}^{p \\times q}$ is equal to the rank of ${\\mathcal{A}} \\in {\\mathbb{R}}^{p \\times q}$ in non-degenerate models (i.e., if $\\Sigma \\succ 0$) because the relation between ${\\mathcal{A}}$ and $\\Theta$ is given by ${\\mathcal{A}} = -[\\Theta_y]^{-1} \\Theta_{yx}$. Based on this algebraic structure desired in $\\Theta$, we propose the following natural convex relaxation for fitting a collection of observations $\\{(y^{(i)},x^{(i)})\\} \\subset {\\mathbb{R}}^{p + q}$ to the composite model~\\eqref{eqn:composite}:\n\\begin{eqnarray}\n(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y) = \\arg\\min_{\\substack{\\Theta \\in {\\mathbb{S}}^{p+q}, ~\\Theta \\succ 0 \\\\ D_y,L_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n) + \\lambda_n [\\gamma\\|\\Theta_{yx}\\|_{\\star} + \\mathrm{trace}(L_y)] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_{y} = D_y - L_y, ~ L_y \\succeq 0, D_y ~\\mathrm{is~diagonal} &\n\\label{eqn:main}\n\\end{eqnarray}\nThe term $\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n)$ is the Gaussian log-likelihood function that enforces fidelity to the data, and it is given as follows (up to some additive and multiplicative terms):\n\n", "index": 7, "text": "\\begin{equation}\n\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n) = \\log\\det(\\Theta) - \\mathrm{trace}\\left[\\Theta \\cdot \\tfrac{1}{n}\\sum_{i=1}^n \\begin{pmatrix} y^{(i)} \\\\ x^{(i)} \\end{pmatrix} \\begin{pmatrix} y^{(i)} \\\\ x^{(i)} \\end{pmatrix}' \\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\ell(\\Theta;\\{y^{(i)},x^{(i)}\\}_{i=1}^{n})=\\log\\det(\\Theta)-\\mathrm{trace}%&#10;\\left[\\Theta\\cdot\\tfrac{1}{n}\\sum_{i=1}^{n}\\begin{pmatrix}y^{(i)}\\\\&#10;x^{(i)}\\end{pmatrix}\\begin{pmatrix}y^{(i)}\\\\&#10;x^{(i)}\\end{pmatrix}^{\\prime}\\right].\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>;</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>log</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">det</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>trace</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u22c5</mo><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mtd></mtr><mtr><mtd columnalign=\"center\"><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mtd></mtr></mtable><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mtd></mtr><mtr><mtd columnalign=\"center\"><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mtd></mtr></mtable><mo>)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwith the covariance of $\\zeta_u$ being the identity matrix of appropriate dimensions and the covariance of $\\bar{\\epsilon}$ being $\\hat{D}_y^{-1}$.\n\n\nOne can specialize the convex relaxation~\\eqref{eqn:main} for the composite factor model to obtain an approach for fitting observations $\\{y^{(i)}\\}_{i=1}^n$ to the factor model \\eqref{eqn:factormodel} without additional covariates:\n\\begin{eqnarray}\n(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y) = \\arg\\min_{\\substack{\\tilde{D}_y,\\tilde{L}_y \\in {\\mathbb{S}}^p \\\\ \\tilde{D}_y - \\tilde{L}_y \\succ 0}} & -\\ell(\\tilde{D}_y-\\tilde{L}_y; \\{y^{(i)}\\}_{i=1}^n) + \\tilde{\\lambda}_n \\mathrm{trace}(\\tilde{L}_y) \\nonumber \\\\ \\mathrm{s.t.} & \\tilde{L}_y \\succeq 0, \\tilde{D}_y ~\\mathrm{is~diagonal}. &\n\\label{eqn:main2}\n\\end{eqnarray}\nThe parameter $\\tilde{\\lambda}_n$ provides a tradeoff between fidelity of the model to the observations and the complexity of the model (i.e., the number of latent variables). In contrast to minimum-trace factor analysis -- in which the objective is to decompose a covariance matrix as the sum of a diagonal matrix and a low-rank matrix \\cite{Ledermann,Shapiro1,S2,S3}-- the regularized maximum-likelihood convex program \\eqref{eqn:main2} fits factor models by decomposing a precision matrix as the difference between a diagonal matrix and a low-rank matrix. As with the composite factor model, one can obtain estimates for the parameters of the factor model \\eqref{eqn:factormodel} based on the optimal solution $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$ of \\eqref{eqn:main2} as follows:\n\n", "itemtype": "equation", "pos": 17669, "prevtext": "\nThis function is concave as a function of the joint precision matrix\\footnote{An additional virtue of parametrizing our problem in terms of precision matrices rather than in terms of covariance matrices is that the log-likelihood function in Gaussian models is not concave over the cone of positive semidefinite matrices when viewed as a function of the covariance matrix.} $\\Theta$. The matrices $D_y, L_y$ represent the diagonal and low-rank components of $\\Theta_y$. As with the idea behind minimum-trace factor analysis, the role of the trace norm penalty on $L_y$ is to induce low-rank structure in this matrix. Based on a more recent line of work originating with the thesis of Fazel \\cite{Fazel,Recht,Candes}, the nuclear norm penalty $\\|\\Theta_{yx}\\|_\\star$ on the submatrix $\\Theta_{yx}$ (which is in general a non-square matrix) is useful for promoting low-rank structure in that submatrix of $\\Theta$. The parameter $\\gamma$ provides a tradeoff between the observed/interpretable and the unobserved parts of the composite factor model \\eqref{eqn:composite}, and the parameter $\\lambda_n$ provides a tradeoff between the fidelity of the model to the data and the overall complexity of the model (the total number of observed and unobserved components in the composite model \\eqref{eqn:composite}). In summary, for $\\lambda_n, \\gamma \\geq 0$ the regularized maximum-likelihood problem \\eqref{eqn:main} is a convex program. From the optimal solution $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ of \\eqref{eqn:main}, we can obtain estimates for the parameters of the composite factor model \\eqref{eqn:composite} as follows:\n\n", "index": 9, "text": "\\begin{equation}\n\\begin{aligned}\n\\hat{{\\mathcal{A}}} &= -[\\hat{\\Theta}_y]^{-1} \\hat{\\Theta}_{yx} \\\\ \\hat{{\\mathcal{B}}}_u &= \\mathrm{any~squareroot~of~}(\\hat{D}_y - \\hat{L}_y)^{-1} - \\hat{D}_y^{-1}~\\mathrm{such~that}~ \\hat{{\\mathcal{B}}}_u \\in {\\mathbb{R}}^{p \\times {\\mathrm{rank}}(\\hat{L}_y)},\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathcal{A}}}\" display=\"inline\"><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">^</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=-[\\hat{\\Theta}_{y}]^{-1}\\hat{\\Theta}_{yx}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msup><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub><mo stretchy=\"false\">]</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathcal{B}}}_{u}\" display=\"inline\"><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo stretchy=\"false\">^</mo></mover><mi>u</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathrm{any~{}squareroot~{}of~{}}(\\hat{D}_{y}-\\hat{L}_{y})^{-1}-%&#10;\\hat{D}_{y}^{-1}~{}\\mathrm{such~{}that}~{}\\hat{{\\mathcal{B}}}_{u}\\in{\\mathbb{R%&#10;}}^{p\\times{\\mathrm{rank}}(\\hat{L}_{y})},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>any</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>squareroot</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>of</mi></mpadded><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mi>L</mi><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>-</mo><mrow><mpadded width=\"+3.3pt\"><msubsup><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">^</mo></mover><mi>y</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>such</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>that</mi></mpadded><mo>\u2062</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo stretchy=\"false\">^</mo></mover><mi>u</mi></msub></mrow></mrow><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mrow><mi>p</mi><mo>\u00d7</mo><mi>rank</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>L</mi><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwith the covariance of $\\zeta$ being the identity matrix of appropriate dimensions and the covariance of $\\epsilon$ being $\\hat{\\tilde{D}}_y^{-1}$.\\\\\n\n{\\par} The convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} are log-determinant semidefinite programs that can be solved efficiently using existing numerical solvers such as the LogDetPPA package \\cite{Toh}.\n\n\\subsection{Our Results}\n\\label{section:results}\n\n\n\n\n\nOur discussion thus far has assumed that a factor model underlying a collection of variables $y \\in {\\mathbb{R}}^p$ is given, and the objective is to obtain semantic interpretation of the latent variables.  However, in many situations, a factor model underlying $y \\in {\\mathbb{R}}^p$ may not be available in advance, and it must be learned from observations of $y \\in {\\mathbb{R}}^p$.  We consider this latter more general setting for our theoretical development as well as our experimental demonstration.\n\nConceptually, a natural (meta-)procedure for learning a factor model and then interpreting the latent variables in this factor model is to proceed in two stages. In the first step, the analyst identifies a factor model \\eqref{eqn:factormodel} based solely on observations of the variables $y \\in {\\mathbb{R}}^p$ using the convex relaxation \\eqref{eqn:main2}, which results in an estimate for a factor model underlying the variables $y$. To attribute interpretable meaning to the latent variables in this factor model, the analyst then obtains simultaneous measurements of the variables $y$ as well as some additional covariates $x \\in {\\mathbb{R}}^q$ of plausibly relevant phenomena. Based on these joint observations, the second step consists of identifying a suitable composite factor model \\eqref{eqn:composite} via the convex program \\eqref{eqn:main}.  In particular, we sweep over the parameters $\\lambda_n,\\gamma$ in \\eqref{eqn:main} to identify composite models that achieve a suitable decomposition -- in terms of effects attributable to the additional covariates $x$ and of effects corresponding to remaining unobserved phenomena -- of the effects of the latent variables in the factor model obtained in the first stage.\n\nIn Section~\\ref{section:theorem} we carry out a theoretical analysis to investigate whether a two-stage framework as outlined above can possibly succeed. If one identifies a factor model for a collection of variables in the first step, are there conditions under which one can identify composite models satisfying the requirements of the second stage, which would then serve to provide interpretations of the latent variables? To address such questions, we discuss a model problem, which serves as the basis for the main theoretical result in Section~\\ref{section:theorem}. Suppose we have Gaussian random vectors $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ that are related to each other via a composite factor model~\\eqref{eqn:composite}.  Note that this composite factor model induces a factor model underlying the variables $y \\in {\\mathbb{R}}^p$ upon marginalization of the covariates $x$.  Now consider the following two questions:\n\\begin{enumerate}\n\\item Given observations jointly of $(y,x) \\in {\\mathbb{R}}^{p+q}$, does the convex relaxation \\eqref{eqn:main} (for suitable choices of regularization parameters $\\lambda_n,\\gamma$) estimate the composite factor model underlying these two random vectors accurately?\n\n\\item Given observations of \\emph{only} the variables $y$, does the convex relaxation~\\eqref{eqn:main2} (for an appropriate choice of regularization $\\tilde{\\lambda}_n$) estimate the factor model underlying $y$ accurately?\n\\end{enumerate}\n\nAn affirmative answer to both of these questions demonstrates the success of the two-stage procedure outlined in the preceding paragraph.  First, assume for simplicity that we are given the factor model underlying $y$ (obtained by marginalizing the covariates $x$).  In other words, we are supposing for now that we do not need to perform stage $1$ in the approach outlined above.  A positive answer to Question $(1)$ implies that we can decompose the effects of the latent variables in the factor model underlying $y$ using the convex relaxation \\eqref{eqn:main}, as the accurate estimation of the composite model underlying $(y,x)$ implies a successful decomposition of the effects of the latent variables in the factor model underlying $y$.  That is, stage $2$ in the algorithmic framework above is successful.  Next, a positive answer to Question $(2)$ implies that we can in fact estimate the factor model underlying $y$ from observations of the only the variables $y$ (i.e., we do not need some sort of oracle knowledge of this factor model in advance), and therefore stage $1$ of our method described above can be performed successfully.\n\nIn Section~\\ref{section:theorem}, we show that under suitable identifiability conditions on the population model of the combined random vector $(y,x)$, the convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} succeed in solving these two questions. Our analysis is carried out in a high-dimensional asymptotic scaling setup in which the dimensions $p,q$, the number of observations $n$, and other model parameters may all grow simultaneously \\cite{Buhlmann,Wainwright}.\n\n\n\n\n\n\nThe implementation of such a two-stage approach in practice requires the specification of several further details, especially for the second step. In Section~\\ref{section:experiments} we give a full algorithmic description of our methodology as well as a concrete demonstration with experiments on real-world financial data. Specifically, we consider as our variables $y$ the monthly averaged stock prices of $p = 66$ companies from the Standard and Poor index over the period June 1990 to July 2014, and we identify a factor model \\eqref{eqn:factormodel} over $y$ with $13$ latent variables influencing the stock return. We then obtain observations of $q = 11$ covariates on quantities related to oil trade, employment levels, etc. (see Section~\\ref{section:experiments} for the full list), as these plausibly influence the stock returns. By suitably employing the convex relaxation \\eqref{eqn:main} for composite factor modeling, we identify a two-dimensional projection of these $11$ covariates that represent an interpretable component of the $13$ latent variables in the factor model, as well as a remaining set of $11$ latent variables that constitute phenomena not observed via the covariates $x$. In further analyzing the characteristics of the two-dimensional projection, we find that EUR to USD exchange rate and inflation rate are the most relevant of the $11$ covariates considered in our experiment, while gold prices and oil exports are less useful. See Section~\\ref{section:experiments} for complete details.\n\n\\subsection{Related Work}\nElements of our approach bear some similarity with \\emph{canonical correlations analysis} \\cite{Hotelling}, which is a classical technique for identifying relationships between two sets of variables. In particular, for a pair of jointly Gaussian random vectors $(y,x) \\in {\\mathbb{R}}^{p \\times q}$, canonical correlations analysis may be used as a technique for identifying the most relevant component(s) of $x$ that influence $y$. However, the composite factor model \\eqref{eqn:composite} allows for the effect of further unobserved phenomena not captured via observations of the covariates $x$. Consequently, our approach in some sense incorporates elements of both canonical correlations analysis and factor analysis. It is important to note that algorithms for factor analysis and for canonical correlations analysis usually operate on covariance and cross-covariance matrices. However, we parametrize our regularized maximum-likelihood problem \\eqref{eqn:main2} in terms of precision matrices, which is a crucial ingredient in leading to a computationally tractable convex program.\n\nThe nuclear-norm heuristic has been employed widely over the past several years in a range of statistical modeling tasks involving rank minimization problems; see \\cite{Wainwright} and the references therein. The proof of our main result in Section~\\ref{section:theorem} incorporates some elements from the theoretical analyses in these previous papers, along with the introduction of some new ingredients. We give specific pointers to the relevant literature in Section~\\ref{section:proofs}. \n\n\n\n\\subsection{Notation}Given a matrix $U \\in \\mathbb{R}^{p_1 \\times p_2}$, the norm $\\|U\\|_{\\ell_\\infty}$ denotes the largest entry in magnitude of $U$, and the norm $\\|U\\|_2$ denotes the spectral norm (the largest singular value of $U$). We define the linear operators $\\mathcal{F}: {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p{\\times}q} \\times {\\mathbb{S}}^q \\rightarrow {\\mathbb{S}}^{(p+q)}$ and its adjoint $\\mathcal{F}^{\\dagger}: {\\mathbb{S}}^{(p+q)} \\rightarrow {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p{\\times}q} \\times {\\mathbb{S}}^q$ as follows:\n\n", "itemtype": "equation", "pos": 19548, "prevtext": "\nwith the covariance of $\\zeta_u$ being the identity matrix of appropriate dimensions and the covariance of $\\bar{\\epsilon}$ being $\\hat{D}_y^{-1}$.\n\n\nOne can specialize the convex relaxation~\\eqref{eqn:main} for the composite factor model to obtain an approach for fitting observations $\\{y^{(i)}\\}_{i=1}^n$ to the factor model \\eqref{eqn:factormodel} without additional covariates:\n\\begin{eqnarray}\n(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y) = \\arg\\min_{\\substack{\\tilde{D}_y,\\tilde{L}_y \\in {\\mathbb{S}}^p \\\\ \\tilde{D}_y - \\tilde{L}_y \\succ 0}} & -\\ell(\\tilde{D}_y-\\tilde{L}_y; \\{y^{(i)}\\}_{i=1}^n) + \\tilde{\\lambda}_n \\mathrm{trace}(\\tilde{L}_y) \\nonumber \\\\ \\mathrm{s.t.} & \\tilde{L}_y \\succeq 0, \\tilde{D}_y ~\\mathrm{is~diagonal}. &\n\\label{eqn:main2}\n\\end{eqnarray}\nThe parameter $\\tilde{\\lambda}_n$ provides a tradeoff between fidelity of the model to the observations and the complexity of the model (i.e., the number of latent variables). In contrast to minimum-trace factor analysis -- in which the objective is to decompose a covariance matrix as the sum of a diagonal matrix and a low-rank matrix \\cite{Ledermann,Shapiro1,S2,S3}-- the regularized maximum-likelihood convex program \\eqref{eqn:main2} fits factor models by decomposing a precision matrix as the difference between a diagonal matrix and a low-rank matrix. As with the composite factor model, one can obtain estimates for the parameters of the factor model \\eqref{eqn:factormodel} based on the optimal solution $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$ of \\eqref{eqn:main2} as follows:\n\n", "index": 11, "text": "\\begin{equation}\n\\hat{{\\mathcal{B}}} = \\mathrm{any~squareroot~of~}\\left(\\hat{\\tilde{D}}_y - \\hat{\\tilde{L}}_y\\right)^{-1} - \\hat{\\tilde{D}}_y^{-1}~\\mathrm{such~that}~ \\hat{{\\mathcal{B}}} \\in {\\mathbb{R}}^{p \\times {\\mathrm{rank}}(\\hat{\\tilde{L}}_y)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\hat{{\\mathcal{B}}}=\\mathrm{any~{}squareroot~{}of~{}}\\left(\\hat{\\tilde{D}}_{y}%&#10;-\\hat{\\tilde{L}}_{y}\\right)^{-1}-\\hat{\\tilde{D}}_{y}^{-1}~{}\\mathrm{such~{}%&#10;that}~{}\\hat{{\\mathcal{B}}}\\in{\\mathbb{R}}^{p\\times{\\mathrm{rank}}(\\hat{\\tilde%&#10;{L}}_{y})},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>any</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>squareroot</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>of</mi></mpadded><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msub><mover accent=\"true\"><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mover accent=\"true\"><mi>L</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>-</mo><mrow><mpadded width=\"+3.3pt\"><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mi>y</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>such</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>that</mi></mpadded><mo>\u2062</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mrow><mi>p</mi><mo>\u00d7</mo><mi>rank</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mover accent=\"true\"><mi>L</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mi>y</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nFinally, for any subspace ${\\mathfrak{H}}$, the projection onto the subspace is denoted by $\\mathcal{P}_{\\mathfrak{H}}$.\n\n\n\n\n\\section{Theoretical Results}\n\\label{section:theorem}\nIn this section, we state theorems that address Questions $(1)$ and $(2)$ from Section~\\ref{section:results}.  These theorems require assumptions on the population precision matrix, which are discussed in Section~\\ref{section:setup}, with the theorem statements given in Section~\\ref{section:theoremstatement}.  The proofs of these theorems are given in Section~\\ref{section:proofs} with some details deferred to the supplementary material.  We confirm the theoretical predictions of with numerical simulations on synthetic data in Section~\\ref{section:simulation}.\n\n\\subsection{Technical Setup}\n\\label{section:setup}\nAs discussed in Section~\\ref{section:results}, our theorems are premised on the existence of a population composite factor model \\eqref{eqn:composite} $y = {\\mathcal{A}}^\\star{x} + {\\mathcal{B}}^\\star_u \\zeta_u + \\epsilon$ underlying a pair of random vectors $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$, with ${\\mathrm{rank}}({\\mathcal{A}}^\\star) = k_x$, ${\\mathcal{B}}_u^\\star \\in {\\mathbb{R}}^{p \\times k_u}$, and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{A}}}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{B}}_u}) = \\{0\\}$. As the convex relaxations \\eqref{eqn:main} and \\eqref{eqn:main2} are solved in the precision matrix parametrization, the conditions for our theorems are more naturally stated in terms of the joint precision matrix $\\Theta^\\star \\in \\mathbb{S}^{p+q}, ~ \\Theta^\\star \\succ 0$ of $(y,x)$.  The algebraic aspects of the parameters underlying the factor model translate to algebraic properties of submatrices of $\\Theta^\\star$.  In particular, the submatrix $\\Theta^\\star_{yx}$ has rank equal to $k_x$, and the submatrix $\\Theta^\\star_y$ is decomposable as $D_y^\\star - L_y^\\star$ with $D^\\star_y$ being diagonal and $L^\\star_y \\succeq 0$ having rank equal to $k_u$.  Finally, the transversality of ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{A}}})$ and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{B}}_u})$ translates to the fact that ${\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\Theta^\\star_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(L_y^\\star) = \\{0\\}$ have a transverse intersection.\n\nTo address the requirements raised in Question $(1)$ in Section~\\ref{section:results}, we seek an estimate $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ from the convex relaxation \\eqref{eqn:main} such that $\\mathrm{rank}(\\hat{\\Theta}_{yx}) = \\mathrm{rank}(\\Theta^\\star_{yx}), \\mathrm{rank}(\\hat{L}_y) = \\mathrm{rank}(L^\\star_y),$ and that $\\|\\hat{\\Theta}-\\Theta^\\star\\|_2$ is small. To satisfy the requirements of Question $(2)$ in Section~\\ref{section:results}, we first need to consider the factor model underlying the random vector $y \\in {\\mathbb{R}}^p$ that is induced upon marginalization of $x$. In particular, the precision matrix of $y$ is given by $\\tilde{\\Theta}^\\star_y = D^\\star_y - L^\\star_y - \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$, and Question $(2)$ asks whether we can obtain an estimate $(\\hat{\\tilde{D}}_y,\\hat{\\tilde{L}}_y)$ such that $\\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$, and the errors $\\|(\\hat{\\tilde{D}}_y - D^\\star_y\\|_2,\\|\\hat{\\tilde{L}}_y) - [L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}]\\|_2$ are small.\n\nBuilding on both classical statistical estimation theory \\cite{Bickel} as well as the recent literature on high-dimensional statistical inference \\cite{Buhlmann,Wainwright}, we describe our conditions in terms of assumptions on the \\emph{Fisher information} at the population model parametrized by the precision matrix $\\Theta^\\star$:\n\n", "itemtype": "equation", "pos": 28785, "prevtext": "\nwith the covariance of $\\zeta$ being the identity matrix of appropriate dimensions and the covariance of $\\epsilon$ being $\\hat{\\tilde{D}}_y^{-1}$.\\\\\n\n{\\par} The convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} are log-determinant semidefinite programs that can be solved efficiently using existing numerical solvers such as the LogDetPPA package \\cite{Toh}.\n\n\\subsection{Our Results}\n\\label{section:results}\n\n\n\n\n\nOur discussion thus far has assumed that a factor model underlying a collection of variables $y \\in {\\mathbb{R}}^p$ is given, and the objective is to obtain semantic interpretation of the latent variables.  However, in many situations, a factor model underlying $y \\in {\\mathbb{R}}^p$ may not be available in advance, and it must be learned from observations of $y \\in {\\mathbb{R}}^p$.  We consider this latter more general setting for our theoretical development as well as our experimental demonstration.\n\nConceptually, a natural (meta-)procedure for learning a factor model and then interpreting the latent variables in this factor model is to proceed in two stages. In the first step, the analyst identifies a factor model \\eqref{eqn:factormodel} based solely on observations of the variables $y \\in {\\mathbb{R}}^p$ using the convex relaxation \\eqref{eqn:main2}, which results in an estimate for a factor model underlying the variables $y$. To attribute interpretable meaning to the latent variables in this factor model, the analyst then obtains simultaneous measurements of the variables $y$ as well as some additional covariates $x \\in {\\mathbb{R}}^q$ of plausibly relevant phenomena. Based on these joint observations, the second step consists of identifying a suitable composite factor model \\eqref{eqn:composite} via the convex program \\eqref{eqn:main}.  In particular, we sweep over the parameters $\\lambda_n,\\gamma$ in \\eqref{eqn:main} to identify composite models that achieve a suitable decomposition -- in terms of effects attributable to the additional covariates $x$ and of effects corresponding to remaining unobserved phenomena -- of the effects of the latent variables in the factor model obtained in the first stage.\n\nIn Section~\\ref{section:theorem} we carry out a theoretical analysis to investigate whether a two-stage framework as outlined above can possibly succeed. If one identifies a factor model for a collection of variables in the first step, are there conditions under which one can identify composite models satisfying the requirements of the second stage, which would then serve to provide interpretations of the latent variables? To address such questions, we discuss a model problem, which serves as the basis for the main theoretical result in Section~\\ref{section:theorem}. Suppose we have Gaussian random vectors $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ that are related to each other via a composite factor model~\\eqref{eqn:composite}.  Note that this composite factor model induces a factor model underlying the variables $y \\in {\\mathbb{R}}^p$ upon marginalization of the covariates $x$.  Now consider the following two questions:\n\\begin{enumerate}\n\\item Given observations jointly of $(y,x) \\in {\\mathbb{R}}^{p+q}$, does the convex relaxation \\eqref{eqn:main} (for suitable choices of regularization parameters $\\lambda_n,\\gamma$) estimate the composite factor model underlying these two random vectors accurately?\n\n\\item Given observations of \\emph{only} the variables $y$, does the convex relaxation~\\eqref{eqn:main2} (for an appropriate choice of regularization $\\tilde{\\lambda}_n$) estimate the factor model underlying $y$ accurately?\n\\end{enumerate}\n\nAn affirmative answer to both of these questions demonstrates the success of the two-stage procedure outlined in the preceding paragraph.  First, assume for simplicity that we are given the factor model underlying $y$ (obtained by marginalizing the covariates $x$).  In other words, we are supposing for now that we do not need to perform stage $1$ in the approach outlined above.  A positive answer to Question $(1)$ implies that we can decompose the effects of the latent variables in the factor model underlying $y$ using the convex relaxation \\eqref{eqn:main}, as the accurate estimation of the composite model underlying $(y,x)$ implies a successful decomposition of the effects of the latent variables in the factor model underlying $y$.  That is, stage $2$ in the algorithmic framework above is successful.  Next, a positive answer to Question $(2)$ implies that we can in fact estimate the factor model underlying $y$ from observations of the only the variables $y$ (i.e., we do not need some sort of oracle knowledge of this factor model in advance), and therefore stage $1$ of our method described above can be performed successfully.\n\nIn Section~\\ref{section:theorem}, we show that under suitable identifiability conditions on the population model of the combined random vector $(y,x)$, the convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} succeed in solving these two questions. Our analysis is carried out in a high-dimensional asymptotic scaling setup in which the dimensions $p,q$, the number of observations $n$, and other model parameters may all grow simultaneously \\cite{Buhlmann,Wainwright}.\n\n\n\n\n\n\nThe implementation of such a two-stage approach in practice requires the specification of several further details, especially for the second step. In Section~\\ref{section:experiments} we give a full algorithmic description of our methodology as well as a concrete demonstration with experiments on real-world financial data. Specifically, we consider as our variables $y$ the monthly averaged stock prices of $p = 66$ companies from the Standard and Poor index over the period June 1990 to July 2014, and we identify a factor model \\eqref{eqn:factormodel} over $y$ with $13$ latent variables influencing the stock return. We then obtain observations of $q = 11$ covariates on quantities related to oil trade, employment levels, etc. (see Section~\\ref{section:experiments} for the full list), as these plausibly influence the stock returns. By suitably employing the convex relaxation \\eqref{eqn:main} for composite factor modeling, we identify a two-dimensional projection of these $11$ covariates that represent an interpretable component of the $13$ latent variables in the factor model, as well as a remaining set of $11$ latent variables that constitute phenomena not observed via the covariates $x$. In further analyzing the characteristics of the two-dimensional projection, we find that EUR to USD exchange rate and inflation rate are the most relevant of the $11$ covariates considered in our experiment, while gold prices and oil exports are less useful. See Section~\\ref{section:experiments} for complete details.\n\n\\subsection{Related Work}\nElements of our approach bear some similarity with \\emph{canonical correlations analysis} \\cite{Hotelling}, which is a classical technique for identifying relationships between two sets of variables. In particular, for a pair of jointly Gaussian random vectors $(y,x) \\in {\\mathbb{R}}^{p \\times q}$, canonical correlations analysis may be used as a technique for identifying the most relevant component(s) of $x$ that influence $y$. However, the composite factor model \\eqref{eqn:composite} allows for the effect of further unobserved phenomena not captured via observations of the covariates $x$. Consequently, our approach in some sense incorporates elements of both canonical correlations analysis and factor analysis. It is important to note that algorithms for factor analysis and for canonical correlations analysis usually operate on covariance and cross-covariance matrices. However, we parametrize our regularized maximum-likelihood problem \\eqref{eqn:main2} in terms of precision matrices, which is a crucial ingredient in leading to a computationally tractable convex program.\n\nThe nuclear-norm heuristic has been employed widely over the past several years in a range of statistical modeling tasks involving rank minimization problems; see \\cite{Wainwright} and the references therein. The proof of our main result in Section~\\ref{section:theorem} incorporates some elements from the theoretical analyses in these previous papers, along with the introduction of some new ingredients. We give specific pointers to the relevant literature in Section~\\ref{section:proofs}. \n\n\n\n\\subsection{Notation}Given a matrix $U \\in \\mathbb{R}^{p_1 \\times p_2}$, the norm $\\|U\\|_{\\ell_\\infty}$ denotes the largest entry in magnitude of $U$, and the norm $\\|U\\|_2$ denotes the spectral norm (the largest singular value of $U$). We define the linear operators $\\mathcal{F}: {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p{\\times}q} \\times {\\mathbb{S}}^q \\rightarrow {\\mathbb{S}}^{(p+q)}$ and its adjoint $\\mathcal{F}^{\\dagger}: {\\mathbb{S}}^{(p+q)} \\rightarrow {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p{\\times}q} \\times {\\mathbb{S}}^q$ as follows:\n\n", "index": 13, "text": "\\begin{equation}\n\\mathcal{F}(M, N, K, O) \\triangleq \\left( \\begin{array}{cc}\nM - N & K \\\\\nK^T & O \\end{array} \\right), \\qquad \\mathcal{F}^{\\dagger}\\left( \\begin{array}{cc}\nQ & K \\\\\nK^{T} & O \\end{array} \\right)\\triangleq (Q,Q,K,O)\n\\label{eqn:OperatorDefs}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{F}(M,N,K,O)\\triangleq\\left(\\begin{array}[]{cc}M-N&amp;K\\\\&#10;K^{T}&amp;O\\end{array}\\right),\\qquad\\mathcal{F}^{\\dagger}\\left(\\begin{array}[]{cc}%&#10;Q&amp;K\\\\&#10;K^{T}&amp;O\\end{array}\\right)\\triangleq(Q,Q,K,O)\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>M</mi><mo>-</mo><mi>N</mi></mrow></mtd><mtd columnalign=\"center\"><mi>K</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><msup><mi>K</mi><mi>T</mi></msup></mtd><mtd columnalign=\"center\"><mi>O</mi></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>Q</mi></mtd><mtd columnalign=\"center\"><mi>K</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><msup><mi>K</mi><mi>T</mi></msup></mtd><mtd columnalign=\"center\"><mi>O</mi></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo>\u225c</mo><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nHere the symbol $\\otimes$ refers to the tensor product, and the Fisher information may be viewed as an operator from the parameter space ${\\mathbb{S}}^{p+q}$ to itself. From a mathematical programming perspective, the importance of the Fisher information can be seen from the fact that the Hessian of the negative-log-likelihood function \\eqref{eqn:main} evaluated at $\\Theta^\\star$ is given by $\\mathbb{I}^\\star$.\n\nTo ensure that the error term $\\|\\hat{\\Theta}-\\Theta^\\star\\|_2$ is small, a classical condition from the statistical estimation literature is to control the minimum gain of the Fisher information $\\mathbb{I}^\\star$ \\cite{Bickel}:\n\\begin{eqnarray}\n\\eta_1^\\star \\triangleq \\min_{M \\in {\\mathbb{S}}^{p+q}, \\|M\\|_{2} = 1} \\|\\mathbb{I}^{\\star}M\\|_{2}.\n\\label{eqn:eta1}\n\\end{eqnarray}\nFrom an optimization viewpoint, the condition that $\\eta_1^\\star$ is large is useful in ensuring that the negative-log-likelihood function at $\\Theta^\\star$ is sufficiently curved.\n\nTo further satisfy the requirements that $\\mathrm{rank}(\\hat{\\Theta}_{yx}) = \\mathrm{rank}(\\Theta^\\star_{yx}), ~ \\mathrm{rank}(\\hat{L}_y) = \\mathrm{rank}(L^\\star_y)$ with the convex relaxation \\eqref{eqn:main} for the composite approach and that $\\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$ with the relaxation \\eqref{eqn:main2} for the factor modeling approach, bounding the minimum gain quantity $\\eta_1^\\star$ \\eqref{eqn:eta1} from below is insufficient. To this end, we need to control two additional quantities associated with the Fisher information $\\mathbb{I}^\\star$. The first of these is the maximum inner-product between orthogonal elements in ${\\mathbb{S}}^{p+q}$ in the metric induced by the Fisher information $\\mathbb{I}^\\star$:\n\\begin{eqnarray}\n\\eta_2^\\star \\triangleq \\max_{\\substack{\\mathbb{W} \\subset {\\mathbb{S}}^{p+q} \\\\ \\mathbb{W}~\\mathrm{is~a~subspace}}} \\max_{M \\in \\mathbb{W}, \\|M\\|_2 \\leq 1} \\|\\mathcal{P}_{\\mathbb{W}^\\perp} \\mathbb{I}^{\\star}\\mathcal{P}_{\\mathbb{W}}(M)\\|_2\n\\label{eqn:eta2}\n\\end{eqnarray}\nAssuming that $\\eta_2^\\star$ is small ensures that errors in the estimation of the submatrix $\\Theta^\\star_{yx}$ do not impact the estimation of the $\\Theta^\\star_y$ submatrix (and vice versa). Indeed, in the absence of an upper bound on $\\eta_2^\\star$, the effect of the term ${\\mathcal{A}}^\\star x$ (represented by the column space of the submatrix $\\Theta^\\star_{yx}$) would not be distinguishable from the effects of the conditional factor model ${\\mathcal{B}}^\\star_u \\zeta_u + \\epsilon$ (represented by the submatrix $\\Theta^\\star_y$).\n\nThe final parameter associated to the Fisher information that we need to control for our main theorem is motivated by two concerns. First, we need to ensure that the diagonal and low-rank components $D^\\star_y$ and $L^\\star_y$ that compose the submatrix $\\Theta^\\star_y$ can be distinguished from each other (for Question $(2)$). For example, if the matrix $L^\\star_y$ consists of just a single nonzero in one of the diagonal entries and is zero elsewhere, identifying such a low-rank matrix from the difference $D^\\star_y - L^\\star_y$ is impossible. Second, we require that the components $D^\\star_y$ and $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ are identifiable given $D^\\star_y - L^\\star_y - \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$. These identifiability issues arising in the decomposition of sums of diagonal (and, more generally, sparse) and low-rank matrices have been investigated thoroughly \\cite{Chandrasekaran,Saunderson}. Specifically, the geometric insights in these papers imply that a natural condition to ensure identifiability in such decomposition problems is to assume the transversality of the intersection between the subspace of diagonal matrices in ${\\mathbb{S}}^p$ and the \\emph{tangent space} with respect to the algebraic variety of low-rank matrices at $L^\\star_y$ (for the composite factor model, i.e. Question $(1)$) or at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (for the factor model, i.e., Question $(2)$). In particular, the tangent space at a rank-$r$ matrix $N$ with respect to the algebraic variety of $p_1 \\times p_2$ matrices with rank less than or equal to $r$ is given by:\n\\begin{eqnarray*}\nT(N) &\\triangleq& \\{N_R + N_C | N_R, N_C \\in {\\mathbb{R}}^{p_1 \\times p_2}, \\\\&~& {\\mathrm{row}\\mbox{-}\\mathrm{space}}{N_R} \\subseteq {\\mathrm{row}\\mbox{-}\\mathrm{space}} {N}, {\\mathrm{column}\\mbox{-}\\mathrm{space}}{N_C} \\subseteq {\\mathrm{column}\\mbox{-}\\mathrm{space}}{N}\\}\n\\end{eqnarray*}\nThe tangent space at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ contains inside it the tangent space at $L^\\star_y$ as the row/column spaces of $L^\\star_y$ are contained inside the row/column spaces of $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (due to the assumption above that \\\\ ${\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\Theta^\\star_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(L_y^\\star) = \\{0\\}$); consequently, we assume that the set of diagonal matrices has a transverse intersection with the tangent space at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (which allows us to address the identifiability issues for both questions) by controlling the following parameter associated to the Fisher information $\\mathbb{I}^\\star$ for some $\\omega > 0$ and for all subspaces $\\tilde{T} \\subset {\\mathbb{S}}^p$ ``close to\u00e2\u0080\u0099\u00e2\u0080\u0099 $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$:\n\n\n\\begin{eqnarray}\n\\eta_3(\\tilde{T};\\omega) = \\max\\Bigg\\{\\max_{\\substack{M \\in \\tilde{T} \\\\ \\|M\\|_{2} \\leq 1}} \\|\\mathcal{P}_{\\mathrm{diag}}\\mathbb{I}^\\star_Y M\\|_{2}, \\max_{\\substack{M \\text{is diagonal} \\\\ \\|M\\|_{2} \\leq 1}} \\|\\mathcal{P}_{\\tilde{T}}\\mathbb{I}^\\star_Y M\\|_{2}\\Bigg\\} \\nonumber\\\\\n\\label{eqn:eta_3}\n\\end{eqnarray}\nHere the operator $\\mathcal{P}_{\\mathrm{diag}}$ represents projection onto the space of diagonal matrices, and $\\mathbb{I}^\\star_y = \\tilde{\\Theta}_y^{-1} \\otimes \\tilde{\\Theta}_y^{-1}$ represents the Fisher information with respect to the precision matrix $\\tilde{\\Theta}_y$ of the random vector $y$. The reason for considering the Fisher information with respect to the marginal precision matrix corresponding to $y$ is that the transversality conditions pertain only to the components of the precision matrix of $y$. We bound the quantity $\\eta_3(\\tilde{T};\\omega)$ for \\emph{all} spaces $\\tilde{T}$ that are within a small distortion of $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$:\n\n", "itemtype": "equation", "pos": 32909, "prevtext": "\nFinally, for any subspace ${\\mathfrak{H}}$, the projection onto the subspace is denoted by $\\mathcal{P}_{\\mathfrak{H}}$.\n\n\n\n\n\\section{Theoretical Results}\n\\label{section:theorem}\nIn this section, we state theorems that address Questions $(1)$ and $(2)$ from Section~\\ref{section:results}.  These theorems require assumptions on the population precision matrix, which are discussed in Section~\\ref{section:setup}, with the theorem statements given in Section~\\ref{section:theoremstatement}.  The proofs of these theorems are given in Section~\\ref{section:proofs} with some details deferred to the supplementary material.  We confirm the theoretical predictions of with numerical simulations on synthetic data in Section~\\ref{section:simulation}.\n\n\\subsection{Technical Setup}\n\\label{section:setup}\nAs discussed in Section~\\ref{section:results}, our theorems are premised on the existence of a population composite factor model \\eqref{eqn:composite} $y = {\\mathcal{A}}^\\star{x} + {\\mathcal{B}}^\\star_u \\zeta_u + \\epsilon$ underlying a pair of random vectors $(y,x) \\in {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$, with ${\\mathrm{rank}}({\\mathcal{A}}^\\star) = k_x$, ${\\mathcal{B}}_u^\\star \\in {\\mathbb{R}}^{p \\times k_u}$, and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{A}}}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{B}}_u}) = \\{0\\}$. As the convex relaxations \\eqref{eqn:main} and \\eqref{eqn:main2} are solved in the precision matrix parametrization, the conditions for our theorems are more naturally stated in terms of the joint precision matrix $\\Theta^\\star \\in \\mathbb{S}^{p+q}, ~ \\Theta^\\star \\succ 0$ of $(y,x)$.  The algebraic aspects of the parameters underlying the factor model translate to algebraic properties of submatrices of $\\Theta^\\star$.  In particular, the submatrix $\\Theta^\\star_{yx}$ has rank equal to $k_x$, and the submatrix $\\Theta^\\star_y$ is decomposable as $D_y^\\star - L_y^\\star$ with $D^\\star_y$ being diagonal and $L^\\star_y \\succeq 0$ having rank equal to $k_u$.  Finally, the transversality of ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{A}}})$ and ${\\mathrm{column}\\mbox{-}\\mathrm{space}}({{\\mathcal{B}}_u})$ translates to the fact that ${\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\Theta^\\star_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(L_y^\\star) = \\{0\\}$ have a transverse intersection.\n\nTo address the requirements raised in Question $(1)$ in Section~\\ref{section:results}, we seek an estimate $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ from the convex relaxation \\eqref{eqn:main} such that $\\mathrm{rank}(\\hat{\\Theta}_{yx}) = \\mathrm{rank}(\\Theta^\\star_{yx}), \\mathrm{rank}(\\hat{L}_y) = \\mathrm{rank}(L^\\star_y),$ and that $\\|\\hat{\\Theta}-\\Theta^\\star\\|_2$ is small. To satisfy the requirements of Question $(2)$ in Section~\\ref{section:results}, we first need to consider the factor model underlying the random vector $y \\in {\\mathbb{R}}^p$ that is induced upon marginalization of $x$. In particular, the precision matrix of $y$ is given by $\\tilde{\\Theta}^\\star_y = D^\\star_y - L^\\star_y - \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$, and Question $(2)$ asks whether we can obtain an estimate $(\\hat{\\tilde{D}}_y,\\hat{\\tilde{L}}_y)$ such that $\\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$, and the errors $\\|(\\hat{\\tilde{D}}_y - D^\\star_y\\|_2,\\|\\hat{\\tilde{L}}_y) - [L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}]\\|_2$ are small.\n\nBuilding on both classical statistical estimation theory \\cite{Bickel} as well as the recent literature on high-dimensional statistical inference \\cite{Buhlmann,Wainwright}, we describe our conditions in terms of assumptions on the \\emph{Fisher information} at the population model parametrized by the precision matrix $\\Theta^\\star$:\n\n", "index": 15, "text": "\\begin{equation*}\n\\mathbb{I}^\\star = {\\Theta^\\star}^{-1} \\otimes {\\Theta^\\star}^{-1}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{I}^{\\star}={\\Theta^{\\star}}^{-1}\\otimes{\\Theta^{\\star}}^{-1}.\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udd40</mi><mo>\u22c6</mo></msup><mo>=</mo><mrow><mmultiscripts><mi mathvariant=\"normal\">\u0398</mi><none/><mo>\u22c6</mo><none/><mrow><mo>-</mo><mn>1</mn></mrow></mmultiscripts><mo>\u2297</mo><mmultiscripts><mi mathvariant=\"normal\">\u0398</mi><none/><mo>\u22c6</mo><none/><mrow><mo>-</mo><mn>1</mn></mrow></mmultiscripts></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwhere the distortion $\\rho$ is measured via the following induced norm:\n\\begin{eqnarray*}\n\\rho(T_1,T_2) \\triangleq \\max_{\\|N\\|_2 \\leq 1} \\|(\\mathcal{P}_{T_1} - \\mathcal{P}_{T_2})(N)\\|_2.\n\\label{eqn:distortion}\n\\end{eqnarray*}\nThe reason for considering such distortions around the tangent space $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$ is that the variety of low-rank matrix are locally curved around their smooth points.  Consequently, the tangent spaces at matrices in a neighborhood around $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ having the same rank are generally not the same as $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$. In particular, the estimate $\\hat{\\tilde{L}}_y$ produced by the convex program \\eqref{eqn:main2} for the population low-rank matrix $L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy}$ may be such that $\\hat{\\tilde{L}}_y \\approx L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy}$ and ${\\mathrm{rank}}(\\hat{\\tilde{L}}_y) = {\\mathrm{rank}}(L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy})$, but it is generically the case that $T(\\hat{\\tilde{L}}_y) \\neq T(L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy})$.  Consequently, it is critical to control the quantity $\\eta_3(\\tilde{T};\\omega)$ in \\eqref{eqn:eta_3} for all $\\tilde{T}$ near the tangent space $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{yx})^{-1} \\Theta^\\star_{xy})$.\\\\\n\n\n\n\\subsection{Theorem Statements}\n\\label{section:theoremstatement}\nWe now describe the performance of the regularized maximum-likelihood programs \\eqref{eqn:main} and \\eqref{eqn:main2} under suitable conditions on the quantities introduced in the previous section. Before formally stating our main result, we introduce some notation. Let $\\sigma_y$ denote the minimum nonzero singular value of $L_y^\\star$, let $\\sigma_{yx}$ denote the minimum nonzero singular value of $\\Theta_{yx}^\\star$, and finally let $\\sigma$ denote the minimum nonzero singular value of $L_{y}^\\star + \\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}'$. In the following theorem statements, suppose that there exists $\\alpha > 0$, $\\beta \\geq 8$, and $\\omega \\in (0,1)$ such that the population Fisher information $\\mathbb{I}^\\star$ satisfies the following properties: $(i)~ \\eta_1^\\star \\geq 3\\alpha$, $(ii)~ \\eta_2^\\star \\leq \\frac{8\\alpha}{3\\beta}$, and $(iii)~\\eta_3^\\star(\\omega) \\leq \\frac{2\\alpha}{\\beta}$.  Theorem~\\ref{theorem:main} pertains to the consistency of the estimator \\eqref{eqn:main}, and Theorem~\\ref{theorem:main2} relates to the consistency of the estimator \\eqref{eqn:main2}.\n\n\\begin{theorem}\n\\label{theorem:main}\nLet $m \\triangleq \\max\\{1, \\frac{1}{\\gamma}\\}$, $\\bar{m} \\triangleq \\max\\{1, {\\gamma}\\}$, and $\\psi \\triangleq \\|(\\Theta^\\star)^{-1}\\|_2$. Further, $C_1 = \\frac{24}{\\alpha} + \\frac{1}{\\psi^2}$, $C_2 = \\frac{4}{\\alpha} (\\frac{1}{3\\beta} + 1)$, $C_{\\sigma_Y} = C_1^2\\psi^2\\max \\{ 12\\beta+1, \\frac{2}{{C_2}\\psi^2}+1\\}$, $C_{\\sigma_{YX}} = C_1^2\\psi^2\\max\\{18\\beta, \\frac{2}{C_2\\psi^2} + 6\\beta\\}$, $C_{samp} = \\max\\{\\frac{1}{48\\psi\\beta},{48{\\beta}{\\psi^3}C_1^2}, 8\\psi{C_2}, \\frac{64{\\psi^3}C_2}{\\alpha}\\}$, and $\\lambda_{\\text{upper}} = \\frac{1}{m\\bar{m}^2C_{samp}}$. Suppose that the following conditions hold:\n\\begin{enumerate}\n\\item $n \\geq \\frac{4608\\psi^2\\beta^2m^2(p+q)}{\\lambda_{\\text{upper}}^2}$; that is $n \\gtrsim \\Big[\\frac{\\beta^4}{\\alpha^2} m^4\\bar{m}^4\\Big] (p+q) $\n\\item $\\lambda_n \\in \\Big[\\sqrt{\\frac{4608\\psi^2\\beta^2m^2(p+q)}{n}}, \\lambda_{\\text{upper}}\\Big]$; \\hspace{.1in} e.g. $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\item $\\gamma \\in \\Big[1, \\frac{8\\alpha}{3\\beta\\eta_2^\\star}\\Big]$ \\\\\n\\item $\\sigma_{Y} \\geq \\frac{m}{\\omega_{}}C_{\\sigma_{Y}} \\lambda_n$; \\hspace{.05in} that is $\\sigma_{Y} \\gtrsim \\frac{\\beta^2}{\\alpha^2\\omega_{}}{m}{}\\sqrt{\\frac{p+q}{n_{}}}$ \\hspace{.05in} if \\hspace{.05in} $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\item $\\sigma_{YX} \\geq {m^2}{} C_{\\sigma_{YX}}\\gamma^2\\lambda_n $; \\hspace{.05in} that is $\\sigma_{YX} \\gtrsim {\\beta^2 \\gamma^2}{\\alpha^2}{m^2}{}\\sqrt{\\frac{p+q}{n}}$ \\hspace{.05in} if ~$\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\end{enumerate}\n\nThen with probability greater than $1-2\\exp\\{-\\frac{n\\lambda_n^2}{4608 \\beta^2 m^2 \\psi^2}\\}$, the optimal solution $(\\hat{\\Theta},\\hat{D}_y,\\hat{L}_y)$ of \\eqref{eqn:main} with i.i.d. observations $\\{y^{(i)}, x^{(i)}\\}_{i = 1}^n$ of $(y,x)$ satisfies the following properties:\n\\begin{enumerate}\n\\item rank($\\hat{L}_y$) = rank(${L}_y^\\star$), rank($\\hat{\\Theta}_{yx}$) = rank(${{\\Theta}^\\star_{yx}}$)\\[.005in]\n\\item $\\|\\hat{D}_y - D_Y^\\star\\|_{2} \\leq C_1\\lambda_n$, $\\|\\hat{L}_y - L_y^\\star\\|_{2} \\leq C_1\\lambda_n$, $\\|\\hat{\\Theta}_{yx} - \\Theta_{yx}^\\star\\|_{2} \\leq C_1\\lambda_n\\gamma$, and $\\|\\hat{\\Theta}_{x} - \\Theta_{x}^\\star\\|_{2} \\leq C_1\\lambda_n$; that is $\\|\\hat{D}_y - D_y^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{L}_y - L_y^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{\\Theta}_{yx} - \\Theta_{yx}^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}\\gamma{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{\\Theta}_{x} - \\Theta_{x}^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$ \\hspace{.05in} if \\hspace{.05in} $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$.\n\\end{enumerate}\n\\end{theorem}\n\n\\begin{theorem}\n\\label{theorem:main2}\nDenote $\\tilde{\\psi} \\triangleq \\|(\\tilde{\\Theta}_y^\\star)^{-1}\\|_2$.  Let $C_1 = \\frac{24}{\\alpha} + \\frac{1}{\\tilde{\\psi}^2}$, $C_2 = \\frac{4}{\\alpha} (\\frac{1}{3\\beta} + 1)$, $C_{\\sigma} = C_1^2\\tilde{\\psi}^2\\max \\{ 12\\beta+1, \\frac{2}{{C_2}\\tilde{\\psi}^2}+1\\}$, $C_{samp} = \\max\\{\\frac{1}{48\\tilde{\\psi}\\beta},{48{\\beta}{\\tilde{\\psi}^3}C_1^2}, 8\\tilde{\\psi}{C_2}, \\frac{64{\\tilde{\\psi}^3}C_2}{\\alpha}\\}$, and $\\lambda_{\\text{upper}} = \\frac{1}{C_{samp}}$. Suppose that the following conditions hold:\n\\begin{enumerate}\n\\item $n \\geq \\frac{4608\\tilde{\\psi}^2\\beta^2m^2p}{\\lambda_{\\text{upper}}^2}$; that is $n \\gtrsim \\frac{\\beta^4}{\\alpha^2}p $\n\\item $\\tilde{\\lambda}_n \\in \\Big[\\sqrt{\\frac{4608\\tilde{\\psi}^2\\beta^2p}{n}}, \\lambda_{\\text{upper}}\\Big]$; \\hspace{.1in} e.g. $\\tilde{\\lambda}_n \\sim \\beta\\sqrt{\\frac{p}{n}}$\n\\item $\\sigma \\geq \\frac{1}{\\omega_{}}C_{\\sigma_{}} \\tilde{\\lambda}_n$; \\hspace{.1in} that is $\\sigma_{} \\gtrsim \\frac{\\beta^2}{\\alpha^2\\omega_{}}{}{}\\sqrt{\\frac{p}{n_{}}}$ \\hspace{.05in} if \\hspace{.05in} $\\tilde{\\lambda}_n \\sim \\beta \\sqrt{\\frac{p}{n}}$\n\\end{enumerate}\n\nThen with probability greater than $1-2\\exp\\{-\\frac{n\\tilde{\\lambda}_n^2}{4608 \\beta^2 \\tilde{\\psi}^2}\\}$, the optimal solution $(\\hat{\\tilde{D}}_y,\\hat{\\tilde{L}}_y)$ of \\eqref{eqn:main2} with i.i.d. observations $\\{y^{(i)}\\}_{i = 1}^n$ of $y$ satisfies the following properties:\n\\begin{enumerate}\n\\item rank($\\hat{\\tilde{L}}_y$) = rank($L_y^\\star + \\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}^{-1}$)\\[.005in]\n\\item $\\|\\hat{\\tilde{D}}_y - D_y^{\\star}\\|_{2} \\leq C_1\\tilde{\\lambda}_n$; $\\|\\hat{\\tilde{L}}_y - L_y^{\\star}-\\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}\\|_{2} \\leq C_1\\tilde{\\lambda}_n$; that is $\\|\\hat{\\tilde{D}}_y - D^{\\star}_y\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{}\\sqrt{\\frac{p}{n}}$ and $\\|\\hat{\\tilde{L}}_y - L_y^{\\star}-\\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{}\\sqrt{\\frac{p}{n}}$.\n\\end{enumerate}\n\\end{theorem}\nWe outline the proof of Theorem~\\ref{theorem:main} in Section~\\ref{section:proofs}. We omit the proof of Theorem~\\ref{theorem:main2} as it follows a very similar sequence of steps to that of Theorem~\\ref{theorem:main}. The quantities $\\alpha, \\beta, \\omega$ as well as the choices of parameters $\\gamma$ play a prominent role in our results. Indeed larger values of $\\alpha, \\omega$ and smaller values of $\\beta$ (leading to a better conditioned Fisher information even for large distortions around the tangent space $T(L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{xy}^\\star})$) lead to less stringent requirements on the sample complexity, on the minimum nonzero singular value of $\\sigma_{y}$ of $L_y^\\star$, on the minimum nonzero singular value of $\\sigma$ of $L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{xy}^\\star}$, and on the minimum nonzero singular value $\\sigma_{yx}$ of $\\Theta_{yx}^\\star$.\n\n\n\\subsection{Synthetic Simulations}\n\\label{section:simulation}\nIn this section, we give experimental evidence for the consistency of our convex program~\\eqref{eqn:main} and~\\eqref{eqn:main2} on synthetic examples. Our choices of regularization parameters $\\lambda_n$ and $\\gamma$ (in the convex program \\eqref{eqn:main}) and $\\tilde{\\lambda}_n$ (in the convex program \\eqref{eqn:main2}) are guided by Theorem~\\ref{theorem:main} and Theorem~\\ref{theorem:main2}. Specifically, the parameters $\\lambda_n$ and $\\tilde{\\lambda}_n$ are to be set in a manner that depends on the number of observations $n$, and we choose these to be proportional to $\\sqrt{\\frac{p+q}{n}}$ and $\\sqrt{\\frac{p}{n}}$, respectively. Further, the theorems suggest that we should expect the ranks of the estimated solutions $\\hat{L}_y$ and $\\hat{\\Theta}_{yx}$ to be the same for a range of values of $\\gamma$ when the correct underlying model structure is identified. Therefore, we solve the convex program \\eqref{eqn:main} for several values of $\\gamma$, and choose a value for which the structure of the estimated model remains the same for the largest contiguous range of values of $\\gamma$. To solve the convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} numerically, we use the special-purpose solver LogDetPPA \\cite{Toh}. \\\\\n\n\n\nWe generate the population model $\\Theta^\\star = \\begin{pmatrix} D_y^\\star - L_y^\\star & \\Theta_{yx}^\\star \\\\ {\\Theta_{yx}^\\star}' & \\Theta_{x}^\\star\\end{pmatrix}$ as follows: we fix the number of responses to be $p = 40$ and the number of covariates to be $q = 10$. We then generate a random Gaussian matrix $W \\in {\\mathbb{R}}^{p \\times k_u}$ and let $L_y^\\star =  WW'$. We let $D_y^\\star = \\mu{I}_{p}$ where $\\mu$ is chosen to be twice the largest eigenvalue of $L_y^\\star$. We also generate random Gaussian matrices $\\tilde{W} \\in {\\mathbb{R}}^{p \\times k_u}$ and $Z \\in {\\mathbb{R}}^{q \\times k_u}$ and let $\\Theta_{yx}^\\star = \\tilde{W}Z'$. Finally, we let $\\Theta_x^\\star = \\Theta_{xy}^\\star{\\Theta_{y}^\\star}^{-1}\\Theta_{yx}^\\star + \\delta{I}_{q}$ where $\\delta$ is chosen large enough so that the overall matrix $\\Theta^\\star$ has a condition number that is relatively small (in our examples, we chose $\\delta$ so that the condition number is less than $20$). This approach generates a composite factor model~\\eqref{eqn:composite} with $\\text{rank}({\\mathcal{A}}) = k_x$ and $\\tilde{{\\mathcal{B}}} \\in {\\mathbb{R}}^{p \\times k_u}$, and a factor model~\\eqref{eqn:factormodel} with $k = k_x + k_u$ latent factors. We obtain three models with $(k_x, k_u) = (1,1)$, $(k_x, k_u) = (1,2)$, and $(k_x, k_u) = 2$. For each model, we generate $n$ samples of responses $y$ and covariates $x$, and use these observations as input to convex programs~\\eqref{eqn:main} and~\\eqref{eqn:main2}. Figure~\\ref{fig:recovery}(a) shows the probability of obtaining structurally correct estimates of the factor model parameters using \\eqref{eqn:main2} (i.e. $\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{yx}^\\star}')$) and Figure~\\ref{fig:recovery}(b) shows the probability of obtaining structurally correct estimates of the composite factor model (i.e. $\\text{rank}(\\hat{{L}}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\Theta_{yx}^\\star) = \\text{rank}(\\hat{\\Theta}_{yx})$). These probabilities are evaluated over $10$ experiments for each value of $n$. These results agree with our theoretical results that given (sufficiently many) samples of responses/covariates, the convex programs \\eqref{eqn:main2} and \\eqref{eqn:main} provide structurally correct estimates of a factor model \\eqref{eqn:factormodel} and composite factor model \\eqref{eqn:composite}, respectively. \n\\FloatBarrier\n\\begin{figure}[!http]\n\\centering\n\\subfigure[factor model]{\n\\includegraphics[width=5cm, height = 5cm]{foo}\n}\n\\subfigure[composite factor model]{\n\\includegraphics[width=5cm, height = 5cm]{foo3}\n}\n\\caption{Synthetic data: plot shows probability of correct structure recovery in factor model and composite factor model. The three models studied are $(i)~(k_x, k_u) = (1,1) $, $(ii)~(k_x, k_u) = (2,1)$, and $(iii)~(k_x, k_u) = (2,2)$. For each plotted point, the probability of structurally correct estimation is obtained over $10$ trials.} \n\\label{fig:recovery}\n\\end{figure}\n\\FloatBarrier\n\n\n\n\\section{Identifying Latent Factors Influencing Stock Returns}\n\\label{section:experiments}\nRecall from Section~\\ref{section:results} that our approach to associate semantics to latent variables in a factor model is a two-stage process. In the first stage, we identify a factor model based on observations of $y$ using the convex relaxation \\eqref{eqn:main2}, which results in an estimate $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$. In the the second step, we use simultaneous observations of $y$ and some additional covariates $x$ to identify a composite factor model using the convex relaxation \\eqref{eqn:main} with the resulting estimates being $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$. As discussed in Section~\\ref{section:theorem} the composite factor model of $(y,x)$ offers an interpretation of the latent variables of the factor model underlying $y$ if $(i)~ \\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(\\hat{L}_y + \\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy})$, $(ii)~{\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\hat{\\Theta}_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\hat{L}_{y}) = \\{0\\}$, and \\\\$(iii) \\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ is small.\nThese observations naturally lead to the following algorithmic approach:\n\\FloatBarrier\n\\begin{algorithm}\n\\caption{Interpreting Latent Variables in a Factor Model}\n\\begin{algorithmic}[1]\n\\STATE {\\bf Input}: A collection of observations $\\{(y^{(i)}, x^{(i))}\\}_{i=1}^n \\subset {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ of the variables $y$ and of some auxiliary covariates $x$.\\\\\n\\vspace{.04in}\n\\STATE{\\bf Factor Modeling}: Supply observations $\\{y^{(i)}\\}_{i=1}^n$ to the convex program \\eqref{eqn:main2} to learn a factor model with parameters $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$.\\\\\n\\vspace{.04in}\n\\STATE {\\bf Composite Factor Modeling}: For each $d = 1, \\dots, q$, sweep over parameters $(\\lambda_n, \\gamma)$ in the convex program \\eqref{eqn:main} (with $\\{y^{(i)}, x^{(i))}\\}_{i=1}^n$ as input) to identify composite models with estimates $(\\hat{\\Theta},\\hat{D}_y,\\hat{L}_y)$ that satisfy the following three properties: $(i)~\\text{rank}(\\hat{\\Theta}_{yx}) = d$, $(ii)$~$\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(\\hat{L}_y) + \\text{rank}(\\hat{\\Theta}_{yx})$, and $(iii)~\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(\\hat{L}_y + \\text{rank}(\\hat{\\Theta}_{yx}\\hat{\\Theta}_{x}^{-1}\\hat{\\Theta}_{xy}))$. \\\\\n\\vspace{.04in}\n\\STATE{\\bf Identifying Subspace}: For each $d = 1, \\dots, q$ and among the candidate composite models (from the previous step), choose the composite factor model that minimizes the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$.\\\\\n\\vspace{.04in}\n\\STATE{\\bf Output}: For each $d = 1,\\dots q$, the $d$-dimensional projection of $x$ into the row-space of $\\hat{\\Theta}_{yx}$ represents the interpretable component of the latent variables in the factor model.\n\\end{algorithmic}\n\\end{algorithm}\n\\FloatBarrier\nWe note that in some cases, a factor model is obtained off-line based on scientific considerations. In these situations, one would proceed to step 3 of the algorithm. Typically, however, factor models are learned in a data-driven approach based on observations of responses $\\{(y^{(i)}\\}_{i=1}^n$. We from step 2 of the algorithm that our approach to learn a factor model via the convex program \\eqref{eqn:main2} requires the specification of the regularization parameter $\\tilde{\\lambda}_n$. In our experimental results on the financial asset dataset, we choose this parameter via cross-validation techniques. Further, we note that the effectiveness of this algorithm in identifying semantics to latent variables in the input factor model is largely dependent on the size of the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$. Indeed, the smaller this quantity, the more confidence one should have that the composite factor model has attributed meaning to some component of the latent variables in the factor model.\n\n\\subsection{Experimental Results on Financial Asset Data}\nWe demonstrate the utility of our algorithmic approach in a financial asset data analysis problem. Specifically, we consider as our responses $y$ the monthly stock return of $p = 66$ companies from the Standard and Poor index over the period June $1990$ to July $2014$, which leads to a total of $n = 277$ observations. We also obtain monthly observations over the same period of the following $q = 11$ covariates, consumer price index, EUR to USD exchange rate, federal reserve rate, gold prices, industrial production, inflation rate, mortgage rate, oil exports, oil imports, treasury rate, and unemployment rate. These covariates were chosen because they plausibly influence the values of stock prices. For the purposes of our experiments, we set aside a random subset of $n_{\\mathrm{train}} = 227$ of these observations as a training set and the remaining subset of $n_{\\mathrm{test}} = 50$ as the test set. Similar to synthetic simulations in Section~\\ref{section:simulation}, we solve the convex program~\\eqref{eqn:main} and~\\eqref{eqn:main2} numerically using the LogDetPPA package \\cite{Toh}.\\\\\n\nWe begin by the second step of our algorithm which is to identify a factor model~\\eqref{eqn:factormodel} that is well-suited for modeling stock returns. We find such a factor model by solving the convex program~\\eqref{eqn:main2} where the regularization parameter $\\tilde{\\lambda}_n$ is chosen via cross-validation. Concretely, for a particular choice of $\\tilde{\\lambda}_n$, we use the training set $\\{y_{{\\mathrm{train}}}^{j}\\}_{j = 1}^{222} \\in \\mathbb{R}^{67}$ as input to the convex program \\eqref{eqn:main}, and solve \\eqref{eqn:main} to obtain a factor model specified by $(\\hat{\\tilde{D}}_y , \\hat{\\tilde{L}}_y)$. We then compute the average log-likelihood over the testing set $\\{y_{{\\mathrm{test}}}^{j}\\}_{j = 1}^{50} \\in \\mathbb{R}^{67}$ using the distribution specified by the precision matrix $\\hat{\\tilde{D}}_y - \\hat{\\tilde{L}}_y$. We perform this procedure as we vary $\\tilde{\\lambda}_n$ from $0.04$ to $4$ in increments of $0.004$. Figure~\\ref{fig:testp} shows a plot of $\\text{rank}(\\hat{\\tilde{L}}_y))$ (i.e. number of latent factors) vs. average log-likelihood performance on the testing set. Notice that fixing the number of latent factors does not lead to a unique factor model as varying the regularization parameter $\\tilde{\\lambda}_n$ may lead to a change in the estimated model, but no change in its structure (i.e. $\\text{rank}(\\hat{\\tilde{L}}_y)$ remains the same). As larger values of average log-likelihood are indicative of a better fit to test samples, these results suggest that $12$ or $13$ latent factors influence stock prices. We thus focus on associating semantics to the factor model with the largest average log-likelihood performance that consists of $12$ latent factors, and the factor model with the largest average log-likelihood performance that consists of $13$ latent factors.\n\\FloatBarrier\n\\begin{figure}[!http]\n\\centering\n\\includegraphics[width=5cm, height = 5cm]{trainingperformance}\n\\caption{Number of latent factors vs. average log-likelihood over testing set. These results are obtained by sweeping over parameters $\\tilde{\\lambda}_n \\in [0.04, 4]$ in increments of $0.004$ and solving the convex program~\\eqref{eqn:main2}}. \n\\label{fig:testp}\n\\end{figure}\n\\FloatBarrier\nWe now proceed with the third step of our algorithm. Using joint observations of responses and covariates $\\{y^j_{{\\mathrm{train}}}, x^{j}_{{\\mathrm{train}}}\\}_{j = 1}^{227}$ as input to the convex program~\\eqref{eqn:main} , we perform an exhaustive sweep over parameter space $(\\lambda_n, \\gamma)$ to learn composite models with estimates $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ such that $\\text{rank}(\\hat{\\Theta}) = 1,2,\\dots 11$, and $\\text{rank}(\\hat{L}_y) = 1,2,\\dots 12$. As we are interested comparing these composite models to the factor model with $12$ or $13$ latent variables, we finely grid the parameter space $(\\lambda_n, \\gamma)$ so that there are a large number of models for which $\\text{rank}(\\hat{\\Theta}) + \\text{rank}(\\hat{L}_y)$ is equal to 12 or 13. Among these models, we restrict to those that satisfy the conditions of step 3 of the algorithm. Table~\\ref{table:nummodels} shows the number of models that satisfy these conditions for $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$. For each $d = 1,\\dots,11$, we then identify the composite factor model which minimizes the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$. Table 2 and Table 3 show the values of this quantity for $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$ with respect to the factor model with $12$ and $13$ latent variables, respectively.\n\n\\FloatBarrier\n\\begin{table}[ht]\n\n\\centering \n\\begin{tabular}{c c} \n\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\#$ models satisfying conditions of step 2. \\\\\n\\hline \n(1,11) & 261 \\\\ \n(1,12) & 174 \\\\\n(2,10) & 84 \\\\\n(2,11) & 126 \\\\\n(3,9) & 112\\\\\n(3,10) & 84 \\\\\n(4,8) & 144 \\\\\n(4,9) & 72 \\\\\n(5,7) & 4 \\\\\n(5,8) & 64 \\\\\n\\hline \n\\end{tabular}\n\\caption{Number of composite factor models with $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$ that satisfy the requirements of step 2 in the algorithm description at the beginning of this section (for the factor model with $12$ or $13$ latent variables).}\n\\label{table:nummodels} \n\\end{table}\n\\FloatBarrier\n\\FloatBarrier\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c} \n\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$\\[0.5ex]\n\\hline \n(1,11) & 0.08 \\\\ \n(2,10) & 0.17 \\\\\n(3,9) & 0.26 \\\\\n(4,8) & 0.31 \\\\\n(5,7) & 0.43 \\\\ [1ex] \n\\hline \n\\end{tabular}\n\\label{table:deviation12} \n\\caption{Deviation of the candidate composite factor model from the factor model consisting of $12$ latent variables } \n\\end{table}\n\\FloatBarrier\n\n\\FloatBarrier\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c } \n\\hline\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ \\\\ [0.5ex] \n\n\\hline \n(1,12) & 0.004 \\\\ \n(2,11) & 0.08 \\\\\n(3,10) & 0.17 \\\\\n(4,9) & 0.26 \\\\\n(5,8) & 0.31 \\\\ [1ex] \n\\hline \n\\end{tabular}\n\\label{table:deviation13} \n\\caption{Deviation of the candidate composite factor model from the factor model consisting of $13$ latent variables } \n\\end{table}\n\\FloatBarrier\nFocussing on the case corresponding to which we identified a $13$-factor model underlying $y$, the results of Table 3 suggest that we identify a $2$-dimensional interpretable component of the $13$ latent variables as the deviation $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ on the right-hand-side of this table is small as long as $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,2$.  For $\\text{rank}(\\hat{\\Theta}_{yx}) = 3,4,5$, the deviation appears to be quite large and may not lead to meaningful conclusions.\n\nAs a final step of the algorithm, we investigate the properties of the two-dimensional row-space of $\\hat{\\Theta}_{yx}$ to shed some light on those covariates that appear to play a significant role in capturing some of the latent phenomena in the $13$-factor model.  In particular, for the composite factor model with $(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y})) = (2,11)$ (second row in Table 3), we let $V \\in {\\mathbb{R}}^{11 \\times 2}$ denote a matrix with orthogonal, unit-norm columns such that $V$ the columns of $V$ form a basis for the row space of $\\hat{\\Theta}_{yx}$ (such a matrix may be computed, for example, via the singular value decomposition).  Recall that $V\u00e2\u0080\u0099x$ represents the component of the $13$ latent variables that is interpretable via the covariates $x$.  We then consider the Euclidean-squared-norm  of the $i$-th row of $V$, as this specifies the relative strength of the $i$-th covariate. As shown in Table~\\ref{table:covariaterelevance}, all covariates have some contribution (as we allow general linear combinations of the covariates $x$ in the composite factor model~\\eqref{eqn:composite}). However, the covariates exchange rate, inflation rate, and oil imports seem to be the most relevant, and the covariates gold prices and oil exports seem to be the least relevant.\n\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c} \n\\hline\\hline \ncovariate & strength\\\\ [0.5ex] \n\n\\hline \nCPI & 0.07 \\\\\nexchange rate & 0.18 \\\\\nfederal reserve rate & 0.06\\\\\ngold prices & 0.04 \\\\\nindustrial production & 0.09 \\\\\ninflation rate & 0.15 \\\\\nmortgage rate & 0.07 \\\\\noil exports & 0.02 \\\\\noil imports & 0.15\\\\\ntreasury rate & 0.08\\\\\nunemployment & 0.07 \\\\\n\\hline \n\\end{tabular}\n\\caption{Strength of each covariate in the composite factor model with $2$-dimensional projection of covariates and $11$ latent variables} \n\\label{table:covariaterelevance} \n\\end{table}\n\n\n\n\n\n\n\\section{Proofs of Main Results}\n\\label{section:proofs}\n\n\n\n\n\\subsection{Proof Strategy}\n\nUnder assumptions of Theorem 1, we construct appropriate primal feasible variables $(\\hat{\\Theta},\\hat{D}_y, \\hat{L}_y)$ that satisfy the conclusions of the theorem - i.e., $\\hat{\\Theta}_{yx}$, $\\hat{L}_y$ are low-rank (with the same ranks as the underlying population quantities $\\Theta_{yx}^\\star$ and $L_y^\\star$) - and for which there exists a corresponding dual variable certifying optimality. This proof technique is sometimes also referred to as a primal-dual witness or certificate approach \\cite{Wai2009}. The high-level proof strategy is similar in spirit to the proofs of consistency results for sparse graphical model recovery \\cite{Ravikumar} and latent variable graphical model recovery \\cite{Chand2012}, although our convex program and the conditions required for its success are different from these previous results`. Consider the following convex program\n\n\\begin{eqnarray}\n(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y) = \\arg\\min_{\\substack{\\Theta \\in {\\mathbb{S}}^{p+q}, ~\\Theta \\succ 0 \\\\ D_y,L_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n) + \\lambda_n [\\gamma\\|\\Theta_{yx}\\|_{\\star} + \\|L_y\\|_{\\star}] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_{y} = D_y - L_y, D_y ~\\mathrm{is~diagonal}\n\\label{eqn:ConvexRelaxed_N}\n\\end{eqnarray}\n\nComparing \\eqref{eqn:ConvexRelaxed_N} with the convex program \\eqref{eqn:main}, the difference is that we no longer constrain ${L}_y$ to be a positive semidefinite matrix. In particular, if ${L}_y \\succeq 0$, then the nuclear norm of the matrix ${L}_y$ in the objective function of \\eqref{eqn:ConvexRelaxed23_N} reduces to the trace of $L_y$. We show in the appendix that with high probability, the matrix $\\tilde{L}_y$ is positive semidefinite. Standard convex analysis states that $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ is the solution of the convex program \\eqref{eqn:ConvexRelaxed_N} if there exists a dual variable $\\Lambda \\in {\\mathbb{S}}^p$ with the following optimality conditions being satisfied:\n\\begin{eqnarray*}\n[\\Sigma_n - {\\hat{\\Theta}}^{-1}]_y + \\Lambda = 0&;& \\hspace{.1in} [\\Sigma_n - {\\hat{\\Theta}}^{-1}]_y \\in \\lambda_n\\partial\\|\\hat{L}_y\\|_\\star\\[.01in]\n[\\Sigma_n - {\\hat{\\Theta}}^{-1}]_{yx}  \\in -\\lambda_n\\gamma\\partial\\|\\hat{\\Theta}_{yx}\\|_{\\star}&;&  \\hspace{.1in} [\\Sigma_n - {\\hat{\\Theta}}^{-1}]_{x}  = 0\\\\ \\hat{\\Theta}_y = \\hat{D}_y - \\hat{L}_y; \\hspace{.1in} \\hat{D}_y \\text{ is diagonal}&;& \\hspace{.1in} \\Lambda_{i,i} = 0 ~ \\text{ for } i = 1,2,\\dots p\n\\end{eqnarray*}\nRecall that elements of the subdifferential with respect to nuclear norm at a matrix $M$ have the key property that they decompose with respect to the tangent space $T(M)$. Specifically, the subdifferential with respect to the nuclear norm at a matrix $M$ with (reduced) SVD given by $M = UQV^T$ is as follows:\n\\begin{eqnarray*}\nN \\in \\partial\\|M\\|_{\\star} \\Leftrightarrow \\mathcal{P}_{T(M)} (N) = UV^T ~, ~ \\|\\mathcal{P}_{T(M)} (N)\\|_2 \\leq 1,\n\\end{eqnarray*}\nwhere $\\mathcal{P}$ denote a projection operator. Let us denote the subspace $W \\in {\\mathbb{S}}^p$ as the set of diagonal matrices with nonnegative entries. Let SVD of $\\hat{L}_y$ and $\\hat{\\Theta}_{yx}$ be given by $\\hat{L}_y = \\bar{U}\\bar{Q}\\bar{V}'$ and $\\hat{\\Theta}_{yx} = \\breve{U}\\breve{Q}{\\breve{V}}'$ respectively, and $Z \\triangleq  (0, \\hspace{.1in} \\lambda_n\\bar{U}\\bar{V}', \\hspace{.1in}  -\\lambda_n\\gamma_{}{\\breve{U}}{\\breve{V}}',  \\hspace{.1in} 0)$. Setting $\\Lambda  = [\\Sigma_n - \\hat{\\Theta}^{-1}]_{Y, \\text{off diagonal}}$, and letting $\\mathbb{H} = W \\times T(\\hat{L}_y) \\times T(\\hat{\\Theta}_{yx}) \\times {\\mathbb{S}}^q$, the optimality conditions of \\eqref{eqn:ConvexRelaxed_N} can be reduced to:\n\\begin{center}\n\\begin{enumerate}\n\\item $\\mathcal{P}_{\\mathbb{H}}\\mathcal{F}^{\\dagger}(\\Sigma_n - \\hat{\\Theta}^{-1}) = Z$\n\\item $\\|\\mathcal{P}_{T(\\hat{L}_y)^\\perp} (\\Sigma_n - \\hat{\\Theta}^{-1})_y\\|_2 < \\lambda_n$;  $\\|\\mathcal{P}_{T(\\hat{\\Theta}_{yx})^\\perp} (\\Sigma_n - \\hat{\\Theta}^{-1})_{yx}\\|_2 < \\lambda_n\\gamma$\n\\end{enumerate}\n\\end{center}\n\nOur analysis proceeds by constructing variables $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ that satisfy the optimality conditions specified above. Consider the optimization program \\eqref{eqn:ConvexRelaxed_N} with additional (nonconvex) constraints that $L_y$ and $\\Theta_{yx}$ belong to algebraic variety of low rank matrices specified by $L_y^\\star$ and $\\Theta_{yx}^\\star$. While this new program is nonconvex, it has a very interesting property that at the global optimal solution (and indead at any locally optimal solution) $\\tilde{L}_y$ and $\\tilde{\\Theta}_{yx}$ are smooth points of their respective algebraic varieties. This observation suggests that the Lagrange multipliers corresponding to the additional variety constraints belongs to $T(\\tilde{L}_y)^\\perp$ and $T(\\tilde{\\Theta}_{yx})^\\perp$ respectively. We show under suitable conditions that $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ also satisfy the second optimality condition of \\eqref{eqn:ConvexRelaxed_N} corresponding to the tangent spaces $T(\\tilde{L}_y)^\\perp$ and $T(\\tilde{\\Theta}_{yx})^\\perp$. Thus $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ is a unique solution of \\eqref{eqn:main} and as constructed, is algebraically consistent (i.e. $\\text{rank}(\\tilde{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\tilde{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$)\n\n\n\\subsection{Results proved in the supplementary material}\nTo ensure that the estimate $\\hat{\\Theta}$ is close to the population quantity $\\Theta^\\star$, the quantity $E = \\hat{\\Theta} - \\Theta^\\star$ must be small. Since the optimality conditions of \\eqref{eqn:ConvexRelaxed_N} are stated in terms of $\\hat{\\Theta}^{-1}$, we bound the deviation between $\\hat{\\Theta}^{-1}$ and ${\\Theta^\\star}^{-1}$. Specifically, the Taylor series expansion of $\\hat{\\Theta}^{-1}$ around $\\Theta^\\star$ is given by:\n\\begin{eqnarray*}\n\\hat{\\Theta}^{-1} = (\\Theta^\\star+ E)^{-1} = {\\Theta^{\\star}}^{-1} + {\\Theta^{\\star}}^{-1}E{\\Theta^{\\star}}^{-1} + R_{\\Sigma^\\star}(E)\n\\end{eqnarray*}\nwhere, $R_{\\Sigma^\\star}(E) = \\Sigma^\\star\\Big[\\sum_{k = 2}^{\\infty}(-E\\Theta^\\star)^k\\Big]$. Recalling that $\\mathbb{I}^\\star = \\Theta^\\star \\otimes \\Theta^\\star$, we note that $\\hat{\\Theta}^{-1} - {\\Theta^\\star}^{-1} = \\mathbb{I}^\\star{E} + {R}_{\\Sigma^\\star}(E)$.  In Section~\\ref{section:theorem},  we imposed a set of conditions on $\\eta_1^\\star, \\eta_2^\\star$ in \\eqref{eqn:eta1} and \\eqref{eqn:eta2} so that $\\mathbb{I}^\\star$ is globally well-conditioned, as well a condition on $\\eta_3^\\star(\\omega)$ in \\eqref{eqn:eta_3} to address identifiability issues in the diagonal-minus-low-rank decomposition. These conditions allow us to control $\\mathbb{I}^\\star(E)$ when $E$ is restricted to certain directions. We bound the remainder term ${R}_{\\Sigma^\\star}(E)$ in Proposition~\\ref{prop:Remainder} where $E$ is restricted to live in a certain space. Specifically, consider the following constrained optimization program:\n\\begin{eqnarray}\n(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{x^{(i)},y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L_y}\\|_{\\star} + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} ~~\\hspace{-0.2in} & \\Theta_y = D_y - {L}_y, ~(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x}) \\in \\mathbb{H}' \\label{eqn:ConvexRelaxed231_N}\n\\end{eqnarray}\nHere $\\mathbb{H}' = W \\times T_y' \\times T_{yx}' \\times {\\mathbb{S}}^{q}$, where $T_y'$  is a subspace in ${\\mathbb{S}}^{p}$, and $T_{yx}'$ is a subspace in ${\\mathbb{R}}^{p \\times q}$. Let $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - L_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_x - \\Theta_x^\\star)$ denote the error in the estimated variables. In the following proposition, we bound the remainder term $R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))$ defined earlier. Before we proceed, we define the following norm on ${\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p \\times q} \\times {\\mathbb{S}}^q$ that is useful in our analysis:\n\n", "itemtype": "equation", "pos": 39705, "prevtext": "\nHere the symbol $\\otimes$ refers to the tensor product, and the Fisher information may be viewed as an operator from the parameter space ${\\mathbb{S}}^{p+q}$ to itself. From a mathematical programming perspective, the importance of the Fisher information can be seen from the fact that the Hessian of the negative-log-likelihood function \\eqref{eqn:main} evaluated at $\\Theta^\\star$ is given by $\\mathbb{I}^\\star$.\n\nTo ensure that the error term $\\|\\hat{\\Theta}-\\Theta^\\star\\|_2$ is small, a classical condition from the statistical estimation literature is to control the minimum gain of the Fisher information $\\mathbb{I}^\\star$ \\cite{Bickel}:\n\\begin{eqnarray}\n\\eta_1^\\star \\triangleq \\min_{M \\in {\\mathbb{S}}^{p+q}, \\|M\\|_{2} = 1} \\|\\mathbb{I}^{\\star}M\\|_{2}.\n\\label{eqn:eta1}\n\\end{eqnarray}\nFrom an optimization viewpoint, the condition that $\\eta_1^\\star$ is large is useful in ensuring that the negative-log-likelihood function at $\\Theta^\\star$ is sufficiently curved.\n\nTo further satisfy the requirements that $\\mathrm{rank}(\\hat{\\Theta}_{yx}) = \\mathrm{rank}(\\Theta^\\star_{yx}), ~ \\mathrm{rank}(\\hat{L}_y) = \\mathrm{rank}(L^\\star_y)$ with the convex relaxation \\eqref{eqn:main} for the composite approach and that $\\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$ with the relaxation \\eqref{eqn:main2} for the factor modeling approach, bounding the minimum gain quantity $\\eta_1^\\star$ \\eqref{eqn:eta1} from below is insufficient. To this end, we need to control two additional quantities associated with the Fisher information $\\mathbb{I}^\\star$. The first of these is the maximum inner-product between orthogonal elements in ${\\mathbb{S}}^{p+q}$ in the metric induced by the Fisher information $\\mathbb{I}^\\star$:\n\\begin{eqnarray}\n\\eta_2^\\star \\triangleq \\max_{\\substack{\\mathbb{W} \\subset {\\mathbb{S}}^{p+q} \\\\ \\mathbb{W}~\\mathrm{is~a~subspace}}} \\max_{M \\in \\mathbb{W}, \\|M\\|_2 \\leq 1} \\|\\mathcal{P}_{\\mathbb{W}^\\perp} \\mathbb{I}^{\\star}\\mathcal{P}_{\\mathbb{W}}(M)\\|_2\n\\label{eqn:eta2}\n\\end{eqnarray}\nAssuming that $\\eta_2^\\star$ is small ensures that errors in the estimation of the submatrix $\\Theta^\\star_{yx}$ do not impact the estimation of the $\\Theta^\\star_y$ submatrix (and vice versa). Indeed, in the absence of an upper bound on $\\eta_2^\\star$, the effect of the term ${\\mathcal{A}}^\\star x$ (represented by the column space of the submatrix $\\Theta^\\star_{yx}$) would not be distinguishable from the effects of the conditional factor model ${\\mathcal{B}}^\\star_u \\zeta_u + \\epsilon$ (represented by the submatrix $\\Theta^\\star_y$).\n\nThe final parameter associated to the Fisher information that we need to control for our main theorem is motivated by two concerns. First, we need to ensure that the diagonal and low-rank components $D^\\star_y$ and $L^\\star_y$ that compose the submatrix $\\Theta^\\star_y$ can be distinguished from each other (for Question $(2)$). For example, if the matrix $L^\\star_y$ consists of just a single nonzero in one of the diagonal entries and is zero elsewhere, identifying such a low-rank matrix from the difference $D^\\star_y - L^\\star_y$ is impossible. Second, we require that the components $D^\\star_y$ and $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ are identifiable given $D^\\star_y - L^\\star_y - \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$. These identifiability issues arising in the decomposition of sums of diagonal (and, more generally, sparse) and low-rank matrices have been investigated thoroughly \\cite{Chandrasekaran,Saunderson}. Specifically, the geometric insights in these papers imply that a natural condition to ensure identifiability in such decomposition problems is to assume the transversality of the intersection between the subspace of diagonal matrices in ${\\mathbb{S}}^p$ and the \\emph{tangent space} with respect to the algebraic variety of low-rank matrices at $L^\\star_y$ (for the composite factor model, i.e. Question $(1)$) or at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (for the factor model, i.e., Question $(2)$). In particular, the tangent space at a rank-$r$ matrix $N$ with respect to the algebraic variety of $p_1 \\times p_2$ matrices with rank less than or equal to $r$ is given by:\n\\begin{eqnarray*}\nT(N) &\\triangleq& \\{N_R + N_C | N_R, N_C \\in {\\mathbb{R}}^{p_1 \\times p_2}, \\\\&~& {\\mathrm{row}\\mbox{-}\\mathrm{space}}{N_R} \\subseteq {\\mathrm{row}\\mbox{-}\\mathrm{space}} {N}, {\\mathrm{column}\\mbox{-}\\mathrm{space}}{N_C} \\subseteq {\\mathrm{column}\\mbox{-}\\mathrm{space}}{N}\\}\n\\end{eqnarray*}\nThe tangent space at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ contains inside it the tangent space at $L^\\star_y$ as the row/column spaces of $L^\\star_y$ are contained inside the row/column spaces of $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (due to the assumption above that \\\\ ${\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\Theta^\\star_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(L_y^\\star) = \\{0\\}$); consequently, we assume that the set of diagonal matrices has a transverse intersection with the tangent space at $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ (which allows us to address the identifiability issues for both questions) by controlling the following parameter associated to the Fisher information $\\mathbb{I}^\\star$ for some $\\omega > 0$ and for all subspaces $\\tilde{T} \\subset {\\mathbb{S}}^p$ ``close to\u00e2\u0080\u0099\u00e2\u0080\u0099 $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$:\n\n\n\\begin{eqnarray}\n\\eta_3(\\tilde{T};\\omega) = \\max\\Bigg\\{\\max_{\\substack{M \\in \\tilde{T} \\\\ \\|M\\|_{2} \\leq 1}} \\|\\mathcal{P}_{\\mathrm{diag}}\\mathbb{I}^\\star_Y M\\|_{2}, \\max_{\\substack{M \\text{is diagonal} \\\\ \\|M\\|_{2} \\leq 1}} \\|\\mathcal{P}_{\\tilde{T}}\\mathbb{I}^\\star_Y M\\|_{2}\\Bigg\\} \\nonumber\\\\\n\\label{eqn:eta_3}\n\\end{eqnarray}\nHere the operator $\\mathcal{P}_{\\mathrm{diag}}$ represents projection onto the space of diagonal matrices, and $\\mathbb{I}^\\star_y = \\tilde{\\Theta}_y^{-1} \\otimes \\tilde{\\Theta}_y^{-1}$ represents the Fisher information with respect to the precision matrix $\\tilde{\\Theta}_y$ of the random vector $y$. The reason for considering the Fisher information with respect to the marginal precision matrix corresponding to $y$ is that the transversality conditions pertain only to the components of the precision matrix of $y$. We bound the quantity $\\eta_3(\\tilde{T};\\omega)$ for \\emph{all} spaces $\\tilde{T}$ that are within a small distortion of $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$:\n\n", "index": 17, "text": "\\begin{equation*}\n\\eta_3^\\star(\\omega) = \\max_{\\rho(\\tilde{T}, T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})) \\leq \\omega} \\eta_3(\\tilde{T};w),\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\eta_{3}^{\\star}(\\omega)=\\max_{\\rho(\\tilde{T},T(L^{\\star}_{y}+\\Theta^{\\star}_{%&#10;yx}(\\Theta^{\\star}_{x})^{-1}\\Theta^{\\star}_{xy}))\\leq\\omega}\\eta_{3}(\\tilde{T}%&#10;;w),\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>\u03b7</mi><mn>3</mn><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>max</mi><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>T</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>L</mi><mi>y</mi><mo>\u22c6</mo></msubsup><mo>+</mo><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u22c6</mo></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi mathvariant=\"normal\">\u0398</mi><mi>x</mi><mo>\u22c6</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>y</mi></mrow><mo>\u22c6</mo></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mi>\u03c9</mi></mrow></munder><mo>\u2061</mo><msub><mi>\u03b7</mi><mn>3</mn></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>T</mi><mo stretchy=\"false\">~</mo></mover><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nNotice this norm is a slight variant of the dual norm of the regularizer $\\|L_y\\|_{\\star} + \\gamma\\|\\Theta\\|_{\\star}$ in \\eqref{eqn:ConvexRelaxed231_N}.\n\n\n\n\\begin{proposition}\n\\label{prop:Remainder}\nLet $C' = (3 + \\gamma)\\psi$. If $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{1}{2C'}$, then $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq 2m{\\psi}C'^2 \\Phi_{\\delta,\\gamma}[\\Delta]^2$.\n\\end{proposition}\nNotice the bound on $R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))$ is dependent on the error term $\\Phi_{\\gamma}[\\Delta]$. In the following proposittion, we bound this error so that we can control the remainder term.  Specifically, suppose we let $T_y'$ and $T_{yx}'$ be tangent spaces to the low-rank matrix varieties and $\\rho(T_y', T(L_y^\\star)) \\leq \\omega$. Let $E_n = \\Sigma^\\star - \\Sigma_n$ denote the difference between the true joint covariance and the sample covariance and let $C_T = (0~, \\mathcal{P}_{{T_y'}^\\perp}(L_y^\\star), \\mathcal{P}_{{T_{yx}'}^\\perp}(\\Theta_{yx}^\\star),~0)$. The proof of the following result uses Brouwer's fixed-point theorem, and is inspired by the proof of a similar result in \\cite{Ravikumar,Chand2012}.\n\\begin{proposition}\n\\label{prop:Brower}\nDefine:\n\\begin{eqnarray}\nr = \\max\\Big\\{\\frac{4}{\\alpha}\\Big(\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}] +\\lambda_n\\Big),\\hspace{.1in} \\Phi_{\\gamma}[C_{T}]\\Big\\}\n\\label{eqn:rdef}\n\\end{eqnarray}\nIf $r \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$, then $\\Phi_{\\gamma}[\\mathcal{F}^\\dagger\\Delta] \\leq 2r$.\n\\end{proposition}\n\nIn the following proposition, we prove algebraic correctness of program \\eqref{eqn:ConvexRelaxed21_N}. The statement theorem relies on the following constants:\\\\\n$C_{\\sigma_{yx}}' = C_1^2\\psi^2\\max\\{12\\beta + \\frac{6\\beta}{\\gamma}, \\frac{2}{C_2\\psi^2} + \\frac{6\\beta}{\\gamma}\\}$, and $C_{samp}' = \\max\\{\\frac{1}{48\\psi\\beta},4C_2C', \\frac{32m\\psi{C}'^2C_2}{\\alpha},\\\\ 12\\beta{m}\\psi{C'}^2C_2^2\\}$.\n\\begin{proposition}\n\\label{proposition:lambdaBound}\nSuppose $\\gamma$ is chosen in the range specified in Theorem~\\ref{theorem:main} and  $\\sigma_y \\geq  \\frac{{m}}{\\omega}C_{\\sigma_y}\\lambda_n$, $\\sigma_{yx} \\geq  {m\\gamma^2}C_{\\sigma_{yx}}'\\lambda_n$. Further, suppose $\\lambda_n$ is chosen so that $\\lambda_n \\leq \\frac{1}{C_{samp}'}$. Then, there exists tangent space $T_y' \\subset {\\mathbb{S}}^{p}$ in the rank-$k_{u}$ variety ($k_u = \\text{rank}(L_y^\\star)$) and tangent space $T_{yx}' \\subset {\\mathbb{R}}^{p \\times q}$ in rank $k_{x}$-variety ($k_x = \\text{rank}(\\Theta_{yx}^\\star)$) where $\\rho(T_y', T(L_y^\\star)) \\leq \\omega$ such that the corresponding solution $(\\hat{\\Theta}, \\hat{S}_y, \\hat{L}_y)$ satisfies the following properties:\n\\begin{enumerate}\n\\item$\\text{rank}(\\hat{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\hat{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$\n\\item Letting $C_{T} = (0~,~\\mathcal{P}_{{T_y'}^\\perp}(L_y^\\star)~, ~\\mathcal{P}_{{T_{yx}'}^\\perp}(\\Theta_{yx}^\\star)~,~ 0)$, we have that $\\Phi_{\\gamma}[\\mathcal{F}^\\dagger \\mathbb{I}^\\star\\mathcal{F}(C_{T})] \\leq \\frac{\\lambda_n}{6\\beta}$ and $\\Phi_{\\gamma}[C_T] \\leq \\frac{16\\alpha}{3\\beta}\\lambda_n$\n\\item $\\Phi_{\\gamma}[\\Delta] \\leq 2C_1\\lambda_n$\n\\end{enumerate}\nFurthermore, suppose that $\\Phi_{\\gamma}(A^{\\dagger}E_n) \\leq \\frac{\\lambda_n}{6\\beta}$ and $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq \\frac{\\lambda_n}{6\\beta}$. Then the tangent space constraints $(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}'$ in \\eqref{eqn:ConvexRelaxed231_N} is inactive, so that  $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ is the unique solution of the original convex program \\eqref{eqn:main}.\n\\label{prop:rec}\n\\end{proposition}\n\nThus far, the analysis of the convex program so has been deterministic in nature. In the following proposition, we present the probabilistic component of our analysis by showning the rate at which the sample covariance matrix $\\Sigma_n$ converges to $\\Sigma^\\star$ in spectral norm. This result is well-known and is a specialization of a result proven by \\cite{Davidson}.\n\\begin{proposition}\n\\label{prop:Enbound}\nSuppose that the number of observed samples obeys \\\\$n \\geq 4608 \\beta^2m^2\\psi^2C_{samp}^2(p+q)$, and the regularization parameter $\\lambda_n$ is chosen in the range specified by Theorem 1. Then, with probability greater than $1 - 2\\text{exp}\\Big\\{-\\frac{n\\lambda_n^2}{4608\\beta^2m^2\\psi^2}\\Big\\}$, $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] \\leq \\frac{\\lambda_n}{6\\beta}$. \\label{prop:prob} \\end{proposition}\n\n\\subsection{Proof of Theorem 1} Noting that $C' \\leq 4\\psi{\\bar{m}}$, the constants $C_{\\sigma_{YX'}}$ and $C_{samp'}$ in Proposition~\\ref{prop:rec} can be related to constants $C_{\\sigma_{yx}}$ and $C_{samp}$ in Theorem 1 as follows:  $C_{\\sigma_{yx}'} \\leq mC_{\\sigma_{yx}}$ and $C_{samp}' \\leq m{\\bar{m}}^2C_{samp}$. Using these relations, it is easy to check that the assumptions of Theorem 1 imply that the assumptions of Proposition~\\ref{prop:rec} are satisfied. Thus we can conclude that the optimal solution $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ of \\eqref{eqn:ConvexRelaxed23_N} (with a particular choice of tangent spaces $T_y'$ and $T_{yx}'$) satisfy results of Proposition~\\ref{prop:rec}. Further, by appealing to Proposition~\\ref{prop:prob}, we have that $\\Phi_{\\gamma}(\\mathcal{F}^\\dagger{E_n})  \\leq \\frac{\\lambda_n}{6\\beta}$. If we show that $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] \\leq \\frac{\\lambda_n}{6\\beta}$, then we conclude that the unique optimum $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ of the original convex program \\eqref{eqn:main} has structurally correct structure (i.e. $\\text{rank}(\\hat{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\hat{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$). To show that  $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] \\leq \\frac{\\lambda_n}{6\\beta}$, we note that\n\\begin{eqnarray*}\n\\frac{4}{\\alpha}\\Big(\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}] +\\lambda_n \\Big) \\leq \\frac{4}{\\alpha} \\Big(\\frac{\\lambda_n}{6\\beta} + \\frac{\\lambda_n}{6\\beta} + \\lambda_n\\Big)\n\\leq \\frac{16\\alpha}{3\\beta} \\lambda_n \\\\ \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}\n\\end{eqnarray*}\nHere, we used the bound on $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}]$ provided by Proposition~\\ref{prop:rec} and the bound on $\\lambda_n$. Furthermore, appealing to Proposition~\\ref{prop:rec} once again, we have $\\Phi_{\\gamma}[C_{T}] \\leq \\frac{16\\alpha}{3\\beta}\\lambda_n \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$. Thus Proposition~\\ref{prop:Brower} provides us with the bound $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{32\\alpha}{3\\beta}\\lambda_n \\leq \\frac{1}{2C'}$.  We subsequently apply the results of Proposition~\\ref{prop:Remainder} to obtain:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq 2m{\\psi}C'^2 \\Phi_{\\delta,\\gamma}[\\Delta]^2 \\leq  \\Big[2m\\psi{C}'^2(\\frac{32\\alpha}{3\\beta})^2\\lambda_n\\Big] \\lambda_n \\leq \\frac{\\lambda_n}{6\\beta}\n\\end{eqnarray*}\nThe last inequality follows from the bound on $\\lambda_n$.\n\n\\section{Discussion} In this paper we describe a new approach for interpreting\nthe latent variables in a factor model.  Our method proceeds by\nobtaining observations of auxiliary covariates that may plausibly be\nrelated to the observed phenomena, and then suitably associating these\nauxiliary covariates to the latent variables.  The procedure involves\nthe solutions of computationally tractable convex optimization\nproblems, which are log-determinant semidefinite programs that can be\nsolved efficiently.  We give both theoretical as well as experimental\nevidence in support of our methodology.  Our technique generalizes\ntransparently to other families beyond factor models such as\nlatent-variable graphical models \\cite{Chand2012}, although we do not pursue\nthese extensions in the present article.\n\n\n\\begin{thebibliography}{7}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\n\n\n\n\n\\bibitem{Bickel}\n\\textsc{Bickel, P. $\\&$ Doksum, K.}\n\\newblock {{Mathematical Statistics, Basic Ideas and Selected Topics}}.\n\\newblock \\textit{Prentice-Hall}, 2007.\n\n\\bibitem{Buhlmann}\n\\textsc{B\\\"{u}hlmann, P $\\&$ van de Geer, S.}.\n\\newblock \\textit{{Statistics for high-dimensional data}}.\n\\newblock New York: Springer, 2011.\n\n\n\\bibitem{Candes}\n\\textsc{Cand\\`es, E. J. $\\&$ Recht, B.}\n\\newblock {{Exact matrix completion\nvia convex optimization}}.\n\\newblock \\textit{Foundation of Computational Mathematics}, 9:717--772, 2009.\n\n\n\\bibitem{Chandrasekaran}\n\\textsc{Chandrasekaran, V., Sanghavi, S., Parrilo, P. A. $\\&$ Willsky, A. S.}\n\\newblock {{Rank-sparsity incoherence\nfor matrix decomposition}}.\n\\newblock \\textit{SIAM Journal of Optimization}, 21:572--596, 2011.\n\n\\bibitem{Chand2012}\n\\textsc{Chandrasekaran, V., Parrilo, P. A. $\\&$ Willsky, A. S.}\n\\newblock {{Latent Variable Graphical Model Selection via Convex Optimization}}.\n\\newblock \\textit{Annals of Statistics}, 40:1935--1967, 2012.\n\n\n\\bibitem{Davidson}\n\\textsc{Davidson, K.R. $\\&$ Szarek, S.J.}\n\\newblock {{Local operator theory, random matrices and {B}anach spaces}}.\n\\newblock \\textit{Handbook of the Geometry of Banach Spaces}, 1:317--366, 2001.\n\n\n\\bibitem{Fazel}\n\\textsc{Fazel, M.}\n\\newblock {{Matrix rank minimization with applications}}.\n\\newblock \\textit{PhD thesis, Department of Electrical Engineering, Stanford\nUniversity}, 2002.\n\n\n\n\\bibitem{Hotelling}\n\\textsc{Hotelling, H.}\n\\newblock {{Relations between two sets of variants}}.\n\\newblock \\textit{Biometrika}, 28:321-377, 2002.\n\n\n\n\n\n\n\n\\bibitem{Kay}\n\\textsc{Kay, S.M.}\n\\newblock {{Modern spectral estimation, theory and application}}.\n\\newblock \\textit{Prentice-Hall}, 1988.\n\n\\bibitem{Ledermann}\n\\textsc{Ledermann, W.}\n\\newblock {{On a problem concerning matrices with variable diagonal elements}}.\n\\newblock \\textit{Proceeding of Royal Society Edinburgh}, 60:1-17, 1940.\n\n\n\\bibitem{Meshabi}\n\\textsc{Mesbahi, M. $\\&$ Papavassilopoulos, G.}\n\\newblock {{On the rank minimization problem over a positive semidefinite\nlinear matrix inequality}}.\n\\newblock \\textit{IEEE Transactions on Automatic Control}, 42:239-243, 1997.\n\n\n\\bibitem{Nataranjan}\n\\textsc{Natarajan, B.K.}\n\\newblock {{Sparse approximate solutions to linear systems}}.\n\\newblock \\textit{SIAM Journal of Computing}, 24:227-234, 1995.\n\n\n\\bibitem{Ravikumar}\n\\textsc{Ravikumar, P., Wainwright, M. J., Raskutti, G. $\\&$ Yu, B.}\n\\newblock{{High-dimensional covariance estimation by minimizing\n$\\ell_1$-penalized log-determinant divergence}}.\n\\newblock \\textit{Electronic Journal of Statistics}, 4:935--980, 2011.\n\n\\bibitem{Recht}\n\\textsc{Recht, B., Fazel, M. $\\&$ Parrilo, P. A.}\n\\newblock{Guaranteed minimum rank solutions to linear matrix equations via nuclear norm\nminimization}.\n\\newblock \\textit{SIAM Review}, 52:471--501, 2010.\n\n\n\\bibitem{Saunderson}\n\\textsc{Saunderson, J., Chandrasekaran, V., Parrillo, P $\\&$ Willsky, S.}\n\\newblock{Diagonal and low-rank matrix decompositions, correlation matrices, and ellipsoid fitting}.\n\\newblock \\textit{SIAM Journal on Matrix Analysis}, 33:1395-1416, 2012.\n\n\n\\bibitem{Shapiro1}\n\\textsc{Shapiro, A.}\n\\newblock{Rank-reducibility of a symmetric matrix and sampling theory of minimum trace\nfactor analysis}.\n\\newblock \\textit{Psychometrika}, 47:187-199, 1982.\n\n\\bibitem{S2}\n\\textsc{Shapiro, A.}\n\\newblock{Weighted minimum trace factor analysis}.\n\\newblock \\textit{Psychometrika}, 47:243-264, 1982.\n\n\\bibitem{S3}\n\\textsc{Shapiro, A.}\n\\newblock{Identifiability of factor analysis: Some results and open problems}.\n\\newblock \\textit{Linear Algebra Applications}, 15:201-292, 1904.\n\n\n\\bibitem{Spearman}\n\\textsc{Spearman, C.}\n\\newblock{`General inteligence', objectively determined and measured}.\n\\newblock \\textit{American Journal of Psychology}, 15:201-292, 1904\n\n\n\n\n\\bibitem{Toh}\n\\textsc{Toh, K. C, Todd, M. J. $\\&$ Tutuncu, R. H.}.\n\\newblock \\textit{SDPT3 - a\nMATLAB software package for semidefinite-quadratic-linear\nprogramming}. Available from\nhttp://www.math.nus.edu.sg/~mattohkc/sdpt3.html.\n\n\n\n\n\n\n\n\n\\bibitem{Wai2009}\n\\textsc{Wainwright, M. J.}\n\\newblock{Sharp thresholds for noisy and\nhigh-dimensional recovery of sparsity using $\\ell_1$-constrained\nquadratic programming (Lasso)}.\n\\newblock \\textit{IEEE Transactions on Information Theory}, 55:2183--2202, 2009.\n\n\\bibitem{Wainwright}\n\\textsc{Wainwright, M. J.}\n\\newblock{Structured regularizers for high-dimensional problems: Statistical and computational issues}.\n\\newblock \\textit{Annual Review of Statistics and its Applications}. 1:233--253, 2014.\n\n\n\\end{thebibliography}\n\n\n\n\\newpage\n\n\\section{Supplementary Material}\nIn the following Proposition, we appeal to conditions on $\\eta_1^\\star, \\eta_2^\\star, \\eta_3^\\star$ to prove a set of irrepresentability-type conditions on the population Fisher information $\\mathbb{I}^\\star$. \n\\begin{proposition}\n\\label{prop':irrep}\nSuppose that $\\eta_1^\\star \\geq 3\\alpha$, $\\eta_2^\\star \\leq \\frac{8\\alpha}{3\\beta}$ and $\\eta_3^\\star(\\omega) \\leq \\frac{2\\alpha}{\\beta}$ (where these conditions were defined in Section~\\ref{section:theoremstatement}). Further, suppose that the regularization parameter $\\gamma$ is chosen in the range specified in Theorem~\\ref{theorem:main}. Then we have that the following two conditions hold for $\\mathbb{H}' = W \\times T'_y \\times T_{yx}' \\times {\\mathbb{S}}^q \\subset {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p \\times q} \\times {\\mathbb{S}}^q$ where $\\rho(T(L_y^\\star),T'_y) \\leq \\omega$ and $T_{yx}'$ is any subspace in $\\mathbb{R}^{p \\times q}$:\n\\begin{enumerate}\n\\item The minimum gain of $\\mathbb{I}^\\star$ restricted to $\\mathbb{H}'$ is bounded below:\n\\begin{eqnarray}\n\\min_{\\substack{(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}' \\\\ \\Phi_{\\gamma}(D_y, L_y, \\Theta_{yx}, \\Theta_x) = 1}}  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'} {\\mathcal{I}}^{\\dagger} \\mathbb{I}^\\star {\\mathcal{I}} \\mathcal{P}_{\\mathbb{H}'}(D_y, L_y, \\Theta_{yx}, \\Theta_{x})] \\geq \\alpha\n\\label{eqn:Fisher1}\n\\end{eqnarray}\n\n\\item The effect of elements in $\\mathbb{H}'$ on the orthagonal complement $\\mathbb{H}'^{\\perp}$ is bounded above:\n\\begin{eqnarray}\n\\max_{{Z \\in \\mathbb{H}';\\hspace{.05in}  \\Phi_{\\gamma}(Z) = 1}}  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'^{\\perp}}{\\mathcal{I}}^{\\dagger} \\mathbb{I}^{\\star}{\\mathcal{I}}\\mathcal{P}_{{{\\mathbb{H}'}}} (\\mathcal{P}_{{{\\mathbb{H}'}}} {\\mathcal{I}}^{\\dagger}\\mathbb{I}^{\\star}{\\mathcal{I}}\\mathcal{P}_{{{\\mathbb{H}'}}})^{-1}(Z)] \\leq 1-\\frac{3}{\\beta+1}\n\\label{eqn:Fisher2}\n\\end{eqnarray}\n\\end{enumerate}\n\\label{prop:fisher}\n\\end{proposition}\nAlthough conditions \\eqref{eqn:Fisher1} and \\eqref{eqn:Fisher2} are satisfied for all subspaces $T_{yx}' \\subset {\\mathbb{R}}^{p\\times{q}}$, we specialize these to tangent spaces of low-rank matrix variety. Conditions \\eqref{eqn:Fisher1} and \\eqref{eqn:Fisher2} are analogous to conditions that play an important role in the analysis of the Lasso for sparse linear regression, graphical model selection via the Graphical Lasso \\cite{Ravikumar}, and in several other approaches for high-dimensional estimation. As a point of comparison with respect to analyses of the Lasso, the role of the Fisher information $\\mathbb{I}^\\star$ is played by $A^TA$, where $A$ is the underlying design\nmatrix. In analyses of both the Lasso and the Graphical Lasso in the papers referenced above, the analog of the subspace $\\mathbb{H}$ is the set of models with support contained inside the support of the underlying sparse population model. Assumptions 1 and 2 are also similar in spirit to conditions employed in the analysis of convex relaxation methods for latent-variable graphical model selection \\cite{Chand2012}.\\\\\n\n\\begin{proof} First, consider an arbitrary subspace $\\mathcal{S} \\in {\\mathbb{S}}^{p+q}$. Let $M \\in \\mathbb{W}$ with $\\|M\\|_2 = 1$. Then,\n\\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathcal{S}}\\mathbb{I}^\\star{M}\\|_{2} \\geq \\|\\mathbb{I}^\\star(M)\\|_{2} - \\|\\mathcal{P}_{\\mathbb{W}^\\perp}\\mathbb{I}^\\star\\mathcal{P}_{\\mathbb{W}}\n(M)\\|_{2} \\geq 3\\alpha - \\alpha(1-\\frac{3}{\\beta+1}) \\geq 2\\alpha\n\\end{eqnarray*}\nIn the subsequent discussion in this section, we employ the following notation to denote restrictions of a subspace $\\mathbb{H} = H_1 \\times H_2 \\times H_3 \\times H_4  \\subset \\mathbb{S}^{ p} \\times \\mathbb{S}^{p} \\times {\\mathbb{R}}^{p \\times q} \\times \\mathbb{S}^{q}$ (here $H_1,H_2,H_3,H_4$ are subspaces in $\\mathbb{S}^{ p},\\mathbb{S}^p,{\\mathbb{R}}^{p \\times q},\\mathbb{S}^q$, respectively) to its individual components. The restriction to the first component of $\\mathbb{H}$ is given by $\\mathbb{H}[1] = H_1 \\times \\{0\\} \\times \\{0\\} \\times \\{0\\} \\subset \\mathbb{S}^{ p} \\times \\mathbb{S}^{p} \\times {\\mathbb{R}}^{p \\times q} \\times \\mathbb{S}^{q}$.  The restrictions $\\mathbb{H}[2],\\mathbb{H}[3],\\mathbb{H}[4]$ to the other components of $\\mathbb{H}$ are defined in an analogous manner. Let $\\mathbb{H}' = W \\times T'_y \\times \\mathbb{R}^{p\\times{q}} \\times {\\mathbb{S}}^q$ with $\\rho(T'_y,T(L_y^\\star)) \\leq \\omega$. Recall, the subspace $W$ is the set of diagonal matrices with nonnegative entries. Consider a set of variables $(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}'$ with $\\|D_y\\|_{2} \\leq 1$, $\\|L_y\\|_2 \\leq 1$, $\\|\\Theta_{yx}\\|_2 \\leq \\gamma$, and $\\|\\Theta_{yx}\\|_2 \\leq 1$. Suppose equality holds in at least one of these set of inequalities so that  $\\Phi_{\\gamma}(D_y, L_y, \\Theta_{yx}, \\Theta_x) = 1$. Then, at least one of the following cases is active (the following results use conditions on $\\eta_1^\\star$, $\\eta_2^\\star$ and $\\eta_3^\\star(\\omega)$ and Proposition 1 (main paper)) :\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{enumerate}\n\\item If $\\|D_y\\|_{2} = 1$, then\n\\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathbb{H}'[1]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq \\Big[\\|\\mathcal{P}_{W}\\mathbb{I}_y^{\\star}(D_y)\\|_{2} - \\|\\mathcal{P}_{W}\\mathbb{I}_y^{\\star}(L)\\|_{2} \\\\ - \\|\\mathcal{P}_{\\mathbb{H}'[1]}\\mathbb{I}^{\\star}\\mathcal{F}(0, 0, \\Theta_{yx}, \\Theta_{x})\\|_{2}\\Big] \\geq 2\\alpha-\\eta_3^\\star-2\\eta_2^\\star\\max\\{\\gamma,1\\} \\\\\n\\geq 2\\alpha-\\frac{8\\alpha}{\\beta}\n \\end{eqnarray*}\n  \\item If $\\|L_y\\|_{2} = 1$, then\n  \\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathbb{H}'[2]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq \\Big[\\|\\mathcal{P}_{T_y'}\\mathbb{I}_y^{\\star}(L_y)\\|_{2} - \\|\\mathcal{P}_{T'_y}\\mathbb{I}_y^{\\star}(D_y)\\|_{2} \\\\ - \\|\\mathcal{P}_{\\mathbb{H}[2]}\\mathbb{I}^{\\star}\\mathcal{F}(0, 0, \\Theta_{yx}, \\Theta_{x})\\|_{2}\\Big] \\geq 2\\alpha-\\eta_3^\\star-2\\eta_2^\\star\\max\\{\\gamma,1\\} \\\\\n\\geq 2\\alpha-\\frac{8\\alpha}{\\beta}\n \\end{eqnarray*}\n{\\noindent}Similarly, one can show\n  \\item If $\\|\\Theta_{yx}\\|_{2} = \\gamma$, then $\\frac{1}{\\gamma} \\|\\mathcal{P}_{\\mathbb{H}[3]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq 2\\alpha - \\frac{8\\alpha}{\\beta}$\n \\item If $\\|\\Theta_{x}\\|_{2} = 1$, then $\\|\\mathcal{P}_{\\mathbb{H}[4]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq 2\\alpha - \\frac{8\\alpha}{\\beta}$\n\\end{enumerate}\nCombining these results, one can conclude that\n\n", "itemtype": "equation", "pos": 74494, "prevtext": "\nwhere the distortion $\\rho$ is measured via the following induced norm:\n\\begin{eqnarray*}\n\\rho(T_1,T_2) \\triangleq \\max_{\\|N\\|_2 \\leq 1} \\|(\\mathcal{P}_{T_1} - \\mathcal{P}_{T_2})(N)\\|_2.\n\\label{eqn:distortion}\n\\end{eqnarray*}\nThe reason for considering such distortions around the tangent space $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$ is that the variety of low-rank matrix are locally curved around their smooth points.  Consequently, the tangent spaces at matrices in a neighborhood around $L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy}$ having the same rank are generally not the same as $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{x})^{-1} \\Theta^\\star_{xy})$. In particular, the estimate $\\hat{\\tilde{L}}_y$ produced by the convex program \\eqref{eqn:main2} for the population low-rank matrix $L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy}$ may be such that $\\hat{\\tilde{L}}_y \\approx L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy}$ and ${\\mathrm{rank}}(\\hat{\\tilde{L}}_y) = {\\mathrm{rank}}(L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy})$, but it is generically the case that $T(\\hat{\\tilde{L}}_y) \\neq T(L^\\star_y + \\Theta^\\star_yx \\Theta^{-1}_{x} \\Theta^\\star_{xy})$.  Consequently, it is critical to control the quantity $\\eta_3(\\tilde{T};\\omega)$ in \\eqref{eqn:eta_3} for all $\\tilde{T}$ near the tangent space $T(L^\\star_y + \\Theta^\\star_{yx} (\\Theta^\\star_{yx})^{-1} \\Theta^\\star_{xy})$.\\\\\n\n\n\n\\subsection{Theorem Statements}\n\\label{section:theoremstatement}\nWe now describe the performance of the regularized maximum-likelihood programs \\eqref{eqn:main} and \\eqref{eqn:main2} under suitable conditions on the quantities introduced in the previous section. Before formally stating our main result, we introduce some notation. Let $\\sigma_y$ denote the minimum nonzero singular value of $L_y^\\star$, let $\\sigma_{yx}$ denote the minimum nonzero singular value of $\\Theta_{yx}^\\star$, and finally let $\\sigma$ denote the minimum nonzero singular value of $L_{y}^\\star + \\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}'$. In the following theorem statements, suppose that there exists $\\alpha > 0$, $\\beta \\geq 8$, and $\\omega \\in (0,1)$ such that the population Fisher information $\\mathbb{I}^\\star$ satisfies the following properties: $(i)~ \\eta_1^\\star \\geq 3\\alpha$, $(ii)~ \\eta_2^\\star \\leq \\frac{8\\alpha}{3\\beta}$, and $(iii)~\\eta_3^\\star(\\omega) \\leq \\frac{2\\alpha}{\\beta}$.  Theorem~\\ref{theorem:main} pertains to the consistency of the estimator \\eqref{eqn:main}, and Theorem~\\ref{theorem:main2} relates to the consistency of the estimator \\eqref{eqn:main2}.\n\n\\begin{theorem}\n\\label{theorem:main}\nLet $m \\triangleq \\max\\{1, \\frac{1}{\\gamma}\\}$, $\\bar{m} \\triangleq \\max\\{1, {\\gamma}\\}$, and $\\psi \\triangleq \\|(\\Theta^\\star)^{-1}\\|_2$. Further, $C_1 = \\frac{24}{\\alpha} + \\frac{1}{\\psi^2}$, $C_2 = \\frac{4}{\\alpha} (\\frac{1}{3\\beta} + 1)$, $C_{\\sigma_Y} = C_1^2\\psi^2\\max \\{ 12\\beta+1, \\frac{2}{{C_2}\\psi^2}+1\\}$, $C_{\\sigma_{YX}} = C_1^2\\psi^2\\max\\{18\\beta, \\frac{2}{C_2\\psi^2} + 6\\beta\\}$, $C_{samp} = \\max\\{\\frac{1}{48\\psi\\beta},{48{\\beta}{\\psi^3}C_1^2}, 8\\psi{C_2}, \\frac{64{\\psi^3}C_2}{\\alpha}\\}$, and $\\lambda_{\\text{upper}} = \\frac{1}{m\\bar{m}^2C_{samp}}$. Suppose that the following conditions hold:\n\\begin{enumerate}\n\\item $n \\geq \\frac{4608\\psi^2\\beta^2m^2(p+q)}{\\lambda_{\\text{upper}}^2}$; that is $n \\gtrsim \\Big[\\frac{\\beta^4}{\\alpha^2} m^4\\bar{m}^4\\Big] (p+q) $\n\\item $\\lambda_n \\in \\Big[\\sqrt{\\frac{4608\\psi^2\\beta^2m^2(p+q)}{n}}, \\lambda_{\\text{upper}}\\Big]$; \\hspace{.1in} e.g. $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\item $\\gamma \\in \\Big[1, \\frac{8\\alpha}{3\\beta\\eta_2^\\star}\\Big]$ \\\\\n\\item $\\sigma_{Y} \\geq \\frac{m}{\\omega_{}}C_{\\sigma_{Y}} \\lambda_n$; \\hspace{.05in} that is $\\sigma_{Y} \\gtrsim \\frac{\\beta^2}{\\alpha^2\\omega_{}}{m}{}\\sqrt{\\frac{p+q}{n_{}}}$ \\hspace{.05in} if \\hspace{.05in} $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\item $\\sigma_{YX} \\geq {m^2}{} C_{\\sigma_{YX}}\\gamma^2\\lambda_n $; \\hspace{.05in} that is $\\sigma_{YX} \\gtrsim {\\beta^2 \\gamma^2}{\\alpha^2}{m^2}{}\\sqrt{\\frac{p+q}{n}}$ \\hspace{.05in} if ~$\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$\n\\end{enumerate}\n\nThen with probability greater than $1-2\\exp\\{-\\frac{n\\lambda_n^2}{4608 \\beta^2 m^2 \\psi^2}\\}$, the optimal solution $(\\hat{\\Theta},\\hat{D}_y,\\hat{L}_y)$ of \\eqref{eqn:main} with i.i.d. observations $\\{y^{(i)}, x^{(i)}\\}_{i = 1}^n$ of $(y,x)$ satisfies the following properties:\n\\begin{enumerate}\n\\item rank($\\hat{L}_y$) = rank(${L}_y^\\star$), rank($\\hat{\\Theta}_{yx}$) = rank(${{\\Theta}^\\star_{yx}}$)\\[.005in]\n\\item $\\|\\hat{D}_y - D_Y^\\star\\|_{2} \\leq C_1\\lambda_n$, $\\|\\hat{L}_y - L_y^\\star\\|_{2} \\leq C_1\\lambda_n$, $\\|\\hat{\\Theta}_{yx} - \\Theta_{yx}^\\star\\|_{2} \\leq C_1\\lambda_n\\gamma$, and $\\|\\hat{\\Theta}_{x} - \\Theta_{x}^\\star\\|_{2} \\leq C_1\\lambda_n$; that is $\\|\\hat{D}_y - D_y^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{L}_y - L_y^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{\\Theta}_{yx} - \\Theta_{yx}^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}\\gamma{m}\\sqrt{\\frac{p+q}{n}}$, $\\|\\hat{\\Theta}_{x} - \\Theta_{x}^\\star\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{m}\\sqrt{\\frac{p+q}{n}}$ \\hspace{.05in} if \\hspace{.05in} $\\lambda_n \\sim \\beta{m}\\sqrt{\\frac{p+q}{n}}$.\n\\end{enumerate}\n\\end{theorem}\n\n\\begin{theorem}\n\\label{theorem:main2}\nDenote $\\tilde{\\psi} \\triangleq \\|(\\tilde{\\Theta}_y^\\star)^{-1}\\|_2$.  Let $C_1 = \\frac{24}{\\alpha} + \\frac{1}{\\tilde{\\psi}^2}$, $C_2 = \\frac{4}{\\alpha} (\\frac{1}{3\\beta} + 1)$, $C_{\\sigma} = C_1^2\\tilde{\\psi}^2\\max \\{ 12\\beta+1, \\frac{2}{{C_2}\\tilde{\\psi}^2}+1\\}$, $C_{samp} = \\max\\{\\frac{1}{48\\tilde{\\psi}\\beta},{48{\\beta}{\\tilde{\\psi}^3}C_1^2}, 8\\tilde{\\psi}{C_2}, \\frac{64{\\tilde{\\psi}^3}C_2}{\\alpha}\\}$, and $\\lambda_{\\text{upper}} = \\frac{1}{C_{samp}}$. Suppose that the following conditions hold:\n\\begin{enumerate}\n\\item $n \\geq \\frac{4608\\tilde{\\psi}^2\\beta^2m^2p}{\\lambda_{\\text{upper}}^2}$; that is $n \\gtrsim \\frac{\\beta^4}{\\alpha^2}p $\n\\item $\\tilde{\\lambda}_n \\in \\Big[\\sqrt{\\frac{4608\\tilde{\\psi}^2\\beta^2p}{n}}, \\lambda_{\\text{upper}}\\Big]$; \\hspace{.1in} e.g. $\\tilde{\\lambda}_n \\sim \\beta\\sqrt{\\frac{p}{n}}$\n\\item $\\sigma \\geq \\frac{1}{\\omega_{}}C_{\\sigma_{}} \\tilde{\\lambda}_n$; \\hspace{.1in} that is $\\sigma_{} \\gtrsim \\frac{\\beta^2}{\\alpha^2\\omega_{}}{}{}\\sqrt{\\frac{p}{n_{}}}$ \\hspace{.05in} if \\hspace{.05in} $\\tilde{\\lambda}_n \\sim \\beta \\sqrt{\\frac{p}{n}}$\n\\end{enumerate}\n\nThen with probability greater than $1-2\\exp\\{-\\frac{n\\tilde{\\lambda}_n^2}{4608 \\beta^2 \\tilde{\\psi}^2}\\}$, the optimal solution $(\\hat{\\tilde{D}}_y,\\hat{\\tilde{L}}_y)$ of \\eqref{eqn:main2} with i.i.d. observations $\\{y^{(i)}\\}_{i = 1}^n$ of $y$ satisfies the following properties:\n\\begin{enumerate}\n\\item rank($\\hat{\\tilde{L}}_y$) = rank($L_y^\\star + \\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}^{-1}$)\\[.005in]\n\\item $\\|\\hat{\\tilde{D}}_y - D_y^{\\star}\\|_{2} \\leq C_1\\tilde{\\lambda}_n$; $\\|\\hat{\\tilde{L}}_y - L_y^{\\star}-\\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}\\|_{2} \\leq C_1\\tilde{\\lambda}_n$; that is $\\|\\hat{\\tilde{D}}_y - D^{\\star}_y\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{}\\sqrt{\\frac{p}{n}}$ and $\\|\\hat{\\tilde{L}}_y - L_y^{\\star}-\\Theta_{yx}^\\star{\\Theta_{x}^\\star}^{-1}{\\Theta_{yx}^\\star}\\|_{2} \\lesssim \\frac{\\beta}{\\alpha}{}\\sqrt{\\frac{p}{n}}$.\n\\end{enumerate}\n\\end{theorem}\nWe outline the proof of Theorem~\\ref{theorem:main} in Section~\\ref{section:proofs}. We omit the proof of Theorem~\\ref{theorem:main2} as it follows a very similar sequence of steps to that of Theorem~\\ref{theorem:main}. The quantities $\\alpha, \\beta, \\omega$ as well as the choices of parameters $\\gamma$ play a prominent role in our results. Indeed larger values of $\\alpha, \\omega$ and smaller values of $\\beta$ (leading to a better conditioned Fisher information even for large distortions around the tangent space $T(L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{xy}^\\star})$) lead to less stringent requirements on the sample complexity, on the minimum nonzero singular value of $\\sigma_{y}$ of $L_y^\\star$, on the minimum nonzero singular value of $\\sigma$ of $L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{xy}^\\star}$, and on the minimum nonzero singular value $\\sigma_{yx}$ of $\\Theta_{yx}^\\star$.\n\n\n\\subsection{Synthetic Simulations}\n\\label{section:simulation}\nIn this section, we give experimental evidence for the consistency of our convex program~\\eqref{eqn:main} and~\\eqref{eqn:main2} on synthetic examples. Our choices of regularization parameters $\\lambda_n$ and $\\gamma$ (in the convex program \\eqref{eqn:main}) and $\\tilde{\\lambda}_n$ (in the convex program \\eqref{eqn:main2}) are guided by Theorem~\\ref{theorem:main} and Theorem~\\ref{theorem:main2}. Specifically, the parameters $\\lambda_n$ and $\\tilde{\\lambda}_n$ are to be set in a manner that depends on the number of observations $n$, and we choose these to be proportional to $\\sqrt{\\frac{p+q}{n}}$ and $\\sqrt{\\frac{p}{n}}$, respectively. Further, the theorems suggest that we should expect the ranks of the estimated solutions $\\hat{L}_y$ and $\\hat{\\Theta}_{yx}$ to be the same for a range of values of $\\gamma$ when the correct underlying model structure is identified. Therefore, we solve the convex program \\eqref{eqn:main} for several values of $\\gamma$, and choose a value for which the structure of the estimated model remains the same for the largest contiguous range of values of $\\gamma$. To solve the convex programs \\eqref{eqn:main} and \\eqref{eqn:main2} numerically, we use the special-purpose solver LogDetPPA \\cite{Toh}. \\\\\n\n\n\nWe generate the population model $\\Theta^\\star = \\begin{pmatrix} D_y^\\star - L_y^\\star & \\Theta_{yx}^\\star \\\\ {\\Theta_{yx}^\\star}' & \\Theta_{x}^\\star\\end{pmatrix}$ as follows: we fix the number of responses to be $p = 40$ and the number of covariates to be $q = 10$. We then generate a random Gaussian matrix $W \\in {\\mathbb{R}}^{p \\times k_u}$ and let $L_y^\\star =  WW'$. We let $D_y^\\star = \\mu{I}_{p}$ where $\\mu$ is chosen to be twice the largest eigenvalue of $L_y^\\star$. We also generate random Gaussian matrices $\\tilde{W} \\in {\\mathbb{R}}^{p \\times k_u}$ and $Z \\in {\\mathbb{R}}^{q \\times k_u}$ and let $\\Theta_{yx}^\\star = \\tilde{W}Z'$. Finally, we let $\\Theta_x^\\star = \\Theta_{xy}^\\star{\\Theta_{y}^\\star}^{-1}\\Theta_{yx}^\\star + \\delta{I}_{q}$ where $\\delta$ is chosen large enough so that the overall matrix $\\Theta^\\star$ has a condition number that is relatively small (in our examples, we chose $\\delta$ so that the condition number is less than $20$). This approach generates a composite factor model~\\eqref{eqn:composite} with $\\text{rank}({\\mathcal{A}}) = k_x$ and $\\tilde{{\\mathcal{B}}} \\in {\\mathbb{R}}^{p \\times k_u}$, and a factor model~\\eqref{eqn:factormodel} with $k = k_x + k_u$ latent factors. We obtain three models with $(k_x, k_u) = (1,1)$, $(k_x, k_u) = (1,2)$, and $(k_x, k_u) = 2$. For each model, we generate $n$ samples of responses $y$ and covariates $x$, and use these observations as input to convex programs~\\eqref{eqn:main} and~\\eqref{eqn:main2}. Figure~\\ref{fig:recovery}(a) shows the probability of obtaining structurally correct estimates of the factor model parameters using \\eqref{eqn:main2} (i.e. $\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(L_y^\\star + \\Theta_{yx}^\\star\\Theta_{x}^{-1}{\\Theta_{yx}^\\star}')$) and Figure~\\ref{fig:recovery}(b) shows the probability of obtaining structurally correct estimates of the composite factor model (i.e. $\\text{rank}(\\hat{{L}}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\Theta_{yx}^\\star) = \\text{rank}(\\hat{\\Theta}_{yx})$). These probabilities are evaluated over $10$ experiments for each value of $n$. These results agree with our theoretical results that given (sufficiently many) samples of responses/covariates, the convex programs \\eqref{eqn:main2} and \\eqref{eqn:main} provide structurally correct estimates of a factor model \\eqref{eqn:factormodel} and composite factor model \\eqref{eqn:composite}, respectively. \n\\FloatBarrier\n\\begin{figure}[!http]\n\\centering\n\\subfigure[factor model]{\n\\includegraphics[width=5cm, height = 5cm]{foo}\n}\n\\subfigure[composite factor model]{\n\\includegraphics[width=5cm, height = 5cm]{foo3}\n}\n\\caption{Synthetic data: plot shows probability of correct structure recovery in factor model and composite factor model. The three models studied are $(i)~(k_x, k_u) = (1,1) $, $(ii)~(k_x, k_u) = (2,1)$, and $(iii)~(k_x, k_u) = (2,2)$. For each plotted point, the probability of structurally correct estimation is obtained over $10$ trials.} \n\\label{fig:recovery}\n\\end{figure}\n\\FloatBarrier\n\n\n\n\\section{Identifying Latent Factors Influencing Stock Returns}\n\\label{section:experiments}\nRecall from Section~\\ref{section:results} that our approach to associate semantics to latent variables in a factor model is a two-stage process. In the first stage, we identify a factor model based on observations of $y$ using the convex relaxation \\eqref{eqn:main2}, which results in an estimate $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$. In the the second step, we use simultaneous observations of $y$ and some additional covariates $x$ to identify a composite factor model using the convex relaxation \\eqref{eqn:main} with the resulting estimates being $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$. As discussed in Section~\\ref{section:theorem} the composite factor model of $(y,x)$ offers an interpretation of the latent variables of the factor model underlying $y$ if $(i)~ \\mathrm{rank}(\\hat{\\tilde{L}}_y) = \\mathrm{rank}(\\hat{L}_y + \\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy})$, $(ii)~{\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\hat{\\Theta}_{yx}) \\cap {\\mathrm{column}\\mbox{-}\\mathrm{space}}(\\hat{L}_{y}) = \\{0\\}$, and \\\\$(iii) \\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ is small.\nThese observations naturally lead to the following algorithmic approach:\n\\FloatBarrier\n\\begin{algorithm}\n\\caption{Interpreting Latent Variables in a Factor Model}\n\\begin{algorithmic}[1]\n\\STATE {\\bf Input}: A collection of observations $\\{(y^{(i)}, x^{(i))}\\}_{i=1}^n \\subset {\\mathbb{R}}^p \\times {\\mathbb{R}}^q$ of the variables $y$ and of some auxiliary covariates $x$.\\\\\n\\vspace{.04in}\n\\STATE{\\bf Factor Modeling}: Supply observations $\\{y^{(i)}\\}_{i=1}^n$ to the convex program \\eqref{eqn:main2} to learn a factor model with parameters $(\\hat{\\tilde{D}}_y, \\hat{\\tilde{L}}_y)$.\\\\\n\\vspace{.04in}\n\\STATE {\\bf Composite Factor Modeling}: For each $d = 1, \\dots, q$, sweep over parameters $(\\lambda_n, \\gamma)$ in the convex program \\eqref{eqn:main} (with $\\{y^{(i)}, x^{(i))}\\}_{i=1}^n$ as input) to identify composite models with estimates $(\\hat{\\Theta},\\hat{D}_y,\\hat{L}_y)$ that satisfy the following three properties: $(i)~\\text{rank}(\\hat{\\Theta}_{yx}) = d$, $(ii)$~$\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(\\hat{L}_y) + \\text{rank}(\\hat{\\Theta}_{yx})$, and $(iii)~\\text{rank}(\\hat{\\tilde{L}}_y) = \\text{rank}(\\hat{L}_y + \\text{rank}(\\hat{\\Theta}_{yx}\\hat{\\Theta}_{x}^{-1}\\hat{\\Theta}_{xy}))$. \\\\\n\\vspace{.04in}\n\\STATE{\\bf Identifying Subspace}: For each $d = 1, \\dots, q$ and among the candidate composite models (from the previous step), choose the composite factor model that minimizes the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$.\\\\\n\\vspace{.04in}\n\\STATE{\\bf Output}: For each $d = 1,\\dots q$, the $d$-dimensional projection of $x$ into the row-space of $\\hat{\\Theta}_{yx}$ represents the interpretable component of the latent variables in the factor model.\n\\end{algorithmic}\n\\end{algorithm}\n\\FloatBarrier\nWe note that in some cases, a factor model is obtained off-line based on scientific considerations. In these situations, one would proceed to step 3 of the algorithm. Typically, however, factor models are learned in a data-driven approach based on observations of responses $\\{(y^{(i)}\\}_{i=1}^n$. We from step 2 of the algorithm that our approach to learn a factor model via the convex program \\eqref{eqn:main2} requires the specification of the regularization parameter $\\tilde{\\lambda}_n$. In our experimental results on the financial asset dataset, we choose this parameter via cross-validation techniques. Further, we note that the effectiveness of this algorithm in identifying semantics to latent variables in the input factor model is largely dependent on the size of the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{xy}]\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$. Indeed, the smaller this quantity, the more confidence one should have that the composite factor model has attributed meaning to some component of the latent variables in the factor model.\n\n\\subsection{Experimental Results on Financial Asset Data}\nWe demonstrate the utility of our algorithmic approach in a financial asset data analysis problem. Specifically, we consider as our responses $y$ the monthly stock return of $p = 66$ companies from the Standard and Poor index over the period June $1990$ to July $2014$, which leads to a total of $n = 277$ observations. We also obtain monthly observations over the same period of the following $q = 11$ covariates, consumer price index, EUR to USD exchange rate, federal reserve rate, gold prices, industrial production, inflation rate, mortgage rate, oil exports, oil imports, treasury rate, and unemployment rate. These covariates were chosen because they plausibly influence the values of stock prices. For the purposes of our experiments, we set aside a random subset of $n_{\\mathrm{train}} = 227$ of these observations as a training set and the remaining subset of $n_{\\mathrm{test}} = 50$ as the test set. Similar to synthetic simulations in Section~\\ref{section:simulation}, we solve the convex program~\\eqref{eqn:main} and~\\eqref{eqn:main2} numerically using the LogDetPPA package \\cite{Toh}.\\\\\n\nWe begin by the second step of our algorithm which is to identify a factor model~\\eqref{eqn:factormodel} that is well-suited for modeling stock returns. We find such a factor model by solving the convex program~\\eqref{eqn:main2} where the regularization parameter $\\tilde{\\lambda}_n$ is chosen via cross-validation. Concretely, for a particular choice of $\\tilde{\\lambda}_n$, we use the training set $\\{y_{{\\mathrm{train}}}^{j}\\}_{j = 1}^{222} \\in \\mathbb{R}^{67}$ as input to the convex program \\eqref{eqn:main}, and solve \\eqref{eqn:main} to obtain a factor model specified by $(\\hat{\\tilde{D}}_y , \\hat{\\tilde{L}}_y)$. We then compute the average log-likelihood over the testing set $\\{y_{{\\mathrm{test}}}^{j}\\}_{j = 1}^{50} \\in \\mathbb{R}^{67}$ using the distribution specified by the precision matrix $\\hat{\\tilde{D}}_y - \\hat{\\tilde{L}}_y$. We perform this procedure as we vary $\\tilde{\\lambda}_n$ from $0.04$ to $4$ in increments of $0.004$. Figure~\\ref{fig:testp} shows a plot of $\\text{rank}(\\hat{\\tilde{L}}_y))$ (i.e. number of latent factors) vs. average log-likelihood performance on the testing set. Notice that fixing the number of latent factors does not lead to a unique factor model as varying the regularization parameter $\\tilde{\\lambda}_n$ may lead to a change in the estimated model, but no change in its structure (i.e. $\\text{rank}(\\hat{\\tilde{L}}_y)$ remains the same). As larger values of average log-likelihood are indicative of a better fit to test samples, these results suggest that $12$ or $13$ latent factors influence stock prices. We thus focus on associating semantics to the factor model with the largest average log-likelihood performance that consists of $12$ latent factors, and the factor model with the largest average log-likelihood performance that consists of $13$ latent factors.\n\\FloatBarrier\n\\begin{figure}[!http]\n\\centering\n\\includegraphics[width=5cm, height = 5cm]{trainingperformance}\n\\caption{Number of latent factors vs. average log-likelihood over testing set. These results are obtained by sweeping over parameters $\\tilde{\\lambda}_n \\in [0.04, 4]$ in increments of $0.004$ and solving the convex program~\\eqref{eqn:main2}}. \n\\label{fig:testp}\n\\end{figure}\n\\FloatBarrier\nWe now proceed with the third step of our algorithm. Using joint observations of responses and covariates $\\{y^j_{{\\mathrm{train}}}, x^{j}_{{\\mathrm{train}}}\\}_{j = 1}^{227}$ as input to the convex program~\\eqref{eqn:main} , we perform an exhaustive sweep over parameter space $(\\lambda_n, \\gamma)$ to learn composite models with estimates $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ such that $\\text{rank}(\\hat{\\Theta}) = 1,2,\\dots 11$, and $\\text{rank}(\\hat{L}_y) = 1,2,\\dots 12$. As we are interested comparing these composite models to the factor model with $12$ or $13$ latent variables, we finely grid the parameter space $(\\lambda_n, \\gamma)$ so that there are a large number of models for which $\\text{rank}(\\hat{\\Theta}) + \\text{rank}(\\hat{L}_y)$ is equal to 12 or 13. Among these models, we restrict to those that satisfy the conditions of step 3 of the algorithm. Table~\\ref{table:nummodels} shows the number of models that satisfy these conditions for $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$. For each $d = 1,\\dots,11$, we then identify the composite factor model which minimizes the quantity $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$. Table 2 and Table 3 show the values of this quantity for $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$ with respect to the factor model with $12$ and $13$ latent variables, respectively.\n\n\\FloatBarrier\n\\begin{table}[ht]\n\n\\centering \n\\begin{tabular}{c c} \n\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\#$ models satisfying conditions of step 2. \\\\\n\\hline \n(1,11) & 261 \\\\ \n(1,12) & 174 \\\\\n(2,10) & 84 \\\\\n(2,11) & 126 \\\\\n(3,9) & 112\\\\\n(3,10) & 84 \\\\\n(4,8) & 144 \\\\\n(4,9) & 72 \\\\\n(5,7) & 4 \\\\\n(5,8) & 64 \\\\\n\\hline \n\\end{tabular}\n\\caption{Number of composite factor models with $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,\\dots,5$ that satisfy the requirements of step 2 in the algorithm description at the beginning of this section (for the factor model with $12$ or $13$ latent variables).}\n\\label{table:nummodels} \n\\end{table}\n\\FloatBarrier\n\\FloatBarrier\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c} \n\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$\\[0.5ex]\n\\hline \n(1,11) & 0.08 \\\\ \n(2,10) & 0.17 \\\\\n(3,9) & 0.26 \\\\\n(4,8) & 0.31 \\\\\n(5,7) & 0.43 \\\\ [1ex] \n\\hline \n\\end{tabular}\n\\label{table:deviation12} \n\\caption{Deviation of the candidate composite factor model from the factor model consisting of $12$ latent variables } \n\\end{table}\n\\FloatBarrier\n\n\\FloatBarrier\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c } \n\\hline\\hline \n$(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y}))$ & $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ \\\\ [0.5ex] \n\n\\hline \n(1,12) & 0.004 \\\\ \n(2,11) & 0.08 \\\\\n(3,10) & 0.17 \\\\\n(4,9) & 0.26 \\\\\n(5,8) & 0.31 \\\\ [1ex] \n\\hline \n\\end{tabular}\n\\label{table:deviation13} \n\\caption{Deviation of the candidate composite factor model from the factor model consisting of $13$ latent variables } \n\\end{table}\n\\FloatBarrier\nFocussing on the case corresponding to which we identified a $13$-factor model underlying $y$, the results of Table 3 suggest that we identify a $2$-dimensional interpretable component of the $13$ latent variables as the deviation $\\max\\{\\|\\hat{\\tilde{D}}_y - \\hat{D}_y \\|_2 /\\|\\hat{\\tilde{D}}_y\\|_2, \\|\\hat{\\tilde{L}}_y - \\hat{L}_y-\\hat{\\Theta}_{yx}\\hat{\\Theta}_x^{-1}\\hat{\\Theta}_{yx}']\\|_2 / \\|\\hat{\\tilde{L}}_y\\|_2\\}$ on the right-hand-side of this table is small as long as $\\text{rank}(\\hat{\\Theta}_{yx}) = 1,2$.  For $\\text{rank}(\\hat{\\Theta}_{yx}) = 3,4,5$, the deviation appears to be quite large and may not lead to meaningful conclusions.\n\nAs a final step of the algorithm, we investigate the properties of the two-dimensional row-space of $\\hat{\\Theta}_{yx}$ to shed some light on those covariates that appear to play a significant role in capturing some of the latent phenomena in the $13$-factor model.  In particular, for the composite factor model with $(\\text{rank}(\\hat{\\Theta}_{yx}), \\text{rank}(\\hat{L}_{y})) = (2,11)$ (second row in Table 3), we let $V \\in {\\mathbb{R}}^{11 \\times 2}$ denote a matrix with orthogonal, unit-norm columns such that $V$ the columns of $V$ form a basis for the row space of $\\hat{\\Theta}_{yx}$ (such a matrix may be computed, for example, via the singular value decomposition).  Recall that $V\u00e2\u0080\u0099x$ represents the component of the $13$ latent variables that is interpretable via the covariates $x$.  We then consider the Euclidean-squared-norm  of the $i$-th row of $V$, as this specifies the relative strength of the $i$-th covariate. As shown in Table~\\ref{table:covariaterelevance}, all covariates have some contribution (as we allow general linear combinations of the covariates $x$ in the composite factor model~\\eqref{eqn:composite}). However, the covariates exchange rate, inflation rate, and oil imports seem to be the most relevant, and the covariates gold prices and oil exports seem to be the least relevant.\n\n\\begin{table}[ht]\n\\centering \n\\begin{tabular}{c c} \n\\hline\\hline \ncovariate & strength\\\\ [0.5ex] \n\n\\hline \nCPI & 0.07 \\\\\nexchange rate & 0.18 \\\\\nfederal reserve rate & 0.06\\\\\ngold prices & 0.04 \\\\\nindustrial production & 0.09 \\\\\ninflation rate & 0.15 \\\\\nmortgage rate & 0.07 \\\\\noil exports & 0.02 \\\\\noil imports & 0.15\\\\\ntreasury rate & 0.08\\\\\nunemployment & 0.07 \\\\\n\\hline \n\\end{tabular}\n\\caption{Strength of each covariate in the composite factor model with $2$-dimensional projection of covariates and $11$ latent variables} \n\\label{table:covariaterelevance} \n\\end{table}\n\n\n\n\n\n\n\\section{Proofs of Main Results}\n\\label{section:proofs}\n\n\n\n\n\\subsection{Proof Strategy}\n\nUnder assumptions of Theorem 1, we construct appropriate primal feasible variables $(\\hat{\\Theta},\\hat{D}_y, \\hat{L}_y)$ that satisfy the conclusions of the theorem - i.e., $\\hat{\\Theta}_{yx}$, $\\hat{L}_y$ are low-rank (with the same ranks as the underlying population quantities $\\Theta_{yx}^\\star$ and $L_y^\\star$) - and for which there exists a corresponding dual variable certifying optimality. This proof technique is sometimes also referred to as a primal-dual witness or certificate approach \\cite{Wai2009}. The high-level proof strategy is similar in spirit to the proofs of consistency results for sparse graphical model recovery \\cite{Ravikumar} and latent variable graphical model recovery \\cite{Chand2012}, although our convex program and the conditions required for its success are different from these previous results`. Consider the following convex program\n\n\\begin{eqnarray}\n(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y) = \\arg\\min_{\\substack{\\Theta \\in {\\mathbb{S}}^{p+q}, ~\\Theta \\succ 0 \\\\ D_y,L_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{y^{(i)},x^{(i)}\\}_{i=1}^n) + \\lambda_n [\\gamma\\|\\Theta_{yx}\\|_{\\star} + \\|L_y\\|_{\\star}] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_{y} = D_y - L_y, D_y ~\\mathrm{is~diagonal}\n\\label{eqn:ConvexRelaxed_N}\n\\end{eqnarray}\n\nComparing \\eqref{eqn:ConvexRelaxed_N} with the convex program \\eqref{eqn:main}, the difference is that we no longer constrain ${L}_y$ to be a positive semidefinite matrix. In particular, if ${L}_y \\succeq 0$, then the nuclear norm of the matrix ${L}_y$ in the objective function of \\eqref{eqn:ConvexRelaxed23_N} reduces to the trace of $L_y$. We show in the appendix that with high probability, the matrix $\\tilde{L}_y$ is positive semidefinite. Standard convex analysis states that $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ is the solution of the convex program \\eqref{eqn:ConvexRelaxed_N} if there exists a dual variable $\\Lambda \\in {\\mathbb{S}}^p$ with the following optimality conditions being satisfied:\n\\begin{eqnarray*}\n[\\Sigma_n - {\\hat{\\Theta}}^{-1}]_y + \\Lambda = 0&;& \\hspace{.1in} [\\Sigma_n - {\\hat{\\Theta}}^{-1}]_y \\in \\lambda_n\\partial\\|\\hat{L}_y\\|_\\star\\[.01in]\n[\\Sigma_n - {\\hat{\\Theta}}^{-1}]_{yx}  \\in -\\lambda_n\\gamma\\partial\\|\\hat{\\Theta}_{yx}\\|_{\\star}&;&  \\hspace{.1in} [\\Sigma_n - {\\hat{\\Theta}}^{-1}]_{x}  = 0\\\\ \\hat{\\Theta}_y = \\hat{D}_y - \\hat{L}_y; \\hspace{.1in} \\hat{D}_y \\text{ is diagonal}&;& \\hspace{.1in} \\Lambda_{i,i} = 0 ~ \\text{ for } i = 1,2,\\dots p\n\\end{eqnarray*}\nRecall that elements of the subdifferential with respect to nuclear norm at a matrix $M$ have the key property that they decompose with respect to the tangent space $T(M)$. Specifically, the subdifferential with respect to the nuclear norm at a matrix $M$ with (reduced) SVD given by $M = UQV^T$ is as follows:\n\\begin{eqnarray*}\nN \\in \\partial\\|M\\|_{\\star} \\Leftrightarrow \\mathcal{P}_{T(M)} (N) = UV^T ~, ~ \\|\\mathcal{P}_{T(M)} (N)\\|_2 \\leq 1,\n\\end{eqnarray*}\nwhere $\\mathcal{P}$ denote a projection operator. Let us denote the subspace $W \\in {\\mathbb{S}}^p$ as the set of diagonal matrices with nonnegative entries. Let SVD of $\\hat{L}_y$ and $\\hat{\\Theta}_{yx}$ be given by $\\hat{L}_y = \\bar{U}\\bar{Q}\\bar{V}'$ and $\\hat{\\Theta}_{yx} = \\breve{U}\\breve{Q}{\\breve{V}}'$ respectively, and $Z \\triangleq  (0, \\hspace{.1in} \\lambda_n\\bar{U}\\bar{V}', \\hspace{.1in}  -\\lambda_n\\gamma_{}{\\breve{U}}{\\breve{V}}',  \\hspace{.1in} 0)$. Setting $\\Lambda  = [\\Sigma_n - \\hat{\\Theta}^{-1}]_{Y, \\text{off diagonal}}$, and letting $\\mathbb{H} = W \\times T(\\hat{L}_y) \\times T(\\hat{\\Theta}_{yx}) \\times {\\mathbb{S}}^q$, the optimality conditions of \\eqref{eqn:ConvexRelaxed_N} can be reduced to:\n\\begin{center}\n\\begin{enumerate}\n\\item $\\mathcal{P}_{\\mathbb{H}}\\mathcal{F}^{\\dagger}(\\Sigma_n - \\hat{\\Theta}^{-1}) = Z$\n\\item $\\|\\mathcal{P}_{T(\\hat{L}_y)^\\perp} (\\Sigma_n - \\hat{\\Theta}^{-1})_y\\|_2 < \\lambda_n$;  $\\|\\mathcal{P}_{T(\\hat{\\Theta}_{yx})^\\perp} (\\Sigma_n - \\hat{\\Theta}^{-1})_{yx}\\|_2 < \\lambda_n\\gamma$\n\\end{enumerate}\n\\end{center}\n\nOur analysis proceeds by constructing variables $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ that satisfy the optimality conditions specified above. Consider the optimization program \\eqref{eqn:ConvexRelaxed_N} with additional (nonconvex) constraints that $L_y$ and $\\Theta_{yx}$ belong to algebraic variety of low rank matrices specified by $L_y^\\star$ and $\\Theta_{yx}^\\star$. While this new program is nonconvex, it has a very interesting property that at the global optimal solution (and indead at any locally optimal solution) $\\tilde{L}_y$ and $\\tilde{\\Theta}_{yx}$ are smooth points of their respective algebraic varieties. This observation suggests that the Lagrange multipliers corresponding to the additional variety constraints belongs to $T(\\tilde{L}_y)^\\perp$ and $T(\\tilde{\\Theta}_{yx})^\\perp$ respectively. We show under suitable conditions that $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ also satisfy the second optimality condition of \\eqref{eqn:ConvexRelaxed_N} corresponding to the tangent spaces $T(\\tilde{L}_y)^\\perp$ and $T(\\tilde{\\Theta}_{yx})^\\perp$. Thus $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ is a unique solution of \\eqref{eqn:main} and as constructed, is algebraically consistent (i.e. $\\text{rank}(\\tilde{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\tilde{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$)\n\n\n\\subsection{Results proved in the supplementary material}\nTo ensure that the estimate $\\hat{\\Theta}$ is close to the population quantity $\\Theta^\\star$, the quantity $E = \\hat{\\Theta} - \\Theta^\\star$ must be small. Since the optimality conditions of \\eqref{eqn:ConvexRelaxed_N} are stated in terms of $\\hat{\\Theta}^{-1}$, we bound the deviation between $\\hat{\\Theta}^{-1}$ and ${\\Theta^\\star}^{-1}$. Specifically, the Taylor series expansion of $\\hat{\\Theta}^{-1}$ around $\\Theta^\\star$ is given by:\n\\begin{eqnarray*}\n\\hat{\\Theta}^{-1} = (\\Theta^\\star+ E)^{-1} = {\\Theta^{\\star}}^{-1} + {\\Theta^{\\star}}^{-1}E{\\Theta^{\\star}}^{-1} + R_{\\Sigma^\\star}(E)\n\\end{eqnarray*}\nwhere, $R_{\\Sigma^\\star}(E) = \\Sigma^\\star\\Big[\\sum_{k = 2}^{\\infty}(-E\\Theta^\\star)^k\\Big]$. Recalling that $\\mathbb{I}^\\star = \\Theta^\\star \\otimes \\Theta^\\star$, we note that $\\hat{\\Theta}^{-1} - {\\Theta^\\star}^{-1} = \\mathbb{I}^\\star{E} + {R}_{\\Sigma^\\star}(E)$.  In Section~\\ref{section:theorem},  we imposed a set of conditions on $\\eta_1^\\star, \\eta_2^\\star$ in \\eqref{eqn:eta1} and \\eqref{eqn:eta2} so that $\\mathbb{I}^\\star$ is globally well-conditioned, as well a condition on $\\eta_3^\\star(\\omega)$ in \\eqref{eqn:eta_3} to address identifiability issues in the diagonal-minus-low-rank decomposition. These conditions allow us to control $\\mathbb{I}^\\star(E)$ when $E$ is restricted to certain directions. We bound the remainder term ${R}_{\\Sigma^\\star}(E)$ in Proposition~\\ref{prop:Remainder} where $E$ is restricted to live in a certain space. Specifically, consider the following constrained optimization program:\n\\begin{eqnarray}\n(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{x^{(i)},y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L_y}\\|_{\\star} + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} ~~\\hspace{-0.2in} & \\Theta_y = D_y - {L}_y, ~(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x}) \\in \\mathbb{H}' \\label{eqn:ConvexRelaxed231_N}\n\\end{eqnarray}\nHere $\\mathbb{H}' = W \\times T_y' \\times T_{yx}' \\times {\\mathbb{S}}^{q}$, where $T_y'$  is a subspace in ${\\mathbb{S}}^{p}$, and $T_{yx}'$ is a subspace in ${\\mathbb{R}}^{p \\times q}$. Let $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - L_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_x - \\Theta_x^\\star)$ denote the error in the estimated variables. In the following proposition, we bound the remainder term $R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))$ defined earlier. Before we proceed, we define the following norm on ${\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p \\times q} \\times {\\mathbb{S}}^q$ that is useful in our analysis:\n\n", "index": 19, "text": "\\begin{equation}\n\\Phi_{\\gamma}(D_y, L_y, \\Theta_{yx}, \\Theta_x )= \\max \\Big\\{ \\|D_y\\|_{2}, \\|L_y\\|_2, \\frac{1}{\\gamma} \\|\\Theta_{yx}\\|_2, \\|\\Theta_x\\|_2\\Big\\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{\\gamma}(D_{y},L_{y},\\Theta_{yx},\\Theta_{x})=\\max\\Big{\\{}\\|D_{y}\\|_{2},\\|%&#10;L_{y}\\|_{2},\\frac{1}{\\gamma}\\|\\Theta_{yx}\\|_{2},\\|\\Theta_{x}\\|_{2}\\Big{\\}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>\u03b3</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><msub><mi>L</mi><mi>y</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">{</mo><msub><mrow><mo>\u2225</mo><msub><mi>D</mi><mi>y</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>,</mo><msub><mrow><mo>\u2225</mo><msub><mi>L</mi><mi>y</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>,</mo><mrow><mfrac><mn>1</mn><mi>\u03b3</mi></mfrac><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>,</mo><msub><mrow><mo>\u2225</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mi>x</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub><mo maxsize=\"160%\" minsize=\"160%\">}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nUsing a similar decoupling technique, it is easy to check that:\n\\begin{eqnarray}\n\\Phi_{\\gamma} \\Big[ \\mathcal{P}_{\\mathbb{H}'^{\\perp}} [\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})] \\Big] \\leq \\eta_2^\\star + \\frac{8\\alpha}{\\beta} \\leq \\frac{8\\alpha}{3\\beta} + \\frac{8\\alpha}{\\beta}\n\\label{bound2}\n\\end{eqnarray}\n\nLetting $Z = (D_y , L_y, \\Theta_{yx}, \\Theta_x)$, we use \\eqref{bound1} and \\eqref{bound2} to conclude:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'^\\perp}{\\mathcal{A}}^{\\dagger} \\mathbb{I}^{\\star}{\\mathcal{A}}\\mathcal{P}_{{{\\mathbb{H}'}}} (\\mathcal{P}_{{{\\mathbb{H}'}}} {\\mathcal{A}}^{\\dagger}\\mathbb{I}^{\\star}{\\mathcal{A}}\\mathcal{P}_{{{\\mathbb{H}'}}})^{-1}(Z)]  &\\leq& \\frac{\\frac{8\\alpha}{3\\beta} + \\frac{8\\alpha_{}}{\\beta}}{2\\alpha_{} - \\frac{8\\alpha_{}}{\\beta}} \\leq 1-\\frac{3}{1+\\beta}\n\\end{eqnarray*}\n\n\\subsection{\\textit{Proof of main paper Proposition 1 -- bounding curvature of the matrix inverse}} Let $(\\tilde{D}_y, \\tilde{L}_y, \\tilde{\\Theta}_{yx}, \\tilde{\\Theta}_x)$ be an estimate for the population quantities \\\\$({D}_y^\\star, {L}_y^\\star, {\\Theta}_{yx}^\\star, {\\Theta}_x^\\star)$, and let $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - L_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_x - \\Theta_x^\\star) \\subset {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times {\\mathbb{R}}^{p \\times q} \\times {\\mathbb{S}}^q$, recall that the taylor expansion of the inverse of matrix perturbation is specified by:\n\n", "itemtype": "equation", "pos": 93870, "prevtext": "\nNotice this norm is a slight variant of the dual norm of the regularizer $\\|L_y\\|_{\\star} + \\gamma\\|\\Theta\\|_{\\star}$ in \\eqref{eqn:ConvexRelaxed231_N}.\n\n\n\n\\begin{proposition}\n\\label{prop:Remainder}\nLet $C' = (3 + \\gamma)\\psi$. If $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{1}{2C'}$, then $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq 2m{\\psi}C'^2 \\Phi_{\\delta,\\gamma}[\\Delta]^2$.\n\\end{proposition}\nNotice the bound on $R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))$ is dependent on the error term $\\Phi_{\\gamma}[\\Delta]$. In the following proposittion, we bound this error so that we can control the remainder term.  Specifically, suppose we let $T_y'$ and $T_{yx}'$ be tangent spaces to the low-rank matrix varieties and $\\rho(T_y', T(L_y^\\star)) \\leq \\omega$. Let $E_n = \\Sigma^\\star - \\Sigma_n$ denote the difference between the true joint covariance and the sample covariance and let $C_T = (0~, \\mathcal{P}_{{T_y'}^\\perp}(L_y^\\star), \\mathcal{P}_{{T_{yx}'}^\\perp}(\\Theta_{yx}^\\star),~0)$. The proof of the following result uses Brouwer's fixed-point theorem, and is inspired by the proof of a similar result in \\cite{Ravikumar,Chand2012}.\n\\begin{proposition}\n\\label{prop:Brower}\nDefine:\n\\begin{eqnarray}\nr = \\max\\Big\\{\\frac{4}{\\alpha}\\Big(\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}] +\\lambda_n\\Big),\\hspace{.1in} \\Phi_{\\gamma}[C_{T}]\\Big\\}\n\\label{eqn:rdef}\n\\end{eqnarray}\nIf $r \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$, then $\\Phi_{\\gamma}[\\mathcal{F}^\\dagger\\Delta] \\leq 2r$.\n\\end{proposition}\n\nIn the following proposition, we prove algebraic correctness of program \\eqref{eqn:ConvexRelaxed21_N}. The statement theorem relies on the following constants:\\\\\n$C_{\\sigma_{yx}}' = C_1^2\\psi^2\\max\\{12\\beta + \\frac{6\\beta}{\\gamma}, \\frac{2}{C_2\\psi^2} + \\frac{6\\beta}{\\gamma}\\}$, and $C_{samp}' = \\max\\{\\frac{1}{48\\psi\\beta},4C_2C', \\frac{32m\\psi{C}'^2C_2}{\\alpha},\\\\ 12\\beta{m}\\psi{C'}^2C_2^2\\}$.\n\\begin{proposition}\n\\label{proposition:lambdaBound}\nSuppose $\\gamma$ is chosen in the range specified in Theorem~\\ref{theorem:main} and  $\\sigma_y \\geq  \\frac{{m}}{\\omega}C_{\\sigma_y}\\lambda_n$, $\\sigma_{yx} \\geq  {m\\gamma^2}C_{\\sigma_{yx}}'\\lambda_n$. Further, suppose $\\lambda_n$ is chosen so that $\\lambda_n \\leq \\frac{1}{C_{samp}'}$. Then, there exists tangent space $T_y' \\subset {\\mathbb{S}}^{p}$ in the rank-$k_{u}$ variety ($k_u = \\text{rank}(L_y^\\star)$) and tangent space $T_{yx}' \\subset {\\mathbb{R}}^{p \\times q}$ in rank $k_{x}$-variety ($k_x = \\text{rank}(\\Theta_{yx}^\\star)$) where $\\rho(T_y', T(L_y^\\star)) \\leq \\omega$ such that the corresponding solution $(\\hat{\\Theta}, \\hat{S}_y, \\hat{L}_y)$ satisfies the following properties:\n\\begin{enumerate}\n\\item$\\text{rank}(\\hat{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\hat{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$\n\\item Letting $C_{T} = (0~,~\\mathcal{P}_{{T_y'}^\\perp}(L_y^\\star)~, ~\\mathcal{P}_{{T_{yx}'}^\\perp}(\\Theta_{yx}^\\star)~,~ 0)$, we have that $\\Phi_{\\gamma}[\\mathcal{F}^\\dagger \\mathbb{I}^\\star\\mathcal{F}(C_{T})] \\leq \\frac{\\lambda_n}{6\\beta}$ and $\\Phi_{\\gamma}[C_T] \\leq \\frac{16\\alpha}{3\\beta}\\lambda_n$\n\\item $\\Phi_{\\gamma}[\\Delta] \\leq 2C_1\\lambda_n$\n\\end{enumerate}\nFurthermore, suppose that $\\Phi_{\\gamma}(A^{\\dagger}E_n) \\leq \\frac{\\lambda_n}{6\\beta}$ and $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq \\frac{\\lambda_n}{6\\beta}$. Then the tangent space constraints $(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}'$ in \\eqref{eqn:ConvexRelaxed231_N} is inactive, so that  $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ is the unique solution of the original convex program \\eqref{eqn:main}.\n\\label{prop:rec}\n\\end{proposition}\n\nThus far, the analysis of the convex program so has been deterministic in nature. In the following proposition, we present the probabilistic component of our analysis by showning the rate at which the sample covariance matrix $\\Sigma_n$ converges to $\\Sigma^\\star$ in spectral norm. This result is well-known and is a specialization of a result proven by \\cite{Davidson}.\n\\begin{proposition}\n\\label{prop:Enbound}\nSuppose that the number of observed samples obeys \\\\$n \\geq 4608 \\beta^2m^2\\psi^2C_{samp}^2(p+q)$, and the regularization parameter $\\lambda_n$ is chosen in the range specified by Theorem 1. Then, with probability greater than $1 - 2\\text{exp}\\Big\\{-\\frac{n\\lambda_n^2}{4608\\beta^2m^2\\psi^2}\\Big\\}$, $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] \\leq \\frac{\\lambda_n}{6\\beta}$. \\label{prop:prob} \\end{proposition}\n\n\\subsection{Proof of Theorem 1} Noting that $C' \\leq 4\\psi{\\bar{m}}$, the constants $C_{\\sigma_{YX'}}$ and $C_{samp'}$ in Proposition~\\ref{prop:rec} can be related to constants $C_{\\sigma_{yx}}$ and $C_{samp}$ in Theorem 1 as follows:  $C_{\\sigma_{yx}'} \\leq mC_{\\sigma_{yx}}$ and $C_{samp}' \\leq m{\\bar{m}}^2C_{samp}$. Using these relations, it is easy to check that the assumptions of Theorem 1 imply that the assumptions of Proposition~\\ref{prop:rec} are satisfied. Thus we can conclude that the optimal solution $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ of \\eqref{eqn:ConvexRelaxed23_N} (with a particular choice of tangent spaces $T_y'$ and $T_{yx}'$) satisfy results of Proposition~\\ref{prop:rec}. Further, by appealing to Proposition~\\ref{prop:prob}, we have that $\\Phi_{\\gamma}(\\mathcal{F}^\\dagger{E_n})  \\leq \\frac{\\lambda_n}{6\\beta}$. If we show that $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] \\leq \\frac{\\lambda_n}{6\\beta}$, then we conclude that the unique optimum $(\\hat{\\Theta}, \\hat{D}_y, \\hat{L}_y)$ of the original convex program \\eqref{eqn:main} has structurally correct structure (i.e. $\\text{rank}(\\hat{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\hat{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$). To show that  $~\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] \\leq \\frac{\\lambda_n}{6\\beta}$, we note that\n\\begin{eqnarray*}\n\\frac{4}{\\alpha}\\Big(\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}] +\\lambda_n \\Big) \\leq \\frac{4}{\\alpha} \\Big(\\frac{\\lambda_n}{6\\beta} + \\frac{\\lambda_n}{6\\beta} + \\lambda_n\\Big)\n\\leq \\frac{16\\alpha}{3\\beta} \\lambda_n \\\\ \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}\n\\end{eqnarray*}\nHere, we used the bound on $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T}]$ provided by Proposition~\\ref{prop:rec} and the bound on $\\lambda_n$. Furthermore, appealing to Proposition~\\ref{prop:rec} once again, we have $\\Phi_{\\gamma}[C_{T}] \\leq \\frac{16\\alpha}{3\\beta}\\lambda_n \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$. Thus Proposition~\\ref{prop:Brower} provides us with the bound $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{32\\alpha}{3\\beta}\\lambda_n \\leq \\frac{1}{2C'}$.  We subsequently apply the results of Proposition~\\ref{prop:Remainder} to obtain:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq 2m{\\psi}C'^2 \\Phi_{\\delta,\\gamma}[\\Delta]^2 \\leq  \\Big[2m\\psi{C}'^2(\\frac{32\\alpha}{3\\beta})^2\\lambda_n\\Big] \\lambda_n \\leq \\frac{\\lambda_n}{6\\beta}\n\\end{eqnarray*}\nThe last inequality follows from the bound on $\\lambda_n$.\n\n\\section{Discussion} In this paper we describe a new approach for interpreting\nthe latent variables in a factor model.  Our method proceeds by\nobtaining observations of auxiliary covariates that may plausibly be\nrelated to the observed phenomena, and then suitably associating these\nauxiliary covariates to the latent variables.  The procedure involves\nthe solutions of computationally tractable convex optimization\nproblems, which are log-determinant semidefinite programs that can be\nsolved efficiently.  We give both theoretical as well as experimental\nevidence in support of our methodology.  Our technique generalizes\ntransparently to other families beyond factor models such as\nlatent-variable graphical models \\cite{Chand2012}, although we do not pursue\nthese extensions in the present article.\n\n\n\\begin{thebibliography}{7}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\n\n\n\n\n\\bibitem{Bickel}\n\\textsc{Bickel, P. $\\&$ Doksum, K.}\n\\newblock {{Mathematical Statistics, Basic Ideas and Selected Topics}}.\n\\newblock \\textit{Prentice-Hall}, 2007.\n\n\\bibitem{Buhlmann}\n\\textsc{B\\\"{u}hlmann, P $\\&$ van de Geer, S.}.\n\\newblock \\textit{{Statistics for high-dimensional data}}.\n\\newblock New York: Springer, 2011.\n\n\n\\bibitem{Candes}\n\\textsc{Cand\\`es, E. J. $\\&$ Recht, B.}\n\\newblock {{Exact matrix completion\nvia convex optimization}}.\n\\newblock \\textit{Foundation of Computational Mathematics}, 9:717--772, 2009.\n\n\n\\bibitem{Chandrasekaran}\n\\textsc{Chandrasekaran, V., Sanghavi, S., Parrilo, P. A. $\\&$ Willsky, A. S.}\n\\newblock {{Rank-sparsity incoherence\nfor matrix decomposition}}.\n\\newblock \\textit{SIAM Journal of Optimization}, 21:572--596, 2011.\n\n\\bibitem{Chand2012}\n\\textsc{Chandrasekaran, V., Parrilo, P. A. $\\&$ Willsky, A. S.}\n\\newblock {{Latent Variable Graphical Model Selection via Convex Optimization}}.\n\\newblock \\textit{Annals of Statistics}, 40:1935--1967, 2012.\n\n\n\\bibitem{Davidson}\n\\textsc{Davidson, K.R. $\\&$ Szarek, S.J.}\n\\newblock {{Local operator theory, random matrices and {B}anach spaces}}.\n\\newblock \\textit{Handbook of the Geometry of Banach Spaces}, 1:317--366, 2001.\n\n\n\\bibitem{Fazel}\n\\textsc{Fazel, M.}\n\\newblock {{Matrix rank minimization with applications}}.\n\\newblock \\textit{PhD thesis, Department of Electrical Engineering, Stanford\nUniversity}, 2002.\n\n\n\n\\bibitem{Hotelling}\n\\textsc{Hotelling, H.}\n\\newblock {{Relations between two sets of variants}}.\n\\newblock \\textit{Biometrika}, 28:321-377, 2002.\n\n\n\n\n\n\n\n\\bibitem{Kay}\n\\textsc{Kay, S.M.}\n\\newblock {{Modern spectral estimation, theory and application}}.\n\\newblock \\textit{Prentice-Hall}, 1988.\n\n\\bibitem{Ledermann}\n\\textsc{Ledermann, W.}\n\\newblock {{On a problem concerning matrices with variable diagonal elements}}.\n\\newblock \\textit{Proceeding of Royal Society Edinburgh}, 60:1-17, 1940.\n\n\n\\bibitem{Meshabi}\n\\textsc{Mesbahi, M. $\\&$ Papavassilopoulos, G.}\n\\newblock {{On the rank minimization problem over a positive semidefinite\nlinear matrix inequality}}.\n\\newblock \\textit{IEEE Transactions on Automatic Control}, 42:239-243, 1997.\n\n\n\\bibitem{Nataranjan}\n\\textsc{Natarajan, B.K.}\n\\newblock {{Sparse approximate solutions to linear systems}}.\n\\newblock \\textit{SIAM Journal of Computing}, 24:227-234, 1995.\n\n\n\\bibitem{Ravikumar}\n\\textsc{Ravikumar, P., Wainwright, M. J., Raskutti, G. $\\&$ Yu, B.}\n\\newblock{{High-dimensional covariance estimation by minimizing\n$\\ell_1$-penalized log-determinant divergence}}.\n\\newblock \\textit{Electronic Journal of Statistics}, 4:935--980, 2011.\n\n\\bibitem{Recht}\n\\textsc{Recht, B., Fazel, M. $\\&$ Parrilo, P. A.}\n\\newblock{Guaranteed minimum rank solutions to linear matrix equations via nuclear norm\nminimization}.\n\\newblock \\textit{SIAM Review}, 52:471--501, 2010.\n\n\n\\bibitem{Saunderson}\n\\textsc{Saunderson, J., Chandrasekaran, V., Parrillo, P $\\&$ Willsky, S.}\n\\newblock{Diagonal and low-rank matrix decompositions, correlation matrices, and ellipsoid fitting}.\n\\newblock \\textit{SIAM Journal on Matrix Analysis}, 33:1395-1416, 2012.\n\n\n\\bibitem{Shapiro1}\n\\textsc{Shapiro, A.}\n\\newblock{Rank-reducibility of a symmetric matrix and sampling theory of minimum trace\nfactor analysis}.\n\\newblock \\textit{Psychometrika}, 47:187-199, 1982.\n\n\\bibitem{S2}\n\\textsc{Shapiro, A.}\n\\newblock{Weighted minimum trace factor analysis}.\n\\newblock \\textit{Psychometrika}, 47:243-264, 1982.\n\n\\bibitem{S3}\n\\textsc{Shapiro, A.}\n\\newblock{Identifiability of factor analysis: Some results and open problems}.\n\\newblock \\textit{Linear Algebra Applications}, 15:201-292, 1904.\n\n\n\\bibitem{Spearman}\n\\textsc{Spearman, C.}\n\\newblock{`General inteligence', objectively determined and measured}.\n\\newblock \\textit{American Journal of Psychology}, 15:201-292, 1904\n\n\n\n\n\\bibitem{Toh}\n\\textsc{Toh, K. C, Todd, M. J. $\\&$ Tutuncu, R. H.}.\n\\newblock \\textit{SDPT3 - a\nMATLAB software package for semidefinite-quadratic-linear\nprogramming}. Available from\nhttp://www.math.nus.edu.sg/~mattohkc/sdpt3.html.\n\n\n\n\n\n\n\n\n\\bibitem{Wai2009}\n\\textsc{Wainwright, M. J.}\n\\newblock{Sharp thresholds for noisy and\nhigh-dimensional recovery of sparsity using $\\ell_1$-constrained\nquadratic programming (Lasso)}.\n\\newblock \\textit{IEEE Transactions on Information Theory}, 55:2183--2202, 2009.\n\n\\bibitem{Wainwright}\n\\textsc{Wainwright, M. J.}\n\\newblock{Structured regularizers for high-dimensional problems: Statistical and computational issues}.\n\\newblock \\textit{Annual Review of Statistics and its Applications}. 1:233--253, 2014.\n\n\n\\end{thebibliography}\n\n\n\n\\newpage\n\n\\section{Supplementary Material}\nIn the following Proposition, we appeal to conditions on $\\eta_1^\\star, \\eta_2^\\star, \\eta_3^\\star$ to prove a set of irrepresentability-type conditions on the population Fisher information $\\mathbb{I}^\\star$. \n\\begin{proposition}\n\\label{prop':irrep}\nSuppose that $\\eta_1^\\star \\geq 3\\alpha$, $\\eta_2^\\star \\leq \\frac{8\\alpha}{3\\beta}$ and $\\eta_3^\\star(\\omega) \\leq \\frac{2\\alpha}{\\beta}$ (where these conditions were defined in Section~\\ref{section:theoremstatement}). Further, suppose that the regularization parameter $\\gamma$ is chosen in the range specified in Theorem~\\ref{theorem:main}. Then we have that the following two conditions hold for $\\mathbb{H}' = W \\times T'_y \\times T_{yx}' \\times {\\mathbb{S}}^q \\subset {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times \\mathbb{R}^{p \\times q} \\times {\\mathbb{S}}^q$ where $\\rho(T(L_y^\\star),T'_y) \\leq \\omega$ and $T_{yx}'$ is any subspace in $\\mathbb{R}^{p \\times q}$:\n\\begin{enumerate}\n\\item The minimum gain of $\\mathbb{I}^\\star$ restricted to $\\mathbb{H}'$ is bounded below:\n\\begin{eqnarray}\n\\min_{\\substack{(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}' \\\\ \\Phi_{\\gamma}(D_y, L_y, \\Theta_{yx}, \\Theta_x) = 1}}  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'} {\\mathcal{I}}^{\\dagger} \\mathbb{I}^\\star {\\mathcal{I}} \\mathcal{P}_{\\mathbb{H}'}(D_y, L_y, \\Theta_{yx}, \\Theta_{x})] \\geq \\alpha\n\\label{eqn:Fisher1}\n\\end{eqnarray}\n\n\\item The effect of elements in $\\mathbb{H}'$ on the orthagonal complement $\\mathbb{H}'^{\\perp}$ is bounded above:\n\\begin{eqnarray}\n\\max_{{Z \\in \\mathbb{H}';\\hspace{.05in}  \\Phi_{\\gamma}(Z) = 1}}  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'^{\\perp}}{\\mathcal{I}}^{\\dagger} \\mathbb{I}^{\\star}{\\mathcal{I}}\\mathcal{P}_{{{\\mathbb{H}'}}} (\\mathcal{P}_{{{\\mathbb{H}'}}} {\\mathcal{I}}^{\\dagger}\\mathbb{I}^{\\star}{\\mathcal{I}}\\mathcal{P}_{{{\\mathbb{H}'}}})^{-1}(Z)] \\leq 1-\\frac{3}{\\beta+1}\n\\label{eqn:Fisher2}\n\\end{eqnarray}\n\\end{enumerate}\n\\label{prop:fisher}\n\\end{proposition}\nAlthough conditions \\eqref{eqn:Fisher1} and \\eqref{eqn:Fisher2} are satisfied for all subspaces $T_{yx}' \\subset {\\mathbb{R}}^{p\\times{q}}$, we specialize these to tangent spaces of low-rank matrix variety. Conditions \\eqref{eqn:Fisher1} and \\eqref{eqn:Fisher2} are analogous to conditions that play an important role in the analysis of the Lasso for sparse linear regression, graphical model selection via the Graphical Lasso \\cite{Ravikumar}, and in several other approaches for high-dimensional estimation. As a point of comparison with respect to analyses of the Lasso, the role of the Fisher information $\\mathbb{I}^\\star$ is played by $A^TA$, where $A$ is the underlying design\nmatrix. In analyses of both the Lasso and the Graphical Lasso in the papers referenced above, the analog of the subspace $\\mathbb{H}$ is the set of models with support contained inside the support of the underlying sparse population model. Assumptions 1 and 2 are also similar in spirit to conditions employed in the analysis of convex relaxation methods for latent-variable graphical model selection \\cite{Chand2012}.\\\\\n\n\\begin{proof} First, consider an arbitrary subspace $\\mathcal{S} \\in {\\mathbb{S}}^{p+q}$. Let $M \\in \\mathbb{W}$ with $\\|M\\|_2 = 1$. Then,\n\\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathcal{S}}\\mathbb{I}^\\star{M}\\|_{2} \\geq \\|\\mathbb{I}^\\star(M)\\|_{2} - \\|\\mathcal{P}_{\\mathbb{W}^\\perp}\\mathbb{I}^\\star\\mathcal{P}_{\\mathbb{W}}\n(M)\\|_{2} \\geq 3\\alpha - \\alpha(1-\\frac{3}{\\beta+1}) \\geq 2\\alpha\n\\end{eqnarray*}\nIn the subsequent discussion in this section, we employ the following notation to denote restrictions of a subspace $\\mathbb{H} = H_1 \\times H_2 \\times H_3 \\times H_4  \\subset \\mathbb{S}^{ p} \\times \\mathbb{S}^{p} \\times {\\mathbb{R}}^{p \\times q} \\times \\mathbb{S}^{q}$ (here $H_1,H_2,H_3,H_4$ are subspaces in $\\mathbb{S}^{ p},\\mathbb{S}^p,{\\mathbb{R}}^{p \\times q},\\mathbb{S}^q$, respectively) to its individual components. The restriction to the first component of $\\mathbb{H}$ is given by $\\mathbb{H}[1] = H_1 \\times \\{0\\} \\times \\{0\\} \\times \\{0\\} \\subset \\mathbb{S}^{ p} \\times \\mathbb{S}^{p} \\times {\\mathbb{R}}^{p \\times q} \\times \\mathbb{S}^{q}$.  The restrictions $\\mathbb{H}[2],\\mathbb{H}[3],\\mathbb{H}[4]$ to the other components of $\\mathbb{H}$ are defined in an analogous manner. Let $\\mathbb{H}' = W \\times T'_y \\times \\mathbb{R}^{p\\times{q}} \\times {\\mathbb{S}}^q$ with $\\rho(T'_y,T(L_y^\\star)) \\leq \\omega$. Recall, the subspace $W$ is the set of diagonal matrices with nonnegative entries. Consider a set of variables $(D_y, L_y, \\Theta_{yx}, \\Theta_x) \\in \\mathbb{H}'$ with $\\|D_y\\|_{2} \\leq 1$, $\\|L_y\\|_2 \\leq 1$, $\\|\\Theta_{yx}\\|_2 \\leq \\gamma$, and $\\|\\Theta_{yx}\\|_2 \\leq 1$. Suppose equality holds in at least one of these set of inequalities so that  $\\Phi_{\\gamma}(D_y, L_y, \\Theta_{yx}, \\Theta_x) = 1$. Then, at least one of the following cases is active (the following results use conditions on $\\eta_1^\\star$, $\\eta_2^\\star$ and $\\eta_3^\\star(\\omega)$ and Proposition 1 (main paper)) :\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{enumerate}\n\\item If $\\|D_y\\|_{2} = 1$, then\n\\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathbb{H}'[1]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq \\Big[\\|\\mathcal{P}_{W}\\mathbb{I}_y^{\\star}(D_y)\\|_{2} - \\|\\mathcal{P}_{W}\\mathbb{I}_y^{\\star}(L)\\|_{2} \\\\ - \\|\\mathcal{P}_{\\mathbb{H}'[1]}\\mathbb{I}^{\\star}\\mathcal{F}(0, 0, \\Theta_{yx}, \\Theta_{x})\\|_{2}\\Big] \\geq 2\\alpha-\\eta_3^\\star-2\\eta_2^\\star\\max\\{\\gamma,1\\} \\\\\n\\geq 2\\alpha-\\frac{8\\alpha}{\\beta}\n \\end{eqnarray*}\n  \\item If $\\|L_y\\|_{2} = 1$, then\n  \\begin{eqnarray*}\n\\|\\mathcal{P}_{\\mathbb{H}'[2]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq \\Big[\\|\\mathcal{P}_{T_y'}\\mathbb{I}_y^{\\star}(L_y)\\|_{2} - \\|\\mathcal{P}_{T'_y}\\mathbb{I}_y^{\\star}(D_y)\\|_{2} \\\\ - \\|\\mathcal{P}_{\\mathbb{H}[2]}\\mathbb{I}^{\\star}\\mathcal{F}(0, 0, \\Theta_{yx}, \\Theta_{x})\\|_{2}\\Big] \\geq 2\\alpha-\\eta_3^\\star-2\\eta_2^\\star\\max\\{\\gamma,1\\} \\\\\n\\geq 2\\alpha-\\frac{8\\alpha}{\\beta}\n \\end{eqnarray*}\n{\\noindent}Similarly, one can show\n  \\item If $\\|\\Theta_{yx}\\|_{2} = \\gamma$, then $\\frac{1}{\\gamma} \\|\\mathcal{P}_{\\mathbb{H}[3]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq 2\\alpha - \\frac{8\\alpha}{\\beta}$\n \\item If $\\|\\Theta_{x}\\|_{2} = 1$, then $\\|\\mathcal{P}_{\\mathbb{H}[4]}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})]\\|_{2} \\geq 2\\alpha - \\frac{8\\alpha}{\\beta}$\n\\end{enumerate}\nCombining these results, one can conclude that\n\n", "index": 21, "text": "\\begin{equation}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'} {\\mathcal{A}}^{\\dagger} \\mathbb{I}^\\star {\\mathcal{A}} \\mathcal{P}_{\\mathbb{H}'}(D_y, L_y, \\Theta_{yx}, \\Theta_{x})] \\geq 2\\alpha - \\frac{8\\alpha}{\\beta} \\geq \\alpha\n\\label{bound1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}^{\\prime}}{\\mathcal{A}}^{\\dagger}\\mathbb{%&#10;I}^{\\star}{\\mathcal{A}}\\mathcal{P}_{\\mathbb{H}^{\\prime}}(D_{y},L_{y},\\Theta_{%&#10;yx},\\Theta_{x})]\\geq 2\\alpha-\\frac{8\\alpha}{\\beta}\\geq\\alpha\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>\u03b3</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><msup><mi>\u210d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2020</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udd40</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><msup><mi>\u210d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><msub><mi>L</mi><mi>y</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>-</mo><mfrac><mrow><mn>8</mn><mo>\u2062</mo><mi>\u03b1</mi></mrow><mi>\u03b2</mi></mfrac></mrow><mo>\u2265</mo><mi>\u03b1</mi></mrow></math>", "type": "latex"}, {"file": "1601.00389.tex", "nexttext": "\nwhere,\n\\begin{eqnarray*}\n R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta)) = \\Sigma^\\star\\Big[\\sum_{k = 2}^{\\infty}(-\\mathcal{F}(\\Delta){\\Sigma^\\star}^{-1})^k\\Big].\n\\end{eqnarray*}\nThe following proposition provides a bound on this second order term:\n\\begin{proposition}\n\\label{lemma:Remainder}\nIf $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{1}{2C'}$, then $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq 2m{\\psi}C'^2 \\Phi_{\\gamma}[\\Delta]^2$.\n\\end{proposition}\n\\begin{proof}\nWe note that:\n\\begin{eqnarray*}\n\\|\\Delta\\|_2 &\\leq& {\\|{\\Delta}D_y\\|_{\\ell_{\\infty}}}{}  + \\|{\\Delta}{L}_y\\|_2 +{\\|{\\Delta}\\Theta_{yx}\\|_2}{} + \\|{\\Delta}\\Theta_{x}\\|_2 \\leq (3+\\gamma)\\Phi_{\\gamma}(\\Delta)\n \\end{eqnarray*}\nUsing this observation and some algebra, we have that:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] \\leq m\\psi \\Big[\\sum_{k = 2}^{\\infty} (\\psi \\|\\Delta\\|_2)^k \\Big]\n &\\leq& m\\psi^3 \\frac{(3+\\gamma)^2\\Phi_{\\gamma}[\\Delta]^2}{1-(3+\\gamma)\\Phi_{\\gamma}[\\Delta]\\psi}\\\\\n&\\leq& 2m{\\psi}C'^2 \\Phi_{\\gamma}[\\Delta]^2\n\\end{eqnarray*}\n\\end{proof}\n\n\n\\subsection{\\textit{Proof of main paper Proposition 2}} Next, we analyze the following convex program subject to certain additional tangent space constraints:\n\\begin{eqnarray}\n(\\tilde{\\Theta}, \\tilde{S}_y, \\tilde{L}_y) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{X^{(i)},Y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L_y}\\|_{\\star} + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} ~~\\hspace{-0.2in} & \\Theta_y = D_y - {L}_y, ~(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x}) \\in \\mathbb{H}' \\label{eqn:ConvexRelaxed23_N}\n\\end{eqnarray}\nwhere $\\mathbb{H}' = W \\times T_y' \\times T_{yx}' \\times {\\mathbb{S}}^{q}$ and $T_y' \\subset {\\mathbb{S}}^p$ and $T_{yx}' \\subset {\\mathbb{R}}^{p \\times q}$ are subspaces. In the following proposition, we show that if $T_y'$ and $T_{yx}'$ are tangent spaces with respect to the variety of low-rank matrices with $\\rho(T_y', T(L_y^\\star)) \\leq \\omega$, then we can bound the error $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - {L}_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_{x} - \\Theta_{x}^\\star)$. We denote $E_n = \\Sigma^\\star - \\Sigma_n$ as the difference between the population covariance matrix and the sample covariance matrix. Further, $C_{T} = (0~,~\\mathcal{P}_{T_y'^\\perp}(L_y^\\star),~\\mathcal{P}_{T_{yx}'^\\perp}(\\Theta_{yx}^\\star)~,~0)$.\n\\end{proof}\n\\begin{proposition}\n\\label{lemma:Brower}\nFinally, define:\n\\begin{eqnarray}\nr = \\max\\Big\\{\\frac{4}{\\alpha}\\Big(\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T_{}}] +\\lambda_n\\Big),\\hspace{.1in} \\Phi_{\\gamma}[C_{T}]\\Big\\}\n\\label{eqn:rdef}\n\\end{eqnarray}\nIf $r \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$, then $\\Phi_{\\gamma}[\\Delta] \\leq 2r$.\n\\end{proposition}\n\\begin{proof}\n{\\noindent} The proof of this result uses Brouwer's fixed-point theorem, and is inspired by the proof of a similar result in \\cite{Ravikumar,Chand2012}.\nThe optimality conditions of \\eqref{eqn:ConvexRelaxed23_N} suggest that there exist Lagrange multipliers $Q_{D_y} \\in W$,  $Q_{T_y} \\in {T_y'}^\\perp$, and $Q_{T_{yx}} \\in {T_{yx}'}^\\perp$ such that\n\\begin{eqnarray*}\n[\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_y + Q_{D_y} = 0; \\hspace{.1in} [\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_y + Q_{T_y}  &\\in& \\lambda_n\\partial\\|\\tilde{L}_y\\|_\\star\\[.01in]\n[\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_{yx} + Q_{T_{yx}}  \\in -\\lambda_n\\gamma\\partial\\|\\tilde{\\Theta}_{yx}\\|_{\\star};  \\hspace{.1in} [\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_{x}  &=& 0\n\\end{eqnarray*}\n{\\noindent}Letting the SVD decomposition of $\\tilde{L}$ and $\\tilde{\\Theta}_{yx}$ be given by $\\tilde{L}_y = \\bar{U}\\bar{D}\\bar{V}'$ and $\\tilde{\\Theta}_{yx} = \\breve{U}\\breve{D}{\\breve{V}}'$ respectively, and $Z \\triangleq  (0, \\hspace{.1in} \\lambda_n\\bar{U}\\bar{V}', \\hspace{.1in}  -\\lambda_n\\gamma_{}{\\breve{U}}{\\breve{V}}',  \\hspace{.1in} 0)$, we can restrict the optimality conditions to the space $\\mathbb{H}'$ to obtain, $\\mathcal{P}_{{\\mathbb{H}}'}\\mathcal{F}^{\\dagger}(\\Sigma_n - \\tilde{\\Theta}^{-1}) = Z$. Further, by appealing to the matrix inversion lemma, this condition can be restated as $\\mathcal{P}_{{\\mathbb{H}}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}(E_{n} - R_{\\Sigma^\\star}(\\Delta) + \\mathbb{I}^{\\star}\\mathcal{F}(\\Delta)) = Z$. Based on the Fisher information condition \\eqref{eqn:Fisher1}, the optimum of \\eqref{eqn:ConvexRelaxed23_N} is unique (this is because the Hessian of the negative log-likelihood term is positive definite restricted to the tangent space constraints). Moreover, using standard Lagrangian duality, one can show that the set of variables  $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ that satisfy the restricted optimality conditions are unique. We now appeal to Brouwer's fixed-point theorem to bound $\\Phi_{\\gamma}[\\Delta]$. Consider the following function $G(\\underline{\\delta})$ restriced to $\\underline{\\delta} \\in W \\times T'_y \\times T'_{yx} \\times {\\mathbb{S}}^q$ with $\\rho(T(L_y^\\star),T_y') \\leq \\omega$:\n\\begin{eqnarray*}\nG(\\underline{\\delta}) = \\underline{\\delta} - (\\mathcal{P}_{\\mathbb{H}_{}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{}})^{-1}\\Big(\\mathcal{P}_{\\mathbb{H}_{}}\\mathcal{F}^{\\dagger} [E_{n} &-& R_{\\Sigma^\\star}\\mathcal{F}(\\underline{\\delta} + C_{T_{}}) \\\\ + \\mathbb{I}^{\\star}\\mathcal{F}(\\underline{\\delta} + C_{T_{}})] - Z\\Big)\n\\end{eqnarray*}\n{\\noindent}The function $G(\\underline{\\delta})$ is well-defined since the operator $\\mathcal{P}_{\\mathbb{H}'_{}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}'_{}}$ is bijective due to Fisher information condition 1 in Proposition 2 (main paper). As a result, $\\underline{\\delta}$ is a fixed point of $G(\\underline{\\delta})$ if and only if $\\mathcal{P}_{\\mathbb{H}'_{}}\\mathcal{F}^{\\dagger} [E_{n} - R_{\\Sigma^\\star}(\\mathcal{F}(\\underline{\\delta} + C_{T_{}})) + \\mathbb{I}^{\\star}\\mathcal{F}(\\underline{\\delta} + C_{T_{}})] =  Z$.\nSince the pair $(\\tilde{\\Theta}, \\tilde{S}_y, \\tilde{L}_y)$ are the unique solution to \\eqref{eqn:ConvexRelaxed23_N}, the only fixed point of $G$ is $\\mathcal{P}_{\\mathbb{H}'_{}}[\\Delta]$. Next we show that this unique optimum lives inside the ball $\\mathbb{B}_r = \\{\\underline{\\delta} \\hspace{.1in} | \\hspace{.1in} \\Phi_{\\gamma}(\\underline{\\delta}) \\leq r,  \\hspace{.05in} \\underline{\\delta} \\in \\mathbb{H}'\\}$. In particular, we show that under the map $G$, the image of $\\mathbb{B}_r$ lies in $\\mathbb{B}_r$ and appeal to Brouwer's fixed point theorem to conclude that $\\mathcal{P}_{\\mathbb{H}'_{}}[\\Delta] \\in \\mathbb{B}_r$. For $\\underline{\\delta} \\in \\mathbb{B}_r$, $\\Phi_{\\gamma}[G(\\underline{\\delta})]$ can be bounded as follows:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[G(\\underline{\\delta})] &=& \\Phi_{\\gamma}\\Big[(\\mathcal{P}_{\\mathbb{H}'_{}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}'_{}})^{-1}\\Big(\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger} [E_{n} - R_{\\Sigma^\\star}(\\mathcal{F}(\\underline{\\delta} + C_{T_{}})) \\\\&+& \\mathbb{I}^{\\star}\\mathcal{F}C_{T_{}}] - Z\\Big)\\Big]\n\\leq \\frac{2}{\\alpha}\\Big[\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}( E_n + \\mathbb{I}^{\\star}\\mathcal{F}(C_{T_{}}))] +  \\lambda_n\\Big]\\\\ &+& \\frac{2}{\\alpha}{\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\underline{\\delta}+C_{T})}] \\leq \\frac{r}{2} +  \\frac{2}{\\alpha}{\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\underline{\\delta}+C_{T})}]\n\\end{eqnarray*}\nThe first inequality holds because of Fisher information condition \\eqref{eqn:Fisher1}, and the properties that $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(.)] \\leq 2\\Phi_{\\gamma}(.)$  (since projecting into the tangent space of a low-rank matrix variety increases the spectral norm by a factor of at most two) and $\\Phi_{\\gamma}(Z) = \\lambda_n$. Moreover, since $r \\leq \\frac{1}{4C'}$, we have $\\Phi_{\\gamma}(\\underline{\\delta} + C_{T_{}}) \\leq \\Phi_{\\gamma}(\\underline{\\delta}) + \\Phi_{\\gamma}(C_{T_{}}) \\leq 2r \\leq \\frac{1}{2C'}$. We can now appeal to Proposition 1 to obtain:\n\\begin{eqnarray*}\n\\frac{2}{\\alpha}{\\Phi_{\\gamma} [\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\underline{\\delta} + C_{T_{}})}] \\leq \\frac{4}{\\alpha}m{\\psi}C'^2 [\\Phi_{\\gamma}(\\underline{\\delta} + C_{T_{}})]^2 \\leq \\frac{r}{2}\n\\end{eqnarray*}\n\\par Thus, we conclude that  $\\Phi_{\\gamma}\n[G(\\underline{\\delta})] \\leq r_{}$ and by Brouwer's fixed-point theorem, $\\Phi_{\\gamma} [\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)] \\leq r$. Furthermore, $\\Phi_{\\gamma}[\\Delta] \\leq \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'_{}}(\\Delta)] + \\Phi_{\\gamma}(C_{T}) \\leq 2r$ \\end{proof}\n\n\n\n\\subsection{\\textit{Proof of main paper Proposition 4 - bounding deviation of population covariance matrix and sample covariance matrix}}\n\\begin{proposition}\n\\label{prop:Enbound}\nSuppose that the number of observed samples obeys \\\\$n \\geq 4608 \\beta^2m^2\\psi^2C_{samp}'^2(p+q)$, and the regularization parameter $\\lambda_n$ is chosen in the range specified by Theorem 1 (main paper). Then, with probability greater than $1 - 2\\text{exp}\\Big\\{-\\frac{n\\lambda_n^2}{4608\\beta^2m^2\\psi^2}\\Big\\}$, $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] \\leq \\frac{\\lambda_n}{6\\beta}$. \\end{proposition}\n\\begin{proof}\nFirst, note that $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] \\leq m \\|\\Sigma_n - \\Sigma^\\star\\|_2$. Using the results in \\cite{Davidson} and the fact that $\\frac{\\lambda_n}{6\\beta} \\leq 8\\psi$ and $n \\geq \\frac{2304(p+q)m^2\\psi^2}{\\lambda_n^2}$, the following bound holds: $\\text{Pr}[ m\\|\\Sigma_n - \\Sigma^\\star\\|_2 \\geq \\frac{\\lambda_n}{6\\beta}] \\leq 2\\text{exp} \\Big\\{\\frac{-n\\lambda_n^2}{4608m^2\\psi^2}\\Big\\}$. Thus, $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_n] \\leq \\frac{\\lambda_n}{6\\beta}$ with probability greater than  $1 - 2\\text{exp}\\Big\\{-\\frac{n\\lambda_n^2}{4608\\beta^2m^2\\psi^2}\\Big\\}$.\n\\end{proof}\n\n\n\\subsection{Proof of the main paper Proposition 3}\nConsider the following convex optimization program:\n \\begin{eqnarray}\n \\label{eqn:ConvexRelaxed4_N}\n(\\bar{\\Theta}, \\bar{D}_y, \\bar{L}_y) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{X^{(i)},Y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L_y}\\|_\\star + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_y = D_y - {L}_y; ~ D_y \\text{ is diagonal}\n\\end{eqnarray}\n{\\noindent}Comparing \\eqref{eqn:ConvexRelaxed4_N} with the convex program \\eqref{eqn:main} (main paper), the difference is that we no longer constrain ${L}_y$ to be a positive semidefinite matrix. In particular, if ${L}_y \\succeq 0$, then the nuclear norm of the matrix ${L}_y$ in the objective function of \\eqref{eqn:ConvexRelaxed4_N} reduces to the trace of $L_y$. We show that the unique optimum $(\\bar{\\Theta}, \\bar{S}_y, \\bar{L}_y)$ of \\eqref{eqn:ConvexRelaxed4_N} has the property that with high probability, $\\tilde{L}_y$ is positive semidefinite. As a result, with high probability, the variables $(\\bar{\\Theta}, \\bar{S}_y, \\bar{L}_y)$ are also the optimum of \\eqref{eqn:main}. Below, we outline our proof strategy:\n\\begin{enumerate}\n\\item We proceed by analyzing \\eqref{eqn:ConvexRelaxed4_N} with additional constraints that the variables  ${L}_y$, and $\\Theta_{yx}$ belong to the algebraic varieties low-rank matrices (specified by rank of $L_y^\\star$, and $\\Theta_{yx}^\\star$) , and that the tangent spaces $T(L_y)$, $T(\\Theta_{yx})$ are close to the nominal tangent spaces $T({L}_y^\\star)$, and $T(\\Theta_{yx}^\\star)$ respectively. We prove that under suitable conditions on the minimum nonzero singular value of ${L}_y^\\star$, and minimum nonzero singular value of $\\Theta_{yx}^\\star$, any optimum pair of variables $(\\Theta, D_y, L_y)$ of this non-convex program are smooth points of the underlying varieties; that is $\\rm{rank}(L_y) = \\rm{rank}(L_y^\\star)$ and $\\rm{rank}(\\Theta_{yx}) = \\rm{rank}(\\Theta_{yx}^\\star)$. Further, we show that $L_y$ has the same inertia as $L_y^\\star$ so that $L_y \\succeq 0$.\n\\item Conclusions of the previous step imply the the variety constraints can be ``linearized\" at the optimum of the non-convex program to obtain tangent-space constraints. Under suitable conditions on the regularization parameter $\\lambda_n$, we prove that with high probability, the unique optimum of this ``linearized\" program coincides with the global optimum of the non-convex program.\n\\item Finally, we show that the tangent-space constraints of the linearized program are inactive at the optimum. Therefore the optimal solution of \\eqref{eqn:ConvexRelaxed4_N} has the property that with high probability: $\\text{rank}(\\bar{L}_y) = \\text{rank}(L_y^\\star)$ and $\\text{rank}(\\bar{\\Theta}_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$. Since $\\bar{L}_y \\succeq 0$, we conclude that the variables $(\\bar{\\Theta}, \\bar{D}_y, \\bar{L}_y)$ are the unique optimum of \\eqref{eqn:main}. \n\\end{enumerate}\n\n\n\\subsubsection{Variety Constrained Program}\nWe begin by considering a variety-constrained optimization program:\n\\begin{eqnarray}\n({\\Theta}^{\\mathcal{M}}, {D}_y^{\\mathcal{M}}, {L}_y^{\\mathcal{M}}) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{X^{(i)},Y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L}_y\\|_{\\star} + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} & \\Theta_y = D_y - {L}_y,  (\\Theta, D_y,  {L}_y) \\in \\mathcal{M}. \\label{eqn:NConveProblem_N}\n\\end{eqnarray}\n{\\noindent}Here, the set $\\mathcal{M}$ is given by:\n\\begin{eqnarray*}\n\\mathcal{M} &\\triangleq& \\Big\\{(\\Theta, D_y, {L}_y) \\in {\\mathbb{S}}^{(p+q)} \\times {\\mathbb{S}}^p \\times {\\mathbb{S}}^{p} \\Big | D_y \\text{ is diagonal}, \\hspace{.1in} \\text{rank}({L}_y) \\leq \\text{rank}({L}_y^\\star) \\\\ &\\text{rank}&(\\Theta_{yx}) \\leq \\text{rank}(\\Theta_{yx}^\\star); \\|\\mathcal{P}_{T({L}_y^\\star)^{\\perp}}({L}_y - {L}_y^\\star)\\|_2 \\leq \\frac{\\lambda_n}{2m\\psi^2} \\\\\n& &\\|\\mathcal{P}_{T(\\Theta_{yx}^\\star)^{\\perp}}(\\Theta_{yx} - \\Theta_{yx}^\\star)\\|_2 \\leq \\frac{\\lambda_n}{2m\\psi^2};  \\hspace{.1in} \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\Delta] \\leq 5\\lambda_n \\Big\\}\n\\end{eqnarray*}\n\n\n\n{\\noindent}The optimization program \\eqref{eqn:NConveProblem_N} is non-convex due to the rank constraints $\\text{rank}(L_y) \\leq \\text{rank}({L}_y^\\star)$ and $\\text{rank}(\\Theta_{yx}) \\leq \\text{rank}(\\Theta_{yx}^\\star)$ in the set $\\mathcal{M}$. These constraints ensure that the matrices ${L}_y$, and $\\Theta_{yx}$ belong to appropriate varieties. The constraints in $\\mathcal{M}$ along $T(L_y^\\star)^\\perp$ and $T(\\Theta_{yx}^\\star)^\\perp$ ensure that the tangent spaces $T({L}_y)$ and $T(\\Theta_{yx})$ are ``close'' to $T({L}_y^\\star)$ and $T(\\Theta_{yx}^\\star)$ respectively. Finally, the last condition roughly controls the error. We begin by proving the following useful proposition:\n\n\\begin{proposition}\n\\label{prop:FirstResultCor}\nLet $(\\Theta, D_y, {L}_y)$ be a set of feasible variables of \\eqref{eqn:NConveProblem_N}. Let $\\Delta = (D_y - D_y^\\star, {L}_y - {L}_y^\\star, \\Theta_{yx} - \\Theta_{yx}^\\star, \\Theta_{x} - \\Theta_{x}^\\star)$ and $C_1 = \\frac{12}{\\alpha_{}} + \\frac{1}{\\psi^2}$. Then, $\\Phi_{\\gamma}[\\Delta] \\leq C_1\\lambda_n$\n\\label{theorem:NonconvexImp}\n\\end{proposition}\n\\begin{proof}\nLet ${\\mathbb{H}}^\\star = W \\times T({L}_y^\\star) \\times T(\\Theta_{yx}^\\star) \\times {\\mathbb{S}}^{q}$. Then,\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}^\\star}(\\Delta)] &\\leq& \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}(\\Delta_{})] + \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{{\\mathbb{H}^\\star}^{\\perp}}(\\Delta)] \\\\\n&\\leq& 5\\lambda_n + m\\psi^2\\Big(\\frac{\\omega_{}\\lambda_n}{2m\\psi^2} + \\frac{\\omega_{}\\lambda_n}{2m\\psi^2}\\Big) \\leq 6\\lambda_n\n\\end{eqnarray*}\nSince $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}^\\star}(\\cdot)] \\leq 2\\Phi_{\\gamma}(\\cdot)$, we have that $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}^\\star}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}^\\star}(\\Delta)] \\leq 12\\lambda_n$. Consequently, we apply the Fisher information condition \\eqref{eqn:Fisher1} to conclude that $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}^\\star}(\\Delta)] \\leq \\frac{12\\lambda_n}{\\alpha}$. Moreover:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\Delta]& \\leq& \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}^\\star}(\\Delta_{})] + \\Phi_{\\gamma}[\\mathcal{P}_{{\\mathbb{H}^\\star}^{\\perp}}(\\Delta)] \\leq \\frac{12\\lambda_n}{\\alpha} + \\frac{\\lambda_n}{\\psi^2} = C_1\\lambda_n\n\\end{eqnarray*}\n\\end{proof}\nProposition~\\ref{theorem:NonconvexImp} leads to powerful implications. In particular, under additional conditions on the minimum nonzero singular values of ${L}_y^\\star$ and $\\Theta_{yx}^\\star$, any feasible set of variables $(\\Theta, D_y, {L}_y)$ of \\eqref{eqn:NConveProblem_N} has two key properties: $(a)$ The variables $(\\Theta_{yx}, {L}_y)$ are smooth points of the underlying varieties, $(b)$ The constraints in $\\mathcal{M}$ along $T({L}_y^\\star)^{\\perp}$ and $T(\\Theta_{yx}^\\star)^{\\perp}$ are locally inactive at $\\Theta_{yx}$ and $L_y$. These properties, among others, are proved in the following corollary.\n\n\\begin{corollary}\n\\label{eqn:Corollary1}\nConsider any feasible variables $( \\Theta, D_y, {L}_y)$ of \\eqref{eqn:NConveProblem_N}. Let $\\sigma_y$ be the smallest nonzero singular value of ${L}_y^\\star$ and $\\sigma_{yx}$ be the smallest nonzero singular value of $\\Theta_{yx}^\\star$. Let $\\mathbb{H}' = W \\times T({L}_y) \\times T(\\Theta_{yx}) \\times {\\mathbb{S}}^q$ and $C_{T'} = \\mathcal{P}_{\\mathbb{H}'^{\\perp}} (0, {L}_y^\\star, \\Theta_{yx}^\\star ,0)$. Furthermore, let $C_1 = \\frac{12}{\\alpha_{}} + \\frac{1}{\\psi^2}$, $C_2 = \\frac{4}{\\alpha_{}} (1+\\frac{1}{3\\beta})$, $C_{\\sigma_y} = C_1^2\\psi^2\\max\\{12\\beta + 1, \\frac{2}{C_2\\psi^2} + 1\\}$ and $C_{\\sigma_{yx}}' = C_1^2\\psi^2\\max\\{12\\beta + \\frac{6\\beta}{\\gamma}, \\frac{2}{C_2\\psi^2} + \\frac{6\\beta}{\\gamma}\\}$. Suppose that the following inequalities are met: $\\sigma_y \\geq  \\frac{{m}}{\\omega}C_{\\sigma_y}\\lambda_n$, \\\\ $\\sigma_{yx} \\geq  {m\\gamma^2}{}C_{\\sigma_{yx}}'\\lambda_n$. Then,\n\\begin{enumerate}\n\\item ${L}_y$ and $\\Theta_{yx}$ are smooth points of their underlying varieties, i.e. $\\rm{rank}({L}_y) = \\rm{rank}({L}_y^\\star)$, $\\rm{rank}(\\Theta_{yx}) = \\rm{rank}(\\Theta_{yx}^\\star)$; Moreover ${L}_y$ has the same inertia as ${L}_y^\\star$.\n\\item $\\|\\mathcal{P}_{T({L}_y^\\star)^{\\perp}}({L}_y - {L}_y^\\star)\\|_2 \\leq \\frac{\\lambda_n\\omega_{}}{48m\\psi^2}$ and $\\|\\mathcal{P}_{T(\\Theta_{yx}^\\star)^{\\perp}}(\\Theta_{yx} - \\Theta_{yx}^\\star)\\|_2 \\leq \\frac{\\lambda_n\\omega_{}}{48m\\psi^2}$\n\\item $\\rho(T({L}_y), T({L}_y^\\star)) \\leq \\omega_{}$; that is, the tangent spaces at $L_y$ is ``close\" to the tangent spaces $L_y^\\star$\n\\item $\\Phi_{\\gamma}[C_{T_{}'}] \\leq \\min\\{\\frac{\\lambda_n}{6\\beta\\psi^2},C_2\\lambda_n\\}$\n\\end{enumerate}\n\\end{corollary}\n\n\n\\begin{proof}\nWe note the following relations before proving each step: $C_1 \\geq \\frac{1}{\\psi^2} \\geq \\frac{1}{m\\psi^2}$, $\\omega_{} \\in (0,1)$, and $\\beta \\geq 8$. We also appeal to the results of  regarding perturbation analysis of the low-rank matrix variety \\cite{Bach2008}.\\\\\n\n1. Based on the assumptions regarding the minimum nonzero singular values of ${L}_y^\\star$ and $\\Theta_{yx}^\\star$, one can check that:\n\\begin{eqnarray*}\n\\sigma_y &\\geq& \\frac{ C_1^2\\lambda_n}{\\omega_{}} m\\psi^2({12\\beta+1})\n\\geq \\frac{ C_1\\lambda_n}{\\omega_{}}({12\\beta+1}) \\geq 8\\|L - L_y^\\star\\|_2\\\\\n\\sigma_{yx} &\\geq& {C_1^2\\lambda_n}{{}} \\gamma^2m\\psi^2{\\Big(\\frac{6\\beta}{\\gamma}+12\\beta\\Big)}\n\\geq  8\\|{\\Theta}_{yx} - \\Theta_{yx}^\\star\\|_2\n\\end{eqnarray*}\n\n{\\noindent}Combining these results and Proposition~\\ref{theorem:NonconvexImp}, we conclude that ${L}_y$ and $\\Theta_{yx}$ are smooth points of their respective varieties, i.e. $\\text{rank}({L}_y)= \\text{rank}({L}_y^\\star)$, and $\\text{rank}(\\Theta_{yx}) = \\text{rank}(\\Theta_{yx}^\\star)$. Furthermore, ${L}_y$ has the same inertia as ${L}_y^\\star$.\n\n\n2. Since $\\sigma_y \\geq 8\\|L_y - L_y^\\star\\|_2$, and $\\sigma_{yx} \\geq 8\\|\\Theta_{yx} - \\Theta_{yx}^\\star\\|_2$, we can appeal to Proposition 2.2 of \\cite{Chand2012} to conclude that the constraints in $\\mathcal{M}$ along $\\mathcal{P}_{T({L}_y^\\star)^{\\perp}}$ and $\\mathcal{P}_{T(\\Theta_{yx}^\\star)^{\\perp}}$ are strictly feasible:\n\\begin{eqnarray*}\n\\|\\mathcal{P}_{T({L}_y^\\star)^{\\perp}}({L}_y - {L}_y^\\star)\\|_2 &\\leq& \\frac{\\|{L}_y - {L}_y^\\star\\|_2^2}{\\sigma_y}\n\\leq \\frac{\\lambda_n}{48m\\psi^2} \\\\\n\\|\\mathcal{P}_{T(\\Theta_{yx}^\\star)^{\\perp}}(\\Theta_{yx} - \\Theta_{yx}^\\star)\\|_2 &\\leq& \\frac{\\|\\Theta_{yx} - \\Theta_{yx}^\\star\\|_2^2}{\\sigma_{yx}}\n\\leq \\frac{\\lambda_n}{48m\\psi^2}\n\\end{eqnarray*}\n\n\n3. Appealing to Proposition 2.1 of \\cite{Chand2012}, we prove that the tangent spaces $T(L_y)$ and $T(\\Theta_{yx})$ are close to $T(L_y^\\star)$ and $T(\\Theta_{yx}^\\star)$ respectively:\n\\begin{eqnarray*}\n\\rho(T({L}_y), T({L}_y^\\star)) &\\leq& \\frac{2\\| {L}_y - {L}_y^\\star \\|_2}{\\sigma_y}\n\\leq \\frac{2C_1\\lambda_n \\omega_{}}{C_1^2\\lambda_nm{\\psi}^2(12\\beta+1)}\n\\leq \\omega_{}\\\\\n\\end{eqnarray*}\n\n4. Letting $\\sigma_y'$ and $\\sigma_{yx}'$ be the minimum nonzero singular value of ${L}$ and $\\Theta_{yx}$ respectively, one can check that:\n\\begin{eqnarray*}\n\\sigma_y' \\geq \\sigma_y - \\|{L}_y - {L}_y^\\star\\|_2 \\geq 8C_1\\lambda_n \\geq 8\\|{L}_y - L_y^\\star\\|_2 \\[.1in]\n\\sigma_{yx}' \\geq \\sigma_{yx} - \\|\\Theta_{yx} - \\Theta_{yx}^\\star\\|_2 \\geq 8C_1\\lambda_n\\gamma \\geq 8\\|\\Theta_{yx} - {\\Theta}_{yx}^\\star\\|_2\n\\end{eqnarray*}\nOnce again appealing to Proposition 2.2 of \\cite{Chand2012} and simple algebra, we have:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}(C_{T'}) &\\leq& m \\|\\mathcal{P}_{T({L_y})^{\\perp}} ({L}_y - {L}_y^\\star)\\|_2 + m \\|\\mathcal{P}_{T(\\Theta_{yx})^{\\perp}} (\\Theta_{yx} - \\Theta_{yx}^\\star)\\|_2 \\\\ &\\leq& m\\frac{\\|{L}_y - {L}_y^\\star\\|_2^2}{\\sigma_y'} + m\\frac{\\|\\Theta_{yx} - \\Theta_{yx}^\\star\\|_2^2}{\\sigma_{yx}'} \\leq \\min \\{\\frac{\\lambda_n}{6\\beta\\psi^2}, C_2\\lambda_n\\}\n\\end{eqnarray*}\n\\end{proof}\n\n\\subsubsection{Variety Constrained Program to Tangent Space Constrained Program}\nConsider any optimal solution $(\\Theta^{\\mathcal{M}}, D_y^{\\mathcal{M}}, {L}_y^{\\mathcal{M}})$ of \\eqref{eqn:NConveProblem_N}. In Corollary~\\ref{eqn:Corollary1}, we concluded that the variables $(\\Theta^{\\mathcal{M}}_{yx}, {L}_y^{\\mathcal{M}})$ are smooth points of their respective varieties. As a result, the rank constraints  $\\rm{rank}({L}_y) \\leq \\rm{rank}({L}_y^\\star)$ and $\\rm{rank}(\\Theta_{yx}) \\leq \\rm{rank}(\\Theta_{yx}^\\star)$ can be ``linearized\" to ${L}_y \\in T(L^{\\mathcal{M}})$ and $\\Theta_{yx} \\in T(\\Theta^{\\mathcal{M}}_{yx})$ respectively. Since all the remaining constraints are convex, the optimum of this linearized program is also the optimum of \\eqref{eqn:NConveProblem_N}. Moreover, we once more appeal to Corollary~\\ref{eqn:Corollary1} to conclude that the constraints in $\\mathcal{M}$ along $\\mathcal{P}_{T({L}_y^\\star)^{\\perp}}$ and $\\mathcal{P}_{T(\\Theta_{yx}^\\star)^{\\perp}}$ are strictly feasible at $(\\Theta^{\\mathcal{M}}, D_y^{\\mathcal{M}}, {L}_y^{\\mathcal{M}})$. As a result, these constraints are locally inactive and can be removed without changing the optimum. Finally, we claim that the constraint $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\Delta] \\leq 5\\lambda_n$ in \\eqref{eqn:NConveProblem_N} can also removed in this ``linearized\" convex program. In particular, letting $\\mathbb{H}_{\\mathcal{M}} \\triangleq W \\times T({L}_y^{\\mathcal{M}}) \\times T(\\Theta^{\\mathcal{M}}_{yx}) \\times {\\mathbb{S}}^q$, consider the following convex optimization program with the constraint $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\Delta] \\leq 5\\lambda_n$ removed :\n\\begin{eqnarray}\n(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y) = {\\operatorname{\\arg\\!\\min}}_{\\substack{\\Theta \\in {\\mathbb{S}}^{q+p}, ~\\Theta \\succ 0 \\\\ D_y,{L}_y \\in {\\mathbb{S}}^p}} & -\\ell(\\Theta; \\{X^{(i)},Y^{(i)}\\}_{i=1}^n) + \\lambda_n [\\|{L_y}\\|_{\\star} + \\gamma \\|\\Theta_{yx}\\|_\\star] \\nonumber \\\\ \\mathrm{s.t.} & \\hspace{-0.1in} \\Theta_y = D_y - {L}_y, ~ (D_y, {L}_y, \\Theta_{yx}, \\Theta_{x}) \\in \\mathbb{H}_{\\mathcal{M}} \\label{eqn:ConvexRelaxed21_N}\n\\end{eqnarray}\n\n{\\noindent}We prove that under conditions imposed on the regularization parameter $\\lambda_n$, the pair of variables $(\\Theta^{\\mathcal{M}}, D_y^{\\mathcal{M}}, {L}_y^{\\mathcal{M}})$ is the unique optimum of \\eqref{eqn:ConvexRelaxed21_N}. That is, we show that $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}(\\tilde{S}_y - D_y^\\star, \\tilde{L}_y - {L}_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}, \\tilde{\\Theta}_{x} - \\Theta_{x}^\\star)] < 5\\lambda_n$.  Appealing to Corollary~\\ref{eqn:Corollary1} and Proposition~\\ref{prop:Enbound}, we have that $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T_{\\mathcal{M}}}]\\leq \\frac{\\lambda_n}{6\\beta}$, $\\Phi_{\\gamma}[C_{T_{\\mathcal{M}}}] \\leq C_2\\lambda_n$ and  (with high probability) $\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_{n}] \\leq \\frac{\\lambda_n}{6\\beta}$. Consequently, based on the bound on $\\lambda_n$ in assumption of Theorem~\\ref{proposition:lambdaBound}, it is straightforward to show that $r \\leq \\min\\{\\frac{1}{4C'}, \\frac{\\alpha}{16m{\\psi}C'^2}\\}$. Hence by Proposition~\\ref{lemma:Brower}, $\\Phi_{\\gamma}[\\Delta] \\leq \\frac{1}{2C'}$. Finally, we can appeal to Proposition~\\ref{lemma:Remainder} and the bound on $\\lambda_n$ to conclude $\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star} (\\mathcal{F}(\\Delta_{}))] \\leq {2m{\\psi}C'^2 \\Phi_{\\gamma}[\\Delta]^2} \\leq {2m{\\psi}C'^2C_1^2\\lambda_n^2} \\leq \\frac{\\lambda_n}{6\\beta}$. Based on the optimality condition of \\eqref{eqn:ConvexRelaxed21_N}, the property that $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}( . )] \\leq 2\\Phi_{\\gamma}(.)$, and the fact that $\\beta \\geq 8$, we have:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)] &\\leq& 2\\lambda_n + \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] + \\Phi_{\\gamma} [\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}} \\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T_{\\mathcal{M}}}]\\nonumber\\\\ &+& \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}} \\mathcal{F}^{\\dagger}E_{n}] \\\\\n &\\leq& 2\\lambda_n + 2 \\lambda_n\\Big(\\frac{1}{6\\beta} + \\frac{1}{6\\beta} + \\frac{1}{6\\beta}\\Big) \\leq 2\\lambda_n + \\frac{\\lambda_n}{\\beta} \\leq \\frac{17\\lambda_n^{}}{8}\n\\end{eqnarray*}\nFurthermore, by appealing to Fisher information condition \\eqref{eqn:Fisher2}, we have:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger} \\mathbb{I}^{\\star}\\mathcal{F}(\\Delta)] &\\leq& \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)] + \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}^\\perp}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)] \\\\ &+& \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}^\\perp}(\\Delta)]\\\\\n&\\leq& \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)] + (1-\\frac{3}{\\beta+1})\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}}(\\Delta)]  \\\\ &+& \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^\\star\\mathcal{F}C_{T_{\\mathcal{M}}}] \\leq \\frac{17\\lambda_n}{8} +\\frac{17\\lambda_n}{8}(1-\\frac{3}{\\beta+1}) + \\frac{\\lambda_n}{6\\beta} <  5\\lambda_n\n\\end{eqnarray*}\n\n\\subsubsection{From Tangent Space Constraints to the Original Problem}\n\nThe optimality conditions of \\eqref{eqn:ConvexRelaxed21_N} suggest that there exist Lagrange multipliers $Q_{D_y} \\in W$,  $Q_{T_y} \\in T(L_y^\\mathcal{M})^\\perp$, and $Q_{T_{yx}} \\in T(\\Theta_{yx}^\\mathcal{M})^{\\perp}$ such that\n\\begin{eqnarray*}\n[\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_y + Q_{D_y} = 0; \\hspace{.1in} [\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_y + Q_{T_y}  &\\in& \\lambda_n\\partial\\|\\tilde{L}_y\\|_\\star\\[.01in]\n[\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_{yx} + Q_{T_{yx}}  \\in -\\lambda_n\\gamma\\partial\\|\\tilde{\\Theta}_{yx}\\|_{\\star};  \\hspace{.1in} [\\Sigma_n - {\\tilde{\\Theta}}^{-1}]_{x}  &=& 0\n\\end{eqnarray*}\nLetting the SVD decomposition of $\\tilde{L}_y$ and $\\tilde{\\Theta}_{yx}$ be given by $\\tilde{L}_y = \\bar{U}\\bar{O}\\bar{V}'$ and $\\tilde{\\Theta}_{yx} = \\breve{U}\\breve{O}{\\breve{V}}'$ respectively, and $Z \\triangleq  (0, \\hspace{.1in} \\lambda_n\\bar{U}\\bar{V}', \\hspace{.1in}  -\\lambda_n\\gamma_{}{\\breve{U}}{\\breve{V}}',  \\hspace{.1in} 0)$, we can restrict the optimality conditions to the space $\\mathbb{H}_\\mathcal{M}$ to obtain, $\\mathcal{P}_{{\\mathbb{H}}_\\mathcal{M}}\\mathcal{F}^{\\dagger}(\\Sigma_n - \\tilde{\\Theta}^{-1}) = Z$.\nWe proceed by proving that the variables $(\\tilde{\\Theta}, \\tilde{D}_y, \\tilde{L}_y)$ satisfy the optimality conditions of the original convex program \\eqref{eqn:main}. That is:\n\\begin{center}\n\\begin{enumerate}\n\\item $\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}} \\mathcal{F}^{\\dagger}(\\Sigma_n - \\tilde{\\Theta}^{-1}) = Z$\n\\item $\\max \\Big\\{\\|\\mathcal{P}_{T_y'^\\perp} (\\Sigma_n - \\tilde{\\Theta}^{-1})_y\\|_2, \\frac{1}{\\gamma}{ \\|\\mathcal{P}_{T_{yx}'^{\\perp}} (\\Sigma_N - \\tilde{\\Theta}^{-1})_{yx}\\|_2}\\Big\\} < \\lambda_n$\n\\end{enumerate}\n\\end{center}\n\n\n{\\noindent}Here, $UDV'$ is the SVD decomposition of $\\tilde{L}_y$ and $\\breve{U}\\breve{D}\\breve{V}'$ is the SVD of $\\tilde{\\Theta}_{yx}$. It is clear that the first condition is satisfied since the pair $(\\tilde{\\Theta}, \\tilde{S}_y, \\tilde{L}_y)$ is optimum for \\eqref{eqn:ConvexRelaxed21_N} To prove that the second condition, we prove a stronger statement that $\\Phi_{\\gamma} [\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}^{\\perp}} \\mathcal{F}^{\\dagger}(\\Sigma_n - \\tilde{\\Theta}^{-1})] < \\lambda_n$. In particular, denoting $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - L_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_{x} - \\Theta_x^\\star)$, we show that:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_{\\mathcal{M}}^{\\perp}} \\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F} \\mathcal{P}_{\\mathbb{H}_\\mathcal{M}}(\\Delta)]  &<& \\lambda_n - \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}}\\mathcal{F}^{\\dagger} E_n] \\\\ &-& \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}} \\mathcal{F}^{\\dagger} R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] - \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}} \\mathcal{F}^{\\dagger} \\mathbb{I}^{\\star}\\mathcal{F}C_{T_\\mathcal{M}}] \\nonumber\n\\end{eqnarray*}\n\n{\\noindent}Using the first optimality condition and the fact that projecting into tangent spaces with respect to rank variety increase the spectral norm by at most a factor of two (i.e. $\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'}(.)] \\leq 2\\Phi_{\\gamma}[.]$), we have that:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}}\\mathcal{F}^{\\dagger}\\mathbb{I}^\\star\\mathcal{F}\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}}(\\Delta)]  &\\leq& \\lambda_n+ 2\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\Delta)] + 2\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star} \\mathcal{F}C_{T_{M}}]\\\\ &+& 2\\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}E_{n}] \\leq \\lambda_n^{} + \\frac{\\lambda_n}{\\beta} = \\frac{(\\beta+1)\\lambda_n}{\\beta}\n\\end{eqnarray*}\n\n{\\noindent}Applying \\eqref{eqn:Fisher2}, we obtain:\n\\begin{eqnarray*}\n\\Phi_{\\delta}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}} \\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F} \\mathcal{P}_{\\mathbb{H}_\\mathcal{M}}(\\Delta)] &\\leq& \\frac{(\\beta+1)\\lambda_n}{\\beta}\\Big(1 - \\frac{3}{\\beta+1}\\Big) = \\lambda_n - \\frac{2\\lambda_n}{\\beta}  < \\lambda_n - \\frac{\\lambda_n}{2\\beta}\\\\\n&\\leq& \\lambda_n - \\Phi_{\\gamma}[\\mathcal{F}^{\\dagger}R_{\\Sigma}(\\mathcal{F}(\\Delta))] - \\Phi_{\\gamma} [\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}C_{T_\\mathcal{M}}] - \\Phi_{\\gamma} [\\mathcal{F}^{\\dagger}E_{n}] \\\\\n&\\leq& \\lambda_n - \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}}\\mathcal{F}^{\\dagger}R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))] -  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}}\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star} \\mathcal{F}C_{T_\\mathcal{M}}]\\\\ &-&  \\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}_\\mathcal{M}^{\\perp}}\\mathcal{F}^{\\dagger}E_n]\n\\end{eqnarray*}\nHere, we used the fact that $\\|\\mathcal{P}_{T^\\perp}(.)\\|_2 \\leq \\|.\\|_2$ for a tangent space $T$ of the low-rank matrix variety. \n\n\n\n\n\\begin{thebibliography}{7}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\n\n\n\\bibitem{Bach2008}\n\\textsc{Bach, F.}\n\\newblock {{Consistency of trace norm minimization}}.\n\\newblock \\textit{Journal of Machine Learning Research}, 9:1019--1048, 2008.\n\n\n\\bibitem{Chand2012}\n\\textsc{Chandrasekaran, V., Parrilo, P. A. $\\&$ Willsky, A. S.}.\n\\newblock {{Latent Variable Graphical Model Selection via Convex Optimization}}.\n\\newblock \\textit{Annals of Statistics}, 40:1935--1967, 2012.\n\n\\bibitem{Davidson}\n\\textsc{Davidson, K.R. $\\&$ Szarek, S.J.}\n\\newblock {{Local operator theory, random matrices and {B}anach spaces}}.\n\\newblock \\textit{Handbook of the Geometry of Banach Spaces}, 1:317--366, 2001.\n\n\n\\bibitem{Kato1995}\n\\textsc{Kato, T.}\n\\newblock {{Perturbation theory for linear operators}}.\n\\newblock \\textit{Springer}, 1995.\n\n\n\\bibitem{Ravikumar}\n\\textsc{Ravikumar, P., Wainwright, M. J., Raskutti, G. $\\&$ Yu, B.}\n\\newblock{{High-dimensional covariance estimation by minimizing\n$\\ell_1$-penalized log-determinant divergence}}.\n\\newblock \\textit{Electronic Journal of Statistics}, 4:935--980, 2011.\n\n\n\n\\end{thebibliography}\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 95625, "prevtext": "\nUsing a similar decoupling technique, it is easy to check that:\n\\begin{eqnarray}\n\\Phi_{\\gamma} \\Big[ \\mathcal{P}_{\\mathbb{H}'^{\\perp}} [\\mathcal{F}^{\\dagger}\\mathbb{I}^{\\star}\\mathcal{F}(D_y, {L}_y, \\Theta_{yx}, \\Theta_{x})] \\Big] \\leq \\eta_2^\\star + \\frac{8\\alpha}{\\beta} \\leq \\frac{8\\alpha}{3\\beta} + \\frac{8\\alpha}{\\beta}\n\\label{bound2}\n\\end{eqnarray}\n\nLetting $Z = (D_y , L_y, \\Theta_{yx}, \\Theta_x)$, we use \\eqref{bound1} and \\eqref{bound2} to conclude:\n\\begin{eqnarray*}\n\\Phi_{\\gamma}[\\mathcal{P}_{\\mathbb{H}'^\\perp}{\\mathcal{A}}^{\\dagger} \\mathbb{I}^{\\star}{\\mathcal{A}}\\mathcal{P}_{{{\\mathbb{H}'}}} (\\mathcal{P}_{{{\\mathbb{H}'}}} {\\mathcal{A}}^{\\dagger}\\mathbb{I}^{\\star}{\\mathcal{A}}\\mathcal{P}_{{{\\mathbb{H}'}}})^{-1}(Z)]  &\\leq& \\frac{\\frac{8\\alpha}{3\\beta} + \\frac{8\\alpha_{}}{\\beta}}{2\\alpha_{} - \\frac{8\\alpha_{}}{\\beta}} \\leq 1-\\frac{3}{1+\\beta}\n\\end{eqnarray*}\n\n\\subsection{\\textit{Proof of main paper Proposition 1 -- bounding curvature of the matrix inverse}} Let $(\\tilde{D}_y, \\tilde{L}_y, \\tilde{\\Theta}_{yx}, \\tilde{\\Theta}_x)$ be an estimate for the population quantities \\\\$({D}_y^\\star, {L}_y^\\star, {\\Theta}_{yx}^\\star, {\\Theta}_x^\\star)$, and let $\\Delta = (\\tilde{D}_y - D_y^\\star, \\tilde{L}_y - L_y^\\star, \\tilde{\\Theta}_{yx} - \\Theta_{yx}^\\star, \\tilde{\\Theta}_x - \\Theta_x^\\star) \\subset {\\mathbb{S}}^p \\times {\\mathbb{S}}^p \\times {\\mathbb{R}}^{p \\times q} \\times {\\mathbb{S}}^q$, recall that the taylor expansion of the inverse of matrix perturbation is specified by:\n\n", "index": 23, "text": "$$ \\tilde{\\Theta}^{-1} = (\\Theta^\\star + \\mathcal{F}(\\Delta))^{-1} = {\\Theta^\\star}^{-1} + {\\Theta^\\star}^{-1}\\mathcal{F}(\\Delta){\\Theta^\\star}^{-1} + R_{\\Sigma^\\star}(\\mathcal{F}(\\Delta))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\Theta}^{-1}=(\\Theta^{\\star}+\\mathcal{F}(\\Delta))^{-1}={\\Theta^{\\star}}%&#10;^{-1}+{\\Theta^{\\star}}^{-1}\\mathcal{F}(\\Delta){\\Theta^{\\star}}^{-1}+R_{\\Sigma^%&#10;{\\star}}(\\mathcal{F}(\\Delta))\" display=\"block\"><mrow><msup><mover accent=\"true\"><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">~</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi mathvariant=\"normal\">\u0398</mi><mo>\u22c6</mo></msup><mo>+</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mmultiscripts><mi mathvariant=\"normal\">\u0398</mi><none/><mo>\u22c6</mo><none/><mrow><mo>-</mo><mn>1</mn></mrow></mmultiscripts><mo>+</mo><mrow><mmultiscripts><mi mathvariant=\"normal\">\u0398</mi><none/><mo>\u22c6</mo><none/><mrow><mo>-</mo><mn>1</mn></mrow></mmultiscripts><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mmultiscripts><mi mathvariant=\"normal\">\u0398</mi><none/><mo>\u22c6</mo><none/><mrow><mo>-</mo><mn>1</mn></mrow></mmultiscripts></mrow><mo>+</mo><mrow><msub><mi>R</mi><msup><mi mathvariant=\"normal\">\u03a3</mi><mo>\u22c6</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]