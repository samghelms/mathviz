[{"file": "1601.07392.tex", "nexttext": "\nWe note that $\\mathbf{H}$ is obtained as a function of $\\mathbf{m}$\nby solving certain partial differential equations \\cite[Section 2.2]{thesis-knittel}.\n\nWe can rewrite (\\ref{eq:eom}) using index notation as:\n\n", "itemtype": "equation", "pos": 26090, "prevtext": "\n\n\n\\setcopyright{acmcopyright}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Nmag micromagnetic simulation tool -- software engineering lessons learned\n  \n\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\numberofauthors{3} \n\n\n\n\\author{\n\n\n\n\n\n\n\n\n\n\n\n\\alignauthor\nHans Fangohr\\\\\n       \\affaddr{University of Southampton}\\\\\n       \\affaddr{SO17 1BJ Southampton}\\\\\n       \\affaddr{United Kingdom}\\\\\n       \\email{\\small  fangohr@soton.ac.uk}\n       \n\\alignauthor\nMaximilian Albert\\\\\n       \\affaddr{University of Southampton}\\\\\n       \\affaddr{SO17 1BJ Southampton}\\\\\n       \\affaddr{United Kingdom}\\\\\n       \\email{\\small maximilian.albert@soton.ac.uk}\\\\\n\n\\alignauthor\nMatteo Franchin\\\\\n       \\affaddr{Cambridge}\\\\\n       \\affaddr{United Kingdom}\\\\\n       \\email{\\small  franchinm@gmail.com}\n}\n       \n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\\begin{abstract}\nWe review design decisions and their impact for the open source code\nNmag from a software engineering in computational science point of view. Key lessons to learn\ninclude that the approach of encapsulating the simulation\nfunctionality in a library of a general purpose language, here Python,\neliminates the need for configuration files, provides greatest\nflexibility in using the simulation, allows mixing of multiple\nsimulations, pre- and post-processing in the same (Python) file, and\nallows to benefit from the rich Python ecosystem of scientific\npackages. The choice of programming language (OCaml) for the\ncomputational core did not resonate with the users of the package (who\nare not computer scientists) and was suboptimal. The choice of Python\nfor the top-level user interface was very well received by users from\nthe science and engineering community. The from-source installation in\nwhich key requirements were compiled from a tarball was remarkably\nrobust. In places, the code is a lot more ambitious than necessary:\nkey routines work for $n$-dimensional space, but in simulations so far\nonly 1d, 2d and 3d simulations were needed. Tests distributed with the\npackage are useful, although more unit tests would have been\ndesirable. The detailed documentation, together with a tutorial for\nthe usage of the system, was welcomed by the community.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\\begin{CCSXML}\n<ccs2012>\n<concept>\n<concept_id>10011007.10011074</concept_id>\n<concept_desc>Software and its engineering~Software creation and management</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n<concept>\n<concept_id>10010147.10010341</concept_id>\n<concept_desc>Computing methodologies~Modeling and simulation</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010147.10010341.10010349.10010358</concept_id>\n<concept_desc>Computing methodologies~Continuous models</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010405.10010432.10010439.10010440</concept_id>\n<concept_desc>Applied computing~Computer-aided design</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010405.10010432.10010441</concept_id>\n<concept_desc>Applied computing~Physics</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n</ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Software and its engineering~Software creation and management}\n\\ccsdesc[300]{Computing methodologies~Modeling and simulation}\n\\ccsdesc[100]{Computing methodologies~Continuous models}\n\\ccsdesc[100]{Applied computing~Computer-aided design}\n\\ccsdesc[100]{Applied computing~Physics}\n\n\n\n\n\n\n\n\n\n\n\n\\printccsdesc\n\n\n\n\n\\keywords{Nmag, Computational Science Software Engineering, Python, Finite Elements}\n\n\n\n\\section{Introduction}\n\\label{sec-1}\n\nThe Nmag software was developed from 2005 to early 2012, and first released in 2007 as\nopen source code. It has since been in use for nearly a decade, and we\ncan begin to summarize some of the lessons we have learned, in\nparticular with respect to the software engineering choices and the\nsuccess of the project.\n\n\\bigskip\n\nNmag is a micromagnetic simulation package. Micromagnetics is a\ncontinuum theory of magnetization at the nano- and micrometre\nscale. The magnetization is a continuous 3d vector field defined\nthroughout a ferromagnetic body. The dynamics of this vector field are determined\nby an equation of motion, which depends on solving a non-linear partial\nintegro-differential equation. Using spatial discretization, this\npartial differential equation (PDE) can be solved numerically. Nmag\nuses a combination of finite element and boundary element\nmethods \\cite[Section 2.2]{thesis-knittel} to\nsolve the integro-PDE in every time step. Within this finite element\nmodel, the magnetization is represented as 3d-vector degrees of\nfreedom at every finite element node.  For the time\nintegration, the degrees of freedom at all nodes are treated as a\nsystem of coupled ordinary differential equations, in line with the related\nopen source software Magpar \\cite{scholz2003} and other tools that are\nnot freely available.\n\nNmag is summarized in a short paper \\cite{fischbacher2007-nmag}.\nMore technical detail, in particular on the underlying multi-physics\nfinite element library \\texttt{nsim}, is given in a manuscript available on\nthe arXiv \\cite{fischbacher2009-nsim}. Some aspects of the parallel\nexecution model have been published\n\\cite{fischbacher2009-parallel}. The project home page\n\\cite{nmag-code-2012} contains the code as a tar-file. For reference,\nthe repository from which the tarball is built, is available as a\nMercurial repository on Bitbucket \\cite{nmag-code-bitbucket}. A number\nof PhD students have contributed to the software, and their theses\nare an additional useful source of information, covering an overview\nof the capabilities of the code \\cite[chapter 5]{thesis-franchin},\nlong-range demagnetization field calculation, boundary elements and\nmatrix compression \\cite{thesis-knittel}, macro-geometry periodic\nboundary conditions and mesh generation \\cite{thesis-bordignon}.\n\n\\bigskip\n\nComputational micromagnetics became feasible and accessible with the\npublic domain release of the OOMMF Software by the National Institute\nof Standards and Technology (NIST) in the late 90s\n\\cite{oommf1999}. The OOMMF software uses a finite difference\ndiscretization (with different strengths and weaknesses than the finite\nelement discretization but outside the scope of this paper), which is\nbased on C++ code with a Tcl and Tk interface.\nA simulation run can be configured through a graphical user interface (see\nfigure \\ref{fig:oommf-problem-editor} for an example), or through\nsetting parameters in a configuration file which uses Tcl syntax.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.65\\columnwidth]{figure1.png}\n\\caption{The OOMMF graphical user interface for problem definitions \\label{fig:oommf-problem-editor}}\n\\end{figure}\n\nThe aim of the Nmag package was, among other things, to provide a\nfinite element based discretization approach to complement the OOMMF\ntool. Simultaneously, and without being aware of each other's\nefforts, the finite element Magpar code was developed, and later\nreleased as open source \\cite{scholz2003}.\n\nIn this report, we focus on software engineering aspects of the\nNmag tool, including the user interface, the choice of languages and\ntools, and the open source model. We summarise the Nmag project in\nsection \\ref{sec:nmag}, before addressing the lessons learned in\nsection \\ref{sec:lessonslearned}.\n\n\\section{Nmag project summary}\n\\label{sec-2}\n\\label{sec:nmag}\n\n\\subsection{Architecture}\n\\label{sec-2-1}\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{figure2.pdf}\n\n\\caption{Nmag architecture overview}\n\\label{fig:nmag-design}\n\\end{figure}\n\nFigure \\ref{fig:nmag-design} summarizes the Nmag architecture: the\nblue box at the top represents the (Python) code that the end user\nassembles to define the simulation computation; this includes material\nparameters, the file name of the finite element mesh to define the\ngeometry, the physical process that should be simulated etc. The\ninstructions are written as a Python script that makes use of the \\texttt{nmag}\nPython library (top green box), which in turn composes its\nfunctionality from the (Python) \\texttt{nsim} multi-physics library. The Python\nNsim multi-physics library is an interface to the functionality\nprovided through the implementation (bottom green box) in OCaml \\cite{OCaml}. We\nmake use of existing high performance computing libraries such as\nPETSC for sparse and dense matrix calculation,\nPARMETIS for partitioning the mesh across multiple\nMPI processes, and the CVODE time integration tools that come with the\nSUNDIALS tool suite. The code is parallelized with MPI.\n\n\nThe code implements a dependency engine for physical fields\n\\cite[Section 4.4]{fischbacher2009-nsim}. This allows lazy\nevaluation to be used to only compute entities when they are truly\nrequired, and to minimize the computation of fields that\ndepend on each others.\n\nPeriodic boundary conditions are difficult in micromagnetic\nsimulations due to the long range nature of the magnetostatic\ninteractions. A new computational model has been developed and\nimplemented (the 'macro geometry' \\cite{fangohr2009-macrogeometry}),\nwhich has subsequently been used by the Nmag successor code Finmag \\cite{Finmag}, and other\nmicromagnetic tools such as the GPU-based package Mumax3\n\\cite{mumax3}.\n\n\\subsection{Time line}\n\\label{sec-2-2}\n\nThe planning for Nmag started around 2003, funding was secured in\n2005, and the work started soon afterwards. The first version was\npublicly released in 2007. The tool was actively maintained and\nfurther developed until January 2012, when key developers moved on and\nno further funding was available. Since then, the software has been\nhosted `as is'.\n\n\\subsection{Uptake}\n\\label{sec-2-3}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.85\\columnwidth]{figure3.pdf}\n\\caption{Nmag citations from Web of Science (12 January 2016). }\n\\label{fig:nmag-citations}\n\\end{figure}\n\n\nAs of January 2016, the official Nmag publication\n\\cite{fischbacher2007-nmag} has been cited 102 times in publications\n(Web of Science, Thomson Reuters) and 177 times on Google\nScholar. More than 150 users are known by name (from the mailing list,\nor off-line queries), and the web site is frequented from academic and\nindustry domain names, with 45000 visits since 2006. Figure\n\\ref{fig:nmag-citations} shows the breakdown of citations per\nyear. Development and maintenance stopped early 2012, which (so far)\nseems to not have affected the usefulness of the tool for research.\n\n\\subsection{Nmag team}\n\\label{sec-2-4}\n\nThe principal funding for the project was for a research fellow for\n2.5 years. The team managed to find 3 PhD student projects that\nsupported the lead developer as part of their work\n\\cite{thesis-bordignon,thesis-franchin,thesis-knittel}, and in the\nlater phases there were smaller contributions from others. Later\n(2009-2012), a research fellow from another project contributed\nsignificantly to the project. The background of the team was in\nphysics and mathematics. None of the participants had any formal\ntraining in software engineering, which is a common situation in\nthe development of research software.\n\n\\subsection{Version control and source}\n\\label{sec-2-5}\n\nDevelopment of Nmag started in 2005 and has used three different version control\nsystems through its history: starting with CVS, before changing to\nSubversion, before switching to Mercurial in 2010. Initially, the\nsource code was included in the tarballs containing the 'source'\nreleases of the project, and later the Mercurial repositories have\nbeen made available on Bitbucket \\cite{nmag-code-bitbucket}. There are\nseparate repositories for the source, the tests, the documentation, the\nwebpage, the distribution, and one additional meta repository that\nprovides a script to clone them all together into the right relative\nsubdirectory structure.\n\\subsection{Community support}\n\\label{sec-2-6}\n\nCommunity support involves the following tools and strategies: the\nNmag webpage hosts the code, installation instructions, the manual\n\\cite{nmag-code-2012}, a link to the mailing list archives\n\\cite{mailing-list-archives} and the Redmine Wiki. The University of\nSouthampton is hosting the mailing list, and Google Groups is used to\narchive all communication in that mailing list. A Redmine server is\nused simply to host a Wiki that users can edit\n(\\url{https://nmag.soton.ac.uk/community/wiki/nmag}).\n\n\\subsection{Software engineering process}\n\\label{sec-2-7}\n\nParts of the software were written in an effectively plan-driven\napproach, broken into separate requirement analysis, design,\nimplementation and testing phases. In particular the Nsim\nmulti-physics core was realized as one large piece of work by the lead\ndeveloper without significant subsequent change.\n\nOther parts, in particular the Python-level micromagnetic interface\n\\texttt{nmag} were developed in a more agile style, with multiple\niterations of development, use of automated tests (section\n\\ref{sec:testing}), where both refactoring and additional feature\nimplementation was carried out in subsequent iterations.\n\n\\section{Lessons learned}\n\\label{sec-3}\n\\label{sec:lessonslearned}\n\\subsection{User interface through Python library}\n\\label{sec-3-1}\nA key design decision was to embed the functionality of the simulation\ninto a general purpose language, in this case Python (see also\n\\cite[section 5.11.1]{fischbacher2009-nsim}).\n\nFigure \\ref{fig:nmag-code-example1} shows an Nmag simulation script,\nwhich is a Python script that imports and uses the \\texttt{nmag}\nlibrary.\n\n\\begin{figure}\n\\footnotesize\n\\includegraphics[width=1\\columnwidth]{figure4.pdf}\n\\normalsize\n\\caption{Nmag end-user script example. \\label{fig:nmag-code-example1}}\n\\end{figure}\n\n\n\n\n\n\nIn comparison to definition of simulation configurations through\nconfiguration text files or graphical user interfaces (see for example Figure~\\ref{fig:oommf-problem-editor}), this approach has a number of\nadvantages: (i)~no parser needs to be written -- Python is the parser,\n(ii)~the user has complete freedom in using Python constructs\nto combine the simulation commands provided by the \\texttt{nmag} library\nas needed for the particular application,\n(iii)~data pre- and post-processing, and calculations that take place\nduring the simulation can make use of the Python ecosystem of\navailable scientific libraries, (iv)~the configuration of the\nsimulation can make use of Python functions to provide, for example,\ninitial magnetization vector field configurations, that show a\ncomplicated spatial dependence. (See also section 5.11.2 in\n\\cite{fischbacher2009-nsim}).\n\\subsection{Choice of programming languages}\n\\label{sec-3-2}\n\n\nFigure \\ref{fig:cloc-output} shows some statistics regarding the\nprogramming languages / tools and respective lines of code in the Nmag\nproject.\n\nNmag uses Python as the language for the user to define how the\nmicromagnetic simulation should be carried out. It should be noted\nthat the micromagnetic capabilities as well as the testing\ninfrastructure and system tests are implemented at the Python level,\nwhile the OCaml code provides a generic multi-physics simulation\nenvironment and contains significant parts of the multi-physics finite\nelement code. The OCaml multi-physics engine is capable of solving\nproblems in areas outside micromagnetism.\n\n\n\n\nPython was a good choice for the high level user-interface, and\ninternals of the package: it is a user-friendly language\n\\cite{fangohr2004comparison} that has gained substantially in\npopularity in computational science and elsewhere since 2005;\nresulting in additional benefits from the critical mass of\nusers. Python is a high productivity language in computational\nscience. In micromagnetism, packages developed after Nmag have\nfollowed the model of providing a Python library to offer their\nfunctionality \\cite{MagnumFE,Finmag}.\n\nFor Objective Caml (OCaml) as the work horse of the multi-physics\nengine, the situation is less clear. There are technical reasons why\nOCaml is a good choice, including its C interoperability interface,\nits (interactive) interpreter and its native code compiler. OCaml also\noffers automatic memory management, expressive power and functional\ncapabilities (see section 5.13 in \\cite{fischbacher2009-nsim}).\n\nIn hindsight, we have identified a disadvantage of a social\nengineering nature: OCaml is not a wide-spread language (certainly not\noutside computer science and mathematics), and there are virtually no\nusers of the Nmag software that have OCaml experience. Furthermore,\nOCaml is very rarely taught and due to its (powerful but somewhat\nnon-mainstream) functional style, it presents a steep learning curve\nto the typical Nmag user, who tends to be a researcher in material\nscience, engineering, physics, biology, geography and medicine (but\ngenerally not a computer scientist). For an open source project, it is\nimportant that the code is accessible by the user community to attract new contributors.\n\nComparison of execution performance of compiled OCaml code with C/C++ code\nhave shown that the OCaml code can be noticeably slower than the C\ncode. We address this point in more detail in\nsection \\ref{sec:ocaml-performance}.\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{lrrrr}\nLanguage & files & comment lines & code lines\\\\\n\\hline\nOCaml & 174 & 15111 & 53445\\\\\nPython & 588 & 17718 & 49286\\\\\nC & 49 & 2548 & 12375\\\\\nBourne Shell & 47 & 1232 & 9184\\\\\nmake & 138 & 391 & 2831\\\\\nC/C++ Header & 14 & 410 & 820\\\\\n\\hline\nSUM: & 1010 & 37410 & 127941\\\\\n\\end{tabular}\n\\end{center}\n\n\n\n\\caption{Output of \\texttt{cloc} (v1.65) run on Nmag source code, tests, and documentation files.\\label{fig:cloc-output}}\n\\end{figure}\n\\subsection{Python interpreter activated from OCaml}\n\\label{sec-3-3}\nFor technical reasons and the capabilities of the existing PyCaml\ninterface \\cite{pycaml} that connects OCaml and Python, the Nmag setup\nis such that the Python interpreter is called from within an OCaml\nexecutable.\n\nIn more detail: the users starts an executable called \\texttt{nsim}\ncompiled from OCaml code. This executable initializes the simulation\nengine, and then calls an embedded Python interpreter, which processes\nthe user's simulation commands (which are typically given through a\nfile \\texttt{mysim.py} to the \\texttt{nsim} executable, \\emph{i.e.} the\ncommand line call would be \\texttt{nsim mysim.py}). If no file to\nprocess is given, a Python interpreter is\ndisplayed in which Python commands can be entered interactively, and\nthe \\texttt{nmag} library is accessible.\n\nAn alternative setup would be that the user starts the 'usual' Python\ninterpreter, and that a \\texttt{nmag} package can be imported, which\ncarries out the housekeeping work, initialization of MPI and and\nexecution of required OCaml code when imported.\n\n\nWith hindsight, we suggest that the alternative arrangement would have\nbeen much preferable for a number of reasons: (i) \\texttt{nsim}\nactivates a Python interpreter (in which the \\texttt{nmag} library is\naccessible), so it will seem more logical to the user if that command\nis \\texttt{python} and not \\texttt{nsim}. (ii) The Python interpreter\ncoming through \\texttt{nsim} is in general different from the system\nPython interpreter (and with the current install, see section\n\\ref{sec:installation}, this Python interpreter is built from source\nand installed locally); and third party Python modules (such as\n\\texttt{numpy}, \\texttt{scipy}, \\texttt{matplotlib}) may end up having\nto be installed separately for both interpreters.\n\n\\subsection{OCaml performance}\n\\label{sec-3-4}\n\\label{sec:ocaml-performance}\n\nThe OCaml multi-physics engine subdivides the computation into two\nphases: initialisation and time integration. In the first phase, a set of\nsparse and dense matrices are pre-computed and stored in memory. These\nmatrices capture the various physical interactions that govern the\nbehavior of the simulated system and are mainly computed by OCaml\ncode. In the second phase, the time integration is carried out, using\nthe pre-computed matrices. These computations are mainly done using the\nPETSC library.\nMicromagnetic simulations of real materials and devices require fine\nmeshes corresponding to pre-computed matrices of size of tens of\nGigabytes and matrix assemble times of the order of hours. It is\ntherefore important to make the OCaml initialisation phase fast and\nefficient. Investigations of the OCaml engine code performance revealed\nsome limitations of the language and its compiler when used in our\nparticular context, which eventually led us to rewrite parts of the\nmatrix initialization code in C.\n\nFirst we comment on array efficiency. OCaml native arrays are\nunidimensional. A rank-2 tensor (i.e. a matrix) can still be represented\nas an array of arrays of floats which, however, is not memory efficient:\nthe floats are not stored in contiguous areas of memory, requiring\nextra memory for storing pointers and thus additional indirections\nduring array accesses. Moreover, the sub-arrays are not guaranteed to\nhave the same lengths. This makes multi-dimensional arrays difficult to\nanalyze and optimize for a compiler. While the \\texttt{Bigarray} module\nin OCaml provides multi-dimensional arrays which are stored contiguously\nin memory and thus allow to overcome some of these problems, accesses to\nOCaml big-arrays are not inlined and this leads to poor\nperformance \\cite{ocaml-performance}.\n\nSecond, we have found that OCaml has limited support for some\noptimisations that are particularly useful in numerical code, such as\nbounds-checking elimination, loop unrolling, and vectorisation. These\noptimization techniques are now common in mainstream languages and are\nperformed by freely available compilers such as \\texttt{gcc} and\n\\texttt{clang}, which of course receive a vastly greater investment\nand contribution from the private software industry. We include\nelectronic supplementary material \\cite{ocaml-performance} with sample\ncode that underpins the results reported in this section, and which shows\nthat rewriting an OCaml loop in C/C++ can give speedups of a\nfactor 4. This is significant because performance critical numerical\ncode often consist of simple for-loops.\n\\subsection{Symbolic derivation of PDE calculations at run-time}\n\\label{sec-3-5}\n\\label{sec:PDEs}\n\nIn finite elements, a mathematical problem in form of a PDE is solved\nin a given number of spatial dimensions $n$; these are typically\n$n=3$, or $n=1$, $n=2$, corresponding to 3d space as we know it, and\nreduced models where a 1d or 2d space is sufficient. On these\n$n$-dimensional domains, we operate with scalars, vectors or tensors,\nwhich can have their dimensionality $k$. Finally, there is a variety\nof basis functions and (taking only continuous Galerkin elements as an\nexample), these have their own polynomial order $p$.\n\nTraditionally, the right equation for a particular mathematical\noperator is derived with pen and paper for particular values of the\ndimensions of space $n$, the dimensionality of the degree of freedom\n$k$ and basis function order $p$ . Once the equation has been derived,\nit is hard-coded as an implementation for this specific case. This\ncode is then used to populate the finite element matrices. In the\nmicromagnetic context, the Magpar package \\cite{scholz2003} is such an\nexample.\n\nNmag's approach is different: here, the relevant analytic operations,\nwhich include differentiation and integration, are carried out\nsymbolically (within the Nsim OCaml code base) to generate specialized\ninstructions to compute the matrix entries for the particular\noperator, dimensionality, basis function order etc. that the user\nrequires. The \\texttt{nsim} code supports arbitrary basis function orders $p$,\ndimensionality of the degree of freedom $k$ and and arbitrary\ndimensionality $n$ of the domain.\n\nThis approach provides much greater flexibility to change the equations\nin the model (an important consideration in the context of exploratory research),\nor the numerical model parameters such as\nthe order of the basis functions. It also avoids repetitive manual\nanalytical work, and -- assuming the symbolic computations are\nimplemented correctly -- reduces the chance of errors. We discuss the\nassociated additional complexity in section\n\\ref{sec:arbitrary-number-of-dimensions}.\n\n\\subsection{Auto-generation of code for local field mappings}\n\\label{sec-3-6}\n\nThe primary entities of interest in finite element simulations are \\emph{fields},\nsuch as the magnetization vector field, the temperature scalar field,\na displacement vector field, etc.\nNmag uses auto-generation of code at run time to allow the user to\ncompute tailored expressions that map from one field to another field\n(details in \\cite[section 5.11.4]{fischbacher2009-nsim}).\nThere are two possible ways of achieving this:\n\n(i) The user provides C code that contains the mathematical mapping\noperation that is required.  At run time (as the user-provided string\nis not know before then), this user-provided C-code string is embedded\ninto a template that provides access to the relevant field arrays and\naccess methods, and the combined C-code is written to disk, compiled, and\ndynamically linked. The auto-generated code will automatically translate\nthe user provided indices to the right memory locations, which is\nnon-trivial for multi-physics simulations where multiple fields are\ndefined at every node.\n\n(ii) The user can also provide algebraic expressions which represent the\nrequired operation, which are automatically translated into C code. We\nprovide an example to demonstrate this. In the micromagnetic problem,\nthere is a magnetization vector field $\\mathbf{m}(\\mathbf{r})$ that\ndefines a 3d vector at every point $\\mathbf{r}$ in 3d space. In this\nexample, we look at the mapping of the magnetization $\\mathbf{m}$ onto\nits time derivative $\\frac{{\\ensuremath{\\mathrm{d}}} \\mathbf{m}}{{\\ensuremath{\\mathrm{d}}} t}$ as is necessary to\ncompute the equation of motion (\\ref{eq:eom}) for this magnetization\nvector function $\\mathbf{m}(\\mathbf{r})$:\n\n", "index": 1, "text": "\\begin{equation}\n\\frac{{\\ensuremath{\\mathrm{d}}} \\mathbf{m}}{{\\ensuremath{\\mathrm{d}}} t} = c_1 \\mathbf{m}\\times\\mathbf{H} + c_2\\mathbf{m} \\times (\\mathbf{m}\\times\\mathbf{H}) \\label{eq:eom}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\frac{{\\mathrm{d}}\\mathbf{m}}{{\\mathrm{d}}t}=c_{1}\\mathbf{m}\\times\\mathbf{H}+c%&#10;_{2}\\mathbf{m}\\times(\\mathbf{m}\\times\\mathbf{H})\" display=\"block\"><mrow><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>\ud835\udc26</mi></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mrow><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>\u2062</mo><mi>\ud835\udc26</mi></mrow><mo>\u00d7</mo><mi>\ud835\udc07</mi></mrow><mo>+</mo><mrow><mrow><msub><mi>c</mi><mn>2</mn></msub><mo>\u2062</mo><mi>\ud835\udc26</mi></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc26</mi><mo>\u00d7</mo><mi>\ud835\udc07</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07392.tex", "nexttext": "\n\nFor this formulation, \\texttt{nsim} provides a small domain specific language. We show how\nequation (\\ref{eq:index}) is represented as a string in this domain specific language:\n\\begin{verbatim}\ndmdt = \"\"\"\n\ndmdt(i)   <-   c1 * eps(i, j, k) * m(j) * H(k)\n             + c2 * eps(i, j, k) * m(j)\n             * eps(k, p, q) * m(p) * H(q)\"\"\"\n\\end{verbatim}\n\n\n\\smallskip\n\nWe have found the interface (ii) useful to quickly and flexibly extend\nthe equation of motion. The ability to specify C code directly through\nmethod (i) allows to sidestep the \\texttt{nsim} framework where functionality\nis required that was not anticipated initially.  By using C code\n(rather than Python code, say) good performance is achieved, in\nparticular when loops over all degrees of freedom are involved.\n\\medskip\n\nA very similar mechanism to method (i) is provided in the multi-physics\nfinite element library FEniCS \\cite{fenics} through\nthe \\texttt{instant} module, which can be used to initialize fields with\narbitrary C expressions. An approach similar to (ii) is used in FEniCS\nfor the non-local PDE operators.\n\nFEniCS \\cite{fenics} started being developed at the same time as Nsim and is\nnow widely used, including\nin the micromagnetic package Magnum.fe \\cite{MagnumFE} and the Nmag\nsuccessor software Finmag \\cite{Finmag}. FEniCS has core routines\nwritten in C++ and provides a Python interface.\n\n\\subsection{Parallel execution model}\n\\label{sec-3-7}\nThe parallel execution model of Nmag is that only one Python process\nis running, driving slaves through MPI from within the OCaml code\n\\cite{fischbacher2009-parallel}. This allows end-users to write truly\nsequential Python code and to completely ignore the parallel execution\nof the micromagnetic equations. This is different from the FEniCS\nparallel model \\cite{fenics}, where also the Python code executes in\nparallel. While the FEniCS model requires more thought at the Python\nlevel, it allows to add computationally demanding operations to be\nexecuted in parallel through Python. For expert users and scalability,\nthe FEniCS model is preferable.\n\n\\subsection{Complexity originating from generality}\n\\label{sec-3-8}\n\\subsubsection{Arbitrary number of dimensions}\n\\label{sec-3-8-1}\n\\label{sec:arbitrary-number-of-dimensions}\n\nAs introduced in section \\ref{sec:PDEs}, in finite elements, some\nmathematical problem is solved in a given number of spatial dimensions\n$n$; where the \\texttt{nsim} code supports calculations in arbitrary number of spatial dimensions.\n\nWhile there are problems defined on space that is higher dimensional\nthan 3d space in science and engineering, none of that functionality\nhas been used in the lifetime of the Nmag software. The complexity of\nthe code could have been reduced (and maintainability and\naccessibility increased) if we had limited its functionality to\nspatial dimensions $n$ of 1, 2 and 3.\n\nWe note for context that the FEniCS \\cite{fenics} multi physics\nlibrary has followed a similar path of using symbolic calculation at\nrun time to derive finite element matrix entries, and that the FEniCS\nfunctionality is limited to dimensions of 3 or less.\n\n\n\\subsubsection{Arbitrary high level language support}\n\\label{sec-3-8-2}\n\nThe operator notation used in (the Python) \\texttt{nsim} library is\nrepresenting differential operators through a string (see section 4.3.1 and\nexample A.2 in \\cite{fischbacher2009-nsim}). An alternative would be\nto create classes in Python that represent mathematical objects and\ndifferential operators, and use operator overloading to integrate the\nmathematical description naturally within the Python language. The\nFEniCS project \\cite{fenics} has followed the latter path with their\nPython interface \\cite{dolfin}.\n\nThe motivation behind the design decision to prefer the string\nrepresentation over the (object-oriented) operator representation for\nthe differential equation was to avoid coupling the notation too\ntightly to the Python scripting language: by sticking to strings, one\ncan substitute Python by another language more easily as and when this\nwould be necessary. Note that when Nmag was created Python was by far\nnot as mainstream in scientific computing as it is today; for example,\nthe Python numerical library \\texttt{numpy} was only created in 2005.\n\nFrom a user's point of view, we believe that it is preferable to use\nPython objects over strings to define the differential equations: this\nis more natural and allows to exploit auto-completion and to explore\ncapabilities and documentation of objects when working\ninteractively with the Python prompt.\n\nIt may have been beneficial to approach this design question in a more agile\nway by fully buying into the Python language and the overloaded\noperator notation initially and revisiting the decision as the\nproject evolved. As a decade has passed, we now know that\nthere is no need to introduce another high level language for Nmag:\nPython is doing fine, and the development of the Nmag project has\nstopped anyway. Another possibility would be to introduce a bridge\nelement that translates the object oriented equations into strings\nwhen required, which would allow combining the long term strategy\nof expressing differential equations through strings with the ability\nto offer operator overloading in Python to the user.\n\n\\subsubsection{Multi-physics capabilities}\n\\label{sec-3-8-3}\n\nNmag is built on the multi-physics library Nsim, and thus the\nmicromagnetic model that is available through Nmag is only one of\nmany possible types of PDE-based simulations that could be built\non top of \\texttt{nsim}.\n\nThe multi-physics capabilities are indeed unique in the micromagnetic\nsimulation landscape, but bring significant additional complexity into the\nsoftware.\n\nWhile we attempted to immediately implement a multi-physics framework,\nand then build a micromagnetic simulation on top of this, it may have\nbeen more efficient to build a micromagnetic prototype first, and use\nthe experience gathered with this in developing a more generic\nframework.\n\nWe note that as the multi-physics capabilities were not part of the\nfunded research programme, they have not been developed and documented\nto the level of the core micromagnetic functionality, and are thus not\nused widely.\n\n\\subsubsection{Associating numbers with units}\n\\label{sec-3-8-4}\nAll Nmag input physical entities have to be expressed as a product of\na number and product of powers of dimensions (such as m, kg, s, A, K,\nmol, cd). For example, the Python expression\n\\texttt{x = SI(100e-9, \"m\")}\ndescribes a length of 100 nano meters. For multi-physics\nsimulations, this provides great advantages as the output fields\nemerging from user-defined operations automatically carry the right\nunits. It also allows to scale numbers internally (to reduce chances\nof overflow etc), and to use arbitrary unit systems (such as SI, CGS,\nor custom). In the context of micromagnetics -- Nmag's application domain -- the capability met a\nmixed reception: some users find it useful, others dislike the\noverhead of having to type the dimensions, when they normally use SI\nunits (as is enforced in the OOMMF \\cite{oommf1999} package).\n\\subsection{Testing}\n\\label{sec-3-9}\n\\label{sec:testing}\nThere about 75 tests coming with the code, combining unit tests (of\nonly some parts of the code), with a fair number of system tests, and\na few regression tests. While undoubtedly more unit tests would have\nbeen desirable, the availability of the existing tests is extremely\nuseful as a first indicator of a working installation,\netc. Test-driven development was not generally used for the project.\n\\subsection{Documentation}\n\\label{sec-3-10}\n\nIn addition to documentation in the source code files, there is the\nofficial Nmag documentation that is available \\cite{nmag-code-2012} as\n\\href{http://nmag.soton.ac.uk/nmag/current/manual/singlehtml/manual.html}{html}\nor \\href{http://nmag.soton.ac.uk/nmag/current/manual/manual.pdf}{pdf}\n(183 pages). Key components include a tutorial-like, step-by-step\nintroduction and walk-through of Nmag, starting from simple and common\nuse cases to more complicated and specialized application examples\n(112 pages), explanation of general concepts (7 pages), a command\nreference, which is built from the Python documentation strings\ndefined in the source code (20 pages), an overview of executable\nscripts including their options and usage examples, and Nmag data file types\n(12 pages), a list of 20 frequently asked questions and answers (9\npages), a mini-tutorial into micromagnetic modeling (not specific for\nNmag, but experience shows that new Nmag users are also often new to\nthe field).\n\nThe documentation is often cited by users as being very good, and useful.\n\n\\subsection{Installation}\n\\label{sec-3-11}\n\n\\label{sec:installation}\n\nNmag depends on a large number of support libraries. Many of these are\nof scientific origin and change often, including changes in the\ninterface. As a result, the Nmag code needs updating to compile\ncorrectly after any such update.\n\nFor some time, there were installations options through a Debian\npackage, a KNOPPIX Live Linux CD (this was before virtual machines were widely\nused), and from source. Installation from source worked on Linux and\nOS X.\n\nLater, we focused our energy on providing an installation setup that\nis as independent as possible from version changes of libraries that\nare installed through the Linux distribution to maximize the chances\nfor long term availability in a situation without maintenance\nresources.\n\nThe resulting setup for Nmag compiles and installs all the required\ndependencies\\footnote{atlas-3.6.0, cryptokit-1.2, gsl-1.14, ipython-0.1, lapack, mpich2-1.2.1p1, numarray-1.5.2, numpy-1.5.0, ocaml-3.12.0, ocamlgsl-0.6.0, ocaml-findlib-1.2.1, ParMetis-3.1.1, Python-2.7.2.tar.bz2, petsc-lite-3.1-p5, PyVTK, qhull-2003.1, tables-2.1.2, scipy-0.7.2 py-0.9.1 ply-3.3, sundials-2.3.0, HLib-1.3p19}\nfrom source, then builds Nmag based on these support libraries and\ntools. Compilation does only work on Linux (although on a large\nvariety of distributions and releases).\n\nThis setup has been remarkable robust, and only failed once in 4\nyears of no maintenance updates. The one failure was due to more\nrecent \\texttt{gcc} versions reporting commenting through a double slash in C\nheader files of the hdf5 library as illegal, which previously only triggered a warning.\n\nThe price to pay for this robustness is that the compressed tarball\ncontaining all the library source code is about $91\\,\\mathrm{MB}$ in size. Once\nNmag has compiled all the support libraries, these take together $1.4\\,\\mathrm{GB}$\nof disk space storage. After removing temporary build files $0.5\\,\\mathrm{GB}$ remain.\nAs the support libraries are snapshots\nfrom their source distribution in early 2012, no improvements or\nbug fixes developed in the support libraries will affect the Nmag\ncompilation and executables.\n\nWhile the size of the installation is undesirable, and the process to\ncompile all support libraries from source can take up to an hour\n(depending on the computer), the from-pack\\-aged-source approach has\nprovided remarkable robustness of the installation process.\n\\section{Summary}\n\\label{sec-4}\n\\label{sec:summary}\n\nEmbedding the simulation capabilities in a general purpose language\nwas an excellent choice, the available tests are very useful. The\nchoice of Python for the high level user interface resonates well with the\nuser community. The extended documentation and tutorial has been\nwarmly welcomed by users.\n\nThe choice of OCaml for the internals of the code has implication for\nthe number of people that can engage with the open source project, as\nit is a fairly specialist skill. We also find OCaml not be as fast and\nmemory efficient as C code. Several aspects of the project have aimed\nfor greater than necessary generality, leading to additional\ncomplexity of the code.\n\nWith this review, we try to share experience to help improve future\ngenerations of software engineering projects in computational science.\n\n\\bigskip\n\nWe acknowledge financial support from the UK's Engineering and\nPhysical Sciences Research Council (EPSRC) from grants EP/E040063/1,\nEP/E039944/1 and Doctoral Training Centre EP/G03690X/1), from the\nEuropean Community's FP7 Grant Agreement no. 233552 (DYNAMAG), from\nHorizon 2020 Research Infrastructures project \\#676541 (OpenDreamKit),\nand from the University of Southampton, all of which contributed in\npart to the development of the Nmag tool suite and this review.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{paper.bib}  \n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 26505, "prevtext": "\nWe note that $\\mathbf{H}$ is obtained as a function of $\\mathbf{m}$\nby solving certain partial differential equations \\cite[Section 2.2]{thesis-knittel}.\n\nWe can rewrite (\\ref{eq:eom}) using index notation as:\n\n", "index": 3, "text": "\\begin{equation}\n\\frac{{\\ensuremath{\\mathrm{d}}} m_i}{{\\ensuremath{\\mathrm{d}}} t} =\n\\sum_{j, k} \\left[\n  c_1 \\epsilon_{ijk}m_jH_k +\n  \\sum_{p, q} c_2 \\epsilon_{ijk}m_j (\\epsilon_{kpq}m_p H_q) \\label{eq:index}\n\\right]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\frac{{\\mathrm{d}}m_{i}}{{\\mathrm{d}}t}=\\sum_{j,k}\\left[c_{1}\\epsilon_{ijk}m_{%&#10;j}H_{k}+\\sum_{p,q}c_{2}\\epsilon_{ijk}m_{j}(\\epsilon_{kpq}m_{p}H_{q})\\right]\" display=\"block\"><mrow><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><msub><mi>m</mi><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></munder><mrow><mo>[</mo><mrow><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>H</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></munder><mrow><msub><mi>c</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03f5</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><msub><mi>m</mi><mi>p</mi></msub><mo>\u2062</mo><msub><mi>H</mi><mi>q</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}]