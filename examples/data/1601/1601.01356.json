[{"file": "1601.01356.tex", "nexttext": "\n\nThe Ndcg (Normalized discounted cumulative gain) metric decides the relevance of the listed items depending on their rank. The Equations \\ref{ndcg} and \\ref{dcg} presents how  Ndcg (Normalized discounted cumulative gain) and Dcg (Discounted cumulative gain) are calculated, respectively. The Idcg (Ideal discounted cumulative gain) value refers to the best case where the output list is sorted by the relevance. In the equations, $k$ is size of the output list, $j$ is the position of the item on the output list and $rel_j$ indicates if the item at rank $j$ is relevant (true recommendation) or not.  $rel_j$ is $1.0$ when the item at $j$th rank is relevant, and $0.0$ otherwise.\n\n\n", "itemtype": "equation", "pos": 19084, "prevtext": "\n\n\n\n\\title{From Word Embeddings to Item Recommendation}\n\n\n\n\n\n\n\\author{\n\\IEEEauthorblockN{Makbule Gulcin Ozsoy}\n\\IEEEauthorblockA{Department of Computer Engineering \\\\ Middle East Technical University \\\\Ankara, Turkey\\\\\nEmail: makbule.ozsoy@ceng.metu.edu.tr}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\n\\begin{abstract}\nSocial network platforms can archive data produced by their users and re-use the data to serve the users better. One of the services that these platforms provide is the recommendation service. Recommendation systems can predict the future preferences of the users using various different techniques. One of the most popular technique for recommendation is matrix-factorization, which uses low-rank approximation of input data. Similarly word embedding methods from natural language processing literature learn low-dimensional vector space representation of input elements. Noticing the similarities among word embedding and matrix factorization techniques and based on the previous works that apply techniques from text processing for recommendation, Word2Vec's skip-gram technique is employed to make recommendations. Unlike previous works that use Word2Vec for recommendation, non-textual features are used. The aim of this work is to make recommendation on next check-in venues and a Foursquare check-in dataset is used for this purpose. The results showed that use of vector space representations of items modelled by skip-gram technique is promising for making recommendations.\n\n\\begin{keywords} \nRecommendation systems, Location based social networks, Word embedding, Word2Vec,  Skip-gram technique\n\\end{keywords}\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\\label{intro}\nThe social network platforms (e.g. Twitter, Facebook, Foursquare) have many active users who produce vast amount of information by interacting with items/services and with other users on the platforms. For example, up to December 2015, 320 million monthly active users use Twitter, more than 55 million users use Foursquare and 1.01 billion daily active users use Facebook. These platforms are able to archive and use the produced information to better serve their users. One of the services that most of the social network platforms provide is the recommendation service. \n\nRecommendation systems predict the future preferences of users' based on their previous interactions with the items. For example, information on previous Foursquare check-ins of users can be used to make recommendation on future check-ins. As already mentioned, there is vast amount of information on historical preferences of users. In the literature, this information is used in several different ways to make recommendation, e.g. by applying neighbourhood based, machine-learning based and matrix-factorization based methods. Recently, matrix factorization (MF) based approaches gained more attention by researchers, as these methods can efficiently deal with large datasets by using low-rank approximation of input data \\cite{Ma:2011:RSS:1935826.1935877}. \n\nSimilar to matrix factorization methods, word embedding methods learn low-dimensional vector space representation of input elements.  They are used to learn linguistic regularities and semantic information from large text datasets and they are gaining more attention especially in natural language processing and text mining fields \\cite{MustoSGL15}. In this work, I aimed to use Word2Vec's \\cite{MikolovSCCD13} skip-gram word embedding technique for recommending next check-in locations.\n\nEfficiency of using text processing techniques in recommendation systems is already exemplified in some of the previous works in the literature (\\hspace{1sp}\\cite{GaoTL12}, \\cite{ShinCL14}, \\cite{MustoSGL15}). \\cite{GaoTL12} is one of the state-of-the-art methods for venue recommendation on Location Based Social Networks (LBSNs) and employs a language model based method.  \\cite{ShinCL14} aims to make recommendation to users about which blog to follow. It uses Word2Vec to model a word based feature, i;e. tags. \\cite{MustoSGL15} employs three different word embedding techniques, one of which is Word2Vec, to make recommendation on MovieLens and DBbook datasets. It uses textual data collected from Wikipedia about the items.\n\nIn this work, I employed Word2Vec's skip-gram technique to make recommendations on LBSNs. Unlike the previous works that use Word2Vec for recommendation (\\hspace{1sp}\\cite{ShinCL14}, \\cite{MustoSGL15}), I used a non-textual feature, namely the past check-ins of the users. For the evaluation I used a Foursquare check-in dataset, which is already used in previous works (\\hspace{1sp}\\cite{GaoTL12}, \\cite{Ozsoy14}). \n\nThe rest of the paper is structured as follows: I give information on the related work in the Section \\ref{relWork}. I explain Word2Vec and our proposed method in the Section \\ref{appDeepRec}. I give the experimental results in the Section \\ref{eval} and I conclude the paper in the Section \\ref{conclusion}.\n\n\\section{Related Work}\\label{relWork}\nRecommendation systems make recommendation of items by estimating their preferences (\\hspace{1sp}\\cite{MassaA07}, \\cite{TavakolifardA12}). In the literature there are three base recommendation approaches: Content based, collaborative filtering and hybrid approaches. Content based approach uses item features and their similarities to make recommendations. Collaborative filtering approach uses past preferences of users to decide which items to recommend. Hybrid methods combine these approaches to make recommendations.\n\n\n\nBesides the above-mentioned methods, matrix factorization based methods gain attention from recommendation systems researchers. These methods use low-rank approximation of input data and can handle large volume of data \\cite{Ma:2011:RSS:1935826.1935877}. In \\cite{KorenBV09}, it is stated that matrix factorization can represent the items and the users as vectors where high correlation between vectors leads to recommendation. Also, in the same work it is stated that these methods have good scalability, high accuracy and flexibility. Some example works that use the matrix factorization for recommendation belong to Ma et al. \\cite{Ma:2008:SSR:1458082.1458205}, Zheng et al. \\cite{Zheng:2010:CLA:1772690.1772795}, Liu et al. \\cite{Liu:2013:SSN:2488388.2488457}, Cheng et al. \\cite{Cheng:2013:YLG:2540128.2540504} and \\cite{Lian:2014:GJG:2623330.2623638}. Among these works \\cite{Zheng:2010:CLA:1772690.1772795} and \\cite{Cheng:2013:YLG:2540128.2540504} have similar purpose as ours and they make location/activity recommendations to the target users, however they do not employ Word2Vec for this purpose.\n\nSimilar to matrix factorization methods, word embedding methods from natural language processing field learn low-dimensional vector space representation of input elements. The word embeddings learn linguistic regularities and semantic information from the input text datasets and represent the the meaning of the words by a vector representation (\\hspace{1sp}\\cite{MustoSGL15},  \\cite{AroraLLMR15}). In \\cite{AroraLLMR15} it is stated that word embeddings can be learnt by Latent Semantic Analysis (LSA), topic models and matrix factorization techniques. Techniques defined in Word2Vec \\cite{MikolovSCCD13}, namely skip-gram and continuous bag of words (CBOW), are commonly used in the literature to represent the word vectors. \n\nSome of the recommendation methods (\\hspace{1sp}\\cite{ShinCL14}, \\cite{MustoSGL15}) use techniques from Word2Vec to represent their text based features. \\cite{ShinCL14} aims to make recommendation to users about which Tumblr blogs to follow. In this work inductive matrix completion (IMC) method is used for recommendation. This method uses side features (i.e. likes, re-blogs and tags) as well as previous preferences of users. It does not directly use techniques from natural language processing, but employ Word2Vec to compute vector representation of tags; which are word based features. \\cite{MustoSGL15} empirically evaluates three word embedding techniques, namely Latent Semantic Indexing, Random Indexing and Word2Vec, to make recommendation. They evaluate their proposed method on MovieLens and DBbook datasets. They mapped the items in the datasets to textual contents using Wikipedia and used the textual contents for the recommendation. Another recommendation method that uses techniques from natural language processing is Socio-Historical method proposed in \\cite{GaoTL12}. It is one of the state-of-the-art methods for venue recommendation on Location Based Social Networks (LBSNs). Observing the similarities in text mining and social network datasets, it employs language models approach from natural language processing to make venue recommendations. It models users historical preferences and social interactions, which can be used separately or in combination. \n\nTechniques in Word2Vec  are generally considered as deep learning technique. There are few other methods that employ deep learning to make recommendations, e.g. \\cite{SalakhutdinovMH07}, \\cite{GeorgievN13} and \\cite{WangWY14}. In \\cite{SalakhutdinovMH07} uses Restricted Boltzmann Machines (RBM's) to make movie recommendations. It models correlation among item ratings. \\cite{GeorgievN13} extends \\cite{SalakhutdinovMH07} by modelling both user-user and item-item correlations. \\cite{WangWY14} proposes a hierarchical Bayesian model thet learns models on both content informationon items and past preferences of users.\n\nIn this work, I employed Word2Vec's skip-gram technique to recommend check-in locations to the target users. Unlike the previous works that use Word2Vec for recommendation (\\hspace{1sp}\\cite{ShinCL14}, \\cite{MustoSGL15}), I used non-textual features, namely the past check-ins of the users.  \n\n\n\\section{Recommendation using Multiple Data Sources}\\label{appDeepRec}\nOur aim is to list the top-k check-in locations (e.g. restaurant, cafe) that the target user will visit in the future. For this purpose I used a technique from Word2Vec toolbox, namely skip-gram. In this section, I give brief information on the techniques in Word2Vec toolbox and explain how skip-gram technique is used for check-in recommendation.\n\nWord2Vec is a group of models which is introduced by Mikolov et al. (\\hspace{1sp}\\cite{MikolovSCCD13}, \\cite{MikolovCoRR13}). It contains two different techniques, namely skip-grams and continuous bag of words (CBOW), which produce word embeddings, i.e. distributed word representations. The word embeddings represent the words in a low dimensional continuous space and carry the semantic and syntactic information of words \\cite{LiXTJZC15}. While the CBOW technique uses the words around the current word to predict the current word, the skip-gram technique does the vice-versa, such that it uses the current word to predict the words around the current word (Figure \\ref{wordvecImg}). In both of the techniques, bag-of-words representation is used, i.e. order of the words in the input does not affect the result. \n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{wordvecImg}\n\\caption{Word2Vec techniques}\n\\label{wordvecImg}\n\\end{figure}\n\n\\cite{LevyGD15} states that CBOW combines words from the context window and cannot be easily expressed as a factorization. However, \\cite{LevyG14} shows that skip-gram performs a matrix factorization implicitly. The factorized word-context matrix is a co-occurence matrix that is known as pointwise mutual information (PMI) in the literature. Noticing the fact that both matrix factorization and Word2Vec techniques create low-rank approximation of the input data, I aimed to apply  Word2Vec's skip-gram technique for recommending next check-in venues. \n \nOur proposed recommendation method is composed of the following steps: First the input data is modelled using the skip-gram technique. Then this model is used to execute the recommendation process.\n\n\\subsection{Modelling the input data using the skip-gram technique} \nThe preferred technique to make recommendations is skip-gram technique: Firstly, use of text processing techniques for recommendation is already examplified in the llterature. Secondly, the skip-gram technique factorizes the input matrix implicitly and matrix factorization techniques are found to be effective in the recommendation systems literature. Lastly, skip-gram s preffered to CBOW, since it performs better than or equally well to CBOW technique (\\hspace{1sp}\\cite{MikolovSCCD13}, \\cite{LevyGD15})\n\nI used the skip-gram technique implemented in the gensim toolbox \\footnote{https://radimrehurek.com/gensim/models/word2vec.html}. This implementation accepts a list of sentences which are theirselves are a list of words. These words are used to create the internal dictionary which holds the words and their frequncies. Afterwards the model is trained using the input data and the dictionary. The output of the technique is the word vectors, which can be used as features by different applications\\cite{Word2VecGoogle}. During the training various parameters can be tuned which affects the performance, in terms of time and quality. I detail how the paramters are tuned and the effects of different values in the evaluation section.\n\nI noticed several similarities between the skip-gram technique and the recommendation process: First, the input data used in skip-gram technique is actually similar to what is used in the recommendation process. In the recommendation process I use a list of items that the user preferred/rated in the past and these lists can be divided into individual items. In other words, the sentences used in skip-gram can be mapped into past preferences of users in recommendation process and the words in skip-gram to individual items used in recommendation process. Second, the purpose of the skip-gram technique and the recommendation process are similar. Skip-gram model aims to predict to context words based on the current word, which can be mapped to predicting the items to be recommended based on already preffered/used items. \n\nIn the traditional recommendation process, the input data is composed of three base elements: user, item and rating. In most of the algorithms, these elements are represented by a \\textit{user $x$ items} matrix, where the matrix entries indicate the ratings. For our check-in recommendation problem, the ratings are asssumed to be binary, such that the user is either checked in at a location(venue) or not. Then, each target user's past preferences can be represented as a list of items (the check-in venues).\n\n\n\n\n\n\n\n\n\nInspiring from \\cite{LeM14}, I used the item lists together with the users, i.e. not only items but all of the user and the items used by this user,  as the input to skip-gram technique. As a result of the skip-gram, the vector representation of items and users are obtained, separately. These vector representations can be used to decide on which item is more similar to other item or which user is contextually closer to which items. These vectors and their similarities are used in the next step of our recommendation method.\n\n\n\n\\subsection{Recommendation using vector representation} \nThe skip-gram model provides the word vectors where the words with similar meaning are located closer in the vector space \\cite{LeM14}. In the recommendation case, instead of words, there are items and users. The output of skip-gram provides the vector representation of the items and users in vector space where similar vectors are located closer to each other. In this report three different recommendation techniques that use the vector representation of items and users are proposed:\n\n\\textbf{Recommendation by k-nearest items (KNI): } In our recommendation by k-nearest items (KNI) approach, the similarity among user and item vectors are used. In this method, directly the most similar k items to the target user are found. For this purpose cosine similarity between the related vectors is used. The collected top-k items are recommended to the target user.\n\n\n\n\n\n\\textbf{Recommendation by N-nearest users (NN): } In recommendation by N-nearest users (NN) approach, the traditional user-based collaborative filtering method is applied on the vector representations. In the user-based collaborative filtering first the most similar users (neighbors) to the target users are selected, and then the items that are previously preffered by the neighbors are recommended to the target user. Similar to the traditional approach, in NN approach, first the top-N neighbors are decided using the similarity among the user vector representations. Then the items that are previously used/preffered by the top-N neighbors are collected. Summing up the votes of neighbors, the top-k items to recommend are decided. The collected top-k items are recommended to the target user.\n\n\n \n\\textbf{Recommendation by N-nearest users and k-nearest items (KIU): } This approach is a comabination of the previous two approaches.  In this approach, first, the top-N neighbors are found by using the vector representation of the users. Then the top-k items that are most similar to the combination of target user and the neighbors are found by using the vector representations calculated in the first step. The collected top-k items are recommended to the target user.\n\n\n\n\n\n\n\\section{Evaluation}\\label{eval}\nThe aim is to recommend k-many check-in venues to each user based on their past check-ins. For this purpose the Checkin2011 dataset \\footnote{http://www.public.asu.edu/~hgao16/dataset.html}, previously used by \\cite{GaoTL12} and \\cite{Ozsoy14}, is used. The original dataset is collected from Foursquare web-site in between January 2011 - December 2011 and contains 11326 users, 187218 locations, 1385223 check-ins and 47164 friendship links. However, in \\cite{Ozsoy14} the researchers used a subset of this dataset by using the check-ins made in January as training set, and named it as CheckinsJan. The CheckinsJan dataset contains 8308 users, 49521 locations and 86375 check-ins.  For the test step, the check-ins made in February are used and the target users are limited to the ones who checked in both January and February. The set of target users contains 7187 users. The evaluation results of the method presented in this report are compared to \\cite{GaoTL12} and \\cite{Ozsoy14}. \n\nThe performance is measured by  Precision@k, Ndcg, Hitrate and Coverage metrics. While giving the evaluation results, first, the performance metrics for each user are calculated separately and then their averages are presented.\n\nPrecision@k measures the relevance of items on the output list. the The Equations \\ref{precision} presents how precision is calculated. In the equations,  $k$ indicates the output list size, $tp$ indicates the true positives, i.e. recommended and actually used items, $fp$ indicates the false positives, i.e. recommended but actually not used items.\n\n\n", "index": 1, "text": "\\begin{equation}\\label{precision}\nPrecision_k = \\frac{tp_k}{tp_k+fp_k} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Precision_{k}=\\frac{tp_{k}}{tp_{k}+fp_{k}}\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><msub><mi>n</mi><mi>k</mi></msub></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>p</mi><mi>k</mi></msub></mrow><mrow><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>p</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mi>f</mi><mo>\u2062</mo><msub><mi>p</mi><mi>k</mi></msub></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01356.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 19854, "prevtext": "\n\nThe Ndcg (Normalized discounted cumulative gain) metric decides the relevance of the listed items depending on their rank. The Equations \\ref{ndcg} and \\ref{dcg} presents how  Ndcg (Normalized discounted cumulative gain) and Dcg (Discounted cumulative gain) are calculated, respectively. The Idcg (Ideal discounted cumulative gain) value refers to the best case where the output list is sorted by the relevance. In the equations, $k$ is size of the output list, $j$ is the position of the item on the output list and $rel_j$ indicates if the item at rank $j$ is relevant (true recommendation) or not.  $rel_j$ is $1.0$ when the item at $j$th rank is relevant, and $0.0$ otherwise.\n\n\n", "index": 3, "text": "\\begin{equation}\\label{ndcg}\nNdcg_k = \\frac{Dcg_k}{Idcg_k}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"Ndcg_{k}=\\frac{Dcg_{k}}{Idcg_{k}}\" display=\"block\"><mrow><mrow><mi>N</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><msub><mi>g</mi><mi>k</mi></msub></mrow><mo>=</mo><mfrac><mrow><mi>D</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><msub><mi>g</mi><mi>k</mi></msub></mrow><mrow><mi>I</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><msub><mi>g</mi><mi>k</mi></msub></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01356.tex", "nexttext": "\n\nHitrate measures the ratio of user who are given at least one true recommendation. In the Equation \\ref{hitRate}, $m$ is one of the users, $M$ is the total set of users, $|M|$ is the size of the users and $HitRate_m$ indicates if there is a hit for the target user $m$ or not. $HitRate_m$ is equal to $1.0$ if there is at least one true recommendation for that user and $0.0$ otherwise. \n\n\n\n\n", "itemtype": "equation", "pos": 19929, "prevtext": "\n\n\n", "index": 5, "text": "\\begin{equation}\\label{dcg}\nDcg_k = rel_1 +\\sum_{j=2}^{k}\\frac{rel_j}{log_2 j}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"Dcg_{k}=rel_{1}+\\sum_{j=2}^{k}\\frac{rel_{j}}{log_{2}j}\" display=\"block\"><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><msub><mi>g</mi><mi>k</mi></msub></mrow><mo>=</mo><mrow><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msub><mi>l</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><mfrac><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msub><mi>l</mi><mi>j</mi></msub></mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><msub><mi>g</mi><mn>2</mn></msub><mo>\u2062</mo><mi>j</mi></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01356.tex", "nexttext": "\n\nCoverage measures the ratio of the users who are given any recommendation, independent from being relevant or not. In the recommendation systems literature, some of the methods may loose coverage in order to increase the accuracy \\cite{Bellogin:2013:ECS:2414425.2414439}.\\cite{Herlocker:2004:ECF:963770.963772} states that coverage and accuracy should be analysed together.\n\n\nThe methods presented in this report use two parameters: Number of neighbors ($N$) and output list size ($k$). \\cite{Ozsoy14} decided that best performing values are $N=30$ and $k=10$ for the CheckinsJan dataset, these values are used for the experiments. Also the upper-bound of the methods based on the decided parameters are presented in \\cite{Ozsoy14}: The upper-bound for Precision metric is found as $0.489$ and for the rest of the metrics they are found as $1.0$. \n\n\nSeveral different settings are evaluated when the input data is modelled using skip-gram technique. The parameters used during the training affects the performance in terms of time and quality \\cite{Word2VecTutorial}. These parameters are based on the gensim toolbox implementation. I detail only the parameters that are set to a different value than the defult; for the rest of the parameters one can refer to gensim webpage. The details of the parameters and how they are tuned are as fallows:\n\n\\begin{itemize}\n\\item $min\\_word\\_count$: The technique ignores the items whose frequency is less than this parameter. Its default value is 5. In natural language processing, the items (words) that are seen only few times can be considered as garbage or typo, however in the recommendation systems the data is very sparse and having items that are observed only few times is normal. In order not to lose any item that is not used frequently, I set $min\\_word\\_count$ parameter to 1.\n\\item $size$: Size parameter represents the dimension of the feature vectors and its default value is 100. In \\cite{Word2VecTutorial} it is stated that bigger $size$ value can lead more accurate model, but requires more data. The suggested values for $size$ parameter is in tens to hundereds \\cite{Word2VecTutorial}. In our experiments this parameter is refferred as $feature\\_count(F)$ and is set to different values in the range of $[10,100]$ with 10 increments.\n\\item $window$: Window parameter assigns the maximum distance between the current and the predicted words and its default value is 5. \\cite{Word2VecTutorial} states that it should be large enough to capture the semantic relationships among words. In our experiments this parameter is refferred as  $context\\_count(C)$ and is set to different values in the range of $[5,20]$ with 5 increments.\n\\item $iter$: Iter parameter represents the number of iterations on the input data and its default value is 1.  In our experiments it is refferred as  $epoch\\_count(E)$ and is set to different values in the range of $[5,25]$ with 5 increments.\n\\end{itemize}\n\nWhile iterating on the different ranges of one parameter, the other parameters are fixed to a constant value. For example while iterating on  $feature\\_count$, the values of $context\\_count$ and $epoch\\_count$ are fixed. The constant values for the parameters are set to: $feature\\_count(F)=100$, $context\\_count(C)=20$ and $epoch\\_count(E)=25$. \n\nThe training time of the model for the CheckinsJan data is presented in the Figure \\ref{trainingTime}. According to the figure, when the feature count increase, the time spent on the training increases from around 40 seconds to around 70 seconds. Similarly, for $context\\_count$ and $epoch\\_count$ the time spent on training increases as values of the parameters increase. Overall results show that training the model for CheckinsJan dataset takes less than 75 seconds.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{trainingTime}\n\\caption{The time needed to model the input data with skip-gram technique}\n\\label{trainingTime}\n\\end{figure}\n\n\nThree different recommendation techniques that use the models trained by skip-gram are proposed. These techniques are `recommendation by k-nearest items (KNI)', `recommendation by N-nearest users (NN)' and `recommendation by N-nearest users and k-nearest items (KIU)'. The time spent to make recommendation to all target users are presented on the Figure \\ref{recTime}. The figure shows that the KNI method spends less time than other method, since it directly uses the output of the model without any further computations, such as finding neighbors. Even the method and setting that spends the most time use less than 1900 seconds for all of the target users in CheckinsJan dataset, i.e. 7187 user.  That means that the methods spend less than 0.26 seconds for each target user in the recommendation step.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{recTime}\n\\caption{The time needed to make recommendation using the model created by skip-gram}\n\\label{recTime}\n\\end{figure}\n\nIn the the Figures \\ref{KNI_results} - \\ref{KIU_results}, the recommendation performance of the proposed methods are shown. For all of the methods, increasing the $feature\\_count$, i.e. the size of the vector representation, improves the performance. The effect of the increase for this parameter is less obvious as it is set to higher values than 50. When $context\\_count(C)=5$ the model is not able to capture the semantic similarity among the items and users. After setting this parameter to $C=10$ and higher it performs better. For both $context\\_count$ and $epoch\\_count$, increase of the parameters slightly improves the performance. \n\n\n\nAccording to the Figures \\ref{KNI_results} - \\ref{KIU_results}, comparison of the performance of the recommendation techniques indicates that the best performing method is KNI, followed by KIU. Both of these methods use the vector representations of the items and their similarities to the target users. Both KNI and KIU uses similarity to item vectors, however KIU additionally uses the neighbors of the target users, which are decided by user vector similarities. Compared to KNI, the use of neighbors in KIU does not provide high performance gain. This may indicate that use of user vector similarities for CheckinsJan dataset is not effective. This may root from the fact that the users could not be differentiated in the vector space efficiently since the number of users in the dataset is not very high, i.e. only 8307 users.\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{KNI_results}\n\\caption{Performance results of KNI}\n\\label{KNI_results}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{NN_results}\n\\caption{Performance results of NN}\n\\label{NN_results}\n\\end{figure}\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{KIU_results}\n\\caption{Performance results of KIU}\n\\label{KIU_results}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Table \\ref{results}, I compare the proposed methods in this report, namely KNI, NN and KIU, to the methods from the literature. The first method from the recommendation literature is traditional collaborative filtering method (CF-C) which uses the past preferences of the users and their similarities. The next two methods are from \\cite{Ozsoy14}, which are based on multi-objective optimization technique and combines past preferences of the users with other features, such as users' hometowns, friendship and their influence on each other. The methods are abbreviated as MO-CH when it uses past check-ins and hometowns of the users and MO-CFIH when it uses all of the above-mentioned features. The last two methods are from \\cite{GaoTL12}, which uses a language model based method from natural language processing literature for recommendation. The method can combine past preferences and social ties of users. I used two versions of the methods, one of which uses only the past preferences of the users (Gao-H) and the other uses the combination of past preferences and social ties (Gao-SH).\n\n\\begin{table}\n\\caption{Comparison of methods}\\label{results}\n\\centering\n\\begin{tabular}{c||c||c||c||c}\n\\hline\nMethod &  Precision &  Ndcg & HitRate &  Coverage \\\\\n\\hline\nKNI \t\t\t&\t0.119\t&\t0.169\t&\t0.618\t&\t\\textbf{1.000} \\\\\nNN \t\t\t&\t0.070\t&\t0.117\t&\t0.450\t&\t\\textbf{1.000} \\\\\nKIU \t\t\t&\t0.112\t&\t0.161\t&\t0.599\t&\t\\textbf{1.000} \\\\\nCF-C \t\t&  0.114 \t&\t0.242\t&  0.621 \t&\t0.955 \\\\\nMO-CFIH \t&\t0.105\t&\t0.218\t&\t0.596\t&\t0.999 \\\\\nMO-CH\t\t&\t0.112\t&\t0.227\t&\t0.616\t&\t0.996\\\\\nGao-H\t\t&\t\\textbf{0.174}\t&\t\\textbf{0.299}\t&\t0.696\t&\t0.952 \\\\\nGao-SH\t\t&\t0.167\t&\t0.295\t&\t\\textbf{0.721}\t&\t0.992 \\\\\t\n\\hline\n\\end{tabular}\n\\end{table}\n\nAccording to the Table \\ref{results}, the methods proposed in this report, namely KNI, NN and KIU, are able to make recommendation to any users, i.e. having $Coverage=1.0$. The best performing methods are Gao-H and Gao-SH, which shows that use of methods from natural language processing can be effective for making recommendations. In terms of Precision and Hitrate metrics, besides methods from Gao et al. \\cite{GaoTL12}, KNI method performs better compared to the other methods. For Ndcg metric, generally the methods that combine multiple features are better than the methods that use a single feature. \n\n\\section{Conclusion}\\label{conclusion}\nRecommendation systems predict the future preferences of users' based on their previous interactions with the items. In the literature, there are many different techniques to make recommendations, e.g. by applying neighbourhood based, machine-learning based and matrix-factorization based methods. One of the most popular methods is the matrix factorization based approaches  which use low-rank approximation of input data. Similarly word embedding methods from natural language processing literature learn low-dimensional vector space representation of input elements.  \n\nNoticing the similarities among word embedding and matrix factorization techniques and inspring from the previous works that apply techniques from text processing for recommendation, Word2Vec's skip-gram techniqueis employed to make recommendations. In this work the aim is to recommend top-k venues to target users based on their past preferences. I used a dataset collected from Foursquare. The results showed that use of techniques from natural language processing is effective and use of skip-gram technique from Word2Vec is promising for making recommendations.\n\nI want to apply the following improvements in the future: First, I want to use CBOW technique from Word2Vec for recommendation and compare its results to skip-gram technique. Second, I want to expand the dataset to observe the effect of data size. Third, I want to combine multiple features using the vector space representations, since  experimental results showed that combining multiple features increases the recommendation performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtranS}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{10}\n\\providecommand{\\url}[1]{#1}\n\\csname url@samestyle\\endcsname\n\\providecommand{\\newblock}{\\relax}\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\BIBentrySTDinterwordspacing}{\\spaceskip=0pt\\relax}\n\\providecommand{\\BIBentryALTinterwordstretchfactor}{4}\n\\providecommand{\\BIBentryALTinterwordspacing}{\\spaceskip=\\fontdimen2\\font plus\n\\BIBentryALTinterwordstretchfactor\\fontdimen3\\font minus\n  \\fontdimen4\\font\\relax}\n\\providecommand{\\BIBforeignlanguage}[2]{{\n\\expandafter\\ifx\\csname l@#1\\endcsname\\relax\n\\typeout{** WARNING: IEEEtranS.bst: No hyphenation pattern has been}\n\\typeout{** loaded for the language `#1'. Using the pattern for}\n\\typeout{** the default language instead.}\n\\else\n\\language=\\csname l@#1\\endcsname\n\\fi\n#2}}\n\\providecommand{\\BIBdecl}{\\relax}\n\\BIBdecl\n\n\\bibitem{AroraLLMR15}\nS.~Arora, Y.~Li, Y.~Liang, T.~Ma, and A.~Risteski, ``Random walks on context\n  spaces: Towards an explanation of the mysteries of semantic word\n  embeddings,'' \\emph{CoRR}, vol. abs/1502.03520, 2015.\n\n\\bibitem{Bellogin:2013:ECS:2414425.2414439}\nA.~Bellog{\\'{\\i}}n, I.~Cantador, F.~D{\\'{\\i}}ez, P.~Castells, and\n  E.~Chavarriaga, ``An empirical comparison of social, collaborative filtering,\n  and hybrid recommenders,'' \\emph{{ACM} {TIST}}, vol.~4, no.~1, p.~14, 2013.\n\n\\bibitem{Cheng:2013:YLG:2540128.2540504}\nC.~Cheng, H.~Yang, M.~R. Lyu, and I.~King, ``Where you like to go next:\n  Successive point-of-interest recommendation,'' in \\emph{{IJCAI} 2013,\n  Proceedings of the 23rd International Joint Conference on Artificial\n  Intelligence, Beijing, China, August 3-9, 2013}, 2013.\n\n\\bibitem{GaoTL12}\nH.~Gao, J.~Tang, and H.~Liu, ``Exploring social-historical ties on\n  location-based social networks,'' in \\emph{Proceedings of the Sixth\n  International Conference on Weblogs and Social Media, Dublin, Ireland, June\n  4-7, 2012}, 2012.\n\n\\bibitem{GeorgievN13}\nK.~Georgiev and P.~Nakov, ``A non-iid framework for collaborative filtering\n  with restricted boltzmann machines,'' in \\emph{Proceedings of the 30th\n  International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA,\n  16-21 June 2013}, 2013, pp. 1148--1156.\n\n\\bibitem{Herlocker:2004:ECF:963770.963772}\nJ.~L. Herlocker, J.~A. Konstan, L.~G. Terveen, and J.~Riedl, ``Evaluating\n  collaborative filtering recommender systems,'' \\emph{{ACM} Trans. Inf.\n  Syst.}, vol.~22, no.~1, pp. 5--53, 2004.\n\n\\bibitem{KorenBV09}\nY.~Koren, R.~M. Bell, and C.~Volinsky, ``Matrix factorization techniques for\n  recommender systems,'' \\emph{{IEEE} Computer}, vol.~42, no.~8, pp. 30--37,\n  2009.\n\n\\bibitem{LeM14}\nQ.~V. Le and T.~Mikolov, ``Distributed representations of sentences and\n  documents,'' in \\emph{Proceedings of the 31th International Conference on\n  Machine Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, 2014, pp.\n  1188--1196.\n\n\\bibitem{LevyG14}\nO.~Levy and Y.~Goldberg, ``Neural word embedding as implicit matrix\n  factorization,'' in \\emph{Advances in Neural Information Processing Systems\n  27: Annual Conference on Neural Information Processing Systems 2014, December\n  8-13 2014, Montreal, Quebec, Canada}, 2014, pp. 2177--2185.\n\n\\bibitem{LevyGD15}\nO.~Levy, Y.~Goldberg, and I.~Dagan, ``Improving distributional similarity with\n  lessons learned from word embeddings,'' \\emph{{TACL}}, vol.~3, pp. 211--225,\n  2015.\n\n\\bibitem{LiXTJZC15}\nY.~Li, L.~Xu, F.~Tian, L.~Jiang, X.~Zhong, and E.~Chen, ``Word embedding\n  revisited: {A} new representation learning and explicit matrix factorization\n  perspective,'' in \\emph{Proceedings of the Twenty-Fourth International Joint\n  Conference on Artificial Intelligence, {IJCAI} 2015, Buenos Aires, Argentina,\n  July 25-31, 2015}, 2015, pp. 3650--3656.\n\n\\bibitem{Lian:2014:GJG:2623330.2623638}\nD.~Lian, C.~Zhao, X.~Xie, G.~Sun, E.~Chen, and Y.~Rui, ``Geomf: Joint\n  geographical modeling and matrix factorization for point-of-interest\n  recommendation,'' in \\emph{Proceedings of the 20th ACM SIGKDD International\n  Conference on Knowledge Discovery and Data Mining}, ser. KDD '14.\\hskip 1em\n  plus 0.5em minus 0.4em\\relax New York, NY, USA: ACM, 2014, pp. 831--840.\n\n\\bibitem{Liu:2013:SSN:2488388.2488457}\nX.~Liu and K.~Aberer, ``Soco: a social network aided context-aware recommender\n  system,'' in \\emph{22nd International World Wide Web Conference, {WWW} '13,\n  Rio de Janeiro, Brazil, May 13-17, 2013}, 2013, pp. 781--802.\n\n\\bibitem{Ma:2008:SSR:1458082.1458205}\nH.~Ma, H.~Yang, M.~R. Lyu, and I.~King, ``Sorec: Social recommendation using\n  probabilistic matrix factorization,'' in \\emph{Proceedings of the 17th ACM\n  Conference on Information and Knowledge Management}, ser. CIKM '08.\\hskip 1em\n  plus 0.5em minus 0.4em\\relax New York, NY, USA: ACM, 2008, pp. 931--940.\n\n\\bibitem{Ma:2011:RSS:1935826.1935877}\nH.~Ma, D.~Zhou, C.~Liu, M.~R. Lyu, and I.~King, ``Recommender systems with\n  social regularization,'' in \\emph{Proceedings of the Forth International\n  Conference on Web Search and Web Data Mining, {WSDM} 2011, Hong Kong, China,\n  February 9-12, 2011}, 2011, pp. 287--296.\n\n\\bibitem{MassaA07}\nP.~Massa and P.~Avesani, ``Trust-aware recommender systems,'' in \\emph{RecSys},\n  2007, pp. 17--24.\n\n\\bibitem{MikolovCoRR13}\nT.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word\n  representations in vector space,'' \\emph{CoRR}, vol. abs/1301.3781, 2013.\n\n\\bibitem{MikolovSCCD13}\nT.~Mikolov, I.~Sutskever, K.~Chen, G.~S. Corrado, and J.~Dean, ``Distributed\n  representations of words and phrases and their compositionality,'' in\n  \\emph{Advances in Neural Information Processing Systems 26: 27th Annual\n  Conference on Neural Information Processing Systems 2013. Proceedings of a\n  meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.}, 2013,\n  pp. 3111--3119.\n\n\\bibitem{MustoSGL15}\nC.~Musto, G.~Semeraro, M.~de~Gemmis, and P.~Lops, ``Word embedding techniques\n  for content-based recommender systems: An empirical evaluation.'' in\n  \\emph{RecSys Posters}, ser. CEUR Workshop Proceedings, P.~Castells, Ed., vol.\n  1441.\\hskip 1em plus 0.5em minus 0.4em\\relax CEUR-WS.org, 2015.\n\n\\bibitem{Ozsoy14}\nM.~G. Ozsoy, F.~Polat, and R.~Alhajj, ``Multi-objective optimization based\n  location and social network aware recommendation,'' in \\emph{10th {IEEE}\n  International Conference on Collaborative Computing: Networking, Applications\n  and Worksharing, CollaborateCom 2014, Miami, Florida, USA, October 22-25,\n  2014}, 2014, pp. 233--242.\n\n\\bibitem{SalakhutdinovMH07}\nR.~Salakhutdinov, A.~Mnih, and G.~E. Hinton, ``Restricted boltzmann machines\n  for collaborative filtering,'' in \\emph{Machine Learning, Proceedings of the\n  Twenty-Fourth International Conference {(ICML} 2007), Corvallis, Oregon, USA,\n  June 20-24, 2007}, 2007, pp. 791--798.\n\n\\bibitem{ShinCL14}\nD.~Shin, S.~Cetintas, and K.-C. Lee, ``Recommending tumblr blogs to follow with\n  inductive matrix completion.'' in \\emph{RecSys Posters}, ser. CEUR Workshop\n  Proceedings, L.~Chen and J.~Mahmud, Eds., vol. 1247.\\hskip 1em plus 0.5em\n  minus 0.4em\\relax CEUR-WS.org, 2014.\n\n\\bibitem{TavakolifardA12}\nM.~Tavakolifard and K.~C. Almeroth, ``Social computing: an intersection of\n  recommender systems, trust/reputation systems, and social networks,''\n  \\emph{IEEE Network}, vol.~26, no.~4, pp. 53--58, 2012.\n\n\\bibitem{WangWY14}\nH.~Wang, N.~Wang, and D.~Yeung, ``Collaborative deep learning for recommender\n  systems,'' \\emph{CoRR}, vol. abs/1409.2944, 2014.\n\n\\bibitem{Word2VecGoogle}\n\\BIBentryALTinterwordspacing\nWord2Vec. Accessed: 2015-14-13. [Online]. Available:\n  \\url{https://code.google.com/p/word2vec/}\n\\BIBentrySTDinterwordspacing\n\n\\bibitem{Word2VecTutorial}\n\\BIBentryALTinterwordspacing\nWord2VecTutorial. Accessed: 2015-14-13. [Online]. Available:\n  \\url{http://rare-technologies.com/word2vec-tutorial/}\n\\BIBentrySTDinterwordspacing\n\n\\bibitem{Zheng:2010:CLA:1772690.1772795}\nV.~W. Zheng, Y.~Zheng, X.~Xie, and Q.~Yang, ``Collaborative location and\n  activity recommendations with {GPS} history data,'' in \\emph{Proceedings of\n  the 19th International Conference on World Wide Web, {WWW} 2010, Raleigh,\n  North Carolina, USA, April 26-30, 2010}, 2010, pp. 1029--1038.\n\n\\end{thebibliography}\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 20415, "prevtext": "\n\nHitrate measures the ratio of user who are given at least one true recommendation. In the Equation \\ref{hitRate}, $m$ is one of the users, $M$ is the total set of users, $|M|$ is the size of the users and $HitRate_m$ indicates if there is a hit for the target user $m$ or not. $HitRate_m$ is equal to $1.0$ if there is at least one true recommendation for that user and $0.0$ otherwise. \n\n\n\n\n", "index": 7, "text": "\\begin{equation}\\label{hitRate}\nHitRate = \\frac{\\sum_{m \\in M} HitRate_m}{|M|} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"HitRate=\\frac{\\sum_{m\\in M}HitRate_{m}}{|M|}\" display=\"block\"><mrow><mrow><mi>H</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi></mrow><mo>=</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>\u2208</mo><mi>M</mi></mrow></msub><mrow><mi>H</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msub><mi>e</mi><mi>m</mi></msub></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow></math>", "type": "latex"}]