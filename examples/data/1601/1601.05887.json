[{"file": "1601.05887.tex", "nexttext": "\n\nHere expectation is with respect to $f(y | {\\bf x})$,\nthe predictive distribution of $y({\\bf x})$ conditional on all runs so far.\nAssuming the sequential design scheme selects one new input point at a time,\nthe location of the new point, ${\\bf x}^{\\rm new}$, is the global maximizer of $E[I({\\bf x})]$\nover ${\\bf x} \\in \\chi$.\n\n\\subsection{EI for global optimization}\\label{sect:EI:opt}\n\n\n\n\nFinding the global minimum, $\\psi(y) = \\min\\{y({\\bf x}): x \\in \\chi\\}$,\nof an expensive to evaluate function\nis an extensively investigated optimization problem.\n(Finding the maximum is reformulated as $\\min -y({\\bf x})$,\nand the following results apply.)\n\\cite{Jones1998} proposed an efficient sequential solution\nvia the improvement function to assess the gain\nif a new evaluation is made at ${\\bf x}$.\nThe improvement function is\n\n", "itemtype": "equation", "pos": 18563, "prevtext": "\n\n\n\\title{Design of Computer Experiments for Optimization, Estimation of Function Contours,\nand Related Objectives}\n\n\\date{}\n\n\\maketitle\n\n\\begin{center}\nDerek Bingham, Pritam Ranjan, and William J.\\ Welch \\\\\nSimon Fraser University, Acadia University, and University of British Columbia\n\\end{center}\n\n\n\n\\section{Introduction}\\label{sect:intro}\n\nA computer code or simulator is a mathematical representation of a physical system,\nfor example a set of differential equations.\n\nSuch simulators take a set of input values or conditions, ${\\bf x}$, \nand from them produce an output value, $y({\\bf x})$, or several such outputs. \n\n\nFor instance, one application we use for illustration simulates\nthe average tidal power, $y$, generated as a function of a turbine location,\n${\\bf x} = (x_1, x_2)$,\nin the Bay of Fundy, Nova Scotia, Canada\n\\citep{RanHayKar2011}.\nPerforming scientific or engineering experiments via such a computer code\nis often more time and cost effective than running a physical experiment\n\nor collecting data directly.\n\n\n\n\n\n\n\nA computer experiment often has similar objectives to a physical experiment.\n\nFor example, computer experiments are often used in manufacturing or process development. \nIf $y$ is a quality measure for a product or process,\nan experiment could aim to optimize $y$ with respect to ${\\bf x}$.\nSimilarly, an experiment might aim to find sets or contours of ${\\bf x}$ values\nthat make $y$ equal a specified target value.\nSuch scientific and engineering objectives are naturally and efficiently achieved\nvia so-called data-adaptive sequential design,\nwhich we describe below.\nEssentially, each new run (that is, new set of input values) is chosen \nbased on analysis of the data so far,\nto make the best expected improvement in the objective. \nIn a computer experiment,\nchoosing new experimental runs, re-starting the experiment, etc.\\ pose\nonly minor logistical challenges if these decisions are also computer-controlled,\na distinct advantage relative to a physical experiment.\n\n\n\nChoosing new runs sequentially for optimization, moving $y$ to a target, etc.\\ has\nbeen formalized using the concept of expected improvement \\citep{Jones1998}.\nThe next experimental run is made where the expected improvement in the function of interest\n\nis largest.\nThis expectation is with respect to the predictive distribution of $y$ from\na statistical model relating $y$ to ${\\bf x}$.\nBy considering a set of possible inputs ${\\bf x}$ for the new run, \nwe can choose that which gives the largest expectation.\n\nWe illustrate this basic idea with two examples in Section~\\ref{sect:basic}.\nThen we describe formulations of improvement functions and their expectations\nin Section~\\ref{sect:EI}.\nExpectation implies a statistical model,\nand in Section~\\ref{sect:GP} we outline the use of Gaussian process models\nfor fast emulation of computer codes.\nIn Section~\\ref{sect:other} we describe some extensions to other, more complex scientific\nobjectives.\n\n\\section{Expected improvement and sequential design: basic ideas \\label{sect:basic}}\n\nWe illustrate the  basic idea of expected improvement and data-adaptive sequential\ndesign via two examples.\nThe first,\na tidal-power application, \nshows the use of expected improvement in sequential optimization.\nWe then use a simulator of volcanic pyroclastic flow\nto illustrate how to map out a contour of a function.\n\n\\subsection{Optimization}\n\n\\cite{RanHayKar2011} described output from a\n2D computer-model simulation of the power produced by\na tidal turbine in the Minas Passage of the Bay of Fundy,\nNova Scotia, Canada.\nIn this simplified version of the problem there are just two inputs \nfor the location of a turbine.\nOriginally, the input space was defined by latitude-longitude coordinates\nfor a rectangular region in the Minas Passage \n\\citep[see Figure~5 of][]{RanHayKar2011}. \nThe coordinates  were transformed so that \n$x_1$ is in the direction of the flow  \nand $x_2$ is perpendicular to the flow.\n\nFurthermore, only an interesting part of the Minas Passage was considered,\nwith $x_1 \\in [0.75, 0.95]$ and $x_2 \\in [0.2, 0.8]$. \nThe code generates $y$, the extractable power in MW,\naveraged over a tidal cycle.\n\nFor the simplified demonstration here,\n$y$ was computed for 533 runs on a $13 \\times 41$ grid of $x_1$ and $x_2$ values,\nwhich produced the contour plot of Figure~\\ref{fig:tidal:ei20}(a).\n\\begin{figure}\n\\centerline{\\epsfig{file=tidal_ei20.ps,width=4.4in}}\n\\vspace{-0.5cm}\n\\caption{Initial 20-run design and analysis for the tidal-power application:\n(a) true power, $y$, in MW;\n(b) predicted power, $\\hat{y}({\\bf x})$;\n(c) standard error, $s({\\bf x})$; and\n(d) expected improvement, $E[I({\\bf x})]$.\nThe design points from an initial 20-run maximin Latin hypercube are\nshown as filled circles.\nAll plots are functions of the two input variables, $x_1$ and $x_2$,\nwhich are transformations of longitude and latitude.\n\n\\label{fig:tidal:ei20}}\n\\end{figure}\n\nWe now demonstrate how the turbine location optimizing the power,\ni.e., $\\max y(x_1, x_2)$, can be found with far fewer than 533 runs of\nthe computer code.\nSuch an approach would be essential for the computer experiment of ultimate interest.\nA more realistic computer model has a grid resolution 10 times finer\nin each coordinate and introduces vertical layers in a 3D code.\nThe running time would be increased by several orders of magnitude.\nMoreover, the final aim is to position several turbines,\nwhich would interfere with each other,\nand so the optimization space is larger than two or three dimensions.\nThus, the ultimate goal is to optimize a high-dimensional function\nwith a limited number of expensive computer model runs.\nInevitably, much of the input space cannot be explicitly explored,\nand a statistical approach to predict outcomes (extractable power) \nalong with an uncertainty measure\nis required to decide where to make runs and when to stop.\nThe expected improvement criterion addresses these two requirements.\n\n\n\n\n\n\n\n\n\n\n\nThus, imagine a more limited computer experiment with just 20 runs,\nas shown by the points in Figure~\\ref{fig:tidal:ei20}.\nThe experimental design  \n\n(that is, the locations of the 20 points)\nis a maximin Latin hypercube \\citep{MorMit1995},\na stratified scheme that is ``space-filling''\neven in higher dimensions.\nThe choice of 20 runs is based on the heuristic rule that\nan initial computer experiment has $n = 10d$ observations \\citep{LoeSacWel2009},\nwhere $d$ is the input dimension; here $d = 2$.\nAmong the 20 initial runs, the largest $y$ observed, denoted by ${y_{\\max}^{({20})}}$,\nis 109.7 MW at $(x_1, x_2) = (0.755, 0.4)$.\nThe expected improvement algorithm tries to improve on the best value\nfound so far as new runs are added.\n\nAt each iteration of the computer experiment we \nobtain a predictive distribution for $y({\\bf x})$ conditional on the runs so far.\nThis allows prediction of the function at input vectors ${\\bf x}$ where\nthe code has not been run.\n\nA Gaussian process (GP) statistical model is commonly used for prediction,\nas outlined in Section~\\ref{sect:GP},\nthough this is not essential.\nA GP model was fit here to the data from the first 20 runs, giving\nthe point-wise predictions, $\\hat{y}({\\bf x})$, of $y({\\bf x})$\nin Figure~\\ref{fig:tidal:ei20}(b)\nand the standard error, $s({\\bf x})$,\nin Figure~\\ref{fig:tidal:ei20}(c).\nThe standard error is a statistical measure of uncertainty concerning the closeness of the predicted value to the actual true value of y(x).  We show the predicted values and standard errors through contours in Figures~\\ref{fig:tidal:ei20}(b) and~\\ref{fig:tidal:ei20}(c).\n\nFigures~\\ref{fig:tidal:ei20}(b) and~(c) are informative about\nregions in the input space that are promising versus unpromising\nfor further runs of the code.\nWhile the $\\hat{y}({\\bf x})$ prediction surface is nonlinear,\nit suggests there is a single, global optimum.\nMoreover, the $s({\\bf x})$ surface is uniformly\nbelow about 15:\nFor much of the input space, $\\hat{y}({\\bf x})$ is so much smaller than\n${y_{\\max}^{({20})}}$ relative to $s({\\bf x})$ \nthat a new run is expected to make virtually zero improvement.\n\nThe expected improvement (EI) for a candidate new run at any ${\\bf x}$\nis computed from the predictive distribution of $y({\\bf x})$.\n(See Section~\\ref{sect:EI:opt} for the formal definition of EI.)\nFigure~\\ref{fig:tidal:ei20}(d) shows the EI surface based\non predictive distributions from a GP that is fitted to the  data from \nthe initial 20 runs of the tidal-power code.\nBy the definition in Section~\\ref{sect:EI:opt},\nimprovement can never be negative\n(if the output from the new run does beat the current optimum, the current optimum stands).\nThus, EI is always non-negative too.\nFigure~\\ref{fig:tidal:ei20}(d) indicates that for most of the input space \nEI is near zero and a new run would be wasted,\nbut there is a sub-region where EI is more than 12 MW.\nEvaluating EI over the $13 \\times 41$ grid shows that the maximum EI is\n13.9 at ${\\bf x} = (0.785, 0.45)$.\nIn other words, a new code run to evaluate $y(0.785, 0.45)$ \nis expected to beat ${y_{\\max}^{({20})}} = 109.7$ by about 14.\n\nThus, run 21 of the sequential design for the computer experiment is at \n${\\bf x}^{(21)} = (0.785, 0.45)$.\nThe actual power obtained from the simulator is $y = 159.7$ MW,\nso the best $y$ found after 21 runs is ${y_{\\max}^{({21})}} = 159.7$,\nand this is the value to beat at the next iteration.\nNote that the actual improvement in the optimum from the new run is $159.7 - 109.7 = 50.0$,\ncompared with an expectation of about 13.9.\n\nThe new run raises concerns about the statistical model. \nBefore making the new run, \nthe predictive distribution of $y({\\bf x}^{(21)})$ \nis approximately normal, $N(123.5, 5.67^2)$,\nan implausible distribution given the large value of the standardized residual\n$(y({\\bf x}^{(21)}) - \\hat{y}({\\bf x}^{(21)})) / s({\\bf x}^{(21)}) = (159.7 - 123.5) / 5.67 = 6.4$.\nOne deficiency is that $s({\\bf x})$ may not reflect all sources of uncertainty in\nestimation of the parameters of the GP (see Section~\\ref{sect:GP}).\nA more important reason here, however, is that the new observation successfully \nfinds a peak in the input space, \na sub-region where the output function is growing rapidly and uncertainty is larger.\nIn contrast, the first 20 runs were at locations where the function is flatter\nand easier to model.\nThe GP model fit to the initial runs under-estimated the uncertainty of prediction \nin a more difficult part of the input space.\n\nCareful consideration of the properties of a GP model and the possible\nneed for transformations is particularly relevant\nfor sequential methods based on predictive distributions.\nUncertainty of prediction is a key component of the EI methodology,\nso checking that a model has plausible standard errors of prediction\nis critical.\n\nOne way of improving the statistical emulator of the tidal-power code\nis to consider transformation of the output.\nThis is described in the context \nof the volcano example of Section~\\ref{sect:basic:contour},\nwhere transformation is essential.\nFor the tidal-power example, \npersisting with the original model will show that it adapts to give\nmore plausible standard errors with a few more runs.\n\nThe GP model and predictive distributions are next updated\nto use the data from all 21 runs now available.\n\n\n\nFigure~\\ref{fig:tidal:ei21}(a) shows the location of the new run as a ``$+$''\nand the updated $\\hat{y}({\\bf x})$.\nSimilarly, Figure~\\ref{fig:tidal:ei21}(b) gives the updated $s({\\bf x})$.\nA property of the GP fit is that $s({\\bf x})$ must be zero at any point ${\\bf x}$\nwhere $y({\\bf x})$ is in the data set for the fit\n(see \\cite{Jones1998} for a derivation of this result).\nThus, $s({\\bf x})$ is zero at the new run, \nand Figure~\\ref{fig:tidal:ei21}(b) shows it is less than 5 near the new run.\nComparing with Figure~\\ref{fig:tidal:ei20}(c),\nit is seen that $s({\\bf x})$ was 5 or more in this neighbourhood for the GP\nfit before the new run.\n\\begin{figure}\n\\centerline{\\epsfig{file=tidal_ei21.ps,width=6.7in}}\n\\vspace{-0.5cm}\n\\caption{Analysis of the tidal-power application after 21 runs:\n(a) predicted power, $\\hat{y}$;\n(b) standard error, $s({\\bf x})$; and\n(c) expected improvement, $I({\\bf x})]$.\nThe new design point is shown as a ``$+$''.\n\\label{fig:tidal:ei21}}\n\\end{figure}\n\nOn the other hand, comparison of Figures~\\ref{fig:tidal:ei20}(c) and~\\ref{fig:tidal:ei21}(b)\nshows that $s({\\bf x})$ has {\\em increased\\/} outside the neighbourhood of the new run.\nFor example, at the right edge of Figure~\\ref{fig:tidal:ei20}(c),\n$s({\\bf x})$ barely reaches 15,\nyet $s({\\bf x})$ often exceeds 15 or even 20 at the same locations in Figure~\\ref{fig:tidal:ei21}(b).\nThe 21-run GP fit has adapted to reflect the observed greater sensitivity of the output\nto $x_1$ and $x_2$.\n(For instance, the estimate of the GP variance parameter $\\sigma^2$, \ndefined in Section~\\ref{sect:GP}, increases.)\nThus, the model has at least partially self corrected and we continue with it.\n\nThe EI contour plot in Figure~\\ref{fig:tidal:ei21}(c)\nsuggests that there is little further improvement to be had from a further run anywhere.\nIf a run number 22 is made, however,\nit is not located where $\\hat{y}({\\bf x})$ is maximized;\nthat location coincides with run 21 and there would be no gain.\nRather, the maximum EI of about 1.3 MW occurs\nat a moderate distance from the location providing maximum $\\hat{y}({\\bf x})$.\nAs we move away from run 21, the standard error increases from zero\nuntil it is large enough to allow a modest expected improvement.\nThus, this iteration illustrates that EI trades off local search\n(evaluate where $\\hat{y}({\\bf x})$ is optimized) and global search\n(evaluate where uncertainty concerning fitted versus actual output values, \ncharacterized by $s({\\bf x})$, is optimized).\n\nWith this approach, EI typically indicates\nsmaller potential gains as the number of iterations increases.\nEventually, the best EI is deemed small enough to stop.\nIt turns out that run 21 found the global maximum for extractable\npower on the $13 \\times 41$ grid of locations.\n\n\\subsection{Contour estimation}\\label{sect:basic:contour}\n\nWe illustrate sequential design for mapping out a contour of a\ncomputer-model function using TITAN2D computer model runs\nprovided by Elaine Spiller.\nThey relate to the Colima volcano in Mexico.\n\nAgain for ease of illustration, there are two input variables:\n$x_1$ is the pyroclastic flow volume ($\\mbox{m}^3$) of fluidized gas and rock fragments\nfrom the eruption;\nand $x_2$ is the basal friction angle in degrees,\ndefined as the the minimum slope for the volcanic material to slide.\nThe output $z$ is the maximum flow height (m) at a single,\ncritical location.\nAs is often the case, the code produces functional output,\nhere flow heights over a 2D grid on the earth's surface,\nbut the output for each run is reduced to a scalar quantity of interest,\nthe height at the critical location.\n\nFollowing \\cite{BayBerCal2009},\nthe scientific objective is to find the values of $x_1$ and $x_2$ where\n$z=1$, a contour delimiting a ``catastrophic'' region.\n\\cite{BayBerCal2009} used the same TITAN2D code but for a different volcano.\nThey also conducted their sequential experiment in a less\nformal way than in our illustration of the use of EI.\n\nThere are 32 initial runs of the TITAN2D code.\nThey are located at the points shown in Figure~\\ref{fig:volcano}(a).\nThe predicted flow height surface also shown in Figure~\\ref{fig:volcano}(a)\nrelates to a GP model fit to the transformed simulator output $y = \\sqrt{z}$.\nThis choice was made by trying GP models on three different scales:\nthe $z$ untransformed height;\n$\\log(z + 1)$, as chosen by \\cite{BayBerCal2009};\nand $\\sqrt{z}$.\nOur final choice of $y = \\sqrt{z}$ results from inspection of\nstandard cross-validation diagnostics for GP models \\citep{Jones1998}.\n\\begin{figure}\n\\centerline{\\epsfig{file=volcano_ei.ps,width=6.5in}}\n\\vspace{-0.5cm}\n\\caption{Analysis of the initial 32-run design for the volcano application:\n(a) predicted height, $\\hat{y}({\\bf x})$, where $y = \\sqrt{z}$;\n(b) standard error, $s({\\bf x})$; and\n(c) expected improvement, $E[I({\\bf x})]$.\nThe design points of the initial 32-run design are\nshown as filled circles.\nThe new design point chosen by the EI criterion is shown as a ``$+$''\nin the lower left corner of (c).\n\\label{fig:volcano}}\n\\end{figure}\n\nThe dashed curve in Figure~\\ref{fig:volcano}(a) shows the contour where $\\hat{y}({\\bf x}) = 1$.\nThis maps out the contour of interest in the $(x_1, x_2)$ input space,\nbut it is based on predictions subject to error.\nThe standard errors in Figure~\\ref{fig:volcano}(b) are substantial,\nand sequential design via EI aims to improve the accuracy of the\nestimate of the true $y({\\bf x}) = 1$ contour.\n\nThe EI criterion adapted for the contouring objective is defined in\nSection~\\ref{sect:EI:contour}.\nIt is computed for the initial design of the volcano example in\nFigure~\\ref{fig:volcano}(c).\nEI suggests improving the accuracy of the contour\nby taking the next run at $(x_1, x_2) = (8.2, 11.1)$.\nInspection of Figures~\\ref{fig:volcano}(a) and~3(b) show that this location\nis intuitively reasonable.\nIt is in the vicinity of the predicted $\\hat{y}({\\bf x}) = 1$ contour and\nhas a relatively large predictive standard error.\nReducing substantial uncertainty in the vicinity of the estimated contour\nis the dominant aspect of the EI measure of equation~(\\ref{eq:EI_contour}).\n\nThe tidal-flow and volcano applications both have a 2D input space\nfor ease of exposition,\nbut the same approaches apply to higher dimensions,\nwhere choosing runs in a sequential design would be more problematic\nwith ad hoc methods.\n\n\\section{Expected Improvement Criteria}\\label{sect:EI}\n\nIn this section, we briefly define improvement and EI in general.\nWe then review two implementations,\nspecific to global optimization and contour estimation, respectively. \n\nLet $I({\\bf x})$ be an improvement function defined for any ${\\bf x}$\nin the input space, $\\chi$.\nIt depends on the scientific objective,\nsuch as improving the largest $y$ found so far in maximization.\nIn general, it is formulated for efficient estimation of a\npre-specified computer-model feature, $\\psi(y)$.\nTypically, before taking another run,\n$I({\\bf x})$ is an unobserved function of ${\\bf x}$,\nthe unknown computer-model output $y({\\bf x})$,\nthe predictive distribution of $y({\\bf x})$,\nand the best estimate so far of $\\psi(y)$.\n\nGiven a definition of $I({\\bf x})$,\nas its name suggests the corresponding EI criterion is given by\nthe expectation of $I({\\bf x})$:\n\n\n", "index": 1, "text": "\\begin{equation*}\\label{eq:EI_general}\nE[I({\\bf x})] = \\int I({\\bf x}) f(y | {\\bf x}) \\,  dy.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"E[I({\\bf x})]=\\int I({\\bf x})f(y|{\\bf x})\\,dy.\" display=\"block\"><mrow><mi>E</mi><mrow><mo stretchy=\"false\">[</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mi>d</mi><mi>y</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nwhere ${y_{\\min}^{({n})}}$ is the minimum value of $y$ found so far with $n$ runs.\nThe objective is improved by ${y_{\\min}^{({n})}} - y({\\bf x})$ if ${y_{\\min}^{({n})}} > y({\\bf x})$,\notherwise there is no improvement.\n\nThe GP statistical model outlined in Section~\\ref{sect:GP}\nleads to a Gaussian predictive distribution for $f(y | {\\bf x})$,\ni.e., $y({\\bf x}) \\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$.\nThe Gaussian predictive model leads to a simple, closed form for the expected improvement:\n\n\n", "itemtype": "equation", "pos": 19498, "prevtext": "\n\nHere expectation is with respect to $f(y | {\\bf x})$,\nthe predictive distribution of $y({\\bf x})$ conditional on all runs so far.\nAssuming the sequential design scheme selects one new input point at a time,\nthe location of the new point, ${\\bf x}^{\\rm new}$, is the global maximizer of $E[I({\\bf x})]$\nover ${\\bf x} \\in \\chi$.\n\n\\subsection{EI for global optimization}\\label{sect:EI:opt}\n\n\n\n\nFinding the global minimum, $\\psi(y) = \\min\\{y({\\bf x}): x \\in \\chi\\}$,\nof an expensive to evaluate function\nis an extensively investigated optimization problem.\n(Finding the maximum is reformulated as $\\min -y({\\bf x})$,\nand the following results apply.)\n\\cite{Jones1998} proposed an efficient sequential solution\nvia the improvement function to assess the gain\nif a new evaluation is made at ${\\bf x}$.\nThe improvement function is\n\n", "index": 3, "text": "$$ I({\\bf x}) = \\max\\{{y_{\\min}^{({n})}} - y({\\bf x}), 0\\}, $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"I({\\bf x})=\\max\\{{y_{\\min}^{({n})}}-y({\\bf x}),0\\},\" display=\"block\"><mrow><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msubsup><mi>y</mi><mi>min</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\n\nwhere $u = ({y_{\\min}^{({n})}}-\\hat{y}({\\bf x}))/s({\\bf x})$,\nand $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ denote the standard normal\nprobability density function (pdf) and cumulative distribution function (cdf), respectively.\n\nLarge values of the first term support global exploration in regions of the input space\nsparsely sampled so far, where $s({\\bf x})$ is large.\nThe second term favours search where $\\hat{y}({\\bf x})$ is small,\nwhich is often close to the location giving ${y_{\\min}^{({n})}}$,\ni.e., local search.\nThis trade-off between local and global search\nmakes EI-based sequential design very efficient,\nand it often requires relatively few computer-model evaluations\nto achieve a desired accuracy in estimating $\\min y$.\n\nFor instance, in the tidal-power application,\nthe EI surface in Figure~\\ref{fig:tidal:ei20}(d)) indicates that the first follow-up run\nis at the location giving the maximum predicted power (see Figure~\\ref{fig:tidal:ei21}(a)).\nThus, the local-search component dominates.\nConversely, the suggested location for the second follow-up run\nis in an unsampled region near the maximum predicted power (see Figure~\\ref{fig:tidal:ei21}(c)).\n\nAttempts have been made to control this local versus global trade-off\nfor faster convergence (that is, using as few runs as possible) to the true global minimum.\nFor instance, \\cite{SchWelJon1998} proposed an\nexponentiated improvement function, $I^g({\\bf x})$, for $g \\ge 1$.\nWith $g > 1$,\nthere is more weight on larger improvements\nwhen expectation is taken to compute EI.\nSuch large improvements will have a non-trivial probability \neven if $\\hat{y}({\\bf x})$ is unfavourable, provided $s({\\bf x})$ is sufficiently large. \nHence, global exploration of high-uncertainty regions can receive more attention\nwith this adaptation.\nSimilarly,\n\\cite{Sobester2005} developed a weighted expected improvement function (WEIF)\nby introducing a user-defined weight parameter $w \\in [0, 1]$\nin the \\cite{Jones1998} EI criterion,\nand \\cite{Ponweiser2008} proposed clustered multiple generalized expected improvement.\n\n\n\\subsection{EI for contour estimation}\\label{sect:EI:contour}\n\n\\cite{Ranjan2008} developed an EI criterion\nspecific to estimating a threshold (or contour) of $y$.\nThey applied it to a 2-queue 1-server computer network simulator\nthat models the average delay in a queue for service.\n\nLet the feature of interest $\\psi(y)$ be the set of input vectors ${\\bf x}$ \ndefining the contour at level $a$:\n\n", "itemtype": "equation", "pos": 20057, "prevtext": "\nwhere ${y_{\\min}^{({n})}}$ is the minimum value of $y$ found so far with $n$ runs.\nThe objective is improved by ${y_{\\min}^{({n})}} - y({\\bf x})$ if ${y_{\\min}^{({n})}} > y({\\bf x})$,\notherwise there is no improvement.\n\nThe GP statistical model outlined in Section~\\ref{sect:GP}\nleads to a Gaussian predictive distribution for $f(y | {\\bf x})$,\ni.e., $y({\\bf x}) \\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$.\nThe Gaussian predictive model leads to a simple, closed form for the expected improvement:\n\n\n", "index": 5, "text": "\\begin{equation}\nE[I({\\bf x})] = s({\\bf x})\\phi(u) + ({y_{\\min}^{({n})}}-\\hat{y}({\\bf x}))\\Phi(u),\n\\label{eq:EI:opt}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"E[I({\\bf x})]=s({\\bf x})\\phi(u)+({y_{\\min}^{({n})}}-\\hat{y}({\\bf x}))\\Phi(u),\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>y</mi><mi>min</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><mrow><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nThe improvement function proposed by \\cite{Ranjan2008} is\n\n", "itemtype": "equation", "pos": 22653, "prevtext": "\n\nwhere $u = ({y_{\\min}^{({n})}}-\\hat{y}({\\bf x}))/s({\\bf x})$,\nand $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ denote the standard normal\nprobability density function (pdf) and cumulative distribution function (cdf), respectively.\n\nLarge values of the first term support global exploration in regions of the input space\nsparsely sampled so far, where $s({\\bf x})$ is large.\nThe second term favours search where $\\hat{y}({\\bf x})$ is small,\nwhich is often close to the location giving ${y_{\\min}^{({n})}}$,\ni.e., local search.\nThis trade-off between local and global search\nmakes EI-based sequential design very efficient,\nand it often requires relatively few computer-model evaluations\nto achieve a desired accuracy in estimating $\\min y$.\n\nFor instance, in the tidal-power application,\nthe EI surface in Figure~\\ref{fig:tidal:ei20}(d)) indicates that the first follow-up run\nis at the location giving the maximum predicted power (see Figure~\\ref{fig:tidal:ei21}(a)).\nThus, the local-search component dominates.\nConversely, the suggested location for the second follow-up run\nis in an unsampled region near the maximum predicted power (see Figure~\\ref{fig:tidal:ei21}(c)).\n\nAttempts have been made to control this local versus global trade-off\nfor faster convergence (that is, using as few runs as possible) to the true global minimum.\nFor instance, \\cite{SchWelJon1998} proposed an\nexponentiated improvement function, $I^g({\\bf x})$, for $g \\ge 1$.\nWith $g > 1$,\nthere is more weight on larger improvements\nwhen expectation is taken to compute EI.\nSuch large improvements will have a non-trivial probability \neven if $\\hat{y}({\\bf x})$ is unfavourable, provided $s({\\bf x})$ is sufficiently large. \nHence, global exploration of high-uncertainty regions can receive more attention\nwith this adaptation.\nSimilarly,\n\\cite{Sobester2005} developed a weighted expected improvement function (WEIF)\nby introducing a user-defined weight parameter $w \\in [0, 1]$\nin the \\cite{Jones1998} EI criterion,\nand \\cite{Ponweiser2008} proposed clustered multiple generalized expected improvement.\n\n\n\\subsection{EI for contour estimation}\\label{sect:EI:contour}\n\n\\cite{Ranjan2008} developed an EI criterion\nspecific to estimating a threshold (or contour) of $y$.\nThey applied it to a 2-queue 1-server computer network simulator\nthat models the average delay in a queue for service.\n\nLet the feature of interest $\\psi(y)$ be the set of input vectors ${\\bf x}$ \ndefining the contour at level $a$:\n\n", "index": 7, "text": "\\begin{equation}\nS(a) = \\{x: y({\\bf x}) = a\\}.\n\\label{eqn:S}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"S(a)=\\{x:y({\\bf x})=a\\}.\" display=\"block\"><mrow><mrow><mrow><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>:</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>a</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nwhere $\\epsilon({\\bf x}) = \\alpha s({\\bf x})$ for a positive constant $\\alpha$\n(e.g., $\\alpha = 1.96$, corresponding to 95\\% confidence/credibility under\napproximate normality).\nThis improvement function defines a limited region of interest around $S(a)$\nfor further experimentation.\nPoint-wise, the extent of the region depends on the uncertainty $s({\\bf x})$\nand hence the tolerance $\\epsilon({\\bf x})$.\n\nUnder a normal predictive distribution, $y({\\bf x}) \\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nthe expectation of $I({\\bf x})$  can again be written in closed form:\n\n\\begin{eqnarray}\nE[I({\\bf x})]\n&=& [\\epsilon^2({\\bf x}) -(\\hat{y}({\\bf x})-a)^2] \\left(\\Phi(u_2)-\\Phi(u_1)\\right) \\nonumber \\\\\n&+& s^2({\\bf x}) \\left[ (u_2\\phi (u_2)- u_1\\phi(u_1)) - (\\Phi(u_2)-\\Phi(u_1))\\right] \\nonumber \\\\\n&+& 2(\\hat{y}({\\bf x})-a)s({\\bf x}) \\left(\\phi (u_2)-\\phi(u_1)\\right),\n\\label{eq:EI_contour}\n\\end{eqnarray}\nwhere $u_1 = (a - \\hat{y}({\\bf x}) - \\epsilon({\\bf x}))/s({\\bf x})$ and\n$u_2 = (a - \\hat{y}({\\bf x}) + \\epsilon({\\bf x}))/s({\\bf x})$.\n\n\n\nLike EI for optimization, the EI criterion in~(\\ref{eq:EI_contour}) trades off\nthe twin aims of local search near the predicted contour of interest\nand global exploration.\nThe first term on the right of~(\\ref{eq:EI_contour}) recommends an input location\nwith a large $s({\\bf x})$ in the vicinity of the predicted contour.\nWhen it dominates, the follow-up point is often\nessentially the maximizer of $\\epsilon^2({\\bf x}) -(\\hat{y}({\\bf x})-a)^2$.\nThis consideration led to the new point in Figure~\\ref{fig:volcano}(c) of\nthe volcano application, for instance.\nThe last term in~(\\ref{eq:EI_contour}) gives weight to points\nfar away from the predicted contour with large uncertainties.\nThe second term is often dominated by the other two terms in the EI criterion.\n\nThe EI criterion in (\\ref{eq:EI_contour}) can easily be extended to related aims.\nFor simultaneous estimation of $k$ contours $S(a_1),..., S(a_k)$,\nwith $S(\\cdot)$ defined in~(\\ref{eqn:S}),\nthe improvement function becomes\n\n", "itemtype": "equation", "pos": 22787, "prevtext": "\nThe improvement function proposed by \\cite{Ranjan2008} is\n\n", "index": 9, "text": "$$ I({\\bf x}) = \\epsilon^2({\\bf x}) - \\min \\{ (y({\\bf x})-a)^2, \\epsilon^2({\\bf x}) \\}, $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"I({\\bf x})=\\epsilon^{2}({\\bf x})-\\min\\{(y({\\bf x})-a)^{2},\\epsilon^{2}({\\bf x}%&#10;)\\},\" display=\"block\"><mrow><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>,</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nand the corresponding EI can also be written in a closed form.\n\nWhen interest centres on the $100p$-th percentile, $\\nu_p$, of the simulator output,\n\\cite{Roy2008} suggested sequential design to estimate $S(\\nu_p)$ using\nthe improvement function\n\n", "itemtype": "equation", "pos": 24906, "prevtext": "\nwhere $\\epsilon({\\bf x}) = \\alpha s({\\bf x})$ for a positive constant $\\alpha$\n(e.g., $\\alpha = 1.96$, corresponding to 95\\% confidence/credibility under\napproximate normality).\nThis improvement function defines a limited region of interest around $S(a)$\nfor further experimentation.\nPoint-wise, the extent of the region depends on the uncertainty $s({\\bf x})$\nand hence the tolerance $\\epsilon({\\bf x})$.\n\nUnder a normal predictive distribution, $y({\\bf x}) \\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nthe expectation of $I({\\bf x})$  can again be written in closed form:\n\n\\begin{eqnarray}\nE[I({\\bf x})]\n&=& [\\epsilon^2({\\bf x}) -(\\hat{y}({\\bf x})-a)^2] \\left(\\Phi(u_2)-\\Phi(u_1)\\right) \\nonumber \\\\\n&+& s^2({\\bf x}) \\left[ (u_2\\phi (u_2)- u_1\\phi(u_1)) - (\\Phi(u_2)-\\Phi(u_1))\\right] \\nonumber \\\\\n&+& 2(\\hat{y}({\\bf x})-a)s({\\bf x}) \\left(\\phi (u_2)-\\phi(u_1)\\right),\n\\label{eq:EI_contour}\n\\end{eqnarray}\nwhere $u_1 = (a - \\hat{y}({\\bf x}) - \\epsilon({\\bf x}))/s({\\bf x})$ and\n$u_2 = (a - \\hat{y}({\\bf x}) + \\epsilon({\\bf x}))/s({\\bf x})$.\n\n\n\nLike EI for optimization, the EI criterion in~(\\ref{eq:EI_contour}) trades off\nthe twin aims of local search near the predicted contour of interest\nand global exploration.\nThe first term on the right of~(\\ref{eq:EI_contour}) recommends an input location\nwith a large $s({\\bf x})$ in the vicinity of the predicted contour.\nWhen it dominates, the follow-up point is often\nessentially the maximizer of $\\epsilon^2({\\bf x}) -(\\hat{y}({\\bf x})-a)^2$.\nThis consideration led to the new point in Figure~\\ref{fig:volcano}(c) of\nthe volcano application, for instance.\nThe last term in~(\\ref{eq:EI_contour}) gives weight to points\nfar away from the predicted contour with large uncertainties.\nThe second term is often dominated by the other two terms in the EI criterion.\n\nThe EI criterion in (\\ref{eq:EI_contour}) can easily be extended to related aims.\nFor simultaneous estimation of $k$ contours $S(a_1),..., S(a_k)$,\nwith $S(\\cdot)$ defined in~(\\ref{eqn:S}),\nthe improvement function becomes\n\n", "index": 11, "text": "$$ I({\\bf x}) = \\epsilon^2({\\bf x})\n- \\min\\left\\{(y({\\bf x})-a_1)^2,\\ldots, (y({\\bf x})-a_k)^2, \\epsilon^2({\\bf x}) \\right\\}, $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"I({\\bf x})=\\epsilon^{2}({\\bf x})-\\min\\left\\{(y({\\bf x})-a_{1})^{2},\\ldots,(y({%&#10;\\bf x})-a_{k})^{2},\\epsilon^{2}({\\bf x})\\right\\},\" display=\"block\"><mrow><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo>{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>a</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>a</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>,</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nHere the contour of interest changes after every follow-up point\nwhen $\\hat{\\nu}_p$ is estimated using Monte Carlo methods.\nNote that $I^g({\\bf x})$ for $g=2$ is the improvement function in (\\ref{eq:EI_contour}).\nHere $a=\\hat{\\nu}_p$ and hence not fixed throughout the sequential procedure. \nThat is, $\\hat{\\nu}_p$ for choosing a run is likely to be different from one run to the next.\n\\cite{Bichon2009} adapted this criterion to estimate\nthe probability of rare events and system failure in reliability-based design optimization.\n\n\\section{Gaussian Process Models and Predictive Distributions}\\label{sect:GP}\n\nEvaluation of an EI criterion requires the computation of the expectation of $I({\\bf x})$\nwith respect to the predictive distribution of $y({\\bf x})$.\nIn principle, any predictive distribution can be used,\nbut for the method to be useful, \nit should faithfully reflect the data obtained up to the run in question.\nIn practice, treating the data from the computer-model runs\nas a realization of a GP is nearly ubiquitous in computer experiments.\nA GP model leads to a Gaussian predictive distribution,\nwhich in turn leads to the closed form expressions\nin (\\ref{eq:EI:opt}) and (\\ref{eq:EI_contour})\nand easy interpretation of the trade off between local and global search.\n\nA GP model is a computationally inexpensive statistical emulator of\na computer code.\nA key feature of many codes is that they are deterministic:\nre-running the computer model with the same values for all input variables\nwill give the same output values.\nSuch a deterministic function is placed within a statistical framework by\nconsidering a given computer-model input-output relationship\nas the realization of a stochastic process,\n$Z({\\bf x})$, indexed by the input vector.\nA single realization of the process is non-random, hence the relevance for a deterministic\ncomputer code.\nFor a continuous function, the process is usually assumed to be Gaussian,\npossibly after transformation, as was done for the volcano application.\n\nThis GP or Gaussian Stochastic Process (GaSP) paradigm for modelling\na computer code dates back to\n\\cite{SacSchWel1989}, \\cite{SacWelMit1989}, \\cite{CurMitMor1991}, and \\cite{Oha1992}.\nSpecifically, the code output function, $y({\\bf x})$,\nis treated as a realization of\n\n", "itemtype": "equation", "pos": 25281, "prevtext": "\nand the corresponding EI can also be written in a closed form.\n\nWhen interest centres on the $100p$-th percentile, $\\nu_p$, of the simulator output,\n\\cite{Roy2008} suggested sequential design to estimate $S(\\nu_p)$ using\nthe improvement function\n\n", "index": 13, "text": "$$ I^g({\\bf x}) = \\epsilon^g({\\bf x}) - \\min\\{ (y({\\bf x}) - \\hat{\\nu}_p)^g, \\epsilon^g({\\bf x})\\}. $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"I^{g}({\\bf x})=\\epsilon^{g}({\\bf x})-\\min\\{(y({\\bf x})-\\hat{\\nu}_{p})^{g},%&#10;\\epsilon^{g}({\\bf x})\\}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>I</mi><mi>g</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>\u03f5</mi><mi>g</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mi>p</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>g</mi></msup><mo>,</mo><mrow><msup><mi>\u03f5</mi><mi>g</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nwhere $\\mu({\\bf x})$ is a mean (regression) function in ${\\bf x}$,\nand $Z({\\bf x})$ is a Gaussian process with mean $0$ and variance $\\sigma^2$.\n\nCrucial to this approach is the assumed correlation structure of $Z({\\bf x})$.\nFor two configurations of the $d$-dimensional input vector,\n${\\bf x} = (x_1,\\ldots, x_d)$ and ${\\bf x}' = (x'_1,\\ldots, x'_d)$,\nthe correlation between $Z({\\bf x})$ and $Z({\\bf x}')$ is denoted by $R({\\bf x}, {\\bf x}')$.\nHere, $R(\\cdot, \\cdot)$ is usually a parametric family of functions,\nfor which there are many choices \\citep[e.g.,][Section 2.3]{SanWilNot2003}.\nThe computations for the applications in Section~\\ref{sect:basic}\nwere based on a constant (intercept) regression only and a stationary\npower-exponential correlation function,  \n\n", "itemtype": "equation", "pos": 27668, "prevtext": "\nHere the contour of interest changes after every follow-up point\nwhen $\\hat{\\nu}_p$ is estimated using Monte Carlo methods.\nNote that $I^g({\\bf x})$ for $g=2$ is the improvement function in (\\ref{eq:EI_contour}).\nHere $a=\\hat{\\nu}_p$ and hence not fixed throughout the sequential procedure. \nThat is, $\\hat{\\nu}_p$ for choosing a run is likely to be different from one run to the next.\n\\cite{Bichon2009} adapted this criterion to estimate\nthe probability of rare events and system failure in reliability-based design optimization.\n\n\\section{Gaussian Process Models and Predictive Distributions}\\label{sect:GP}\n\nEvaluation of an EI criterion requires the computation of the expectation of $I({\\bf x})$\nwith respect to the predictive distribution of $y({\\bf x})$.\nIn principle, any predictive distribution can be used,\nbut for the method to be useful, \nit should faithfully reflect the data obtained up to the run in question.\nIn practice, treating the data from the computer-model runs\nas a realization of a GP is nearly ubiquitous in computer experiments.\nA GP model leads to a Gaussian predictive distribution,\nwhich in turn leads to the closed form expressions\nin (\\ref{eq:EI:opt}) and (\\ref{eq:EI_contour})\nand easy interpretation of the trade off between local and global search.\n\nA GP model is a computationally inexpensive statistical emulator of\na computer code.\nA key feature of many codes is that they are deterministic:\nre-running the computer model with the same values for all input variables\nwill give the same output values.\nSuch a deterministic function is placed within a statistical framework by\nconsidering a given computer-model input-output relationship\nas the realization of a stochastic process,\n$Z({\\bf x})$, indexed by the input vector.\nA single realization of the process is non-random, hence the relevance for a deterministic\ncomputer code.\nFor a continuous function, the process is usually assumed to be Gaussian,\npossibly after transformation, as was done for the volcano application.\n\nThis GP or Gaussian Stochastic Process (GaSP) paradigm for modelling\na computer code dates back to\n\\cite{SacSchWel1989}, \\cite{SacWelMit1989}, \\cite{CurMitMor1991}, and \\cite{Oha1992}.\nSpecifically, the code output function, $y({\\bf x})$,\nis treated as a realization of\n\n", "index": 15, "text": "$$ Y({\\bf x}) =  \\mu({\\bf x}) + Z({\\bf x}), $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"Y({\\bf x})=\\mu({\\bf x})+Z({\\bf x}),\" display=\"block\"><mrow><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\nHere, $\\theta_j$ (with $\\theta_j \\geq 0$) and $p_j$ (with $1 \\leq p_j \\leq 2$) \ncontrol the properties of the effect of input variable $j$ on the output. \nA larger value of $\\theta_j$ implies greater sensitivity (activity) of $y$ \nwith respect to $x_j$,\nwhereas a larger value of $p_j$ implies smoother behaviour of $y$ as a function of $x_j$.\n\nUnder this model the output values from $n$ runs of the code, $Y_1,\\ldots,\\allowbreak Y_n$,\nhave a joint multivariate normal distribution.\n\n\nIf the parameters in the statistical model---in the mean function, \nin the correlation function,\nand $\\sigma^2$---are treated as known,\nthe predictive distribution of $Y$ at a new ${\\bf x}$ has a normal distribution:\n$N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nwhere $\\hat{y}({\\bf x})$ is the conditional mean of $Y({\\bf x})$ given\n$Y_1,\\ldots,\\allowbreak Y_n$,\nand $s^2({\\bf x})$ is the conditional variance.\nWithout assuming normality, $\\hat{y}({\\bf x})$ can also be interpreted as\nthe best linear unbiased predictor, and $s^2({\\bf x})$ is the associated mean squared error.\nIn practice, the unknown parameters have to be estimated,\nusually by maximum likelihood or Bayesian methods.\nThe predictive distribution is then only approximately normal.\nMoreover, Bayesian estimation of the correlation parameters may be necessary\nto capture all sources of uncertainty in the predictive distribution.\n\nAs mentioned already,\nneither a GP model nor a normal predictive distribution are essential\nfor sequential design with an EI criterion.\n\nFor instance, \\cite{Chipman2012} used the optimization improvement function of \\cite{Jones1998}\nwith Bayesian additive regression trees (BART).\nThus, the emulator was a non-parametric ensemble of tree models.\n\n\\section{Other EI-based criteria}\\label{sect:other}\n\nOver the last two decades, a plethora of EI-based criteria have been proposed for\nother scientific and engineering objectives.\n\n\n\nApplications can involve several outputs of interest.\nFor instance,\nconstrained optimization problems arise \nwhere the code generating the objective function $y({\\bf x})$ or another code \ngives values for a constraint function, $c({\\bf x})$, (or several functions).\nFor a feasible solution, $c({\\bf x})$ must lie in $[a, b]$.\nIf $c({\\bf x})$ is also expensive to compute, \none can build an emulator, $\\hat{c}({\\bf x})$, for it too.\nThe predictive distribution for $c({\\bf x})$ leads to an estimate of the \nprobability that $a < c({\\bf x}) < b$ for any new run ${\\bf x}$ under consideration.\nEI in~(\\ref{eq:EI:opt}) is multiplied by this probability of feasibility\nto steer the search to locations where EI for the objective $y({\\bf x})$ \nis large and $c({\\bf x})$ is likely to be feasible \\citep{SchWelJon1998}.\nFor a code with multivariate output,\n\\cite{Henkenjohann2007} proposed an EI criterion for estimating\nthe global maximum of the desirability scores of simulator outputs.\n\n\\cite{lehman_etal} developed improvement functions for finding $M$- and $V$-robust designs\nfor optimization of an engineering process.\nHere the simulator inputs include\nboth controllable and environmental (uncontrollable noise) variables.\n(``Uncontrollable'' here means in the field; all inputs are typically set\nat specified values in a simulator run.)\nFor a given configuration of the control variables, ${\\bf x}_c$,\nlet $\\mu({\\bf x}_c)$ and $\\sigma^2({\\bf x}_c)$ be the unknown mean and variance\nof the simulator output $y$ with respect to the distribution of the environmental variables.\nAn $M$-robust engineering design minimizes $\\mu({\\bf x}_c)$ with respect to ${\\bf x}_c$\nsubject to a constraint on $\\sigma^2(x_c)$,\nwhereas  $V$-robust engineering design minimizes $\\sigma^2(x_c)$ subject\nto a constraint on $\\mu(x_c)$.\n\n\nThe inclusion of measurement error\n(or equivalently, considering a nondeterministic simulator)\nis becoming more popular in computer experiments,\noften due to unavoidable simulator biases and inaccurate modelling assumptions.\nMinimizing a noisy mean output response is perhaps undesirable,\nand \\cite{Ranjan2013} recommended minimizing a lower quantile, $q({\\bf x})$,\nvia an estimate $\\hat{q}({\\bf x})$\nfrom the predictive distribution, e.g.,\n$\\hat{q}({\\bf x}) = \\hat{y}({\\bf x}) - 1.96 s({\\bf x})$ under a normal predictive distribution.\nThe proposed improvement function is\n$I({\\bf x}) =\\max\\{0, \\hat{q}_{min}^{(n)} - q({\\bf x})\\}$,\nwhere $\\hat{q}_{min}^{(n)}$ is the minimum $\\hat{q}({\\bf x})$ from $n$ runs so far,\nand $q({\\bf x}) = y({\\bf x}) - 1.96 s({\\bf x})$ is an unobservable random quantity.\nTreating $s({\\bf x})$ as non-stochastic and assuming $y({\\bf x})\\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nthe corresponding EI criterion is\n\n\n", "itemtype": "equation", "pos": 28484, "prevtext": "\nwhere $\\mu({\\bf x})$ is a mean (regression) function in ${\\bf x}$,\nand $Z({\\bf x})$ is a Gaussian process with mean $0$ and variance $\\sigma^2$.\n\nCrucial to this approach is the assumed correlation structure of $Z({\\bf x})$.\nFor two configurations of the $d$-dimensional input vector,\n${\\bf x} = (x_1,\\ldots, x_d)$ and ${\\bf x}' = (x'_1,\\ldots, x'_d)$,\nthe correlation between $Z({\\bf x})$ and $Z({\\bf x}')$ is denoted by $R({\\bf x}, {\\bf x}')$.\nHere, $R(\\cdot, \\cdot)$ is usually a parametric family of functions,\nfor which there are many choices \\citep[e.g.,][Section 2.3]{SanWilNot2003}.\nThe computations for the applications in Section~\\ref{sect:basic}\nwere based on a constant (intercept) regression only and a stationary\npower-exponential correlation function,  \n\n", "index": 17, "text": "$$ R({\\bf x}, {\\bf x}') = \\exp\\left(-\\sum_{j=1}^d \\theta_j | x_j - x'_j |^{p_j} \\right). $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"R({\\bf x},{\\bf x}^{\\prime})=\\exp\\left(-\\sum_{j=1}^{d}\\theta_{j}|x_{j}-x^{%&#10;\\prime}_{j}|^{p_{j}}\\right).\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><msup><mi>\ud835\udc31</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>\u03b8</mi><mi>j</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mo>\u2032</mo></msubsup></mrow><mo stretchy=\"false\">|</mo></mrow><msub><mi>p</mi><mi>j</mi></msub></msup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05887.tex", "nexttext": "\n\nwhere $u = (\\hat{q}_{min}^{(n)} - \\hat{y}({\\bf x}) + 1.96 s({\\bf x}))/ s({\\bf x})$.\nLike the EI criterion in (\\ref{eq:EI:opt}),\nEI in (\\ref{eq:EI_noisy}) facilitates the trade-off between local and global search.\nOne can easily generalize this EI criterion to $E[I^g({\\bf x})]$ \\citep[as in][]{SchWelJon1998}\nor introducing a user specified weight \\citep[as in][]{Sobester2005}.\n\nFor complex physical phenomena like climate and tidal power,\nmultiple computer simulators with different computational demands \nare often available for experimentation.\nFor instance, \nthere are 2D and 3D codes for the tidal-power application; \nthe 3D version is a higher-fidelity representation of reality \nbut is much more expensive to run.\nThey can be combined to obtain more informed prediction,\n\nand \\cite{Huang2006} proposed augmented expected improvement\nfor finding the global minimum of the highest-fidelity process, subject to noise.\n\n\n\n\n\n\n\\section{Summary}\n\nThe essence of these approaches for sequential computer experiments is to formulate\nthe scientific objective through an improvement function.\nFollowing some initial runs, \nthe next run is chosen to maximize the expected improvement.\nIn contrast to physical experiments,\nsequential design is convenient, with the computer handling\nthe logistics of iterating analysis of the data so far, choice of the next run,\nand making the new run.\n\nWith objectives like optimization and contouring in high-dimensional applications,\nsequential strategies are efficient in terms of solving the problem with a\nrelatively small number of runs.\nFor these reasons, we expect this area of the design of computer experiments\nwill continue to receive considerable research attention from methodologists and users.\n\nOf course, the usefulness of this strategy depends \non having a computer model that provides a satisfactory \ndescription of the physical process of interest. \nSuch models have to be checked by reference to real data from the physical process\n\\citep{BayBerPau2007}. \nHowever, once a model has been adequately validated it provides an efficient route \nto achieving the objectives discussed here.\n\n\n\n\n\n\n\\bibliographystyle{agsm}\n\\bibliography{SSCchap}\n\n\n\n", "itemtype": "equation", "pos": 33235, "prevtext": "\nHere, $\\theta_j$ (with $\\theta_j \\geq 0$) and $p_j$ (with $1 \\leq p_j \\leq 2$) \ncontrol the properties of the effect of input variable $j$ on the output. \nA larger value of $\\theta_j$ implies greater sensitivity (activity) of $y$ \nwith respect to $x_j$,\nwhereas a larger value of $p_j$ implies smoother behaviour of $y$ as a function of $x_j$.\n\nUnder this model the output values from $n$ runs of the code, $Y_1,\\ldots,\\allowbreak Y_n$,\nhave a joint multivariate normal distribution.\n\n\nIf the parameters in the statistical model---in the mean function, \nin the correlation function,\nand $\\sigma^2$---are treated as known,\nthe predictive distribution of $Y$ at a new ${\\bf x}$ has a normal distribution:\n$N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nwhere $\\hat{y}({\\bf x})$ is the conditional mean of $Y({\\bf x})$ given\n$Y_1,\\ldots,\\allowbreak Y_n$,\nand $s^2({\\bf x})$ is the conditional variance.\nWithout assuming normality, $\\hat{y}({\\bf x})$ can also be interpreted as\nthe best linear unbiased predictor, and $s^2({\\bf x})$ is the associated mean squared error.\nIn practice, the unknown parameters have to be estimated,\nusually by maximum likelihood or Bayesian methods.\nThe predictive distribution is then only approximately normal.\nMoreover, Bayesian estimation of the correlation parameters may be necessary\nto capture all sources of uncertainty in the predictive distribution.\n\nAs mentioned already,\nneither a GP model nor a normal predictive distribution are essential\nfor sequential design with an EI criterion.\n\nFor instance, \\cite{Chipman2012} used the optimization improvement function of \\cite{Jones1998}\nwith Bayesian additive regression trees (BART).\nThus, the emulator was a non-parametric ensemble of tree models.\n\n\\section{Other EI-based criteria}\\label{sect:other}\n\nOver the last two decades, a plethora of EI-based criteria have been proposed for\nother scientific and engineering objectives.\n\n\n\nApplications can involve several outputs of interest.\nFor instance,\nconstrained optimization problems arise \nwhere the code generating the objective function $y({\\bf x})$ or another code \ngives values for a constraint function, $c({\\bf x})$, (or several functions).\nFor a feasible solution, $c({\\bf x})$ must lie in $[a, b]$.\nIf $c({\\bf x})$ is also expensive to compute, \none can build an emulator, $\\hat{c}({\\bf x})$, for it too.\nThe predictive distribution for $c({\\bf x})$ leads to an estimate of the \nprobability that $a < c({\\bf x}) < b$ for any new run ${\\bf x}$ under consideration.\nEI in~(\\ref{eq:EI:opt}) is multiplied by this probability of feasibility\nto steer the search to locations where EI for the objective $y({\\bf x})$ \nis large and $c({\\bf x})$ is likely to be feasible \\citep{SchWelJon1998}.\nFor a code with multivariate output,\n\\cite{Henkenjohann2007} proposed an EI criterion for estimating\nthe global maximum of the desirability scores of simulator outputs.\n\n\\cite{lehman_etal} developed improvement functions for finding $M$- and $V$-robust designs\nfor optimization of an engineering process.\nHere the simulator inputs include\nboth controllable and environmental (uncontrollable noise) variables.\n(``Uncontrollable'' here means in the field; all inputs are typically set\nat specified values in a simulator run.)\nFor a given configuration of the control variables, ${\\bf x}_c$,\nlet $\\mu({\\bf x}_c)$ and $\\sigma^2({\\bf x}_c)$ be the unknown mean and variance\nof the simulator output $y$ with respect to the distribution of the environmental variables.\nAn $M$-robust engineering design minimizes $\\mu({\\bf x}_c)$ with respect to ${\\bf x}_c$\nsubject to a constraint on $\\sigma^2(x_c)$,\nwhereas  $V$-robust engineering design minimizes $\\sigma^2(x_c)$ subject\nto a constraint on $\\mu(x_c)$.\n\n\nThe inclusion of measurement error\n(or equivalently, considering a nondeterministic simulator)\nis becoming more popular in computer experiments,\noften due to unavoidable simulator biases and inaccurate modelling assumptions.\nMinimizing a noisy mean output response is perhaps undesirable,\nand \\cite{Ranjan2013} recommended minimizing a lower quantile, $q({\\bf x})$,\nvia an estimate $\\hat{q}({\\bf x})$\nfrom the predictive distribution, e.g.,\n$\\hat{q}({\\bf x}) = \\hat{y}({\\bf x}) - 1.96 s({\\bf x})$ under a normal predictive distribution.\nThe proposed improvement function is\n$I({\\bf x}) =\\max\\{0, \\hat{q}_{min}^{(n)} - q({\\bf x})\\}$,\nwhere $\\hat{q}_{min}^{(n)}$ is the minimum $\\hat{q}({\\bf x})$ from $n$ runs so far,\nand $q({\\bf x}) = y({\\bf x}) - 1.96 s({\\bf x})$ is an unobservable random quantity.\nTreating $s({\\bf x})$ as non-stochastic and assuming $y({\\bf x})\\sim N(\\hat{y}({\\bf x}), s^2({\\bf x}))$,\nthe corresponding EI criterion is\n\n\n", "index": 19, "text": "\\begin{equation}\\label{eq:EI_noisy}\nE[I({\\bf x})] = s({\\bf x}) \\phi(u) + (\\hat{q}_{min}^{(n)} - \\hat{y}({\\bf x}) + 1.96s({\\bf x}))\\Phi(u),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"E[I({\\bf x})]=s({\\bf x})\\phi(u)+(\\hat{q}_{min}^{(n)}-\\hat{y}({\\bf x})+1.96s({%&#10;\\bf x}))\\Phi(u),\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mover accent=\"true\"><mi>q</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><mrow><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mn>1.96</mn><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]