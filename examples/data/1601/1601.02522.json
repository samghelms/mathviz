[{"file": "1601.02522.tex", "nexttext": " \nwhere ${\\mathbf{D}}$ is the diagonal degree matrix (${\\mathbf{D}}_{ii}=d[i]$). \n\n\n\n\n\n\\paragraph{Spectral theory}\nSince the Laplacian ${\\mathcal{L}}$ is always a symmetric positive semi-definite matrix, we know from the spectral theorem that it possesses a complete set of orthonormal eigenvectors. We denote them by $\\{ {u}_\\ell \\}_{\\ell=0,1,..., N-1}$. For convenience, we order the set of real, non-negative eigenvalues as follows: $0=\\lambda_0 < \\lambda_1 \\leq \\lambda_2 \\leq ... \\leq \\lambda_{N-1} = \\lambda_{\\rm max}$. When the graph is connected\\footnote{a path connects each pair of nodes in the graph},\nthere is only one zero eigenvalue. In fact, the multiplicity of the zero eigenvalue is equal to the number of connected components. See for example~\\cite{chung1997spectral,chung2005laplacians} for more details on spectral graph theory. \nThe eigenvectors of the Laplacian are used to define a graph Fourier basis~\\cite{shuman2013emerging,shuman2013vertex} which will be denoted as $U$. The eigenvalues are considered as a generalization of squared frequencies. The Laplacian matrix can thus be decomposed as \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\title{Stationary signal processing on graphs}\n\n\\author{Nathana\\\"el Perraudin and Pierre Vandergheynst \n\\thanks{\nEPFL, Ecole Polytechnique F\u00c3\u00a9d\u00c3\u00a9rale de Lausanne,\nLTS2 Laboratoire de traitement du signal, CH-1015 Lausanne, Switzerland}\n}\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\\begin{abstract} \nGraphs are a central tool in machine learning and information processing as they allow to conveniently capture the structure of complex datasets. In this context, it is of high importance to develop flexible models of signals defined over graphs or networks. \nIn this paper, we generalize the traditional concept of wide sense stationarity to signals defined over the vertices of arbitrary weighted undirected graphs. We show that stationarity is intimately linked to statistical invariance under a localization operator reminiscent of translation. We prove that stationary graph signals are characterized by a well-defined Power Spectral Density that can be efficiently estimated even for large graphs. We leverage this new concept to derive Wiener-type estimation procedures of noisy and partially observed signals and illustrate the performance of this new model for denoising and regression.    \n\\end{abstract}\n\\keywords{Stationarity, graphs, spectral graph theory, power spectral density, Wiener filter, covariance}\n\n\n\n\n\n\\section{Introduction}\n\nStationarity\n\n is a traditional hypothesis in signal processing used to represent a special type of statistical relationship between samples of a time series. For instance, wide-sense stationarity assumes that the first two statistical moments are invariant under translation. In other words, we can expect statistically similar consequences from the same causes. \nStationarity is a corner stone of many signal analysis methods. The expected frequency content of stationary signals, called Power Spectral Density (PSD), provides an essential source of information used to build signal models, generate realistic surrogate data or perform predictions. \n\nIn Figure~\\ref{fig:intro_stationarity_time}, we present an example of a stationary process (blue curve) and two predictions (red and green curves).\n\nAs the blue signal is a realization of a stationary process, the red curve is more probable than the green one because it respects the frequency content of the observed signal.  \n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/intro_stationary_signal.png} \n\\end{center}\n\\caption{Signal prediction. The red curve is more likely to occur than the green curves because it respects the frequency statistics of the blue curve.}\n\\label{fig:intro_stationarity_time}\n\\end{figure}\nClassical stationarity is a statement of statistical regularity under arbitrary translations and thus requires a regular structure (often \"time\"). However many signals do not live on such a regular structure. Imagine that instead of having one sensor returning a time-series, we have multiple sensors living in a two-dimensional space, each of which delivers only one value. \nIn this case\n\n(see Figure~\\ref{fig:intro_stationarity_graph} left), the signal support is no longer regular. Since there exist an underlying continuum in this example (2D space), one could assume the existence of a 2D stationarity field and use Kriging \\cite{Williams:1998uk} to interpolate observations to arbitrary locations. This technique thus generalizes stationarity for a regular domain but irregularly spaced samples. The goal of this contribution is to generalize stationarity for an irregular domain that is represented by a graph, without resorting to any underlying regular continuum. Instead we use a weak notion of translation invariance that captures the structure (if any) of the samples set.\n\n\nGraphs are convenient data structures able to capture complicated topologies and are used in numerous domains of applications such as social, transportation or neuronal networks. In this work a graph is composed of vertices connected by (weighted but undirected) edges. As we shall recall, a graph possess a localization operator that generalizes the classical translation. Signals are now scalar values observed at the vertices of the graph.\n\n\n\nFigure~\\ref{fig:intro_stationarity_graph} (left) presents an example of some data living in a 2-dimensional space. Seen as scattered samples of an underlying 2D process, one would (rightly) conclude it is not stationary. However, under closer inspection, the observed values look stationary \\emph{within} the spiral-like structure depicted by the graph in Figure~\\ref{fig:intro_stationarity_graph} (right). \nThe traditional Kriging interpolation technique would ignore this underlying structure and conclude that there are always rapid two dimensional variations in the underlying continuum process.  This problem does not occur in the graph case where the statistical relationships inside the data follow the graph connections (edges) \nresulting in this case in signals oscillating smoothly over the graph at low frequency.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/intro_irr_signal.png} \n\\includegraphics[width=0.45\\linewidth]{figures/intro_graph.png} \n\\end{center}\n\\caption{Example of a stationary process on a graph. The graph connections express relationships between the different elements of one signal. In this case, the signal varies smoothly along the snail shape of the graph.}\n\\label{fig:intro_stationarity_graph}\n\\end{figure}\nA typical example of a stationary signal on a graph would be the result of a survey performed by the users of a social network. If there is a relationship between a user\u00e2\u0080\u0099s answers and those of his neighbours, this relationship is expected to be constant among all users. Using stationarity on the graph, we could build the most probable answer for users that did not answer the survey.\n\n\\subsection{Contributions}\nWe use spectral graph theory to extend the notion of stationarity to a broader class of signals. To do so, we establish in Section~\\ref{sec:theory} the theoretical basis of this extension.\nIn order to evaluate the usefulness of our new model, we generalize Wiener filters~\\cite{Wiener1930generalized,Wiener1949extrapolation}. \nBased on our generalization of Wiener filters, we propose a new regularization term for graph signal optimization instead of the traditional Dirichlet prior. This term depends on the noise level and on the PSD of the signal. The new optimization scheme presented in Section~\\ref{sec:wiener} has two main advantages: 1) it allows to get rid of an arbitrary regularization parameter and 2) it adapts to the data. We also prove that the new optimization model is optimal under mild hypotheses.\n\nSince Wiener filters use the power spectral density (PSD), we generalize the Welch method~\\cite{welch1967use,bartlett1950periodogram} in Section~\\ref{sec:psd_estimation} and obtain a scalable way to estimate the PSD.\n\nFinally, in Sections~\\ref{sec:USPS} and \\ref{sec:experiments}, we show experimentally that common datasets closely follow our stationarity assumption. We exploit this fact to perform missing data imputation and we suggest that nearest neighbor graph construction often leads to stationarity.\n\n\n\n\\subsection{Review of the literature} \nGraphs have been used as regularizers in data application for more than a decade~\\cite{shuman2013emerging,smola2003kernels,zhou2004regularization,peyre2008non} and two of the most used models will be presented in Section~\\ref{sec:convex_model}. \n\nThe idea of graph filtering was hinted at by the machine learning community~\\cite{smola} but developed for the spectral graph wavelets proposed by Hammond et al.~\\cite{hammond2011wavelets} and extended by Shuman et al. in~\\cite{shuman2013vertex}. Moura and collaborators have suggested to use the adjacency matrix rather than the Laplacian as the basis of graph filtering~\\cite{sandryhaila2013discrete}. \n\nA notion of stationarity on graphs has been recently proposed in~\\cite{girault2015translation,girault2014semi}. However these contributions use a completely different translation operator, promoting unitarity over localization, and therefore end up with a fairly different framework. Finally, we also note that a probabilistic model using Gaussian random fields has been proposed in~\\cite{gadde2015probabilistic,zhang2015graph}. \nIn this model, signals are automatically graph stationary. A detailled explanation is given at the end of section \\ref{sec:theory}.\n\n\n\\section{Background results}\n\n\\subsection{Graph signal processing}\n\n\\paragraph{Graph nomenclature}\nA graph consists of two sets: ${\\mathcal{V}},{\\mathcal{E}}$ and a weight function.\n ${\\mathcal{V}}$ is the set of vertices representing the nodes of the graph and\n ${\\mathcal{E}}$ is the set of edges that connect two nodes if there is a particular relation between them. In this work all graphs are undirected. To obtain a finer  structure, this relation can be quantified by a weight function $\\mathcal{W} : {\\mathcal{V}} \\times {\\mathcal{V}} \\rightarrow {\\mathbb{R}}$ that reflects to what extent two nodes are related to each other. A graph is therefore a tuple denoted by ${\\mathcal{G}}=\\{ {\\mathcal{V}},{\\mathcal{E}},\\mathcal{W}\\}$. Let us index the nodes from $1,\\dots, N=|{\\mathcal{V}}|$ and  construct the weight matrix ${\\mathbf{W}} \\in {\\mathbb{R}}^{N \\times N}$ by setting $W_{i,j} = \\mathcal{W}(v_i,v_j)$ as the weight associated to the edge connecting the node $i$ and the node $j$. When no edge exists between $i$ and $j$, the weight is set to $0$. For a node $v_i\\in {\\mathcal{V}}$, the degree $d(i)$ is defined as $d(i)=\\sum_{j=1}^N {\\mathbf{W}}(i,j)$. \nIn this framework, a signal is defined as a function $f: {\\mathcal{V}} \\rightarrow  \\mathbb{R}$ (or $\\mathbb{C}$) assigning a scalar value to each vertex. It is convenient to consider a signal $f$ as a vector of size $N$ with the $n^{th}$ component representing the signal value at the $n^{th}$ vertex. \n\nThe most fundamental operator in graph signal-processing is the (combinatorial) graph Laplacian, defined as:\n\n", "index": 1, "text": "$${\\mathcal{L}} = \\mathbf{D}-{\\mathbf{W}},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{L}}=\\mathbf{D}-{\\mathbf{W}},\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>=</mo><mrow><mi>\ud835\udc03</mi><mo>-</mo><mi>\ud835\udc16</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $U^*$ denotes the transposed conjugate of $U$.\nThe graph Fourier transform is written $\\hat{f} = {\\mathcal{F}} (f) = U^*f$ and its inverse $f = {\\mathcal{F}}^{-1}(\\hat{f})=U\\hat{f}$. This Graph Fourier Transform possesses interesting properties further studied in~\\cite{shuman2013vertex}. \n\n\\paragraph{Graph filters}\nThe graph Fourier-transform plays a central role in graph signal processing since it allows a natural extension of filtering operations. In the classical setting, applying a filter to a signal is carried out with a convolution, which is simply a point-wise multiplication in the spectral domain. \nSimilarly, filtering a graph signal is also a multiplication in the graph Fourier domain. A graph filter $g$ is defined as a continuous function $g:{\\mathbb{R}}_+ \\rightarrow {\\mathbb{R}}$. In the spectral domain, filtering a signal $f$ with a filter $g$ is therefore written as $\\hat{f'}[\\ell] =  g(\\lambda_\\ell) \\cdot \\hat{f}[\\ell]$ or in matrix notation\n\n", "itemtype": "equation", "pos": 11204, "prevtext": " \nwhere ${\\mathbf{D}}$ is the diagonal degree matrix (${\\mathbf{D}}_{ii}=d[i]$). \n\n\n\n\n\n\\paragraph{Spectral theory}\nSince the Laplacian ${\\mathcal{L}}$ is always a symmetric positive semi-definite matrix, we know from the spectral theorem that it possesses a complete set of orthonormal eigenvectors. We denote them by $\\{ {u}_\\ell \\}_{\\ell=0,1,..., N-1}$. For convenience, we order the set of real, non-negative eigenvalues as follows: $0=\\lambda_0 < \\lambda_1 \\leq \\lambda_2 \\leq ... \\leq \\lambda_{N-1} = \\lambda_{\\rm max}$. When the graph is connected\\footnote{a path connects each pair of nodes in the graph},\nthere is only one zero eigenvalue. In fact, the multiplicity of the zero eigenvalue is equal to the number of connected components. See for example~\\cite{chung1997spectral,chung2005laplacians} for more details on spectral graph theory. \nThe eigenvectors of the Laplacian are used to define a graph Fourier basis~\\cite{shuman2013emerging,shuman2013vertex} which will be denoted as $U$. The eigenvalues are considered as a generalization of squared frequencies. The Laplacian matrix can thus be decomposed as \n\n", "index": 3, "text": "$$\n{\\mathcal{L}} = U\\Lambda U^*,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{L}}=U\\Lambda U^{*},\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi><mo>\u2062</mo><msup><mi>U</mi><mo>*</mo></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $\\hat{f'}$ is the Fourier transform of the filtered signal $f'$, $g$ the filter and $\\hat{f}$ the Fourier transform of the signal $f$. Equivalently, a filter defines the following matrix-valued function: \n", "itemtype": "equation", "pos": 12217, "prevtext": "\nwhere $U^*$ denotes the transposed conjugate of $U$.\nThe graph Fourier transform is written $\\hat{f} = {\\mathcal{F}} (f) = U^*f$ and its inverse $f = {\\mathcal{F}}^{-1}(\\hat{f})=U\\hat{f}$. This Graph Fourier Transform possesses interesting properties further studied in~\\cite{shuman2013vertex}. \n\n\\paragraph{Graph filters}\nThe graph Fourier-transform plays a central role in graph signal processing since it allows a natural extension of filtering operations. In the classical setting, applying a filter to a signal is carried out with a convolution, which is simply a point-wise multiplication in the spectral domain. \nSimilarly, filtering a graph signal is also a multiplication in the graph Fourier domain. A graph filter $g$ is defined as a continuous function $g:{\\mathbb{R}}_+ \\rightarrow {\\mathbb{R}}$. In the spectral domain, filtering a signal $f$ with a filter $g$ is therefore written as $\\hat{f'}[\\ell] =  g(\\lambda_\\ell) \\cdot \\hat{f}[\\ell]$ or in matrix notation\n\n", "index": 5, "text": "\\begin{equation*}\nf' = U g(\\Lambda) U^* f = g({\\mathcal{L}}) f,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"f^{\\prime}=Ug(\\Lambda)U^{*}f=g({\\mathcal{L}})f,\" display=\"block\"><mrow><mrow><msup><mi>f</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>U</mi><mo>*</mo></msup><mo>\u2062</mo><mi>f</mi></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $g(\\Lambda)$ is a diagonal matrix with entries $g(\\lambda_\\ell)$.\nIt is worth noting that these formulas make explicit use of the Laplacian eigenvectors and thus its diagonalization. The complexity of this operation is in general ${\\mathcal{O}}(N^3)$. In order to avoid this, there exist fast filtering algorithms based on Chebyshev polynomials or the Lanczos method~\\cite{hammond2011wavelets,susnjara2015accelerated}. These methods scale with the number of edges $|E|$ and reduce the complexity to ${\\mathcal{O}}(|E|)$, which is advantageous in the case of sparse graphs.\n\n\\paragraph{Localization operator}\nBecause most graphs do not possess a regular structure, we cannot translate a signal around the vertex set. This is problematic since translation plays a central role in stationarity. In order to overcome this issue, we use the localization operator defined in~\\cite{hammond2011wavelets,shuman2013vertex}, which is simply the convolution with a Kroneker delta. Localizing a filter $g$ onto  node $i$ reads:\n\n\n\n", "itemtype": "equation", "pos": 12507, "prevtext": "\nwhere $\\hat{f'}$ is the Fourier transform of the filtered signal $f'$, $g$ the filter and $\\hat{f}$ the Fourier transform of the signal $f$. Equivalently, a filter defines the following matrix-valued function: \n", "index": 7, "text": "$$g({\\mathcal{L}}) := U g(\\Lambda) U^*,$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"g({\\mathcal{L}}):=Ug(\\Lambda)U^{*},\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>U</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>U</mi><mo>*</mo></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $\\overline{{u}_\\ell}$ is the complex conjugate of ${u}_\\ell$. For a sufficiently regular function $g$, it has been proved in~\\cite{shuman2013vertex} that this operation localizes the filter $g$ around the vertex $i$.\nIn the classical case, the concept of translation is equivalent to localization since the convolution with a Kroneker delta is precisely a translation. However, for irregular graphs, localization differs from translation because the shape of the localized signal adapts to the graph and varies as a function of its topology. \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Stationarity for time series}\nLet ${\\bf x }(t)$ be a time indexed stochastic process. Throughout this contribution, all stochastic variables will be written in bold fonts. \n\n\n\n\\begin{definition}[Time Wide-Sense Stationarity]\nA signal is Time Wide-Sense Stationary  (WSS) if its first two statistical moments are invariant under translation, i.e:\n\\begin{enumerate}\n\t\\item ${\\mathbb{E}} \\big( {\\bf x}(t) \\big) = M$,\n\t\\item ${\\mathbb{E}} \\big( {\\bf x}(t){ \\bf x}^*(s) \\big) = \\gamma_{{\\bf x }}(t-s)$\n\\end{enumerate}\nwhere $\\gamma_{{\\bf x}}$ is the autocorrelation function of ${\\bf x}$.\n\\end{definition}\n\nFor a WSS signal, the autocorrelation function depends only on one parameter, $t-s$, and is linked to the Power Spectral Density (PSD) through the Wiener-Khintchine Theorem~\\cite{Wiener1930generalized}. The latter states that the PSD of the stochastic process ${\\bf x}$ denoted $S_{{\\bf x}}(\\omega)$ is the Fourier transform of its auto-correlation~:\n\n", "itemtype": "equation", "pos": 13572, "prevtext": "\nwhere $g(\\Lambda)$ is a diagonal matrix with entries $g(\\lambda_\\ell)$.\nIt is worth noting that these formulas make explicit use of the Laplacian eigenvectors and thus its diagonalization. The complexity of this operation is in general ${\\mathcal{O}}(N^3)$. In order to avoid this, there exist fast filtering algorithms based on Chebyshev polynomials or the Lanczos method~\\cite{hammond2011wavelets,susnjara2015accelerated}. These methods scale with the number of edges $|E|$ and reduce the complexity to ${\\mathcal{O}}(|E|)$, which is advantageous in the case of sparse graphs.\n\n\\paragraph{Localization operator}\nBecause most graphs do not possess a regular structure, we cannot translate a signal around the vertex set. This is problematic since translation plays a central role in stationarity. In order to overcome this issue, we use the localization operator defined in~\\cite{hammond2011wavelets,shuman2013vertex}, which is simply the convolution with a Kroneker delta. Localizing a filter $g$ onto  node $i$ reads:\n\n\n\n", "index": 9, "text": "\\begin{equation}\n\\label{def:localization_operator}\n{\\mathcal{T}}_i g[n]  = \\sum_{\\ell=0}^{N-1} g(\\lambda_\\ell) \\overline{{u}_\\ell}[i] {u}_\\ell[n] = \\left(g({\\mathcal{L}})\\right)_{in},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{T}}_{i}g[n]=\\sum_{\\ell=0}^{N-1}g(\\lambda_{\\ell})\\overline{{u}_{\\ell}%&#10;}[i]{u}_{\\ell}[n]=\\left(g({\\mathcal{L}})\\right)_{in},\" display=\"block\"><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mi>i</mi></msub><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><msub><mi>u</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>=</mo><msub><mrow><mo>(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nAs a consequence, when a signal is filtered, its PSD is multiplied by the energy of the filter: for ${\\bf y} = h \\ast {\\bf x}$, we have\n\n", "itemtype": "equation", "pos": 15328, "prevtext": "\nwhere $\\overline{{u}_\\ell}$ is the complex conjugate of ${u}_\\ell$. For a sufficiently regular function $g$, it has been proved in~\\cite{shuman2013vertex} that this operation localizes the filter $g$ around the vertex $i$.\nIn the classical case, the concept of translation is equivalent to localization since the convolution with a Kroneker delta is precisely a translation. However, for irregular graphs, localization differs from translation because the shape of the localized signal adapts to the graph and varies as a function of its topology. \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Stationarity for time series}\nLet ${\\bf x }(t)$ be a time indexed stochastic process. Throughout this contribution, all stochastic variables will be written in bold fonts. \n\n\n\n\\begin{definition}[Time Wide-Sense Stationarity]\nA signal is Time Wide-Sense Stationary  (WSS) if its first two statistical moments are invariant under translation, i.e:\n\\begin{enumerate}\n\t\\item ${\\mathbb{E}} \\big( {\\bf x}(t) \\big) = M$,\n\t\\item ${\\mathbb{E}} \\big( {\\bf x}(t){ \\bf x}^*(s) \\big) = \\gamma_{{\\bf x }}(t-s)$\n\\end{enumerate}\nwhere $\\gamma_{{\\bf x}}$ is the autocorrelation function of ${\\bf x}$.\n\\end{definition}\n\nFor a WSS signal, the autocorrelation function depends only on one parameter, $t-s$, and is linked to the Power Spectral Density (PSD) through the Wiener-Khintchine Theorem~\\cite{Wiener1930generalized}. The latter states that the PSD of the stochastic process ${\\bf x}$ denoted $S_{{\\bf x}}(\\omega)$ is the Fourier transform of its auto-correlation~:\n\n", "index": 11, "text": "\\begin{equation} \\label{theo:Wiener_khintchine}\nS_{{\\bf x}}(\\omega) = \\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty} \\gamma_{{\\bf x}}(t)e^{-i\\omega t} \\text{d}t.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"S_{{\\bf x}}(\\omega)=\\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty}\\gamma_{{\\bf x}}(t)e%&#10;^{-i\\omega t}\\text{d}t.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></msubsup><mrow><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>\u03c9</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2062</mo><mtext>d</mtext><mo>\u2062</mo><mi>t</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $\\hat{h}(\\omega) $ is the Fourier transform of the filter. For more information about stationarity, we refer the reader to~\\cite{papoulis2002probability}.\n\nNote that equivalent definitions exist for continuous, discrete and periodic signals. When generalizing these concepts to graphs, the underlying structure for the stationarity will no longer be time but graph vertices. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Stationarity of graph signals} \\label{sec:theory}\nLet ${\\bf x} \\in {\\mathbb{R}}^N$ be a stochastic process with a finite number of variables indexed by the vertices of a weighted undirected graph. The expected value of each variable is written ${\\mathbb{E}} \\big({\\bf x}[i]\\big)$ and the covariance matrix of the process is $\\Sigma_{{\\bf x}}[i,j] = {\\mathbb{E}} \\big( {\\bf x}^*[i] {\\bf x}[j] \\big)$. Note that the covariance $\\Sigma_{{\\bf x}}$ can obviously be estimated empirically with no information about the graph itself via \n\n", "itemtype": "equation", "pos": 15633, "prevtext": "\nAs a consequence, when a signal is filtered, its PSD is multiplied by the energy of the filter: for ${\\bf y} = h \\ast {\\bf x}$, we have\n\n", "index": 13, "text": "$$\nS_{{\\bf y}}(\\omega) = \\left|\\hat{h}(\\omega) \\right|^2 S_{{\\bf x}}(\\omega),\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"S_{{\\bf y}}(\\omega)=\\left|\\hat{h}(\\omega)\\right|^{2}S_{{\\bf x}}(\\omega),\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>\ud835\udc32</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mrow><mo>|</mo><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msub><mi>S</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $X$ are $N_s$ instances of the stochastic processes stacked together column-wise and $\\tilde{X} = X - \\text{mean}(X)$ is the centered process. \n\nFor discrete time WSS processes, the covariance matrix $\\Sigma_{{\\bf x}}$ is Toeplitz, or circulant for periodic boundary conditions, reflecting translation invariance. In that case, the covariance is diagonalized by the Fourier transform. We are now going to generalize this property to take into account the intricate graph structure. \n\n\n\\begin{definition} \\label{def:GWSS}\nA stochastic process ${\\bf x}$ defined on the vertices of a graph ${\\mathcal{G}}$ is called Graph Wide-Sense (or second order) Stationary (GWSS), if and only if it satisfies the following properties~:\n\\begin{enumerate}\n\\item its first moment is constant over the vertex set, i.e ${\\mathbb{E}}\\big({\\bf x}[i]\\big) = M$ and\n\\item its covariance matrix $\\Sigma_{{\\bf x}}[i,j] = {\\mathbb{E}}\\big( {\\bf x}[i] {\\bf x}[j]\\big)$ is jointly diagonalizable with the Laplacian of ${\\mathcal{G}}$.\n\\end{enumerate}\n\\end{definition}\nThe first part of the above definition is equivalent to the first property of time WSS signal. The requirement for the second moment is a natural generalization where we are imposing the use of the graph Fourier transform but seems to obscure any notion of translation invariance. We shall now see that this condition is equivalent to imposing that the covariance is invariant under the localization operator. The easiest way to observe this fact is to generalize the Wiener Khintchine Theorem \\ref{theo:Wiener_khintchine}, which makes the link between the localization operator and the covariance matrix.\n\\begin{theorem} \\label{theo:Wiener_khintchine_graph}\nIf a signal is GWSS, its covariance matrix is given by graph localization of a kernel $\\gamma_{{\\bf x}}$, i.e:\n\n", "itemtype": "equation", "pos": 16652, "prevtext": "\nwhere $\\hat{h}(\\omega) $ is the Fourier transform of the filter. For more information about stationarity, we refer the reader to~\\cite{papoulis2002probability}.\n\nNote that equivalent definitions exist for continuous, discrete and periodic signals. When generalizing these concepts to graphs, the underlying structure for the stationarity will no longer be time but graph vertices. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Stationarity of graph signals} \\label{sec:theory}\nLet ${\\bf x} \\in {\\mathbb{R}}^N$ be a stochastic process with a finite number of variables indexed by the vertices of a weighted undirected graph. The expected value of each variable is written ${\\mathbb{E}} \\big({\\bf x}[i]\\big)$ and the covariance matrix of the process is $\\Sigma_{{\\bf x}}[i,j] = {\\mathbb{E}} \\big( {\\bf x}^*[i] {\\bf x}[j] \\big)$. Note that the covariance $\\Sigma_{{\\bf x}}$ can obviously be estimated empirically with no information about the graph itself via \n\n", "index": 15, "text": "$$\n\\Sigma_{{\\bf x}} \\approx \\tilde{X} \\tilde{X}^\\top/N_s,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\Sigma_{{\\bf x}}\\approx\\tilde{X}\\tilde{X}^{\\top}/N_{s},\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo>\u2248</mo><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">~</mo></mover><mo>\u22a4</mo></msup></mrow><mo>/</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\n\\begin{proof}\nBy Definition~\\ref{def:GWSS}, the covariance matrix of a GWSS signal can be written as:\n\n", "itemtype": "equation", "pos": 18529, "prevtext": "\nwhere $X$ are $N_s$ instances of the stochastic processes stacked together column-wise and $\\tilde{X} = X - \\text{mean}(X)$ is the centered process. \n\nFor discrete time WSS processes, the covariance matrix $\\Sigma_{{\\bf x}}$ is Toeplitz, or circulant for periodic boundary conditions, reflecting translation invariance. In that case, the covariance is diagonalized by the Fourier transform. We are now going to generalize this property to take into account the intricate graph structure. \n\n\n\\begin{definition} \\label{def:GWSS}\nA stochastic process ${\\bf x}$ defined on the vertices of a graph ${\\mathcal{G}}$ is called Graph Wide-Sense (or second order) Stationary (GWSS), if and only if it satisfies the following properties~:\n\\begin{enumerate}\n\\item its first moment is constant over the vertex set, i.e ${\\mathbb{E}}\\big({\\bf x}[i]\\big) = M$ and\n\\item its covariance matrix $\\Sigma_{{\\bf x}}[i,j] = {\\mathbb{E}}\\big( {\\bf x}[i] {\\bf x}[j]\\big)$ is jointly diagonalizable with the Laplacian of ${\\mathcal{G}}$.\n\\end{enumerate}\n\\end{definition}\nThe first part of the above definition is equivalent to the first property of time WSS signal. The requirement for the second moment is a natural generalization where we are imposing the use of the graph Fourier transform but seems to obscure any notion of translation invariance. We shall now see that this condition is equivalent to imposing that the covariance is invariant under the localization operator. The easiest way to observe this fact is to generalize the Wiener Khintchine Theorem \\ref{theo:Wiener_khintchine}, which makes the link between the localization operator and the covariance matrix.\n\\begin{theorem} \\label{theo:Wiener_khintchine_graph}\nIf a signal is GWSS, its covariance matrix is given by graph localization of a kernel $\\gamma_{{\\bf x}}$, i.e:\n\n", "index": 17, "text": "\\begin{equation} \n{\\mathbb{E}} ({\\bf x}[i]{\\bf x}[j])  = \\Sigma_{ {\\bf x} }[i,j] =  {\\mathcal{T}}_i\\gamma_{ { \\bf x} }(j) = \\left( \\gamma_{{ \\bf x}} ( {\\mathcal{L}} ) \\right)_{ij}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}({\\bf x}[i]{\\bf x}[j])=\\Sigma_{{\\bf x}}[i,j]={\\mathcal{T}}_{i}%&#10;\\gamma_{{\\bf x}}(j)=\\left(\\gamma_{{\\bf x}}({\\mathcal{L}})\\right)_{ij}\" display=\"block\"><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>(</mo><mrow><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $\\Gamma_{{\\bf x}}$ is a diagonal matrix. For any filter $\\gamma_{{\\bf x}}$ satisfying $\\gamma_{{\\bf x}}({\\lambda_\\ell}) = \\Gamma_{{\\bf x}}(\\ell,\\ell)$, we observe that $\\Sigma_{{\\bf x}}$ is the graph localized version of $\\gamma_{{\\bf x}}$.\n\\end{proof}\n\\end{theorem}\nThe choice of $\\gamma_{{\\bf x}}$ in this result is somewhat arbitrary but we shall soon see that we are interested in localized kernels. In that case, $\\gamma_{{\\bf x}}$ will be typically be the lowest degree polynomial satisfying the constraints and can be constructed using Lagrange interpolation for instance. \n\nTheorem~\\ref{theo:Wiener_khintchine_graph} provides a fundamental information about the covariance. The size of the correlation (distance over the graph) depends on the size of the support of localized the kernel ${\\mathcal{T}}_i \\gamma_{{\\bf x}}$. In~\\cite{shuman2013vertex} (Theorem 1 and Corrolary 2), it has been proved that the size of ${\\mathcal{T}}_i \\gamma_{{\\bf x}}$ depend on the regularity of $\\gamma_{{\\bf x}}$. For example, if $\\gamma_{{\\bf x}}$ is polynomial of degree $K$, it is exactly localized in a ball of radius $K$. Hence we will be mostly interested in such low degree polynomial kernels. \n\n The graph power spectral density (PSD) matrix of a stochastic process is given by $\\Gamma_{{\\bf x}} = U^*\\Sigma_{{\\bf x}} U$. For a GWSS signal this matrix is diagonal and the graph PSD of ${\\bf x}$ becomes:\n\n", "itemtype": "equation", "pos": 18826, "prevtext": "\n\\begin{proof}\nBy Definition~\\ref{def:GWSS}, the covariance matrix of a GWSS signal can be written as:\n\n", "index": 19, "text": "\\begin{equation}\n\\Sigma_{{\\bf x}} = U \\Gamma_{{\\bf x}} U^*\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\Sigma_{{\\bf x}}=U\\Gamma_{{\\bf x}}U^{*}\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><msup><mi>U</mi><mo>*</mo></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": " \nTable \\ref{tab:summary_stationarity} present the difference and the similarities between the classical and the graph case.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{example}[Gaussian noise]\nGaussian noise is WSS for any graph. Indeed, the first moment is ${\\mathbb{E}}\\big({\\bf x}[k]\\big) = 0 $. Moreover, the covariance matrix can be written as $I = \\Sigma_{{\\bf x}} = U I U^*$ with any orthonormal matrix $U$ and thus is diagonalizable with any graph Laplacian. We also observe that the PSD is constant, which implies that similar to the classical case, white noise contains all \"graph frequencies\".\n\\end{example}\n\nWhen $\\gamma_{{\\bf x}}$ is a bijective function, the covariance matrix contains an important part of the graph structure: the laplacian eigenvectors. \n\n\nOn the contrary, if $\\gamma_{{\\bf x}}$ is not bijective, some of the graph structure is lost as it is not possible to recover all eigenvectors. This is for instance the case when the covariance matrix is low-rank.\nAs another example, let us consider completely uncorrelated samples. In this case, the covariance matrix becomes $\\Sigma_{{\\bf x}} = I$ and loses all graph information, even if by definition the process remains stationary on the graph. \n\n\nOne of the crucial benefits of stationarity is that it is preserved by linear filtering, while the PSD is simply reshaped by the filter. The same property holds on graphs.\n\\begin{theorem} \\label{theo:graph_psd_trans}\nWhen a graph filter $g$ is applied to a GWSS process, the result remains GWSS and the PSD satisfies:\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $\\Gamma_{{\\bf x}}$ is a diagonal matrix. For any filter $\\gamma_{{\\bf x}}$ satisfying $\\gamma_{{\\bf x}}({\\lambda_\\ell}) = \\Gamma_{{\\bf x}}(\\ell,\\ell)$, we observe that $\\Sigma_{{\\bf x}}$ is the graph localized version of $\\gamma_{{\\bf x}}$.\n\\end{proof}\n\\end{theorem}\nThe choice of $\\gamma_{{\\bf x}}$ in this result is somewhat arbitrary but we shall soon see that we are interested in localized kernels. In that case, $\\gamma_{{\\bf x}}$ will be typically be the lowest degree polynomial satisfying the constraints and can be constructed using Lagrange interpolation for instance. \n\nTheorem~\\ref{theo:Wiener_khintchine_graph} provides a fundamental information about the covariance. The size of the correlation (distance over the graph) depends on the size of the support of localized the kernel ${\\mathcal{T}}_i \\gamma_{{\\bf x}}$. In~\\cite{shuman2013vertex} (Theorem 1 and Corrolary 2), it has been proved that the size of ${\\mathcal{T}}_i \\gamma_{{\\bf x}}$ depend on the regularity of $\\gamma_{{\\bf x}}$. For example, if $\\gamma_{{\\bf x}}$ is polynomial of degree $K$, it is exactly localized in a ball of radius $K$. Hence we will be mostly interested in such low degree polynomial kernels. \n\n The graph power spectral density (PSD) matrix of a stochastic process is given by $\\Gamma_{{\\bf x}} = U^*\\Sigma_{{\\bf x}} U$. For a GWSS signal this matrix is diagonal and the graph PSD of ${\\bf x}$ becomes:\n\n", "index": 21, "text": "\\begin{equation} \\label{eq:cov_mat_fourier}\n\\gamma_{{\\bf x}}(\\lambda_\\ell) =  \\left( U^*\\Sigma_{{\\bf x}} U \\right)_{\\ell,\\ell}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\gamma_{{\\bf x}}(\\lambda_{\\ell})=\\left(U^{*}\\Sigma_{{\\bf x}}U\\right)_{\\ell,%&#10;\\ell}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>(</mo><mrow><msup><mi>U</mi><mo>*</mo></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mi>U</mi></mrow><mo>)</mo></mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>,</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\n\\begin{proof}\nThe output of a filter $g$ can be written ${\\bf x }' = g(L){\\bf x}$. If the input signal ${\\bf x }$ is GWSS, we can check easily that the first moment of the filter's output is constant, ${\\mathbb{E}} (g(L){\\bf x}[i])= g(0) M $. The computation of the second moment gives:\n\\begin{eqnarray*}\n{\\mathbb{E}} \\left(g({\\mathcal{L}})x \\big(g({\\mathcal{L}})x\\big)^*\\right) & = & g({\\mathcal{L}}) {\\mathbb{E}} \\left( x x^* \\right) g({\\mathcal{L}}) \\\\ \n& = & g({\\mathcal{L}}) \\Sigma_{{\\bf x}} g({\\mathcal{L}}) \\\\\n& = & U g^2(\\Lambda) \\gamma_{{\\bf x}}(\\Lambda) U^*.\n\\end{eqnarray*}\n\\end{proof}\n\\end{theorem}\n\nTheorem \\ref{theo:graph_psd_trans} provides a simple way to artificially produce stationary signals with a presecribed PSD by simply filtering white noise.\n\\begin{center}\n\\includegraphics[width=0.15\\textwidth]{figures/white_noise_filtering.png}\n\\end{center}\nThe resulting signal will be stationary with PSD $g^2$. \n\n\n\n\n\n\n\n\n\n\\begin{table*}[ht!]\n\\begin{center}\n\n\\begin{tabular}{|l|c|c|}\n\\hline\n & Classical & Graph \\\\\n\\hline\n\\hline\nStationary with respect to & Translation & The localization operator\\\\ \n\\hline\n First Moment & ${\\mathbb{E}} \\big( {\\bf x}[i] \\big) = M$ & ${\\mathbb{E}}\\big({\\bf x}[i]\\big) = M$ \\\\\n \\hline\n Second Moment & $\\Sigma_{{\\bf x}}[i,n] = {\\mathbb{E}} \\big( {\\bf x}[i]){ \\bf x}^*[n] \\big) = \\gamma_{{\\bf x }}[t-s]$ & $\\Sigma_{{\\bf x}}[i,n] = {\\mathbb{E}}\\big( {\\bf x}[i] {\\bf x}[n]\\big) = \\gamma_{{\\bf x}}({\\mathcal{L}})_{i,n}$ \\\\\n  & $\\Sigma_{{\\bf x}}$ Toeplitz & $\\Sigma_{{\\bf x}}$ diagonalizable with ${\\mathcal{L}}$\\\\\n \\hline\nWiener Khintchine & $S_{{\\bf x}}[\\ell] = \\frac{1}{\\sqrt{N}}\\sum_{i=1}^N \\gamma_{{\\bf x}}[n]e^{-j 2\\pi \\frac{n \\ell }{N}}$ & $ \\gamma_{{\\bf x}}(\\lambda_\\ell) = \\left(\\Gamma_{{\\bf x}}\\right)_{\\ell,\\ell} = \\left(U^* \\Sigma_{{\\bf x}} U\\right)_{\\ell,\\ell} $\\\\\n \\hline\nResult of filtering & $\\gamma_{g \\ast {\\bf x}}[\\ell] = g^2[{\\lambda_\\ell}] \\cdot \\gamma_{{\\bf x}} [\\ell]$  & $\\gamma_{g({\\mathcal{L}}){\\bf x}}[\\ell] = g^2({\\lambda_\\ell}) \\cdot \\gamma_{{\\bf x}} [\\ell]$ \\\\\n \\hline\n\\end{tabular}\n\\end{center}\n\\caption{Difference and similarities between classical and graph stationarity. In the classical case, we work with a $N$ periodic discrete signal.}\n\\label{tab:summary_stationarity}\n\\end{table*}\n\n\\paragraph{Gaussian random field interpretation}\nThe framework of stationary signals on graphs can be interpreted probabilistically using Gaussian Markov Random Field (GMRF). Generalizing the generative model of Gadde and Ortega (\\cite{gadde2015probabilistic} section 3), the authors of \\cite{zhang2015graph} assume that the signal ${\\bf x}$ is drawn from a distribution\n\n", "itemtype": "equation", "pos": 21972, "prevtext": " \nTable \\ref{tab:summary_stationarity} present the difference and the similarities between the classical and the graph case.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{example}[Gaussian noise]\nGaussian noise is WSS for any graph. Indeed, the first moment is ${\\mathbb{E}}\\big({\\bf x}[k]\\big) = 0 $. Moreover, the covariance matrix can be written as $I = \\Sigma_{{\\bf x}} = U I U^*$ with any orthonormal matrix $U$ and thus is diagonalizable with any graph Laplacian. We also observe that the PSD is constant, which implies that similar to the classical case, white noise contains all \"graph frequencies\".\n\\end{example}\n\nWhen $\\gamma_{{\\bf x}}$ is a bijective function, the covariance matrix contains an important part of the graph structure: the laplacian eigenvectors. \n\n\nOn the contrary, if $\\gamma_{{\\bf x}}$ is not bijective, some of the graph structure is lost as it is not possible to recover all eigenvectors. This is for instance the case when the covariance matrix is low-rank.\nAs another example, let us consider completely uncorrelated samples. In this case, the covariance matrix becomes $\\Sigma_{{\\bf x}} = I$ and loses all graph information, even if by definition the process remains stationary on the graph. \n\n\nOne of the crucial benefits of stationarity is that it is preserved by linear filtering, while the PSD is simply reshaped by the filter. The same property holds on graphs.\n\\begin{theorem} \\label{theo:graph_psd_trans}\nWhen a graph filter $g$ is applied to a GWSS process, the result remains GWSS and the PSD satisfies:\n\n", "index": 23, "text": "\\begin{equation}\n\\gamma_{g({\\mathcal{L}}){\\bf x}}[\\ell] = g^2({\\lambda_\\ell}) \\cdot \\gamma_{{\\bf x}} [\\ell]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\gamma_{g({\\mathcal{L}}){\\bf x}}[\\ell]=g^{2}({\\lambda_{\\ell}})\\cdot\\gamma_{{%&#10;\\bf x}}[\\ell]\" display=\"block\"><mrow><mrow><msub><mi>\u03b3</mi><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msup><mi>g</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $Z_p = \\int \\exp \\left( -  {\\bf x}^T p({\\mathcal{L}}) {\\bf x} \\right) \\text{d}{\\bf x} $. This leads to a stationary process ${\\bf x}$ with a covariance matrix given by: \n\n", "itemtype": "equation", "pos": 24728, "prevtext": "\n\\begin{proof}\nThe output of a filter $g$ can be written ${\\bf x }' = g(L){\\bf x}$. If the input signal ${\\bf x }$ is GWSS, we can check easily that the first moment of the filter's output is constant, ${\\mathbb{E}} (g(L){\\bf x}[i])= g(0) M $. The computation of the second moment gives:\n\\begin{eqnarray*}\n{\\mathbb{E}} \\left(g({\\mathcal{L}})x \\big(g({\\mathcal{L}})x\\big)^*\\right) & = & g({\\mathcal{L}}) {\\mathbb{E}} \\left( x x^* \\right) g({\\mathcal{L}}) \\\\ \n& = & g({\\mathcal{L}}) \\Sigma_{{\\bf x}} g({\\mathcal{L}}) \\\\\n& = & U g^2(\\Lambda) \\gamma_{{\\bf x}}(\\Lambda) U^*.\n\\end{eqnarray*}\n\\end{proof}\n\\end{theorem}\n\nTheorem \\ref{theo:graph_psd_trans} provides a simple way to artificially produce stationary signals with a presecribed PSD by simply filtering white noise.\n\\begin{center}\n\\includegraphics[width=0.15\\textwidth]{figures/white_noise_filtering.png}\n\\end{center}\nThe resulting signal will be stationary with PSD $g^2$. \n\n\n\n\n\n\n\n\n\n\\begin{table*}[ht!]\n\\begin{center}\n\n\\begin{tabular}{|l|c|c|}\n\\hline\n & Classical & Graph \\\\\n\\hline\n\\hline\nStationary with respect to & Translation & The localization operator\\\\ \n\\hline\n First Moment & ${\\mathbb{E}} \\big( {\\bf x}[i] \\big) = M$ & ${\\mathbb{E}}\\big({\\bf x}[i]\\big) = M$ \\\\\n \\hline\n Second Moment & $\\Sigma_{{\\bf x}}[i,n] = {\\mathbb{E}} \\big( {\\bf x}[i]){ \\bf x}^*[n] \\big) = \\gamma_{{\\bf x }}[t-s]$ & $\\Sigma_{{\\bf x}}[i,n] = {\\mathbb{E}}\\big( {\\bf x}[i] {\\bf x}[n]\\big) = \\gamma_{{\\bf x}}({\\mathcal{L}})_{i,n}$ \\\\\n  & $\\Sigma_{{\\bf x}}$ Toeplitz & $\\Sigma_{{\\bf x}}$ diagonalizable with ${\\mathcal{L}}$\\\\\n \\hline\nWiener Khintchine & $S_{{\\bf x}}[\\ell] = \\frac{1}{\\sqrt{N}}\\sum_{i=1}^N \\gamma_{{\\bf x}}[n]e^{-j 2\\pi \\frac{n \\ell }{N}}$ & $ \\gamma_{{\\bf x}}(\\lambda_\\ell) = \\left(\\Gamma_{{\\bf x}}\\right)_{\\ell,\\ell} = \\left(U^* \\Sigma_{{\\bf x}} U\\right)_{\\ell,\\ell} $\\\\\n \\hline\nResult of filtering & $\\gamma_{g \\ast {\\bf x}}[\\ell] = g^2[{\\lambda_\\ell}] \\cdot \\gamma_{{\\bf x}} [\\ell]$  & $\\gamma_{g({\\mathcal{L}}){\\bf x}}[\\ell] = g^2({\\lambda_\\ell}) \\cdot \\gamma_{{\\bf x}} [\\ell]$ \\\\\n \\hline\n\\end{tabular}\n\\end{center}\n\\caption{Difference and similarities between classical and graph stationarity. In the classical case, we work with a $N$ periodic discrete signal.}\n\\label{tab:summary_stationarity}\n\\end{table*}\n\n\\paragraph{Gaussian random field interpretation}\nThe framework of stationary signals on graphs can be interpreted probabilistically using Gaussian Markov Random Field (GMRF). Generalizing the generative model of Gadde and Ortega (\\cite{gadde2015probabilistic} section 3), the authors of \\cite{zhang2015graph} assume that the signal ${\\bf x}$ is drawn from a distribution\n\n", "index": 25, "text": "\\begin{equation}\np( {\\bf x} ) = \\frac{1}{Z_p} \\exp \\left( -  {\\bf x}^T p({\\mathcal{L}}) {\\bf x} \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"p({\\bf x})=\\frac{1}{Z_{p}}\\exp\\left(-{\\bf x}^{T}p({\\mathcal{L}}){\\bf x}\\right)\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>Z</mi><mi>p</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc31</mi><mi>T</mi></msup><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nStationarity and the GMRF probabilistic model are thus linked by $\\textrm{PSD} = p^{-1}$.\n\n\n\\section{Graph Wiener filters and optimization framework} \\label{sec:wiener}\nUsing stationary signals, we can naturally extend the framework of Wiener filters~\\cite{Wiener1949extrapolation} largely used in signal processing for Mean Square Error (MSE) optimal linear prediction. Since the construction of Wiener filters is very similar for non-graph and graph signals, we present only the latter here. The main difference is that the traditional frequencies are replaced by the graph Laplacian eigenvalues\\footnote{The graph eigenvalues are equivalent to classical squared frequencies.} ${\\lambda_\\ell}$. The full Wiener recovery process is summarized in Figure~\\ref{fig:winner_filter}. \n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.35\\textwidth]{figures/winner_filter.png} \n\\end{center}\n\\caption{Wiener process model. $w_s$ and $w_n$ are white noises with different scaling,  $h$ is the convolution filter and $g$ the Wiener filter. $x$ is the original signal, $y$ the measurements, $\\tilde{x}$ the recovery and $e$ the final error.}\n\\label{fig:winner_filter}\n\\end{figure}\n\n\nThe Wiener filter can be used to produce a mean-square error optimal estimate of a stationary signal under a linear but noisy observation model.\n\nLet us consider a GWSS process ${\\bf x}$ with PSD of $s^2({\\lambda_\\ell})$. The measurements ${\\bf y}$ are given by:\n\n\n", "itemtype": "equation", "pos": 25023, "prevtext": "\nwhere $Z_p = \\int \\exp \\left( -  {\\bf x}^T p({\\mathcal{L}}) {\\bf x} \\right) \\text{d}{\\bf x} $. This leads to a stationary process ${\\bf x}$ with a covariance matrix given by: \n\n", "index": 27, "text": "$$\n\\Sigma_{{\\bf x}} = \\left(p({\\mathcal{L}}) \\right)^{-1} = p^{-1}({\\mathcal{L}}).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\Sigma_{{\\bf x}}=\\left(p({\\mathcal{L}})\\right)^{-1}=p^{-1}({\\mathcal{L}}).\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>p</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $h$ is a graph filter and $w_n$ additive noise of PSD $n({\\lambda_\\ell})$.\n\n\nTo recover ${\\bf x}$, we extend Wiener filters to the graph case:\n\n", "itemtype": "equation", "pos": 26557, "prevtext": "\nStationarity and the GMRF probabilistic model are thus linked by $\\textrm{PSD} = p^{-1}$.\n\n\n\\section{Graph Wiener filters and optimization framework} \\label{sec:wiener}\nUsing stationary signals, we can naturally extend the framework of Wiener filters~\\cite{Wiener1949extrapolation} largely used in signal processing for Mean Square Error (MSE) optimal linear prediction. Since the construction of Wiener filters is very similar for non-graph and graph signals, we present only the latter here. The main difference is that the traditional frequencies are replaced by the graph Laplacian eigenvalues\\footnote{The graph eigenvalues are equivalent to classical squared frequencies.} ${\\lambda_\\ell}$. The full Wiener recovery process is summarized in Figure~\\ref{fig:winner_filter}. \n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.35\\textwidth]{figures/winner_filter.png} \n\\end{center}\n\\caption{Wiener process model. $w_s$ and $w_n$ are white noises with different scaling,  $h$ is the convolution filter and $g$ the Wiener filter. $x$ is the original signal, $y$ the measurements, $\\tilde{x}$ the recovery and $e$ the final error.}\n\\label{fig:winner_filter}\n\\end{figure}\n\n\nThe Wiener filter can be used to produce a mean-square error optimal estimate of a stationary signal under a linear but noisy observation model.\n\nLet us consider a GWSS process ${\\bf x}$ with PSD of $s^2({\\lambda_\\ell})$. The measurements ${\\bf y}$ are given by:\n\n\n", "index": 29, "text": "\\begin{equation}\ny = h({\\mathcal{L}})  x + w_n,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"y=h({\\mathcal{L}})x+w_{n},\" display=\"block\"><mrow><mrow><mi>y</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThe expression above can be derived identically as in the classical case and minimizes the expected quadratic error, which can be written as:\n\n", "itemtype": "equation", "pos": 26769, "prevtext": "\nwhere $h$ is a graph filter and $w_n$ additive noise of PSD $n({\\lambda_\\ell})$.\n\n\nTo recover ${\\bf x}$, we extend Wiener filters to the graph case:\n\n", "index": 31, "text": "\\begin{equation} \\label{eq:Wiener_filter}\ng({\\lambda_\\ell}) = \\frac{h({\\lambda_\\ell})s^2({\\lambda_\\ell})}{h^2({\\lambda_\\ell})s^2({\\lambda_\\ell}) + n({\\lambda_\\ell})}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"g({\\lambda_{\\ell}})=\\frac{h({\\lambda_{\\ell}})s^{2}({\\lambda_{\\ell}})}{h^{2}({%&#10;\\lambda_{\\ell}})s^{2}({\\lambda_{\\ell}})+n({\\lambda_{\\ell}})}.\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><msup><mi>h</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nTheorem~\\ref{theo:optimality} proves the optimality of this filter for the graph case.\n\n\\paragraph{Wiener optimization}\nIn this contribution, we would like to address a  more general problem. \nLet us suppose that our measurements are generated as:\n\n", "itemtype": "equation", "pos": 27093, "prevtext": "\nThe expression above can be derived identically as in the classical case and minimizes the expected quadratic error, which can be written as:\n\n", "index": 33, "text": "$$\ne({\\lambda_\\ell}) = {\\mathbb{E}} \\left( \\hat{x}({\\lambda_\\ell}) - \\hat{\\tilde{x}}({\\lambda_\\ell})\\right)^2= {\\mathbb{E}} \\left( \\hat{x}({\\lambda_\\ell}) - g({\\lambda_\\ell})\\hat{y}({\\lambda_\\ell})\\right)^2\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"e({\\lambda_{\\ell}})={\\mathbb{E}}\\left(\\hat{x}({\\lambda_{\\ell}})-\\hat{\\tilde{x}%&#10;}({\\lambda_{\\ell}})\\right)^{2}={\\mathbb{E}}\\left(\\hat{x}({\\lambda_{\\ell}})-g({%&#10;\\lambda_{\\ell}})\\hat{y}({\\lambda_{\\ell}})\\right)^{2}\" display=\"block\"><mrow><mrow><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere the original signal $x_o$ has a PSD denoted $s^2({\\lambda_\\ell})$ and the noise $w_n$ a PSD of $n({\\lambda_\\ell})$. \nSince the operator $A$ is not assumed to be diagonalizable with ${\\mathcal{L}}$, we cannot build a Wiener filter that constructs a direct estimation of the signal $x$.\nIf the signal $x$ varies smoothly on the graph, i.e is low frequency based, a classic optimization scheme would be the following:\n\n", "itemtype": "equation", "pos": 27551, "prevtext": "\nTheorem~\\ref{theo:optimality} proves the optimality of this filter for the graph case.\n\n\\paragraph{Wiener optimization}\nIn this contribution, we would like to address a  more general problem. \nLet us suppose that our measurements are generated as:\n\n", "index": 35, "text": "\\begin{equation} \\label{eq:general_data_model}\ny = Ax_o+w_n,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"y=Ax_{o}+w_{n},\" display=\"block\"><mrow><mrow><mi>y</mi><mo>=</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>x</mi><mi>o</mi></msub></mrow><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThis optimization scheme presents two main disadvantages. Firstly, the parameter $\\gamma$ must be tuned in order to remove the best amount of noise. Secondly, it does not take into account of the data structure charcterized by the PSD $s^2({\\lambda_\\ell})$.\n\nOur solution to overcome these issues is to solve the following optimization problem (that we suggestively call  Wiener optimization):\n\n\n\n\n", "itemtype": "equation", "pos": 28048, "prevtext": "\nwhere the original signal $x_o$ has a PSD denoted $s^2({\\lambda_\\ell})$ and the noise $w_n$ a PSD of $n({\\lambda_\\ell})$. \nSince the operator $A$ is not assumed to be diagonalizable with ${\\mathcal{L}}$, we cannot build a Wiener filter that constructs a direct estimation of the signal $x$.\nIf the signal $x$ varies smoothly on the graph, i.e is low frequency based, a classic optimization scheme would be the following:\n\n", "index": 37, "text": "\\begin{equation} \\label{prob:graph_tik}\n{\\operatorname{arg\\,min}}_x \\|Ax-y\\|_2^2 + \\gamma x^tLx \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{arg\\,min}}_{x}\\|Ax-y\\|_{2}^{2}+\\gamma x^{t}Lx\" display=\"block\"><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mi>x</mi><mi>t</mi></msup><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $w({\\lambda_\\ell})$ is the Fourier penalization weights. These weights are defined as \n\n", "itemtype": "equation", "pos": 28557, "prevtext": "\nThis optimization scheme presents two main disadvantages. Firstly, the parameter $\\gamma$ must be tuned in order to remove the best amount of noise. Secondly, it does not take into account of the data structure charcterized by the PSD $s^2({\\lambda_\\ell})$.\n\nOur solution to overcome these issues is to solve the following optimization problem (that we suggestively call  Wiener optimization):\n\n\n\n\n", "index": 39, "text": "\\begin{equation} \\label{prob:Wiener-opt}\n \n \\tilde{x} = {\\operatorname{arg\\,min}}_x  \\|Ax - y\\|_2^2 + \\|w({\\mathcal{L}}) x \\|_2^2,\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\tilde{x}={\\operatorname{arg\\,min}}_{x}\\|Ax-y\\|_{2}^{2}+\\|w({\\mathcal{L}}%&#10;)x\\|_{2}^{2},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": " \nProblem~\\eqref{prob:Wiener-opt} generalizes Problem~\\eqref{prob:graph_tik} which assumes implicitly a PSD of $\\frac{1}{{\\lambda_\\ell}}$ and a constant noise level of $\\gamma$ across all frequencies.\nNote that this framework generalizes two main assumptions done on the data in practice:\n\\begin{enumerate}\n\t\\item The signal is smooth on the graph, i.e: the edge derivative has a small $\\ell_2$-norm. As seen before this is done by setting the PSD as $\\frac{1}{{\\lambda_\\ell}}$. \n\t\\item The signal is band-limited, i.e: is it a linear combination of the $k$ lowest graph Laplacian eigenvectors. This class of signal simply have a null PSD for $\\lambda_\\ell>\\lambda_k$.\n\\end{enumerate}\n\n\\paragraph{3 motivations for the optimization framework}\nThe first motivation is intuitive. The weight $w({\\lambda_\\ell})$ heavily penalizes frequencies associated to low SNR and vice versa.\n\nThe second and main motivation is theoretical. When $A$ is diagonalizable, the solution to Problem~\\eqref{prob:Wiener-opt} is the application of the corresponding Wiener filter. \n\\begin{theorem} \\label{theo:optimality}\nIf the operator $A$ is diagonalizable with ${\\mathcal{L}}$, (i.e: $A = a({\\mathcal{L}}) = U a(\\Lambda)U^*$), then problem \\ref{prob:Wiener-opt} is optimal with respect of the weighting $w$ in the sense that its solution minimizes the mean square error:\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $w({\\lambda_\\ell})$ is the Fourier penalization weights. These weights are defined as \n\n", "index": 41, "text": "$$\nw({\\lambda_\\ell})=\\left|\\frac{\\sqrt{n({\\lambda_\\ell})}}{s({\\lambda_\\ell})}\\right|=\\frac{1}{\\sqrt{SNR({\\lambda_\\ell})}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"w({\\lambda_{\\ell}})=\\left|\\frac{\\sqrt{n({\\lambda_{\\ell}})}}{s({\\lambda_{\\ell}}%&#10;)}\\right|=\\frac{1}{\\sqrt{SNR({\\lambda_{\\ell}})}}.\" display=\"block\"><mrow><mrow><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>|</mo><mfrac><msqrt><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>|</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mi>S</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nAdditionally, the solution can be computed by the application of the corresponding Wiener filter.\n\\end{theorem}\nThe proof is given in Appendix \\ref{sec:proof of optimality}. \n\n\nThe last motivation is algorithmic and requires the knowledge of proximal splitting methods~\\cite{combettes2011proximal,komodakis2014playing}. Problem~\\eqref{prob:Wiener-opt} can be solved by a splitting scheme that minimizes iteratively each of the terms. The minimization of the regularizer, i.e the proximal operator of $\\|w({\\mathcal{L}}) x \\|_2^2 $, becomes a Wiener de-noising operation:\n\\begin{eqnarray*}\n\\mbox{prox}_{\\frac{1}{2}\\|w(L))x\\|_{2}^{2}}(y) \n& = & {\\operatorname{arg\\,min}}_{x}\\|w\\mathcal{F}x\\|_{2}^{2}+\\|x-y\\|_{2}^{2} \\\\ \n& = & g({\\mathcal{L}})y\n\\end{eqnarray*}\nwith \n\n", "itemtype": "equation", "pos": 30272, "prevtext": " \nProblem~\\eqref{prob:Wiener-opt} generalizes Problem~\\eqref{prob:graph_tik} which assumes implicitly a PSD of $\\frac{1}{{\\lambda_\\ell}}$ and a constant noise level of $\\gamma$ across all frequencies.\nNote that this framework generalizes two main assumptions done on the data in practice:\n\\begin{enumerate}\n\t\\item The signal is smooth on the graph, i.e: the edge derivative has a small $\\ell_2$-norm. As seen before this is done by setting the PSD as $\\frac{1}{{\\lambda_\\ell}}$. \n\t\\item The signal is band-limited, i.e: is it a linear combination of the $k$ lowest graph Laplacian eigenvectors. This class of signal simply have a null PSD for $\\lambda_\\ell>\\lambda_k$.\n\\end{enumerate}\n\n\\paragraph{3 motivations for the optimization framework}\nThe first motivation is intuitive. The weight $w({\\lambda_\\ell})$ heavily penalizes frequencies associated to low SNR and vice versa.\n\nThe second and main motivation is theoretical. When $A$ is diagonalizable, the solution to Problem~\\eqref{prob:Wiener-opt} is the application of the corresponding Wiener filter. \n\\begin{theorem} \\label{theo:optimality}\nIf the operator $A$ is diagonalizable with ${\\mathcal{L}}$, (i.e: $A = a({\\mathcal{L}}) = U a(\\Lambda)U^*$), then problem \\ref{prob:Wiener-opt} is optimal with respect of the weighting $w$ in the sense that its solution minimizes the mean square error:\n\n", "index": 43, "text": "\\begin{equation*}\n{\\mathbb{E}} \\left(\\| e \\|_2^2 \\right) = {\\mathbb{E}} \\left(\\| \\dot{x} - x_o \\|_2^2 \\right) = {\\mathbb{E}} \\left( \\sum_{i=1}^N \\left(\\dot{x}(i) - x_o(i) \\right)^2 \\right).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}\\left(\\|e\\|_{2}^{2}\\right)={\\mathbb{E}}\\left(\\|\\dot{x}-x_{o}\\|_{2}%&#10;^{2}\\right)={\\mathbb{E}}\\left(\\sum_{i=1}^{N}\\left(\\dot{x}(i)-x_{o}(i)\\right)^{%&#10;2}\\right).\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mrow><mo>\u2225</mo><mi>e</mi><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>x</mi><mo>\u02d9</mo></mover><mo>-</mo><msub><mi>x</mi><mi>o</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>(</mo><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo>\u02d9</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\n\n\n\n\n\\paragraph{Solving the problem}\nNote that Problem~\\eqref{prob:Wiener-opt} can be solved with a simple gradient descent. However, for a large number of nodes $N$, the matrix $w({\\mathcal{L}})$ requires $O(N^3)$ operations to be computed and $O(N^2)$ bits to be stored. This difficulty can be overcome by applying its corresponding filter operator at each iteration. As already mentioned, the cost of the approximation is $O(|E|)$~\\cite{susnjara2015accelerated}.\n\nWhen $s({\\lambda_\\ell})\\approx 0 $ for some ${\\lambda_\\ell}$ the operator $w({\\mathcal{L}})$ becomes badly conditioned. To overcome this issue, Problem~\\eqref{prob:Wiener-opt} can be solved efficiently using a forward-backward splitting scheme~\\cite{combettes2005signal,combettes2011proximal,komodakis2014playing}. The proximal operator of $\\|w({\\mathcal{L}})x\\|_2^2 $ has been given above and we use the term $ \\|Ax-y\\|_2^2$ as the differentiable function. Algorithm~\\ref{CHalgorithm} uses an accelerated forward backward scheme~\\cite{beck2009fast} to solve Problem~\\eqref{prob:Wiener-opt} where $\\gamma$ is the step size (we use $\\gamma = \\frac{1}{2{\\lambda_{\\rm max}}(A)^2}$), $\\epsilon$ the stopping tolerance, $J$ the maximum number of iterations and $\\delta$ is a very small number to avoid a possible division by $0$.\n\\begin{algorithm}[ht!]\n\\caption{Fast Wiener optimization to solve~\\eqref{prob:Wiener-opt}}\n\\label{CHalgorithm}\n\\begin{algorithmic}\n\\State INPUT: $z_1 = x$, $u_0 = x$, $t_1 = 1$, $\\epsilon > 0$\n\\For{ $j = 1,\\dots J$ }\n\\State  $v = A^*(Az_{j}-y) g(z_{j})$ \n\\State  $u_{j+1} = \\gamma g(L) v$ with $g(\\lambda) = \\frac{s^2(\\lambda)}{s^2(\\lambda)+\\gamma n(\\lambda)}$\n\\State  $t_{j+1} = \\frac{1+\\sqrt{1+4t_j^2}}{2}$\n\\State  $z_{j+1} = z_j +\\frac{t_j-1}{t_{j+1}} (u_j-u_{j-1})$ \n\\If{$\\frac{\\|z_{j+1} - z_{j}\\|_F^2}{\\| z_{j}\\|_F^2+\\delta}<\\epsilon$}\n\\State BREAK\n\\EndIf\n\\EndFor\n\\State SOLUTION: $z_J$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Estimation of the signal PSD} \\label{sec:psd_estimation}\nAs the PSD is central in our method, we need a reliable and scalable way to compute it. Equation~\\eqref{eq:cov_mat_fourier} suggests a direct estimation method using the Fourier transform of the covariance matrix. Unfortunately, when the number of nodes is considerable, this method requires the diagonalization of the Laplacian, an operation whose complexity in the general case scales as $O(N^3)$ for the number of operations and $O(N^2)$ for memory requirements. Additionally, when the number of available realizations of the process is small, it is not possible to obtain a good estimate of the covariance matrix. To overcome these issues, inspired by Bartlett~\\cite{bartlett1950periodogram} and Welch~\\cite{welch1967use}, we propose to use a graph generalization of the Short Time Fourier transform~\\cite{shuman2013vertex} to construct a scalable estimation method.\n\nBartlett's method can be summarized as follows. The signal is first cut into equally sized segments without overlap. Then, the Fourier transform of each segment is computed. Finally, the PSD  is obtained by averaging over segments the squared amplitude of the Fourier coefficients. Welch's method is a generalization that works with segment overlap.\n\nA direct generalization of these method is complicated because it requires to cut the signal in the vertex domain. Additionally, it would not be scalable, since it still requires to compute the Fourier transform of the signal.\nIn this contribution, we use another angle. Both Bartlett and Welch methods can be generalized with the idea that a PSD estimation is obtained by averaging the squared short time Fourier coefficients over the time.\n\nWe propose a method based on this last principle. Instead of a translated (rectangular) window, we use a kernel $g$ localized over all the nodes of the graph. This kernel is then uniformly shifted in the spectral domain ($g_m({\\lambda_\\ell}) = g({\\lambda_\\ell}-m\\tau)$) to obtain a generalization of the short time Fourier transform. This kind of filterbank has been largely used in classical signal processing and recently introduced for graphs \\cite{shuman2013vertex}. Our algorithm simply consists in averaging the squared coefficients of this transform over the vertex set.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause graphs have an irregular spectrum, we additionally need a normalization factor which is given by the norm of the window $g_m$: $\\|g_m\\|_2^2 = \\sum_\\ell g({\\lambda_\\ell}-m\\tau)^2$. Note that this norm will vary for the different $m$. Our final estimator reads~:\n\n", "itemtype": "equation", "pos": 31242, "prevtext": "\nAdditionally, the solution can be computed by the application of the corresponding Wiener filter.\n\\end{theorem}\nThe proof is given in Appendix \\ref{sec:proof of optimality}. \n\n\nThe last motivation is algorithmic and requires the knowledge of proximal splitting methods~\\cite{combettes2011proximal,komodakis2014playing}. Problem~\\eqref{prob:Wiener-opt} can be solved by a splitting scheme that minimizes iteratively each of the terms. The minimization of the regularizer, i.e the proximal operator of $\\|w({\\mathcal{L}}) x \\|_2^2 $, becomes a Wiener de-noising operation:\n\\begin{eqnarray*}\n\\mbox{prox}_{\\frac{1}{2}\\|w(L))x\\|_{2}^{2}}(y) \n& = & {\\operatorname{arg\\,min}}_{x}\\|w\\mathcal{F}x\\|_{2}^{2}+\\|x-y\\|_{2}^{2} \\\\ \n& = & g({\\mathcal{L}})y\n\\end{eqnarray*}\nwith \n\n", "index": 45, "text": "$$\ng({\\lambda_\\ell})=\\frac{1}{1+w^2({\\lambda_\\ell})}=\\frac{s^2({\\lambda_\\ell})}{s^2({\\lambda_\\ell})+n({\\lambda_\\ell})}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"g({\\lambda_{\\ell}})=\\frac{1}{1+w^{2}({\\lambda_{\\ell}})}=\\frac{s^{2}({\\lambda_{%&#10;\\ell}})}{s^{2}({\\lambda_{\\ell}})+n({\\lambda_{\\ell}})}.\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $x$ is a single realization of the stationary process ${\\bf x}$. Studying the bias of \\eqref{eq:PSD_estimator} reveals its interest~:\n\n\n", "itemtype": "equation", "pos": 35875, "prevtext": "\n\n\n\n\n\\paragraph{Solving the problem}\nNote that Problem~\\eqref{prob:Wiener-opt} can be solved with a simple gradient descent. However, for a large number of nodes $N$, the matrix $w({\\mathcal{L}})$ requires $O(N^3)$ operations to be computed and $O(N^2)$ bits to be stored. This difficulty can be overcome by applying its corresponding filter operator at each iteration. As already mentioned, the cost of the approximation is $O(|E|)$~\\cite{susnjara2015accelerated}.\n\nWhen $s({\\lambda_\\ell})\\approx 0 $ for some ${\\lambda_\\ell}$ the operator $w({\\mathcal{L}})$ becomes badly conditioned. To overcome this issue, Problem~\\eqref{prob:Wiener-opt} can be solved efficiently using a forward-backward splitting scheme~\\cite{combettes2005signal,combettes2011proximal,komodakis2014playing}. The proximal operator of $\\|w({\\mathcal{L}})x\\|_2^2 $ has been given above and we use the term $ \\|Ax-y\\|_2^2$ as the differentiable function. Algorithm~\\ref{CHalgorithm} uses an accelerated forward backward scheme~\\cite{beck2009fast} to solve Problem~\\eqref{prob:Wiener-opt} where $\\gamma$ is the step size (we use $\\gamma = \\frac{1}{2{\\lambda_{\\rm max}}(A)^2}$), $\\epsilon$ the stopping tolerance, $J$ the maximum number of iterations and $\\delta$ is a very small number to avoid a possible division by $0$.\n\\begin{algorithm}[ht!]\n\\caption{Fast Wiener optimization to solve~\\eqref{prob:Wiener-opt}}\n\\label{CHalgorithm}\n\\begin{algorithmic}\n\\State INPUT: $z_1 = x$, $u_0 = x$, $t_1 = 1$, $\\epsilon > 0$\n\\For{ $j = 1,\\dots J$ }\n\\State  $v = A^*(Az_{j}-y) g(z_{j})$ \n\\State  $u_{j+1} = \\gamma g(L) v$ with $g(\\lambda) = \\frac{s^2(\\lambda)}{s^2(\\lambda)+\\gamma n(\\lambda)}$\n\\State  $t_{j+1} = \\frac{1+\\sqrt{1+4t_j^2}}{2}$\n\\State  $z_{j+1} = z_j +\\frac{t_j-1}{t_{j+1}} (u_j-u_{j-1})$ \n\\If{$\\frac{\\|z_{j+1} - z_{j}\\|_F^2}{\\| z_{j}\\|_F^2+\\delta}<\\epsilon$}\n\\State BREAK\n\\EndIf\n\\EndFor\n\\State SOLUTION: $z_J$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Estimation of the signal PSD} \\label{sec:psd_estimation}\nAs the PSD is central in our method, we need a reliable and scalable way to compute it. Equation~\\eqref{eq:cov_mat_fourier} suggests a direct estimation method using the Fourier transform of the covariance matrix. Unfortunately, when the number of nodes is considerable, this method requires the diagonalization of the Laplacian, an operation whose complexity in the general case scales as $O(N^3)$ for the number of operations and $O(N^2)$ for memory requirements. Additionally, when the number of available realizations of the process is small, it is not possible to obtain a good estimate of the covariance matrix. To overcome these issues, inspired by Bartlett~\\cite{bartlett1950periodogram} and Welch~\\cite{welch1967use}, we propose to use a graph generalization of the Short Time Fourier transform~\\cite{shuman2013vertex} to construct a scalable estimation method.\n\nBartlett's method can be summarized as follows. The signal is first cut into equally sized segments without overlap. Then, the Fourier transform of each segment is computed. Finally, the PSD  is obtained by averaging over segments the squared amplitude of the Fourier coefficients. Welch's method is a generalization that works with segment overlap.\n\nA direct generalization of these method is complicated because it requires to cut the signal in the vertex domain. Additionally, it would not be scalable, since it still requires to compute the Fourier transform of the signal.\nIn this contribution, we use another angle. Both Bartlett and Welch methods can be generalized with the idea that a PSD estimation is obtained by averaging the squared short time Fourier coefficients over the time.\n\nWe propose a method based on this last principle. Instead of a translated (rectangular) window, we use a kernel $g$ localized over all the nodes of the graph. This kernel is then uniformly shifted in the spectral domain ($g_m({\\lambda_\\ell}) = g({\\lambda_\\ell}-m\\tau)$) to obtain a generalization of the short time Fourier transform. This kind of filterbank has been largely used in classical signal processing and recently introduced for graphs \\cite{shuman2013vertex}. Our algorithm simply consists in averaging the squared coefficients of this transform over the vertex set.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause graphs have an irregular spectrum, we additionally need a normalization factor which is given by the norm of the window $g_m$: $\\|g_m\\|_2^2 = \\sum_\\ell g({\\lambda_\\ell}-m\\tau)^2$. Note that this norm will vary for the different $m$. Our final estimator reads~:\n\n", "index": 47, "text": "\\begin{equation}\n\t\\label{eq:PSD_estimator}\n\t\\widehat{\\gamma_{x}}(m\\tau) = \\frac{  \\| g_m({\\mathcal{L}}) x \\|_2^2 }{ \\|g_m\\|_2^2 },\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\widehat{\\gamma_{x}}(m\\tau)=\\frac{\\|g_{m}({\\mathcal{L}})x\\|_{2}^{2}}{\\|g_{m}\\|%&#10;_{2}^{2}},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><msub><mi>\u03b3</mi><mi>x</mi></msub><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>g</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><msubsup><mrow><mo>\u2225</mo><msub><mi>g</mi><mi>m</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere ${\\bf x}$ is the graph stationary process.\n\nFor a filter $g$ well concentrated at the origin, \\eqref{eq:psd_estimation} gives a smoothed estimate of $\\gamma_{{\\bf x}}(m \\tau)$. This smoothing corresponds to the windowing operation in the vertex domain: the larger the kernel $g$ (and the smoothing), the smaller the window in the vertex domain. It is very interesting to note we recover the traditional trade-off between bias and variance in non-parametric spectral estimation. Indeed, if $g$ is very sharply localized on the spectrum, ultimately a Dirac delta, the estimator \\eqref{eq:PSD_estimator} is unbiased. Let us now study the variance. Intuitively, if the signal is correlated only over small regions of the vertex set, we could isolate them with localized windows of a small size and averaging those uncorrelated estimates together would reduce the variance. These small size windows on the vertex set correspond to wide filters $g_m$ and therefore large bias. However, if those correlated regions are large, and this happens when the PSD is localized in low-frequencies, we cannot hope to benefit from vertex-domain averaging since the graph is finite. Indeed the corresponding windows $g_m$ on the vertex set are so large that a single window spans the whole graph and there is no averaging effect: the variance increases precisely when we try to suppress the bias.\n\n\n\nOur complete estimation procedure is as follows. First, we design a filterbank by choosing a mother function $g$ (for example a Gaussian $g(x) = e^{-x^2/\\sigma^2}$). A frame is then created by shifting uniformly $M$ times $g$ in the spectral domain: $g_m(x) = g(x-m\\tau) = e^{-(x-m\\tau)^2/\\sigma^2}$.\nSecond, we compute the estimator $\\widehat{\\gamma_{x}}(m\\tau)$ from the stationary signal $x$. Note that if we have access to $K$ realizations of the stationary process, we can of course average them to further reduce the variance using ${\\mathbb{E}} \\left(\\| g_m({\\mathcal{L}}) {\\bf x } \\|_2^2 \\right)\\approx 1/K \\sum_k \\| g_m({\\mathcal{L}}) x_k  \\|_2^2$. Third we use the following trick to quickly approximate $\\| g_m \\|_2^2$. Using $K_2$ randomly-generated Gaussian white signals, we estimate \n\n", "itemtype": "equation", "pos": 36162, "prevtext": "\nwhere $x$ is a single realization of the stationary process ${\\bf x}$. Studying the bias of \\eqref{eq:PSD_estimator} reveals its interest~:\n\n\n", "index": 49, "text": "\\begin{equation} \\label{eq:psd_estimation}\n\\frac{ {\\mathbb{E}} \\left(\\| g_m({\\mathcal{L}}) {\\bf x } \\|_2^2 \\right)}{ \\|g_m\\|_2^2 } = \\frac{\\sum_{\\ell=0}^{N-1} \\left(g({\\lambda_\\ell}-m\\tau) \\right)^2 \\gamma_{{\\bf x}}({\\lambda_\\ell})}{\\sum_{\\ell=0}^{N-1} \\left( g({\\lambda_\\ell}-m\\tau)\\right)^2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\frac{{\\mathbb{E}}\\left(\\|g_{m}({\\mathcal{L}}){\\bf x}\\|_{2}^{2}\\right)}{\\|g_{m%&#10;}\\|_{2}^{2}}=\\frac{\\sum_{\\ell=0}^{N-1}\\left(g({\\lambda_{\\ell}}-m\\tau)\\right)^{%&#10;2}\\gamma_{{\\bf x}}({\\lambda_{\\ell}})}{\\sum_{\\ell=0}^{N-1}\\left(g({\\lambda_{%&#10;\\ell}}-m\\tau)\\right)^{2}},\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>g</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>)</mo></mrow></mrow><msubsup><mrow><mo>\u2225</mo><msub><mi>g</mi><mi>m</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><msup><mrow><mo>(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\u03b3</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></msubsup><msup><mrow><mo>(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nFinally, the last step consists in computing the ratio between the two quantities and interpolating the discrete points $(k\\tau, \\big(g \\ast \\gamma \\big)(m\\tau))$. \n\n\\paragraph{Experimental assessment of the method} \nFigure~\\ref{fig:psd_estimation} shows the results of our PSD-estimation algorithm on a $10$-nearest neighbors graph of $20'000$ nodes (sensor type) and only $1$ signal. We compare the estimation using frames of $10$, $20$, $30$, $100$ Gaussian filters. The parameters $\\sigma$ and $\\tau$ are adapted to the number of filters. For this experiment $K_2$ is set to $10$ and the Chebysheff polynomial order is $30$ (Except for $M=100$ where we took $100$). The estimated curves are smoothed versions of the PSD. Note that with $100$ filters, the windows are very concentrated in the spectral domain and broad in the vertex domain. Thus, we loose the averaging effect of the algorithm resulting in a PSD looking like the Fourier transform of the original signal.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_estimation.png}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_time.png} \n\\end{center}\n\\caption{Left: PSD estimation on a graph of $20'000$ nodes with $1$ measurements. Our algorithm is able to successively estimate the PSD of a signal. Right: Computation time versus size of the graph. We use $M=30$ filters. The algorithm scales linearly with the number of edges.}\n\\label{fig:psd_estimation}\n\\end{figure}\n\n\\paragraph{Complexity analysis} \n\nThe approximation scales with the number of edges of the graph: $\\mathcal{O}(|{\\mathcal{E}}|)$, (which is proportional to $N$ in many graphs). Precisely, our PSD estimation method necessitates $ (K + K_2) M $ filtering operations (with $M$ the number of shifts of $g$). A filtering operation costs approximatively $O_c |E|$, with $O_c$ the order of the Chebysheff polynomial. The final computational cost of the method is thus $\\mathcal{O}\\left( O_c (K + K_2) M |{\\mathcal{E}}| \\right)$.\n\n\n\\paragraph{Error analysis} The difference between the approximation and the exact PSD is caused by three different factors.\n\\begin{enumerate}\n\\item The inherent bias of the estimator, which is now directly controlled by the parameter $\\sigma$.\n\n\\item We estimate the expected value using $K$ signals (often $K=1$). For large graphs $N>>K$ and a few filters $M<<N$, this error is usually low because the variance of $\\| g_m({\\mathcal{L}}) {{\\bf x}} \\|_2^2$ is inversely proportional to bias. The estimation error improves as $\\frac{1}{K}$.\n\\item We use a fast-filtering method based on a polynomial approximation of the filter. For a rough approximation, $\\sigma>>\\frac{{\\lambda_{\\rm max}}}{N}$, this error is usually negligible. However, in the other cases, this error may become large. \n\\end{enumerate}\n\n\n\n\n\n\\section{Illustrative experiment on USPS} \\label{sec:USPS}\nStationarity may not be an obvious hypothesis for a general dataset, since our intuition does not allow us to easily capture the kind of shift invariance that is really implied. In this section we give additional insights on stationarity from a more experimental point of view. To do so, we use the well-known USPS dataset.\n\nImages can be considered as signals on the 2-dimensional euclidean plane and, naturally, when the signal is sampled, a grid graph is used as a discretization of this manifold. The corresponding eigenbasis is the 2 dimensional DCT\\footnote{This is a natural extension of \\cite{strang1999discrete}}. Following Julesz's initial assumption~\\cite{Julesz:1962gl} many papers have exploited the fact that natural texture images are stationary 2-dimensional processes, i.e stationary signals on the grid graph~\\cite{Dubes:1989ty}. In \\cite{roux2008learning}, the authors go one step further and ask the following question: suppose that pixels of images have been permuted, can we recover their relative two-dimensional location? Amazingly, they answer positively adding that only a few thousand images are enough to approximatively recover the relative location of the pixels. The grid graph seems naturally encoded within images.\n\nThe observation of \\cite{roux2008learning} motivate the following experiment involving stationarity on graphs. Let us select the USPS data set which contains $9298$ digit images of $16 \\times 16$ pixels. We create 5 classes of data: (a) the circularly shifted digits\\footnote{We performed all possible shifts. Because of this, the covariance matrix becomes Toeplitz}, (b) the original digits and (c), (d) and (e) the classes of digit $3$, $7$ and $9$. For those 5 cases, we compute the covariance matrix $\\Sigma$ and its graph PSD,\n\n", "itemtype": "equation", "pos": 38658, "prevtext": "\nwhere ${\\bf x}$ is the graph stationary process.\n\nFor a filter $g$ well concentrated at the origin, \\eqref{eq:psd_estimation} gives a smoothed estimate of $\\gamma_{{\\bf x}}(m \\tau)$. This smoothing corresponds to the windowing operation in the vertex domain: the larger the kernel $g$ (and the smoothing), the smaller the window in the vertex domain. It is very interesting to note we recover the traditional trade-off between bias and variance in non-parametric spectral estimation. Indeed, if $g$ is very sharply localized on the spectrum, ultimately a Dirac delta, the estimator \\eqref{eq:PSD_estimator} is unbiased. Let us now study the variance. Intuitively, if the signal is correlated only over small regions of the vertex set, we could isolate them with localized windows of a small size and averaging those uncorrelated estimates together would reduce the variance. These small size windows on the vertex set correspond to wide filters $g_m$ and therefore large bias. However, if those correlated regions are large, and this happens when the PSD is localized in low-frequencies, we cannot hope to benefit from vertex-domain averaging since the graph is finite. Indeed the corresponding windows $g_m$ on the vertex set are so large that a single window spans the whole graph and there is no averaging effect: the variance increases precisely when we try to suppress the bias.\n\n\n\nOur complete estimation procedure is as follows. First, we design a filterbank by choosing a mother function $g$ (for example a Gaussian $g(x) = e^{-x^2/\\sigma^2}$). A frame is then created by shifting uniformly $M$ times $g$ in the spectral domain: $g_m(x) = g(x-m\\tau) = e^{-(x-m\\tau)^2/\\sigma^2}$.\nSecond, we compute the estimator $\\widehat{\\gamma_{x}}(m\\tau)$ from the stationary signal $x$. Note that if we have access to $K$ realizations of the stationary process, we can of course average them to further reduce the variance using ${\\mathbb{E}} \\left(\\| g_m({\\mathcal{L}}) {\\bf x } \\|_2^2 \\right)\\approx 1/K \\sum_k \\| g_m({\\mathcal{L}}) x_k  \\|_2^2$. Third we use the following trick to quickly approximate $\\| g_m \\|_2^2$. Using $K_2$ randomly-generated Gaussian white signals, we estimate \n\n", "index": 51, "text": "$$ {\\mathbb{E}} \\left( \\| g_m({\\mathcal{L}}) {\\bf w } \\|_2^2 \\right) = \\|g_m\\|_2^2.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}\\left(\\|g_{m}({\\mathcal{L}}){\\bf w}\\|_{2}^{2}\\right)=\\|g_{m}\\|_{2}%&#10;^{2}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>g</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>)</mo></mrow></mrow><mo>=</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>g</mi><mi>m</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nfor 2 different graphs: (a) the grid and (b) the $20$ nearest neighbors graph. In this latter case, each node is a pixel and is associated to a feature vector containing the corresponding pixel value of all images. We use the squared euclidean distance between feature vectors to define edge weights. We then compute the stationarity level of each class of data with both graph using the following measure:\n\n", "itemtype": "equation", "pos": 43391, "prevtext": "\nFinally, the last step consists in computing the ratio between the two quantities and interpolating the discrete points $(k\\tau, \\big(g \\ast \\gamma \\big)(m\\tau))$. \n\n\\paragraph{Experimental assessment of the method} \nFigure~\\ref{fig:psd_estimation} shows the results of our PSD-estimation algorithm on a $10$-nearest neighbors graph of $20'000$ nodes (sensor type) and only $1$ signal. We compare the estimation using frames of $10$, $20$, $30$, $100$ Gaussian filters. The parameters $\\sigma$ and $\\tau$ are adapted to the number of filters. For this experiment $K_2$ is set to $10$ and the Chebysheff polynomial order is $30$ (Except for $M=100$ where we took $100$). The estimated curves are smoothed versions of the PSD. Note that with $100$ filters, the windows are very concentrated in the spectral domain and broad in the vertex domain. Thus, we loose the averaging effect of the algorithm resulting in a PSD looking like the Fourier transform of the original signal.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_estimation.png}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_time.png} \n\\end{center}\n\\caption{Left: PSD estimation on a graph of $20'000$ nodes with $1$ measurements. Our algorithm is able to successively estimate the PSD of a signal. Right: Computation time versus size of the graph. We use $M=30$ filters. The algorithm scales linearly with the number of edges.}\n\\label{fig:psd_estimation}\n\\end{figure}\n\n\\paragraph{Complexity analysis} \n\nThe approximation scales with the number of edges of the graph: $\\mathcal{O}(|{\\mathcal{E}}|)$, (which is proportional to $N$ in many graphs). Precisely, our PSD estimation method necessitates $ (K + K_2) M $ filtering operations (with $M$ the number of shifts of $g$). A filtering operation costs approximatively $O_c |E|$, with $O_c$ the order of the Chebysheff polynomial. The final computational cost of the method is thus $\\mathcal{O}\\left( O_c (K + K_2) M |{\\mathcal{E}}| \\right)$.\n\n\n\\paragraph{Error analysis} The difference between the approximation and the exact PSD is caused by three different factors.\n\\begin{enumerate}\n\\item The inherent bias of the estimator, which is now directly controlled by the parameter $\\sigma$.\n\n\\item We estimate the expected value using $K$ signals (often $K=1$). For large graphs $N>>K$ and a few filters $M<<N$, this error is usually low because the variance of $\\| g_m({\\mathcal{L}}) {{\\bf x}} \\|_2^2$ is inversely proportional to bias. The estimation error improves as $\\frac{1}{K}$.\n\\item We use a fast-filtering method based on a polynomial approximation of the filter. For a rough approximation, $\\sigma>>\\frac{{\\lambda_{\\rm max}}}{N}$, this error is usually negligible. However, in the other cases, this error may become large. \n\\end{enumerate}\n\n\n\n\n\n\\section{Illustrative experiment on USPS} \\label{sec:USPS}\nStationarity may not be an obvious hypothesis for a general dataset, since our intuition does not allow us to easily capture the kind of shift invariance that is really implied. In this section we give additional insights on stationarity from a more experimental point of view. To do so, we use the well-known USPS dataset.\n\nImages can be considered as signals on the 2-dimensional euclidean plane and, naturally, when the signal is sampled, a grid graph is used as a discretization of this manifold. The corresponding eigenbasis is the 2 dimensional DCT\\footnote{This is a natural extension of \\cite{strang1999discrete}}. Following Julesz's initial assumption~\\cite{Julesz:1962gl} many papers have exploited the fact that natural texture images are stationary 2-dimensional processes, i.e stationary signals on the grid graph~\\cite{Dubes:1989ty}. In \\cite{roux2008learning}, the authors go one step further and ask the following question: suppose that pixels of images have been permuted, can we recover their relative two-dimensional location? Amazingly, they answer positively adding that only a few thousand images are enough to approximatively recover the relative location of the pixels. The grid graph seems naturally encoded within images.\n\nThe observation of \\cite{roux2008learning} motivate the following experiment involving stationarity on graphs. Let us select the USPS data set which contains $9298$ digit images of $16 \\times 16$ pixels. We create 5 classes of data: (a) the circularly shifted digits\\footnote{We performed all possible shifts. Because of this, the covariance matrix becomes Toeplitz}, (b) the original digits and (c), (d) and (e) the classes of digit $3$, $7$ and $9$. For those 5 cases, we compute the covariance matrix $\\Sigma$ and its graph PSD,\n\n", "index": 53, "text": "\\begin{equation} \\label{eq:Fourier_cov_matrix}\n\\Gamma = U^* \\Sigma U,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\Gamma=U^{*}\\Sigma U,\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>=</mo><mrow><msup><mi>U</mi><mo>*</mo></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><mi>U</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThe closer $s_r(\\Gamma)$ is to $1$, the more diagonal the matrix $\\Gamma$ is and the more stationary the process. Table \\ref{tab:stationarity measures} shows the obtained stationarity measures. The less universal the data, the less stationary it is on the grid. Clearly, specificity inside the data requires a finer structure than a grid. This is confirmed by the behavior of the nearest neighbors graph. When only one digit class is selected the nearest neighbors graph still yields very stationary signals. \n\\begin{table}\n\\begin{tabular}{|l | c | c |}\n \\hline\n Data \\textbackslash Graph  & 2-dimensional grid & $20$ nearest neighbors graph \\\\\n \\hline\n \\hline\n Shifted all digits  & $0.86$ & $1$ \n \n \\\\\n \\hline\n All digits  & $0.73$ & $0.84$ \\\\\n \\hline\n Digit 3  & $0.53$ & $0.97$ \\\\\n \\hline\n Digit 7  & $0.44$ & $0.97$\\\\\n \\hline\n Digit 9  & $0.49$ & $0.97$ \\\\\n \\hline\n\\end{tabular}\n\\caption{$s_r(\\Gamma) =\\frac{\\| \\rm{diag}(\\Gamma) \\|_2}{\\| \\Gamma \\|_F}$ measures for different graphs and different datasets. The nearest neighbors graph adapts to the data. The individual digits are stationary with the nearest neighbor graph.}\n\\label{tab:stationarity measures}\n\\end{table}\n\nLet us focus on the digit $3$. For this experiment, we build a $20$ nearest neighbors graph with only $50$ samples. Figure~\\ref{fig:exp3} shows the eigenvectors of the Laplacian and of the covariance matrix. Because of stationarity, they are very similar. Moreover, they have $3$-like shape. Using the graph and the PSD, it is also possible generate samples by filtering Gaussian random noise with the following PSD based kernel: $g(\\lambda_\\ell) = \\sqrt{\\Gamma_{\\ell,\\ell}}$. The resulting digits have $3$-like shape confirming the that the class is stationary on the nearest neighbors graph.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/exp1_psd.png} \n\\includegraphics[width=0.45\\linewidth]{figures/exp1_gen_sample.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/exp1_cov_eig.png} \n\\includegraphics[width=0.45\\linewidth]{figures/exp1_lap_eig.png} \\\\\n\\end{center}\n\\caption{Studying the number $3$ of USPS. Top left: PSD of the data (Note the diagonal shape of the matrix). Top right: generated samples by filtering Gaussian random noise on the graph. Bottom left: Covariance eigenvectors associated with the $16$ highest eigenvalues. Bottom right: Laplacian eigenvectors associated to the $16$ smallest non-zero eigenvalues. Because of stationarity, Laplacian eigenvectors are similar to the covariance eigenvectors.}\n\\label{fig:exp3}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo intuitively motivate the effectiveness of nearest neighbors at producing stationary signals, let us define the centering operator $J = I - \\boldsymbol{1}\\boldsymbol{1}^{\\top}/N$. A straightforward calculation shows that the matrix of average squared distances between the centered features is directly proportional to the covariance matrix~:\n\n", "itemtype": "equation", "pos": 43883, "prevtext": "\nfor 2 different graphs: (a) the grid and (b) the $20$ nearest neighbors graph. In this latter case, each node is a pixel and is associated to a feature vector containing the corresponding pixel value of all images. We use the squared euclidean distance between feature vectors to define edge weights. We then compute the stationarity level of each class of data with both graph using the following measure:\n\n", "index": 55, "text": "\\begin{equation}\ns_r(\\Gamma) = \\left(\\frac{\\sum_\\ell \\Gamma_{\\ell,\\ell}^2}{\\sum_{\\ell_1}\\sum_{\\ell_2} \\Gamma_{\\ell_1,\\ell_2}^2} \\right)^{\\frac{1}{2}} =\\frac{\\| \\rm{diag}(\\Gamma) \\|_2}{\\| \\Gamma \\|_F}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"s_{r}(\\Gamma)=\\left(\\frac{\\sum_{\\ell}\\Gamma_{\\ell,\\ell}^{2}}{\\sum_{\\ell_{1}}%&#10;\\sum_{\\ell_{2}}\\Gamma_{\\ell_{1},\\ell_{2}}^{2}}\\right)^{\\frac{1}{2}}=\\frac{\\|%&#10;\\rm{diag}(\\Gamma)\\|_{2}}{\\|\\Gamma\\|_{F}}\" display=\"block\"><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0393</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo>(</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi mathvariant=\"normal\">\u2113</mi></msub><msubsup><mi mathvariant=\"normal\">\u0393</mi><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>,</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mn>2</mn></msubsup></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub></msub><msubsup><mi mathvariant=\"normal\">\u0393</mi><mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub></mrow><mn>2</mn></msubsup></mrow></mrow></mfrac><mo>)</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>=</mo><mfrac><msub><mrow><mo>\u2225</mo><mrow><mi>diag</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0393</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><msub><mrow><mo>\u2225</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2225</mo></mrow><mi>F</mi></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $D_{ij} = \\sum_k \\left( x_k(i) -x_k(j) \\right)^2$ and ${\\mathbb{E}} D = D_{{\\bf x}}$.\nThe nearest-neighbors graph can be seen as an approximation of the original distance matrix, which pleads for using it as a good proxy destined at leveraging the spectral content of the covariance. Put differently, when using realizations of the process as features and computing the k-NN graph we are connecting strongly correlated variables via strong edge weights.\n\n\n\n\n\n\n\n\\section{Experiments} \\label{sec:experiments}\nThe experiments were performed with the GSPBox~\\cite{perraudin2014gspbox} and the UNLocBoX~\\cite{perraudin2014unlocbox} two open-source softwares. The code to reproduce all figures of the paper can be downloaded at: \\url{https://lts2.epfl.ch/rrp/stationarity/}. As the stationary signals are random, the reader may obtain slightly different results. However, conclusions shall remain identical. The models used in our comparisons are detailed in the Appendix~\\ref{sec:convex_model} for completeness.\n\n\\subsection{Synthetic dataset}\nIn order to obtain a first insight into applications using stationarity, we begin with some classical problems solved on a synthetic dataset. Compared to real data, this framework allows us to be sure that the signal is stationary on the graph. \n\\paragraph{Graph Wiener deconvolution}\nWe start with a de-convolution example on a random geometric graph. This can model an array of sensors distributed in space or simply a mesh. The signal is chosen with a low frequency band-limited PSD. To produce the measurements, the signal is convolved with the heat kernel $e^{-\\tau x}$. Additionally, we add some Gaussian noise. The heat kernel is chosen because it simulates a diffusion process. Using de-convolution we aim at recovering the original signal before  diffusion. For this experiment, we put ourselves in an ideal case and suppose that  both the PSD  of the input signal and the noise level are known.\n\nFigure~\\ref{fig:synthetic-deconvolution} presents the results. We observe that Wiener filtering is able to de-convolve the measurements. The second plot shows the reconstruction errors for three different methods: Tikonov presented in problem \\eqref{prob:deconvolution-tik}, TV in \\eqref{prob:dec-in-den-tv} and Wiener filtering in \\eqref{eq:Wiener_filter}. Wiener filtering performs clearly much better than the other methods because it has a much better prior assumption.\n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/synthetic_deconvolution.png} \n\\includegraphics[width=0.45\\linewidth]{figures/synthetic_deconvolution_errors.png} \n\\end{center}\n\\caption{Graph de-convolution on a geometric random graph. The convolution kernel is $e^{-\\frac{10 x}{{\\lambda_{\\rm max}}}}$. Top: Signal and filters for a noise level of $0.16$. Bottom: evolution of the error with respect of the noise.}\n\\label{fig:synthetic-deconvolution}\n\\end{figure}\n\n\\paragraph{Graph Wiener in-painting}\nIn our second example, we use Wiener optimization to solve an in-painting problem. This time, we suppose that the PSD of the input signal is unknown and we estimate it using $50$ signals. Figure~\\ref{fig:synthetic-in-painting} presents quantitative results for the in-painting. Again, we compare three different optimization methods: Tikonov \\eqref{prob:inpainting-tik}, TV \\eqref{prob:dec-in-den-tv} and Wiener \\eqref{prob:Wiener-opt}. Wiener optimization performs much better than traditional methods because the generated data fits the stationarity model. Moreover, we also observe that the PSD approximation does not affect the result.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_approx.png} \n\\includegraphics[width=0.45\\linewidth]{figures/approx_wiener_filter.png} \n\\includegraphics[width=0.45\\linewidth]{figures/synthetic_inpainting_errors.png} \n\n\\end{center}\n\\caption{Wiener in-painting on a geometric graph of 400 nodes. Top: true VS approximated PSD and resulting Wiener filters. Bottom: in-painting relative error with respect to number of measurements.}\n\\label{fig:synthetic-in-painting}\n\\end{figure}\n\n\n\\subsection{Real dataset}\nWe apply our methods to a weather measurements dataset, more precisely to the temperature and the humidity. Since these two quantities change smoothly across  space, it suggests that they are more or less stationary on a nearest neighbour graph. \n\nThe French national meteorological service has published in open access a dataset\\footnote{Access to the raw data is possible directly through our code.} with hourly weather observations collected during the Month of January 2014 in the region of Brest (France). From these data, we wish to ascertain that our method still performs better than the two other models (TV and Tikonov) on real measurements. The graph is built from the coordinates of the weather stations by connecting all the neighbours in a given radius with a weight function $w(i,j) = e^{-d^2 \\tau}$ where $\\tau$ is adjusted to obtain a average degree around $3$ ($\\tau$, however, is not a sensitive parameter). For our experiments, we consider every time step as an independent realization of a WSS process. As sole pre-processing, we remove the mean of the temperature. Thanks to the $744$ time observation, we can estimate the covariance matrix and check wether the process is stationary on the graph. \n\n\\paragraph{In-painting - Temperature} The result of the experiment with temperatures is displayed in Figure~\\ref{fig:molene-temperature}. The covariance matrix shows a strong correlation between the different weather stations. Diagonalizing it with the Fourier basis of the graph assesses that the meteorological instances are more or less stationary within the distance graph by highlighting its diagonal characteristic. Moreover this diagonal gives us access to the PSD of the data. \n\nIn our experiment, we solve an in-painting/de-noising problem with a mask operator covering 50 per cent of measurements and various amount of noise. We then average the result over $744$ experiments (corresponding to the $744$ observations) to obtain the curves displayed in Figure~\\ref{fig:molene-temperature}. \nWe observe that Wiener optimization performs significantly better when the noise level is high and equivalently well to the two other methods for low noise level. \n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/temperature_cov.png} \\\\\n\\includegraphics[width=0.9\\linewidth]{figures/stations_graph_rm.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/temperature_time.png} \n\\includegraphics[width=0.45\\linewidth]{figures/temperature_inpainting_errors.png} \n\\end{center}\n\\caption{Top: Covariance matrices. Bottom left: An example of the process on the graph (first measure). Bottom center: the temperature of the Island of Brehat. Bottom right: Recovery errors for different noise levels.}\n\\label{fig:molene-temperature}\n\\end{figure}\n\n\\paragraph{In-painting - Humidity} Using the same graph, we have performed another set of experiments on humidity observations. The testing framework is identical as for the temperature and the conclusions are similar.\n \n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/humidity_cov.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/humidity_inpainting_errors.png} \n\\end{center}\n\\caption{Top: Covariance matrices. Bottom: Recovery errors for different noise levels.}\n\\label{fig:molene-humidity}\n\\end{figure}\n\n\\subsection{USPS dataset}\nWe perform the same kind of in-painting/de-noising experiment with the USPS dataset. We compute the graph using the first $300$ digits and use $4349$ of the remaining ones to test our algorithm. We use a mask covering $50$ per cent of the pixel and various amount of noise. We then average the result over $4349$ experiments (corresponding to the $4349$ digits) to obtain the curves displayed in Figure~\\ref{fig:usps_inpainting}.\nFor this experiment, we also compare to traditional TV de-noising~\\cite{chambolle2004algorithm} and Tikonow de-noising. The optimization problems used are similar to \\ref{prob:inpainting-tik}.\nThe results presented in Figure~\\ref{fig:usps_inpainting} show that graph optimization is outperforming classical techniques meaning that the grid is not the optimal graph for the USPS dataset. Moreover, Wiener once again outperforms the other graph-based models.\n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/usps_cov.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/usps_inpainting_errors.png} \n\\end{center}\n\\caption{Top left: Weights matrix of the $20$ nearest neighbor graph (The diagonal shape indicate the grid base topology of the graph). Top right: PSD matrix for the first $50$ graph frequencies. Bottom: Recovery errors for different noise levels. Methods using the nearest neighbors graph performs better.}\n\\label{fig:usps_inpainting}\n\\end{figure}\n\n\n\\section{Conclusion} \\label{sec:conclusion}\nIn this contribution, we have extended the common concept of stationarity to graph signals. Using this statistical model, we proposed a new regularization framework that leverages the stationarity hypothesis by using the Power Spectral Density (PSD) of the signal. Since the PSD can be efficiently estimated, even for large graphs, the proposed Wiener regularization framework offers a compelling way to solve traditional problems such as denoising, regression or semi-supervised learning. We believe that stationarity is a natural hypothesis for many signals on graphs and showed experimentally that it is deeply connected with the popular nearest neighbor graph construction. As future work, it would be very interesting to clarify this connection and explore if stationarity could be used to infer the graph structure from training signals, in the spirit of~\\cite{Dong:2014tj}.\n\n\n\n\\section*{Acknowledgment}\nThis work has been supported by the Swiss National Science Foundation research project \\textit{Towards Signal Processing on Graphs}, grant number: 2000\\_21/154350/1.\n\n\n\n\n\\appendix\n\n\n\\section{Convex models} \\label{sec:convex_model}\nConvex optimization has recently become a standard tool for problems such as de-noising, de-convolution or in-painting. Graph priors have been used in this field for more than a decade~\\cite{smola2003kernels,zhou2004regularization,peyre2008non}. The general assumption is that the signal varies smoothly along the edges, which is equivalent to saying that the signal is low-frequency-based. Using this assumption, one way to express mathematically an in-painting problem is the following:\n\n", "itemtype": "equation", "pos": 47030, "prevtext": "\nThe closer $s_r(\\Gamma)$ is to $1$, the more diagonal the matrix $\\Gamma$ is and the more stationary the process. Table \\ref{tab:stationarity measures} shows the obtained stationarity measures. The less universal the data, the less stationary it is on the grid. Clearly, specificity inside the data requires a finer structure than a grid. This is confirmed by the behavior of the nearest neighbors graph. When only one digit class is selected the nearest neighbors graph still yields very stationary signals. \n\\begin{table}\n\\begin{tabular}{|l | c | c |}\n \\hline\n Data \\textbackslash Graph  & 2-dimensional grid & $20$ nearest neighbors graph \\\\\n \\hline\n \\hline\n Shifted all digits  & $0.86$ & $1$ \n \n \\\\\n \\hline\n All digits  & $0.73$ & $0.84$ \\\\\n \\hline\n Digit 3  & $0.53$ & $0.97$ \\\\\n \\hline\n Digit 7  & $0.44$ & $0.97$\\\\\n \\hline\n Digit 9  & $0.49$ & $0.97$ \\\\\n \\hline\n\\end{tabular}\n\\caption{$s_r(\\Gamma) =\\frac{\\| \\rm{diag}(\\Gamma) \\|_2}{\\| \\Gamma \\|_F}$ measures for different graphs and different datasets. The nearest neighbors graph adapts to the data. The individual digits are stationary with the nearest neighbor graph.}\n\\label{tab:stationarity measures}\n\\end{table}\n\nLet us focus on the digit $3$. For this experiment, we build a $20$ nearest neighbors graph with only $50$ samples. Figure~\\ref{fig:exp3} shows the eigenvectors of the Laplacian and of the covariance matrix. Because of stationarity, they are very similar. Moreover, they have $3$-like shape. Using the graph and the PSD, it is also possible generate samples by filtering Gaussian random noise with the following PSD based kernel: $g(\\lambda_\\ell) = \\sqrt{\\Gamma_{\\ell,\\ell}}$. The resulting digits have $3$-like shape confirming the that the class is stationary on the nearest neighbors graph.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/exp1_psd.png} \n\\includegraphics[width=0.45\\linewidth]{figures/exp1_gen_sample.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/exp1_cov_eig.png} \n\\includegraphics[width=0.45\\linewidth]{figures/exp1_lap_eig.png} \\\\\n\\end{center}\n\\caption{Studying the number $3$ of USPS. Top left: PSD of the data (Note the diagonal shape of the matrix). Top right: generated samples by filtering Gaussian random noise on the graph. Bottom left: Covariance eigenvectors associated with the $16$ highest eigenvalues. Bottom right: Laplacian eigenvectors associated to the $16$ smallest non-zero eigenvalues. Because of stationarity, Laplacian eigenvectors are similar to the covariance eigenvectors.}\n\\label{fig:exp3}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo intuitively motivate the effectiveness of nearest neighbors at producing stationary signals, let us define the centering operator $J = I - \\boldsymbol{1}\\boldsymbol{1}^{\\top}/N$. A straightforward calculation shows that the matrix of average squared distances between the centered features is directly proportional to the covariance matrix~:\n\n", "index": 57, "text": "$$\n\\frac{1}{d} \\Sigma_{{\\bf x}} = - \\frac{1}{2} J D_{{\\bf x}} J,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{d}\\Sigma_{{\\bf x}}=-\\frac{1}{2}JD_{{\\bf x}}J,\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mi>J</mi><mo>\u2062</mo><msub><mi>D</mi><mi>\ud835\udc31</mi></msub><mo>\u2062</mo><mi>J</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $M$ is a masking operator and $\\epsilon$ a constant computed thanks to the noise level. We could also rewrite the objective function as $x^T {\\mathcal{L}} x + \\gamma \\|M x - y\\|_2^2$, but this implies a greedy search of the regularization parameter $\\gamma$ even when the level of noise is known. In general $\\epsilon$ is easier to set. For example, when the noise is uniformly Gaussian, we have $\\epsilon = n \\sqrt{\\#y}$, where $\\#y$ is the number of elements of $y$. \n\nGraph de-convolution can also be addressed with the same prior assumption leading to \n\n", "itemtype": "equation", "pos": 57710, "prevtext": "\nwhere $D_{ij} = \\sum_k \\left( x_k(i) -x_k(j) \\right)^2$ and ${\\mathbb{E}} D = D_{{\\bf x}}$.\nThe nearest-neighbors graph can be seen as an approximation of the original distance matrix, which pleads for using it as a good proxy destined at leveraging the spectral content of the covariance. Put differently, when using realizations of the process as features and computing the k-NN graph we are connecting strongly correlated variables via strong edge weights.\n\n\n\n\n\n\n\n\\section{Experiments} \\label{sec:experiments}\nThe experiments were performed with the GSPBox~\\cite{perraudin2014gspbox} and the UNLocBoX~\\cite{perraudin2014unlocbox} two open-source softwares. The code to reproduce all figures of the paper can be downloaded at: \\url{https://lts2.epfl.ch/rrp/stationarity/}. As the stationary signals are random, the reader may obtain slightly different results. However, conclusions shall remain identical. The models used in our comparisons are detailed in the Appendix~\\ref{sec:convex_model} for completeness.\n\n\\subsection{Synthetic dataset}\nIn order to obtain a first insight into applications using stationarity, we begin with some classical problems solved on a synthetic dataset. Compared to real data, this framework allows us to be sure that the signal is stationary on the graph. \n\\paragraph{Graph Wiener deconvolution}\nWe start with a de-convolution example on a random geometric graph. This can model an array of sensors distributed in space or simply a mesh. The signal is chosen with a low frequency band-limited PSD. To produce the measurements, the signal is convolved with the heat kernel $e^{-\\tau x}$. Additionally, we add some Gaussian noise. The heat kernel is chosen because it simulates a diffusion process. Using de-convolution we aim at recovering the original signal before  diffusion. For this experiment, we put ourselves in an ideal case and suppose that  both the PSD  of the input signal and the noise level are known.\n\nFigure~\\ref{fig:synthetic-deconvolution} presents the results. We observe that Wiener filtering is able to de-convolve the measurements. The second plot shows the reconstruction errors for three different methods: Tikonov presented in problem \\eqref{prob:deconvolution-tik}, TV in \\eqref{prob:dec-in-den-tv} and Wiener filtering in \\eqref{eq:Wiener_filter}. Wiener filtering performs clearly much better than the other methods because it has a much better prior assumption.\n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/synthetic_deconvolution.png} \n\\includegraphics[width=0.45\\linewidth]{figures/synthetic_deconvolution_errors.png} \n\\end{center}\n\\caption{Graph de-convolution on a geometric random graph. The convolution kernel is $e^{-\\frac{10 x}{{\\lambda_{\\rm max}}}}$. Top: Signal and filters for a noise level of $0.16$. Bottom: evolution of the error with respect of the noise.}\n\\label{fig:synthetic-deconvolution}\n\\end{figure}\n\n\\paragraph{Graph Wiener in-painting}\nIn our second example, we use Wiener optimization to solve an in-painting problem. This time, we suppose that the PSD of the input signal is unknown and we estimate it using $50$ signals. Figure~\\ref{fig:synthetic-in-painting} presents quantitative results for the in-painting. Again, we compare three different optimization methods: Tikonov \\eqref{prob:inpainting-tik}, TV \\eqref{prob:dec-in-den-tv} and Wiener \\eqref{prob:Wiener-opt}. Wiener optimization performs much better than traditional methods because the generated data fits the stationarity model. Moreover, we also observe that the PSD approximation does not affect the result.\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.45\\linewidth]{figures/psd_approx.png} \n\\includegraphics[width=0.45\\linewidth]{figures/approx_wiener_filter.png} \n\\includegraphics[width=0.45\\linewidth]{figures/synthetic_inpainting_errors.png} \n\n\\end{center}\n\\caption{Wiener in-painting on a geometric graph of 400 nodes. Top: true VS approximated PSD and resulting Wiener filters. Bottom: in-painting relative error with respect to number of measurements.}\n\\label{fig:synthetic-in-painting}\n\\end{figure}\n\n\n\\subsection{Real dataset}\nWe apply our methods to a weather measurements dataset, more precisely to the temperature and the humidity. Since these two quantities change smoothly across  space, it suggests that they are more or less stationary on a nearest neighbour graph. \n\nThe French national meteorological service has published in open access a dataset\\footnote{Access to the raw data is possible directly through our code.} with hourly weather observations collected during the Month of January 2014 in the region of Brest (France). From these data, we wish to ascertain that our method still performs better than the two other models (TV and Tikonov) on real measurements. The graph is built from the coordinates of the weather stations by connecting all the neighbours in a given radius with a weight function $w(i,j) = e^{-d^2 \\tau}$ where $\\tau$ is adjusted to obtain a average degree around $3$ ($\\tau$, however, is not a sensitive parameter). For our experiments, we consider every time step as an independent realization of a WSS process. As sole pre-processing, we remove the mean of the temperature. Thanks to the $744$ time observation, we can estimate the covariance matrix and check wether the process is stationary on the graph. \n\n\\paragraph{In-painting - Temperature} The result of the experiment with temperatures is displayed in Figure~\\ref{fig:molene-temperature}. The covariance matrix shows a strong correlation between the different weather stations. Diagonalizing it with the Fourier basis of the graph assesses that the meteorological instances are more or less stationary within the distance graph by highlighting its diagonal characteristic. Moreover this diagonal gives us access to the PSD of the data. \n\nIn our experiment, we solve an in-painting/de-noising problem with a mask operator covering 50 per cent of measurements and various amount of noise. We then average the result over $744$ experiments (corresponding to the $744$ observations) to obtain the curves displayed in Figure~\\ref{fig:molene-temperature}. \nWe observe that Wiener optimization performs significantly better when the noise level is high and equivalently well to the two other methods for low noise level. \n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/temperature_cov.png} \\\\\n\\includegraphics[width=0.9\\linewidth]{figures/stations_graph_rm.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/temperature_time.png} \n\\includegraphics[width=0.45\\linewidth]{figures/temperature_inpainting_errors.png} \n\\end{center}\n\\caption{Top: Covariance matrices. Bottom left: An example of the process on the graph (first measure). Bottom center: the temperature of the Island of Brehat. Bottom right: Recovery errors for different noise levels.}\n\\label{fig:molene-temperature}\n\\end{figure}\n\n\\paragraph{In-painting - Humidity} Using the same graph, we have performed another set of experiments on humidity observations. The testing framework is identical as for the temperature and the conclusions are similar.\n \n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/humidity_cov.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/humidity_inpainting_errors.png} \n\\end{center}\n\\caption{Top: Covariance matrices. Bottom: Recovery errors for different noise levels.}\n\\label{fig:molene-humidity}\n\\end{figure}\n\n\\subsection{USPS dataset}\nWe perform the same kind of in-painting/de-noising experiment with the USPS dataset. We compute the graph using the first $300$ digits and use $4349$ of the remaining ones to test our algorithm. We use a mask covering $50$ per cent of the pixel and various amount of noise. We then average the result over $4349$ experiments (corresponding to the $4349$ digits) to obtain the curves displayed in Figure~\\ref{fig:usps_inpainting}.\nFor this experiment, we also compare to traditional TV de-noising~\\cite{chambolle2004algorithm} and Tikonow de-noising. The optimization problems used are similar to \\ref{prob:inpainting-tik}.\nThe results presented in Figure~\\ref{fig:usps_inpainting} show that graph optimization is outperforming classical techniques meaning that the grid is not the optimal graph for the USPS dataset. Moreover, Wiener once again outperforms the other graph-based models.\n\n\\begin{figure}[htb!]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{figures/usps_cov.png} \\\\\n\\includegraphics[width=0.45\\linewidth]{figures/usps_inpainting_errors.png} \n\\end{center}\n\\caption{Top left: Weights matrix of the $20$ nearest neighbor graph (The diagonal shape indicate the grid base topology of the graph). Top right: PSD matrix for the first $50$ graph frequencies. Bottom: Recovery errors for different noise levels. Methods using the nearest neighbors graph performs better.}\n\\label{fig:usps_inpainting}\n\\end{figure}\n\n\n\\section{Conclusion} \\label{sec:conclusion}\nIn this contribution, we have extended the common concept of stationarity to graph signals. Using this statistical model, we proposed a new regularization framework that leverages the stationarity hypothesis by using the Power Spectral Density (PSD) of the signal. Since the PSD can be efficiently estimated, even for large graphs, the proposed Wiener regularization framework offers a compelling way to solve traditional problems such as denoising, regression or semi-supervised learning. We believe that stationarity is a natural hypothesis for many signals on graphs and showed experimentally that it is deeply connected with the popular nearest neighbor graph construction. As future work, it would be very interesting to clarify this connection and explore if stationarity could be used to infer the graph structure from training signals, in the spirit of~\\cite{Dong:2014tj}.\n\n\n\n\\section*{Acknowledgment}\nThis work has been supported by the Swiss National Science Foundation research project \\textit{Towards Signal Processing on Graphs}, grant number: 2000\\_21/154350/1.\n\n\n\n\n\\appendix\n\n\n\\section{Convex models} \\label{sec:convex_model}\nConvex optimization has recently become a standard tool for problems such as de-noising, de-convolution or in-painting. Graph priors have been used in this field for more than a decade~\\cite{smola2003kernels,zhou2004regularization,peyre2008non}. The general assumption is that the signal varies smoothly along the edges, which is equivalent to saying that the signal is low-frequency-based. Using this assumption, one way to express mathematically an in-painting problem is the following:\n\n", "index": 59, "text": "\\begin{equation} \\label{prob:inpainting-tik}\n\\tilde{x} = {\\operatorname{arg\\,min}}_x x^T{\\mathcal{L}} x \\hspace{0.5cm} \\text{ s.t. } \\hspace{0.5cm} \\|M x - y\\|_2\\leq \\epsilon\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}={\\operatorname{arg\\,min}}_{x}x^{T}{\\mathcal{L}}x\\hskip 14.226378pt%&#10;\\text{ s.t. }\\hskip 14.226378pt\\|Mx-y\\|_{2}\\leq\\epsilon\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2061</mo><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>\u00a0s.t.\u00a0</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03f5</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhere $h$ is the convolution kernel. To be as generic as possible, we combine problems \\eqref{prob:inpainting-tik} and \\eqref{prob:deconvolution-tik} together leading to a model capable of performing de-convolution, in-painting and de-noising at the same time:\n\n", "itemtype": "equation", "pos": 58463, "prevtext": "\nwhere $M$ is a masking operator and $\\epsilon$ a constant computed thanks to the noise level. We could also rewrite the objective function as $x^T {\\mathcal{L}} x + \\gamma \\|M x - y\\|_2^2$, but this implies a greedy search of the regularization parameter $\\gamma$ even when the level of noise is known. In general $\\epsilon$ is easier to set. For example, when the noise is uniformly Gaussian, we have $\\epsilon = n \\sqrt{\\#y}$, where $\\#y$ is the number of elements of $y$. \n\nGraph de-convolution can also be addressed with the same prior assumption leading to \n\n", "index": 61, "text": "\\begin{equation} \\label{prob:deconvolution-tik}\n\\tilde{x} = {\\operatorname{arg\\,min}}_x x^T{\\mathcal{L}} x \\hspace{0.5cm} \\text{ s.t. } \\hspace{0.5cm} \\|h({\\mathcal{L}}) x - y\\|_2\\leq \\epsilon\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}={\\operatorname{arg\\,min}}_{x}x^{T}{\\mathcal{L}}x\\hskip 14.226378pt%&#10;\\text{ s.t. }\\hskip 14.226378pt\\|h({\\mathcal{L}})x-y\\|_{2}\\leq\\epsilon\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2061</mo><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>\u00a0s.t.\u00a0</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03f5</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\n\nWhen the signal is piecewise smooth on the graph, another regularization term can be used instead of $x^T{\\mathcal{L}} x = \\|\\nabla_G x\\|_2^2$, which is the $\\ell_2$-norm of the gradient on the graph\\footnote{The gradient on the graph is defined as $\\nabla_G x (i,j) = \\frac{1}{2}\\sqrt{W(i,j)} \\left(x(i)-x(j)\\right)$}. \nUsing the $\\ell_1$-norm of the gradient favours a small number of major changes in signal and thus is better for piecewise smooth signals. The resulting model is: \n\n", "itemtype": "equation", "pos": 58932, "prevtext": "\nwhere $h$ is the convolution kernel. To be as generic as possible, we combine problems \\eqref{prob:inpainting-tik} and \\eqref{prob:deconvolution-tik} together leading to a model capable of performing de-convolution, in-painting and de-noising at the same time:\n\n", "index": 63, "text": "\\begin{equation} \\label{prob:dec-in-den-tik}\n\\tilde{x} = {\\operatorname{arg\\,min}}_x x^T{\\mathcal{L}} x \\hspace{0.5cm} \\text{ s.t. } \\hspace{0.5cm} \\|M h({\\mathcal{L}}) x - y\\|_2\\leq \\epsilon.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}={\\operatorname{arg\\,min}}_{x}x^{T}{\\mathcal{L}}x\\hskip 14.226378pt%&#10;\\text{ s.t. }\\hskip 14.226378pt\\|Mh({\\mathcal{L}})x-y\\|_{2}\\leq\\epsilon.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2061</mo><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>\u00a0s.t.\u00a0</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03f5</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\n\nIn order to solve these problems, we use a subset of convex optimization tools called proximal splitting methods. Since we are not going to summarise them here, we encourage a novice reader to consult~\\cite{combettes2011proximal,komodakis2014playing} and the references therein for an introduction to the field.\n\n\\section{Proof of Theorem \\ref{theo:optimality}} \\label{sec:proof of optimality}\nThe following is a generalization of the classical proof.\n\\begin{proof}\nBecause, by hypothesis $A=a({\\mathcal{L}}) = U a(\\Lambda) U^*$, we can rewrite the optimization problem \\ref{prob:Wiener-opt} in the graph Fourier domain using Parseval relationship $\\|x\\|_2 = \\|Ux\\|_2 = \\|\\hat{x}\\|_2$: \n", "itemtype": "equation", "pos": 59626, "prevtext": "\n\nWhen the signal is piecewise smooth on the graph, another regularization term can be used instead of $x^T{\\mathcal{L}} x = \\|\\nabla_G x\\|_2^2$, which is the $\\ell_2$-norm of the gradient on the graph\\footnote{The gradient on the graph is defined as $\\nabla_G x (i,j) = \\frac{1}{2}\\sqrt{W(i,j)} \\left(x(i)-x(j)\\right)$}. \nUsing the $\\ell_1$-norm of the gradient favours a small number of major changes in signal and thus is better for piecewise smooth signals. The resulting model is: \n\n", "index": 65, "text": "\\begin{equation} \\label{prob:dec-in-den-tv}\n\\tilde{x} = {\\operatorname{arg\\,min}}_x \\|\\nabla_G x\\|_1 \\hspace{0.5cm} \\text{ s.t. } \\hspace{0.5cm} \\|M h({\\mathcal{L}}) x - y\\|_2\\leq \\epsilon\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}={\\operatorname{arg\\,min}}_{x}\\|\\nabla_{G}x\\|_{1}\\hskip 14.226378pt%&#10;\\text{ s.t. }\\hskip 14.226378pt\\|Mh({\\mathcal{L}})x-y\\|_{2}\\leq\\epsilon\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mi>x</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msub><mo>\u2207</mo><mi>G</mi></msub><mo>\u2061</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>\u00a0s.t.\u00a0</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03f5</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nSince the matrix $\\Lambda$ is diagonal, the solution of this problem satisfies for all graph eigenvalue ${\\lambda_\\ell}$\n\n", "itemtype": "equation", "pos": 60517, "prevtext": "\n\nIn order to solve these problems, we use a subset of convex optimization tools called proximal splitting methods. Since we are not going to summarise them here, we encourage a novice reader to consult~\\cite{combettes2011proximal,komodakis2014playing} and the references therein for an introduction to the field.\n\n\\section{Proof of Theorem \\ref{theo:optimality}} \\label{sec:proof of optimality}\nThe following is a generalization of the classical proof.\n\\begin{proof}\nBecause, by hypothesis $A=a({\\mathcal{L}}) = U a(\\Lambda) U^*$, we can rewrite the optimization problem \\ref{prob:Wiener-opt} in the graph Fourier domain using Parseval relationship $\\|x\\|_2 = \\|Ux\\|_2 = \\|\\hat{x}\\|_2$: \n", "index": 67, "text": "\n\\[\n\\dot{\\hat{x}} = \\arg\\min_{\\hat{x}}\\|w(\\Lambda)\\hat{x}\\|_{2}^{2}+\\|a(\\Lambda)\\hat{x}-\\hat{y}\\|_{2}^{2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\dot{\\hat{x}}=\\arg\\min_{\\hat{x}}\\|w(\\Lambda)\\hat{x}\\|_{2}^{2}+\\|a(\\Lambda)\\hat%&#10;{x}-\\hat{y}\\|_{2}^{2}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u02d9</mo></mover><mo>=</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover></munder></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>-</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nFor simplicity, we drop the notation $({\\lambda_\\ell})$. The previous equation is transformed in   \n", "itemtype": "equation", "pos": 60748, "prevtext": "\nSince the matrix $\\Lambda$ is diagonal, the solution of this problem satisfies for all graph eigenvalue ${\\lambda_\\ell}$\n\n", "index": 69, "text": "\\begin{equation} \\label{eq:app_optimality_constraint}\nw^{2}({\\lambda_\\ell})\\hat{x}({\\lambda_\\ell})+a^{2}({\\lambda_\\ell})\\hat{x}({\\lambda_\\ell})-a({\\lambda_\\ell})\\hat{y}({\\lambda_\\ell})=0\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"w^{2}({\\lambda_{\\ell}})\\hat{x}({\\lambda_{\\ell}})+a^{2}({\\lambda_{\\ell}})\\hat{x%&#10;}({\\lambda_{\\ell}})-a({\\lambda_{\\ell}})\\hat{y}({\\lambda_{\\ell}})=0\" display=\"block\"><mrow><mrow><mrow><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nAs a next step, we use the fact that\n$\\hat{y}=a\\hat{x}_o+\\hat{w}_n$ to find:\n", "itemtype": "equation", "pos": 61049, "prevtext": "\nFor simplicity, we drop the notation $({\\lambda_\\ell})$. The previous equation is transformed in   \n", "index": 71, "text": "\n\\[\n\\dot{\\hat{x}}=\\frac{a}{w^{2}+a^{2}}\\hat{y}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\dot{\\hat{x}}=\\frac{a}{w^{2}+a^{2}}\\hat{y}\" display=\"block\"><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u02d9</mo></mover><mo>=</mo><mrow><mfrac><mi>a</mi><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>+</mo><msup><mi>a</mi><mn>2</mn></msup></mrow></mfrac><mo>\u2062</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThe error performed by the algorithm becomes\n", "itemtype": "equation", "pos": 61175, "prevtext": "\nAs a next step, we use the fact that\n$\\hat{y}=a\\hat{x}_o+\\hat{w}_n$ to find:\n", "index": 73, "text": "\n\\[\n\\dot{\\hat{x}}=\\frac{a^{2}\\hat{x}_o+a\\hat{w}_n}{w^{2}+a^{2}}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\dot{\\hat{x}}=\\frac{a^{2}\\hat{x}_{o}+a\\hat{w}_{n}}{w^{2}+a^{2}}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u02d9</mo></mover><mo>=</mo><mfrac><mrow><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mi>o</mi></msub></mrow><mo>+</mo><mrow><mi>a</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mi>n</mi></msub></mrow></mrow><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>+</mo><msup><mi>a</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThe expectation of the error can thus be computed:\n\\begin{eqnarray*}\n{\\mathbb{E}}\\left(\\hat{e}^{2}\\right) \n\n & = & \\frac{w^{4}{\\mathbb{E}}\\left(\\hat{x}_o^{2}\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}+\\frac{a^{2}{\\mathbb{E}}\\left(\\hat{w}_n^{2}\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}-\\frac{aw^{2}{\\mathbb{E}}\\left(\\hat{x}_o\\hat{w}_n\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}\\\\\n \n & = & \\frac{w^{4}s^2+a^{2}n}{\\left(w^{2}+a^{2}\\right)^{2}}\n\\end{eqnarray*}\nwith $s^2$ the PSD of $x_o$ and $n$ the PSD of the noise $w_n$. Note that ${\\mathbb{E}}\\left(\\hat{x}_o\\hat{w}_n\\right) = 0$ because $x$ and $w_n$ are uncorrelated. Let us now substitute $w^{2}$ by $z$ and minimize the expected error (for each ${\\lambda_\\ell}$) with respect of $z$\n\\begin{eqnarray*}\n\\frac{\\partial}{\\partial z}{\\mathbb{E}}\\left(\\hat{e}^{2}\\right) & = & \\frac{\\partial}{\\partial z}\\frac{zs^2+a^{2}n}{\\left(z+a^{2}\\right)^{2}}\\\\\n & = & \\frac{2zs^2\\left(z+a^{2}\\right)-2\\left(z^{2}s^2+a^{2}n\\right)}{\\left(z+a^{2}\\right)^{3}} = 0\n \n\\end{eqnarray*}\n\n\n\n\nFrom the numerator, we get:\n", "itemtype": "equation", "pos": 61287, "prevtext": "\nThe error performed by the algorithm becomes\n", "index": 75, "text": "\n\\[\n\\hat{e} = \\dot{\\hat{x}}-\\hat{x}_o \n\n\n=\\frac{-w^{2}\\hat{x}_o}{w^{2}+a^{2}}+\\frac{a\\hat{n}}{w^{2}+a^{2}}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\hat{e}=\\dot{\\hat{x}}-\\hat{x}_{o}\\par&#10;\\par&#10;=\\frac{-w^{2}\\hat{x}_{o}}{w^{2}+a^{%&#10;2}}+\\frac{a\\hat{n}}{w^{2}+a^{2}}\" display=\"block\"><mrow><mover accent=\"true\"><mi>e</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u02d9</mo></mover><mo>-</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mi>o</mi></msub></mrow><mo>=</mo><mrow><mfrac><mrow><mo>-</mo><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mi>o</mi></msub></mrow></mrow><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>+</mo><msup><mi>a</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>a</mi><mo>\u2062</mo><mover accent=\"true\"><mi>n</mi><mo stretchy=\"false\">^</mo></mover></mrow><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>+</mo><msup><mi>a</mi><mn>2</mn></msup></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThe three possible solution for $z$ are $z_{1} =  \\frac{n}{s^2}$, $z_{2}=\\infty$ and $z_{3}=-\\infty$. $z_3$ is not possible because $z$ is required to be positive. $z_2$ leads to $\\dot{x} = 0$ which is optimal only if $s^2=0$. This makes the optimal value of $z({\\lambda_\\ell}) = \\frac{n({\\lambda_\\ell})}{s^2({\\lambda_\\ell})}$, resulting in \n\n", "itemtype": "equation", "pos": 62433, "prevtext": "\nThe expectation of the error can thus be computed:\n\\begin{eqnarray*}\n{\\mathbb{E}}\\left(\\hat{e}^{2}\\right) \n\n & = & \\frac{w^{4}{\\mathbb{E}}\\left(\\hat{x}_o^{2}\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}+\\frac{a^{2}{\\mathbb{E}}\\left(\\hat{w}_n^{2}\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}-\\frac{aw^{2}{\\mathbb{E}}\\left(\\hat{x}_o\\hat{w}_n\\right)}{\\left(w^{2}+a^{2}\\right)^{2}}\\\\\n \n & = & \\frac{w^{4}s^2+a^{2}n}{\\left(w^{2}+a^{2}\\right)^{2}}\n\\end{eqnarray*}\nwith $s^2$ the PSD of $x_o$ and $n$ the PSD of the noise $w_n$. Note that ${\\mathbb{E}}\\left(\\hat{x}_o\\hat{w}_n\\right) = 0$ because $x$ and $w_n$ are uncorrelated. Let us now substitute $w^{2}$ by $z$ and minimize the expected error (for each ${\\lambda_\\ell}$) with respect of $z$\n\\begin{eqnarray*}\n\\frac{\\partial}{\\partial z}{\\mathbb{E}}\\left(\\hat{e}^{2}\\right) & = & \\frac{\\partial}{\\partial z}\\frac{zs^2+a^{2}n}{\\left(z+a^{2}\\right)^{2}}\\\\\n & = & \\frac{2zs^2\\left(z+a^{2}\\right)-2\\left(z^{2}s^2+a^{2}n\\right)}{\\left(z+a^{2}\\right)^{3}} = 0\n \n\\end{eqnarray*}\n\n\n\n\nFrom the numerator, we get:\n", "index": 77, "text": "\n\\[\n2zs^2a^{2}-2a^{2}n=0\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"2zs^{2}a^{2}-2a^{2}n=0\" display=\"block\"><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>z</mi><mo>\u2062</mo><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mi>n</mi></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nThis close the first part of the proof.\n\nTo show that the solution to \\eqref{prob:Wiener-opt} is a wiener filtering operation, we replace $w^2({\\lambda_\\ell})$ by $\\frac{n({\\lambda_\\ell})}{s^2({\\lambda_\\ell})}$ in \\eqref{eq:app_optimality_constraint}. We find\n\n", "itemtype": "equation", "pos": 62803, "prevtext": "\nThe three possible solution for $z$ are $z_{1} =  \\frac{n}{s^2}$, $z_{2}=\\infty$ and $z_{3}=-\\infty$. $z_3$ is not possible because $z$ is required to be positive. $z_2$ leads to $\\dot{x} = 0$ which is optimal only if $s^2=0$. This makes the optimal value of $z({\\lambda_\\ell}) = \\frac{n({\\lambda_\\ell})}{s^2({\\lambda_\\ell})}$, resulting in \n\n", "index": 79, "text": "$$\nw({\\lambda_\\ell}) =\\sqrt{\\frac{n({\\lambda_\\ell})}{s^2({\\lambda_\\ell})}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"w({\\lambda_{\\ell}})=\\sqrt{\\frac{n({\\lambda_{\\ell}})}{s^{2}({\\lambda_{\\ell}})}}.\" display=\"block\"><mrow><mrow><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mfrac><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></msqrt></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02522.tex", "nexttext": "\nwhich is the wiener filter associated to the convolution $a({\\mathcal{L}})=A$.\n\\end{proof}\n\\bibliographystyle{IEEEtran}\n\\bibliography{biblio}\n\n\n\n\n", "itemtype": "equation", "pos": 63142, "prevtext": "\nThis close the first part of the proof.\n\nTo show that the solution to \\eqref{prob:Wiener-opt} is a wiener filtering operation, we replace $w^2({\\lambda_\\ell})$ by $\\frac{n({\\lambda_\\ell})}{s^2({\\lambda_\\ell})}$ in \\eqref{eq:app_optimality_constraint}. We find\n\n", "index": 81, "text": "$$\n\\hat{x}({\\lambda_\\ell}) = \\frac{s^2({\\lambda_\\ell})a({\\lambda_\\ell})\\hat{y}({\\lambda_\\ell})}{a^2({\\lambda_\\ell})s^2({\\lambda_\\ell})+n({\\lambda_\\ell})},\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\hat{x}({\\lambda_{\\ell}})=\\frac{s^{2}({\\lambda_{\\ell}})a({\\lambda_{\\ell}})\\hat%&#10;{y}({\\lambda_{\\ell}})}{a^{2}({\\lambda_{\\ell}})s^{2}({\\lambda_{\\ell}})+n({%&#10;\\lambda_{\\ell}})},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>s</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}]