[{"file": "1601.07483.tex", "nexttext": "\n\nIn non-ideal cases where generating $h^*$ is expensive, we assume that,\n\n\t$h^*(\\pi_i) \\approx h(\\pi_i) \\approx cost(R_i) + h(\\pi_{i+1})$.\n\n\nIn Figure~1, for a given partial plan $\\pi_i$, \nsuppose a minimum of $\\mathit{{n}}$ refinement steps are required to make $\\pi_i$ a solution plan.\n\nHere we assume that at each stage of the refinement process, the best child for each $\\pi_i$ is chosen, \nfor all \\emph{i}. \nHere, $\\mathit{{n}}$ is an estimate of adding new actions in $\\pi_i$. \n\n\n\n\nIf we consider only unit cost actions then in the ideal case, $h^*(\\pi_{i+1})$ = $h^*(\\pi_i)$ - 1. \nGenerally, $h$ commits some error at each refinement step called {single-step-error} $\\mathrm{(\\epsilon_h)}$.\n\nThe single-step-error associated with $h$, when $\\pi_i$ gets refined by an algorithm using $h$, \ndenoted as $\\epsilon_{h(\\pi_i)}$ is,\n\n", "itemtype": "equation", "pos": 19724, "prevtext": "\n\n\n\n\n\\title\n{\n\n\n\nLearning and Tuning Meta-heuristics in Plan Space Planning\n\n}\n\n\n\\author{ Shashank Shekhar and Deepak Khemani\\\\\nDepartment of Computer Science \\& Engineering\\\\\nIIT Madras, Chennai\\\\\nTamil Nadu, India 600 036\\\\\n\\url{{sshekhar,khemani}@cse.iitm.ac.in}\n}\n\n\\maketitle\n\n\n\\begin{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn recent years, the planning community has observed that techniques for learning heuristic functions \nhave yielded improvements in performance. \n\nOne approach is to use offline learning to learn predictive models from existing heuristics in a domain dependent manner. \nThese learned models are deployed as new heuristic functions.\n\n\n\n\nThe learned models can in turn be tuned online using a domain independent error correction \napproach to further enhance their informativeness. \nThe online tuning  approach is domain independent but instance specific, \nand contributes to improved performance for individual instances as planning proceeds. \nConsequently it is more effective in larger problems.\n\n\nIn this paper, we mention two approaches applicable in Partial Order Causal Link (POCL) Planning that is also known as \nPlan Space Planning. \nFirst, we endeavour to enhance the performance of a POCL planner by giving an algorithm for supervised learning.\n\n\n~Second, we then discuss an online error minimization approach in POCL framework to minimize the \nstep-error associated with the offline learned models thus enhancing their informativeness.\n\n\nOur evaluation shows that the learning approaches scale up the performance of the planner over standard benchmarks, specially for larger problems.  \n\n\n\n\\end{abstract}\n\n\n\\section{Introduction}\n\n\nIn the recent International Planning Competitions (IPC) state-space based and total-order planners\nlike LAMA~\\cite{richter2010lama}, \nFast Downward Stone Soup~\\cite{helmert2011fast},\nand \nFast Downward Stone Soup 2014~\\cite{rogerfast14} have performed well. \n\nThese planners are very efficient, generate consistent states fast, and use powerful state-based heuristics.\nBut they often commit too early to ordering of actions, giving up on flexibility in the generated plans.\n\n\\textcolor{black}\n{\nIn contrast, the POCL framework~\\cite{ColesCFL10} generates more flexible plans, but in general \nis computationally more intensive than the state-space based approaches.\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe POCL framework has found applicability in multi-agent planning~\\cite{Kvarnstrom11}\nand temporal planning~\\cite{BentonCC12}.\n\n\n\n\n\n\n\n\n\nResearchers have recently investigated the applicability of state-space heuristic learning \napproaches~\\cite{sapena2014combining,ColesCFL10} in POCL planning.\n\nThis revival of interest is due to the idea of delayed commitment of\nRePOP~\\cite{nguyen2001reviving} and VHPOP~\\cite{younes2003vhpop}.\n\n\n\n\n\n\n\nIn this paper  we further investigate the adaptation of state space approaches in POCL planners yielding quality plans over the same or even larger problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn general, due to the diverse nature of planning problems characterized by the degree of interactions between subgoals, \na heuristic function does not always work well in all planning domains.\n\nVarious techniques have been devised to increase the informativeness of heuristics in the state space arena.\n\nOne approach strengthens the delete relaxation heuristic by incrementing lower bounds to \ntighten the admissibility of the heuristic, repeatedly by solving relaxed versions of a planning problem~\\cite{Haslum12}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn another approach, \nto circumvent the trade-offs of combining multiple heuristics, \na decision rule is used for selecting \na heuristic function in a given state. \nAn active online learning technique is applied to learn a model for that given decision rule~\\cite{DomshlakKM10}. \n\n\n\nWe present a couple of machine learning techniques,  \ninfluenced  from~\\cite{arfaee2011learning,thayer2011learning,samadi2008learning,us2013learning}, \nwhich improve the effectiveness of heuristics in the POCL framework. \n\nFirst, we apply domain wise regression techniques in a supervised manner, using existing POCL heuristics as features. \n\n\nFor target generation we use our planner named RegPOCL.\nThis is based on VHPOP and uses grounded actions to speed up the planning process.\n\nNext, we further give another technique for POCL framework \nwhich enhances the informativeness of these offline learned models. \nThis technique is an adapted version of an online heuristic adjustment \napproach based on temporal difference (TD) learning~\\cite{sutton1988learning,thayer2011learning}.\n\nWe extend this domain independent approach for learning instance specific details, \nwhich corrects the error associated with the learned models, thus making them more informed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\textcolor{black}\n{\nThe RegPOCL planner employs these two approaches and evaluation shows that it is more efficient on the benchmarks.\n} \nWe have confined the evaluation to non-temporal STRIPS domains.\n\n\n\nThe rest of the paper is structured as follows. \nAfter looking at the motivation for exploring the POCL approach, \nwe describe the learning approaches used along with the experiment design. \nThese are followed by the experimental results and the concluding discussion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{POCL Planning}\n\nA POCL planner starts search with a null partial plan and progresses over a space of partial plans,\nby adding a resolver to a partial plan to remove a flaw.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use heuristics from~\\cite{younes2003vhpop} for selecting the most adverse flaw to resolve in the selected \npartial plan.\n\n\n\n\nThe POCL framework has certain advantages over forward state-space search {(FSSS)}.  \n{FSSS} has the problem of premature commitment to an ordering between two actions which reduces the plan flexibility.  \n\n\nIt does so to avoid any mutual interference between actions, though there may not be any. \nPOCL planning avoids committing unnecessarily to ordering actions.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\textcolor{black}\n{\n\nFSSS faces problems while solving instances with deadlines.\nDeadlines may arise within the interval(s) of one or more durative actions. \nIn general, the actions may produce some delayed effects, and this may have ramifications on deadlines as well, \nwhich creates deadlines relative to their starting points~\\cite{ColesCFL10}. \n\nFSSS also suffers from significant backtracking as it may explore all possible plan permutations \nin the absence of effective guidance.\n}\n\n\nThe POCL framework also has several advantages in temporal planning,\nspecially in planning with complex temporal constraints beyond actions with duration. \n\n\n\nThese limitations of {FSSS} motivate us to explore the POCL framework.    \n\n\n\\subsubsection{Example.}\nSuppose we are required to add four actions [$a_1$, $a_2$, $a_3$, $a_4$] to a plan, \nwhere $a_2$ is dependent on $a_1$ and $a_4$ is dependent on $a_3$. \nThere is no interference between any two actions apart from the above dependencies. \n\nIn this case, FSSS gives an ordering or timestamp [0, 1, 2, 3], with a makespan 4,\nwhilst the delayed commitment strategy would give more choices with flexibility in the orderings like \n[2, 0, 1, 3] and [0, 2, 1, 3]. If parallel execution is allowed, makespan would be 2.\n\n\nIf another action $a_5$, which is dependent on $a_3$, has to be introduced in the plan then FSSS will allot it a timestamp 4, whereas delayed-commitment strategy could allot it 1. \n\n\nHowever, if we ignore the absence of the flexibility and action parallelism in {FSSS}, \nit is very fast in recovering from a situation that would arise due to committing to some wrong choices during planning. \n{FSSS} has the advantage of faster search state generation and powerful state-based heuristics.  \n\n\n\n\n\n\\section{Learning Approaches Used}\n\n\nWe propose a two fold approach to learn better heuristic functions. \nFirst, existing heuristic function are combined by a process of offline learning that generates learned predictive models. \nThis is followed by an online technique of adjusting the step-error associated with these models during partial plan refinement.\n\n\nWe divide this section into two parts: the first describes the offline learning techniques to perform regression, and \nthe second the technique of further strengthening the informativeness of a learned predictive model.  \n\n\n\n\\subsection{Offline Learning}\n\n\nOffline learning is an attractive approach because generating training sets in most planning domains is \na fast and simple process. \n\n\n\n\n\n\n\n\n\nThe learning process has three phases: \n\n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item dataset preparation, \n\\item training, and \n\\item testing.\n\\end{inparaenum}\n\n\n\nThe training instances gathered by solving small problems become the inputs to the used regression techniques\n(\\emph{e.g.} linear regression and M5P, described later), \nwhich learn effective ways of combining existing heuristic functions. \nThe testing phase is used to validate the best ways of combining the known heuristic functions.  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm~1, described below, embodies the complete training process. \n\n\n\n\\begin{algorithm}\n\\caption{The algorithm used during training phase}\n\\begin{algorithmic}[1]\n\\State  \\textbf{Input} \n\\State \\hspace{0.15in} \\emph{AS} - Attribute Selection; $T$ - Training Dataset; \n\\State \\hspace{0.15in} $S$ - Problem Set; $L$ - Learning Technique;\n\\State \\hspace{0.15in} $H$ - Heuristic Set; \\emph{RegPOCL} - The Planner.\n\n\\State \\textbf{Output}\n\\State \\hspace{0.15in} $\\mathrm{M}:\\mathrm{T}\\rightarrow\\mathbb{R}$~~~// \\emph{A learned predictive model}\n\\Procedure {Training-Algorithm}{$\\emph{AS}$, $T$, $L$}\n\\State $T$ $\\leftarrow$ $\\phi$~~~// \\emph{Domain specific}.\n\\State $T$ $\\leftarrow$ \\Call{Dataset-prep}{\\emph{RegPOCL}, $S$, $H$, $T$}\n\\State Training Instances $\\leftarrow$ Apply($\\emph{AS}$, $T$)\n\n\\State \\Return $M$ $\\leftarrow$ Apply($L$, Training Instances)\n\\EndProcedure\n\\Statex\n\n\\Procedure{Dataset-prep}{\\textit{{RegPOCL}}, $S$, $H$, $T$} \n\n\\State \\hspace{0.0in} $F$ - Check; $\\mathcal{T}$ - Target Value;\n\\State \\hspace{0.0in} $\\Pi$ - A set of seed partial plans.\n\n\t\\For{\\textbf{each} $p$ $\\in$ $S$}\n\t\n\t\t\\State $\\Pi$ $\\leftarrow$ Null partial plan of $p$\n\t\t\n    \t\n    \t\\For{a random $sp$ $\\in$ $\\Pi$} \n\n\n\n\n    \t\t\\State ($F$, $\\mathcal{T}$, $\\Pi_{loc}$) $\\leftarrow$ {Solve(\\emph{RegPOCL}, $sp$, $H$)}\n    \t\t\n    \t\t\\If{$F$}~~~// $sp$ \\emph{refines completely}. \n\t\t\t\t\\State $\\Pi$ $\\leftarrow$ $\\Pi$ $\\cup$ $\\Pi_{loc}$\n\t\t\t\t\\State Ins $\\leftarrow$ {Comp-inst}({\\emph{RegPOCL}, $H$, $sp$, $\\mathcal{T}$)}\n\t\t\t\t\\State $T$ $\\leftarrow$ $T$ $\\cup$ Ins\n    \t\t\\EndIf\n    \t\t\n    \t\\EndFor~~~// \\emph{Bounded number of iterations}.\n\n    \t\n\t\\EndFor\n\t\n    \\State \\Return $T$\n    \n\\EndProcedure\n\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\subsubsection{Dataset Preparation}\n\n\n\n\n\n\n\n\n\nThe procedure \\sc{dataset-prep()} \\normalfont\nLine 13 in Algorithm~1 is used to solve a set ($S$) of planning problems. \nWe consider only small problems that are gathered from each planning domain selected from previous IPCs.\nThe output of Algorithm~1 is a trained predictive model (as shown in line~6).\nWe consider each problem from $S$ for the dataset preparation in each domain (line~16).\n\n\nIn this algorithm, a seed partial plan is a new partial plan that gets generated due to\na possible refinement of the selected partial plan. \nWe select a seed $sp$ from a set of seed partial plans $\\Pi$ (line~18). \n\n$sp$ will be provided to {RegPOCL} for its further refinements.\nIf {RegPOCL} is able to generate one consistent solution by refining $sp$ completely using \\emph{Solve()} function (line~19), \nthen the flag $F$ will be true. \n\n\nWe capture the newly generated partial plans in local instance of $\\Pi$ called $\\Pi_{loc}$. \nThe target $\\mathcal{T}$ captures the number of new actions that get added in $sp$.\n\n$\\mathcal{T}$ is calculated when the planner refines $sp$ completely using heuristics from $H$.\nNote that, $\\mathcal{T}$ is not a heuristic value but the number of new actions added during the refinement process. \nThe value of $\\mathcal{T}$ is also the plan length found which might not be optimal.\n\nSince $sp$ is refined completely, $\\Pi_{loc}$ is updated to $\\Pi$ (line~21). \nLine~22 computes a training instance $\\mathrm{Ins}$, using \\emph{Comp-inst()} function. \n\nFor a given $sp$, the planner generates a training instance of the form \n$t(sp)$ = $\\langle\\langle h_1(sp)$, $h_2(sp),$ $ h_3(sp),$ $h_4(sp),$ $h_5(sp),$ $h_6(sp)\\rangle,$ $\\mathcal{T}\\rangle$,\nwhere $h_1$ to $h_6$ are the feature heuristics.\n\n\n\n\n\n\n\nTo maintain consistency, we update the training set $T$ (line~23) only when {RegPOCL} refines the current seed $sp$ completely.\n\nIf complete refinement was not possible, all new seeds from $\\Pi_{loc}$ are dropped, \neven though it might be the case that $\\Pi_{loc}$ contains some consistent seeds particularly in the case of time out. \n\n\n\nTo maintain diversity in $T$, for a given domain we randomly select a fixed number of seeds for \nthe complete refinement process (line~18). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe execute Algorithm~1 once for each selected domain with a given set of feature heuristics. \n\nNote that learning does not guarantee optimal predictive models even though optimal targets \nhave been used in the training~\\cite{us2013learning}. \n\nAlgorithm~1 hunts for a well informed heuristic using learning and does not bother about its admissibility.\n\n\nSince the state-of-the-art POCL heuristics are not optimal in nature~\\cite{younes2003vhpop},\nthe usage of {RegPOCL} for generating training instances should not affect the performance of {RegPOCL} on large \nproblems in the testing phase.\n\nThe selection of {RegPOCL} for generating training sets might deteriorate the actual target values, \nas the targets calculated by {RegPOCL} are not optimal in general. \n\n\nThus there is a possibility of learning inaccurate predictive models in the training phase,\nwhich might reduce the informativeness of the models.\n\nWe enhance the informativeness of the models by correcting the step-error associated with them using an\nonline heuristic tuning approach. \n\n\n\n\n\n\n\n\n\\subsubsection{Training}\n\nOnce Algorithm~1 generates enough number of training instances for a given domain, it moves to the next step (line~10).\n\n\n\n\nWe define a regression model $\\mathrm{M}:\\mathrm{T}\\rightarrow\\mathbb{R}$, \nwhere $\\mathrm{T}$ is a training set and $\\mathbb{R}$ is a set of real numbers.\n\n\n\n\nFollowing the general model training strategy, \nwe use WEKA~\\cite{Hall2009} to remove irrelevant or redundant attributes (line~10) from the training set.\nThis reduces the effort of the planner because the planner must calculate the \nselected feature heuristics at each step of the planning process. \n\nNext, we apply model training process (line~11).\nWe feed the training instances to different machine learning approaches to learn different predictive models. \n\n\n\n\n\n\n\\subsubsection{Testing}\n\nWe test the predictive models on large problems.\nThe models are directly compared to the current best heuristics in the POCL framework. \n\nFor using machine learning approaches in planning efficiently, we select the best learned regression models\nand test the efficiency of {RegPOCL} by using them over different benchmarks. \nThese models help in selecting the most suitable partial plan for refinement.\n\n\n\nOffline learning learns a better model in terms of search guidance and accuracy than\nonline learning~\\cite{samadi2008learning,thayer2011learning}.\n\nAn offline learned predictor is more accurate than an online one because \nin the offline case a complete training set is available.\n\n\n\n\nAnother alternative to the above approaches would be bootstrapping methods~\\cite{arfaee2011learning},\n\\textcolor{black}\n{\nwhere a set of problems is solved using a base heuristic within a specified time limit. \n\nLater, the solutions obtained for learning are used to generate a new more informed heuristic function.\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Online Error Adjustment of A Predictive Model}\n\nThe offline approach assumes that the learned knowledge can effectively be transferred from \nthe training instances to the test instances.\nThis is not always true as the planning problems are not always similar even though they belong to the same domain. \n\nAlso, the training instances are required before the training process.\nFor a planning domain, during the dataset preparation, calculation of features is not computationally hard,\nbut calculating actual number of new actions $\\mathcal{(T)}$ needed for $sp$ is relatively expensive. \n\n\n\n\nOnline learning attempts to avoid these limitations. \n\n\n\n\nIn our hybrid approach we use small instances for offline training, thus saving on time. \nThis is followed up with online learning to improve the heuristic estimate on-the-fly.\n\nThe online error tuning is based on temporal difference (TD) learning~\\cite{sutton1988learning}. \n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[ht]\n\\centering\n     \\includegraphics[width=0.45 \\textwidth, decodearray={3 5}]{pop-ex.pdf}\n      \\caption{A POCL framework - The refinement \\emph{($R$)} starts from $\\pi_0$ and it goes to the solution plan $(\\pi_{sol})$. \n      At each step, for a selected partial plan, many refinements are possible like refinements of $\\pi_0$ which lead to \n      $\\pi_1$, $\\pi'_1$, and $\\pi''_1$. Here, the best child is shown in the horizontal refinements.}\n      \\label{normal_case}\n\\end{figure}\n\nThe online heuristic adjustment approach is inspired from a recent work presented as technical communication~\\cite{ShekharK15}\ntested in a few planning domains.\n\n\n\nWe further develop this approach, and provide a complete derivation, and\na proof (in Appendix) of the theorem used in the cited work. \n\nWe assume that\na predictive model $(h)$ for a given partial plan $(\\pi)$ \nestimates the actual number of new actions required to be added in $\\pi$ to refine it completely to a solution plan,\ndenoted by $h(\\pi)$.\n\n\nWhilst $h^*(\\pi)$ is the minimum number of new actions required for the same purpose~\\cite{nguyen2001reviving}. \n\n\n\n\nIn the POCL framework, it is computationally expensive to predict $h^*(\\pi)$. \nSince $h$ is also likely to be uninformed, we do adjustments to $h$ by observing \nsingle-step-error $\\mathrm{(\\epsilon_h)}$ on-the-fly. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe minimum number of total refinements needed for a partial plan $\\pi_i$  to make it a\nsolution plan, goes through its best child $\\pi_{i+1}$ that is obtained after refinement $R_i$. \n\nA child $\\pi_{i+1}$ is the {best} child when it has the lowest prediction of the number of new actions needed \nfor its complete refinement among its siblings (Figure~1). We break ties in favor of minimum number of actions in \nthe children partial plans.\n\n\n\n\n\nThe set of successors of a partial plan is potentially infinite. \nThis is due to the introduction of loops in the plan which simply achieve and destroy subgoals \n\n(for example: \\emph{$\\langle$(stack A B), (unstack A B)$\\rangle$} or  \n\\emph{$\\langle$(pickup A), (putdown A)$\\rangle$}, in Blocksworld domain). \nSuch looping is common during refinement process, specially when the heuristic is not well informed. \n\n\nWe avoid such looping explicitly. This is crucial in real world scenarios. \nFor example, a pair of actions like \\emph{$\\langle$(load-truck~obj~truck~loc), (unload-truck~obj~truck~loc)$\\rangle$}, \nin Driverlog domain, could be expensive.\n\n\n\n\n\n\nIn general in plan space planning there is no backtracking.\nEach refinement of a partial plan leads to a different node in the plan space. \nThis necessitates that we consider looping explicitly.\n\n\n\nIdeally,\n\n", "index": 1, "text": "\\begin{equation}\\label{eq1:test}\n\th^*(\\pi_i) = cost(R_i) + h^*(\\pi_{i+1})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"h^{*}(\\pi_{i})=cost(R_{i})+h^{*}(\\pi_{i+1})\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\n\n\n\n\nUsing Eq.~\\eqref{eq1:test}, we derive (proof is in the Appendix) the following in a non-ideal case: \n\n\n\\subsubsection{Theorem 1.} \n\\emph\n{\nFor a given learned predictive model ($h$) and partial plan ($\\pi_i$) in Figure~1 which leads to the solution plan ($\\pi_{sol}$)\nafter certain refinement steps, the enhanced version of the predictive model ($h^e$) is,\n\n\n\n", "itemtype": "equation", "pos": 20646, "prevtext": "\n\nIn non-ideal cases where generating $h^*$ is expensive, we assume that,\n\n\t$h^*(\\pi_i) \\approx h(\\pi_i) \\approx cost(R_i) + h(\\pi_{i+1})$.\n\n\nIn Figure~1, for a given partial plan $\\pi_i$, \nsuppose a minimum of $\\mathit{{n}}$ refinement steps are required to make $\\pi_i$ a solution plan.\n\nHere we assume that at each stage of the refinement process, the best child for each $\\pi_i$ is chosen, \nfor all \\emph{i}. \nHere, $\\mathit{{n}}$ is an estimate of adding new actions in $\\pi_i$. \n\n\n\n\nIf we consider only unit cost actions then in the ideal case, $h^*(\\pi_{i+1})$ = $h^*(\\pi_i)$ - 1. \nGenerally, $h$ commits some error at each refinement step called {single-step-error} $\\mathrm{(\\epsilon_h)}$.\n\nThe single-step-error associated with $h$, when $\\pi_i$ gets refined by an algorithm using $h$, \ndenoted as $\\epsilon_{h(\\pi_i)}$ is,\n\n", "index": 3, "text": "\\begin{equation}\n\t\\epsilon_{h(\\pi_i)} = \\big((1 + h(\\pi_{i+1})) - h(\\pi_{i})\\big)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\epsilon_{h(\\pi_{i})}=\\big{(}(1+h(\\pi_{i+1}))-h(\\pi_{i})\\big{)}\" display=\"block\"><mrow><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\nwhere $\\pi_i$ $\\rightsquigarrow$ $\\pi_{sol}$ is a path in Figure~1 which captures each partial plan ($\\pi'$) along \nthe path between $\\pi_i$ and $\\pi_{sol}$.\nThe path includes $\\pi_i$  and excludes $\\pi_{sol}$. The term $\\epsilon_h$ is single-step-error associated with \n$\\mathit{h}$ during refinement.\n\n}\n\n\n\\subsubsection {Enhancement of A Predictive Model}\n\nIn Theorem~1, $h^e(\\pi_i)$ is an online approximated version of $h^*(\\pi_i)$. \nThis approximation uses the parent and the best child relationships to measure the step-error of each refinement in the path and \ncorrect it for the evaluations of further refinements. \n\n\n\nUsing Theorem~1 and Figure~1, we calculate the average-step-error associated with $h$, denoted by $\\epsilon^{avg}_{h}$,\nalong the path from $\\pi_i$ to $\\pi_{sol}$ as,\n\n\n\n\n\n", "itemtype": "equation", "pos": 21106, "prevtext": "\n\n\n\n\nUsing Eq.~\\eqref{eq1:test}, we derive (proof is in the Appendix) the following in a non-ideal case: \n\n\n\\subsubsection{Theorem 1.} \n\\emph\n{\nFor a given learned predictive model ($h$) and partial plan ($\\pi_i$) in Figure~1 which leads to the solution plan ($\\pi_{sol}$)\nafter certain refinement steps, the enhanced version of the predictive model ($h^e$) is,\n\n\n\n", "index": 5, "text": "\\begin{equation}\\label{eq2:test}\n\t\th^e(\\pi_i) \\ = \\ h(\\pi_{i}) \\ + \n\n\t\t\t\t\\sum_{\t\\substack{\\pi{'}~\\text{{from}} \\ \\pi_i \\rightsquigarrow \\pi_{sol}} } \\epsilon_{h(\\pi')}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"h^{e}(\\pi_{i})\\ =\\ h(\\pi_{i})\\ +\\par&#10;\\sum_{\\begin{subarray}{c}\\pi{{}^{\\prime}}%&#10;~{}\\text{{from}}\\ \\pi_{i}\\rightsquigarrow\\pi_{sol}\\end{subarray}}\\epsilon_{h(%&#10;\\pi^{\\prime})}\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mi>e</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\"><mtr><mtd><mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><mmultiscripts><mpadded width=\"+5pt\"><mtext>from</mtext></mpadded><mprescripts/><none/><mo>\u2032</mo></mmultiscripts><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>i</mi></msub></mrow><mo>\u219d</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mtd></mtr></mtable></munder><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c0</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\n\n\nRewriting Eq.~\\eqref{eq31:test},\n\n\n", "itemtype": "equation", "pos": 22088, "prevtext": "\nwhere $\\pi_i$ $\\rightsquigarrow$ $\\pi_{sol}$ is a path in Figure~1 which captures each partial plan ($\\pi'$) along \nthe path between $\\pi_i$ and $\\pi_{sol}$.\nThe path includes $\\pi_i$  and excludes $\\pi_{sol}$. The term $\\epsilon_h$ is single-step-error associated with \n$\\mathit{h}$ during refinement.\n\n}\n\n\n\\subsubsection {Enhancement of A Predictive Model}\n\nIn Theorem~1, $h^e(\\pi_i)$ is an online approximated version of $h^*(\\pi_i)$. \nThis approximation uses the parent and the best child relationships to measure the step-error of each refinement in the path and \ncorrect it for the evaluations of further refinements. \n\n\n\nUsing Theorem~1 and Figure~1, we calculate the average-step-error associated with $h$, denoted by $\\epsilon^{avg}_{h}$,\nalong the path from $\\pi_i$ to $\\pi_{sol}$ as,\n\n\n\n\n\n", "index": 7, "text": "\\begin{equation}\\label{eq31:test}\n\t\t\\epsilon_{h}^{avg} \\ = \\ \n\t\t\t\t\t {\\displaystyle \\sum_{\\substack{\\pi'~{\\mathit{from}} \\ \\pi_i \\rightsquigarrow \\pi_{sol}} \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\\epsilon_{h(\\pi')}} \\Big/ {h^e(\\pi_i)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\epsilon_{h}^{avg}\\ =\\ {\\displaystyle\\sum_{\\begin{subarray}{c}\\pi^{\\prime}~{}{%&#10;\\mathit{from}}\\ \\pi_{i}\\rightsquigarrow\\pi_{sol}\\end{subarray}}\\epsilon_{h(\\pi%&#10;^{\\prime})}}\\Big{/}{h^{e}(\\pi_{i})}\" display=\"block\"><mrow><mpadded width=\"+5pt\"><msubsup><mi>\u03f5</mi><mi>h</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>g</mi></mrow></msubsup></mpadded><mo rspace=\"7.5pt\">=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\"><mtr><mtd><mrow><mrow><mpadded width=\"+3.3pt\"><msup><mi>\u03c0</mi><mo>\u2032</mo></msup></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>\ud835\udc53\ud835\udc5f\ud835\udc5c\ud835\udc5a</mi></mpadded><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>i</mi></msub></mrow><mo>\u219d</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mtd></mtr></mtable></munder><mrow><mrow><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c0</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo mathsize=\"160%\" stretchy=\"false\">/</mo><msup><mi>h</mi><mi>e</mi></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\n\nUsing Eq.~\\eqref{eq32:test}, Eq.~~\\eqref{eq2:test} simplifies to,\n\n\n", "itemtype": "equation", "pos": 22354, "prevtext": "\n\n\nRewriting Eq.~\\eqref{eq31:test},\n\n\n", "index": 9, "text": "\\begin{equation}\\label{eq32:test}\n{\\displaystyle \\sum_{\\substack{\\pi'~{\\mathit{from}} \\ \\pi_i \\rightsquigarrow \\pi_{sol}}}\\epsilon_{h(\\pi')}}\n\t= \\epsilon_{h}^{avg} \\times {h^e(\\pi_i)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\displaystyle\\sum_{\\begin{subarray}{c}\\pi^{\\prime}~{}{\\mathit{from}}\\ \\pi_{i}%&#10;\\rightsquigarrow\\pi_{sol}\\end{subarray}}\\epsilon_{h(\\pi^{\\prime})}}=\\epsilon_{%&#10;h}^{avg}\\times{h^{e}(\\pi_{i})}\" display=\"block\"><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\"><mtr><mtd><mrow><mrow><mpadded width=\"+3.3pt\"><msup><mi>\u03c0</mi><mo>\u2032</mo></msup></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>\ud835\udc53\ud835\udc5f\ud835\udc5c\ud835\udc5a</mi></mpadded><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>i</mi></msub></mrow><mo>\u219d</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mtd></mtr></mtable></munder><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c0</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\u03f5</mi><mi>h</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>g</mi></mrow></msubsup><mo>\u00d7</mo><msup><mi>h</mi><mi>e</mi></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\n\n\nFurther simplification of Eq.~\\eqref{eq33:test} yields,\n\n\\begin{eqnarray}\\label{eq51:test}\n\t\t h^e(\\pi_{i}) =\n\t\t\t\t\t\\ {h(\\pi_{i})} \\Big / {(1 - \\epsilon_{h}^{avg})} \n\\end{eqnarray}  \n\nAnother possible expansion, using infinite geometric progression, of Eq.~\\eqref{eq51:test} would be,\n\\begin{eqnarray}\\label{eq51:test}\n\t\t h^e(\\pi_{i}) = \\ {h(\\pi_{i})} \\times \\sum_{i=0}^{\\infty}{(\\epsilon_{h}^{avg})}^i\n\\end{eqnarray}  \n\n\n\nWe use RegPOCL to test the effectiveness of $h^e(\\pi_{i})$ in the POCL framework, where it selects the best partial plan.\n\n\n\n\\section{Experiment Design}\n\n\nIn this section we describe the evaluation phase settings. \n\nThis includes\n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item the heuristics selected as features, and\n\\item the domains selected.\n\\end{inparaenum}\n\n\n\\subsection{Feature Heuristics for Learning} The features used for learning are non-temporal heuristics from the literature of \nPOCL planning. \n\nConsidering the applicability of some of the POCL heuristics in the literature~\\cite{younes2003vhpop,nguyen2001reviving}, \nwe select six different heuristic functions. \nSome of these heuristics are informed but their informativeness varies over different planning domains. \n\n\n\n\nOur aim is to learn a more informed combination from these individual heuristics. \nThe six heuristics are,\n\n\n\n\\subsubsection{G Value ($h_{{g\\text{-}val}}$)} \n\nThis returns the number of actions in a selected partial plan \n$\\pi$ not counting the two dummy actions ($a_0$ and $a_{\\infty}$). \nIt signifies how far the search has progressed from the starting state.\n\n\n\n\\subsubsection{Number of Open Conditions ($h_{{OC}}$)} \n\nThis is total number of unsupported causal links present in a partial plan,\n$h_{{OC}}(\\pi) = \\left| \\mathrm{OC} \\right|$~\\cite{nguyen2001reviving}.\n\n\n\n\\subsubsection{Additive Heuristic ($h_{{add}}$)} \nThe additive heuristic $h_{{add}}$~\\cite{haslum2000admissible}, \nadds up the steps required by each individual open goal. \nYounes and Simmons (2003) use an adapted version of additive heuristic in POCL planning for the first time.\n\n\n\n\\subsubsection{Additive Heuristic with Effort ($h_{{add,w}}$)}  \nThe estimate is similar to $h_{add}$ but it considers the cost of an action as the \nnumber of preconditions of that action, plus the {linking cost} \\textbf{1} if the action supports any unsupported causal \nlink~\\cite{younes2003vhpop}. We call it $h_{{add,w}}$ as its notation is not used earlier. Here, $w$ signifies the extra work\nrequired.\n\n\n\\subsubsection{Accounting for Positive Interaction ($h_{{add}}^{r}$)}\nThis returns an estimate which takes into account the positive interactions between subgoals \nwhile ignoring the negative interactions. \nThis is represented as $h_{{add}}^{r}$ that is a variant of $h_{add}$~\\cite{younes2003vhpop}. \n\n\n\n\\subsubsection{Accounting for Positive Interaction with Effort ($h_{{add,w}}^{r}$)}\n\nThis is similar to the above heuristic which considers the total effort required~\\cite{younes2003vhpop}. \nA standard notation of this heuristic is also not used in the literature.\n\n\n\n\n\\subsection{Domains Selected} \n\n\n\nWe consider the following domains: \n\n\nLogistics and Gripper from IPC~1, Logistics and Elevator from IPC~2, Rovers and Zenotravel from IPC~3,\nand Rovers from IPC~5. \n\nIn our experiments we do not consider other domains from these competitions because either the state-of-the-art \nheuristics are not able\nto create enough training instances for learning, or {RegPOCL} does not support the domain definition language features. \n\nIPC~4 domains are not selected since the planner is not able to generate \nenough instances to initiate offline learning. \nThe domains from IPC~6 and later are not supported by {RegPOCL} \nbecause the representations use action costs, fluents, and hard and soft constraints.\nSome of them can be included by some preprocessing like removal of actions with cost from the domain description files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each selected domain, we consider problems that are represented in STRIPS style. \nWe select small sized problems for learning and test the learned predictive models over large sized problems in the same domain. \n\nWe have a total of 109 small sized problems from the selected domains. \n\nThe last four feature heuristics from the previous subsection have been used for calculating targets in each domain. \nThis means that we generate four different datasets in each selected domain from which best two are selected.\n\nWe choose satisficing track problems for generating training instances.\n\nFor the training set preparation, we fix a time limit of 3 minutes and \nan upper limit of 500,000 on the node generation.\n\n\n\n\nWe generate a few thousand training instances except for the Zenotravel domain where the total instances are 950. \n\nTo avoid overfitting, we pick training instances between 250 to 350 from the larger training sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Selected Learning Approaches}\n\nIn this section, we discuss in brief a procedure for feature selection in each dataset for \ntraining regression models,\nand different regression techniques with their references.\n\n\n\\subsection{Feature Selection} \n\n\nIn general, the training sets contain irrelevant or redundant attributes\n(out of the six selected heuristics).\n\nTo reduce the training effort and increase the efficiency of our planner,\nwe discard them from the training set.\n\nThe planner is bound to calculate all the selected features at each stage of refinement. \n\nThe correlation based feature selection technique~\\cite{hall1999correlation} is used to find the correlated features.\n\n\n\n\n\n\n\\begin{table*}[t]\n\\centering\n\n\n\\scalebox{0.70}\n{ \n\t\n\n\n\\begin{tabular}\n\n{ @{} l | r || r r | r r || r r | r r || r r r r || r r r r ||r @{}}\n\n\\hline\n\n\t\\multirow{3}{*}{\\textbf{Domain}} &\n\t\\multirow{3}{*}{\\textbf{\\#}} &\n\t\\multicolumn{8}{c||}{\\textbf{POCL Heuristics}} &\n\t\\multicolumn{8}{c||}{\\textbf{Evaluation using Fast Downward (FD)}} &\n\t\\multirow{3}{*}{\\textbf{LAMA}} \n\t\\\\ \\cline{3-18}\n\n\t&\n\t&\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c||}{\\textbf{Via learning approaches}} &\n\t\\multicolumn{4}{c||}{\\textbf{Lazy}} &\n\t\\multicolumn{4}{c||}{\\textbf{Eager}} &\n\t\\\\ \\cline{3-18} \n\t\n\t & &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\t\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&20\t\t&16\t\t&\\textbf{20}\t\t&1\t\t&1\t\t&\\textbf{20}\t\t&\\textbf{20}\t&\\textbf{20}\t\t&\\textbf{20}\t\t\n\t\t\t\t\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \t&\\textbf{20}\t\t\n\t\t\t\t\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \\\\\n\t\n\tRovers-3 \t&20\t\t&19\t\t&19\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{20}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t&18\t\t&17\t\t&14\t\t&\\textbf{20}\t\t&19\t\t&18\t\t&15\t\t&\\textbf{20}\t\t&\\textbf{20}\t \\\\\n\t\n\tRovers-5\t&40\t\t&28\t\t&31\t\t&32\t\t&36\t\t&${}^{\\text{\\small +}}$39\t\t&${}^{\\text{\\small +}}$39\t\n\t\t\t\t\t\t&${}^{\\text{\\small +}}$36\t\t&${}^{\\text{\\small +}}$39\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t&22\t\t&25\t\t&15\t\t&28\t\t&24\t\t&29\t\t&17\t\t&30\t\t&\\textbf{40} \\\\\n\n\t\\cline {1-19} \t\t\t \n\n    &\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{r,l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} & \\\\ \n\t\n\t\n\t\\hline\n\t\n\tLogistics-1\t&35 \t&25\t\t&1\t\t&32\t\t&28\t\t&${}^{\\text{\\small +}}$32\t\t&${}^{\\text{\\small +}}$32 \t&22\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$32\t\n\t\t\t\t\t\t\t&25\t\t&34\t\t&21\t\t&28\t\t&28\t\t&34\t\t&16\t\t&25\t\t&\\textbf{35} \\\\\n\t\n\tElevator-2\t\t&150\t&148\t&14\t\t&\\textbf{150}\t&58\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\n\t\t\t\t\t\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t\\\\\n\t\n\tLogistics-2\t&40\t\t&36\t\t&12\t\t&36\t\t&34\t\t&\\textbf{40}\t&\\textbf{40}\t&\\textbf{40}\t&\\textbf{40}\n\t\t\t\t\t\t\t&36\t\t&36\t\t&35\t\t&36\t\t&36\t\t&36\t\t&36\t\t&36\t\t&\\textbf{40}\t\\\\\n\t\n\tZenotravel-3\t&20\t\t&5\t\t&4\t\t&9\t\t&10\t\t&${}^{\\text{\\small +}}$16\t\t&${}^{\\text{\\small +}}$16\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$16\t\t&${}^{\\text{\\small +}}$16\n\t\t\t\t\t\t\t&\\textbf{20}\t\t&\\textbf{20} \t&17\t\t&\\textbf{20}\t\n\t\t\t\t\t\t\t&\\textbf{20}\t\t&\\textbf{20} \t&16\t\t&\\textbf{20}\t&\\textbf{20} \\\\\n\n\n\\hline\t\t\t \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nNumber of solved problems using each heuristic. \n\\textbf{\\#} is the number of problems in each domain.\nState-of-the-art POCL heuristics are compared with the learned ones in the left half. \n\nThe state-based heuristics FF, CEA, and LM-Cut \nand their combination using MHFS strategy are also compared with POCL heuristics.\n\nThe last column captures the performance of LAMA11. \n\nBest results are shown in \\textbf{bold}, \nand a number with ``+'' mark (\\emph{e.g.}${}^{\\text{\\small +}}$36) shows competitive performance by \nthe learned models and their enhancements over each base heuristic. \n\n\nSimilar representations are followed in other tables.\n\n}\n\n\\end{table*}\n\n\n\n\\subsection{Regression Techniques}\nWe use the following regression techniques to learn predictive models. These techniques have been applied in planning \nfor learning in recent years~\\cite{samadi2008learning,thayer2011learning,us2013learning}.\n\n\n\\subsubsection{Linear Regression (LR)}\n\nThe regression model learns a linear function that minimizes the sum of squared \nerror over the training instances~\\cite{bishop2006pattern}. \n\n\n\\subsubsection{M5P}\n\nM5P gives more flexibility than LR due to its nature of \ncapturing non linear relationships.\nM5P technique learns a regression tree~\\cite{quinlan1992learning} that approximates the class value. \n\n\n\\subsubsection{M5Rules}\n\nSimilar to M5P but generates rules instead of modeling regression trees~\\cite{quinlan1992learning}.\n\n\n\\subsubsection{Least Median Squared (LMS)}\n\nLMS is similar to LR with median squared error.\nFunctions are generated from subsamples of data with least squared error function. \nUsually a model with lowest median squared error is selected~\\cite{rousseeuw2005robust}.\n\n\n\\subsubsection{Multilayer Perceptron (MLP)} \n\nMLP can learn more complex relationships compared to the other four \nregression techniques~\\cite{bishop2006pattern}.\n\n\n\n\nThe techniques discussed above are used to learn models through WEKA~\\cite{Hall2009} using \na 10-fold cross-validation in each domain. \n\nA regression technique called SVMreg that implements support vector machine for regression purposes, \nis not used in this work due to some technical difficulty. \n\nHowever, it has not much influenced planning processes in the past~\\cite{us2013learning}.\n\n\n\n\n\\section{Experimental Evaluation} \n\n\nWe use MC-Loc and MW-Loc~\\cite{younes2003vhpop} as flaw selecting heuristics for refining a partial plan.\n\n\n\nThey give higher preference to the local flaws present in the partial plan.  \n\nWe employ Greedy Best First Search algorithm for selecting the next partial plan for refinement.\n\n\n\n\n\n\\subsection{Environment}\n\nWe perform the experiments on Intel Core~2 Quad with 2.83~GHz 64-bit processor and 4GB of RAM. \nTo evaluate the effectiveness of learned models and to correct the single-step-error associated with the models, \na time limit of 15 minutes and a node generation limit of 1 million is used.\n\n\n\n\\subsection{Evaluations}\n\nWe use RegPOCL to compare the performances of the offline predictive models $h^l$, their corresponding enhanced models\n$h^{l,e}$ and the last four base features. \n\n\n\n\n\n\n\n\n\n\nThese are also compared with some of the recent effective state-space based heuristics and approaches that are introduced later.\n\n\n\n\n\n\nWe exclude the first two base features from comparison since they are weak heuristics and RegPOCL \ndoes not solve sufficient problems using them.\n\nHowever, they are useful while working jointly with other informed heuristics. \n\nNext, we discuss the observations made during the training phase. \n\n\n\\subsubsection{Training} \n\nUsing Algorithm~1, we prepare datasets and learn different predictive models by applying the various regression techniques \ndiscussed earlier. \n\nWe select each of the last four features to solve a set of problems.\nThe target value is the plan length found by RegPOCL using the base features.  \nThe dataset preparation phase took less than two hours on an average in each domain, for each of the four features.\n\nOnce we have enough instances, we begin the training process. \n\nIn each domain, the best two datasets out of four with sufficient training points\nis selected for model training by visualizing the distribution of training points using WEKA.   \n\nNote that, in Figure~1, different heuristics for calculating target values will prefer different paths. \nTherefore, the four base features will generate four different datasets. \n\n\nThe attribute selection strategy allows us to select a subset of attributes in the training set by removing\ncorrelated and redundant features.\n\nWe learn a total of 70 (7 domains $\\times$ 2 datasets $\\times$ 5 regression techniques) regression models. \n\nThe training phase took 20ms (milliseconds) using LR, 270ms using LMS, 600ms using MLP, 82ms  \nusing M5Rule, and  58ms using M5P on an average per model.  \n\nAll the learned models have high accuracy but LR is the fastest, followed by M5P and M5Rule.\nNext, we test these models on different benchmarks.\n\n\n\n\n\\subsubsection{Testing}\n\nWe test the effectiveness of our approaches by selecting partial plans $(\\pi)$ for refinement using RegPOCL. \nWe assume that an informed heuristic leads to minimal possible refinements needed for $\\pi$.\n\nNext, for the comparison we compute score\\footnote{\\url{https://helios.hud.ac.uk/scommv/IPC-14/}} \nas in IPC for satisficing track problems.\n\nThe better score on each standard signifies the better performance.\nWe compare the performance of the learned models with the selected base features \n$h_{add}$, $h_{add,w}$, $h_{add}^{r}$, and $h_{add,w}^{r}$. \n\n\nThe comparison is done on the basis of \n\\begin{inparaenum}[(\\itshape i.\\upshape)]\n\\item the number of solved problems, and \n\\item the score obtained on plan quality, execution time, nodes (partial plan) visited, and makespan quality.\n\\end{inparaenum}\n\n\n\n\nFor example, the offline learned model $h_{add}^{r,l}$ is learned on a dataset prepared using $h_{add}^{r}$. \nIn other words, RegPOCL uses $h_{add}^{r}$ for calculating the target values in the dataset.\n\n$h_{add}^{r,l}$ can be enhanced to $h_{add}^{r,l,e}$ \nusing the online heuristic adjustment approach which is expected to be more informed than $h_{add}^{r,l}$. \n\n\nIt is similar for other learned heuristics.\n\nThese models are applied in the POCL framework for selecting the most suitable partial plan,\nfollowed by the heuristic MW-Loc~\\cite{younes2003vhpop} for selecting the most adverse flaw in it .\n\n\n\nWe also compare our approaches with state-space based approaches on the basis of the number of problems solved, and \nscore obtained on plan quality and total execution time. \n\nWe select fast forward heuristic (FF)~\\cite{hoffmann2001ff}, context-enhanced additive heuristic (CEA)~\\cite{helmert2008unifying},\nand landmark-cut heuristic (LM-Cut)~\\cite{helmert2011lm}. \n\nWe also use these heuristics together by applying them in multi-heuristic first solution strategy (MHFS)~\\cite{roger2010more}. \nIn general, the strategy performs better with alternating usage of different heuristics instead of combining them. \n\nWe also compare the effectiveness of our techniques with \nLAMA11~\\cite{richter2011lama}; the winner of IPC-2011 in the sequential satisficing track. \nLAMA11 applies FF and LM-Count~\\cite{richter2008landmarks} heuristics together using multi-queue search.\n\n\nWe set a 20 minute time limit while evaluating LAMA11 over these domains,\nsince it has an internal time limit of 5 minutes for the invariant synthesis part of translator.\n\n\n\n\n\n\n\\begin{table*}[t]\n\\centering\n\n\n\n\\scalebox{0.7}\n{ \n\t\n\n\n\\begin{tabular}\n\n{ @{} l || r r | r r || r r | r r || r r r r || r r r r ||r @{}}\n\n\\hline\n\n\t\\multirow{3}{*}{\\textbf{Domain}} &\n\t\\multicolumn{8}{c||}{\\textbf{POCL Heuristics}} &\n\t\\multicolumn{8}{c||}{\\textbf{Evaluation using Fast Downward (FD)}} &\n\t\\multirow{3}{*}{\\textbf{LAMA}} \n\t\\\\ \\cline{2-17}\n\n\t&\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c||}{\\textbf{Via learning approaches}} &\n\t\\multicolumn{4}{c||}{\\textbf{Lazy}} &\n\t\\multicolumn{4}{c||}{\\textbf{Eager}} & \n\t\\\\ \\cline{2-17} \n\t\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\t\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&16.0\t&14.9\t&0.7\t&0.7\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t\n\t\t\t\t\t\t&15.45 \t\t&14.9\t\t\t&15.5 \t\t&15.5\t\t\n\t\t\t\t\t\t&15.5\t\t&14.9\t\t\t&15.5 \t&15.5\t&\\textbf{20.0} \\\\\n\t\n\tRovers-3 \t&17.3\t&16.2\t&18.1\t&16.9\t\t&17.8\t\t&${}^{\\text{\\small +}}$18.6\t&17.8 &${}^{\\text{\\small +}}$18.6\n\t\t\t\t\t\t&17.1\t&15.9\t&12.5\t&18.8\t\t&18.1\t\t&16.9\t\t&13.9\t&19.0\t\t&\\textbf{19.8}\t \\\\\n\t\n\tRovers-5\t&25.7\t\t&26.3\t\t&28.0\t&30.2\t\t&${}^{\\text{\\small +}}$33.1\t&${}^{\\text{\\small +}}$36.3\t&30.1\t\t\n\t\t\t\t\t\t&${}^{\\text{\\small +}}$33.6\t&20.8\t\t&23.4\t&13.5\t\t&26.0\t\t&22.7\t&27.3\t&15.7\t&28.3\t\t\n\t\t\t\t\t\t&\\textbf{39.8} \\\\\n\n\t\n\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{r,l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} & \\\\ \n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&24.3\t&0.8\t&31.2\t&26.1\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$31.2\t&${}^{\\text{\\small +}}$31.2\t&20.9\t\t&\\textbf{32.0}\n\t\t\t\t\t\t\t&23.7\t\t&30.3\t\t&19.7\t\t&27.4\t\t&27.0\t\t\t&30.8\t\t&15.1\t\t&24.6\t\t&30.8 \\\\\n\t\n\tElevator-2\t\t&142.8\t&11.8\t&144.9\t&49.7\t\t&142.7\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$144.9\t\t&142.7\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$144.9\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t&117.1\t&112.0\t&117.1\t&116.9\n\t\t\t\t\t\t\t&144.1\t&136.0\t&144.2\t&141.0\t&\\textbf{148.2}\t\\\\\n\t\n\tLogistics-2\t&33.8\t\t&10.7\t\t&34.9\t\t&30.7\t\t&\\textbf{38.1}\t\t&${}^{\\text{\\small +}}$36.3\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{38.1}\t\t&${}^{\\text{\\small +}}$36.3\n\t\t\t\t\t\t\t&33.72\t\t&29.5\t\t&32.4\t\t&33.3\t\t&34.1\t\t&30\t\t&33.7\t\t&35.6\t\t\t&37.8\t\\\\\n\t\n\tZenotravel-3\t&4.6\t\t&3.7\t\t&8.5\t\t&8.8\t\t&${}^{\\text{\\small +}}$13.5\t&${}^{\\text{\\small +}}$13.7\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$13.5 \t&${}^{\\text{\\small +}}$13.7\n\t\t\t\t\t\t\t&17.3 \t&17.7\t\t&14.7 \t&18.6\t\n\t\t\t\t\t\t\t&18.1\t\t&14.3 \t&16\t\t&18.8\t&\\textbf{19.2} \\\\\n\t\n\t\n\t\\hline \\hline\n\t\\multicolumn{1}{@{}c||}{\\textbf{Time Score}}\n\t\t\t\t\t\t\t&204.4\t\t&76.5\t\t&197.7\t \t&148.6\t\t&${}^{\\text{\\small +}}$272.9\t\t&${}^{\\text{\\small +}}$260.8\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$258.6\t\t&${}^{\\text{\\small +}}$264.3\t\t&255.6\t\n\t\t\t\t\t\t\t&\\textbf{280.0}\t\t&228.0\t\t&233.8\t\t&259.6\t\t&271.3\t\t&223.0\t\t&222.5\t\t&137 \\\\\t\t\t\t\t\n\n\n\\hline\t\t\t \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nScores on plan quality and overall time. We compare state-of-the-art POCL heuristics with learned ones. \nThe effectiveness of the POCL heuristics is compared with some latest state based approaches. \nThe last row demonstrates the overall time score of each heuristic. \n\nThe numerical representations and column details are similar to Table~1.\n}\n\n\\end{table*}\n\n\n\n\nAll the state-based approaches are evaluated using Greedy Best First Search algorithm in \nthe fast downward planning system~\\cite{helmert2006fast}.\nWe use ``eager'' and ``greedy'' types of evaluations with no preferred operators.\n\n\n\n\n\nThe regression models selected in the chosen domains are trained using,\n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item M5P in Gripper-1 and Elevator-2, \n\\item LR in Rovers-3, Rovers-5, Logistics-2, and Zenotravel-3, and\n\\item M5Rule in Logistics-1.\n\\end{inparaenum}\n\n\n\nIn Table~1, we compare \n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item the base features, \n\\item offline learned models and their enhanced versions\n\\item state-space based heuristics FF, CEA, and LM-Cut, and \n\\item the strategies used in MHFS, and LAMA11, \n\\end{inparaenum}\non the basis of number of problems solved. \n\n\n\nIn this table, RegPOCL solves equal number of problems as LAMA11\nin Gripper-1, Rovers-3, Elevator-2, and Logistics-2 using each of learned heuristics and their enhanced versions.\n\nThe base features have performed well in some domains but are not consistent overall. \n\nIn Rovers-5, our approaches solved 1 problem less than LAMA11, \nbut they beat other state-space based competitors comprehensively. \n\nAlso, each learned model has performed better than all the base features. \n\nFor Logistics-2, we are competitive with LAMA11 and  solve at least 4 more problems than other good heuristics \nlike CEA and LM-Cut.\n\nIn Zenotravel-3, RegPOCL solved 6 problems more by applying our approaches but loses to the state-based competitors.\n\n\nOur second approach improves the performance of the learned models in Rovers-5 by solving 3 more problems, \nand in Logistics-1 where it solves 10 more problems.  \n\nThis approach could not increase the coverage in other domains. \nLAMA11 wins on the basis of the total number of problems solved in each domain.\n\n\n\nIn Table~2, we compare the score obtained on plan quality by each of the \nbase features, \nlearned models with their corresponding enhancements, and\nstate-space based heuristics and techniques. \n\n\n\n\\begin{table}[t]\n\\centering\n\n\n\\scalebox{0.65}\n{ \n\t\n\n\n\\begin{tabular}\n\n{@{} l || r r | r r || r r | r r @{}}\n\n\\hline\n\n\n\n\n\n\t\\multirow{2}{*}{\\textbf{Domain}} &\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c}{\\textbf{Via learning approaches}} \n\t\\\\ \\cline{2-9} \n\t\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c@{}}{$h_{{add,w}}^{{l,e}}$} \n\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&7.0\t&14.0\t&0.0\t&0.0\t&${}^{\\text{\\small +}}$18.0  &\\textbf{19.7}\t&${}^{\\text{\\small +}}$18.0\t&\\textbf{19.7} \\\\\n\t\n\tRovers-3 \t&8.6\t&8.3\t&12.6 \t&\\textbf{19.0}\t\t&13.3\t&13.6 \t&13.3\t&13.6 \\\\\n\t\n\tRovers-5\t&8.2\t&14.7\t&13.5\t&30.9\t\t&\\textbf{31.4}\t\t&\\textbf{31.4}\t\t&24.7 &\\textbf{31.4}\t\\\\\n\n\t\n\t\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l,e}}$} \\\\\t\n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&5.1\t&0.6\t&16.5\t&21.6\t\n\t\t\t\t\t\t\t&\\textbf{29.9}\t\t&\\textbf{29.9}\t\t&{20.7}\t&\\textbf{29.9} \\\\\n\t\n\tElevator-2\t\t&41.4\t&8.2\t&58.9\t&39.7\t\t&\\textbf{145.0} \t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$135.0\t&\\textbf{145.0}\t&${}^{\\text{\\small +}}$135.0\t\\\\\n\t\n\tLogistics-2\t&24.5\t&6.5\t&23.4\t&26.7\t\t&${}^{\\text{\\small +}}$33.4\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{35.3}\t &${}^{\\text{\\small +}}$33.4\t&\\textbf{35.3}   \\\\\n\t\n\tZenotravel-3\t&0.0\t&1.0\t&2.4\t&7.7 \t&7.4\t&\\textbf{11.07}\t&7.4\t&\\textbf{11.07} \\\\\n\t\n\t\n\t\\hline \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nResults of refinement score (the number nodes visited) of RegPOCL using each heuristic. \nWe compare state-of-the-art heuristics with the learned models ($h^{l}$) and their further enhancements ($h^{l,e}$). \nThe best results are in \\textbf{bold}.\n}\n\n\n\\end{table}\n\n\n\n\\begin{table}[t]\n\\centering\n\n\\scalebox{0.64}\n{ \n\t\n\n\n\\begin{tabular}\n\n{@{}l || r r | r r || r r | r r@{}}\n\n\\hline\n\n\n\n\n\n\n\n\n\n\t\n\t\\multirow{1}{*}{\\textbf{Domain}} &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c@{}}{$h_{{add,w}}^{{l,e}}$} \n\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&16.0\t&9.8\t&0.5\t&0.5\t\t&\\textbf{20.0}  &\\textbf{20.0}\t&\\textbf{20.0}\t&\\textbf{20.0} \\\\\n\t\n\tRovers-3 \t&\\textbf{17.9}\t&15.5\t&17.8\t&15.8\t\t&17.4\t\t&{17.7} \t&17.4\t\t&{17.7} \\\\\n\t\n\tRovers-5\t&26.1\t&24.2\t&28.4\t&26.5\t\t&\\textbf{30.5}\t\t&${}^{\\text{\\small +}}$29.8\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$27.6\t&\\textbf{30.5} \\\\\n\t\n\t\n\t\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l,e}}$}  \\\\\t\n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&21.8\t&0.9\t&25.2\t&20.8\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$30.4\t&${}^{\\text{\\small +}}$30.4\t&17.6\t&\\textbf{30.5} \\\\\n\t\n\tElevator-2\t\t&147.1\t&11.7\t&\\textbf{149.1}\t&50.5\t\t&\t\\textbf{149.1}\t\n\t\t\t\t\t\t\t&146.8\t\t&\\textbf{149.1} &\t146.8\t\\\\\n\t\n\tLogistics-2\t&26.3\t&7.0\t&33.0\t&19.3\t&${}^{\\text{\\small +}}$33.5\t&\\textbf{35.4}\t \n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$33.4\t&\\textbf{35.4}   \\\\\n\t\n\tZenotravel-3\t&3.7\t&2.7\t&7.5\t&7.7\t\t&${}^{\\text{\\small +}}$12.1\n\t\t\t\t\t\t\t\t\t&\\textbf{13.5}\t\t&${}^{\\text{\\small +}}$12.1\t\t&\\textbf{13.5} \\\\\n\t\n\t\n\t\\hline \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nMakespan quality score of the heuristics. Columns are identical to corresponding ones in Table~3.\n}\n\n\n\\end{table}\n\n\n\n\nLAMA11 is an anytime planner which gives solution close to the optimal but takes more time to compute \nshorter plans.\n\nFor Gripper-1 and Logistics-2, RegPOCL using the learned heuristics solves equal number of problems but finds shorter \nplans compared to LAMA11. \n\n\nIn Logistics-1, RegPOCL lost to LAMA11 by 3 problems in the number of problems solved, \nbut obtained a higher score \nin the other problems it solved as it produces shorter plans \nusing $h_{add,w}^{r,l,e}$, $h_{add}^{r,l}$, and $h_{add}^{r,l,e}$.\n\nThe effectiveness of $h_{add,w}^{r,l,e}$ wins in this domain with best plan quality. \n$h_{add,w}^{r,l,e}$ also increases the plan quality score from 20.9 that is obtained by $h_{add,w}^{r,l}$ to 32.0\nusing our online error correcting strategy. \n\n\n\nHowever, in Logistics-2, the performance has decreased after further enhancements \nof $h_{add}^{r,l}$ and $h_{add,w}^{r,l}$.\n\n\n\nIn general, the performance of RegPOCL using either of the offline learned models and their enhancements \nis often better than that using the base features.\nIn most of the cases, the online error adjustment approach has further enhanced the performance of these learned models. \n\n\nThe last row of Table~2 gives the score obtained on total time taken by the process. \nIf a planner takes less than 1 second for solving a problem then it gets full score. \nOn the total time score the winner is CEA with ``lazy'' evaluation.\n\nThe learned models and their enhanced versions \nhave obtained better scores than other competitors except CEA. \nThese scores are very close to the winning score and almost twice that of LAMA11. \n\n\nIn Table~3, we compare the score obtained on the number of nodes RegPOCL visits for solving the planning problems.\nThis is obtained for the base features, the learned heuristics, and their enhanced versions. \n\n\nThe models obtained after offline learning are more informed toward goals and refines fewer partial plans. \nThe score obtained by the learned models is further increased by a good factor in Zenotravel-3 \nusing the error correcting approach. \n\nFor Elevator-2, the error correcting approach has shown some negative effect which continues in Table~4 too. \n\nIn Table~4, we demonstrate the score obtained on the makespan quality. \nHigher score signifies smaller makespan and more flexibility in the generated solution plan.\n\nIn Elevator-2 and Rovers-5, \nthe scores of $h_{add}^l$ and $h_{add}^{r,l}$ have decreased due to the negative effects of our error adjustment approach,\nwhile the score obtained by $h_{add,w}^{r,l,e}$ is almost 1.5 times the score of $h_{add,w}^{r,l}$ in Logistics-1. \n\nIn general, the offline learned models have generated more flexible plans with \nshorter makespan than the base features. \n\nThese qualities are further improved using the enhanced versions of these models.\n\n\n\\section{Discussion}\n\nWe have already discussed the advantages of our approaches but they also have limitations. \nIn our offline approach,\nwe are bound to do some poor generalization while learning heuristics.\n\n\n\n\n\n\n\nCurrent literature supports the idea of selecting a large feature set for more accurate learning~\\cite{RobertsHWd08}. \nAccuracy can also be improved using an empirical performance model of all components of a portfolio to decide \nwhich component to pick next~\\cite{FawcettVH0HL14}.\n\n\nIn our work, a large feature set may have some drawbacks. For example, computing the features at each refinement step during \nthe planning process is computationally expensive.\n\n\n\nThe online error adjustment approach could also perform poorly in certain domains. \n\nIn Figure~1, if the orientation of objects in a domain is such that $h(\\pi_{i+1})$ is larger than $h(\\pi_{i})$\nthen $\\epsilon_{h({\\pi_{i})}}$ may not be accurate. The inaccuracy in $\\epsilon_{h({\\pi_{i})}}$ is compounded \nif the above condition holds at the beginning of the planning process.\n\n\nThis results in an inaccurate $\\epsilon_{h}^{avg}$ value, leading to wrong selection of the partial plan to refine next. \nConsequently, the planner may end up finding longer and less flexible plans. \n\n\nAnother limitation is that \na refinement may change the existing priorities of partial plans in the set due to the single-step-error adjustment.\nConsidering the time factor, we avoid changing the decided priorities of those partial plans.\nThis may also lead to inaccuracy in $\\epsilon_{h}^{avg}$.\n\n\n\n\n\n\n\n\n\n\n\n\nOur approaches do not utilize the advantage of strategies like alternation queue, and \ncandidate selection using concept of pareto optimality~\\cite{roger2010more}. \n\n\n\n\n\n\n\n\n\nRecently, the planning community has tried coming up with effective portfolios of heuristics or planners. \nThe techniques of generating good portfolios are not new to theoretical machine learning.\n\nA follow up work done in the past is \n\ncombining multiple heuristics online~\\cite{StreeterGS07}. \n\nOne could form a portfolio of different algorithms to reduce the total makespan for a set of jobs \nto solve~\\cite{StreeterS08}.\n\nThe authors provide a bound on the performance of the portfolio approaches. \nFor example, an execution of a greedy schedule of algorithms cannot exceed four times the optimal schedule. \n\n\n\n\n\n\n\n\nIn planning, a sequential portfolio of planners or heuristics aims to optimize the performance metrics. \n\nIn general, such configurations automatically generate sequential orderings of best planning algorithms. \nIn the portfolio the participants are allotted some timestamp to participate in solving problems in the ordering.\n\n\nA similar approach is used in ~\\cite{SeippSHH15}. \nThe authors outline their procedure for optimal and satisficing planning.\n\nThe procedure used in this work starts with a set of planning algorithms and a time bound.\nIt uses another procedure \\emph{OPTIMIZE} that focuses on the marginal improvements of the performance.\n\nHere, the quality of the portfolio is bounded by $\\mathrm{(1-(1/e))~\\times}$~\\sc {opt}, \\normalfont and \nthe running time cannot exceed {4}~\\sc{opt}. \\normalfont\n\n\nThe components can be allowed to act in a round-robin fashion~\\cite{gerevini2014planning}.\n\n\n\n\\textcolor{black}\n{\nThe state-of-the-art planners exhibit variations in their runtime for a given problem instance,\nso no planner always dominates over others. \n\nA good approach would be to select a planner for a given instance by looking at its processing time.\nThis is done by building an empirical performance model (EPM) for each planner.\nEPM is derived from sets of planning problems and performance observation.\nIt predicts whether the planner could solve a given instance~\\cite{FawcettVH0HL14}.  \nThe authors consider a large set of instance features and \nshow that the runtime predictor is often superior to the individual planners. \n\n\n\n\n\n\n}\n\n\nPerformance wise sorting of components in a portfolio is also possible~\\cite{NunezBL15}.\nThe portfolio is sorted\nsuch that the probability of the performance of that portfolio is maximum at any time.\nExperiments show that performance of a greedy strategy can be enhanced to near optimal over time.\n\n\n\n\n\n\n\n\nThe last two paragraphs cover recent literature in brief which explain previous strategies of combining different base methods.\nThe literature shows that they have performed well over different benchmarks. \n\nOur current settings do not capture any such ideas for combining different components of heuristics.  \n\nA direct comparison with any of the above mentioned works is therefore out of scope for our current work.\nThis is because, we are more concerned about working with unit cost based POCL heuristics in isolation.\n\n\nOn the other hand, we suspect that many of these strategies, in some adapted form, \nwould likely be beneficial in the POCL framework.\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Summary and Future Work}\n\nWe demonstrate the use of different regression models to combine different heuristic values to arrive at consistently better \nestimates over a range of planning domains and problems.\n\n\n\nWe extend some recent attempts to learn combinations of heuristic functions in \nstate-space based planning to POCL planning.\n\nWe also show that the learned models can be further enhanced by an online error correction approach. \n\n\n\n\nIn future we intend to explore online learning further, and continue our experiments with combining heuristic functions. \nWe also aim to explore the use of an optimizing planner in tandem with bootstrapping methods. \nApart from these, we will be giving a complete generalization of our current learning approaches for temporal planning and \nplanning with deadlines.\n\n\n\n\n\n\n\\bibliography{formatting-instructions-latex}\n\\bibliographystyle{aaai}\n\n\n\\section*{Appendix}\n\n\\subsection*{Proof of Theorem~1}\n\t\t\n\n\n\\subsubsection{Theorem 1.} \n\\emph\n{\nFor a given learned predictive model ($h$) and partial plan ($\\pi_i$) in Figure~1 which leads to the solution plan ($\\pi_{sol}$)\nafter certain refinement steps, the enhanced version of the predictive model ($h^e$) is,\n\n\n\n", "itemtype": "equation", "pos": 22621, "prevtext": "\n\nUsing Eq.~\\eqref{eq32:test}, Eq.~~\\eqref{eq2:test} simplifies to,\n\n\n", "index": 11, "text": "\\begin{equation}\\label{eq33:test}\n\t\th^e(\\pi_i) \\ = \\ h(\\pi_{i}) + \\epsilon_{h}^{avg} \\times {h^e(\\pi_i)}\t\t\t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"h^{e}(\\pi_{i})\\ =\\ h(\\pi_{i})+\\epsilon_{h}^{avg}\\times{h^{e}(\\pi_{i})}\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mi>e</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><msubsup><mi>\u03f5</mi><mi>h</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>g</mi></mrow></msubsup><mo>\u00d7</mo><msup><mi>h</mi><mi>e</mi></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07483.tex", "nexttext": "\nwhere $\\pi_i$ $\\rightsquigarrow$ $\\pi_{sol}$ is a path in Figure~1 which captures each partial plan ($\\pi'$) along \nthe path between $\\pi_i$ and $\\pi_{sol}$.\nThe path includes $\\pi_i$  and excludes the $\\pi_{sol}$. The term $\\epsilon_h$ is single-step-error associated with \n$\\mathit{h}$ during refinement.\n\n}\n\n\n\n\n\\begin{Proof} \n\n\nWe use the principle of mathematical induction to prove this theorem.\n\n\\noindent $\\mathit{\\bold{Basis:}}$ We assume that $\\pi_i$ needs only one refinement to become $\\pi_{sol}$ that\nwould also be $\\pi_i$'s best child.\n\nHere, the best child always keeps the lowest estimate of requirement of new actions for its refinement among its siblings.\n\nOne possible refinement of $\\pi_i$ is, $\\pi_i \\xrightarrow{R_i} \\pi_{sol}$. \nUsing Eq.~\\eqref{eq2a:test}, we say,\n\n\\begin{eqnarray}\\label{aeq2:test}\t\n\t\th^e(\\pi_i) \\ &=& \\ h(\\pi_{i}) \\ +  \\ \\epsilon_{h(\\pi_i)}\n\\end{eqnarray}\t\n\nThe term $\\epsilon_{h(\\pi_i)}$ is the single-step-error associated with \\emph{h} that estimates the total effort required for\n$\\pi_i$ to refine it completely. For unit cost refinements (cost($R_i$) = 1), $\\epsilon_{h(\\pi_i)}$ is computed as,\n\\begin{eqnarray}\\label{aeq3:test}\n\t\\epsilon_{h(\\pi_i)} = (cost(R_i) + h(\\pi_{i+1})) - h(\\pi_{i})\n\\end{eqnarray}\t\n\nHere, the partial plan $\\pi_{i+1}$ is also the $\\pi_{sol}$, therefore h($\\pi_{i+1}$) = 0. By using Eq.~\\eqref{aeq2:test}\nand Eq.~\\eqref{aeq3:test} together, we get $h^e(\\pi_i)$ = 1. Therefore, the base step holds.\n\n\n\n\n\n\nIn the base case, we assume that after refinement step $R_i$, to be the best child, there is no unsupported causal link present in \n$\\pi_{i+1}$ and a threat (if any) will be resolved by the planner immediately to make it a solution plan. \n\nIf there is an unsupported causal link then there must be an existing action in $\\pi_{i+1}$ to support it. \nIn this case, the estimate of requirement of new actions is still be 0.\n\n\n\n\\noindent $\\mathit{\\bold{Hypothesis:}}$ We select an arbitrary partial plan $\\pi_{i+1}$ and assume that Eq.~\\eqref{eq2a:test} holds for it.\n\n\n\\noindent $\\mathit{\\bold{Proof \\ Step:}}$\nHere, we show that Eq.~\\eqref{eq2a:test} holds for $\\pi_i$ too.\n\n\n\n\\begin{eqnarray*}\n\t\th^e(\\pi_i) &=& cost(R_i)  +  h^e(\\pi_{i+1}) \\\\\t\t\t\t\t\t\n\t\t\t\t\t\t&=&  cost(R_i)  +  h(\\pi_{i+1}) +  \n\t\t\t\t\t\t\t\t\t\t\\sum_{\\substack{\\pi'~{\\textit{from}} \\\n\t\t\t\t\t\t\t\t\t\t\\pi_{i+1} \\rightsquigarrow \\pi_{sol}}} \\epsilon_{h(\\pi')} \\\\\n\t\t\t\t\t\t\t\t\t\t\t&& \\text{By the induction hypothesis.}\n\t\t\t\t\t\t\t\t\t\t\\\\ &=& h(\\pi_{i})  +  \\epsilon_{h(\\pi_i)}\n\t\t\t\t\t\t\t  \t\t\t +  \\sum_{\\substack{\\pi'~\\textit{{from}} \n\t\t\t\t\t\t\t  \t\t\t \\ \\pi_{i+1} \\rightsquigarrow \\pi_{sol}}} \\epsilon_{h(\\pi')} \n\t\t\t\t\t\t  \t\t\t\t\t\t\t\t~~\\text{by Eq.~}\\eqref{aeq3:test}.\n\n\n\t\t\t\t\t\t\t\\\\ &=& \th(\\pi_{i})  + \\sum_{\\substack{\\pi'~\\textit{{from}} \\ \\pi_{i} \\rightsquigarrow \n\t\t\t\t\t\t\t\\pi_{sol}}} \\epsilon_{h(\\pi')}\n\\end{eqnarray*}\nTherefore, the the relationship holds for the parent partial plan $\\pi_i$ as well.\nThus, by induction for all partial plans $\\pi$, our assumption is correct. $\\hspace*{143pt}$ \\tiny $\\blacksquare$ \n\\end{Proof}\n\n\n\n\n\n", "itemtype": "equation", "pos": 58199, "prevtext": "\n\n\nFurther simplification of Eq.~\\eqref{eq33:test} yields,\n\n\\begin{eqnarray}\\label{eq51:test}\n\t\t h^e(\\pi_{i}) =\n\t\t\t\t\t\\ {h(\\pi_{i})} \\Big / {(1 - \\epsilon_{h}^{avg})} \n\\end{eqnarray}  \n\nAnother possible expansion, using infinite geometric progression, of Eq.~\\eqref{eq51:test} would be,\n\\begin{eqnarray}\\label{eq51:test}\n\t\t h^e(\\pi_{i}) = \\ {h(\\pi_{i})} \\times \\sum_{i=0}^{\\infty}{(\\epsilon_{h}^{avg})}^i\n\\end{eqnarray}  \n\n\n\nWe use RegPOCL to test the effectiveness of $h^e(\\pi_{i})$ in the POCL framework, where it selects the best partial plan.\n\n\n\n\\section{Experiment Design}\n\n\nIn this section we describe the evaluation phase settings. \n\nThis includes\n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item the heuristics selected as features, and\n\\item the domains selected.\n\\end{inparaenum}\n\n\n\\subsection{Feature Heuristics for Learning} The features used for learning are non-temporal heuristics from the literature of \nPOCL planning. \n\nConsidering the applicability of some of the POCL heuristics in the literature~\\cite{younes2003vhpop,nguyen2001reviving}, \nwe select six different heuristic functions. \nSome of these heuristics are informed but their informativeness varies over different planning domains. \n\n\n\n\nOur aim is to learn a more informed combination from these individual heuristics. \nThe six heuristics are,\n\n\n\n\\subsubsection{G Value ($h_{{g\\text{-}val}}$)} \n\nThis returns the number of actions in a selected partial plan \n$\\pi$ not counting the two dummy actions ($a_0$ and $a_{\\infty}$). \nIt signifies how far the search has progressed from the starting state.\n\n\n\n\\subsubsection{Number of Open Conditions ($h_{{OC}}$)} \n\nThis is total number of unsupported causal links present in a partial plan,\n$h_{{OC}}(\\pi) = \\left| \\mathrm{OC} \\right|$~\\cite{nguyen2001reviving}.\n\n\n\n\\subsubsection{Additive Heuristic ($h_{{add}}$)} \nThe additive heuristic $h_{{add}}$~\\cite{haslum2000admissible}, \nadds up the steps required by each individual open goal. \nYounes and Simmons (2003) use an adapted version of additive heuristic in POCL planning for the first time.\n\n\n\n\\subsubsection{Additive Heuristic with Effort ($h_{{add,w}}$)}  \nThe estimate is similar to $h_{add}$ but it considers the cost of an action as the \nnumber of preconditions of that action, plus the {linking cost} \\textbf{1} if the action supports any unsupported causal \nlink~\\cite{younes2003vhpop}. We call it $h_{{add,w}}$ as its notation is not used earlier. Here, $w$ signifies the extra work\nrequired.\n\n\n\\subsubsection{Accounting for Positive Interaction ($h_{{add}}^{r}$)}\nThis returns an estimate which takes into account the positive interactions between subgoals \nwhile ignoring the negative interactions. \nThis is represented as $h_{{add}}^{r}$ that is a variant of $h_{add}$~\\cite{younes2003vhpop}. \n\n\n\n\\subsubsection{Accounting for Positive Interaction with Effort ($h_{{add,w}}^{r}$)}\n\nThis is similar to the above heuristic which considers the total effort required~\\cite{younes2003vhpop}. \nA standard notation of this heuristic is also not used in the literature.\n\n\n\n\n\\subsection{Domains Selected} \n\n\n\nWe consider the following domains: \n\n\nLogistics and Gripper from IPC~1, Logistics and Elevator from IPC~2, Rovers and Zenotravel from IPC~3,\nand Rovers from IPC~5. \n\nIn our experiments we do not consider other domains from these competitions because either the state-of-the-art \nheuristics are not able\nto create enough training instances for learning, or {RegPOCL} does not support the domain definition language features. \n\nIPC~4 domains are not selected since the planner is not able to generate \nenough instances to initiate offline learning. \nThe domains from IPC~6 and later are not supported by {RegPOCL} \nbecause the representations use action costs, fluents, and hard and soft constraints.\nSome of them can be included by some preprocessing like removal of actions with cost from the domain description files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each selected domain, we consider problems that are represented in STRIPS style. \nWe select small sized problems for learning and test the learned predictive models over large sized problems in the same domain. \n\nWe have a total of 109 small sized problems from the selected domains. \n\nThe last four feature heuristics from the previous subsection have been used for calculating targets in each domain. \nThis means that we generate four different datasets in each selected domain from which best two are selected.\n\nWe choose satisficing track problems for generating training instances.\n\nFor the training set preparation, we fix a time limit of 3 minutes and \nan upper limit of 500,000 on the node generation.\n\n\n\n\nWe generate a few thousand training instances except for the Zenotravel domain where the total instances are 950. \n\nTo avoid overfitting, we pick training instances between 250 to 350 from the larger training sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Selected Learning Approaches}\n\nIn this section, we discuss in brief a procedure for feature selection in each dataset for \ntraining regression models,\nand different regression techniques with their references.\n\n\n\\subsection{Feature Selection} \n\n\nIn general, the training sets contain irrelevant or redundant attributes\n(out of the six selected heuristics).\n\nTo reduce the training effort and increase the efficiency of our planner,\nwe discard them from the training set.\n\nThe planner is bound to calculate all the selected features at each stage of refinement. \n\nThe correlation based feature selection technique~\\cite{hall1999correlation} is used to find the correlated features.\n\n\n\n\n\n\n\\begin{table*}[t]\n\\centering\n\n\n\\scalebox{0.70}\n{ \n\t\n\n\n\\begin{tabular}\n\n{ @{} l | r || r r | r r || r r | r r || r r r r || r r r r ||r @{}}\n\n\\hline\n\n\t\\multirow{3}{*}{\\textbf{Domain}} &\n\t\\multirow{3}{*}{\\textbf{\\#}} &\n\t\\multicolumn{8}{c||}{\\textbf{POCL Heuristics}} &\n\t\\multicolumn{8}{c||}{\\textbf{Evaluation using Fast Downward (FD)}} &\n\t\\multirow{3}{*}{\\textbf{LAMA}} \n\t\\\\ \\cline{3-18}\n\n\t&\n\t&\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c||}{\\textbf{Via learning approaches}} &\n\t\\multicolumn{4}{c||}{\\textbf{Lazy}} &\n\t\\multicolumn{4}{c||}{\\textbf{Eager}} &\n\t\\\\ \\cline{3-18} \n\t\n\t & &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\t\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&20\t\t&16\t\t&\\textbf{20}\t\t&1\t\t&1\t\t&\\textbf{20}\t\t&\\textbf{20}\t&\\textbf{20}\t\t&\\textbf{20}\t\t\n\t\t\t\t\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \t&\\textbf{20}\t\t\n\t\t\t\t\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \t&\\textbf{20}\t\t&\\textbf{20} \\\\\n\t\n\tRovers-3 \t&20\t\t&19\t\t&19\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t&\\textbf{20}\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{20}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t&18\t\t&17\t\t&14\t\t&\\textbf{20}\t\t&19\t\t&18\t\t&15\t\t&\\textbf{20}\t\t&\\textbf{20}\t \\\\\n\t\n\tRovers-5\t&40\t\t&28\t\t&31\t\t&32\t\t&36\t\t&${}^{\\text{\\small +}}$39\t\t&${}^{\\text{\\small +}}$39\t\n\t\t\t\t\t\t&${}^{\\text{\\small +}}$36\t\t&${}^{\\text{\\small +}}$39\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t&22\t\t&25\t\t&15\t\t&28\t\t&24\t\t&29\t\t&17\t\t&30\t\t&\\textbf{40} \\\\\n\n\t\\cline {1-19} \t\t\t \n\n    &\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{r,l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} & \\\\ \n\t\n\t\n\t\\hline\n\t\n\tLogistics-1\t&35 \t&25\t\t&1\t\t&32\t\t&28\t\t&${}^{\\text{\\small +}}$32\t\t&${}^{\\text{\\small +}}$32 \t&22\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$32\t\n\t\t\t\t\t\t\t&25\t\t&34\t\t&21\t\t&28\t\t&28\t\t&34\t\t&16\t\t&25\t\t&\\textbf{35} \\\\\n\t\n\tElevator-2\t\t&150\t&148\t&14\t\t&\\textbf{150}\t&58\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\n\t\t\t\t\t\t\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t&\\textbf{150}\t\\\\\n\t\n\tLogistics-2\t&40\t\t&36\t\t&12\t\t&36\t\t&34\t\t&\\textbf{40}\t&\\textbf{40}\t&\\textbf{40}\t&\\textbf{40}\n\t\t\t\t\t\t\t&36\t\t&36\t\t&35\t\t&36\t\t&36\t\t&36\t\t&36\t\t&36\t\t&\\textbf{40}\t\\\\\n\t\n\tZenotravel-3\t&20\t\t&5\t\t&4\t\t&9\t\t&10\t\t&${}^{\\text{\\small +}}$16\t\t&${}^{\\text{\\small +}}$16\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$16\t\t&${}^{\\text{\\small +}}$16\n\t\t\t\t\t\t\t&\\textbf{20}\t\t&\\textbf{20} \t&17\t\t&\\textbf{20}\t\n\t\t\t\t\t\t\t&\\textbf{20}\t\t&\\textbf{20} \t&16\t\t&\\textbf{20}\t&\\textbf{20} \\\\\n\n\n\\hline\t\t\t \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nNumber of solved problems using each heuristic. \n\\textbf{\\#} is the number of problems in each domain.\nState-of-the-art POCL heuristics are compared with the learned ones in the left half. \n\nThe state-based heuristics FF, CEA, and LM-Cut \nand their combination using MHFS strategy are also compared with POCL heuristics.\n\nThe last column captures the performance of LAMA11. \n\nBest results are shown in \\textbf{bold}, \nand a number with ``+'' mark (\\emph{e.g.}${}^{\\text{\\small +}}$36) shows competitive performance by \nthe learned models and their enhancements over each base heuristic. \n\n\nSimilar representations are followed in other tables.\n\n}\n\n\\end{table*}\n\n\n\n\\subsection{Regression Techniques}\nWe use the following regression techniques to learn predictive models. These techniques have been applied in planning \nfor learning in recent years~\\cite{samadi2008learning,thayer2011learning,us2013learning}.\n\n\n\\subsubsection{Linear Regression (LR)}\n\nThe regression model learns a linear function that minimizes the sum of squared \nerror over the training instances~\\cite{bishop2006pattern}. \n\n\n\\subsubsection{M5P}\n\nM5P gives more flexibility than LR due to its nature of \ncapturing non linear relationships.\nM5P technique learns a regression tree~\\cite{quinlan1992learning} that approximates the class value. \n\n\n\\subsubsection{M5Rules}\n\nSimilar to M5P but generates rules instead of modeling regression trees~\\cite{quinlan1992learning}.\n\n\n\\subsubsection{Least Median Squared (LMS)}\n\nLMS is similar to LR with median squared error.\nFunctions are generated from subsamples of data with least squared error function. \nUsually a model with lowest median squared error is selected~\\cite{rousseeuw2005robust}.\n\n\n\\subsubsection{Multilayer Perceptron (MLP)} \n\nMLP can learn more complex relationships compared to the other four \nregression techniques~\\cite{bishop2006pattern}.\n\n\n\n\nThe techniques discussed above are used to learn models through WEKA~\\cite{Hall2009} using \na 10-fold cross-validation in each domain. \n\nA regression technique called SVMreg that implements support vector machine for regression purposes, \nis not used in this work due to some technical difficulty. \n\nHowever, it has not much influenced planning processes in the past~\\cite{us2013learning}.\n\n\n\n\n\\section{Experimental Evaluation} \n\n\nWe use MC-Loc and MW-Loc~\\cite{younes2003vhpop} as flaw selecting heuristics for refining a partial plan.\n\n\n\nThey give higher preference to the local flaws present in the partial plan.  \n\nWe employ Greedy Best First Search algorithm for selecting the next partial plan for refinement.\n\n\n\n\n\n\\subsection{Environment}\n\nWe perform the experiments on Intel Core~2 Quad with 2.83~GHz 64-bit processor and 4GB of RAM. \nTo evaluate the effectiveness of learned models and to correct the single-step-error associated with the models, \na time limit of 15 minutes and a node generation limit of 1 million is used.\n\n\n\n\\subsection{Evaluations}\n\nWe use RegPOCL to compare the performances of the offline predictive models $h^l$, their corresponding enhanced models\n$h^{l,e}$ and the last four base features. \n\n\n\n\n\n\n\n\n\n\nThese are also compared with some of the recent effective state-space based heuristics and approaches that are introduced later.\n\n\n\n\n\n\nWe exclude the first two base features from comparison since they are weak heuristics and RegPOCL \ndoes not solve sufficient problems using them.\n\nHowever, they are useful while working jointly with other informed heuristics. \n\nNext, we discuss the observations made during the training phase. \n\n\n\\subsubsection{Training} \n\nUsing Algorithm~1, we prepare datasets and learn different predictive models by applying the various regression techniques \ndiscussed earlier. \n\nWe select each of the last four features to solve a set of problems.\nThe target value is the plan length found by RegPOCL using the base features.  \nThe dataset preparation phase took less than two hours on an average in each domain, for each of the four features.\n\nOnce we have enough instances, we begin the training process. \n\nIn each domain, the best two datasets out of four with sufficient training points\nis selected for model training by visualizing the distribution of training points using WEKA.   \n\nNote that, in Figure~1, different heuristics for calculating target values will prefer different paths. \nTherefore, the four base features will generate four different datasets. \n\n\nThe attribute selection strategy allows us to select a subset of attributes in the training set by removing\ncorrelated and redundant features.\n\nWe learn a total of 70 (7 domains $\\times$ 2 datasets $\\times$ 5 regression techniques) regression models. \n\nThe training phase took 20ms (milliseconds) using LR, 270ms using LMS, 600ms using MLP, 82ms  \nusing M5Rule, and  58ms using M5P on an average per model.  \n\nAll the learned models have high accuracy but LR is the fastest, followed by M5P and M5Rule.\nNext, we test these models on different benchmarks.\n\n\n\n\n\\subsubsection{Testing}\n\nWe test the effectiveness of our approaches by selecting partial plans $(\\pi)$ for refinement using RegPOCL. \nWe assume that an informed heuristic leads to minimal possible refinements needed for $\\pi$.\n\nNext, for the comparison we compute score\\footnote{\\url{https://helios.hud.ac.uk/scommv/IPC-14/}} \nas in IPC for satisficing track problems.\n\nThe better score on each standard signifies the better performance.\nWe compare the performance of the learned models with the selected base features \n$h_{add}$, $h_{add,w}$, $h_{add}^{r}$, and $h_{add,w}^{r}$. \n\n\nThe comparison is done on the basis of \n\\begin{inparaenum}[(\\itshape i.\\upshape)]\n\\item the number of solved problems, and \n\\item the score obtained on plan quality, execution time, nodes (partial plan) visited, and makespan quality.\n\\end{inparaenum}\n\n\n\n\nFor example, the offline learned model $h_{add}^{r,l}$ is learned on a dataset prepared using $h_{add}^{r}$. \nIn other words, RegPOCL uses $h_{add}^{r}$ for calculating the target values in the dataset.\n\n$h_{add}^{r,l}$ can be enhanced to $h_{add}^{r,l,e}$ \nusing the online heuristic adjustment approach which is expected to be more informed than $h_{add}^{r,l}$. \n\n\nIt is similar for other learned heuristics.\n\nThese models are applied in the POCL framework for selecting the most suitable partial plan,\nfollowed by the heuristic MW-Loc~\\cite{younes2003vhpop} for selecting the most adverse flaw in it .\n\n\n\nWe also compare our approaches with state-space based approaches on the basis of the number of problems solved, and \nscore obtained on plan quality and total execution time. \n\nWe select fast forward heuristic (FF)~\\cite{hoffmann2001ff}, context-enhanced additive heuristic (CEA)~\\cite{helmert2008unifying},\nand landmark-cut heuristic (LM-Cut)~\\cite{helmert2011lm}. \n\nWe also use these heuristics together by applying them in multi-heuristic first solution strategy (MHFS)~\\cite{roger2010more}. \nIn general, the strategy performs better with alternating usage of different heuristics instead of combining them. \n\nWe also compare the effectiveness of our techniques with \nLAMA11~\\cite{richter2011lama}; the winner of IPC-2011 in the sequential satisficing track. \nLAMA11 applies FF and LM-Count~\\cite{richter2008landmarks} heuristics together using multi-queue search.\n\n\nWe set a 20 minute time limit while evaluating LAMA11 over these domains,\nsince it has an internal time limit of 5 minutes for the invariant synthesis part of translator.\n\n\n\n\n\n\n\\begin{table*}[t]\n\\centering\n\n\n\n\\scalebox{0.7}\n{ \n\t\n\n\n\\begin{tabular}\n\n{ @{} l || r r | r r || r r | r r || r r r r || r r r r ||r @{}}\n\n\\hline\n\n\t\\multirow{3}{*}{\\textbf{Domain}} &\n\t\\multicolumn{8}{c||}{\\textbf{POCL Heuristics}} &\n\t\\multicolumn{8}{c||}{\\textbf{Evaluation using Fast Downward (FD)}} &\n\t\\multirow{3}{*}{\\textbf{LAMA}} \n\t\\\\ \\cline{2-17}\n\n\t&\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c||}{\\textbf{Via learning approaches}} &\n\t\\multicolumn{4}{c||}{\\textbf{Lazy}} &\n\t\\multicolumn{4}{c||}{\\textbf{Eager}} & \n\t\\\\ \\cline{2-17} \n\t\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\t\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&16.0\t&14.9\t&0.7\t&0.7\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t&\\textbf{20.0}\t\t\n\t\t\t\t\t\t&15.45 \t\t&14.9\t\t\t&15.5 \t\t&15.5\t\t\n\t\t\t\t\t\t&15.5\t\t&14.9\t\t\t&15.5 \t&15.5\t&\\textbf{20.0} \\\\\n\t\n\tRovers-3 \t&17.3\t&16.2\t&18.1\t&16.9\t\t&17.8\t\t&${}^{\\text{\\small +}}$18.6\t&17.8 &${}^{\\text{\\small +}}$18.6\n\t\t\t\t\t\t&17.1\t&15.9\t&12.5\t&18.8\t\t&18.1\t\t&16.9\t\t&13.9\t&19.0\t\t&\\textbf{19.8}\t \\\\\n\t\n\tRovers-5\t&25.7\t\t&26.3\t\t&28.0\t&30.2\t\t&${}^{\\text{\\small +}}$33.1\t&${}^{\\text{\\small +}}$36.3\t&30.1\t\t\n\t\t\t\t\t\t&${}^{\\text{\\small +}}$33.6\t&20.8\t\t&23.4\t&13.5\t\t&26.0\t\t&22.7\t&27.3\t&15.7\t&28.3\t\t\n\t\t\t\t\t\t&\\textbf{39.8} \\\\\n\n\t\n\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{{r,l,e}}$} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} &\n\t\n\t\\multicolumn{1}{c}{FF} & \n\t\\multicolumn{1}{c}{CEA} &\n\t\\multicolumn{1}{c}{LM-Cut} &\n\t\\multicolumn{1}{c||}{MHFS} & \\\\ \n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&24.3\t&0.8\t&31.2\t&26.1\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$31.2\t&${}^{\\text{\\small +}}$31.2\t&20.9\t\t&\\textbf{32.0}\n\t\t\t\t\t\t\t&23.7\t\t&30.3\t\t&19.7\t\t&27.4\t\t&27.0\t\t\t&30.8\t\t&15.1\t\t&24.6\t\t&30.8 \\\\\n\t\n\tElevator-2\t\t&142.8\t&11.8\t&144.9\t&49.7\t\t&142.7\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$144.9\t\t&142.7\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$144.9\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t&117.1\t&112.0\t&117.1\t&116.9\n\t\t\t\t\t\t\t&144.1\t&136.0\t&144.2\t&141.0\t&\\textbf{148.2}\t\\\\\n\t\n\tLogistics-2\t&33.8\t\t&10.7\t\t&34.9\t\t&30.7\t\t&\\textbf{38.1}\t\t&${}^{\\text{\\small +}}$36.3\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{38.1}\t\t&${}^{\\text{\\small +}}$36.3\n\t\t\t\t\t\t\t&33.72\t\t&29.5\t\t&32.4\t\t&33.3\t\t&34.1\t\t&30\t\t&33.7\t\t&35.6\t\t\t&37.8\t\\\\\n\t\n\tZenotravel-3\t&4.6\t\t&3.7\t\t&8.5\t\t&8.8\t\t&${}^{\\text{\\small +}}$13.5\t&${}^{\\text{\\small +}}$13.7\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$13.5 \t&${}^{\\text{\\small +}}$13.7\n\t\t\t\t\t\t\t&17.3 \t&17.7\t\t&14.7 \t&18.6\t\n\t\t\t\t\t\t\t&18.1\t\t&14.3 \t&16\t\t&18.8\t&\\textbf{19.2} \\\\\n\t\n\t\n\t\\hline \\hline\n\t\\multicolumn{1}{@{}c||}{\\textbf{Time Score}}\n\t\t\t\t\t\t\t&204.4\t\t&76.5\t\t&197.7\t \t&148.6\t\t&${}^{\\text{\\small +}}$272.9\t\t&${}^{\\text{\\small +}}$260.8\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$258.6\t\t&${}^{\\text{\\small +}}$264.3\t\t&255.6\t\n\t\t\t\t\t\t\t&\\textbf{280.0}\t\t&228.0\t\t&233.8\t\t&259.6\t\t&271.3\t\t&223.0\t\t&222.5\t\t&137 \\\\\t\t\t\t\t\n\n\n\\hline\t\t\t \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nScores on plan quality and overall time. We compare state-of-the-art POCL heuristics with learned ones. \nThe effectiveness of the POCL heuristics is compared with some latest state based approaches. \nThe last row demonstrates the overall time score of each heuristic. \n\nThe numerical representations and column details are similar to Table~1.\n}\n\n\\end{table*}\n\n\n\n\nAll the state-based approaches are evaluated using Greedy Best First Search algorithm in \nthe fast downward planning system~\\cite{helmert2006fast}.\nWe use ``eager'' and ``greedy'' types of evaluations with no preferred operators.\n\n\n\n\n\nThe regression models selected in the chosen domains are trained using,\n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item M5P in Gripper-1 and Elevator-2, \n\\item LR in Rovers-3, Rovers-5, Logistics-2, and Zenotravel-3, and\n\\item M5Rule in Logistics-1.\n\\end{inparaenum}\n\n\n\nIn Table~1, we compare \n\\begin{inparaenum}[(\\itshape i\\upshape)]\n\\item the base features, \n\\item offline learned models and their enhanced versions\n\\item state-space based heuristics FF, CEA, and LM-Cut, and \n\\item the strategies used in MHFS, and LAMA11, \n\\end{inparaenum}\non the basis of number of problems solved. \n\n\n\nIn this table, RegPOCL solves equal number of problems as LAMA11\nin Gripper-1, Rovers-3, Elevator-2, and Logistics-2 using each of learned heuristics and their enhanced versions.\n\nThe base features have performed well in some domains but are not consistent overall. \n\nIn Rovers-5, our approaches solved 1 problem less than LAMA11, \nbut they beat other state-space based competitors comprehensively. \n\nAlso, each learned model has performed better than all the base features. \n\nFor Logistics-2, we are competitive with LAMA11 and  solve at least 4 more problems than other good heuristics \nlike CEA and LM-Cut.\n\nIn Zenotravel-3, RegPOCL solved 6 problems more by applying our approaches but loses to the state-based competitors.\n\n\nOur second approach improves the performance of the learned models in Rovers-5 by solving 3 more problems, \nand in Logistics-1 where it solves 10 more problems.  \n\nThis approach could not increase the coverage in other domains. \nLAMA11 wins on the basis of the total number of problems solved in each domain.\n\n\n\nIn Table~2, we compare the score obtained on plan quality by each of the \nbase features, \nlearned models with their corresponding enhancements, and\nstate-space based heuristics and techniques. \n\n\n\n\\begin{table}[t]\n\\centering\n\n\n\\scalebox{0.65}\n{ \n\t\n\n\n\\begin{tabular}\n\n{@{} l || r r | r r || r r | r r @{}}\n\n\\hline\n\n\n\n\n\n\t\\multirow{2}{*}{\\textbf{Domain}} &\n\t\\multicolumn{4}{c||}{\\textbf{State-of-the-art}} &\n\t\\multicolumn{4}{c}{\\textbf{Via learning approaches}} \n\t\\\\ \\cline{2-9} \n\t\n\t&\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c@{}}{$h_{{add,w}}^{{l,e}}$} \n\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&7.0\t&14.0\t&0.0\t&0.0\t&${}^{\\text{\\small +}}$18.0  &\\textbf{19.7}\t&${}^{\\text{\\small +}}$18.0\t&\\textbf{19.7} \\\\\n\t\n\tRovers-3 \t&8.6\t&8.3\t&12.6 \t&\\textbf{19.0}\t\t&13.3\t&13.6 \t&13.3\t&13.6 \\\\\n\t\n\tRovers-5\t&8.2\t&14.7\t&13.5\t&30.9\t\t&\\textbf{31.4}\t\t&\\textbf{31.4}\t\t&24.7 &\\textbf{31.4}\t\\\\\n\n\t\n\t\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l,e}}$} \\\\\t\n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&5.1\t&0.6\t&16.5\t&21.6\t\n\t\t\t\t\t\t\t&\\textbf{29.9}\t\t&\\textbf{29.9}\t\t&{20.7}\t&\\textbf{29.9} \\\\\n\t\n\tElevator-2\t\t&41.4\t&8.2\t&58.9\t&39.7\t\t&\\textbf{145.0} \t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$135.0\t&\\textbf{145.0}\t&${}^{\\text{\\small +}}$135.0\t\\\\\n\t\n\tLogistics-2\t&24.5\t&6.5\t&23.4\t&26.7\t\t&${}^{\\text{\\small +}}$33.4\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&\\textbf{35.3}\t &${}^{\\text{\\small +}}$33.4\t&\\textbf{35.3}   \\\\\n\t\n\tZenotravel-3\t&0.0\t&1.0\t&2.4\t&7.7 \t&7.4\t&\\textbf{11.07}\t&7.4\t&\\textbf{11.07} \\\\\n\t\n\t\n\t\\hline \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nResults of refinement score (the number nodes visited) of RegPOCL using each heuristic. \nWe compare state-of-the-art heuristics with the learned models ($h^{l}$) and their further enhancements ($h^{l,e}$). \nThe best results are in \\textbf{bold}.\n}\n\n\n\\end{table}\n\n\n\n\\begin{table}[t]\n\\centering\n\n\\scalebox{0.64}\n{ \n\t\n\n\n\\begin{tabular}\n\n{@{}l || r r | r r || r r | r r@{}}\n\n\\hline\n\n\n\n\n\n\n\n\n\n\t\n\t\\multirow{1}{*}{\\textbf{Domain}} &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{l}}$} & \n\t\\multicolumn{1}{c@{}}{$h_{{add,w}}^{{l,e}}$} \n\n\n\n\t\\\\ \t\\hline \\hline\n\n\tGripper-1 \t&16.0\t&9.8\t&0.5\t&0.5\t\t&\\textbf{20.0}  &\\textbf{20.0}\t&\\textbf{20.0}\t&\\textbf{20.0} \\\\\n\t\n\tRovers-3 \t&\\textbf{17.9}\t&15.5\t&17.8\t&15.8\t\t&17.4\t\t&{17.7} \t&17.4\t\t&{17.7} \\\\\n\t\n\tRovers-5\t&26.1\t&24.2\t&28.4\t&26.5\t\t&\\textbf{30.5}\t\t&${}^{\\text{\\small +}}$29.8\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$27.6\t&\\textbf{30.5} \\\\\n\t\n\t\n\t\\hline\n\t\n    &\n\t\\multicolumn{1}{c}{$h_{{add}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add,w}}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^r$} & \n\t\\multicolumn{1}{c||}{$h_{{add,w}}^{r}$} & \n\t\\multicolumn{1}{c}{$h_{{add}}^{{r,l}}$} & \n\t\\multicolumn{1}{c|}{$h_{{add}}^{r,l,e}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l}}$} & \n\t\\multicolumn{1}{c}{$h_{{add,w}}^{{r,l,e}}$}  \\\\\t\n\t\n\t\\hline\n\t\n\t\n\tLogistics-1\t&21.8\t&0.9\t&25.2\t&20.8\t\n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$30.4\t&${}^{\\text{\\small +}}$30.4\t&17.6\t&\\textbf{30.5} \\\\\n\t\n\tElevator-2\t\t&147.1\t&11.7\t&\\textbf{149.1}\t&50.5\t\t&\t\\textbf{149.1}\t\n\t\t\t\t\t\t\t&146.8\t\t&\\textbf{149.1} &\t146.8\t\\\\\n\t\n\tLogistics-2\t&26.3\t&7.0\t&33.0\t&19.3\t&${}^{\\text{\\small +}}$33.5\t&\\textbf{35.4}\t \n\t\t\t\t\t\t\t&${}^{\\text{\\small +}}$33.4\t&\\textbf{35.4}   \\\\\n\t\n\tZenotravel-3\t&3.7\t&2.7\t&7.5\t&7.7\t\t&${}^{\\text{\\small +}}$12.1\n\t\t\t\t\t\t\t\t\t&\\textbf{13.5}\t\t&${}^{\\text{\\small +}}$12.1\t\t&\\textbf{13.5} \\\\\n\t\n\t\n\t\\hline \n\n\\end{tabular}\n\n\n}\n\n\\caption\n{\nMakespan quality score of the heuristics. Columns are identical to corresponding ones in Table~3.\n}\n\n\n\\end{table}\n\n\n\n\nLAMA11 is an anytime planner which gives solution close to the optimal but takes more time to compute \nshorter plans.\n\nFor Gripper-1 and Logistics-2, RegPOCL using the learned heuristics solves equal number of problems but finds shorter \nplans compared to LAMA11. \n\n\nIn Logistics-1, RegPOCL lost to LAMA11 by 3 problems in the number of problems solved, \nbut obtained a higher score \nin the other problems it solved as it produces shorter plans \nusing $h_{add,w}^{r,l,e}$, $h_{add}^{r,l}$, and $h_{add}^{r,l,e}$.\n\nThe effectiveness of $h_{add,w}^{r,l,e}$ wins in this domain with best plan quality. \n$h_{add,w}^{r,l,e}$ also increases the plan quality score from 20.9 that is obtained by $h_{add,w}^{r,l}$ to 32.0\nusing our online error correcting strategy. \n\n\n\nHowever, in Logistics-2, the performance has decreased after further enhancements \nof $h_{add}^{r,l}$ and $h_{add,w}^{r,l}$.\n\n\n\nIn general, the performance of RegPOCL using either of the offline learned models and their enhancements \nis often better than that using the base features.\nIn most of the cases, the online error adjustment approach has further enhanced the performance of these learned models. \n\n\nThe last row of Table~2 gives the score obtained on total time taken by the process. \nIf a planner takes less than 1 second for solving a problem then it gets full score. \nOn the total time score the winner is CEA with ``lazy'' evaluation.\n\nThe learned models and their enhanced versions \nhave obtained better scores than other competitors except CEA. \nThese scores are very close to the winning score and almost twice that of LAMA11. \n\n\nIn Table~3, we compare the score obtained on the number of nodes RegPOCL visits for solving the planning problems.\nThis is obtained for the base features, the learned heuristics, and their enhanced versions. \n\n\nThe models obtained after offline learning are more informed toward goals and refines fewer partial plans. \nThe score obtained by the learned models is further increased by a good factor in Zenotravel-3 \nusing the error correcting approach. \n\nFor Elevator-2, the error correcting approach has shown some negative effect which continues in Table~4 too. \n\nIn Table~4, we demonstrate the score obtained on the makespan quality. \nHigher score signifies smaller makespan and more flexibility in the generated solution plan.\n\nIn Elevator-2 and Rovers-5, \nthe scores of $h_{add}^l$ and $h_{add}^{r,l}$ have decreased due to the negative effects of our error adjustment approach,\nwhile the score obtained by $h_{add,w}^{r,l,e}$ is almost 1.5 times the score of $h_{add,w}^{r,l}$ in Logistics-1. \n\nIn general, the offline learned models have generated more flexible plans with \nshorter makespan than the base features. \n\nThese qualities are further improved using the enhanced versions of these models.\n\n\n\\section{Discussion}\n\nWe have already discussed the advantages of our approaches but they also have limitations. \nIn our offline approach,\nwe are bound to do some poor generalization while learning heuristics.\n\n\n\n\n\n\n\nCurrent literature supports the idea of selecting a large feature set for more accurate learning~\\cite{RobertsHWd08}. \nAccuracy can also be improved using an empirical performance model of all components of a portfolio to decide \nwhich component to pick next~\\cite{FawcettVH0HL14}.\n\n\nIn our work, a large feature set may have some drawbacks. For example, computing the features at each refinement step during \nthe planning process is computationally expensive.\n\n\n\nThe online error adjustment approach could also perform poorly in certain domains. \n\nIn Figure~1, if the orientation of objects in a domain is such that $h(\\pi_{i+1})$ is larger than $h(\\pi_{i})$\nthen $\\epsilon_{h({\\pi_{i})}}$ may not be accurate. The inaccuracy in $\\epsilon_{h({\\pi_{i})}}$ is compounded \nif the above condition holds at the beginning of the planning process.\n\n\nThis results in an inaccurate $\\epsilon_{h}^{avg}$ value, leading to wrong selection of the partial plan to refine next. \nConsequently, the planner may end up finding longer and less flexible plans. \n\n\nAnother limitation is that \na refinement may change the existing priorities of partial plans in the set due to the single-step-error adjustment.\nConsidering the time factor, we avoid changing the decided priorities of those partial plans.\nThis may also lead to inaccuracy in $\\epsilon_{h}^{avg}$.\n\n\n\n\n\n\n\n\n\n\n\n\nOur approaches do not utilize the advantage of strategies like alternation queue, and \ncandidate selection using concept of pareto optimality~\\cite{roger2010more}. \n\n\n\n\n\n\n\n\n\nRecently, the planning community has tried coming up with effective portfolios of heuristics or planners. \nThe techniques of generating good portfolios are not new to theoretical machine learning.\n\nA follow up work done in the past is \n\ncombining multiple heuristics online~\\cite{StreeterGS07}. \n\nOne could form a portfolio of different algorithms to reduce the total makespan for a set of jobs \nto solve~\\cite{StreeterS08}.\n\nThe authors provide a bound on the performance of the portfolio approaches. \nFor example, an execution of a greedy schedule of algorithms cannot exceed four times the optimal schedule. \n\n\n\n\n\n\n\n\nIn planning, a sequential portfolio of planners or heuristics aims to optimize the performance metrics. \n\nIn general, such configurations automatically generate sequential orderings of best planning algorithms. \nIn the portfolio the participants are allotted some timestamp to participate in solving problems in the ordering.\n\n\nA similar approach is used in ~\\cite{SeippSHH15}. \nThe authors outline their procedure for optimal and satisficing planning.\n\nThe procedure used in this work starts with a set of planning algorithms and a time bound.\nIt uses another procedure \\emph{OPTIMIZE} that focuses on the marginal improvements of the performance.\n\nHere, the quality of the portfolio is bounded by $\\mathrm{(1-(1/e))~\\times}$~\\sc {opt}, \\normalfont and \nthe running time cannot exceed {4}~\\sc{opt}. \\normalfont\n\n\nThe components can be allowed to act in a round-robin fashion~\\cite{gerevini2014planning}.\n\n\n\n\\textcolor{black}\n{\nThe state-of-the-art planners exhibit variations in their runtime for a given problem instance,\nso no planner always dominates over others. \n\nA good approach would be to select a planner for a given instance by looking at its processing time.\nThis is done by building an empirical performance model (EPM) for each planner.\nEPM is derived from sets of planning problems and performance observation.\nIt predicts whether the planner could solve a given instance~\\cite{FawcettVH0HL14}.  \nThe authors consider a large set of instance features and \nshow that the runtime predictor is often superior to the individual planners. \n\n\n\n\n\n\n}\n\n\nPerformance wise sorting of components in a portfolio is also possible~\\cite{NunezBL15}.\nThe portfolio is sorted\nsuch that the probability of the performance of that portfolio is maximum at any time.\nExperiments show that performance of a greedy strategy can be enhanced to near optimal over time.\n\n\n\n\n\n\n\n\nThe last two paragraphs cover recent literature in brief which explain previous strategies of combining different base methods.\nThe literature shows that they have performed well over different benchmarks. \n\nOur current settings do not capture any such ideas for combining different components of heuristics.  \n\nA direct comparison with any of the above mentioned works is therefore out of scope for our current work.\nThis is because, we are more concerned about working with unit cost based POCL heuristics in isolation.\n\n\nOn the other hand, we suspect that many of these strategies, in some adapted form, \nwould likely be beneficial in the POCL framework.\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Summary and Future Work}\n\nWe demonstrate the use of different regression models to combine different heuristic values to arrive at consistently better \nestimates over a range of planning domains and problems.\n\n\n\nWe extend some recent attempts to learn combinations of heuristic functions in \nstate-space based planning to POCL planning.\n\nWe also show that the learned models can be further enhanced by an online error correction approach. \n\n\n\n\nIn future we intend to explore online learning further, and continue our experiments with combining heuristic functions. \nWe also aim to explore the use of an optimizing planner in tandem with bootstrapping methods. \nApart from these, we will be giving a complete generalization of our current learning approaches for temporal planning and \nplanning with deadlines.\n\n\n\n\n\n\n\\bibliography{formatting-instructions-latex}\n\\bibliographystyle{aaai}\n\n\n\\section*{Appendix}\n\n\\subsection*{Proof of Theorem~1}\n\t\t\n\n\n\\subsubsection{Theorem 1.} \n\\emph\n{\nFor a given learned predictive model ($h$) and partial plan ($\\pi_i$) in Figure~1 which leads to the solution plan ($\\pi_{sol}$)\nafter certain refinement steps, the enhanced version of the predictive model ($h^e$) is,\n\n\n\n", "index": 13, "text": "\\begin{equation}\\label{eq2a:test}\n\t\th^e(\\pi_i) \\ = \\ h(\\pi_{i}) \\ + \n\n\t\t\t\t\\sum_{\t\\substack{\\pi{'}~\\text{{from}} \\\\ \\pi_i \\rightsquigarrow \\pi_{sol}} } \\epsilon_{h(\\pi')}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"h^{e}(\\pi_{i})\\ =\\ h(\\pi_{i})\\ +\\par&#10;\\sum_{\\begin{subarray}{c}\\pi{{}^{\\prime}}%&#10;~{}\\text{{from}}\\\\&#10;\\pi_{i}\\rightsquigarrow\\pi_{sol}\\end{subarray}}\\epsilon_{h(\\pi^{\\prime})}\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mi>e</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mi>\u03c0</mi><mo>\u2062</mo><mmultiscripts><mtext>from</mtext><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>\u03c0</mi><mi>i</mi></msub><mo>\u219d</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mtd></mtr></mtable></munder><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c0</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mrow></mrow></mrow></math>", "type": "latex"}]