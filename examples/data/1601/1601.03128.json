[{"file": "1601.03128.tex", "nexttext": "\nwhere $\\cal{C} \\subset \\mathcal{P}(\\mathcal{V})$, with\n$\\mathcal{P}(\\mathcal{V})$ denoting the powerset of $\\mathcal{V}$. Each $x_{c}$\ndefines a set of random variables included in subset $c$, referred to as a\nclique. The function $\\psi_c$ defines a constraint (potential) on the\ncorresponding clique $c$. We use unary, pairwise and higher order potentials in\nthis work, and define them in Section~\\ref{sec:GC}. The set of potential\ncharacters is obtained by the character detection step discussed in\nSection~\\ref{sec:charDet}. The neighbourhood relations among characters,\nmodelled as pairwise and higher order potentials, are based on the spatial\narrangement of characters in the word image.\n\nIn the following we show an example energy function composed of unary, pairwise\nand higher order (of clique size three) terms on a sample word with four\ncharacters. For a word to be recognized as ``OPEN'' the following energy\nfunction should be the minimum.\n\n", "itemtype": "equation", "pos": 19332, "prevtext": "\n\n\\begin{frontmatter}\n\n\\title{Enhancing Energy Minimization Framework for\\\\ Scene Text Recognition with Top-Down Cues}\n\n\n\n\\author{Anand Mishra$^{1*}$}\n\\author{Karteek Alahari$^2$}\n\\author{C.~V.~Jawahar$^1$}\n       \\address{$^1$IIIT Hyderabad\\fnref{cor1} \\hspace{1cm} $^2$Inria\\fnref{cor2}\\\\\n       }\n       \\fntext[cor1]{Center for Visual Information Technology, IIIT Hyderabad, India.}\n       \\fntext[cor2]{LEAR team, Inria Grenoble Rhone-Alpes, Laboratoire Jean Kuntzmann, CNRS, Univ.\\ Grenoble Alpes, France.\\newline\n\\hspace*{0.22cm}*Corresponding author --\\\\\n\\hspace*{0.22cm}       Email:~\\texttt{anand.mishra@research.iiit.ac.in}\\\\\n\\hspace*{0.22cm}       Phone: +91-40-6653 1000~~~Fax: +91-40-6653 1413}\n       \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{abstract}\nRecognizing scene text is a challenging problem, even more so than the\nrecognition of scanned documents. This problem has gained significant attention\nfrom the computer vision community in recent years, and several methods based\non energy minimization frameworks and deep learning approaches have been\nproposed. In this work, we focus on the energy minimization framework and\npropose a model that exploits both bottom-up and top-down cues for recognizing\ncropped words extracted from street images. The bottom-up cues are derived from\nindividual character detections from an image. We build a conditional random\nfield model on these detections to jointly model the strength of the detections\nand the interactions between them. These interactions are top-down cues\nobtained from a lexicon-based prior, i.e., language statistics. The optimal\nword represented by the text image is obtained by minimizing the energy\nfunction corresponding to the random field model. We evaluate our proposed\nalgorithm extensively on a number of cropped scene text benchmark datasets, namely\nStreet View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word, and\nshow better performance than comparable methods. We perform a rigorous analysis\nof all the steps in our approach and analyze the results. We also show that\nstate-of-the-art convolutional neural network features can be integrated in our\nframework to further improve the recognition performance.\n\\end{abstract}\n\n\\begin{keyword}\nScene text understanding, text recognition, lexicon priors, character recognition, random field models.\n\\end{keyword}\n\\end{frontmatter}\n\n\n\\section{Introduction}\n\\label{sec:intro}\nThe problem of understanding scenes semantically has been one of the\nchallenging goals in computer vision for many decades. It has gained\nconsiderable attention over the past few years, in particular, in the context\nof street scenes~\\cite{Brostow08,Ladicky10,Geiger2012CVPR}. This problem has\nmanifested itself in various forms, namely, object\ndetection~\\cite{Desai09,Felzen10}, object recognition and\nsegmentation~\\cite{Levin09,Shotton09}. There have also been significant\nattempts at addressing all these tasks\njointly~\\cite{Ladicky10,Gould09b,YaoFU12}. Although these approaches interpret\nmost of the scene successfully, regions containing text are overlooked. As an\nexample, consider an image of a typical street scene taken from Google Street\nView in Fig.~\\ref{fig:streetscene}. One of the first things we notice in this\nscene is the sign board and the text it contains. However, popular recognition\nmethods ignore the text, and identify other objects such as car, person, tree,\nand regions such as road, sky. The importance of text in images is also\nhighlighted in the experimental study conducted by Judd~\\textit{et\nal.}~\\cite{Judd09}. They found that viewers fixate on text when shown images\ncontaining text and other objects. This is further evidence that text\nrecognition forms a useful component in understanding scenes.\n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=8cm,height=6cm]{figures/fig1.eps}\n\\caption{A typical street scene image taken from Google Street View. It\ncontains very prominent sign boards with text on the building and its windows.\nIt also contains objects such as car, person, tree, and regions such as road,\nsky. Many scene understanding methods recognize these objects and regions in\nthe image successfully, but overlook the text on the sign board, which contains\nrich, useful information. The goal of this work is to address this gap in\nunderstanding scenes.}\n\\label{fig:streetscene}\n\\end{figure}\n\n\n\\begin{figure*}[!t]\n\\centering\n\\begin{tabular}{cccccc}\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s1.eps} &\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s3.eps} &\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s5.eps} &\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s7.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s9.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s11.eps}\\\\\n\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s2.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s4.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s6.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s8.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s10.eps}&\n\\includegraphics[width=2.3cm,height=1.2cm]{figures/challenges/s12.eps}\\\\\n\\end{tabular}\n\\caption{Challenges in scene text recognition. A few sample images from the SVT\nand IIIT 5K-word datasets are shown to highlight the variation in view point,\norientation, non-uniform background, non-standard font styles and also issues\nsuch as occlusion, noise, and inconsistent lighting. Standard OCRs perform\npoorly on these datasets (as seen in Table~\\ref{tab:ourdataset}\nand~\\cite{WangB11,NeumannM10}).}\n\\label{fig:challenges2}\n\\end{figure*}\n\nIn addition to being an important component of scene understanding, scene text\nrecognition has many potential applications, such as image retrieval, auto\nnavigation, scene text to speech systems, developing apps for visually impaired\npeople~\\cite{Mishra13,text2speech}. Our method for solving this task is\ninspired by the many advancements made in the object detection and recognition\nproblems~\\cite{Desai09,Felzen10,Shotton09,DalalT05}. We present a framework for\nrecognizing text that exploits bottom-up and top-down cues. The bottom-up cues\nare derived from individual character detections from an image. Naturally,\nthese windows contain true as well as false positive detections of characters.\nWe build a conditional random field (CRF) model~\\cite{Lafferty01} on these\ndetections to determine not only the true positive detections, but also the\nword they represent jointly. We impose top-down cues obtained from a\nlexicon-based prior, i.e., language statistics, on the model. In addition to\ndisambiguating between characters, this prior also helps us in recognizing\nwords.\n\nThe first contribution of this work is a joint framework with seamless\nintegration of multiple cues---individual character detections and their\nspatial arrangements, pairwise lexicon priors, and higher-order priors---into a CRF\nframework which can be optimized effectively. The proposed method performs\nsignificantly better than other related energy minimization based methods for\nscene text recognition. Our second contribution is devising a cropped word recognition\nframework which is applicable not only to closed vocabulary text recognition\n(where a small lexicon containing the ground truth word is provided with each\nimage), but also to a more general setting of the problem, i.e., open\nvocabulary scene text recognition (where the ground truth word may or may not\nbelong to a generic large lexicon or the English dictionary). The third\ncontribution is comprehensive experimental evaluation, in contrast to many\nrecent works, which either consider a subset of benchmark datasets or are\nlimited to the closed vocabulary setting. We evaluate on a number of cropped word datasets (ICDAR 2003, 2011 and 2013~\\cite{ICDAR}, SVT~\\cite{SVT}, and IIIT\n5K-word~\\cite{MishraBMVC12}) and show results in closed and open vocabulary\nsettings. Additionally, we analyzed the effectiveness of individual components\nof the framework, the influence of parameter settings, and the use of\nconvolutional neural network (CNN) based features~\\cite{AZ14}.\n\nThe remainder of the paper is organized as follows. In\nSection~\\ref{sec:relWork} we discuss related work. Section~\\ref{sec:recMod}\ndescribes our scene text recognition model and its components. We then present\nthe evaluation protocols and the datasets used in experimental analysis in\nSection~\\ref{datasets}. Comparison with related approaches is shown in\nSection~\\ref{sec:expts}, along with implementation details. We then make\nconcluding remarks in Section~\\ref{sec:conclusion}.\n\n\\section{Related Work}\n\\label{sec:relWork}\nThe task of understanding scene text has gained a huge interest for more than a\ndecade~\\cite{WangB11,NeumannM10,EpshteinW10,WeinmanLH09,weinman2013toward,pushmeetLexi,JoseBMVC13,fieldICDAR13,CamposBV09,ChenOB02,neumann2012real,photoOCR,AZ14,stroklets}.\nIt is closely related to the problem of Optical Character Recognition (OCR),\nwhich has a long history in the computer vision and pattern recognition\ncommunities~\\cite{TwentyYears}. However, the success of OCR systems is largely\nrestricted to text from scanned documents. Scene text exhibits a large\nvariability in appearance, as shown in Fig.~\\ref{fig:challenges2}, and can\nprove to be challenging even for the state-of-the-art OCR methods (see\nTable~\\ref{tab:ourdataset} and~\\cite{WangB11,NeumannM10}). The problems in this\ncontext are: (1) text localization, (2) cropped word recognition, and (3)\nisolated character recognition. They have been tackled either\nindividually~\\cite{EpshteinW10,CamposBV09,ChenY04}, or\njointly~\\cite{WangB11,AZ14,weinman2013toward,neumann2012real}. This paper\nfocuses on addressing the cropped word recognition problem. In other words,\ngiven an image region (e.g., in the form of a bounding box) containing text,\nthe task is to recognize this content. The core components of a typical cropped\nword recognition framework are: localize the characters, recognize them, and use\nstatistical language models to compose the characters into words. Our framework\nbuilds on these components, but differs from previous work in several ways. In\nthe following, we review the prior art and highlight these differences. The\nreader is encouraged to refer to~\\cite{PAMIsurvey} for a more comprehensive\nsurvey of scene text recognition methods.\n\nA popular technique for localizing characters in an OCR system is to\nbinarize the image and determine the potential character locations based on\nconnected components~\\cite{OCRlexi}. Such techniques have also been adapted for\nscene text recognition~\\cite{NeumannM10}, although with limited success. This\nis mainly because obtaining a clean binary output for scene text images is\noften challenging; see Fig.~\\ref{fig:challenges1} for examples. An\nalternative approach is proposed in~\\cite{shivakumarICDAR11} using gradient\ninformation to find potential character locations. More recently,\nYao~\\textit{et al.}~\\cite{stroklets} proposed a mid-level feature based\ntechnique to localize characters in scene text. We follow an alternative strategy\nand cast the character localization problem as an object detection task, where\ncharacters are the {\\it objects}.  We then define an energy function on all the\npotential characters.\n\nOne of the earliest works on large-scale natural scene character recognition\nwas presented in~\\cite{CamposBV09}. This work develops a multiple kernel\nlearning approach using a set of shape-based features. Recent\nwork~\\cite{WangB11,MishraCVPR12} has improved over this with histogram of\ngradient features~\\cite{DalalT05}. We perform an extensive analysis on\nfeatures, classifiers, and propose methods to improve character recognition\nfurther, for example, by augmenting the training set. In addition to this, we\nshow that the state-of-the-art CNN features~\\cite{AZ14} can be successfully\nintegrated with our word recognition framework to further boost its\nperformance.\n\nA study on human reading psychology shows that our reading improves\nsignificantly with prior knowledge of the language~\\cite{bookreading}.\nMotivated by such studies, OCR systems have used, often in post-processing\nsteps~\\cite{OCRlexi,tongOCRLexi}, statistical language models like $n$-grams to\nimprove their performance. Bigrams or trigrams have also been used in the\ncontext of scene text recognition as a post-processing step,\ne.g.,~\\cite{STRlexi}. A few other\nworks~\\cite{thillou2005,elagouni2011,elagouni2012combining} integrate character\nrecognition and linguistic knowledge to deal with recognition errors. For\nexample,~\\cite{thillou2005} computes $n$-gram probabilities from more than 100\nmillion characters and uses a Viterbi algorithm to find the correct word.  The\nmethod in~\\cite{elagouni2012combining}, developed in the same year as our CVPR\n2012 work~\\cite{MishraCVPR12}, builds a graph on potential character locations\nand uses $n$-gram scores to constrain the inference algorithm to predict the\nword. In contrast, our approach uses a novel location-specific prior\n(cf.~(\\ref{eq:lexicon2})).\n\n\n\nThe word recognition problem has been looked at in two contexts---\nwith~\\cite{WangB11,JoseBMVC13,MishraCVPR12,ngICPR12,GoelICDAR13} and\nwithout~\\cite{WeinmanLH09,MishraBMVC12,weinmanLH08} the use of an\nimage-specific lexicon. In the case of image-specific lexicon-driven word\nrecognition, also known as the closed vocabulary setting, a list of words is\navailable for every scene text image. The task of recognizing the word now\nreduces to that of finding the best match from this list. This is relevant in\nmany applications, e.g., recognizing text in a grocery store, where a list of\ngrocery items can serve as a lexicon. Wang~{\\emph{et al.}}~\\cite{ngICPR12} adapted a\nmulti-layer neural network for this scenario. In~\\cite{WangB11}, each word in\nthe lexicon is matched to the detected set of character windows, and the one\nwith the highest score is reported as the predicted word. In one of our\nprevious works~\\cite{GoelICDAR13}, we compared features computed on the entire\nscene text image and those generated from synthetic font renderings of lexicon\nwords with a novel weighted dynamic time warping (wDTW) approach to recognize\nwords. In~\\cite{JoseBMVC13} Rodriguez-Serrano and Perronnin proposed to embed\nword labels and word images into a common Euclidean space, wherein the text\nrecognition task is posed as a retrieval problem to find the closest word label\nfor a given word image. While all these approaches are interesting, their\nsuccess is largely restricted to the closed vocabulary setting and cannot be\neasily extended to the more general cases, for instance, when image-specific\nlexicon is unavailable. Weinman~{\\emph{et al.}}~\\cite{WeinmanLH09} proposed a method to\naddress this issue, although with a strong assumption of known character\nboundaries, which are not trivial to obtain with high precision on the datasets\nwe use. The work in~\\cite{weinmanLH08} generalizes their previous approach by\nrelaxing the character-boundary requirement. It is, however, evaluated only on\n``roughly fronto-parallel'' images of signs, which are less challenging than\nthe scene text images used in our work. \n\n\\begin{figure}[!t]\n\\centering\n\\subfigure{\n\\label{fig:subfig3}\n\\includegraphics[height=1.8cm,width=3.5cm]{figures/challenges/I1.eps}\n\n\\includegraphics[height=1.8cm,width=3.5cm]{figures/challenges/B1.eps}\n}\n\\hspace{0.8cm}\n\\subfigure{\n\\label{fig:subfig4}\n\\includegraphics[height=1.8cm,width=3.5cm]{figures/challenges/I2.eps}\n\n\\includegraphics[height=1.8cm,width=3.5cm]{figures/challenges/B2.eps}\n}\n\\caption{Binarization results obtained with one of the state-of-the-art\nmethods~\\cite{MishraJ11} are shown for two sample images. We observed similar\npoor performance on most of the images in scene text datasets, and hence do not\nuse binarization in our framework.}\n\\label{fig:challenges1}\n\\end{figure}\n\nOur work belongs to the class of word recognition methods\nwhich build on individual character localization, similar to methods such\nas~\\cite{NeumannM10,HoweFM09}. In this framework, the potential characters are\nlocalized, then a graph is constructed from these locations, and then the\nproblem of recognizing the word is formulated as finding an optimal path in\nthis graph~\\cite{NeumannM13} or inferring from an ensemble of\nHMMs~\\cite{HoweFM09}. Our approach shows a seamless integration of higher order\nlanguage priors into the graph (in the form of a CRF model), and uses\nmore effective modern computer vision features, thus making it clearly\ndifferent from previous works.\n\nSince the publication of our original work in CVPR 2012~\\cite{MishraCVPR12} and\nBMVC 2012~\\cite{MishraBMVC12} papers, several approaches for scene text\nunderstanding (e.g., text\nlocalization~\\cite{Milyaev13,neumann2012real,Jaderberg14a,Huang14}, word\nrecognition~\\cite{AZ14,weinman2013toward,photoOCR,stroklets,shiCVPR13,Jaderberg14a}\nand text-to-image retrieval~\\cite{Mishra13,Jaderberg14a,AlmazanGFV14,Roy14})\nhave been proposed. Notably, there has been an increasing interest in exploring\ndeep convolutional network based methods for scene text tasks\n(see~\\cite{AZ14,photoOCR,ngICPR12,Jaderberg14a,Huang14} for example). These\napproaches are very effective in general, but the deep convolutional network,\nwhich is at the core of these approaches, lacks the capability to elegantly\nhandle structured output data. To understand this with the help of an example,\nlet us consider the problem of estimating human\npose~\\cite{toshev2013deeppose,Tompson14}, where the task is to predict the\nlocations of human body joints such as head, shoulders, elbows and wrists.\nThese locations are constrained by human body kinematics and in essence form a\nstructured output. To deal with such structured output data, state-of-the-art\ndeep learning algorithms include an additional regression\nstep~\\cite{toshev2013deeppose} or a graphical model~\\cite{Tompson14}, thus\nshowing that these techniques are complementary to the deep learning\nphilosophy. Similar to human pose, text is structured output\ndata~\\cite{Jaderberg14b}. To better handle this structured data, we develop our\nenergy minimization framework~\\cite{MishraBMVC12,MishraCVPR12} with the\nmotivation of building a complementary approach, which can further benefit\nmethods built on the deep learning paradigm. Indeed, we see that combining the\ntwo frameworks further improves text recognition results\n(Section~\\ref{sec:expts}).\n\n\\section{The Recognition Model} \n\\label{sec:recMod}\nWe propose a conditional random field (CRF) model for recognizing words. The\nCRF is defined over a set of $N$ random variables $x = \\{x_{i} |i \\in\n\\mathcal{V}\\}$, where $\\mathcal{V} = \\{1, 2, \\ldots, N\\}$. Each random variable\n$x_i$ denotes a potential character in the word, and can take a label from the\nlabel set $\\mathcal{L} = \\{l_{1}, l_{2}, \\ldots, l_{k}\\} \\cup \\epsilon$, which\nis the set of English characters, digits and a null label $\\epsilon$ to discard\nfalse character detections. The most likely word represented by the set of\ncharacters $x$ is found by minimizing the energy function, $E:\\mathcal{L}^{n}\n\\rightarrow \\mathbb{R}$,\ncorresponding to the random field. The energy function $E$ can be \nwritten as sum of potential functions:\n\n", "index": 1, "text": "\\begin{equation}\nE(x)= \\sum_{c \\in \\cal{C}}{\\psi_{c} (x_{c})},\n\\label{eqn:energy}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"E(x)=\\sum_{c\\in\\cal{C}}{\\psi_{c}(x_{c})},\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>c</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi></mrow></munder><mrow><msub><mi>\u03c8</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>c</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nThe third order terms $\\psi_3(O,P,E)$ and $\\psi_3(P,E,N)$ are decomposed as\nfollows.\n\n", "itemtype": "equation", "pos": 20382, "prevtext": "\nwhere $\\cal{C} \\subset \\mathcal{P}(\\mathcal{V})$, with\n$\\mathcal{P}(\\mathcal{V})$ denoting the powerset of $\\mathcal{V}$. Each $x_{c}$\ndefines a set of random variables included in subset $c$, referred to as a\nclique. The function $\\psi_c$ defines a constraint (potential) on the\ncorresponding clique $c$. We use unary, pairwise and higher order potentials in\nthis work, and define them in Section~\\ref{sec:GC}. The set of potential\ncharacters is obtained by the character detection step discussed in\nSection~\\ref{sec:charDet}. The neighbourhood relations among characters,\nmodelled as pairwise and higher order potentials, are based on the spatial\narrangement of characters in the word image.\n\nIn the following we show an example energy function composed of unary, pairwise\nand higher order (of clique size three) terms on a sample word with four\ncharacters. For a word to be recognized as ``OPEN'' the following energy\nfunction should be the minimum.\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{aligned}\n\\psi(O,P,E,N)& = \\psi_{1}(O)+\\psi_{1}(P)+\\psi_{1}(E)+\\psi_{1}(N)\\nonumber\\\\\n&\\quad + \\psi_{2}(O,P) + \\psi_{2}(P,E) + \\psi_{2}(E,N)\\\\\n&\\quad + \\psi_3(O,P,E) + \\psi_3(P,E,N).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\psi(O,P,E,N)\" display=\"inline\"><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>O</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\psi_{1}(O)+\\psi_{1}(P)+\\psi_{1}(E)+\\psi_{1}(N)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>O</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad+\\psi_{2}(O,P)+\\psi_{2}(P,E)+\\psi_{2}(E,N)\" display=\"inline\"><mrow><mrow><mo lspace=\"12.5pt\">+</mo><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>O</mi><mo>,</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad+\\psi_{3}(O,P,E)+\\psi_{3}(P,E,N).\" display=\"inline\"><mrow><mrow><mrow><mo lspace=\"12.5pt\">+</mo><mrow><msub><mi>\u03c8</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>O</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03c8</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 20702, "prevtext": "\nThe third order terms $\\psi_3(O,P,E)$ and $\\psi_3(P,E,N)$ are decomposed as\nfollows.\n\n", "index": 5, "text": "\\begin{equation}\n\\begin{aligned}\n\\psi_3(O,P,E) & = \\psi^a_1(OPE) + \\psi^a_2(OPE,O) \\nonumber \\\\\n              &\\quad + \\psi^a_2(OPE,P) + \\psi^a_2(OPE,E).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\psi_{3}(O,P,E)\" display=\"inline\"><mrow><msub><mi>\u03c8</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>O</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\psi^{a}_{1}(OPE)+\\psi^{a}_{2}(OPE,O)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msubsup><mi>\u03c8</mi><mn>1</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>O</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>O</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi></mrow><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad+\\psi^{a}_{2}(OPE,P)+\\psi^{a}_{2}(OPE,E).\" display=\"inline\"><mrow><mrow><mrow><mo lspace=\"12.5pt\">+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>O</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi></mrow><mo>,</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>O</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi></mrow><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\n\\subsection{Character Detection}\n\\label{sec:charDet}\nThe first step in our approach is to detect potential locations of characters\nin a word image. In this work we use a sliding window based approach for\ndetecting characters, but other methods, e.g.,~\\cite{stroklets}, can also be\nused instead.\n\n\\begin{figure}[!t]\n\n\\subfigure{\n\\label{fig:subfig1}\n\\includegraphics[height=1.5cm,width=3cm]{figures/I11.eps}\n\n\\includegraphics[height=1.0cm,width=0.6cm]{figures/I1.eps}\n}\n\\hspace{0.4cm}\n\\subfigure{\n\\label{fig:subfig2}\n\\includegraphics[height=1.5cm,width=3cm]{figures/I21.eps}\n\n\\includegraphics[height=1.5cm,width=0.5cm]{figures/I2.eps}\n}\n\\caption{Typical challenges in character detection. (a) Inter-character\nconfusion: A window containing parts of the two $o$'s is falsely detected as\n$x$. (b) Intra-character confusion: A window containing a part of the character\nB is recognized as E.}\n\\label{fig:challenges3}\n\\end{figure}\n\n\\paragraph{Sliding window detection}\nThis technique has been very successful for tasks such as, face~\\cite{Viola01}\nand pedestrian~\\cite{DalalT05} detection, and also for recognizing handwritten\nwords using HMM based methods~\\cite{hmm1}. Although character detection in\nscene images is similar to such problems, it has its unique challenges.\nFirstly, there is the issue of dealing with many categories ($63$ in all)\njointly. Secondly, there is a large amount of inter-character and\nintra-character confusion, as illustrated in Fig.~\\ref{fig:challenges3}. When a\nwindow contains parts of two characters next to each other, it may have a very\nsimilar appearance to another character. In Fig.~\\ref{fig:challenges3}(a), the\nwindow containing parts of the characters `$o$' can be confused with `$x$'.\nFurthermore, a part of one character can have the same appearance as that of\nanother. In Fig.~\\ref{fig:challenges3}(b), a part of the character `B' can be\nconfused with `E'. We build a robust character classifier and adopt an\nadditional pruning stage to overcome these issues.\n\nThe problem of classifying natural scene characters typically suffers from the\nlack of training data, e.g., \\cite{CamposBV09} uses only 15 samples per class.\nIt is not trivial to model the large variations in characters using only a few\nexamples. To address this, we add more examples to the training set by applying\nsmall affine transformations~\\cite{simardNIPS91,MozerNIPS97} to the original\ncharacter images. We further enrich the training set by adding many\nnon-character negative examples, i.e., from the background. With this strategy,\nwe achieve a significant boost in character classification accuracy (see\nTable~\\ref{tab:charClassification}).\n\nWe consider windows at multiple scales and spatial locations. The location of\nthe $i$th window, $d_i$, is given by its center and size. The set $\\mathcal{K}\n= \\{c_1, c_2, \\ldots, c_k\\}$, denotes label set. Note that $k=63$ for the set\nof English characters, digits and a background class (null label) in our work.\nLet $\\phi_{i}$ denote the features extracted from a window location $d_i$.\nGiven the window $d_i$, we compute the likelihood, $p(c_j|\\phi_i)$, of it\ntaking a label $c_j$ for all the classes in $\\mathcal{K}$. In our\nimplementation, we used explicit feature representation~\\cite{VedaldiPAMI12} of\nhistogram of gradient (HOG) features~\\cite{DalalT05} for $\\phi_i$, and the\nlikelihoods $p$ are (normalized) scores from a one vs rest multi-class support\nvector machine (SVM). Implementation details of the training procedure are\nprovided in Section~\\ref{subsec:charclassif}.\n\nThis basic sliding window detection approach produces many potential character\nwindows, but not all of them are useful for recognizing words. We discard some\nof the weak detection windows using the following pruning method.\n\\begin{figure}[!t]\n\\centering\n\\subfigure[]\n{\n \\includegraphics[scale=0.2]{figures/AR/1.eps}\n}\n\\subfigure[]\n{\n\\includegraphics[scale=0.2]{figures/AR/3.eps}\n} \n\\subfigure[]\n{\n \\includegraphics[scale=0.2]{figures/AR/37.eps}\n}\n\\subfigure[]\n{\n\\includegraphics[scale=0.2]{figures/AR/61.eps}\n} \n\\caption{Distribution of aspect ratios of few digits and characters: (a) 0 (b)\n2 (c) B (d) Y. The aspect ratios are computed on character from the IIIT-5K\nword training set.}\n\\label{fig:ar}\n\\end{figure}\n\n\\begin{figure*}[!t]\n\\centering\n \\includegraphics[scale=0.5]{figures/pgm3.eps}\n\\caption{The proposed model illustrated as a graph. Given a word image (shown\non the left), we evaluate character detectors and obtain potential character\nwindows, which are then represented in a graph. These nodes are connected with\nedges based on their spatial positioning. Each node can take a label from the\nlabel set containing English characters, digits, and a null label (to suppress\nfalse detections). To integrate language models, i.e., $n$-grams, into the\ngraph, we add auxiliary nodes (shown in red), which constrain several character\nwindows together (sets of $4$ characters in this example). Auxiliary nodes take\nlabels from a label set containing all valid English $n$-grams and an\nadditional label to enforce high cost for an invalid $n$-gram.}\n\\label{fig:pgm}\n\\end{figure*}\n\n\\paragraph{Pruning windows}\nFor every potential character window, we compute a score based on: (i) SVM\nclassifier confidence, and (ii) a measure of the aspect ratio of the character\ndetected and the aspect ratio learnt for that character from training data. The\nintuition behind this score is that, a strong character window candidate should\nhave a high classifier confidence score, and must fall within some range of the\nsizes observed in the training data. In order to define the aspect ratio\nmeasure, we observed the distribution of aspect ratios of characters from the\nIIIT-5K word training set. A few examples of these distributions are shown in\nFig.~\\ref{fig:ar}. Since they follow a Gaussian distribution, we chose this\nscore accordingly. For a window $d_i$ with an aspect ratio $a_i$, let $c_j$\ndenote the character with the best classifier confidence value given by\n$S_{ij}$. The mean aspect ratio for the character $c_j$ computed from training\ndata is denoted by $\\mu_{a_j}$. We define a goodness score (GS) for the window\n$d_i$ as:\n\n", "itemtype": "equation", "pos": 20885, "prevtext": "\n\n", "index": 7, "text": "\\begin{equation}\n\\begin{aligned}\n\\psi_3(P,E,N)& = \\psi^a_1(PEN) + \\psi^a_2(PEN,P)\\nonumber\\\\\n             &\\quad + \\psi^a_2(PEN,E) + \\psi^a_2(PEN,N).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\psi_{3}(P,E,N)\" display=\"inline\"><mrow><msub><mi>\u03c8</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>E</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\psi^{a}_{1}(PEN)+\\psi^{a}_{2}(PEN,P)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msubsup><mi>\u03c8</mi><mn>1</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>N</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>N</mi></mrow><mo>,</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad+\\psi^{a}_{2}(PEN,E)+\\psi^{a}_{2}(PEN,N).\" display=\"inline\"><mrow><mrow><mrow><mo lspace=\"12.5pt\">+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>N</mi></mrow><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>N</mi></mrow><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $\\sigma_{a_{j}}$ is the variance of the aspect ratio for character\n$c_{j}$ in the training data. A low goodness score indicates a weak detection,\nwhich is then removed from the set of candidate character windows.\n\nWe then apply character-specific non-maximum suppression (NMS), similar to\nother sliding window detection methods~\\cite{Felzen10}, to address the issue of\nmultiple overlapping detections for each instance of a character. In other\nwords, for every character class, we select detections which have a high\nconfidence score, and do not overlap significantly with any of the other\nstronger detections of the same character class. We perform NMS after aspect\nratio pruning to avoid wide windows with many characters suppressing weaker\nsingle character windows they overlap with. The pruning and NMS steps are\nperformed conservatively, to discard only the obvious false detections. The\nremaining false positives are modelled in an energy minimization framework with\nlanguage priors and other cues, as discussed below.\n\n\n\n\\subsection{Graph Construction and Energy Formulation}\n\\label{sec:GC}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe solve the problem of minimizing the energy function (\\ref{eqn:energy}) on a\ncorresponding graph, where each random variable is represented as a node in the\ngraph. We begin by ordering the character windows based on their horizontal\nlocation in the image, and add one node each for every window sequentially from\nleft to right. The nodes are then connected by edges. Since it is not natural\nfor a window on the extreme left to be strongly related to another window on\nthe extreme right, we only connect windows which are close to each other. The\nintuition behind close-proximity windows is that they could represent\ndetections of two separate characters. As we will see later, the edges are used\nto encode the language model as top-down cues. Such pairwise language priors\nalone may not be sufficient in some cases, for example, when an image-specific\nlexicon is unavailable. Thus, we also integrate higher order language priors in\nthe form of $n$-grams computed from the English dictionary by adding an\nauxiliary node connecting a set of $n$ character detection nodes.\n\nEach (non-auxiliary) node in the graph takes one label from the label set\n$\\mathcal{L} = \\{l_{1}, l_{2}, \\ldots, l_{k}\\} \\cup \\epsilon$. Recall that each\n$l_{u}$ is an English character or digit, and the null label $\\epsilon$ is used\nto discard false windows that represent background or parts of characters. The\ncost associated with this label assignment is known as the unary cost. The cost\nfor two neighbouring nodes taking labels $l_{u}$ and $l_{v}$ is known as the\npairwise cost. This cost is computed from bigram scores of character\npairs in the English dictionary or an image-specific lexicon. The auxiliary\nnodes in the graph take labels from the extended label set $\\mathcal{L}_{e}$.\nEach element of $\\mathcal{L}_{e}$ represents one of the $n$-grams present in\nthe dictionary and an additional label to assign a constant (high) cost to all\n$n$-grams that are not in the dictionary. The proposed model is illustrated in\nFig.~\\ref{fig:pgm}, where we show a CRF of order four as an example. Once the\ngraph is constructed, we compute its corresponding cost functions as follows.\n\n\\subsubsection{Unary cost}\nThe unary cost of a node taking a character label is determined by the SVM\nconfidence scores. The unary term $\\psi_{1}$, which denotes the cost of a node\n$x_i$ taking label $l_u$, is defined as:\n\n", "itemtype": "equation", "pos": 27229, "prevtext": "\n\\subsection{Character Detection}\n\\label{sec:charDet}\nThe first step in our approach is to detect potential locations of characters\nin a word image. In this work we use a sliding window based approach for\ndetecting characters, but other methods, e.g.,~\\cite{stroklets}, can also be\nused instead.\n\n\\begin{figure}[!t]\n\n\\subfigure{\n\\label{fig:subfig1}\n\\includegraphics[height=1.5cm,width=3cm]{figures/I11.eps}\n\n\\includegraphics[height=1.0cm,width=0.6cm]{figures/I1.eps}\n}\n\\hspace{0.4cm}\n\\subfigure{\n\\label{fig:subfig2}\n\\includegraphics[height=1.5cm,width=3cm]{figures/I21.eps}\n\n\\includegraphics[height=1.5cm,width=0.5cm]{figures/I2.eps}\n}\n\\caption{Typical challenges in character detection. (a) Inter-character\nconfusion: A window containing parts of the two $o$'s is falsely detected as\n$x$. (b) Intra-character confusion: A window containing a part of the character\nB is recognized as E.}\n\\label{fig:challenges3}\n\\end{figure}\n\n\\paragraph{Sliding window detection}\nThis technique has been very successful for tasks such as, face~\\cite{Viola01}\nand pedestrian~\\cite{DalalT05} detection, and also for recognizing handwritten\nwords using HMM based methods~\\cite{hmm1}. Although character detection in\nscene images is similar to such problems, it has its unique challenges.\nFirstly, there is the issue of dealing with many categories ($63$ in all)\njointly. Secondly, there is a large amount of inter-character and\nintra-character confusion, as illustrated in Fig.~\\ref{fig:challenges3}. When a\nwindow contains parts of two characters next to each other, it may have a very\nsimilar appearance to another character. In Fig.~\\ref{fig:challenges3}(a), the\nwindow containing parts of the characters `$o$' can be confused with `$x$'.\nFurthermore, a part of one character can have the same appearance as that of\nanother. In Fig.~\\ref{fig:challenges3}(b), a part of the character `B' can be\nconfused with `E'. We build a robust character classifier and adopt an\nadditional pruning stage to overcome these issues.\n\nThe problem of classifying natural scene characters typically suffers from the\nlack of training data, e.g., \\cite{CamposBV09} uses only 15 samples per class.\nIt is not trivial to model the large variations in characters using only a few\nexamples. To address this, we add more examples to the training set by applying\nsmall affine transformations~\\cite{simardNIPS91,MozerNIPS97} to the original\ncharacter images. We further enrich the training set by adding many\nnon-character negative examples, i.e., from the background. With this strategy,\nwe achieve a significant boost in character classification accuracy (see\nTable~\\ref{tab:charClassification}).\n\nWe consider windows at multiple scales and spatial locations. The location of\nthe $i$th window, $d_i$, is given by its center and size. The set $\\mathcal{K}\n= \\{c_1, c_2, \\ldots, c_k\\}$, denotes label set. Note that $k=63$ for the set\nof English characters, digits and a background class (null label) in our work.\nLet $\\phi_{i}$ denote the features extracted from a window location $d_i$.\nGiven the window $d_i$, we compute the likelihood, $p(c_j|\\phi_i)$, of it\ntaking a label $c_j$ for all the classes in $\\mathcal{K}$. In our\nimplementation, we used explicit feature representation~\\cite{VedaldiPAMI12} of\nhistogram of gradient (HOG) features~\\cite{DalalT05} for $\\phi_i$, and the\nlikelihoods $p$ are (normalized) scores from a one vs rest multi-class support\nvector machine (SVM). Implementation details of the training procedure are\nprovided in Section~\\ref{subsec:charclassif}.\n\nThis basic sliding window detection approach produces many potential character\nwindows, but not all of them are useful for recognizing words. We discard some\nof the weak detection windows using the following pruning method.\n\\begin{figure}[!t]\n\\centering\n\\subfigure[]\n{\n \\includegraphics[scale=0.2]{figures/AR/1.eps}\n}\n\\subfigure[]\n{\n\\includegraphics[scale=0.2]{figures/AR/3.eps}\n} \n\\subfigure[]\n{\n \\includegraphics[scale=0.2]{figures/AR/37.eps}\n}\n\\subfigure[]\n{\n\\includegraphics[scale=0.2]{figures/AR/61.eps}\n} \n\\caption{Distribution of aspect ratios of few digits and characters: (a) 0 (b)\n2 (c) B (d) Y. The aspect ratios are computed on character from the IIIT-5K\nword training set.}\n\\label{fig:ar}\n\\end{figure}\n\n\\begin{figure*}[!t]\n\\centering\n \\includegraphics[scale=0.5]{figures/pgm3.eps}\n\\caption{The proposed model illustrated as a graph. Given a word image (shown\non the left), we evaluate character detectors and obtain potential character\nwindows, which are then represented in a graph. These nodes are connected with\nedges based on their spatial positioning. Each node can take a label from the\nlabel set containing English characters, digits, and a null label (to suppress\nfalse detections). To integrate language models, i.e., $n$-grams, into the\ngraph, we add auxiliary nodes (shown in red), which constrain several character\nwindows together (sets of $4$ characters in this example). Auxiliary nodes take\nlabels from a label set containing all valid English $n$-grams and an\nadditional label to enforce high cost for an invalid $n$-gram.}\n\\label{fig:pgm}\n\\end{figure*}\n\n\\paragraph{Pruning windows}\nFor every potential character window, we compute a score based on: (i) SVM\nclassifier confidence, and (ii) a measure of the aspect ratio of the character\ndetected and the aspect ratio learnt for that character from training data. The\nintuition behind this score is that, a strong character window candidate should\nhave a high classifier confidence score, and must fall within some range of the\nsizes observed in the training data. In order to define the aspect ratio\nmeasure, we observed the distribution of aspect ratios of characters from the\nIIIT-5K word training set. A few examples of these distributions are shown in\nFig.~\\ref{fig:ar}. Since they follow a Gaussian distribution, we chose this\nscore accordingly. For a window $d_i$ with an aspect ratio $a_i$, let $c_j$\ndenote the character with the best classifier confidence value given by\n$S_{ij}$. The mean aspect ratio for the character $c_j$ computed from training\ndata is denoted by $\\mu_{a_j}$. We define a goodness score (GS) for the window\n$d_i$ as:\n\n", "index": 9, "text": "\\begin{equation}\n\\text{GS}(d_{i}) = S_{ij} \\exp \\left (-\\frac{(\\mu_{a_{j}} - a_{i})^{2}}{2\\sigma_{a_{j}}^2} \\right ),\n\\label{eq:gs}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\text{GS}(d_{i})=S_{ij}\\exp\\left(-\\frac{(\\mu_{a_{j}}-a_{i})^{2}}{2\\sigma_{a_{j%&#10;}}^{2}}\\right),\" display=\"block\"><mrow><mrow><mrow><mtext>GS</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>-</mo><msub><mi>a</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03c3</mi><msub><mi>a</mi><mi>j</mi></msub><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $p(l_{u} | x_{i})$ is the SVM score of character class $l_{u}$ for node\n$x_{i}$, normalized with Platt's method~\\cite{Platt99SVM}. The cost of $x_{i}$\ntaking the null label $\\epsilon$ is given by:\n\n", "itemtype": "equation", "pos": 30877, "prevtext": "\nwhere $\\sigma_{a_{j}}$ is the variance of the aspect ratio for character\n$c_{j}$ in the training data. A low goodness score indicates a weak detection,\nwhich is then removed from the set of candidate character windows.\n\nWe then apply character-specific non-maximum suppression (NMS), similar to\nother sliding window detection methods~\\cite{Felzen10}, to address the issue of\nmultiple overlapping detections for each instance of a character. In other\nwords, for every character class, we select detections which have a high\nconfidence score, and do not overlap significantly with any of the other\nstronger detections of the same character class. We perform NMS after aspect\nratio pruning to avoid wide windows with many characters suppressing weaker\nsingle character windows they overlap with. The pruning and NMS steps are\nperformed conservatively, to discard only the obvious false detections. The\nremaining false positives are modelled in an energy minimization framework with\nlanguage priors and other cues, as discussed below.\n\n\n\n\\subsection{Graph Construction and Energy Formulation}\n\\label{sec:GC}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe solve the problem of minimizing the energy function (\\ref{eqn:energy}) on a\ncorresponding graph, where each random variable is represented as a node in the\ngraph. We begin by ordering the character windows based on their horizontal\nlocation in the image, and add one node each for every window sequentially from\nleft to right. The nodes are then connected by edges. Since it is not natural\nfor a window on the extreme left to be strongly related to another window on\nthe extreme right, we only connect windows which are close to each other. The\nintuition behind close-proximity windows is that they could represent\ndetections of two separate characters. As we will see later, the edges are used\nto encode the language model as top-down cues. Such pairwise language priors\nalone may not be sufficient in some cases, for example, when an image-specific\nlexicon is unavailable. Thus, we also integrate higher order language priors in\nthe form of $n$-grams computed from the English dictionary by adding an\nauxiliary node connecting a set of $n$ character detection nodes.\n\nEach (non-auxiliary) node in the graph takes one label from the label set\n$\\mathcal{L} = \\{l_{1}, l_{2}, \\ldots, l_{k}\\} \\cup \\epsilon$. Recall that each\n$l_{u}$ is an English character or digit, and the null label $\\epsilon$ is used\nto discard false windows that represent background or parts of characters. The\ncost associated with this label assignment is known as the unary cost. The cost\nfor two neighbouring nodes taking labels $l_{u}$ and $l_{v}$ is known as the\npairwise cost. This cost is computed from bigram scores of character\npairs in the English dictionary or an image-specific lexicon. The auxiliary\nnodes in the graph take labels from the extended label set $\\mathcal{L}_{e}$.\nEach element of $\\mathcal{L}_{e}$ represents one of the $n$-grams present in\nthe dictionary and an additional label to assign a constant (high) cost to all\n$n$-grams that are not in the dictionary. The proposed model is illustrated in\nFig.~\\ref{fig:pgm}, where we show a CRF of order four as an example. Once the\ngraph is constructed, we compute its corresponding cost functions as follows.\n\n\\subsubsection{Unary cost}\nThe unary cost of a node taking a character label is determined by the SVM\nconfidence scores. The unary term $\\psi_{1}$, which denotes the cost of a node\n$x_i$ taking label $l_u$, is defined as:\n\n", "index": 11, "text": "\\begin{equation}\n\\psi_{1}(x_{i}=l_{u}) = 1 - p(l_{u} | x_{i}),\n\\label{eq:unary1} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\psi_{1}(x_{i}=l_{u})=1-p(l_{u}|x_{i}),\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>l</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>1</mn><mo>-</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $a_{i}$ is the aspect ratio of the window corresponding to node $x_{i}$,\n$\\mu_{a_u}$ and $\\sigma_{a_u}$ are the mean and variance of the aspect ratio\nrespectively of the character $l_u$, computed from the\ntraining data. The intuition behind this cost function is that, for taking a\ncharacter label, the detected window should have a high classifier confidence\nand its aspect ratio should agree with that of the corresponding character in\nthe training data.\n\n\\subsubsection{Pairwise cost}\nThe pairwise cost of two neighbouring nodes $x_{i}$ and $x_{j}$ taking a pair\nof labels $l_{u}$ and $l_{v}$ respectively is determined by the cost of\ntheir joint occurrence in the dictionary. This cost $\\psi_2$ is given by:\n\n", "itemtype": "equation", "pos": 31177, "prevtext": "\nwhere $p(l_{u} | x_{i})$ is the SVM score of character class $l_{u}$ for node\n$x_{i}$, normalized with Platt's method~\\cite{Platt99SVM}. The cost of $x_{i}$\ntaking the null label $\\epsilon$ is given by:\n\n", "index": 13, "text": "\\begin{equation}\n\\psi_{1}(x_{i}=\\epsilon) = \\max_{u} p(l_{u} | x_{i}) \\exp\\left(-\\frac{(\\mu_{a_u} - a_{i})^{2}}{\\sigma_{a_u}^2}\\right),\n\\label{eq:unary2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\psi_{1}(x_{i}=\\epsilon)=\\max_{u}p(l_{u}|x_{i})\\exp\\left(-\\frac{(\\mu_{a_{u}}-a%&#10;_{i})^{2}}{\\sigma_{a_{u}}^{2}}\\right),\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mi>max</mi><mi>u</mi></munder><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>exp</mi><mrow><mo>(</mo><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><msub><mi>a</mi><mi>u</mi></msub></msub><mo>-</mo><msub><mi>a</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msubsup><mi>\u03c3</mi><msub><mi>a</mi><mi>u</mi></msub><mn>2</mn></msubsup></mfrac><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $p(l_{u},l_{v})$ is the score determining the likelihood of the pair\n$l_{u}$ and $l_{v}$ occurring together in the dictionary. The parameters\n$\\lambda_{\\textup{{l}}}$ and $\\beta$ are set empirically as $\\lambda_{\\textup{{l}}}=2$ and\n$\\beta=50$ in all our experiments. The score $p(l_{u},l_{v})$ is commonly\ncomputed from joint occurrences of characters in the\nlexicon~\\cite{thillou2005,elagouni2011,elagouni2012combining,SmithR11}. This\nprior is effective when the lexicon size is small, but it is less so as the\nlexicon increases in size. Furthermore, it fails to capture the\nlocation-specific information of pairs of characters. As a toy example,\nconsider a lexicon with only two words CVPR and ICPR. Here, the character pair\n{(P,R)} is more likely to occur at the end of the word, but a standard bigram\nprior model does not incorporate this location-specific information.\n\n\n\n\n\nTo overcome the lack of location-specific information, we devise a\nnode-specific pairwise cost by adapting~\\cite{Riseman} to the scene text\nrecognition problem.\n\nWe divide a given word image into $T$ parts, where $T$ is an estimate of the\nnumber of characters in the image. This estimate $T$ is given by the image\nwidth divided by the average character window width, with the average computed\nover all the detected characters in the image. To determine the pairwise cost\ninvolving windows in the $t$ th image part, we define a region of interest\n(ROI) which includes the two adjacent parts $t-1$, $t+1$, in addition to $t$.\nWith this, we do a ROI based search in the lexicon. In other words, we consider\nall the character pairs involving characters in locations $t-1$, $t$ and $t+1$\nin all the lexicon words to compute the likelihood of a pair occurring\ntogether. Note that the extreme cases (involving the leftmost and rightmost\ncharacter in the lexicon word) are treated appropriately by considering only\none of the two pairs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis pairwise cost using the node-specific prior is given by:\n\n", "itemtype": "equation", "pos": 32064, "prevtext": "\nwhere $a_{i}$ is the aspect ratio of the window corresponding to node $x_{i}$,\n$\\mu_{a_u}$ and $\\sigma_{a_u}$ are the mean and variance of the aspect ratio\nrespectively of the character $l_u$, computed from the\ntraining data. The intuition behind this cost function is that, for taking a\ncharacter label, the detected window should have a high classifier confidence\nand its aspect ratio should agree with that of the corresponding character in\nthe training data.\n\n\\subsubsection{Pairwise cost}\nThe pairwise cost of two neighbouring nodes $x_{i}$ and $x_{j}$ taking a pair\nof labels $l_{u}$ and $l_{v}$ respectively is determined by the cost of\ntheir joint occurrence in the dictionary. This cost $\\psi_2$ is given by:\n\n", "index": 15, "text": "\\begin{equation}\n\\psi_{2}(x_{i}=l_{u},x_{j}=l_{v}) = \\lambda_{{\\textup{{l}}}} \\exp(- \\beta p(l_{u},l_{v})),\n\\label{eq:pair1} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\psi_{2}(x_{i}=l_{u},x_{j}=l_{v})=\\lambda_{{\\textup{{l}}}}\\exp(-\\beta p(l_{u},%&#10;l_{v})),\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi>\u03bb</mi><mtext>l</mtext></msub><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mi>\u03b2</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nWe evaluated our approach with both the pairwise terms (\\ref{eq:pair1}) and\n(\\ref{eq:lexicon2}), and found that the node-specific prior (\\ref{eq:lexicon2})\nachieves better performance. The cost of nodes $x_{i}$ and $x_{j}$ taking label\n$l_{u}$ and $\\epsilon$ respectively is defined as:\n\n", "itemtype": "equation", "pos": 34196, "prevtext": "\nwhere $p(l_{u},l_{v})$ is the score determining the likelihood of the pair\n$l_{u}$ and $l_{v}$ occurring together in the dictionary. The parameters\n$\\lambda_{\\textup{{l}}}$ and $\\beta$ are set empirically as $\\lambda_{\\textup{{l}}}=2$ and\n$\\beta=50$ in all our experiments. The score $p(l_{u},l_{v})$ is commonly\ncomputed from joint occurrences of characters in the\nlexicon~\\cite{thillou2005,elagouni2011,elagouni2012combining,SmithR11}. This\nprior is effective when the lexicon size is small, but it is less so as the\nlexicon increases in size. Furthermore, it fails to capture the\nlocation-specific information of pairs of characters. As a toy example,\nconsider a lexicon with only two words CVPR and ICPR. Here, the character pair\n{(P,R)} is more likely to occur at the end of the word, but a standard bigram\nprior model does not incorporate this location-specific information.\n\n\n\n\n\nTo overcome the lack of location-specific information, we devise a\nnode-specific pairwise cost by adapting~\\cite{Riseman} to the scene text\nrecognition problem.\n\nWe divide a given word image into $T$ parts, where $T$ is an estimate of the\nnumber of characters in the image. This estimate $T$ is given by the image\nwidth divided by the average character window width, with the average computed\nover all the detected characters in the image. To determine the pairwise cost\ninvolving windows in the $t$ th image part, we define a region of interest\n(ROI) which includes the two adjacent parts $t-1$, $t+1$, in addition to $t$.\nWith this, we do a ROI based search in the lexicon. In other words, we consider\nall the character pairs involving characters in locations $t-1$, $t$ and $t+1$\nin all the lexicon words to compute the likelihood of a pair occurring\ntogether. Note that the extreme cases (involving the leftmost and rightmost\ncharacter in the lexicon word) are treated appropriately by considering only\none of the two pairs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis pairwise cost using the node-specific prior is given by:\n\n", "index": 17, "text": "\\begin{equation}\n\\psi_{2}(x_{i}=l_{u},x_{j}=l_{v}) = \\left\\{\n\\begin{array}{ll}\n0 & \\text{if $(l_{u},l_{v}) \\in$ \\textsc{roi},} \\\\\n\\lambda_{\\textup{{l}}} & \\mbox{otherwise}. \\end{array}\n\\right.\n\\label{eq:lexicon2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\psi_{2}(x_{i}=l_{u},x_{j}=l_{v})=\\left\\{\\begin{array}[]{ll}0&amp;\\text{if $(l_{u}%&#10;,l_{v})\\in$ {roi},}\\\\&#10;\\lambda_{\\textup{{l}}}&amp;\\mbox{otherwise}.\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi/></mrow><mtext>\u00a0</mtext><mtext class=\"ltx_font_smallcaps\" mathvariant=\"normal\">roi</mtext><mtext>,</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>\u03bb</mi><mtext>l</mtext></msub></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $O(x_{i},x_{j})$ is the overlap fraction between windows corresponding to\nthe nodes $x_{i}$ and $x_{j}$. The pairwise cost $\\psi_2(x_{i} = \\epsilon,\nx_{j} = l_{u})$ is defined similarly. The parameters are set empirically as\n$\\lambda_{\\textup{{o}}}=2$ and $\\beta=50$ in our experiments. This cost ensures that when two character windows overlap significantly, only one of them are assigned a\ncharacter/digit label in order to avoid parts of characters being labelled.\n\n\\subsubsection{Higher order cost}\nLet us consider a CRF of order $n = 3$ as an example to understand this cost.\nAn auxiliary node corresponding to every clique of size $3$ is added to\nrepresent this third order cost in the graph. The higher order cost is then\ndecomposed into unary and pairwise terms with respect to this node, similar\nto~\\cite{luborUAI10}. Each auxiliary node in the graph takes one of the labels\nfrom the extended label set $\\{L_{1},L_{2},\\ldots,L_{M}\\} \\cup L_{M+1}$, where\nlabels $L_{1} \\dots L_{M}$ represent all the trigrams in the dictionary. The\nadditional label $L_{M+1}$ denotes all those trigrams which are absent in the\ndictionary. The unary cost $\\psi^a_{1}$ for an auxiliary variable $y_i$ taking\nlabel $L_m$ is:\n\n", "itemtype": "equation", "pos": 34711, "prevtext": "\nWe evaluated our approach with both the pairwise terms (\\ref{eq:pair1}) and\n(\\ref{eq:lexicon2}), and found that the node-specific prior (\\ref{eq:lexicon2})\nachieves better performance. The cost of nodes $x_{i}$ and $x_{j}$ taking label\n$l_{u}$ and $\\epsilon$ respectively is defined as:\n\n", "index": 19, "text": "\\begin{equation}\n\\psi_2(x_{i} = l_{u},x_{j} = \\epsilon) = \\lambda_{\\textup{{o}}} \\exp(-\\beta(1-O(x_{i},x_{j}))^{2}),\n\\label{eq:pair2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\psi_{2}(x_{i}=l_{u},x_{j}=\\epsilon)=\\lambda_{\\textup{{o}}}\\exp(-\\beta(1-O(x_{%&#10;i},x_{j}))^{2}),\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi>\u03bb</mi><mtext>o</mtext></msub><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mi>\u03b2</mi><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>O</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $\\lambda_{\\textup{{a}}}$ is a constant. We set $\\lambda_{\\textup{{a}}}=5$\nempirically, in all our experiments, unless stated otherwise. The parameter\n$\\beta$ controls penalty between dictionary and non-dictionary $n$-grams, and\nis empirically set to $50$. The score $P(L_m)$ denotes the likelihood of\ntrigram $L_m$ in the English, and is further described in\nSection~\\ref{sec:langPriors}. The pairwise cost between the auxiliary node\n$y_i$ taking a label $L_{m} = l_{u} l_{v} l_{w}$ and the left-most\nnon-auxiliary node in the clique, $x_{i}$, taking a label $l_{r}$ is given by:\n\n", "itemtype": "equation", "pos": 36079, "prevtext": "\nwhere $O(x_{i},x_{j})$ is the overlap fraction between windows corresponding to\nthe nodes $x_{i}$ and $x_{j}$. The pairwise cost $\\psi_2(x_{i} = \\epsilon,\nx_{j} = l_{u})$ is defined similarly. The parameters are set empirically as\n$\\lambda_{\\textup{{o}}}=2$ and $\\beta=50$ in our experiments. This cost ensures that when two character windows overlap significantly, only one of them are assigned a\ncharacter/digit label in order to avoid parts of characters being labelled.\n\n\\subsubsection{Higher order cost}\nLet us consider a CRF of order $n = 3$ as an example to understand this cost.\nAn auxiliary node corresponding to every clique of size $3$ is added to\nrepresent this third order cost in the graph. The higher order cost is then\ndecomposed into unary and pairwise terms with respect to this node, similar\nto~\\cite{luborUAI10}. Each auxiliary node in the graph takes one of the labels\nfrom the extended label set $\\{L_{1},L_{2},\\ldots,L_{M}\\} \\cup L_{M+1}$, where\nlabels $L_{1} \\dots L_{M}$ represent all the trigrams in the dictionary. The\nadditional label $L_{M+1}$ denotes all those trigrams which are absent in the\ndictionary. The unary cost $\\psi^a_{1}$ for an auxiliary variable $y_i$ taking\nlabel $L_m$ is:\n\n", "index": 21, "text": "\\begin{equation}\n\\psi^a_{1}(y_{i}=L_{m}) = \\lambda_{\\textup{{a}}} \\exp(-\\beta P(L_{m})),\n\\label{eq:auxiuni}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\psi^{a}_{1}(y_{i}=L_{m})=\\lambda_{\\textup{{a}}}\\exp(-\\beta P(L_{m})),\" display=\"block\"><mrow><msubsup><mi>\u03c8</mi><mn>1</mn><mi>a</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>L</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi>\u03bb</mi><mtext>a</mtext></msub><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mi>\u03b2</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>L</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nwhere $\\lambda_{{\\textup{{b}}}}$ penalizes a disagreement between the auxiliary and\nnon-auxiliary nodes, and is empirically set to $1$. The other two pairwise\nterms for the second and third nodes are defined similarly. Note that when one\nor more $x_i$'s take null label, the corresponding pairwise term(s) between\n$x_i$(s) and the auxiliary node are set to $0$.\n\n\\subsubsection{Computing language priors}\n\\label{sec:langPriors}\nWe compute $n$-gram based priors from the lexicon (or dictionary) and then\nadapt standard techniques for smoothing these\nscores~\\cite{thillou2005,katz,Goodman01abit} to the open and closed\nvocabulary cases.\n\nOur method uses the score denoting the likelihood of joint occurrence of pair\nof labels $l_u$ and $l_v$ represented as $P(l_u,l_v)$, triplets of labels\n$l_u$, $l_v$ and $l_w$ denoted by $P(l_u,l_v,l_w)$ and even higher order (e.g.,\nfourth order). Let $C(l_u)$ denote the number of occurrences of $l_u$,\n$C(l_u,l_v)$ be the number of joint occurrences of $l_u$ and $l_v$ next to each\nother, and similarly $C(l_u,l_v,l_w)$ is the number of joint occurrences of all\nthree labels $l_u, l_v, l_w$ next to each other. The smoothed\nscores~\\cite{katz} $P(l_u,l_v)$ and $P(l_u,l_v,l_w)$ are now:\n\n", "itemtype": "equation", "pos": 36788, "prevtext": "\nwhere $\\lambda_{\\textup{{a}}}$ is a constant. We set $\\lambda_{\\textup{{a}}}=5$\nempirically, in all our experiments, unless stated otherwise. The parameter\n$\\beta$ controls penalty between dictionary and non-dictionary $n$-grams, and\nis empirically set to $50$. The score $P(L_m)$ denotes the likelihood of\ntrigram $L_m$ in the English, and is further described in\nSection~\\ref{sec:langPriors}. The pairwise cost between the auxiliary node\n$y_i$ taking a label $L_{m} = l_{u} l_{v} l_{w}$ and the left-most\nnon-auxiliary node in the clique, $x_{i}$, taking a label $l_{r}$ is given by:\n\n", "index": 23, "text": "\\begin{equation}\n\\psi^a_{2}(y_{i}=L_{m},x_{i} = l_{r}) = \\left\\{\n\\begin{array}{ll}\n0 & \\text{if $r=u$}\\\\\n0 & \\text{if $l_r=\\epsilon$}\\\\\n\\lambda_{{\\textup{{b}}}} & \\mbox{otherwise}, \\end{array}\n\\right.\n\\label{eq:auxipair}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\psi^{a}_{2}(y_{i}=L_{m},x_{i}=l_{r})=\\left\\{\\begin{array}[]{ll}0&amp;\\text{if $r=%&#10;u$}\\\\&#10;0&amp;\\text{if $l_{r}=\\epsilon$}\\\\&#10;\\lambda_{{\\textup{{b}}}}&amp;\\mbox{otherwise},\\end{array}\\right.\" display=\"block\"><mrow><msubsup><mi>\u03c8</mi><mn>2</mn><mi>a</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>L</mi><mi>m</mi></msub><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>l</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><mi>r</mi><mo>=</mo><mi>u</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>l</mi><mi>r</mi></msub><mo>=</mo><mi>\u03f5</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>\u03bb</mi><mtext>b</mtext></msub></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 38247, "prevtext": "\nwhere $\\lambda_{{\\textup{{b}}}}$ penalizes a disagreement between the auxiliary and\nnon-auxiliary nodes, and is empirically set to $1$. The other two pairwise\nterms for the second and third nodes are defined similarly. Note that when one\nor more $x_i$'s take null label, the corresponding pairwise term(s) between\n$x_i$(s) and the auxiliary node are set to $0$.\n\n\\subsubsection{Computing language priors}\n\\label{sec:langPriors}\nWe compute $n$-gram based priors from the lexicon (or dictionary) and then\nadapt standard techniques for smoothing these\nscores~\\cite{thillou2005,katz,Goodman01abit} to the open and closed\nvocabulary cases.\n\nOur method uses the score denoting the likelihood of joint occurrence of pair\nof labels $l_u$ and $l_v$ represented as $P(l_u,l_v)$, triplets of labels\n$l_u$, $l_v$ and $l_w$ denoted by $P(l_u,l_v,l_w)$ and even higher order (e.g.,\nfourth order). Let $C(l_u)$ denote the number of occurrences of $l_u$,\n$C(l_u,l_v)$ be the number of joint occurrences of $l_u$ and $l_v$ next to each\nother, and similarly $C(l_u,l_v,l_w)$ is the number of joint occurrences of all\nthree labels $l_u, l_v, l_w$ next to each other. The smoothed\nscores~\\cite{katz} $P(l_u,l_v)$ and $P(l_u,l_v,l_w)$ are now:\n\n", "index": 25, "text": "\\begin{equation}\nP(l_u,l_v) = \\left\\{\n\\begin{array}{ll}\n0.4 & \\text{if $l_u, l_v$ are digits},\\\\\n\\frac{C(l_u,l_v)}{C(l_v)} & \\text{if $C(l_u,l_v) > 0$},\\\\\n\\alpha_{l_u}P(l_v) & \\mbox{otherwise}, \\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"P(l_{u},l_{v})=\\left\\{\\begin{array}[]{ll}0.4&amp;\\text{if $l_{u},l_{v}$ are digits%&#10;},\\\\&#10;\\frac{C(l_{u},l_{v})}{C(l_{v})}&amp;\\text{if $C(l_{u},l_{v})&gt;0$},\\\\&#10;\\alpha_{l_{u}}P(l_{v})&amp;\\mbox{otherwise},\\end{array}\\right.\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0.4</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub></mrow><mtext>\u00a0are digits</mtext></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mfrac><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>\u03b1</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03128.tex", "nexttext": "\nImage-specific lexicons (small or medium) are used in the closed vocabulary\nsetting, while in the open vocabulary case we use a lexicon containing half a\nmillion words (henceforth referred to as large lexicon) provided\nby~\\cite{WeinmanLH09} to compute these scores. The parameters\n$\\alpha_{l_u}$ and $\\alpha_{l_u,l_v}$ are learnt on the large lexicon using\nSRILM toolbox.\\footnote{Available at:\n\\url{http://www.speech.sri.com/projects/srilm/}} They determine the low\nscore values for $n$-grams not present in the lexicon. We assign a\nconstant value ($0.4$) when the labels are digits, which do not occur in the\nlarge lexicon. \n\\subsubsection{Inference}\nHaving computed the unary, pairwise and higher order terms, we use the\nsequential tree-reweighted message passing (TRW-S)\nalgorithm~\\cite{Kolmogorov06} to minimize the energy function. The TRW-S\nalgorithm maximizes a concave lower bound of the energy. It begins by\nconsidering a set of trees from the random field, and computes probability\ndistributions over each tree. These distributions are then used to reweight the\nmessages being passed during loopy belief propagation~\\cite{Pearl88} on each\ntree. The algorithm terminates when the lower bound cannot be increased\nfurther, or the maximum number of iterations has been reached.\n\nIn summary, given an image containing a word, we: (i) locate the potential\ncharacters in it with a character detection scheme, (ii) define a random field\nover all these potential characters, (iii) compute the language priors and\nintegrate them into the random field model, and then (iv) infer the most likely\nword by minimizing the energy function corresponding to the random field.\n\n\\begin{table}[!t]\n\\caption{Our IIIT 5K-word dataset contains a few less challenging (Easy) and\nmany very challenging (Hard) images. To present analysis of the dataset, we\nmanually divided the words in the training and test sets into {\\it easy} and\n{\\it hard} categories based on their visual appearance. The recognition\naccuracy of a state-of-the-art commercial OCR -- ABBYY9.0 -- for this dataset\nis shown in the last column. Here we also show the total number of characters,\nwhose annotations are also provided, in the dataset.}\n\\centering\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n &\\multicolumn{3}{c|}{{Training Set}}\\\\\n\\cline{2-4}\n &{$\\#$words}&{$\\#$characters}&{ABBYY9.0(\\%)}\\\\\n\\hline\n{Easy} & 658 & - & 44.98 \\\\\n{Hard} & 1342 & - & 16.57 \\\\\n{Total}& 2000 & 9658 & 20.25\\\\ \n\n\\hline\\hline\n&\\multicolumn{3}{c|}{{Test Set}}\\\\\n\\cline{2-4}\n&{$\\#$words}&{$\\#$characters}&{ABBYY9.0(\\%)}\\\\\n\\hline\n{Easy}&734 & - & 44.96\\\\\n{Hard}& 2266 & - & 5.00 \\\\\n{Total}& 3000 & 15269 & 14.60 \\\\\n\\hline\n\\end{tabular}\n\\label{tab:ourdataset}\n\\end{table}\n\\section{Datasets and Evaluation Protocols}\n\\label{datasets}\nSeveral public benchmark datasets for scene text understanding have been\nreleased in recent years. ICDAR~\\cite{ICDAR} and Street View Text\n(SVT)~\\cite{SVT} datasets are two of the initial datasets for this problem.\nThey both contain data for text localization, cropped word recognition and\nisolated character recognition tasks. In this paper we use the cropped word\nrecognition part from these datasets. Although these datasets have served well\nin building interest in the scene text understanding problem, they are limited\nby their size of a few hundred images. To address this issue, we introduced the\nIIIT 5K-word dataset~\\cite{MishraBMVC12}, containing a diverse set of 5000\nwords. Here, we provide details of all these datasets and the evaluation\nprotocol.\n\n\\paragraph{SVT} The street view text (SVT) dataset contains images taken from\nGoogle Street View. As noted in~\\cite{WangB10}, most of the images come from\nbusiness signage and exhibit a high degree of variability in appearance and\nresolution. The dataset is divided into SVT-spot and SVT-word, meant for the\ntasks of locating and recognizing words respectively. We use the SVT-word\ndataset, which contains 647 word images.\n\nOur basic unit of recognition is a character, which needs to be localized\nbefore classification. Failing to detect characters will result in poorer word\nrecognition, making it a critical component of our framework. To quantitatively\nmeasure the accuracy of the character detection module, we created ground truth\ndata for characters in the SVT-word dataset. This ground truth dataset contains\naround 4000 characters of 52 classes, and is referred to as as SVT-char, which\nis available for download~\\cite{project}.\n\\setlength{\\tabcolsep}{2pt}\n\\begin{table}[!t]\n\\centering\n\\caption{Analysis of the IIIT 5K-word dataset. We show the percentage of\nnon-dictionary words (Non-dict.), including digits, and the percentage of words\ncontaining only digits (Digits) in the first two rows. We also show the\npercentage of words that are composed from valid English trigrams (Dict.\\\n3-grams), four-grams (Dict.\\ 4-grams) and five-grams (Dict.\\ 5-grams) in the\nlast three rows. These statistics are computed using the large lexicon.}\n\\begin{tabular}{|l|c|c|}\n\\hline\n                 & IIIT 5K train & IIIT 5K test \\\\\n\\hline\nNon-dict.\\ words &23.65 &  22.03\\\\\nDigits           &11.05 &  7.97\\\\\nDict.\\ 3-grams   &90.27 &  88.05\\\\\nDict.\\ 4-grams   &81.40 &  79.27\\\\\nDict.\\ 5-grams   &68.92 &  62.48\\\\ \n\\hline\n\\end{tabular}\n\\label{tab:langModel}\n\\end{table}\n\\paragraph{ICDAR 2003 dataset} The ICDAR 2003 dataset was originally created\nfor text detection, cropped character classification, cropped and full image\nword recognition, and other tasks in document analysis~\\cite{ICDAR}. We used\nthe part corresponding to the cropped word recognition called robust word\nrecognition. Following the protocol of~\\cite{WangB11}, we ignore words with\nless than two characters or with non-alphanumeric characters, which results in\n859 words overall. For subsequent discussion we refer to this dataset as\nICDAR(50) for the image-specific lexicon-driven case (closed vocabulary), and\nICDAR 2003 when this lexicon is unavailable (open vocabulary case).\n\n\\paragraph{ICDAR 2011/2013 datasets} These datasets were introduced as part of\nthe ICDAR robust reading competitions~\\cite{ICDAR11Comp,ICDAR13Comp}. They\ncontain 1189 and 1095 word images respectively. We show case-sensitive open\nvocabulary results on both these datasets. Also, following the ICDAR\ncompetition evaluation protocol, we do not exclude words containing special\ncharacters (such as \\&, :), and report results on the entire dataset.\n\n\\paragraph{IIIT 5K-word dataset} The IIIT 5K-word\ndataset~\\cite{MishraBMVC12,project} contains both scene text and born-digital\nimages. Born-digital images---category of images which has gained interest in\nICDAR 2011 competitions~\\cite{ICDAR11Comp}---are inherently low-resolution,\nmade for online transmission, and have a variety of font sizes and styles. This\ndataset is not only much larger than SVT and the ICDAR datasets, but also more\nchallenging. All the images were harvested through Google image search. Query\nwords like billboard, signboard, house number, house name plate, movie poster\nwere used to collect images. The text in the images was manually annotated with\nbounding boxes and their corresponding ground truth words. The IIIT 5K-word\ndataset contains in all 1120 scene images and 5000 word images. We split it\ninto a training set of 380 scene images and 2000 word images, and a test set of\n740 scene images and 3000 word images. To analyze the difficulty of the IIIT\n5K-word dataset, we manually divided the words in the training and test sets\ninto {\\it easy} and {\\it hard} categories based on their visual appearance.\nAn annotation team consisting of three people have done three independent splits. Each word is then tagged as either being easy or hard by taking a majority vote. This \nsplit is available on our project page~\\cite{project}. Table~\\ref{tab:ourdataset} shows these splits in detail. We observe that a commercial OCR performs poorly on both the train and test splits. Furthermore, to evaluate components like character detection and recognition, we also\nprovide annotated character bounding boxes. It should be noted that around 22\\%\nof the words in this dataset are not in the English dictionary, e.g., proper\nnouns, house numbers, alphanumeric words. This makes this dataset suitable for\nopen vocabulary cropped word recognition. We show an analysis of dictionary and\nnon-dictionary words in Table~\\ref{tab:langModel}.\n\\paragraph{Evaluation protocol} We evaluate the word recognition accuracy in\ntwo settings: closed and open vocabulary. Following previous\nwork~\\cite{WangB11,shiCVPR13,MishraBMVC12}, we evaluate case-insensitive word\nrecognition on SVT, ICDAR 2003, IIIT 5K-word, and case-sensitive word\nrecognition on ICDAR 2011 and ICDAR 2013. For the closed vocabulary recognition\ncase, we perform a minimum edit distance correction, since the ground truth\nword belongs to the image-specific lexicon. On the other hand, in the case of\nopen vocabulary recognition, where the ground truth word may or may not belong\nto the large lexicon, we do not perform edit distance based correction. We\nperform many of our analyses on the IIIT 5K-word dataset, unless otherwise\nstated, since it is the largest dataset for this task, and also comes with\ncharacter bounding box annotations.\n\\begin{table*}[!t]\n\n\\centering\n\\caption{Character classification accuracy (in \\%). A smart choice of features,\ntraining examples and classifier is key to improving character classification.\nWe enrich the training set by including many affine transformed (AT) versions\nof the original training data from ICDAR and Chars74K (c74k). The three variants of our approach (H-13, H-31 and H-36) show noticeable improvement over several methods.\n\n\nThe character classification results shown here\nare case sensitive (all rows except the last two). It is to be noted\nthat~\\cite{CamposBV09} only uses 15 training samples per class. The last two\nrows show a case insensitive (CI) evaluation. $*$We do not evaluate the\nconvolutional neural network classifier in~\\cite{AZ14} (CNN feat+classifier) on\nthe c74K dataset, since the entire dataset was used to train the network.}\n\\begin{tabular}{|l|c|c|c|c|c|}\n\\hline\n{Method} & {SVT} & {ICDAR} & {c74K} & {IIIT 5K} & {Time} \\\\\n\\hline\nExempler SVM~\\cite{SheshadriD12}&-&71&-&-&-\\\\\nElagouni {\\it et al}.~\\cite{elagouni2012combining}&-&70&-&-&-\\\\\nCoates {\\it et al}.~\\cite{CoatesCCSSWWN11}&-&{82}&-&-&-\\\\\nFERNS~\\cite{WangB11}& - & 52 & 47 & - & -\\\\\nRBF~\\cite{MishraCVPR12} & 62 & 62 & 64 & 61 &3ms  \\\\\nMKL+RBF~\\cite{CamposBV09} & - & - & 57 & - &11ms \\\\\nH-36+AT+Linear &69 &73&68 &{66}& 2ms  \\\\\nH-31+AT+Linear & 64 & 73 & 67 &63 &1.8ms \\\\\nH-13+AT+Linear & 65 & 72 & 66 &64&0.8ms \\\\\n\\hline\\hline\nH-36+AT+Linear (CI) & 75 & 77 & {79} & 75 &0.8ms\\\\\nCNN feat+classifier~\\cite{AZ14} (CI) & {83} & {86} & $*$ & {85} & 1ms \\\\\n\\hline\n\\end{tabular}\n\\label{tab:charClassification}\n\\end{table*}\n\n\\section{Experiments}\n\\label{sec:expts}\nGiven an image region containing text, cropped from a street scene, our task is\nto recognize the word it contains. In the process, we develop several\ncomponents (such as a character recognizer) and also evaluate them to justify\nour choices. The proposed method is evaluated in two settings, namely, closed\nvocabulary (with an image-specific lexicon) and open vocabulary (using an\nEnglish dictionary for the language model). We compare our results with the best-performing recent methods for these two cases. For baseline comparisons\nwe choose commercial OCR namely ABBYY~\\cite{ABBYY} and a public implementation\nof a recent method~\\cite{Gomez14} in combination with an open source OCR.\n\\subsection{Character Classifier}\n\\label{subsec:charclassif}\nWe use the training sets of ICDAR 2003 character~\\cite{ICDAR} and\nChars74K~\\cite{CamposBV09} datasets to train the character classifiers. This\ntraining set is augmented with $48\\times48$ patches harvested from scene\nimages, with buildings, sky, road and cars, which do not contain text, as\nadditional negative training examples. We then apply affine transformations to\nall the character images, resize them to $48\\times48$, and compute HOG\nfeatures. Three variations (13, 31 and 36-dimensional) of HOG were analyzed\n(see Table~\\ref{tab:charClassification}). We then use an explicit feature\nmap~\\cite{VedaldiPAMI12} and the $\\chi^2$ kernel to learn the SVM classifier.\nThe SVM parameters are estimated by cross-validating on a validation set. The\nexplicit feature map not only allows a significant reduction in classification\ntime, compared to non-linear kernels like RBF, but also achieves a good\nperformance.\n\nThe two main differences from our previous work~\\cite{MishraCVPR12} in the\ndesign of the character classifier are: (i) enriching the training set, and\n(ii) using an explicit feature map and a linear kernel (instead of RBF).\nTable~\\ref{tab:charClassification} compares our character classification\nperformance\nwith~\\cite{WangB11,CamposBV09,MishraCVPR12,SheshadriD12,CoatesCCSSWWN11,elagouni2012combining}\non several test sets. We achieve at least 4\\% improvement over our\nprevious work (RBF~\\cite{MishraCVPR12}) on all the datasets, and also perform\nbetter than~\\cite{WangB11,CamposBV09}. We are also comparable to a few other\nrecent methods~\\cite{elagouni2012combining,SheshadriD12}, which show a limited\nevaluation on the ICDAR 2003 dataset. Following an evaluation insensitive to\ncase (as done in a few benchmarks, e.g.,~\\cite{AZ14,shiCVPR13},\nwe obtain 77\\% on ICDAR 2003, 75\\% on SVT-char, 79\\% on Chars74K, and 75\\% on\nIIIT 5K-word. It should be noted that feature learning methods based on\nconvolutional neural networks, e.g.,~\\cite{CoatesCCSSWWN11,AZ14}, show an\nexcellent performance. This inspired us to integrate them into our framework.\nWe used publicly available features~\\cite{AZ14}. This will be further discussed\nin Section~\\ref{sec:wordReco}. We could not compare with other related recent\nmethods~\\cite{photoOCR,weinman2013toward} since they did not report isolated\ncharacter classification accuracy.\n\nIn terms of computation time, linear SVMs trained with HOG-13 features\noutperform others, but since our main focus is on word recognition performance,\nwe use the most accurate combination, i.e., linear SVMs with HOG-36. We\nobserved that this smart selection of training data and features not only\nimproves character recognition accuracy but also improves the second and third\nbest predictions for characters.\n\\subsection{Character Detection}\n\\label{subsec:chars}\nSliding window based character detection is an important component of our\nframework, since our random field model is defined on these detections. \nWe use windows of aspect ratio ranging from 0.1 to 2.5 for sliding window and at every possible location of the sliding window, we evaluate a character\nclassifier. This provides the likelihood of the window containing the\nrespective character. We pruned some of the windows based on their aspect\nratio, and then used the goodness measure~(\\ref{eq:gs}) to discard the windows\nwith a score less than $0.1$ (refer Section~\\ref{sec:charDet}).\nCharacter-specific NMS is done on the remaining windows with an overlap\nthreshold of $40\\%$, i.e., if two detections have more than 40\\% overlap and\nrepresent the same character class, we suppress the weaker detection.  We\nevaluated the character detection results with the intersection over union\nmeasure and a threshold of 50\\%, following ICDAR 2003~\\cite{ICDAR} and\nPASCAL-VOC~\\cite{Everingham10} evaluation protocol. Our sliding window approach\nachieves recall of 80\\% on the IIIT 5K-word dataset, significantly better than\nusing a binarization scheme for detecting characters and also superior to\ntechniques like MSER~\\cite{mser} and CSER~\\cite{Gomez14} (see\nTable~\\ref{tab:binResults} and Section~\\ref{binMethods}).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[!t]\n\n\\centering\n\\caption{Word recognition accuracy (in~\\%): closed vocabulary setting. We\npresent results of our proposed higher order model (``This work'') with HOG as\nwell as CNN features. See text for details.}\n\\begin{tabular}{ |l|c| }\n  \\hline\n   {Method} & {Accuracy}\\\\\n  \\hline \\hline\n  \\multicolumn{2}{|c|}{{ICDAR 2003 (50) dataset}} \\\\\n  \\hline \\hline\n~~~Baseline (ABBYY)~\\cite{ABBYY} & 56.04 \\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 57.27 \\\\\n~~~Novikova {\\it et al}.~\\cite{pushmeetLexi}&82.80\\\\\n~~~Our Holistic recognition~\\cite{GoelICDAR13}&89.69\\\\\n~~\\textit{Deep learning approaches} & \\\\\n~~~Wang {\\it et al}.~\\cite{ngICPR12}&90.00\\\\\n~~~Deep features~\\cite{AZ14}&{96.20}\\\\\n~~\\textit{Other energy min.\\ approaches} & \\\\\n~~~{PLEX}~\\cite{WangB11} & 72.00 \\\\\n~~~Shi {\\it et al.}~\\cite{shiCVPR13}&87.04\\\\\n~~\\textit{Our variants:} &\\\\\n~~~Pairwise CRF~\\cite{MishraCVPR12}& 81.74\\\\\n~~~Higher order [This work, HOG] & {84.07}  \\\\\n~~~Higher order [This work, CNN] & {88.02} \\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{SVT-Word dataset}} \\\\\n  \\hline \\hline\n~~~Baseline(ABBYY)~\\cite{ABBYY} & 35.00  \\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} &37.71\\\\\n~~~Novikova {\\it et al.}~\\cite{pushmeetLexi}&72.90\\\\\n~~~Our Holistic recognition~\\cite{GoelICDAR13}&77.28\\\\\n~~\\textit{Deep learning approaches} & \\\\\n~~~Wang {\\it et al.}~\\cite{ngICPR12}&70.00\\\\\n~~~PhotoOCR~\\cite{photoOCR}&{90.39}\\\\\n~~~Deep features~\\cite{AZ14}& 86.10\\\\\n~~\\textit{Other energy min.\\ approaches} & \\\\\n~~~{PICT}~\\cite{WangB10} & 59.00\\\\\n~~~{PLEX}~\\cite{WangB11} & 57.00 \\\\\n~~~Shi {\\it et al.}~\\cite{shiCVPR13}&73.51\\\\\n~~~Weinman {\\it et al.}~\\cite{weinman2013toward}&78.05\\\\\n~~\\textit{Our variants:} & \\\\\n~~~Pairwise {CRF}~\\cite{MishraCVPR12}& 73.26 \\\\\n~~~Higher order [This work, HOG] & 75.27 \\\\\n~~~Higher order [This work, CNN] & 78.21 \\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{IIIT 5K-Word (Small)}} \\\\\n  \\hline \\hline\n~~~Baseline(ABBYY)~\\cite{ABBYY} & 24.50\\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 33.07\\\\\n~~~Rodriguez \\& Perronnin~\\cite{JoseBMVC13}&76.10\\\\\n~~~Strokelets~\\cite{stroklets}&{80.20}\\\\\n~~\\textit{Our variants:} & \\\\\n~~~Pairwise {CRF}~\\cite{MishraCVPR12}& 66.13\\\\\n~~~Higher order [This work, HOG] & 71.80  \\\\\n~~~Higher order [This work, CNN] & {78.07}  \\\\\n\\hline\n\\end{tabular}\n\\label{tab:smallLexiRes}\n\\end{table}\n\\subsection{Word Recognition}\n\\label{sec:wordReco}\n\\paragraph{Closed vocabulary recognition}\nThe results of the proposed CRF model in closed vocabulary setting are\npresented in Table~\\ref{tab:smallLexiRes}. We compare our method with many\nrecent works for this task. To compute the language priors we use lexicons\nprovided by authors of~\\cite{WangB11} for SVT and ICDAR(50). The image-specific\nlexicon for every word in the IIIT 5K-word dataset was developed following the\nmethod described in~\\cite{WangB11}. These lexicons contain the ground truth\nword and a set of distractors obtained from randomly chosen words (from all the\nground truth words in the dataset). We used a CRF with higher order term\n($n$=4), and similar to other approaches, applied edit distance based\ncorrection after inference. The constant $\\lambda_{\\textup{{a}}}$ in\n(\\ref{eq:auxiuni}) to 1, given the small size of the lexicon.\n\nThe gain in accuracy over our previous work~\\cite{MishraCVPR12}, seen in\nTable~\\ref{tab:smallLexiRes}, can be attributed to the higher order CRF and an\nimproved character classifier. The character classifier uses: (i) enriched\ntraining data, and (ii) an explicit feature map, to achieve about 5\\% gain (see\nSection~\\ref{subsec:charclassif} for details). Other methods, in particular,\nour previous work on holistic word recognition~\\cite{GoelICDAR13}, label\nembedding~\\cite{JoseBMVC13} achieve a reasonably good performance, but are\nrestricted to the closed vocabulary setting, and their extension to more\ngeneral settings, such as the open vocabulary case, is unclear. Methods\npublished since our original work~\\cite{MishraCVPR12}, such\nas~\\cite{weinman2013toward,shiCVPR13}, also perform well. Very recently,\nmethods based on convolutional neural networks~\\cite{photoOCR,AZ14} have shown\nvery impressive results for this problem. It should be noted that such methods\nare typically trained on much larger datasets, for example, 10M compared to\n0.1M typically used in state-of-the-art methods, which are not publicly\navailable~\\cite{photoOCR}. Inspired by these successes, we use a CNN\nclassifier~\\cite{AZ14} to recognize characters, instead of our SVM classifier\nbased on HOG features (see Sec.~\\ref{sec:charDet}). We show results with this CNN\nclassifier on SVT, ICDAR 2003 and IIIT-5K word datasets in\nTable~\\ref{tab:smallLexiRes} and observe significant improvement in accuracy,\nshowing its complementary nature to our energy based method. However, there remains a difference in performance between the deep feature based method~\\cite{AZ14} and [This work, CNN]. This is primarily due to use of CNN features for learning classifiers for individual character as well as bi-grams in~\\cite{AZ14}. In contrast, our method only uses the pre-trained character classifier provided by~\\cite{AZ14}. Nevertheless, the improvement observed over [This work, HOG] does show the complementary nature of the two approaches, and integrating the two further would be an interesting avenue for future research.\n\n\\paragraph{Open vocabulary recognition}\nIn this setting we use a lexicon of 0.5 million words from~\\cite{WeinmanLH09}\ninstead of image-specific lexicons to compute the language priors. Many character pairs are equally likely in such a large lexicon, thereby rendering pairwise priors is less effective than in the case of a small lexicon. We use priors of order four to address this (see also analysis on the CRF order in Section~\\ref{sec:analysis}). Results on\nvarious datasets in this setting are shown in Table~\\ref{tab:largeLexiRes}.  We\ncompare our method with recent work by Feild and Miller~\\cite{fieldICDAR13} on\nthe ICDAR 2003 dataset, where our method with HOG features shows a comparable\nperformance. Note that~\\cite{fieldICDAR13} additionally uses web-based\ncorrections, unlike our method, where the results are obtained directly by\nperforming inference on the higher order {CRF} model. On the ICDAR 2011 and\n2013 datasets we compare our method with the top performers from the respective\ncompetitions. Our method outperforms the ICDAR 2011 robust reading competition\nwinner (TH-OCR method) method by 17\\%. This performance is also better than\na recently published work from 2014 by Weinman {\\it et\nal.}~\\cite{weinman2013toward}. On the ICDAR 2013 dataset, the proposed higher\norder model is significantly better than the baseline and is in the top-5\nperformers among the competition entries. The winner of this competition\n(PhotoOCR) uses a large proprietary training dataset, which is unavailable\npublicly, making it infeasible to do a fair comparison. Other methods\n(NESP~\\cite{NESP}, MAPS~\\cite{MAPS}, PLT~\\cite{PLT}) use many preprocessing\ntechniques, followed by off-the-self OCR. Such preprocessing techniques are\nhighly dataset dependent and may not generalize easily to all the challenging\ndatasets we use. Despite the lack of these preprocessing steps, our method\nshows a comparable performance. On the IIIT 5K-word dataset, which is large\n(three times the size of ICDAR 2013 dataset) and challenging, the only\npublished result to our knowledge\n\nis Strokelets~\\cite{stroklets} from CVPR 2014. Our method performs 7\\% better\nthan Strokelets. Using CNN features instead of HOG further improves our word\nrecognition accuracy, as shown in Table~\\ref{tab:largeLexiRes}.\n\nThe main focus of this work is on evaluating datasets containing scene text images or a mixture of scene text and born-digital images. Nevertheless, we also tested our method on the born-digital image dataset from the recent ICDAR 2013 competition. Our approach with pre-trained CNN features achieves 78\\% accuracy on this dataset, which is comparable to other top performers (80.40\\%, 80.26\\%, 79.40\\%), and lower than PhotoOCR (82\\%), the competition winner using an end-to-end deep learning approach.\n \nTo sum up, our proposed method performs well consistently on several popular\nscene text datasets. Fig.~\\ref{fig:word_results} shows the qualitative\nperformance of the proposed method on a few sample images. The higher order CRF\noutperforms the unary and pairwise CRFs. This is intuitive due to the better\nexpressiveness of the higher order potentials. One of the failure cases is\nshown in the last row in Fig.~\\ref{fig:word_results}, where the higher order\npotential is computed from a lexicon which does not have sufficient examples to\nhandle alphanumeric words.\n\\begin{table}[!t]\n\\centering\n\\caption{\\small{Word recognition accuracy (in \\%): open vocabulary setting. The\nresults of our proposed higher order model (``This work'') with HOG as well as\nCNN features are presented here. Since the network used here to compute CNN\nfeatures, i.e.~\\cite{AZ14}, is learnt on data from several sources (e.g., ICDAR\n2013), we evaluated with CNN features only on ICDAR 2003 and IIIT-5K word\ndatasets, as recommended by the authors. Note that we also compare with top performers (as given in~\\cite{ICDAR11Comp,ICDAR13Comp}) in the ICDAR 2011 and 2013 robust\nreading competitions. We follow standard protocols for evaluation -- case\nsensitive on ICDAR 2011 and 2013 and case insensitive on ICDAR 2003 and IIIT\n5K-Word.}}\n\\begin{tabular}{ |l|c| }\n  \\hline\n   {Method} & {Accuracy}\\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{ICDAR 2003 dataset}} \\\\\n  \\hline \\hline\n~~~Baseline (ABBYY) & 46.51\\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 50.99\\\\\n~~~Feild and Miller~\\cite{fieldICDAR13} & 62.76\\\\\n~~\\textit{Our variants} & \\\\\n\n\n~~~Pairwise~\\cite{MishraCVPR12}& 50.99  \\\\\n~~~Higher order [This work, HOG]& {63.02} \\\\\n~~~Higher order [This work, CNN]& {67.67} \\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{ICDAR 2011 dataset}} \\\\\n  \\hline \\hline\n~~~Baseline (ABBYY) &46.00\\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 51.98 \\\\\n~~~Weinman~\\textit{et al.}~\\cite{weinman2013toward} & 57.70 \\\\\n~~~Feild and Miller~\\cite{fieldICDAR13} & 48.86\\\\\n~~\\textit{ICDAR'11 competition}~\\cite{ICDAR11Comp} & \\\\\n~~~TH-OCR System & 41.20\\\\\n~~~KAIST AIPR System  & 35.60\\\\\n~~~Neumann's Method & 33.11 \\\\\n~~\\textit{Our variants} & \\\\\n\n\n~~~Pairwise~\\cite{MishraCVPR12}& 48.11  \\\\\n~~~Higher order [This work, HOG]& {58.03}\\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{ICDAR 2013 dataset}} \\\\\n  \\hline \\hline\n~~~Baseline (ABBYY) & 45.30\\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 50.26 \\\\\n~~\\textit{ICDAR'13 competition}~\\cite{ICDAR13Comp}&\\\\\n~~~PhotoOCR~\\cite{photoOCR} & {82.83}\\\\\n~~~NESP~\\cite{NESP}   & 64.20 \\\\\n~~~MAPS~\\cite{MAPS} & 62.74\\\\\n~~~PLT~\\cite{PLT} & 62.37\\\\\n~~~PicRead~\\cite{pushmeetLexi}  & 57.99\\\\\n~~~POINEER~\\cite{WeinmanLH09,weinman2013toward} &53.70\\\\\n~~~Field's Method~\\cite{fieldICDAR13} &47.95\\\\\n~~~TextSpotter~\\cite{NeumannM10,neumann2012real,NeumannM13} & 26.85\\\\\n~~\\textit{Our variants} & \\\\\n\n\n~~~Pairwise~\\cite{MishraCVPR12}& 49.86   \\\\\n~~~Higher order [This work, HOG]& 60.18  \\\\\n  \\hline \\hline\n\\multicolumn{2}{|c|}{{IIIT 5K-Word}} \\\\\n  \\hline \\hline\n~~~Baseline (ABBYY) & 14.60 \\\\\n~~~Baseline (CSER+tesseract)~\\cite{Gomez14} & 25.00\\\\\n~~~Stroklets~\\cite{stroklets}&38.30\\\\\n~~\\textit{Our variants} & \\\\\n\n\n~~~Pairwise~\\cite{MishraCVPR12}& 32.00  \\\\\n~~~Higher order [This work, HOG]& {44.50} \\\\\n~~~Higher order [This work, CNN]& {46.73} \\\\\n \\hline\n\\end{tabular}\n\\label{tab:largeLexiRes}\n\\end{table}\n\n\\begin{table}[!t]\n\n\\caption{Studying the influence of the lexicon size -- small (S), medium (M),\nlarge (L) -- on the IIIT 5K-word dataset in the closed vocabulary setting.\n}\n\\centering\n\\begin{tabular}{|l|c|c|c|}\n\\hline\nMethod & S & M & L \\\\\n\\hline\nRodriguez \\& Perronnin~\\cite{JoseBMVC13} & 76.10 & 57.50 &- \\\\\nStrokelets~\\cite{stroklets}& {80.20} & 69.30 & 38.30 \\\\\nHigher order [This work, HOG] & 71.80 & 62.17 & 44.50\\\\\nHigher order [This work, CNN] & 78.07 & {70.13} & {46.73} \\\\\n\\hline\n\\end{tabular}\n\\label{tab:lexiSize}\n\\end{table}\n\n\\subsection{Further Analysis}\n\\label{sec:analysis}\n\\paragraph{Lexicon size} The size of the lexicon plays an important role in the\nword recognition performance. With a small-size lexicon, we obtain strong\nlanguage priors which help overcome inaccurate character detection and\nrecognition in the closed vocabulary setting. A small lexicon provides much\nstronger priors than the large lexicon in this case, as the performance\ndegrades with increase in the lexicon size. We show this behaviour on the IIIT\n5K-word dataset in Table~\\ref{tab:lexiSize} with small (50), medium (1000) and\nlarge (0.5 million) lexicons. We also compare our results with a\nstate-of-the-art methods~\\cite{JoseBMVC13,stroklets}. We observe that~\\cite{JoseBMVC13,stroklets} shows better recognition performance with the small lexicon, when we use HOG features, but as the size of the lexicon increases, our method\noutperforms~\\cite{JoseBMVC13}.\n\n\\paragraph{Alternatives for character detection.}\n\\label{binMethods}\nWhile our sliding window approach for character detection performs well in\nseveral scenarios, including text that is not aligned with the image axes to a\nsmall extent (e.g., rows 4 - 6 in Figure~\\ref{fig:word_results}), there are\nother alternatives. In particular, we investigated the use of binarization,\nMSER~\\cite{mser}, and CSER~\\cite{NeumannM13} algorithms.\nIn the first experiment, we replaced our detection module with a binarization\nbased character extraction scheme -- either a traditional binarization\ntechnique~\\cite{otsu} or a more recent random field based\napproach~\\cite{MishraJ11}. A connected component analysis was performed on the\nbinarized images to obtain a set of potential character locations. We then\ndefined the CRF on these characters and performed inference to get the text\ncontained in the image. These results are summarized in\nTable~\\ref{tab:binResults}. We observe that binarization based methods perform\npoorly compared to our model using a sliding window detector, both in terms of\ncharacter-level recall and word recognition. They fail in extracting characters\nin the presence of noise, blur or large foreground-background variations. MSER~\\cite{mser} or related algorithms (e.g., CSER~\\cite{NeumannM13}) may also help to deal with text that is not axis-oriented, but they are not necessarily ideal for character extraction\ncompared to a sliding window method. To study this, we replaced our sliding window based\ncharacter detection scheme with either one of these approaches.\nFrom Table~\\ref{tab:binResults} we observe that sliding window character extraction is marginally better than CSER and\nsignificantly better than MSER. One of the reasons for this is that the\nclassifier used in the sliding window detector is trained on a large variety of\ncharacter classes and is less prone to errors than the MSER equivalent. These\nresults further justify our choice of sliding window based character detection,\nalthough the challenging problem of effectively dealing with text that is not\naxis-oriented remains an interesting task for the future.\n\n\\paragraph{Effect of pruning}\nWe propose a pruning step to discard candidates based\non a combination of character-specific aspect ratio and classification scores\n(\\ref{eq:gs}), instead of simply using extreme aspect ratio to discard character\ncandidates. This pruning helps in removing many false positive windows, and thus\nimproves recognition performance. We conducted an experiment to study the\neffect of pruning on the IIIT-5K dataset in the open vocabulary setting, and\nobserved a gain of 4.23\\% (46.73\\% vs 42.50\\%) due to pruning. \n\n\\paragraph{CRF order} \nWe varied the order of the CRF from two to six and obtained accuracy of 32\\%, 43\\%,\n45\\%, 43\\%, 42\\% respectively on the IIIT 5K-word dataset in the open vocabulary\nsetting. Increasing the CRF order beyond four forces a recognized word to be\none from the dictionary, which leads to poor recognition performance for\nnon-dictionary words, and thus deteriorates the overall accuracy. Empirically,\nthe fourth order prior shows the best performance.\n\n\n\\paragraph{Limits of statistical language models}\nStatistical language models have been very useful in improving traditional OCR\nperformance, but they are indeed limited~\\cite{SmithR11,kornai1994}. For\ninstance, using a large weight for language prior potentials may bias the\nrecognition towards the closest dictionary word. This is especially true when\nthe character recognition part of the pipeline is weak. We study such impact of\nlanguage models in this experiment. Our analysis on the IIIT 5K-word dataset\nsuggests that many of the non-dictionary words are composed of valid English\n$n$-grams (see Table~\\ref{tab:langModel}). However, there are few exceptions,\ne.g., words like 35KM, 21P, which are composed of digits and characters; see\nlast row of Fig.~\\ref{fig:word_results}. Using language models has an adverse\neffect on the recognition performance in such cases. This results in inferior\nrecognition performance on non-dictionary words as compared to dictionary words,\ne.g. on IIIT-5K dataset our method achieves 51\\% and 24\\% word recognition accuracy\non dictionary and non-dictionary words respectively.\n\n\\begin{figure*}[!t]\n\n\\centering\n\\begin{tabular}{cccc}\n\\textbf{Test Image} &~~~~~\\textbf{Unary} &~~~~~\\textbf{Pairwise}&~~~~~\\textbf{Higher order(=4)}\\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/1.eps} &~~~~~\\raisebox{.30cm}{TWI\\textcolor{red}{1}I\\textcolor{red}{O}HT}  &~~~~~~~~~\\raisebox{.30cm}{TWILI\\textcolor{red}{O}HT}~~~~~&~~~~~\\raisebox{.30cm}{TWILIGHT} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/2.eps} &~~~~~\\raisebox{.30cm}{SRIS\\textcolor{red}{N}TI}&~~~~~\\raisebox{.30cm}{SRIS\\textcolor{red}{N}TI}&~~~~~\\raisebox{.30cm}{SRISHTI} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/3.eps} &~~~~~\\raisebox{.30cm}{LIIIIPUT}&~~~~~\\raisebox{.30cm}{LIIIIPUT}&~~~~~~~\\raisebox{.30cm}{LILLIPUT} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/4.eps} &~~~~~~~\\raisebox{.30cm}{\\textcolor{red}{E}UMMER} &~~~~~~~\\raisebox{.30cm}{\\textcolor{red}{E}UMMER}&~~~~~~~\\raisebox{.30cm}{SUMMER} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/5.eps} & ~~~~~~~\\raisebox{.30cm}{I\\textcolor{red}{D}TERNAL} &~~~~~~~\\raisebox{.30cm}{I\\textcolor{red}{D}TERNAL}&~~~~~~~\\raisebox{.30cm}{INTERNAL} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/6.eps} &~~~~~~~\\raisebox{.30cm}{364203903105\\textcolor{red}{S}}&~~~~~~~\\raisebox{.30cm}{3642039031055}&~~~~~~~\\raisebox{.30cm}{3642039031055} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/7.eps} &~~~~~~~\\raisebox{.30cm}{R\\textcolor{red}{E}GHT}& ~~~~~~~\\raisebox{.30cm}{R\\textcolor{red}{E}GHT}&~~~~~~~\\raisebox{.30cm}{RIGHT} \\\\\n\\includegraphics[width=2.5cm,height=0.8cm]{figures/expRes/8.eps} & ~~~~~~~\\raisebox{.30cm}{83KM}&~~~~~~~\\raisebox{.30cm}{\\textcolor{red}{BO}KM}&~~~~~~~\\raisebox{.30cm}{\\textcolor{red}{BOOM}} \\\\\n\\end{tabular}\n\\caption{Results of our higher order model on a few sample images. Characters\nin red represent incorrect recognition. The unary term alone, based on the SVM\nclassifier, yields poor accuracy, and adding pairwise terms to it improves\nthis. Due to their limited expressiveness, they do not correct all the errors.\nHigher order potentials capture larger context from the English language, and\nhelp address this issue. Note that our method also deals with non-dictionary\nwords (e.g., second row) and non-horizontal text (sixth row). A typical failure\ncase containing alphanumeric words is shown in the last row. (\\textbf{Best\nviewed in colour}).}\n\\label{fig:word_results}\n\\end{figure*}\n\n\\section{Summary}\n\\label{sec:conclusion}\nThis paper proposes an effective method to recognize scene text. Our model\ncombines bottom-up cues from character detections and top-down cues from\nlexicon. We jointly infer the location of true characters and the word they\nrepresent as a whole. We evaluated our method extensively on several\nchallenging street scene text datasets, namely SVT, ICDAR 2003/2011/2013, and\nIIIT 5K-word and showed that our approach significantly advances the energy\nminimization based approach for scene text recognition. In addition to\npresenting the word recognition results, we analyzed the different components\nof our pipeline, presenting their pros and cons. Finally, we showed that the\nenergy minimization framework is complementary to the resurgence of\nconvolutional neural network based techniques, which can help build better\nscene understanding systems.\n\\setlength{\\tabcolsep}{2pt}\n\\begin{table}[!t]\n\\caption{Character recall (C.\\ recall) and recognition accuracy, with unary\nonly (Unary), unary and pairwise (Pairwise) and the full higher order (H.\\\norder) models, (all in \\%), on the IIIT 5K-word dataset with various character\nextraction schemes (Char.\\ method). See text for details.}\n\\centering\n\\begin{tabular}{|l|c|c|c|c|}\n\\hline\n{Char.\\ method} & {C.\\ recall} & {Unary} & {Pairwise} & {H.\\ order}\\\\\n\\hline\nOtsu~\\cite{otsu}           & 56 & 17.07 & 20.20 & 24.87\\\\\nMRF model~\\cite{MishraJ11} & 62 & 20.10 & 22.97 & 28.03\\\\\nMSER~\\cite{mser}                    & 72 & 23.20 & 28.50 & 34.70 \\\\\nCSER~\\cite{NeumannM13}~\\cite{Gomez14}   & 78  & 24.50  & 30.00 & 42.87\\\\\nSliding window             & {80} & {25.83} & {32.00} & {44.50}\\\\\n\\hline\n\\end{tabular}\n\\label{tab:binResults}\n\\end{table}\n\n\\paragraph{Acknowledgements}\nWe thank Jerod Weinman for providing the large lexicon. This work was partially\nsupported by the Ministry of Communications and Information Technology,\nGovernment of India, New Delhi. Anand Mishra is supported by Microsoft\nCorporation and Microsoft Research India under the Microsoft Research India PhD\nfellowship award.\n\n\n\n\\bibliography{mybibfile}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 38477, "prevtext": "\n\n\n", "index": 27, "text": "\\begin{equation}\nP(l_u,l_v,l_w) = \\left\\{\n\\begin{array}{ll}\n0.4 & \\text{if $l_u, l_v, l_w$ are digits},\\\\\n\\frac{C(l_u,l_v,l_w)}{C(l_v,l_w)} & \\text{if $C(l_u,l_v,l_w) > 0$},\\\\\n\\alpha_{l_u}P(l_v,l_w) & \\text{else if $C(l_u,l_v) > 0$},\\\\\n\\alpha_{l_u,l_v} P(l_w) & \\mbox{otherwise}, \\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"P(l_{u},l_{v},l_{w})=\\left\\{\\begin{array}[]{ll}0.4&amp;\\text{if $l_{u},l_{v},l_{w}%&#10;$ are digits},\\\\&#10;\\frac{C(l_{u},l_{v},l_{w})}{C(l_{v},l_{w})}&amp;\\text{if $C(l_{u},l_{v},l_{w})&gt;0$}%&#10;,\\\\&#10;\\alpha_{l_{u}}P(l_{v},l_{w})&amp;\\text{else if $C(l_{u},l_{v})&gt;0$},\\\\&#10;\\alpha_{l_{u},l_{v}}P(l_{w})&amp;\\mbox{otherwise},\\end{array}\\right.\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0.4</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub></mrow><mtext>\u00a0are digits</mtext></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mfrac><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>\u03b1</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>v</mi></msub><mo>,</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>else if\u00a0</mtext><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>\u03b1</mi><mrow><msub><mi>l</mi><mi>u</mi></msub><mo>,</mo><msub><mi>l</mi><mi>v</mi></msub></mrow></msub><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}]