[{"file": "1601.02603.tex", "nexttext": "\nFor example, a database studying the evolution of democratic states~\\cite{ARM11} will store, for each country and each year, the value of multiple economical, social, political and financial indicators.\nThe countries are the entities, and the years are the timestamps.\n\nStarting from such a database, one of the interests of Political Studies researchers is to detect typical evolution patterns. \nThere is a double interest: \na) obtaining a broader understanding of the phases that the entity collection went through over time (\\textit{e.g.} detecting the periods of global political instability, of economic crisis, of wealthiness \\textit{etc.});\nb) constructing the trajectory of an entity through the different phases (\\textit{e.g.} a country may have gone through a period of military dictatorship, followed by a period of wealthy democracy).\nThe criteria describing each phase are not known beforehand (which indicators announce a world economic crisis?) and may differ from one phase to another.\n\n\\begin{figure}[tb]\n\t\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{cluster-structuring}\n\t\t\\label{subfig:cluster-structuring}\n\t}\n\t\\hfill\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{entity-contiguity}\n\t\t\\label{subfig:entity-contiguity}\n\t}\n\t\\caption{Desired output: (a) the evolution phases and the entity trajectories, (b) the observations of 3 entities contiguously partitioned into 5 clusters.}\n\t\\label{fig:desired-output}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nWe address these issues by proposing a novel temporal-driven constrained clustering algorithm.\nThe proposed algorithm partitions the observations into clusters $\\mu_j \\in \\mathcal{M}$, that are coherent both in the multidimensional description space and in the temporal space.\nWe consider that the obtained clusters can be used to represent the typical phases of the evolution of the entities through time. \nFigure~\\ref{fig:desired-output} shows the desired result of our clustering algorithm.\nEach of the three depicted entities ($\\phi_1, \\phi_2$ and $\\phi_3$) is described at 10 moments of time ($t_m, m=1,2,...,10$).\nThe 30 observations of the dataset are partitioned into 5 clusters ($\\mu_j, j=1,2,...,5$).\nIn Figure~\\ref{subfig:cluster-structuring} we observe how clusters $\\mu_j$ are organized in time.\nEach of the clusters has a limited extent in time, and the time extents of clusters can overlap.\nThe temporal extent of a cluster is the minimal interval of time that contains all the timestamps of the observations in that cluster.\nThe entities navigate through clusters.\nWhen an observation belonging to an entity is assigned to cluster $\\mu_2$ and the anterior observation of the same entity is assigned in cluster $\\mu_1$, then we consider that the entity has a transition from phase $\\mu_1$ to phase $\\mu_2$.\nFigure~\\ref{subfig:entity-contiguity} shows how the series of observations belonging to each entity are assigned to clusters, thus forming continuous segments.\nThis succession of segments is interpreted as the succession of phases through which the entity passes. \nFor this succession to be meaningful, each entity should be assigned to a rather limited number of continuous segments.\nPassing through too many phases reduces the comprehension.\nSimilarly, evolutions which are alternations between two phases (\\textit{e.g.}, $\\mu_1 \\longrightarrow \\mu_2 \\longrightarrow \\mu_1 \\longrightarrow \\mu_2 $) hinder the comprehension.\n\nBased on these observations, we assume that the resulting partition must:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} }     \n\t\\item \\textbf{regroup observations having similar descriptions into the same cluster} (just as traditional clustering does).\n\tThe clusters represent a certain type of evolution;\n\t\\item \\textbf{create temporally coherent clusters, with limited extent in time.} \n\tIn order for a cluster to be meaningful, it should regroup observations which are temporally close (be contiguous on the temporal dimension).\n\tIf there are two different periods with similar evolutions (\\textit{e.g.} two economical crises), it is preferable to have them regrouped separately, as they represent two distinct phases.\n\tFurthermore, while it is acceptable that some evolutions exist during the entire period, usually the resulted clusters should have a limited temporal extent;\n\t\\item \\textbf{segment, as contiguously as possible, the series of observations for each entity.} \n\tThe sequence of segments will be interpreted as the sequence of phases through which the entity passes.\n   \\end{list}      \n\nIn this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension.\nObservations that are close in the description space, but distant in time are considered as dissimilar.\nWe also propose a method to enforce the segmentation contiguity, by introducing a penalty term based on the Normal Distribution Function.\nWe combine the two propositions into a novel time-driven constrained clustering algorithm, \\textbf{{\\mbox{TDCK-Means}}{}}, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space.\nThis algorithm uses soft semi-supervised constraints to encourage adjacent observations belonging to the same entity to be assigned to the same cluster.\nThe proposed algorithm constructs the clusters that serve as evolution phases and segments the observations series for each entity.\n\nThe paper is organized as follows.\nIn Section~\\ref{sec:related-work} we present some previous related works and, in Section~\\ref{sec:TDCK-Means-proposal}, we introduce the temporal-aware dissimilarity function, the contiguity penalty, function  the {\\mbox{TDCK-Means}}{} algorithm and the graph structure induction method.\nIn Section~\\ref{sec:xp}, we present the dataset that we use, the proposed evaluation measures and the obtained results.\nFinally, in Section~\\ref{sec:conclusion}, we draw the conclusion and plan some future extensions.\n\n\\section{Related work}\n\\label{sec:related-work}\n\nLeveraging partial expert knowledge into clustering represents the domain of semi-supervised clustering.\nThe expert knowledge is under the form of either class labels, or pairwise constraints.\nPairwise constraints~\\cite{WAG01} are either ``must-link'' (the observations must be placed in the same cluster) or ``cannot-link'' (the two observations cannot be placed in the same cluster).\nDepending on the method in which supervision is introduced into clustering, \\cite{GRI05} divides the semi-supervised clustering methods into two classes:\na) the similarity-adapting methods~\\cite{BIL03,COH03,KLE02,XIN02}, which seek to learn new similarity measures in order to satisfy the constraints, and \nb) the search-based methods~\\cite{BAS02,DEM99,WAG01} in which the clustering algorithm itself is modified.\n\nThe literature presents some examples of algorithms used to segment a series of observations into continuous chunks.\nIn \\cite{LIN06}, the daily tasks of a user are detected by segmenting scenes from the recordings of his activities.\nSemi-supervised must-link constraints are set between all pairs of observations, and a fixed penalty is inflicted when the following conditions are fulfilled simultaneously: the observations are not assigned to the same cluster and the time difference between their timestamps is less than a certain threshold.\nA similar technique is used in \\cite{TOR07}, where constraints are used to penalize non-smooth changes (over time) on the assigned clusters.\nThis segmenting technique is used to detect tasks performed during a day, based on video, on sound and on GPS information.\nIn~\\cite{SAN01}, the objects appearing in an image sequence are detected by using a hierarchical descending clustering, that regroups pixels into large temporally coherent clusters.\nThis method seeks to maximize the cluster size, while guaranteeing intra-cluster temporal consistency.\nAll of these techniques consider only one series of observations (a single entity) and must be adapted for the case of multiple series.\nThe main problem of a threshold based penalty function is to set the value of the threshold, which is usually data-dependent.\nOptimal matching is used in~\\cite{WID09} to discover trajectory models, while studying  the de-standardization of typical life courses.\n\nThe temporal dimension of the data is also used in some other fields of Information Retrieval.\nIn \\cite{TAL12}, constrained clustering is used to scope temporal relational facts in the knowledge bases, by exploiting temporal containment, alignment, succession, and mutual exclusion constraints among facts. \nIn \\cite{CHE09}, clustering is used to segment temporal observations into continuous chunks, as a preprocessing phase.\nA graphical model is proposed in \\cite{QAM06}, that uses a probabilistic model in which the timestamp is part of the observed variables, and the story is the hidden variable to be inferred.\nBut still, none of these approaches seek to create temporally coherent partitions of the data, mainly using the temporal dimension as a secondary information.\n\nIn the following sections, we propose a dissimilarity measure, a penalty function and a clustering algorithm in which the temporal dimension has a central role, and which address the limitations existing in the above presented work.\n\n\\section{Temporal-Driven Constrained Clustering}\n\\label{sec:TDCK-Means-proposal}\n\nThe observations $x_i \\in \\mathcal{X} $ that need to be structured can be written as triples $(entity, time, description)$: $x_i = (x_i^\\phi, x_i^t, x_i^d)$.\n$x_i^d \\in \\mathcal{D}$ is the vector in the multidimensional description space which describes the entity $x_i^\\phi \\in \\Phi$ at the moment of time $x_i^t \\in \\mathcal{T}$.\n\nTraditional clustering algorithms input a set of multidimensional vectors, which they regroup in such a way that observations inside a group resemble each other as much as possible, and resemble observations in other groups as little as possible.\n{\\mbox{K-Means}}{}~\\cite{MAC67} is a clustering algorithm based on iterative relocation, that partitions a dataset into $m$ clusters, locally minimizing the sum of distances between each data points $x_i$ and its assigned cluster centroids $\\mu_j \\in \\mathcal{M}$.\nAt each iteration, the objective function \n", "itemtype": "equation", "pos": 3093, "prevtext": "\n\n\\markboth{Rizoiu et al.}\n{How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions}\n\n\n\n\\catchline{}{}{}{}{}\n\n\n\n\\title{ \\uppercase{How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions}\n}\n\n\\author{\\footnotesize MARIAN-ANDREI RIZOIU }\n\n\\address{ERIC Laboratory, University Lumi\\`ere Lyon 2\\\\\n5, avenue Pierre Mend\u00c3\u00a8s France \\\\\n69676 Bron Cedex, France \\\\\nMarian-Andrei@rizoiu.eu}\n\n\\author{\\footnotesize JULIEN VELCIN }\n\n\\address{ERIC Laboratory, University Lumi\\`ere Lyon 2\\\\\nJulien.Velcin@univ-lyon2.fr}\n\n\\author{\\footnotesize ST\\'{E}PHANE LALLICH}\n\n\\address{ERIC Laboratory, University Lumi\\`ere Lyon 2\\\\\nStephane.Lallich@univ-lyon2.fr}\n\n\\maketitle\n\n\\begin{history}\n\\received{(Day Month Year)}\n\\revised{(Day Month Year)}\n\\accepted{(Day Month Year)}\n\n\\end{history}\n\n\\begin{abstract}\nIn this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension.\nObservations that are close in the description space, but distant in time are considered as dissimilar.\nWe also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function.\nWe combine the two propositions into a novel time-driven constrained clustering algorithm, called \\textbf{{\\mbox{TDCK-Means}}{}}, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space.\nThis algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster.\nWe apply our algorithm to a Political Studies dataset in order to detect typical evolution phases.\nWe adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.\n\\end{abstract}\n\n\\keywords{semi-supervised clustering; temporal clustering; temporal-aware dissimilarity measure; contiguity penalty function; temporal cluster graph structure.}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nResearchers in Social Sciences and Humanities (like Political Studies) have always gathered data and compiled databases of knowledge.\nThis information often has a temporal component, the evolution of a certain number of entities is recorded over a period of time.\nEach entity is described using multiple attributes, which form the multidimensional description space.\nTherefore, an entry in such a database would be an observation, a triple $(entity, timestamp, description)$.\nAn observation $x_i = (\\phi_l, t_m, x_i^d)$ signifies that the entity $\\phi_l$ is described by the vector $x_i^d$ at the moment of time $t_m$.\nWe denote by $x_i^\\phi$ the entity to which the observation $x_i$ is associated.\nSimilarly, $x_i^t$ is the timestamp associated with the observation $x_i$.\nEach observation belongs to a single entity and, consequently, each entity is associated with multiple observations, for different moments of time.\nFormally:\n\n", "index": 1, "text": "\\begin{align*}\n\t\\forall x_i \\in \\mathcal{D} &: \\exists ! \\, \\phi_l \\in \\Phi \\text{ so that } x_i^\\phi = \\phi_l \\\\\n\t\\forall (\\phi_l, t_m) \\in \\Phi \\times \\mathcal{T} &: \\exists ! \\, x_i=(x_i^\\phi, x_i^t, x_i^d) \\text{ so that } x_i^\\phi = \\phi_l \\text{ and } x_i^t = t_m\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall x_{i}\\in\\mathcal{D}\" display=\"inline\"><mrow><mrow><mo>\u2200</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\exists!\\,\\phi_{l}\\in\\Phi\\text{ so that }x_{i}^{\\phi}=\\phi_{l}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mrow><mrow><mo>\u2203</mo><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow><mo>\u2061</mo><msub><mi>\u03d5</mi><mi>l</mi></msub></mrow><mo>\u2208</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mtext>\u00a0so that\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup></mrow><mo>=</mo><msub><mi>\u03d5</mi><mi>l</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall(\\phi_{l},t_{m})\\in\\Phi\\times\\mathcal{T}\" display=\"inline\"><mrow><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u00d7</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\exists!\\,x_{i}=(x_{i}^{\\phi},x_{i}^{t},x_{i}^{d})\\text{ so that%&#10; }x_{i}^{\\phi}=\\phi_{l}\\text{ and }x_{i}^{t}=t_{m}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mrow><mrow><mo>\u2203</mo><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow><mo>\u2061</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0so that\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup></mrow><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>=</mo><msub><mi>t</mi><mi>m</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nis minimized until it reaches a local optimum.\n\nSuch a system is appropriate for constructing partitions based solely on $x_i^d$, the description in the multidimensional space.\nIt does not take into account the temporal order of the observations, nor the structure of the dataset, the fact that observations belong to entities.\nWe extend to the temporal case by adding to the centroids a temporal dimension $\\mu_j^t$, described in the same temporal space $\\mathcal{T}$ as the observations.\nJust like its multidimensional description vector $\\mu_j^d$, the temporal component does not necessary need to exist in the temporal set of the observation.\nIt is an abstraction of the temporal information in the group, serving as a cluster timestamp.\nTherefore, a centroid $\\mu_j$ will be the couple $(\\mu_j^t, \\mu_j^d)$.\n\nWe propose to adapt the {\\mbox{K-Means}}{} algorithm to the temporal case by adapting the Euclidean distance, normally used to measure the distance between an element and its centroid.\nThis novel temporal-aware dissimilarity measure takes into account both the distance in the multidimensional space and in the temporal space.\nIn order to ensure the temporal contiguity of observations for the entities, we add a penalty whenever two observations that belong to the same entity are assigned to different clusters. \nThe penalty depends on the time difference between the two: the lower the difference, the higher the penalty.\nWe integrate both into the \\textbf{Temporal-Driven Constrained {\\mbox{K-Means}}{}} (\\textbf{{\\mbox{TDCK-Means}}{}}), which is a temporal extension of {\\mbox{K-Means}}{}.\n{\\mbox{TDCK-Means}}{} searches to minimize the following objective function:\n\n", "itemtype": "equation", "pos": 13861, "prevtext": "\nFor example, a database studying the evolution of democratic states~\\cite{ARM11} will store, for each country and each year, the value of multiple economical, social, political and financial indicators.\nThe countries are the entities, and the years are the timestamps.\n\nStarting from such a database, one of the interests of Political Studies researchers is to detect typical evolution patterns. \nThere is a double interest: \na) obtaining a broader understanding of the phases that the entity collection went through over time (\\textit{e.g.} detecting the periods of global political instability, of economic crisis, of wealthiness \\textit{etc.});\nb) constructing the trajectory of an entity through the different phases (\\textit{e.g.} a country may have gone through a period of military dictatorship, followed by a period of wealthy democracy).\nThe criteria describing each phase are not known beforehand (which indicators announce a world economic crisis?) and may differ from one phase to another.\n\n\\begin{figure}[tb]\n\t\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{cluster-structuring}\n\t\t\\label{subfig:cluster-structuring}\n\t}\n\t\\hfill\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{entity-contiguity}\n\t\t\\label{subfig:entity-contiguity}\n\t}\n\t\\caption{Desired output: (a) the evolution phases and the entity trajectories, (b) the observations of 3 entities contiguously partitioned into 5 clusters.}\n\t\\label{fig:desired-output}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nWe address these issues by proposing a novel temporal-driven constrained clustering algorithm.\nThe proposed algorithm partitions the observations into clusters $\\mu_j \\in \\mathcal{M}$, that are coherent both in the multidimensional description space and in the temporal space.\nWe consider that the obtained clusters can be used to represent the typical phases of the evolution of the entities through time. \nFigure~\\ref{fig:desired-output} shows the desired result of our clustering algorithm.\nEach of the three depicted entities ($\\phi_1, \\phi_2$ and $\\phi_3$) is described at 10 moments of time ($t_m, m=1,2,...,10$).\nThe 30 observations of the dataset are partitioned into 5 clusters ($\\mu_j, j=1,2,...,5$).\nIn Figure~\\ref{subfig:cluster-structuring} we observe how clusters $\\mu_j$ are organized in time.\nEach of the clusters has a limited extent in time, and the time extents of clusters can overlap.\nThe temporal extent of a cluster is the minimal interval of time that contains all the timestamps of the observations in that cluster.\nThe entities navigate through clusters.\nWhen an observation belonging to an entity is assigned to cluster $\\mu_2$ and the anterior observation of the same entity is assigned in cluster $\\mu_1$, then we consider that the entity has a transition from phase $\\mu_1$ to phase $\\mu_2$.\nFigure~\\ref{subfig:entity-contiguity} shows how the series of observations belonging to each entity are assigned to clusters, thus forming continuous segments.\nThis succession of segments is interpreted as the succession of phases through which the entity passes. \nFor this succession to be meaningful, each entity should be assigned to a rather limited number of continuous segments.\nPassing through too many phases reduces the comprehension.\nSimilarly, evolutions which are alternations between two phases (\\textit{e.g.}, $\\mu_1 \\longrightarrow \\mu_2 \\longrightarrow \\mu_1 \\longrightarrow \\mu_2 $) hinder the comprehension.\n\nBased on these observations, we assume that the resulting partition must:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} }     \n\t\\item \\textbf{regroup observations having similar descriptions into the same cluster} (just as traditional clustering does).\n\tThe clusters represent a certain type of evolution;\n\t\\item \\textbf{create temporally coherent clusters, with limited extent in time.} \n\tIn order for a cluster to be meaningful, it should regroup observations which are temporally close (be contiguous on the temporal dimension).\n\tIf there are two different periods with similar evolutions (\\textit{e.g.} two economical crises), it is preferable to have them regrouped separately, as they represent two distinct phases.\n\tFurthermore, while it is acceptable that some evolutions exist during the entire period, usually the resulted clusters should have a limited temporal extent;\n\t\\item \\textbf{segment, as contiguously as possible, the series of observations for each entity.} \n\tThe sequence of segments will be interpreted as the sequence of phases through which the entity passes.\n   \\end{list}      \n\nIn this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension.\nObservations that are close in the description space, but distant in time are considered as dissimilar.\nWe also propose a method to enforce the segmentation contiguity, by introducing a penalty term based on the Normal Distribution Function.\nWe combine the two propositions into a novel time-driven constrained clustering algorithm, \\textbf{{\\mbox{TDCK-Means}}{}}, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space.\nThis algorithm uses soft semi-supervised constraints to encourage adjacent observations belonging to the same entity to be assigned to the same cluster.\nThe proposed algorithm constructs the clusters that serve as evolution phases and segments the observations series for each entity.\n\nThe paper is organized as follows.\nIn Section~\\ref{sec:related-work} we present some previous related works and, in Section~\\ref{sec:TDCK-Means-proposal}, we introduce the temporal-aware dissimilarity function, the contiguity penalty, function  the {\\mbox{TDCK-Means}}{} algorithm and the graph structure induction method.\nIn Section~\\ref{sec:xp}, we present the dataset that we use, the proposed evaluation measures and the obtained results.\nFinally, in Section~\\ref{sec:conclusion}, we draw the conclusion and plan some future extensions.\n\n\\section{Related work}\n\\label{sec:related-work}\n\nLeveraging partial expert knowledge into clustering represents the domain of semi-supervised clustering.\nThe expert knowledge is under the form of either class labels, or pairwise constraints.\nPairwise constraints~\\cite{WAG01} are either ``must-link'' (the observations must be placed in the same cluster) or ``cannot-link'' (the two observations cannot be placed in the same cluster).\nDepending on the method in which supervision is introduced into clustering, \\cite{GRI05} divides the semi-supervised clustering methods into two classes:\na) the similarity-adapting methods~\\cite{BIL03,COH03,KLE02,XIN02}, which seek to learn new similarity measures in order to satisfy the constraints, and \nb) the search-based methods~\\cite{BAS02,DEM99,WAG01} in which the clustering algorithm itself is modified.\n\nThe literature presents some examples of algorithms used to segment a series of observations into continuous chunks.\nIn \\cite{LIN06}, the daily tasks of a user are detected by segmenting scenes from the recordings of his activities.\nSemi-supervised must-link constraints are set between all pairs of observations, and a fixed penalty is inflicted when the following conditions are fulfilled simultaneously: the observations are not assigned to the same cluster and the time difference between their timestamps is less than a certain threshold.\nA similar technique is used in \\cite{TOR07}, where constraints are used to penalize non-smooth changes (over time) on the assigned clusters.\nThis segmenting technique is used to detect tasks performed during a day, based on video, on sound and on GPS information.\nIn~\\cite{SAN01}, the objects appearing in an image sequence are detected by using a hierarchical descending clustering, that regroups pixels into large temporally coherent clusters.\nThis method seeks to maximize the cluster size, while guaranteeing intra-cluster temporal consistency.\nAll of these techniques consider only one series of observations (a single entity) and must be adapted for the case of multiple series.\nThe main problem of a threshold based penalty function is to set the value of the threshold, which is usually data-dependent.\nOptimal matching is used in~\\cite{WID09} to discover trajectory models, while studying  the de-standardization of typical life courses.\n\nThe temporal dimension of the data is also used in some other fields of Information Retrieval.\nIn \\cite{TAL12}, constrained clustering is used to scope temporal relational facts in the knowledge bases, by exploiting temporal containment, alignment, succession, and mutual exclusion constraints among facts. \nIn \\cite{CHE09}, clustering is used to segment temporal observations into continuous chunks, as a preprocessing phase.\nA graphical model is proposed in \\cite{QAM06}, that uses a probabilistic model in which the timestamp is part of the observed variables, and the story is the hidden variable to be inferred.\nBut still, none of these approaches seek to create temporally coherent partitions of the data, mainly using the temporal dimension as a secondary information.\n\nIn the following sections, we propose a dissimilarity measure, a penalty function and a clustering algorithm in which the temporal dimension has a central role, and which address the limitations existing in the above presented work.\n\n\\section{Temporal-Driven Constrained Clustering}\n\\label{sec:TDCK-Means-proposal}\n\nThe observations $x_i \\in \\mathcal{X} $ that need to be structured can be written as triples $(entity, time, description)$: $x_i = (x_i^\\phi, x_i^t, x_i^d)$.\n$x_i^d \\in \\mathcal{D}$ is the vector in the multidimensional description space which describes the entity $x_i^\\phi \\in \\Phi$ at the moment of time $x_i^t \\in \\mathcal{T}$.\n\nTraditional clustering algorithms input a set of multidimensional vectors, which they regroup in such a way that observations inside a group resemble each other as much as possible, and resemble observations in other groups as little as possible.\n{\\mbox{K-Means}}{}~\\cite{MAC67} is a clustering algorithm based on iterative relocation, that partitions a dataset into $m$ clusters, locally minimizing the sum of distances between each data points $x_i$ and its assigned cluster centroids $\\mu_j \\in \\mathcal{M}$.\nAt each iteration, the objective function \n", "index": 3, "text": "\\begin{equation*}\n\t{I} = \\Sigma_{\\mu_j \\in \\mathcal{M}}\\Sigma_{x_i \\in \\mathcal{C}_j} || x_i^d - \\mu_j^d ||^2\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"{I}=\\Sigma_{\\mu_{j}\\in\\mathcal{M}}\\Sigma_{x_{i}\\in\\mathcal{C}_{j}}||x_{i}^{d}-%&#10;\\mu_{j}^{d}||^{2}\" display=\"block\"><mrow><mi>I</mi><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mo>\u2062</mo><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $||\\bullet||_{TA}$ is our temporal-aware (TA) dissimilarity measure (detailed in the next section), $w(x_i,x_j)$ is the cost function that determines the penalty of clustering adjacent observations of the same entity into different clusters, and $\\mathcal{C}_j$ is the set of observations in cluster $j$.\n\n\\subsection{The temporal-aware dissimilarity measure}\n\\label{subsec:proposal-temporal-dissimilarity-measure}\n\nThe proposed temporal-aware dissimilarity measure \\mbox{$|| x_i - x_j||_{TA}$} combines the Euclidean distance in the multidimensional space $\\mathcal{D}$ and the distance between the timestamps.\nWe propose to use the following formula:\n\n", "itemtype": "equation", "pos": 15673, "prevtext": "\nis minimized until it reaches a local optimum.\n\nSuch a system is appropriate for constructing partitions based solely on $x_i^d$, the description in the multidimensional space.\nIt does not take into account the temporal order of the observations, nor the structure of the dataset, the fact that observations belong to entities.\nWe extend to the temporal case by adding to the centroids a temporal dimension $\\mu_j^t$, described in the same temporal space $\\mathcal{T}$ as the observations.\nJust like its multidimensional description vector $\\mu_j^d$, the temporal component does not necessary need to exist in the temporal set of the observation.\nIt is an abstraction of the temporal information in the group, serving as a cluster timestamp.\nTherefore, a centroid $\\mu_j$ will be the couple $(\\mu_j^t, \\mu_j^d)$.\n\nWe propose to adapt the {\\mbox{K-Means}}{} algorithm to the temporal case by adapting the Euclidean distance, normally used to measure the distance between an element and its centroid.\nThis novel temporal-aware dissimilarity measure takes into account both the distance in the multidimensional space and in the temporal space.\nIn order to ensure the temporal contiguity of observations for the entities, we add a penalty whenever two observations that belong to the same entity are assigned to different clusters. \nThe penalty depends on the time difference between the two: the lower the difference, the higher the penalty.\nWe integrate both into the \\textbf{Temporal-Driven Constrained {\\mbox{K-Means}}{}} (\\textbf{{\\mbox{TDCK-Means}}{}}), which is a temporal extension of {\\mbox{K-Means}}{}.\n{\\mbox{TDCK-Means}}{} searches to minimize the following objective function:\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:obj-function}\n\t\\mathcal{J} = \\sum_{\\mu_j \\in \\mathcal{M}}\\sum_{x_i \\in \\mathcal{C}_j} \\left( || x_i - \\mu_j||_{TA} + \\sum_{\\substack{x_k \\not\\in \\mathcal{C}_j\\\\x_k^\\phi = x_i^\\phi}} w(x_i, x_k) \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{J}=\\sum_{\\mu_{j}\\in\\mathcal{M}}\\sum_{x_{i}\\in\\mathcal{C}_{j}}\\left(||%&#10;x_{i}-\\mu_{j}||_{TA}+\\sum_{\\begin{subarray}{c}x_{k}\\not\\in\\mathcal{C}_{j}\\\\&#10;x_{k}^{\\phi}=x_{i}^{\\phi}\\end{subarray}}w(x_{i},x_{k})\\right)\" display=\"block\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></munder><mrow><mo>(</mo><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03bc</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>A</mi></mrow></msub><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>\u2209</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></mtd></mtr><mtr><mtd><mrow><msubsup><mi>x</mi><mi>k</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup></mrow></mtd></mtr></mtable></munder><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $||\\bullet||$ is the classical $L^2$ norm and $\\Delta x_{max}$ and $\\Delta t_{max}$ are the diameters of $\\mathcal{D}$, and $\\mathcal{T}$ respectively (the largest distance encountered between two observations in the multidimensional description space and, respectively, in the temporal space).\nThe following properties are immediate:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} }   \n\t\\item $ || x_i - x_j||_{TA} \\in [0,1], \\forall x_i, x_j \\in \\mathcal{X} $\n\t\\item $ || x_i - x_j||_{TA} = 0 \\Leftrightarrow x_i^d = x_j^d$ and $x_i^t = x_j^t$\n\t\\item $ || x_i - x_j||_{TA} = 1(maximum) \\Leftrightarrow ||x_i^d - x_j^d|| = \\Delta x_{max}$ or $||x_i^t - x_j^t|| = \\Delta t_{max}$\n   \\end{list}     \n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{temporal-aware-measure-colormap}\n\t\\caption{Color map of the temporal-aware dissimilarity measure as a function of the multidimensional component and the temporal component.}\n\t\\label{fig:temporal-measure-colormap}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFigure~\\ref{fig:temporal-measure-colormap} plots the temporal-aware dissimilarity measure as a color map, depending on the multidimensional component and the temporal component.\nThe horizontal axis represents the normalized multidimensional distance ($\\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^2}$).\nThe vertical axis represents the normalized temporal distance ($\\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}}$).\nThe blue color shows a temporal-aware measure close to the minimum and the red color represents the maximum.\nThe dissimilarity measure is zero if and only if the two observations have equal timestamps and equal multidimensional description vectors.\nStill, it suffices for only one of the components (temporal, multidimensional) to attend the maximum value for the measure to reach its maximum.\nThe measure behaves similar to a MAX operator, always choosing a value closer to the maximum of the two components.\nThe formula for the temporal-aware dissimilarity measure was chosen so that any algorithm that seeks to minimize an objective function based on this measure, will need to minimize both its components.\nThis makes it suitable for algorithms that search to minimize both the multidimensional and the temporal variance in clusters.\n\nBoth components that intervene in the measure follow a function like $ 1 - \\epsilon^2, \\epsilon \\in [0,1]$.\nThis function provides a good compromise: it is tolerant for small values of $\\epsilon$ (small time difference, small multidimensional distance), but decreases rapidly when $\\epsilon$ augments.\nThe temporal-aware dissimilarity measure is an extension of the Euclidean function.\nIf the timestamps are unknown and set to be all equal, the temporal component is canceled and the temporal-aware dissimilarity measure becomes a normalized Euclidean distance.\nIn Section~\\ref{subsec:quantitative-evaluation}, we evaluate the behavior of the proposed dissimilarity function.\nWe will call \\textbf{Temporal-Driven {\\mbox{K-Means}}{}} the algorithm that is based on the {\\mbox{K-Means}}{}' iterative structure and uses the temporal-aware dissimilarity measure to asses similarity between observations.\nNote that \\textbf{Temporal-Driven {\\mbox{K-Means}}{}}, relative to {\\mbox{TDCK-Means}}{}, has no contiguous segmentation penalty function (the contiguous segmentation penalty function is detailed in the next section).\n\n\\subsection{The contiguity penalty function}\n\\label{subsec:proposal-penalty-function}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{penalty_vs_delta}\n\t\\caption{Penalty function vs. time difference for multiple $\\delta$. $(\\beta = 1)$}\n\t\\label{fig:penalty-delta}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe penalty function encourages temporally adjacent observations of the same entity to  be assigned to the same cluster.\nWe use the notion of \\textit{soft pair-wise constraints} from semi-supervised clustering.\nA ``must-link'' soft constraint is added between all pairs of observations belonging to the same entity.\nThe clustering is allowed to break the constraints, while inflicting a penalty for each of these violations.\nThe penalty is more severe if the observations are closer in time.\nThe function is defined as:\n\n", "itemtype": "equation", "pos": 16576, "prevtext": "\nwhere $||\\bullet||_{TA}$ is our temporal-aware (TA) dissimilarity measure (detailed in the next section), $w(x_i,x_j)$ is the cost function that determines the penalty of clustering adjacent observations of the same entity into different clusters, and $\\mathcal{C}_j$ is the set of observations in cluster $j$.\n\n\\subsection{The temporal-aware dissimilarity measure}\n\\label{subsec:proposal-temporal-dissimilarity-measure}\n\nThe proposed temporal-aware dissimilarity measure \\mbox{$|| x_i - x_j||_{TA}$} combines the Euclidean distance in the multidimensional space $\\mathcal{D}$ and the distance between the timestamps.\nWe propose to use the following formula:\n\n", "index": 7, "text": "\\begin{equation} \\label{eq:temp-distance}\n\t|| x_i - x_j||_{TA} = 1 - \\left(1 - \\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^2}\\right)\\left(1 - \\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}}\\right) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"||x_{i}-x_{j}||_{TA}=1-\\left(1-\\frac{||x_{i}^{d}-x_{j}^{d}||^{2}}{\\Delta x_{%&#10;max}^{2}}\\right)\\left(1-\\frac{||x_{i}^{t}-x_{j}^{t}||^{2}}{\\Delta t_{max}^{2}}\\right)\" display=\"block\"><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>A</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $\\beta$ is a scaling factor and, at the same time, the maximum value taken by the penalty function;\n$\\delta$ is a parameter which controls the width of the function.\n$\\beta$ is dataset dependent and can be set as a percentage of the average distance between observations.\n$\\mathbbm{1} \\left[ statement \\right]$ is a function that returns $1$ if $statement$ is true and $0$ otherwise.\n\nThe function resembles to the positive side of the Normal Distribution function, centered in zero.\nThe function has a particular shape, as represented in Figure~\\ref{fig:penalty-delta}.\nFor small time differences, it descends very slowly, thus inflicting a high penalty for breaking a constraint.\nAs the time difference increases, the penalty decreases rapidly, converging towards zero.\nWhen $\\delta$ is small, the functions value descends very quickly with the time difference.\nThe function produces penalties only if the constraint is broken for adjacent observation.\nFor high values of $\\delta$, breaking constraints for distant observations cause high penalties, therefore creating segmentations with large segments.\nFigure~\\ref{fig:penalty-delta} shows the evolution of the penalty function with the time difference between two observations, for multiple values of $\\delta$ and for $\\beta=1$.\n\nAn advantage of the proposed function is that it requires no time discretization or setting a fixed window width, as proposed in \\cite{LIN06}.\nThe $\\delta$ parameter permits the fine tuning of the penalty function.\nIn Section~\\ref{subsec:quantitative-evaluation}, we evaluate \\textbf{Constrained {\\mbox{K-Means}}{}}, which is an extension of {\\mbox{K-Means}}{}, to which we add the proposed contiguity penalty function (but which does not take into account the temporal dimension when measuring the distance between observations).\nThe influence of both $\\beta$ and $\\delta$ will be studied in Section~\\ref{subsec:parameters-beta-delta}.\n\n\\subsection{The {\\mbox{TDCK-Means}}{} algorithm}\n\\label{subsec:proposal-tdck-means}\n\nThe time dependent distance $|| x_i - \\mu_j ||_{TA}$ encourages the decrease of both the temporal and multidimensional variance of clusters;\nmeanwhile the penalty function $w(x_i,x_k)$ favors the adjacent observations belonging to the same entity to be assigned to the same cluster.\nThe rest of the {\\mbox{TDCK-Means}}{} algorithm is similar to the {\\mbox{K-Means}}{} algorithm.\nIt seeks to minimize $\\mathcal{J}$ by iterating an assignment phase and a centroid update phase until the partition does not change between two iterations.\nThe outline of the algorithm is given in Algorithm~\\ref{algo:proposed-algo}.\n\nThe \\textbf{choose\\_random} function chooses randomly, for each centroid $\\mu_j$, an observation $x_i$ and sets $\\mu_j = (x_i^t, x_i^d)$.\nIn the assignment phase, for every observation $x_i$, the \\textbf{best\\_cluster} function chooses a cluster $\\mathcal{C}_j$ so that the temporal-aware dissimilarity measure from $x_i$ to the cluster's centroid $\\mu_j$, added to the cost of penalties possibly incurred by this cluster assignment, is minimized.\nIt resumes to solving the following equation:\n\n", "itemtype": "equation", "pos": 21220, "prevtext": "\nwhere $||\\bullet||$ is the classical $L^2$ norm and $\\Delta x_{max}$ and $\\Delta t_{max}$ are the diameters of $\\mathcal{D}$, and $\\mathcal{T}$ respectively (the largest distance encountered between two observations in the multidimensional description space and, respectively, in the temporal space).\nThe following properties are immediate:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} }   \n\t\\item $ || x_i - x_j||_{TA} \\in [0,1], \\forall x_i, x_j \\in \\mathcal{X} $\n\t\\item $ || x_i - x_j||_{TA} = 0 \\Leftrightarrow x_i^d = x_j^d$ and $x_i^t = x_j^t$\n\t\\item $ || x_i - x_j||_{TA} = 1(maximum) \\Leftrightarrow ||x_i^d - x_j^d|| = \\Delta x_{max}$ or $||x_i^t - x_j^t|| = \\Delta t_{max}$\n   \\end{list}     \n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{temporal-aware-measure-colormap}\n\t\\caption{Color map of the temporal-aware dissimilarity measure as a function of the multidimensional component and the temporal component.}\n\t\\label{fig:temporal-measure-colormap}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFigure~\\ref{fig:temporal-measure-colormap} plots the temporal-aware dissimilarity measure as a color map, depending on the multidimensional component and the temporal component.\nThe horizontal axis represents the normalized multidimensional distance ($\\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^2}$).\nThe vertical axis represents the normalized temporal distance ($\\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}}$).\nThe blue color shows a temporal-aware measure close to the minimum and the red color represents the maximum.\nThe dissimilarity measure is zero if and only if the two observations have equal timestamps and equal multidimensional description vectors.\nStill, it suffices for only one of the components (temporal, multidimensional) to attend the maximum value for the measure to reach its maximum.\nThe measure behaves similar to a MAX operator, always choosing a value closer to the maximum of the two components.\nThe formula for the temporal-aware dissimilarity measure was chosen so that any algorithm that seeks to minimize an objective function based on this measure, will need to minimize both its components.\nThis makes it suitable for algorithms that search to minimize both the multidimensional and the temporal variance in clusters.\n\nBoth components that intervene in the measure follow a function like $ 1 - \\epsilon^2, \\epsilon \\in [0,1]$.\nThis function provides a good compromise: it is tolerant for small values of $\\epsilon$ (small time difference, small multidimensional distance), but decreases rapidly when $\\epsilon$ augments.\nThe temporal-aware dissimilarity measure is an extension of the Euclidean function.\nIf the timestamps are unknown and set to be all equal, the temporal component is canceled and the temporal-aware dissimilarity measure becomes a normalized Euclidean distance.\nIn Section~\\ref{subsec:quantitative-evaluation}, we evaluate the behavior of the proposed dissimilarity function.\nWe will call \\textbf{Temporal-Driven {\\mbox{K-Means}}{}} the algorithm that is based on the {\\mbox{K-Means}}{}' iterative structure and uses the temporal-aware dissimilarity measure to asses similarity between observations.\nNote that \\textbf{Temporal-Driven {\\mbox{K-Means}}{}}, relative to {\\mbox{TDCK-Means}}{}, has no contiguous segmentation penalty function (the contiguous segmentation penalty function is detailed in the next section).\n\n\\subsection{The contiguity penalty function}\n\\label{subsec:proposal-penalty-function}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{penalty_vs_delta}\n\t\\caption{Penalty function vs. time difference for multiple $\\delta$. $(\\beta = 1)$}\n\t\\label{fig:penalty-delta}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe penalty function encourages temporally adjacent observations of the same entity to  be assigned to the same cluster.\nWe use the notion of \\textit{soft pair-wise constraints} from semi-supervised clustering.\nA ``must-link'' soft constraint is added between all pairs of observations belonging to the same entity.\nThe clustering is allowed to break the constraints, while inflicting a penalty for each of these violations.\nThe penalty is more severe if the observations are closer in time.\nThe function is defined as:\n\n", "index": 9, "text": "\\begin{equation} \\label{eq:penalty-function}\n\tw(x_i, x_k) = \\beta * e^{-\\frac{1}{2} \\left( \\frac{||x_i^t - x_k^t||}{\\delta} \\right)^2} \\mathbbm{1} \\left[x_i^\\phi = x_k^\\phi \\right] \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"w(x_{i},x_{k})=\\beta*e^{-\\frac{1}{2}\\left(\\frac{||x_{i}^{t}-x_{k}^{t}||}{%&#10;\\delta}\\right)^{2}}\\mathbbm{1}\\left[x_{i}^{\\phi}=x_{k}^{\\phi}\\right]\" display=\"block\"><mrow><mi>w</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03b2</mi><mo>*</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>k</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mi>\u03b4</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msup><mn>\ud835\udfd9</mn><mrow><mo>[</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>k</mi><mi>\u03d5</mi></msubsup><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nThis guaranties that the contribution of $x_i$ to the value of $\\mathcal{J}$ diminishes or stays constant.\nOverall, this assures that $\\mathcal{J}$ diminishes in the assignment phase (or stays constant).\n\n\\begin{algorithm}[ht]                     \n\\caption{Outline of the {\\mbox{TDCK-Means}}{} algorithm.}         \n\\label{algo:proposed-algo} \n\n\\begin{algorithmic} \n\n\\REQUIRE $ x_i \\in \\mathcal{X} $ - observations to cluster;\n\\REQUIRE $ m $ - number of requested clusters;\n\\ENSURE $\\mathcal{C}_j, j = 1,2,...,m$ - $m$ clusters;\n\\ENSURE $\\mu_j, j = 1,2,...,m$ - centroids for each cluster; \n\n\\FOR {$j = 1,2,..,m$}\n\t\\STATE $ \\mu_j \\gets $ \\textbf{choose\\_random}($\\mathcal{X}$)\n\\ENDFOR\n\\STATE $ iter \\gets 0$\n\\STATE $\\mathcal{M}^{(iter)} \\gets \\emptyset$ \\hspace{5mm} \\textit{//set of centroids}\n\\STATE $\\mathcal{P}^{(iter)} \\gets \\emptyset$ \\hspace{5mm} \\textit{//set of clusters}\n\n\\REPEAT\t\n\t\\STATE $iter \\gets iter + 1$\n\t\\FOR {$j = 1,2,...,m $}\n\t\t\\STATE $\\mathcal{C}_j^{(iter)} \\gets \\emptyset$\n\t\\ENDFOR\n\t\n\t\\STATE \\textit{// assignment phase}\n\t\\FOR { $x_i \\in \\mathcal{X}$ }\n\t\t\\STATE $\\mathcal{C}_j^{(iter)} = \\mathcal{C}_j^{(iter)} \\cup x_i | $ where $ j = $ \\textbf{best\\_cluster}($\\mathcal{X}$, $\\mathcal{M}^{(iter-1)}$, $\\mathcal{P}^{(iter-1)}$)\n\t\\ENDFOR\n\t\n\t\\STATE \\textit{// centroids update phase}\n\t\\FOR {$j = 1,2,...,m $}\n\t\t\\STATE $ (\\mu_j^{\\phi, (iter)}, \\mu_j^{t, (iter)}) \\gets $ \\textbf{update\\_centroid}($j$, $\\mathcal{X}$, $\\mathcal{M}^{(iter-1)}$, $\\mathcal{P}^{(iter-1)}$)\n\t\\ENDFOR\n\n\t\\STATE $\\mathcal{M}^{(iter)} \\gets \\{\\mu_j^{(iter)} | j = 1,2,...,m\\}$\n\t\\STATE $\\mathcal{P}^{(iter)} \\gets \\{\\mathcal{C}_j^{(iter)} | j = 1,2,...,m\\}$ \n\\UNTIL {$\\mathcal{C}_j^{(iter)} = \\mathcal{C}_j^{(iter-1)}, \\forall j \\in [1,m]$}\n\\end{algorithmic}\n\\end{algorithm}\n\nIn the centroid update phase, the \\textbf{update\\_centroid} function recalculates the cluster centroids using the observations in $\\mathcal{X}$ and the assignment at the previous iteration.\nTherefore the contribution of each cluster to the $\\mathcal{J}$ function is minimized.\nEach of the temporal and the multidimensional components is calculated individually.\nIn order to find the values that minimize the objective function, we need to solve the equations:\n\n", "itemtype": "equation", "pos": 24537, "prevtext": "\nwhere $\\beta$ is a scaling factor and, at the same time, the maximum value taken by the penalty function;\n$\\delta$ is a parameter which controls the width of the function.\n$\\beta$ is dataset dependent and can be set as a percentage of the average distance between observations.\n$\\mathbbm{1} \\left[ statement \\right]$ is a function that returns $1$ if $statement$ is true and $0$ otherwise.\n\nThe function resembles to the positive side of the Normal Distribution function, centered in zero.\nThe function has a particular shape, as represented in Figure~\\ref{fig:penalty-delta}.\nFor small time differences, it descends very slowly, thus inflicting a high penalty for breaking a constraint.\nAs the time difference increases, the penalty decreases rapidly, converging towards zero.\nWhen $\\delta$ is small, the functions value descends very quickly with the time difference.\nThe function produces penalties only if the constraint is broken for adjacent observation.\nFor high values of $\\delta$, breaking constraints for distant observations cause high penalties, therefore creating segmentations with large segments.\nFigure~\\ref{fig:penalty-delta} shows the evolution of the penalty function with the time difference between two observations, for multiple values of $\\delta$ and for $\\beta=1$.\n\nAn advantage of the proposed function is that it requires no time discretization or setting a fixed window width, as proposed in \\cite{LIN06}.\nThe $\\delta$ parameter permits the fine tuning of the penalty function.\nIn Section~\\ref{subsec:quantitative-evaluation}, we evaluate \\textbf{Constrained {\\mbox{K-Means}}{}}, which is an extension of {\\mbox{K-Means}}{}, to which we add the proposed contiguity penalty function (but which does not take into account the temporal dimension when measuring the distance between observations).\nThe influence of both $\\beta$ and $\\delta$ will be studied in Section~\\ref{subsec:parameters-beta-delta}.\n\n\\subsection{The {\\mbox{TDCK-Means}}{} algorithm}\n\\label{subsec:proposal-tdck-means}\n\nThe time dependent distance $|| x_i - \\mu_j ||_{TA}$ encourages the decrease of both the temporal and multidimensional variance of clusters;\nmeanwhile the penalty function $w(x_i,x_k)$ favors the adjacent observations belonging to the same entity to be assigned to the same cluster.\nThe rest of the {\\mbox{TDCK-Means}}{} algorithm is similar to the {\\mbox{K-Means}}{} algorithm.\nIt seeks to minimize $\\mathcal{J}$ by iterating an assignment phase and a centroid update phase until the partition does not change between two iterations.\nThe outline of the algorithm is given in Algorithm~\\ref{algo:proposed-algo}.\n\nThe \\textbf{choose\\_random} function chooses randomly, for each centroid $\\mu_j$, an observation $x_i$ and sets $\\mu_j = (x_i^t, x_i^d)$.\nIn the assignment phase, for every observation $x_i$, the \\textbf{best\\_cluster} function chooses a cluster $\\mathcal{C}_j$ so that the temporal-aware dissimilarity measure from $x_i$ to the cluster's centroid $\\mu_j$, added to the cost of penalties possibly incurred by this cluster assignment, is minimized.\nIt resumes to solving the following equation:\n\n", "index": 11, "text": "\\begin{equation*}\n\\mathbf{best\\_cluster}(i) = \\underset{j = 1,2,...,m}{argmin} \\left( || x_i - \\mu_j^{(iter-1)}\\,\\,||_{TA}^2 + \\sum_{\\substack{x_k \\not\\in \\mathcal{C}_j^{(iter-1)}}}^{x_k^\\phi = x_i^\\phi} w(x_i, x_k) \\right)\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{best\\_cluster}(i)=\\underset{j=1,2,...,m}{argmin}\\left(||x_{i}-\\mu_{j}^%&#10;{(iter-1)}\\,\\,||_{TA}^{2}+\\sum_{\\begin{subarray}{c}x_{k}\\not\\in\\mathcal{C}_{j}%&#10;^{(iter-1)}\\end{subarray}}^{x_{k}^{\\phi}=x_{i}^{\\phi}}w(x_{i},x_{k})\\right)\" display=\"block\"><mrow><mrow><mi>\ud835\udc1b\ud835\udc1e\ud835\udc2c\ud835\udc2d</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>\ud835\udc1c\ud835\udc25\ud835\udc2e\ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc2b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder accentunder=\"true\"><mrow><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></munder><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mpadded width=\"+3.4pt\"><msubsup><mi>\u03bc</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mpadded></mrow><mo fence=\"true\">||</mo></mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>A</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\"><mtr><mtd><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>\u2209</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mtd></mtr></mtable><mrow><msubsup><mi>x</mi><mi>k</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup></mrow></munderover><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nBy replacing equations~(\\ref{eq:temp-distance}) and~(\\ref{eq:penalty-function}) in~(\\ref{eq:obj-function}), we obtain the following formula for the objective function:\n\n", "itemtype": "equation", "pos": 27005, "prevtext": "\nThis guaranties that the contribution of $x_i$ to the value of $\\mathcal{J}$ diminishes or stays constant.\nOverall, this assures that $\\mathcal{J}$ diminishes in the assignment phase (or stays constant).\n\n\\begin{algorithm}[ht]                     \n\\caption{Outline of the {\\mbox{TDCK-Means}}{} algorithm.}         \n\\label{algo:proposed-algo} \n\n\\begin{algorithmic} \n\n\\REQUIRE $ x_i \\in \\mathcal{X} $ - observations to cluster;\n\\REQUIRE $ m $ - number of requested clusters;\n\\ENSURE $\\mathcal{C}_j, j = 1,2,...,m$ - $m$ clusters;\n\\ENSURE $\\mu_j, j = 1,2,...,m$ - centroids for each cluster; \n\n\\FOR {$j = 1,2,..,m$}\n\t\\STATE $ \\mu_j \\gets $ \\textbf{choose\\_random}($\\mathcal{X}$)\n\\ENDFOR\n\\STATE $ iter \\gets 0$\n\\STATE $\\mathcal{M}^{(iter)} \\gets \\emptyset$ \\hspace{5mm} \\textit{//set of centroids}\n\\STATE $\\mathcal{P}^{(iter)} \\gets \\emptyset$ \\hspace{5mm} \\textit{//set of clusters}\n\n\\REPEAT\t\n\t\\STATE $iter \\gets iter + 1$\n\t\\FOR {$j = 1,2,...,m $}\n\t\t\\STATE $\\mathcal{C}_j^{(iter)} \\gets \\emptyset$\n\t\\ENDFOR\n\t\n\t\\STATE \\textit{// assignment phase}\n\t\\FOR { $x_i \\in \\mathcal{X}$ }\n\t\t\\STATE $\\mathcal{C}_j^{(iter)} = \\mathcal{C}_j^{(iter)} \\cup x_i | $ where $ j = $ \\textbf{best\\_cluster}($\\mathcal{X}$, $\\mathcal{M}^{(iter-1)}$, $\\mathcal{P}^{(iter-1)}$)\n\t\\ENDFOR\n\t\n\t\\STATE \\textit{// centroids update phase}\n\t\\FOR {$j = 1,2,...,m $}\n\t\t\\STATE $ (\\mu_j^{\\phi, (iter)}, \\mu_j^{t, (iter)}) \\gets $ \\textbf{update\\_centroid}($j$, $\\mathcal{X}$, $\\mathcal{M}^{(iter-1)}$, $\\mathcal{P}^{(iter-1)}$)\n\t\\ENDFOR\n\n\t\\STATE $\\mathcal{M}^{(iter)} \\gets \\{\\mu_j^{(iter)} | j = 1,2,...,m\\}$\n\t\\STATE $\\mathcal{P}^{(iter)} \\gets \\{\\mathcal{C}_j^{(iter)} | j = 1,2,...,m\\}$ \n\\UNTIL {$\\mathcal{C}_j^{(iter)} = \\mathcal{C}_j^{(iter-1)}, \\forall j \\in [1,m]$}\n\\end{algorithmic}\n\\end{algorithm}\n\nIn the centroid update phase, the \\textbf{update\\_centroid} function recalculates the cluster centroids using the observations in $\\mathcal{X}$ and the assignment at the previous iteration.\nTherefore the contribution of each cluster to the $\\mathcal{J}$ function is minimized.\nEach of the temporal and the multidimensional components is calculated individually.\nIn order to find the values that minimize the objective function, we need to solve the equations:\n\n", "index": 13, "text": "\\begin{equation} \\label{eq:derivatives}\n\t\\frac{\\partial \\mathcal{J}}{\\partial \\mu_j^d} = 0 \\; ; \\; \\;\n\t\\frac{\\partial \\mathcal{J}}{\\partial \\mu_j^t} = 0\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\mathcal{J}}{\\partial\\mu_{j}^{d}}=0\\;;\\;\\;\\frac{\\partial\\mathcal%&#10;{J}}{\\partial\\mu_{j}^{t}}=0\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow></mfrac><mo>=</mo><mpadded width=\"+2.8pt\"><mn>0</mn></mpadded></mrow><mo rspace=\"8.1pt\">;</mo><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nTherefore, from equations~(\\ref{eq:derivatives}) and~(\\ref{eq:objective-function-complete}), we obtain the centroid update formulas:\n\n", "itemtype": "equation", "pos": 27341, "prevtext": "\nBy replacing equations~(\\ref{eq:temp-distance}) and~(\\ref{eq:penalty-function}) in~(\\ref{eq:obj-function}), we obtain the following formula for the objective function:\n\n", "index": 15, "text": "\\begin{align} \\label{eq:objective-function-complete}\n \t\\mathcal{J} = |\\mathcal{X}| &- \\sum_{j = 1}^{m} \\sum_{x_i \\in \\mathcal{C}_{j}} \\left[ \\left(1 - \\frac{||x_i^d - \\mu_j^d||^2}{\\Delta x_{max}^2}\\right)\\left(1 - \\frac{||x_i^t - \\mu_j^t||^2}{\\Delta t_{max}^{2}} \\right) \\right] \\notag \\\\\n\t&+ \\sum_{x_i \\in \\mathcal{X}}\\sum_{x_k \\not\\in \\mathcal{C}_j} \\beta * e^{-\\frac{1}{2} \\left( \\frac{||x_i^t - x_k^t||}{\\delta} \\right)^2} \\mathbbm{1} \\left[x_i^\\phi = x_k^\\phi \\right]\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{J}=|\\mathcal{X}|\" display=\"inline\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\sum_{j=1}^{m}\\sum_{x_{i}\\in\\mathcal{C}_{j}}\\left[\\left(1-\\frac{%&#10;||x_{i}^{d}-\\mu_{j}^{d}||^{2}}{\\Delta x_{max}^{2}}\\right)\\left(1-\\frac{||x_{i}%&#10;^{t}-\\mu_{j}^{t}||^{2}}{\\Delta t_{max}^{2}}\\right)\\right]\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></munder></mstyle><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{x_{i}\\in\\mathcal{X}}\\sum_{x_{k}\\not\\in\\mathcal{C}_{j}}%&#10;\\beta*e^{-\\frac{1}{2}\\left(\\frac{||x_{i}^{t}-x_{k}^{t}||}{\\delta}\\right)^{2}}%&#10;\\mathbbm{1}\\left[x_{i}^{\\phi}=x_{k}^{\\phi}\\right]\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>\u2209</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></munder></mstyle><mi>\u03b2</mi><mo>*</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>k</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mi>\u03b4</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msup><mn>\ud835\udfd9</mn><mrow><mo>[</mo><msubsup><mi>x</mi><mi>i</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>k</mi><mi>\u03d5</mi></msubsup><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n\nJust like the centroid update phase in {\\mbox{K-Means}}{}, the new centroids in {\\mbox{TDCK-Means}}{} are also averages over the observations.\nUnlike {\\mbox{K-Means}}{}, the averages are weighted for each component, using the distance from the other.\nFor example, each observation contributes to the multidimensional description of the new centroid, proportional with its temporal centrality in the cluster.\nObservations that are more distant in time (from the centroid) contribute less to the multidimensional description than the ones being closer in time.\nA similar logic applies to the temporal component.\nThe consequence is that the new clusters are coherent both in the multidimensional space and in the temporal one.\n\n\\paragraph*{Algorithm's complexity} Equation~(\\ref{eq:objective-function-complete}) shows that {\\mbox{TDCK-Means}}{}' complexity is $\\mathcal{O}(n^{2}m)$, due to the penalty term.\nStill, the equation can be rewritten, so that only observations belonging to the same entity are tested.\nIf $p$ is the number of entities and $q$ is the maximum number of observations associated with each entity, then $n = p \\times q$.\nThe complexity of {\\mbox{TDCK-Means}}{} is $\\mathcal{O}(pq^{2}m)$, which is well adapted to Social Science and Humanities datasets, where often a large number of individuals is studied over a relatively short period of time ($p > q$).\n\n\\subsection{Fine-tuning the ratio between components}\n\\label{subsec:measure-tuning-alpha}\n\nThe temporal-aware dissimilarity measure, as presented in Equation~(\\ref{eq:temp-distance}), gives equal importance to both the multidimensional component and the temporal component.\nThis might pose problems when the data are not uniformly distributed both in the multidimensional descriptive space and in the temporal space.\nIf the medium standard deviation reported to the medium distance between pairs of observations is greater in one space than in the other, giving equal weight to the components can lead to important bias in the clustering process.\n\\textit{E.g.} observations that are very uniformly distributed in the temporal space (same number of observations for each timestamp) and, at the same time, rather compactly distributed in the description space.\nIn this case, in average, the temporal component weight more in the dissimilarity measure than the multidimensional component.\nConsequently, the clustering is biased towards the temporal cohesion of clusters.\nSimilarly, in some applications, it is desirable to privilege one component over the other.\n\\textit{E.g.} on a large enough scale, user roles in social networks have a temporal component (new types of roles might appear over the years).\nBut in a limited time span, it is perfectly acceptable that the roles can coexist simultaneously.\nTherefore, the temporal component should have only a mild impact on the overall measure.\n\nWe adjust the ratio between the two components by using two tuning factors $\\gamma_d$ and $\\gamma_t$.\n$\\gamma_d$ weights the multidimensional component of the temporal-aware dissimilarity measure, whereas $\\gamma_t$ weights the temporal component.\nEquation~(\\ref{eq:temp-distance}) can be rewritten as:\n\n", "itemtype": "equation", "pos": 27959, "prevtext": "\nTherefore, from equations~(\\ref{eq:derivatives}) and~(\\ref{eq:objective-function-complete}), we obtain the centroid update formulas:\n\n", "index": 17, "text": "\\begin{equation} \\label{eq:centroid-update}\n\t\\mu_j^d = \\frac{\\sum_{x_i \\in \\mathcal{C}_j} x_i^d \\times \\left(1 - \\frac{||x_i^t - \\mu_j^t||^2}{\\Delta t_{max}^{2}} \\right)}{\\sum_{x_i \\in \\mathcal{C}_j} \\left(1 - \\frac{||x_i^t - \\mu_j^t||^2}{\\Delta t_{max}^{2}} \\right)} \\; ; \\; \\;\n\t\\mu_j^t = \\frac{\\sum_{x_i \\in \\mathcal{C}_j} x_i^t \\times \\left(1 - \\frac{||x_i^d - \\mu_j^d||^2}{\\Delta x_{max}^{2}} \\right)}{\\sum_{x_i \\in \\mathcal{C}_j} \\left(1 - \\frac{||x_i^d - \\mu_j^d||^2}{\\Delta x_{max}^{2}} \\right)} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mu_{j}^{d}=\\frac{\\sum_{x_{i}\\in\\mathcal{C}_{j}}x_{i}^{d}\\times\\left(1-\\frac{|%&#10;|x_{i}^{t}-\\mu_{j}^{t}||^{2}}{\\Delta t_{max}^{2}}\\right)}{\\sum_{x_{i}\\in%&#10;\\mathcal{C}_{j}}\\left(1-\\frac{||x_{i}^{t}-\\mu_{j}^{t}||^{2}}{\\Delta t_{max}^{2%&#10;}}\\right)}\\;;\\;\\;\\mu_{j}^{t}=\\frac{\\sum_{x_{i}\\in\\mathcal{C}_{j}}x_{i}^{t}%&#10;\\times\\left(1-\\frac{||x_{i}^{d}-\\mu_{j}^{d}||^{2}}{\\Delta x_{max}^{2}}\\right)}%&#10;{\\sum_{x_{i}\\in\\mathcal{C}_{j}}\\left(1-\\frac{||x_{i}^{d}-\\mu_{j}^{d}||^{2}}{%&#10;\\Delta x_{max}^{2}}\\right)}\" display=\"block\"><mrow><mrow><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup><mo>=</mo><mpadded width=\"+2.8pt\"><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mfrac></mpadded></mrow><mo rspace=\"8.1pt\">;</mo><mrow><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n\nWhen the tuning factor for a certain component is set at zero, the respective component does not contribute to the temporal-aware measure.\nWhen the tuning factor is set to one, no penalty is inflicted to the contribution of the respective component to the measure.\nIt is immediate that equation~(\\ref{eq:temp-distance}) is a special case of equation~(\\ref{eq:tuned-temp-distance}), with $\\gamma_d = 1$ and $\\gamma_t = 1$ (no weights).\n\n\\paragraph{Setting the weights $\\gamma_d$ and $\\gamma_t$}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{alpha-param}\n\t\\caption{Multidimensional component, temporal component and temporal-aware dissimilarity measure function of $\\alpha$}\n\t\\label{fig:alpha-parameter}\n\t\\vspace{-0.1in}\n\\end{figure}\n\n\n\\begin{figure}[!t]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=-1}\n\t\t\\label{subfig:alpha--1}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=-0,5}\n\t\t\\label{subfig:alpha--0.5}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=0,5}\n\t\t\\label{subfig:alpha-0.5}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=1}\n\t\t\\label{subfig:alpha-1}\n\t}\n\n\t\\caption{Color map of the temporal-aware dissimilarity measure for $\\alpha=-1$ (a), $\\alpha=-0.5$ (b), $\\alpha=0.5$ (c) and $\\alpha=1$ (d) .}\n\t\\label{fig:colormap-multiple-alpha}\n\t\\vspace{-0.2in}\n\\end{figure}\n\n$\\gamma_d$ and $\\gamma_t$ are not independent one from another, their values are set using a unique parameter $\\alpha$.\n\n", "itemtype": "equation", "pos": 31652, "prevtext": "\n\nJust like the centroid update phase in {\\mbox{K-Means}}{}, the new centroids in {\\mbox{TDCK-Means}}{} are also averages over the observations.\nUnlike {\\mbox{K-Means}}{}, the averages are weighted for each component, using the distance from the other.\nFor example, each observation contributes to the multidimensional description of the new centroid, proportional with its temporal centrality in the cluster.\nObservations that are more distant in time (from the centroid) contribute less to the multidimensional description than the ones being closer in time.\nA similar logic applies to the temporal component.\nThe consequence is that the new clusters are coherent both in the multidimensional space and in the temporal one.\n\n\\paragraph*{Algorithm's complexity} Equation~(\\ref{eq:objective-function-complete}) shows that {\\mbox{TDCK-Means}}{}' complexity is $\\mathcal{O}(n^{2}m)$, due to the penalty term.\nStill, the equation can be rewritten, so that only observations belonging to the same entity are tested.\nIf $p$ is the number of entities and $q$ is the maximum number of observations associated with each entity, then $n = p \\times q$.\nThe complexity of {\\mbox{TDCK-Means}}{} is $\\mathcal{O}(pq^{2}m)$, which is well adapted to Social Science and Humanities datasets, where often a large number of individuals is studied over a relatively short period of time ($p > q$).\n\n\\subsection{Fine-tuning the ratio between components}\n\\label{subsec:measure-tuning-alpha}\n\nThe temporal-aware dissimilarity measure, as presented in Equation~(\\ref{eq:temp-distance}), gives equal importance to both the multidimensional component and the temporal component.\nThis might pose problems when the data are not uniformly distributed both in the multidimensional descriptive space and in the temporal space.\nIf the medium standard deviation reported to the medium distance between pairs of observations is greater in one space than in the other, giving equal weight to the components can lead to important bias in the clustering process.\n\\textit{E.g.} observations that are very uniformly distributed in the temporal space (same number of observations for each timestamp) and, at the same time, rather compactly distributed in the description space.\nIn this case, in average, the temporal component weight more in the dissimilarity measure than the multidimensional component.\nConsequently, the clustering is biased towards the temporal cohesion of clusters.\nSimilarly, in some applications, it is desirable to privilege one component over the other.\n\\textit{E.g.} on a large enough scale, user roles in social networks have a temporal component (new types of roles might appear over the years).\nBut in a limited time span, it is perfectly acceptable that the roles can coexist simultaneously.\nTherefore, the temporal component should have only a mild impact on the overall measure.\n\nWe adjust the ratio between the two components by using two tuning factors $\\gamma_d$ and $\\gamma_t$.\n$\\gamma_d$ weights the multidimensional component of the temporal-aware dissimilarity measure, whereas $\\gamma_t$ weights the temporal component.\nEquation~(\\ref{eq:temp-distance}) can be rewritten as:\n\n", "index": 19, "text": "\\begin{equation} \\label{eq:tuned-temp-distance}\n\t|| x_i - x_j||_{TA} = 1 - \\left(1 - \\gamma_d \\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^2}\\right)\\left(1 - \\gamma_t \\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}}\\right) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"||x_{i}-x_{j}||_{TA}=1-\\left(1-\\gamma_{d}\\frac{||x_{i}^{d}-x_{j}^{d}||^{2}}{%&#10;\\Delta x_{max}^{2}}\\right)\\left(1-\\gamma_{t}\\frac{||x_{i}^{t}-x_{j}^{t}||^{2}}%&#10;{\\Delta t_{max}^{2}}\\right)\" display=\"block\"><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>A</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>d</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n$\\alpha$ acts as a slider, taking values from $-1$ to $1$.\nFigure~\\ref{fig:alpha-parameter} shows the evolution $\\gamma_d$ and $\\gamma_t$ with $\\alpha$.\nAlso, Figure~\\ref{fig:colormap-multiple-alpha} shows the color map of the temporal-aware dissimilarity measure for multiple values of $\\alpha$.\n\nWhen $\\alpha=-1$, then $\\gamma_d=0$ and $\\gamma_t=1$.\nThe multidimensional component is eliminated and only the time difference between the two observations is considered.\nThe temporal-aware measure becomes a normalized time difference ($ || x_i - x_j ||_{TA} = \\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}} $).\nThe color map in Figure~\\ref{subfig:alpha--1} ($\\alpha=-1$) shows that the values of the dissimilarity measure is independent of the multidimensional component.\n\nAs the value of $\\alpha$ increases, the weight of the descriptive component increases as well.\nIn Figure~\\ref{subfig:alpha--0.5} ($\\alpha=-0.5$), the multidimensional component has a limited impact on the overall measure.\nWhen $\\alpha=0$, then $\\gamma_d=1$ and $\\gamma_t=1$, both components have equal importance, as proposed initially in Equation~(\\ref{eq:temp-distance}).\nIn Figure~\\ref{subfig:alpha-0.5} ($\\alpha=0.5$), the color map shows that the multidimensional component has a larger impact then the temporal component.\nLarge values of the temporal component have only moderate influence over the measure.\nWhen $\\alpha=1$ (color map in Figure~\\ref{subfig:alpha-1}), then $\\gamma_d=1$ and $\\gamma_t=0$, the temporal dimension is eliminated and the measure becomes a normalized Euclidean distance ($ || x_i - x_j ||_{TA} = \\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^{2}} $).\n\nSince the temporal-aware dissimilarity measure is used in the objective function in Equation~(\\ref{eq:objective-function-complete}), the later changes accordingly to integrate the tuning factors.\n$\\gamma_d$ and $\\gamma_t$ behave as constants in the derivation formulas in Equation~(\\ref{eq:derivatives}).\nAs a result, the centroid update formulas in Equation~(\\ref{eq:centroid-update}) are rewritten as:\n\n", "itemtype": "equation", "pos": 33450, "prevtext": "\n\nWhen the tuning factor for a certain component is set at zero, the respective component does not contribute to the temporal-aware measure.\nWhen the tuning factor is set to one, no penalty is inflicted to the contribution of the respective component to the measure.\nIt is immediate that equation~(\\ref{eq:temp-distance}) is a special case of equation~(\\ref{eq:tuned-temp-distance}), with $\\gamma_d = 1$ and $\\gamma_t = 1$ (no weights).\n\n\\paragraph{Setting the weights $\\gamma_d$ and $\\gamma_t$}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{alpha-param}\n\t\\caption{Multidimensional component, temporal component and temporal-aware dissimilarity measure function of $\\alpha$}\n\t\\label{fig:alpha-parameter}\n\t\\vspace{-0.1in}\n\\end{figure}\n\n\n\\begin{figure}[!t]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=-1}\n\t\t\\label{subfig:alpha--1}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=-0,5}\n\t\t\\label{subfig:alpha--0.5}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=0,5}\n\t\t\\label{subfig:alpha-0.5}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{colormap-Alpha=1}\n\t\t\\label{subfig:alpha-1}\n\t}\n\n\t\\caption{Color map of the temporal-aware dissimilarity measure for $\\alpha=-1$ (a), $\\alpha=-0.5$ (b), $\\alpha=0.5$ (c) and $\\alpha=1$ (d) .}\n\t\\label{fig:colormap-multiple-alpha}\n\t\\vspace{-0.2in}\n\\end{figure}\n\n$\\gamma_d$ and $\\gamma_t$ are not independent one from another, their values are set using a unique parameter $\\alpha$.\n\n", "index": 21, "text": "\\begin{equation} \\label{eq:alpha-fine-tune}\n    \\gamma_d= \n\\begin{cases}\n    1 + \\alpha ,& \\text{if } \\alpha \\leq 0 \\\\\n    1, \t\t\t& \\text{if } \\alpha > 0\n\\end{cases} \n\t\\;\\;;\\;\\;\n\t\\gamma_t= \n\\begin{cases}\n    1 ,\t\t   & \\text{if } \\alpha \\leq 0 \\\\\n    1 - \\alpha,& \\text{if } \\alpha > 0\n\\end{cases} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\gamma_{d}=\\begin{cases}1+\\alpha,&amp;\\text{if }\\alpha\\leq 0\\\\&#10;1,&amp;\\text{if }\\alpha&gt;0\\end{cases}\\;\\;;\\;\\;\\gamma_{t}=\\begin{cases}1,&amp;\\text{if }%&#10;\\alpha\\leq 0\\\\&#10;1-\\alpha,&amp;\\text{if }\\alpha&gt;0\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>\u03b3</mi><mi>d</mi></msub><mo>=</mo><mpadded width=\"+5.6pt\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>+</mo><mi>\u03b1</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>\u2264</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></mrow></mpadded></mrow><mo rspace=\"8.1pt\">;</mo><mrow><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>\u2264</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nThe tuning between the multidimensional and temporal component in the temporal-aware dissimilarity measure propagates into the centroid update formula of {\\mbox{TDCK-Means}}{}.\nWe study, in Section~\\ref{subsec:parameters-alpha}, the influence of the tuning parameter and we propose an heuristic to set its value.\n\n\\subsection{Inferring a graph structure for the temporal clusters}\n\\label{subsec:graph-structure}\n\nIn Figure~\\ref{subfig:cluster-structuring}, when discussing the desired output of our system, we \npresented the obtained temporal clusters under the form of a graph.\nThe nodes represent the evolution phases and an edge between two nodes $\\mu_p$ and $\\mu_q$ indicates that the transition $\\mu_p \\longrightarrow \\mu_q$ is part of a typical evolution.\nSince each temporal cluster is interpreted as an evolution phase, the visualization under the form of a graph allows quick understanding of how the different phases are organized both (i) in time (phases on the left side of Figure~\\ref{subfig:cluster-structuring} have a lower timestamp than those on the right side) and (ii) considering the transitions of the entities through phases.\nIntuitively, the strength of the connection between two phases is proportional with the number of entities which present transitions between the two given phases.\n\nWe consider that an entity $\\phi_l$ presents a transition between $\\mu_p$ and $\\mu_q$ ($\\mu_p \\xrightarrow{\\phi_l} \\mu_q$) if and only if two consecutive observations exist, associated with the given entity, where the first observation (ordered by their timestamp) is clustered under $\\mu_p$ and the second one is clustered under $\\mu_q$.\nFormally:\n\n", "itemtype": "equation", "pos": 35820, "prevtext": "\n$\\alpha$ acts as a slider, taking values from $-1$ to $1$.\nFigure~\\ref{fig:alpha-parameter} shows the evolution $\\gamma_d$ and $\\gamma_t$ with $\\alpha$.\nAlso, Figure~\\ref{fig:colormap-multiple-alpha} shows the color map of the temporal-aware dissimilarity measure for multiple values of $\\alpha$.\n\nWhen $\\alpha=-1$, then $\\gamma_d=0$ and $\\gamma_t=1$.\nThe multidimensional component is eliminated and only the time difference between the two observations is considered.\nThe temporal-aware measure becomes a normalized time difference ($ || x_i - x_j ||_{TA} = \\frac{||x_i^t - x_j^t||^2}{\\Delta t_{max}^{2}} $).\nThe color map in Figure~\\ref{subfig:alpha--1} ($\\alpha=-1$) shows that the values of the dissimilarity measure is independent of the multidimensional component.\n\nAs the value of $\\alpha$ increases, the weight of the descriptive component increases as well.\nIn Figure~\\ref{subfig:alpha--0.5} ($\\alpha=-0.5$), the multidimensional component has a limited impact on the overall measure.\nWhen $\\alpha=0$, then $\\gamma_d=1$ and $\\gamma_t=1$, both components have equal importance, as proposed initially in Equation~(\\ref{eq:temp-distance}).\nIn Figure~\\ref{subfig:alpha-0.5} ($\\alpha=0.5$), the color map shows that the multidimensional component has a larger impact then the temporal component.\nLarge values of the temporal component have only moderate influence over the measure.\nWhen $\\alpha=1$ (color map in Figure~\\ref{subfig:alpha-1}), then $\\gamma_d=1$ and $\\gamma_t=0$, the temporal dimension is eliminated and the measure becomes a normalized Euclidean distance ($ || x_i - x_j ||_{TA} = \\frac{||x_i^d - x_j^d||^2}{\\Delta x_{max}^{2}} $).\n\nSince the temporal-aware dissimilarity measure is used in the objective function in Equation~(\\ref{eq:objective-function-complete}), the later changes accordingly to integrate the tuning factors.\n$\\gamma_d$ and $\\gamma_t$ behave as constants in the derivation formulas in Equation~(\\ref{eq:derivatives}).\nAs a result, the centroid update formulas in Equation~(\\ref{eq:centroid-update}) are rewritten as:\n\n", "index": 23, "text": "\\begin{equation*}\n\t\\mu_j^d = \\frac{\\sum_{x_i \\in \\mathcal{C}_j} x_i^d \\times \\left(1 - \\gamma_t \\frac{||x_i^t - \\mu_j^t||^2}{\\Delta t_{max}^{2}} \\right)}{\\sum_{x_i \\in \\mathcal{C}_j} \\left(1 - \\gamma_t \\frac{||x_i^t - \\mu_j^t||^2}{\\Delta t_{max}^{2}} \\right)} \\;\\; ; \\; \\;\\mu_j^t = \\frac{\\sum_{x_i \\in \\mathcal{C}_j} x_i^t \\times \\left(1 - \\gamma_d \\frac{||x_i^d - \\mu_j^d||^2}{\\Delta x_{max}^{2}} \\right)}{\\sum_{x_i \\in \\mathcal{C}_j} \\left(1 - \\gamma_d \\frac{||x_i^d - \\mu_j^d||^2}{\\Delta x_{max}^{2}} \\right)}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\mu_{j}^{d}=\\frac{\\sum_{x_{i}\\in\\mathcal{C}_{j}}x_{i}^{d}\\times\\left(1-\\gamma_%&#10;{t}\\frac{||x_{i}^{t}-\\mu_{j}^{t}||^{2}}{\\Delta t_{max}^{2}}\\right)}{\\sum_{x_{i%&#10;}\\in\\mathcal{C}_{j}}\\left(1-\\gamma_{t}\\frac{||x_{i}^{t}-\\mu_{j}^{t}||^{2}}{%&#10;\\Delta t_{max}^{2}}\\right)}\\;\\;;\\;\\;\\mu_{j}^{t}=\\frac{\\sum_{x_{i}\\in\\mathcal{C%&#10;}_{j}}x_{i}^{t}\\times\\left(1-\\gamma_{d}\\frac{||x_{i}^{d}-\\mu_{j}^{d}||^{2}}{%&#10;\\Delta x_{max}^{2}}\\right)}{\\sum_{x_{i}\\in\\mathcal{C}_{j}}\\left(1-\\gamma_{d}%&#10;\\frac{||x_{i}^{d}-\\mu_{j}^{d}||^{2}}{\\Delta x_{max}^{2}}\\right)}\" display=\"block\"><mrow><mrow><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup><mo>=</mo><mpadded width=\"+5.6pt\"><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>t</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow></mfrac></mpadded></mrow><mo rspace=\"8.1pt\">;</mo><mrow><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup><mo>=</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>d</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></msub><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>d</mi></msub><mo>\u2062</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>2</mn></msup><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n\\vspace{-0.2in}\n\n", "itemtype": "equation", "pos": 38010, "prevtext": "\nThe tuning between the multidimensional and temporal component in the temporal-aware dissimilarity measure propagates into the centroid update formula of {\\mbox{TDCK-Means}}{}.\nWe study, in Section~\\ref{subsec:parameters-alpha}, the influence of the tuning parameter and we propose an heuristic to set its value.\n\n\\subsection{Inferring a graph structure for the temporal clusters}\n\\label{subsec:graph-structure}\n\nIn Figure~\\ref{subfig:cluster-structuring}, when discussing the desired output of our system, we \npresented the obtained temporal clusters under the form of a graph.\nThe nodes represent the evolution phases and an edge between two nodes $\\mu_p$ and $\\mu_q$ indicates that the transition $\\mu_p \\longrightarrow \\mu_q$ is part of a typical evolution.\nSince each temporal cluster is interpreted as an evolution phase, the visualization under the form of a graph allows quick understanding of how the different phases are organized both (i) in time (phases on the left side of Figure~\\ref{subfig:cluster-structuring} have a lower timestamp than those on the right side) and (ii) considering the transitions of the entities through phases.\nIntuitively, the strength of the connection between two phases is proportional with the number of entities which present transitions between the two given phases.\n\nWe consider that an entity $\\phi_l$ presents a transition between $\\mu_p$ and $\\mu_q$ ($\\mu_p \\xrightarrow{\\phi_l} \\mu_q$) if and only if two consecutive observations exist, associated with the given entity, where the first observation (ordered by their timestamp) is clustered under $\\mu_p$ and the second one is clustered under $\\mu_q$.\nFormally:\n\n", "index": 25, "text": "\\begin{align*} \n\tx_a, x_b \\in \\mathcal{D} \\text{ consecutive } \\Leftrightarrow \\text{ } & x_a^{\\phi} = x_b^{\\phi} = \\phi_l, x_a^{t} \\leq x_b^{t} \\text{ and } \\\\\n\t& \\nexists x_c \\in \\mathcal{D}, x_c^{\\phi} = \\phi_l \\text{ so that } x_a^{t} \\leq x_c^t \\leq x_b^{t}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{a},x_{b}\\in\\mathcal{D}\\text{ consecutive }\\Leftrightarrow%&#10;\\text{ }\" display=\"inline\"><mrow><mrow><mrow><msub><mi>x</mi><mi>a</mi></msub><mo>,</mo><msub><mi>x</mi><mi>b</mi></msub></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>\u2062</mo><mtext>\u00a0consecutive\u00a0</mtext></mrow></mrow><mo>\u21d4</mo><mrow/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{a}^{\\phi}=x_{b}^{\\phi}=\\phi_{l},x_{a}^{t}\\leq x_{b}^{t}\\text{%&#10; and }\" display=\"inline\"><mrow><mrow><msubsup><mi>x</mi><mi>a</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>b</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msub><mi>\u03d5</mi><mi>l</mi></msub></mrow><mo>,</mo><mrow><msubsup><mi>x</mi><mi>a</mi><mi>t</mi></msubsup><mo>\u2264</mo><mrow><msubsup><mi>x</mi><mi>b</mi><mi>t</mi></msubsup><mo>\u2062</mo><mtext>\u00a0and</mtext></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\nexists x_{c}\\in\\mathcal{D},x_{c}^{\\phi}=\\phi_{l}\\text{ so that %&#10;}x_{a}^{t}\\leq x_{c}^{t}\\leq x_{b}^{t}\" display=\"inline\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2204</mi><mo>\u2062</mo><msub><mi>x</mi><mi>c</mi></msub></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo>,</mo><mrow><msubsup><mi>x</mi><mi>c</mi><mi>\u03d5</mi></msubsup><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>\u2062</mo><mtext>\u00a0so that\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>a</mi><mi>t</mi></msubsup></mrow><mo>\u2264</mo><msubsup><mi>x</mi><mi>c</mi><mi>t</mi></msubsup><mo>\u2264</mo><msubsup><mi>x</mi><mi>b</mi><mi>t</mi></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nFurthermore, we define the intersection similarity measure between two phases, which is based on the normalized number of entities that present a transition between the two phases.\nFormally, we define the $inter_{\\phi} (\\mu_p, \\mu_q)$:\n\\vspace{-0.1in}\n\n", "itemtype": "equation", "pos": 38302, "prevtext": "\n\\vspace{-0.2in}\n\n", "index": 27, "text": "\\begin{align}\t\n\t\\mu_p \\xrightarrow{\\phi_l} \\mu_q \\Leftrightarrow\n\t\\exists x_a, x_b \\in \\mathcal{D} \\text{ so that }\n\t\\begin{cases}\n    \tx_a^{\\phi} = x_b^{\\phi} = \\phi_l \\text{ and}\\\\\n    \tx_a, x_b \\text{ consecutive and } \\\\\n    \tx_a \\in \\mathcal{C}_p , x_b \\in \\mathcal{C}_q\n\t\\end{cases} \\label{eq:transition-definition}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu_{p}\\xrightarrow{\\phi_{l}}\\mu_{q}\\Leftrightarrow\\exists x_{a},%&#10;x_{b}\\in\\mathcal{D}\\text{ so that }\\begin{cases}x_{a}^{\\phi}=x_{b}^{\\phi}=\\phi%&#10;_{l}\\text{ and}\\\\&#10;x_{a},x_{b}\\text{ consecutive and }\\\\&#10;x_{a}\\in\\mathcal{C}_{p},x_{b}\\in\\mathcal{C}_{q}\\end{cases}\" display=\"inline\"><mrow><mrow><msub><mi>\u03bc</mi><mi>p</mi></msub><mover accent=\"true\"><mo>\u2192</mo><msub><mi>\u03d5</mi><mi>l</mi></msub></mover><msub><mi>\u03bc</mi><mi>q</mi></msub></mrow><mo>\u21d4</mo><mrow><mrow><mrow><mo>\u2203</mo><msub><mi>x</mi><mi>a</mi></msub></mrow><mo>,</mo><msub><mi>x</mi><mi>b</mi></msub></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>\u2062</mo><mtext>\u00a0so that\u00a0</mtext><mo>\u2062</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msubsup><mi>x</mi><mi>a</mi><mi>\u03d5</mi></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>b</mi><mi>\u03d5</mi></msubsup><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>\u2062</mo><mtext>\u00a0and</mtext></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>x</mi><mi>a</mi></msub><mo>,</mo><mrow><msub><mi>x</mi><mi>b</mi></msub><mo>\u2062</mo><mtext>\u00a0consecutive and\u00a0</mtext></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>x</mi><mi>a</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>p</mi></msub></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>b</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>q</mi></msub></mrow></mrow></mtd><mtd/></mtr></mtable></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $inter_{\\phi} (\\mu_p, \\mu_q) \\in [0,1]$ and needs to be maximized.\n\nWe infer a graph structure between the temporal clusters, by constructing an adjacency matrix using the intersection similarity measure.\nThe graph construction is performed \\textit{a posteriori}, after the temporal clusters are calculated.\nWe define the adjacency matrix $ A = (a_{p,q})$, where $ a_{p,q} = inter_{\\phi} (\\mu_p, \\mu_q)$.\nBy replacing equations~\\ref{eq:transition-definition} and~\\ref{eq:intersection-measure} into this definition, we obtain:\n\n", "itemtype": "equation", "pos": 38888, "prevtext": "\nFurthermore, we define the intersection similarity measure between two phases, which is based on the normalized number of entities that present a transition between the two phases.\nFormally, we define the $inter_{\\phi} (\\mu_p, \\mu_q)$:\n\\vspace{-0.1in}\n\n", "index": 29, "text": "\\begin{equation} \\label{eq:intersection-measure}\n\tinter_{\\phi} (\\mu_p, \\mu_q) = \\frac{|\\{ \\phi_l \\in \\Phi | \\mu_p \\xrightarrow{\\phi_l} \\mu_q \\}|}{|\\Phi|}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"inter_{\\phi}(\\mu_{p},\\mu_{q})=\\frac{|\\{\\phi_{l}\\in\\Phi|\\mu_{p}\\xrightarrow{%&#10;\\phi_{l}}\\mu_{q}\\}|}{|\\Phi|}\" display=\"block\"><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msub><mi>r</mi><mi>\u03d5</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>p</mi></msub><mo>,</mo><msub><mi>\u03bc</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>\u03bc</mi><mi>p</mi></msub><mover accent=\"true\"><mo>\u2192</mo><msub><mi>\u03d5</mi><mi>l</mi></msub></mover><msub><mi>\u03bc</mi><mi>q</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a6</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nWe construct $A^{*}$, a binary adjacency matrix, by using a threshold $\\gamma$:\n\\vspace{-0.1in}\n\n", "itemtype": "equation", "pos": 39589, "prevtext": "\nwhere $inter_{\\phi} (\\mu_p, \\mu_q) \\in [0,1]$ and needs to be maximized.\n\nWe infer a graph structure between the temporal clusters, by constructing an adjacency matrix using the intersection similarity measure.\nThe graph construction is performed \\textit{a posteriori}, after the temporal clusters are calculated.\nWe define the adjacency matrix $ A = (a_{p,q})$, where $ a_{p,q} = inter_{\\phi} (\\mu_p, \\mu_q)$.\nBy replacing equations~\\ref{eq:transition-definition} and~\\ref{eq:intersection-measure} into this definition, we obtain:\n\n", "index": 31, "text": "\\begin{equation} \\label{eq:adjacency-matrix}\n\ta_{p,q} = \\frac{|\\{ 1 \\leq l \\leq |\\Phi| | \\exists x_a, x_b \\in (\\mathcal{C}_p,\\mathcal{C}_q)  \\text{ so that } x_a^{\\phi} = x_b^{\\phi} = \\phi_l \\text{ and } x_a^{t}, x_b^{t} \\text{ consecutive}\\}|}{|\\Phi|}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"a_{p,q}=\\frac{|\\{1\\leq l\\leq|\\Phi||\\exists x_{a},x_{b}\\in(\\mathcal{C}_{p},%&#10;\\mathcal{C}_{q})\\text{ so that }x_{a}^{\\phi}=x_{b}^{\\phi}=\\phi_{l}\\text{ and }%&#10;x_{a}^{t},x_{b}^{t}\\text{ consecutive}\\}|}{|\\Phi|}\" display=\"block\"><mrow><msub><mi>a</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>l</mi><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a6</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mrow><mrow><mo>\u2203</mo><msub><mi>x</mi><mi>a</mi></msub></mrow><mo>,</mo><msub><mi>x</mi><mi>b</mi></msub></mrow><mo>\u2208</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>p</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0so that\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>a</mi><mi>\u03d5</mi></msubsup></mrow><mo>=</mo><msubsup><mi>x</mi><mi>b</mi><mi>\u03d5</mi></msubsup><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>l</mi></msub><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msubsup><mi>x</mi><mi>a</mi><mi>t</mi></msubsup></mrow></mrow><mo>,</mo><mrow><msubsup><mi>x</mi><mi>b</mi><mi>t</mi></msubsup><mo>\u2062</mo><mtext>\u00a0consecutive</mtext></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a6</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nThe filtering parameter $\\gamma$ is dataset dependent and automatically setting its value is part of the perspectives of our work.\n\n\\section{Experiments}\n\\label{sec:xp}\n\n\\subsection{Dataset}\n\nExperimentations with Time-Driven Constrained {\\mbox{K-Means}}{} are performed on a dataset issued from political sciences: \\textit{Comparative Political Data Set I}~\\cite{ARM11}.\nIt is a collection of political and institutional data, which consists of annual data for 23 democratic countries for the period from 1960 to 2009.\nThe dataset contains 207 political, demographic, social and economic variables.\n\nThe dataset was cleaned by removing redundant variables (\\textit{e.g.} country identifier and postal code) and the corpus was preprocessed by removing entity bias from the data.\nFor example, it is difficult to compare, on the raw data, the evolution of population between populous country and one with fewer inhabitants, since any evolution in the 50 years timespan of the dataset will be rendered meaningless by the initial difference.\nInspired from panel data econometrics~\\cite{DOR89}, we remove the entity-specific, time-invariant effects, since we assume them to be fixed over time.\nWe subtract from each value the average over each attribute and over each entity.\nWe retain the time-variant component, which is in turn normalized, in order to avoid giving too much importance to certain variables.\nThe obtained dataset is under the form of triples $(country, year, description)$.\n\n\\subsection{Qualitative evaluation}\n\nWhen studying the evolution of countries over the years, it is quite obvious for the human reader why the evolutions of the eastern European countries resemble each other for most of the second half of the twentieth century.\nThe reader would create a group entitled ``Communism'', extending from right after the Second World War until roughly 1990, for defining the typical evolution of communist countries.\nOne would expect that, based on a political dataset, the algorithms would succeed in identifying such typical evolutions and segment the time series of each of these countries accordingly.\nFigure~\\ref{fig:example-run} shows the typical evolution patterns constructed by {\\mbox{TDCK-Means}}{} (with $\\beta = 0.003$ and $\\delta = 3$, obtained as shows in Section~\\ref{subsec:parameters-beta-delta}), when asked for 8 clusters.\nThe distribution over time of observations in each cluster is given in Figure~\\ref{subfig:example-obs-clus-time}.\nAll constructed clusters are fairly compact in time and have limited temporal extents.\nThey can be divided into two temporal groups.\nIn the first one, clusters $\\mu_1$ to $\\mu_5$ consistently overlap.\nSame for clusters $\\mu_6$ to $\\mu_8$, in the second group.\nThis indicates that the evolution of each country passes by at least one cluster from each group.\nThe turning point between the two groups is around 1990.\nFigure~\\ref{subfig:example-clus-time} shows how many countries belong in a certain cluster for each year.\nClusters $\\mu_5$ and $\\mu_6$ contain most of the observations, suggesting the general typical evolution.\n\n\\begin{figure}[!t]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[height=0.169\\textheight]{observations-clusters-vs-time}\n\t\t\\label{subfig:example-obs-clus-time}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[height=0.169\\textheight]{clusters-vs-time}\n\t\t\\label{subfig:example-clus-time}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[height=0.169\\textheight]{entity-segmentation}\n\t\t\\label{subfig:example-entity-seg}\n\t}\n\n\t\\caption{Typical evolution patterns constructed by {\\mbox{TDCK-Means}}{} on \\textit{Comparative Political Data Set I} with 8 clusters. The distribution over time of observations in each cluster (a), how many entities belong in a certain clusters for each year (b) and the segmentation of entities over clusters (c).}\n\t\\label{fig:example-run}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe meaning of each constructed cluster starts to unravel only when studying the segmentation of countries over clusters, in Figure~\\ref{subfig:example-entity-seg}.\nFor example, cluster $\\mu_2$ regroups the observations belonging to Spain, Portugal and Greece from 1960 up until around 1975.\nHistorically, this coincides with the non-democratic regimes in those countries (Franco's dictatorship in Spain, the ``Regime of the Colonels'' in Greece).\nLikewise, cluster $\\mu_4$ contains observations of countries like Denmark, Finland, Iceland, Norway, Sweden and New Zealand.\nThis cluster can be interpreted as the ``Swedish Social and Economical Model'' of the Nordic countries, to which the algorithm added, interestingly enough, New Zealand.\nIn the second period, cluster $\\mu_8$ regroups observations of Greece, Ireland, Spain, Portugal and Belgium, the countries which seemed the most fragile in the aftermaths of the economical crises of 2008.\n\n\\begin{figure}[htb]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.99\\textwidth]{output-graph-o1-t04}\n\t\t\\label{subfig:graph-complete}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.9\\textwidth]{output-graph-o1-t20}\n\t\t\\label{subfig:graph-filtered}\n\t}\n\n\t\\caption{Structuring the temporal clusters as a graph, without filtering ($\\gamma = 0$) (a) and filtered with $\\gamma = 0.2$ (b).}\n\t\\label{fig:graph-structure}\n\t\\vspace{-0.1in}\n\\end{figure}\n\nSimilar conclusions can be drawn from the constructed graph structure, presented in Figure~\\ref{fig:graph-structure}.\nEach temporal cluster is represented as a node and the scores indicated on each edge are calculated as shown in Equation~\\ref{eq:adjacency-matrix}.\nThe graph containing all transitions are represented in Figure~\\ref{subfig:graph-complete} (no filtering, $\\gamma = 0$).\nWe obtain a graph containing more general evolutions, by filtering with the threshold $\\gamma = 0.2$.\nSome ``rare'' phases completely disappear (\\textit{i.e.}, phases $\\mu_2$ and $\\mu_3$) together with some of the arcs.\nWe recognize in the resulted graph, shown in Figure~\\ref{subfig:graph-filtered}, some of the evolutions identified earlier.\nThe evolution $\\mu_4 \\longrightarrow \\mu_5 \\longrightarrow \\mu_6$ corresponds to the ``Swedish Social and Economical Model'', whereas the evolution $\\mu_5 \\longrightarrow \\mu_6 \\longrightarrow \\mu_8$ identifies the fragile European economies of the 2008 economical crises.\nFrom the filtered evolution graph, another typical evolution emerges: $\\mu_1 \\longrightarrow \\mu_5 \\longrightarrow \\mu_7$, which is present for countries as USA, Germany, Italy and France.\nWe interpret this evolution as that of countries with stable social and economical environments.\n\n\\subsection{Evaluation measures}\n\\label{subsec:xp-evaluation-measures}\n\nSince the dataset contains no labels to report to as ground truth, we use the classical Information Theory measures in order to numerically evaluate the proposed algorithms.\nWe evaluate separately each of the three goals that we propose in Section~\\ref{sec:introduction}.\n\n\\textbf{Create clusters that are coherent in the multidimensional description space.}\nIt is desirable that observations that have similar multidimensional descriptions to be partitioned under the same cluster.\nThe similarity in the description space is measured by the multidimensional component of the temporal-aware dissimilarity measure.\nThis goal is pursued by all classical clustering algorithms (like {\\mbox{K-Means}}{}) and any traditional clustering evaluation measure~\\cite{HAL01} can be used to asses it.\nWe choose the mean cluster variance, which is traditionally used in clustering to quantify the dispersion of observations in clusters.\nThe \\textit{MDvar} measure is defined as:\n\\vspace*{-0.22cm}\n\n", "itemtype": "equation", "pos": 39953, "prevtext": "\nWe construct $A^{*}$, a binary adjacency matrix, by using a threshold $\\gamma$:\n\\vspace{-0.1in}\n\n", "index": 33, "text": "\\begin{equation*}\n\tA^{*} = (a^{*}_{p,q}) \\text{ with } \n\ta^{*}_{p,q} = \n\t\\begin{cases}\n    \t0 &, \\text{ if } a_{p,q} < \\gamma \\\\\n    \t1 &, \\text{ if } a_{p,q} \\geq \\gamma\n\t\\end{cases}  \n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"A^{*}=(a^{*}_{p,q})\\text{ with }a^{*}_{p,q}=\\begin{cases}0&amp;,\\text{ if }a_{p,q}%&#10;&lt;\\gamma\\\\&#10;1&amp;,\\text{ if }a_{p,q}\\geq\\gamma\\end{cases}\" display=\"block\"><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>a</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0with\u00a0</mtext><mo>\u2062</mo><msubsup><mi>a</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow><mo>*</mo></msubsup></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mo>,</mo><mtext>\u00a0if\u00a0</mtext><msub><mi>a</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>&lt;</mo><mi>\u03b3</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mo>,</mo><mtext>\u00a0if\u00a0</mtext><msub><mi>a</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>\u2265</mo><mi>\u03b3</mi></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n\n\\textbf{Create temporally coherent clusters, with limited extend in time.}\nThis goal is very similar to the previous one, translated in the temporal space.\nIt is desirable that observations that are assigned to the same cluster to be similar in the temporal space (\\textit{i.e.} to be close in time).\nThe similarity in the temporal space is measured by the temporal component in the temporal-aware dissimilarity measure.\nThe limited time extent of a centroid implies small temporal distances between observations timestamp and the centroid timestamp.\nAs a result, the variance can also be used to measure the dispersion of clusters in the temporal space.\nSimilarly to \\textit{MDvar}, the \\textit{Tvar} measure is defined as:\n\\vspace*{-0.22cm}\n\n", "itemtype": "equation", "pos": 47787, "prevtext": "\nThe filtering parameter $\\gamma$ is dataset dependent and automatically setting its value is part of the perspectives of our work.\n\n\\section{Experiments}\n\\label{sec:xp}\n\n\\subsection{Dataset}\n\nExperimentations with Time-Driven Constrained {\\mbox{K-Means}}{} are performed on a dataset issued from political sciences: \\textit{Comparative Political Data Set I}~\\cite{ARM11}.\nIt is a collection of political and institutional data, which consists of annual data for 23 democratic countries for the period from 1960 to 2009.\nThe dataset contains 207 political, demographic, social and economic variables.\n\nThe dataset was cleaned by removing redundant variables (\\textit{e.g.} country identifier and postal code) and the corpus was preprocessed by removing entity bias from the data.\nFor example, it is difficult to compare, on the raw data, the evolution of population between populous country and one with fewer inhabitants, since any evolution in the 50 years timespan of the dataset will be rendered meaningless by the initial difference.\nInspired from panel data econometrics~\\cite{DOR89}, we remove the entity-specific, time-invariant effects, since we assume them to be fixed over time.\nWe subtract from each value the average over each attribute and over each entity.\nWe retain the time-variant component, which is in turn normalized, in order to avoid giving too much importance to certain variables.\nThe obtained dataset is under the form of triples $(country, year, description)$.\n\n\\subsection{Qualitative evaluation}\n\nWhen studying the evolution of countries over the years, it is quite obvious for the human reader why the evolutions of the eastern European countries resemble each other for most of the second half of the twentieth century.\nThe reader would create a group entitled ``Communism'', extending from right after the Second World War until roughly 1990, for defining the typical evolution of communist countries.\nOne would expect that, based on a political dataset, the algorithms would succeed in identifying such typical evolutions and segment the time series of each of these countries accordingly.\nFigure~\\ref{fig:example-run} shows the typical evolution patterns constructed by {\\mbox{TDCK-Means}}{} (with $\\beta = 0.003$ and $\\delta = 3$, obtained as shows in Section~\\ref{subsec:parameters-beta-delta}), when asked for 8 clusters.\nThe distribution over time of observations in each cluster is given in Figure~\\ref{subfig:example-obs-clus-time}.\nAll constructed clusters are fairly compact in time and have limited temporal extents.\nThey can be divided into two temporal groups.\nIn the first one, clusters $\\mu_1$ to $\\mu_5$ consistently overlap.\nSame for clusters $\\mu_6$ to $\\mu_8$, in the second group.\nThis indicates that the evolution of each country passes by at least one cluster from each group.\nThe turning point between the two groups is around 1990.\nFigure~\\ref{subfig:example-clus-time} shows how many countries belong in a certain cluster for each year.\nClusters $\\mu_5$ and $\\mu_6$ contain most of the observations, suggesting the general typical evolution.\n\n\\begin{figure}[!t]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[height=0.169\\textheight]{observations-clusters-vs-time}\n\t\t\\label{subfig:example-obs-clus-time}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[height=0.169\\textheight]{clusters-vs-time}\n\t\t\\label{subfig:example-clus-time}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[height=0.169\\textheight]{entity-segmentation}\n\t\t\\label{subfig:example-entity-seg}\n\t}\n\n\t\\caption{Typical evolution patterns constructed by {\\mbox{TDCK-Means}}{} on \\textit{Comparative Political Data Set I} with 8 clusters. The distribution over time of observations in each cluster (a), how many entities belong in a certain clusters for each year (b) and the segmentation of entities over clusters (c).}\n\t\\label{fig:example-run}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe meaning of each constructed cluster starts to unravel only when studying the segmentation of countries over clusters, in Figure~\\ref{subfig:example-entity-seg}.\nFor example, cluster $\\mu_2$ regroups the observations belonging to Spain, Portugal and Greece from 1960 up until around 1975.\nHistorically, this coincides with the non-democratic regimes in those countries (Franco's dictatorship in Spain, the ``Regime of the Colonels'' in Greece).\nLikewise, cluster $\\mu_4$ contains observations of countries like Denmark, Finland, Iceland, Norway, Sweden and New Zealand.\nThis cluster can be interpreted as the ``Swedish Social and Economical Model'' of the Nordic countries, to which the algorithm added, interestingly enough, New Zealand.\nIn the second period, cluster $\\mu_8$ regroups observations of Greece, Ireland, Spain, Portugal and Belgium, the countries which seemed the most fragile in the aftermaths of the economical crises of 2008.\n\n\\begin{figure}[htb]\n\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.99\\textwidth]{output-graph-o1-t04}\n\t\t\\label{subfig:graph-complete}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.9\\textwidth]{output-graph-o1-t20}\n\t\t\\label{subfig:graph-filtered}\n\t}\n\n\t\\caption{Structuring the temporal clusters as a graph, without filtering ($\\gamma = 0$) (a) and filtered with $\\gamma = 0.2$ (b).}\n\t\\label{fig:graph-structure}\n\t\\vspace{-0.1in}\n\\end{figure}\n\nSimilar conclusions can be drawn from the constructed graph structure, presented in Figure~\\ref{fig:graph-structure}.\nEach temporal cluster is represented as a node and the scores indicated on each edge are calculated as shown in Equation~\\ref{eq:adjacency-matrix}.\nThe graph containing all transitions are represented in Figure~\\ref{subfig:graph-complete} (no filtering, $\\gamma = 0$).\nWe obtain a graph containing more general evolutions, by filtering with the threshold $\\gamma = 0.2$.\nSome ``rare'' phases completely disappear (\\textit{i.e.}, phases $\\mu_2$ and $\\mu_3$) together with some of the arcs.\nWe recognize in the resulted graph, shown in Figure~\\ref{subfig:graph-filtered}, some of the evolutions identified earlier.\nThe evolution $\\mu_4 \\longrightarrow \\mu_5 \\longrightarrow \\mu_6$ corresponds to the ``Swedish Social and Economical Model'', whereas the evolution $\\mu_5 \\longrightarrow \\mu_6 \\longrightarrow \\mu_8$ identifies the fragile European economies of the 2008 economical crises.\nFrom the filtered evolution graph, another typical evolution emerges: $\\mu_1 \\longrightarrow \\mu_5 \\longrightarrow \\mu_7$, which is present for countries as USA, Germany, Italy and France.\nWe interpret this evolution as that of countries with stable social and economical environments.\n\n\\subsection{Evaluation measures}\n\\label{subsec:xp-evaluation-measures}\n\nSince the dataset contains no labels to report to as ground truth, we use the classical Information Theory measures in order to numerically evaluate the proposed algorithms.\nWe evaluate separately each of the three goals that we propose in Section~\\ref{sec:introduction}.\n\n\\textbf{Create clusters that are coherent in the multidimensional description space.}\nIt is desirable that observations that have similar multidimensional descriptions to be partitioned under the same cluster.\nThe similarity in the description space is measured by the multidimensional component of the temporal-aware dissimilarity measure.\nThis goal is pursued by all classical clustering algorithms (like {\\mbox{K-Means}}{}) and any traditional clustering evaluation measure~\\cite{HAL01} can be used to asses it.\nWe choose the mean cluster variance, which is traditionally used in clustering to quantify the dispersion of observations in clusters.\nThe \\textit{MDvar} measure is defined as:\n\\vspace*{-0.22cm}\n\n", "index": 35, "text": "\\begin{equation*}\n\tMDvar = \\frac{1}{|\\mathcal{X}|} \\times \\sum_{j = 1}^{m} \\sum_{x_i \\in \\mathcal{C}_{j}} ||x_i^d - \\mu_j^d||^2\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"MDvar=\\frac{1}{|\\mathcal{X}|}\\times\\sum_{j=1}^{m}\\sum_{x_{i}\\in\\mathcal{C}_{j}%&#10;}||x_{i}^{d}-\\mu_{j}^{d}||^{2}\" display=\"block\"><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u00d7</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></munder><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>d</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\n\n\\textbf{Segment the temporal series of observations of each entity into a relatively small number of contiguous segments.}\nThe goal is to have successive observations belonging to an entity grouped together, rather that scattered in different clusters.\nThe Shannon entropy can quantify the number of clusters which regroup the observations of an entity, but it is insensible to alternations between two classes (evolutions like $\\mu_1 \\longrightarrow \\mu_2 \\longrightarrow \\mu_1 \\longrightarrow \\mu_2 $).\nWe evaluate using an adapted mean Shannon entropy of clusters over entities, which weights the entropy by a penalty factor depending on the number of continuous segments in the series of each entity.\nThe \\textit{ShaP} measure is calculated as:\n\\vspace*{-0.22cm}\n\n", "itemtype": "equation", "pos": 48675, "prevtext": "\n\n\\textbf{Create temporally coherent clusters, with limited extend in time.}\nThis goal is very similar to the previous one, translated in the temporal space.\nIt is desirable that observations that are assigned to the same cluster to be similar in the temporal space (\\textit{i.e.} to be close in time).\nThe similarity in the temporal space is measured by the temporal component in the temporal-aware dissimilarity measure.\nThe limited time extent of a centroid implies small temporal distances between observations timestamp and the centroid timestamp.\nAs a result, the variance can also be used to measure the dispersion of clusters in the temporal space.\nSimilarly to \\textit{MDvar}, the \\textit{Tvar} measure is defined as:\n\\vspace*{-0.22cm}\n\n", "index": 37, "text": "\\begin{equation*}\n\tTvar = \\frac{1}{|\\mathcal{X}|} \\times \\sum_{j = 1}^{m} \\sum_{x_i \\in \\mathcal{C}_{j}} ||x_i^t - \\mu_j^t||^2\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"Tvar=\\frac{1}{|\\mathcal{X}|}\\times\\sum_{j=1}^{m}\\sum_{x_{i}\\in\\mathcal{C}_{j}}%&#10;||x_{i}^{t}-\\mu_{j}^{t}||^{2}\" display=\"block\"><mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u00d7</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>j</mi></msub></mrow></munder><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>\u03bc</mi><mi>j</mi><mi>t</mi></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $n_{ch}$ is the number of changes in the cluster assignment series of an entity, $n_{min}$ is the minimal required number of changes and $n_{obs}$ is the number of observations for an entity.\nFor example, in Figure~\\ref{fig:good-bad-shap}, if the series of 11 observations of an entity is assigned to two clusters, but it presents 4 changes, the entropy penalty factor will be $1 + \\frac{4 - 1}{11 - 1} = 1.33$.\nThe \\textit{ShaP} score for this segmentation will be $1.23$, compared to a score of $0.94$ of the ``ideal'' segmentation (only two contiguous chunks).\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width=0.7\\textwidth]{good-bad-shap}\n\t\\caption{Examples of a good and a bad segmentation in contiguous chunks and their related \\textit{ShaP} score.}\n\t\\label{fig:good-bad-shap}\n\t\\vspace{-0.18in}\n\\end{figure}\n\nThe ``ideal'' values for \\textit{MDvar}, \\textit{Tvar} and \\textit{ShaP} is zero and, in all of the experiments presented in the following sections, we search to minimize the values of the three measures.\n\n\\subsection{Quantitative evaluation}\n\\label{subsec:quantitative-evaluation}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.31\\textwidth]{MDvar_vs_noclus}\n\t\t\\label{subfig:mdvar-noclus}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.31\\textwidth]{Tvar_vs_noclus}\n\t\t\\label{subfig:tvar-noclus}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.31\\textwidth]{ShaP_vs_noclus}\n\t\t\\label{subfig:shap-noclus}\n\t}\n\n\t\\caption{\\textit{MDvar} (a), \\textit{Tvar} (b) and \\textit{ShaP} (c) values of the considered algorithms when varying the number of clusters.}\n\t\\label{fig:measures-clus}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFor each combination of algorithms and parameters, we execute 10 times and compute only the average and the standard deviation.\nWe vary $m$, the number of clusters, from 2 to 36.\nThe performances of five algorithms are compared from a quantitative point of view:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} } \n\t\\item \\textbf{Simple {\\mbox{K-Means}}{}} - clusters the observations based solely on their resemblance in the multidimensional space;\n\t\n\t\\item \\textbf{Temporal-Driven {\\mbox{K-Means}}{}} - optimizes only the temporal and multidimensional components, without any contiguity constraints;\n\tcombines {\\mbox{K-Means}}{} with the temporal-aware dissimilarity measure defined in Section~\\ref{subsec:proposal-temporal-dissimilarity-measure}.\n\tParameters: $\\alpha=0$ ($\\alpha$ defined in Equation~\\ref{eq:centroid-update}) and $\\beta=0$ ($\\beta$ defined in Equation~\\ref{eq:penalty-function});\n\t\n\t\\item \\textbf{Constrained {\\mbox{K-Means}}{}} - uses only the multidimensional space (and not the temporal component) together with the penalty component, as proposed in Section~\\ref{subsec:proposal-penalty-function}. \n\tParameters: $\\alpha=1$, $\\beta = 0.003$ and $\\delta = 3$ ($\\delta$ defined in Equation~\\ref{eq:penalty-function});\n\t\n\t\\item \\textbf{{\\mbox{TDCK-Means}}{}} - the Temporal-Driven Constrained Clustering algorithm proposed in Section~\\ref{subsec:proposal-tdck-means}. \n\t$\\alpha=0$, $\\beta = 0.003$ and $\\delta = 3$;\n\t\n\t\\item \\textbf{tcK-Means} - the temporal constrained clustering algorithm proposed in~\\cite{LIN06}. \n\tIt uses a threshold penalty function $w(x_i^{t_i},x_j^{t_i}) = \\alpha^* \\mathbbm{1} (|x_i^t - x_j^t| < d^*)$ when observations $ x_i$ and $x_j$ are not assigned to the same cluster. \n\tWe adapt it to the multi-entity case by applying it only to observations belonging to the same entity. \n\tParameters: $\\alpha^* = 2, d^* = 4$.\n   \\end{list}  \nThe $\\alpha^*$ parameter in \\textbf{tcK-Means} should not be mistaken with the $\\alpha$ parameter in \\textbf{{\\mbox{TDCK-Means}}{}}, as they do not have the same meaning.\n\tIn \\textbf{tcK-Means}, $\\alpha^*$ controls the weight of the penalty function, whereas in \\textbf{{\\mbox{TDCK-Means}}{}} $\\alpha$ is the fine-tuning parameter.\n\n\\begin{table}[t]\n\\caption{Mean values for indicators and standard deviations}\n\\label{tab:result-all}\n\n\\small\n\\centering\n\\begin{tabular}{llrr|rr|rr}\n\\toprule\n & \\multicolumn{1}{c}{\\textit{Algorithm}} &  \\multicolumn{2}{c}{\\textit{MDvar}} & \\multicolumn{2}{c}{\\textit{Tvar}} & \\multicolumn{2}{c}{\\textit{ShaP}} \\\\ \\midrule\n \n\\multirow{4}{*}{\\begin{sideways}\\texttt{Scores}\\end{sideways}} \n& \\textbf{Simple {\\mbox{K-Means}}{}} & \\textbf{120.59} & \\textit{2.97} & 48.01 & \\textit{8.87} & 2.15 & \\textit{0.23} \\\\ \n& \\textbf{Temp-Driven {\\mbox{K-Means}}{}} & 122.98 & \\textit{2.85} & \\textbf{19.97} & \\textit{5.39} & 2.58 & \\textit{0.18} \\\\ \n& \\textbf{Constrained {\\mbox{K-Means}}{}} & 132.69 & \\textit{8.07} & 103.15 & \\textit{42.98} & \\textbf{1.24} & \\textit{0.5} \\\\ \n& \\textbf{{\\mbox{TDCK-Means}}{}} & 127.81 & \\textit{3.96} & 27.54 & \\textit{5.81} & 2.06 & \\textit{0.2} \\\\ \n& \\textbf{tcK-Means} & 123,04 & \\textit{3.8} & 62.44 & \\textit{24.16} & 1.79 & \\textit{0.32} \\\\ \n\n\\midrule\n\n\\multirow{3}{*}{\\begin{sideways}\\texttt{\\% Gain}\\end{sideways}} \n& \\textbf{Temp-Driven {\\mbox{K-Means}}{}} & \\multicolumn{2}{c|}{-1.99\\%} & \\multicolumn{2}{c|}{\\textbf{58.40\\%}} & \\multicolumn{2}{c}{-19.63\\%} \\\\ \n& \\textbf{Constrained {\\mbox{K-Means}}{}} & \\multicolumn{2}{c|}{-10.04\\%} & \\multicolumn{2}{c|}{-114.84\\%} & \\multicolumn{2}{c}{\\textbf{42.21\\%}} \\\\ \n& \\textbf{{\\mbox{TDCK-Means}}{}} & \\multicolumn{2}{c|}{-5.99\\%} & \\multicolumn{2}{c|}{42.64\\%} & \\multicolumn{2}{c}{4.19\\%} \\\\ \n& \\textbf{tcK-Means} & \\multicolumn{2}{c|}{-2.03\\%} & \\multicolumn{2}{c|}{-30.05\\%} & \\multicolumn{2}{c}{16.99\\%} \\\\ \n\n\\bottomrule\n\\end{tabular}\n\n\\end{table}\n\t\n\\paragraph{Obtained results.}\nAll the parameters are determined as shown in Section~\\ref{subsec:parameters-beta-delta}.\nTable~\\ref{tab:result-all} shows the average values for the indicators, as well as the average standard deviation (in italic) obtained by each algorithm over all values of $m$.\nThe average standard deviation is only used to give an idea of the order of magnitude of the stability of each algorithm.\nSince Simple {\\mbox{K-Means}}{}, Temporal-Driven {\\mbox{K-Means}}{} and Constrained {\\mbox{K-Means}}{} are designed to optimize mainly one component, it is not surprising that they show the best scores for, respectively, the multidimensional variance, the temporal variance and the entropy (best results in boldface). \n{\\mbox{TDCK-Means}}{} seeks to provide a compromise, obtaining in two out of three cases the second best score.\nIt is noteworthy that the proposed temporal-aware dissimilarity measure used in Temporal-Driven {\\mbox{K-Means}}{} provides the highest stability (the lowest average standard deviation) for all indicators.\nMeanwhile, the constrained algorithms (Constrained {\\mbox{K-Means}}{} and tcK-Means) show high instability, especially on \\textit{Tvar}.\n{\\mbox{TDCK-Means}}{} shows a very good stability.\nThe second part of Table~\\ref{tab:result-all} gives the relative gain of performance of each of the proposed algorithms over Simple {\\mbox{K-Means}}{}.\nIt is noteworthy the effectiveness of the temporal-aware dissimilarity measure proposed in Section~\\ref{subsec:proposal-temporal-dissimilarity-measure}, with a 58\\% gain of Temporal Variance and less than 2\\% loss of multidimensional variance.\nThe proposed dissimilarity measure greatly enhances the temporal cohesion of the resulted clusters, without a significant scattering of observations in the multidimensional space.\nSimilarly, the Constrained KM shows an improvement in the contiguity measure \\textit{ShaP} of 42\\%, while losing 10\\% multidimensional variance.\nBy comparison, tcK-Means shows modest results, improving \\textit{ShaP} by only 17\\% and still showing important losses on both \\textit{Tvar} (-30\\%) and \\textit{MDvar} (-2\\%).\nThis proves that the threshold penalty function proposed in literature has lower performances than our newly proposed contiguity penalty function.\n{\\mbox{TDCK-Means}}{} combines the advantages of the other two algorithms, providing an important gain of 43\\% of temporal variance and increasing the \\textit{ShaP} measure by more than 4\\%.\nNonetheless, it shows a 6\\% loss of \\textit{MDvar}.\n\n\\paragraph{Varying the number of clusters}\nSimilar conclusions can be drawn when varying the number of clusters.\n\\textit{MDvar} (Figure~\\ref{subfig:mdvar-noclus}) decreases, for all algorithms, as the number of cluster increases.\nIt is well known in clustering literature that the intra-cluster variance decreases steadily with the increase of number of clusters.\nAs the number of clusters augments, so does the differences of {\\mbox{TDCK-Means}}{} and Constrained {\\mbox{K-Means}}{}, when compared to the Simple {\\mbox{K-Means}}{} algorithm.\nThis is due to the fact that the constraints do not let too many clusters to be assigned to the same entity, resulting in the convergence towards a local optimum, with a higher value of \\textit{MDvar}.\nAn opposite behavior is shown by the \\textit{ShaP} measure in Figure~\\ref{subfig:shap-noclus}, which increases with the number of clusters.\nIt is interesting to observe how the \\textit{MDvar} and the \\textit{ShaP} measures have almost opposite behaviors.\nAn algorithm that shows the best performances on one of the measures, also shows the worst on the other.\nThe temporal divergence in Figure~\\ref{subfig:tvar-noclus} shows a very sharp decrease for a low number of clusters, and afterwards remains relatively constant.\n\n\\subsection{Impact of parameters $\\beta$ and $\\delta$}\n\\label{subsec:parameters-beta-delta}\n\n\\begin{figure}[!t]\n\t\\centering\n\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_ShaP_vs_beta}\n\t\t\\label{subfig:mdvar-shap-beta}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_ShaP_vs_delta}\n\t\t\\label{subfig:mdvar-shap-delta}\n\t}\n\t\n\t\\caption{\\textit{MDvar} and \\textit{ShaP} function of $\\beta$ (a) and of $\\delta$ (b)}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe $\\beta$ parameter controls the impact of the contiguity constraints in Equation~(\\ref{eq:penalty-function}).\nWhen set to zero, no constraints are imposed, and the algorithm behaves just like the Simple {\\mbox{K-Means}}{}.\nThe higher the values of $\\beta$, the higher the penalty inflicted when breaking a constraint.\nWhen $\\beta$ is set to large values, the penalty factor will take precedence over the similarity measure in the objective function.\nObservations that belong to a certain entity will be assigned to the same cluster, regardless of their resemblance in the description space.\nWhen this happens, the algorithm cannot create partitions with higher number of clusters than the number of entities.\nIn order to evaluate the influence of parameter $\\beta$, we execute the Constrained {\\mbox{K-Means}}{} algorithm with $\\beta$ varying from 0 to 0.017 with a step of 0.0005.\nThe value of $\\delta$ is set at 3, and 5 clusters are constructed.\nFor each value of $\\beta$, we executed 10 times the algorithm and we plot the average obtained values.\nFigure~\\ref{subfig:mdvar-shap-beta} shows the evolution of measures \\textit{MDvar} and \\textit{ShaP} with $\\beta$.\nWhen $\\beta=0$ both \\textit{MDvar} and \\textit{ShaP} have the same values as for Simple {\\mbox{K-Means}}{}.\nAs $\\beta$ increases, so does the penalty for non-contiguous segmentation of entities.\n\\textit{MDvar} starts to increase rapidly, while \\textit{ShaP} decreases rapidly.\nOnce $\\beta$ reaches higher values, the measures continue their evolution, but with a leaner slope.\nIn the extreme case, in which all observations are assigned to the same cluster regardless of their similarity, the \\textit{ShaP} measure will reach zero.\n\nThe $\\delta$ parameter controls the width of the penalty function in Equation~(\\ref{eq:penalty-function}).\nAs Figure~\\ref{fig:penalty-delta} shows, when $\\delta$ has a low value, a penalty is inflicted only if the time difference of a pair of observations is small.\nAs the time difference increases, the function quickly converges to zero.\nAs $\\delta$ increases, the function decreases with a leaner slope, thus also taking into account observations which are farther away in time.\nIn order to analyze the behavior of the penalty function when varying $\\delta$, we have executed the Constrained {\\mbox{K-Means}}{}, with $\\delta$ ranging from 0.1 to 8, using a step of 0.1.\n$\\beta$ was set at 0.003 and 10 clusters were constructed.\nFigure~\\ref{subfig:mdvar-shap-delta} plots the evolution of measures \\textit{MDvar} and \\textit{ShaP} with $\\delta$.\nThe contiguity measure \\textit{ShaP} decreases almost linearly as $\\delta$ increases, as the series of observations belonging to each entity gets segmented in larger chunks.\nAt the same time, the multidimensional variance \\textit{MDvar} increases linearly with $\\delta$.\nClusters become more heterogeneous and variance increases, as observations get assigned to clusters based on their membership to an entity, rather than their descriptive similarities.\n\nVarying $\\alpha^*$ and $d^*$ for the tcK-Means proposed in~\\cite{LIN06} yields similar results, with the \\textit{MDvar} augmenting and the \\textit{ShaP} descending, when $\\alpha^*$ and $d^*$ increase.\nFor the tcK-Means, these evolutions are linear, whereas for the Constrained {\\mbox{K-Means}}{} they are exponential, following a trend line of function $e^{-\\frac{const}{x}}$.\nPlotting the evolution of the \\textit{MDvar} and the \\textit{ShaP} indicators on the same graphic, provides a heuristic for choosing the optimum values for the $(\\beta, \\delta)$ parameters of the Constrained {\\mbox{K-Means}}{} and the {\\mbox{TDCK-Means}}{}, respectively the $(\\alpha^*, d^*)$ parameters of the tcK-Means.\nBoth curves are plotted with the vertical axis scaled to the interval $[min_{value}, max_{value}]$. \nTheir point of intersection determines the values of the parameters (as shown in Figure~\\ref{subfig:mdvar-shap-beta} and~\\ref{subfig:mdvar-shap-delta}).\nThe disadvantage of such a heuristic would be that a large number of executions must be performed with multiple values for the parameters before the ``optimum'' can be found.\n\n\\subsection{The tuning parameter $\\alpha$}\n\\label{subsec:parameters-alpha}\n\nThe parameter $\\alpha$, proposed in Section~\\ref{subsec:measure-tuning-alpha}, allows the fine tuning of the ratio between the multidimensional component and the temporal component in the temporal-aware dissimilarity measure.\nWhen $\\alpha$ is close to -1, the temporal component is predominant.\nConversely, when $\\alpha$ is close to 1, the multidimensional component takes precedence.\nThe two components have equal weights when $\\alpha = 0$.\nTo evaluate the influence of parameter $\\alpha$, we executed Temporal-Driven {\\mbox{K-Means}}{} with $\\alpha$ varying from -1 to 1 with a step of 0.1.\nIn order not to bias the results and to evaluate only the impact of the tuning parameter, we remove the contiguity constraints from the objective function $\\mathcal{J}$, by setting $\\beta = 0$.\nFor each value of $\\alpha$, we executed 10 times and we present the average values.\n\n\\begin{figure}[!t]\n\t\\centering\n\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_Tvar_vs_alpha}\n\t\t\\label{subfig:mdvar-tvar-alpha}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{Tvar_ShaP_vs_alpha}\n\t\t\\label{subfig:tvar-shap-alpha}\n\t}\n\t\n\t\\caption{Influence of tuning parameter $\\alpha$ on \\textit{MDvar} and \\textit{Tvar} (a) and \\textit{Tvar} and \\textit{ShaP} (b)}\n\t\\label{fig:parameter-alpha}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFigure~\\ref{subfig:mdvar-tvar-alpha} shows the evolution of measures \\textit{MDvar} and \\textit{Tvar} with $\\alpha$.\nFor low values of $\\alpha$, the value of the temporal-aware dissimilarity measure is given mainly by the temporal component, so \\textit{Tvar} shows its lowest value, while \\textit{MDvar} presents its maximum.\nAs $\\alpha$ increases, \\textit{MDvar} decreases as more importance is given to the multidimensional component.\nFor $\\alpha \\in (-1, 0]$, the importance of the temporal component remains intact, the increase of \\textit{Tvar} is solely the result of the algorithm converging to a local optimum which also takes into account the multidimensional component.\nFor $\\alpha \\in [0, 1)$, the impact of the multidimensional component stays constant, whereas the importance of the temporal components diminishes.\nAs a result, \\textit{MDvar} continues its decrease and \\textit{Tvar} increases sharply.\nFor $\\alpha = 1$ the temporal component is basically ignored from the measure.\nThe Temporal-Driven {\\mbox{K-Means}}{} behaves just like Simple {\\mbox{K-Means}}{}.\nFigure~\\ref{subfig:tvar-shap-alpha} shows the evolution of \\textit{ShaP} alongside \\textit{MDvar}.\nEven if the contiguity penalty component was neutralized by setting $\\beta = 0$, the value of \\textit{ShaP} is not constant, but it descends with $\\alpha$.\nFor low values of $\\alpha$, the temporal component is predominant in the similarity measure.\nThis generates partitions where every cluster regroups all observations from a specific period, regardless of their multidimensional description.\nThis means that all entities have segments in all the clusters, which leads to a high value of \\textit{ShaP}.\n\nIt is noteworthy that the evolution of the indicators is not linear with $\\alpha$.\nAs $\\alpha$ increases, \\textit{Tvar} augments only very slowly and picks up the pace only for large values of $\\alpha$.\nThis indicates that the temporal component has an inherent advantage over the multidimensional one.\nAs we presumed in Section~\\ref{subsec:measure-tuning-alpha}, this is due to the intrinsic nature of the dataset and the main reason why the tuning parameter $\\alpha$ was introduced.\nThe distributions of observations in the multidimensional and temporal spaces is different: in the temporal space, the observations tend to be evenly distributed, whereas in the multidimensional description space, they cluster together.\nTo quantify this, we calculate the ratio between average standard deviation and average distances between pairs of observations:\n\\vspace{-0.1in}\n\n", "itemtype": "equation", "pos": 49586, "prevtext": "\n\n\\textbf{Segment the temporal series of observations of each entity into a relatively small number of contiguous segments.}\nThe goal is to have successive observations belonging to an entity grouped together, rather that scattered in different clusters.\nThe Shannon entropy can quantify the number of clusters which regroup the observations of an entity, but it is insensible to alternations between two classes (evolutions like $\\mu_1 \\longrightarrow \\mu_2 \\longrightarrow \\mu_1 \\longrightarrow \\mu_2 $).\nWe evaluate using an adapted mean Shannon entropy of clusters over entities, which weights the entropy by a penalty factor depending on the number of continuous segments in the series of each entity.\nThe \\textit{ShaP} measure is calculated as:\n\\vspace*{-0.22cm}\n\n", "index": 39, "text": "\\begin{equation*}\n\tShaP = \\frac{1}{|\\mathcal{X}|} \\times \\sum_{x_i \\in \\mathcal{X}} \\sum_{j=1}^m \\left( -p(\\mu_j)\\times \\log_2(p(\\mu_j)) \\times \\left( 1 + \\frac{n_{ch} - n_{min}}{n_{obs} - 1}\\right) \\right)\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"ShaP=\\frac{1}{|\\mathcal{X}|}\\times\\sum_{x_{i}\\in\\mathcal{X}}\\sum_{j=1}^{m}%&#10;\\left(-p(\\mu_{j})\\times\\log_{2}(p(\\mu_{j}))\\times\\left(1+\\frac{n_{ch}-n_{min}}%&#10;{n_{obs}-1}\\right)\\right)\" display=\"block\"><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>P</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u00d7</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00d7</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><msub><mi>n</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>n</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></mrow><mrow><msub><mi>n</mi><mrow><mi>o</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><mo>-</mo><mn>1</mn></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02603.tex", "nexttext": "\nwhere $dim$ is replaced with $d$ or $t$ ( the descriptive or the temporal dimension).\nOn \\textit{Comparative Political Data Set I}, $r_{d} =  29.5\\%$ and $r_{t} = 65.3\\%$.\nThis shows that observations are a lot more dispersed in the temporal space than in the multidimensional description space.\nThis explains why \\textit{Tvar} augments very slowly with $\\alpha$ and starts to increase more rapidly only starting from $\\alpha=0.4$.\n\nFollowing the heuristic proposed in Section~\\ref{subsec:parameters-beta-delta}, we can determine a trade-off value for $\\alpha$.\nAs shown in Figure~\\ref{fig:parameter-alpha}, all vertical axes are magnified between their functions' minimum and maximum values.\nThe trade-off value for $\\alpha$ is found at the intersection point of \\textit{MDvar} and \\textit{Tvar} (and \\textit{MDvar} and \\textit{Shap}).\nThis value is set around $0.7$, showing the dataset's bias towards the temporal component.\nThis technique for setting the value of the tunning parameter is just a heuristic, the actual value of $\\alpha$ is dependent on the dataset.\nThis is why we are currently working on a method, inspired from multi-objective optimization using evolutionary algorithms~\\cite{ZHA07} to automatically determine the values of $\\alpha$, as well as the other parameters of {\\mbox{TDCK-Means}}{} ($\\beta$, $\\delta$ and $\\gamma$).\n\n\\section{Conclusion and future work}\n\\label{sec:conclusion}\n\nIn this paper, we have studied the detection of typical evolutions from a collection of observations.\nWe have proposed a novel way to introduce temporal information directly into the dissimilarity measure, weighting the Euclidean component in the description space by the temporal component.\nWe have proposed {\\mbox{TDCK-Means}}{}, an extension of {\\mbox{K-Means}}{}, which uses the temporal-aware dissimilarity measure and a new objective function which takes into consideration the temporal dimension.\nWe use a penalty factor to make sure that the observation series related to an entity get segmented into continuous chunks.\nWe infer a new centroid update formula, where elements distant in time contribute less to the centroid than the temporally close ones.\nWe have proposed an intersection similarity measure between two temporal clusters and a method to calculate \\textit{a posteriori} an adjacency matrix.\nWe use this adjacency matrix in order to structure the detected evolution phases as a graph.\nFrom a qualitative point of view, we have shown that our algorithm is capable of detecting comprehensible evolutions based on a Political Science dataset.\nQuantitatively, we have shown that our proposition consistently improves temporal variance, without any significant losses in the multidimensional variance.\n\nWe are currently experimenting with applying the algorithm to other applications, \\textit{e.g.}, detection of social roles in social networks, by passing through temporal behavioral roles.\nA social role is defined as a typical succession of behavioral roles.\nIn our current work, we have inferred a temporal cluster graph structure \\textit{a posteriori} to the construction of the clusters.\nWe have ongoing work toward incorporating the graph construction into the {\\mbox{TDCK-Means}}{} algorithm, by modifying the objective function in order to take into account the intersection similarity measure and a temporal distance.\nAnother direction of research will be describing the clusters in a human readable form.\nWe work on means to provide them with an easily comprehensible description by introducing temporal information into an unsupervised feature construction algorithm.\nWe are also experimenting a method for setting automatically the values of {\\mbox{TDCK-Means}}{}'s parameters ($\\alpha$, $\\beta$, $\\delta$ and $\\gamma$), by using an approach inspired from multi-objective optimization using evolutionary algorithms~\\cite{ZHA07}.\n\n{\\scriptsize\n\\bibliographystyle{plain}\n\\bibliography{biblio-tempered}\n}\n\n\n", "itemtype": "equation", "pos": 67896, "prevtext": "\nwhere $n_{ch}$ is the number of changes in the cluster assignment series of an entity, $n_{min}$ is the minimal required number of changes and $n_{obs}$ is the number of observations for an entity.\nFor example, in Figure~\\ref{fig:good-bad-shap}, if the series of 11 observations of an entity is assigned to two clusters, but it presents 4 changes, the entropy penalty factor will be $1 + \\frac{4 - 1}{11 - 1} = 1.33$.\nThe \\textit{ShaP} score for this segmentation will be $1.23$, compared to a score of $0.94$ of the ``ideal'' segmentation (only two contiguous chunks).\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width=0.7\\textwidth]{good-bad-shap}\n\t\\caption{Examples of a good and a bad segmentation in contiguous chunks and their related \\textit{ShaP} score.}\n\t\\label{fig:good-bad-shap}\n\t\\vspace{-0.18in}\n\\end{figure}\n\nThe ``ideal'' values for \\textit{MDvar}, \\textit{Tvar} and \\textit{ShaP} is zero and, in all of the experiments presented in the following sections, we search to minimize the values of the three measures.\n\n\\subsection{Quantitative evaluation}\n\\label{subsec:quantitative-evaluation}\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\subfloat[] {\n\t\t\\includegraphics[width=0.31\\textwidth]{MDvar_vs_noclus}\n\t\t\\label{subfig:mdvar-noclus}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.31\\textwidth]{Tvar_vs_noclus}\n\t\t\\label{subfig:tvar-noclus}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.31\\textwidth]{ShaP_vs_noclus}\n\t\t\\label{subfig:shap-noclus}\n\t}\n\n\t\\caption{\\textit{MDvar} (a), \\textit{Tvar} (b) and \\textit{ShaP} (c) values of the considered algorithms when varying the number of clusters.}\n\t\\label{fig:measures-clus}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFor each combination of algorithms and parameters, we execute 10 times and compute only the average and the standard deviation.\nWe vary $m$, the number of clusters, from 2 to 36.\nThe performances of five algorithms are compared from a quantitative point of view:\n  \\begin{list}{$\\bullet$}   { \\setlength{\\itemsep}{0pt}     \\setlength{\\parsep}{0pt}     \\setlength{\\topsep}{0pt}     \\setlength{\\partopsep}{0pt}     \\setlength{\\leftmargin}{1.5em}     \\setlength{\\labelwidth}{1.5em}     \\setlength{\\labelsep}{0.5em} } \n\t\\item \\textbf{Simple {\\mbox{K-Means}}{}} - clusters the observations based solely on their resemblance in the multidimensional space;\n\t\n\t\\item \\textbf{Temporal-Driven {\\mbox{K-Means}}{}} - optimizes only the temporal and multidimensional components, without any contiguity constraints;\n\tcombines {\\mbox{K-Means}}{} with the temporal-aware dissimilarity measure defined in Section~\\ref{subsec:proposal-temporal-dissimilarity-measure}.\n\tParameters: $\\alpha=0$ ($\\alpha$ defined in Equation~\\ref{eq:centroid-update}) and $\\beta=0$ ($\\beta$ defined in Equation~\\ref{eq:penalty-function});\n\t\n\t\\item \\textbf{Constrained {\\mbox{K-Means}}{}} - uses only the multidimensional space (and not the temporal component) together with the penalty component, as proposed in Section~\\ref{subsec:proposal-penalty-function}. \n\tParameters: $\\alpha=1$, $\\beta = 0.003$ and $\\delta = 3$ ($\\delta$ defined in Equation~\\ref{eq:penalty-function});\n\t\n\t\\item \\textbf{{\\mbox{TDCK-Means}}{}} - the Temporal-Driven Constrained Clustering algorithm proposed in Section~\\ref{subsec:proposal-tdck-means}. \n\t$\\alpha=0$, $\\beta = 0.003$ and $\\delta = 3$;\n\t\n\t\\item \\textbf{tcK-Means} - the temporal constrained clustering algorithm proposed in~\\cite{LIN06}. \n\tIt uses a threshold penalty function $w(x_i^{t_i},x_j^{t_i}) = \\alpha^* \\mathbbm{1} (|x_i^t - x_j^t| < d^*)$ when observations $ x_i$ and $x_j$ are not assigned to the same cluster. \n\tWe adapt it to the multi-entity case by applying it only to observations belonging to the same entity. \n\tParameters: $\\alpha^* = 2, d^* = 4$.\n   \\end{list}  \nThe $\\alpha^*$ parameter in \\textbf{tcK-Means} should not be mistaken with the $\\alpha$ parameter in \\textbf{{\\mbox{TDCK-Means}}{}}, as they do not have the same meaning.\n\tIn \\textbf{tcK-Means}, $\\alpha^*$ controls the weight of the penalty function, whereas in \\textbf{{\\mbox{TDCK-Means}}{}} $\\alpha$ is the fine-tuning parameter.\n\n\\begin{table}[t]\n\\caption{Mean values for indicators and standard deviations}\n\\label{tab:result-all}\n\n\\small\n\\centering\n\\begin{tabular}{llrr|rr|rr}\n\\toprule\n & \\multicolumn{1}{c}{\\textit{Algorithm}} &  \\multicolumn{2}{c}{\\textit{MDvar}} & \\multicolumn{2}{c}{\\textit{Tvar}} & \\multicolumn{2}{c}{\\textit{ShaP}} \\\\ \\midrule\n \n\\multirow{4}{*}{\\begin{sideways}\\texttt{Scores}\\end{sideways}} \n& \\textbf{Simple {\\mbox{K-Means}}{}} & \\textbf{120.59} & \\textit{2.97} & 48.01 & \\textit{8.87} & 2.15 & \\textit{0.23} \\\\ \n& \\textbf{Temp-Driven {\\mbox{K-Means}}{}} & 122.98 & \\textit{2.85} & \\textbf{19.97} & \\textit{5.39} & 2.58 & \\textit{0.18} \\\\ \n& \\textbf{Constrained {\\mbox{K-Means}}{}} & 132.69 & \\textit{8.07} & 103.15 & \\textit{42.98} & \\textbf{1.24} & \\textit{0.5} \\\\ \n& \\textbf{{\\mbox{TDCK-Means}}{}} & 127.81 & \\textit{3.96} & 27.54 & \\textit{5.81} & 2.06 & \\textit{0.2} \\\\ \n& \\textbf{tcK-Means} & 123,04 & \\textit{3.8} & 62.44 & \\textit{24.16} & 1.79 & \\textit{0.32} \\\\ \n\n\\midrule\n\n\\multirow{3}{*}{\\begin{sideways}\\texttt{\\% Gain}\\end{sideways}} \n& \\textbf{Temp-Driven {\\mbox{K-Means}}{}} & \\multicolumn{2}{c|}{-1.99\\%} & \\multicolumn{2}{c|}{\\textbf{58.40\\%}} & \\multicolumn{2}{c}{-19.63\\%} \\\\ \n& \\textbf{Constrained {\\mbox{K-Means}}{}} & \\multicolumn{2}{c|}{-10.04\\%} & \\multicolumn{2}{c|}{-114.84\\%} & \\multicolumn{2}{c}{\\textbf{42.21\\%}} \\\\ \n& \\textbf{{\\mbox{TDCK-Means}}{}} & \\multicolumn{2}{c|}{-5.99\\%} & \\multicolumn{2}{c|}{42.64\\%} & \\multicolumn{2}{c}{4.19\\%} \\\\ \n& \\textbf{tcK-Means} & \\multicolumn{2}{c|}{-2.03\\%} & \\multicolumn{2}{c|}{-30.05\\%} & \\multicolumn{2}{c}{16.99\\%} \\\\ \n\n\\bottomrule\n\\end{tabular}\n\n\\end{table}\n\t\n\\paragraph{Obtained results.}\nAll the parameters are determined as shown in Section~\\ref{subsec:parameters-beta-delta}.\nTable~\\ref{tab:result-all} shows the average values for the indicators, as well as the average standard deviation (in italic) obtained by each algorithm over all values of $m$.\nThe average standard deviation is only used to give an idea of the order of magnitude of the stability of each algorithm.\nSince Simple {\\mbox{K-Means}}{}, Temporal-Driven {\\mbox{K-Means}}{} and Constrained {\\mbox{K-Means}}{} are designed to optimize mainly one component, it is not surprising that they show the best scores for, respectively, the multidimensional variance, the temporal variance and the entropy (best results in boldface). \n{\\mbox{TDCK-Means}}{} seeks to provide a compromise, obtaining in two out of three cases the second best score.\nIt is noteworthy that the proposed temporal-aware dissimilarity measure used in Temporal-Driven {\\mbox{K-Means}}{} provides the highest stability (the lowest average standard deviation) for all indicators.\nMeanwhile, the constrained algorithms (Constrained {\\mbox{K-Means}}{} and tcK-Means) show high instability, especially on \\textit{Tvar}.\n{\\mbox{TDCK-Means}}{} shows a very good stability.\nThe second part of Table~\\ref{tab:result-all} gives the relative gain of performance of each of the proposed algorithms over Simple {\\mbox{K-Means}}{}.\nIt is noteworthy the effectiveness of the temporal-aware dissimilarity measure proposed in Section~\\ref{subsec:proposal-temporal-dissimilarity-measure}, with a 58\\% gain of Temporal Variance and less than 2\\% loss of multidimensional variance.\nThe proposed dissimilarity measure greatly enhances the temporal cohesion of the resulted clusters, without a significant scattering of observations in the multidimensional space.\nSimilarly, the Constrained KM shows an improvement in the contiguity measure \\textit{ShaP} of 42\\%, while losing 10\\% multidimensional variance.\nBy comparison, tcK-Means shows modest results, improving \\textit{ShaP} by only 17\\% and still showing important losses on both \\textit{Tvar} (-30\\%) and \\textit{MDvar} (-2\\%).\nThis proves that the threshold penalty function proposed in literature has lower performances than our newly proposed contiguity penalty function.\n{\\mbox{TDCK-Means}}{} combines the advantages of the other two algorithms, providing an important gain of 43\\% of temporal variance and increasing the \\textit{ShaP} measure by more than 4\\%.\nNonetheless, it shows a 6\\% loss of \\textit{MDvar}.\n\n\\paragraph{Varying the number of clusters}\nSimilar conclusions can be drawn when varying the number of clusters.\n\\textit{MDvar} (Figure~\\ref{subfig:mdvar-noclus}) decreases, for all algorithms, as the number of cluster increases.\nIt is well known in clustering literature that the intra-cluster variance decreases steadily with the increase of number of clusters.\nAs the number of clusters augments, so does the differences of {\\mbox{TDCK-Means}}{} and Constrained {\\mbox{K-Means}}{}, when compared to the Simple {\\mbox{K-Means}}{} algorithm.\nThis is due to the fact that the constraints do not let too many clusters to be assigned to the same entity, resulting in the convergence towards a local optimum, with a higher value of \\textit{MDvar}.\nAn opposite behavior is shown by the \\textit{ShaP} measure in Figure~\\ref{subfig:shap-noclus}, which increases with the number of clusters.\nIt is interesting to observe how the \\textit{MDvar} and the \\textit{ShaP} measures have almost opposite behaviors.\nAn algorithm that shows the best performances on one of the measures, also shows the worst on the other.\nThe temporal divergence in Figure~\\ref{subfig:tvar-noclus} shows a very sharp decrease for a low number of clusters, and afterwards remains relatively constant.\n\n\\subsection{Impact of parameters $\\beta$ and $\\delta$}\n\\label{subsec:parameters-beta-delta}\n\n\\begin{figure}[!t]\n\t\\centering\n\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_ShaP_vs_beta}\n\t\t\\label{subfig:mdvar-shap-beta}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_ShaP_vs_delta}\n\t\t\\label{subfig:mdvar-shap-delta}\n\t}\n\t\n\t\\caption{\\textit{MDvar} and \\textit{ShaP} function of $\\beta$ (a) and of $\\delta$ (b)}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nThe $\\beta$ parameter controls the impact of the contiguity constraints in Equation~(\\ref{eq:penalty-function}).\nWhen set to zero, no constraints are imposed, and the algorithm behaves just like the Simple {\\mbox{K-Means}}{}.\nThe higher the values of $\\beta$, the higher the penalty inflicted when breaking a constraint.\nWhen $\\beta$ is set to large values, the penalty factor will take precedence over the similarity measure in the objective function.\nObservations that belong to a certain entity will be assigned to the same cluster, regardless of their resemblance in the description space.\nWhen this happens, the algorithm cannot create partitions with higher number of clusters than the number of entities.\nIn order to evaluate the influence of parameter $\\beta$, we execute the Constrained {\\mbox{K-Means}}{} algorithm with $\\beta$ varying from 0 to 0.017 with a step of 0.0005.\nThe value of $\\delta$ is set at 3, and 5 clusters are constructed.\nFor each value of $\\beta$, we executed 10 times the algorithm and we plot the average obtained values.\nFigure~\\ref{subfig:mdvar-shap-beta} shows the evolution of measures \\textit{MDvar} and \\textit{ShaP} with $\\beta$.\nWhen $\\beta=0$ both \\textit{MDvar} and \\textit{ShaP} have the same values as for Simple {\\mbox{K-Means}}{}.\nAs $\\beta$ increases, so does the penalty for non-contiguous segmentation of entities.\n\\textit{MDvar} starts to increase rapidly, while \\textit{ShaP} decreases rapidly.\nOnce $\\beta$ reaches higher values, the measures continue their evolution, but with a leaner slope.\nIn the extreme case, in which all observations are assigned to the same cluster regardless of their similarity, the \\textit{ShaP} measure will reach zero.\n\nThe $\\delta$ parameter controls the width of the penalty function in Equation~(\\ref{eq:penalty-function}).\nAs Figure~\\ref{fig:penalty-delta} shows, when $\\delta$ has a low value, a penalty is inflicted only if the time difference of a pair of observations is small.\nAs the time difference increases, the function quickly converges to zero.\nAs $\\delta$ increases, the function decreases with a leaner slope, thus also taking into account observations which are farther away in time.\nIn order to analyze the behavior of the penalty function when varying $\\delta$, we have executed the Constrained {\\mbox{K-Means}}{}, with $\\delta$ ranging from 0.1 to 8, using a step of 0.1.\n$\\beta$ was set at 0.003 and 10 clusters were constructed.\nFigure~\\ref{subfig:mdvar-shap-delta} plots the evolution of measures \\textit{MDvar} and \\textit{ShaP} with $\\delta$.\nThe contiguity measure \\textit{ShaP} decreases almost linearly as $\\delta$ increases, as the series of observations belonging to each entity gets segmented in larger chunks.\nAt the same time, the multidimensional variance \\textit{MDvar} increases linearly with $\\delta$.\nClusters become more heterogeneous and variance increases, as observations get assigned to clusters based on their membership to an entity, rather than their descriptive similarities.\n\nVarying $\\alpha^*$ and $d^*$ for the tcK-Means proposed in~\\cite{LIN06} yields similar results, with the \\textit{MDvar} augmenting and the \\textit{ShaP} descending, when $\\alpha^*$ and $d^*$ increase.\nFor the tcK-Means, these evolutions are linear, whereas for the Constrained {\\mbox{K-Means}}{} they are exponential, following a trend line of function $e^{-\\frac{const}{x}}$.\nPlotting the evolution of the \\textit{MDvar} and the \\textit{ShaP} indicators on the same graphic, provides a heuristic for choosing the optimum values for the $(\\beta, \\delta)$ parameters of the Constrained {\\mbox{K-Means}}{} and the {\\mbox{TDCK-Means}}{}, respectively the $(\\alpha^*, d^*)$ parameters of the tcK-Means.\nBoth curves are plotted with the vertical axis scaled to the interval $[min_{value}, max_{value}]$. \nTheir point of intersection determines the values of the parameters (as shown in Figure~\\ref{subfig:mdvar-shap-beta} and~\\ref{subfig:mdvar-shap-delta}).\nThe disadvantage of such a heuristic would be that a large number of executions must be performed with multiple values for the parameters before the ``optimum'' can be found.\n\n\\subsection{The tuning parameter $\\alpha$}\n\\label{subsec:parameters-alpha}\n\nThe parameter $\\alpha$, proposed in Section~\\ref{subsec:measure-tuning-alpha}, allows the fine tuning of the ratio between the multidimensional component and the temporal component in the temporal-aware dissimilarity measure.\nWhen $\\alpha$ is close to -1, the temporal component is predominant.\nConversely, when $\\alpha$ is close to 1, the multidimensional component takes precedence.\nThe two components have equal weights when $\\alpha = 0$.\nTo evaluate the influence of parameter $\\alpha$, we executed Temporal-Driven {\\mbox{K-Means}}{} with $\\alpha$ varying from -1 to 1 with a step of 0.1.\nIn order not to bias the results and to evaluate only the impact of the tuning parameter, we remove the contiguity constraints from the objective function $\\mathcal{J}$, by setting $\\beta = 0$.\nFor each value of $\\alpha$, we executed 10 times and we present the average values.\n\n\\begin{figure}[!t]\n\t\\centering\n\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{MDvar_Tvar_vs_alpha}\n\t\t\\label{subfig:mdvar-tvar-alpha}\n\t}\n\t\\hfill\n\t\\subfloat[]{\n\t\t\\includegraphics[width=0.45\\textwidth]{Tvar_ShaP_vs_alpha}\n\t\t\\label{subfig:tvar-shap-alpha}\n\t}\n\t\n\t\\caption{Influence of tuning parameter $\\alpha$ on \\textit{MDvar} and \\textit{Tvar} (a) and \\textit{Tvar} and \\textit{ShaP} (b)}\n\t\\label{fig:parameter-alpha}\n\t\\vspace{-0.2in}\n\\end{figure}\n\nFigure~\\ref{subfig:mdvar-tvar-alpha} shows the evolution of measures \\textit{MDvar} and \\textit{Tvar} with $\\alpha$.\nFor low values of $\\alpha$, the value of the temporal-aware dissimilarity measure is given mainly by the temporal component, so \\textit{Tvar} shows its lowest value, while \\textit{MDvar} presents its maximum.\nAs $\\alpha$ increases, \\textit{MDvar} decreases as more importance is given to the multidimensional component.\nFor $\\alpha \\in (-1, 0]$, the importance of the temporal component remains intact, the increase of \\textit{Tvar} is solely the result of the algorithm converging to a local optimum which also takes into account the multidimensional component.\nFor $\\alpha \\in [0, 1)$, the impact of the multidimensional component stays constant, whereas the importance of the temporal components diminishes.\nAs a result, \\textit{MDvar} continues its decrease and \\textit{Tvar} increases sharply.\nFor $\\alpha = 1$ the temporal component is basically ignored from the measure.\nThe Temporal-Driven {\\mbox{K-Means}}{} behaves just like Simple {\\mbox{K-Means}}{}.\nFigure~\\ref{subfig:tvar-shap-alpha} shows the evolution of \\textit{ShaP} alongside \\textit{MDvar}.\nEven if the contiguity penalty component was neutralized by setting $\\beta = 0$, the value of \\textit{ShaP} is not constant, but it descends with $\\alpha$.\nFor low values of $\\alpha$, the temporal component is predominant in the similarity measure.\nThis generates partitions where every cluster regroups all observations from a specific period, regardless of their multidimensional description.\nThis means that all entities have segments in all the clusters, which leads to a high value of \\textit{ShaP}.\n\nIt is noteworthy that the evolution of the indicators is not linear with $\\alpha$.\nAs $\\alpha$ increases, \\textit{Tvar} augments only very slowly and picks up the pace only for large values of $\\alpha$.\nThis indicates that the temporal component has an inherent advantage over the multidimensional one.\nAs we presumed in Section~\\ref{subsec:measure-tuning-alpha}, this is due to the intrinsic nature of the dataset and the main reason why the tuning parameter $\\alpha$ was introduced.\nThe distributions of observations in the multidimensional and temporal spaces is different: in the temporal space, the observations tend to be evenly distributed, whereas in the multidimensional description space, they cluster together.\nTo quantify this, we calculate the ratio between average standard deviation and average distances between pairs of observations:\n\\vspace{-0.1in}\n\n", "index": 41, "text": "\\begin{equation*}\n\tr_{dim} = \\dfrac{1}{|\\mathcal{X}|}\\sum_{i=1}^{|\\mathcal{X}| } \\dfrac{stdev \\left( \\left\\lbrace || x_i^{dim} - x_j^{dim} ||^2 \\middle| x_j \\in \\mathcal{X}, \\; i \\neq j \\right\\rbrace \\right)}{\\dfrac{1}{|\\mathcal{X}|}\\sum_{\\substack{j=1\\\\j \\neq i}}^{|\\mathcal{X}|} || x_i^{dim} - x_j^{dim} ||^2 }\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"r_{dim}=\\dfrac{1}{|\\mathcal{X}|}\\sum_{i=1}^{|\\mathcal{X}|}\\dfrac{stdev\\left(%&#10;\\left\\{||x_{i}^{dim}-x_{j}^{dim}||^{2}\\middle|x_{j}\\in\\mathcal{X},\\;i\\neq j%&#10;\\right\\}\\right)}{\\dfrac{1}{|\\mathcal{X}|}\\sum_{\\begin{subarray}{c}j=1\\\\&#10;j\\neq i\\end{subarray}}^{|\\mathcal{X}|}||x_{i}^{dim}-x_{j}^{dim}||^{2}}\" display=\"block\"><mrow><msub><mi>r</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></munderover><mfrac><mrow><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>{</mo><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mo>|</mo><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo rspace=\"5.3pt\">,</mo><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow><mo>}</mo></mrow><mo>)</mo></mrow></mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd><mrow><mi>j</mi><mo>\u2260</mo><mi>i</mi></mrow></mtd></mtr></mtable><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><msup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}]