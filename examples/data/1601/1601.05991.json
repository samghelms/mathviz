[{"file": "1601.05991.tex", "nexttext": "\nwhere $P$ is the LPC order, $h(.)$ denotes a non-linear activation\nfunction (forward propagation) of the trained phonological\nsynthesizer, and ${\\mathbf{{z}}}_n$ are phonological posteriors for frame\n$n$.\n\nLPC re-synthesis can be done either with synthesised or original pitch\nfeatures.\n\n\n\\section{Experimental setup for laboratory phonology}\n\\label{sec:experiments}\n\n\nIn this section, the experimental protocol of the phonological\nanalysis and synthesis is described. In Table~\\ref{tab:data}, the data\nused in these experiments \nare presented. The analyser is trained on the Wall Street Journal\n(WSJ0 and WSJ1) continuous speech recognition corpora \\citep{WSJDB}.\nThe \\textit{si\\_tr\\_s\\_284} set of 37,514 utterances was used, split\nto 90\\% training and 10\\%  cross-validation sets. The synthesizer is\ntrained and evaluated on an English audiobook ``Anna Karenina'' of\nLeo\nTolstoy\\footnote{\\tiny\\url{https://librivox.org/anna-karenina-by-leo-tolstoy-2/}},\nthat is around 36 hours long. Recordings were organised into 238\nsections, and we used sections 1--209 as a training set, 210--230 as a\ndevelopment set and 231--238 as a testing set. The development and\ntesting sets were 3 hours and 1 hour long, respectively.\n\n\\begin{table} [th]\n  \\caption{\\label{tab:data} {\\it Data used for training the\n  phonological analysis and synthesis.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|c|c|c|c|}\n\\hline \nUse of data & Database & Set/Section & Size \\\\\n\\hline  \\hline\nAnalyser (train. set) & WSJ & si\\_tr\\_s\\_284 & 33.765 (utts) \\\\\nAnalyser (cross-val. set) & WSJ & si\\_tr\\_s\\_284 & 3.749 (utts) \\\\\nSynthesizer (train. set) & Tolstoy & 1-209 & 36 (hours) \\\\\nSynthesizer (dev. set) & Tolstoy & 210-230 & 3 (hours) \\\\\nSynthesizer (test set) & Tolstoy & 231-238 & 1 (hour) \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\\subsection{Phonological analysis}\n\nThe analyser is based on a bank of phonological analysers\nrealised as neural network classifiers -- multilayer perceptrons\n(MLPs) -- that estimate phonological posterior probabilities. Each MLP\nis designed to classify a binary phonological feature. \n\n\n\n\n\n\n\n\n\nFirst, we trained a phoneme-based automatic speech recognition system\nusing Perceptual Linear Prediction (PLP) features. The phoneme set\ncomprising of 40 phonemes (including ``sil'', representing silence)\nwas defined by the CMU pronunciation dictionary. The three-state,\ncross-word triphone models were trained with the HTS variant of\n~\\cite{Zen:HTS} of the HTK toolkit on the 90\\% subset of the\n\\textit{si\\_tr\\_s\\_284} set. The remaining 10\\% subset was used for\ncross-validation. We tied triphone models with decision tree state\nclustering based on the minimum description length (MDL)\ncriterion~\\citep{shinoda:mdl}. The MDL criterion allows an\nunsupervised determination of the number of states. In this study, we\nobtained 12,685 states and modeled each state with a Gaussian mixture\nmodel (GMM) consisting of 16 Gaussians.\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Alignment}\n\nA bootstrapping phoneme alignment was obtained using forced\nalignment with cross-word triphones. The bootstrapping alignment was\nused for the training of the bootstrapping MLP, using as the input 39\norder PLP features with the temporal context of 9 successive frames,\nand a softmax output function. The architecture of the MLP, 3-hidden\nlayers 351x2000x500x2000x40 (input 351=39x9 and output 40 is the\nnumber of phonemes), was determined  empirically. Using a hybrid\nspeech decoder fed with the phoneme posteriors, the re-alignment was\nperformed. After two iterations of the MLP training and re-alignment,\nthe best phoneme alignment of the speech data was obtained. This\nre-alignment increased the cross-validation accuracy of the MLP\ntraining from 77.54\\% to 82.36\\%.\n\n\\subsubsection{Training of the bank of phonological analysers}\n\nThe representation given in \\ref{sec:spefeatures} was used to\nmap the phonemes of the best alignment to phonological features \nfor the training of the\nanalysers. $K$ analysers were trained with the frame alignment having\ntwo output labels, the $k$-th phonological feature occurs for the\naligned phoneme or not. The analysis MLPs were finally trained with\nthe same input features as used for the alignment MLP, and the same\nnetwork architecture except for 2 output units instead of 40. The\noutput that encodes the phonological feature occurrence is used as the\nphonological posterior probability ${\\mathbf{{z}}}_n$.\n\nTables~\\ref{tab:accuracies}, \\ref{tab:SPEaccuracies}, and\n\\ref{tab:eSPEaccuracies} show classification accuracies at frame\nlevel of the GP, SPE and eSPE MLPs for the ``feature occurs'' output,\nrespectively. The accuracies are reported for the training and\ncross-validation sets. The analysers performances are high, with an\naverage cross-validation training accuracy of 95.5\\%, 95.6\\% and\n96.3\\%, respectively.\n\n\\begin{table} [t]\n  \\caption{\\label{tab:accuracies} {\\it Classification accuracies (\\%)\n      of the GP prime analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|c|c|c|[0.8pt]c|c|c|}\n\\hline\nPrime & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Prime &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\n & train & cv & & train & cv \\\\\n\\hline \\hline\nA & 92.3 & 91.7 & a & 97.9 & 97.6 \\\\\nI & 94.9 & 94.6 & i & 96.1 & 96.4 \\\\\nU & 94.4 & 94.1 & u & 97.5 & 97.7 \\\\\nE & 92.6 & 91.9 & H & 95.4 & 95.0 \\\\\nS & 95.2 & 94.7 & N & 98.2 & 98.1 \\\\\nh & 95.9 & 95.4 & sil & 99.0 & 98.9  \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\\begin{table} [t]\n  \\caption{\\label{tab:SPEaccuracies} {\\it Classification accuracies\n      (\\%) of the SPE analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}} \n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nNatural & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Natural &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nclasses & train & cv & classes & train & cv \\\\\n\\hline \\hline\nvocalic & 96.0 & 95.8 & round & 97.8 & 97.7 \\\\\nconsonantal & 94.5 & 93.9 & tense & 94.8 & 94.1 \\\\\nhigh & 94.8 & 94.4 & voice & 94.6 & 94.4 \\\\\nback & 93.9 & 93.4 & continuant & 95.6 & 95.2 \\\\\nlow & 97.4 & 97.1 & nasal & 98.1 & 98.0 \\\\\nanterior & 94.4 & 94.0 & strident & 97.8 & 97.6  \\\\\ncoronal & 94.3 & 93.9 & sil & 99.0 & 98.9 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n\n\\begin{table} [t]\n  \\caption{\\label{tab:eSPEaccuracies} {\\it Classification accuracies (\\%)\n      of the eSPE analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nNatural & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Natural &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nclasses & train & cv & classes & train & cv \\\\\n\\hline \\hline\nvowel & 94.7 & 94.3 & low & 97.5 & 97.2 \\\\\nfricative & 97.3 & 97.0 & mid & 94.5 & 94.0 \\\\\nnasal & 98.2 & 98.1 & retroflex & 98.6 & 98.4 \\\\\nstop & 96.7 & 96.4 & velar & 98.9 & 98.8 \\\\\napproximant & 97.2 & 96.9 & anterior & 94.8 & 94.3 \\\\\ncoronal & 94.8 & 94.4 & back & 94.2 & 93.8  \\\\\nhigh & 94.6 & 94.2 & continuant & 95.8 & 98.4 \\\\\ndental & 99.3 & 99.2 & round & 95.3 & 94.9 \\\\\nglottal & 99.7 & 99.7 & tense & 91.4 & 90.7 \\\\\nlabial & 97.6 & 97.4 & voiced & 95.2 & 94.9 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Phonological synthesis}\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Training}\n\\label{sec:training}\n\nThe speech signals from the training and cross-validation sets of the\nTolstoy database, down-sampled to 16 kHz, framed by 25-ms windows with\n10-ms frame shift, were used for extracting both DNN input and\noutput features. The input features, phonological posteriors\n${\\mathbf{{z}}}_n$, were generated by the phonological analyser trained on the\nWSJ database. The temporal context of 11 successive frames resulted in\ninput feature vectors of 132 ($12*11*1$), 165 ($15*11*1$) and 231\n($21*11*1$) dimensions, for the GP, SPE and eSPE schemes,\nrespectively. The output features, the LPC speech parameters, were \nextracted by the Speech Signal Processing (SSP) python\ntoolkit\\footnote{\\tiny\\url{https://github.com/idiap/ssp}}. We used\nstatic speech parametrization of 29th order along with its dynamic\nfeatures, altogether of 87th order. \n\nCepstral mean normalisation of the input features was\napplied before DNN training. The DNN was initialised using\n$(K*11)$x1024x1024x1024x1024 Deep Belief Network pre-training by\ncontrastive divergence with 1 sampling step\n(CD1)~\\citep{Hinton06}. The DNN with a linear output function was then\ntrained using a mini-batch based stochastic gradient descent algorithm\nwith mean square error cost function of the KALDI\ntoolkit~\\citep{PoveyASRU2011}. The DNN had 3.4 million parameters.\n\n\\subsubsection{Synthesis}\n\nThe test set of the Tolstoy database was used for the synthesis. There\nare three options to generate a particular speech sound: (i) by $Z$\ninferred from the speech, (ii) by $Z$ inferred from the text, or (iii)\nby compositional speech modelling.\n\n\n\n\n\n \n\n\n\nIn the two first cases, we refer to this generation\nas \\textbf{network-based}. The speech parameters generated by the\nforward DNN pass are smoothed using dynamic features and pre-computed\n(global) variances, and formant enhancement is performed \n\nto mitigate over-smoothing of the formant frequencies. \n\n\n\nIn the latter case, speech sounds are generated using compositional\nphonological speech models, introduced next in\nSection~\\ref{sec:soundsofprimes}. Briefly, we set a single\nphonological feature as an active input and the rest are set to\nzeros. This generates an artificial audio sound that characterises the\ninput phonological feature. Then, the speech sound is generated by the\ncomposition (audio mixing) of the particular artificial audio\nsounds. We refer to this process as \\textbf{compositional-based}. We\nwanted a) to show that compositional phonological speech modelling is\npossible, b) to test the suitability of speech sound synthesis without\nthe DNN, i.e., only by mixing  the artificial phonological sound\ncomponents, and c) to allow the reader to experiment with the sound\ncomponents that are embedded in this manuscript.\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application examples for laboratory phonology}\n\\label{sec:examples}\nIn this section we show how to use the phonological vocoder, described\nin Section~\\ref{sec:experiments}, for a) compositional phonological\nspeech modelling, b) a comparison of phonological systems and c) for\nparametric phonological TTS system.\n\n\n\\subsection{Compositional phonological speech models}\n\\label{sec:soundsofprimes}\n\n\n\n\\cite{Virtanen15} investigate the constructive compositionality of\nspeech signals, i.e., representing the speech signal as non-negative\nlinear combinations of atomic units (``atoms''), which themselves are\nalso non-negative to ensure that such a combination does not result in\nsubtraction or diminishment. The power of the sum of uncorrelated\natomic signals in any frequency band is the sum of the powers of the  \nindividual signals within that band. The central point is to define\nthe sound atoms that are used as the compositional models.\n\nFollowing this line of research, we hypothesise that the acoustic\nrepresentation of the phonological features, produced by a\nphonological vocoder, forms a set of speech signal atoms (the\nphonological sound components) that define the phones. We\ncall these sound components \\textbf{phonological atoms}.\n\n\n\n\nIt is possible to generate the atoms for any phonological\nsystem. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven our hypothesis above (that these phonological atoms form a set\nof acoustic templates that might be taken to define speech acoustic\nspace), we created artificial phonological atom representations $Z$ \nwhere ${\\mathbf{{z}}}_n$  consists of $K$ binary phonological posteriors\ndefined as follows:\n\n", "itemtype": "equation", "pos": 21774, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Speech vocoding for laboratory phonology}\n  \n\n\n\n\n\n\n\n\\author[label1]{Milos Cernak\\corref{cor1}}\n\\ead{milos.cernak@idiap.ch}\n\\author[label2]{Stefan Benus}\n\\author[label1]{Alexandros Lazaridis}\n\n\n\n\\address[label1]{Idiap Research Institute, Martigny, Switzerland}\n\\address[label2]{Constantine the Philosopher University in Nitra, Slovakia and Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovakia}\n\n\\cortext[cor1]{Corresponding author}\n\n\\begin{abstract}\n\n\n\nUsing phonological speech vocoding, we propose a platform for exploring relations between\nphonology and speech processing, and in broader terms, for exploring relations between the abstract and\nphysical structures of a speech signal. Our goal is to make a step towards bridging phonology and speech processing and to contribute to the\nprogram of Laboratory Phonology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe show three application examples for laboratory phonology:\ncompositional phonological speech modelling, a comparison of\nphonological systems and an experimental phonological parametric\ntext-to-speech (TTS) system. The featural representations of the\nfollowing three phonological systems are considered in this work: (i)\nGovernment Phonology (GP), (ii) the Sound Pattern of English (SPE),\nand (iii) the extended SPE (eSPE).\n\n\n\n\n\n\n\n\n\nComparing GP- and eSPE-based vocoded speech, we conclude that the\nlatter achieves slightly better results than the former. However, GP\n-- the most compact phonological speech representation -- performs\ncomparably to the systems with a higher number of phonological\nfeatures. The parametric TTS based on phonological speech\nrepresentation, and trained from an unlabelled audiobook in an\nunsupervised manner, achieves intelligibility of 85\\% of the\nstate-of-the-art parametric speech synthesis.\n\nWe envision that the presented approach paves the way for researchers\nin both fields to form meaningful hypotheses that are explicitly\ntestable using the concepts developed and exemplified in this\npaper. On the one hand, laboratory phonologists might test the applied\nconcepts of their theoretical models, and on the other hand, the\nspeech processing community may utilize the concepts developed for the theoretical phonological models for \nimprovements of the current state-of-the-art applications.\n\n\\end{abstract}\n\n\\begin{keyword}\n\nPhonological speech representation \\sep parametric speech synthesis \\sep laboratory phonology\n\n\n\n\n\n\n\\end{keyword}\n\n\\end{frontmatter}\n\n\n\n\\section{Introduction}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeech is a domain exemplifying the dichotomy between\ncontinuous and discrete, and in broader terms, between the body and the\nmind. On the one hand, the articulatory activity and the resulting\nspeech signal are continuously varying. On the other hand, for speech\ncommunication to convey meaning, this continuous signal must be, at\nthe same time, perceivable as discretely varying, and thus\ncontrastive. Traditionally, these two aspects have been studied under\nthe labels of phonetics and phonology respectively. Following\nsignificant successes of this dichotomous approach, for example in\nspeech synthesis and recognition, recent decades have witnessed a lot of\nprogress in understanding and formal modelling of the\nrelationship between these two aspects, e.g. the program of Laboratory\nPhonology \\citep{Pierrehumbert00} or the renewed interest in the\napparoaches based on Analysis by Synthesis\n\\citep{Hirst11,Bever10}. The goal of this paper is to follow these\ndevelopments by proposing a platform for exploring relations between\nthe abstract and physical structures of the speech signal. In this, we\naim at mutual cross-fertilisation between phonology, as a quest for\nunderstanding and modelling of cognitive abilities that underlie\nsystematic patterns in our speech, and speech processing, as a quest\nfor natural, robust, and reliable automatic systems for synthesising\nand recognising speech.\n\nAs a first step in this direction we examine vocoding based on\nphonological representations and how this might inform both quests\nmentioned above. In vocoding --- i.e. parametric speech coding ---\nspeech segments of different time-domain granularity, ranging from speech\nframes, e.g., formant~\\citep{Holmes1973},\narticulatory~\\citep{Goodyear1996, Laprie2013}, phones\n\\citep{Tokuda98,Lee2001}, and syllables \\citep{Cernocky98}, are used\nin sequential processing.\n\n\n\n\n\n\n\n\n\n\nPhonological representations have also been shown\nto be useful for  speech processing e.g. by~\\cite{King00}. We expand\non this approach by exploring a direct link between phonological\nfeatures and their engineered acoustic realizations. In other words,\nwe believe that abstract phonological sub-segmental, segmental, and\nsuprasegmental structures may be related to the physical speech signal\nthrough a speech engineering approach, and that this relationship is\ninformative for both phonology and speech processing.\n\nThe motivation for this approach is two-fold. Firstly, phonological\nrepresentations (together with grammar) create a formal phonological\nmodel whose overall goal is to capture the core properties of the\ncognitive system underlying speech production and perception.\nThis model, linking subsegmental, segmental, and suprasegmental\nphonological features of speech, finds independent support in the\ncorrespondence between a) the brain-generated cortical oscillations in\nthe `delta' (1-3 Hz), `theta' (4-7 Hz), and faster `gamma' ranges\n(25-40 Hz), and b) the temporal scales for the domains of prosodic\nphrases, syllables, and certain phonetic features respectively. In\nthis sense, we may consider phonological representations\nembodied~\\citep{Giraud12}.\n\n\n\n\n\nHence, speech processing utilizing such a system might\nlead to a biologically sensible and empirically testable computational\nmodel of speech.\n\nSecondly, phonological representations are inherently\nmultilingual~\\citep{Siniscalchi2012}. This in turn has an attractive\nadvantage in the context of multilingual speech processing in\nlessening the reliance on purely phonetic decisions. The independence of the phonological representations from a particular language\non the one hand and\nthe availability of language specific mapping between these\nrepresentations and the acoustic signal through speech processing\nmethods on the other hand, offer (we hope) a path towards a context-based\ninterpretation of the phonological representation that is grounded in the\nphonetic substance but at the same time abstract enough to allow for a more streamlined\napproach to multilingual speech processing.\n\nIn this work, we propose to use the phonological\nvocoding of~\\cite{Cernak15} for bridging theoretical phonology and\napplied computer science. We consider the following phonological systems\nin this work:\n\\begin{itemize}\n  \\item The Government Phonology (GP) features \\citep{Harris95}\n  describing sounds by fusing and splitting of 11 primes.\n  \\item The Sound Pattern of English (SPE) system with 13 features\n  established from natural (articulatory)\n  features~\\citep{chomsky68sound}.\n  \\item The extended SPE system (eSPE)\n  ~\\citep{Yu2012,Siniscalchi2012} consisting of 21\n  phonological features.\n\\end{itemize}\nHaving trained phonological vocoders for the three phonological models\nof sound representation, we describe several application examples for\nthis bridging. Our primary goal is to demonstrate the usefulness of\nthe approach by showing that (i) the vocoder can generate acoustic\nrealizations of phonological features used by compositional speech\nmodelling, (ii) speech sounds (both individual sounds not seen in\ntraining and intelligible continuous speech) can be generated from the\nphonological speech representation, and (iii) the testing of\nhypotheses relating phonetics and phonology is possible; we test the\nhypothesis that the best phonological speech representation achieves\nthe best quality vocoded speech, by evaluating the phonological\nfeatures in both  directions, recognition and synthesis,\nsimultaneously. Additionally, we compare the segmental properties of\nthe phonological systems, and describe results and\nadvantages of experimental phonological parametric TTS synthesis.\n\n\n\nThe structure of the paper is as follows: the phonological\nrepresentations used in this work are introduced in\nSection~\\ref{sec:phonology}. Section~\\ref{sec:phonovoc} introduces\nspeech vocoding based on phonological speech\nrepresentation. Section~\\ref{sec:experiments} describes the\nexperimental setup. The application examples of the proposed platform\n(along with the experiments and results) are shown in\nSection~\\ref{sec:examples}. Finally the conclusions follow in\nSection~\\ref{sec:conclusions}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n    \n    \n    \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Phonological systems}\n\\label{sec:phonology}\n\n\n\nPhonology is construed in this work as a formal model that\n  represents\n     \n  \n     \n  \n  \n  \n  \n  \n  \n  \n  \n  \ncognitive (subconscious) knowledge of native speakers regarding the\nsystematic sound patterns of a language. The two traditional\ncomponents of such models are i) system primitives, that is, the units\nof representation for cognitively relevant objects such as sounds or\nsyllables, and ii) a set of permissible operations over these\nrepresentations that is able to generate the observed\npatterns. Naturally, these two facets are closely linked and\ninter-dependent. In this paper we focus on the theory of representation.\n\n\n\n\\subsection{Segmental representations}\n\nThe minimal unit of meaning contrast, i.e. cognitive relevance, is\nassumed to be the phoneme. In the tradition of~\\cite{Jakobson56}\nand~\\cite{chomsky68sound}, phonemes are assumed to consist of feature\nbundles. In the former model, 12 basic perceptual-acoustic domains\n(e.g. acute-grave, or compact-diffuse) define the space for\ncharacterising all the phonemes. The model uses polar opposites for\nthese 12 continua, which are necessarily relational, and thus\nlanguage-specific. Hence, a vowel characterised as, e.g. \u00e2\u0080\u0098grave\u00e2\u0080\u0099 in\none language might be phonetically different from the same grave vowel\nin another language since their grave quality might be at a different\npoint of the acute-grave continuum. The latter system\nof~\\cite{chomsky68sound}, known also as SPE, differed \nfrom the former~\\cite{Jakobson56} in two fundamental aspects relevant for this\npaper. First, it took the articulatory production mechanism as the\nunderlying principle of phoneme organisation; hence, in their 13 basic\nbinary features, we talk about the position (or activity) of the\nactive articulators rather than percepts they\ncreate. Second, SPE assumed that the flat, unstructured binary feature\nspecifications are language independent and characterise the set of\npossible phonemes in languages of the world.\n\n\n\n\n\n\n\n\n\n\nThe developments of phonological theory after SPE focused on both the\ntheory of representations as well as the operations. The most relevant\nfor this paper are proposals for establishing the non-linear nature of\nphonological representations, i.e., the fact that individual features\nare not strictly linked to the linear sequence of sounds but may span\ngreater domains or occupy independent non-overlapping tiers. These proposals started\nwith the representation of\nlexical tones \\citep{Leben73phd,Goldsmith76phd}, continued with the\nfeatural geometry approach \\citep{Sagey86phd,Clements95} and received\nnovel formal treatments in the theories of Dependency and\nGovernment Phonology (GP, e.g. \\cite{Kaye90}, \\cite{Harris94}). These\nlatter approaches posit the so called primes, or basic elements, that are\nmonovalent (c.f. binary SPE features). For example, there are four\nbasic resonance primes commonly labelled as A, U, I, and @; the first\nthree denoting the peripheral vowel qualities {\\textipa}{[a]}, {\\textipa}{[u]} and {\\textipa}{[i]}\nrespectively, the last one describing the most central vowel quality of\nschwa. English {\\textipa}{[i:]} would correspond to the I prime while {\\textipa}{[e]}\nresults in fusing the I and A primes. In addition to these `vocalic'\nprimes, GP proposes the `consonantal' primes ?, h, H, N, denoting\nclosure, friction, voicelessness and nasality respectively. To account\nfor the observed variability in inventories (e.g. English has 20 contrastive vowels\nand thus 20 different phonetic representations for them), phonetic\nqualities, and types of phonological behaviour, \n\n\n\nthe phoneme representations based on simple primes become\ninsufficient. The solution, given the name of the framework, is that\nsome elements can optionally be heads\n   \n \nand govern the presence or realisation of other (dependent) elements.\n\nThese developments established the relevance of the internal structure\nof phonological primitives and their hierarchical\nnature. Furthermore, the non-linear nature of phonological\nrepresentations became the\naccepted norm for formal theories of phonology. Importantly, while the\nSPE-style features were assumed to require the full interpretation of\nall features for a surface phonetic realisation of a phoneme, the\nprimes of GP are assumed to be interpretable alone despite their\nsub-phonemic nature. \\cite{Harris95} call this GP assumption the\nautonomous interpretation hypothesis.\n\nThis hypothesis, and its testing, is at the core of our approach. One\nof the goals of this work is to provide an interpretation for\nsub-phonemic representational components of phonology that is grounded\nin the acoustic signal.\n\n\n\n\\subsection{Representing CMUbet with SPE and GP}\n\nTo characterise the phoneme inventory of American English in the SPE\nand the GP frameworks, we have adopted the approach of~\\cite{King00}\nwith some modifications. We use the reduced set of 39 phonemes\nin the CMUbet\nsystem\\footnote{\\url{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}}. The\nmajor difference regarding the SPE-style representations is our \naddition of a [rising] feature used to differentiate diphthongs from\nmonophthongs. In the original SPE framework, this difference was\ntreated with the [long] feature and the surface representation of\ndiphthongs was derived from long monophthongs through a rule. Given\nthe absence of the `rule module' in our approach to synthesis, we\nopted for a unified feature specification of the full vowel inventory\nof American English using the added feature. This allowed for\ndiphthongs to form a natural class with vowels rather than with glides\n{\\textipa}{[j, w]} as in~\\cite{King00}. Additional minor adjustments \n assured the uniqueness of a feature matrix for each\nphoneme. The full specification of all 39 phonemes with 14 binary\nfeatures is shown in Table~\\ref{tab:SPEatts}.\n\nThe set of GP-style specification for English phonemes can be seen in\nTable~\\ref{tab:GPatts}. Again, we followed King \\& Taylor,\nmostly in formalizing headedness with pseudo-features, which allows\nfor GP phoneme specifications that are comparable to SPE.\n\n\n\nAdditionally, the vowel specifications differ somewhat from King \\& Taylor\nstemming from our effort to approximate the phonetic characteristics\nof the vowels, and the differences among them, as closely as\npossible. For example, if the back lax {\\textipa}{[U]} is specified with E,\nthe same was employed for the front lax {\\textipa}{[I]}, or, the front quality\nof {\\textipa}{[\\ae]} was formalized with A heading I compared to King \\&\nTaylor's formalism with only non-headed A.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Phonological vocoding}\n\\label{sec:phonovoc}\nBoth SPE and GP phonological systems represent a phone by a\ncombination of phonological features. For example, a consonant \n{\\textipa}[j] is articulated using the mediodorsal part of the tongue\n[+Dorsal], in the motionless, mediopalatal, part of the vocal tract\n[+High], generated with simultaneous vocal fold vibration\n[+Voiced]. These three features then comprise the phonological\nrepresentation for {\\textipa}[j] in the SPE system.\n\n\\cite{Cernak15} have recently designed a phonological vocoder for a\nlow bit rate parametric speech coding as a cascaded artificial neural\nnetwork composed of speech analyser and synthesizer that use shared\nphonological speech representation. Figure~\\ref{fig:phonovocTRN} shows\na sequential processing of the vocoding, briefly introduced in the\nfollowing text.\n\n\\subsection{Phonological analysis}\n\n\n\\begin{figure}[!t]\n\\centering\n\\resizebox{4.5in}{!}{\n\\begin{tikzpicture}[font=\\scriptsize]\n  \n  \\draw[rounded corners, thick] (0,1.25) rectangle (1,1.75); \\node at\n  (0.5,1.5) {Speech};\n\n  \\draw[rounded corners, thick] (1.75,1) rectangle (3,2);\n  \\node[align=center] at (2.4,1.5) {Speech\\\\analysis};\n  \\draw[thick, ->] (1,1.5) -- (1.75,1.5);\n  \n  \\draw[thick, ->] (3,1.5) -- (4,1.5);\n  \\node[align=center] at (5,3.5) {\\textbf{Phonological analysers}};\n  \\draw[rounded corners, thick, fill=lightgray] (4,2.5) rectangle (6,3);\n  \\node[align=center] at (5,2.75) {Segmental};\n\n  \\draw[thick, <->] (4,2.75) -- (3.5,2.75) -- (3.5,0.25) -- (4,0.25);\n  \\draw[rounded corners, thick, fill=lightgray] (4,1) rectangle (6,2);\n  \\node[align=center] at (5,1.5) {Supra-\\\\segmental};\n  \\draw[thick] (6,2.75) -- (6.5,2.75) -- (6.5,0.25) -- (6,0.25);\n\n  \\draw[rounded corners, thick, fill=lightgray] (4,0) rectangle (6,0.5);\n  \\node[align=center] at (5,0.25) {Silence};\n  \n  \n  \n\n  \\draw[thick, ->] (6,1.5) -- (7.5,1.5); \\node[above] at (7,1.5)\n       {${\\mathbf{{z}}}_n$};\n\n  \\draw[rounded corners, thick, fill=lightgray] (7.5,1) rectangle (9.5,2);\n  \\node[align=center] at (8.5,1.5) {\\textbf{Phonological}\\\\\\textbf{synthesizer}};\n\n  \\draw[thick, ->] (9.5,1.8) -- (11,1.8); \\node[above] at (10.25,1.8)\n       {LSPs};\n\n  \\draw[thick, fill=black] (10,1.5) circle[radius=0.04];\n  \\draw[thick, fill=black] (10.25,1.5) circle[radius=0.04];\n  \\draw[thick, fill=black] (10.5,1.5) circle[radius=0.04];\n\n  \\draw[thick, ->] (9.5,1.2) -- (11,1.2); \\node[align=center] at (10.25,0.8)\n       {Glottal\\\\signal};\n\n  \\draw[rounded corners, thick] (11,1) rectangle (13,2);\n  \\node[align=center] at (12,1.5) {LPC\\\\re-synthesis};\n\n\\end{tikzpicture}\n}\n\\caption{The process of the phonological vocoding. The speech signal\n  is internally represented by phonological posterior probabilities\n  ${\\mathbf{{z}}}_n$, consisting of $K$ phonological posteriors per $n$-th\n  frame). The DNN-based synthesizer generates speech parameters --\n  line spectral pairs (LSPs) and source parameters for LPC speech\n  re-synthesis.}\n\\label{fig:phonovocTRN}\n\\end{figure}\n\nPhonological analysis starts with speech analysis that turns speech\nsamples into a sequence of acoustic feature observations\n$X=\\{{\\mathbf{{x}}}_1,\\ldots,{\\mathbf{{x}}}_n,\\ldots,{\\mathbf{{x}}}_N\\}$ where $N$ denotes\nthe number of frames in the speech signal. Conventional cepstral\ncoefficients can be used in this speech analysis step. Then, a bank of\nphonological analysers realised as neural network \nclassifiers converts the acoustic feature observation sequence $X$\ninto a sequence of vectors\n$Z=\\{{\\mathbf{{z}}}_1,\\ldots,{\\mathbf{{z}}}_n,\\ldots,{\\mathbf{{z}}}_N\\}$ where a vector\n${\\mathbf{{z}}}_n=[z_n^1,\\ldots,z_n^k,\\ldots,z_n^K]^T$ consists of $K$\nphonological posterior probabilities of phonological features.\n\n\n\n\n\nThe posteriors $z_n^k$ are probabilities that the $k$-th feature\noccurs (versus does not occur), and they compose the\ninformation which is transferred to the synthesizer's side in order to\nsynthesize speech as described in the next subsection. These\nposteriors can be optionally pruned (compressed) if there is need for\nreducing the amount of transferred information (i.e. reducing the\nbit rate). For example, the binary nature of the phonological features\nconsidered by~\\cite{Cernak15} allowed for using binary values of the\nphonological features instead of continuous values, which resulted\nonly in minimal perceptual degradation of speech quality.\n\n\n\n\n\nThere are two main groups of the phonological features:\n\\begin{itemize}\n  \\item segmental: phonological features that define the phonetic\n    surface of individual sounds,\n  \\item supra-segmental: phonological features at the timescales of\n    syllables and feet (including a single stressed syllable and one\n    or more unstressed ones.\n\\end{itemize}\nIn this work, we focus on the segmental phonological features and leave the supra-segmental for future research.\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Phonological synthesis}\n\nThe phonological synthesizer is based on a Deep Neural Network (DNN)\nthat learns the highly-complex regression problem of mapping\nposteriors $z_n^k$ to the speech parameters. More specifically, it\nconsists of two computational steps. The first step is a DNN forward\npass that generates the speech parameters, and the second one is a\nconversion of the speech parameters into the speech samples. For our\ncomputational platform, we selected an open-source LPC re-synthesis\nwith minimum-phase complex cepstrum glottal model\nestimation~\\citep{Phil15}. The modelled speech parameters are thus:\n\\begin{itemize} \n  \\item ${\\mathbf{{p}}}_n$: static Line Spectral Pairs (LSPs) of 24th order\n    plus gain, \n  \\item $\\log({\\mathbf{{r}}}_n)$: a Harmonic-To-Noise (HNR) ratio,\n  \\item and ${\\mathbf{{t}}}_n$, $\\log({\\mathbf{{m}}}_n)$: two glottal model\n    parameters -- angle $t$ and magnitude $\\log(m)$ of a glottal\n    pole.\n\\end{itemize}\n\n\n\n\n\nThe generated speech parameter vectors -- ${\\mathbf{{p}}}_n$,\n${\\mathbf{{t}}}_n$, $\\log({\\mathbf{{r}}}_n)$ and $\\log({\\mathbf{{m}}}_n)$ for $n$-th frame\n-- from the first computational step are smoothed using dynamic\nfeatures and pre-computed (global) variances~\\citep{Tokuda95}, and\nformant enhancement~\\citep{Ling2006} is performed to compensate for\nover-smoothing of the formant frequencies. Re-synthesised speech\nsamples ${\\mathbf{{y}}}_n$ are generated frame by frame followed with\noverlap-and-add, using Eq.~\\ref{eq:cepgm}: \n\n\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:cepgm}\n  {\\mathbf{{y}}}_n = \\underbrace{\\sum\\nolimits_{i=1}^P h({\\mathbf{{p}}}_n| {\\mathbf{{z}}}_n)\n    {\\mathbf{{y}}}_{n-i}}_\\textrm{spectra} + \\underbrace{h({\\mathbf{{t}}}_n, {\\mathbf{{m}}}_n,\n    {\\mathbf{{r}}}_n| {\\mathbf{{z}}}_n)}_\\textrm{source},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{{y}}}_{n}=\\underbrace{\\sum\\nolimits_{i=1}^{P}h({\\mathbf{{p}}}_{n}|{%&#10;\\mathbf{{z}}}_{n}){\\mathbf{{y}}}_{n-i}}_{\\textrm{spectra}}+\\underbrace{h({%&#10;\\mathbf{{t}}}_{n},{\\mathbf{{m}}}_{n},{\\mathbf{{r}}}_{n}|{\\mathbf{{z}}}_{n})}_{%&#10;\\textrm{source}},\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc32</mi><mi>n</mi></msub><mo>=</mo><mrow><munder><munder accentunder=\"true\"><mrow><msubsup><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo movablelimits=\"false\">=</mo><mn>1</mn></mrow><mi>P</mi></msubsup><mi>h</mi><mrow><mo movablelimits=\"false\" stretchy=\"false\">(</mo><msub><mi>\ud835\udc29</mi><mi>n</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">|</mo><msub><mi>\ud835\udc33</mi><mi>n</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">)</mo></mrow><msub><mi>\ud835\udc32</mi><mrow><mi>n</mi><mo movablelimits=\"false\">-</mo><mi>i</mi></mrow></msub></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mtext>spectra</mtext></munder><mo>+</mo><munder><munder accentunder=\"true\"><mrow><mi>h</mi><mrow><mo movablelimits=\"false\" stretchy=\"false\">(</mo><msub><mi>\ud835\udc2d</mi><mi>n</mi></msub><mo movablelimits=\"false\">,</mo><msub><mi>\ud835\udc26</mi><mi>n</mi></msub><mo movablelimits=\"false\">,</mo><msub><mi>\ud835\udc2b</mi><mi>n</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">|</mo><msub><mi>\ud835\udc33</mi><mi>n</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">)</mo></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mtext>source</mtext></munder></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05991.tex", "nexttext": "\nThe speech samples ${\\mathbf{{a}}}^k$ of the $k$-th phonological atom are\nthen generated by  Eq.~\\ref{eq:cepgm}, with a repeated sequence of\n$z_n^k$ at the input of the phonological synthesizer.\n\nFinally, the composition of the speech sounds, the phones, driven by\nthe canonical phone representation, is done as follows:\n\n", "itemtype": "equation", "pos": 33856, "prevtext": "\nwhere $P$ is the LPC order, $h(.)$ denotes a non-linear activation\nfunction (forward propagation) of the trained phonological\nsynthesizer, and ${\\mathbf{{z}}}_n$ are phonological posteriors for frame\n$n$.\n\nLPC re-synthesis can be done either with synthesised or original pitch\nfeatures.\n\n\n\\section{Experimental setup for laboratory phonology}\n\\label{sec:experiments}\n\n\nIn this section, the experimental protocol of the phonological\nanalysis and synthesis is described. In Table~\\ref{tab:data}, the data\nused in these experiments \nare presented. The analyser is trained on the Wall Street Journal\n(WSJ0 and WSJ1) continuous speech recognition corpora \\citep{WSJDB}.\nThe \\textit{si\\_tr\\_s\\_284} set of 37,514 utterances was used, split\nto 90\\% training and 10\\%  cross-validation sets. The synthesizer is\ntrained and evaluated on an English audiobook ``Anna Karenina'' of\nLeo\nTolstoy\\footnote{\\tiny\\url{https://librivox.org/anna-karenina-by-leo-tolstoy-2/}},\nthat is around 36 hours long. Recordings were organised into 238\nsections, and we used sections 1--209 as a training set, 210--230 as a\ndevelopment set and 231--238 as a testing set. The development and\ntesting sets were 3 hours and 1 hour long, respectively.\n\n\\begin{table} [th]\n  \\caption{\\label{tab:data} {\\it Data used for training the\n  phonological analysis and synthesis.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|c|c|c|c|}\n\\hline \nUse of data & Database & Set/Section & Size \\\\\n\\hline  \\hline\nAnalyser (train. set) & WSJ & si\\_tr\\_s\\_284 & 33.765 (utts) \\\\\nAnalyser (cross-val. set) & WSJ & si\\_tr\\_s\\_284 & 3.749 (utts) \\\\\nSynthesizer (train. set) & Tolstoy & 1-209 & 36 (hours) \\\\\nSynthesizer (dev. set) & Tolstoy & 210-230 & 3 (hours) \\\\\nSynthesizer (test set) & Tolstoy & 231-238 & 1 (hour) \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\\subsection{Phonological analysis}\n\nThe analyser is based on a bank of phonological analysers\nrealised as neural network classifiers -- multilayer perceptrons\n(MLPs) -- that estimate phonological posterior probabilities. Each MLP\nis designed to classify a binary phonological feature. \n\n\n\n\n\n\n\n\n\nFirst, we trained a phoneme-based automatic speech recognition system\nusing Perceptual Linear Prediction (PLP) features. The phoneme set\ncomprising of 40 phonemes (including ``sil'', representing silence)\nwas defined by the CMU pronunciation dictionary. The three-state,\ncross-word triphone models were trained with the HTS variant of\n~\\cite{Zen:HTS} of the HTK toolkit on the 90\\% subset of the\n\\textit{si\\_tr\\_s\\_284} set. The remaining 10\\% subset was used for\ncross-validation. We tied triphone models with decision tree state\nclustering based on the minimum description length (MDL)\ncriterion~\\citep{shinoda:mdl}. The MDL criterion allows an\nunsupervised determination of the number of states. In this study, we\nobtained 12,685 states and modeled each state with a Gaussian mixture\nmodel (GMM) consisting of 16 Gaussians.\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Alignment}\n\nA bootstrapping phoneme alignment was obtained using forced\nalignment with cross-word triphones. The bootstrapping alignment was\nused for the training of the bootstrapping MLP, using as the input 39\norder PLP features with the temporal context of 9 successive frames,\nand a softmax output function. The architecture of the MLP, 3-hidden\nlayers 351x2000x500x2000x40 (input 351=39x9 and output 40 is the\nnumber of phonemes), was determined  empirically. Using a hybrid\nspeech decoder fed with the phoneme posteriors, the re-alignment was\nperformed. After two iterations of the MLP training and re-alignment,\nthe best phoneme alignment of the speech data was obtained. This\nre-alignment increased the cross-validation accuracy of the MLP\ntraining from 77.54\\% to 82.36\\%.\n\n\\subsubsection{Training of the bank of phonological analysers}\n\nThe representation given in \\ref{sec:spefeatures} was used to\nmap the phonemes of the best alignment to phonological features \nfor the training of the\nanalysers. $K$ analysers were trained with the frame alignment having\ntwo output labels, the $k$-th phonological feature occurs for the\naligned phoneme or not. The analysis MLPs were finally trained with\nthe same input features as used for the alignment MLP, and the same\nnetwork architecture except for 2 output units instead of 40. The\noutput that encodes the phonological feature occurrence is used as the\nphonological posterior probability ${\\mathbf{{z}}}_n$.\n\nTables~\\ref{tab:accuracies}, \\ref{tab:SPEaccuracies}, and\n\\ref{tab:eSPEaccuracies} show classification accuracies at frame\nlevel of the GP, SPE and eSPE MLPs for the ``feature occurs'' output,\nrespectively. The accuracies are reported for the training and\ncross-validation sets. The analysers performances are high, with an\naverage cross-validation training accuracy of 95.5\\%, 95.6\\% and\n96.3\\%, respectively.\n\n\\begin{table} [t]\n  \\caption{\\label{tab:accuracies} {\\it Classification accuracies (\\%)\n      of the GP prime analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|c|c|c|[0.8pt]c|c|c|}\n\\hline\nPrime & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Prime &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\n & train & cv & & train & cv \\\\\n\\hline \\hline\nA & 92.3 & 91.7 & a & 97.9 & 97.6 \\\\\nI & 94.9 & 94.6 & i & 96.1 & 96.4 \\\\\nU & 94.4 & 94.1 & u & 97.5 & 97.7 \\\\\nE & 92.6 & 91.9 & H & 95.4 & 95.0 \\\\\nS & 95.2 & 94.7 & N & 98.2 & 98.1 \\\\\nh & 95.9 & 95.4 & sil & 99.0 & 98.9  \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\\begin{table} [t]\n  \\caption{\\label{tab:SPEaccuracies} {\\it Classification accuracies\n      (\\%) of the SPE analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}} \n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nNatural & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Natural &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nclasses & train & cv & classes & train & cv \\\\\n\\hline \\hline\nvocalic & 96.0 & 95.8 & round & 97.8 & 97.7 \\\\\nconsonantal & 94.5 & 93.9 & tense & 94.8 & 94.1 \\\\\nhigh & 94.8 & 94.4 & voice & 94.6 & 94.4 \\\\\nback & 93.9 & 93.4 & continuant & 95.6 & 95.2 \\\\\nlow & 97.4 & 97.1 & nasal & 98.1 & 98.0 \\\\\nanterior & 94.4 & 94.0 & strident & 97.8 & 97.6  \\\\\ncoronal & 94.3 & 93.9 & sil & 99.0 & 98.9 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n\n\\begin{table} [t]\n  \\caption{\\label{tab:eSPEaccuracies} {\\it Classification accuracies (\\%)\n      of the eSPE analysers at frame level on cross-val. set\n      of \\textit{si\\_tr\\_s\\_284} set.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nNatural & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Natural &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nclasses & train & cv & classes & train & cv \\\\\n\\hline \\hline\nvowel & 94.7 & 94.3 & low & 97.5 & 97.2 \\\\\nfricative & 97.3 & 97.0 & mid & 94.5 & 94.0 \\\\\nnasal & 98.2 & 98.1 & retroflex & 98.6 & 98.4 \\\\\nstop & 96.7 & 96.4 & velar & 98.9 & 98.8 \\\\\napproximant & 97.2 & 96.9 & anterior & 94.8 & 94.3 \\\\\ncoronal & 94.8 & 94.4 & back & 94.2 & 93.8  \\\\\nhigh & 94.6 & 94.2 & continuant & 95.8 & 98.4 \\\\\ndental & 99.3 & 99.2 & round & 95.3 & 94.9 \\\\\nglottal & 99.7 & 99.7 & tense & 91.4 & 90.7 \\\\\nlabial & 97.6 & 97.4 & voiced & 95.2 & 94.9 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Phonological synthesis}\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Training}\n\\label{sec:training}\n\nThe speech signals from the training and cross-validation sets of the\nTolstoy database, down-sampled to 16 kHz, framed by 25-ms windows with\n10-ms frame shift, were used for extracting both DNN input and\noutput features. The input features, phonological posteriors\n${\\mathbf{{z}}}_n$, were generated by the phonological analyser trained on the\nWSJ database. The temporal context of 11 successive frames resulted in\ninput feature vectors of 132 ($12*11*1$), 165 ($15*11*1$) and 231\n($21*11*1$) dimensions, for the GP, SPE and eSPE schemes,\nrespectively. The output features, the LPC speech parameters, were \nextracted by the Speech Signal Processing (SSP) python\ntoolkit\\footnote{\\tiny\\url{https://github.com/idiap/ssp}}. We used\nstatic speech parametrization of 29th order along with its dynamic\nfeatures, altogether of 87th order. \n\nCepstral mean normalisation of the input features was\napplied before DNN training. The DNN was initialised using\n$(K*11)$x1024x1024x1024x1024 Deep Belief Network pre-training by\ncontrastive divergence with 1 sampling step\n(CD1)~\\citep{Hinton06}. The DNN with a linear output function was then\ntrained using a mini-batch based stochastic gradient descent algorithm\nwith mean square error cost function of the KALDI\ntoolkit~\\citep{PoveyASRU2011}. The DNN had 3.4 million parameters.\n\n\\subsubsection{Synthesis}\n\nThe test set of the Tolstoy database was used for the synthesis. There\nare three options to generate a particular speech sound: (i) by $Z$\ninferred from the speech, (ii) by $Z$ inferred from the text, or (iii)\nby compositional speech modelling.\n\n\n\n\n\n \n\n\n\nIn the two first cases, we refer to this generation\nas \\textbf{network-based}. The speech parameters generated by the\nforward DNN pass are smoothed using dynamic features and pre-computed\n(global) variances, and formant enhancement is performed \n\nto mitigate over-smoothing of the formant frequencies. \n\n\n\nIn the latter case, speech sounds are generated using compositional\nphonological speech models, introduced next in\nSection~\\ref{sec:soundsofprimes}. Briefly, we set a single\nphonological feature as an active input and the rest are set to\nzeros. This generates an artificial audio sound that characterises the\ninput phonological feature. Then, the speech sound is generated by the\ncomposition (audio mixing) of the particular artificial audio\nsounds. We refer to this process as \\textbf{compositional-based}. We\nwanted a) to show that compositional phonological speech modelling is\npossible, b) to test the suitability of speech sound synthesis without\nthe DNN, i.e., only by mixing  the artificial phonological sound\ncomponents, and c) to allow the reader to experiment with the sound\ncomponents that are embedded in this manuscript.\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application examples for laboratory phonology}\n\\label{sec:examples}\nIn this section we show how to use the phonological vocoder, described\nin Section~\\ref{sec:experiments}, for a) compositional phonological\nspeech modelling, b) a comparison of phonological systems and c) for\nparametric phonological TTS system.\n\n\n\\subsection{Compositional phonological speech models}\n\\label{sec:soundsofprimes}\n\n\n\n\\cite{Virtanen15} investigate the constructive compositionality of\nspeech signals, i.e., representing the speech signal as non-negative\nlinear combinations of atomic units (``atoms''), which themselves are\nalso non-negative to ensure that such a combination does not result in\nsubtraction or diminishment. The power of the sum of uncorrelated\natomic signals in any frequency band is the sum of the powers of the  \nindividual signals within that band. The central point is to define\nthe sound atoms that are used as the compositional models.\n\nFollowing this line of research, we hypothesise that the acoustic\nrepresentation of the phonological features, produced by a\nphonological vocoder, forms a set of speech signal atoms (the\nphonological sound components) that define the phones. We\ncall these sound components \\textbf{phonological atoms}.\n\n\n\n\nIt is possible to generate the atoms for any phonological\nsystem. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven our hypothesis above (that these phonological atoms form a set\nof acoustic templates that might be taken to define speech acoustic\nspace), we created artificial phonological atom representations $Z$ \nwhere ${\\mathbf{{z}}}_n$  consists of $K$ binary phonological posteriors\ndefined as follows:\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:sod}\nz_n^k = \\left\\{\n  \\begin{array}{l l}\n    1 & \\quad \\text{for $k \\in {1,2,\\ldots,K}$}\\\\\n    0 & \\quad \\text{for $k \\notin {1,2,\\ldots,K}$}\\\\\n  \\end{array} \\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"z_{n}^{k}=\\left\\{\\begin{array}[]{l l}1&amp;\\quad\\text{for $k\\in{1,2,\\ldots,K}$}\\\\&#10;0&amp;\\quad\\text{for $k\\notin{1,2,\\ldots,K}$}\\\\&#10;\\end{array}\\right.\" display=\"block\"><mrow><msubsup><mi>z</mi><mi>n</mi><mi>k</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mpadded lspace=\"10pt\" width=\"+10pt\"><mrow><mtext>for\u00a0</mtext><mrow><mi>k</mi><mo>\u2208</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow></mpadded></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mpadded lspace=\"10pt\" width=\"+10pt\"><mrow><mtext>for\u00a0</mtext><mrow><mi>k</mi><mo>\u2209</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow></mpadded></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05991.tex", "nexttext": "\nwhere ${\\mathbf{{y}}}_n$ is a composition of $S$ (a particular subset of $K$\nphonological features) atoms ${\\mathbf{{a}}}^s$, and $w_n^s$ are weights of\nthe composition.\n\n\n\n\n\n\n\nAs an example, Table~\\ref{tab:SOPaudio} (in \\ref{sec:samples}) demonstrates recordings\n\n\nof individual\natoms of the GP system. We generated the phonological atoms also for\nthe SPE and eSPE phonological systems and used all of them in the\nfollowing experiments.\n\n\\subsection{Comparison of the phonological systems}\n\nWe start with context-independent vocoding in\nSection~\\ref{sec:civ}, i.e., the vocoding of isolated speech sounds,\nand continue with context-dependent vocoding in Section~\\ref{sec:cdv}.\n\n\\subsubsection{Context-independent vocoding}\n\\label{sec:civ}\n\nThe aim of this subsection is to objectively evaluate the phonological\nsystems in respect to context-independent vocoding, i.e., the\nability of phonological systems to produce isolated speech sounds.\n\n\nIn order to achieve this, instead of inferring the posteriors from the\nspeech, we generated the canonical phonological posteriors, i.e., we\nused only the features that represent the specific isolated phones\n(rows of Tables \\ref{tab:GPatts}, \\ref{tab:SPEatts}, and\n\\ref{tab:eSPEatts}). Original speech, manually phonetically labelled\n76 utterances from the audiobook test set, was used as the reference\nfor the comparison.\n\n\n\n\n\n\nHaving the phoneme boundaries, we extracted 25ms windows\nfrom the central (stationary) part of the phones, to obtain human\nspoken acoustic references.\n\n\n\n\nFinally, we used the Mel Cepstral Distortion (MCD)~\\citep{Kubichek93}\nto calculate perceptual acoustic distances between the vocoded phones\nand the spoken references, a confusion matrix $C_{vocoded}$.\n\n\n\n\n\n\n\n\nThe MCD values are in dB and higher values mean more confused\nphones. In order to visually compare confusions introduced by the\ndifferent phonological representations, we normalised $C_{vocoded}$ by\nconfusions between the spoken references only, a confusion matrix\n$N$:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 34386, "prevtext": "\nThe speech samples ${\\mathbf{{a}}}^k$ of the $k$-th phonological atom are\nthen generated by  Eq.~\\ref{eq:cepgm}, with a repeated sequence of\n$z_n^k$ at the input of the phonological synthesizer.\n\nFinally, the composition of the speech sounds, the phones, driven by\nthe canonical phone representation, is done as follows:\n\n", "index": 5, "text": "\\begin{equation}\n  \\label{eq:fusion}\n  {\\mathbf{{y}}}_n=\\frac{1}{S}\\sum_{s=1}^S w_n^s a_n^s,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{{y}}}_{n}=\\frac{1}{S}\\sum_{s=1}^{S}w_{n}^{s}a_{n}^{s},\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc32</mi><mi>n</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><msubsup><mi>w</mi><mi>n</mi><mi>s</mi></msubsup><mo>\u2062</mo><msubsup><mi>a</mi><mi>n</mi><mi>s</mi></msubsup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05991.tex", "nexttext": "\n\n\n\nwhere $\\odot$ stands for element-wise matrix multiplication (the\nHadamard product), $\\oslash$ stands for element-wise matrix division,\nand max and min functions stand for column-wise max and min functions \nrespectively and are used to get linear scaling factor\nof a range of values between $<0,1>$. The confusion matrix\n$C_{natural}$ is thus a scaled matrix $N$, having a range of values\nbetween $<1,2>$, where $1$ refers to no confusion.\n\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:human-mcd} shows the scaled confusion matrix\n$C_{natural}$.\n\n\n\n\n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=.73\\linewidth]{figs/humanCM-crop}\n  \\caption{{\\it Scaled confusion matrix $C_{natural}$ of the spoken\n      acoustic references.}}\n\\label{fig:human-mcd}\n\\end{figure}\n \nFigures~\\ref{fig:GPSoundTests}, \\ref{fig:SPESoundTests}\nand~\\ref{fig:eSPESoundTests} show normalised confusion matrices\n$C_{norm}$ of vocoded context-independent phones and the spoken\nacoustic references of the same speaker, for the GP, SPE and eSPE\nphonological systems, respectively. The diagonal elements of the\nconfusion matrices represent an acoustical distance (dissimilarity) of\nvocoded and spoken phones. If the phonological features represent\nspeech well, the matrices show only strong diagonals. The missing\ndiagonal values, the higher confusions between spoken and vocoded\nphones, imply errors of the phonological speech representation. These\nerrors probably stem from wrongly assigned phonological features to\nphones (given in \\ref{sec:spefeatures}) during the training of the\nphonological analysis.\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/GP-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:GPsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/GP-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:GPsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of  GP vocoded and spoken phones.}}\n\\label{fig:GPSoundTests}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/SPE-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:SPEsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/SPE-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:SPEsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of SPE vocoded and spoken phones.}}\n\\label{fig:SPESoundTests}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/eSPE-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:eSPEsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/eSPE-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:eSPEsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of eSPE vocoded and spoken phones.}}\n\\label{fig:eSPESoundTests}\n\\end{figure}\n\nThe figures show two aspects of the evaluation. In the first, the\nconfusion matrices of (a) the network-based and (b) the\ncompositional-based phonological synthesis are shown.\n\n\n\n\n\n\n\nThe network- and compositional-based synthesis differ in the way\nfeatures are combined. In the first case, the DNN inputs are combined\nso as to generate specific phones. In the second case, phonological\natoms of the specific phone are combined in order to generate\ncompositional phones. We can therefore  consider this as two different\nevaluation metrics, hypothesising that both contribute partially to a\nfinal evaluation. In both cases, the ideal performance is to have dark\ndiagonals, i.e., the lowest acoustic distance between the spoken\nreference and vocoded phones. Table~\\ref{tab:diagonals} shows the\naverages of the diagonal MCD values of the confusion matrices.\n\n\\begin{table} [htb]\n  \\caption{\\label{tab:diagonals} {\\it The average MCD values given in\n      [dB] of the diagonals of the confusion matrices shown in\n      Figures~~\\ref{fig:GPSoundTests}, \\ref{fig:SPESoundTests}\n      and~\\ref{fig:eSPESoundTests}.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabular}{|r|c|c|c|}\n\\hline\nSystem & Network & Compositional \\\\\n\\hline\nGP & 8.12 & 8.67 \\\\\n\\hline\nSPE & 7.79 & 8.77 \\\\\n\\hline\neSPE & 7.87 & 8.79 \\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\n\nFor all three phonological systems, the results of\ncompositional-based context-independent vocoding show greater errors\n(i.e., higher values in Table~\\ref{tab:diagonals}) than the ones of\nnetwork-based vocoding. This might be caused by the fact that\nthe composition of the phonological atoms is a linear operation (as shown\nin Section~\\ref{sec:soundsofprimes}), and it is an approximation to a\nnon-linear function that is modelled by network-based phonological\nsynthesis. Additionally, the types of reported confusions in the\nnetwork-based vocoding that are missing in the compositional-based one\nmake sense phonetically. For example, in the left panel of\nFigure~\\ref{fig:GPsoundsPruned}, phonetically very similar [{\\textipa}{Z}],\n[{\\textipa}{S}], [{\\textipa}{dZ}], [{\\textipa}{tS}] with voicing and closure being highly\ncontext dependent are confused. Nevertheless, compositional-based\nvocoding tends to produce ``tighter'' confusions in all three\nframeworks. For example, while [{\\textipa}{D}] in the left panel of\nFigure~\\ref{fig:GPsoundsPruned} shows confusions with nasals, voiced\nplosives and [{\\textipa}{v}], it shows only minor confusions with labials in\nthe right panel.\n\n\n\nIn the second aspect of the evaluation, the three phonological systems\n(GP, SPE and eSPE) are compared among\nthemselves using network-based synthesis. In\nFigures~\\ref{fig:GPsoundsPruned}, \\ref{fig:SPEsoundsPruned}, and\n\\ref{fig:eSPEsoundsPruned}, we see different error patterns. In all\nthree phonological systems the biggest confusions are shown with the\nnasals [{\\textipa}{m n N}]. GP in addition produces confusions of the\nconsonants and vowels, such as for  nasals. SPE seems to represent\nspeech better, namely for vowel [{\\textipa}{A}] and glide [{\\textipa}{w}], and\nsuppresses most of the vowel-consonant confusions. On the other hand,\nit fails with proper [{\\textipa}{dZ}] vocoding. We speculate that phone\nfrequency in the evaluation data may be one of the causes of these\nerrors; for example, the phone [{\\textipa}{dZ}] was the least frequent one in\nour data.\n\n\n\nFinally, according to our data, eSPE further improves on the SPE\nspeech representation. It generates fewer confusions in the vowel\nspace, and also in the consonant space, for example in the voiced\nstops class [{\\textipa}{b d g}].\n\n\n\n\\subsubsection{Context-dependent vocoding}\n\\label{sec:cdv}\n\nThe previous experiment evaluated the vocoding of the isolated\nsounds, using canonical posteriors ${\\mathbf{{z}}}_n$. We continued with the\nevaluation of continuous speech vocoding using ${\\mathbf{{z}}}_n$ inferred\nfrom the reference speech signals. Network-based phonological\nsynthesis was used for the following experiments.\n\n\n\n\n\n\n\nIn this evaluation, we were interested if the segmental errors found\nin context-independent vocoding impact the context-dependent\nvocoding. We employed an ABX subjective evaluation listening\ntest~\\citep{Grancharov2008}, suitable for comparing two different\nsystems. In this test, listeners were presented with\npairs of samples produced by two systems (A and B) and for each pair\nthey were indicating their preference for A, B, or neither of the two\n(X). The material for the test consisted of 16 pairs of sentences such\nthat one member of the pair was generated using the GP-based vocoder\n(system A) and the other member was generated using the eSPE-based\nvocoded speech (system B).\n\nRandom utterances from the test set of the Tolstoy database were used\nto generate the vocoded speech.\nWe chose these two systems because they displayed the greatest\ndifferences in context-independent results. The subjects for the ABX\ntest were 37 listeners, roughly equally pooled from experts in speech\nprocessing on the one hand, and completely naive subjects on the other\nhand. The subjects were presented with pairs of sentences in a random\norder with no indication of which system they represented with. They were\nasked to listen to these pairs of sentences (as many times as they\nwanted), and choose between them in terms of their overall\nquality. Additionally, the option X, i.e. \\emph{both samples sound the\nsame}, was available if they had no preference for either of them. To\ndecrease the demands on the listeners, we divided the material to\ntwo different sets, each consisting of 8 paired sentences randomly\nselected from the test set. The first set was presented to 19\nlisteners, and the second set to 18 different listeners.\n\n\\begin{figure}[h]\n\\centering\n\n\n\\includegraphics[width=.8\\linewidth]{figs/ABX-crop}\n\\caption{ABX subjective evaluation listening test between the GP-based\nand the eSPE-based vocoded speech}\n\\label{fig:SubEvalGP_eSPE}\n\\end{figure}\n\nIn Figure~\\ref{fig:SubEvalGP_eSPE} the results of the ABX test for the\nGP and eSPE phonological systems, are shown. As can be seen, the\neSPE-based phonological vocoder outperforms the GP one by 36.8\\%\ncompared with 18.1\\% preference score in the ABX test. Even though\nthere is a preference of the listeners towards the eSPE-based system\n(double preference percentage), it is clearly shown that with a very\nhigh percentage, 45\\%, the two systems are perceived by the listeners\nas having the same overall quality. It should be pointed out that a\n$t$-test confirmed that this difference between the GP-based and\neSPE-based phonological vocoders is statistically significant ($p <\n0.01$). \n\nWe hypothesize that the preference for eSPE is linked to\ngreater perceptual clarity of individual phones. Subsequent auditory\nand visual analyses of the sample sentences and their generated\nacoustic signals suggest that eSPE sentences displayed longer closures\nfor plosives, stronger plosive releases, and also slightly greater\ndisjunctures at some word boundaries.\n\n\nThese features correspond to a decreased overlap of sounds,\ni.e. decreased coarticulation, commonly present in hyper-articulated\nor clearly enunciated speech.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Experimental parametric phonological TTS}\n\\label{sec:ptts}\n\nIn this section we show how compositional phonological speech models\ncould be combined to generate arbitrary speech sounds, and how to\nsynthesise continuous speech from the canonical phonological\nrepresentation.\n\n\\subsubsection{Generation of sounds from unseen language}\n\nIn this experiment, we arbitrarily selected the GP system to demonstrate\nthe phonological composition of new speech sounds. \\cite{Harris94}\nclaims that fusing and splitting of primes accounts for phonological\ndescription of the sound. We selected the phonological rule number 29\n[I, U, E] $\\rightarrow$ {\\textipa}{y}, and  [A, I, U, E] $\\rightarrow$\n{\\textipa}{\\oe}, of \\cite{Harris94} and tried to synthesise non-English\nsounds by the composition of involved phonological atoms. According to\nSection~\\ref{sec:soundsofprimes}, we claim that new sounds can be\ngenerated by time-domain mixing of the corresponding atoms\n${\\mathbf{{a}}}^s$. Table~\\ref{tab:SOPcombaudio} demonstrates the synthesis of\nstandard German sounds [{\\textipa}{y}] and [{\\textipa}{\\oe}] from English\nphonological atoms ${\\mathbf{{a}}}^s$, generated as in\nEq.~\\ref{eq:fusion}. For $w_n^s=1$, it can be done easily with\navailable free tools, e.g.:\n\\begin{itemize}\n  \\item [[{\\textipa}{y}]] :  \\texttt{sox -m I.wav U.wav E.wav y.wav}\n  \\item [[{\\textipa}{\\oe}]] : \\texttt{sox -m A.wav I.wav U.wav E.wav oe.wav}\n\\end{itemize}\n\nWe performed a formant analysis of all phonological atoms, and concluded\nthat they contain the same number of formants as human speech sounds\n(i.e., 4 in the 5 kHz bandwidth). In addition, the combined sounds\nalso contain the proper number of formants. The first two formants play a\nmajor perceptual role in distinguishing different English\nvowels~\\citep{Ladefoged14}, and Figure~\\ref{fig:formants} shows\nF1 and F2 of [{\\textipa}{\\oe}] phone from Table~\\ref{tab:SOPcombaudio}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=.75\\linewidth]{figs/formants-crop}\n\\caption{First two formants of involved phonological atoms and their\n  composition -- a phone [{\\textipa}{\\oe}].}\n\\label{fig:formants}\n\\end{figure}\n\nThe composition of Eq.~\\ref{eq:fusion} represents a static mixing of\n$S$ phonological atoms, i.e., it cannot be applied to model\nco-articulation. To include co-articulation into the synthesis, the\nphonological synthesizer has to be used. As it was trained with the\ntemporal context of 11 successive frames, around 50 ms before and 50\nms after the current processing frame, it learnt how speech parameters\nchange with trajectories of the phonological posteriors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results of this experiment support the hypothesis mentioned in\nSection  \\ref{sec:soundsofprimes}, that phonological atoms may define\nthe phones. In addition,  Section \\ref{sec:civ} demonstrates that\ncompositional-based phonological synthesis works well also for the SPE\nand eSPE phonological systems.\n\n\n\n\n\n\n\n\\begin{figure}[!th]\n\\centering\n\\resizebox{4in}{!}{\n\\begin{tikzpicture}[font=\\scriptsize]\n  \n  \\draw[rounded corners, thick] (0,0) rectangle (2,1.5);\n  \\node[align=center] at (1,0.75) {Text to\\\\phonological\\\\features};\n\n  \\draw[thick, ->] (2,0.75) -- (3.5,0.75); \\node[above] at (2.75,0.75)\n       {${\\mathbf{{z}}}_n$};\n  \\draw[rounded corners, thick, fill=lightgray] (3.5,0) rectangle (5.5,1.5);\n  \\node[align=center] at (4.5,0.75) {\\textbf{Phonological}\\\\synthesizer};\n\n  \\draw[thick, ->] (5.5,1.1) -- (7,1.1); \\node[above] at (6.25,1.1)\n       {LSPs};\n\n  \\draw[thick, fill=black] (6,0.9) circle[radius=0.04];\n  \\draw[thick, fill=black] (6.25,0.9) circle[radius=0.04];\n  \\draw[thick, fill=black] (6.5,0.9) circle[radius=0.04];\n\n  \\draw[thick, ->] (5.5,0.35) -- (7,0.35); \\node[align=center] at (6.25,0.35)\n       {Glottal\\\\signal};\n\n  \\draw[rounded corners, thick] (7,0) rectangle (9,1.5);\n  \\node[align=center] at (8,0.75) {LPC\\\\re-synthesis};\n  \\draw[thick, ->] (9,0.75) -- (10.7,0.75); \\node[above] at (9.9,0.75)\n       {Synth. ${\\mathbf{{y}}}_n$};\n\\end{tikzpicture}\n}\n\\caption{Phonological TTS synthesis. Speech parameters, the speech\n  line spectral pairs LSPs and source parameters, are generated by the\n  DNN. Speech samples are generated by subsequent LPC re-synthesis.}\n\\label{fig:phonovocTTS}\n\\end{figure}\n\n\\subsubsection{Continuous speech synthesis}\n\nExperimental parametric phonological TTS can be designed by a\nsimplistic text processing front end: text $\\rightarrow$ phonemes\n$\\rightarrow$ phonological features. Figure~\\ref{fig:phonovocTTS}\nshows the TTS process with the phonological synthesis. The binary\nphonological representation to be synthesized is obtained again from\nthe canonical phone representation.\n\nTo demonstrate the potential of our parametric phonological TTS\nsystem, we randomly selected three utterances from a \\texttt{slt}\nsubset of the CMU-ARCTIC speech database~\\citep{Kominek2004}, and used\ntheir text labels to generate continuous speech. Specifically, we used\nthe phoneme symbols along with their durations from the forced-aligned\nfull-context labels provided with the database, and mapped it to the\nphonological representation. Then we synthesised the sentences using\nthe already trained phonological synthesizers as described in\nSection~\\ref{sec:training}.\n\nTable~\\ref{tab:SPEaudio} lists recordings that demonstrates speech\nsynthesis from the phonological speech representation. The example\n\\texttt{a0453} illustrates how the phonological\nvocoder learns the context. Figure~\\ref{fig:a543-examples} visualises\nthe generated GP and eSPE examples. The phoneme sequence of the first\nword is {\\textipa}{[eI t i n]}, while the synthesised sequences using both\nphonological systems are rather {\\textipa}{[eI tS i n]}. The substitution of\n{\\textipa}{[t]} by {\\textipa}{[tS]} illustrates the assimilation of the place of\narticulation in the synthesised phoneme {\\textipa}{[t]}. If {\\textipa}{[t]} starts a\nstressed syllable and is followed by {\\textipa}{[i]} this alveolar stop is\naspirated and commonly more palatal due to coarticulation with the\nfollowing vowel. The acoustic result of a release burst when the\ntongue is in the alveo-palatal region is similar to the frication\nphase of alveo-palatal affricate {\\textipa}{[tS]}.\nWe conclude that the phonological synthesizer learns some contextual\ninformation because of using the temporal window of 11 successive\nframes -- around 100 ms of speech, that may correspond to the formant\ntransitions and differences in voice onset times. This is probably\nenough to learn certain aspects of co-articulation well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-orig.png}\n  \\caption{The natural speech of the arctic\\_a0453 example.}\n  \\label{fig:a543-orig}\n\\end{subfigure}\n\\\\\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-GP.png}\n  \\caption{The example generated with the GP features.}\n  \\label{fig:a543-GP}\n\\end{subfigure}\n\\\\\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-eSPE.png}\n  \\caption{The example generated with the eSPE features}\n  \\label{fig:a543-eSPE}\n\\end{subfigure}\n\\caption{{\\it Visualisation of vocoded arctic\\_a0453 examples:\n    ``Eighteen hundred, he calculated.''. The GP vocoding seems to\n    better synthesise the higher frequencies such as for fricatives,\n    whereas eSPE vocoding seems to synthesise stronger formant\n    frequencies. The recordings are available in \\ref{sec:samples}.}}\n\\label{fig:a543-examples}\n\\end{figure}\n\n\\paragraph{Subjective Intelligibility Test}\n\n\n\n\n\n\nWe compared the phonological TTS with a conventional hidden Markov\nmodel (HMM) parametric speech synthesizer, trained on the same\ntraining set of the audiobook which was used for training the\nphonological vocoder. For building the HMM models, the HTS V.2.1\ntoolkit~\\citep{HTS2010} was used. Specifically, the implementation\nfrom the EMIME project~\\citep{Wester2010} was taken. Five-state,\nleft-to-right, no-skip HSMMs were used. The speech parameters which\nwere used for training the HSMMs were 39 order mel-cepstral\ncoefficients, log-F0 and 21-band aperiodicities, along with their\ndelta and delta-delta features, extracted every 5 ms.\n\n\n\n\nFor evaluating the phonological TTS, an intelligibility test\nwas conducted using semantically unpredictable sentences (SUSs). Two\nsets of sentences were used in this test. \nEach set contained 14 unique SUSs.\nThe SUSs were taken from SIWIS project\\footnote{Spoken Interaction with Interpretation in Switzerland\n(SIWIS), https://www.idiap.ch/project/siwis/downloads/siwis-database}. \nThe length of the sentences varied from 6 to 8 words. Each set\nconsisted of 7 sentences synthesised by the phonological TTS, and\nanother 7 ones synthesised by the reference HTS system.\n\nTwenty native English speakers, experts in the speech processing\nfield, participated in the listening test.\n\n\nThe listeners could listen to each synthesized sentence\nonly one or two times, and were asked to transcribe the audio. Eleven and\nnine listeners respectively participated in the two sets of the\nlistening test.\n\n\n\n\nIntelligibility score was calculated by:\n\n", "itemtype": "equation", "pos": 36513, "prevtext": "\nwhere ${\\mathbf{{y}}}_n$ is a composition of $S$ (a particular subset of $K$\nphonological features) atoms ${\\mathbf{{a}}}^s$, and $w_n^s$ are weights of\nthe composition.\n\n\n\n\n\n\n\nAs an example, Table~\\ref{tab:SOPaudio} (in \\ref{sec:samples}) demonstrates recordings\n\n\nof individual\natoms of the GP system. We generated the phonological atoms also for\nthe SPE and eSPE phonological systems and used all of them in the\nfollowing experiments.\n\n\\subsection{Comparison of the phonological systems}\n\nWe start with context-independent vocoding in\nSection~\\ref{sec:civ}, i.e., the vocoding of isolated speech sounds,\nand continue with context-dependent vocoding in Section~\\ref{sec:cdv}.\n\n\\subsubsection{Context-independent vocoding}\n\\label{sec:civ}\n\nThe aim of this subsection is to objectively evaluate the phonological\nsystems in respect to context-independent vocoding, i.e., the\nability of phonological systems to produce isolated speech sounds.\n\n\nIn order to achieve this, instead of inferring the posteriors from the\nspeech, we generated the canonical phonological posteriors, i.e., we\nused only the features that represent the specific isolated phones\n(rows of Tables \\ref{tab:GPatts}, \\ref{tab:SPEatts}, and\n\\ref{tab:eSPEatts}). Original speech, manually phonetically labelled\n76 utterances from the audiobook test set, was used as the reference\nfor the comparison.\n\n\n\n\n\n\nHaving the phoneme boundaries, we extracted 25ms windows\nfrom the central (stationary) part of the phones, to obtain human\nspoken acoustic references.\n\n\n\n\nFinally, we used the Mel Cepstral Distortion (MCD)~\\citep{Kubichek93}\nto calculate perceptual acoustic distances between the vocoded phones\nand the spoken references, a confusion matrix $C_{vocoded}$.\n\n\n\n\n\n\n\n\nThe MCD values are in dB and higher values mean more confused\nphones. In order to visually compare confusions introduced by the\ndifferent phonological representations, we normalised $C_{vocoded}$ by\nconfusions between the spoken references only, a confusion matrix\n$N$:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "index": 7, "text": "\\begin{align}\n\\label{eq:scalingC}\n  C_{norm} &= C_{vocoded} \\odot C_{natural}\\\\\n  C_{natural}&= 1 + \\left(1 - \\left(\\max\\limits_{1 < i \\leq 40} n_{ij} - N\\right)\\oslash\\left(\\max\\limits_{1 < i \\leq 40} n_{ij} - \\min\\limits_{1 < i \\leq 40} n_{ij}\\right) \\right)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{norm}\" display=\"inline\"><msub><mi>C</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>m</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=C_{vocoded}\\odot C_{natural}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>C</mi><mrow><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>d</mi></mrow></msub><mo>\u2299</mo><msub><mi>C</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{natural}\" display=\"inline\"><msub><mi>C</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=1+\\left(1-\\left(\\max\\limits_{1&lt;i\\leq 40}n_{ij}-N\\right)\\oslash%&#10;\\left(\\max\\limits_{1&lt;i\\leq 40}n_{ij}-\\min\\limits_{1&lt;i\\leq 40}n_{ij}\\right)\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mn>1</mn><mo>+</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>&lt;</mo><mi>i</mi><mo>\u2264</mo><mn>40</mn></mrow></munder><mo>\u2061</mo><msub><mi>n</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>-</mo><mi>N</mi></mrow><mo>)</mo></mrow><mo>\u2298</mo><mrow><mo>(</mo><mrow><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>&lt;</mo><mi>i</mi><mo>\u2264</mo><mn>40</mn></mrow></munder><mo>\u2061</mo><msub><mi>n</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>-</mo><mrow><munder><mi>min</mi><mrow><mn>1</mn><mo>&lt;</mo><mi>i</mi><mo>\u2264</mo><mn>40</mn></mrow></munder><mo>\u2061</mo><msub><mi>n</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05991.tex", "nexttext": "\nwhere $H$ is the number of correctly transcribed words, $I$ is the\nnumber of insertions, and $N$ is the total number of words in the\nreference transcription.\n\nThe average intelligibility score of the phonological TTS was 71\\%\nin comparison to the HMM-based TTS where the listeners achieved the\nintelligibility of 84\\%. The phonological TTS thus achieved\nintelligibility of 85\\% of the state-of-the-art parametric TTS.\n\nWe trained the synthesis DNN only from audio without the text\ntranscription; and in this sense, we consider this approach as\nunsupervised TTS training.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\n\nWe have proposed to use speech vocoding as a platform for laboratory\nphonology. The proposal consists of a cascaded phonological analysis\nand synthesis. The objective and subjective evaluations supported the\nhypothesis that the most informative feature set -- with the best\ncoverage of the acoustic space (see confusion matrices of\ncontext-independent vocoding in Sec.~\\ref{sec:civ}) -- achieves the\nbest quality vocoded speech (see context-dependent vocoding in\nSec.~\\ref{sec:cdv}), where the features are verified in both\ndirections, recognition and synthesis, simultaneously.\n\nWe have showed three application examples of our proposed\napproach. First, we compared three systems of phonological\nrepresentations and concluded that eSPE achieves slightly better\nresults than the other two. Our results thus support other recent work\nshowing that eSPE is  suitable for phonological analysis, for speech\nrecognition and language identification tasks\n\\citep{Yu2012,Siniscalchi2012}. However, GP -- the most compact\nphonological speech representation, performs in the\nanalysis/synthesis tasks comparably to the systems with higher number\nof phonological features.\n\nSecond, we presented compositional phonological speech modelling,\nwhere phonological atoms can generate arbitrary speech\nsounds. Third, we explored phonological parametric TTS without\nany front-end, trained from an unlabelled audiobook in an\nunsupervised manner, and achieving intelligibility of 85\\% of the\nstate-of-the-art parametric speech synthesis. \nThis seems to be a promising approach for unsupervised and\nmultilingual text-to-speech systems.\n\nIn this work we have focused on the segmental evaluation of the\nphonological systems. In the future, we plan to address this\nlimitation by incorporating supra-segmental features, and using the\nproposed laboratory phonology platform for further experimentation\nsuch as generation of speech stimuli for perception experiments.\n\n\n\n\nWe envision that the presented approach paves the way for researchers\nin both fields to form meaningful hypotheses that are explicitly\ntestable using the concepts developed and exemplified in this\npaper. Laboratory phonologists might test the compactness,\nconfusability, perceptual viability, and other applied concepts of\ntheir theoretical models. This might be done in at least two\nways. Firstly, synthesis/recognition tests might follow hand in hand\nwith their analysis of data from human speech production and\nperception, which would allow for accumulating much needed data on the\ndifferences between human and machine performance. Secondly,\nsynthesis/recognition might be used for pre-testing before undergoing\nexperiments and analysis of human data, which is commonly time and\neffort demanding. This framework can be used in speech processing\nfield as an evaluation platform for improving the performance of\ncurrent state-of-the-art applications, for example in multi-lingual\nprocessing, using the concepts developed for the theoretical\nphonological models. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Acknowledgements}\n\\label{sec:ack}\n\nWe would like to thank Xingyu Na from Institute of Acoustics, Chinese\nAcademy of Sciences, for his valuable comments on an early\ndraft of this paper.\n\nThis work has been conducted with the support of the Swiss NSF under\ngrant CRSII2 141903: Spoken Interaction with Interpretation in Switzerland\n(SIWIS) and grant 2/0197/15 by Scientific Granting Agency in Slovakia.\n\n\n\n\n\n\\section{References}\n\\label{sec:ref}\n\n\n\n\n\\bibliographystyle{elsarticle/elsarticle-harv}\n\\bibliography{refs,from_alex}\n\n\n\n\\appendix\n\n\\setcounter{table}{0}\n\\section{Mapping of the phonological features to CMUbet}\n\\label{sec:spefeatures}\nTables \\ref{tab:GPatts}, \\ref{tab:SPEatts} and\n\\ref{tab:eSPEatts} show the mapping of the phonological features to the\nused phonemes in this work.\n\n\\begin{table} [t]\n\\footnotesize\n  \\caption{\\label{tab:GPatts} {\\it GP phonological features and\n      their association to CMUbet phonemes used in this paper.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabular}{ll|cccccccccccc}\n\\hline\n{\\rotatebox{90}}{CMUbet} & {\\rotatebox{90}}{IPA} & A & I & U & E & S & h & H & N & a & i & u & {\\rotatebox{90}}{silence} \\\\\n\\hline\niy & {\\textipa}{i}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\nih & {\\textipa}{I}   & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nuw & {\\textipa}{u}   & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nuh & {\\textipa}{U}   & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\ney & {\\textipa}{eI}  & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\now & {\\textipa}{oU}  & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\noy & {\\textipa}{oI}  & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ \\\\\nao & {\\textipa}{O}   & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\naa & {\\textipa}{A}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ \\\\\nae & {\\textipa}{\\ae} & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ \\\\\nah & {\\textipa}{2}   & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\naw & {\\textipa}{aU}  & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nay & {\\textipa}{aI}  & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\ny  & {\\textipa}{j}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nw  & {\\textipa}{w}   & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\neh & {\\textipa}{e}   & $+$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\ner & {\\textipa}{3\\textrhoticity}\n              & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nr  & {\\textipa}{\\*r} & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nl  & {\\textipa}{l}   & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\np  & {\\textipa}{p}   & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nb  & {\\textipa}{b}   & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nf  & {\\textipa}{f}   & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nv  & {\\textipa}{v}   & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nm  & {\\textipa}{m}   & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\nt  & {\\textipa}{t}   & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nd  & {\\textipa}{d}   & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nth & {\\textipa}{T}   & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\ndh & {\\textipa}{D}   & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nn  & {\\textipa}{n}   & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\ns  & {\\textipa}{s}   & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nz  & {\\textipa}{z}   & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nch & {\\textipa}{tS}  & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\njh & {\\textipa}{dZ}  & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nsh & {\\textipa}{S}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nzh & {\\textipa}{Z}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nk  & {\\textipa}{k}   & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\ng  & {\\textipa}{g}   & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nng & {\\textipa}{N}   & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\nhh & {\\textipa}{h}   & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\n\\hline\n\n\n\n\n\\end{tabular}}\n\\end{table}\n\n\\begin{table} [t]\n\\footnotesize\n  \\caption{\\label{tab:SPEatts} {\\it SPE phonological features and\n      their association to CMUbet phonemes used in this paper.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabular}{ll|ccccccccccccccc}\n\\hline\n{\\rotatebox{90}}{CMUbet} & {\\rotatebox{90}}{IPA} & {\\rotatebox{90}}{vocalic} & {\\rotatebox{90}}{consonantal} & {\\rotatebox{90}}{high} &\n{\\rotatebox{90}}{back} & {\\rotatebox{90}}{low} & {\\rotatebox{90}}{anterior} & {\\rotatebox{90}}{coronal} & {\\rotatebox{90}}{round} &\n{\\rotatebox{90}}{rising} & {\\rotatebox{90}}{tense} & {\\rotatebox{90}}{voice} & {\\rotatebox{90}}{continuant} & {\\rotatebox{90}}{nasal} &\n{\\rotatebox{90}}{strident} & {\\rotatebox{90}}{silence} \\\\\n\\hline\niy & {\\textipa}{i}   & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nih & {\\textipa}{I}   & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nuw & {\\textipa}{u}   & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nuh & {\\textipa}{U}   & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\ney & {\\textipa}{eI}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\now & {\\textipa}{oU}  & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\noy & {\\textipa}{oI}  & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nao & {\\textipa}{O}   & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\naa & {\\textipa}{A}   & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nae & {\\textipa}{\\ae} & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nah & {\\textipa}{2}   & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\naw & {\\textipa}{aU}  & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nay & {\\textipa}{aI}  & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\ny  & {\\textipa}{j}   & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nw  & {\\textipa}{w}   & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\neh & {\\textipa}{e}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\ner & {\\textipa}{3\\textrhoticity}\n              & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nr  & {\\textipa}{\\*r} & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nl  & {\\textipa}{l}   & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\np  & {\\textipa}{p}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nb  & {\\textipa}{b}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\nf  & {\\textipa}{f}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ \\\\\nv  & {\\textipa}{v}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nm  & {\\textipa}{m}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\nt  & {\\textipa}{t}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nd  & {\\textipa}{d}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\nth & {\\textipa}{T}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ \\\\\ndh & {\\textipa}{D}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ \\\\\nn  & {\\textipa}{n}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\ns  & {\\textipa}{s}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ \\\\\nz  & {\\textipa}{z}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nch & {\\textipa}{tS}  & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\njh & {\\textipa}{dZ}  & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\nsh & {\\textipa}{S}   & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ \\\\\nzh & {\\textipa}{Z}   & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nk  & {\\textipa}{k}   & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\ng  & {\\textipa}{g}   & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ \\\\\nng & {\\textipa}{N}   & $-$ & $+$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\nhh & {\\textipa}{h}   & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ \\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\\begin{table} [t]\n\\footnotesize\n  \\caption{\\label{tab:eSPEatts} {\\it eSPE phonological features\n      and their association to CMUbet phonemes used in this paper.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabular}{ll|ccccccccccccccccccccc}\n\\hline\n{\\rotatebox{90}}{CMUbet} & {\\rotatebox{90}}{IPA} & {\\rotatebox{90}}{vowel} & {\\rotatebox{90}}{fricative} & {\\rotatebox{90}}{nasal} &\n{\\rotatebox{90}}{stop} & {\\rotatebox{90}}{approxim.} & {\\rotatebox{90}}{coronal} & {\\rotatebox{90}}{high} & {\\rotatebox{90}}{dental} &\n{\\rotatebox{90}}{glottal} & {\\rotatebox{90}}{labial} & {\\rotatebox{90}}{low} & {\\rotatebox{90}}{mid} & {\\rotatebox{90}}{retroflex} &\n{\\rotatebox{90}}{velar} & {\\rotatebox{90}}{anterior} & {\\rotatebox{90}}{back} & {\\rotatebox{90}}{continuant} & {\\rotatebox{90}}{round} &\n{\\rotatebox{90}}{tense} & {\\rotatebox{90}}{voiced} & {\\rotatebox{90}}{silence} \\\\\n\n\n\\hline\n\niy & {\\textipa}{i}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ \\\\\nih & {\\textipa}{I}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\nuw & {\\textipa}{u}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $+$ & $-$ \\\\\nuh & {\\textipa}{U}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\ney & {\\textipa}{eI}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ \\\\\now & {\\textipa}{oU}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $+$ & $-$ \\\\\noy & {\\textipa}{oI}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nao & {\\textipa}{O}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $+$ & $-$ \\\\\naa & {\\textipa}{A}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $+$ & $-$ \\\\\nae & {\\textipa}{\\ae} & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ \\\\\nah & {\\textipa}{2}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\naw & {\\textipa}{aU}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $+$ & $+$ & $+$ & $-$ \\\\\nay & {\\textipa}{aI}  & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $+$ & $-$ \\\\\ny  & {\\textipa}{j}   & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nw  & {\\textipa}{w}   & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\neh & {\\textipa}{e}   & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\ner & {\\textipa}{3\\textrhoticity}\n              & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\nr  & {\\textipa}{\\*r} & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nl  & {\\textipa}{l}   & $-$ & $-$ & $-$ & $-$ & $+$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\np  & {\\textipa}{p}   & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\nb  & {\\textipa}{b}   & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nf  & {\\textipa}{f}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\nv  & {\\textipa}{v}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $+$ & $-$ & $+$ & $-$ \\\\\nm  & {\\textipa}{m}   & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nt  & {\\textipa}{t}   & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\nd  & {\\textipa}{d}   & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nth & {\\textipa}{T}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\ndh & {\\textipa}{D}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\nn  & {\\textipa}{n}   & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\ns  & {\\textipa}{s}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\nz  & {\\textipa}{z}   & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ \\\\\nch & {\\textipa}{tS}  & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\njh & {\\textipa}{dZ}  & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nsh & {\\textipa}{S}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ \\\\\nzh & {\\textipa}{Z}   & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\nk  & {\\textipa}{k}   & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ \\\\\ng  & {\\textipa}{g}   & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nng & {\\textipa}{N}   & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ \\\\\nhh & {\\textipa}{h}   & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $+$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ \\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\setcounter{table}{0}\n\\section{Demonstrating speech synthesis samples}\n\\label{sec:samples}\nTables \\ref{tab:SOPaudio} and \\ref{tab:SOPcombaudio} contain\nrecordings demonstrating GP phonological atoms and their composition,\nrespectively, and Table \\ref{tab:SPEaudio} contains recordings\ndemonstrating phonological speech synthesis.\n\n\\begin{table} [htb]\n  \\caption{\\label{tab:SOPaudio} {\\it Recordings demonstrating\n      individual phonological atoms. The $z_n^k$ patterns were\n      repeated to get phonological atoms ${\\mathbf{{a}}}^k$ of two seconds\n      long.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|c|c|r|}\n\\hline\nPhonolog. atom & Recording & Offline link\\\\\n\\hline \\hline\n${\\mathbf{{a}}}^A$ &\n\\textattachfile{audio/A.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/A.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^a$ &\n\\textattachfile{audio/a.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/a.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^I$ &\n\\textattachfile{audio/I.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/I.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^i$ &\n\\textattachfile{audio/i.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/i.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^U$ &\n\\textattachfile{audio/U.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/U.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^u$ &\n\\textattachfile{audio/u.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/u.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^E$ &\n\\textattachfile{audio/E.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/E.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^H$ &\n\\textattachfile{audio/H.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/H.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^h$ &\n\\textattachfile{audio/h.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/h.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^S$ &\n\\textattachfile{audio/S.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/S.wav}\\\\\n\\hline\n${\\mathbf{{a}}}^N$ &\n\\textattachfile{audio/N.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}} (wav)\n& \\url{www.idiap.ch/paper/3120/N.wav}\\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table} [htb]\n  \\caption{\\label{tab:SOPcombaudio} {\\it Recordings demonstrating\n      composition of GP phonological atoms, resulting into the\n      synthesis of new sounds.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|r|}\n\\hline\nRule & IPA & Composition & Offline link\\\\\n\\hline \\hline\n[A, I, U, E] & {\\textipa}{\\oe} &\n\\textattachfile{audio/oe.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/oe.wav}\\\\\n\\hline\n[I, U, E] & {\\textipa}{y} &\n\\textattachfile{audio/u.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/u.wav}\\\\\n\\hline\n\n\n\n\n\n\\end{tabu}}\n\\end{table}\n\n\\begin{table} [htb]\n  \\caption{\\label{tab:SPEaudio} {\\it Recordings demonstrating\n      phonological speech synthesis.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|r|}\n\\hline\nSentence & Scheme & Recording & Offline link\\\\\n\\hline \\hline\n\\multirow{3}{*}{a0453} & GP &\n\\textattachfile{audio/gp-syn/a0453.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0453-GP.wav} \\\\\n\\cline{2-4}\n & SPE &\n\\textattachfile{audio/spe-syn/a0453.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0453-SPE.wav} \\\\\n\\cline{2-4}\n & eSPE &\n\\textattachfile{audio/pp-syn/a0453.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0453-eSPE.wav}\\\\\n\\hline\n\\multirow{3}{*}{a0457} & GP &\n\\textattachfile{audio/gp-syn/a0457.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0457-GP.wav} \\\\\n\\cline{2-4}\n & SPE &\n\\textattachfile{audio/spe-syn/a0457.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0457-SPE.wav}\\\\\n\\cline{2-4}\n & eSPE  &\n\\textattachfile{audio/pp-syn/a0457.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a047-eSPE.wav}\\\\\n\\hline\n\\multirow{3}{*}{a0460} & GP &\n\\textattachfile{audio/gp-syn/a0460.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0460-GP.wav}\\\\\n\\cline{2-4}\n & SPE &\n\\textattachfile{audio/spe-syn/a0460.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0460-SPE.wav}\\\\\n\\cline{2-4}\n & eSPE &\n\\textattachfile{audio/pp-syn/a0460.wav}\n{\\includegraphics[height=1.8ex]{figs/speaker-volume-button.jpg}}\n(wav) & \\url{www.idiap.ch/paper/3120/a0460-eSPE.wav}\\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\n\n\n", "itemtype": "equation", "pos": 56342, "prevtext": "\n\n\n\nwhere $\\odot$ stands for element-wise matrix multiplication (the\nHadamard product), $\\oslash$ stands for element-wise matrix division,\nand max and min functions stand for column-wise max and min functions \nrespectively and are used to get linear scaling factor\nof a range of values between $<0,1>$. The confusion matrix\n$C_{natural}$ is thus a scaled matrix $N$, having a range of values\nbetween $<1,2>$, where $1$ refers to no confusion.\n\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:human-mcd} shows the scaled confusion matrix\n$C_{natural}$.\n\n\n\n\n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=.73\\linewidth]{figs/humanCM-crop}\n  \\caption{{\\it Scaled confusion matrix $C_{natural}$ of the spoken\n      acoustic references.}}\n\\label{fig:human-mcd}\n\\end{figure}\n \nFigures~\\ref{fig:GPSoundTests}, \\ref{fig:SPESoundTests}\nand~\\ref{fig:eSPESoundTests} show normalised confusion matrices\n$C_{norm}$ of vocoded context-independent phones and the spoken\nacoustic references of the same speaker, for the GP, SPE and eSPE\nphonological systems, respectively. The diagonal elements of the\nconfusion matrices represent an acoustical distance (dissimilarity) of\nvocoded and spoken phones. If the phonological features represent\nspeech well, the matrices show only strong diagonals. The missing\ndiagonal values, the higher confusions between spoken and vocoded\nphones, imply errors of the phonological speech representation. These\nerrors probably stem from wrongly assigned phonological features to\nphones (given in \\ref{sec:spefeatures}) during the training of the\nphonological analysis.\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/GP-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:GPsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/GP-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:GPsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of  GP vocoded and spoken phones.}}\n\\label{fig:GPSoundTests}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/SPE-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:SPEsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/SPE-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:SPEsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of SPE vocoded and spoken phones.}}\n\\label{fig:SPESoundTests}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/eSPE-sounds-pruned-crop}\n  \\caption{Network-based}\n  \\label{fig:eSPEsoundsPruned}\n\\end{subfigure}\n\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1.\\linewidth]{figs/eSPE-sounds-combined-crop}\n  \\caption{Compositional-based}\n  \\label{fig:eSPEsoundsCombined}\n\\end{subfigure}\n\\caption{{\\it $C_{norm}$ of eSPE vocoded and spoken phones.}}\n\\label{fig:eSPESoundTests}\n\\end{figure}\n\nThe figures show two aspects of the evaluation. In the first, the\nconfusion matrices of (a) the network-based and (b) the\ncompositional-based phonological synthesis are shown.\n\n\n\n\n\n\n\nThe network- and compositional-based synthesis differ in the way\nfeatures are combined. In the first case, the DNN inputs are combined\nso as to generate specific phones. In the second case, phonological\natoms of the specific phone are combined in order to generate\ncompositional phones. We can therefore  consider this as two different\nevaluation metrics, hypothesising that both contribute partially to a\nfinal evaluation. In both cases, the ideal performance is to have dark\ndiagonals, i.e., the lowest acoustic distance between the spoken\nreference and vocoded phones. Table~\\ref{tab:diagonals} shows the\naverages of the diagonal MCD values of the confusion matrices.\n\n\\begin{table} [htb]\n  \\caption{\\label{tab:diagonals} {\\it The average MCD values given in\n      [dB] of the diagonals of the confusion matrices shown in\n      Figures~~\\ref{fig:GPSoundTests}, \\ref{fig:SPESoundTests}\n      and~\\ref{fig:eSPESoundTests}.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabular}{|r|c|c|c|}\n\\hline\nSystem & Network & Compositional \\\\\n\\hline\nGP & 8.12 & 8.67 \\\\\n\\hline\nSPE & 7.79 & 8.77 \\\\\n\\hline\neSPE & 7.87 & 8.79 \\\\\n\\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\n\nFor all three phonological systems, the results of\ncompositional-based context-independent vocoding show greater errors\n(i.e., higher values in Table~\\ref{tab:diagonals}) than the ones of\nnetwork-based vocoding. This might be caused by the fact that\nthe composition of the phonological atoms is a linear operation (as shown\nin Section~\\ref{sec:soundsofprimes}), and it is an approximation to a\nnon-linear function that is modelled by network-based phonological\nsynthesis. Additionally, the types of reported confusions in the\nnetwork-based vocoding that are missing in the compositional-based one\nmake sense phonetically. For example, in the left panel of\nFigure~\\ref{fig:GPsoundsPruned}, phonetically very similar [{\\textipa}{Z}],\n[{\\textipa}{S}], [{\\textipa}{dZ}], [{\\textipa}{tS}] with voicing and closure being highly\ncontext dependent are confused. Nevertheless, compositional-based\nvocoding tends to produce ``tighter'' confusions in all three\nframeworks. For example, while [{\\textipa}{D}] in the left panel of\nFigure~\\ref{fig:GPsoundsPruned} shows confusions with nasals, voiced\nplosives and [{\\textipa}{v}], it shows only minor confusions with labials in\nthe right panel.\n\n\n\nIn the second aspect of the evaluation, the three phonological systems\n(GP, SPE and eSPE) are compared among\nthemselves using network-based synthesis. In\nFigures~\\ref{fig:GPsoundsPruned}, \\ref{fig:SPEsoundsPruned}, and\n\\ref{fig:eSPEsoundsPruned}, we see different error patterns. In all\nthree phonological systems the biggest confusions are shown with the\nnasals [{\\textipa}{m n N}]. GP in addition produces confusions of the\nconsonants and vowels, such as for  nasals. SPE seems to represent\nspeech better, namely for vowel [{\\textipa}{A}] and glide [{\\textipa}{w}], and\nsuppresses most of the vowel-consonant confusions. On the other hand,\nit fails with proper [{\\textipa}{dZ}] vocoding. We speculate that phone\nfrequency in the evaluation data may be one of the causes of these\nerrors; for example, the phone [{\\textipa}{dZ}] was the least frequent one in\nour data.\n\n\n\nFinally, according to our data, eSPE further improves on the SPE\nspeech representation. It generates fewer confusions in the vowel\nspace, and also in the consonant space, for example in the voiced\nstops class [{\\textipa}{b d g}].\n\n\n\n\\subsubsection{Context-dependent vocoding}\n\\label{sec:cdv}\n\nThe previous experiment evaluated the vocoding of the isolated\nsounds, using canonical posteriors ${\\mathbf{{z}}}_n$. We continued with the\nevaluation of continuous speech vocoding using ${\\mathbf{{z}}}_n$ inferred\nfrom the reference speech signals. Network-based phonological\nsynthesis was used for the following experiments.\n\n\n\n\n\n\n\nIn this evaluation, we were interested if the segmental errors found\nin context-independent vocoding impact the context-dependent\nvocoding. We employed an ABX subjective evaluation listening\ntest~\\citep{Grancharov2008}, suitable for comparing two different\nsystems. In this test, listeners were presented with\npairs of samples produced by two systems (A and B) and for each pair\nthey were indicating their preference for A, B, or neither of the two\n(X). The material for the test consisted of 16 pairs of sentences such\nthat one member of the pair was generated using the GP-based vocoder\n(system A) and the other member was generated using the eSPE-based\nvocoded speech (system B).\n\nRandom utterances from the test set of the Tolstoy database were used\nto generate the vocoded speech.\nWe chose these two systems because they displayed the greatest\ndifferences in context-independent results. The subjects for the ABX\ntest were 37 listeners, roughly equally pooled from experts in speech\nprocessing on the one hand, and completely naive subjects on the other\nhand. The subjects were presented with pairs of sentences in a random\norder with no indication of which system they represented with. They were\nasked to listen to these pairs of sentences (as many times as they\nwanted), and choose between them in terms of their overall\nquality. Additionally, the option X, i.e. \\emph{both samples sound the\nsame}, was available if they had no preference for either of them. To\ndecrease the demands on the listeners, we divided the material to\ntwo different sets, each consisting of 8 paired sentences randomly\nselected from the test set. The first set was presented to 19\nlisteners, and the second set to 18 different listeners.\n\n\\begin{figure}[h]\n\\centering\n\n\n\\includegraphics[width=.8\\linewidth]{figs/ABX-crop}\n\\caption{ABX subjective evaluation listening test between the GP-based\nand the eSPE-based vocoded speech}\n\\label{fig:SubEvalGP_eSPE}\n\\end{figure}\n\nIn Figure~\\ref{fig:SubEvalGP_eSPE} the results of the ABX test for the\nGP and eSPE phonological systems, are shown. As can be seen, the\neSPE-based phonological vocoder outperforms the GP one by 36.8\\%\ncompared with 18.1\\% preference score in the ABX test. Even though\nthere is a preference of the listeners towards the eSPE-based system\n(double preference percentage), it is clearly shown that with a very\nhigh percentage, 45\\%, the two systems are perceived by the listeners\nas having the same overall quality. It should be pointed out that a\n$t$-test confirmed that this difference between the GP-based and\neSPE-based phonological vocoders is statistically significant ($p <\n0.01$). \n\nWe hypothesize that the preference for eSPE is linked to\ngreater perceptual clarity of individual phones. Subsequent auditory\nand visual analyses of the sample sentences and their generated\nacoustic signals suggest that eSPE sentences displayed longer closures\nfor plosives, stronger plosive releases, and also slightly greater\ndisjunctures at some word boundaries.\n\n\nThese features correspond to a decreased overlap of sounds,\ni.e. decreased coarticulation, commonly present in hyper-articulated\nor clearly enunciated speech.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Experimental parametric phonological TTS}\n\\label{sec:ptts}\n\nIn this section we show how compositional phonological speech models\ncould be combined to generate arbitrary speech sounds, and how to\nsynthesise continuous speech from the canonical phonological\nrepresentation.\n\n\\subsubsection{Generation of sounds from unseen language}\n\nIn this experiment, we arbitrarily selected the GP system to demonstrate\nthe phonological composition of new speech sounds. \\cite{Harris94}\nclaims that fusing and splitting of primes accounts for phonological\ndescription of the sound. We selected the phonological rule number 29\n[I, U, E] $\\rightarrow$ {\\textipa}{y}, and  [A, I, U, E] $\\rightarrow$\n{\\textipa}{\\oe}, of \\cite{Harris94} and tried to synthesise non-English\nsounds by the composition of involved phonological atoms. According to\nSection~\\ref{sec:soundsofprimes}, we claim that new sounds can be\ngenerated by time-domain mixing of the corresponding atoms\n${\\mathbf{{a}}}^s$. Table~\\ref{tab:SOPcombaudio} demonstrates the synthesis of\nstandard German sounds [{\\textipa}{y}] and [{\\textipa}{\\oe}] from English\nphonological atoms ${\\mathbf{{a}}}^s$, generated as in\nEq.~\\ref{eq:fusion}. For $w_n^s=1$, it can be done easily with\navailable free tools, e.g.:\n\\begin{itemize}\n  \\item [[{\\textipa}{y}]] :  \\texttt{sox -m I.wav U.wav E.wav y.wav}\n  \\item [[{\\textipa}{\\oe}]] : \\texttt{sox -m A.wav I.wav U.wav E.wav oe.wav}\n\\end{itemize}\n\nWe performed a formant analysis of all phonological atoms, and concluded\nthat they contain the same number of formants as human speech sounds\n(i.e., 4 in the 5 kHz bandwidth). In addition, the combined sounds\nalso contain the proper number of formants. The first two formants play a\nmajor perceptual role in distinguishing different English\nvowels~\\citep{Ladefoged14}, and Figure~\\ref{fig:formants} shows\nF1 and F2 of [{\\textipa}{\\oe}] phone from Table~\\ref{tab:SOPcombaudio}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=.75\\linewidth]{figs/formants-crop}\n\\caption{First two formants of involved phonological atoms and their\n  composition -- a phone [{\\textipa}{\\oe}].}\n\\label{fig:formants}\n\\end{figure}\n\nThe composition of Eq.~\\ref{eq:fusion} represents a static mixing of\n$S$ phonological atoms, i.e., it cannot be applied to model\nco-articulation. To include co-articulation into the synthesis, the\nphonological synthesizer has to be used. As it was trained with the\ntemporal context of 11 successive frames, around 50 ms before and 50\nms after the current processing frame, it learnt how speech parameters\nchange with trajectories of the phonological posteriors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results of this experiment support the hypothesis mentioned in\nSection  \\ref{sec:soundsofprimes}, that phonological atoms may define\nthe phones. In addition,  Section \\ref{sec:civ} demonstrates that\ncompositional-based phonological synthesis works well also for the SPE\nand eSPE phonological systems.\n\n\n\n\n\n\n\n\\begin{figure}[!th]\n\\centering\n\\resizebox{4in}{!}{\n\\begin{tikzpicture}[font=\\scriptsize]\n  \n  \\draw[rounded corners, thick] (0,0) rectangle (2,1.5);\n  \\node[align=center] at (1,0.75) {Text to\\\\phonological\\\\features};\n\n  \\draw[thick, ->] (2,0.75) -- (3.5,0.75); \\node[above] at (2.75,0.75)\n       {${\\mathbf{{z}}}_n$};\n  \\draw[rounded corners, thick, fill=lightgray] (3.5,0) rectangle (5.5,1.5);\n  \\node[align=center] at (4.5,0.75) {\\textbf{Phonological}\\\\synthesizer};\n\n  \\draw[thick, ->] (5.5,1.1) -- (7,1.1); \\node[above] at (6.25,1.1)\n       {LSPs};\n\n  \\draw[thick, fill=black] (6,0.9) circle[radius=0.04];\n  \\draw[thick, fill=black] (6.25,0.9) circle[radius=0.04];\n  \\draw[thick, fill=black] (6.5,0.9) circle[radius=0.04];\n\n  \\draw[thick, ->] (5.5,0.35) -- (7,0.35); \\node[align=center] at (6.25,0.35)\n       {Glottal\\\\signal};\n\n  \\draw[rounded corners, thick] (7,0) rectangle (9,1.5);\n  \\node[align=center] at (8,0.75) {LPC\\\\re-synthesis};\n  \\draw[thick, ->] (9,0.75) -- (10.7,0.75); \\node[above] at (9.9,0.75)\n       {Synth. ${\\mathbf{{y}}}_n$};\n\\end{tikzpicture}\n}\n\\caption{Phonological TTS synthesis. Speech parameters, the speech\n  line spectral pairs LSPs and source parameters, are generated by the\n  DNN. Speech samples are generated by subsequent LPC re-synthesis.}\n\\label{fig:phonovocTTS}\n\\end{figure}\n\n\\subsubsection{Continuous speech synthesis}\n\nExperimental parametric phonological TTS can be designed by a\nsimplistic text processing front end: text $\\rightarrow$ phonemes\n$\\rightarrow$ phonological features. Figure~\\ref{fig:phonovocTTS}\nshows the TTS process with the phonological synthesis. The binary\nphonological representation to be synthesized is obtained again from\nthe canonical phone representation.\n\nTo demonstrate the potential of our parametric phonological TTS\nsystem, we randomly selected three utterances from a \\texttt{slt}\nsubset of the CMU-ARCTIC speech database~\\citep{Kominek2004}, and used\ntheir text labels to generate continuous speech. Specifically, we used\nthe phoneme symbols along with their durations from the forced-aligned\nfull-context labels provided with the database, and mapped it to the\nphonological representation. Then we synthesised the sentences using\nthe already trained phonological synthesizers as described in\nSection~\\ref{sec:training}.\n\nTable~\\ref{tab:SPEaudio} lists recordings that demonstrates speech\nsynthesis from the phonological speech representation. The example\n\\texttt{a0453} illustrates how the phonological\nvocoder learns the context. Figure~\\ref{fig:a543-examples} visualises\nthe generated GP and eSPE examples. The phoneme sequence of the first\nword is {\\textipa}{[eI t i n]}, while the synthesised sequences using both\nphonological systems are rather {\\textipa}{[eI tS i n]}. The substitution of\n{\\textipa}{[t]} by {\\textipa}{[tS]} illustrates the assimilation of the place of\narticulation in the synthesised phoneme {\\textipa}{[t]}. If {\\textipa}{[t]} starts a\nstressed syllable and is followed by {\\textipa}{[i]} this alveolar stop is\naspirated and commonly more palatal due to coarticulation with the\nfollowing vowel. The acoustic result of a release burst when the\ntongue is in the alveo-palatal region is similar to the frication\nphase of alveo-palatal affricate {\\textipa}{[tS]}.\nWe conclude that the phonological synthesizer learns some contextual\ninformation because of using the temporal window of 11 successive\nframes -- around 100 ms of speech, that may correspond to the formant\ntransitions and differences in voice onset times. This is probably\nenough to learn certain aspects of co-articulation well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-orig.png}\n  \\caption{The natural speech of the arctic\\_a0453 example.}\n  \\label{fig:a543-orig}\n\\end{subfigure}\n\\\\\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-GP.png}\n  \\caption{The example generated with the GP features.}\n  \\label{fig:a543-GP}\n\\end{subfigure}\n\\\\\n\\begin{subfigure}{1.0\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{figs/screen-a453-eSPE.png}\n  \\caption{The example generated with the eSPE features}\n  \\label{fig:a543-eSPE}\n\\end{subfigure}\n\\caption{{\\it Visualisation of vocoded arctic\\_a0453 examples:\n    ``Eighteen hundred, he calculated.''. The GP vocoding seems to\n    better synthesise the higher frequencies such as for fricatives,\n    whereas eSPE vocoding seems to synthesise stronger formant\n    frequencies. The recordings are available in \\ref{sec:samples}.}}\n\\label{fig:a543-examples}\n\\end{figure}\n\n\\paragraph{Subjective Intelligibility Test}\n\n\n\n\n\n\nWe compared the phonological TTS with a conventional hidden Markov\nmodel (HMM) parametric speech synthesizer, trained on the same\ntraining set of the audiobook which was used for training the\nphonological vocoder. For building the HMM models, the HTS V.2.1\ntoolkit~\\citep{HTS2010} was used. Specifically, the implementation\nfrom the EMIME project~\\citep{Wester2010} was taken. Five-state,\nleft-to-right, no-skip HSMMs were used. The speech parameters which\nwere used for training the HSMMs were 39 order mel-cepstral\ncoefficients, log-F0 and 21-band aperiodicities, along with their\ndelta and delta-delta features, extracted every 5 ms.\n\n\n\n\nFor evaluating the phonological TTS, an intelligibility test\nwas conducted using semantically unpredictable sentences (SUSs). Two\nsets of sentences were used in this test. \nEach set contained 14 unique SUSs.\nThe SUSs were taken from SIWIS project\\footnote{Spoken Interaction with Interpretation in Switzerland\n(SIWIS), https://www.idiap.ch/project/siwis/downloads/siwis-database}. \nThe length of the sentences varied from 6 to 8 words. Each set\nconsisted of 7 sentences synthesised by the phonological TTS, and\nanother 7 ones synthesised by the reference HTS system.\n\nTwenty native English speakers, experts in the speech processing\nfield, participated in the listening test.\n\n\nThe listeners could listen to each synthesized sentence\nonly one or two times, and were asked to transcribe the audio. Eleven and\nnine listeners respectively participated in the two sets of the\nlistening test.\n\n\n\n\nIntelligibility score was calculated by:\n\n", "index": 9, "text": "\\begin{equation}\n\\mbox{Intelligibility}=\\frac{H-I}{N}\\mbox{ x 100\\%}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mbox{Intelligibility}=\\frac{H-I}{N}\\mbox{ x 100\\%}\" display=\"block\"><mrow><mtext>Intelligibility</mtext><mo>=</mo><mrow><mfrac><mrow><mi>H</mi><mo>-</mo><mi>I</mi></mrow><mi>N</mi></mfrac><mo>\u2062</mo><mtext>\u00a0x 100%</mtext></mrow></mrow></math>", "type": "latex"}]