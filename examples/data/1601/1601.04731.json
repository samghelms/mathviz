[{"file": "1601.04731.tex", "nexttext": "\nConsider the following non-negative linear combinations of such r.v's\n\n", "itemtype": "equation", "pos": 2888, "prevtext": "\n\n\n\\title{Schur properties of convolutions of gamma random variables}\n\\author{Farbod Roosta-Khorasani\\thanks{Dept. of Computer Science, University of British Columbia, Vancouver, Canada.\n{\\tt farbod@cs.ubc.ca}.} \\and G\\'{a}bor J. Sz\\'{e}kely\\thanks{National Science Foundation, Arlington, Virginia. {\\tt gszekely@nsf.gov} and Alfr\\'{e}d R\\'{e}nyi Institute of Mathematics, Hungarian Academy of Sciences,  Budapest, Hungary.}}\n\n\n\n\\maketitle\n\\begin{abstract}\nSufficient conditions for comparing the convolutions of heterogeneous gamma random \nvariables in terms of the usual stochastic order are established. \nSuch comparisons are characterized by the Schur convexity properties of the cumulative distribution function of the convolutions.\n\nSome examples of the practical applications of our results are given. \n\\end{abstract}\n\n\\vspace{1pc}\n\\noindent\n\\textbf{Keywords}: Schur-convexity of  tails; majorization order; linear combinations; gamma \ndistribution; tail probabilities.\n\n\\vspace{0.5pc} \\noindent\n\\textbf{MSC 2010}: Primary 60E15; secondary 62E99\n\n\n\\section{Introduction and Main Result}\n\nLinear combinations (i.e., convolutions) of independent gamma random variables (r.v's) often naturally arise in many applications in statistics, engineering, insurance, actuarial science and reliability. As such there has been extensive study of their stochastic properties in the literature. For examples of such  theoretical studies as well as applications see~\\cite{kochar2010right, kochar2011tail, kochar2012some, furman2006tail, yu2011some, lihong2005stochastic, bon1999ordering, zhao2009mean, zhao2011some, merkle1994schur, amiri2011skewness, boland1994schur, szba, roszas} and references therein.\n\nBock et al.,~\\cite{bock}, and Diaconis and Perlman,~\\cite{diaconis1990bounds}, in their seminal works, first \nstudied the Schur convexity properties of the cumulative distribution function of the linear combinations of independent gamma r.v's. Ever since, this topic and its variants have been studied by many researchers; see the references mentioned above. However despite all the efforts, the results in~\\cite{bock} remained the best available, yet far from the best possible. Bakirov,~\\cite{bakirov95} provided a tighter bound for the special case of convolutions of chi-squared r.v's of degree one, which often arise from quadratic forms. Here, we prove results regarding the Schur properties of the tails of convolutions of gamma r.v's with respect to the mixing weights and in terms of the usual stochastic order, and indeed sharpen some results given in~\\cite{bock}. As a consequence, the result in~\\cite{bakirov95} is also generalized. \n\nMore specifically, let $X_{i},\\; i=1,2,\\ldots,n$, be $n$ independent and identically distributed (i.i.d) gamma distributed r.v's, parametrized by shape $\\alpha > 0$ and rate $\\beta > 0$ parameters with the probability density function (PDF)\n\n", "index": 1, "text": "\\begin{equation*}\nf(x) = \\left\\{\n  \\begin{array}{l l}\n\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1 } e^{-\\beta x}  &\\text{$x \\ge 0$} \\\\\n0 &\\text{$x \\leq 0$}\n\\end{array} \\right..\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"f(x)=\\left\\{\\begin{array}[]{l l}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha%&#10;-1}e^{-\\beta x}&amp;\\text{$x\\geq 0$}\\\\&#10;0&amp;\\text{$x\\leq 0$}\\end{array}\\right..\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mfrac><msup><mi>\u03b2</mi><mi>\u03b1</mi></msup><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><msup><mi>x</mi><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow></msup></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>\u2265</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>\u2264</mo><mn>0</mn></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $\\lambda_{i} \\geq 0, \\; i=1,2,\\ldots,n$, are real numbers. For a given $\\alpha > 0$, $\\beta > 0$ and $x > 0$, define \n\n", "itemtype": "equation", "pos": 3159, "prevtext": "\nConsider the following non-negative linear combinations of such r.v's\n\n", "index": 3, "text": "\\begin{equation*}\n\\sum_{i=1}^{n}   \\lambda_{i}  X_{i}, \n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{n}\\lambda_{i}X_{i},\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $\\bm{\\lambda} = (\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}) \\in \\mathbb{R}^{n}$. The aim of the present paper is to find the conditions allowing one to compare tail probabilities of the form~\\eqref{P_x} with respect to the mixing weights $\\bm{\\lambda}$. In~\\cite[Theorem 2.2]{roszas}, results regarding the extremal values of~\\eqref{P_x} (i.e., maximal and minimal values with respect to $\\bm{\\lambda}$ and for given $\\alpha,\\beta$ and $x$) are proved. Here, we extend those results to be able to compare $\\eqref{P_x}$ for any pair of weight vectors.\n\nTo that end, let us recall that the vector $\\bm \\lambda$ is said to \\textit{majorize} the vector $\\bm \\mu$, denoted by $\\bm \\mu \\prec \\bm \\lambda$, if \n\\begin{subequations}\n\\begin{eqnarray}\n&& 0 \\leq \\lambda_{n} \\leq \\ldots \\leq \\lambda_{2} \\leq \\lambda_{1}, \\\\\n&& 0 < \\mu_{n} \\leq \\ldots \\leq \\mu_{2} \\leq \\mu_{1}, \\\\\n&& \\sum_{i=1}^{k} \\mu_{i} \\leq \\sum_{i=1}^{k} \\lambda_{i}, \\quad \\forall k < n,\\\\\n&& \\sum_{i=1}^{n} \\mu_{i} = \\sum_{i=1}^{n} \\lambda_{i}. \n\\end{eqnarray}\n\\label{major}\n\\end{subequations}\nNote that there is ``strict positivity'' assumption on $\\bm{\\mu}$, but not on $\\bm{\\lambda}$. Of course, it is clear that padding $\\bm{\\mu}$ (and as a result $\\bm{\\lambda}$) with 0's would not change the majorization order and simply add redundant components to both vectors.\nGiven $\\bm \\mu \\prec \\bm \\lambda$, $P(.;\\alpha,\\beta,x)$ is said to be Schur convex if  $P(\\bm{\\mu};\\alpha,\\beta,x) \\leq P(\\bm{\\lambda};\\alpha,\\beta,x)$, and it is said to be Schur concave if $P(\\bm{\\mu};\\alpha,\\beta,x) \\geq P(\\bm{\\lambda};\\alpha,\\beta,x)$. For comprehensive details on the theory of majorization and its applications, refer to the classic book of Marshall and Olkin~\\cite{marsholk79}. \n\nIt is not hard to show that the variance of $\\sum_{i=1}^{n}   \\lambda_{i}  X_{i}$ is a Schur-convex function of $\\bm{\\lambda}$ and, indeed, it would be useful to know when $P(.;\\alpha,\\beta,x)$ exhibit similar properties. For symmetric distributions a fairly general result is known.  If $X_{1},X_{2}\\ldots,X_{n}$ are independent random variables with a common\nsymmetric and log-concave PDF, in~\\cite{proschan}, it was shown that  $\\Pr \\left( \\sum_{i=1}^{n} \\lambda_{i} X_{i} < x \\right)$ is Schur-convex in $\\bm{\\lambda}$ for any $x > 0$. However, for positive random variables with non-symmetric distributions (such as gamma r.v's), to the best of our knowledge, no such general results, except for those in~\\cite{bock} and~\\cite{bakirov95}, exist.\n\nFor the case of $n \\geq 3$, Bock et al.,~\\cite[Theorem 3]{bock}, showed that if $\\bm \\mu \\prec \\bm \\lambda$ and $\\lambda_{i} > 0\\; \\forall i$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac{(n \\alpha + 1) \\max_{i} \\lambda_{i}}{\\beta}, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x < \\frac{(n \\alpha + 1) \\min_{i} \\lambda_{i}}{\\beta}.\n\\end{eqnarray*}\nFor the special case of chi-squared r.v's of degree one (i.e., $\\alpha = \\beta = 1/2$), Bakirov,~\\cite{bakirov95}, provided the following tighter bound (for $\\forall n \\geq 1$) for the Schur concavity of $P(. ;1/2,1/2,x)$:\n\n", "itemtype": "equation", "pos": 3355, "prevtext": "\nwhere $\\lambda_{i} \\geq 0, \\; i=1,2,\\ldots,n$, are real numbers. For a given $\\alpha > 0$, $\\beta > 0$ and $x > 0$, define \n\n", "index": 5, "text": "\\begin{equation}\nP(\\bm{\\lambda};\\alpha,\\beta,x) {\\mathrel{\\mathop:}=} \\Pr \\left( \\sum_{i=1}^{n} \\lambda_{i} X_{i} < x \\right),\n\\label{P_x}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"P(\\bm{\\lambda};\\alpha,\\beta,x){\\mathrel{\\mathop{:}}=}\\Pr\\left(\\sum_{i=1}^{n}%&#10;\\lambda_{i}X_{i}&lt;x\\right),\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf40</mi><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>Pr</mi><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>&lt;</mo><mi>x</mi><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $s = \\sum_{i=1}^{n} \\lambda_{i} = \\sum_{i=1}^{n} \\mu_{i}$. No results concerning the Schur convexity of $P(. ;1/2,1/2,x)$ was given in~\\cite{bakirov95}.\n\n\\input exline\n\nOur main result is stated as follows (the details of its proof are given in the Appendix). The discussions regarding the relative improvements compared to~\\cite{bock} and~\\cite{bakirov95}, as well as further extensions are deferred to Section~\\ref{sec:discuss}.\n\\begin{theorem}\nLet $X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots,n$, be $n$ i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. If $\\bm \\mu \\prec \\bm \\lambda$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s}{2 \\beta }, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n  \\begin{dcases}\n\t\t\\frac{\\alpha s}{\\beta},~& n = 2 \\cr\n\t\t\\frac{(\\alpha-1) s}{\\beta} ,~& n \\geq 3, \\; \\alpha > 1 \n\t\\end{dcases},\n\\end{eqnarray*}\nwhere $s = \\sum_{i=1}^{n} \\lambda_{i} = \\sum_{i=1}^{n} \\mu_{i}$.\n\\label{majorization_thm}\n\\end{theorem}\n\n\nThis paper is organized as follows. In Section~\\ref{sec:examples}, we give some examples of the practical applications of our results. In Section~\\ref{sec:discuss}, we discuss the relative improvements of our results compared to those in~\\cite{bock} and~\\cite{bakirov95}. We also extend Theorem~\\ref{majorization_thm} by weakening the majorization requirement. In addition, we give similar results for the case where $n = \\infty$. The proofs of our results are given in the Appendix.\n\n\\section{Examples}\n\\label{sec:examples}\nIn this section, we give examples to demonstrate some practical applications of our results.\n\\subsection{Experimental Design in Signal Detection}\n\\label{ex:signal}\nConsider the additive model of observations\n\n", "itemtype": "equation", "pos": 6686, "prevtext": "\nwhere $\\bm{\\lambda} = (\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}) \\in \\mathbb{R}^{n}$. The aim of the present paper is to find the conditions allowing one to compare tail probabilities of the form~\\eqref{P_x} with respect to the mixing weights $\\bm{\\lambda}$. In~\\cite[Theorem 2.2]{roszas}, results regarding the extremal values of~\\eqref{P_x} (i.e., maximal and minimal values with respect to $\\bm{\\lambda}$ and for given $\\alpha,\\beta$ and $x$) are proved. Here, we extend those results to be able to compare $\\eqref{P_x}$ for any pair of weight vectors.\n\nTo that end, let us recall that the vector $\\bm \\lambda$ is said to \\textit{majorize} the vector $\\bm \\mu$, denoted by $\\bm \\mu \\prec \\bm \\lambda$, if \n\\begin{subequations}\n\\begin{eqnarray}\n&& 0 \\leq \\lambda_{n} \\leq \\ldots \\leq \\lambda_{2} \\leq \\lambda_{1}, \\\\\n&& 0 < \\mu_{n} \\leq \\ldots \\leq \\mu_{2} \\leq \\mu_{1}, \\\\\n&& \\sum_{i=1}^{k} \\mu_{i} \\leq \\sum_{i=1}^{k} \\lambda_{i}, \\quad \\forall k < n,\\\\\n&& \\sum_{i=1}^{n} \\mu_{i} = \\sum_{i=1}^{n} \\lambda_{i}. \n\\end{eqnarray}\n\\label{major}\n\\end{subequations}\nNote that there is ``strict positivity'' assumption on $\\bm{\\mu}$, but not on $\\bm{\\lambda}$. Of course, it is clear that padding $\\bm{\\mu}$ (and as a result $\\bm{\\lambda}$) with 0's would not change the majorization order and simply add redundant components to both vectors.\nGiven $\\bm \\mu \\prec \\bm \\lambda$, $P(.;\\alpha,\\beta,x)$ is said to be Schur convex if  $P(\\bm{\\mu};\\alpha,\\beta,x) \\leq P(\\bm{\\lambda};\\alpha,\\beta,x)$, and it is said to be Schur concave if $P(\\bm{\\mu};\\alpha,\\beta,x) \\geq P(\\bm{\\lambda};\\alpha,\\beta,x)$. For comprehensive details on the theory of majorization and its applications, refer to the classic book of Marshall and Olkin~\\cite{marsholk79}. \n\nIt is not hard to show that the variance of $\\sum_{i=1}^{n}   \\lambda_{i}  X_{i}$ is a Schur-convex function of $\\bm{\\lambda}$ and, indeed, it would be useful to know when $P(.;\\alpha,\\beta,x)$ exhibit similar properties. For symmetric distributions a fairly general result is known.  If $X_{1},X_{2}\\ldots,X_{n}$ are independent random variables with a common\nsymmetric and log-concave PDF, in~\\cite{proschan}, it was shown that  $\\Pr \\left( \\sum_{i=1}^{n} \\lambda_{i} X_{i} < x \\right)$ is Schur-convex in $\\bm{\\lambda}$ for any $x > 0$. However, for positive random variables with non-symmetric distributions (such as gamma r.v's), to the best of our knowledge, no such general results, except for those in~\\cite{bock} and~\\cite{bakirov95}, exist.\n\nFor the case of $n \\geq 3$, Bock et al.,~\\cite[Theorem 3]{bock}, showed that if $\\bm \\mu \\prec \\bm \\lambda$ and $\\lambda_{i} > 0\\; \\forall i$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac{(n \\alpha + 1) \\max_{i} \\lambda_{i}}{\\beta}, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x < \\frac{(n \\alpha + 1) \\min_{i} \\lambda_{i}}{\\beta}.\n\\end{eqnarray*}\nFor the special case of chi-squared r.v's of degree one (i.e., $\\alpha = \\beta = 1/2$), Bakirov,~\\cite{bakirov95}, provided the following tighter bound (for $\\forall n \\geq 1$) for the Schur concavity of $P(. ;1/2,1/2,x)$:\n\n", "index": 7, "text": "\\begin{equation*}\nP(\\bm{\\mu};{\\frac12},{\\frac12},x) \\geq P(\\bm{\\lambda};{\\frac12},{\\frac12},x), \\quad \\forall x > 2 s,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"P(\\bm{\\mu};{\\frac{1}{2}},{\\frac{1}{2}},x)\\geq P(\\bm{\\lambda};{\\frac{1}{2}},{%&#10;\\frac{1}{2}},x),\\quad\\forall x&gt;2s,\" display=\"block\"><mrow><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf41</mi><mo>;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf40</mi><mo>;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>x</mi></mrow><mo>&gt;</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>s</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $D(t)$ is the measured data, $s(t)$ is the signal of interest and $\\eta(t)$ is the additive noise. Suppose we have $N$ measurements, taken at discrete time intervals, $0 = t_{1} \\leq t_{1} \\leq \\ldots \\leq t_{N} = T$. In addition, suppose that $\\{\\eta(t_{i}): i = 1,2,\\ldots,N\\}$ is a collection of i.i.d Laplace r.v's with mean zero and variance $\\sigma_{i}^{2}$. In addition, let $\\tau = 1$ if there is a signal, and $\\tau = 0$ otherwise (i.e., the measured data is in fact entirely the noise). To detect if the signal is present, we can use\nthe signal to noise ratio (SNR)\n\n", "itemtype": "equation", "pos": 8638, "prevtext": "\nwhere $s = \\sum_{i=1}^{n} \\lambda_{i} = \\sum_{i=1}^{n} \\mu_{i}$. No results concerning the Schur convexity of $P(. ;1/2,1/2,x)$ was given in~\\cite{bakirov95}.\n\n\\input exline\n\nOur main result is stated as follows (the details of its proof are given in the Appendix). The discussions regarding the relative improvements compared to~\\cite{bock} and~\\cite{bakirov95}, as well as further extensions are deferred to Section~\\ref{sec:discuss}.\n\\begin{theorem}\nLet $X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots,n$, be $n$ i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. If $\\bm \\mu \\prec \\bm \\lambda$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s}{2 \\beta }, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n  \\begin{dcases}\n\t\t\\frac{\\alpha s}{\\beta},~& n = 2 \\cr\n\t\t\\frac{(\\alpha-1) s}{\\beta} ,~& n \\geq 3, \\; \\alpha > 1 \n\t\\end{dcases},\n\\end{eqnarray*}\nwhere $s = \\sum_{i=1}^{n} \\lambda_{i} = \\sum_{i=1}^{n} \\mu_{i}$.\n\\label{majorization_thm}\n\\end{theorem}\n\n\nThis paper is organized as follows. In Section~\\ref{sec:examples}, we give some examples of the practical applications of our results. In Section~\\ref{sec:discuss}, we discuss the relative improvements of our results compared to those in~\\cite{bock} and~\\cite{bakirov95}. We also extend Theorem~\\ref{majorization_thm} by weakening the majorization requirement. In addition, we give similar results for the case where $n = \\infty$. The proofs of our results are given in the Appendix.\n\n\\section{Examples}\n\\label{sec:examples}\nIn this section, we give examples to demonstrate some practical applications of our results.\n\\subsection{Experimental Design in Signal Detection}\n\\label{ex:signal}\nConsider the additive model of observations\n\n", "index": 9, "text": "\\begin{equation*}\nD(t) = \\tau s(t) + \\eta(t), \\quad t \\in [0, T]\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"D(t)=\\tau s(t)+\\eta(t),\\quad t\\in[0,T]\" display=\"block\"><mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03c4</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nNote that $\\sigma_{i}^{-1} D(t)$ is a Laplace r.v, with mean $\\tau s(t)$ and variance $1$. \n\nA design question to answer is that of , at least how many measurements, a priori, is needed to make sure the probability of Type I error (i.e., when we conclude that the signal is present, when in fact it is missing) is below a desired tolerance $0 < \\delta \\ll 1$.\n\nIf $\\tau = 0$, then $\\sigma_{i}^{-1} | D(t)|  \\sim Gamma (1,\\sqrt{2})$, so by Theorem~\\ref{majorization_thm} we get\n\n", "itemtype": "equation", "pos": 9301, "prevtext": "\nwhere $D(t)$ is the measured data, $s(t)$ is the signal of interest and $\\eta(t)$ is the additive noise. Suppose we have $N$ measurements, taken at discrete time intervals, $0 = t_{1} \\leq t_{1} \\leq \\ldots \\leq t_{N} = T$. In addition, suppose that $\\{\\eta(t_{i}): i = 1,2,\\ldots,N\\}$ is a collection of i.i.d Laplace r.v's with mean zero and variance $\\sigma_{i}^{2}$. In addition, let $\\tau = 1$ if there is a signal, and $\\tau = 0$ otherwise (i.e., the measured data is in fact entirely the noise). To detect if the signal is present, we can use\nthe signal to noise ratio (SNR)\n\n", "index": 11, "text": "\\begin{equation*}\nQ(N) {\\mathrel{\\mathop:}=} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}}| D(t_{i})|.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"Q(N){\\mathrel{\\mathop{:}}=}\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}}|D(t_{%&#10;i})|.\" display=\"block\"><mrow><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mn>1</mn><msub><mi>\u03c3</mi><mi>i</mi></msub></mfrac><mo stretchy=\"false\">|</mo><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nFor a given $N$, we can easily compute $P(Q(N) \\geq x)$. Hence, in order to find the minimum $N$ required, we can increase $N$ until $P(Q(N) \\geq x) \\leq \\delta$.\n\n\n\\subsection{Matrix Trace Estimation}\n\\label{sec:trace}\n\nThe need to estimate the trace of an implicit symmetric positive semi-definite (SPSD) matrix\n\nis of fundamental importance (see~\\cite{sdr}) and arises in many applications; \nsee for instance \\cite{hutchinson,bafago,avto,HaberChungHermann2010,doas3,yori,rodoas1,rodoas2,learhe,gohewa,avron}\nand references therein.\n\n\nThe standard approach for estimating the trace of such a matrix $A$, denoted here by $tr(A)$, is based on a Monte-Carlo method, where one generates $N$ \nrandom vector realizations ${ {\\bf w} }_{i}$ from a suitable probability distribution $D$ and computes \n\n", "itemtype": "equation", "pos": 9899, "prevtext": "\nNote that $\\sigma_{i}^{-1} D(t)$ is a Laplace r.v, with mean $\\tau s(t)$ and variance $1$. \n\nA design question to answer is that of , at least how many measurements, a priori, is needed to make sure the probability of Type I error (i.e., when we conclude that the signal is present, when in fact it is missing) is below a desired tolerance $0 < \\delta \\ll 1$.\n\nIf $\\tau = 0$, then $\\sigma_{i}^{-1} | D(t)|  \\sim Gamma (1,\\sqrt{2})$, so by Theorem~\\ref{majorization_thm} we get\n\n", "index": 13, "text": "\\begin{equation*}\nP(Q(1) \\geq x) \\geq P(Q(2) \\geq x) \\geq \\ldots \\geq P(Q(N) \\geq x) \\geq \\ldots ,\\quad \\forall x > \\frac{3}{2\\sqrt{2}}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"P(Q(1)\\geq x)\\geq P(Q(2)\\geq x)\\geq\\ldots\\geq P(Q(N)\\geq x)\\geq\\ldots,\\quad%&#10;\\forall x&gt;\\frac{3}{2\\sqrt{2}}.\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2265</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi mathvariant=\"normal\">\u2026</mi><mo rspace=\"12.5pt\">,</mo><mo>\u2200</mo><mi>x</mi><mo>&gt;</mo><mfrac><mn>3</mn><mrow><mn>2</mn><mo>\u2062</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nOne such suitable probability distribution is the standard normal, $\\mathcal{N}(0,\\mathbb{I})$. This estimator is known as the \\textit{Gaussian estimator}, denoted here by $tr_{G}^{N}(A)$.\n\nNow, given a pair of small positive real numbers $({\\varepsilon},\\delta)$, \nconsider finding an appropriate sample size $N$ such that\n\\begin{subequations}\n\\begin{eqnarray}\n\\Pr\\Big( tr_{G}^{N}(A) \\geq (1-{\\varepsilon}) tr(A) \\Big) \\geq 1-\\delta \\label{prob_ineq_lower}, \\\\\n\\Pr\\Big( tr_{G}^{N}(A) \\leq (1+{\\varepsilon}) tr(A) \\Big) \\geq 1-\\delta \\label{prob_ineq_upper}.\n\\end{eqnarray}\n\\label{prob_ineq_lower_upper}\n\\end{subequations}\nSuch question was first studied in~\\cite{avto} and further improved in~\\cite{roas1}. In particular, in~\\cite{roas1} it was proved that the inequalities~\\eqref{prob_ineq_lower_upper}\nhold if\n\n", "itemtype": "equation", "pos": 10846, "prevtext": "\nFor a given $N$, we can easily compute $P(Q(N) \\geq x)$. Hence, in order to find the minimum $N$ required, we can increase $N$ until $P(Q(N) \\geq x) \\leq \\delta$.\n\n\n\\subsection{Matrix Trace Estimation}\n\\label{sec:trace}\n\nThe need to estimate the trace of an implicit symmetric positive semi-definite (SPSD) matrix\n\nis of fundamental importance (see~\\cite{sdr}) and arises in many applications; \nsee for instance \\cite{hutchinson,bafago,avto,HaberChungHermann2010,doas3,yori,rodoas1,rodoas2,learhe,gohewa,avron}\nand references therein.\n\n\nThe standard approach for estimating the trace of such a matrix $A$, denoted here by $tr(A)$, is based on a Monte-Carlo method, where one generates $N$ \nrandom vector realizations ${ {\\bf w} }_{i}$ from a suitable probability distribution $D$ and computes \n\n", "index": 15, "text": "\\begin{equation*}\ntr_{D}^{N}(A) {\\mathrel{\\mathop:}=} \\frac{1}{N} \\sum_{i=1}^{N} { {\\bf w} }_{i}^{t} A { {\\bf w} }_{i} .\n\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"tr_{D}^{N}(A){\\mathrel{\\mathop{:}}=}\\frac{1}{N}\\sum_{i=1}^{N}{{\\bf w}}_{i}^{t}%&#10;A{{\\bf w}}_{i}.\\par&#10;\" display=\"block\"><mrow><mi>t</mi><msubsup><mi>r</mi><mi>D</mi><mi>N</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>\ud835\udc30</mi><mi>i</mi><mi>t</mi></msubsup><mi>A</mi><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $\\| A \\|$ denotes the $L_{2}$ norm of the matrix $A$. The ratio $tr(A)/ \\| A \\|$ is known as the \\textit{effective rank} of the matrix (see~\\cite{eldar2012compressed}) and it is a stable quantity compared with the usual rank. \n\nThe appearance of effective rank in the bound~\\eqref{gauss_bd} is an indication of a possible relation between the ``\\textit{skewness}'' of the eigenvalues of $A$ and the efficiency of the Gaussian estimator. In other words, the more skewed the eigenvalue distribution is, the worse we expect the Gaussian estimator to perform (i.e., the larger the true sample size required would be). However effective rank is not a consistent measure for skewness and, as such,\n\nin~\\cite{roas1}, this relationship was demonstrated only numerically and no consistent definition for how the relative skewness could be measured was given. Now using the majorization order among eigenvalue vectors as a consistent measure of skewness, the new theoretical results in the present paper fully  describe the observations from the numerical examples in~\\cite{roas1}. As in the proof of~\\cite[Theorem 1]{roas1}, we see that \n\n", "itemtype": "equation", "pos": 11797, "prevtext": "\nOne such suitable probability distribution is the standard normal, $\\mathcal{N}(0,\\mathbb{I})$. This estimator is known as the \\textit{Gaussian estimator}, denoted here by $tr_{G}^{N}(A)$.\n\nNow, given a pair of small positive real numbers $({\\varepsilon},\\delta)$, \nconsider finding an appropriate sample size $N$ such that\n\\begin{subequations}\n\\begin{eqnarray}\n\\Pr\\Big( tr_{G}^{N}(A) \\geq (1-{\\varepsilon}) tr(A) \\Big) \\geq 1-\\delta \\label{prob_ineq_lower}, \\\\\n\\Pr\\Big( tr_{G}^{N}(A) \\leq (1+{\\varepsilon}) tr(A) \\Big) \\geq 1-\\delta \\label{prob_ineq_upper}.\n\\end{eqnarray}\n\\label{prob_ineq_lower_upper}\n\\end{subequations}\nSuch question was first studied in~\\cite{avto} and further improved in~\\cite{roas1}. In particular, in~\\cite{roas1} it was proved that the inequalities~\\eqref{prob_ineq_lower_upper}\nhold if\n\n", "index": 17, "text": "\\begin{equation}\nN > \\frac{\\| A \\|}{tr(A)} \\frac{8}{{\\varepsilon}^{2}} \\ln (\\frac{1}{\\delta}),\n\\label{gauss_bd}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"N&gt;\\frac{\\|A\\|}{tr(A)}\\frac{8}{{\\varepsilon}^{2}}\\ln(\\frac{1}{\\delta}),\" display=\"block\"><mrow><mrow><mi>N</mi><mo>&gt;</mo><mrow><mfrac><mrow><mo>\u2225</mo><mi>A</mi><mo>\u2225</mo></mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mfrac><mn>8</mn><msup><mi>\u03b5</mi><mn>2</mn></msup></mfrac><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><mn>1</mn><mi>\u03b4</mi></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $\\bm{\\lambda}$ the vector of eigenvalues of $A$ sorted in the decreasing order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider two SPSD matrices, $A_{1}$ and $A_{2}$, such that $tr(A_{1}) = tr(A_{2})$ and whose respective eigenvalue vectors, $\\bm{\\lambda}_{1}$ and $\\bm{\\lambda}_{2}$, are sorted in the decreasing order. If $\\bm{\\lambda}_{2} \\prec \\bm{\\lambda}_{1}$, then we say that the eigenvalue distribution of $A_{1}$ is more skewed that that of $A_{2}$. If $N > 2/{\\varepsilon}$, then from Theorem~\\ref{majorization_thm} we obtain\n\n", "itemtype": "equation", "pos": 13059, "prevtext": "\nwhere $\\| A \\|$ denotes the $L_{2}$ norm of the matrix $A$. The ratio $tr(A)/ \\| A \\|$ is known as the \\textit{effective rank} of the matrix (see~\\cite{eldar2012compressed}) and it is a stable quantity compared with the usual rank. \n\nThe appearance of effective rank in the bound~\\eqref{gauss_bd} is an indication of a possible relation between the ``\\textit{skewness}'' of the eigenvalues of $A$ and the efficiency of the Gaussian estimator. In other words, the more skewed the eigenvalue distribution is, the worse we expect the Gaussian estimator to perform (i.e., the larger the true sample size required would be). However effective rank is not a consistent measure for skewness and, as such,\n\nin~\\cite{roas1}, this relationship was demonstrated only numerically and no consistent definition for how the relative skewness could be measured was given. Now using the majorization order among eigenvalue vectors as a consistent measure of skewness, the new theoretical results in the present paper fully  describe the observations from the numerical examples in~\\cite{roas1}. As in the proof of~\\cite[Theorem 1]{roas1}, we see that \n\n", "index": 19, "text": "\\begin{equation}\nPr\\left( tr_{H}^{N}(A) \\leq (1-{\\varepsilon})tr(A) \\right)  = P\\left(\\bm{\\lambda};\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon}) tr(A)\\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"Pr\\left(tr_{H}^{N}(A)\\leq(1-{\\varepsilon})tr(A)\\right)=P\\left(\\bm{\\lambda};%&#10;\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon})tr(A)\\right),\" display=\"block\"><mrow><mi>P</mi><mi>r</mi><mrow><mo>(</mo><mi>t</mi><msubsup><mi>r</mi><mi>H</mi><mi>N</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><mi>t</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo>(</mo><mi>\ud835\udf40</mi><mo>;</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><mi>t</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nIn other words, using the same sample size $N$, our estimate with $A_{1}$ is more likely to be located further away to the left of the true value than that with $A_{2}$. Hence in order to make the former estimate better, we need to increase the sample size which, in turn, in some algorithms translates into more computational costs; see~\\cite{HaberChungHermann2010,doas3, learhe,rodoas1,rodoas2,roszas}. \n\n\n\n\n\nSimilar comparisons can be made for $Pr\\left( tr_{H}^{N}(A) \\leq (1+{\\varepsilon})tr(A) \\right)$. \n\n\n\\section{Discussions and Further Extensions}\n\\label{sec:discuss}\n\n\n\n\n\nThe comparison between the results in Theorem~\\ref{majorization_thm} and those in~\\cite{bock} and~\\cite{bakirov95} can be summarized as follows:\n\\begin{itemize}\n\\item For the special case of chi-squared distribution, i.e., $\\alpha = \\beta = 1/2$, our Schur concavity result is the same as that in~\\cite{bakirov95}.\n\\item For $n=2$, the results of Theorem~\\ref{majorization_thm} coincide with~\\cite[Theorem 1]{bock}. However, for $n \\geq 3$, our results are more uniform than~\\cite[Theorem 3]{bock}. Namely, the sufficient conditions given in Theorem~\\ref{majorization_thm} are not dependent on the dimensions of the vectors. Additionally, our bounds for $x$ are independent of the particular values of the vector components, such as ``$\\min_{i} \\lambda_{i}$'' or ``$\\max_{i} \\lambda_{i}$''. In other words, for all vectors whose sums are equal, we give a fixed bound for $x$. \n\\item For $n \\geq 3$, our Schur convexity result is sharper than that of~\\cite[Theorem 3]{bock} when \n", "itemtype": "equation", "pos": 13750, "prevtext": "\nwhere $\\bm{\\lambda}$ the vector of eigenvalues of $A$ sorted in the decreasing order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider two SPSD matrices, $A_{1}$ and $A_{2}$, such that $tr(A_{1}) = tr(A_{2})$ and whose respective eigenvalue vectors, $\\bm{\\lambda}_{1}$ and $\\bm{\\lambda}_{2}$, are sorted in the decreasing order. If $\\bm{\\lambda}_{2} \\prec \\bm{\\lambda}_{1}$, then we say that the eigenvalue distribution of $A_{1}$ is more skewed that that of $A_{2}$. If $N > 2/{\\varepsilon}$, then from Theorem~\\ref{majorization_thm} we obtain\n\n", "index": 21, "text": "\\begin{equation*}\nP \\left( \\bm{\\lambda}_{2};\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon}) tr(A_{2}) \\right) \\leq P \\left( \\bm{\\lambda}_{1};\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon}) tr(A_{1}) \\right).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"P\\left(\\bm{\\lambda}_{2};\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon})tr(A_{2})%&#10;\\right)\\leq P\\left(\\bm{\\lambda}_{1};\\frac{N}{2},\\frac{N}{2},(1-{\\varepsilon})%&#10;tr(A_{1})\\right).\" display=\"block\"><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udf40</mi><mn>2</mn></msub><mo>;</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udf40</mi><mn>1</mn></msub><mo>;</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": " Similarly, for $n \\geq 3$ and  $\\alpha > 1$, our Schur concavity result improves that of ~\\cite[Theorem 3]{bock} when \n", "itemtype": "equation", "pos": -1, "prevtext": "\nIn other words, using the same sample size $N$, our estimate with $A_{1}$ is more likely to be located further away to the left of the true value than that with $A_{2}$. Hence in order to make the former estimate better, we need to increase the sample size which, in turn, in some algorithms translates into more computational costs; see~\\cite{HaberChungHermann2010,doas3, learhe,rodoas1,rodoas2,roszas}. \n\n\n\n\n\nSimilar comparisons can be made for $Pr\\left( tr_{H}^{N}(A) \\leq (1+{\\varepsilon})tr(A) \\right)$. \n\n\n\\section{Discussions and Further Extensions}\n\\label{sec:discuss}\n\n\n\n\n\nThe comparison between the results in Theorem~\\ref{majorization_thm} and those in~\\cite{bock} and~\\cite{bakirov95} can be summarized as follows:\n\\begin{itemize}\n\\item For the special case of chi-squared distribution, i.e., $\\alpha = \\beta = 1/2$, our Schur concavity result is the same as that in~\\cite{bakirov95}.\n\\item For $n=2$, the results of Theorem~\\ref{majorization_thm} coincide with~\\cite[Theorem 1]{bock}. However, for $n \\geq 3$, our results are more uniform than~\\cite[Theorem 3]{bock}. Namely, the sufficient conditions given in Theorem~\\ref{majorization_thm} are not dependent on the dimensions of the vectors. Additionally, our bounds for $x$ are independent of the particular values of the vector components, such as ``$\\min_{i} \\lambda_{i}$'' or ``$\\max_{i} \\lambda_{i}$''. In other words, for all vectors whose sums are equal, we give a fixed bound for $x$. \n\\item For $n \\geq 3$, our Schur convexity result is sharper than that of~\\cite[Theorem 3]{bock} when \n", "index": 23, "text": "$$\\max_{i} \\lambda_{i} > \\frac{\\sum_{i} \\lambda_{i}}{n}\\frac{\\alpha+{\\frac12}}{\\alpha+1/n}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\max_{i}\\lambda_{i}&gt;\\frac{\\sum_{i}\\lambda_{i}}{n}\\frac{\\alpha+{\\frac{1}{2}}}{%&#10;\\alpha+1/n}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mi>i</mi></munder><mo>\u2061</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>&gt;</mo><mrow><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mi>n</mi></mfrac><mo>\u2062</mo><mfrac><mrow><mi>\u03b1</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mrow><mi>\u03b1</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\n\\item Most importantly, in~\\cite[Theorem 3]{bock}, it is required that $\\lambda_{i} > 0,\\; i = 1,2,\\ldots,n$. This is a rather strong condition as many interesting comparisons cannot be performed this way. Our results do not make any strict positivity assumption on the components of $\\bm{\\lambda}$ and it suffices if they are simply non-negative. As a simple example, consider $\\alpha = 2$, $\\beta = 1$ and $x < 1$. Then using Theorem~\\ref{majorization_thm}, we get $P(\\bm{\\lambda}_{1};2,1,x) \\geq P(\\bm{\\lambda}_{2};2,1,x) \\geq P(\\bm{\\lambda}_{3};2,1,x) \\geq P(\\bm{\\lambda}_{4};2,1,x)$ with $\\bm{\\lambda}_{1} = (1,0,0,0)$, $\\bm{\\lambda}_{2} = (2/3,1/3,0,0)$, $\\bm{\\lambda}_{3} = (3/6,2/6,1/6,0)$, and $\\bm{\\lambda}_{4} = (4/10,3/10,2/10,1/10)$. This comparison is not possible with~\\cite[Theorem 3]{bock}. \n\\item For the Schur convexity of $P(.;\\alpha,\\beta,x)$ in the case of $\\alpha \\leq 1$ and $n \\geq 3$, the result in~\\cite[Theorem 3]{bock} remains the best available, as we were not able to improve upon it here. In fact, from our method of proof, it seems likely that obtaining a general result for this case is impossible. Indeed, Bock et al.\\ \\cite[p. 394]{bock} give an example which corroborates this observation.\n\\end{itemize}\n\nIt is possible to weaken the majorization requirement and obtain even more general results. More specifically, relaxing the equality condition in~\\eqref{major} gives the following weak majorization order. Recall that the vector \n$\\bm \\lambda$ is said to \\textit{weakly majorize} the vector $\\bm \\mu$, denoted by $\\bm \\mu \\prec_{w} \\bm \\lambda$, if \n\\begin{subequations}\n\\begin{eqnarray*}\n&& 0 \\leq \\lambda_{n} \\leq \\ldots \\leq \\lambda_{2} \\leq \\lambda_{1}, \\\\\n&& 0 < \\mu_{n} \\leq \\ldots \\leq \\mu_{2} \\leq \\mu_{1}, \\\\\n&& \\sum_{i=1}^{k} \\mu_{i} \\leq \\sum_{i=1}^{k} \\lambda_{i}, \\quad \\forall k \\leq n.\n\\end{eqnarray*}\n\\label{major_w}\n\\end{subequations}\n\n\nIn the case of weak majorization order, we have the following almost immediate corollary.\n\\begin{corollary}\nLet $X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots,n$, be $n$ i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. If $\\bm \\mu \\prec_{w} \\bm \\lambda$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s_{\\bm{\\lambda}}}{2 \\beta }, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n  \\begin{dcases}\n\t\t\\frac{\\alpha s_{\\bm{\\mu}}}{\\beta},~& n = 2 \\cr\n\t\t\\frac{(\\alpha-1) s_{\\bm{\\mu}}}{\\beta} ,~& n \\geq 3, \\; \\alpha > 1 \n\t\\end{dcases},\n\\end{eqnarray*}\nwhere $s_{\\bm{\\lambda}} = \\sum_{i=1}^{n} \\lambda_{i}$ and $s_{\\bm{\\mu}} = \\sum_{i=1}^{n} \\mu_{i}$.\n\\label{majorization_cor_weak}\n\\end{corollary}\n\nWe can also extend Theorem~\\ref{majorization_thm}, as well as Corollary~\\ref{majorization_cor_weak}, for the case where $ n = \\infty$. For any non-negative  $\\ell_{1}$ sequence $\\bm{\\lambda} = (\\lambda_{1}, \\lambda_{1}, \\ldots)$, i.e., $\\sum_{i=1}^{\\infty} \\lambda_{i} < \\infty$, define\n\n", "itemtype": "equation", "pos": 15734, "prevtext": " Similarly, for $n \\geq 3$ and  $\\alpha > 1$, our Schur concavity result improves that of ~\\cite[Theorem 3]{bock} when \n", "index": 25, "text": "$$\\min_{i} \\lambda_{i} < \\frac{\\sum_{i} \\lambda_{i}}{n}\\frac{\\alpha-1}{\\alpha+1/n}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\min_{i}\\lambda_{i}&lt;\\frac{\\sum_{i}\\lambda_{i}}{n}\\frac{\\alpha-1}{\\alpha+1/n}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>i</mi></munder><mo>\u2061</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>&lt;</mo><mrow><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mi>n</mi></mfrac><mo>\u2062</mo><mfrac><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>\u03b1</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nThe (weak) majorization order is naturally extended to such $\\ell_{1}$ sequences.\n\\begin{corollary}\nLet $\\left \\{X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots \\right \\}$ be a countably infinite  collection of i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. For any two non-negative $\\ell_{1}$ sequences, $\\bm \\mu$ and $\\bm \\lambda$, such that $\\bm \\mu \\prec_{w} \\bm \\lambda$, we have \n\\begin{eqnarray*}\nP_{\\infty}(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s_{\\bm{\\lambda}}}{2 \\beta }, \\\\\nP_{\\infty}(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n\t\t\\frac{(\\alpha-1) s_{\\bm{\\mu}}}{\\beta} , \\quad \\alpha > 1,\n\\end{eqnarray*}\nwhere $s_{\\bm{\\lambda}} = \\sum_{i=1}^{\\infty} \\lambda_{i}$ and $s_{\\bm{\\mu}} = \\sum_{i=1}^{\\infty} \\mu_{i}$.\n\\label{majorization_cor_weak_infty}\n\\end{corollary}\n\n\\input exline\n\nFinally, it might be worth noting that the results such as Theorem~\\ref{majorization_thm} show that if $\\bm{\\mu} \\prec \\bm{\\lambda}$, then $P(\\bm{\\mu};\\alpha,\\beta,x)$ and $P(\\bm{\\lambda};\\alpha,\\beta,x)$ must have at least one crossing on $x \\in (0,\\infty)$.  \nDiaconis and Perlman in~\\cite{diaconis1990bounds} tried to answer whether this crossing point is unique. However, they only proved this uniqueness for $n=2$ and for $n \\geq 3$, they required to impose further restrictions. Ever since, this has been an open problem which is known as the Unique Crossing Conjecture (UCC) and it is quite remarkable that the UCC has remained open, although all the evidence points towards the direction of it being true.\n\n\\input exline\n\n\\noindent{\\bf Acknowledgment}\nWe wish to thank Profs.\\ Milan Merkle and Maochao Xu for their valuable comments during the preparation of the text.\n\n\n\\bibliographystyle{plain}\n\\bibliography{biblio}\n\n\\appendix\n\\section{Proof}\n\\label{proof}\nIn what follows $X \\sim Gamma(\\alpha,\\beta)$ denotes a gamma r.v parametrized by shape $\\alpha > 0$ and rate $\\beta > 0$, $f_{X}$ and $F_{X}$ stand, respectively, for the probability density function (PDF) and cumulative distribution function (CDF) of a r.v $X$. Bold face letters denote vectors and the vector components are denoted as ${ {\\bf v} } = (v_{1},v_{2},\\ldots,v_{n})$. \n\nFor the proof of Theorem~\\ref{majorization_thm}, we need to make use of the following additional results. The proof of Theorem~\\ref{theorem_unimodal} is identical to the proof of~\\cite[Theorem 4]{szba}. Note that~\\cite[Theorem 4]{szba} has been stated in terms of convolution of chi-squared r.v's but the proof, there, has been given for the more general case of arbitrary gamma r.v's. The details of the proofs for Lemmas~\\ref{lemma_A*} and~\\ref{lemma_2*} can be found in~\\cite{roszas}. Lemma~\\ref{lemma_1*} has been stated in~\\cite{roszas} but the proof is omitted there. We give a detailed proof of Lemma~\\ref{lemma_1*} here for completeness.\n\nTheorem~\\ref{theorem_unimodal} is essential in proving our results and it states that an arbitrary convolution of heterogeneous gamma random variables (not necessarily with a common shape or a common rate) has an unique mode. Recall that a PDF, $f(x)$, is called unimodal if there exists a unique $x = a$ such that $f(x)$ is non-decreasing for $x < a$ and $f(x)$ is non-increasing for $x > a$. The point $a$ is called the unique mode of $f(x)$.\n\n\\begin{theorem}\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\beta_{i}),\\;  i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i}, \\beta_{i} > 0 \\; \\forall i$. The PDF of $Y_{n} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ is unimodal where $\\lambda_{i} \\geq 0 \\; \\forall i$.\n\\label{theorem_unimodal}\n\\end{theorem}\n\n\n\\begin{lemma}[{\\cite[Lemma B.1]{roszas}}]\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\beta_{i}),\\;  i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i}, \\beta_{i} > 0 \\; \\forall i$. Define $Y_{n} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ for $\\lambda_{i} > 0$, $\\forall i$ and $\\rho_{j} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{j} \\alpha_{i}$.\nThen for the PDF of $Y_{n}$, $f_{Y_{n}}$, we have\n\\begin{enumerate}[(i)]\n\t\\item $f_{Y_{n}} > 0$, $\\forall x > 0$,\n\t\\item $f_{Y_{n}}$ is analytic on $\\mathbb{R}^{+} = \\{x | x > 0\\}$,\n\t\\item $f_{Y_{n}}^{(k)}(0) = 0$, if $0 \\leq k < \\rho_{n} - 1$, where $f_{Y_{n}}^{(k)}$ denotes the $k^{th}$ derivative of $f_{Y_{n}}$.\n\\end{enumerate} \n\\label{lemma_A*}\n\\end{lemma}\n\n\\begin{lemma}[{\\cite[Lemma B.2]{roszas}}]\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\alpha), \\; i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i} > 0 \\; \\forall i$ and $\\alpha > 0$. Also let $\\psi \\sim Gamma(1,\\alpha)$ be another r.v independent of all $X_{i}$'s. If $\\sum_{i=1}^{n} \\alpha_{i} > 1$, then the mode, $\\bar{x}(\\lambda)$, of the r.v \n\n$W({\\lambda}) = Y + \\lambda \\psi$\n\nis strictly increasing in $\\lambda>0$, where $Y = \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ with $\\lambda_{i} > 0$, $\\forall i$.\n\\label{lemma_1*}\n\\end{lemma}\n\\begin{proof}\nBy Lemma~\\ref{lemma_A*},  $\\bar{x}(\\lambda) > 0$ for $\\lambda \\geq 0$. By the unimodality of $W({\\lambda})$, for any $\\lambda > \\lambda_{0} > 0$, it is enough to show that \n\n", "itemtype": "equation", "pos": 18822, "prevtext": "\n\\item Most importantly, in~\\cite[Theorem 3]{bock}, it is required that $\\lambda_{i} > 0,\\; i = 1,2,\\ldots,n$. This is a rather strong condition as many interesting comparisons cannot be performed this way. Our results do not make any strict positivity assumption on the components of $\\bm{\\lambda}$ and it suffices if they are simply non-negative. As a simple example, consider $\\alpha = 2$, $\\beta = 1$ and $x < 1$. Then using Theorem~\\ref{majorization_thm}, we get $P(\\bm{\\lambda}_{1};2,1,x) \\geq P(\\bm{\\lambda}_{2};2,1,x) \\geq P(\\bm{\\lambda}_{3};2,1,x) \\geq P(\\bm{\\lambda}_{4};2,1,x)$ with $\\bm{\\lambda}_{1} = (1,0,0,0)$, $\\bm{\\lambda}_{2} = (2/3,1/3,0,0)$, $\\bm{\\lambda}_{3} = (3/6,2/6,1/6,0)$, and $\\bm{\\lambda}_{4} = (4/10,3/10,2/10,1/10)$. This comparison is not possible with~\\cite[Theorem 3]{bock}. \n\\item For the Schur convexity of $P(.;\\alpha,\\beta,x)$ in the case of $\\alpha \\leq 1$ and $n \\geq 3$, the result in~\\cite[Theorem 3]{bock} remains the best available, as we were not able to improve upon it here. In fact, from our method of proof, it seems likely that obtaining a general result for this case is impossible. Indeed, Bock et al.\\ \\cite[p. 394]{bock} give an example which corroborates this observation.\n\\end{itemize}\n\nIt is possible to weaken the majorization requirement and obtain even more general results. More specifically, relaxing the equality condition in~\\eqref{major} gives the following weak majorization order. Recall that the vector \n$\\bm \\lambda$ is said to \\textit{weakly majorize} the vector $\\bm \\mu$, denoted by $\\bm \\mu \\prec_{w} \\bm \\lambda$, if \n\\begin{subequations}\n\\begin{eqnarray*}\n&& 0 \\leq \\lambda_{n} \\leq \\ldots \\leq \\lambda_{2} \\leq \\lambda_{1}, \\\\\n&& 0 < \\mu_{n} \\leq \\ldots \\leq \\mu_{2} \\leq \\mu_{1}, \\\\\n&& \\sum_{i=1}^{k} \\mu_{i} \\leq \\sum_{i=1}^{k} \\lambda_{i}, \\quad \\forall k \\leq n.\n\\end{eqnarray*}\n\\label{major_w}\n\\end{subequations}\n\n\nIn the case of weak majorization order, we have the following almost immediate corollary.\n\\begin{corollary}\nLet $X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots,n$, be $n$ i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. If $\\bm \\mu \\prec_{w} \\bm \\lambda$, then \n\\begin{eqnarray*}\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s_{\\bm{\\lambda}}}{2 \\beta }, \\\\\nP(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n  \\begin{dcases}\n\t\t\\frac{\\alpha s_{\\bm{\\mu}}}{\\beta},~& n = 2 \\cr\n\t\t\\frac{(\\alpha-1) s_{\\bm{\\mu}}}{\\beta} ,~& n \\geq 3, \\; \\alpha > 1 \n\t\\end{dcases},\n\\end{eqnarray*}\nwhere $s_{\\bm{\\lambda}} = \\sum_{i=1}^{n} \\lambda_{i}$ and $s_{\\bm{\\mu}} = \\sum_{i=1}^{n} \\mu_{i}$.\n\\label{majorization_cor_weak}\n\\end{corollary}\n\nWe can also extend Theorem~\\ref{majorization_thm}, as well as Corollary~\\ref{majorization_cor_weak}, for the case where $ n = \\infty$. For any non-negative  $\\ell_{1}$ sequence $\\bm{\\lambda} = (\\lambda_{1}, \\lambda_{1}, \\ldots)$, i.e., $\\sum_{i=1}^{\\infty} \\lambda_{i} < \\infty$, define\n\n", "index": 27, "text": "\\begin{equation*}\nP_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x) {\\mathrel{\\mathop:}=} \\Pr \\left( \\sum_{i=1}^{\\infty} \\lambda_{i} X_{i} < x \\right).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x){\\mathrel{\\mathop{:}}=}\\Pr\\left(\\sum_{i%&#10;=1}^{\\infty}\\lambda_{i}X_{i}&lt;x\\right).\" display=\"block\"><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u221e</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf40</mi><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>Pr</mi><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>&lt;</mo><mi>x</mi><mo>)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nNote that $J \\big( \\lambda_{0},\\bar{x}(\\lambda_{0}) \\big) = 0$ and since $\\sum_{i=1}^{n} \\alpha_{i} > 1$, by Lemma~\\ref{lemma_A*}(iii), $f_{Y} (0) = 0$. So we have\n\\begin{eqnarray*}\nJ \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big)  &=& \\left[ \\frac{d}{d x} \\int_{0}^{x} f_{Y}(x-z) \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} z}  dz \\right]_{x = \\bar{x}(\\lambda_{0})} \\\\\n&=&  \\left[ \\int_{0}^{x} \\frac{d}{d x} f_{Y}\\big(x-z\\big) \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} z} dz \\right]_{x = \\bar{x}(\\lambda_{0})}\\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} \\big(\\bar{x}(\\lambda_{0})-z\\big)} dz .\n\\end{eqnarray*}\nTherefore,\n\n", "itemtype": "equation", "pos": 24093, "prevtext": "\nThe (weak) majorization order is naturally extended to such $\\ell_{1}$ sequences.\n\\begin{corollary}\nLet $\\left \\{X_{i} \\sim Gamma(\\alpha,\\beta),\\; i=1,2,\\ldots \\right \\}$ be a countably infinite  collection of i.i.d gamma r.v's, where $\\alpha >0$ and $\\beta > 0$. For any two non-negative $\\ell_{1}$ sequences, $\\bm \\mu$ and $\\bm \\lambda$, such that $\\bm \\mu \\prec_{w} \\bm \\lambda$, we have \n\\begin{eqnarray*}\nP_{\\infty}(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) s_{\\bm{\\lambda}}}{2 \\beta }, \\\\\nP_{\\infty}(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n\t\t\\frac{(\\alpha-1) s_{\\bm{\\mu}}}{\\beta} , \\quad \\alpha > 1,\n\\end{eqnarray*}\nwhere $s_{\\bm{\\lambda}} = \\sum_{i=1}^{\\infty} \\lambda_{i}$ and $s_{\\bm{\\mu}} = \\sum_{i=1}^{\\infty} \\mu_{i}$.\n\\label{majorization_cor_weak_infty}\n\\end{corollary}\n\n\\input exline\n\nFinally, it might be worth noting that the results such as Theorem~\\ref{majorization_thm} show that if $\\bm{\\mu} \\prec \\bm{\\lambda}$, then $P(\\bm{\\mu};\\alpha,\\beta,x)$ and $P(\\bm{\\lambda};\\alpha,\\beta,x)$ must have at least one crossing on $x \\in (0,\\infty)$.  \nDiaconis and Perlman in~\\cite{diaconis1990bounds} tried to answer whether this crossing point is unique. However, they only proved this uniqueness for $n=2$ and for $n \\geq 3$, they required to impose further restrictions. Ever since, this has been an open problem which is known as the Unique Crossing Conjecture (UCC) and it is quite remarkable that the UCC has remained open, although all the evidence points towards the direction of it being true.\n\n\\input exline\n\n\\noindent{\\bf Acknowledgment}\nWe wish to thank Profs.\\ Milan Merkle and Maochao Xu for their valuable comments during the preparation of the text.\n\n\n\\bibliographystyle{plain}\n\\bibliography{biblio}\n\n\\appendix\n\\section{Proof}\n\\label{proof}\nIn what follows $X \\sim Gamma(\\alpha,\\beta)$ denotes a gamma r.v parametrized by shape $\\alpha > 0$ and rate $\\beta > 0$, $f_{X}$ and $F_{X}$ stand, respectively, for the probability density function (PDF) and cumulative distribution function (CDF) of a r.v $X$. Bold face letters denote vectors and the vector components are denoted as ${ {\\bf v} } = (v_{1},v_{2},\\ldots,v_{n})$. \n\nFor the proof of Theorem~\\ref{majorization_thm}, we need to make use of the following additional results. The proof of Theorem~\\ref{theorem_unimodal} is identical to the proof of~\\cite[Theorem 4]{szba}. Note that~\\cite[Theorem 4]{szba} has been stated in terms of convolution of chi-squared r.v's but the proof, there, has been given for the more general case of arbitrary gamma r.v's. The details of the proofs for Lemmas~\\ref{lemma_A*} and~\\ref{lemma_2*} can be found in~\\cite{roszas}. Lemma~\\ref{lemma_1*} has been stated in~\\cite{roszas} but the proof is omitted there. We give a detailed proof of Lemma~\\ref{lemma_1*} here for completeness.\n\nTheorem~\\ref{theorem_unimodal} is essential in proving our results and it states that an arbitrary convolution of heterogeneous gamma random variables (not necessarily with a common shape or a common rate) has an unique mode. Recall that a PDF, $f(x)$, is called unimodal if there exists a unique $x = a$ such that $f(x)$ is non-decreasing for $x < a$ and $f(x)$ is non-increasing for $x > a$. The point $a$ is called the unique mode of $f(x)$.\n\n\\begin{theorem}\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\beta_{i}),\\;  i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i}, \\beta_{i} > 0 \\; \\forall i$. The PDF of $Y_{n} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ is unimodal where $\\lambda_{i} \\geq 0 \\; \\forall i$.\n\\label{theorem_unimodal}\n\\end{theorem}\n\n\n\\begin{lemma}[{\\cite[Lemma B.1]{roszas}}]\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\beta_{i}),\\;  i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i}, \\beta_{i} > 0 \\; \\forall i$. Define $Y_{n} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ for $\\lambda_{i} > 0$, $\\forall i$ and $\\rho_{j} {\\mathrel{\\mathop:}=} \\sum_{i=1}^{j} \\alpha_{i}$.\nThen for the PDF of $Y_{n}$, $f_{Y_{n}}$, we have\n\\begin{enumerate}[(i)]\n\t\\item $f_{Y_{n}} > 0$, $\\forall x > 0$,\n\t\\item $f_{Y_{n}}$ is analytic on $\\mathbb{R}^{+} = \\{x | x > 0\\}$,\n\t\\item $f_{Y_{n}}^{(k)}(0) = 0$, if $0 \\leq k < \\rho_{n} - 1$, where $f_{Y_{n}}^{(k)}$ denotes the $k^{th}$ derivative of $f_{Y_{n}}$.\n\\end{enumerate} \n\\label{lemma_A*}\n\\end{lemma}\n\n\\begin{lemma}[{\\cite[Lemma B.2]{roszas}}]\nLet $X_{i} \\sim\tGamma(\\alpha_{i},\\alpha), \\; i=1,2,\\ldots,n,$ be independent r.v's, where $\\alpha_{i} > 0 \\; \\forall i$ and $\\alpha > 0$. Also let $\\psi \\sim Gamma(1,\\alpha)$ be another r.v independent of all $X_{i}$'s. If $\\sum_{i=1}^{n} \\alpha_{i} > 1$, then the mode, $\\bar{x}(\\lambda)$, of the r.v \n\n$W({\\lambda}) = Y + \\lambda \\psi$\n\nis strictly increasing in $\\lambda>0$, where $Y = \\sum_{i=1}^{n} \\lambda_{i} X_{i}$ with $\\lambda_{i} > 0$, $\\forall i$.\n\\label{lemma_1*}\n\\end{lemma}\n\\begin{proof}\nBy Lemma~\\ref{lemma_A*},  $\\bar{x}(\\lambda) > 0$ for $\\lambda \\geq 0$. By the unimodality of $W({\\lambda})$, for any $\\lambda > \\lambda_{0} > 0$, it is enough to show that \n\n", "index": 29, "text": "\\begin{equation}\nJ \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) {\\mathrel{\\mathop:}=} \\left[ \\frac{d^{2}}{d x^{2}} \\Pr \\left( W({\\lambda}) \\leq x \\right) \\right]_{x = \\bar{x}(\\lambda_{0})} > 0.\n\\label{J_lemma_1*}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"J\\big{(}\\lambda,\\bar{x}(\\lambda_{0})\\big{)}{\\mathrel{\\mathop{:}}=}\\left[\\frac{%&#10;d^{2}}{dx^{2}}\\Pr\\left(W({\\lambda})\\leq x\\right)\\right]_{x=\\bar{x}(\\lambda_{0}%&#10;)}&gt;0.\" display=\"block\"><mrow><mi>J</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\u03bb</mi><mo>,</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><msub><mrow><mo>[</mo><mfrac><msup><mi>d</mi><mn>2</mn></msup><mrow><mi>d</mi><mo>\u2062</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mfrac><mi>Pr</mi><mrow><mo>(</mo><mi>W</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><mi>x</mi><mo>)</mo></mrow><mo>]</mo></mrow><mrow><mi>x</mi><mo>=</mo><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>&gt;</mo><mn>0</mn><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nThus for $\\lambda > \\lambda_{0} > 0$, we have\n\\begin{eqnarray*}\n\\frac{\\lambda}{\\alpha} e^{\\frac{\\alpha \\bar{x}(\\lambda)}{\\lambda}} J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) &=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda}} dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda}} - f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda_{0}}} e^{\\alpha \\bar{x}(0)  \\left(\\frac{1}{\\lambda} - \\frac{1}{\\lambda_{0}} \\right)} dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\left( e^{\\frac{\\alpha z}{\\lambda}} -  e^{\\frac{\\alpha z}{\\lambda_{0}} + \\alpha \\bar{x}(0) \\left(\\frac{1}{\\lambda} - \\frac{1}{\\lambda_{0}} \\right) } \\right) dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\left( e^{\\frac{\\alpha z}{\\lambda}} -  e^{\\frac{\\alpha z}{\\lambda} + \\Phi \\big(z,\\bar{x}(0) \\big) } \\right) dz ,\n\\end{eqnarray*}\nwhere $\\bar{x}(0) > 0$ is the mode of r.v $Y$ and \n\n", "itemtype": "equation", "pos": 25004, "prevtext": "\nNote that $J \\big( \\lambda_{0},\\bar{x}(\\lambda_{0}) \\big) = 0$ and since $\\sum_{i=1}^{n} \\alpha_{i} > 1$, by Lemma~\\ref{lemma_A*}(iii), $f_{Y} (0) = 0$. So we have\n\\begin{eqnarray*}\nJ \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big)  &=& \\left[ \\frac{d}{d x} \\int_{0}^{x} f_{Y}(x-z) \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} z}  dz \\right]_{x = \\bar{x}(\\lambda_{0})} \\\\\n&=&  \\left[ \\int_{0}^{x} \\frac{d}{d x} f_{Y}\\big(x-z\\big) \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} z} dz \\right]_{x = \\bar{x}(\\lambda_{0})}\\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\frac{\\alpha}{\\lambda} e^{-\\frac{\\alpha}{\\lambda} \\big(\\bar{x}(\\lambda_{0})-z\\big)} dz .\n\\end{eqnarray*}\nTherefore,\n\n", "index": 31, "text": "\\begin{equation*}\n\\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda_{0}}} dz = \\frac{\\lambda_{0}}{\\alpha} e^{\\frac{\\alpha \\bar{x}(\\lambda_{0})}{\\lambda_{0}}} J \\big( \\lambda_{0},\\bar{x}(\\lambda_{0}) \\big) = 0.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\int_{0}^{\\bar{x}(\\lambda_{0})}f^{{}^{\\prime}}_{Y}(z)e^{\\frac{\\alpha z}{%&#10;\\lambda_{0}}}dz=\\frac{\\lambda_{0}}{\\alpha}e^{\\frac{\\alpha\\bar{x}(\\lambda_{0})}%&#10;{\\lambda_{0}}}J\\big{(}\\lambda_{0},\\bar{x}(\\lambda_{0})\\big{)}=0.\" display=\"block\"><mrow><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mrow><msubsup><mi>f</mi><mi>Y</mi><msup><mi/><mo>\u2032</mo></msup></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mfrac><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>z</mi></mrow><msub><mi>\u03bb</mi><mn>0</mn></msub></mfrac></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>z</mi></mrow></mrow></mrow><mo>=</mo><mrow><mfrac><msub><mi>\u03bb</mi><mn>0</mn></msub><mi>\u03b1</mi></mfrac><mo>\u2062</mo><msup><mi>e</mi><mfrac><mrow><mi>\u03b1</mi><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mi>\u03bb</mi><mn>0</mn></msub></mfrac></msup><mo>\u2062</mo><mi>J</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>,</mo><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nNow if $z < \\bar{x}(0)$ then $\\Phi \\big(z,\\bar{x}(0)\\big) < 0$ and $f^{'}_{Y}(z) > 0$ so we get $J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) > 0$. Similarly if $z > \\bar{x}(0)$ then $\\Phi \\big(z,\\bar{x}(0)\\big) > 0$ and $f^{'}_{Y}(z) < 0$ and again we have $J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) > 0$.\n\\end{proof}\n\n\\begin{lemma}[{\\cite[Lemma B.3]{roszas}}]\nFor some $ \\alpha_{2} \\geq \\alpha_{1} > 0$, let $\\xi_{1} \\sim Gamma(1+\\alpha_{1},\\alpha_{1})$ and $\\xi_{2} \\sim Gamma(1+\\alpha_{2}, \\alpha_{2})$ be independent gamma r.v's. Also let $\\bar{x} = \\bar{x}(\\lambda)$ denote the mode of the r.v $\\xi(\\lambda) = \\lambda \\xi_{1}  + (1- \\lambda) \\xi_{2}$ for $0 \\le \\lambda \\le 1$. Then $1 \\le  \\bar{x}(\\lambda)   \\le  \\left(2  \\sqrt{\\alpha_{1}  \\alpha_{2}} + 1\\right)/\\left(2 \\sqrt{\\alpha_{1}  \\alpha_{2}} \\right), \\quad \\forall 0 \\le \\lambda \\le 1$, with $\\bar{x}(0) = \\bar{x}(1) = 1$ and, in case of $\\alpha_{i} = \\alpha_{j} = \\alpha$, $\\bar{x}(1/2) = \\left(2 \\alpha + 1\\right)/\\left(2 \\alpha \\right)$, otherwise the inequalities are strict.\n\\label{lemma_2*}\n\\end{lemma}\n\n\\subsection{Proof of Theorem~\\ref{majorization_thm}}\n\\label{proof_majorization_thm} \nWe prove the theorem for the case where $\\alpha = \\beta$ and $s=1$. The general case follows from the scaling properties of gamma distribution. \n\nWe first consider the case where $n \\geq 3$. If $\\bm \\lambda \\succ \\bm \\mu$, then there exists a finite number, $r$, of vectors $\\bm{\\eta}_{i}, i = 1,2,\\ldots r$, such that $\\bm \\lambda = \\bm{\\eta}_{1} \\succ \\bm{\\eta}_{2} \\succ \\ldots \\succ \\bm{\\eta} _{r-1} \\succ \\bm{\\eta}_{r} = \\bm{\\mu}$, and $\\bm{\\eta}_{i}$ and $\\bm{\\eta}_{i+1}$ differ in two coordinates only, $i = 1,2,\\ldots,r-1$, see~\\cite[12.5.a]{peprto}. Thus we may, without loss of generality assume that $\\bm \\lambda$, and $\\bm \\mu$ differ only in two coordinates, and in fact, assume that for some $1\\leq j < k \\leq n$, we have\n\n\\begin{eqnarray}\n(\\lambda_{j},\\lambda_{k}) \\succ (\\mu_{j},\\mu_{k}) \\; \\text{ and } \\; \n\\lambda_{i} = \\mu_{i} \\text{ for } i \\in \\{1,\\ldots,n\\} \\backslash \\{j,k\\}.\n\\label{shcur_vec_2_componts}\n\\end{eqnarray}\n\nFor $t \\in [0,1]$, define\n\n\\begin{eqnarray}\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& t \\lambda_{i} + (1-t) \\mu_{i}, \\quad i = j,k, \\nonumber \\\\\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& \\lambda_{i} , \\quad i \\neq j,k, \\nonumber \\\\\nY(t) &{\\mathrel{\\mathop:}=}& \\sum_{i = 1}^{n} \\nu_{i}(t) X_{i}.\n\\label{schur_vect}\n\\end{eqnarray}\n\nIt suffices to show that the CDF of $Y(t)$, in $t \\in [0,1]$, is non-increasing for $x > (2  \\alpha + 1)/(2 \\alpha)$ and non-decreasing for $x < (\\alpha - 1)/\\alpha$ . Now we take the Laplace transform of $F_{Y(t)}$ as \n\\begin{eqnarray*}\nJ(t,z) {\\mathrel{\\mathop:}=} \\mathcal{L}[F_{Y(t)}](z) &=& \\int_{0}^{\\infty} e^{-zx} F_{Y(t)}(x) dx \\\\\n&=& \\frac{-1}{z} \\int_{0}^{\\infty} F_{Y(t)}(x) d\\left( e^{-zx} \\right) \\\\\n&=& \\frac{1}{z} \\int_{0}^{\\infty} e^{-zx} dF_{Y(t)}(x) \\\\\n&=& \\frac{1}{z} \\mathcal{L}[Y(t)](z),\n\\end{eqnarray*}\nwhere $\\mathcal{L}[Y(t)](z)$ is the Laplace transform of $Y(t)$ as\n\\begin{eqnarray*}\n\\mathcal{L}[Y(t)](z) = \\prod \\limits_{i =1}^{n} \\left(1 + \\frac {\\nu_{i}(t) z}{\\alpha}\\right)^{-\\alpha}, \\text{ for } z \\in \\mathbb{C}, \\;\nRe(z) > -\\min_{1\\leq i \\leq n} \\frac{\\alpha}{\\nu_{i}(t)}.\n\\end{eqnarray*}\nDifferentiating with respect to $t$ yields\n\\begin{eqnarray*}\n\\frac{\\partial J}{\\partial t} (t,z) &=& J (t,z) \\frac{\\partial}{\\partial t} (\\ln(J)) \\\\\n&=& J (t,z)\\sum_{i=j,k} \\frac{(\\mu_{i} - \\lambda_{i})z}{1 + \\frac {\\nu_{i}(t) z}{\\alpha}}. \n\\end{eqnarray*}\nWe take the inverse transform to get\n\\begin{eqnarray*}\n\\frac{\\partial }{\\partial t} F_{Y(t)}(x) &=& \\sum_{i=j,k} (\\mu_{i} - \\lambda_{i}) \\frac{\\partial}{\\partial x} \\Pr( Y(t) + \\nu_{i}(t) \\psi_{i} \\leq x) \\\\\n&=& \\sum_{i=j,k} (\\mu_{i} - \\lambda_{i})  f_{ Y(t) + \\nu_{i}(t) \\psi_{i}} (x) \\\\\n&=& (\\mu_{j} - \\lambda_{j}) [f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) - f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)],\n\\end{eqnarray*}\nwhere $\\psi_{j},\\psi_{k} \\sim Gamma(1,\\alpha)$ are i.i.d gamma r.v's which are also independent of all $X_{i}$'s. By~\\eqref{shcur_vec_2_componts}, we must have that $\\lambda_{j} \\geq \\mu_{j}$, so it suffices to show that $ f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) \\geq  f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)$ for $x > (2  \\alpha + 1)/(2 \\alpha )$ and $ f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) \\leq  f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)$ for $x < (\\alpha - 1)/\\alpha$. On the other hand, using the Laplace transform and inverting it again, one can show the following identity (for any integer $k \\geq 1$ and reals $a,b \\geq 0$)\n\\begin{eqnarray}\n\\frac{d^{k-1}}{d x^{k-1}} f_{ X + a \\psi_{1}} (x) -  \\frac{d^{k-1}}{d x^{k-1}} f_{X + b \\psi_{2}}(x) = \\frac{1}{\\alpha} (b-a) \\frac{d^{k}}{d x^{k}} f_{X + a \\psi_{1} + b \\psi_{2}}(x),\n\\label{diff_f_gen}\n\\end{eqnarray}\nwhere $X$ is  an arbitrary continuous positive r.v and $\\psi_{i} \\sim Gamma(1,\\alpha) \\;,\\; i=1,2$, are i.i.d gamma r.v's which are \nalso independent of $X$. As such we have\n\n", "itemtype": "equation", "pos": 26168, "prevtext": "\nThus for $\\lambda > \\lambda_{0} > 0$, we have\n\\begin{eqnarray*}\n\\frac{\\lambda}{\\alpha} e^{\\frac{\\alpha \\bar{x}(\\lambda)}{\\lambda}} J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) &=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda}} dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda}} - f^{'}_{Y}(z)  e^{\\frac{\\alpha z}{\\lambda_{0}}} e^{\\alpha \\bar{x}(0)  \\left(\\frac{1}{\\lambda} - \\frac{1}{\\lambda_{0}} \\right)} dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\left( e^{\\frac{\\alpha z}{\\lambda}} -  e^{\\frac{\\alpha z}{\\lambda_{0}} + \\alpha \\bar{x}(0) \\left(\\frac{1}{\\lambda} - \\frac{1}{\\lambda_{0}} \\right) } \\right) dz \\\\\n&=& \\int_{0}^{\\bar{x}(\\lambda_{0})} f^{'}_{Y}(z)  \\left( e^{\\frac{\\alpha z}{\\lambda}} -  e^{\\frac{\\alpha z}{\\lambda} + \\Phi \\big(z,\\bar{x}(0) \\big) } \\right) dz ,\n\\end{eqnarray*}\nwhere $\\bar{x}(0) > 0$ is the mode of r.v $Y$ and \n\n", "index": 33, "text": "\\begin{equation*}\n\\Phi \\big(z,\\bar{x}(0)\\big) {\\mathrel{\\mathop:}=} \\alpha \\Big(z - \\bar{x}(0)\\Big) \\Big(\\frac{1}{\\lambda_{0}} - \\frac{1}{\\lambda} \\Big) .\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\Phi\\big{(}z,\\bar{x}(0)\\big{)}{\\mathrel{\\mathop{:}}=}\\alpha\\Big{(}z-\\bar{x}(0)%&#10;\\Big{)}\\Big{(}\\frac{1}{\\lambda_{0}}-\\frac{1}{\\lambda}\\Big{)}.\" display=\"block\"><mrow><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>z</mi><mo>,</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>\u03b1</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mi>z</mi><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><mn>1</mn><msub><mi>\u03bb</mi><mn>0</mn></msub></mfrac><mo>-</mo><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nFor given $1 \\leq j < k \\leq n$, consider the r.v $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$. By~\\eqref{schur_vect}, we get that $\\nu_{j}(t) \\geq \\nu_{k}(t)$ for $t \\in [0,1]$, hence, it is only left to show that, for any $t \\in [0,1]$, the mode of this r.v, under the conditions \n\\begin{eqnarray*}\n&& \\nu_{1}(t) \\geq \\ldots  \\geq \\nu_{n}(t) \\geq 0, \\\\\n&& \\sum_{i=1}^{n} \\nu_{i}(t) = 1,\n\\end{eqnarray*}\nfalls between $(\\alpha-1)/\\alpha$ and $(2 \\alpha+1)/(2 \\alpha)$. By Lemma~\\ref{lemma_1*}, the mode of r.v $Y(t) + \\nu_{1}(t) \\psi_{1} + \\nu_{2}(t) \\psi_{2}$ is greater than that of $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$ for $1 < j < k$, and also the mode of $Y(t) + \\nu_{n-1}(t) \\psi_{n-1} + \\nu_{n}(t) \\psi_{n}$ is smaller than that of $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$ for $j < k < n$. Hence, we only need to show that for the mode of the r.v \n\\begin{subequations}\n\n", "itemtype": "equation", "pos": 31269, "prevtext": "\nNow if $z < \\bar{x}(0)$ then $\\Phi \\big(z,\\bar{x}(0)\\big) < 0$ and $f^{'}_{Y}(z) > 0$ so we get $J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) > 0$. Similarly if $z > \\bar{x}(0)$ then $\\Phi \\big(z,\\bar{x}(0)\\big) > 0$ and $f^{'}_{Y}(z) < 0$ and again we have $J \\big( \\lambda,\\bar{x}(\\lambda_{0}) \\big) > 0$.\n\\end{proof}\n\n\\begin{lemma}[{\\cite[Lemma B.3]{roszas}}]\nFor some $ \\alpha_{2} \\geq \\alpha_{1} > 0$, let $\\xi_{1} \\sim Gamma(1+\\alpha_{1},\\alpha_{1})$ and $\\xi_{2} \\sim Gamma(1+\\alpha_{2}, \\alpha_{2})$ be independent gamma r.v's. Also let $\\bar{x} = \\bar{x}(\\lambda)$ denote the mode of the r.v $\\xi(\\lambda) = \\lambda \\xi_{1}  + (1- \\lambda) \\xi_{2}$ for $0 \\le \\lambda \\le 1$. Then $1 \\le  \\bar{x}(\\lambda)   \\le  \\left(2  \\sqrt{\\alpha_{1}  \\alpha_{2}} + 1\\right)/\\left(2 \\sqrt{\\alpha_{1}  \\alpha_{2}} \\right), \\quad \\forall 0 \\le \\lambda \\le 1$, with $\\bar{x}(0) = \\bar{x}(1) = 1$ and, in case of $\\alpha_{i} = \\alpha_{j} = \\alpha$, $\\bar{x}(1/2) = \\left(2 \\alpha + 1\\right)/\\left(2 \\alpha \\right)$, otherwise the inequalities are strict.\n\\label{lemma_2*}\n\\end{lemma}\n\n\\subsection{Proof of Theorem~\\ref{majorization_thm}}\n\\label{proof_majorization_thm} \nWe prove the theorem for the case where $\\alpha = \\beta$ and $s=1$. The general case follows from the scaling properties of gamma distribution. \n\nWe first consider the case where $n \\geq 3$. If $\\bm \\lambda \\succ \\bm \\mu$, then there exists a finite number, $r$, of vectors $\\bm{\\eta}_{i}, i = 1,2,\\ldots r$, such that $\\bm \\lambda = \\bm{\\eta}_{1} \\succ \\bm{\\eta}_{2} \\succ \\ldots \\succ \\bm{\\eta} _{r-1} \\succ \\bm{\\eta}_{r} = \\bm{\\mu}$, and $\\bm{\\eta}_{i}$ and $\\bm{\\eta}_{i+1}$ differ in two coordinates only, $i = 1,2,\\ldots,r-1$, see~\\cite[12.5.a]{peprto}. Thus we may, without loss of generality assume that $\\bm \\lambda$, and $\\bm \\mu$ differ only in two coordinates, and in fact, assume that for some $1\\leq j < k \\leq n$, we have\n\n\\begin{eqnarray}\n(\\lambda_{j},\\lambda_{k}) \\succ (\\mu_{j},\\mu_{k}) \\; \\text{ and } \\; \n\\lambda_{i} = \\mu_{i} \\text{ for } i \\in \\{1,\\ldots,n\\} \\backslash \\{j,k\\}.\n\\label{shcur_vec_2_componts}\n\\end{eqnarray}\n\nFor $t \\in [0,1]$, define\n\n\\begin{eqnarray}\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& t \\lambda_{i} + (1-t) \\mu_{i}, \\quad i = j,k, \\nonumber \\\\\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& \\lambda_{i} , \\quad i \\neq j,k, \\nonumber \\\\\nY(t) &{\\mathrel{\\mathop:}=}& \\sum_{i = 1}^{n} \\nu_{i}(t) X_{i}.\n\\label{schur_vect}\n\\end{eqnarray}\n\nIt suffices to show that the CDF of $Y(t)$, in $t \\in [0,1]$, is non-increasing for $x > (2  \\alpha + 1)/(2 \\alpha)$ and non-decreasing for $x < (\\alpha - 1)/\\alpha$ . Now we take the Laplace transform of $F_{Y(t)}$ as \n\\begin{eqnarray*}\nJ(t,z) {\\mathrel{\\mathop:}=} \\mathcal{L}[F_{Y(t)}](z) &=& \\int_{0}^{\\infty} e^{-zx} F_{Y(t)}(x) dx \\\\\n&=& \\frac{-1}{z} \\int_{0}^{\\infty} F_{Y(t)}(x) d\\left( e^{-zx} \\right) \\\\\n&=& \\frac{1}{z} \\int_{0}^{\\infty} e^{-zx} dF_{Y(t)}(x) \\\\\n&=& \\frac{1}{z} \\mathcal{L}[Y(t)](z),\n\\end{eqnarray*}\nwhere $\\mathcal{L}[Y(t)](z)$ is the Laplace transform of $Y(t)$ as\n\\begin{eqnarray*}\n\\mathcal{L}[Y(t)](z) = \\prod \\limits_{i =1}^{n} \\left(1 + \\frac {\\nu_{i}(t) z}{\\alpha}\\right)^{-\\alpha}, \\text{ for } z \\in \\mathbb{C}, \\;\nRe(z) > -\\min_{1\\leq i \\leq n} \\frac{\\alpha}{\\nu_{i}(t)}.\n\\end{eqnarray*}\nDifferentiating with respect to $t$ yields\n\\begin{eqnarray*}\n\\frac{\\partial J}{\\partial t} (t,z) &=& J (t,z) \\frac{\\partial}{\\partial t} (\\ln(J)) \\\\\n&=& J (t,z)\\sum_{i=j,k} \\frac{(\\mu_{i} - \\lambda_{i})z}{1 + \\frac {\\nu_{i}(t) z}{\\alpha}}. \n\\end{eqnarray*}\nWe take the inverse transform to get\n\\begin{eqnarray*}\n\\frac{\\partial }{\\partial t} F_{Y(t)}(x) &=& \\sum_{i=j,k} (\\mu_{i} - \\lambda_{i}) \\frac{\\partial}{\\partial x} \\Pr( Y(t) + \\nu_{i}(t) \\psi_{i} \\leq x) \\\\\n&=& \\sum_{i=j,k} (\\mu_{i} - \\lambda_{i})  f_{ Y(t) + \\nu_{i}(t) \\psi_{i}} (x) \\\\\n&=& (\\mu_{j} - \\lambda_{j}) [f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) - f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)],\n\\end{eqnarray*}\nwhere $\\psi_{j},\\psi_{k} \\sim Gamma(1,\\alpha)$ are i.i.d gamma r.v's which are also independent of all $X_{i}$'s. By~\\eqref{shcur_vec_2_componts}, we must have that $\\lambda_{j} \\geq \\mu_{j}$, so it suffices to show that $ f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) \\geq  f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)$ for $x > (2  \\alpha + 1)/(2 \\alpha )$ and $ f_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) \\leq  f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x)$ for $x < (\\alpha - 1)/\\alpha$. On the other hand, using the Laplace transform and inverting it again, one can show the following identity (for any integer $k \\geq 1$ and reals $a,b \\geq 0$)\n\\begin{eqnarray}\n\\frac{d^{k-1}}{d x^{k-1}} f_{ X + a \\psi_{1}} (x) -  \\frac{d^{k-1}}{d x^{k-1}} f_{X + b \\psi_{2}}(x) = \\frac{1}{\\alpha} (b-a) \\frac{d^{k}}{d x^{k}} f_{X + a \\psi_{1} + b \\psi_{2}}(x),\n\\label{diff_f_gen}\n\\end{eqnarray}\nwhere $X$ is  an arbitrary continuous positive r.v and $\\psi_{i} \\sim Gamma(1,\\alpha) \\;,\\; i=1,2$, are i.i.d gamma r.v's which are \nalso independent of $X$. As such we have\n\n", "index": 35, "text": "\\begin{equation*}\nf_{ Y(t) + \\nu_{j}(t) \\psi_{j}}(x) -  f_{Y(t) + \\nu_{k}(t) \\psi_{k}}(x) = \\frac{1}{\\alpha} (\\nu_{k}(t) - \\nu_{j}(t)) \\frac{\\partial}{\\partial x} f_{Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}}(x).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"f_{Y(t)+\\nu_{j}(t)\\psi_{j}}(x)-f_{Y(t)+\\nu_{k}(t)\\psi_{k}}(x)=\\frac{1}{\\alpha}%&#10;(\\nu_{k}(t)-\\nu_{j}(t))\\frac{\\partial}{\\partial x}f_{Y(t)+\\nu_{j}(t)\\psi_{j}+%&#10;\\nu_{k}(t)\\psi_{k}}(x).\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>f</mi><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bd</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bd</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>k</mi></msub></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>\u03b1</mi></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\u03bd</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03bd</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>x</mi></mrow></mfrac><mo>\u2062</mo><msub><mi>f</mi><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bd</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\u03bd</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>k</mi></msub></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\ndenoted by $\\bar{x}_{1}(t)$, we have $\\bar{x}_{1}(t) \\leq (2 \\alpha+1)/(2 \\alpha)$ and for the mode of the r.v \n\n", "itemtype": "equation", "pos": 32412, "prevtext": "\nFor given $1 \\leq j < k \\leq n$, consider the r.v $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$. By~\\eqref{schur_vect}, we get that $\\nu_{j}(t) \\geq \\nu_{k}(t)$ for $t \\in [0,1]$, hence, it is only left to show that, for any $t \\in [0,1]$, the mode of this r.v, under the conditions \n\\begin{eqnarray*}\n&& \\nu_{1}(t) \\geq \\ldots  \\geq \\nu_{n}(t) \\geq 0, \\\\\n&& \\sum_{i=1}^{n} \\nu_{i}(t) = 1,\n\\end{eqnarray*}\nfalls between $(\\alpha-1)/\\alpha$ and $(2 \\alpha+1)/(2 \\alpha)$. By Lemma~\\ref{lemma_1*}, the mode of r.v $Y(t) + \\nu_{1}(t) \\psi_{1} + \\nu_{2}(t) \\psi_{2}$ is greater than that of $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$ for $1 < j < k$, and also the mode of $Y(t) + \\nu_{n-1}(t) \\psi_{n-1} + \\nu_{n}(t) \\psi_{n}$ is smaller than that of $Y(t) + \\nu_{j}(t) \\psi_{j} + \\nu_{k}(t) \\psi_{k}$ for $j < k < n$. Hence, we only need to show that for the mode of the r.v \n\\begin{subequations}\n\n", "index": 37, "text": "\\begin{equation}\nY_{1}(t) {\\mathrel{\\mathop:}=} Y(t) + \\nu_{1}(t) \\psi_{1} + \\nu_{2}(t) \\psi_{2}\n\\label{Y_1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"Y_{1}(t){\\mathrel{\\mathop{:}}=}Y(t)+\\nu_{1}(t)\\psi_{1}+\\nu_{2}(t)\\psi_{2}\" display=\"block\"><mrow><msub><mi>Y</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>+</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mn>2</mn></msub></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\n\\label{Y_1_Y_n}\n\\end{subequations}\ndenoted by $\\bar{x}_{n}(t)$, we have $\\bar{x}_{n}(t) \\geq (\\alpha-1)/\\alpha$. \n\n\n\nIn what follows, we fix any $t \\in [0,1]$ and, for notational simplicity, denote $Y_{1}(t), \\bar{x}_{1}(t), Y_{n}(t)$ and $\\bar{x}_{n}(t)$ by $Y_{1}, \\bar{x}_{1}, Y_{n}$ and $\\bar{x}_{n}$, respectively. \n\nWe first prove the case for $Y_{1}$. At any mode of $Y_{1}$, we have\\footnote{Since mode is unique, thus f must be strictly concave at the mode} \n\\begin{eqnarray*}\n\\left[ \\frac{\\partial}{\\partial x} f_{Y_{1}}(x, \\bm \\nu) \\right]_{({\\bar{x}_{1}(\\bm \\nu), \\bm \\nu})} = 0, \\\\\n\\left[ \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1}}(x, \\bm \\nu) \\right]_{({\\bar{x}_{1}(\\bm \\nu), \\bm \\nu})} < 0,\n\\end{eqnarray*}\nhence implicit function theorem yields\n\n", "itemtype": "equation", "pos": 32648, "prevtext": "\ndenoted by $\\bar{x}_{1}(t)$, we have $\\bar{x}_{1}(t) \\leq (2 \\alpha+1)/(2 \\alpha)$ and for the mode of the r.v \n\n", "index": 39, "text": "\\begin{equation}\nY_{n}(t) {\\mathrel{\\mathop:}=} Y(t) + \\nu_{n-1}(t) \\psi_{n-1} + \\nu_{n}(t) \\psi_{n}\n\\label{Y_n}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"Y_{n}(t){\\mathrel{\\mathop{:}}=}Y(t)+\\nu_{n-1}(t)\\psi_{n-1}+\\nu_{n}(t)\\psi_{n}\" display=\"block\"><mrow><msub><mi>Y</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mi>\u03bd</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>\u03bd</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mi>n</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nLet $\\bm \\nu^{*}$ be where the global maximum of $\\bar{x}_{1}(\\bm \\nu)$, denoted by $\\bar{x}^{*}_{1}$, occurs. Thus by the necessary condition of maximality, we must have\n\n", "itemtype": "equation", "pos": 33540, "prevtext": "\n\\label{Y_1_Y_n}\n\\end{subequations}\ndenoted by $\\bar{x}_{n}(t)$, we have $\\bar{x}_{n}(t) \\geq (\\alpha-1)/\\alpha$. \n\n\n\nIn what follows, we fix any $t \\in [0,1]$ and, for notational simplicity, denote $Y_{1}(t), \\bar{x}_{1}(t), Y_{n}(t)$ and $\\bar{x}_{n}(t)$ by $Y_{1}, \\bar{x}_{1}, Y_{n}$ and $\\bar{x}_{n}$, respectively. \n\nWe first prove the case for $Y_{1}$. At any mode of $Y_{1}$, we have\\footnote{Since mode is unique, thus f must be strictly concave at the mode} \n\\begin{eqnarray*}\n\\left[ \\frac{\\partial}{\\partial x} f_{Y_{1}}(x, \\bm \\nu) \\right]_{({\\bar{x}_{1}(\\bm \\nu), \\bm \\nu})} = 0, \\\\\n\\left[ \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1}}(x, \\bm \\nu) \\right]_{({\\bar{x}_{1}(\\bm \\nu), \\bm \\nu})} < 0,\n\\end{eqnarray*}\nhence implicit function theorem yields\n\n", "index": 41, "text": "\\begin{equation}\n\\frac{\\partial \\bar{x}_{1}}{\\partial \\nu_{1}} (\\bm \\nu) = - \\frac{\\frac{\\partial^{2} f_{Y_{1}}}{\\partial \\nu_{1}\\partial \\bar{x}_{1}} (\\bar{x}_{1}(\\bm \\nu), \\bm \\nu)}{\\frac{\\partial^{2} f_{Y_{1}}}{\\partial {\\bar{x}_{1}}^{2}} (\\bar{x}_{1}(\\bm \\nu), \\bm \\nu) }.\n\\label{d_bar_x_d_nu}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\bar{x}_{1}}{\\partial\\nu_{1}}(\\bm{\\nu})=-\\frac{\\frac{\\partial^{2%&#10;}f_{Y_{1}}}{\\partial\\nu_{1}\\partial\\bar{x}_{1}}(\\bar{x}_{1}(\\bm{\\nu}),\\bm{\\nu}%&#10;)}{\\frac{\\partial^{2}f_{Y_{1}}}{\\partial{\\bar{x}_{1}}^{2}}(\\bar{x}_{1}(\\bm{\\nu%&#10;}),\\bm{\\nu})}.\" display=\"block\"><mrow><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bd</mi><mn>1</mn></msub></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mrow><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi>f</mi><msub><mi>Y</mi><mn>1</mn></msub></msub></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bd</mi><mn>1</mn></msub></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn></msub></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi>f</mi><msub><mi>Y</mi><mn>1</mn></msub></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mmultiscripts><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><none/><none/><mn>2</mn></mmultiscripts></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\n\nSuppose where the maximum occurs, i.e., $\\bm \\nu^{*}$, there is a nonzero coefficient, $\\nu_{3}$, such that $0 < \\nu_{3} < \\nu_{2} \\leq \\nu_{1}$. The case of $\\nu_{3} = 0$ will be dealt with at the end of the proof. Fixing all other coefficients, we vary $\\nu_{2}$ and $\\nu_{3}$ under the condition $\\nu_{2} + \\nu_{3} = const$. That is, we take the directional derivative in the direction $\\delta\\bm{\\nu} = (0, 1, -1, 0 \\ldots, 0)$ for the ``$n$'' dimensional vector $\\bm{\\nu} = (\\nu_{1},\\nu_{2},\\nu_{3},\\ldots,\\nu_{n})$. In other words, we consider the change in the direction of $\\bm{\\nu} + \\gamma \\delta\\bm{\\nu}$. Using Laplace transform, we get\n\\begin{eqnarray*}\n&&\\frac{d}{d\\gamma}\\mathcal{L}[Y_{1}](z) = \\mathcal{L}[Y_{1}](z) \\frac{d}{d\\gamma} \\ln \\left[ \\mathcal{L}[Y_{1}](z) \\right] \\\\\n&=&  \\mathcal{L}[Y_{1}](z) \\frac{d}{d\\gamma} \\Big( - (1+\\alpha) \\ln (1 + \\frac {\\nu_{2} z}{\\alpha}) -\\alpha \\sum_{\\substack{i=0 \\\\ i\\neq 2}}^{n} \\ln (1 + \\frac {\\nu_{i} z}{\\alpha}) - \\ln (1 + \\frac {\\nu_{1} z}{\\alpha}) \\Big) \\\\\n&=&  \\mathcal{L}[Y_{1}](z) \\left( - \\frac{(1+\\alpha) z}{\\alpha} \\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} +   \\frac{z}{(1 + \\frac {\\nu_{3} z}{\\alpha})} \\right) \\\\\n&=&  z \\mathcal{L}[Y_{1}](z) \\left( \\frac{1}{(1 + \\frac {\\nu_{3} z}{\\alpha})} -  \\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} - \\frac{1}{\\alpha}\\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} \\right) \\\\\n&=&  z \\mathcal{L}[Y_{1}](z) \\left( \\frac{\\frac{(\\nu_{2} - \\nu_{3}) z}{\\alpha}}{(1 + \\frac {\\nu_{3} z}{\\alpha})(1 + \\frac {\\nu_{2} z}{\\alpha})} - \\frac{1}{\\alpha}\\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} \\right).\n\\end{eqnarray*}\nNow inverting the above and differentiating (w.r.t\\ $x$), we get\n\\begin{eqnarray}\n\\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) &=& \\frac{(\\nu_{2} - \\nu_{3}) }{\\alpha} \\frac{\\partial^{3}}{\\partial x^{3}} f_{\\tilde{Y}_{23}} (x, \\bm \\nu) - \\frac{1}{\\alpha} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu),\n\\label{partial_13_1}\n\\end{eqnarray}\nwhere $\\tilde{Y}_{23} = Y_{1} + \\nu_{2} \\xi_{2} + \\nu_{3} \\xi_{3}$ with $\\xi_{2}, \\xi_{3} \\sim Gamma(1,\\alpha)$ being  i.i.d r.v's, independent of all others appearing before. So at the maximum, $\\bar{x}^{*}_{1}$, we must have\n\n", "itemtype": "equation", "pos": 34024, "prevtext": "\nLet $\\bm \\nu^{*}$ be where the global maximum of $\\bar{x}_{1}(\\bm \\nu)$, denoted by $\\bar{x}^{*}_{1}$, occurs. Thus by the necessary condition of maximality, we must have\n\n", "index": 43, "text": "\\begin{equation*}\n\\left[ \\frac{\\partial^{2}}{\\partial \\nu_{1} \\partial x} f_{Y_{1}}(x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1},\\bm \\nu^{*})} = 0.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\left[\\frac{\\partial^{2}}{\\partial\\nu_{1}\\partial x}f_{Y_{1}}(x,\\bm{\\nu})%&#10;\\right]_{(\\bar{x}^{*}_{1},\\bm{\\nu}^{*})}=0.\" display=\"block\"><mrow><mrow><msub><mrow><mo>[</mo><mrow><mfrac><msup><mo>\u2202</mo><mn>2</mn></msup><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bd</mi><mn>1</mn></msub></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>x</mi></mrow></mrow></mfrac><mo>\u2062</mo><msub><mi>f</mi><msub><mi>Y</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><msup><mi>\ud835\udf42</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nNow consider a slight perturbation of $\\delta\\bm{\\nu} = (0, 1, -1, 0 \\ldots, 0)$ as $\\delta\\bm{\\nu}({\\varepsilon}_{0}) = (0, 1, -(1 + {\\varepsilon}_{0}), 0 \\ldots, 0)$ for some ${\\varepsilon}_{0} > 0$. Computations similar as above yields\n\\begin{eqnarray*}\n\\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) = \\frac{(\\nu_{2} - \\nu_{3}) }{\\alpha} \\frac{\\partial^{3}}{\\partial x^{3}} f_{\\tilde{Y}_{23}} (x, \\bm \\nu) &-& \\frac{1}{\\alpha} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu) \\nonumber \\\\\n&+& {\\varepsilon}_{0} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu).\n\n\\end{eqnarray*}\nSo at the maximum, $\\bar{x}^{*}_{1}$, using~\\eqref{partial_13_1_maximum}, we must have\n\\begin{eqnarray*}\n\\left[ \\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} &=& {\\varepsilon}_{0} \\left[ \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} \\\\\n&=& - \\frac{\\alpha {\\varepsilon}_{0}}{\\nu_{3}} \\left[ \\frac{\\partial}{\\partial x} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} < 0.\n\n\\end{eqnarray*}\nThis followed from~\\eqref{diff_f_gen} (with $k=2$, $a = 0$ and $b = \\nu_{3}$) and Lemma~\\ref{lemma_1*}, as well as noting that $\\left[ \\frac{\\partial}{\\partial x} f_{Y_{1}} (x, \\bm \\nu)\\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*} )}= 0$. Using~\\eqref{d_bar_x_d_nu}, the previous relation implies that the mode must increase if a sufficiently small step is taken along the perturbed direction $\\delta\\bm{\\nu}({\\varepsilon}_{0}) = (0, -1, (1 + {\\varepsilon}_{0}), 0 \\ldots, 0)$. \nIn other words, for ${\\varepsilon}_{1} >0$ small enough, for the mode of r.v \n\n", "itemtype": "equation", "pos": 36411, "prevtext": "\n\nSuppose where the maximum occurs, i.e., $\\bm \\nu^{*}$, there is a nonzero coefficient, $\\nu_{3}$, such that $0 < \\nu_{3} < \\nu_{2} \\leq \\nu_{1}$. The case of $\\nu_{3} = 0$ will be dealt with at the end of the proof. Fixing all other coefficients, we vary $\\nu_{2}$ and $\\nu_{3}$ under the condition $\\nu_{2} + \\nu_{3} = const$. That is, we take the directional derivative in the direction $\\delta\\bm{\\nu} = (0, 1, -1, 0 \\ldots, 0)$ for the ``$n$'' dimensional vector $\\bm{\\nu} = (\\nu_{1},\\nu_{2},\\nu_{3},\\ldots,\\nu_{n})$. In other words, we consider the change in the direction of $\\bm{\\nu} + \\gamma \\delta\\bm{\\nu}$. Using Laplace transform, we get\n\\begin{eqnarray*}\n&&\\frac{d}{d\\gamma}\\mathcal{L}[Y_{1}](z) = \\mathcal{L}[Y_{1}](z) \\frac{d}{d\\gamma} \\ln \\left[ \\mathcal{L}[Y_{1}](z) \\right] \\\\\n&=&  \\mathcal{L}[Y_{1}](z) \\frac{d}{d\\gamma} \\Big( - (1+\\alpha) \\ln (1 + \\frac {\\nu_{2} z}{\\alpha}) -\\alpha \\sum_{\\substack{i=0 \\\\ i\\neq 2}}^{n} \\ln (1 + \\frac {\\nu_{i} z}{\\alpha}) - \\ln (1 + \\frac {\\nu_{1} z}{\\alpha}) \\Big) \\\\\n&=&  \\mathcal{L}[Y_{1}](z) \\left( - \\frac{(1+\\alpha) z}{\\alpha} \\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} +   \\frac{z}{(1 + \\frac {\\nu_{3} z}{\\alpha})} \\right) \\\\\n&=&  z \\mathcal{L}[Y_{1}](z) \\left( \\frac{1}{(1 + \\frac {\\nu_{3} z}{\\alpha})} -  \\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} - \\frac{1}{\\alpha}\\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} \\right) \\\\\n&=&  z \\mathcal{L}[Y_{1}](z) \\left( \\frac{\\frac{(\\nu_{2} - \\nu_{3}) z}{\\alpha}}{(1 + \\frac {\\nu_{3} z}{\\alpha})(1 + \\frac {\\nu_{2} z}{\\alpha})} - \\frac{1}{\\alpha}\\frac{1}{(1 + \\frac {\\nu_{2} z}{\\alpha})} \\right).\n\\end{eqnarray*}\nNow inverting the above and differentiating (w.r.t\\ $x$), we get\n\\begin{eqnarray}\n\\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) &=& \\frac{(\\nu_{2} - \\nu_{3}) }{\\alpha} \\frac{\\partial^{3}}{\\partial x^{3}} f_{\\tilde{Y}_{23}} (x, \\bm \\nu) - \\frac{1}{\\alpha} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu),\n\\label{partial_13_1}\n\\end{eqnarray}\nwhere $\\tilde{Y}_{23} = Y_{1} + \\nu_{2} \\xi_{2} + \\nu_{3} \\xi_{3}$ with $\\xi_{2}, \\xi_{3} \\sim Gamma(1,\\alpha)$ being  i.i.d r.v's, independent of all others appearing before. So at the maximum, $\\bar{x}^{*}_{1}$, we must have\n\n", "index": 45, "text": "\\begin{equation}\n\\left[ \\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} = 0.\n\\label{partial_13_1_maximum}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\left[\\frac{\\partial^{2}}{\\partial\\gamma\\partial x}f_{Y_{1}}(x,\\bm{\\nu})\\right%&#10;]_{(\\bar{x}^{*}_{1},\\bm{\\nu}^{*})}=0.\" display=\"block\"><mrow><mrow><msub><mrow><mo>[</mo><mrow><mfrac><msup><mo>\u2202</mo><mn>2</mn></msup><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b3</mi></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>x</mi></mrow></mrow></mfrac><mo>\u2062</mo><msub><mi>f</mi><msub><mi>Y</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><msup><mi>\ud835\udf42</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\ndenoted by $\\bar{x}^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1}$, we must have that \n\n", "itemtype": "equation", "pos": 38368, "prevtext": "\nNow consider a slight perturbation of $\\delta\\bm{\\nu} = (0, 1, -1, 0 \\ldots, 0)$ as $\\delta\\bm{\\nu}({\\varepsilon}_{0}) = (0, 1, -(1 + {\\varepsilon}_{0}), 0 \\ldots, 0)$ for some ${\\varepsilon}_{0} > 0$. Computations similar as above yields\n\\begin{eqnarray*}\n\\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) = \\frac{(\\nu_{2} - \\nu_{3}) }{\\alpha} \\frac{\\partial^{3}}{\\partial x^{3}} f_{\\tilde{Y}_{23}} (x, \\bm \\nu) &-& \\frac{1}{\\alpha} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu) \\nonumber \\\\\n&+& {\\varepsilon}_{0} \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu).\n\n\\end{eqnarray*}\nSo at the maximum, $\\bar{x}^{*}_{1}$, using~\\eqref{partial_13_1_maximum}, we must have\n\\begin{eqnarray*}\n\\left[ \\frac{\\partial^{2}}{\\partial \\gamma \\partial x} f_{Y_{1}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} &=& {\\varepsilon}_{0} \\left[ \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} \\\\\n&=& - \\frac{\\alpha {\\varepsilon}_{0}}{\\nu_{3}} \\left[ \\frac{\\partial}{\\partial x} f_{Y_{1} + \\nu_{3} \\xi_{3}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} < 0.\n\n\\end{eqnarray*}\nThis followed from~\\eqref{diff_f_gen} (with $k=2$, $a = 0$ and $b = \\nu_{3}$) and Lemma~\\ref{lemma_1*}, as well as noting that $\\left[ \\frac{\\partial}{\\partial x} f_{Y_{1}} (x, \\bm \\nu)\\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*} )}= 0$. Using~\\eqref{d_bar_x_d_nu}, the previous relation implies that the mode must increase if a sufficiently small step is taken along the perturbed direction $\\delta\\bm{\\nu}({\\varepsilon}_{0}) = (0, -1, (1 + {\\varepsilon}_{0}), 0 \\ldots, 0)$. \nIn other words, for ${\\varepsilon}_{1} >0$ small enough, for the mode of r.v \n\n", "index": 47, "text": "\\begin{equation}\nY^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1} {\\mathrel{\\mathop:}=} \\nu_{1}  X_{1} + (\\nu_{2} -{\\varepsilon}_{1}) X_{2} + \\big(\\nu_{3} + {\\varepsilon}_{1} (1 + {\\varepsilon}_{0}) \\big) X_{3} + \\sum_{i = 4}^{n} \\nu_{i} X_{i} + \\nu_{1} \\psi_{1} + (\\nu_{2} - {\\varepsilon}_{1}) \\psi_{2},\n\\label{y_veps_1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"Y^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1}{\\mathrel{\\mathop{:}}=}\\nu_{1}X_{%&#10;1}+(\\nu_{2}-{\\varepsilon}_{1})X_{2}+\\big{(}\\nu_{3}+{\\varepsilon}_{1}(1+{%&#10;\\varepsilon}_{0})\\big{)}X_{3}+\\sum_{i=4}^{n}\\nu_{i}X_{i}+\\nu_{1}\\psi_{1}+(\\nu_%&#10;{2}-{\\varepsilon}_{1})\\psi_{2},\" display=\"block\"><mrow><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b5</mi><mn>0</mn></msub><mo>,</mo><msub><mi>\u03b5</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msubsup><mo movablelimits=\"false\">:</mo><mo>=</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>-</mo><msub><mi>\u03b5</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>\u03bd</mi><mn>3</mn></msub><mo>+</mo><msub><mi>\u03b5</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><msub><mi>\u03b5</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><msub><mi>X</mi><mn>3</mn></msub><mo>+</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>4</mn></mrow><mi>n</mi></munderover><msub><mi>\u03bd</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>-</mo><msub><mi>\u03b5</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nIt is clear that ${\\varepsilon}_{1}$ depends on ${\\varepsilon}_{0}$ and consequently, as ${\\varepsilon}_{0}$ gets smaller, ${\\varepsilon}_{1}$ might get smaller as well. However, ${\\varepsilon}_{1} {\\varepsilon}_{0} \\in o({\\varepsilon}_{0})$ and ${\\varepsilon}_{1} {\\varepsilon}_{0} \\in o({\\varepsilon}_{1})$, where ``$o$'' denotes the ``\\textit{little o}''. In other words, consider decreasing sequences of $({\\varepsilon}_{0}^{n})_{n=1}^{\\infty}$ and $({\\varepsilon}^{n}_{1})_{n=1}^{\\infty}$. Since $\\lim_{n \\rightarrow \\infty} ({\\varepsilon}_{1}^{n} {\\varepsilon}_{0}^{n})/ {\\varepsilon}^{n}_{0} = \\lim_{n \\rightarrow \\infty} ({\\varepsilon}_{1}^{n} {\\varepsilon}^{n}_{0})/ {\\varepsilon}_{1}^{n} = 0$, it follows that, as ${\\varepsilon}_{0}$ gets smaller, ${\\varepsilon}_{1} {\\varepsilon}_{0}$ term in~\\eqref{y_veps_1} vanishes at a faster rate than either of ${\\varepsilon}_{0}$ or ${\\varepsilon}_{1}$. Hence \nusing~\\eqref{y_veps_1} and~\\eqref{ineq_mode_veps_1}, as well as the continuity of $\\bar{x}_{1}$, we can consider a r.v \n\n", "itemtype": "equation", "pos": 38787, "prevtext": "\ndenoted by $\\bar{x}^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1}$, we must have that \n\n", "index": 49, "text": "\\begin{equation}\n\\bar{x}^{*}_{1} < \\bar{x}^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1}.\n\\label{ineq_mode_veps_1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{*}_{1}&lt;\\bar{x}^{({\\varepsilon}_{0},{\\varepsilon}_{1})}_{1}.\" display=\"block\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>&lt;</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b5</mi><mn>0</mn></msub><mo>,</mo><msub><mi>\u03b5</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhich has the same form as $Y_{1}$ but whose mode, denoted by $\\bar{x}^{{\\varepsilon}}_{1}$, satisfies\n\n", "itemtype": "equation", "pos": 39948, "prevtext": "\nIt is clear that ${\\varepsilon}_{1}$ depends on ${\\varepsilon}_{0}$ and consequently, as ${\\varepsilon}_{0}$ gets smaller, ${\\varepsilon}_{1}$ might get smaller as well. However, ${\\varepsilon}_{1} {\\varepsilon}_{0} \\in o({\\varepsilon}_{0})$ and ${\\varepsilon}_{1} {\\varepsilon}_{0} \\in o({\\varepsilon}_{1})$, where ``$o$'' denotes the ``\\textit{little o}''. In other words, consider decreasing sequences of $({\\varepsilon}_{0}^{n})_{n=1}^{\\infty}$ and $({\\varepsilon}^{n}_{1})_{n=1}^{\\infty}$. Since $\\lim_{n \\rightarrow \\infty} ({\\varepsilon}_{1}^{n} {\\varepsilon}_{0}^{n})/ {\\varepsilon}^{n}_{0} = \\lim_{n \\rightarrow \\infty} ({\\varepsilon}_{1}^{n} {\\varepsilon}^{n}_{0})/ {\\varepsilon}_{1}^{n} = 0$, it follows that, as ${\\varepsilon}_{0}$ gets smaller, ${\\varepsilon}_{1} {\\varepsilon}_{0}$ term in~\\eqref{y_veps_1} vanishes at a faster rate than either of ${\\varepsilon}_{0}$ or ${\\varepsilon}_{1}$. Hence \nusing~\\eqref{y_veps_1} and~\\eqref{ineq_mode_veps_1}, as well as the continuity of $\\bar{x}_{1}$, we can consider a r.v \n\n", "index": 51, "text": "\\begin{equation*}\nY^{{\\varepsilon}}_{1} {\\mathrel{\\mathop:}=} \\nu_{1} X_{1} + (\\nu_{2}  - {\\varepsilon}) X_{2} + (\\nu_{3} + {\\varepsilon}) X_{3} + \\sum_{i = 4}^{n} \\nu_{i} X_{i} + \\nu_{1}  \\psi_{1} + (\\nu_{2} -{\\varepsilon}) \\psi_{2},\n\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"Y^{{\\varepsilon}}_{1}{\\mathrel{\\mathop{:}}=}\\nu_{1}X_{1}+(\\nu_{2}-{\\varepsilon%&#10;})X_{2}+(\\nu_{3}+{\\varepsilon})X_{3}+\\sum_{i=4}^{n}\\nu_{i}X_{i}+\\nu_{1}\\psi_{1%&#10;}+(\\nu_{2}-{\\varepsilon})\\psi_{2},\\par&#10;\" display=\"block\"><mrow><msubsup><mi>Y</mi><mn>1</mn><mi>\u03b5</mi></msubsup><mo movablelimits=\"false\">:</mo><mo>=</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>-</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>X</mi><mn>2</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mn>3</mn></msub><mo>+</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>X</mi><mn>3</mn></msub><mo>+</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>4</mn></mrow><mi>n</mi></munderover><msub><mi>\u03bd</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>-</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mn>2</mn></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhich contradicts the maximality of $\\bar{x}^{*}_{1}$. This means that, if $\\nu_{3} \\neq 0$, then the maximum must occurs at $\\bm \\nu^{*}$ with $\\nu_{3} = \\nu_{2} \\leq \\nu_{1}$. Similar arguments show that at the maximum, we must have $\\nu_{n} = \\ldots = \\nu_{3} = \\nu_{2} \\leq \\nu_{1}$. But by~\\eqref{partial_13_1} and the fact that\n\n", "itemtype": "equation", "pos": 40303, "prevtext": "\nwhich has the same form as $Y_{1}$ but whose mode, denoted by $\\bar{x}^{{\\varepsilon}}_{1}$, satisfies\n\n", "index": 53, "text": "\\begin{equation*}\n\\bar{x}^{*}_{1} < \\bar{x}^{{\\varepsilon}}_{1},\n\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{*}_{1}&lt;\\bar{x}^{{\\varepsilon}}_{1},\\par&#10;\" display=\"block\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>&lt;</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mi>\u03b5</mi></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwe must have that $\\nu_{3} \\neq \\nu_{2}$. As such, the only possibility is $\\nu_{n} = \\ldots = \\nu_{3} = 0$, i.e., the maximum mod of $Y_{1}$ coincides with that of r.v $\\zeta = \\nu \\zeta_{1} + (1-\\nu) \\zeta_{2}$ where $\\zeta_{1}, \\zeta_{2} \\sim Gamma(1+\\alpha,\\alpha)$. So now lemma~\\ref{lemma_2*} gives \n\n", "itemtype": "equation", "pos": 40719, "prevtext": "\nwhich contradicts the maximality of $\\bar{x}^{*}_{1}$. This means that, if $\\nu_{3} \\neq 0$, then the maximum must occurs at $\\bm \\nu^{*}$ with $\\nu_{3} = \\nu_{2} \\leq \\nu_{1}$. Similar arguments show that at the maximum, we must have $\\nu_{n} = \\ldots = \\nu_{3} = \\nu_{2} \\leq \\nu_{1}$. But by~\\eqref{partial_13_1} and the fact that\n\n", "index": 55, "text": "\\begin{equation*}\n\\left[ \\frac{\\partial^{2}}{\\partial x^{2}} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} = - \\frac{\\alpha}{\\nu_{2}} \\left[ \\frac{\\partial}{\\partial x} f_{Y_{1} + \\nu_{2} \\xi_{2}} (x, \\bm \\nu) \\right]_{(\\bar{x}^{*}_{1}, \\bm \\nu^{*})} < 0,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\left[\\frac{\\partial^{2}}{\\partial x^{2}}f_{Y_{1}+\\nu_{2}\\xi_{2}}(x,\\bm{\\nu})%&#10;\\right]_{(\\bar{x}^{*}_{1},\\bm{\\nu}^{*})}=-\\frac{\\alpha}{\\nu_{2}}\\left[\\frac{%&#10;\\partial}{\\partial x}f_{Y_{1}+\\nu_{2}\\xi_{2}}(x,\\bm{\\nu})\\right]_{(\\bar{x}^{*}%&#10;_{1},\\bm{\\nu}^{*})}&lt;0,\" display=\"block\"><mrow><mrow><msub><mrow><mo>[</mo><mrow><mfrac><msup><mo>\u2202</mo><mn>2</mn></msup><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mfrac><mo>\u2062</mo><msub><mi>f</mi><mrow><msub><mi>Y</mi><mn>1</mn></msub><mo>+</mo><mrow><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mn>2</mn></msub></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><msup><mi>\ud835\udf42</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mrow><mo>-</mo><mrow><mfrac><mi>\u03b1</mi><msub><mi>\u03bd</mi><mn>2</mn></msub></mfrac><mo>\u2062</mo><msub><mrow><mo>[</mo><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>x</mi></mrow></mfrac><mo>\u2062</mo><msub><mi>f</mi><mrow><msub><mi>Y</mi><mn>1</mn></msub><mo>+</mo><mrow><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mn>2</mn></msub></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>\ud835\udf42</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><msup><mi>\ud835\udf42</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></msub></mrow></mrow><mo>&lt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\n\n\nNow consider the case for $Y_{n}$. Suppose where the minimum occurs, i.e., $\\bm \\nu^{*}$, we have, $\\nu_{n-1} > 0$ and $\\nu_{n-2} > \\nu_{n-1}$. Now by the same reasoning as in the case of $Y_{1}$, we can consider a r.v \n\n", "itemtype": "equation", "pos": 41333, "prevtext": "\nwe must have that $\\nu_{3} \\neq \\nu_{2}$. As such, the only possibility is $\\nu_{n} = \\ldots = \\nu_{3} = 0$, i.e., the maximum mod of $Y_{1}$ coincides with that of r.v $\\zeta = \\nu \\zeta_{1} + (1-\\nu) \\zeta_{2}$ where $\\zeta_{1}, \\zeta_{2} \\sim Gamma(1+\\alpha,\\alpha)$. So now lemma~\\ref{lemma_2*} gives \n\n", "index": 57, "text": "\\begin{equation*}\n\\bar{x}^{*}_{1} \\leq \\frac{2 \\alpha + 1}{2 \\alpha }.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{*}_{1}\\leq\\frac{2\\alpha+1}{2\\alpha}.\" display=\"block\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>\u2264</mo><mfrac><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>+</mo><mn>1</mn></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b1</mi></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhich has the same form as $Y_{n}$ but whose mode, denoted by $\\bar{x}^{{\\varepsilon}}_{n}$, satisfies\n\n", "itemtype": "equation", "pos": 41642, "prevtext": "\n\n\nNow consider the case for $Y_{n}$. Suppose where the minimum occurs, i.e., $\\bm \\nu^{*}$, we have, $\\nu_{n-1} > 0$ and $\\nu_{n-2} > \\nu_{n-1}$. Now by the same reasoning as in the case of $Y_{1}$, we can consider a r.v \n\n", "index": 59, "text": "\\begin{equation*}\nY^{{\\varepsilon}}_{n} {\\mathrel{\\mathop:}=} \\sum_{i = 1}^{n-3} \\nu_{i} X_{i} \\nu_{1} X_{1} + (\\nu_{n-2}  - {\\varepsilon}) X_{n-2} + (\\nu_{n-1} + {\\varepsilon}) X_{n-1} + \\nu_{n} X_{n} + (\\nu_{n-1} + {\\varepsilon})  \\psi_{n-1} + \\nu_{n} \\psi_{n},\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"Y^{{\\varepsilon}}_{n}{\\mathrel{\\mathop{:}}=}\\sum_{i=1}^{n-3}\\nu_{i}X_{i}\\nu_{1%&#10;}X_{1}+(\\nu_{n-2}-{\\varepsilon})X_{n-2}+(\\nu_{n-1}+{\\varepsilon})X_{n-1}+\\nu_{%&#10;n}X_{n}+(\\nu_{n-1}+{\\varepsilon})\\psi_{n-1}+\\nu_{n}\\psi_{n},\" display=\"block\"><mrow><msubsup><mi>Y</mi><mi>n</mi><mi>\u03b5</mi></msubsup><mo movablelimits=\"false\">:</mo><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>3</mn></mrow></munderover><msub><mi>\u03bd</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><msub><mi>\u03bd</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub><mo>-</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>\u03bd</mi><mi>n</mi></msub><msub><mi>X</mi><mi>n</mi></msub><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bd</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>\u03bd</mi><mi>n</mi></msub><msub><mi>\u03c8</mi><mi>n</mi></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhich contradicts the minimality of $\\bar{x}^{*}_{n}$. This means that, if $\\nu_{n-1} \\neq 0$, then the minimum must occurs at $\\bm \\nu^{*}$ with $\\nu_{n-2} = \\nu_{n-1}$. But again, as in the end of the proof for the case of $Y_{1}$, we must have that $\\nu_{n-2} \\neq \\nu_{n-1}$. As such the only possibility is $\\nu_{n-1} = 0$ (which would also mean that $\\nu_{n} = 0$, although this case can also be independently established with the same reasoning as above). Hence, the minimum mode of $Y_{n}$ must coincide with that of the r.v $\\sum_{i = 1}^{n-2} \\nu_{i} X_{i}$. Again, appealing to the same line of reasoning, at the minimum mode of $\\sum_{i = 1}^{n-2} \\nu_{i} X_{i}$, we must either have $\\nu_{j} = 0$ or $\\nu_{j-1} = \\nu_{j}$ for any $1 < j \\leq n-2$. Hence, for $n \\geq 3$, we get\n\n", "itemtype": "equation", "pos": 42025, "prevtext": "\nwhich has the same form as $Y_{n}$ but whose mode, denoted by $\\bar{x}^{{\\varepsilon}}_{n}$, satisfies\n\n", "index": 61, "text": "\\begin{equation*}\n\\bar{x}^{{\\varepsilon}}_{n} < \\bar{x}^{*}_{n},\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{{\\varepsilon}}_{n}&lt;\\bar{x}^{*}_{n},\" display=\"block\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>n</mi><mi>\u03b5</mi></msubsup><mo>&lt;</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>n</mi><mo>*</mo></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere $\\bar{x}_{j \\alpha}$ denotes the mode of the r.v $Gamma\\big(j\\alpha, j \\alpha \\big)$.\n\nThe case of $n=2$, amounts to studying the mode of r.v $\\zeta = \\nu \\zeta_{1} + (1-\\nu) \\zeta_{2}$, denoted by $\\bar{x}(\\zeta)$, where $\\zeta_{1}, \\zeta_{2} \\sim Gamma(1+\\alpha,\\alpha)$. Direct application of Lemma~\\ref{lemma_2*} yields $1 \\leq \\bar{x}(\\zeta) \\leq (2 \\alpha + 1)/(2 \\alpha)$. Theorem~\\ref{majorization_thm} is proved.\n$\\blacksquare$\n\n\\subsection{Proof of Corollary~\\ref{majorization_cor_weak}}\n\\label{proof_majorization_cor_weak} \nThe proof goes along the same line as that of~Theorem~\\ref{majorization_thm}. For $t \\in [0,1]$, we again define \n\\begin{eqnarray*}\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& t \\lambda_{i} + (1-t) \\mu_{i}, \\quad i = j,k,\\\\\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& \\lambda_{i} , \\quad i \\neq j,k,\\\\\nY(t) &{\\mathrel{\\mathop:}=}& \\sum_{i = 1}^{n} \\nu_{i}(t) X_{i}.\n\\end{eqnarray*}\nwhere \n\\begin{eqnarray*}\n&& \\nu_{1}(t) \\geq \\ldots  \\geq \\nu_{n}(t) \\geq 0, \\quad t \\in [0,1], \\\\\n&& \\sum_{i=1}^{n} \\nu_{i}(t) = s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}).\n\\end{eqnarray*}\nComputations identical to the proof of Theorem~\\ref{majorization_thm} give\n\\begin{eqnarray*}\n\\bar{x}^{*}_{1}(t) &\\leq& \\big( s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}) \\big) \\frac{2 \\alpha + 1}{2 \\alpha }, \\\\\n\\bar{x}^{*}_{n}(t) &\\geq& \\big( s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}) \\big) \\frac{\\alpha - 1}{\\alpha },\n\\end{eqnarray*}\nwhere $\\bar{x}^{*}_{1}(t)$ and $\\bar{x}^{*}_{n}(t)$ are, respectively, the maximum and minimum mode of r.v's $Y_{1}(t)$ and $Y_{n}(t)$ which are defined similarly as in~\\eqref{Y_1_Y_n}.\nNow for $t \\in [0,1]$, we get\n\\begin{eqnarray*}\n\\bar{x}^{*}_{1}  &=& \\max_{t \\in [0,1]} \\bar{x}^{*}_{1}(t) \\leq \\frac{( 2 \\alpha + 1 ) s_{\\bm{\\lambda}} }{2 \\alpha }, \\\\\n\\bar{x}^{*}_{n}  &=& \\min_{t \\in [0,1]} \\bar{x}^{*}_{n}(t) \\geq \\frac{( \\alpha - 1 ) s_{\\bm{\\mu}} }{\\alpha },\n\\end{eqnarray*}\nwhich give the desired results.\n$\\blacksquare$\n\n\n\\subsection{Proof of Corollary~\\ref{majorization_cor_weak_infty}}\n\\label{proof_majorization_cor_weak_infty}\nDefine\n\n", "itemtype": "equation", "pos": 42897, "prevtext": "\nwhich contradicts the minimality of $\\bar{x}^{*}_{n}$. This means that, if $\\nu_{n-1} \\neq 0$, then the minimum must occurs at $\\bm \\nu^{*}$ with $\\nu_{n-2} = \\nu_{n-1}$. But again, as in the end of the proof for the case of $Y_{1}$, we must have that $\\nu_{n-2} \\neq \\nu_{n-1}$. As such the only possibility is $\\nu_{n-1} = 0$ (which would also mean that $\\nu_{n} = 0$, although this case can also be independently established with the same reasoning as above). Hence, the minimum mode of $Y_{n}$ must coincide with that of the r.v $\\sum_{i = 1}^{n-2} \\nu_{i} X_{i}$. Again, appealing to the same line of reasoning, at the minimum mode of $\\sum_{i = 1}^{n-2} \\nu_{i} X_{i}$, we must either have $\\nu_{j} = 0$ or $\\nu_{j-1} = \\nu_{j}$ for any $1 < j \\leq n-2$. Hence, for $n \\geq 3$, we get\n\n", "index": 63, "text": "\\begin{equation*}\n\\bar{x}^{*}_{n} = \\min_{1 \\leq j \\leq n-2} \\bar{x}_{j \\alpha} = \\frac{\\alpha - 1}{\\alpha},  \n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{*}_{n}=\\min_{1\\leq j\\leq n-2}\\bar{x}_{j\\alpha}=\\frac{\\alpha-1}{\\alpha},\" display=\"block\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>n</mi><mo>*</mo></msubsup><mo>=</mo><mrow><munder><mi>min</mi><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mrow></munder><mo>\u2061</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mi>j</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow></msub></mrow><mo>=</mo><mfrac><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow><mi>\u03b1</mi></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 45121, "prevtext": "\nwhere $\\bar{x}_{j \\alpha}$ denotes the mode of the r.v $Gamma\\big(j\\alpha, j \\alpha \\big)$.\n\nThe case of $n=2$, amounts to studying the mode of r.v $\\zeta = \\nu \\zeta_{1} + (1-\\nu) \\zeta_{2}$, denoted by $\\bar{x}(\\zeta)$, where $\\zeta_{1}, \\zeta_{2} \\sim Gamma(1+\\alpha,\\alpha)$. Direct application of Lemma~\\ref{lemma_2*} yields $1 \\leq \\bar{x}(\\zeta) \\leq (2 \\alpha + 1)/(2 \\alpha)$. Theorem~\\ref{majorization_thm} is proved.\n$\\blacksquare$\n\n\\subsection{Proof of Corollary~\\ref{majorization_cor_weak}}\n\\label{proof_majorization_cor_weak} \nThe proof goes along the same line as that of~Theorem~\\ref{majorization_thm}. For $t \\in [0,1]$, we again define \n\\begin{eqnarray*}\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& t \\lambda_{i} + (1-t) \\mu_{i}, \\quad i = j,k,\\\\\n\\nu_{i}(t) &{\\mathrel{\\mathop:}=}& \\lambda_{i} , \\quad i \\neq j,k,\\\\\nY(t) &{\\mathrel{\\mathop:}=}& \\sum_{i = 1}^{n} \\nu_{i}(t) X_{i}.\n\\end{eqnarray*}\nwhere \n\\begin{eqnarray*}\n&& \\nu_{1}(t) \\geq \\ldots  \\geq \\nu_{n}(t) \\geq 0, \\quad t \\in [0,1], \\\\\n&& \\sum_{i=1}^{n} \\nu_{i}(t) = s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}).\n\\end{eqnarray*}\nComputations identical to the proof of Theorem~\\ref{majorization_thm} give\n\\begin{eqnarray*}\n\\bar{x}^{*}_{1}(t) &\\leq& \\big( s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}) \\big) \\frac{2 \\alpha + 1}{2 \\alpha }, \\\\\n\\bar{x}^{*}_{n}(t) &\\geq& \\big( s_{\\bm{\\mu}} + t (s_{\\bm{\\lambda}} - s_{\\bm{\\mu}}) \\big) \\frac{\\alpha - 1}{\\alpha },\n\\end{eqnarray*}\nwhere $\\bar{x}^{*}_{1}(t)$ and $\\bar{x}^{*}_{n}(t)$ are, respectively, the maximum and minimum mode of r.v's $Y_{1}(t)$ and $Y_{n}(t)$ which are defined similarly as in~\\eqref{Y_1_Y_n}.\nNow for $t \\in [0,1]$, we get\n\\begin{eqnarray*}\n\\bar{x}^{*}_{1}  &=& \\max_{t \\in [0,1]} \\bar{x}^{*}_{1}(t) \\leq \\frac{( 2 \\alpha + 1 ) s_{\\bm{\\lambda}} }{2 \\alpha }, \\\\\n\\bar{x}^{*}_{n}  &=& \\min_{t \\in [0,1]} \\bar{x}^{*}_{n}(t) \\geq \\frac{( \\alpha - 1 ) s_{\\bm{\\mu}} }{\\alpha },\n\\end{eqnarray*}\nwhich give the desired results.\n$\\blacksquare$\n\n\n\\subsection{Proof of Corollary~\\ref{majorization_cor_weak_infty}}\n\\label{proof_majorization_cor_weak_infty}\nDefine\n\n", "index": 65, "text": "\\begin{equation*}\nP_{n}(\\bm{\\lambda}_{n};\\alpha,\\beta,x) {\\mathrel{\\mathop:}=} \\Pr \\left( \\sum_{i=1}^{n} \\lambda_{i} X_{i} < x \\right),\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"P_{n}(\\bm{\\lambda}_{n};\\alpha,\\beta,x){\\mathrel{\\mathop{:}}=}\\Pr\\left(\\sum_{i=%&#10;1}^{n}\\lambda_{i}X_{i}&lt;x\\right),\" display=\"block\"><mrow><msub><mi>P</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf40</mi><mi>n</mi></msub><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo movablelimits=\"false\">:</mo><mo>=</mo><mi>Pr</mi><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>&lt;</mo><mi>x</mi><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nBy the continuity from above, it is clear that\n\n", "itemtype": "equation", "pos": 45279, "prevtext": "\nwhere\n\n", "index": 67, "text": "\\begin{equation*}\n\\bm{\\lambda}_{n} {\\mathrel{\\mathop:}=} (\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\bm{\\lambda}_{n}{\\mathrel{\\mathop{:}}=}(\\lambda_{1},\\lambda_{2},\\ldots,\\lambda%&#10;_{n}).\" display=\"block\"><mrow><msub><mi>\ud835\udf40</mi><mi>n</mi></msub><mo movablelimits=\"false\">:</mo><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\u03bb</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04731.tex", "nexttext": "\nNow since $\\bm{\\mu}_{n} \\prec_{w} \\bm{\\lambda}_{n}$, Corollary~\\ref{majorization_cor_weak} yields\n\\begin{eqnarray*}\nP_{n}(\\bm{\\mu};\\alpha,\\beta,x) &\\geq& P_{n}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x > \\frac {(2  \\alpha + 1) \\sum_{i=1}^{n} \\lambda_{i} }{2 \\beta }, \\\\\nP_{n}(\\bm{\\mu};\\alpha,\\beta,x) &\\leq& P_{n}(\\bm{\\lambda};\\alpha,\\beta,x), \\quad \\forall x <\n  \t\\frac{(\\alpha-1) \\sum_{i=1}^{n} \\mu_{i}}{\\beta} , \\quad \\alpha > 1.\n\\end{eqnarray*}\nTaking the limit as $n \\rightarrow \\infty$ gives the desired result.\n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 45445, "prevtext": "\nBy the continuity from above, it is clear that\n\n", "index": 69, "text": "\\begin{equation*}\nP_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x) = \\lim_{n \\rightarrow \\infty} P_{n}(\\bm{\\lambda}_{n};\\alpha,\\beta,x).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"P_{\\infty}(\\bm{\\lambda};\\alpha,\\beta,x)=\\lim_{n\\rightarrow\\infty}P_{n}(\\bm{%&#10;\\lambda}_{n};\\alpha,\\beta,x).\" display=\"block\"><mrow><mrow><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf40</mi><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><msub><mi>P</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf40</mi><mi>n</mi></msub><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]