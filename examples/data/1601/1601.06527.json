[{"file": "1601.06527.tex", "nexttext": "\nwhere $P(k)$ is the probability of the degree $k$ of a node. Usually $2 \\leq \\gamma \\leq3$.\n\nFrom this distribution we get a lot of nodes with a low node degree and only less nodes with higher degrees.\nThese highly connected nodes are called hubs. \\label{keyword:hubs}\nIn our algorithm, we use these hubs as starting points for the clusters.\n\n\n\\subsection{Related Algorithms}\n\\label{sub:related_algorithms}\n\n\\subsubsection{Spectral Clustering}\n\nSpectral clustering \\cite{Shi2000} uses an eigenvalue analysis of the normalized Laplacian matrix.\nAs input, we use the connectivity matrix of our graphs.\nThe first $k$ eigenvectors are used for a dimension reduction.\n\nOn the lower-space data, we use either k-means \\cite{Ng2002} as partition algorithm or the discretization proposed by Yu and Shi~\\cite{Yu2003}.\n\n\\subsubsection{Louvain Method}\n\nIn 2008 Blandel et al.~\\cite{Blondel2008} propose an algorithm to extract community structure from large datasets.\nThe method is based on heuristics and modularity optimization.\nThe main idea is, that first each node is assigned an individual community.\nThen iteratively, for every node a modularity gain for switching to adjacent communities is calculated.\nIf there is a positive modularity gain the node will put into the corresponding community with the highest modularity gain.\nThis process is done until no further increment is possible.\n\nNow, there is a new graph build on the community structure of the input graph.\nEach community becomes a node.\nThe edges in the new graph are representing the sum of edges between two communities.\n\nThe procedure will be repeated until there is a complete hierarchy of communities with only two communities on top.\n\n\n\n\n\n\n\n\n\n\\section{Algorithm}\n\\label{sec:algorithm}\n\nOur algorithm to find clusters in dynamic graphs is based on two steps.\nFirst, we have to determine all hubs in the network, which will be used as cluster centers.\nStarting from them, we can propagate shortest paths to these hubs trough the whole network.\nIf the graph is altered, the clustering can be updated locally.\n\nIn the following we start with the selecting of hub nodes.\nAfterwards, we present the basic algorithm.\nAdditionally, we propose some optimizations.\n\n\n\\subsection{Selecting Hubs}\n\nAs mentioned in \\autoref{keyword:hubs}, hubs are highly connected nodes.\nThe simplest method is to determine the top $n$, relative or absolute, connected nodes and use them as hubs.\nThe main drawback of this method is, that all hubs has to be tracked and a decision if another node gets a hub could be done only with this tracked hubs.\n\nTo decide whether a node is a suitable hub individually, we introduce a threshold $d_{min}$ as minimal node degree.\nEvery node with $\\deg(node) \\ge d_{min}$ will be marked as an hub node.\nThis has the advantage that not all hubs have to be known for decision, so the analysis can be done without having the full graph. \n\nThe threshold $d_{min}$ could ether set manually by the user or could be estimated from the graph.\nIf we have a scale-free network, the fraction of all nodes with an degree of $k$ follows the power-law distribution \n", "itemtype": "equation", "pos": 4953, "prevtext": "\n\n\\mainmatter  \n\n\n\\title{Online Community Detection\\\\by Using Nearest Hubs}\n\n\n\\titlerunning{Online Community Detection by Using Nearest Hubs}\n\n\n\n\n\n\n\n\\author{Pascal Held\\and Rudolf Kruse}\n\n\\authorrunning{P. Held, R. Kruse}\n\n\n\n\n\\institute{Otto von Guericke University of Magdeburg\\\\\nDepartment of Knowledge Processing and Language Engineering\\\\\nUniversit\\\"atsplatz 2, 39106 Magdeburg, GERMANY\\\\\n\\url{pascal.held@ovgu.de}\\\\\n\\url{rudolf.kruse@ovgu.de}\\\\\n\\url{http://fuzzy.cs.uni-magdeburg.de}}\n\n\n\n\n\n\n\n\n\\toctitle{Lecture Notes in Computer Science}\n\\tocauthor{Authors' Instructions}\n\\maketitle\n\n\n\n\\begin{abstract}\n\n\n\n\n\n\n\n\n\n\n\nCommunity and cluster detection is a popular field of social network analysis.\nMost algorithms focus on static graphs or series of snapshots.\n\nIn this paper we present an algorithm, which detects communities in dynamic graphs.\nThe method is based on shortest paths to high-connected nodes, so called hubs.\nDue to local message passing we can update the clustering results with low computational power.\n\nThe presented algorithm is compared with other for some static social networks. The reached modularity is not as high as the Louvain method, but even higher then spectral clustering.\nFor large-scale real-world datasets with given ground truth, we could reconstruct most of the given community structure.\nThe advantage of the algorithm is the good performance in dynamic scenarios.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nSocial network analysis has become very popular in the last years.\nOne part of this scientific field is cluster and community detection.\nThis could be used to describe changes in the network structure.\nIf we consider the famous Karate club example from \\cite{Zachary1977}.\nThe community analysis from the member relation can describe, why the group has split up into two subgroups.\n\nMost algorithms focus on a single analysis of a static graph, e.g.\\ \\cite{Donath1973, Macqueen1967}.\nThe next step is to use these algorithms on several snapshots of the same graph.\nChanges in the clustering could be tracked with different algorithms, e.g.\\ \\cite{Takaffoli2011}.\nThere is also work done on dynamic graphs, e.g.\\ \\cite{Falkowski2007, Falkowski2008}.\n\nIn this paper we present an online capable algorithm to find communities based on high-connected hubs.\n\nThe paper is structured as follows. First, we give a brief introduction in \\autoref{sec:related_work} into cluster and community structure and into related algorithms. Next, we present the proposed algorithm in \\autoref{sec:algorithm} and experiments in \\autoref{sec:experiments}. The paper will end with an conclusion in \\autoref{sec:conclusion}.\n\n\n\n\n\n\n\\section{Related Work}\n\\label{sec:related_work}\n\nIn this section, we introduce the term of cluster and community structure.\nAlso, we present some properties of social networks.\nIn the second part of this section we give a brief introduction into related algorithms.\nThese algorithms are used to compare the results of our algorithm.\n\n\n\n\\subsection{Cluster- and Community Structure}\n\\label{sub:cluster_community}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{cluster-structure}\n\\caption{Cluster structure}\n\\label{fig:cluster-structure}\n\\end{figure}\n\nSocial networks have the property, that they are clustered.\nThis means, that there are nodes, which are connected in a more dense way then other nodes, e.g.~\\autoref{fig:cluster-structure}.\n\nThe main question is, how to find these clusters.\nIn social network analysis we can distinguish between a graph partition and covering.\nThe partition is related to the cluster structure. \nEvery node is assigned to exactly one cluster.\nIn the more advanced covering communities (or fuzzy partitions) can overlap, so a node can be assigned to more then one community at a time.\nThis fact makes it hard to evaluate the quality of the community structure.\n\nTwo simple measures for evaluating the quality of a partition are the intra-cluster density and the inter-cluster sparseness. \nThe intra-cluster density describes the ratio of existing and possible edges within the clusters.\nThis measure should be high, so clusters are connected strong.\nThe inter-cluster sparseness describes the ratio of existing and possible edges between nodes of different clusters. This value should be small, to get good separations between the clusters \\cite{Fortunato2010}.\n\nAnother very popular measurement is the q-modularity proposed by Newman and Girvan \\cite{Newman2004}.\nThe basic idea is, that random graphs have no cluster structure.\nA comparison between the observed cluster density within a cluster should be higher then the density in these random graph.\nIf we maximize the q-modularity we come up with a good clustering.\n\nIn 1999, Barab\\'asi et al.\\ \\cite{Barabasi1999} introduced the idea of scale free networks.\nThis means that node degree distribution of all nodes follows a power-law function.\n", "index": 1, "text": "\n\\[\n\tP(k) \\sim k^{-\\gamma},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"P(k)\\sim k^{-\\gamma},\" display=\"block\"><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><msup><mi>k</mi><mrow><mo>-</mo><mi>\u03b3</mi></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\nThe $\\gamma$ must be estimated from the network, e.g. by using the maximum likelihood method.\nIf the user defines a fraction of hubs $h$ in the networks, $d_{min}$ could be estimated as follows:\n", "itemtype": "equation", "pos": 8107, "prevtext": "\nwhere $P(k)$ is the probability of the degree $k$ of a node. Usually $2 \\leq \\gamma \\leq3$.\n\nFrom this distribution we get a lot of nodes with a low node degree and only less nodes with higher degrees.\nThese highly connected nodes are called hubs. \\label{keyword:hubs}\nIn our algorithm, we use these hubs as starting points for the clusters.\n\n\n\\subsection{Related Algorithms}\n\\label{sub:related_algorithms}\n\n\\subsubsection{Spectral Clustering}\n\nSpectral clustering \\cite{Shi2000} uses an eigenvalue analysis of the normalized Laplacian matrix.\nAs input, we use the connectivity matrix of our graphs.\nThe first $k$ eigenvectors are used for a dimension reduction.\n\nOn the lower-space data, we use either k-means \\cite{Ng2002} as partition algorithm or the discretization proposed by Yu and Shi~\\cite{Yu2003}.\n\n\\subsubsection{Louvain Method}\n\nIn 2008 Blandel et al.~\\cite{Blondel2008} propose an algorithm to extract community structure from large datasets.\nThe method is based on heuristics and modularity optimization.\nThe main idea is, that first each node is assigned an individual community.\nThen iteratively, for every node a modularity gain for switching to adjacent communities is calculated.\nIf there is a positive modularity gain the node will put into the corresponding community with the highest modularity gain.\nThis process is done until no further increment is possible.\n\nNow, there is a new graph build on the community structure of the input graph.\nEach community becomes a node.\nThe edges in the new graph are representing the sum of edges between two communities.\n\nThe procedure will be repeated until there is a complete hierarchy of communities with only two communities on top.\n\n\n\n\n\n\n\n\n\n\\section{Algorithm}\n\\label{sec:algorithm}\n\nOur algorithm to find clusters in dynamic graphs is based on two steps.\nFirst, we have to determine all hubs in the network, which will be used as cluster centers.\nStarting from them, we can propagate shortest paths to these hubs trough the whole network.\nIf the graph is altered, the clustering can be updated locally.\n\nIn the following we start with the selecting of hub nodes.\nAfterwards, we present the basic algorithm.\nAdditionally, we propose some optimizations.\n\n\n\\subsection{Selecting Hubs}\n\nAs mentioned in \\autoref{keyword:hubs}, hubs are highly connected nodes.\nThe simplest method is to determine the top $n$, relative or absolute, connected nodes and use them as hubs.\nThe main drawback of this method is, that all hubs has to be tracked and a decision if another node gets a hub could be done only with this tracked hubs.\n\nTo decide whether a node is a suitable hub individually, we introduce a threshold $d_{min}$ as minimal node degree.\nEvery node with $\\deg(node) \\ge d_{min}$ will be marked as an hub node.\nThis has the advantage that not all hubs have to be known for decision, so the analysis can be done without having the full graph. \n\nThe threshold $d_{min}$ could ether set manually by the user or could be estimated from the graph.\nIf we have a scale-free network, the fraction of all nodes with an degree of $k$ follows the power-law distribution \n", "index": 3, "text": "\n\\[\n\tP(k) \\sim k^{-\\gamma}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"P(k)\\sim k^{-\\gamma}.\" display=\"block\"><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><msup><mi>k</mi><mrow><mo>-</mo><mi>\u03b3</mi></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\nwith $n$ is number of nodes in the network, and\n", "itemtype": "equation", "pos": 8332, "prevtext": "\nThe $\\gamma$ must be estimated from the network, e.g. by using the maximum likelihood method.\nIf the user defines a fraction of hubs $h$ in the networks, $d_{min}$ could be estimated as follows:\n", "index": 5, "text": "\n\\[\n\td_{min} = {\\operatornamewithlimits{argmax}}_x D_n(x) \\wedge D_n(x) \\ge h\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"d_{min}={\\operatornamewithlimits{argmax}}_{x}D_{n}(x)\\wedge D_{n}(x)\\geq h\" display=\"block\"><mrow><msub><mi>d</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mrow><mrow><munder><mo movablelimits=\"false\">argmax</mo><mi>x</mi></munder><mo>\u2061</mo><msub><mi>D</mi><mi>n</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2227</mo><mrow><msub><mi>D</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mi>h</mi></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\n\n\\subsection{Basic Algorithm}\n\nThe algorithm is based on passing hub information through the network.\nImportant changes in the network structure are propagated to all relevant nodes.\n\nEach node stores a hub information table $T$ with the tuple entries $(h, p, \\alpha)$, where $h$ represents the corresponding hub and $p$ the parent node, with the shortest path to the hub. \n$\\alpha$ represents a weight of this information. \nAdditionally we store the hub distance $d$.\n\n\\subsubsection{The Message} \n$M_{x \\to y}(T', d')$ sent from a node to the neighbor nodes contains the basic hub information table \n", "itemtype": "equation", "pos": 8460, "prevtext": "\nwith $n$ is number of nodes in the network, and\n", "index": 7, "text": "\n\\[\n\tD_n(x) = \\sum_{k=d_{min}}^{n} P(k).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"D_{n}(x)=\\sum_{k=d_{min}}^{n}P(k).\" display=\"block\"><mrow><mrow><mrow><msub><mi>D</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><msub><mi>d</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></mrow><mi>n</mi></munderover><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\nThe distance is set to \n", "itemtype": "equation", "pos": 9105, "prevtext": "\n\n\\subsection{Basic Algorithm}\n\nThe algorithm is based on passing hub information through the network.\nImportant changes in the network structure are propagated to all relevant nodes.\n\nEach node stores a hub information table $T$ with the tuple entries $(h, p, \\alpha)$, where $h$ represents the corresponding hub and $p$ the parent node, with the shortest path to the hub. \n$\\alpha$ represents a weight of this information. \nAdditionally we store the hub distance $d$.\n\n\\subsubsection{The Message} \n$M_{x \\to y}(T', d')$ sent from a node to the neighbor nodes contains the basic hub information table \n", "index": 9, "text": "\n\\[\n\tT' = \\left\\lbrace\\left(h, \\frac{\\alpha}{\\sum_i \\alpha_i}\\right) : (h,p,\\alpha) \\in T\\right\\rbrace.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"T^{\\prime}=\\left\\{\\left(h,\\frac{\\alpha}{\\sum_{i}\\alpha_{i}}\\right):(h,p,\\alpha%&#10;)\\in T\\right\\}.\" display=\"block\"><mrow><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mi>h</mi><mo>,</mo><mfrac><mi>\u03b1</mi><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></mfrac><mo>)</mo></mrow><mo>:</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>T</mi></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\nwhere $\\omega(x,y)$ is the weight of the $(x,y)$-edge.\n\n\\subsubsection{Processing Messages}\n$M_{x \\to y}(T', d')$ in the target node.\nWe focus on three different cases. First, if $d'<d$ the new node distance in lower then the current distance. \nThe hub information table is set to the table from the message, where $p$ is set to the sending node.\nAlso the distance is updated to the new distance.\n\nThe second case is, that the message has the same distance value then the current one $d'=d$. In this case we removed all tuples concerning the sending node and append the new information.\n", "itemtype": "equation", "pos": 9235, "prevtext": "\nThe distance is set to \n", "index": 11, "text": "\n\\[\n\td' = d + \\omega(x, y),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"d^{\\prime}=d+\\omega(x,y),\" display=\"block\"><mrow><mrow><msup><mi>d</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mi>d</mi><mo>+</mo><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06527.tex", "nexttext": "\n\nIn both cases, the new hub information table will be propagated to all other neighbors.\n\nIf $d' \\geq d + \\omega(y,x)$, the sender has a worse connection then the receiver. \nThen the table is not updated, but the current table is propagated to the sender, so the sending node can update its distances.\n\nOtherwise the message is dropped, with no further steps.\n\n\\subsubsection{Altering the graph}\n\nChanges in the graph structure are handled as follows: If a new edge $(x,y)$ is added, node $s$ sends an information message to $y$. \nDue to message processing ether $y$ will update its hub information or sends its information to $x$. \nAdditional the new structure is propagated through the network.\n\nWhenever an edge is removed, we have to check both nodes, if the current edge was the connection to the parent node. \nAssociated hub information tuple have to be removed. \nIf this clears the table, the distance is set to $d=\\infty$.\nChanges have to be propagated to all neighbors.\n\nIf a node is removed, also all connected edges has to be removed and processed. \nPure node creation does not influence the structure and does not have to be handled.\n\n\\subsubsection{Defining new Hubs}\nIf a node $n$ gets higher connected and becomes a hub, the distance value is set to $d=0$ and the hub information table is set to $T=(n, nil, 1)$.\nThe information has to be propagated to all neighbors.\n\n\\subsubsection{Removing Hubs}\nIf a node $n$ loses his hub state, the distance is set to $d=\\infty$ and the information table is cleared.\nAfter propagating this information all neighbor nodes send alternative hub information.\n\n\\subsubsection{Assign Cluster or Community}\nBased on the $\\alpha$ value in the hub information table of each node, we can assign cluster and community labels.\nIf we want to have crisp cluster assignments, we sum up all $\\alpha$ values for each $h$.\nThe $h$ with the highest sum, will be the cluster label.\n\nIf we do not need crisp assignments, we normalize the $\\alpha$ sums and use them as membership degree for a certain community.\n\n\\subsection{Optimization}\n\n\\subsubsection{Running Initial Steps parallel}\nWe propose to collected all open messages in a priority queue, ordered by distance.\nThis yields into a breath first search around the hubs.\nEspecially if there are multiple new detected hubs at the same time, e.g.\\ in the static scenario, where the whole network is given.\nThis can decrease the number of messages to be processed dramatically.\n\n\\subsubsection{Hash message -- avoid duplicates}\nAnother optimization step is the hashing of open message edges. \nNodes that are on the border of two clusters get multiple information from both hubs.\nThese combined information have to be send to all following nodes.\nSo, if there is a message $M_{x \\to y}$ in the queue and $x$ gets new information another message $M_{x\\to y}$ is created and the old message could be removed.\nThis could be done by storing the message content on the edge, while having the not processed edges in the priority queue.\n\n\n\n\n\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nWe will do a two step evaluation of our presented algorithm, first a static and second a dynamic test.\nIn the static test, we will compare the clustering results with other algorithms.\nThe dynamic test should check the performance for dynamic graphs.\n\n\\subsection{Static Test}\n\\label{sub:test_settings}\n\nFirst we will check the algorithm in a static test setting.\nWe take some well-known datasets to evaluate and compare clustering results with the spectral clustering and louvain method.\n\nWe will start with a deeper view on the well-known karate dataset from Zachary~\\cite{Zachary1977}.\nHe observed the relation of the members of the club, after the club has split up into two groups.\n\n\\begin{figure}\n\\centering\n\\includegraphics[trim=3cm 2cm 3cm 2cm, clip=true, width=0.8\\columnwidth]{nhc-karate}\n\\caption{NHC-clustering: karate club}\n\\label{fig:nhc-karate}\n\\end{figure}\n\n\\autoref{fig:nhc-karate} shows the results of the clustering with our algorithm.\nThe outer circle describes the community membership distribution of the different communities.\nThe inner circle is the crisp cluster association described in \\autoref{sec:algorithm}.\nThe solid lines describe connections, which are used for next hub propagation.\nThe dotted lines are irrelevant for the algorithm and could be removed without need to do further processing steps.\n\nIn the figure we can see the two cluster center nodes 1 and 34.\nThe nodes 9, 14, 20, and 32 are exactly in-between the two center nodes.\nDue to crisp partition they are associated to cluster 34, but they could also be assigned to cluster 1.\n\n\\begin{figure}\n\\centering\n\\includegraphics[trim=3cm 2cm 3cm 2cm, clip=true, width=0.8\\columnwidth]{louvain-karate}\n\\caption{Louvain-clustering: karate club}\n\\label{fig:louvain-karate}\n\\end{figure}\n\n\\autoref{fig:louvain-karate} shows the same graph clustered with the louvain method.\nThis method offers four clusters, where the two main clusters are similar to the NHC results.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{mod-karate}\n\\caption{Modularity comparison for different number of clusters (karate dataset)}\n\\label{fig:mod-karate}\n\\end{figure}\n\nIn \\autoref{fig:mod-karate} we show the reached modularity for NHC, spectral clustering, and the louvain method for different number of clusters.\nThe louvain method only offers results for four and six clusters. \nA more detailed structure is not possible.\nNHC outperforms the spectral clustering with k-means for every number of clusters.\nEspecially for small number of clusters the spectral clustering with discretization and the louvain method produce good results for the modularity.\nFor 10 and more clusters NHC produces similar results to discretized spectral clustering.\nThe lower values for lower number of clusters could be caused by the assignment method for equal distributed memberships.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{mod-dolphins}\n\\caption{Modularity comparison for different number of clusters (dolphins dataset)}\n\\label{fig:mod-dolphins}\n\\end{figure}\n\nIn \\autoref{fig:mod-dolphins} we can see the modularity for the dolphins network~\\cite{Lusseau2003}. \nThe dataset represents the social structure of 62 dolphins.\nNHC does not reach the modularity of the other methods for lower number of clusters, but for higher numbers the values are similar to spectral clustering with discretization.\nAgain the louvain method could not produce results for finer structures.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{mod-netscience}\n\\caption{Modularity comparison for different number of clusters (netscience dataset)}\n\\label{fig:mod-netscience}\n\\end{figure}\n\nOur last dataset is the netscience dataset assembled by Newman~\\cite{Newman2006}.\nThe dataset contains a coauthorship network of scientists.\nThe network contains 1589 nodes, with weighted edges.\nWe focus on an unweighted version.\nIn \\autoref{fig:nhc-netscience} you will find the cluster results from our algorithm.\nA comparison to the louvain-clustering in \\autoref{fig:louvain-netscience} shows a high similarity in cluster assignments of the nodes.\nThe reached modularity by our algorithm is lower then the louvain method, and also lower then spectral clustering with discretization, but even higher then spectral clustering with k-means.\n\n\\begin{figure}\n\\centering\n\\includegraphics[trim=10cm 7cm 10cm 6.5cm, clip=true, width=.8\\textwidth]{nhc-netscience-huge}\n\\caption{NHC-clustering: netscience}\n\\label{fig:nhc-netscience}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[trim=10cm 7cm 10cm 6.5cm, clip=true, width=.8\\textwidth]{louvain-netscience-huge}\n\\caption{Louvain-clustering: netscience}\n\\label{fig:louvain-netscience}\n\\end{figure}\n\n\\newpage\n\n\\subsubsection{Large Scale Networks}\n\nYang and Leskovec~\\cite{Yang2013} provide a set of large scale online communities including a ground truth.\nWe used there the DBLP, YouTube, and Amazon dataset to check the cluster prediction performance of our algorithm. These datasets contain from $300,000$ to $1,100,000$ nodes and from $925.000$ to almost $3,000,000$ edges. In the dataset ground truth, they found from $8,300$ to $75,000$ communities.\n\nWe use the V-measure-score~\\cite{Rosenberg2007} to evaluate the cluster performance.\nDue to the computational complexity of this measure, especially for large amount of clusters, we will use the top 5000 communities provided by Yang and Leskovec.\nThe selection was done by different community measures.\n\nDue to the complexity of spectral clustering and the fact we have to check a lot of different number of communities we skip this algorithm for this experiment. We will compare Louvain results with our proposed algorithm.\n\n\n\\begin{table}[htb]\n\\centering\n\\caption{Large scale network comparison for Louvain and Nearest Hub Clustering}\n\\label{tab:large}\n\\begin{tabular}{p{5cm}ccc}\n\\toprule\n~ & DBLP & Amazon & YouTube \\\\\n\\midrule\nNumber of nodes & $ 317,080 $ & $ 334,863 $ & $1,134,890$ \\\\\nNumber of edges & ~ $1,049,866$ ~ & ~ $925,872$ ~ & ~ $2,987,624$ ~ \\\\\nNumber of communities & $13,477$ & $75,149$ & $8,385$ \\\\\nTop 5000 communities - nodes & $ 112,228 $ & $ 67,462 $ & $72,959$ \\\\\n\\midrule\nLouvain - time & \\unit[98,7]{s}&  \\unit[81.7]{s} & \\unit[252,2]{s}\\\\\nLouvain - V-measure-score & $0.525$ & $0.863$ & $0.450$\\\\\nLouvain - NMI-score & $0.530$ & $0.871$ & $0.510$\\\\\nLouvain - q-modularity  & $0.818$ & $0.926$ & $0.710$\\\\\n\\midrule\nNHC - time & \\unit[135.35]{s}& \\unit[168.31]{s} & \\unit[154,8]{s}\\\\\nNHC - min-degree &12& 16& 4\\\\\nNHC - V-measure-score & $0.726$ & $0.944$ & $0.832$\\\\\nNHC - NMI-score & $0.746$ & $0.945$ & $0.842$\\\\\nNHC - q-modularity  & $0.432$ & $0.613$ & $0.297$\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nFrom Table~\\ref{tab:large} we get, that the Louvain method gets much higher values for q-modularity, but this is not correlated to better community structure given by the ground truth from the datasets.\nWe got V-measure-scores from $0.73$ to $0.944$ which indicates that a large amount of community structure could be found by our algorithm.\n\n\\subsection{Dynamic Test}\n\\label{sub:dynamic_test}\n\nIn this section we will check the dynamic behavior of the presented algorithm.\nWe generate random clustered powerlaw-graphs with the algorithm proposed by Holme and Kim~\\cite{Holme2002}.\nAs parameters we choose $n=1000$ nodes, which are connected each to $m=10$ other nodes.\nWith a probability of $p=0.7$ the model will create a triangle to increase the cluster coefficient of the resulting network.\n\n\n\\subsubsection{Adding Edges}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{add_hist}\n\\caption{Logarithmic histogram of messages send during 10000 adding events.}\n\\label{fig:add-hist}\n\\end{figure}\n\nTo test the adding edges behavior we generated 100 graphs and added 100 random edges to each graph.\nThis yields into 10000 adding edged events.\nIn \\autoref{fig:add-hist} we show a logarithmic histogram of the processed messages distribution.\nIf a new edge does not create a new shorter path to the hubs, which happens in \\unit[63.3]{\\%} of the cases, only two messages between the two new connected nodes have to be processed.\nOn average 7.73 messages and maximal 549 messages have been send.\n\nDue to the low amount of messages needed to update the clustering during adding edges to the network, the presented algorithm performs well in dynamic adding nodes and edges.\n\n\\subsubsection{Removing Edges}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{remove_hist}\n\\caption{Logarithmic histogram of messages send during 10000 removing events.}\n\\label{fig:remove-hist}\n\\end{figure}\n\nWe performed similar test for removing edges.\nAgain we created 100 graphs and removed randomly 100 existing edges from each graph, so we get 10000 removing events.\nIn \\autoref{fig:remove-hist} is a logarithmic histogram of the send messages distribution presented.\nIf the removed edge does not destroy the shortest path to an hub, we do not have to send any messages at all, which happens in \\unit[60.1]{\\%} of the events.\nOn average we need 14.2 messages. Maximal 674 messages have send, this is a really rare case where the structure near the hubs is changed.\n\nAlso the removing process works with a low amount of send messages.\nThis shows, that the presented algorithm performs well on dynamic graph structures.\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe presented an algorithm for graph clustering.\nIn static tests it shows that the algorithm produces high modularity results for finer structure.\nIn contrast to the louvain method or the spectral clustering with discretization the algorithm did not reach the global optima of modularity.\n\nTests on large-scale real-world datasets show, that large parts of the underlying community structure could be found by our algorithm.\nA V-measure-score analysis again a ground truth given by~\\cite{Yang2013} showed scores from $0.73$ to $0.944$. This is much more then the Louvain method reached.\n\nThe main advantage of the algorithm is the dynamic behavior.\nIf the graph changes over time, only a small amount of processing steps have to be done to update the clustering.\nThis enables the it for online cluster and community analysis.\n\nThe algorithm itself generates overlapping communities and provides membership degrees.\nThese results are deterministic.\nRandomness influences only the crisp cluster assignment.\n\nThe next steps we focus on is the optimization of the hub threshold.\nAn implementation in python will be provided at \\url{http://bitbucket.org/paheld/dynamix}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{splncs03}\n\\bibliography{Held}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 9852, "prevtext": "\nwhere $\\omega(x,y)$ is the weight of the $(x,y)$-edge.\n\n\\subsubsection{Processing Messages}\n$M_{x \\to y}(T', d')$ in the target node.\nWe focus on three different cases. First, if $d'<d$ the new node distance in lower then the current distance. \nThe hub information table is set to the table from the message, where $p$ is set to the sending node.\nAlso the distance is updated to the new distance.\n\nThe second case is, that the message has the same distance value then the current one $d'=d$. In this case we removed all tuples concerning the sending node and append the new information.\n", "index": 13, "text": "\n\\[\n\tT_{new} = \\left( \\bigcup_{(h, p, \\alpha) \\in T \\wedge p \\neq x} (h, p, \\alpha) \\right) \\cup \\left\\lbrace (h, x, \\alpha) : (h, \\alpha) \\in T' \\right\\rbrace\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"T_{new}=\\left(\\bigcup_{(h,p,\\alpha)\\in T\\wedge p\\neq x}(h,p,\\alpha)\\right)\\cup%&#10;\\left\\{(h,x,\\alpha):(h,\\alpha)\\in T^{\\prime}\\right\\}\" display=\"block\"><mrow><msub><mi>T</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo>=</mo><mrow><mrow><mo>(</mo><mrow><munder><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2227</mo><mi>p</mi></mrow><mo>\u2260</mo><mi>x</mi></mrow></munder><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u222a</mo><mrow><mo>{</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>:</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msup><mi>T</mi><mo>\u2032</mo></msup></mrow><mo>}</mo></mrow></mrow></mrow></math>", "type": "latex"}]