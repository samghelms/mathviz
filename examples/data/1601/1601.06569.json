[{"file": "1601.06569.tex", "nexttext": "\n\nSince $K(E,\\pi)$ is an intersection of linear constraints, it defines a convex polytope, a fact which will be of later algorithmic importance. An immediate corollary of Theorem \\ref{thm:ngrussel}, is that given a sequence of experiments $\\mathcal{E} = ((E^1,\\pi^1), ..., (E^n, \\pi^n))$, the set of rewards consistent with $\\mathcal{E}$ are precisely those in \n\n\n", "itemtype": "equation", "pos": 12166, "prevtext": "\n\n\\title{Towards Resolving Unidentifiability in Inverse Reinforcement Learning}\n\n\\numberofauthors{3}\n\n\\author{ Kareem Amin \\\\ University of Michigan \\\\ amkareem@umich.edu  \\alignauthor Satinder Singh \\\\ University of Michigan \\\\ baveja@umich.edu }\n\n\\maketitle\n\n\\begin{abstract}\nWe consider a setting for Inverse Reinforcement Learning (IRL) where the learner is extended with the ability to \\emph{actively} select multiple environments, observing an agent's behavior on each environment. We first demonstrate that if the learner can experiment with \\emph{any} transition dynamic on some fixed set of states and actions, then there exists an algorithm that reconstructs the agent's reward function to the fullest extent theoretically possible, and that requires only a small (logarithmic) number of experiments. We contrast this result to what is known about IRL in single fixed environments, namely that the true reward function is fundamentally unidentifiable. We then extend this setting to the more realistic case where the learner may not select any transition dynamic, but rather is restricted to some fixed set of environments that it may try. We connect the problem of maximizing the information derived from experiments to active submodular function maximization, and demonstrate that a greedy algorithm is near optimal (up to logarithmic factors). Finally, we empirically validate our algorithm on an environment inspired by behavioral psychology.  \n\\end{abstract}\n\n\\section{Introduction}\n\nInverse reinforcement learning (IRL), first introduced by Ng and Russell {(\\citeyear{{ng2000algorithms}})}, is concerned with the problem of inferring the (unknown) reward function of an agent behaving optimally in a Markov decision process. The most basic formulation of the problem asks: given a known environment\\footnote{We will use the terminology environment to refer to an MDP without a reward function.} $E$, and an optimal agent policy $\\pi$, can we deduce the reward function $R$ which makes $\\pi$ optimal for the MDP $(E,R)$? \n\nIRL has seen a number of applications in the development of autonomous systems, such as autonomous vehicle operation, where even a cooperative (human) agent might have great difficultly describing her incentives \\cite{smart2002effective,abbeel2004apprenticeship,abbeel2007application,coates2009apprenticeship}. However, the problem is fundamental to almost any study which involves behavioral modeling. Consider an experimental psychologist attempting to understand the internal motivations of a subject, say a mouse, or consider a marketer observing user behavior on a website, hoping to understand the potential consumer's value for various offers. \n\nAs noted by Ng and Russell, a fundamental complication to the goals of IRL is the impossibility of identifying the exact reward function of the agent from its behavior. In general, there may be infinitely many reward functions consistent with any observed policy $\\pi$ in some fixed environment. Since the true reward function is fundamentally unidentifiable, much of the previous work in IRL has been concerned with the development of heuristics which prefer certain rewards as better explanations for behavior than others \\cite{ng2000algorithms,ziebart2008maximum,ramachandran2007bayesian}.\nIn contrast, we make several major contributions towards directly resolving the issue of unidentifiability in IRL in this paper. \n\nAs a first contribution, we separate the causes of this unidentifiability into three classes. 1) A trivial reward function, assigning constant reward to all state-action pairs, makes all behaviors optimal; the agent with constant reward can execute any policy, including the observed $\\pi$. 2) Any reward function is behaviorally invariant under certain arithmetic operations, such as re-scaling. Finally, 3) the behavior expressed by some observed policy $\\pi$ may not be sufficient to distinguish between two possible reward functions both of which \\emph{rationalize the observed behavior}, i.e., the observed behavior could be optimal under both reward functions. We will refer to the first two cases of unidentifiability as \\emph{representational unidentifiability}, and the third as \\emph{experimental unidentifiability}. \n\nAs a second contribution, we will demonstrate that, while representational unidentifiability is unavoidable, experimental unidentifiability is not. In contrast to previous methods, we will demonstrate how the latter can be eliminated completely in some cases. Moreover, in a manner which we will make more precise in Section \\ref{sec:identification}, we will argue that in some ways representational unidentifiability is superficial; by eliminating experimental unidentifiability, one arrives at the fullest possible  characterization of an agent's reward function that one can hope for.\n\nAs a third contribution, we develop a slightly richer model for IRL. We will suppose that the learner can observe the agent behaving optimally in \\emph{a number of environments of the learner's choosing}. Notice that in many of our motivating examples it is reasonable to assume that the learner does indeed have this power. One can ask the operator of a vehicle to drive through multiple terrains, while the experimental psychologist might observe a mouse across a number of environments. It is up to the experimenter to organize the dynamics of the maze. One of our key results will be that, with the right choice of environments, the learner can eliminate experimental unidentifiability. We will study our {\\bf repeated experimentation for IRL} in two settings, one in which the learner is omnipotent in that there are no restrictions on what environments can be presented to the agent, and another in which there are restrictions on the type of environments the learner can present. We show that in the former case, experimental unidentifiability can be eliminated with just a small number of environments. In the latter case, we cast the problem as budgeted exploration, and show that for some number of environments $B$, a simple greedy algorithm approximately maximizes the information revealed about $R$ in $B$ environments. \n\n\n\n\\paragraph{Most Closely Related Work}\n\nPrior work in IRL has mostly focused on inferring an agent's reward function from data acquired from a fixed environment \\cite{ng2000algorithms,abbeel2004apprenticeship,coates2008learning,ziebart2008maximum,ramachandran2007bayesian,syed2007game,regan2010robust}. We consider a setting in which the learner can actively select multiple environments to explore, before using the observations obtained from these environments to infer an agent's reward. Studying a model where the agent can make active selections of environments in an IRL setting is novel to the best of our knowledge. Previous applications of active learning to IRL have considered settings where, \\emph{in a single environment}, the learner can query the agent for its action in some state \\cite{lopes2009active}, or for information about its reward \\cite{regan2009regret}. \n\nThere is prior work on using data collected from multiple --- but exogenously fixed --- environments to predict agent behavior \\cite{ratliff2006maximum}. There are also applications where methods for single-environment MDPs have been adapted to multiple environments \\cite{ziebart2008maximum}. Nevertheless, both these works do not attempt to resolve the ambiguity inherent in recovering the true reward in IRL, and describe IRL as being an ``ill-posed'' problem. As a result these works ultimately consider the objective of mimicking or predicting an agent's optimal behavior. While this is a perfectly reasonable objective, we will more be interested in settings where the identification of $R$ is the goal in itself. Among many other reasons, this may be because the learner explicitly desires an interpretable model of the agent's behavior, or because the learner desires to transfer the learned reward function to new settings. \n\nIn the economics literature, the problem of inferring an agent's utility from behavior has long been studied under the heading of utility or preference elicitation \\cite{chajewska2000making,von2007theory, regan2011eliciting,rothkopf2011preference,regan2009regret,regan2011eliciting}. When these models analyze Markovian environments, they will assume a fixed environment where the learner can ask certain types of queries, such as bound queries eliciting whether some state-action reward $r(s,a) \\geq b$. We will instead be interested in cases where the learner can only make inferences from agent behavior (with no external source of information), but can manipulate the environments on which the agent acts. \n\n\\section{Setting and Preliminaries}\\label{sec:prelim}\n\n\n\nWe denote an environment by a tuple $E = (S,A,P,\\gamma)$, where $S = \\{1,...,d\\}$ is a finite set of states in which the agent can find itself, $A$ is a finite set of actions available to the agent, and $P$ is a collection of transition dynamics for each $a \\in A$, so that $P = \\{P_a\\}_{a \\in A}$. We represent each $P_a$ as a row-stochastic matrix, with $P_a \\in \\mathbb{R}^{d \\times d}$, and $P_a(s,s')$ denoting the agent's probability of transitioning to state $s'$ from state $s$ when selecting action $a$. The agent's discount factor is $\\gamma \\in (0,1)$.\n\nWe represent an agent's reward function as a vector $R \\in \\mathbb{R}^d$ with $R(s)$ indicating the (undiscounted) payout for arriving at state $s$. Note that a joint choice of Markovian environment $E$ with reward function $R$ fixes an MDP $M = (E,R)$. A policy is a mapping $\\pi : S \\rightarrow A$. With slight abuse of notation, we can represent $\\pi$ as a matrix $P_{\\pi}$ where $P_{\\pi}(s,\\cdot) = P_{\\pi(s)}(s,\\cdot)$ (we take the $s$-row of $P_{\\pi}$ to be the $s$-row of $P_a$, where $a$ is the action chosen in state $s$).\n\nLet $\\mathrm{OPT}(E,R)$ denote the set of policies that are optimal, maximizing the agent's expected time-discounted rewards, for the MDP $(E,R)$. We consider a {\\bf repeated experimentation} setting, where we suppose that the learner is able to select a sequence of environments\\footnote{Defined on the same state and action spaces.} $E^1,E^2,...$, sequentially observing $\\pi^1,\\pi^2,...$ satisfying $\\pi^i \\in \\mathrm{OPT}(E^i, R)$, for some unknown agent reward function $R$. We call each $(E^i, \\pi^i)$ an \\emph{experiment}. The goal of the experimenter is to output a reward estimate $\\hat{R}$, approximating the true reward function. In many settings, the assumption that the learner can directly observe the agent's full policy $\\pi^i$ is too strong, and a more realistic assumption is the learner observes only  \\emph{trajectories} $T^i$, where $T^i$ denotes a sequence of state-action, pairs drawn according to the distribution induced by the agent playing policy $\\pi^i$ in environment $E^i$. We will refer to the former feedback model as the {\\bf policy observation setting}, and the latter as the {\\bf trajectory observation setting}.\n\n\nA fundamental theorem for IRL follows from rewriting the Bellman equations associated with the optimal policy in a single MDP, noting that the components of the vector $P_a (I - \\gamma P_{\\pi})^{-1}R$ correspond to the Q-value for action $a$, under policy $\\pi$ and reward $R$, for each of $d$ states. \n\n\\begin{thm}[Ng, Russell \\cite{ng2000algorithms}]\\label{thm:ngrussel}\nLet $E = (S, A, P, \\gamma)$ be an arbitrary environment, and $R \\in \\mathbb{R}^d$. $\\pi \\in \\mathrm{OPT}(E,R)$ if and only if $\\forall a \\in A$, $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0$.\\footnote{The inequality is read component-wise. That is, the relation holds if standard $\\geq$ holds for each component.}\n\\end{thm}\n\nThe key take-away from this theorem is that in a policy observation setting, the set of reward functions $R$ consistent with some observed optimal policy $\\pi$ are precisely those satisfying some set of linear constraints. Furthermore, those constraints can be computed from the environment $E$ and policy $\\pi$. Thus, an object that we will make recurring reference to is the set of reward functions consistent with experiment $(E, \\pi)$, denoted $K(E,\\pi)$:\n\n\n", "index": 1, "text": "\\begin{align*}\nK(E, \\pi) = \\{ R \\in \\mathbb{R}^d \\mid &\\forall a \\in A, (P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0 \\\\\n&\\forall s \\in \\mathcal{S}, R_{\\min} \\leq R(s) \\leq R_{\\max}\\}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle K(E,\\pi)=\\{R\\in\\mathbb{R}^{d}\\mid\" display=\"inline\"><mrow><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>\u03c0</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>R</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup><mo>\u2223</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall a\\in A,(P_{\\pi}-P_{a})(I-\\gamma P_{\\pi})^{-1}R\\geq 0\" display=\"inline\"><mrow><mrow><mrow><mo>\u2200</mo><mi>a</mi></mrow><mo>\u2208</mo><mi>A</mi></mrow><mo>,</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>R</mi></mrow><mo>\u2265</mo><mn>0</mn></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall s\\in\\mathcal{S},R_{\\min}\\leq R(s)\\leq R_{\\max}\\}.\" display=\"inline\"><mrow><mo>\u2200</mo><mi>s</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>,</mo><msub><mi>R</mi><mi>min</mi></msub><mo>\u2264</mo><mi>R</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><msub><mi>R</mi><mi>max</mi></msub><mo stretchy=\"false\">}</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": " \n\nWe can also think of a trajectory $T$ as inducing a \\emph{partial} policy $\\pi_T$ on the states visited by the trajectory. In particular, let $\\mathcal{D}(T)$ denote the domain of $T$, $\\mathcal{D}(T) = \\{s \\mid \\exists (s, a) \\in T\\}$. We say two policies $\\pi,\\pi'$ are consistent on $\\mathcal{D}(T) \\subset S$, denoted $\\pi \\equiv_{\\mathcal{D}(T)} \\pi'$, iff $\\pi(s) = \\pi'(s)$ for all $s \\in \\mathcal{D}(T)$. Thus, given $(E,T)$, the set of rewards consistent with the observation are precisely $K(E,T) = \\{R \\in \\mathbb{R}^d \\mid \\exists \\pi \\equiv_{\\mathcal{D}(T)} \\pi_T, \\forall a \\in A, (P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0, R_{\\min} \\leq R_i \\leq R_{\\max}\\}$, and given a sequence $\\mathcal{E} = \\{(E^1, T^1),...,(E^n,T^n)\\}$, we can define $K(\\mathcal{E})$ in the trajectory setting. \n\n\\section{On Identification}\\label{sec:identification} \n\n\nIn this section we will give a more nuanced characterization of what it means to identify a reward function. We will argue that there are multiple types of uncertainty involved in identifying $R$, which we categorize as \\emph{representational unidentifiability} and \\emph{experimental unidentifiability}. Furthermore, we argue that first type is in some ways superficial, and ought to be ignored, while the second type can be eliminated.\n\nWe begin with a definition. Let $R$ and $R'$ be reward functions defined on the same state space $\\mathcal{S}$. We say that $R$ and $R'$ are \\emph{behaviorally equivalent} if for \\emph{any} environment (also defined on $\\mathcal{S}$), the agent whose reward function is $R$ behaves identically to the agent whose reward function is $R'$. \n\n\\begin{defn}\\label{defn:equiv}\nTwo reward vectors $R,R' \\in \\mathbb{R}^d$ defined on $\\mathcal{S}$ are \\emph{behaviorally equivalent}, denoted $R \\equiv R'$ if for any set of actions, transition dynamics, and discount, $(\\mathcal{A},\\mathcal{P},\\gamma)$, defining an environment $E = (\\mathcal{S},\\mathcal{A},\\mathcal{P},\\gamma)$ we have that \\\\\n$\\mathrm{OPT}(E, R) = \\mathrm{OPT}(E,R')$.\n\\end{defn}\n\nBehavioral equivalence defines an equivalence relation over vectors in $\\mathbb{R}^d$, and we let $[R] = \\{R' \\in \\mathbb{R}^d \\mid R' \\equiv R\\}$ denote the equivalence classes defined in this manner. Intuitively, if $R$ and $R'$ are behaviorally equivalent, they induce identical optimal policies in every single environment, and therefore are not really ``different'' reward functions. They are simply different representations of the same incentives. \n\nWe now observe that behavioral equivalence classes are invariant under multiplicative scaling by positive scalars, and component-wise translation by a constant. Intuitively, this is easy to see. Adding $c$ reward to every state in some reward function $R$ does not affect an agent's decision-making. This is simply ``background'' reward that the agent gets for free. Similarly, scaling $R$ by a positive constant simply changes the ``units\" used to represent rewards.  The agent does not, and should not, care whether its reward is represented in dollars or cents. We prove this formally in the following Theorem.\n\n\\begin{thm}\\label{thm:equiv}\nFor any $c \\in \\mathbb{R}^d$, let $\\vec{c} \\in \\mathbb{R}^d$ denote the vector with all components equal to $c$. For any $\\alpha > 0$, and $R \\in \\mathbb{R}^d$, $R \\equiv \\alpha R + \\vec{c}$. \n\\end{thm}\n\\begin{proof}\nFirst consider $\\vec{c}$ as defined in the statement of the Theorem. Fix any environment $E = (\\mathcal{S},\\mathcal{A}, \\mathcal{P}, \\gamma)$, action $a \\in \\mathcal{A}$ and arbitrary policy $\\pi$. We begin by claiming that $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}\\vec{c} = \\vec{0}$. \n\nThe Woodbury formula for matrix inversion tells us that $(I - \\gamma P_{\\pi})^{-1} = I + (I - \\gamma P_{\\pi})^{-1} \\gamma P_{\\pi}$. Furthermore, for any row-stochastic matrix $P$, $P\\vec{c} = \\vec{c}$. Therefore:\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nSince $K(E,\\pi)$ is an intersection of linear constraints, it defines a convex polytope, a fact which will be of later algorithmic importance. An immediate corollary of Theorem \\ref{thm:ngrussel}, is that given a sequence of experiments $\\mathcal{E} = ((E^1,\\pi^1), ..., (E^n, \\pi^n))$, the set of rewards consistent with $\\mathcal{E}$ are precisely those in \n\n\n", "index": 3, "text": "$$K(\\mathcal{E}) \\triangleq \\cap_{(E, \\pi) \\in \\mathcal{E}} K(E, \\pi)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"K(\\mathcal{E})\\triangleq\\cap_{(E,\\pi)\\in\\mathcal{E}}K(E,\\pi)\" display=\"block\"><mrow><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><msub><mo>\u2229</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>\u03c0</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>\u03c0</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nSince $\\gamma \\in (0,1)$, it must be that $v = \\vec{0}$. \n\nNow fix a reward function $R \\in \\mathbb{R}^d$, and arbitrary environment $E$, and consider $\\mathrm{OPT}(E,R)$. By Theorem \\ref{thm:ngrussel}, we know that $\\pi \\in \\mathrm{OPT}(E,R)$ iff for any $a \\in A$, $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0$, which occurs iff  $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}( \\alpha R) \\geq 0$, since $\\alpha$ is a positive scalar. Finally, we can conclude that $\\pi \\in \\mathrm{OPT}(E,R)$ iff for all $a \\in \\mathcal{A}$, $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}( \\alpha R + \\vec{c}) \\geq 0$, this last condition implying that $\\pi \\in \\mathrm{OPT}(E,\\alpha R + \\vec{c})$, again by Theorem \\ref{thm:ngrussel}. \n\nSince our choice of $E$ was arbitrary, by Definition \\ref{defn:equiv}, $R \\equiv \\alpha R + \\vec{c}$, concluding the proof.\n\\end{proof}\n\nThus, we argue that one reason why reward functions cannot be identified is a trivial one: the classic IRL problem does not fix a consistent representation for reward functions.  For any $R \\in \\mathbb{R}^d$ there are an uncountable number of other functions in $[R]$, namely $\\alpha R + \\vec{c}$ for any $\\alpha$ and $\\vec{c}$, all of which are behaviorally identical to $R$. However, distinguishing between these functions is irrelevant; whether an agent's ``true'' reward function is $(1,2,3,4)$ or $(0,1/3,2/3,1)$\\footnote{We get $(0,1/3,2/3,1)$ from $(1,2,3,4)$ by subtracting $1$ from every state and dividing by $3$} is simply a matter of what units are used to represent rewards. \n\nIn light of this observation, it is convenient to fix a \\emph{canonical element} of each equivalence class $[R]$. For any constant reward function $R$, we will take its canonicalized representation to be $\\vec{0}$. Otherwise we note, by way of Theorem \\ref{thm:equiv}, that any $R$ can be translated and re-scaled so that $\\max_s R(s) = 1$ and $\\min_s R(s) = 0$. More carefully, for any non-constant $R$, we take its canonicalized representation to be $(R - \\min_s R(s))/(\\max_s R(s) - \\min_s R(s)) \\in [R]$. This canonicalization is consistent with behavioral equivalence, and we state the following Theorem whose proof can be found in the appendix. As a consequence of this Theorem, we can use the notation $[R]$ interchangeably to refer to the equivalence class of $R$, or the the unique canonical element of $[R]$.\n\n\\begin{thm}\nFor any $R,R' \\in \\mathbb{R}^d$, $R \\equiv R'$ if and only if they have the same canonicalized representation. \n\\end{thm}\n\nWe next consider the issue of trivial/constant rewards $[\\vec{0}]$. Since the IRL problem was first formulated, it has been observed that no single experiment can ever determine that the agent's reward function is not a constant reward function. The algebraic reason for this is the fact that $\\vec{0}$ is always a solution to the linear system $K(E,\\pi)$, for any $E$ and $\\pi$. The intuitive reason for this is the fact that any $\\pi$ on some $E$ is as optimal as any other policy for an agent whose reward is $\\vec{0}$. Therefore, if we consider an agent whose true reward is some $R \\in \\mathbb{R}^d, R \\not= \\vec{0}$, then \\emph{even in the policy observation setting}, both $R, \\vec{0} \\in K(E, \\pi)$.  Furthermore, this will not disappear with multiple experimentation. After any sequence of experiments $\\mathcal{E}$, it also remains that both $R, \\vec{0} \\in K(\\mathcal{E})$. \n\nConsider an agent whose true reward function is $\\vec{0}$. A crucial consequence of the above is that if an IRL algorithm guarantees that it will identify $\\vec{0}$, then it necessarily misidentifies non-trivial reward functions. This is because an agent with a trivial reward function is allowed to behave arbitrarily, and therefore may choose to behave consistently with some non-trivial reward $R$. An IRL algorithm that guarantees identification of trivial rewards will therefore misidentify the agent whose true reward is $R$. \n\nThis leads us to the following revised definition of identification, which  accounts for what we call representational unidentifiability:\n\n\\begin{defn}\\label{defn:ident}\nWe say that an IRL algorithm succeeds at identification if for any $R \\in \\mathbb{R}^d$, after observing behavior from an agent with true reward $R$, the algorithm outputs a $\\hat{R}$ such that $\\hat{R} \\equiv R$ whenever $R \\not\\in [\\vec{0}]$.\n\\end{defn}\n\n\nNotice that this definition accomplishes two things. First, it excuses an algorithm for decisions about how $R$ is represented. In other words, it asserts that the salient task in IRL is computing a member of $[R]$, not the literal $R$. Secondly, if the true reward function $R$ is not constant (i.e. $R \\not\\in [\\vec{0}]$), it demands the that algorithm identify $R$ (up to representational decisions). However, if the agent really does have a reward function of $\\vec{0}$, the algorithm is allowed to output anything. In other words, the Algorithm is only allowed to behave arbitrarily if the agent behaves arbitrarily.\\footnote{We comment that, as a practical matter, one is usually interested in rationalizing the behavior of an agent believed to be non-trivial.}\n \nWe also note that Definition \\ref{defn:ident} can be relaxed to give a notion of approximate identification, which we state here:\n\n\\begin{defn}\nWe say that an IRL algorithm $\\epsilon$-identifies a reward function if for any $R \\in \\mathbb{R}^d$, after observing behavior from an agent with true reward $R$, the algorithm outputs a $\\hat{R}$ such that $||[R] - [\\hat{R}]||_{\\infty} \\leq \\epsilon$  whenever $R \\not\\in [\\vec{0}]$. \n\\end{defn}\n\n\nEven Definition \\ref{defn:ident} may not be attainable from a single experiment, as $K(E,\\pi)$ may contain multiple behavioral classes $[R]$. We call this phenonmenon \\emph{experimental unidentifiability}, due to the fact that the experiment $(E,\\pi)$ may simply be insufficient to distinguish between some $[R]$ and $[R']$. In the next section, we will observe that this source of uncertainty in the reward function can be decreased with multiple experimentation, as depicted in Figure \\ref{fig:cartoon} (see Caption for details). In other words, by distinguishing representational unidentifiability from experimental unidentifiability, we can formally resolve the latter.\n\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoona.png}\n  \\caption{}\n  \\end{subfigure}\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoonb.png}\n  \\caption{}\n  \\end{subfigure}\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoonc.png}\n  \\caption{}\n  \\end{subfigure}\n  \\caption{{\\small (a) After observing an agent's behavior in an environment, there is some set of rewards $K(E,\\pi)$ consistent with the observed behavior, depicted by the shaded region. Previous work has been concerned with designing selection rules that pick some point in this region, depicted here by the red circle. (b) No amount of experimentation can remove the representational unidentifiability from the setting, depicted here by the darker shaded region. (c) Nevertheless, adding the constraints $K(E',\\pi')$ induced by a second experiment disproves the original selection.}, removing some experimental unidentifiability.}\n  \\label{fig:cartoon}\n\\end{figure}\n\nA more concrete example is given in Figure \\ref{fig:mazes}, which depicts a grid-world with each square representing a state. In each of the figures, thick lines represent impenetrable walls, and an agent's policy is depicted by arrows, with a circle indicating the agent deciding to stay at a grid location. The goal of the learner is to infer the reward of each state. Figures \\ref{fig:mazes}(a) and \\ref{fig:mazes}(b), depict the same agent policy, which takes the shortest path to the location labeled $x$ from any starting location. One explanation for such behavior, depicted in Figure \\ref{fig:mazes}(a), is that the agent has large reward for state $x$, and zero reward for every other state. However, an equally possible explanation is that the state $y$ also gives positive reward (but smaller than that of $x$) such that if there exists a shortest path to $x$ that also passes through $y$, the agent will take it (depicted in Figure \\ref{fig:mazes}(b)). Without additional information, these two explanations cannot be distinguished. \n\nThis is an example of experimental unidentifiability that can nevertheless be resolved with additional experimentation. By observing the same agent in the environment depicted in Figure \\ref{fig:mazes}(c), the learner infers that $y$ is indeed a rewarding state. Finally, observing the agent's behavior in the environment of Figure \\ref{fig:mazes}(d) reveals that the agent will prefer traveling to state $y$ if getting to $x$ requires 11 steps or more, while getting to $y$ requires 4 steps of fewer. These subsequent observations allow the learner to relate the agent's reward at state $x$ with the agent's reward at state $y$. \n\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[b]{0.33\\textwidth}\n  \\includegraphics[width=\\textwidth]{./mazequad.png}\n  \\end{subfigure}\n  \\caption{{\\small (a) An agent's policy in a fixed environment. An agent can move in one of four directions or can stay at a location (represented by the black circle). Thick purple lines represent impassable walls. (d) An experiment revealing that if getting to $x$ requires 11 steps or more, and getting to $y$ requires 4 or fewer, the agent prefers $y$.}}\n  \\label{fig:mazes}\n\\end{figure}\n\n\n\n\n\n\n\\section{Omnipotent Experimenter\\\\ Setting}\\label{sec:omni}\n\n\nWe now consider a repeated experimentation setting in which the environments available for selection by the experimenter are completely unrestricted. \n\nFormally, \n\neach environment $E$ selected by the experimenter belongs to a class $\\mathcal{\\mathcal{U}}^*$ containing an environment $(S, A, P, \\gamma)$ for every feasible set of transition dynamics $P$ on $S$. We call this the {\\bf omnipotent experimenter setting}. \n\n\nWe will describe an algorithm for the omnipotent experimenter setting that $\\epsilon$-identifies $R$, using just $O(\\log(d/\\epsilon))$ experiments. While the omnipotent experimenter is extremely powerful, the result demonstrates \n\nthat the guarantee obtained in a repeated IRL setting can be far stronger than available in a standard single-environment IRL setting.\nFurthermore, it clarifies the distinction between experimental unidentifiability and representational unidentifiability.\n\n\n\\subsection{Omnipotent Identification Algorithm}\\label{sec:omnialg} The algorithm proceeds in two stages, both of which involve simple binary searches. The first stage will identify states $s_{\\min}, s_{\\max}$ such that $R(s_{\\min}) = R_{\\min}$ and $R(s_{\\max}) = R_{\\max}$. The second stage identifies for each $s \\in S$ an $\\alpha_s$ such that $R(s) = \\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max}$. Throughout, the algorithm only makes use of two agent actions which we will denote $a_1,a_2$. Therefore, in describing the algorithm, we will assume that $|A| = 2$, and the environment selected by the algorithm is fully determined by its choices for $P_{a_1}$ and $P_{a_2}$. If in fact $|A| > 2$, in the omnipotent experimenter setting, one can reduce to the two-action setting by making the remaining actions in $A$ equivalent to either $a_1$ or $a_2$.\\footnote{Doing so is possible in this setting because transition dynamics can be set arbitrarily.} \n\nWe first address the task of identifying $s_{\\max}$. Suppose we have two candidates $s$ and $s'$ for $s_{\\max}$. The key idea in this first stage of the algorithm is to give the agent an absolute choice between the two states by setting $P_{a_1}(s,s) = 1$, $P_{a_1}(s',s') = 1$, while setting $P_{a_2}(s,s') = 1$ and $P_{a_2}(s',s) = 1$. An agent selecting $\\pi(s) = a_1$ reveals (for any $\\gamma$) that $R(s) \\geq R(s')$, while an agent selecting $\\pi(s) = a_2$ reveals that $R(s) \\leq R(s')$. This test can be conducted for up to $d/2$ distinct pairs of states in a single experiment. Thus given $k$ candidates for $s_{\\max}$, in a single experiment, we can narrow the set of candidates to $k/2$, and are guaranteed that one of the remaining states $s$ satisfies $R(s) = R_{\\max}$. After $\\log(d)$ such experiments we can identify a single state $s_{\\max}$ which satisfies $R(s_{\\max}) \\geq R(s)$ for all $s$. Conducting an analogous procedure identifies a state $s_{\\min}$. \n\nOnce $s_{\\min}$ and $s_{\\max}$ are identified, take $s_{1},...,s_{d-2}$ to be the remaining states, and consider an environment with transition dynamics parameterized by $\\mathbf{\\alpha} = (\\alpha_{s_1},...,\\alpha_{s_{d-2}})$. A typical environment in this phase is depicted in Figure \\ref{fig:omni}. The environment sets $s_{\\min}, s_{\\max}$ to be sinks with $P_{a_1}(s_{\\min},s_{\\min}) = P_{a_1}(s_{\\max},s_{\\max}) = P_{a_2}(s_{\\min},s_{\\min}) = P_{a_2}(s_{\\max}, s_{\\max}) = 1$. For each remaining $s_i$, $P_{a_1}(s_i,s_{\\min}) = \\alpha_{s_i}$ and $P_{a_1}(s_i,s_{\\max}) = (1-\\alpha_{s_i})$, so that taking action $a_1$ in state $s_i$ represents an $\\alpha_i$ probability gamble between the best and worst state. Finally, $P_{\\mathbf{\\alpha}}$ also sets $P_{a_2}(s,s) = 1$, and so taking action $a_2$ in state $s_i$ represents receiving $R(s_i)$ for sure. By selecting $\\pi(s) = a_1$, the agent reveals $\\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max} \\geq R(s)$, while a choice $\\pi(s) = a_2$ reveals that $\\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max} \\leq R(s)$. Thus, a binary search can be conducted on each $\\alpha_s \\in [0,1]$ independently in order to determine an $\\epsilon$ approximation of the $\\alpha^*_s$ such that $R(s) = \\alpha_s^* R_{\\min} + (1-\\alpha^*_s)R_{\\max}$. \n\n\n\n\\begin{figure}\n        \\centering\n    \\begin{subfigure}[b]{0.2\\textwidth}\n      \\includegraphics[width=\\textwidth]{./omni_2.png}\n    \\end{subfigure}\n  \\caption{{\\small Typical environment in the second phase of the algorithm. The dotted lines represent transitions for action $a_1$, while the solid lines represent transitions for action $a_2$.}}\n\\vspace{-1em}\n  \\label{fig:omni}\n\\end{figure}\n\nThe algorithm succeeds at $\\epsilon$-identification, summarized in the following theorem. The proof of the theorem is a straightforward analysis of binary search.\n\n\\begin{thm}\\label{thm:simple}\nLet $\\hat{R}$ be defined by letting $\\hat{R}(s_{\\min}) = 0$, $\\hat{R}(s_{\\max}) = 1$, and $\\hat{R}(s) = 1 - \\alpha_s$ for all other $s$ (where $s_{\\min}$, $s_{\\max}$, and $\\alpha_s$ are identified as described above). For any true reward function $R \\not\\in [\\vec{0}]$ with canonical form $[R]$, $||[R] - \\hat{R}||_{\\infty} \\leq \\epsilon$. \n\\end{thm}\n\n\nThe takeaway of this setting is that the problems regarding identification in IRL can be circumvented with repeated experimentation. It is thought that even with policy observations, the IRL question is fundamentally ill-posed. However, here we see that with repeated experimentation it is in fact possible to identify $R$ to arbitrary precision in a well-defined sense. While these results are informative, we believe that it is unrealistic to imagine that the learner can arbitrarily influence the environment of the agent. In the next section, we develop a theory for repeated experimentation when the learner is restricted to select environments from some restricted subset of all possible transition dynamics. \n\n\\section{Restricted Experimenter Setting}\n\nWe now consider a setting in which the experimenter has a restricted universe $\\mathcal{U}$ of environments to choose from. $\\mathcal{U}$ need not contain every possible transition dynamic, an assumption required to execute the binary search algorithm of the previous section. \n\nThe best the experimenter could ever hope for is to try every environment in $\\mathcal{U}$. This gives the experimenter all the available information about the agent's reward function $R$. Thus, we will be more interested in maximizing the information gained by the experimenter while minimizing the number of experiments conducted. In practice, observing an agent may be expensive, or hard to come by, and so for even a small budget of experiments $B$, the learner would like select the environments from $\\mathcal{U}$ which maximally reduce experimental unidentifiability.\n\nOnce a sequence of experiments $\\mathcal{E}$ has been observed, we know that $R$ is consistent with the observed sequence if and only if $R \\in K(\\mathcal{E})$. Thus, the value of repeated experimentation is allowing the learner to select environments so that $K(\\mathcal{E})$ is as informative as possible. In contrast, we note that previous work on IRL has largely been focused on designing heuristics for the selection problem of picking some $R$ from a fixed set (of equally possible reward functions). Thus, we will be interested in making $K(\\mathcal{E})$ ``small,'' while IRL has traditionally been focused on selecting $R$ from exogenously fixed $K(\\mathcal{E})$. Before defining what we mean by ``small'', we will review preexisting methods for selecting $R \\in K(\\mathcal{E})$.\n\n\\subsection{Generalized Selection Heuristics}\n\\label{sec:heuristics}\n\nIn the standard (single-environment) setting, given an environment $E$ and observed policy $\\pi$, the learner must make a selection among one of the rewards in $K(E, \\pi)$. The heuristic suggested by {\\cite{{ng2000algorithms}}} is motivated by the idea that for a given state $s$, the reward function that maximizes the difference in Q-value between the observed action in state $s$, $\\pi(s)$, and any other action $a \\not= \\pi(s)$, gives the strongest explanation of the behavior observed from the agent. Thus, a reasonable linear selection criterion is to maximize the sum of these differences across states. Adding a regularization term, encourages the selection of reward functions that are also sparse. Putting these together, the standard selection heuristic for single-environment IRL is to select the $R$ which maximizes:\n\n\n", "itemtype": "equation", "pos": 16661, "prevtext": " \n\nWe can also think of a trajectory $T$ as inducing a \\emph{partial} policy $\\pi_T$ on the states visited by the trajectory. In particular, let $\\mathcal{D}(T)$ denote the domain of $T$, $\\mathcal{D}(T) = \\{s \\mid \\exists (s, a) \\in T\\}$. We say two policies $\\pi,\\pi'$ are consistent on $\\mathcal{D}(T) \\subset S$, denoted $\\pi \\equiv_{\\mathcal{D}(T)} \\pi'$, iff $\\pi(s) = \\pi'(s)$ for all $s \\in \\mathcal{D}(T)$. Thus, given $(E,T)$, the set of rewards consistent with the observation are precisely $K(E,T) = \\{R \\in \\mathbb{R}^d \\mid \\exists \\pi \\equiv_{\\mathcal{D}(T)} \\pi_T, \\forall a \\in A, (P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0, R_{\\min} \\leq R_i \\leq R_{\\max}\\}$, and given a sequence $\\mathcal{E} = \\{(E^1, T^1),...,(E^n,T^n)\\}$, we can define $K(\\mathcal{E})$ in the trajectory setting. \n\n\\section{On Identification}\\label{sec:identification} \n\n\nIn this section we will give a more nuanced characterization of what it means to identify a reward function. We will argue that there are multiple types of uncertainty involved in identifying $R$, which we categorize as \\emph{representational unidentifiability} and \\emph{experimental unidentifiability}. Furthermore, we argue that first type is in some ways superficial, and ought to be ignored, while the second type can be eliminated.\n\nWe begin with a definition. Let $R$ and $R'$ be reward functions defined on the same state space $\\mathcal{S}$. We say that $R$ and $R'$ are \\emph{behaviorally equivalent} if for \\emph{any} environment (also defined on $\\mathcal{S}$), the agent whose reward function is $R$ behaves identically to the agent whose reward function is $R'$. \n\n\\begin{defn}\\label{defn:equiv}\nTwo reward vectors $R,R' \\in \\mathbb{R}^d$ defined on $\\mathcal{S}$ are \\emph{behaviorally equivalent}, denoted $R \\equiv R'$ if for any set of actions, transition dynamics, and discount, $(\\mathcal{A},\\mathcal{P},\\gamma)$, defining an environment $E = (\\mathcal{S},\\mathcal{A},\\mathcal{P},\\gamma)$ we have that \\\\\n$\\mathrm{OPT}(E, R) = \\mathrm{OPT}(E,R')$.\n\\end{defn}\n\nBehavioral equivalence defines an equivalence relation over vectors in $\\mathbb{R}^d$, and we let $[R] = \\{R' \\in \\mathbb{R}^d \\mid R' \\equiv R\\}$ denote the equivalence classes defined in this manner. Intuitively, if $R$ and $R'$ are behaviorally equivalent, they induce identical optimal policies in every single environment, and therefore are not really ``different'' reward functions. They are simply different representations of the same incentives. \n\nWe now observe that behavioral equivalence classes are invariant under multiplicative scaling by positive scalars, and component-wise translation by a constant. Intuitively, this is easy to see. Adding $c$ reward to every state in some reward function $R$ does not affect an agent's decision-making. This is simply ``background'' reward that the agent gets for free. Similarly, scaling $R$ by a positive constant simply changes the ``units\" used to represent rewards.  The agent does not, and should not, care whether its reward is represented in dollars or cents. We prove this formally in the following Theorem.\n\n\\begin{thm}\\label{thm:equiv}\nFor any $c \\in \\mathbb{R}^d$, let $\\vec{c} \\in \\mathbb{R}^d$ denote the vector with all components equal to $c$. For any $\\alpha > 0$, and $R \\in \\mathbb{R}^d$, $R \\equiv \\alpha R + \\vec{c}$. \n\\end{thm}\n\\begin{proof}\nFirst consider $\\vec{c}$ as defined in the statement of the Theorem. Fix any environment $E = (\\mathcal{S},\\mathcal{A}, \\mathcal{P}, \\gamma)$, action $a \\in \\mathcal{A}$ and arbitrary policy $\\pi$. We begin by claiming that $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}\\vec{c} = \\vec{0}$. \n\nThe Woodbury formula for matrix inversion tells us that $(I - \\gamma P_{\\pi})^{-1} = I + (I - \\gamma P_{\\pi})^{-1} \\gamma P_{\\pi}$. Furthermore, for any row-stochastic matrix $P$, $P\\vec{c} = \\vec{c}$. Therefore:\n\n\n", "index": 5, "text": "\\begin{align*}\nv &= (P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}\\vec{c}\\\\ &= (P_\\pi - P_a)(I + (I - \\gamma P_{\\pi})^{-1}\\gamma P_{\\pi})\\vec{c}\\\\\n&= (P_\\pi - P_a)\\vec{c} + (P_{\\pi} - P_a)(I - \\gamma P_{\\pi})^{-1}\\gamma P_{\\pi}\\vec{c}\\\\\n&= \\vec{0} + (P_{\\pi} - P_a)(I - \\gamma P_{\\pi})^{-1}\\gamma \\vec{c} = \\gamma v\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle v\" display=\"inline\"><mi>v</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(P_{\\pi}-P_{a})(I-\\gamma P_{\\pi})^{-1}\\vec{c}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(P_{\\pi}-P_{a})(I+(I-\\gamma P_{\\pi})^{-1}\\gamma P_{\\pi})\\vec{c}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(P_{\\pi}-P_{a})\\vec{c}+(P_{\\pi}-P_{a})(I-\\gamma P_{\\pi})^{-1}%&#10;\\gamma P_{\\pi}\\vec{c}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\vec{0}+(P_{\\pi}-P_{a})(I-\\gamma P_{\\pi})^{-1}\\gamma\\vec{c}=\\gamma&#10;v\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mn>0</mn><mo stretchy=\"false\">\u2192</mo></mover><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi>\u03c0</mi></msub><mo>-</mo><msub><mi>P</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>P</mi><mi>\u03c0</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow><mo>=</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nThere are two natural candidates for generalizing this selection rule to the repeated experimentation setting, where now instead of a single experiment, the experimenter has encountered a sequence of observations $\\mathcal{E}$. The first is to \\emph{sum over all (environment, state), pairs}, the minimum difference in Q-value between the action selected by the agent and any other action. The second is to sum over states, taking the \\emph{minimum over all (environment, action), pairs}. While one could make arguments motivating each of these, ultimately any such objective is heuristic. However, we do argue that there is a strong algorithmic reason for preferring the latter objective. In particular, the former objective grows in dimensionality as environments are added, quickly resulting in an intractable LP. The dimension of the objective in the latter (Equation \\ref{eq:rule}), however, remains constant.\\footnote{Writing Equation \\ref{eq:rule} as an LP in standard form requires translating the $\\min$ into constraints, and thus the number of constraints grows with the number of experiments, but as we demonstrate in our experimental results, this is tractable for most LP solvers.}\n\n{\\small \n", "itemtype": "equation", "pos": 34932, "prevtext": "\n\nSince $\\gamma \\in (0,1)$, it must be that $v = \\vec{0}$. \n\nNow fix a reward function $R \\in \\mathbb{R}^d$, and arbitrary environment $E$, and consider $\\mathrm{OPT}(E,R)$. By Theorem \\ref{thm:ngrussel}, we know that $\\pi \\in \\mathrm{OPT}(E,R)$ iff for any $a \\in A$, $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}R \\geq 0$, which occurs iff  $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}( \\alpha R) \\geq 0$, since $\\alpha$ is a positive scalar. Finally, we can conclude that $\\pi \\in \\mathrm{OPT}(E,R)$ iff for all $a \\in \\mathcal{A}$, $(P_\\pi - P_a)(I - \\gamma P_{\\pi})^{-1}( \\alpha R + \\vec{c}) \\geq 0$, this last condition implying that $\\pi \\in \\mathrm{OPT}(E,\\alpha R + \\vec{c})$, again by Theorem \\ref{thm:ngrussel}. \n\nSince our choice of $E$ was arbitrary, by Definition \\ref{defn:equiv}, $R \\equiv \\alpha R + \\vec{c}$, concluding the proof.\n\\end{proof}\n\nThus, we argue that one reason why reward functions cannot be identified is a trivial one: the classic IRL problem does not fix a consistent representation for reward functions.  For any $R \\in \\mathbb{R}^d$ there are an uncountable number of other functions in $[R]$, namely $\\alpha R + \\vec{c}$ for any $\\alpha$ and $\\vec{c}$, all of which are behaviorally identical to $R$. However, distinguishing between these functions is irrelevant; whether an agent's ``true'' reward function is $(1,2,3,4)$ or $(0,1/3,2/3,1)$\\footnote{We get $(0,1/3,2/3,1)$ from $(1,2,3,4)$ by subtracting $1$ from every state and dividing by $3$} is simply a matter of what units are used to represent rewards. \n\nIn light of this observation, it is convenient to fix a \\emph{canonical element} of each equivalence class $[R]$. For any constant reward function $R$, we will take its canonicalized representation to be $\\vec{0}$. Otherwise we note, by way of Theorem \\ref{thm:equiv}, that any $R$ can be translated and re-scaled so that $\\max_s R(s) = 1$ and $\\min_s R(s) = 0$. More carefully, for any non-constant $R$, we take its canonicalized representation to be $(R - \\min_s R(s))/(\\max_s R(s) - \\min_s R(s)) \\in [R]$. This canonicalization is consistent with behavioral equivalence, and we state the following Theorem whose proof can be found in the appendix. As a consequence of this Theorem, we can use the notation $[R]$ interchangeably to refer to the equivalence class of $R$, or the the unique canonical element of $[R]$.\n\n\\begin{thm}\nFor any $R,R' \\in \\mathbb{R}^d$, $R \\equiv R'$ if and only if they have the same canonicalized representation. \n\\end{thm}\n\nWe next consider the issue of trivial/constant rewards $[\\vec{0}]$. Since the IRL problem was first formulated, it has been observed that no single experiment can ever determine that the agent's reward function is not a constant reward function. The algebraic reason for this is the fact that $\\vec{0}$ is always a solution to the linear system $K(E,\\pi)$, for any $E$ and $\\pi$. The intuitive reason for this is the fact that any $\\pi$ on some $E$ is as optimal as any other policy for an agent whose reward is $\\vec{0}$. Therefore, if we consider an agent whose true reward is some $R \\in \\mathbb{R}^d, R \\not= \\vec{0}$, then \\emph{even in the policy observation setting}, both $R, \\vec{0} \\in K(E, \\pi)$.  Furthermore, this will not disappear with multiple experimentation. After any sequence of experiments $\\mathcal{E}$, it also remains that both $R, \\vec{0} \\in K(\\mathcal{E})$. \n\nConsider an agent whose true reward function is $\\vec{0}$. A crucial consequence of the above is that if an IRL algorithm guarantees that it will identify $\\vec{0}$, then it necessarily misidentifies non-trivial reward functions. This is because an agent with a trivial reward function is allowed to behave arbitrarily, and therefore may choose to behave consistently with some non-trivial reward $R$. An IRL algorithm that guarantees identification of trivial rewards will therefore misidentify the agent whose true reward is $R$. \n\nThis leads us to the following revised definition of identification, which  accounts for what we call representational unidentifiability:\n\n\\begin{defn}\\label{defn:ident}\nWe say that an IRL algorithm succeeds at identification if for any $R \\in \\mathbb{R}^d$, after observing behavior from an agent with true reward $R$, the algorithm outputs a $\\hat{R}$ such that $\\hat{R} \\equiv R$ whenever $R \\not\\in [\\vec{0}]$.\n\\end{defn}\n\n\nNotice that this definition accomplishes two things. First, it excuses an algorithm for decisions about how $R$ is represented. In other words, it asserts that the salient task in IRL is computing a member of $[R]$, not the literal $R$. Secondly, if the true reward function $R$ is not constant (i.e. $R \\not\\in [\\vec{0}]$), it demands the that algorithm identify $R$ (up to representational decisions). However, if the agent really does have a reward function of $\\vec{0}$, the algorithm is allowed to output anything. In other words, the Algorithm is only allowed to behave arbitrarily if the agent behaves arbitrarily.\\footnote{We comment that, as a practical matter, one is usually interested in rationalizing the behavior of an agent believed to be non-trivial.}\n \nWe also note that Definition \\ref{defn:ident} can be relaxed to give a notion of approximate identification, which we state here:\n\n\\begin{defn}\nWe say that an IRL algorithm $\\epsilon$-identifies a reward function if for any $R \\in \\mathbb{R}^d$, after observing behavior from an agent with true reward $R$, the algorithm outputs a $\\hat{R}$ such that $||[R] - [\\hat{R}]||_{\\infty} \\leq \\epsilon$  whenever $R \\not\\in [\\vec{0}]$. \n\\end{defn}\n\n\nEven Definition \\ref{defn:ident} may not be attainable from a single experiment, as $K(E,\\pi)$ may contain multiple behavioral classes $[R]$. We call this phenonmenon \\emph{experimental unidentifiability}, due to the fact that the experiment $(E,\\pi)$ may simply be insufficient to distinguish between some $[R]$ and $[R']$. In the next section, we will observe that this source of uncertainty in the reward function can be decreased with multiple experimentation, as depicted in Figure \\ref{fig:cartoon} (see Caption for details). In other words, by distinguishing representational unidentifiability from experimental unidentifiability, we can formally resolve the latter.\n\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoona.png}\n  \\caption{}\n  \\end{subfigure}\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoonb.png}\n  \\caption{}\n  \\end{subfigure}\n    \\begin{subfigure}[b]{0.12\\textwidth}\n  \\includegraphics[width=\\textwidth]{./cartoonc.png}\n  \\caption{}\n  \\end{subfigure}\n  \\caption{{\\small (a) After observing an agent's behavior in an environment, there is some set of rewards $K(E,\\pi)$ consistent with the observed behavior, depicted by the shaded region. Previous work has been concerned with designing selection rules that pick some point in this region, depicted here by the red circle. (b) No amount of experimentation can remove the representational unidentifiability from the setting, depicted here by the darker shaded region. (c) Nevertheless, adding the constraints $K(E',\\pi')$ induced by a second experiment disproves the original selection.}, removing some experimental unidentifiability.}\n  \\label{fig:cartoon}\n\\end{figure}\n\nA more concrete example is given in Figure \\ref{fig:mazes}, which depicts a grid-world with each square representing a state. In each of the figures, thick lines represent impenetrable walls, and an agent's policy is depicted by arrows, with a circle indicating the agent deciding to stay at a grid location. The goal of the learner is to infer the reward of each state. Figures \\ref{fig:mazes}(a) and \\ref{fig:mazes}(b), depict the same agent policy, which takes the shortest path to the location labeled $x$ from any starting location. One explanation for such behavior, depicted in Figure \\ref{fig:mazes}(a), is that the agent has large reward for state $x$, and zero reward for every other state. However, an equally possible explanation is that the state $y$ also gives positive reward (but smaller than that of $x$) such that if there exists a shortest path to $x$ that also passes through $y$, the agent will take it (depicted in Figure \\ref{fig:mazes}(b)). Without additional information, these two explanations cannot be distinguished. \n\nThis is an example of experimental unidentifiability that can nevertheless be resolved with additional experimentation. By observing the same agent in the environment depicted in Figure \\ref{fig:mazes}(c), the learner infers that $y$ is indeed a rewarding state. Finally, observing the agent's behavior in the environment of Figure \\ref{fig:mazes}(d) reveals that the agent will prefer traveling to state $y$ if getting to $x$ requires 11 steps or more, while getting to $y$ requires 4 steps of fewer. These subsequent observations allow the learner to relate the agent's reward at state $x$ with the agent's reward at state $y$. \n\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[b]{0.33\\textwidth}\n  \\includegraphics[width=\\textwidth]{./mazequad.png}\n  \\end{subfigure}\n  \\caption{{\\small (a) An agent's policy in a fixed environment. An agent can move in one of four directions or can stay at a location (represented by the black circle). Thick purple lines represent impassable walls. (d) An experiment revealing that if getting to $x$ requires 11 steps or more, and getting to $y$ requires 4 or fewer, the agent prefers $y$.}}\n  \\label{fig:mazes}\n\\end{figure}\n\n\n\n\n\n\n\\section{Omnipotent Experimenter\\\\ Setting}\\label{sec:omni}\n\n\nWe now consider a repeated experimentation setting in which the environments available for selection by the experimenter are completely unrestricted. \n\nFormally, \n\neach environment $E$ selected by the experimenter belongs to a class $\\mathcal{\\mathcal{U}}^*$ containing an environment $(S, A, P, \\gamma)$ for every feasible set of transition dynamics $P$ on $S$. We call this the {\\bf omnipotent experimenter setting}. \n\n\nWe will describe an algorithm for the omnipotent experimenter setting that $\\epsilon$-identifies $R$, using just $O(\\log(d/\\epsilon))$ experiments. While the omnipotent experimenter is extremely powerful, the result demonstrates \n\nthat the guarantee obtained in a repeated IRL setting can be far stronger than available in a standard single-environment IRL setting.\nFurthermore, it clarifies the distinction between experimental unidentifiability and representational unidentifiability.\n\n\n\\subsection{Omnipotent Identification Algorithm}\\label{sec:omnialg} The algorithm proceeds in two stages, both of which involve simple binary searches. The first stage will identify states $s_{\\min}, s_{\\max}$ such that $R(s_{\\min}) = R_{\\min}$ and $R(s_{\\max}) = R_{\\max}$. The second stage identifies for each $s \\in S$ an $\\alpha_s$ such that $R(s) = \\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max}$. Throughout, the algorithm only makes use of two agent actions which we will denote $a_1,a_2$. Therefore, in describing the algorithm, we will assume that $|A| = 2$, and the environment selected by the algorithm is fully determined by its choices for $P_{a_1}$ and $P_{a_2}$. If in fact $|A| > 2$, in the omnipotent experimenter setting, one can reduce to the two-action setting by making the remaining actions in $A$ equivalent to either $a_1$ or $a_2$.\\footnote{Doing so is possible in this setting because transition dynamics can be set arbitrarily.} \n\nWe first address the task of identifying $s_{\\max}$. Suppose we have two candidates $s$ and $s'$ for $s_{\\max}$. The key idea in this first stage of the algorithm is to give the agent an absolute choice between the two states by setting $P_{a_1}(s,s) = 1$, $P_{a_1}(s',s') = 1$, while setting $P_{a_2}(s,s') = 1$ and $P_{a_2}(s',s) = 1$. An agent selecting $\\pi(s) = a_1$ reveals (for any $\\gamma$) that $R(s) \\geq R(s')$, while an agent selecting $\\pi(s) = a_2$ reveals that $R(s) \\leq R(s')$. This test can be conducted for up to $d/2$ distinct pairs of states in a single experiment. Thus given $k$ candidates for $s_{\\max}$, in a single experiment, we can narrow the set of candidates to $k/2$, and are guaranteed that one of the remaining states $s$ satisfies $R(s) = R_{\\max}$. After $\\log(d)$ such experiments we can identify a single state $s_{\\max}$ which satisfies $R(s_{\\max}) \\geq R(s)$ for all $s$. Conducting an analogous procedure identifies a state $s_{\\min}$. \n\nOnce $s_{\\min}$ and $s_{\\max}$ are identified, take $s_{1},...,s_{d-2}$ to be the remaining states, and consider an environment with transition dynamics parameterized by $\\mathbf{\\alpha} = (\\alpha_{s_1},...,\\alpha_{s_{d-2}})$. A typical environment in this phase is depicted in Figure \\ref{fig:omni}. The environment sets $s_{\\min}, s_{\\max}$ to be sinks with $P_{a_1}(s_{\\min},s_{\\min}) = P_{a_1}(s_{\\max},s_{\\max}) = P_{a_2}(s_{\\min},s_{\\min}) = P_{a_2}(s_{\\max}, s_{\\max}) = 1$. For each remaining $s_i$, $P_{a_1}(s_i,s_{\\min}) = \\alpha_{s_i}$ and $P_{a_1}(s_i,s_{\\max}) = (1-\\alpha_{s_i})$, so that taking action $a_1$ in state $s_i$ represents an $\\alpha_i$ probability gamble between the best and worst state. Finally, $P_{\\mathbf{\\alpha}}$ also sets $P_{a_2}(s,s) = 1$, and so taking action $a_2$ in state $s_i$ represents receiving $R(s_i)$ for sure. By selecting $\\pi(s) = a_1$, the agent reveals $\\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max} \\geq R(s)$, while a choice $\\pi(s) = a_2$ reveals that $\\alpha_s R_{\\min} + (1-\\alpha_s) R_{\\max} \\leq R(s)$. Thus, a binary search can be conducted on each $\\alpha_s \\in [0,1]$ independently in order to determine an $\\epsilon$ approximation of the $\\alpha^*_s$ such that $R(s) = \\alpha_s^* R_{\\min} + (1-\\alpha^*_s)R_{\\max}$. \n\n\n\n\\begin{figure}\n        \\centering\n    \\begin{subfigure}[b]{0.2\\textwidth}\n      \\includegraphics[width=\\textwidth]{./omni_2.png}\n    \\end{subfigure}\n  \\caption{{\\small Typical environment in the second phase of the algorithm. The dotted lines represent transitions for action $a_1$, while the solid lines represent transitions for action $a_2$.}}\n\\vspace{-1em}\n  \\label{fig:omni}\n\\end{figure}\n\nThe algorithm succeeds at $\\epsilon$-identification, summarized in the following theorem. The proof of the theorem is a straightforward analysis of binary search.\n\n\\begin{thm}\\label{thm:simple}\nLet $\\hat{R}$ be defined by letting $\\hat{R}(s_{\\min}) = 0$, $\\hat{R}(s_{\\max}) = 1$, and $\\hat{R}(s) = 1 - \\alpha_s$ for all other $s$ (where $s_{\\min}$, $s_{\\max}$, and $\\alpha_s$ are identified as described above). For any true reward function $R \\not\\in [\\vec{0}]$ with canonical form $[R]$, $||[R] - \\hat{R}||_{\\infty} \\leq \\epsilon$. \n\\end{thm}\n\n\nThe takeaway of this setting is that the problems regarding identification in IRL can be circumvented with repeated experimentation. It is thought that even with policy observations, the IRL question is fundamentally ill-posed. However, here we see that with repeated experimentation it is in fact possible to identify $R$ to arbitrary precision in a well-defined sense. While these results are informative, we believe that it is unrealistic to imagine that the learner can arbitrarily influence the environment of the agent. In the next section, we develop a theory for repeated experimentation when the learner is restricted to select environments from some restricted subset of all possible transition dynamics. \n\n\\section{Restricted Experimenter Setting}\n\nWe now consider a setting in which the experimenter has a restricted universe $\\mathcal{U}$ of environments to choose from. $\\mathcal{U}$ need not contain every possible transition dynamic, an assumption required to execute the binary search algorithm of the previous section. \n\nThe best the experimenter could ever hope for is to try every environment in $\\mathcal{U}$. This gives the experimenter all the available information about the agent's reward function $R$. Thus, we will be more interested in maximizing the information gained by the experimenter while minimizing the number of experiments conducted. In practice, observing an agent may be expensive, or hard to come by, and so for even a small budget of experiments $B$, the learner would like select the environments from $\\mathcal{U}$ which maximally reduce experimental unidentifiability.\n\nOnce a sequence of experiments $\\mathcal{E}$ has been observed, we know that $R$ is consistent with the observed sequence if and only if $R \\in K(\\mathcal{E})$. Thus, the value of repeated experimentation is allowing the learner to select environments so that $K(\\mathcal{E})$ is as informative as possible. In contrast, we note that previous work on IRL has largely been focused on designing heuristics for the selection problem of picking some $R$ from a fixed set (of equally possible reward functions). Thus, we will be interested in making $K(\\mathcal{E})$ ``small,'' while IRL has traditionally been focused on selecting $R$ from exogenously fixed $K(\\mathcal{E})$. Before defining what we mean by ``small'', we will review preexisting methods for selecting $R \\in K(\\mathcal{E})$.\n\n\\subsection{Generalized Selection Heuristics}\n\\label{sec:heuristics}\n\nIn the standard (single-environment) setting, given an environment $E$ and observed policy $\\pi$, the learner must make a selection among one of the rewards in $K(E, \\pi)$. The heuristic suggested by {\\cite{{ng2000algorithms}}} is motivated by the idea that for a given state $s$, the reward function that maximizes the difference in Q-value between the observed action in state $s$, $\\pi(s)$, and any other action $a \\not= \\pi(s)$, gives the strongest explanation of the behavior observed from the agent. Thus, a reasonable linear selection criterion is to maximize the sum of these differences across states. Adding a regularization term, encourages the selection of reward functions that are also sparse. Putting these together, the standard selection heuristic for single-environment IRL is to select the $R$ which maximizes:\n\n\n", "index": 7, "text": "\\begin{equation}\\label{eqn:classic}\n{\\small \\sum_{s \\in S} \\left( \\min_{a \\not= \\pi(s)} (P_\\pi(s) - P_a(s))(I - \\gamma P_{\\pi})^{-1}R \\right) - \\lambda |R(s)|}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\small\\sum_{s\\in S}\\left(\\min_{a\\not=\\pi(s)}(P_{\\pi}(s)-P_{a}(s))(I-\\gamma P_%&#10;{\\pi})^{-1}R\\right)-\\lambda|R(s)|}\" display=\"block\"><mrow><mrow><munder><mo largeop=\"true\" mathsize=\"90%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"90%\">s</mi><mo mathsize=\"90%\" stretchy=\"false\">\u2208</mo><mi mathsize=\"90%\">S</mi></mrow></munder><mrow><mo>(</mo><mrow><mrow><munder><mi mathsize=\"90%\">min</mi><mrow><mi mathsize=\"90%\">a</mi><mo mathsize=\"90%\" stretchy=\"false\">\u2260</mo><mrow><mi mathsize=\"90%\">\u03c0</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mi mathsize=\"90%\">s</mi><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mrow><mrow><msub><mi mathsize=\"90%\">P</mi><mi mathsize=\"90%\">\u03c0</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mi mathsize=\"90%\">s</mi><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><msub><mi mathsize=\"90%\">P</mi><mi mathsize=\"90%\">a</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mi mathsize=\"90%\">s</mi><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow></mrow><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow><mo>\u2062</mo><msup><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mrow><mi mathsize=\"90%\">I</mi><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"90%\">\u03b3</mi><mo>\u2062</mo><msub><mi mathsize=\"90%\">P</mi><mi mathsize=\"90%\">\u03c0</mi></msub></mrow></mrow><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow><mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mn mathsize=\"90%\">1</mn></mrow></msup><mo>\u2062</mo><mi mathsize=\"90%\">R</mi></mrow><mo>)</mo></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"90%\">\u03bb</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">|</mo><mrow><mi mathsize=\"90%\">R</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mi mathsize=\"90%\">s</mi><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow><mo maxsize=\"90%\" minsize=\"90%\">|</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "}\n\nThere are other selection rules for the single-environment setting, which are generalizable to the repeated experimentation setting, including heuristics for the infinite state setting, trajectory heuristics, as well as approaches already adapted to multiple environments \\cite{ratliff2006maximum}. Due to space constraints, we discuss only the foundational approach of {\\cite{{ng2000algorithms}}}. Our goal here is simply to emphasize the dichotomy between adapting pre-existing IRL methods to data gathered from multiple environments (however that data was generated), and the problem of how to best select those environments to begin with, this latter problem being the focus of the next section. \n\n\n\\begin{figure*}[t]\n  \\centering\n    \\begin{subfigure}[b]{.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{./results1b.png}\n      \\caption{Policy Observations}\n      \\label{fig:lambda0}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{./trajectories_bwfriendly.png}\n      \\caption{Trajectory Observations}\n      \\label{fig:traj}\n    \\end{subfigure}\n\n\n  \\caption{ Plot (a) displays $||\\hat{R}||_{\\infty}$ error for predicted vector $\\hat{R}$ in the policy observation setting, with bars indicating standard error. Plot (b) displays the same in the trajectory setting. }\n  \\label{fig:results2}\n\\end{figure*}\n\n\n\\subsection{Adaptive Experimentation}\n\\label{sec:greedy}\nGiven a universe $\\mathcal{U}$ of candidate environments, we now ask how to select a small number of environments from $\\mathcal{U}$ so that the environments are maximally informative. We must first decide what we mean by ``informative.'' We propose that for a set of experiments $\\mathcal{E}$ (either in the policy or trajectory setting), a natural objective is to minimize the mass of the resulting space of possible rewards $K(\\mathcal{E})$ with respect to some measure (or distribution) $\\mu$. Under the Lebesgue measure (or uniform distribution), this corresponds to the natural goal of reducing the volume of the $K(\\mathcal{E})$ as much as possible. Thus we define: \n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nThere are two natural candidates for generalizing this selection rule to the repeated experimentation setting, where now instead of a single experiment, the experimenter has encountered a sequence of observations $\\mathcal{E}$. The first is to \\emph{sum over all (environment, state), pairs}, the minimum difference in Q-value between the action selected by the agent and any other action. The second is to sum over states, taking the \\emph{minimum over all (environment, action), pairs}. While one could make arguments motivating each of these, ultimately any such objective is heuristic. However, we do argue that there is a strong algorithmic reason for preferring the latter objective. In particular, the former objective grows in dimensionality as environments are added, quickly resulting in an intractable LP. The dimension of the objective in the latter (Equation \\ref{eq:rule}), however, remains constant.\\footnote{Writing Equation \\ref{eq:rule} as an LP in standard form requires translating the $\\min$ into constraints, and thus the number of constraints grows with the number of experiments, but as we demonstrate in our experimental results, this is tractable for most LP solvers.}\n\n{\\small \n", "index": 9, "text": "\\begin{equation}\\label{eq:rule}\n\\underset{R \\in K(\\mathcal{E})}{\\mathrm{maximize}} \\sum_{s \\in S} \\left( \\min_{\\underset{a \\not= \\pi^i(s)}{ (E^i, \\pi^i) \\in \\mathcal{E}}} (P_{\\pi}^i(s) - P_{a}^i(s))(I - \\gamma P_{\\pi}^i)^{-1}R \\right) - \\lambda |R(s)|\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\underset{R\\in K(\\mathcal{E})}{\\mathrm{maximize}}\\sum_{s\\in S}\\left(\\min_{%&#10;\\underset{a\\not=\\pi^{i}(s)}{(E^{i},\\pi^{i})\\in\\mathcal{E}}}(P_{\\pi}^{i}(s)-P_{%&#10;a}^{i}(s))(I-\\gamma P_{\\pi}^{i})^{-1}R\\right)-\\lambda|R(s)|\" display=\"block\"><mrow><mrow><munder accentunder=\"true\"><mi>maximize</mi><mrow><mi>R</mi><mo>\u2208</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mo>(</mo><mrow><mrow><munder><mi>min</mi><munder accentunder=\"true\"><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mi>E</mi><mi>i</mi></msup><mo>,</mo><msup><mi>\u03c0</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow><mrow><mi>a</mi><mo>\u2260</mo><mrow><msup><mi>\u03c0</mi><mi>i</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>P</mi><mi>\u03c0</mi><mi>i</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msubsup><mi>P</mi><mi>a</mi><mi>i</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msubsup><mi>P</mi><mi>\u03c0</mi><mi>i</mi></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>R</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nWe will find it convenient to cast this as a maximization problem, and therefore also define $f(\\mathcal{E}) = V - \\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))$, where $V$ is an upper bound on the volume of $[-R_{\\max}, R_{\\max}]^d$, and our goal to maximize $f(\\mathcal{E})$. \n\nThis objective has several desirable properties. First and foremost, by reducing the volume of $K(\\mathcal{E})$ we eliminate the space of possible reward functions (i.e. experimental unidentifiability). Secondly, the repeated experimentation setting is fundamentally an active learning setting. We can think of the true, unknown, $R$ as a function that labels environments $E$ with either a corresponding policy $\\pi$ or trajectory $T$. Thus, the volume operator corresponds to reducing the \\emph{version space} of possible rewards. Furthermore, as we will see later in this section, the objective is a monotone submodular function, an assumption well-studied in the active learning literature \\cite{guillory2010interactive,golovin2010adaptive}, allowing us to prove guarantees for a greedy algorithm. \n\n\n\n\nFinally, we will normally think of $\\mu$ as being the Lebesgue measure, and $\\mathrm{Vol}(\\cdot)$ as volume in $d$-dimensional Euclidean space (or the uniform distribution on $[-R_{\\max}, R_{min}]^d$). However, the choice of $\\mu$ makes the objective quite general. For example, by making $\\mu$ uniform on an $\\epsilon$-net on $\\mathbb{R}^d$, $\\mathrm{Vol}$ corresponds to counting the number of rewards that are $\\epsilon$-apart with respect to some metric. In many settings, $R$ naturally comes from some discrete space, such as the corners of the hypercube $\\{0,1\\}^d$. Again, this is readily modeled by the correct choice of $\\mu$. In fact, $\\mu$ can be thought of simply as any prior on $[-R_{\\max}, R_{\\max}]^d$. \n\nWe are now ready to describe a simple algorithm that adaptively selects environments $E \\in \\mathcal{U}$, attempting to greedily maximize $f(\\cdot)$, depicted as Algorithm \\ref{alg:greedy}.\n\n\\begin{algorithm}[h]\n\\begin{algorithmic}[1]\n\\small \\State {\\bf Input} $B$\n\\State $\\mathrm{i} := 1$\n\\State $\\mathcal{E} := \\emptyset$\n\\While {$i \\leq B$}\n\\State $E_i := \\underset{E}{\\arg\\max} \\underset{R \\in K(\\mathcal{E})}{\\min} \\underset{\\pi \\in \\mathrm{OPT}(E, R)}{\\min} \\small{f(\\mathcal{E} \\cup (E, \\pi)) - f(\\mathcal{E})}$ \\label{line:max}\n\\State Observe policy $\\pi^i$ for $E^i$. \n\\State $\\mathcal{E} := (\\mathcal{E}, (E^i, \\pi^i))$\n\\State $i := i + 1$\n\\EndWhile\n\\State \\Return $\\mathcal{E}$\n\\end{algorithmic}\n\\caption{Greedy Environment Selection}\n\\label{alg:greedy}\n\\end{algorithm}\n\nIn order to state a performance guarantee about Algorithm \\ref{alg:greedy}, we will use the fact that $f(\\cdot)$ is a submodular, non-decreasing, function on subsets of environment, observation pairs, $2^{\\mathcal{\\mathcal{U}} \\times O}$, where $O$ is the set of possible observations.\n\n\\begin{lem}\\label{lem:submod}\n$f$ is a submodular, non-decreasing function. \n\\end{lem}\n\\begin{proof}\nGiven a set $\\mathcal{S}$ and component $s$, we use $\\mathcal{S} + s$ to denote the union of the singleton set $\\{s\\}$ with $\\mathcal{S}$. Let $O$ be the set of possible observations, so that $o$ is a trajectory in the trajectory setting, and a policy in the policy setting. Let $\\mathcal{U}$ be the space of possible environments. \n\nFix any $\\hat{\\mathcal{E}} \\subset \\mathcal{E} \\subset 2^{\\mathcal{U} \\times O}$, and $(E,o) \\not\\in \\mathcal{E}$. By definition of $K(\\cdot)$, we have that $K(\\mathcal{E} + (E, o)) = K(\\mathcal{E}) \\cap K(E, o)$ and $K(\\mathcal{E}) \\subset K(\\hat{\\mathcal{E}})$, and so: $f((\\mathcal{E}, (E,o))) - f(\\mathcal{E}) = \\mathrm{Vol}(K(\\mathcal{E})) - \\mathrm{Vol}(K(\\mathcal{E}, (E,o))) = \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\mathcal{E}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R)$\\\\ $\\leq \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\hat{\\mathcal{E}}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R) = f((\\hat{\\mathcal{E}}, (E,o))) - f(\\hat{\\mathcal{E}})$\nThis establishes submodularity of $f$. Since $\\mathcal{E}$ is arbitrary and the right-hand-side of the second equality is non-zero, $f$ is also monotone.\n\\end{proof}\n\nThe performance of any algorithm is a function of how many experiments are attempted, and thus our analysis must take this into account. Let $\\mathcal{A}_n$ be a deterministic algorithm that deploys at most $n$ experiments. $\\mathcal{A}_n$ has a worst-case performance, which depends on the true reward $R$ and what policies were observed. We say a sequence of experiments $\\mathcal{E} = ((E^1, o^1),...,(E^n, o^n))$ is consistent with $\\mathcal{A}_n$ and $R$, if $\\mathcal{A}_n$ chooses environment $E^{j+1}$ after observing the subsequence of experiments $((E^1, o^1),...,(E^j, o^j))$, and $o^j$ is either a trajectory or policy consistent with $(E,R)$. Denoting the set of consistent experiments $\\mathcal{C}(\\mathcal{A}_n, R)$, the best performance that any algorithm can \\emph{guarantee} with $n$ experiments is: $\\mathrm{OPT}_n = \\max_{\\mathcal{A}_n} \\min_{R} \\min_{\\mathcal{E} \\in \\mathcal{C}(\\mathcal{A}_n, R)} f(\\mathcal{E})$\n\n\n\nThe submodularity of $f$, allows us to prove that for any $n$, the Greedy Environment Selection Algorithm\\footnote{n.b. in the trajectory setting, one would replace the minimization over $\\pi \\in \\mathrm{OPT}(E,R)$ in line \\ref{line:max} of the algorithm, with a minimization over $T$ consistent with $\\pi, \\pi \\in \\mathrm{OPT}(E,R)$.}  needs slightly more than $n$ experiments (by a logarithmic factor) to attain $f(\\mathcal{E}) \\approx \\mathrm{OPT}_n$.  \n\n\n\\begin{thm}\\label{thm:greedy}\n$\\mathcal{E}$ returned by the Greedy Environment Selection algorithm satisfies $f(\\mathcal{E}) \\geq \\mathrm{OPT}_n - \\epsilon$ when $B = n\\ln(\\mathrm{OPT}_n/\\epsilon) \\leq n \\ln(V/\\epsilon)$. \n\\end{thm}\n\nThe proof of Theorem \\ref{thm:greedy} uses many of the same techniques used by Guillory et. al ({\\cite{{guillory2010interactive}}}), in their work on interactive set cover. For technical reasons, we cannot state our theorem directly as a corollary of these results, which assume a finite hypothesis class, whereas we have an infinite space of possible rewards. Nevertheless, these proofs are easily adapted to our setting, and the full proofs are given in the appendix. \n\n\n\n\nFinally we note that Line (\\ref{line:max}) is not computable exactly without parametric assumptions on the class of environments or space of rewards. In practice, and as we will describe in the next section, we approximate the exact maximization by sampling environments and rewards from $K(\\mathcal{E})$, and optimizing on the sampled sets. \n\n\n\n\n\n\n\\section{Experimental Analysis}\n\n\n\n\n\n\n\n\n\n\nWe now deploy the techniques discussed in a setting, demonstrating that maximizing $f(\\cdot)$ is indeed effective for identifying $R$. We imagine that we have an agent that will be dropped into a grid world. The experimenter would like to infer the agent's reward for each space in the grid. We imagine that the experimenter has the power to construct walls in the agent's environment, and so we will alternatively refer to an environment as a \\emph{maze}. To motivate the value of repeated experimentation, recall Figure \\ref{fig:mazes}. \n\nThis is a restricted environment for the learner. The learner cannot, for example, make it so that an action causes the agent to travel from a bottom corner of the maze to a top corner. However, the learner can modify the dynamics of the environment in so far as it can construct maze walls. \n\n\n\nWe evaluate Algorithm \\ref{alg:greedy} on grids of size $10 \\times 10$. An agent's reward is given by a vector $R \\in \\mathbb{R}^{100}$, with $|| R ||_{\\infty} \\leq R_{\\max}$, where $R_{\\max}$ is taken to be $10$ in all that follows. In each simulation \nwe randomly assign some state in $R$ to have reward $R_{\\max}$, and assign $5$ states to have reward $1$.\\footnote{For motivation, one might think of the agent as being a mouse, with these rewards corresponding to food pellets or various shiny objects in a mouse's cage.} \nThe remaining states give reward $0$. The agent's discount rate is taken to be $0.8$. The goal of the learner is not just to determine which states are rewarding, but to further determine that the latter states yield $1/10$ the reward of the former.\n\nIn Figure \\ref{fig:lambda0}, we display our main experimental results for four different algorithms in the policy observation setting, and in Figure \\ref{fig:traj} for the trajectory setting. Error represents $||R - \\hat{R}||_\\infty$, where $\\hat{R}$ is an algorithm's prediction, with error bars representing standard error over $20$ simulations. \n\nIn Figure \\ref{fig:lambda0}, the horizontal line displays the best results we achieved without repeated experimentation. If the learner only selects a single environment $E$, observing policy $\\pi$, it is stuck with whatever experimental unidentifiability exists in $K(E,\\pi)$. In such a scenario, we can select a $K(E,\\pi)$ according to a classic IRL heuristic, given by LP (\\ref{eqn:classic}) in Section \\ref{sec:heuristics}, for some choice of $\\lambda$ in LP (\\ref{eqn:classic}). Since the performance of this method depends both on which environment is used, and the choice of $\\lambda$, we randomly generated $100$ different environments, and for each of those environments selected $\\lambda \\in \\{0.05,.1,.5,1,5,6,7,8,9,10\\}$. We then evaluated each of these single-environment approaches with $20$ simulations, the best error among these $1300$ different single-environment algorithms is displayed by the horizontal line. Immediately we see that the experimental unidentifiability  from using a single environment makes it difficult to distinguish the actual reward function, with $\\mathrm{err}$ for the best choice of $E$ and $\\lambda$ greater than $5$. \n\n\n\nThe remaining algorithms --- which we will describe in greater detail below --- conduct repeated experimentation. Each of these algorithms uses a different rule to select a new environment on each round. Given the sequence of (environment, policy) pairs $\\mathcal{E}$ generated by each of these algorithms, we solve the LP (\\ref{eq:rule}) on $K(\\mathcal{E})$ at the end of each round. This is done with the same choice of $\\lambda (=0.5)$ for each of the algorithms. \n\nBesides the $\\mathrm{Greedy}$ algorithm of the previous section, we implement two other algorithms, which conduct repeated experiments, but do so non-adaptively. $\\mathrm{RandUniform}$, in each round, selects a maze uniformly at random from the space of possible mazes (each wall is present with probability $0.5$). Note that $\\mathrm{RandUniform}$ will tend to select mazes where roughly half of the walls are present. Thus, we also consider $\\mathrm{RandVaried}$ which, in each round, selects a maze from a different distribution $\\mathcal{D}$. Mazes drawn from $\\mathcal{D}$ are generated by a two-step process. First, for each row $r$ and column $c$, we select numbers $d_r,d_c$ i.i.d. from the uniform distribution on $[0,1]$. Then each wall along row $r$ (column $c$ respectively) is created with probability $d_r$ ($d_c$ respectively). Although the probability any particular wall is present is still $0.5$, the correlations in $\\mathcal{D}$ creates more variable mazes (e.g. allowing an entire row to be sparsely populated with walls). \n\nWe implement Algorithm \\ref{alg:greedy}, $\\mathrm{Greedy}$, of the previous section, by approximating the maximization in Line \\ref{line:max} in Algorithm \\ref{alg:greedy}. This approximation is done by sampling $10$ environments from $\\mathcal{D}$, the same distribution used by $\\mathrm{RandVaried}$. In the policy observation setting, $1000$ samples are first drawn from the consistent set $K(\\mathcal{E})$ using a hit-and-run sampler \\cite{lovasz1999hit}, which is an MCMC method for uniformly sampling high-dimensional convex sets in polynomial time. These same samples are also used to estimate the volume $f(\\cdot)$. In the trajectory setting, we first sample trajectories $T$ on an environment $E$, then we use $K(E,\\hat{\\pi})$ for an arbitrary $\\hat{\\pi}, \\hat{\\pi} \\equiv_{\\mathcal{D}(T)} \\pi_T$, as a proxy for $K(E,T)$. \n\n\n\nExamining the results, we see that $\\mathrm{Greedy}$ converges significantly quicker than either of the non-adaptive approaches. After $25$ rounds of experimentation in the policy observation setting, $\\mathrm{Greedy}$ attains error $0.2687 (\\pm 0.0302)$, while the best non-adaptive approach attains $0.9691 (\\pm 0.24310)$. $\\mathrm{Greedy}$ only requires $16$ rounds to reach a similar error of $0.9678 (\\pm 0.0701)$. We note further that the performance of $\\mathrm{Greedy}$ seems to continue to improve, while the non-adaptive approaches appear to stagnate. This could be due to the fact that after a certain number of rounds, the non-adaptive approaches have received all the information available from the environments typically sampled from their distributions. In order to make progress they must receive new information, in contrast to $\\mathrm{Greedy}$, which is designed to actively select the environments that will do just that. \n\nFinally, $\\mathrm{Greedy}$ runs by selecting a sequence of environments, resulting in observations $\\mathcal{E}$. It then selects $R$ from $K(\\mathcal{E})$ using LP (\\ref{eq:rule}). Thus, the regularization parameter $\\lambda$ in LP (\\ref{eq:rule}) is a free parameter for $\\mathrm{Greedy}$ that we took to be equal to $0.5$ for results (Figure \\ref{fig:lambda0}). We conclude by experimentally analyzing the sensitivity of $\\mathrm{Greedy}$ to the choice of this parameter, as well as of $\\mathrm{RandUniform}$, and $\\mathrm{RandVaried}$, which also select $R$ according to LP (\\ref{eq:rule}). As $\\lambda$ is increased, eventually the LP over-regularizes, and is optimized taking $\\mathrm{R} = \\vec{0}$. In our setting, once $\\lambda \\approx 1$ this begins to occur, and we begin to see pathological behavior (Figure \\ref{fig:lambda1}). This problem occurs in standard IRL, and one approach (\\cite{ng2000algorithms}) is to select a large lambda before this transition, hence our choice of $\\lambda = 0.5$. However, even for significantly smaller $\\lambda$, the results are qualitatively similar (Figure \\ref{fig:lambda2}) to those in Figure \\ref{fig:lambda0}. We find that as long as $\\lambda$ is not too large, the results are not sensitive to the choice of $\\lambda$. \n\\begin{figure}[h!]\n    \\begin{subfigure}[b]{.23\\textwidth}\n      \\includegraphics[width=\\textwidth]{./lambda1_bigger_nokey.png}\n      \\caption{$\\lambda = 1$}\n      \\label{fig:lambda1}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{.23\\textwidth}\n      \\includegraphics[width=\\textwidth]{./lambda05_bigger_test.png}\n      \\caption{$\\lambda = 0.05$}\n      \\label{fig:lambda2}\n    \\end{subfigure}\n    \\caption{Result for all repeated experimentation algorithms using large and small regularization parameter $\\lambda$.}\n\\end{figure}\n\\section{Conclusions}\nWe provide a number of contributions in this work. First, we separate the causes of unidentifiability in IRL problems into two classes: representational, and experimental.  We argue that representational unidentifiability is superficial, leading us to redefine the problem of identification in IRL according to Definition \\ref{defn:ident}. While previous work does not distinguish between these two classes, we demonstrate that, by doing so, algorithms can be designed to eliminate experimental unidentifiability while providing formal guarantees. \n\nAlong the way, we derive a new model for IRL where the learner can observe behavior in multiple environments, a model which we believe is interesting in its own right, but also is key to eliminating experimental unidentifiability. We give an algorithm for a very powerful learner who can observe agent behavior in any environment, and show that the algorithm $\\epsilon$-identifies an agent reward defined on $d$ states, while observing behavior on only $O(\\log(d/\\epsilon))$ environments. We then weaken this learner to model more realistic settings where the learner might be restricted in the types of environments it may choose, and where it may only be able to elicit a small number of demonstrations from the agent. We derive a simple adaptive greedy algorithm which will select a nearly optimal (with respect to reducing the volume of possible reward function) set of environments. The value of the solution found by this greedy algorithm will be a comparable to the optimal algorithm which uses a logarithmic factor fewer number of experiments. \n\nFinally, we implement the algorithm in a simple maze environment that nevertheless demonstrates the value of eliminating experimental unidentifiability, significantly outperforming methods that attempt to perform IRL from a single environment. \n\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{inverse.bib}\n\n\\appendix\n\n\\section{Proof of Theorem 3}\n\n\\begin{thm}\nFor any $R,R' \\in \\mathbb{R}^d$, $R \\equiv R'$ if and only if they have the same canonicalized representation. \n\\end{thm}\n\\begin{proof}\nBy definition, the canonicalized representation of any reward function is attained by scaling and translation. Therefore, by Theorem \\ref{thm:equiv}, if $R$ and $R'$ are both canonicalized as $R_c$, we have that $R \\equiv R_c$ and $R' \\equiv R_c$, and therefore $R \\equiv R'$. \n\nIn the other direction, suppose $R$ and $R'$ are canonicalized to $R_c$ and $R_c'$ respectively, where $R_c \\not= R_c'$. Again, by Theorem \\ref{thm:equiv}, we have that $R \\equiv R_c$ and $R' \\equiv R_c'$. Thus, to prove the theorem, it is sufficient to argue that $R_c$ and $R_c'$ are not behaviorally equivalent. \n\nIf one of $R_c,R_c'$ is $\\vec{0}$ and the other is not, then it is straightforward to show that they are not behaviorally equivalent. Thus, we focus on the case where both $R_c$ and $R'_c$ are not $\\vec{0}$. We consider three cases. \n\nFirst, suppose that $R_c \\not= R_c'$ because they have different minimally-rewarding states. Without loss of generality suppose that there is some $s_0$ with $R_c(s_0) = 0$ but $R_c'(s_0) > 0$. Furthermore, let $s'_0$ be any state such that $R_c'(s'_0) = 0$. Consider an environment $E$ with two actions $a$ and $a'$. Action $a$ deterministically transitions to state $s_0$ from any other state, while action $a'$ determininstically transitions to state $s_0'$ from any other state. Let $\\pi_a$ be the policy that always takes action $a$. $\\mathrm{OPT}(E,R'_c) = \\{\\pi_a\\}$. However, if $\\pi_a \\in \\mathrm{OPT}(E, R_c)$, this means that $R_c(s_0') = 0$, and therefore all policies are in $\\mathrm{OPT}(E,R_c)$. Thus, $\\mathrm{OPT}(E,R_c) \\not= \\mathrm{OPT}(E,R'_c)$, and $R_c,R'_c$ are not behaviorally equivalent. \n\nNext, suppose that $R_c \\not= R_c'$ because they have different maximally-rewarding states. Analagously to the previous case, suppose without loss of generality there is some $s_0$ with $R_c(s_0) = 1$ by $R'_c(s_0) < 0$, and let $s'_0$ be any state such that $R_c'(s_0') = 1$ (which exists since $R_c' \\not= \\vec{0}$). Define the environment $E$ in the same way as the previous case. This time, $\\mathrm{OPT}(E,R'_c) = \\{\\pi_{a'}\\}$, while $\\mathrm{OPT}(E,R_c) \\not= \\{\\pi_{a'}\\}$. \n\nFinally, suppose that $R_c$ and $R_c'$ share the same maximally and minimally rewarding states, but there exists some $s$ such that $R_c(s) \\not= R_c'(s)$. Let $s_0$ be any state such that $R_c(s_0) = R_c'(s_0) = 0$ and let $s_1$ be any state such that $R_c(s_1) = R'_c(s_1) = 1$. Without loss of generality suppose that $R_c(s) < R_c'(s)$. Let $E$ be the environment with two actions $a$ and $a_p$. Let $p$ be any real number $0 \\leq R_c(s) < p < R_c'(s) \\leq 1$. From every state, action $a_p$ transitions to state $s_1$ with probabiity $p$ and to state $s_0$ with the remaining probability. From every state action $a$ transtions to state $s$ deterministically. The reward for taking action $a_p$ in any state under either reward function is $p$, while action $a$ gives a reward of $R_c(s) < p$ under $R_c$ and $R'_c(s) > p$ under $R_c'$. Thus, $\\mathrm{OPT}(E,R_c) = \\{\\pi_{a_p}\\} \\not= \\{\\pi_{a}\\} = \\mathrm{OPT}(E,R'_c)$, concluding the proof.\n\\end{proof}\n\n\\section{Proof of Greedy's Performance}\n\nGiven a set $\\mathcal{S}$ and component $s$, we use $\\mathcal{S} + s$ to denote the union of the singleton set $\\{s\\}$ with $\\mathcal{S}$. We begin by redefining:\n\n\n", "itemtype": "equation", "pos": 38680, "prevtext": "}\n\nThere are other selection rules for the single-environment setting, which are generalizable to the repeated experimentation setting, including heuristics for the infinite state setting, trajectory heuristics, as well as approaches already adapted to multiple environments \\cite{ratliff2006maximum}. Due to space constraints, we discuss only the foundational approach of {\\cite{{ng2000algorithms}}}. Our goal here is simply to emphasize the dichotomy between adapting pre-existing IRL methods to data gathered from multiple environments (however that data was generated), and the problem of how to best select those environments to begin with, this latter problem being the focus of the next section. \n\n\n\\begin{figure*}[t]\n  \\centering\n    \\begin{subfigure}[b]{.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{./results1b.png}\n      \\caption{Policy Observations}\n      \\label{fig:lambda0}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{.45\\textwidth}\n      \\includegraphics[width=\\textwidth]{./trajectories_bwfriendly.png}\n      \\caption{Trajectory Observations}\n      \\label{fig:traj}\n    \\end{subfigure}\n\n\n  \\caption{ Plot (a) displays $||\\hat{R}||_{\\infty}$ error for predicted vector $\\hat{R}$ in the policy observation setting, with bars indicating standard error. Plot (b) displays the same in the trajectory setting. }\n  \\label{fig:results2}\n\\end{figure*}\n\n\n\\subsection{Adaptive Experimentation}\n\\label{sec:greedy}\nGiven a universe $\\mathcal{U}$ of candidate environments, we now ask how to select a small number of environments from $\\mathcal{U}$ so that the environments are maximally informative. We must first decide what we mean by ``informative.'' We propose that for a set of experiments $\\mathcal{E}$ (either in the policy or trajectory setting), a natural objective is to minimize the mass of the resulting space of possible rewards $K(\\mathcal{E})$ with respect to some measure (or distribution) $\\mu$. Under the Lebesgue measure (or uniform distribution), this corresponds to the natural goal of reducing the volume of the $K(\\mathcal{E})$ as much as possible. Thus we define: \n", "index": 11, "text": "\\begin{align*}\\mathrm{Vol}_{\\mu}(K(\\mathcal{E})) &= \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\mathcal{E})] \\mathrm{d}\\mu(R)\\\\\n \t\t\t\t &= \\mathbb{P}_{R \\sim \\mu}\\left[ R \\in K(\\mathcal{E}) \\right]\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))\" display=\"inline\"><mrow><msub><mi>Vol</mi><mi>\u03bc</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{\\mathbb{R}^{d}}\\mathbf{1}[R\\in K(\\mathcal{E})]\\mathrm{d}%&#10;\\mu(R)\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msup><mi>\u211d</mi><mi>d</mi></msup></msub></mstyle><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">[</mo><mi>R</mi><mo>\u2208</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>\u03bc</mi><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathbb{P}_{R\\sim\\mu}\\left[R\\in K(\\mathcal{E})\\right]\" display=\"inline\"><mrow><mo>=</mo><msub><mi>\u2119</mi><mrow><mi>R</mi><mo>\u223c</mo><mi>\u03bc</mi></mrow></msub><mrow><mo>[</mo><mi>R</mi><mo>\u2208</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": " \n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nWe will find it convenient to cast this as a maximization problem, and therefore also define $f(\\mathcal{E}) = V - \\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))$, where $V$ is an upper bound on the volume of $[-R_{\\max}, R_{\\max}]^d$, and our goal to maximize $f(\\mathcal{E})$. \n\nThis objective has several desirable properties. First and foremost, by reducing the volume of $K(\\mathcal{E})$ we eliminate the space of possible reward functions (i.e. experimental unidentifiability). Secondly, the repeated experimentation setting is fundamentally an active learning setting. We can think of the true, unknown, $R$ as a function that labels environments $E$ with either a corresponding policy $\\pi$ or trajectory $T$. Thus, the volume operator corresponds to reducing the \\emph{version space} of possible rewards. Furthermore, as we will see later in this section, the objective is a monotone submodular function, an assumption well-studied in the active learning literature \\cite{guillory2010interactive,golovin2010adaptive}, allowing us to prove guarantees for a greedy algorithm. \n\n\n\n\nFinally, we will normally think of $\\mu$ as being the Lebesgue measure, and $\\mathrm{Vol}(\\cdot)$ as volume in $d$-dimensional Euclidean space (or the uniform distribution on $[-R_{\\max}, R_{min}]^d$). However, the choice of $\\mu$ makes the objective quite general. For example, by making $\\mu$ uniform on an $\\epsilon$-net on $\\mathbb{R}^d$, $\\mathrm{Vol}$ corresponds to counting the number of rewards that are $\\epsilon$-apart with respect to some metric. In many settings, $R$ naturally comes from some discrete space, such as the corners of the hypercube $\\{0,1\\}^d$. Again, this is readily modeled by the correct choice of $\\mu$. In fact, $\\mu$ can be thought of simply as any prior on $[-R_{\\max}, R_{\\max}]^d$. \n\nWe are now ready to describe a simple algorithm that adaptively selects environments $E \\in \\mathcal{U}$, attempting to greedily maximize $f(\\cdot)$, depicted as Algorithm \\ref{alg:greedy}.\n\n\\begin{algorithm}[h]\n\\begin{algorithmic}[1]\n\\small \\State {\\bf Input} $B$\n\\State $\\mathrm{i} := 1$\n\\State $\\mathcal{E} := \\emptyset$\n\\While {$i \\leq B$}\n\\State $E_i := \\underset{E}{\\arg\\max} \\underset{R \\in K(\\mathcal{E})}{\\min} \\underset{\\pi \\in \\mathrm{OPT}(E, R)}{\\min} \\small{f(\\mathcal{E} \\cup (E, \\pi)) - f(\\mathcal{E})}$ \\label{line:max}\n\\State Observe policy $\\pi^i$ for $E^i$. \n\\State $\\mathcal{E} := (\\mathcal{E}, (E^i, \\pi^i))$\n\\State $i := i + 1$\n\\EndWhile\n\\State \\Return $\\mathcal{E}$\n\\end{algorithmic}\n\\caption{Greedy Environment Selection}\n\\label{alg:greedy}\n\\end{algorithm}\n\nIn order to state a performance guarantee about Algorithm \\ref{alg:greedy}, we will use the fact that $f(\\cdot)$ is a submodular, non-decreasing, function on subsets of environment, observation pairs, $2^{\\mathcal{\\mathcal{U}} \\times O}$, where $O$ is the set of possible observations.\n\n\\begin{lem}\\label{lem:submod}\n$f$ is a submodular, non-decreasing function. \n\\end{lem}\n\\begin{proof}\nGiven a set $\\mathcal{S}$ and component $s$, we use $\\mathcal{S} + s$ to denote the union of the singleton set $\\{s\\}$ with $\\mathcal{S}$. Let $O$ be the set of possible observations, so that $o$ is a trajectory in the trajectory setting, and a policy in the policy setting. Let $\\mathcal{U}$ be the space of possible environments. \n\nFix any $\\hat{\\mathcal{E}} \\subset \\mathcal{E} \\subset 2^{\\mathcal{U} \\times O}$, and $(E,o) \\not\\in \\mathcal{E}$. By definition of $K(\\cdot)$, we have that $K(\\mathcal{E} + (E, o)) = K(\\mathcal{E}) \\cap K(E, o)$ and $K(\\mathcal{E}) \\subset K(\\hat{\\mathcal{E}})$, and so: $f((\\mathcal{E}, (E,o))) - f(\\mathcal{E}) = \\mathrm{Vol}(K(\\mathcal{E})) - \\mathrm{Vol}(K(\\mathcal{E}, (E,o))) = \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\mathcal{E}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R)$\\\\ $\\leq \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\hat{\\mathcal{E}}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R) = f((\\hat{\\mathcal{E}}, (E,o))) - f(\\hat{\\mathcal{E}})$\nThis establishes submodularity of $f$. Since $\\mathcal{E}$ is arbitrary and the right-hand-side of the second equality is non-zero, $f$ is also monotone.\n\\end{proof}\n\nThe performance of any algorithm is a function of how many experiments are attempted, and thus our analysis must take this into account. Let $\\mathcal{A}_n$ be a deterministic algorithm that deploys at most $n$ experiments. $\\mathcal{A}_n$ has a worst-case performance, which depends on the true reward $R$ and what policies were observed. We say a sequence of experiments $\\mathcal{E} = ((E^1, o^1),...,(E^n, o^n))$ is consistent with $\\mathcal{A}_n$ and $R$, if $\\mathcal{A}_n$ chooses environment $E^{j+1}$ after observing the subsequence of experiments $((E^1, o^1),...,(E^j, o^j))$, and $o^j$ is either a trajectory or policy consistent with $(E,R)$. Denoting the set of consistent experiments $\\mathcal{C}(\\mathcal{A}_n, R)$, the best performance that any algorithm can \\emph{guarantee} with $n$ experiments is: $\\mathrm{OPT}_n = \\max_{\\mathcal{A}_n} \\min_{R} \\min_{\\mathcal{E} \\in \\mathcal{C}(\\mathcal{A}_n, R)} f(\\mathcal{E})$\n\n\n\nThe submodularity of $f$, allows us to prove that for any $n$, the Greedy Environment Selection Algorithm\\footnote{n.b. in the trajectory setting, one would replace the minimization over $\\pi \\in \\mathrm{OPT}(E,R)$ in line \\ref{line:max} of the algorithm, with a minimization over $T$ consistent with $\\pi, \\pi \\in \\mathrm{OPT}(E,R)$.}  needs slightly more than $n$ experiments (by a logarithmic factor) to attain $f(\\mathcal{E}) \\approx \\mathrm{OPT}_n$.  \n\n\n\\begin{thm}\\label{thm:greedy}\n$\\mathcal{E}$ returned by the Greedy Environment Selection algorithm satisfies $f(\\mathcal{E}) \\geq \\mathrm{OPT}_n - \\epsilon$ when $B = n\\ln(\\mathrm{OPT}_n/\\epsilon) \\leq n \\ln(V/\\epsilon)$. \n\\end{thm}\n\nThe proof of Theorem \\ref{thm:greedy} uses many of the same techniques used by Guillory et. al ({\\cite{{guillory2010interactive}}}), in their work on interactive set cover. For technical reasons, we cannot state our theorem directly as a corollary of these results, which assume a finite hypothesis class, whereas we have an infinite space of possible rewards. Nevertheless, these proofs are easily adapted to our setting, and the full proofs are given in the appendix. \n\n\n\n\nFinally we note that Line (\\ref{line:max}) is not computable exactly without parametric assumptions on the class of environments or space of rewards. In practice, and as we will describe in the next section, we approximate the exact maximization by sampling environments and rewards from $K(\\mathcal{E})$, and optimizing on the sampled sets. \n\n\n\n\n\n\n\\section{Experimental Analysis}\n\n\n\n\n\n\n\n\n\n\nWe now deploy the techniques discussed in a setting, demonstrating that maximizing $f(\\cdot)$ is indeed effective for identifying $R$. We imagine that we have an agent that will be dropped into a grid world. The experimenter would like to infer the agent's reward for each space in the grid. We imagine that the experimenter has the power to construct walls in the agent's environment, and so we will alternatively refer to an environment as a \\emph{maze}. To motivate the value of repeated experimentation, recall Figure \\ref{fig:mazes}. \n\nThis is a restricted environment for the learner. The learner cannot, for example, make it so that an action causes the agent to travel from a bottom corner of the maze to a top corner. However, the learner can modify the dynamics of the environment in so far as it can construct maze walls. \n\n\n\nWe evaluate Algorithm \\ref{alg:greedy} on grids of size $10 \\times 10$. An agent's reward is given by a vector $R \\in \\mathbb{R}^{100}$, with $|| R ||_{\\infty} \\leq R_{\\max}$, where $R_{\\max}$ is taken to be $10$ in all that follows. In each simulation \nwe randomly assign some state in $R$ to have reward $R_{\\max}$, and assign $5$ states to have reward $1$.\\footnote{For motivation, one might think of the agent as being a mouse, with these rewards corresponding to food pellets or various shiny objects in a mouse's cage.} \nThe remaining states give reward $0$. The agent's discount rate is taken to be $0.8$. The goal of the learner is not just to determine which states are rewarding, but to further determine that the latter states yield $1/10$ the reward of the former.\n\nIn Figure \\ref{fig:lambda0}, we display our main experimental results for four different algorithms in the policy observation setting, and in Figure \\ref{fig:traj} for the trajectory setting. Error represents $||R - \\hat{R}||_\\infty$, where $\\hat{R}$ is an algorithm's prediction, with error bars representing standard error over $20$ simulations. \n\nIn Figure \\ref{fig:lambda0}, the horizontal line displays the best results we achieved without repeated experimentation. If the learner only selects a single environment $E$, observing policy $\\pi$, it is stuck with whatever experimental unidentifiability exists in $K(E,\\pi)$. In such a scenario, we can select a $K(E,\\pi)$ according to a classic IRL heuristic, given by LP (\\ref{eqn:classic}) in Section \\ref{sec:heuristics}, for some choice of $\\lambda$ in LP (\\ref{eqn:classic}). Since the performance of this method depends both on which environment is used, and the choice of $\\lambda$, we randomly generated $100$ different environments, and for each of those environments selected $\\lambda \\in \\{0.05,.1,.5,1,5,6,7,8,9,10\\}$. We then evaluated each of these single-environment approaches with $20$ simulations, the best error among these $1300$ different single-environment algorithms is displayed by the horizontal line. Immediately we see that the experimental unidentifiability  from using a single environment makes it difficult to distinguish the actual reward function, with $\\mathrm{err}$ for the best choice of $E$ and $\\lambda$ greater than $5$. \n\n\n\nThe remaining algorithms --- which we will describe in greater detail below --- conduct repeated experimentation. Each of these algorithms uses a different rule to select a new environment on each round. Given the sequence of (environment, policy) pairs $\\mathcal{E}$ generated by each of these algorithms, we solve the LP (\\ref{eq:rule}) on $K(\\mathcal{E})$ at the end of each round. This is done with the same choice of $\\lambda (=0.5)$ for each of the algorithms. \n\nBesides the $\\mathrm{Greedy}$ algorithm of the previous section, we implement two other algorithms, which conduct repeated experiments, but do so non-adaptively. $\\mathrm{RandUniform}$, in each round, selects a maze uniformly at random from the space of possible mazes (each wall is present with probability $0.5$). Note that $\\mathrm{RandUniform}$ will tend to select mazes where roughly half of the walls are present. Thus, we also consider $\\mathrm{RandVaried}$ which, in each round, selects a maze from a different distribution $\\mathcal{D}$. Mazes drawn from $\\mathcal{D}$ are generated by a two-step process. First, for each row $r$ and column $c$, we select numbers $d_r,d_c$ i.i.d. from the uniform distribution on $[0,1]$. Then each wall along row $r$ (column $c$ respectively) is created with probability $d_r$ ($d_c$ respectively). Although the probability any particular wall is present is still $0.5$, the correlations in $\\mathcal{D}$ creates more variable mazes (e.g. allowing an entire row to be sparsely populated with walls). \n\nWe implement Algorithm \\ref{alg:greedy}, $\\mathrm{Greedy}$, of the previous section, by approximating the maximization in Line \\ref{line:max} in Algorithm \\ref{alg:greedy}. This approximation is done by sampling $10$ environments from $\\mathcal{D}$, the same distribution used by $\\mathrm{RandVaried}$. In the policy observation setting, $1000$ samples are first drawn from the consistent set $K(\\mathcal{E})$ using a hit-and-run sampler \\cite{lovasz1999hit}, which is an MCMC method for uniformly sampling high-dimensional convex sets in polynomial time. These same samples are also used to estimate the volume $f(\\cdot)$. In the trajectory setting, we first sample trajectories $T$ on an environment $E$, then we use $K(E,\\hat{\\pi})$ for an arbitrary $\\hat{\\pi}, \\hat{\\pi} \\equiv_{\\mathcal{D}(T)} \\pi_T$, as a proxy for $K(E,T)$. \n\n\n\nExamining the results, we see that $\\mathrm{Greedy}$ converges significantly quicker than either of the non-adaptive approaches. After $25$ rounds of experimentation in the policy observation setting, $\\mathrm{Greedy}$ attains error $0.2687 (\\pm 0.0302)$, while the best non-adaptive approach attains $0.9691 (\\pm 0.24310)$. $\\mathrm{Greedy}$ only requires $16$ rounds to reach a similar error of $0.9678 (\\pm 0.0701)$. We note further that the performance of $\\mathrm{Greedy}$ seems to continue to improve, while the non-adaptive approaches appear to stagnate. This could be due to the fact that after a certain number of rounds, the non-adaptive approaches have received all the information available from the environments typically sampled from their distributions. In order to make progress they must receive new information, in contrast to $\\mathrm{Greedy}$, which is designed to actively select the environments that will do just that. \n\nFinally, $\\mathrm{Greedy}$ runs by selecting a sequence of environments, resulting in observations $\\mathcal{E}$. It then selects $R$ from $K(\\mathcal{E})$ using LP (\\ref{eq:rule}). Thus, the regularization parameter $\\lambda$ in LP (\\ref{eq:rule}) is a free parameter for $\\mathrm{Greedy}$ that we took to be equal to $0.5$ for results (Figure \\ref{fig:lambda0}). We conclude by experimentally analyzing the sensitivity of $\\mathrm{Greedy}$ to the choice of this parameter, as well as of $\\mathrm{RandUniform}$, and $\\mathrm{RandVaried}$, which also select $R$ according to LP (\\ref{eq:rule}). As $\\lambda$ is increased, eventually the LP over-regularizes, and is optimized taking $\\mathrm{R} = \\vec{0}$. In our setting, once $\\lambda \\approx 1$ this begins to occur, and we begin to see pathological behavior (Figure \\ref{fig:lambda1}). This problem occurs in standard IRL, and one approach (\\cite{ng2000algorithms}) is to select a large lambda before this transition, hence our choice of $\\lambda = 0.5$. However, even for significantly smaller $\\lambda$, the results are qualitatively similar (Figure \\ref{fig:lambda2}) to those in Figure \\ref{fig:lambda0}. We find that as long as $\\lambda$ is not too large, the results are not sensitive to the choice of $\\lambda$. \n\\begin{figure}[h!]\n    \\begin{subfigure}[b]{.23\\textwidth}\n      \\includegraphics[width=\\textwidth]{./lambda1_bigger_nokey.png}\n      \\caption{$\\lambda = 1$}\n      \\label{fig:lambda1}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{.23\\textwidth}\n      \\includegraphics[width=\\textwidth]{./lambda05_bigger_test.png}\n      \\caption{$\\lambda = 0.05$}\n      \\label{fig:lambda2}\n    \\end{subfigure}\n    \\caption{Result for all repeated experimentation algorithms using large and small regularization parameter $\\lambda$.}\n\\end{figure}\n\\section{Conclusions}\nWe provide a number of contributions in this work. First, we separate the causes of unidentifiability in IRL problems into two classes: representational, and experimental.  We argue that representational unidentifiability is superficial, leading us to redefine the problem of identification in IRL according to Definition \\ref{defn:ident}. While previous work does not distinguish between these two classes, we demonstrate that, by doing so, algorithms can be designed to eliminate experimental unidentifiability while providing formal guarantees. \n\nAlong the way, we derive a new model for IRL where the learner can observe behavior in multiple environments, a model which we believe is interesting in its own right, but also is key to eliminating experimental unidentifiability. We give an algorithm for a very powerful learner who can observe agent behavior in any environment, and show that the algorithm $\\epsilon$-identifies an agent reward defined on $d$ states, while observing behavior on only $O(\\log(d/\\epsilon))$ environments. We then weaken this learner to model more realistic settings where the learner might be restricted in the types of environments it may choose, and where it may only be able to elicit a small number of demonstrations from the agent. We derive a simple adaptive greedy algorithm which will select a nearly optimal (with respect to reducing the volume of possible reward function) set of environments. The value of the solution found by this greedy algorithm will be a comparable to the optimal algorithm which uses a logarithmic factor fewer number of experiments. \n\nFinally, we implement the algorithm in a simple maze environment that nevertheless demonstrates the value of eliminating experimental unidentifiability, significantly outperforming methods that attempt to perform IRL from a single environment. \n\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{inverse.bib}\n\n\\appendix\n\n\\section{Proof of Theorem 3}\n\n\\begin{thm}\nFor any $R,R' \\in \\mathbb{R}^d$, $R \\equiv R'$ if and only if they have the same canonicalized representation. \n\\end{thm}\n\\begin{proof}\nBy definition, the canonicalized representation of any reward function is attained by scaling and translation. Therefore, by Theorem \\ref{thm:equiv}, if $R$ and $R'$ are both canonicalized as $R_c$, we have that $R \\equiv R_c$ and $R' \\equiv R_c$, and therefore $R \\equiv R'$. \n\nIn the other direction, suppose $R$ and $R'$ are canonicalized to $R_c$ and $R_c'$ respectively, where $R_c \\not= R_c'$. Again, by Theorem \\ref{thm:equiv}, we have that $R \\equiv R_c$ and $R' \\equiv R_c'$. Thus, to prove the theorem, it is sufficient to argue that $R_c$ and $R_c'$ are not behaviorally equivalent. \n\nIf one of $R_c,R_c'$ is $\\vec{0}$ and the other is not, then it is straightforward to show that they are not behaviorally equivalent. Thus, we focus on the case where both $R_c$ and $R'_c$ are not $\\vec{0}$. We consider three cases. \n\nFirst, suppose that $R_c \\not= R_c'$ because they have different minimally-rewarding states. Without loss of generality suppose that there is some $s_0$ with $R_c(s_0) = 0$ but $R_c'(s_0) > 0$. Furthermore, let $s'_0$ be any state such that $R_c'(s'_0) = 0$. Consider an environment $E$ with two actions $a$ and $a'$. Action $a$ deterministically transitions to state $s_0$ from any other state, while action $a'$ determininstically transitions to state $s_0'$ from any other state. Let $\\pi_a$ be the policy that always takes action $a$. $\\mathrm{OPT}(E,R'_c) = \\{\\pi_a\\}$. However, if $\\pi_a \\in \\mathrm{OPT}(E, R_c)$, this means that $R_c(s_0') = 0$, and therefore all policies are in $\\mathrm{OPT}(E,R_c)$. Thus, $\\mathrm{OPT}(E,R_c) \\not= \\mathrm{OPT}(E,R'_c)$, and $R_c,R'_c$ are not behaviorally equivalent. \n\nNext, suppose that $R_c \\not= R_c'$ because they have different maximally-rewarding states. Analagously to the previous case, suppose without loss of generality there is some $s_0$ with $R_c(s_0) = 1$ by $R'_c(s_0) < 0$, and let $s'_0$ be any state such that $R_c'(s_0') = 1$ (which exists since $R_c' \\not= \\vec{0}$). Define the environment $E$ in the same way as the previous case. This time, $\\mathrm{OPT}(E,R'_c) = \\{\\pi_{a'}\\}$, while $\\mathrm{OPT}(E,R_c) \\not= \\{\\pi_{a'}\\}$. \n\nFinally, suppose that $R_c$ and $R_c'$ share the same maximally and minimally rewarding states, but there exists some $s$ such that $R_c(s) \\not= R_c'(s)$. Let $s_0$ be any state such that $R_c(s_0) = R_c'(s_0) = 0$ and let $s_1$ be any state such that $R_c(s_1) = R'_c(s_1) = 1$. Without loss of generality suppose that $R_c(s) < R_c'(s)$. Let $E$ be the environment with two actions $a$ and $a_p$. Let $p$ be any real number $0 \\leq R_c(s) < p < R_c'(s) \\leq 1$. From every state, action $a_p$ transitions to state $s_1$ with probabiity $p$ and to state $s_0$ with the remaining probability. From every state action $a$ transtions to state $s$ deterministically. The reward for taking action $a_p$ in any state under either reward function is $p$, while action $a$ gives a reward of $R_c(s) < p$ under $R_c$ and $R'_c(s) > p$ under $R_c'$. Thus, $\\mathrm{OPT}(E,R_c) = \\{\\pi_{a_p}\\} \\not= \\{\\pi_{a}\\} = \\mathrm{OPT}(E,R'_c)$, concluding the proof.\n\\end{proof}\n\n\\section{Proof of Greedy's Performance}\n\nGiven a set $\\mathcal{S}$ and component $s$, we use $\\mathcal{S} + s$ to denote the union of the singleton set $\\{s\\}$ with $\\mathcal{S}$. We begin by redefining:\n\n\n", "index": 13, "text": "$$\\mathrm{Vol}_{\\mu}(K(\\mathcal{E})) = \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\mathcal{E})] \\mathrm{d}\\mu(R)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))=\\int_{\\mathbb{R}^{d}}\\mathbf{1}[R\\in K(%&#10;\\mathcal{E})]\\mathrm{d}\\mu(R)\" display=\"block\"><mrow><msub><mi>Vol</mi><mi>\u03bc</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msup><mi>\u211d</mi><mi>d</mi></msup></msub><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">[</mo><mi>R</mi><mo>\u2208</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>\u03bc</mi><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": " where $V$ is an upper bound $\\mathrm{Vol}_{\\mu}([-R_{\\max}, R_{\\max}]^d)$.\n\nLet $O$ be the set of possible observations, so that $o$ is a trajectory in the trajectory setting, and a policy in the policy setting. Let $\\mathcal{U}$ be the space of possible environments. WWe first establish that $f$ is indeed submodular. \n\n\\begin{lem}\n$f$ is a submodular, non-decreasing function on $2^{\\mathcal{U} \\times O}$. \n\\end{lem}\n\\begin{proof}\nFix any $\\hat{\\mathcal{E}} \\subset \\mathcal{E} \\subset 2^{\\mathcal{U} \\times O}$, and $(E,o) \\not\\in \\mathcal{E}$. By definition of $K(\\cdot)$, we have that $K(\\mathcal{E} + (E, o)) = K(\\mathcal{E}) \\cap K(E, o)$ and $K(\\mathcal{E}) \\subset K(\\hat{\\mathcal{E}})$, and so: \n", "itemtype": "equation", "pos": -1, "prevtext": " \n\n\n", "index": 15, "text": "$$f(\\mathcal{E}) = V - \\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"f(\\mathcal{E})=V-\\mathrm{Vol}_{\\mu}(K(\\mathcal{E}))\" display=\"block\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>V</mi><mo>-</mo><mrow><msub><mi>Vol</mi><mi>\u03bc</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\nThis establishes submodularity of $f$. Since $\\mathcal{E}$ is arbitrary and the right-hand-side of the second equality is non-zero, $f$ is also monotone. \n\\end{proof}\n\nLet $\\mathcal{T} = \\{ T : \\mathcal{U} \\rightarrow O\\}$ denote the set of functions mapping environments to observations. For any $T \\in \\mathcal{T}$ and $S \\subset \\mathcal{U}$, overload $T$, so that $T(S) = \\cup_{E \\in S} (E, T(E))$. \n\nNow suppose that environments where labeled according to some $T \\in \\mathcal{T}$, and consider an algorithm which knowing $T$, selects the fewest number of environments $S$, so that $f(T(S)) \\geq \\alpha$. Given such an algorithm, we can now define the General Identification Cost, which identifies the worst-possible labelling strategy in $\\mathcal{T}$. In particular:\n\n\n", "itemtype": "equation", "pos": 60008, "prevtext": " where $V$ is an upper bound $\\mathrm{Vol}_{\\mu}([-R_{\\max}, R_{\\max}]^d)$.\n\nLet $O$ be the set of possible observations, so that $o$ is a trajectory in the trajectory setting, and a policy in the policy setting. Let $\\mathcal{U}$ be the space of possible environments. WWe first establish that $f$ is indeed submodular. \n\n\\begin{lem}\n$f$ is a submodular, non-decreasing function on $2^{\\mathcal{U} \\times O}$. \n\\end{lem}\n\\begin{proof}\nFix any $\\hat{\\mathcal{E}} \\subset \\mathcal{E} \\subset 2^{\\mathcal{U} \\times O}$, and $(E,o) \\not\\in \\mathcal{E}$. By definition of $K(\\cdot)$, we have that $K(\\mathcal{E} + (E, o)) = K(\\mathcal{E}) \\cap K(E, o)$ and $K(\\mathcal{E}) \\subset K(\\hat{\\mathcal{E}})$, and so: \n", "index": 17, "text": "\\begin{align*}\nf((\\mathcal{E}, (E,o))) &- f(\\mathcal{E}) = \\mathrm{Vol}(K(\\mathcal{E})) - \\mathrm{Vol}(K(\\mathcal{E}, (E,o))) \\\\ &= \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\mathcal{E}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R) \\\\&\\leq \\int_{\\mathbb{R}^d} \\mathbf{1}[R \\in K(\\hat{\\mathcal{E}}), R \\not\\in K(E,o)] \\mathrm{d}\\mu(R) \\\\ &= f((\\hat{\\mathcal{E}}, (E,o))) - f(\\hat{\\mathcal{E}})\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f((\\mathcal{E},(E,o)))\" display=\"inline\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-f(\\mathcal{E})=\\mathrm{Vol}(K(\\mathcal{E}))-\\mathrm{Vol}(K(%&#10;\\mathcal{E},(E,o)))\" display=\"inline\"><mrow><mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>Vol</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>Vol</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{\\mathbb{R}^{d}}\\mathbf{1}[R\\in K(\\mathcal{E}),R\\not\\in K(E%&#10;,o)]\\mathrm{d}\\mu(R)\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msup><mi>\u211d</mi><mi>d</mi></msup></msub></mstyle><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">[</mo><mi>R</mi><mo>\u2208</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>R</mi><mo>\u2209</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>\u03bc</mi><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\int_{\\mathbb{R}^{d}}\\mathbf{1}[R\\in K(\\hat{\\mathcal{E}}),R%&#10;\\not\\in K(E,o)]\\mathrm{d}\\mu(R)\" display=\"inline\"><mrow><mo>\u2264</mo><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msup><mi>\u211d</mi><mi>d</mi></msup></msub></mstyle><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">[</mo><mi>R</mi><mo>\u2208</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>R</mi><mo>\u2209</mo><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>\u03bc</mi><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=f((\\hat{\\mathcal{E}},(E,o)))-f(\\hat{\\mathcal{E}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nRecall the definition from the main body:\n\n", "itemtype": "equation", "pos": 61177, "prevtext": "\nThis establishes submodularity of $f$. Since $\\mathcal{E}$ is arbitrary and the right-hand-side of the second equality is non-zero, $f$ is also monotone. \n\\end{proof}\n\nLet $\\mathcal{T} = \\{ T : \\mathcal{U} \\rightarrow O\\}$ denote the set of functions mapping environments to observations. For any $T \\in \\mathcal{T}$ and $S \\subset \\mathcal{U}$, overload $T$, so that $T(S) = \\cup_{E \\in S} (E, T(E))$. \n\nNow suppose that environments where labeled according to some $T \\in \\mathcal{T}$, and consider an algorithm which knowing $T$, selects the fewest number of environments $S$, so that $f(T(S)) \\geq \\alpha$. Given such an algorithm, we can now define the General Identification Cost, which identifies the worst-possible labelling strategy in $\\mathcal{T}$. In particular:\n\n\n", "index": 19, "text": "$$\\mathrm{GIC}_{\\alpha} = \\max_{T \\in \\mathcal{T}} \\min_{S \\subset \\mathcal{U} : f(T(S)) \\geq \\alpha} |S|$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{GIC}_{\\alpha}=\\max_{T\\in\\mathcal{T}}\\min_{S\\subset\\mathcal{U}:f(T(S))%&#10;\\geq\\alpha}|S|\" display=\"block\"><mrow><msub><mi>GIC</mi><mi>\u03b1</mi></msub><mo>=</mo><mrow><munder><mi>max</mi><mrow><mi>T</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mrow><mi>S</mi><mo>\u2282</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi></mrow><mo>:</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mi>\u03b1</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nThis is the largest that an algorithm can guarantee to make $f(\\cdot)$ with $n$ environments, when environments are consistently labeled by some $R$. Let $A^*$ be the algorithm satisfying the $\\max$.  \n\n\\begin{lem}\\label{lem1}\n$\\mathrm{GIC}_{\\mathrm{OPT}_n} \\leq n$\n\\end{lem}\n\\begin{proof}\nFix any $T \\in \\mathcal{T}$. Consider two cases. First suppose that there exists some $S \\subset \\mathcal{U}$ such that $|S| \\leq n$, but $T(S)$ is inconsistent with the labeling of any $R$. By defintion of $f$, $f(T(S)) = V \\geq \\mathrm{OPT}_n$, and since $|S| \\leq n$, the Lemma is proven. \n\nOtherwise, it must be that all $S \\subset \\mathcal{U}$, $|S| \\leq n$, $T(S)$ is consistent with the labeling of some $R$. By definition of $\\mathrm{OPT}_n$, running $A^*$ against the labels provided by $T$ is guaranteed to result in a sequence of environments $S^*$, $|S^*| \\leq n$, satisfying $f(T(S^*)) \\geq \\mathrm{OPT_n}$. $S^*$ is a witness that $\\min_{S \\subset \\mathcal{U} : f(T(S)) \\geq \\mathrm{OPT}_n} |S|$ is at most $n$.  \n\\end{proof}\n\nGiven an environment $E$ and true reward $R$, let $O(E,R)$ denote the set of possible observations (in either the policy or trajectory setting). \n\n\\begin{lem}\\label{lem2}\nFor any $\\mathcal{E}$, such that $f(\\mathcal{E}) \\leq \\mathrm{OPT}_n$, there exists an environment $E$ such that:\n\n\n", "itemtype": "equation", "pos": 61328, "prevtext": "\n\nRecall the definition from the main body:\n\n", "index": 21, "text": "$$\\mathrm{OPT}_n = \\max_{\\mathcal{A}_n} \\min_{R} \\min_{\\mathcal{E} \\in \\mathcal{C}(\\mathcal{A}_n, R)} f(\\mathcal{E})$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{OPT}_{n}=\\max_{\\mathcal{A}_{n}}\\min_{R}\\min_{\\mathcal{E}\\in\\mathcal{C}%&#10;(\\mathcal{A}_{n},R)}f(\\mathcal{E})\" display=\"block\"><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>=</mo><mrow><mrow><munder><mi>max</mi><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>R</mi></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\\end{lem}\n\\begin{proof}\nSuppose not. Then for every environment $E$, there exists some $R \\in K(\\mathcal{E})$ and $o \\in O(E,R)$ such that:\n\n", "itemtype": "equation", "pos": 62765, "prevtext": "\n\nThis is the largest that an algorithm can guarantee to make $f(\\cdot)$ with $n$ environments, when environments are consistently labeled by some $R$. Let $A^*$ be the algorithm satisfying the $\\max$.  \n\n\\begin{lem}\\label{lem1}\n$\\mathrm{GIC}_{\\mathrm{OPT}_n} \\leq n$\n\\end{lem}\n\\begin{proof}\nFix any $T \\in \\mathcal{T}$. Consider two cases. First suppose that there exists some $S \\subset \\mathcal{U}$ such that $|S| \\leq n$, but $T(S)$ is inconsistent with the labeling of any $R$. By defintion of $f$, $f(T(S)) = V \\geq \\mathrm{OPT}_n$, and since $|S| \\leq n$, the Lemma is proven. \n\nOtherwise, it must be that all $S \\subset \\mathcal{U}$, $|S| \\leq n$, $T(S)$ is consistent with the labeling of some $R$. By definition of $\\mathrm{OPT}_n$, running $A^*$ against the labels provided by $T$ is guaranteed to result in a sequence of environments $S^*$, $|S^*| \\leq n$, satisfying $f(T(S^*)) \\geq \\mathrm{OPT_n}$. $S^*$ is a witness that $\\min_{S \\subset \\mathcal{U} : f(T(S)) \\geq \\mathrm{OPT}_n} |S|$ is at most $n$.  \n\\end{proof}\n\nGiven an environment $E$ and true reward $R$, let $O(E,R)$ denote the set of possible observations (in either the policy or trajectory setting). \n\n\\begin{lem}\\label{lem2}\nFor any $\\mathcal{E}$, such that $f(\\mathcal{E}) \\leq \\mathrm{OPT}_n$, there exists an environment $E$ such that:\n\n\n", "index": 23, "text": "$$\\min_{R \\in K(\\mathcal{E})} \\min_{o \\in O(E,R)} f(\\mathcal{E} + (E,o)) - f(\\mathcal{E}) \\geq (\\mathrm{OPT}_n - f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_n}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\min_{R\\in K(\\mathcal{E})}\\min_{o\\in O(E,R)}f(\\mathcal{E}+(E,o))-f(\\mathcal{E}%&#10;)\\geq(\\mathrm{OPT}_{n}-f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_{n}}\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>R</mi><mo>\u2208</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>o</mi><mo>\u2208</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nNow let $T' \\in \\mathcal{T}$ be defined so that:\n\n\n", "itemtype": "equation", "pos": 63066, "prevtext": "\n\\end{lem}\n\\begin{proof}\nSuppose not. Then for every environment $E$, there exists some $R \\in K(\\mathcal{E})$ and $o \\in O(E,R)$ such that:\n\n", "index": 25, "text": "$$f(\\mathcal{E} + (E,o)) - f(\\mathcal{E}) < (\\mathrm{OPT}_n - f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_n}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"f(\\mathcal{E}+(E,o))-f(\\mathcal{E})&lt;(\\mathrm{OPT}_{n}-f(\\mathcal{E}))/\\mathrm{%&#10;GIC}_{\\mathrm{OPT}_{n}}\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nBy definition of $\\mathrm{GIC}$, we have that:\n\n\n", "itemtype": "equation", "pos": 63227, "prevtext": "\n\nNow let $T' \\in \\mathcal{T}$ be defined so that:\n\n\n", "index": 27, "text": "\\begin{align}\nT'(E) &\\triangleq \\arg\\min_{R, o \\in O(E,R)} f(\\mathcal{E} + (E,o)) - f(\\mathcal{E}) \\\\ \n      &< (\\mathrm{OPT}_n - f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_n}\\label{eqn:x}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T^{\\prime}(E)\" display=\"inline\"><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\triangleq\\arg\\min_{R,o\\in O(E,R)}f(\\mathcal{E}+(E,o))-f(\\mathcal%&#10;{E})\" display=\"inline\"><mrow><mi/><mo>\u225c</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mrow><mi>R</mi><mo>,</mo><mi>o</mi></mrow><mo>\u2208</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;(\\mathrm{OPT}_{n}-f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_{n}}\" display=\"inline\"><mrow><mi/><mo>&lt;</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nSo there exists a set of environments $S$, with $|S| \\leq \\mathrm{GIC}_{\\mathrm{OPT}_n}$, such that $f(T'(S)) \\geq \\mathrm{OPT}_n$, and by monotonicity of $f$, we know that $f(T'(S) \\cup \\mathcal{E}) \\geq \\mathrm{OPT}_n$. Let $\\gamma = |S|/\\mathrm{GIC}_{\\mathrm{OPT}_n}$.\n\nHowever, despite $f(T'(S) \\cup \\mathcal{E}) \\geq \\mathrm{OPT}_n$, repeatedly applying the submodularity of $f$, then applying equation (\\ref{eqn:x}) implies:\n\n\n", "itemtype": "equation", "pos": 63477, "prevtext": "\n\nBy definition of $\\mathrm{GIC}$, we have that:\n\n\n", "index": 29, "text": "$$ \\min_{S \\subset \\mathcal{U} : f(T'(S)) \\geq \\mathrm{OPT}_n} |S| \\leq \\mathrm{GIC}_{\\mathrm{OPT}_n}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\min_{S\\subset\\mathcal{U}:f(T^{\\prime}(S))\\geq\\mathrm{OPT}_{n}}|S|\\leq\\mathrm{%&#10;GIC}_{\\mathrm{OPT}_{n}}\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mrow><mi>S</mi><mo>\u2282</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi></mrow><mo>:</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><msub><mi>OPT</mi><mi>n</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2264</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\nThis establishes a contradiction.\n\\end{proof}\n\nWe can now prove the main theorem:\n\\begin{thm}\n$\\mathcal{E}$ returned by the Greedy Environment Selection algorithm satisfies $f(\\mathcal{E}) \\geq \\mathrm{OPT}_n - \\epsilon$ when $B = n \\ln(\\mathrm{OPT_n/\\epsilon})$. \n\\end{thm}\n\\begin{proof}\nLet $\\mathcal{E}_i$ denote the subsequence consisting of the first $i$ environment, observation pairs. If for some $i \\leq B$, $f(\\mathcal{E}_i) \\geq \\mathrm{OPT}_n$, then there is nothing to prove. Otherwise, by applying Lemma \\ref{lem1}, and the definition of the algorithm, we know that:\n\n\n", "itemtype": "equation", "pos": 64014, "prevtext": "\n\nSo there exists a set of environments $S$, with $|S| \\leq \\mathrm{GIC}_{\\mathrm{OPT}_n}$, such that $f(T'(S)) \\geq \\mathrm{OPT}_n$, and by monotonicity of $f$, we know that $f(T'(S) \\cup \\mathcal{E}) \\geq \\mathrm{OPT}_n$. Let $\\gamma = |S|/\\mathrm{GIC}_{\\mathrm{OPT}_n}$.\n\nHowever, despite $f(T'(S) \\cup \\mathcal{E}) \\geq \\mathrm{OPT}_n$, repeatedly applying the submodularity of $f$, then applying equation (\\ref{eqn:x}) implies:\n\n\n", "index": 31, "text": "\\begin{align*}\nf(T'(S) \\cup \\mathcal{E}) &\\leq f(\\mathcal{E}) + \\sum_{E \\in S} (f(\\mathcal{E} + (E,T'(E))) - f(\\mathcal{E})\\\\\n                          &< f(\\mathcal{E}) + |S|(\\mathrm{OPT}_n - f(\\mathcal{E}))/\\mathrm{GIC}_{\\mathrm{OPT}_n}\\\\\n                          &= f(\\mathcal{E}) + \\gamma(\\mathrm{OPT}_n - f(\\mathcal{E})) \\\\\n                          &= (1-\\gamma)f(\\mathcal{E}) + \\gamma \\mathrm{OPT}_n \\leq \\mathrm{OPT}_n\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f(T^{\\prime}(S)\\cup\\mathcal{E})\" display=\"inline\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u222a</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq f(\\mathcal{E})+\\sum_{E\\in S}(f(\\mathcal{E}+(E,T^{\\prime}(E))%&#10;)-f(\\mathcal{E})\" display=\"inline\"><mrow><mo>\u2264</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>E</mi><mo>\u2208</mo><mi>S</mi></mrow></munder></mstyle><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>,</mo><msup><mi>T</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;f(\\mathcal{E})+|S|(\\mathrm{OPT}_{n}-f(\\mathcal{E}))/\\mathrm{GIC}%&#10;_{\\mathrm{OPT}_{n}}\" display=\"inline\"><mrow><mi/><mo>&lt;</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=f(\\mathcal{E})+\\gamma(\\mathrm{OPT}_{n}-f(\\mathcal{E}))\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(1-\\gamma)f(\\mathcal{E})+\\gamma\\mathrm{OPT}_{n}\\leq\\mathrm{OPT}_%&#10;{n}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>OPT</mi><mi>n</mi></msub></mrow></mrow><mo>\u2264</mo><msub><mi>OPT</mi><mi>n</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n which implies:\n\n", "itemtype": "equation", "pos": 65036, "prevtext": "\nThis establishes a contradiction.\n\\end{proof}\n\nWe can now prove the main theorem:\n\\begin{thm}\n$\\mathcal{E}$ returned by the Greedy Environment Selection algorithm satisfies $f(\\mathcal{E}) \\geq \\mathrm{OPT}_n - \\epsilon$ when $B = n \\ln(\\mathrm{OPT_n/\\epsilon})$. \n\\end{thm}\n\\begin{proof}\nLet $\\mathcal{E}_i$ denote the subsequence consisting of the first $i$ environment, observation pairs. If for some $i \\leq B$, $f(\\mathcal{E}_i) \\geq \\mathrm{OPT}_n$, then there is nothing to prove. Otherwise, by applying Lemma \\ref{lem1}, and the definition of the algorithm, we know that:\n\n\n", "index": 33, "text": "$$f(\\mathcal{E}_i) - f(\\mathcal{E}_{i-1}) \\geq (\\mathrm{OPT}_n - f(\\mathcal{E}_{i-1}))/\\mathrm{GIC}_{\\mathrm{OPT}_n}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"f(\\mathcal{E}_{i})-f(\\mathcal{E}_{i-1})\\geq(\\mathrm{OPT}_{n}-f(\\mathcal{E}_{i-%&#10;1}))/\\mathrm{GIC}_{\\mathrm{OPT}_{n}}\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nUsing the fact $1-x \\leq e^{-x}$, we can conclude:\n\n\n", "itemtype": "equation", "pos": 65171, "prevtext": "\n which implies:\n\n", "index": 35, "text": "$$\\mathrm{OPT}_n - f(\\mathcal{E}_i) \\leq (\\mathrm{OPT}_n - f(\\mathcal{E}_{i-1}))(1 - 1/\\mathrm{GIC}_{\\mathrm{OPT}_n})$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{OPT}_{n}-f(\\mathcal{E}_{i})\\leq(\\mathrm{OPT}_{n}-f(\\mathcal{E}_{i-1}))%&#10;(1-1/\\mathrm{GIC}_{\\mathrm{OPT}_{n}})\" display=\"block\"><mrow><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06569.tex", "nexttext": "\n\nApplying Lemma \\ref{lem2} and substituting $B$ completes the proof. \n\n\\end{proof}\n\n\n", "itemtype": "equation", "pos": 65344, "prevtext": "\n\nUsing the fact $1-x \\leq e^{-x}$, we can conclude:\n\n\n", "index": 37, "text": "$$\\mathrm{OPT}_n - f(\\mathcal{E}) \\leq \\mathrm{OPT}_n \\exp(-B/\\mathrm{GIC}_{\\mathrm{OPT}_n}))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{OPT}_{n}-f(\\mathcal{E})\\leq\\mathrm{OPT}_{n}\\exp(-B/\\mathrm{GIC}_{%&#10;\\mathrm{OPT}_{n}}))\" display=\"block\"><mrow><msub><mi>OPT</mi><mi>n</mi></msub><mo>-</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><msub><mi>OPT</mi><mi>n</mi></msub><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mi>B</mi><mo>/</mo><msub><mi>GIC</mi><msub><mi>OPT</mi><mi>n</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></math>", "type": "latex"}]