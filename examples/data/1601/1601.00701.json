[{"file": "1601.00701.tex", "nexttext": "\nwhere we define the \\emph{effective Hebbian nonlinearity} $f:=h\\circ g$\nas the composition of the nonlinearity in the plasticity rule and\nthe neuron's f-I curve. In an experimental setting, the pre-synaptic\nactivity $x$ is determined by the set of sensory stimuli (influenced\nby, e.g., the rearing conditions during sensory development \\citep{Wiesel_Hubel_others_1963}).\nTherefore, the evolution of synaptic strength, Eq. \\ref{fgh},\nis determined by the effective nonlinearity $f$ and the statistics\nof the input $\\mathbf{x}$. \n\nMany existing models can be formulated in the framework of Eq.\n\\ref{fgh}. For instance, in a classic study of simple-cell formation\n\\citep{law_formation_1994}, the Bienenstock-Cooper-Munro (BCM) model\n\\citep{Bienenstock_Cooper_Munro_1982} has a quadratic plasticity nonlinearity,\n$h(y)=y(y-\\theta)$, with a variable plasticity threshold $\\theta$,\nand a sigmoidal f-I curve, $\\sigma(\\mathbf{w}^{T}\\mathbf{x})$, which\ncombine into nonlinear Hebbian learning dynamics, $\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ h(\\sigma(\\mathbf{w}^{T}\\mathbf{x}))$. \n\nMore realistic cortical networks have dynamical properties which are\nnot accounted for by rate models. By analyzing state-of-the-art models\nof cortical neurons and synaptic plasticity, we inspected whether\nplastic spiking networks can be reduced to nonlinear Hebbian learning.\nWe considered a generalized leaky integrate-and-fire model (GIF),\nwhich includes adaptation, stochastic firing and predicts experimental\nspikes with high accuracy \\citep{pozzorini_temporal_2013}, and we\napproximate its f-I curve by a linear rectifier, $g(u)=a(u-\\theta)_{+}$,\nwith slope $a$ and threshold $\\theta$ (Fig. \\ref{fig:intro}b). \n\nAs a phenomenological model of synaptic plasticity grounded on experimental\ndata \\citep{Sjostrom_Turrigiano_Nelson_2001}, we implemented triplet\nspike-timing dependent plasticity (STDP) \\citep{pfister_triplets_2006}.\nIn this STDP model, the dependence of long-term potentiation (LTP)\nupon two post-synaptic spikes induces in the corresponding rate model\na quadratic dependence on the post-synaptic rate, while long-term\ndepression (LTD) is linear. The resulting rate plasticity \\citep{pfister_triplets_2006}\nis $h(y)=y^{2}-by$, with an LTD factor $b$ (post-synaptic activity\nthreshold separating LTD from LTP, Fig. \\ref{fig:intro}c),\nsimilar to the classic BCM model \\citep{Bienenstock_Cooper_Munro_1982,law_formation_1994}.\n\nComposing the f-I curve of the GIF with the $h(y)$ for the triplet\nplasticity model, we have an approximation of the effective learning\nnonlinearity $f=h\\circ g$ in cortical spiking neurons (Fig\n\\ref{fig:intro}d), that can be described as a quadratic rectifier,\nwith LTD threshold given by $\\theta_{1}=\\theta$ and LTP threshold\ngiven by $\\theta_{2}=\\theta+b/a$. Interestingly, the f-I slope $a$\nand LTD factor $b$ are redundant parameters of the learning dynamics:\nonly their ratio counts in nonlinear Hebbian plasticity. Metaplasticity\ncan control the LTD factor \\citep{pfister_triplets_2006,turrigiano_too_2011},\nthus regulating the LTP threshold.\n\nIf one considers a linear STDP model \\citep{Song_Miller_Abbott_2000,Gerstner_Kempter_Van_Hemmen_1996}\ninstead of the triplet STDP \\citep{pfister_triplets_2006}, the plasticity\ncurve is linear \\citep{Gerstner_Kistler_Naud_Paninski_2014}, as in\nstandard Hebbian learning, and the effective nonlinearity is shaped\nby the properties of the f-I curve (Fig.  \\ref{fig:winners}a). \n\n\\begin{figure}[!htb]\n\\begin{centering}\n\\includegraphics{figures/fig_intro_spike.pdf}\n\\par\\end{centering}\n\n\\caption[The effective Hebbian nonlinearity of plastic cortical networks.]{\\textbf{The effective Hebbian nonlinearity of plastic cortical networks.} (\\textbf{a})\nReceptive field development between an input layer of neurons with\nactivities $x_{i}$, connected by synaptic projections $w_{i}$ to\na neuron with firing rate $y$, given by an f-I curve $y=g(\\mathbf{w}^{T}\\mathbf{x}))$.\nSynaptic connections change according to a Hebbian rule $\\Delta w_{i}\\propto x_{i}\\ h(y)$.\n(\\textbf{b}) f-I curve (blue) of a GIF model \\citep{pozzorini_temporal_2013}\nof a pyramidal neurons in response to step currents of $\\mbox{500}$\nms duration (dashed line: piece-wise linear fit, with slope $a=143$\nHz/nA and threshold $\\theta=0.08$ nA). (\\textbf{c}) Plasticity function\nof the triplet STDP model \\citep{pfister_triplets_2006} (blue), fitted\nto visual cortex plasticity data \\citep{Sjostrom_Turrigiano_Nelson_2001,pfister_triplets_2006},\nshowing the weight change $\\Delta w_{i}$ as a function of the post-synaptic\nrate $y$, under a constant pre-synaptic stimulation $x_{i}$ (dashed\nline: fit by quadratic function, with LTD factor $b=22.1$ Hz). (\\textbf{d})\nThe combination of the f-I curve and plasticity function generates\nthe effective Hebbian nonlinearity (dashed line: quadratic nonlinearity\nwith LTD threshold $\\theta_{1}=0.08$ nA, LTP threshold $\\theta_{2}=0.23$\nnA). }\n\\label{fig:intro}\n\\end{figure}\n\n\n\n\\subsection*{Sparse coding as nonlinear Hebbian learning}\n\nBeyond phenomenological modeling, normative principles that explain\nreceptive fields development have been one of the goals of theoretical\nneuroscience \\citep{dayan_theoretical_2001}. Sparse coding \\citep{olshausen_emergence_1996}\nstarts from the assumptions that V1 aims at maximizing the sparseness\nof the activity in the sensory representation, and became a well-known\nnormative model to develop orientation selective receptive fields\n\\citep{Rehn_Sommer_2007,zylberberg_sparse_2011,olshausen_sparse_2004}.\nWe demonstrate that the algorithm implemented in the sparse coding\nmodel is in fact a particular example of nonlinear Hebbian learning.\n\nThe sparse coding model aims at minimizing an input reconstruction\nerror $E=\\frac{1}{2}||\\mathbf{x}-\\mathbf{W}\\mathbf{y}||^{2}+\\lambda S(\\mathbf{y})$,\nunder a sparsity constraint $S$ with relative importance $\\lambda>0$.\nFor $K$ hidden neurons $y_{j},$ such a model implicitly assumes\nthat the vector $\\mathbf{w_{j}}$ of feed-forward weights onto neuron\n$j$ are mirrored by hypothetical \\textquotedbl{}reconstruction weights\\textquotedbl{},\n$\\mathbf{W}=[\\mathbf{w}_{1}\\dots\\mathbf{w}_{K}]$. The resulting encoding\nalgorithm can be recast as a neural model \\citep{rozell_sparse_2008},\nif neurons are embedded in a feedforward model with lateral inhibition,\n$\\mathbf{y}=g(\\mathbf{w}^{T}\\mathbf{x}-\\mathbf{v}^{T}\\mathbf{y})$,\nwhere $v$ are inhibitory recurrent synaptic connections (see Methods).\nIn the case of a single output neuron, its firing rate is simply $y=g(\\mathbf{w}^{T}\\mathbf{x})$.\nThe nonlinearity $g$ of the f-I curve is threshold-like, and determined\nby the choice of the sparsity constraint \\citep{rozell_sparse_2008},\nsuch as the Cauchy, $L_{0}$ , or $L_{1}$ constraints (Fig\n\\ref{fig:winners}a, see Methods).\n\nIf weights are updated through gradient descent so as to minimize\n$E$, the resulting plasticity rule is Oja's learning rule \\citep{Oja_1982},\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ y-\\mathbf{w}\\ y^{2}$. The second\nterm $-\\mathbf{w}\\ y^{2}$ has a multiplicative effect on the strength\nof synapses projecting onto the same neuron (weight rescaling), but\ndoes not affect the receptive field shape, whereas the first term\n$\\mathbf{x}\\ y$ drives feature selectivity and receptive field formation.\nTogether, these derivations imply that the one-unit sparse coding\nalgorithm can be implemented by an effective nonlinear Hebbian rule\ncombined with weight normalization. Although the plasticity mechanism\nis linear, $\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ y$, a nonlinearity\narises from the f-I curve, $y=g(\\mathbf{w}^{T}\\mathbf{x})$, so that\nthe effective plasticity is\n\n", "itemtype": "equation", "pos": 6365, "prevtext": "\n\n{\\centering\n\\textbf{\\Large { Nonlinear Hebbian learning as a unifying principle \\\\ in receptive field formation}}\n\n\\vspace{0.5cm}\n\nCarlos S. N. Brito*, Wulfram Gerstner\n\n\\vspace{0.3cm}\n\n{\\singlespacing\nSchool of Computer and Communication Sciences and School of Life Sciences, \\\\ \nEcole Polytechnique Federale de Lausanne (EPFL), Switzerland \\\\\n*Corresponding author: carlos.stein@epfl.ch \n}\n\n\\vspace{1.0cm}\n\n\\textbf{\\large {Abstract}}\n\n\\vspace{0.6cm}\n\n\\par\n}\n\n\\begin{addmargin}[4em]{4em}\n\\noindent The development of sensory receptive fields has\nbeen modeled in the past by a variety of models including normative\nmodels such as sparse coding or independent component analysis and\nbottom-up models such as spike-timing dependent plasticity or the\nBienenstock-Cooper-Munro model of synaptic plasticity. Here we show\nthat the above variety of approaches can all be unified into a single\ncommon principle, namely Nonlinear Hebbian Learning. When Nonlinear\nHebbian Learning is applied to natural images, receptive field shapes\nwere strongly constrained by the input statistics and preprocessing,\nbut exhibited only modest variation across different choices of nonlinearities\nin neuron models or synaptic plasticity rules. Neither overcompleteness\nnor sparse network activity are necessary for the development of localized\nreceptive fields. The analysis of alternative sensory modalities such\nas auditory models or V2 development lead to the same conclusions.\nIn all examples, receptive fields can be predicted a priori by reformulating\nan abstract model as nonlinear Hebbian learning. Thus nonlinear Hebbian\nlearning and natural statistics can account for many aspects of receptive\nfield formation across models and sensory modalities. \n\\end{addmargin}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\pagebreak{}\n\n\\vspace{1cm}\n\n\\section*{Introduction}\n\nNeurons in sensory areas of the cortex are optimally driven by stimuli\nwith characteristic features that define the 'receptive field' of\nthe cell. While receptive fields of simple cells in primary visual\ncortex (V1) are localized in visual space and sensitive to the orientation\nof light contrast \\citep{hubel_receptive_1959}, those of auditory neurons\nare sensitive to specific time-frequency patterns in sounds \\citep{Miller_Escabi_Read_Schreiner_2002}.\nThe concept of a receptive field is also useful when studying higher-order\nsensory areas, for instance when analyzing the degree of selectivity\nand invariance of neurons to stimulus properties \\citep{DiCarlo_Zoccolan_Rust_2012,Freeman_Simoncelli_2011}.\n\nThe characteristic receptive fields of simple cells in V1 have been\nrelated to statistical properties of natural images \\citep{Field_1994}.\nThese findings inspired various models, based on principles as diverse\nas sparse sensory representations \\citep{olshausen_emergence_1996},\noptimal information transmission \\citep{bell_independent_1997}, or\nsynaptic plasticity \\citep{law_formation_1994}. Several studies highlighted\npossible connections between biological and normative justifications\nof sensory receptive fields \\citep{Rehn_Sommer_2007,clopath_connectivity_2010,Savin_Joshi_Triesch_2010,zylberberg_sparse_2011},\nnot only in V1, but also in other sensory areas \\citep{olshausen_sparse_2004},\nsuch as auditory \\citep{Smith_Lewicki_2006,Saxe_Bhand_Mudur_Suresh_Ng_2011}\nand secondary visual cortex (V2)  \\citep{Lee_Ekanadham_Ng_2007}. \n\nSince disparate models appear to achieve similar results, the question\narises whether there exists a general underlying concept in unsupervised\nlearning models \\citep{Saxe_Bhand_Mudur_Suresh_Ng_2011,yamins_performance-optimized_2014}.\nHere we show that the principle of nonlinear Hebbian learning is sufficient\nfor receptive field development under rather general conditions. The\nnonlinearity is defined by the neuron's f-I curve combined with the\nnonlinearity of the plasticity function. The outcome of such nonlinear\nlearning is equivalent to projection pursuit \\citep{Friedman_1987,Oja_Ogawa_Wangviwattana_1991,Fyfe_Baddeley_1995},\nwhich focuses on features with non-trivial statistical structure,\nand therefore links receptive field development to optimality principles.\n\nHere we unify and broaden the above concepts and show that plastic\nneural networks, sparse coding models and independent component analysis\ncan all be reformulated as nonlinear Hebbian learning. For natural\nimages as sensory input, we find that a broad class of nonlinear Hebbian\nrules lead to orientation selective receptive fields, and explain\nhow seemingly disparate approaches may lead to similar receptive fields.\nThe theory predicts the diversity of receptive field shapes obtained\nin simulations for several different families of nonlinearities. The\nrobustness to model assumptions also applies to alternative sensory\nmodalities, implying that the statistical properties of the input\nstrongly constrain the type of receptive fields that can be learned.\nSince the conclusions are robust to specific properties of neurons\nand plasticity mechanisms, our results support the idea that synaptic\nplasticity can be interpreted as nonlinear Hebbian learning, implementing\na statistical optimization of the neuron's receptive field properties. \n\n\n\\section*{Results }\n\n\n\\subsection*{The effective Hebbian nonlinearity}\n\nIn classic rate models of sensory development \\citep{Miller_Keller_Stryker_1989,law_formation_1994,olshausen_emergence_1996},\na first layer of neurons, representing the sensory input $\\mathbf{x}$,\nis connected to a downstream neuron with activity $y$, through synaptic\nconnections with weights $\\mathbf{w}$ (Fig. \\ref{fig:intro}a).\nThe response to a specific input is $y=g(\\mathbf{w}^{T}\\mathbf{x})$,\nwhere $g$ is the frequency-current (f-I) curve. In most models of\nHebbian plasticity \\citep{Bienenstock_Cooper_Munro_1982,Gerstner_Kistler_Naud_Paninski_2014},\nsynaptic changes $\\Delta\\mathbf{w}$ of the connection weights depend\non pre- and post-synaptic activity, with a linear dependence on the\npre-synaptic and a nonlinear dependence on the post-synaptic activity,\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ h(y)$, in accordance with models\nof pairing experiments \\citep{pfister_triplets_2006,clopath_connectivity_2010}.\nThe learning dynamics arise from a combination of the neuronal f-I\ncurve $y=g(\\mathbf{w^{T}x})$ and the Hebbian plasticity function\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ h(y)$:\n\n\n", "index": 1, "text": "\\begin{align}\n\\Delta\\mathbf{w} & \\propto\\mathbf{x}\\ h(g(\\mathbf{w^{T}x}))=\\mathbf{x}\\ f(\\mathbf{w^{T}x})\\label{fgh}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta\\mathbf{w}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\mathbf{x}\\ h(g(\\mathbf{w^{T}x}))=\\mathbf{x}\\ f(\\mathbf{w^%&#10;{T}x})\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc30</mi><mi>\ud835\udc13</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc30</mi><mi>\ud835\udc13</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\n\n\nThis analysis reveals an equivalence between sparse coding models\nand neural networks with linear plasticity mechanisms, where the sparsity\nconstraint is determined by the f-I curve $g$.\n\nSimilarly, algorithms performing independent component analysis (ICA),\na model class closely related to sparse coding, also perform effective\nnonlinear Hebbian learning, albeit inversely, with linear neurons\nand a nonlinear plasticity rule \\citep{hyvarinen_independent_2000}.\nFor variants of ICA based on information maximization \\citep{bell_independent_1997}\nor kurtosis \\citep{hyvarinen_independent_2000} different nonlinearities\narise (Fig. \\ref{fig:winners}a), but Eq. \\ref{SCfgh}\napplies equally well. Hence, various instantiations of sparse coding\nand ICA models not only relate to each other in their normative assumptions\n\\citep{olshausen_sparse_1997}, but when implemented as iterative gradient\nupdate rules, they all employ nonlinear Hebbian learning.\n\n\n\\subsection*{Simple cell development for a large class of nonlinearities}\n\nSince the models described above can be implemented by similar plasticity\nrules, we hypothesized nonlinear Hebbian learning to be a general\nprinciple that explains the development of receptive field selectivity.\nNonlinear Hebbian learning with an effective nonlinearity $f$ is\nlinked to an optimization principle with a function $F=\\int f$ \\citep{Oja_Ogawa_Wangviwattana_1991,Fyfe_Baddeley_1995}.\nFor an input ensemble $\\mathbf{x}$, optimality is achieved by weights\n$\\tilde{\\mathbf{w}}$ that maximize $\\langle F(\\tilde{\\mathbf{w}}{}^{T}\\mathbf{x})\\rangle$,\nwhere angular brackets denote the average over the input statistics.\nNonlinear Hebbian learning is a stochastic gradient ascent implementation\nof this optimization process, called projection pursuit \\citep{Friedman_1987,Oja_Ogawa_Wangviwattana_1991,Fyfe_Baddeley_1995}:\n\n\n", "itemtype": "equation", "pos": 14124, "prevtext": "\nwhere we define the \\emph{effective Hebbian nonlinearity} $f:=h\\circ g$\nas the composition of the nonlinearity in the plasticity rule and\nthe neuron's f-I curve. In an experimental setting, the pre-synaptic\nactivity $x$ is determined by the set of sensory stimuli (influenced\nby, e.g., the rearing conditions during sensory development \\citep{Wiesel_Hubel_others_1963}).\nTherefore, the evolution of synaptic strength, Eq. \\ref{fgh},\nis determined by the effective nonlinearity $f$ and the statistics\nof the input $\\mathbf{x}$. \n\nMany existing models can be formulated in the framework of Eq.\n\\ref{fgh}. For instance, in a classic study of simple-cell formation\n\\citep{law_formation_1994}, the Bienenstock-Cooper-Munro (BCM) model\n\\citep{Bienenstock_Cooper_Munro_1982} has a quadratic plasticity nonlinearity,\n$h(y)=y(y-\\theta)$, with a variable plasticity threshold $\\theta$,\nand a sigmoidal f-I curve, $\\sigma(\\mathbf{w}^{T}\\mathbf{x})$, which\ncombine into nonlinear Hebbian learning dynamics, $\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ h(\\sigma(\\mathbf{w}^{T}\\mathbf{x}))$. \n\nMore realistic cortical networks have dynamical properties which are\nnot accounted for by rate models. By analyzing state-of-the-art models\nof cortical neurons and synaptic plasticity, we inspected whether\nplastic spiking networks can be reduced to nonlinear Hebbian learning.\nWe considered a generalized leaky integrate-and-fire model (GIF),\nwhich includes adaptation, stochastic firing and predicts experimental\nspikes with high accuracy \\citep{pozzorini_temporal_2013}, and we\napproximate its f-I curve by a linear rectifier, $g(u)=a(u-\\theta)_{+}$,\nwith slope $a$ and threshold $\\theta$ (Fig. \\ref{fig:intro}b). \n\nAs a phenomenological model of synaptic plasticity grounded on experimental\ndata \\citep{Sjostrom_Turrigiano_Nelson_2001}, we implemented triplet\nspike-timing dependent plasticity (STDP) \\citep{pfister_triplets_2006}.\nIn this STDP model, the dependence of long-term potentiation (LTP)\nupon two post-synaptic spikes induces in the corresponding rate model\na quadratic dependence on the post-synaptic rate, while long-term\ndepression (LTD) is linear. The resulting rate plasticity \\citep{pfister_triplets_2006}\nis $h(y)=y^{2}-by$, with an LTD factor $b$ (post-synaptic activity\nthreshold separating LTD from LTP, Fig. \\ref{fig:intro}c),\nsimilar to the classic BCM model \\citep{Bienenstock_Cooper_Munro_1982,law_formation_1994}.\n\nComposing the f-I curve of the GIF with the $h(y)$ for the triplet\nplasticity model, we have an approximation of the effective learning\nnonlinearity $f=h\\circ g$ in cortical spiking neurons (Fig\n\\ref{fig:intro}d), that can be described as a quadratic rectifier,\nwith LTD threshold given by $\\theta_{1}=\\theta$ and LTP threshold\ngiven by $\\theta_{2}=\\theta+b/a$. Interestingly, the f-I slope $a$\nand LTD factor $b$ are redundant parameters of the learning dynamics:\nonly their ratio counts in nonlinear Hebbian plasticity. Metaplasticity\ncan control the LTD factor \\citep{pfister_triplets_2006,turrigiano_too_2011},\nthus regulating the LTP threshold.\n\nIf one considers a linear STDP model \\citep{Song_Miller_Abbott_2000,Gerstner_Kempter_Van_Hemmen_1996}\ninstead of the triplet STDP \\citep{pfister_triplets_2006}, the plasticity\ncurve is linear \\citep{Gerstner_Kistler_Naud_Paninski_2014}, as in\nstandard Hebbian learning, and the effective nonlinearity is shaped\nby the properties of the f-I curve (Fig.  \\ref{fig:winners}a). \n\n\\begin{figure}[!htb]\n\\begin{centering}\n\\includegraphics{figures/fig_intro_spike.pdf}\n\\par\\end{centering}\n\n\\caption[The effective Hebbian nonlinearity of plastic cortical networks.]{\\textbf{The effective Hebbian nonlinearity of plastic cortical networks.} (\\textbf{a})\nReceptive field development between an input layer of neurons with\nactivities $x_{i}$, connected by synaptic projections $w_{i}$ to\na neuron with firing rate $y$, given by an f-I curve $y=g(\\mathbf{w}^{T}\\mathbf{x}))$.\nSynaptic connections change according to a Hebbian rule $\\Delta w_{i}\\propto x_{i}\\ h(y)$.\n(\\textbf{b}) f-I curve (blue) of a GIF model \\citep{pozzorini_temporal_2013}\nof a pyramidal neurons in response to step currents of $\\mbox{500}$\nms duration (dashed line: piece-wise linear fit, with slope $a=143$\nHz/nA and threshold $\\theta=0.08$ nA). (\\textbf{c}) Plasticity function\nof the triplet STDP model \\citep{pfister_triplets_2006} (blue), fitted\nto visual cortex plasticity data \\citep{Sjostrom_Turrigiano_Nelson_2001,pfister_triplets_2006},\nshowing the weight change $\\Delta w_{i}$ as a function of the post-synaptic\nrate $y$, under a constant pre-synaptic stimulation $x_{i}$ (dashed\nline: fit by quadratic function, with LTD factor $b=22.1$ Hz). (\\textbf{d})\nThe combination of the f-I curve and plasticity function generates\nthe effective Hebbian nonlinearity (dashed line: quadratic nonlinearity\nwith LTD threshold $\\theta_{1}=0.08$ nA, LTP threshold $\\theta_{2}=0.23$\nnA). }\n\\label{fig:intro}\n\\end{figure}\n\n\n\n\\subsection*{Sparse coding as nonlinear Hebbian learning}\n\nBeyond phenomenological modeling, normative principles that explain\nreceptive fields development have been one of the goals of theoretical\nneuroscience \\citep{dayan_theoretical_2001}. Sparse coding \\citep{olshausen_emergence_1996}\nstarts from the assumptions that V1 aims at maximizing the sparseness\nof the activity in the sensory representation, and became a well-known\nnormative model to develop orientation selective receptive fields\n\\citep{Rehn_Sommer_2007,zylberberg_sparse_2011,olshausen_sparse_2004}.\nWe demonstrate that the algorithm implemented in the sparse coding\nmodel is in fact a particular example of nonlinear Hebbian learning.\n\nThe sparse coding model aims at minimizing an input reconstruction\nerror $E=\\frac{1}{2}||\\mathbf{x}-\\mathbf{W}\\mathbf{y}||^{2}+\\lambda S(\\mathbf{y})$,\nunder a sparsity constraint $S$ with relative importance $\\lambda>0$.\nFor $K$ hidden neurons $y_{j},$ such a model implicitly assumes\nthat the vector $\\mathbf{w_{j}}$ of feed-forward weights onto neuron\n$j$ are mirrored by hypothetical \\textquotedbl{}reconstruction weights\\textquotedbl{},\n$\\mathbf{W}=[\\mathbf{w}_{1}\\dots\\mathbf{w}_{K}]$. The resulting encoding\nalgorithm can be recast as a neural model \\citep{rozell_sparse_2008},\nif neurons are embedded in a feedforward model with lateral inhibition,\n$\\mathbf{y}=g(\\mathbf{w}^{T}\\mathbf{x}-\\mathbf{v}^{T}\\mathbf{y})$,\nwhere $v$ are inhibitory recurrent synaptic connections (see Methods).\nIn the case of a single output neuron, its firing rate is simply $y=g(\\mathbf{w}^{T}\\mathbf{x})$.\nThe nonlinearity $g$ of the f-I curve is threshold-like, and determined\nby the choice of the sparsity constraint \\citep{rozell_sparse_2008},\nsuch as the Cauchy, $L_{0}$ , or $L_{1}$ constraints (Fig\n\\ref{fig:winners}a, see Methods).\n\nIf weights are updated through gradient descent so as to minimize\n$E$, the resulting plasticity rule is Oja's learning rule \\citep{Oja_1982},\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ y-\\mathbf{w}\\ y^{2}$. The second\nterm $-\\mathbf{w}\\ y^{2}$ has a multiplicative effect on the strength\nof synapses projecting onto the same neuron (weight rescaling), but\ndoes not affect the receptive field shape, whereas the first term\n$\\mathbf{x}\\ y$ drives feature selectivity and receptive field formation.\nTogether, these derivations imply that the one-unit sparse coding\nalgorithm can be implemented by an effective nonlinear Hebbian rule\ncombined with weight normalization. Although the plasticity mechanism\nis linear, $\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ y$, a nonlinearity\narises from the f-I curve, $y=g(\\mathbf{w}^{T}\\mathbf{x})$, so that\nthe effective plasticity is\n\n", "index": 3, "text": "\\begin{align}\n\\Delta\\mathbf{w} & \\propto\\mathbf{x}\\ g(\\mathbf{w}^{T}\\mathbf{x})\\label{SCfgh}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta\\mathbf{w}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\mathbf{x}\\ g(\\mathbf{w}^{T}\\mathbf{x})\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc30</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\n\n\nMotivated by results from ICA theory \\citep{hyvarinen_independent_1998}\nand statistical properties of whitened natural images \\citep{Field_1994},\nwe selected diverse Hebbian nonlinearities $f$ (Fig.  \\ref{fig:winners}a)\nand calculated the corresponding optimization value $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor different features of interest that we consider as candidate RF\nshapes, with a whitened ensemble of patches extracted from natural\nimages as input (see Methods). These include a random connectivity\npattern, a non-local oriented edge (as in principal components of\nnatural images) and localized oriented edges (as in cat and monkey\nsimple cells in the visual cortex), shown in Fig. \\ref{fig:winners}b.\nThe relative value of $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nbetween one feature and another was remarkably consistent across various\nchoices of the nonlinearity $f$, with localized orientation-selective\nreceptive fields as maxima (Fig.  \\ref{fig:winners}b). Furthermore,\nwe also searched for the maxima through gradient ascent, so as to\nconfirm that the maxima are orientation selective (Fig.  \\ref{fig:winners}c,\nleft). Our results indicate that receptive field development of simple\ncells is mainly governed by the statistical properties of natural\nimages, while robust to specific model assumptions. \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[height=0.6\\textheight]{figures/fig_winners_v5.pdf}\n\\par\\end{centering}\n\n\\caption[Simple cell development from natural images regardless of specific\neffective Hebbian nonlinearity.]{\\textbf{Simple cell development from natural images regardless of specific\neffective Hebbian nonlinearity.} (\\textbf{a}) Effective nonlinearity\nof five common models (arbitrary units): quadratic rectifier (green,\nas in cortical and BCM models, $\\theta_{1}=1.$, $\\theta_{2}=2.$),\nlinear rectifier (dark blue, as in $L_{1}$ sparse coding or networks\nwith linear STDP, $\\theta=3.$), Cauchy sparse coding nonlinearity\n(light blue, $\\lambda=3.$), $L_{0}$ sparse coding nonlinearity (orange,\n$\\lambda=3.$), and negative sigmoid (purple, as in ICA models). (\\textbf{b})\nRelative optimization value $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor each of the five models in \\textbf{a}, for different preselected\nfeatures $\\mathbf{w}$, averaged over natural image patches \\textbf{$\\mathbf{x}$}.\nCandidate features are represented as two-dimensional receptive fields.\nFor all models, the optimum is achieved at the localized oriented\nreceptive field. Inset: Example of natural image and image patch (red\nsquare) used as sensory input. (\\textbf{c}) Receptive fields learned\nin four trials for ten effective Hebbian functions $f$ (from top:\nthe five functions considered above, $u^{3}$, $-sin(u)$, $u$, $(|u|-2)_{+}$,\n$-cos(u)$)\\textbf{ }(\\textbf{left} \\textbf{column}), and their opposites\n$-f$ (\\textbf{right column}). The first seven functions (above the\ndashed line) lead to localized oriented filters, while a sign-flip\nleads to random patterns. Linear or symmetric functions are exceptions\nand do not develop oriented filters (\\textbf{bottom} \\textbf{rows}).}\n\n\n\\label{fig:winners}\n\\end{figure}\n\n\nThe relevant property of natural image statistics is that the distribution\nof a feature derived from typical localized oriented patterns has\nhigh kurtosis \\citep{Field_1994,olshausen_emergence_1996,ruderman_statistics_1994}.\nThus to establish a quantitative measure whether a nonlinearity is\nsuitable for feature learning, we define a \\emph{selectivity index}\n(\\emph{SI}), which measures the relative value of $\\langle F(.)\\rangle$\nbetween a variable $l$ with a Laplacian distribution and a variable\n$g$ with Gaussian distribution \\citep{hyvarinen_independent_1998}:\n$SI=(\\langle F(l)\\rangle-\\langle F(g)\\rangle)/\\sigma_{F}$ (see Methods).\nThe Laplacian variable has higher kurtosis than the Gaussian variable,\nserving as a prototype of a kurtotic distribution. Since values obtained\nby filtering natural images with localized oriented patterns have\na distribution with longer tails than other patterns \\citep{Field_1994},\nas does the Laplacian variable compared to the Gaussian, positive\nvalues $SI>0$ indicate good candidate functions for learning simple\ncell-like receptive fields from natural images. We find that each\nmodel has an appropriate parameter range where $SI>0$ (Fig.\n \\ref{fig:selectivity}). For example the quadratic rectifier nonlinearity\nneeds an LTP threshold $\\theta_{2}$ below some critical level, so\nas to be useful for feature learning (Fig. \\ref{fig:selectivity}a).\n\nA sigmoidal function with threshold at zero has \\emph{negative SI},\nbut a \\emph{negative} sigmoid, as used in ICA studies \\citep{bell_independent_1997},\nhas $SI>0$. More generally, whenever an effective nonlinearity $f$\nis not suited for feature learning, its opposite $-f$ should be,\nsince its $SI$ will have the opposite sign (Fig. \\ref{fig:winners}c).\nThis implies that, in general, half of the function space could be\nsuitable for feature learning \\citep{hyvarinen_independent_1998},\ni.e. it finds weights $w$ such that the distribution of the feature\n$\\mathbf{w}^{T}\\mathbf{x}$ has a long tail, indicating high kurtosis\n(\\textquotedbl{}kurtotic feature\\textquotedbl{}). The other half of\nthe function space learns the least kurtotic features (e.g. random\nconnectivity patterns for natural images, Fig. \\ref{fig:winners}b,c). \n\nThis universality strongly constrains the possible shape of receptive\nfields that may arise during development for a given input dataset.\nFor whitened natural images, a learnable receptive field is in general\neither a localized edge detector or a non-localized random connectivity\npattern. \n\n\n\n\nAn important special case is an effective linear curve, $f(u)=u$,\nwhich arises when both f-I and plasticity curves are linear \\citep{Miller_Keller_Stryker_1989}.\nBecause the linear model maximizes variance $\\langle(\\mathbf{w}^{T}\\mathbf{x})^{2}\\rangle$,\nit can perform principal component analysis \\citep{Oja_1982}, but\ndoes not have any feature selectivity on whitened input datasets,\nwhere variance is constant (Fig. \\ref{fig:winners}c). \n\nSymmetric effective nonlinearities, $f(u)=f(-u)$, are also exceptions,\nsince their corresponding optimization functions are asymmetric, $F(u)=-F(-u)$,\nso that for datasets with symmetric statistical distributions, $P(\\mathbf{x})=P(-\\mathbf{x})$,\nthe optimization value will be zero, $\\langle F_{asym.}(\\mathbf{w}^{T}\\mathbf{x}_{sym.})\\rangle=0$.\nAs natural images are not completely symmetric, localized receptive\nfields do develop, though without orientation selectivity, as illustrated\nby a cosine function and a symmetric piece-wise linear function as\neffective nonlinearities (Fig. \\ref{fig:winners}c, bottom\nrows).\n\n\\begin{figure}[!htbp]\n\\centering{}\\includegraphics{figures/fig_selectivity.pdf}\n\\caption[Selectivity index for different effective nonlinearities.]\n{\\textbf{Selectivity index for different effective nonlinearities.} (\\textbf{a})\nQuadratic rectifier (small graphic, three examples with different\nLTP thresholds) with LTD threshold at $\\theta_{1}=1$: LTP threshold\nmust be below $3.5$ to secure positive selectivity index (green region,\nmain Fig) and learn localized oriented receptive fields (inset). A\nnegative selectivity index (red region) leads to a random connectivity\npattern (inset) (\\textbf{b}) Linear rectifier: activation threshold\nmust be above zero. (\\textbf{c}) Sigmoid: center must be below $a=-1.2$\nor, for a stronger effect, above $a=+1.2$. The opposite conditions\napply to the negative sigmoid. (\\textbf{d}) Cauchy sparse coding nonlinearity:\npositive but weak feature selectivity for any sparseness penalty $\\lambda>0$.\nInsets show the nonlinearities for different choices of parameters. }\n\\label{fig:selectivity}\n\\end{figure}\n\n\n\n\\subsection*{Receptive field diversity}\n\nSensory neurons display a variety of receptive field shapes \\citep{Ringach_2002},\nand recent modeling efforts \\citep{Rehn_Sommer_2007,zylberberg_sparse_2011}\nhave attempted to understand the properties that give rise to the\nspecific receptive fields seen in experiments. We show here that the\nshape diversity of a model can be predicted by our projection pursuit\nanalysis, and is primarily determined by the statistics of input representation,\nwhile relatively robust to the specific effective nonlinearity.\n\nWe studied a model with multiple neurons in the second layer, which\ncompete with each other for the representation of specific features\nof the input. Each neuron had a piece-wise linear f-I curve and a\nquadratic rectifier plasticity function (see Methods) and projected\ninhibitory connections $v$ onto all others. These inhibitory connections\nare learned by anti-Hebbian plasticity and enforce decorrelation of\nneurons, so that receptive fields represent different positions, orientations\nand shapes \\citep{foldiak_forming_1990,vogels_inhibitory_2011,King_Zylberberg_DeWeese_2013}.\nFor 50 neurons, the resulting receptive fields became diversified\n(Fig. \\ref{fig:gabormap}a-c, colored dots). In an overcomplete\nnetwork of 1000 neurons, the diversity further increased (Fig. \\ref{fig:gabormap}d-f, colored dots). \n\nFor the analysis of the simulation results, we refined our inspection\nof optimal oriented receptive fields for natural images by numerical\nevaluation of the optimality criterion $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor receptive fields \\textbf{$\\mathbf{w}=\\mathbf{w}_{Gabor}$}, described\nas Gabor functions of variable length, width and spatial frequency.\nFor all tested nonlinearities, the optimization function for single-neuron\nreceptive fields varies smoothly with these parameters (Fig\n\\ref{fig:gabormap}, grey-shaded background). The single-neuron optimality\nlandscape was then used to analyze the multi-neuron simulation results.\nWe found that receptive fields are located in the area where the single-neuron\noptimality criterion is near its maximum, but spread out so as to\nrepresent different features of the input (Fig. \\ref{fig:gabormap}).\nThus the map of optimization values, calculated from the theory of\neffective nonlinearity, enables us to qualitatively predict the shape\ndiversity of receptive fields. \n\nAlthough qualitatively similar, there are differences in the receptive\nfields developed for each model, such as smaller lengths for the $L_{0}$\nsparse coding model (Fig. \\ref{fig:gabormap}c). While potentially\nsignificant, these differences across models may be overwhelmed by\ndifferences due to other model properties, including different network\nsizes or input representations. This is illustrated by observing that\nreceptive field diversity for a given model differ substantially across\nnetwork sizes (Fig. \\ref{fig:gabormap}), and the difference\nis even greater from simulations with an input that is not completely\nwhite (Fig. \\ref{fig:nonwhitened}c). Thus our results suggests\nthat efforts to model receptive field shapes observed experimentally\n\\citep{Ringach_2002,Rehn_Sommer_2007,zylberberg_sparse_2011} should\nfocus on network size and input representation, which potentially\nhave a stronger effect than the nonlinear properties of the specific\nmodel under consideration.\n\n\n\n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[width=12cm]{figures/fig_gabormap_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Optimal receptive field shapes in model networks induce diversity.]{\\textbf{Optimal receptive field shapes in model networks induce diversity.}\n(\\textbf{a-f}) Gray level indicates the optimization value for different\nlengths and widths (see inset in \\textbf{a}) of oriented receptive\nfields for natural images, for the quadratic rectifier (left, see\nFig.  \\ref{fig:winners}a), linear rectifier (middle) and\n$L_{0}$ sparse coding (right). Optima marked with a black cross.\n(\\textbf{a-c}) Colored circles indicate the receptive fields of different\nshapes developed in a network of 50 neurons with lateral inhibitory\nconnections. Insets on the right show example receptive fields developed\nduring simulation. (\\textbf{d-f}) Same for a network of 1000 neurons. }\n\\label{fig:gabormap}\n\\end{figure}\n\n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics{figures/fig_nonwhitened.pdf}\n\\par\\end{centering}\n\n\\caption[Receptive fields for non-whitened natural images.]{\\textbf{Receptive fields for non-whitened natural images.}Images were preprocessed\nas in the original sparse coding study \\citep{olshausen_sparse_1997}.\nWe simulated linear rectifier neurons ($\\theta=0.5)$ with a quadratic\nplasticity nonlinearity ($b=0.5$). (\\textbf{a}) Multiple-neuron simulations,\nwith 4 neurons. The principal components dominate the optimization\nand receptive fields are not local, since they extend over most of\nthe image patch.\\textbf{ }With 50 (\\textbf{b}) and 1000 (\\textbf{c})\nneurons, lateral inhibition promotes diversity, and more localized\nreceptive field are formed. (\\textbf{insets}) Sample receptive fields\ndeveloped for each simulation.}\n \\label{fig:nonwhitened}\n\\end{figure}\n\n\n\n\n\nWe also studied the variation of receptive field position and orientation.\nFor all five nonlinearities considered, the optimization value is\nequal for different positions of the receptive field centers, confirming\nthe translation invariance in the image statistics, as long as the\nreceptive field is not too close to the border of the anatomically\nallowed fan-in of synaptic connections (Fig. \\ref{fig:position}b).\nAlso, all nonlinearities reveal the same bias towards the horizontal\nand vertical orientations (Fig. \\ref{fig:position}c). These\noptimality predictions are confirmed in single neuron simulations,\nwhich lead mostly to either horizontal or vertical orientations, at\nrandom positions (Fig. \\ref{fig:position}d). When the network\nis expanded to 50 neurons, recurrent inhibition forces receptive fields\nto cover different positions, though excluding border positions, and\nsome neurons have non-cardinal orientations (Fig. \\ref{fig:position}e).\nWith 1000 neurons, receptive fields diversify to many possible combinations\nof position, orientation and length (Fig. \\ref{fig:position}f). \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics{figures/fig_position_spread_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Diversity of receptive field size, position and orientation.]{\\textbf{Diversity of receptive field size, position and orientation.} (\\textbf{a})\nThe optimization value of localized oriented receptive fields, within\na 16x16 pixel patch of sensors, as a function of size (see Methods),\nfor five nonlinearities (colors as in Fig. \\ref{fig:winners}a).\nOptimal size is a receptive field of width around 3 to 4 pixels (filled\ntriangles). (\\textbf{b}) The optimization value as a function of position\nof the receptive field center, for a receptive field width of 4 pixels,\nindicates invariance to position within the 16x16 patch, except near\nthe borders. (\\textbf{c}) The optimization value as a function of\norientation shows preference toward horizontal and vertical directions,\nfor all five nonlinearities.\\textbf{ }(\\textbf{d}) Receptive field\nposition, orientation and length (colored bars) learned for 50 single-neuron\ntrials. The color code indicates different orientations. (\\textbf{e})\nReceptive field positions and orientations learned in a 50 neuron\nnetwork reveal diversification of positions, except at the borders.\n(\\textbf{f}) With 1000 neurons, positions and orientations cover the\nfull range of combinations (top). Selecting 50 randomly chosen receptive\nfields highlights the diversification of position, orientation and\nsize (bottom). Receptive fields were learned through the quadratic\nrectifier nonlinearity ($\\theta_{1}=1.$, $\\theta_{2}=2.$).}\n\n\n\\label{fig:position}\n\\end{figure}\n\n\n\n\\subsection*{Beyond V1 simple cells}\n\nNonlinear Hebbian learning is not limited to explaining simple cells\nin V1. We investigated if the same learning principles apply to receptive\nfield development in other visual or auditory areas or under different\nrearing conditions. \n\nFor auditory neurons \\citep{Smith_Lewicki_2006}, we used segments\nof speech as input (Fig. \\ref{fig:othermodalities}a) and\nobserved the development of spectrotemporal receptive fields localized\nin both frequency and time \\citep{Miller_Escabi_Read_Schreiner_2002}\n(Fig. \\ref{fig:othermodalities}d). The statistical distribution\nof input patterns aligned with the learned receptive fields had longer\ntails than for random or non-local receptive fields, indicating temporal\nsparsity of responses (Fig. \\ref{fig:othermodalities}d).\nSimilar to our simple cell results, the learned receptive fields show\nhigher optimization value for all five effective nonlinearities (Fig\n\\ref{fig:othermodalities}g).\n\nFor a study of receptive field development in the secondary visual\ncortex (V2)  \\citep{Lee_Ekanadham_Ng_2007}, we used natural images\nand the standard energy model \\citep{Hyvarinen_Hurri_Hoyer_2009} of\nV1 complex cells to generate input to V2 (Fig. \\ref{fig:othermodalities}b).\nThe learned receptive field was selective to a single orientation\nover neighboring positions, indicating a higher level of translation\ninvariance. When inputs were processed with this receptive field,\nwe found longer tails in the feature distribution than with random\nfeatures or receptive fields without orientation coherence (Fig\n\\ref{fig:othermodalities}e), and the learned receptive field had\na higher optimization value for all choices of nonlinearity (Fig\n\\ref{fig:othermodalities}h).\n\nAnother important constraint for developmental models are characteristic\ndeviations, such as strabismus, caused by abnormal sensory rearing.\nUnder normal binocular rearing conditions, the fan-in of synaptic\ninput from the left and right eyes overlap in visual space (Fig\n\\ref{fig:othermodalities}c). In this case, binocular receptive fields\nwith similar features for left and right eyes develop. In the strabismic\ncondition, the left and right eyes are not aligned, modeled as binocular\nrearing with non-overlapping input from each eye (Fig. \\ref{fig:othermodalities}c).\nIn this scenario, a monocular simple cell-like receptive field developed\n(Fig. \\ref{fig:othermodalities}f), as observed in experiments\nand earlier models \\citep{Cooper_Intrator_Blais_Shouval_2004}. The\nstatistical distributions confirm that for disparate inputs the monocular\nreceptive field is more kurtotic than a binocular one, explaining\nits formation in diverse models \\citep{Hunt_Dayan_Goodhill_2013} (Fig\n\\ref{fig:othermodalities}f,i).\n\nOur results demonstrate the generality of the theory across multiple\ncortical areas. Selecting a relevant feature space for an extensive\nanalysis, as we have done with simple cells and natural images, may\nnot be possible in general. Nonetheless, nonlinear Hebbian learning\nhelps to explain why some features (and not others) are learnable\nin network models \\citep{Saxe_Bhand_Mudur_Suresh_Ng_2011}. \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[height=0.6\\textheight]{figures/fig_otherdata_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Nonlinear Hebbian learning across sensory modalities.]{\\textbf{Nonlinear Hebbian learning across sensory modalities.} (\\textbf{a})\nThe auditory input is modeled as segments over time and frequency\n(red) of the spectrotemporal representation of speech signals. (\\textbf{b})\nThe V2 input is assembled from the output of modeled V1 complex cells\nat different positions and orientations. Receptive fields are represented\nby bars with size proportional to the connection strength to the complex\ncell with the respective position and orientation. (\\textbf{c}) Strabismic\nrearing is modeled as binocular stimuli with non-overlapping left\nand right eye input patches (red). (\\textbf{d-f}) Statistical distribution\n(log scale) of the input projected onto three different features for\nspeech (\\textbf{d}), V2 (\\textbf{e}) and strabismus (\\textbf{f}).\nIn all three cases, the learned receptive field (blue, inset) is characterized\nby a longer tailed distribution (arrows) than the random (red) and\ncomparative (green) features. (\\textbf{g-i})\\textbf{ }Relative optimization\nvalue for five nonlinearities (same as in Fig. \\ref{fig:winners}),\nfor the three selected patterns (\\textbf{insets}). The receptive fields\nlearned with the quadratic rectifier nonlinearity ($\\theta_{1}=1.$,\n$\\theta_{2}=2.$) are the maxima among the three patterns, for all\nfive nonlinearities, for all three datasets.}\n\\label{fig:othermodalities}\n\\end{figure}\n\n\n\n\\section*{Discussion}\n\nHistorically, a variety of models have been proposed to explain the\ndevelopment and distribution of receptive fields. We have shown that\nnonlinear Hebbian learning is a parsimonious principle which is implicitly\nor explicitly present in many developmental models \\citep{olshausen_emergence_1996,bell_independent_1997,law_formation_1994,Rehn_Sommer_2007,clopath_connectivity_2010,Savin_Joshi_Triesch_2010,zylberberg_sparse_2011,pfister_triplets_2006,hyvarinen_independent_1998,foldiak_forming_1990,Hunt_Dayan_Goodhill_2013}.\nThe fact that receptive field development is robust to the specific\nnonlinearity highlights a functional relation between different models.\nIt also unifies feature learning across sensory modalities: receptive\nfields form around features with a long-tailed distribution.\n\n\n\\subsection*{Relation to previous studies}\n\nEarlier studies have already placed developmental models side by side,\ncomparing their normative assumptions, algorithmic implementation\nor receptive fields developed. Though consistent with their findings,\nour results lead to revised interpretations and predictions.\n\nThe similarities between sparse coding and ICA are clear from their\nnormative correspondence \\citep{olshausen_sparse_1997}. Nevertheless,\nthe additional constraint in ICA, of having at most as many features\nas inputs, makes it an easier problem to solve, allowing for a range\nof suitable algorithms \\citep{hyvarinen_independent_2000}. These differ\nfrom algorithms derived for sparse coding, in which the inference\nstep is difficult due to overcompleteness. We have shown that regardless\nof the specific normative assumptions, it is the common implementation\nof nonlinear Hebbian learning that explains similarities in their\nlearning properties. \n\nIn contrast to the idea that in sparse coding algorithms overcompleteness\nis required for development of localized oriented edges \\citep{olshausen_sparse_1997},\nwe have demonstrated that a sparse coding model with a single neuron\nis mathematically equivalent to nonlinear Hebbian learning and learns\nlocalized filters in a setting that is clearly \\textquotedbl{}undercomplete\\textquotedbl{}.\nThus differences observed in receptive field shapes between sparse\ncoding and ICA models \\citep{Ringach_2002} are likely due to differences\nin network size and input preprocessing. For instance, the original\nsparse coding model \\citep{olshausen_sparse_1997} applied a preprocessing\nfilter that did not completely whiten the input, leading to larger\nreceptive fields (Fig. \\ref{fig:nonwhitened}).\n\nStudies that derive spiking models from normative theories often interpret\nthe development of oriented receptive fields as a consequence of its\nnormative assumptions \\citep{Savin_Joshi_Triesch_2010,zylberberg_sparse_2011}.\nIn a recent example, a spiking network has been related to the sparse\ncoding model \\citep{zylberberg_sparse_2011}, using neural properties\ndefined ad hoc. Our results suggest that many other choices of neural\nactivations would have given qualitatively similar receptive fields,\nindependent of the sparse coding assumption. While in sparse coding\nthe effective nonlinearity derives from a linear plasticity rule combined\nwith a nonlinear f-I curve, our results indicate that a nonlinear\nplasticity rule combined with a linear neuron model would give the\nsame outcome.\n\nIn order to distinguish between different normative assumptions, or\nparticular neural implementations, the observation of \\textquotedbl{}oriented\nfilters\\textquotedbl{} is not sufficient and additional constraints\nare needed. Similarly receptive shape diversity, another important\nexperimental constraint, should also be considered with care, since\nit cannot easily distinguish between models either. Studies that confront\nthe receptive field diversity of a model to experimental data \\citep{Rehn_Sommer_2007,zylberberg_sparse_2011,Ringach_2002}\nshould also take into account input preprocessing choices and how\nthe shape changes with an increasing network size, since we have observed\nthat these aspects may have a larger effect on receptive field shape\nthan the particulars of the learning model.\n\nEmpirical studies of alternative datasets, including abnormal visual\nrearing \\citep{Hunt_Dayan_Goodhill_2013}, tactile and auditory stimuli\n\\citep{Saxe_Bhand_Mudur_Suresh_Ng_2011}, have also observed that different\nunsupervised learning algorithms lead to comparable receptive fields\nshapes. Our results offer a plausible theoretical explanation for\nthese findings. \n\nPast investigations on nonlinear Hebbian learning \\citep{Fyfe_Baddeley_1995,hyvarinen_independent_1998}\ndemonstrated that many nonlinearities were capable of solving the\ncocktail party problem. Since it is a specific toy model, that asks\nfor the unmixing of linearly mixed independent features, it is not\nclear a priori whether the same conclusions would hold in other settings.\nWe have shown that the results of \\citet{Fyfe_Baddeley_1995} and \\citet{hyvarinen_independent_1998}\ngeneralize in two directions. First, the effective nonlinear Hebbian\nlearning mechanism is also behind other models beyond ICA, such as\nsparse coding models and plastic spiking networks. Second, the robustness\nto the choice of nonlinearity is not limited to a toy example, but\nalso holds in multiple real world data. Together, these insights explain\nand predict the outcome of many developmental models, in diverse applications.\n\n\n\\subsection*{Robustness to normative assumptions}\n\nMany theoretical studies start from normative assumptions \\citep{bell_independent_1997,Rehn_Sommer_2007,Savin_Joshi_Triesch_2010,olshausen_sparse_1997},\nsuch as a statistical model of the sensory input or a functional objective,\nand derive neural and synaptic dynamics from them. Our claim of universality\nof feature learning indicates that details of normative assumptions\nmay be of lower importance.\n\nFor instance, in sparse coding one assumes features with a specific\nstatistical prior \\citep{Rehn_Sommer_2007,olshausen_sparse_1997}.\nAfter learning, this prior is expected to match the posterior distribution\nof the neuron's firing activity \\citep{Rehn_Sommer_2007,olshausen_sparse_1997}.\nNevertheless, we have shown that receptive field learning is largely\nunaffected by the choice of prior. Thus, one cannot claim that the\nfeatures were learned because they match the assumed prior distribution,\nand indeed in general they do not. For a coherent statistical interpretation,\none could search for a prior that would match the feature statistics.\nHowever, since the outcome of learning is largely unaffected by the\nchoice of prior, such a statistical approach would have limited predictive\npower. Generally, kurtotic prior assumptions enable feature learning,\nbut the specific priors are not as decisive as one might expect. Because\nnormative approaches have assumptions, such as independence of hidden\nfeatures, that are not generally satisfied by the data they are applied\nto, the actual algorithm that is used for optimization becomes more\ncritical than the formal statistical model.\n\nThe concept of sparseness of neural activity is used with two distinct\nmeanings. The first one is a single-neuron concept and specifically\nrefers to the long-tailed distribution statistics of neural activity,\nindicating a \\textquotedbl{}kurtotic\\textquotedbl{} distribution.\nThe second notion of sparseness is an ensemble concept and refers\nto the very low firing rate of neurons, observed in cortical activity\n\\citep{barth_experimental_2012}, which may arise from lateral competition\nin overcomplete representations. Overcompleteness of ensembles makes\nsparse coding different from ICA \\citep{olshausen_sparse_1997}. We\nhave shown here that competition between multiple neurons is fundamental\nfor receptive field diversity, whereas it is not required for simple\ncell formation per se. Kurtotic features can be learned even by a\nsingle neuron with nonlinear Hebbian learning, and with no restrictions\non the sparseness of its firing activity. \n\n\n\\subsection*{Interaction of selectivity with preprocessing and homeostasis}\n\nThe concept of nonlinear Hebbian learning also clarifies the interaction\nof feature selectivity with preprocessing mechanisms. We have assumed\nwhitened data throughout the study, except Fig. \\ref{fig:nonwhitened}.\nSince after whitening second-order correlations are uninformative,\nneurons can develop sensitivity to higher order features. While whitened\ndata is formally not required for our analysis, second-order correlations\nmay dominate the optimization for non-white input, so that principal\ncomponents will be learned (Fig. \\ref{fig:nonwhitened}a).\nOnly when multiple neurons are added and receptive fields diversify,\nare localized simple cells formed with an input that is not completely\nwhite \\citep{olshausen_sparse_1997} (Fig. \\ref{fig:nonwhitened}c). \n\nIn studies of spiking networks, the input is restricted to positive\nrates, possibly through an on/off representation, as observed in the\nLGN \\citep{Miller_1994}. While the center-surround properties of LGN\ncontributes to a partial decorrelation of neuronal activity \\citep{Dan_Atick_Reid_1996},\nin such alternative representations, trivial receptive fields may\ndevelop, such as a single non-zero synapse, and additional mechanisms,\nsuch as hard bounds on each synaptic strength, $a\\le w_{j}\\le b$,\nmay be necessary to restrict the optimization space to desirable features\n\\citep{clopath_connectivity_2010}. \n\nInstead of constraining the synaptic weights, one may implement a\nsynaptic decay as in Oja's plasticity rule \\citep{Oja_1982}, $\\Delta w\\propto x\\cdot y-w\\cdot y^{2}$\n(see also \\citep{Chen_Lonjers_Lee_Chistiakova_Volgushev_Bazhenov_2013}).\nBecause of its multiplicative effect, the decay term does not alter\nthe receptive field, but only scales its strength. Thus, it is equivalent\nto rescaling the input in the f-I curve, so as to shift it to the\nappropriate range (Fig. \\ref{fig:selectivity}). Similar scaling\neffects arise from f-I changes due to intrinsic plasticity \\citep{Savin_Joshi_Triesch_2010,turrigiano_too_2011,Elliott_2014}.\nThe precise relation between nonlinear Hebbian learning, spiking representations\nand homeostasis in the cortex is an important topic for further studies. \n\n\n\\subsection*{Universality supports biological instantiation}\n\nThe principle of nonlinear Hebbian learning has a direct correspondence\nto biological neurons and is compatible with a large variety of plasticity\nmechanisms. It is not uncommon for biological systems to have diverse\nimplementations with comparable functional properties \\citep{Prinz_Bucher_Marder_2004}.\nDifferent species, or brain areas, could have different neural and\nplasticity characteristics, and still have similar feature learning\nproperties \\citep{Sharma_Angelucci_Sur_2000,Kaschube_Schnabel_Lowel_Coppola_White_Wolf_2010}.\nThe generality of the results discussed in this paper reveals learning\nsimple cell-like receptive fields from natural images to be much easier\nthan previously thought. It implies that a biological interpretation\nof models is possible even if some aspects of a model appear simplified\nor even wrong in some biological aspects. Universality also implies\nthat the study of receptive field development is not sufficient to\ndistinguish between different models.\n\n\n\n\nThe relation of nonlinear Hebbian learning to projection pursuit endorses\nthe interpretation of cortical plasticity as an optimization process.\nUnder the rate coding assumptions considered here, the crucial property\nis an effective synaptic change linear in the pre-synaptic rate, and\nnonlinear in the post-synaptic input. Pairing experiments with random\nfiring and independently varying pre- and post-synaptic rates would\nbe valuable to investigate these properties \\citep{Sjostrom_Turrigiano_Nelson_2001,Sjostrom_Rancz_Roth_Hausser_2008,Graupner_Brunel_2012}.\nAltogether, the robustness to details in both input modality and neural\nimplementation suggests nonlinear Hebbian learning as a fundamental\nprinciple underlying the development of sensory representations.\n\n\\section*{Methods}\n\n\\textbf{Spiking model.} A generalized leaky integrate-and-fire neuron\n\\citep{pozzorini_temporal_2013} was used as spiking model, which includes\npower-law spike-triggered adaptation and stochastic firing, with parameters\n\\citep{pozzorini_temporal_2013} fitted to pyramidal neurons. The f-I\ncurve $g(I)$ was estimated by injecting step currents and calculating\nthe trial average of the spike count over the first $500$ ms. The\nminimal triplet-STDP model\\citep{pfister_triplets_2006} was implemented,\nin which synaptic changes follow \n\n\n", "itemtype": "equation", "pos": 16087, "prevtext": "\n\n\nThis analysis reveals an equivalence between sparse coding models\nand neural networks with linear plasticity mechanisms, where the sparsity\nconstraint is determined by the f-I curve $g$.\n\nSimilarly, algorithms performing independent component analysis (ICA),\na model class closely related to sparse coding, also perform effective\nnonlinear Hebbian learning, albeit inversely, with linear neurons\nand a nonlinear plasticity rule \\citep{hyvarinen_independent_2000}.\nFor variants of ICA based on information maximization \\citep{bell_independent_1997}\nor kurtosis \\citep{hyvarinen_independent_2000} different nonlinearities\narise (Fig. \\ref{fig:winners}a), but Eq. \\ref{SCfgh}\napplies equally well. Hence, various instantiations of sparse coding\nand ICA models not only relate to each other in their normative assumptions\n\\citep{olshausen_sparse_1997}, but when implemented as iterative gradient\nupdate rules, they all employ nonlinear Hebbian learning.\n\n\n\\subsection*{Simple cell development for a large class of nonlinearities}\n\nSince the models described above can be implemented by similar plasticity\nrules, we hypothesized nonlinear Hebbian learning to be a general\nprinciple that explains the development of receptive field selectivity.\nNonlinear Hebbian learning with an effective nonlinearity $f$ is\nlinked to an optimization principle with a function $F=\\int f$ \\citep{Oja_Ogawa_Wangviwattana_1991,Fyfe_Baddeley_1995}.\nFor an input ensemble $\\mathbf{x}$, optimality is achieved by weights\n$\\tilde{\\mathbf{w}}$ that maximize $\\langle F(\\tilde{\\mathbf{w}}{}^{T}\\mathbf{x})\\rangle$,\nwhere angular brackets denote the average over the input statistics.\nNonlinear Hebbian learning is a stochastic gradient ascent implementation\nof this optimization process, called projection pursuit \\citep{Friedman_1987,Oja_Ogawa_Wangviwattana_1991,Fyfe_Baddeley_1995}:\n\n\n", "index": 5, "text": "\\begin{equation}\n\\tilde{\\mathbf{w}}=max_{\\mathbf{w}}\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle\\implies\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ f(\\mathbf{w}^{T}\\mathbf{x})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\mathbf{w}}=max_{\\mathbf{w}}\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle%&#10;\\implies\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ f(\\mathbf{w}^{T}\\mathbf{x})\" display=\"block\"><mrow><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>x</mi><mi>\ud835\udc30</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc30</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>\u27f9</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow><mo>\u221d</mo><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc30</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\nwhere $y(t)$ and $x(t)$ are the post- and pre-synaptic spike trains,\nrespectively: $y(t)=\\sum_{f}\\delta(t-t^{f})$, where $t^{f}$ are\nthe firing times and $\\delta$ denotes the Dirac $\\delta$-function;\n$x(t)$ is a vector with components $x_{i}(t)=\\sum_{f}\\delta(t-t_{i}^{f})$,\nwhere $t_{i}^{f}$ are the firing times of pre-synaptic neuron $i$;\n$w$ is a vector comprising the synaptic weights $w_{i}$ connecting\na pre-synaptic neuron $i$ to a post-synaptic cell. $A^{+}=6.5\\cdot10^{-3}$\nand $A^{-}=5.3\\cdot10^{-3}$ are constants, and $\\bar{y}^{+}$, $\\bar{x}^{+}$\nand $\\bar{y}^{-}$ are moving averages, implemented by integration\n(e.g. $\\tau\\frac{\\partial\\bar{{y}}}{\\partial t}=-\\bar{y}+y$), with\ntime scales $114.0$ ms, $16.8$ ms and $33.7$ ms, respectively \\citep{pfister_triplets_2006}.\nFor estimating the nonlinearity $h(y)$ of the plasticity, pre- and\npost-synaptic spike trains were generated as Poisson processes, with\nthe pre-synaptic rate set to $20$ Hz.\n\nA linear rectifier $g(x)=a(x-b)_{+}$ was fitted to the f-I curve\nof the spiking neuron model by squared error optimization. Similarly,\na quadratic function $h(x)=a(x^{2}-bx)$ was fitted to the nonlinearity\nof the triplet STDP model. The combination of these two fitted functions\nwas plotted as fit for the effective nonlinearity $f(x)=h(g(x))$.\n\n\\textbf{Sparse coding analysis.} A sparse coding model, with $K$\nneurons $y_{1},\\dots,y_{K}$, has a nonlinear Hebbian learning formulation.\nThe sparse coding model minimizes a least square reconstruction error\nbetween the vector of inputs $\\mathbf{x}$ and the reconstruction\nvector $\\mathbf{W}\\mathbf{y},$ where $\\mathbf{W}=[\\mathbf{w}_{1}\\dots\\mathbf{w}_{K}]$,\nand $\\mathbf{y}=(y_{1},\\dots,y_{K})$ is the vector of neuronal activities,\nwith $y_{j}\\ge0$ for $1\\le j\\le K$. The total error $E$ combines\na sparsity constraint $S$ with weight $\\lambda$ and the reconstruction\nerror, $E=\\frac{1}{2}||\\mathbf{x}-\\mathbf{W}\\mathbf{y}||^{2}+\\lambda\\sum S(y_{k})$.\n$E$ has to be minimal, averaged across all input samples, under the\nconstraint $y_{j}\\ge0$ for all $j$. \n\nThe minimization problem is solved by a two-step procedure. In the\nfirst step, for each input sample, one minimizes $E$ with respect\nto all hidden units $y_{j}$\n\n", "itemtype": "equation", "pos": 49331, "prevtext": "\n\n\nMotivated by results from ICA theory \\citep{hyvarinen_independent_1998}\nand statistical properties of whitened natural images \\citep{Field_1994},\nwe selected diverse Hebbian nonlinearities $f$ (Fig.  \\ref{fig:winners}a)\nand calculated the corresponding optimization value $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor different features of interest that we consider as candidate RF\nshapes, with a whitened ensemble of patches extracted from natural\nimages as input (see Methods). These include a random connectivity\npattern, a non-local oriented edge (as in principal components of\nnatural images) and localized oriented edges (as in cat and monkey\nsimple cells in the visual cortex), shown in Fig. \\ref{fig:winners}b.\nThe relative value of $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nbetween one feature and another was remarkably consistent across various\nchoices of the nonlinearity $f$, with localized orientation-selective\nreceptive fields as maxima (Fig.  \\ref{fig:winners}b). Furthermore,\nwe also searched for the maxima through gradient ascent, so as to\nconfirm that the maxima are orientation selective (Fig.  \\ref{fig:winners}c,\nleft). Our results indicate that receptive field development of simple\ncells is mainly governed by the statistical properties of natural\nimages, while robust to specific model assumptions. \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[height=0.6\\textheight]{figures/fig_winners_v5.pdf}\n\\par\\end{centering}\n\n\\caption[Simple cell development from natural images regardless of specific\neffective Hebbian nonlinearity.]{\\textbf{Simple cell development from natural images regardless of specific\neffective Hebbian nonlinearity.} (\\textbf{a}) Effective nonlinearity\nof five common models (arbitrary units): quadratic rectifier (green,\nas in cortical and BCM models, $\\theta_{1}=1.$, $\\theta_{2}=2.$),\nlinear rectifier (dark blue, as in $L_{1}$ sparse coding or networks\nwith linear STDP, $\\theta=3.$), Cauchy sparse coding nonlinearity\n(light blue, $\\lambda=3.$), $L_{0}$ sparse coding nonlinearity (orange,\n$\\lambda=3.$), and negative sigmoid (purple, as in ICA models). (\\textbf{b})\nRelative optimization value $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor each of the five models in \\textbf{a}, for different preselected\nfeatures $\\mathbf{w}$, averaged over natural image patches \\textbf{$\\mathbf{x}$}.\nCandidate features are represented as two-dimensional receptive fields.\nFor all models, the optimum is achieved at the localized oriented\nreceptive field. Inset: Example of natural image and image patch (red\nsquare) used as sensory input. (\\textbf{c}) Receptive fields learned\nin four trials for ten effective Hebbian functions $f$ (from top:\nthe five functions considered above, $u^{3}$, $-sin(u)$, $u$, $(|u|-2)_{+}$,\n$-cos(u)$)\\textbf{ }(\\textbf{left} \\textbf{column}), and their opposites\n$-f$ (\\textbf{right column}). The first seven functions (above the\ndashed line) lead to localized oriented filters, while a sign-flip\nleads to random patterns. Linear or symmetric functions are exceptions\nand do not develop oriented filters (\\textbf{bottom} \\textbf{rows}).}\n\n\n\\label{fig:winners}\n\\end{figure}\n\n\nThe relevant property of natural image statistics is that the distribution\nof a feature derived from typical localized oriented patterns has\nhigh kurtosis \\citep{Field_1994,olshausen_emergence_1996,ruderman_statistics_1994}.\nThus to establish a quantitative measure whether a nonlinearity is\nsuitable for feature learning, we define a \\emph{selectivity index}\n(\\emph{SI}), which measures the relative value of $\\langle F(.)\\rangle$\nbetween a variable $l$ with a Laplacian distribution and a variable\n$g$ with Gaussian distribution \\citep{hyvarinen_independent_1998}:\n$SI=(\\langle F(l)\\rangle-\\langle F(g)\\rangle)/\\sigma_{F}$ (see Methods).\nThe Laplacian variable has higher kurtosis than the Gaussian variable,\nserving as a prototype of a kurtotic distribution. Since values obtained\nby filtering natural images with localized oriented patterns have\na distribution with longer tails than other patterns \\citep{Field_1994},\nas does the Laplacian variable compared to the Gaussian, positive\nvalues $SI>0$ indicate good candidate functions for learning simple\ncell-like receptive fields from natural images. We find that each\nmodel has an appropriate parameter range where $SI>0$ (Fig.\n \\ref{fig:selectivity}). For example the quadratic rectifier nonlinearity\nneeds an LTP threshold $\\theta_{2}$ below some critical level, so\nas to be useful for feature learning (Fig. \\ref{fig:selectivity}a).\n\nA sigmoidal function with threshold at zero has \\emph{negative SI},\nbut a \\emph{negative} sigmoid, as used in ICA studies \\citep{bell_independent_1997},\nhas $SI>0$. More generally, whenever an effective nonlinearity $f$\nis not suited for feature learning, its opposite $-f$ should be,\nsince its $SI$ will have the opposite sign (Fig. \\ref{fig:winners}c).\nThis implies that, in general, half of the function space could be\nsuitable for feature learning \\citep{hyvarinen_independent_1998},\ni.e. it finds weights $w$ such that the distribution of the feature\n$\\mathbf{w}^{T}\\mathbf{x}$ has a long tail, indicating high kurtosis\n(\\textquotedbl{}kurtotic feature\\textquotedbl{}). The other half of\nthe function space learns the least kurtotic features (e.g. random\nconnectivity patterns for natural images, Fig. \\ref{fig:winners}b,c). \n\nThis universality strongly constrains the possible shape of receptive\nfields that may arise during development for a given input dataset.\nFor whitened natural images, a learnable receptive field is in general\neither a localized edge detector or a non-localized random connectivity\npattern. \n\n\n\n\nAn important special case is an effective linear curve, $f(u)=u$,\nwhich arises when both f-I and plasticity curves are linear \\citep{Miller_Keller_Stryker_1989}.\nBecause the linear model maximizes variance $\\langle(\\mathbf{w}^{T}\\mathbf{x})^{2}\\rangle$,\nit can perform principal component analysis \\citep{Oja_1982}, but\ndoes not have any feature selectivity on whitened input datasets,\nwhere variance is constant (Fig. \\ref{fig:winners}c). \n\nSymmetric effective nonlinearities, $f(u)=f(-u)$, are also exceptions,\nsince their corresponding optimization functions are asymmetric, $F(u)=-F(-u)$,\nso that for datasets with symmetric statistical distributions, $P(\\mathbf{x})=P(-\\mathbf{x})$,\nthe optimization value will be zero, $\\langle F_{asym.}(\\mathbf{w}^{T}\\mathbf{x}_{sym.})\\rangle=0$.\nAs natural images are not completely symmetric, localized receptive\nfields do develop, though without orientation selectivity, as illustrated\nby a cosine function and a symmetric piece-wise linear function as\neffective nonlinearities (Fig. \\ref{fig:winners}c, bottom\nrows).\n\n\\begin{figure}[!htbp]\n\\centering{}\\includegraphics{figures/fig_selectivity.pdf}\n\\caption[Selectivity index for different effective nonlinearities.]\n{\\textbf{Selectivity index for different effective nonlinearities.} (\\textbf{a})\nQuadratic rectifier (small graphic, three examples with different\nLTP thresholds) with LTD threshold at $\\theta_{1}=1$: LTP threshold\nmust be below $3.5$ to secure positive selectivity index (green region,\nmain Fig) and learn localized oriented receptive fields (inset). A\nnegative selectivity index (red region) leads to a random connectivity\npattern (inset) (\\textbf{b}) Linear rectifier: activation threshold\nmust be above zero. (\\textbf{c}) Sigmoid: center must be below $a=-1.2$\nor, for a stronger effect, above $a=+1.2$. The opposite conditions\napply to the negative sigmoid. (\\textbf{d}) Cauchy sparse coding nonlinearity:\npositive but weak feature selectivity for any sparseness penalty $\\lambda>0$.\nInsets show the nonlinearities for different choices of parameters. }\n\\label{fig:selectivity}\n\\end{figure}\n\n\n\n\\subsection*{Receptive field diversity}\n\nSensory neurons display a variety of receptive field shapes \\citep{Ringach_2002},\nand recent modeling efforts \\citep{Rehn_Sommer_2007,zylberberg_sparse_2011}\nhave attempted to understand the properties that give rise to the\nspecific receptive fields seen in experiments. We show here that the\nshape diversity of a model can be predicted by our projection pursuit\nanalysis, and is primarily determined by the statistics of input representation,\nwhile relatively robust to the specific effective nonlinearity.\n\nWe studied a model with multiple neurons in the second layer, which\ncompete with each other for the representation of specific features\nof the input. Each neuron had a piece-wise linear f-I curve and a\nquadratic rectifier plasticity function (see Methods) and projected\ninhibitory connections $v$ onto all others. These inhibitory connections\nare learned by anti-Hebbian plasticity and enforce decorrelation of\nneurons, so that receptive fields represent different positions, orientations\nand shapes \\citep{foldiak_forming_1990,vogels_inhibitory_2011,King_Zylberberg_DeWeese_2013}.\nFor 50 neurons, the resulting receptive fields became diversified\n(Fig. \\ref{fig:gabormap}a-c, colored dots). In an overcomplete\nnetwork of 1000 neurons, the diversity further increased (Fig. \\ref{fig:gabormap}d-f, colored dots). \n\nFor the analysis of the simulation results, we refined our inspection\nof optimal oriented receptive fields for natural images by numerical\nevaluation of the optimality criterion $\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$\nfor receptive fields \\textbf{$\\mathbf{w}=\\mathbf{w}_{Gabor}$}, described\nas Gabor functions of variable length, width and spatial frequency.\nFor all tested nonlinearities, the optimization function for single-neuron\nreceptive fields varies smoothly with these parameters (Fig\n\\ref{fig:gabormap}, grey-shaded background). The single-neuron optimality\nlandscape was then used to analyze the multi-neuron simulation results.\nWe found that receptive fields are located in the area where the single-neuron\noptimality criterion is near its maximum, but spread out so as to\nrepresent different features of the input (Fig. \\ref{fig:gabormap}).\nThus the map of optimization values, calculated from the theory of\neffective nonlinearity, enables us to qualitatively predict the shape\ndiversity of receptive fields. \n\nAlthough qualitatively similar, there are differences in the receptive\nfields developed for each model, such as smaller lengths for the $L_{0}$\nsparse coding model (Fig. \\ref{fig:gabormap}c). While potentially\nsignificant, these differences across models may be overwhelmed by\ndifferences due to other model properties, including different network\nsizes or input representations. This is illustrated by observing that\nreceptive field diversity for a given model differ substantially across\nnetwork sizes (Fig. \\ref{fig:gabormap}), and the difference\nis even greater from simulations with an input that is not completely\nwhite (Fig. \\ref{fig:nonwhitened}c). Thus our results suggests\nthat efforts to model receptive field shapes observed experimentally\n\\citep{Ringach_2002,Rehn_Sommer_2007,zylberberg_sparse_2011} should\nfocus on network size and input representation, which potentially\nhave a stronger effect than the nonlinear properties of the specific\nmodel under consideration.\n\n\n\n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[width=12cm]{figures/fig_gabormap_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Optimal receptive field shapes in model networks induce diversity.]{\\textbf{Optimal receptive field shapes in model networks induce diversity.}\n(\\textbf{a-f}) Gray level indicates the optimization value for different\nlengths and widths (see inset in \\textbf{a}) of oriented receptive\nfields for natural images, for the quadratic rectifier (left, see\nFig.  \\ref{fig:winners}a), linear rectifier (middle) and\n$L_{0}$ sparse coding (right). Optima marked with a black cross.\n(\\textbf{a-c}) Colored circles indicate the receptive fields of different\nshapes developed in a network of 50 neurons with lateral inhibitory\nconnections. Insets on the right show example receptive fields developed\nduring simulation. (\\textbf{d-f}) Same for a network of 1000 neurons. }\n\\label{fig:gabormap}\n\\end{figure}\n\n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics{figures/fig_nonwhitened.pdf}\n\\par\\end{centering}\n\n\\caption[Receptive fields for non-whitened natural images.]{\\textbf{Receptive fields for non-whitened natural images.}Images were preprocessed\nas in the original sparse coding study \\citep{olshausen_sparse_1997}.\nWe simulated linear rectifier neurons ($\\theta=0.5)$ with a quadratic\nplasticity nonlinearity ($b=0.5$). (\\textbf{a}) Multiple-neuron simulations,\nwith 4 neurons. The principal components dominate the optimization\nand receptive fields are not local, since they extend over most of\nthe image patch.\\textbf{ }With 50 (\\textbf{b}) and 1000 (\\textbf{c})\nneurons, lateral inhibition promotes diversity, and more localized\nreceptive field are formed. (\\textbf{insets}) Sample receptive fields\ndeveloped for each simulation.}\n \\label{fig:nonwhitened}\n\\end{figure}\n\n\n\n\n\nWe also studied the variation of receptive field position and orientation.\nFor all five nonlinearities considered, the optimization value is\nequal for different positions of the receptive field centers, confirming\nthe translation invariance in the image statistics, as long as the\nreceptive field is not too close to the border of the anatomically\nallowed fan-in of synaptic connections (Fig. \\ref{fig:position}b).\nAlso, all nonlinearities reveal the same bias towards the horizontal\nand vertical orientations (Fig. \\ref{fig:position}c). These\noptimality predictions are confirmed in single neuron simulations,\nwhich lead mostly to either horizontal or vertical orientations, at\nrandom positions (Fig. \\ref{fig:position}d). When the network\nis expanded to 50 neurons, recurrent inhibition forces receptive fields\nto cover different positions, though excluding border positions, and\nsome neurons have non-cardinal orientations (Fig. \\ref{fig:position}e).\nWith 1000 neurons, receptive fields diversify to many possible combinations\nof position, orientation and length (Fig. \\ref{fig:position}f). \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics{figures/fig_position_spread_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Diversity of receptive field size, position and orientation.]{\\textbf{Diversity of receptive field size, position and orientation.} (\\textbf{a})\nThe optimization value of localized oriented receptive fields, within\na 16x16 pixel patch of sensors, as a function of size (see Methods),\nfor five nonlinearities (colors as in Fig. \\ref{fig:winners}a).\nOptimal size is a receptive field of width around 3 to 4 pixels (filled\ntriangles). (\\textbf{b}) The optimization value as a function of position\nof the receptive field center, for a receptive field width of 4 pixels,\nindicates invariance to position within the 16x16 patch, except near\nthe borders. (\\textbf{c}) The optimization value as a function of\norientation shows preference toward horizontal and vertical directions,\nfor all five nonlinearities.\\textbf{ }(\\textbf{d}) Receptive field\nposition, orientation and length (colored bars) learned for 50 single-neuron\ntrials. The color code indicates different orientations. (\\textbf{e})\nReceptive field positions and orientations learned in a 50 neuron\nnetwork reveal diversification of positions, except at the borders.\n(\\textbf{f}) With 1000 neurons, positions and orientations cover the\nfull range of combinations (top). Selecting 50 randomly chosen receptive\nfields highlights the diversification of position, orientation and\nsize (bottom). Receptive fields were learned through the quadratic\nrectifier nonlinearity ($\\theta_{1}=1.$, $\\theta_{2}=2.$).}\n\n\n\\label{fig:position}\n\\end{figure}\n\n\n\n\\subsection*{Beyond V1 simple cells}\n\nNonlinear Hebbian learning is not limited to explaining simple cells\nin V1. We investigated if the same learning principles apply to receptive\nfield development in other visual or auditory areas or under different\nrearing conditions. \n\nFor auditory neurons \\citep{Smith_Lewicki_2006}, we used segments\nof speech as input (Fig. \\ref{fig:othermodalities}a) and\nobserved the development of spectrotemporal receptive fields localized\nin both frequency and time \\citep{Miller_Escabi_Read_Schreiner_2002}\n(Fig. \\ref{fig:othermodalities}d). The statistical distribution\nof input patterns aligned with the learned receptive fields had longer\ntails than for random or non-local receptive fields, indicating temporal\nsparsity of responses (Fig. \\ref{fig:othermodalities}d).\nSimilar to our simple cell results, the learned receptive fields show\nhigher optimization value for all five effective nonlinearities (Fig\n\\ref{fig:othermodalities}g).\n\nFor a study of receptive field development in the secondary visual\ncortex (V2)  \\citep{Lee_Ekanadham_Ng_2007}, we used natural images\nand the standard energy model \\citep{Hyvarinen_Hurri_Hoyer_2009} of\nV1 complex cells to generate input to V2 (Fig. \\ref{fig:othermodalities}b).\nThe learned receptive field was selective to a single orientation\nover neighboring positions, indicating a higher level of translation\ninvariance. When inputs were processed with this receptive field,\nwe found longer tails in the feature distribution than with random\nfeatures or receptive fields without orientation coherence (Fig\n\\ref{fig:othermodalities}e), and the learned receptive field had\na higher optimization value for all choices of nonlinearity (Fig\n\\ref{fig:othermodalities}h).\n\nAnother important constraint for developmental models are characteristic\ndeviations, such as strabismus, caused by abnormal sensory rearing.\nUnder normal binocular rearing conditions, the fan-in of synaptic\ninput from the left and right eyes overlap in visual space (Fig\n\\ref{fig:othermodalities}c). In this case, binocular receptive fields\nwith similar features for left and right eyes develop. In the strabismic\ncondition, the left and right eyes are not aligned, modeled as binocular\nrearing with non-overlapping input from each eye (Fig. \\ref{fig:othermodalities}c).\nIn this scenario, a monocular simple cell-like receptive field developed\n(Fig. \\ref{fig:othermodalities}f), as observed in experiments\nand earlier models \\citep{Cooper_Intrator_Blais_Shouval_2004}. The\nstatistical distributions confirm that for disparate inputs the monocular\nreceptive field is more kurtotic than a binocular one, explaining\nits formation in diverse models \\citep{Hunt_Dayan_Goodhill_2013} (Fig\n\\ref{fig:othermodalities}f,i).\n\nOur results demonstrate the generality of the theory across multiple\ncortical areas. Selecting a relevant feature space for an extensive\nanalysis, as we have done with simple cells and natural images, may\nnot be possible in general. Nonetheless, nonlinear Hebbian learning\nhelps to explain why some features (and not others) are learnable\nin network models \\citep{Saxe_Bhand_Mudur_Suresh_Ng_2011}. \n\n\\begin{figure}[!htbp]\n\\begin{centering}\n\\includegraphics[height=0.6\\textheight]{figures/fig_otherdata_v2.pdf}\n\\par\\end{centering}\n\n\\caption[Nonlinear Hebbian learning across sensory modalities.]{\\textbf{Nonlinear Hebbian learning across sensory modalities.} (\\textbf{a})\nThe auditory input is modeled as segments over time and frequency\n(red) of the spectrotemporal representation of speech signals. (\\textbf{b})\nThe V2 input is assembled from the output of modeled V1 complex cells\nat different positions and orientations. Receptive fields are represented\nby bars with size proportional to the connection strength to the complex\ncell with the respective position and orientation. (\\textbf{c}) Strabismic\nrearing is modeled as binocular stimuli with non-overlapping left\nand right eye input patches (red). (\\textbf{d-f}) Statistical distribution\n(log scale) of the input projected onto three different features for\nspeech (\\textbf{d}), V2 (\\textbf{e}) and strabismus (\\textbf{f}).\nIn all three cases, the learned receptive field (blue, inset) is characterized\nby a longer tailed distribution (arrows) than the random (red) and\ncomparative (green) features. (\\textbf{g-i})\\textbf{ }Relative optimization\nvalue for five nonlinearities (same as in Fig. \\ref{fig:winners}),\nfor the three selected patterns (\\textbf{insets}). The receptive fields\nlearned with the quadratic rectifier nonlinearity ($\\theta_{1}=1.$,\n$\\theta_{2}=2.$) are the maxima among the three patterns, for all\nfive nonlinearities, for all three datasets.}\n\\label{fig:othermodalities}\n\\end{figure}\n\n\n\n\\section*{Discussion}\n\nHistorically, a variety of models have been proposed to explain the\ndevelopment and distribution of receptive fields. We have shown that\nnonlinear Hebbian learning is a parsimonious principle which is implicitly\nor explicitly present in many developmental models \\citep{olshausen_emergence_1996,bell_independent_1997,law_formation_1994,Rehn_Sommer_2007,clopath_connectivity_2010,Savin_Joshi_Triesch_2010,zylberberg_sparse_2011,pfister_triplets_2006,hyvarinen_independent_1998,foldiak_forming_1990,Hunt_Dayan_Goodhill_2013}.\nThe fact that receptive field development is robust to the specific\nnonlinearity highlights a functional relation between different models.\nIt also unifies feature learning across sensory modalities: receptive\nfields form around features with a long-tailed distribution.\n\n\n\\subsection*{Relation to previous studies}\n\nEarlier studies have already placed developmental models side by side,\ncomparing their normative assumptions, algorithmic implementation\nor receptive fields developed. Though consistent with their findings,\nour results lead to revised interpretations and predictions.\n\nThe similarities between sparse coding and ICA are clear from their\nnormative correspondence \\citep{olshausen_sparse_1997}. Nevertheless,\nthe additional constraint in ICA, of having at most as many features\nas inputs, makes it an easier problem to solve, allowing for a range\nof suitable algorithms \\citep{hyvarinen_independent_2000}. These differ\nfrom algorithms derived for sparse coding, in which the inference\nstep is difficult due to overcompleteness. We have shown that regardless\nof the specific normative assumptions, it is the common implementation\nof nonlinear Hebbian learning that explains similarities in their\nlearning properties. \n\nIn contrast to the idea that in sparse coding algorithms overcompleteness\nis required for development of localized oriented edges \\citep{olshausen_sparse_1997},\nwe have demonstrated that a sparse coding model with a single neuron\nis mathematically equivalent to nonlinear Hebbian learning and learns\nlocalized filters in a setting that is clearly \\textquotedbl{}undercomplete\\textquotedbl{}.\nThus differences observed in receptive field shapes between sparse\ncoding and ICA models \\citep{Ringach_2002} are likely due to differences\nin network size and input preprocessing. For instance, the original\nsparse coding model \\citep{olshausen_sparse_1997} applied a preprocessing\nfilter that did not completely whiten the input, leading to larger\nreceptive fields (Fig. \\ref{fig:nonwhitened}).\n\nStudies that derive spiking models from normative theories often interpret\nthe development of oriented receptive fields as a consequence of its\nnormative assumptions \\citep{Savin_Joshi_Triesch_2010,zylberberg_sparse_2011}.\nIn a recent example, a spiking network has been related to the sparse\ncoding model \\citep{zylberberg_sparse_2011}, using neural properties\ndefined ad hoc. Our results suggest that many other choices of neural\nactivations would have given qualitatively similar receptive fields,\nindependent of the sparse coding assumption. While in sparse coding\nthe effective nonlinearity derives from a linear plasticity rule combined\nwith a nonlinear f-I curve, our results indicate that a nonlinear\nplasticity rule combined with a linear neuron model would give the\nsame outcome.\n\nIn order to distinguish between different normative assumptions, or\nparticular neural implementations, the observation of \\textquotedbl{}oriented\nfilters\\textquotedbl{} is not sufficient and additional constraints\nare needed. Similarly receptive shape diversity, another important\nexperimental constraint, should also be considered with care, since\nit cannot easily distinguish between models either. Studies that confront\nthe receptive field diversity of a model to experimental data \\citep{Rehn_Sommer_2007,zylberberg_sparse_2011,Ringach_2002}\nshould also take into account input preprocessing choices and how\nthe shape changes with an increasing network size, since we have observed\nthat these aspects may have a larger effect on receptive field shape\nthan the particulars of the learning model.\n\nEmpirical studies of alternative datasets, including abnormal visual\nrearing \\citep{Hunt_Dayan_Goodhill_2013}, tactile and auditory stimuli\n\\citep{Saxe_Bhand_Mudur_Suresh_Ng_2011}, have also observed that different\nunsupervised learning algorithms lead to comparable receptive fields\nshapes. Our results offer a plausible theoretical explanation for\nthese findings. \n\nPast investigations on nonlinear Hebbian learning \\citep{Fyfe_Baddeley_1995,hyvarinen_independent_1998}\ndemonstrated that many nonlinearities were capable of solving the\ncocktail party problem. Since it is a specific toy model, that asks\nfor the unmixing of linearly mixed independent features, it is not\nclear a priori whether the same conclusions would hold in other settings.\nWe have shown that the results of \\citet{Fyfe_Baddeley_1995} and \\citet{hyvarinen_independent_1998}\ngeneralize in two directions. First, the effective nonlinear Hebbian\nlearning mechanism is also behind other models beyond ICA, such as\nsparse coding models and plastic spiking networks. Second, the robustness\nto the choice of nonlinearity is not limited to a toy example, but\nalso holds in multiple real world data. Together, these insights explain\nand predict the outcome of many developmental models, in diverse applications.\n\n\n\\subsection*{Robustness to normative assumptions}\n\nMany theoretical studies start from normative assumptions \\citep{bell_independent_1997,Rehn_Sommer_2007,Savin_Joshi_Triesch_2010,olshausen_sparse_1997},\nsuch as a statistical model of the sensory input or a functional objective,\nand derive neural and synaptic dynamics from them. Our claim of universality\nof feature learning indicates that details of normative assumptions\nmay be of lower importance.\n\nFor instance, in sparse coding one assumes features with a specific\nstatistical prior \\citep{Rehn_Sommer_2007,olshausen_sparse_1997}.\nAfter learning, this prior is expected to match the posterior distribution\nof the neuron's firing activity \\citep{Rehn_Sommer_2007,olshausen_sparse_1997}.\nNevertheless, we have shown that receptive field learning is largely\nunaffected by the choice of prior. Thus, one cannot claim that the\nfeatures were learned because they match the assumed prior distribution,\nand indeed in general they do not. For a coherent statistical interpretation,\none could search for a prior that would match the feature statistics.\nHowever, since the outcome of learning is largely unaffected by the\nchoice of prior, such a statistical approach would have limited predictive\npower. Generally, kurtotic prior assumptions enable feature learning,\nbut the specific priors are not as decisive as one might expect. Because\nnormative approaches have assumptions, such as independence of hidden\nfeatures, that are not generally satisfied by the data they are applied\nto, the actual algorithm that is used for optimization becomes more\ncritical than the formal statistical model.\n\nThe concept of sparseness of neural activity is used with two distinct\nmeanings. The first one is a single-neuron concept and specifically\nrefers to the long-tailed distribution statistics of neural activity,\nindicating a \\textquotedbl{}kurtotic\\textquotedbl{} distribution.\nThe second notion of sparseness is an ensemble concept and refers\nto the very low firing rate of neurons, observed in cortical activity\n\\citep{barth_experimental_2012}, which may arise from lateral competition\nin overcomplete representations. Overcompleteness of ensembles makes\nsparse coding different from ICA \\citep{olshausen_sparse_1997}. We\nhave shown here that competition between multiple neurons is fundamental\nfor receptive field diversity, whereas it is not required for simple\ncell formation per se. Kurtotic features can be learned even by a\nsingle neuron with nonlinear Hebbian learning, and with no restrictions\non the sparseness of its firing activity. \n\n\n\\subsection*{Interaction of selectivity with preprocessing and homeostasis}\n\nThe concept of nonlinear Hebbian learning also clarifies the interaction\nof feature selectivity with preprocessing mechanisms. We have assumed\nwhitened data throughout the study, except Fig. \\ref{fig:nonwhitened}.\nSince after whitening second-order correlations are uninformative,\nneurons can develop sensitivity to higher order features. While whitened\ndata is formally not required for our analysis, second-order correlations\nmay dominate the optimization for non-white input, so that principal\ncomponents will be learned (Fig. \\ref{fig:nonwhitened}a).\nOnly when multiple neurons are added and receptive fields diversify,\nare localized simple cells formed with an input that is not completely\nwhite \\citep{olshausen_sparse_1997} (Fig. \\ref{fig:nonwhitened}c). \n\nIn studies of spiking networks, the input is restricted to positive\nrates, possibly through an on/off representation, as observed in the\nLGN \\citep{Miller_1994}. While the center-surround properties of LGN\ncontributes to a partial decorrelation of neuronal activity \\citep{Dan_Atick_Reid_1996},\nin such alternative representations, trivial receptive fields may\ndevelop, such as a single non-zero synapse, and additional mechanisms,\nsuch as hard bounds on each synaptic strength, $a\\le w_{j}\\le b$,\nmay be necessary to restrict the optimization space to desirable features\n\\citep{clopath_connectivity_2010}. \n\nInstead of constraining the synaptic weights, one may implement a\nsynaptic decay as in Oja's plasticity rule \\citep{Oja_1982}, $\\Delta w\\propto x\\cdot y-w\\cdot y^{2}$\n(see also \\citep{Chen_Lonjers_Lee_Chistiakova_Volgushev_Bazhenov_2013}).\nBecause of its multiplicative effect, the decay term does not alter\nthe receptive field, but only scales its strength. Thus, it is equivalent\nto rescaling the input in the f-I curve, so as to shift it to the\nappropriate range (Fig. \\ref{fig:selectivity}). Similar scaling\neffects arise from f-I changes due to intrinsic plasticity \\citep{Savin_Joshi_Triesch_2010,turrigiano_too_2011,Elliott_2014}.\nThe precise relation between nonlinear Hebbian learning, spiking representations\nand homeostasis in the cortex is an important topic for further studies. \n\n\n\\subsection*{Universality supports biological instantiation}\n\nThe principle of nonlinear Hebbian learning has a direct correspondence\nto biological neurons and is compatible with a large variety of plasticity\nmechanisms. It is not uncommon for biological systems to have diverse\nimplementations with comparable functional properties \\citep{Prinz_Bucher_Marder_2004}.\nDifferent species, or brain areas, could have different neural and\nplasticity characteristics, and still have similar feature learning\nproperties \\citep{Sharma_Angelucci_Sur_2000,Kaschube_Schnabel_Lowel_Coppola_White_Wolf_2010}.\nThe generality of the results discussed in this paper reveals learning\nsimple cell-like receptive fields from natural images to be much easier\nthan previously thought. It implies that a biological interpretation\nof models is possible even if some aspects of a model appear simplified\nor even wrong in some biological aspects. Universality also implies\nthat the study of receptive field development is not sufficient to\ndistinguish between different models.\n\n\n\n\nThe relation of nonlinear Hebbian learning to projection pursuit endorses\nthe interpretation of cortical plasticity as an optimization process.\nUnder the rate coding assumptions considered here, the crucial property\nis an effective synaptic change linear in the pre-synaptic rate, and\nnonlinear in the post-synaptic input. Pairing experiments with random\nfiring and independently varying pre- and post-synaptic rates would\nbe valuable to investigate these properties \\citep{Sjostrom_Turrigiano_Nelson_2001,Sjostrom_Rancz_Roth_Hausser_2008,Graupner_Brunel_2012}.\nAltogether, the robustness to details in both input modality and neural\nimplementation suggests nonlinear Hebbian learning as a fundamental\nprinciple underlying the development of sensory representations.\n\n\\section*{Methods}\n\n\\textbf{Spiking model.} A generalized leaky integrate-and-fire neuron\n\\citep{pozzorini_temporal_2013} was used as spiking model, which includes\npower-law spike-triggered adaptation and stochastic firing, with parameters\n\\citep{pozzorini_temporal_2013} fitted to pyramidal neurons. The f-I\ncurve $g(I)$ was estimated by injecting step currents and calculating\nthe trial average of the spike count over the first $500$ ms. The\nminimal triplet-STDP model\\citep{pfister_triplets_2006} was implemented,\nin which synaptic changes follow \n\n\n", "index": 7, "text": "\\begin{equation}\n\\frac{d}{dt}w(t)=A^{+}y(t)\\bar{y}^{+}(t)\\bar{x}^{+}(t)-A^{-}x(t)\\bar{y}^{-}(t)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\frac{d}{dt}w(t)=A^{+}y(t)\\bar{y}^{+}(t)\\bar{x}^{+}(t)-A^{-}x(t)\\bar{y}^{-}(t)\" display=\"block\"><mrow><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>A</mi><mo>+</mo></msup><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>+</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>+</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msup><mi>A</mi><mo>-</mo></msup><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\nwhere we constrained the vector $\\mathbf{w}_{j}$ of synapses projecting\nonto unit $y_{j}$ by $||\\mathbf{w}_{j}||^{2}=1$, defined the activation\nfunction $g(.)=T^{-1}(.)$, the inverse of $T(y)=(y+\\lambda S'(y))$,\nand defined recurrent synaptic weights $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$.\nFor each input sample $\\mathbf{x}$, this equation shall be iterated\nuntil convergence. The equation can be interpreted as a recurrent\nneural network, where each neuron has an activation function $g$,\nand the input is given by the sum of the feedforward drive $\\mathbf{w}_{j}^{T}\\mathbf{x}$\nand a recurrent inhibition term $-\\sum_{k\\neq j}v_{jk}y_{k}$. To\navoid instability, we implement a smooth membrane potential $u_{j}$,\nwhich has the same convergence point \\citep{rozell_sparse_2008}\n\n", "itemtype": "equation", "pos": 51673, "prevtext": "\nwhere $y(t)$ and $x(t)$ are the post- and pre-synaptic spike trains,\nrespectively: $y(t)=\\sum_{f}\\delta(t-t^{f})$, where $t^{f}$ are\nthe firing times and $\\delta$ denotes the Dirac $\\delta$-function;\n$x(t)$ is a vector with components $x_{i}(t)=\\sum_{f}\\delta(t-t_{i}^{f})$,\nwhere $t_{i}^{f}$ are the firing times of pre-synaptic neuron $i$;\n$w$ is a vector comprising the synaptic weights $w_{i}$ connecting\na pre-synaptic neuron $i$ to a post-synaptic cell. $A^{+}=6.5\\cdot10^{-3}$\nand $A^{-}=5.3\\cdot10^{-3}$ are constants, and $\\bar{y}^{+}$, $\\bar{x}^{+}$\nand $\\bar{y}^{-}$ are moving averages, implemented by integration\n(e.g. $\\tau\\frac{\\partial\\bar{{y}}}{\\partial t}=-\\bar{y}+y$), with\ntime scales $114.0$ ms, $16.8$ ms and $33.7$ ms, respectively \\citep{pfister_triplets_2006}.\nFor estimating the nonlinearity $h(y)$ of the plasticity, pre- and\npost-synaptic spike trains were generated as Poisson processes, with\nthe pre-synaptic rate set to $20$ Hz.\n\nA linear rectifier $g(x)=a(x-b)_{+}$ was fitted to the f-I curve\nof the spiking neuron model by squared error optimization. Similarly,\na quadratic function $h(x)=a(x^{2}-bx)$ was fitted to the nonlinearity\nof the triplet STDP model. The combination of these two fitted functions\nwas plotted as fit for the effective nonlinearity $f(x)=h(g(x))$.\n\n\\textbf{Sparse coding analysis.} A sparse coding model, with $K$\nneurons $y_{1},\\dots,y_{K}$, has a nonlinear Hebbian learning formulation.\nThe sparse coding model minimizes a least square reconstruction error\nbetween the vector of inputs $\\mathbf{x}$ and the reconstruction\nvector $\\mathbf{W}\\mathbf{y},$ where $\\mathbf{W}=[\\mathbf{w}_{1}\\dots\\mathbf{w}_{K}]$,\nand $\\mathbf{y}=(y_{1},\\dots,y_{K})$ is the vector of neuronal activities,\nwith $y_{j}\\ge0$ for $1\\le j\\le K$. The total error $E$ combines\na sparsity constraint $S$ with weight $\\lambda$ and the reconstruction\nerror, $E=\\frac{1}{2}||\\mathbf{x}-\\mathbf{W}\\mathbf{y}||^{2}+\\lambda\\sum S(y_{k})$.\n$E$ has to be minimal, averaged across all input samples, under the\nconstraint $y_{j}\\ge0$ for all $j$. \n\nThe minimization problem is solved by a two-step procedure. In the\nfirst step, for each input sample, one minimizes $E$ with respect\nto all hidden units $y_{j}$\n\n", "index": 9, "text": "\\begin{equation}\n\\begin{aligned}\\frac{d}{dy_{j}}E=0 & \\iff\\mathbf{w}_{j}(\\mathbf{x}-\\mathbf{W}\\mathbf{y})-\\lambda S'(y_{j})=0\\\\\n & \\iff\\mathbf{w}_{j}\\mathbf{x}-\\sum_{k\\neq j}(\\mathbf{w}_{j}^{T}\\mathbf{w}_{k})y_{k}-||\\mathbf{w}_{j}||^{2}y_{j}-\\lambda S'(y_{j})=0\\\\\n & \\iff y_{j}+\\lambda S'(y_{j})=\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}(\\mathbf{w}_{j}^{T}\\mathbf{w}_{k})y_{k}\\\\\n & \\iff y_{j}=g(\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}v_{jk}y_{k})\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{d}{dy_{j}}E=0\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mi>E</mi></mrow><mo>=</mo><mn>0</mn></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\iff\\mathbf{w}_{j}(\\mathbf{x}-\\mathbf{W}\\mathbf{y})-\\lambda S^{%&#10;\\prime}(y_{j})=0\" display=\"inline\"><mrow><mi/><mo>\u21d4</mo><mrow><mrow><mrow><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><mi>\ud835\udc16\ud835\udc32</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>S</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\iff\\mathbf{w}_{j}\\mathbf{x}-\\sum_{k\\neq j}(\\mathbf{w}_{j}^{T}%&#10;\\mathbf{w}_{k})y_{k}-||\\mathbf{w}_{j}||^{2}y_{j}-\\lambda S^{\\prime}(y_{j})=0\" display=\"inline\"><mrow><mi/><mo>\u21d4</mo><mrow><mrow><mrow><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc30</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub></mrow></mrow><mo>-</mo><mrow><msup><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>S</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\iff y_{j}+\\lambda S^{\\prime}(y_{j})=\\mathbf{w}_{j}^{T}\\mathbf{x}%&#10;-\\sum_{k\\neq j}(\\mathbf{w}_{j}^{T}\\mathbf{w}_{k})y_{k}\" display=\"inline\"><mrow><mi/><mo>\u21d4</mo><mrow><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>S</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc30</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\iff y_{j}=g(\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}v_{jk}y_{%&#10;k})\" display=\"inline\"><mrow><mi/><mo>\u21d4</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\ninitialized with $u_{j}(t)=0$. \n\nThe second step is a standard gradient descent implementation of the\nleast square regression optimization, leading to an learning rule\n", "itemtype": "equation", "pos": 52937, "prevtext": "\nwhere we constrained the vector $\\mathbf{w}_{j}$ of synapses projecting\nonto unit $y_{j}$ by $||\\mathbf{w}_{j}||^{2}=1$, defined the activation\nfunction $g(.)=T^{-1}(.)$, the inverse of $T(y)=(y+\\lambda S'(y))$,\nand defined recurrent synaptic weights $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$.\nFor each input sample $\\mathbf{x}$, this equation shall be iterated\nuntil convergence. The equation can be interpreted as a recurrent\nneural network, where each neuron has an activation function $g$,\nand the input is given by the sum of the feedforward drive $\\mathbf{w}_{j}^{T}\\mathbf{x}$\nand a recurrent inhibition term $-\\sum_{k\\neq j}v_{jk}y_{k}$. To\navoid instability, we implement a smooth membrane potential $u_{j}$,\nwhich has the same convergence point \\citep{rozell_sparse_2008}\n\n", "index": 11, "text": "\\begin{equation}\n\\begin{aligned} & \\tau_{u}\\frac{d}{dt}u_{j}(t)=-u_{j}(t)+(\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}v_{jk}y_{k}(t))\\\\\n & y_{j}(t)=g(u_{j}(t))\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tau_{u}\\frac{d}{dt}u_{j}(t)=-u_{j}(t)+(\\mathbf{w}_{j}^{T}\\mathbf%&#10;{x}-\\sum_{k\\neq j}v_{jk}y_{k}(t))\" display=\"inline\"><mrow><mrow><msub><mi>\u03c4</mi><mi>u</mi></msub><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle y_{j}(t)=g(u_{j}(t))\" display=\"inline\"><mrow><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\n\n\nThe decay term $\\mathbf{w}_{j}\\ y_{j}^{2}$ has no effect, since the\nnorm is constrained to $||\\mathbf{w}_{j}||=1$ at each step. For a\nsingle unit $y,$ the model simplifies to a nonlinear Hebbian formulation,\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ g(\\mathbf{w}_{j}^{T}\\mathbf{x})$.\nFor multiple units, it can be interpreted as projection pursuit on\nan effective input, not yet represented by other neurons, $\\tilde{\\mathbf{x}_{j}}=\\mathbf{x}-\\sum_{k\\neq j}\\mathbf{w}_{k}y_{k}$,\nwhich simplifies to $\\Delta\\mathbf{w}_{j}\\propto\\tilde{\\mathbf{x}}_{j}\\cdot g(\\mathbf{w}_{j}^{T}\\tilde{\\mathbf{x}_{j}})$\n. \n\nThere are two non-local terms that need to be implemented by local\nmechanisms so as to be biologically plausible. First, the recurrent\nweights depend on the overlap between receptive fields, $\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$,\nwhich is non-local. The sparse coding model assumes independent hidden\nneurons, which implies that after learning neurons should be pair-wise\nuncorrelated, $cov(y_{j},y_{k})=0$. As an aside we note that the\nchoice $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$ does not automatically\nguarantee decorrelation. Decorrelation may be enforced through plastic\nlateral connections, following an anti-Hebbian rule \\citep{foldiak_forming_1990,zylberberg_sparse_2011},\n$\\Delta v_{jk}\\propto(y_{j}-\\langle y_{j}\\rangle)\\cdot y_{k}$, where\n$\\langle y_{j}\\rangle$ is a moving average (we use $\\tau=1000$ input\nsamples). Thus by substituting fixed recurrent connections by anti-Hebbian\nplasticity, convergence $\\Delta v_{jk}=0$ implies $cov(y_{j},y_{k})=0$.\nWhile this implementation does not guarantee $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$\nafter convergence, neither does $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$\nguarantee decorrelation $cov(y_{j},y_{k})=0$, it does lead to optimal\ndecorrelation, which is the basis of the normative assumption. Additionally\nwe constrain $v_{jk}\\geq0$ to satisfy Dale's law. Although some weights\nwould converge to negative values otherwise, most neuron pairs have\ncorrelated receptive fields, and thus positive recurrent weights. \n\n\n\n\nSecond, we ignore the non-local term $\\sum_{k\\neq j}\\mathbf{w}_{k}y_{k}y_{j}$\nin the update rule. Although this approximation is not theoretically\njustified, we observed in simulations that receptive fields do not\nqualitatively differ when this term is removed.\n\nThe resulting Hebbian formulation can be summarized as\n\n", "itemtype": "equation", "pos": 53293, "prevtext": "\ninitialized with $u_{j}(t)=0$. \n\nThe second step is a standard gradient descent implementation of the\nleast square regression optimization, leading to an learning rule\n", "index": 13, "text": "\n\\[\n\\Delta w_{j}\\propto\\frac{d}{dw_{j}}E=(\\mathbf{x}-\\mathbf{W}^{T}\\mathbf{y})\\ y_{j}=\\mathbf{x}\\ y_{j}-\\mathbf{w}_{j}\\ y_{j}^{2}-\\sum_{k\\neq j}\\mathbf{w}_{k}y_{k}y_{j}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\Delta w_{j}\\propto\\frac{d}{dw_{j}}E=(\\mathbf{x}-\\mathbf{W}^{T}\\mathbf{y})\\ y_%&#10;{j}=\\mathbf{x}\\ y_{j}-\\mathbf{w}_{j}\\ y_{j}^{2}-\\sum_{k\\neq j}\\mathbf{w}_{k}y_%&#10;{k}y_{j}\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>w</mi><mi>j</mi></msub></mrow><mo>\u221d</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><msub><mi>w</mi><mi>j</mi></msub></mrow></mfrac><mo>\u2062</mo><mi>E</mi></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><mrow><msup><mi>\ud835\udc16</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><mo>=</mo><mrow><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><mo>-</mo><mrow><mpadded width=\"+5pt\"><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mpadded><mo>\u2062</mo><msubsup><mi>y</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder><mrow><msub><mi>\ud835\udc30</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00701.tex", "nexttext": "\n\n\nThis derivation unifies previous results on the biological implementation\nof sparse coding: the relation of the sparseness constraint to a specific\nactivation function \\citep{rozell_sparse_2008}, the derivation of\na Hebbian learning rule from quadratic error minimization \\citep{Oja_1982},\nand the possibility of approximating lateral interaction terms by\nlearned lateral inhibition \\citep{foldiak_forming_1990,zylberberg_sparse_2011}.\n\n\\textbf{Nonlinearities and optimization value. }The optimization value\nfor a given effective nonlinearity $f$, synaptic weights $w$, and\ninput samples $x$, is given by $R=\\langle F(\\mathbf{w}^{T}\\mathbf{x})\\rangle$,\nwhere $F=\\int f$ and angular brackets indicate the ensemble average\nover $x$. Relative optimization values in Figs. \\ref{fig:winners}b\nand \\ref{fig:position} were normalized to $[0,1]$, relative\nto the minimum and maximum values among the considered choice of features\n$w$, $R^{*}=(R-R_{min})/(R_{max}-R_{min})$.\\textbf{ }The selectivity\nindex of a nonlinearity $f$ is defined as $SI=(\\langle F(l)\\rangle-\\langle F(g)\\rangle)/\\sigma_{F}$,\nwhere $l$ and $g$ are Laplacian and Gaussian variables respectively,\nnormalized to unit variance. $\\sigma_{F}=\\sqrt{\\sigma_{F(l)}\\sigma_{F(g)}}$\nis a normalization factor, with \\textrm{$\\sigma_{F(.)}=\\sqrt{\\langle F(.)^{2}\\rangle}$}.\nThe selectivity of an effective nonlinearity $f$ is not altered by\nmultiplicative scaling, $\\tilde{f}(u)=\\alpha f(u)$, neither by additive\nconstants when the input distribution is symmetric, $\\tilde{f}(u)=\\alpha f(u)+\\beta$.\nThe effective nonlinearities in Fig. \\ref{fig:winners} included\nthe linear rectifier $f(u)=\\begin{cases}\n0, & if\\ u<\\theta\\\\\nu-\\theta, & if\\ u\\ge\\theta\n\\end{cases}$, the quadratic rectifier $f(u)=\\begin{cases}\n0, & if\\ u<\\theta\\\\\n(u-\\theta)(u-\\theta-b), & if\\ u\\ge\\theta\n\\end{cases}$, the $L_{0}$ sparse coding nonlinearity $f(u)=\\begin{cases}\n0, & if\\ u<\\lambda\\\\\nu, & if\\ u\\ge\\lambda\n\\end{cases}$, the Cauchy sparse coding nonlinearity $f=T^{-1}$, where $T(y)=\\begin{cases}\n0, & if\\ y<0\\\\\ny+2\\lambda y/(1+y^{2}), & if\\ y\\ge0\n\\end{cases}$, the negative sigmoid $f(u)=1-2/(1+e^{-2u})$, a polynomial function\n$f(u)=u^{3}$, trigonometric functions $sin(u)$ and \\textbf{$cos(u)$},\na symmetric piece-wise linear function $f(u)=\\begin{cases}\n0, & if\\ |u|<\\theta\\\\\n|u|-\\theta, & if\\ |u|\\ge\\theta\n\\end{cases}$, as well as, for comparison, a linear function $f(u)=u$.\n\n\\textbf{Receptive field learning. }Natural image patches (16 by 16\npixel windows) were sampled from a standard dataset \\citep{olshausen_emergence_1996}\n($10^{6}$ patches). Patches were randomly rotated by $\\pm90{}^{\\circ}$\ndegrees to avoid biases in orientation. The dataset was whitened by\nmean subtraction and a standard linear transformation $\\mathbf{x}^{*}=\\mathbf{M}\\mathbf{x}$,\nwhere $\\mathbf{M}=\\mathbf{R}\\mathbf{D}^{-1/2}\\mathbf{R}^{T}$ and\n$\\langle\\mathbf{x}\\mathbf{x}^{T}\\rangle=\\mathbf{R}\\mathbf{D}\\mathbf{R}^{T}$\nis the eigenvalue decomposition of the input correlation matrix. In\nFig. \\ref{fig:nonwhitened}, we used images preprocessed as\nin \\citet{olshausen_emergence_1996}, filtered in the spatial frequency\ndomain by $M(f)=f\\ e^{-(f/f_{0})^{4}}$. The exponential factor is\na low-pass filter that attenuates high-frequency spatial noise, with\n$f_{0}=200$ cycles per image. The linear factor $f$ was designed\nto whiten the images by canceling the approximately $1/f$ power law\nspatial correlation observed in natural images \\citep{ruderman_statistics_1994}.\nBut since the exponent of the power law for this particular dataset\nhas an exponent closer to $1.2$, the preprocessed image exhibit higher\nvariance at lower spatial frequencies.\n\nSynaptic weights were initialized randomly (normal distribution with\nzero mean) and, for an effective nonlinearity $f$, evolved through\n$\\mathbf{w}_{k+1}=\\mathbf{w}_{k}+\\eta\\ \\mathbf{x}\\ f(\\mathbf{w}_{k}^{T}\\mathbf{x}_{k})$,\nfor each input sample $x_{k}$, with a small learning rate $\\eta$.\nWe enforced normalized weights at each time step, $||\\mathbf{w}||_{2}=1$,\nthrough multiplicative normalization, implicitly assuming rapid homeostatic\nmechanisms \\citep{turrigiano_too_2011,Zenke_Hennequin_Gerstner_2013}.\nFor multiple neurons, the neural version of the sparse coding model\ndescribed in Eq \\ref{eq:sc_hebb} was implemented. In Fig\n\\ref{fig:gabormap} and \\ref{fig:nonwhitened}, the learned\nreceptive fields were fitted to Gabor filters by least square optimization.\nReceptive fields with less than $0.6$ variance explained were rejected\n(less than 5\\% of all receptive fields). \n\n\\textbf{Receptive field selection.} In Fig. \\ref{fig:winners}b,\nthe five selected candidate patterns are: random connectivity filter\n(weights sampled independently from the normal distribution with zero\nmean), high-frequency Fourier filter (with equal horizontal and vertical\nspatial periods, $T_{x}=T_{y}=8$ pixels), difference of Gaussians\nfilter ($\\sigma_{1}=3.$, $\\sigma_{2}=4.$), low-frequency Fourier\nfilter ($T_{x}=16$, $T_{y}=32$), and centered localized Gabor filter\n($\\sigma_{x}=1.5$, $\\sigma_{y}=2.0$, $f=0.2$, $\\theta=\\pi/3$,\n$\\phi=\\pi/2$). Fourier filters were modeled as $w_{ab}=sin(2\\pi a/T_{x})*cos(2\\pi b/T_{y})$;\ndifference of Gaussians filters as the difference between two centered\n2D Gaussians with same amplitude and standard deviations $\\sigma_{1}$\nand $\\sigma_{2}$; and we considered standard Gabor filters, with\ncenter $(x_{c},y_{c}),$ spatial frequency $f$, width $\\sigma_{x}$,\nlength $\\sigma_{y},$ phase $\\phi$ and angle $\\theta$. In Fig\n\\ref{fig:gabormap} and \\ref{fig:nonwhitened} we define\nthe Gabor width and length in pixels as 2.5 times the standard deviation\nof the respective Gaussian envelopes, $\\sigma_{x}$ and $\\sigma_{y}$.\nIn Fig. \\ref{fig:position}a, a Gabor filter of size $s$\nhad parameters $\\sigma_{x}=0.3\\cdot s$, $\\sigma_{y}=0.6\\cdot s$,\n$f=1/s$ and $\\theta=\\pi/3$. In Fig. \\ref{fig:position}b-c,\nthe Gabor filter parameters were $\\sigma_{x}=1.2$, $\\sigma_{y}=2.4$,\n$f=0.25$. All receptive fields were normalized to $||\\mathbf{w}||_{2}=1$.\nIn Fig. \\ref{fig:gabormap} and \\ref{fig:nonwhitened}, the\nbackground optimization value was calculated for Gabor filters of\ndifferent widths, lengths, frequencies, phases $\\phi=0$ and $\\phi=\\pi/2$.\nFor each width and length, the maximum value among frequencies and\nphases was plotted. \n\n\n\n\n\\textbf{Additional datasets. }For the strabismus model, two independent\nnatural image patches were concatenated, representing non-overlapping\nleft and right eye inputs, forming a dataset with 16 by 32 patches\n\\citep{Cooper_Intrator_Blais_Shouval_2004}. For the binocular receptive\nfield in the strabismus statistical analysis (Fig. \\ref{fig:othermodalities}a),\na receptive field was learned with a binocular input with same input\nfrom left and right eyes. As V2 input, V1 complex cell responses were\nobtained from natural images as in standard energy models \\citep{Hyvarinen_Hurri_Hoyer_2009},\nmodeled as the sum of the squared responses of simple cells with alternated\nphases. These simple cells were modeled as linear neurons with Gabor\nreceptive fields ($\\sigma_{x}=1.2$, $\\sigma_{y}=2.4$, $f=0.3$),\nwith centers placed on a 8 by 8 grid (3.1 pixels spacing), with 8\ndifferent orientations at each position (total of 512 input dimensions).\nFor the non-orientation selective receptive field in the V2 statistical\nanalysis (Fig. \\ref{fig:othermodalities}d), the orientations\nof the input complex cells for the learned receptive field were randomized.\nAs auditory input, spectrotemporal segments were sampled from utterances\nspoken by a US English male speaker (CMU US BDL ARCTIC database, \\citet{Kominek_Black_2004}).\nFor the frequency decomposition \\citep{Smith_Lewicki_2006}, each audio\nsegment was filtered by gammatone kernels, absolute and log value\ntaken and downsampled to $50$ Hz. Each sample was 20 time points\nlong ($400$ ms segment) and 20 frequency points wide (equally spaced\nbetween $0.2$ kHz and $4.0$ kHz). For the non-local receptive field\nin the auditory statistical analysis (Fig. \\ref{fig:othermodalities}g),\na Fourier filter was used ($T_{t}=T_{f}=10$). For all datasets, the\ninput ensemble was whitened after the preprocessing steps, by the\nsame linear transformation described above for natural images, and\nall receptive fields were normalized to $||\\mathbf{w}||_{2}=1$.\n \n\\section*{Acknowledgments} \n\nWe thank C. Pozzorini and J. Brea for valuable comments, and D.S. Corneil for critical reading of the manuscript. This research was supported by the European Research Council under grant agreement no. 268689 (MultiRules). \n \n\\bibliographystyle{unsrtnat}\n\\bibliography{nonlinear}\n\n\n", "itemtype": "equation", "pos": 55869, "prevtext": "\n\n\nThe decay term $\\mathbf{w}_{j}\\ y_{j}^{2}$ has no effect, since the\nnorm is constrained to $||\\mathbf{w}_{j}||=1$ at each step. For a\nsingle unit $y,$ the model simplifies to a nonlinear Hebbian formulation,\n$\\Delta\\mathbf{w}\\propto\\mathbf{x}\\ g(\\mathbf{w}_{j}^{T}\\mathbf{x})$.\nFor multiple units, it can be interpreted as projection pursuit on\nan effective input, not yet represented by other neurons, $\\tilde{\\mathbf{x}_{j}}=\\mathbf{x}-\\sum_{k\\neq j}\\mathbf{w}_{k}y_{k}$,\nwhich simplifies to $\\Delta\\mathbf{w}_{j}\\propto\\tilde{\\mathbf{x}}_{j}\\cdot g(\\mathbf{w}_{j}^{T}\\tilde{\\mathbf{x}_{j}})$\n. \n\nThere are two non-local terms that need to be implemented by local\nmechanisms so as to be biologically plausible. First, the recurrent\nweights depend on the overlap between receptive fields, $\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$,\nwhich is non-local. The sparse coding model assumes independent hidden\nneurons, which implies that after learning neurons should be pair-wise\nuncorrelated, $cov(y_{j},y_{k})=0$. As an aside we note that the\nchoice $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$ does not automatically\nguarantee decorrelation. Decorrelation may be enforced through plastic\nlateral connections, following an anti-Hebbian rule \\citep{foldiak_forming_1990,zylberberg_sparse_2011},\n$\\Delta v_{jk}\\propto(y_{j}-\\langle y_{j}\\rangle)\\cdot y_{k}$, where\n$\\langle y_{j}\\rangle$ is a moving average (we use $\\tau=1000$ input\nsamples). Thus by substituting fixed recurrent connections by anti-Hebbian\nplasticity, convergence $\\Delta v_{jk}=0$ implies $cov(y_{j},y_{k})=0$.\nWhile this implementation does not guarantee $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$\nafter convergence, neither does $v_{jk}=\\mathbf{w}_{j}^{T}\\mathbf{w}_{k}$\nguarantee decorrelation $cov(y_{j},y_{k})=0$, it does lead to optimal\ndecorrelation, which is the basis of the normative assumption. Additionally\nwe constrain $v_{jk}\\geq0$ to satisfy Dale's law. Although some weights\nwould converge to negative values otherwise, most neuron pairs have\ncorrelated receptive fields, and thus positive recurrent weights. \n\n\n\n\nSecond, we ignore the non-local term $\\sum_{k\\neq j}\\mathbf{w}_{k}y_{k}y_{j}$\nin the update rule. Although this approximation is not theoretically\njustified, we observed in simulations that receptive fields do not\nqualitatively differ when this term is removed.\n\nThe resulting Hebbian formulation can be summarized as\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{aligned} & y_{j}=g(\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}v_{jk}y_{k})\\\\\n & \\Delta\\mathbf{w}_{j}\\propto\\mathbf{x\\ }y_{j}\\\\\n & \\Delta v_{jk}\\propto(y_{j}-\\langle y_{j}\\rangle)\\cdot y_{k}\n\\end{aligned}\n\\label{eq:sc_hebb}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle y_{j}=g(\\mathbf{w}_{j}^{T}\\mathbf{x}-\\sum_{k\\neq j}v_{jk}y_{k})\" display=\"inline\"><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta\\mathbf{w}_{j}\\propto\\mathbf{x\\ }y_{j}\" display=\"inline\"><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow><mo>\u221d</mo><mrow><mpadded width=\"+5pt\"><mi>\ud835\udc31</mi></mpadded><mo>\u2062</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta v_{jk}\\propto(y_{j}-\\langle y_{j}\\rangle)\\cdot y_{k}\" display=\"inline\"><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>\u221d</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><msub><mi>y</mi><mi>k</mi></msub></mrow></mrow></math>", "type": "latex"}]