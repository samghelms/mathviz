[{"file": "1601.02043.tex", "nexttext": "\n\\noindent\nThis equation specifies that the current error is similar to the preceding\nerror by a factor $\\rho$, with Gaussian noise added. As the current error\ndepends only on the preceding error, this is a first-order autoregressive\nprocess.  Second-order or higher autoregressive process would also take into\naccount the error at $t-k, k=2, 3, \\ldots$ .  The {\\tt bam} function in the {\\tt\nmgcv} package offers the possibility of taking a first-order autoregressive\nprocess into account by specifying the autoregressive proportionality $\\rho$\n(with the {\\tt rho} directive in the function call) and by supplying a variable\nin the data frame, here {\\tt NewTimeSeries} (with levels {\\sc true, false}),\nindicating the beginning of each new time series with the value {\\sc true}\n(here, the first trial for each subject), to be supplied to the directive {\\tt\nAR.start} in the call to {\\tt bam}:\n\n\n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.345,0.345,0.345}{{naming.r.gam}}} {\\textcolor[rgb]{0.69,0.353,0.396}{{=}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(RT}}} {\\textcolor[rgb]{0,0,0}{{~}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Regularity}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Number}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Voicing}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{InitialNeighbors}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                     {\\textcolor[rgb]{0.345,0.345,0.345}{{InflectionalEntropy}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Frequency)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                     {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Trial, Subject,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}}{\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Verb,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n                     {\\textcolor[rgb]{0.333,0.667,0.333}{{rho}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{0.3}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{AR.start}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=naming}}}{\\textcolor[rgb]{0,0,0}{{$}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{NewTimeSeries,}}}\n                     {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=naming)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\nThere is no automatic procedure for the selection of the value of $\\rho$. The\nautocorrelation at lag 1 is a good guide for an initial guesstimate, which may\nneed further adjusting.  When changing $\\rho$, it is important not to increase\n$\\rho$ when this does not lead to a visible reduction in autocorrelation, at\nthe cost of inflated goodness of fit and warped effects of key predictors.  It\nshould be kept in mind that an {\\sc ar(1)} autocorrelative process is only the\nsimplest of possible autocorrelative processes that may be going on in the\ndata, and that hence increasing $\\rho$ beyond where it is functional can\ndistort results.  The final row of Figure~\\ref{fig:residsNaming} shows that for\nthis example, nearly all autocorrelational structure is eliminated with a small\n$\\rho = 0.3$.  \n\nThe summary of this model, shown in Table~\\ref{tab:gamRT}, shows strong support\nfor the random effects structure for {\\tt Verb} and {\\tt Subject}, with large\n$t$-values and small $p$-values.\\footnote{The parametric coefficients suggest\nthat regularity is irrelevant as predictor of naming times, that singulars are\nnamed faster than plurals, that words with voiced initial segments have longer\nnaming times, as do words with a large number of words at Hamming distance 1 at\nthe initial segment.  Words with a greater Shannon entropy calculated over the\nprobability distribution of their inflectional variants elicited shorter\nresponse times.  A thin plate regression spline for log-transformed word\nfrequency suggests a roughly U-shaped effect (not shown) for this predictor.}\nTypical examples of by-subject random wiggly curves are shown in\nFigure~\\ref{fig:namingWigglies}.  These curves capture both changes in\nintercept, as well as changes over time.  For some subjects, the changes are\nnegligible, but for others, they can be substantial, and non-linear.\n\n\n\\begin{table}[ht]\n\n\\caption{A {\\sc gamm} fitted to log-transformed picture naming latencies ($\\rho = 0.3$);\n{\\tt s}: thin plate regression spline, {\\tt fs}: factor smooth, {\\tt re}: random effect.} \n\\label{tab:gamRT}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lrrrr}\\hline\n\nA. parametric coefficients & Estimate & Std. Error & t-value & p-value \\\\ \n\n  Intercept & 6.5531 & 0.0512 & 127.9396 & $<$ 0.0001 \\\\ \n  Regularity=regular & 0.0093 & 0.0094 & 0.9986 & 0.3180 \\\\ \n  Number=singular & -0.1147 & 0.0513 & -2.2377 & 0.0253 \\\\ \n  Voicing=present & 0.0269 & 0.0101 & 2.6734 & 0.0075 \\\\ \n  Initial Neighborhood Size & 0.0179 & 0.0055 & 3.2499 & 0.0012 \\\\ \n  Inflectional Entropy & -0.0343 & 0.0159 & -2.1616 & 0.0307 \\\\ \\hline\n\nB. smooth terms & edf & Ref.df & F-value & p-value \\\\ \n\n  s(word frequency)   & 4.2914 & 4.6233 & 7.7445 & $<$ 0.0001 \\\\ \n  fs(Trial, subject)  & 99.4223 & 358.0000 & 5.6670 & $<$ 0.0001 \\\\ \n  re(verb)            & 190.1753 & 280.0000 & 2.1085 & $<$ 0.0001 \\\\ \n   \\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{namingXYplotCol.pdf}\n  \n\n  \\caption{Selected by-subject random wiggly curves for Trial (penalized factor smooths) in the\n  {\\sc gamm} fitted to word naming latencies.}\n\n  \\label{fig:namingWigglies}\n\\end{figure}\n\nOne could consider replacing the factor smooths by by-subject random intercepts,\nwhile at the same time increasing $\\rho$.  However, a model such as \n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(RT}}} {\\textcolor[rgb]{0,0,0}{{~}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Regularity}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Number}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Voicing}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{InitialNeighbors}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n         {\\textcolor[rgb]{0.345,0.345,0.345}{{InflectionalEntropy}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Frequency)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n         {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Subject,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Verb,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n         {\\textcolor[rgb]{0.333,0.667,0.333}{{rho}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{0.9}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{AR.start}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=naming}}}{\\textcolor[rgb]{0,0,0}{{$}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{NewTimeSeries,}}}\n         {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=naming)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\nprovides an inferior fit with an adjusted R-squared of 0.07 (compare 0.36) and\nan f{\\sc reml} score of 2655 (compare 684).  This suggests that in this data\nset, two very different kinds of processes unfold.  One of these processes is\nautoregressive in nature, with a relatively small $\\rho$.   Possibly, these\nautoregressive processes reflect minor fluctuations in attention.  The other\nprocess may reflect higher-order cognitive processes relating to practice and\nfatigue, such as exemplified by the fastest subject ({\\tt s11}) in\nFigure~\\ref{fig:namingWigglies}, who initially improved her speed, but then, as\nthe experiment progressed, was not able to maintain her rapid rate of\nresponding.\n\nAlthough these task effects typically are not of interest to an investigator's\ncentral research question, careful modeling of these task effects is important\nfor the evaluation of one's hypotheses.  For instance, the linear mixed effects\nmodel mentioned previously does not support an effect of inflectional entropy\n(Shannon's entropy calculated over the probabilities of a verb's inflectional\nvariants) with $t = -1.87$, whereas the {\\sc gamm} offers more confidence in\nthis covariate ($t = -2.16$).  However, as we shall see next, predictors may\nalso lose significance as autocorrelational structure is brought into the\nmodel.\n\n\n\n\\section{Pitch contours as time series}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\citet{Koesling:Kunter:Baayen:Plag:2012} were interested in the stress patterns\nof English three-constituent compounds, and measured the fundamental frequency\nof such compounds as realized by a sample of speakers.  In what follows, the\nresponse variable of this study, pitch, is measured in semitones.\n\nAs can be seen by inspecting the top panels of Figure~\\ref{fig:residsPitch},\nthere are autocorrelations in the pitch contours that are much stronger than\nthose observed for the naming latencies discussed above.   In this figure,\npanels represent the autocorrelation functions for selected {\\em events}, where\nan event is defined as an elementary time series consisting of the pitch\nmeasured at 100 moments in normalized time for the combination of a given\ncompound and a given speaker.  Whereas for the naming experiment, there are as\nmany time series as there are subjects, the number of time series in the\npresent phonetics study is equal to the number of unique combinations of\nsubjects and compounds ($12 \\times 40 = 480$).  \n\n\n\n\n\n\n\n\n\\begin{figure}\n  \n  \\includegraphics[width=\\textwidth]{pitchNewAcfPlotTrellisCol.pdf}\n\n  \\caption{Autocorrelation functions for pitch (in semitones, top row) and\n  model residuals (remaining rows) of selected events. Second row: {\\sc gamm}\n  with by-participant random intercepts and random slopes for {\\tt Time} and\n  by-compound random intercepts; Third row: {\\sc gamm} with by-participant and\n  by-compound random wiggly curves; Fourth row: {\\sc gamm} with by-compound and\n  by-participant random wiggly curves as well as a correction for {\\sc ar(1)}\n  with $\\rho = 0.98$.}\n\n  \\label{fig:residsPitch}\n\\end{figure}\n\nThe second row of panels in Figure~\\ref{fig:residsPitch} indicates that a model\nwith by-speaker random intercepts and slopes for (normalized) time does not\nsucceed in consistently reducing the autoregressive structure of this data.\nSome improvement is achieved when by-subject and by-compound random wiggly\ncurves are added to the model specification (third row of panels), but the\nerrors are only whitened substantially, albeit not completely, by additionally\nincluding an autoregressive parameter $\\rho = 0.98$ (bottom row of\npanels).  This fourth model was specified as follows.\n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.345,0.345,0.345}{{pitch.gam}}} {\\textcolor[rgb]{0.69,0.353,0.396}{{=}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(PitchSemiTone}}} {\\textcolor[rgb]{0,0,0}{{~}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Sex}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{BranchingOrd}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{by}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=BranchingOrd)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime, Speaker,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime, Compound,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Compound, Sex,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n  {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=pitch,}}}\n  {\\textcolor[rgb]{0.333,0.667,0.333}{{rho}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{0.98}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{AR.start}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=pitch}}}{\\textcolor[rgb]{0,0,0}{{$}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{NewTimeSeries)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\n{\\tt BranchingOrd} is an ordered factor specifying four different compound types\n(defined by stress position and branching structure).  The first smooth,\n{\\tt s(NormalizedTime)}, specifies a wiggly curve for the reference level of this factor.\nThe second smooth term, {\\tt s(NormalizedTime, by = BranchingOrd)},\nrequests difference curves for the remaining three levels of {\\tt BranchingOrd}.\\footnote{\nFor this to work properly, it is necessary to use treatment\ncontrasts for ordinal factors, in {\\tt R}:\n{\\tt options(contrasts = c(\"contr.treatment\", \"contr.treatment\"))}.\n}\n\\noindent\nA summary of this model is presented in Table~\\ref{tab:pitch}.\nFigure~\\ref{fig:pitchWigglies} clarifies that the variability across speakers\nmainly concerns differences in the intercept (height of voice) with variation\nover time that is quite mild compared to the variability over time present for\nthe compounds.\n\n\n\\begin{table}[ht]\n\\caption{Summary of a {\\sc gamm} for pitch as realized on English three-constituent\ncompounds ($\\rho=0.98$); {\\tt s}: thin plate regression spline, {\\tt ds}:\ndifference spline, {\\tt fs}: factor smooth, {\\tt re(compound, sex)}:\nby-compound random effects for sex.} \n\n\\label{tab:pitch}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lrrrr}\\hline\n\nA. parametric coefficients & Estimate & Std. Error & t-value & p-value \\\\ \n\n  Intercept & 91.3134 & 1.4594 & 62.5689 & $<$ 0.0001 \\\\ \n  Sex = male & -13.6336 & 1.4649 & -9.3066 & $<$ 0.0001 \\\\ \n  Branching = LN2 & 0.7739 & 0.4271 & 1.8121 & 0.0700 \\\\ \n  Branching = RN2 & 0.2415 & 0.3657 & 0.6605 & 0.5089 \\\\ \n  Branching = RN3 & 0.6460 & 0.4320 & 1.4955 & 0.1348 \\\\ \\hline\n\nB. smooth terms & edf & Ref.df & F-value & p-value \\\\ \n\n  s(Time)        & 7.6892 & 7.9403 & 2.7398 & 0.0064 \\\\ \n  ds(Time, LN2)  & 6.5392 & 7.0804 & 0.6255 & 0.7418 \\\\ \n  ds(Time, RN2)  & 1.4097 & 1.5555 & 2.4744 & 0.1344 \\\\ \n  ds(Time, RN3)  & 6.4987 & 7.1541 & 1.9566 & 0.0411 \\\\ \n  fs(Time, speaker)  & 85.7092 & 105.0000 & 14.2675 & $<$ 0.0001 \\\\ \n  fs(Time, compound) & 248.5172 & 348.0000 & 3.5294 & $<$ 0.0001 \\\\ \n  re(compound, sex)  & 19.0558 & 75.0000 & 0.4566 & $<$ 0.0001 \\\\ \\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\\begin{figure}\n  \\centering\n  \n  \\includegraphics[width=\\textwidth]{pitchSubjectsXYplotCol.pdf}\n  \\includegraphics[width=\\textwidth]{pitchCompoundsXYplotCol.pdf}\n\n  \\caption{By-speaker (upper trellis) and by-compound (lower trellis) random\n  wiggly curves in normalized time in the {\\sc gamm} predicting the pitch\n  contour for English three-constituent compounds ($\\rho =0.98$).}\n\n  \\label{fig:pitchWigglies}\n\\end{figure}\n\nIn principle, one could consider fitting a penalized factor smooth to each of\nthe 480 individual events (time series), although this is currently\ncomputationally prohibitively expensive for the large number of events in the\npresent study.  The way the model has been specified here is optimistic in the\nsense that it assumes that how pitch contours are realized can be factored out\ninto orthogonal contributions from individual subjects and from individual\ncompounds.  In a more pessimistic scenario, each event makes its own,\nidiosyncratic, contribution to the model's predictions.  In other words, the\npresent model seeks to capture part of the structure in the elementary time\nseries by means of crossed wiggly curves `by subject' and `by item'.\n\nCurrently, only a single autoregressive parameter $\\rho$ can be specified for\nall events jointly.  Inspection of the last row of panels of\nFigure~\\ref{fig:pitchWigglies} suggests that it is desirable to relax the\nassumption that $\\rho$ is exactly the same for each event.  Although for some\nevents the autocorrelation function is properly flat already for a moderate\n$\\rho$, see, e.g., the second panel on the first row ($\\rho = 0.4$), events\nremain for which autocorrelations persist across several lags.  \n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth]{rhoProblemsNewLatticeCol.pdf}\n\n  \\caption{Autocorrelation functions for the residuals of {\\sc gamm} models\n  with $\\rho = 0.2, 0.4, 0.8, 0.98$ (columns) for selected events (rows) where\n  the largest value of $\\rho$, although for most events optimal, induces\n  artifical negative autocorrelations at some lags.}\n\n  \\label{fig:rhoProblems}\n\\end{figure}\n\nIncreasing $\\rho$ would remove such persistent autocorrelations, but,\nunfortunately, at the same time induce artificial autocorrelations for other\nevents.  This is illustrated in Figure~\\ref{fig:rhoProblems}, which presents,\nfor four events (rows) the autocorrelation function for increasing values of\n$\\rho$ (columns).   For events with hardly any autocorrelation to begin with\n(upper panels), increasing $\\rho$ artificially creates a strong negative\nautocorrelation at lag 1.  The events in the second and third row show how\nincreasing $\\rho$ can induce artefactual autocorrelations both at shorter lags\n(second row) and at longer lags (third row).  The event in the fourth row\nillustrates how increasing $\\rho$ attenuates but not removes autocorrelation at\nshorter lags, while giving rise to new negative autocorrelation at\nintermediate lags.  \n\nAlthough higher-order autoregressive processes might be more appropriate for\nmany events, they currently resist incorporation into {\\sc gamm}s.  Thus, the\nanalysist is left with two strategies.  The first is to select a value of\n$\\rho$ that finds a balance between removing strong autocorrelations, while at\nthe same time avoiding the introduction of artefactual autocorrelation for\nevents which show little autocorrelation to begin with --- inappropriate use of\n$\\rho$ may completely obscure the actual patterns in the data.\n\nThe second strategy is to remove from the data set those events that show\npersistent autocorrelations for the optimal $\\rho$ obtained with strategy one.\nWhen refitting the model to the remaining data points yields qualitatively\nsimilar results, it is safe to conclude that the remaining autocorrelational\nstructure in the original model is not an issue.\n\n\n\nTwo aspects of the present model are of further interest.  First, the model\nincludes a thin plate regression smooth for the reference level of compound\ntype ({\\tt LN1}), with difference smooths for the remaining three compound\ntypes.  Inspection of Table~\\ref{tab:pitch} reveals only limited support for\nsignificant differences between the pitch contours on the four kinds of\ncompounds, and inspection of the difference curves (in panels 2--4 in\nFigure~\\ref{fig:pitchCurves}) clarifies that there is little evidence for\nsignificant differences with the reference curve.  In fact, a simpler model\n(not shown) with just a spline for normalized time and no main effect or\ninteractions involving branching condition fits the data just as well.\n\n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth]{pitchCurves.pdf}\n\n  \\caption{The pitch contour for the {\\tt LN1} branching condition, and\n  difference curves for the remaining three branching conditions. As\n  the confidence regions for the difference curves always contain the\n  zero line, there is little support for differences in pitch contour\n  as a function of branching condition.}\n\n  \\label{fig:pitchCurves}\n\\end{figure}\n\nThe main reason for the absence of the effect of branching condition reported\noriginally by \\citet{Koesling:Kunter:Baayen:Plag:2012} is the inclusion of the\nrandom wiggly curves for compound.  When the factor smooth for compound is\nreplaced by random intercepts and random slopes for compound, enforcing\nlinearity, the main effect of branching condition and its interaction with\nnormalized time is fully significant, just as in the original study.  This\nindicates that the variability in the realization of the pitch contours of the\nindividual compounds is too large to support a main effect of branching condition. \n\n\n\n\n\n\nWe therefore remove branching condition from the model specification, and\ncompleting the model with a smooth for the frequency of occurrence of the compound, \n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(PitchSemiTone}}} {\\textcolor[rgb]{0,0,0}{{~}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Sex}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(LogFrequency)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                      {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                      {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Compound, Sex,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                      {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime, Speaker,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                      {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(NormalizedTime, Compound,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n      {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=pitchc,}}}\n      {\\textcolor[rgb]{0.333,0.667,0.333}{{rho}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{0.98}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{AR.start}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=pitchc}}}{\\textcolor[rgb]{0,0,0}{{$}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{NewTimeSeries)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\nwe zoom in on the interaction of compound (random-effect factor) by sex\n(fixed-effect factor), specified above as {\\tt s(Compound, Sex, bs=\"re\")}.\nFigure~\\ref{fig:pitchSexesFreq} presents a dotplot for the coefficients for the\nfemales on the horizontal axis against the coefficients for the males on the\nvertical axis.  Words for which the males tend to raise their pitch are {\\em\npassenger test flight, family christmas dinner}, and {\\em kidney stone\nremoval}, whereas males lower their pitch for {\\em money market fund}.\nFemales, on the other hand, lower their pitch for {\\em tennis grass court, lung\ncancer surgery}, and {\\em passenger test flight}, but raise their pitch for\n{\\em maple syrup production, piano sheet music}, and {\\em hay fever treatment}.\nThe two sets of coefficients may even be correlated ($r = -0.31, t(38) =\n0.049$), such that where males substantially raise their pitch, females lower\ntheir pitch, and vice versa, possibly reflecting subtle differences in what\ntopics the different sexes find exciting and unexciting \\citep[for pitch\nraising as an index of excitement, see,\ne.g.,][]{paeschke1999f0,trouvain2000prosody,traunmuller1995frequency}.\\footnote{\nThe details of the coefficients in the present model differ from those obtained in the analysis of\n\\citet{Baayen:2013}.  Thanks to the factor smooths for subject and compound and\nthe inclusion of a thin plate regression spline for word frequency, the present\nmodel provides a better fit ({\\sc aic} 177077.4 versus 187308), suggesting the\npresent reanalysis may provide a more accurate window on sex-specific\nrealizations of compounds' pitch.}\n\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth]{pitchSexesFreq.pdf}\n\n  \\caption{By-compound random contrasts for sex in the {\\sc gamm} fitted to the\n  pitch contour of English tri-constituent compounds.  Positive adjustments\n  indicate a higher pitch.}\n\n  \\label{fig:pitchSexesFreq}\n\\end{figure}\n\n\n\nThis case study illustrates three methodological points.  First, including\nrandom effect curves (by means of factor smooths) for subjects and items may\nlead to substantially different conclusions about the form of smooth terms in\nthe fixed-effect part of the model specification.   Just as including random\nslopes for a factor $X$ may render the main effect of $X$ non-significant in\nthe context of a linear mixed-effects model, so inclusion of random wiggly\ncurves for a time series $t$ may render an interaction {\\tt s(t, by=X)}\nnon-significant.  Second, the coefficients of random-effect interactions such\nas {\\tt Compound} by {\\tt Sex} may yield novel insights, especially in the\npresence of correlational structure.  Third, when residuals reveal\nautocorrelational structure, the {\\sc ar(1)} parameter $\\rho$ should be chosen\nhigh enough to remove substantial autocorrelational structure, but not so high\nthat new, artificial autocorrelational structure is artefactually forced onto\nthe data.\n\n\n\\section{Time series in EEG registration}\n\n\n\nSimilar to the pitch data, {\\sc eeg} data comprise many small time series, one\nfor each event for which a subject's electrophysiological response to a\nparticular stimulus is recorded.\n\\citet{DeCat:Baayen:Klepousniotou:2014,DeCat:Klepousniotou:Baayen:2015} used\nEnglish compounds as stimuli, presented in their grammatical order ({\\em coal\ndust}) and in a manipulated, reversed and ungrammatical order ({\\em dust coal})\nto native speakers of English as well as advanced Spanish and German learners\nof English.  The goal of this study was to clarify whether proficiency and\nlanguage background would be reflected in different electrophysiological\nprocessing signatures for these compounds.  For the purposes of the present\nstudy, the specification of the random-effects structure and the measures taken\nto bring autocorrelational structure in the residuals under control, and the\neffects of the choice of $\\rho$ on the fixed-effect predictors and covariates\nin the model are of particular interest.  In what follows, the analysis is\nrestricted to the subset of native speakers of English, and to the {\\sc eeg}\nat channel C1.\\footnote{Data points with an absolute amplitude exceeding 15 $\\mu V$,\napproximately 2.6\\% of the data points, were removed to obtain an approximately\nGaussian response variable.}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model for these data,\n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.345,0.345,0.345}{{eeg.gam}}} {\\textcolor[rgb]{0.69,0.353,0.396}{{=}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Amplitude}}} {\\textcolor[rgb]{0,0,0}{{~}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Time,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{10}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Time,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{by}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=ConstituentOrder,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{10}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{te}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(LogFreqC1, LogFreqC2,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{4}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{te}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(LogFreqC1, LogFreqC2,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{by}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=ConstituentOrder,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{4}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(LogCompFreq,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{4}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(LogCompFreq,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{by}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=ConstituentOrder,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{k}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{4}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Compound,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}}{\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Trial, Subject,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}}{\\textcolor[rgb]{0,0,0}{{+}}}\n  {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Time, Subject,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n  {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=eegC1,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{family}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"scat\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}}\n  {\\textcolor[rgb]{0.333,0.667,0.333}{{AR.start}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=Start,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{rho}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{0.85}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\ncomprises a smooth for time for the compounds presented with their constituents\nin the normal order (e.g., {\\em goldfish}), and a difference curve for the\ncondition in which constituent order is reversed ({\\em fishgold}).  The model\nfurthermore takes an interaction of the constituent frequencies into account by\nmeans of a tensor product smooth, as well as the corresponding difference\nsurface for the reversed order condition.  In the light of the very large\nnumber of observations (207,600), we slightly lowered the upper bound of the\nnumber of basis functions in a given dimension to $k = 4$, in order to avoid\nfitting overly wiggly surfaces.  A thin plate regression spline is introduced\nto account for the effect of compound frequency, again allowing for a\ndifference between the standard and reversed word order.  Random intercepts for\ncompound, and two by-subject factor smooths, one for {\\tt Time} and one for the\nsequence of trials in the experiment ({\\tt Trial}, complete the model\ndescription.  The model summary is given by Table~\\ref{tab:gam}.\n\n\n\n\\begin{table}[ht]\n  \\caption{Generalized additive mixed model fitted to the electrophysiological\n  response of the brain at channel C1 to compound stimuli. Rev: reversed\n  constituent order in the compound, Norm: normal order. {\\tt s}: thin plate\n  regression spline, {\\tt te}: tensor product smooth, {\\tt re}: random\n  intercepts, {\\tt fs}: factor smooth. ($\\rho = 0.85$)} \n\\label{tab:gam}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lrrrr}\\hline\n\nA. parametric coefficients & Estimate & Std. Error & t-value & p-value \\\\ \n\n  (Intercept) & 0.0552 & 0.4221 & 0.1308 & 0.8960 \\\\ \\hline\n\nB. smooth terms & edf & Ref.df & F-value & p-value \\\\ \n\n  s(Time) & 8.5653 & 8.6645 & 14.6953 & $<$ 0.0001 \\\\ \n  s(Time):Order=reversed & 1.5768 & 1.9624 & 0.9999 & 0.4139 \\\\ \n  s(CompFreq) & 1.7242 & 1.7703 & 0.7804 & 0.3172 \\\\ \n  s(CompFreq):Order=reversed & 2.6384 & 2.8746 & 21.1108 & $<$ 0.0001 \\\\ \n  te(FreqC1,FreqC2) & 6.5652 & 6.6936 & 4.4840 & 0.0032 \\\\ \n  te(FreqC1,FreqC2):Order=reversed & 9.6440 & 10.5906 & 10.9593 & $<$ 0.0001 \\\\ \n  re(Compound) & 99.1995 & 112.0000 & 10.1991 & $<$ 0.0001 \\\\ \n  fs(Trial,Subject) & 49.5668 & 89.0000 & 12.6940 & $<$ 0.0001 \\\\ \n  fs(Time,Subject) & 67.4796 & 89.0000 & 8.5343 & $<$ 0.0001 \\\\  \\hline\n\\end{tabular*}\n\\end{table}\n\nThe contributions of the by-subject factor smooths to the model fit is\npresented in Figure~\\ref{fig:factorSmoothSubject}. The grey dots represent the\nby-subject average amplitude for each of the points in time $t = 4, 8, 12,\n\\ldots$ milliseconds.  The red line shows the average of the model fit for\nthe same points in time.  The blue lines visualize the by-subject factor\nsmooths for {\\tt Trial}.  Comparing the red and blue lines, it is clear that\na substantial part of the wiggliness of the model fit is contributed by the\nfactor smooths.  This figure also illustrates the limitations of the factor\nsmooths: When trends are spiky, as for instance for subjects s5 ad s6 early in\ntime, a strongly penalized smooth will not be able to fit the data points in\nthe spike.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{eegSmoothers2Color.pdf}\n\n  \\caption{The by-subject factor smooths for {\\tt Time} in the {\\sc gamm}\n  fitted to the {\\sc eeg} data.  Dots represent average response times, the\n  red lines represent the corresponding average for the model fit, and the \n  blue lines the individual factor smooths.}\n\n  \\label{fig:factorSmoothSubject}\n\\end{figure}\n\n\nFigure~\\ref{fig:residsEEG} illustrates, for four events, that $\\rho$ cannot be\nextended much beyond 0.85 without introducing artefactual negative\nautocorrelations.  Interestingly, changing $\\rho$ may have consequences for the\npredictors of theoretical interest.  Figure~\\ref{fig:EEGconsequences}\nillustrates this point for four smooths in the model.  The top panels show that\nby increasing $\\rho$, the effect of word frequency, which at first blush\nappears to be nonlinear, becomes a straightforward linear effect.  The second\nrow of panels clarifies that the difference curve for Time, contrasting the\nreversed word-order condition with the normal order, is not trustworthy (see\nalso Table~\\ref{tab:gam}).  The increase in the 95\\% confidence interval that\nis a consequence of increasing $\\rho$ to 0.85, which is required to remove the\nthick autocorrelative structure in the residuals (Figure~\\ref{fig:residsEEG},\nleft columns), is noteworthy.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{eegNewPlotAcfsNov9Col.pdf}\n\n  \\caption{Autocorrelation functions for the residuals of {\\sc gamm}s fitted\n  to the amplitude of the {\\sc eeg} response to visually presented compounds,\n  for four events (rows), for $\\rho = 0, 0.4, 0.6, 0.85$ (columns).}\n\n  \\label{fig:residsEEG}\n\\end{figure}\n\n\nThe third and fourth rows of Figure~\\ref{fig:EEGconsequences} illustrate that\nthe regression surface for the frequencies of the compound's constituents\ndepends on constituent order (a threeway interaction of the frequency of the\nfirst constituent, the frequency of the second constituent, and constituent\norder).  The contour plots in the third row show the combined effect of the\nconstituent frequencies for the normal constituent order, modeled with a tensor\nproduct smooth.  Amplitudes are greater along most of the main diagonal,\nsuggesting qualitative differences in lexical processing for similar versus\ndissimilar constituent frequencies.  For the normal constituent order, this\nsurface is hardly affected by increasing $\\rho$.  This does not hold for the\ncorresponding difference surface, as can be seen in the bottom row of\nFigure~\\ref{fig:EEGconsequences}.  In the presence of strong autocorrelations,\nautocorrelative noise is incorporated into the tensor surface, leading to\noveraccentuated and uninterpretable patterns in the lower right corner of the\npartial effect plots. It is only for $\\rho=0.9$ that these irregularities\ndisappear, to give way to a more interpretable difference surface: Amplitudes\nin the reversed order condition are reduced compared to the normal constituent\norder when both constituents are of a high frequency, whereas amplitudes\nincrease when both frequencies are low. Thus, this difference surface suggests\nthat the effect of the constituent frequencies in the normal order is largely\nabsent when constituent order is reversed.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth]{eegNewFigNov2015.pdf}\n\n  \\caption{\n    The consequences of increasing $\\rho$ from 0 to 0.9 (columns) for the\n    effect of frequency (top), the difference curve for Time contrasting the\n    reversed constituent order with the normal order,  the interaction of the\n    frequencies of the first and second constituents (third row), and the\n    difference surface for these predictors contrasting the reversed with the\n    normal constituent order (fourth row). \n  }\n\n  \\label{fig:EEGconsequences}\n\\end{figure}\n\nIn summary, removal of autocorrelative structure in the residuals by means of\nthe $\\rho$ parameter for an {\\sc ar(1)} error process may have two important\nconsequences.  First of all, analyses will tend to become more conservative.\nSecond, the functional form of nonlinear partial effects may change.  In the\npresent examples, excess wiggliness is removed.  \n\n\n\\section{Concluding remarks}\n\n\nThis study illustrates with three examples the potential of generalized\nadditive mixed models for the analysis of language data: response latencies for\nreading aloud, pitch contours of three-constituent compounds, and the\nelectrophysiological response of the brain to grammatical and ungrammatical\ncompounds.  \n\n{\\sc Gamm}s provide the analyst with two tools for coming to grips with\nautocorrelational structure in the model residuals: factor smooths and the {\\sc\nar(1)} $\\rho$ parameter.  In the standard linear mixed effects model, systematic\nchanges in how a subject performs over the course of an experiment, or during\nan experimental trial with a time-series structure, can only be accounted for\nby means of random intercepts and random slopes.   Factor smooths relax this\nassumption of linearity, and thereby have the potential to provide much tighter\nfits when random-effect factors indeed behave in a non-linear way.  \n\nAutocorrelational structure in the errors may, however, remain even after\ninclusion of factor smooths.  For the reaction times revisited in this study,\nmost of the autocorrelational structure was accounted for by means of factor\nsmooths for the time series constituted by a participant's responses over the\ntime course of the experiment.  A mild value of the {\\sc ar(1)} correlation\nparameter ($\\rho = 0.3$) was sufficient to further whiten the residuals.  For\nthe pitch data, and the same holds for the {\\sc eeg} data, inclusion of\nby-participant and by-item factor smooths was not successful at all for\nremoving the autocorrelation.  Here, a high value for the {\\sc ar(1)} correlation\nparameter was necessary for approximate whitening of the errors. \n\nWhitening the errors is important for two reasons \\citep[see also][for further\ndiscussion]{Baayen:Vasishth:Bates:Kliegl:2015}. First, it protects the analyst\nagainst anti-conservative p-values.  Second, models with whitened errors are\nmore likely to provide an accurate window on the quantitative structure of the\ndata.  The analysis of pitch contours provided an example of the inclusion of a\nfactor smooth rendering a time by fixed-factor interaction non-significant.\nFurthermore, whitening {\\sc ar(1)} errors may change the functional form of the\neffect of predictors of interest.  The analysis of the {\\sc eeg} data\nillustrated how an effect that initially seemed nonlinear became\nstraightforwardly linear, as well as a non-linear regression surface that\nbecame simplified and better interpretable thanks to whitening.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 13665, "prevtext": "\n\n\\begin{center}\n  {\\bf Autocorrelated errors in experimental data in the language sciences: \\\\\n       Some solutions offered by Generalized Additive Mixed Models} \\\\\n\\ \\\\\n\\ \\\\\n\nR. Harald Baayen$^{a,b}$, Jacolien van Rij$^a$, Cecile de Cat$^c$ and Simon Wood$^d$ \\\\\n\\ \\\\\n$^a$Eberhard Karls University, T\\\"{u}bingen, Germany \\\\\n$^b$The University of Alberta, Edmonton, Canada \\\\\n$^c$University of Leeds, UK \\\\\n$^d$University of Bath, UK.\n\\ \\\\\n\\ \\\\\n\\date{\\today}\n\\end{center}\n\n\\vspace*{2\\baselineskip}\n\n\n\n\\section{Introduction}\n\\label{sec:1}\n\nA problem that tends to be ignored in the statistical analysis of experimental\ndata in the language sciences is that responses often constitute time series,\nwhich raises the problem of autocorrelated errors.  If the errors indeed show\nautocorrelational structure, evaluation of the significance of predictors in\nthe model becomes problematic due to potential anti-conservatism of p-values.\n\nThis paper illustrates two tools offered by Generalized Additive Mixed Models\n({\\sc gamm}s) \\citep{Lin:Zhang:1999,Wood:2006,Wood:2011,Wood:2013} for dealing\nwith autocorrelated errors, as implemented in the current version of the fourth\nauthor's {\\sc mgcv} package (1.8.9): the possibility to specify an {\\sc ar(1)}\nerror model for Gaussian models, and the possibility of using factor smooths\nfor random-effect factors such as subject and item.  These factor smooths are\nset up to have the same smoothing parameters, and are penalized to yield the\nnon-linear equivalent of random intercepts and random slopes in the\nclassical linear framework.\n\nThree examples illustrate the possibilities offered by {\\sc gamm}s.  First, a\nstandard chronometric task, word naming, is examined, using data originally\nreported in \\citet{Tabak:2010}.  In this task, and similar tasks such as\nlexical decision, a participant is asked to respond to stimuli presented\nsequentially.  The resulting sequence of responses constitute a time series in\nwhich the response at time $t$ may not be independent from the response at time\n$t-1$.  For some participants, this non-independence may stretch across 20\nor more lags in time.   Second, a study investigating the pitch contour\nrealized on English three-constituent compounds\n\\citep{Koesling:Kunter:Baayen:Plag:2012} is re-examined.  As pitch changes\nrelatively slowly and relatively continuously, autocorrelation structure is\nstrongly present.  A reanalysis that brings the autocorrelation under\nstatistical control leads to conclusions that differ substantially from those\nof the original analysis.  The third case study follows up on a model reported\nby \\citet{DeCat:Baayen:Klepousniotou:2014,DeCat:Klepousniotou:Baayen:2015}\nfitted to the amplitude over time of the brain's electrophysiological response\nto visually presented compound words.  We begin with a short general introduction\nto {\\sc gamm}s.\n\n\n\\section{Generalized additive mixed models}\n\n\nGeneralized additive mixed models extend the generalized linear mixed model\nwith a large array of tools for modeling nonlinear dependencies between a\nresponse variable and one or more numeric predictors.   For nonlinear\ndependencies involving a single predictor, thin plate regression splines are\navailable.  Thin plate regression splines ({\\sc tprs}) model the response by\nmeans of a weighted sum of smooth regular basis functions that are chosen such\nthat they optimally approximate the response, if that response is indeed a\nsmooth function.  The basis functions of {\\sc tprs} have much better\nmathematical properties compared to basis functions that are simple powers of\nthe predictor (quadratic or higher-order polynomials).  Importantly, the\nsmoother is penalized for wiggliness, such that when fitting a {\\sc gamm}, an\noptimal balance is found between undersmoothing and oversmoothing.\n\nWhen a response depends in a nonlinear way on two or more numeric predictors\nthat are on the same scale, {\\sc tprs} can also be used to fit wiggly\nregression surfaces or hypersurfaces, approximated by means of weighted sums of\nregular surfaces which are again penalized for wiggliness.  When predictors are\nnot isometric, tensor product smooths should be used.  Tensor product smooths\n({\\sc tps}) approximate a wiggly surface or hypersurface using as basis\nfunctions restricted cubic splines, again with penalization for wiggliness.  \n\nInteractions of numerical predictors with a factorial predictor can be\naccomodated in two ways.  One option is to fit a different wiggly line or\nsurface for each level of such a factor.  Alternatively, one may want to take\none of the factor levels as reference level, fit a smooth for the reference\nlevel, and then fit difference curves or difference surfaces for the remaining\nfactor levels.  These difference curves have an interpretation similar to\ntreatment contrasts for dummy coding of factors: The difference curve for level\n$k$, when added to the curve for the reference level, results in the actual\npredicted curve for factor level $k$.\n\nWhen a factor has many different levels, as is typically the case for\nrandom-effect factors, it may be desirable to require the individual smooths\nfor the different factor levels to have the same smoothing parameter.  Together\nwith a heavier penalty for moving away from zero, the resulting `factor\nsmooths' are the nonlinear equivalent of the combination of random intercepts\nand random slopes in the linear mixed model.\n\nIn what follows, examples are discussed using {\\tt R}, which follows\n\\citet{Wilkinson:Rogers:1973} for the specification of statistical models.\nExtensions to the notation for model formulae made within the context of the\npackage for linear mixed models \\citep[{\\sc\nlme4},][]{Bates:Maechler:Bolker:Walker:2015} and the {\\tt mgcv} package for\ngeneralized additive mixed models \\citep{Wood:2006,Wood:2011} are explained\nwhere used first.  \n\n\n\n\n\\section{Time series in a word naming task}\n\n\n\nAlthough there is awareness in the field of inter-trial dependencies in\nchronometric behavioral experiments\n\\citep{Broadbent:1971,Welford:1980,Sanders:1998,Taylor:Lupker:2001}, efforts to\ntake such dependencies into account are scarce.\n\\citet{deVaan:Schreuder:Baayen:2007} and \\citet{Baayen:Milin:2010} attempted to\ntake the autocorrelation out of the residual error by including as a covariate\nthe response latency elicited at the preceding trial.  This solution, however,\nalthough effective, is not optimal from a model-building perspective, as the\nsource of the autocorrelation is not properly separated out from the other\nfactors that co-determine the response latency at the preceding timestep.\n\nTo illustrate the phenomenon, consider data from a word naming study on Dutch\n\\citep{Tabak:2010}, in which subjects were shown a verb on a computer screen,\nand were requested to read out loud the corresponding past (or present) tense\nform.  The upper row of panels of Figure~\\ref{fig:residsNaming} presents the\nautocorrelation function for selected, exemplary, subjects.  The\nautocorrelation function presents, for lags 0, 1, 2, 3, \\ldots the correlation\ncoefficient obtained when the vector of responses $\\boldmath{v}_1$ at trials 1,\n2, 3, \\ldots is correlated with the vector of responses $\\boldmath{v}_l$ at\ntrials 1+l, 2+l, 3+l, \\ldots ($l >= 0$).  At lag $l=0$, the correlation is\nnecessarily 1.  As the lag increases, the correlation tends to decrease.  For\nsome subjects, there is significant autocorrelation at short lags, as\nillustrated in the first two panels. The subject in the third panel shows a\n``seasonal'' effect, with an initial positive correlation morphing into a\nnegative correlation around lag 10.  The subjects in the next two panels show a\nvery different pattern, with autocorrelations persisting across more than 20\nlags.\n\n\n\n\n\n\n\n\n\\begin{figure}\n  \n\n  \n  \\includegraphics[width=\\textwidth]{namingNewCol.pdf}\n\n  \\caption{Autocorrelation functions for the residuals of selected participants\n  in the word naming task: top: observed response latencies; second row:\n  residuals of a linear mixed-effects model with random by-participant\n  intercepts and slopes for Trial; third row: residuals of a {\\sc gamm} with\n  by-participant wiggly curves; fourth row: residuals of a {\\sc gamm} with\n  by-participant wiggly curves and correction for {\\sc ar(1)} with $\\rho =\n  0.3$. Significant autocorrelations are shown in red, non-significant\n  autocorrelations are presented in blue.}\n\n  \\label{fig:residsNaming}\n\\end{figure}\n\n\nThe second row of panels in Figure~\\ref{fig:residsNaming} presents the\nautocorrelation functions for the residuals of a linear mixed-effects model\nfitted to the word naming latencies with random intercepts for item (verb) and\nby-subject random intercepts as well as by-subject random slopes for Trial (the\norder number of the word in the experimental list, i.e., the variable defining\nthe time series in this data set).  Using the {\\tt lme4} package\n\\citep{Bates:Maechler:Bolker:Walker:2015} for {\\tt R} (version 3.0.2), the\nspecification of the random effects ({\\tt (1 + Trial|Subject)}) requests\nby-subject random intercepts, by-subject random slopes for {\\tt Trial}, and a\ncorrelation parameter for the random intercepts and slopes.\n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.345,0.345,0.345}{{naming.lmer}}} {\\textcolor[rgb]{0.69,0.353,0.396}{{=}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{lmer}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(RT}}} {\\textcolor[rgb]{0,0,0}{{~}}}  {\\textcolor[rgb]{0.345,0.345,0.345}{{Regularity}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Number}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Voicing}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{InitialNeighbors}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                   {\\textcolor[rgb]{0.345,0.345,0.345}{{InflectionalEntropy}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{poly}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Frequency,}}} {\\textcolor[rgb]{0.686,0.059,0.569}{{2}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Trial}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                   {\\textcolor[rgb]{0.345,0.345,0.345}{{(}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Trial}}}{\\textcolor[rgb]{0,0,0}{{|}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{Subject)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{(}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0,0,0}{{|}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{Verb),}}}\n                   {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{= naming)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\\noindent\nFigure~\\ref{fig:residsNaming} (second row) indicates that the thick\nautocorrelational structure for subjects 17 and 10 has been eliminated by the\nby-subject random regression lines, but the less prominent autocorrelational\nstructure for the other subjects has remained virtually unchanged.\n\nThe third row of panels of Figure~\\ref{fig:residsNaming} shows that a {\\sc\ngamm} with by-subject factor smooths for Trial, replacing the by-subject\nstraight lines of the linear mixed model yields very similar results.\nUsing the {\\tt bam} function from {\\tt mgcv} for {\\tt R}, \nthe model specification\n\\begin{knitrout}\\small\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{kframe}\n\\begin{alltt}\n{\\textcolor[rgb]{0.345,0.345,0.345}{{naming.gam}}} {\\textcolor[rgb]{0.69,0.353,0.396}{{=}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{bam}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(RT}}} {\\textcolor[rgb]{0,0,0}{{~}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Regularity}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Number}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{Voicing}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.345,0.345,0.345}{{InitialNeighbors}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                 {\\textcolor[rgb]{0.345,0.345,0.345}{{InflectionalEntropy}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Frequency)}}} {\\textcolor[rgb]{0,0,0}{{+}}}\n                 {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Trial, Subject,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"fs\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{,}}}{\\textcolor[rgb]{0.333,0.667,0.333}{{m}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.686,0.059,0.569}{{1}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{)}}} {\\textcolor[rgb]{0,0,0}{{+}}} {\\textcolor[rgb]{0.737,0.353,0.396}{\\textbf{{s}}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{(Verb,}}} {\\textcolor[rgb]{0.333,0.667,0.333}{{bs}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=}}}{\\textcolor[rgb]{0.192,0.494,0.8}{{\"re\"}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{),}}}\n                 {\\textcolor[rgb]{0.333,0.667,0.333}{{data}}}{\\textcolor[rgb]{0.345,0.345,0.345}{{=naming)}}}\n\\end{alltt}\n\\end{kframe}\n\\end{knitrout}\n\n\\noindent\nrequests random intercepts for the verbs ({\\tt s(Verb, bs=\"re\")}) and\nby-subject wiggly penalized curves for {\\tt Trial} ({\\tt s(Trial, Subject,\nbs=\"fs\", m=1)}, here, {\\tt bs=\"fs\"} requests factor smooths with the same\nsmoothing parameters across subjects, and {\\tt m=1} requests shrinkage to\nobtain wiggly random effects).\n\nAn improvement is obtained by including an autoregressive {\\sc ar(1)} process\nfor the errors:\n\n", "index": 1, "text": "\\begin{equation}\n  e_{t} = \\rho e_{t-1} + \\epsilon_{t},  \\hspace*{1em}  \\epsilon_{t} \\sim {\\cal N}(0, \\sigma).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"e_{t}=\\rho e_{t-1}+\\epsilon_{t},\\hskip 10.0pt\\epsilon_{t}\\sim{\\cal N}(0,\\sigma).\" display=\"block\"><mrow><mrow><mrow><msub><mi>e</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><msub><mi>e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>\u03f5</mi><mi>t</mi></msub></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><msub><mi>\u03f5</mi><mi>t</mi></msub><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>\u03c3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]