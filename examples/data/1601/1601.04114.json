[{"file": "1601.04114.tex", "nexttext": "\n\nHere $f$ is the original objective function, and $g$ is its time evolution according to the heat equation. Here $\\Delta_{\\boldsymbol{x}}$ is the Laplace operator w.r.t. the variable $\\boldsymbol{x}$. Diffusion is a powerful tool for simplifying the objective function. For example, the number of local minima in the Ackley's function \\cite{ackley} is exponential in the number of variables. By diffusing this function via the heat equation, however, all local minima eventually disappear (see Figure \\ref{fig:ackley}).\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.3\\textwidth]{ackley1.png}\n\\includegraphics[width=.3\\textwidth]{ackley2.png}\n\\includegraphics[width=.3\\textwidth]{ackley3.png}\n\\caption{Diffusion of Ackley's function with time progressing from the left to the right plot.}\n\\label{fig:ackley}\n\\end{figure}\n\nGoing from the nonlinear PDE of \\cite{Vese} to the (linear) heat equation is computationally of great value. That is, the solution to the heat equation is known analytically \\cite{Widder75}: it is the Gaussian convolution of the original (objective) function and the bandwidth parameter of the Gaussian determines the time point at which the diffused function is evaluated. Diffusion combined with the path following lead to a simple optimization algorithm listed in Algorithm \\ref{alg:alg_goal}.\n\n\\begin{algorithm} [t]\n\\caption{Algorithm for Optimization by Diffusion and Continuation}\n\\label{alg:alg_goal}\n\\begin{algorithmic} [1]\n\\STATE Input: $f:\\mathcal{X} \\rightarrow \\mathbb{R}$, Sequence $\\infty>\\sigma_0>\\sigma_1>\\dots>\\sigma_m = 0$.\n\\STATE $\\boldsymbol{x}_0=$ global minimizer of $g(\\boldsymbol{x};\\sigma_0)$.\n\\FOR {{$k=1$} \\textbf{to} {$m$}}\n\\STATE $\\boldsymbol{x}_{k}=$ Local minimizer of $g(\\boldsymbol{x};\\sigma_k)$, initialized at $\\boldsymbol{x}_{k-1}$.\n\\ENDFOR\n\\STATE Output: $\\boldsymbol{x}_m$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\section{Diffused Activation Functions}\n\n\nLet $k_\\sigma(\\boldsymbol{w})$ be the Gaussian kernel with zero mean and covariance $\\sigma^2 \\boldsymbol{I}$. The diffused activation functions listed in Table \\ref{tab:Table} are simply obtained\\footnote{All listed diffused functions are exact except $\\tanh$. Unfortunately, $\\tanh \\star k_\\sigma$ does not have a closed form. We leverage the approximation $\\tanh(y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} y)$. Notice that we know the exact diffused form for ${\\operatorname{erf}}$ as listed in the table. Thus, by convolving both sides with $k_\\sigma$ we obtain $[\\tanh \\star k_\\sigma] (y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} \\frac{y}{\\sqrt{1+\\frac{\\pi}{2} \\sigma^2}})$. The R.H.S. of the latter form can be again approximated via $\\tanh(y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} y)$. This leads to the approximate identity $[\\tanh \\star k_\\sigma] (y) \\approx \\tanh( \\frac{y}{\\sqrt{1+\\frac{\\pi}{2} \\sigma^2}})$.} by convolving them with the Gaussian $k_\\sigma$. Similar forms of smoothed ReLU and ${\\operatorname{sign}}$ are used by \\cite{jordan} with a fixed $\\sigma=\\frac{1}{\\sqrt{2 \\pi}}$, for a proving learnability of deep networks.\n\n\\begin{table}\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\n\\bf Name & \\bf Original & \\bf Diffused \\\\\n\\hline\nSign & ${\\operatorname{sign}}(x)$ & ${\\operatorname{erf}}(\\frac{x}{\\sqrt{2}\\sigma})$ \\\\\n\\hline\nError & ${\\operatorname{erf}}(a x)$ & ${\\operatorname{erf}}(\\frac{a x}{\\sqrt{1+2 (a \\sigma)^2}})$ \\\\\n\\hline\nTanh & $\\tanh(x)$ & $\\tanh( \\frac{x}{\\sqrt{1 +\\frac{\\pi}{2}\\sigma^2 } })$ \\\\\n\\hline\nReLU & $\\max(0,x)$ & $\\frac{\\sigma}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2 \\sigma^2} } + \\frac{1}{2} x \\big (1+{\\operatorname{erf}}(\\frac{x}{\\sqrt{2}\\sigma}) \\big)$ \\\\\n\\hline\n\\end{tabular}\n\\caption{List of some functions and their diffused form by the heat kernel.}\n\\end{center}\n\\label{tab:Table}\n\\end{table}\n\n\\begin{center}\n\\begin{tabular}{c c c}\n\\includegraphics[width=.32\\textwidth]{sign.png} &\n\\includegraphics[width=.32\\textwidth]{tanh.png} &\n\\includegraphics[width=.32\\textwidth]{relu.png} \\\\\n$Sign$ & $Tanh$ & $ReLU$\n\\end{tabular}\n\\end{center}\n{\\footnotesize{Plot of smoothed responses of activation functions within $x \\in [-2,2]$. Blue is the original function. Red, green, and orange show the suggested functions with $\\sigma_{\\mbox{red}}< \\sigma_{\\mbox{grn}} <\\sigma_{\\mbox{orn}} $.}}\n\n\n\\section{Training RNNs}\n\n\\subsection{RNN Cost Function}\n\nGiven a set of $S$ training sequences, each of length $T$. Denote the $s$'th sequence by $\\langle (\\boldsymbol{x}_{s,1},\\boldsymbol{y}_{s,1}), \\dots, (\\boldsymbol{x}_{s,T},\\boldsymbol{y}_{s,T}) \\rangle$. Given some discrepancy function $d$. The problem of sequence learning by an RNN can be stated as below,\n\n\\begin{eqnarray}\n& & \\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}} \\sum_{s=1}^S \\sum_{t=1}^T d(h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t}) \\\\\n\\mbox{s.t.} & & \\boldsymbol{n}_{s,t} \\triangleq \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} \\\\\n& & \\boldsymbol{m}_{s,t} \\triangleq \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} \\,,\n\\end{eqnarray}\n\nwhere $\\boldsymbol{a}$, $\\boldsymbol{b}$, $\\boldsymbol{m}_0$, $\\boldsymbol{W}$, $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are the weights of the network. Denote the dimension of $\\boldsymbol{x}_{s,t}$ and $\\boldsymbol{y}_{s,t}$ be $X$ and $Y$ respectively. Also denote the number of neurons by $H$. Then, $\\boldsymbol{a}$ is $H \\times 1$, $\\boldsymbol{b}$ is $Y \\times 1$, $\\boldsymbol{m}_0$ is $H \\times 1$, $\\boldsymbol{W}$ is $Y \\times H$, $\\boldsymbol{U}$ is $H \\times X$, and $\\boldsymbol{V}$ is $H \\times H$. Obviously $\\boldsymbol{n}_{s,t}$ is $Y \\times 1$ and $\\boldsymbol{m}_{s,t}$ is $H \\times 1$.\n\nSuppose $\\boldsymbol{m}_{s,0}=\\boldsymbol{m}_0$, i.e. the initial state is independent of the training sequence. Here $h$ is some activation function. When the argument of $h$ is a vector, the result will be a vector of the same size, whose entries consists of the element-wise application of $h$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1\\textwidth]{rnn.png}\n\\end{center}\n\\caption{A Recurrent Neural Network. Figure is adapted with permission from \\cite{RNNHard} and slightly modified.}\n\\end{figure}\n\n\nTreating each $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ as {\\textbf{\\textit{\\color{Myblue}{independent variables}}}} and forcing their definition (equality) by some penalty function, we arrive at the following {\\textbf{\\textit{\\color{Myblue}{unconstrained}}}} problem,\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N}} \\sum_{s=1}^S \\sum_{t=1}^T & & d(h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t}) \\nonumber \\\\\n&+& \\lambda \\Big( p \\big( \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\big) \\,+\\,p \\big( \\boldsymbol{U} \\boldsymbol{x}_t + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\big) \\Big)\\,, \\nonumber\n\\end{eqnarray}\n\nwhere the notation $\\boldsymbol{N}$ and $\\boldsymbol{M}$ are matrices whose columns are comprised of $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ for all choices of $(s,t)$.\n\nLetting, $d(\\boldsymbol{e}) \\triangleq \\| \\boldsymbol{e}\\|^2$ (mean squared error) and $p(\\boldsymbol{e}) \\triangleq \\| \\boldsymbol{e}\\|^2$ (quadratic penalty), the problem can be expressed as below,\n\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N}} \\sum_{s=1}^S \\sum_{t=1}^T & & \\| h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\|^2 \\big)\\,. \\nonumber\n\\end{eqnarray}\n\nHere $\\lambda$ determines the weight of the penalty for constraint violation.\n\n\\subsection{Diffused Cost}\n\nWhen the objective function is evolved according to the diffusion equation (\\ref{eq:diffusion}), the diffused objective has a closed form expression. Specifically, it is obtained by the convolution of the original objective with the Gaussian kernel. This can be more formally expressed as the following. Arrange all optimization variables into a long vector $\\boldsymbol{w}$, i.e. $\\boldsymbol{w} \\triangleq {\\operatorname{vec}}(\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N})$. Hence, the cost function can be denoted by $f(\\boldsymbol{w})$. The diffused cost function $g$ is obtained by:\n\n\n", "itemtype": "equation", "pos": 7873, "prevtext": "\n\n\n\\maketitle\n\n\n\\begin{abstract}\nThis work presents a new algorithm for training recurrent neural networks (although ideas are applicable to feedforward networks as well). The algorithm is derived from a theory in nonconvex optimization related to the diffusion equation. The contributions made in this work are two fold. First, we show how some seemingly disconnected mechanisms used in deep learning such as smart initialization, annealed learning rate, layerwise pretraining, and noise injection (as done in dropout and SGD) arise naturally and automatically from this framework, without manually crafting them into the algorithms. Second, we present some preliminary results on comparing the proposed method against SGD. It turns out that the new algorithm can achieve similar level of generalization accuracy of SGD in much fewer number of epochs.\n\\end{abstract}\n\n\n\n\\section{Introduction}\n\nDeep learning has recently beaten records in image recognition \\cite{AlexNet}, speech recognition \\cite{HintonSpeech} and has made significant improvements in natural language processing \\cite{BahdanauCB14,Sut}. However, currently ``training'' deep networks, and specially recurrent neural networks (RNNs), is a challenging task \\cite{RNNHard}. To improve learning (in terms of convergence speed, attained training cost and generalization error) gradient based optimization methods are often used in combination with other techniques such as smart initialization \\cite{Init13}, layerwise pretraining \\cite{Bengio06}, dropout \\cite{dropout}, annealed learning rate, and curriculum learning \\cite{Beng09Cur}.\n\nThe difficulty in training deep networks is mainly attributed to their optimization landscape, where saddle points \\cite{Dauphin,PascanuDGB14}, plateaus, and sharp curvatures are prevalent. A general strategy for tackling difficult optimization problems is the {\\textbf{\\textit{\\color{Myblue}{continuation method}}}}. This method gradually transforms a highly simplified version of the problem back to its original form while following the solution along the way. The simplified problem is supposedly easy to solve. Then, each intermediate subproblem is initialized by the solution from the previous subproblem until reaching the final problem (see Figure \\ref{fig:continuation}).\n\n\\begin{figure}\n\\centering \\includegraphics[width=.4\\textwidth,height=.3\\textwidth]{path_following.png}\n\\caption{Optimization by the continuation method. Top is the simplified function and bottom is the original complex objective function. The solution of each subproblem initializes the subproblem below it.}\n\\label{fig:continuation}\n\\end{figure}\n\nThere are two loose ends for using optimization by continuation: 1. how to choose the simplified problem, 2. how to transform the simplified problem to the main task. For both of these questions, there are infinite answers. More precisely, given an objective function, there are infinite ways infinite smooth convex functions that could be used as initial ``easy'' task, and also infinite ways to gradually transform that to the main objective function. The quality of the solution attained by the continuation method {\\textbf{\\textit{\\color{Myblue}{critically depends}}}} on these choices. Recently we have proved that these choices can be made optimally via the {\\textbf{\\textit{\\color{Myblue}{diffusion equation}}}} \\cite{MobahiEMMCVPR}. Specifically, the objective function is considered as the initial heat distribution on a domain, and the heat is diffused over time according to the {\\textbf{\\textit{\\color{Myblue}{heat equation}}}}.\n\nThe solution to the heat equation on $\\mathbb{R}^n$ is known analytically: it is the {\\textbf{\\textit{\\color{Myblue}{convolution}}}} of the initial heat distribution (i.e., the objective function) with the {\\textbf{\\textit{\\color{Myblue}{Gaussian kernel}}}}. Obviously, convolution with the Gaussian kernel smoothes the objective function\\footnote{This happens when the objective function has well-defined Fourier transform. Then the convolution transform to product in the frequency domain. As the Fourier transform of the Gaussian is also a Gaussian, the resulted product attenuates higher frequencies.}. The bandwidth parameter $\\sigma$ of the Gaussian kernel determines the amount of smoothing. The algorithm for optimization by diffusion starts from a large $\\sigma$ (highly simplified objective function), and then follows the minimizer as $\\sigma$ shrinks toward zero (which leads to the original cost function).\n\nThe optimality result we derived in \\cite{MobahiEMMCVPR} is a stepping stone for developing practical algorithms. Specifically, it suggests using Gaussian convolution for creating intermediate optimization tasks, but it does not answer whether the resulted convolution could be computed efficiently or not. In fact, the answer to this question is problem specific. We have shown that for some family of functions such as multivariate polynomials, the resulted convolution can be computed in {\\textbf{\\textit{\\color{Myblue}{closed form}}}} \\cite{mobahiclosed}. In this work, we push that result further and show that, up to very reasonable approximation, common objective functions arising in deep learning also have a closed form Gaussian convolution. This is surprising because such objective function is highly nonlinear; involving a nested form of ill-behaved activation functions as such ${\\operatorname{sign}}$ and ReLU.\n\nBy studying deep learning through the diffusion and continuation method, we discover two interesting observations. First, from theoretical viewpoint, some common and successful techniques to improve learning, such as noise injection \\cite{dropout}, layerwise pretraining \\cite{Bengio06}, and annealed learning rate, {\\textbf{\\textit{\\color{Myblue}{automatically emerge}}}} from the diffused cost function. Therefore, this theory {\\textbf{\\textit{\\color{Myblue}{unifies}}}} some seemingly isolated techniques. Second, from a practical viewpoint, training deep networks by this method seems to result in a significant speed up compared to stochastic gradient descent (SGD) method. The preliminary results presented in this draft indicate up to {\\textbf{\\textit{\\color{Myblue}{$25\\%$ reduction in training time}}}} for learning RNNs.\n\nThis article is organized as follows. We first show that the diffused form of common activation functions has a closed form expression. After that, when we compute the diffused cost function for training a deep network, where the result depends on the diffused activation function introduced earlier. We discuss some properties of the diffused cost function and make connections to noise injection \\cite{dropout}, layerwise pretraining \\cite{Bengio06}, and annealed learning rate. We conclude this article by presenting a preliminary evaluation of the proposed algorithm against SGD.\n\n\\section{Optimization by Diffusion and Continuation}\n\nThe optimality of using the diffusion equation for creating intermediate optimization problems is studied in our earlier work \\cite{MobahiEMMCVPR}. Briefly, diffusion is a relaxation of a time evolution process that converts an objective function to its convex envelope\\footnote{The convex envelope of a function is an interesting choice (versus any other convex function) for the initial simplified version of it for various reasons. 1. Any global minimizer of the cost function is also a global minimizer of its convex envelope. 2. it provides the tightest convex underestimator of the cost function. 3. Geometrically, tt is the function whose epigraph coincides with the convex hull of the epigraph of the cost function.} \\cite{Vese}. The latter is a {\\textbf{\\textit{\\color{Myblue}{nonlinear}}}} partial differential equation that lacks a closed form, but once {\\textbf{\\textit{\\color{Myblue}{linearized}}}}, the heat equation (a special type of diffusion equation) arises,\n\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:diffusion}\n\\frac{d}{dt} g(\\boldsymbol{x},t) = \\Delta_{\\boldsymbol{x}} g(\\boldsymbol{x},t) \\quad,\\quad \\mbox{s.t. } g(\\boldsymbol{x},0)=f(\\boldsymbol{x}) \\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\frac{d}{dt}g(\\boldsymbol{x},t)=\\Delta_{\\boldsymbol{x}}g(\\boldsymbol{x},t)%&#10;\\quad,\\quad\\mbox{s.t. }g(\\boldsymbol{x},0)=f(\\boldsymbol{x})\\,.\" display=\"block\"><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>\ud835\udc99</mi></msub><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mo rspace=\"12.5pt\">,</mo><mtext>s.t.\u00a0</mtext><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nAfter computing this convolution, the variables in $\\boldsymbol{w}$ can be replaced by their original names according to the arrangements made in $\\boldsymbol{w} \\triangleq {\\operatorname{vec}}(\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N})$.\n\nDenote the diffused form of the activation function $h$ by $\\tilde{h}_\\sigma$, that is $\\tilde{h}_\\sigma(x) \\triangleq [h \\star k_\\sigma](x)$. Similarly, define $\\widetilde{h^2_\\sigma}(x) \\triangleq [h^2 \\star k_\\sigma](x)$. The diffused cost w.r.t. optimization variables has the following closed form (see Appendix \\ref{sec:diffused_cost}):\n\n\n\\begin{eqnarray}\n\\sum_{s=1}^S \\Bigg( \\sum_{t=1}^T & & \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 + \\| \\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{n}_{s,t}) \\|^2 - \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t} ) \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\|^2 \\nonumber \\\\\n& & \\quad + \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 Y \\,  \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )\\|^2 \\big) \\nonumber \\\\\n+ \\lambda \\sum_{t=0}^{T-1} & & \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 H \\, \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t})\\|^2 \\Bigg)\\,. \\nonumber\n\\end{eqnarray}\n\nHere $\\| \\,.\\,\\|_F$ denotes the Frobenius norm of a matrix.\n\n\\subsection{Approximate Diffused Cost}\n\nIdeal solution requires $S \\times T$ auxiliary variables for $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$. This is not practical as often $S$ is large. Thus, we resort to an {\\textbf{\\textit{\\color{Myblue}{approximate}}}} formulation here. Instead of solving for the optimal $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$, we fix them as below,\n\n\n\n", "itemtype": "equation", "pos": 16801, "prevtext": "\n\nHere $f$ is the original objective function, and $g$ is its time evolution according to the heat equation. Here $\\Delta_{\\boldsymbol{x}}$ is the Laplace operator w.r.t. the variable $\\boldsymbol{x}$. Diffusion is a powerful tool for simplifying the objective function. For example, the number of local minima in the Ackley's function \\cite{ackley} is exponential in the number of variables. By diffusing this function via the heat equation, however, all local minima eventually disappear (see Figure \\ref{fig:ackley}).\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.3\\textwidth]{ackley1.png}\n\\includegraphics[width=.3\\textwidth]{ackley2.png}\n\\includegraphics[width=.3\\textwidth]{ackley3.png}\n\\caption{Diffusion of Ackley's function with time progressing from the left to the right plot.}\n\\label{fig:ackley}\n\\end{figure}\n\nGoing from the nonlinear PDE of \\cite{Vese} to the (linear) heat equation is computationally of great value. That is, the solution to the heat equation is known analytically \\cite{Widder75}: it is the Gaussian convolution of the original (objective) function and the bandwidth parameter of the Gaussian determines the time point at which the diffused function is evaluated. Diffusion combined with the path following lead to a simple optimization algorithm listed in Algorithm \\ref{alg:alg_goal}.\n\n\\begin{algorithm} [t]\n\\caption{Algorithm for Optimization by Diffusion and Continuation}\n\\label{alg:alg_goal}\n\\begin{algorithmic} [1]\n\\STATE Input: $f:\\mathcal{X} \\rightarrow \\mathbb{R}$, Sequence $\\infty>\\sigma_0>\\sigma_1>\\dots>\\sigma_m = 0$.\n\\STATE $\\boldsymbol{x}_0=$ global minimizer of $g(\\boldsymbol{x};\\sigma_0)$.\n\\FOR {{$k=1$} \\textbf{to} {$m$}}\n\\STATE $\\boldsymbol{x}_{k}=$ Local minimizer of $g(\\boldsymbol{x};\\sigma_k)$, initialized at $\\boldsymbol{x}_{k-1}$.\n\\ENDFOR\n\\STATE Output: $\\boldsymbol{x}_m$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\section{Diffused Activation Functions}\n\n\nLet $k_\\sigma(\\boldsymbol{w})$ be the Gaussian kernel with zero mean and covariance $\\sigma^2 \\boldsymbol{I}$. The diffused activation functions listed in Table \\ref{tab:Table} are simply obtained\\footnote{All listed diffused functions are exact except $\\tanh$. Unfortunately, $\\tanh \\star k_\\sigma$ does not have a closed form. We leverage the approximation $\\tanh(y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} y)$. Notice that we know the exact diffused form for ${\\operatorname{erf}}$ as listed in the table. Thus, by convolving both sides with $k_\\sigma$ we obtain $[\\tanh \\star k_\\sigma] (y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} \\frac{y}{\\sqrt{1+\\frac{\\pi}{2} \\sigma^2}})$. The R.H.S. of the latter form can be again approximated via $\\tanh(y) \\approx {\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} y)$. This leads to the approximate identity $[\\tanh \\star k_\\sigma] (y) \\approx \\tanh( \\frac{y}{\\sqrt{1+\\frac{\\pi}{2} \\sigma^2}})$.} by convolving them with the Gaussian $k_\\sigma$. Similar forms of smoothed ReLU and ${\\operatorname{sign}}$ are used by \\cite{jordan} with a fixed $\\sigma=\\frac{1}{\\sqrt{2 \\pi}}$, for a proving learnability of deep networks.\n\n\\begin{table}\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\n\\bf Name & \\bf Original & \\bf Diffused \\\\\n\\hline\nSign & ${\\operatorname{sign}}(x)$ & ${\\operatorname{erf}}(\\frac{x}{\\sqrt{2}\\sigma})$ \\\\\n\\hline\nError & ${\\operatorname{erf}}(a x)$ & ${\\operatorname{erf}}(\\frac{a x}{\\sqrt{1+2 (a \\sigma)^2}})$ \\\\\n\\hline\nTanh & $\\tanh(x)$ & $\\tanh( \\frac{x}{\\sqrt{1 +\\frac{\\pi}{2}\\sigma^2 } })$ \\\\\n\\hline\nReLU & $\\max(0,x)$ & $\\frac{\\sigma}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2 \\sigma^2} } + \\frac{1}{2} x \\big (1+{\\operatorname{erf}}(\\frac{x}{\\sqrt{2}\\sigma}) \\big)$ \\\\\n\\hline\n\\end{tabular}\n\\caption{List of some functions and their diffused form by the heat kernel.}\n\\end{center}\n\\label{tab:Table}\n\\end{table}\n\n\\begin{center}\n\\begin{tabular}{c c c}\n\\includegraphics[width=.32\\textwidth]{sign.png} &\n\\includegraphics[width=.32\\textwidth]{tanh.png} &\n\\includegraphics[width=.32\\textwidth]{relu.png} \\\\\n$Sign$ & $Tanh$ & $ReLU$\n\\end{tabular}\n\\end{center}\n{\\footnotesize{Plot of smoothed responses of activation functions within $x \\in [-2,2]$. Blue is the original function. Red, green, and orange show the suggested functions with $\\sigma_{\\mbox{red}}< \\sigma_{\\mbox{grn}} <\\sigma_{\\mbox{orn}} $.}}\n\n\n\\section{Training RNNs}\n\n\\subsection{RNN Cost Function}\n\nGiven a set of $S$ training sequences, each of length $T$. Denote the $s$'th sequence by $\\langle (\\boldsymbol{x}_{s,1},\\boldsymbol{y}_{s,1}), \\dots, (\\boldsymbol{x}_{s,T},\\boldsymbol{y}_{s,T}) \\rangle$. Given some discrepancy function $d$. The problem of sequence learning by an RNN can be stated as below,\n\n\\begin{eqnarray}\n& & \\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}} \\sum_{s=1}^S \\sum_{t=1}^T d(h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t}) \\\\\n\\mbox{s.t.} & & \\boldsymbol{n}_{s,t} \\triangleq \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} \\\\\n& & \\boldsymbol{m}_{s,t} \\triangleq \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} \\,,\n\\end{eqnarray}\n\nwhere $\\boldsymbol{a}$, $\\boldsymbol{b}$, $\\boldsymbol{m}_0$, $\\boldsymbol{W}$, $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are the weights of the network. Denote the dimension of $\\boldsymbol{x}_{s,t}$ and $\\boldsymbol{y}_{s,t}$ be $X$ and $Y$ respectively. Also denote the number of neurons by $H$. Then, $\\boldsymbol{a}$ is $H \\times 1$, $\\boldsymbol{b}$ is $Y \\times 1$, $\\boldsymbol{m}_0$ is $H \\times 1$, $\\boldsymbol{W}$ is $Y \\times H$, $\\boldsymbol{U}$ is $H \\times X$, and $\\boldsymbol{V}$ is $H \\times H$. Obviously $\\boldsymbol{n}_{s,t}$ is $Y \\times 1$ and $\\boldsymbol{m}_{s,t}$ is $H \\times 1$.\n\nSuppose $\\boldsymbol{m}_{s,0}=\\boldsymbol{m}_0$, i.e. the initial state is independent of the training sequence. Here $h$ is some activation function. When the argument of $h$ is a vector, the result will be a vector of the same size, whose entries consists of the element-wise application of $h$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1\\textwidth]{rnn.png}\n\\end{center}\n\\caption{A Recurrent Neural Network. Figure is adapted with permission from \\cite{RNNHard} and slightly modified.}\n\\end{figure}\n\n\nTreating each $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ as {\\textbf{\\textit{\\color{Myblue}{independent variables}}}} and forcing their definition (equality) by some penalty function, we arrive at the following {\\textbf{\\textit{\\color{Myblue}{unconstrained}}}} problem,\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N}} \\sum_{s=1}^S \\sum_{t=1}^T & & d(h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t}) \\nonumber \\\\\n&+& \\lambda \\Big( p \\big( \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\big) \\,+\\,p \\big( \\boldsymbol{U} \\boldsymbol{x}_t + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\big) \\Big)\\,, \\nonumber\n\\end{eqnarray}\n\nwhere the notation $\\boldsymbol{N}$ and $\\boldsymbol{M}$ are matrices whose columns are comprised of $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ for all choices of $(s,t)$.\n\nLetting, $d(\\boldsymbol{e}) \\triangleq \\| \\boldsymbol{e}\\|^2$ (mean squared error) and $p(\\boldsymbol{e}) \\triangleq \\| \\boldsymbol{e}\\|^2$ (quadratic penalty), the problem can be expressed as below,\n\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N}} \\sum_{s=1}^S \\sum_{t=1}^T & & \\| h(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, h(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} h(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\|^2 \\big)\\,. \\nonumber\n\\end{eqnarray}\n\nHere $\\lambda$ determines the weight of the penalty for constraint violation.\n\n\\subsection{Diffused Cost}\n\nWhen the objective function is evolved according to the diffusion equation (\\ref{eq:diffusion}), the diffused objective has a closed form expression. Specifically, it is obtained by the convolution of the original objective with the Gaussian kernel. This can be more formally expressed as the following. Arrange all optimization variables into a long vector $\\boldsymbol{w}$, i.e. $\\boldsymbol{w} \\triangleq {\\operatorname{vec}}(\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N})$. Hence, the cost function can be denoted by $f(\\boldsymbol{w})$. The diffused cost function $g$ is obtained by:\n\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:diffused_cost}\ng(\\boldsymbol{w}; \\sigma) \\triangleq [f \\star k_\\sigma](\\boldsymbol{w}) \\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"g(\\boldsymbol{w};\\sigma)\\triangleq[f\\star k_{\\sigma}](\\boldsymbol{w})\\,.\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>;</mo><mi>\u03c3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><mi>f</mi><mo>\u22c6</mo><msub><mi>k</mi><mi>\u03c3</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nThis allows us to {\\textbf{\\textit{\\color{Myblue}{drop}}}} $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ from the optimization variables. We computing the gradient, however, derivatives involving $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ must be handled carefully to recognize the dependency stated in (\\ref{eq:m_n_def}). The simplified optimization problem is as below,\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}} & & \\nonumber \\\\ \\sum_{s=1}^S \\Bigg( \\sum_{t=1}^T & & \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 + \\| \\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{n}_{s,t}) \\|^2 - \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t} ) \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 Y \\,  \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )\\|^2 \\big) \\nonumber \\\\\n+ \\lambda \\sum_{t=0}^{T-1} & & \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 H \\, \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t})\\|^2 \\Bigg) \\nonumber \\\\\n\\mbox{s.t. }& & \\boldsymbol{n}_{s,t} \\triangleq \\boldsymbol{W} \\, \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} \\quad,\\quad \\boldsymbol{m}_{s,t} \\triangleq \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} \\,. \\nonumber\n\\end{eqnarray}\nThe gradient of this cost w.r.t. learning parameters are provided in Appendix \\ref{sec:gradient}.\n\n\n\n\n\n\\section{Properties of Diffused Cost}\n\nThe optimization problem that arises from training a deep network is often challenging. Therefore, local optimization methods (e.g., SGD) are used with a combination of some helping techniques. Although these techniques seem disconnected from each other, some of them emerge automatically from the diffused cost function. Therefore, these techniques might be {\\textbf{\\textit{\\color{Myblue}{unified}}}} under one simple theory. These methods and their connection to the diffused cost are discussed in the following.\n\n\n\\subsection{Careful Initialization}\n\nLocal optimization methods are generally sensitive to initialization when it comes to nonconvex cost functions. Deep learning is not an exception \\cite{Init13}; a recent study shows that the performance of deep networks and recurrent networks critically depends on initialization \\cite{SafranS15}. In contrast, the diffusion algorithm is deterministic and almost independent of initialization\\footnote{Path following process could be sensitive to initialization when it reaches a saddle point. Due to instability of saddle points, the direction the algorithm takes could be affected even by small perturbations. Thus, different initializations may end up reaching different solutions. However, these saddle points often occur due to the symmetry in the problem (either the original or the diffused) and the chosen direction does not affect the quality of the solution. This contrasts to gradient descent on a nonconvex objective, where depending on initialization, very solutions of different quality might be reached.} for two reasons. First, after enough smoothing the cost function becomes unimodal, and in case of convexity, will have one global minimum. In fact, the minimizer of the heavily smoothed function coincides with its center mass \\cite{mobahi2012phd}. Thus, diffusion provides an interesting deterministic initialization. Second, the update rules are completely deterministic (unless one chooses to use SGD instead of GD for local optimization in Algorithm \\ref{alg:alg_goal}) and no notion of randomness is involved in the updates. \n\n\n\\subsection{Annealed Learning Rate}\n\nEach iteration of the gradient descent essentially sees the first order Taylor expansion of the cost function $g(\\boldsymbol{x})$ at the current estimate of the solution point $\\boldsymbol{x}_0$. The linear approximation has good accuracy only within a small neighborhood of $\\boldsymbol{x}_0$, say of radius $\\rho$. Enforcing accuracy by the constraint $\\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho$, we arrive at the following problem,\n\n\n", "itemtype": "equation", "pos": 19246, "prevtext": "\n\nAfter computing this convolution, the variables in $\\boldsymbol{w}$ can be replaced by their original names according to the arrangements made in $\\boldsymbol{w} \\triangleq {\\operatorname{vec}}(\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M}, \\boldsymbol{N})$.\n\nDenote the diffused form of the activation function $h$ by $\\tilde{h}_\\sigma$, that is $\\tilde{h}_\\sigma(x) \\triangleq [h \\star k_\\sigma](x)$. Similarly, define $\\widetilde{h^2_\\sigma}(x) \\triangleq [h^2 \\star k_\\sigma](x)$. The diffused cost w.r.t. optimization variables has the following closed form (see Appendix \\ref{sec:diffused_cost}):\n\n\n\\begin{eqnarray}\n\\sum_{s=1}^S \\Bigg( \\sum_{t=1}^T & & \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 + \\| \\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{n}_{s,t}) \\|^2 - \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t} ) \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} - \\boldsymbol{n}_{s,t} \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_{s,t} \\|^2 \\nonumber \\\\\n& & \\quad + \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 Y \\,  \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )\\|^2 \\big) \\nonumber \\\\\n+ \\lambda \\sum_{t=0}^{T-1} & & \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 H \\, \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t})\\|^2 \\Bigg)\\,. \\nonumber\n\\end{eqnarray}\n\nHere $\\| \\,.\\,\\|_F$ denotes the Frobenius norm of a matrix.\n\n\\subsection{Approximate Diffused Cost}\n\nIdeal solution requires $S \\times T$ auxiliary variables for $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$. This is not practical as often $S$ is large. Thus, we resort to an {\\textbf{\\textit{\\color{Myblue}{approximate}}}} formulation here. Instead of solving for the optimal $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$, we fix them as below,\n\n\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:m_n_def}\n\\boldsymbol{n}_{s,t} \\triangleq \\boldsymbol{W} \\, \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} \\quad,\\quad \\boldsymbol{m}_{s,t} \\triangleq \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} \\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{n}_{s,t}\\triangleq\\boldsymbol{W}\\,\\widetilde{h_{\\sigma}}(%&#10;\\boldsymbol{m}_{s,t})+\\boldsymbol{b}\\quad,\\quad\\boldsymbol{m}_{s,t}\\triangleq%&#10;\\boldsymbol{U}\\boldsymbol{x}_{s,t}+\\boldsymbol{V}\\widetilde{h_{\\sigma}}(%&#10;\\boldsymbol{m}_{s,t-1})\\,+\\boldsymbol{a}\\,.\" display=\"block\"><mrow><msub><mi>\ud835\udc8f</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>\u225c</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc7e</mi></mpadded><mover accent=\"true\"><msub><mi>h</mi><mi>\u03c3</mi></msub><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\ud835\udc83</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mo rspace=\"12.5pt\">,</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>\u225c</mo><mi>\ud835\udc7c</mi><msub><mi>\ud835\udc99</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>+</mo><mi>\ud835\udc7d</mi><mover accent=\"true\"><msub><mi>h</mi><mi>\u03c3</mi></msub><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>s</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>+</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc82</mi></mpadded><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nUsing Lagrange multipliers method, the solution of this optimization turns out to be ${\\boldsymbol{x}}^* =\\boldsymbol{x}_0-\\rho \\frac{\\nabla g(\\boldsymbol{x}_0)}{ \\|\\nabla g(\\boldsymbol{x}_0) \\|} $.\n\nThe radius $\\rho$ could be chosen intelligently, e.g., by restricting the tolerated  amount of linearization error. Specifically, in order to ensure $\\forall \\boldsymbol{x} \\,;\\, \\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho  \\Rightarrow | g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) - g(\\boldsymbol{x})|\\leq \\epsilon$, we can choose $\\rho = \\sqrt{\\frac{\\epsilon}{c_f}} \\sigma $ (see Appendix \\ref{sec:linear} for proof). Here $c_f$ is some number satisfying $c_f \\geq \\frac{1}{2\\pi}\\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}}$, which obviously exists when the norm is bounded.\n\nPutting the pieces together, the solution of the linearized problem can be expressed as ${\\boldsymbol{x}}^* =\\boldsymbol{x}_0- \\eta \\, \\sigma \\, \\frac{\\nabla g(\\boldsymbol{x}_0)}{ \\|\\nabla g(\\boldsymbol{x}_0) \\|} $, where $\\eta \\triangleq \\sqrt{\\frac{\\epsilon}{c_f}}$ is a constant. This is essentially a gradient descent update with a specific choice of the step size. Since $\\sigma$ decays toward zero within the continuation loop, the step size (also called learning rate) anneals form an initially large value to eventually a small value.  \n\n\\subsection{Noise Injection}\n\\label{sec:noise_inject}\n\nInjection of random noise into the training process can lead to more stable solutions. This is often crucial in order to obtain satisfactory generalization in deep learning. The well known {\\textbf{\\textit{\\color{Myblue}{dropout}}}} is a specific way of noise injection: in each iteration, it eliminates a random subset of nodes throughout the learning \\cite{dropout}. The stochasticity in SGD is another relevant example. It is known that SGD achieves better generalization compared to a full batch gradient descent. More recently, it has been shown that adding Gaussian noise to the computed gradient can significantly improve learning for very deep networks \\cite{GradientNoise}. Although these schemes differ in details, e.g., the distribution of the noise or how it is applied to the learning process, they share the same idea of noise injection in learning.\n\nIt turns out that the diffused cost function also has this property. In order to see that, recall the definition of the diffused cost function from (\\ref{eq:diffused_cost}):\n\n\n", "itemtype": "equation", "pos": 24005, "prevtext": "\n\nThis allows us to {\\textbf{\\textit{\\color{Myblue}{drop}}}} $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ from the optimization variables. We computing the gradient, however, derivatives involving $\\boldsymbol{n}_{s,t}$ and $\\boldsymbol{m}_{s,t}$ must be handled carefully to recognize the dependency stated in (\\ref{eq:m_n_def}). The simplified optimization problem is as below,\n\n\\begin{eqnarray}\n\\min_{\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{m}_0, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}} & & \\nonumber \\\\ \\sum_{s=1}^S \\Bigg( \\sum_{t=1}^T & & \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t})-\\boldsymbol{y}_{s,t} \\|^2 + \\| \\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{n}_{s,t}) \\|^2 - \\| \\widetilde{h_\\sigma}(\\boldsymbol{n}_{s,t} ) \\|^2 \\nonumber \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 Y \\,  \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )\\|^2 \\big) \\nonumber \\\\\n+ \\lambda \\sum_{t=0}^{T-1} & & \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h_\\sigma^2}}(\\boldsymbol{m}_{s,t} )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} )) \\|_F^2 + \\sigma^2 H \\, \\| \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t})\\|^2 \\Bigg) \\nonumber \\\\\n\\mbox{s.t. }& & \\boldsymbol{n}_{s,t} \\triangleq \\boldsymbol{W} \\, \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t} ) + \\boldsymbol{b} \\quad,\\quad \\boldsymbol{m}_{s,t} \\triangleq \\boldsymbol{U} \\boldsymbol{x}_{s,t} + \\boldsymbol{V} \\widetilde{h_\\sigma}(\\boldsymbol{m}_{s,t-1}) \\, + \\boldsymbol{a} \\,. \\nonumber\n\\end{eqnarray}\nThe gradient of this cost w.r.t. learning parameters are provided in Appendix \\ref{sec:gradient}.\n\n\n\n\n\n\\section{Properties of Diffused Cost}\n\nThe optimization problem that arises from training a deep network is often challenging. Therefore, local optimization methods (e.g., SGD) are used with a combination of some helping techniques. Although these techniques seem disconnected from each other, some of them emerge automatically from the diffused cost function. Therefore, these techniques might be {\\textbf{\\textit{\\color{Myblue}{unified}}}} under one simple theory. These methods and their connection to the diffused cost are discussed in the following.\n\n\n\\subsection{Careful Initialization}\n\nLocal optimization methods are generally sensitive to initialization when it comes to nonconvex cost functions. Deep learning is not an exception \\cite{Init13}; a recent study shows that the performance of deep networks and recurrent networks critically depends on initialization \\cite{SafranS15}. In contrast, the diffusion algorithm is deterministic and almost independent of initialization\\footnote{Path following process could be sensitive to initialization when it reaches a saddle point. Due to instability of saddle points, the direction the algorithm takes could be affected even by small perturbations. Thus, different initializations may end up reaching different solutions. However, these saddle points often occur due to the symmetry in the problem (either the original or the diffused) and the chosen direction does not affect the quality of the solution. This contrasts to gradient descent on a nonconvex objective, where depending on initialization, very solutions of different quality might be reached.} for two reasons. First, after enough smoothing the cost function becomes unimodal, and in case of convexity, will have one global minimum. In fact, the minimizer of the heavily smoothed function coincides with its center mass \\cite{mobahi2012phd}. Thus, diffusion provides an interesting deterministic initialization. Second, the update rules are completely deterministic (unless one chooses to use SGD instead of GD for local optimization in Algorithm \\ref{alg:alg_goal}) and no notion of randomness is involved in the updates. \n\n\n\\subsection{Annealed Learning Rate}\n\nEach iteration of the gradient descent essentially sees the first order Taylor expansion of the cost function $g(\\boldsymbol{x})$ at the current estimate of the solution point $\\boldsymbol{x}_0$. The linear approximation has good accuracy only within a small neighborhood of $\\boldsymbol{x}_0$, say of radius $\\rho$. Enforcing accuracy by the constraint $\\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho$, we arrive at the following problem,\n\n\n", "index": 7, "text": "\\begin{equation}\n\\min_{\\boldsymbol{x}} g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) \\quad \\quad {s.t.} \\quad \\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho \\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\boldsymbol{x}}g(\\boldsymbol{x}_{0})+(\\boldsymbol{x}-\\boldsymbol{x}_{0})%&#10;^{T}\\nabla g(\\boldsymbol{x}_{0})\\quad\\quad{s.t.}\\quad\\|\\boldsymbol{x}-%&#10;\\boldsymbol{x}_{0}\\|\\leq\\rho\\,.\" display=\"block\"><mrow><munder><mi>min</mi><mi>\ud835\udc99</mi></munder><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>-</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2207</mo><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mi>s</mi><mo>.</mo><mi>t</mi><mo>.</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mo>\u2225</mo><mi>\ud835\udc99</mi><mo>-</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo>\u2225</mo><mo>\u2264</mo><mpadded width=\"+1.7pt\"><mi>\u03c1</mi></mpadded><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nThus, the gradient at a point $\\boldsymbol{w}_0$ has the following form.\n\\begin{eqnarray}\n\\label{eq:diffused_gradient}\n\\nabla g(\\boldsymbol{w}_0;\\sigma) &=& \\int_{\\mathcal{W}} \\nabla f(\\boldsymbol{w}_0-\\boldsymbol{t}) k_\\sigma(\\boldsymbol{t}) \\, d \\boldsymbol{t} \\\\\n\\label{eq:diffused_gradient_sample}\n&\\approx& \\frac{1}{J} \\sum_{j=1}^J \\nabla f(\\boldsymbol{w}_0-\\boldsymbol{t}_j) \\quad,\\quad \\boldsymbol{t}_j \\sim \\mathcal{N}(\\boldsymbol{0},\\sigma^2 \\boldsymbol{I}) \\,.\n\\end{eqnarray}\n\nThis means if we were to approximate the gradient of the diffused cost by MCMC method, it would average over a number of {\\textbf{\\textit{\\color{Myblue}{noisified}}}} gradients. Specifically, the noise would be {\\textbf{\\textit{\\color{Myblue}{additive}}}} w.r.t. the weights of the network and it would have a {\\textbf{\\textit{\\color{Myblue}{normal distribution}}}} with zero mean and variance of $\\sigma^2$. The noise injection of (\\ref{eq:diffused_gradient_sample}) has also been used by \\cite{fuzz} via numerical sampling exactly as in (\\ref{eq:diffused_gradient_sample}). From a higher level perspective, this noise injection has some similarity to SGD; the latter also averages (over multiple epochs) the effect of noisified gradients.\n\nA key advantage of using the diffusion framework for noise injection, however, is that the expected noisified gradient (the integral in (\\ref{eq:diffused_gradient})) has a {\\textbf{\\textit{\\color{Myblue}{closed form}}}} expression, while the other schemes are mainly {\\textbf{\\textit{\\color{Myblue}{sampling}}}} based. This leads to a huge computational gain for the diffusion method: while other methods would need a lot of sampling iterations in order to reach a reasonable approximation to the expected noisified gradient (and the number of these samples could grow exponentially in the number of weights), the diffusion method achieves this with almost no computational effort and without any sampling.\n\n\n\n\\subsection{Layerwise Pretraining}\n\nWe argue that when $\\sigma$ is large, the network only focuses on short range dependencies, and as $\\sigma$ shrinks toward zero, longer range dependencies are gradually learned. In order to see why this happens, let's for example inspect the partial gradient $\\nabla_{\\boldsymbol{a}} \\, {g}$, which has the form $\\sum_{t=1}^T \\boldsymbol{r}_t \\, \\boldsymbol{M}_t$ (see Appendix \\ref{sec:gradient} for derivations and the definition of $\\boldsymbol{r}_t$), where $\\boldsymbol{M}_t \\triangleq  \\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}^\\prime (\\boldsymbol{m}_{t-1}) \\big) \\boldsymbol{M}_{t-1}$ and $\\boldsymbol{M}_1 \\triangleq \\boldsymbol{I}$. Resolving the recursion in $\\boldsymbol{M}_t$ leads to,\n\n\n", "itemtype": "equation", "pos": 26708, "prevtext": "\n\nUsing Lagrange multipliers method, the solution of this optimization turns out to be ${\\boldsymbol{x}}^* =\\boldsymbol{x}_0-\\rho \\frac{\\nabla g(\\boldsymbol{x}_0)}{ \\|\\nabla g(\\boldsymbol{x}_0) \\|} $.\n\nThe radius $\\rho$ could be chosen intelligently, e.g., by restricting the tolerated  amount of linearization error. Specifically, in order to ensure $\\forall \\boldsymbol{x} \\,;\\, \\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho  \\Rightarrow | g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) - g(\\boldsymbol{x})|\\leq \\epsilon$, we can choose $\\rho = \\sqrt{\\frac{\\epsilon}{c_f}} \\sigma $ (see Appendix \\ref{sec:linear} for proof). Here $c_f$ is some number satisfying $c_f \\geq \\frac{1}{2\\pi}\\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}}$, which obviously exists when the norm is bounded.\n\nPutting the pieces together, the solution of the linearized problem can be expressed as ${\\boldsymbol{x}}^* =\\boldsymbol{x}_0- \\eta \\, \\sigma \\, \\frac{\\nabla g(\\boldsymbol{x}_0)}{ \\|\\nabla g(\\boldsymbol{x}_0) \\|} $, where $\\eta \\triangleq \\sqrt{\\frac{\\epsilon}{c_f}}$ is a constant. This is essentially a gradient descent update with a specific choice of the step size. Since $\\sigma$ decays toward zero within the continuation loop, the step size (also called learning rate) anneals form an initially large value to eventually a small value.  \n\n\\subsection{Noise Injection}\n\\label{sec:noise_inject}\n\nInjection of random noise into the training process can lead to more stable solutions. This is often crucial in order to obtain satisfactory generalization in deep learning. The well known {\\textbf{\\textit{\\color{Myblue}{dropout}}}} is a specific way of noise injection: in each iteration, it eliminates a random subset of nodes throughout the learning \\cite{dropout}. The stochasticity in SGD is another relevant example. It is known that SGD achieves better generalization compared to a full batch gradient descent. More recently, it has been shown that adding Gaussian noise to the computed gradient can significantly improve learning for very deep networks \\cite{GradientNoise}. Although these schemes differ in details, e.g., the distribution of the noise or how it is applied to the learning process, they share the same idea of noise injection in learning.\n\nIt turns out that the diffused cost function also has this property. In order to see that, recall the definition of the diffused cost function from (\\ref{eq:diffused_cost}):\n\n\n", "index": 9, "text": "\\begin{equation}\ng(\\boldsymbol{w};\\sigma) \\triangleq [f \\star k_\\sigma](\\boldsymbol{w}) =\\int_{\\mathcal{W}} f(\\boldsymbol{w}-\\boldsymbol{t}) k_\\sigma(\\boldsymbol{t}) \\, d \\boldsymbol{t}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"g(\\boldsymbol{w};\\sigma)\\triangleq[f\\star k_{\\sigma}](\\boldsymbol{w})=\\int_{%&#10;\\mathcal{W}}f(\\boldsymbol{w}-\\boldsymbol{t})k_{\\sigma}(\\boldsymbol{t})\\,d%&#10;\\boldsymbol{t}\" display=\"block\"><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>;</mo><mi>\u03c3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><mi>f</mi><mo>\u22c6</mo><msub><mi>k</mi><mi>\u03c3</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></msub><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc98</mi><mo>-</mo><mi>\ud835\udc95</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>k</mi><mi>\u03c3</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc95</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc95</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\n\nWhen $\\sigma \\rightarrow \\infty$, all the sigmoid-like activation functions listed in (\\ref{tab:Table}) become flat and their gradient vanishes ${\\widetilde{h}}_\\sigma^\\prime \\rightarrow 0$. This implies that by choosing $\\sigma$ large enough, one can find a small enough $\\epsilon$ that satisfies $\\|{\\operatorname{diag}}({\\widetilde{h}}_\\sigma^\\prime)\\| \\leq \\epsilon$. Since the contribution of each term in the above sum will be at most equal to its matrix norm, we can derive,\n\n\n", "itemtype": "equation", "pos": 29618, "prevtext": "\n\nThus, the gradient at a point $\\boldsymbol{w}_0$ has the following form.\n\\begin{eqnarray}\n\\label{eq:diffused_gradient}\n\\nabla g(\\boldsymbol{w}_0;\\sigma) &=& \\int_{\\mathcal{W}} \\nabla f(\\boldsymbol{w}_0-\\boldsymbol{t}) k_\\sigma(\\boldsymbol{t}) \\, d \\boldsymbol{t} \\\\\n\\label{eq:diffused_gradient_sample}\n&\\approx& \\frac{1}{J} \\sum_{j=1}^J \\nabla f(\\boldsymbol{w}_0-\\boldsymbol{t}_j) \\quad,\\quad \\boldsymbol{t}_j \\sim \\mathcal{N}(\\boldsymbol{0},\\sigma^2 \\boldsymbol{I}) \\,.\n\\end{eqnarray}\n\nThis means if we were to approximate the gradient of the diffused cost by MCMC method, it would average over a number of {\\textbf{\\textit{\\color{Myblue}{noisified}}}} gradients. Specifically, the noise would be {\\textbf{\\textit{\\color{Myblue}{additive}}}} w.r.t. the weights of the network and it would have a {\\textbf{\\textit{\\color{Myblue}{normal distribution}}}} with zero mean and variance of $\\sigma^2$. The noise injection of (\\ref{eq:diffused_gradient_sample}) has also been used by \\cite{fuzz} via numerical sampling exactly as in (\\ref{eq:diffused_gradient_sample}). From a higher level perspective, this noise injection has some similarity to SGD; the latter also averages (over multiple epochs) the effect of noisified gradients.\n\nA key advantage of using the diffusion framework for noise injection, however, is that the expected noisified gradient (the integral in (\\ref{eq:diffused_gradient})) has a {\\textbf{\\textit{\\color{Myblue}{closed form}}}} expression, while the other schemes are mainly {\\textbf{\\textit{\\color{Myblue}{sampling}}}} based. This leads to a huge computational gain for the diffusion method: while other methods would need a lot of sampling iterations in order to reach a reasonable approximation to the expected noisified gradient (and the number of these samples could grow exponentially in the number of weights), the diffusion method achieves this with almost no computational effort and without any sampling.\n\n\n\n\\subsection{Layerwise Pretraining}\n\nWe argue that when $\\sigma$ is large, the network only focuses on short range dependencies, and as $\\sigma$ shrinks toward zero, longer range dependencies are gradually learned. In order to see why this happens, let's for example inspect the partial gradient $\\nabla_{\\boldsymbol{a}} \\, {g}$, which has the form $\\sum_{t=1}^T \\boldsymbol{r}_t \\, \\boldsymbol{M}_t$ (see Appendix \\ref{sec:gradient} for derivations and the definition of $\\boldsymbol{r}_t$), where $\\boldsymbol{M}_t \\triangleq  \\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}^\\prime (\\boldsymbol{m}_{t-1}) \\big) \\boldsymbol{M}_{t-1}$ and $\\boldsymbol{M}_1 \\triangleq \\boldsymbol{I}$. Resolving the recursion in $\\boldsymbol{M}_t$ leads to,\n\n\n", "index": 11, "text": "\\begin{equation}\n\\boldsymbol{M}_t = \\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\big) + \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\, \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-2})+ \\dots \\,. \\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{M}_{t}=\\boldsymbol{I}+\\boldsymbol{V}{\\operatorname{diag}}\\big{(}{%&#10;\\widetilde{h}}_{\\sigma}^{\\prime}(\\boldsymbol{m}_{t-1})\\big{)}+\\boldsymbol{V}{%&#10;\\widetilde{h}}_{\\sigma}^{\\prime}(\\boldsymbol{m}_{t-1})\\,\\boldsymbol{V}{%&#10;\\widetilde{h}}_{\\sigma}^{\\prime}(\\boldsymbol{m}_{t-2})+\\dots\\,.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc74</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>\ud835\udc70</mi><mo>+</mo><mrow><mi>\ud835\udc7d</mi><mo>\u2062</mo><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>h</mi><mo>~</mo></mover><mi>\u03c3</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udc7d</mi><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>~</mo></mover><mi>\u03c3</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc7d</mi><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>~</mo></mover><mi>\u03c3</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc8e</mi><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">\u2026</mi></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nwhen $\\sigma$ is very large, and thus $\\epsilon$ is very small, we can ignore all the terms involving $\\epsilon$, which leaves us with $\\boldsymbol{M}_t\\approx \\boldsymbol{I}$. As we gradually reduce $\\sigma$, and thus increase $\\epsilon$, we can reconsider terms involving smaller exponents, while the higher order terms still remain negligible. By gradually decreasing $\\sigma$, $\\boldsymbol{M}_t$ can be approximated by $\\boldsymbol{I}$, then, $\\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\big) $, then $\\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\big) + \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\, \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-2})$ and so on. \n\nThis is conceptually very similar to layerwise pretraining \\cite{Bengio06}, as the learning in each layer starts from considering only its immediate previous layer and then gradually switches to the full consideration by considering larger and larger number of previous layers. For example, $M_t$ at the first layer (i.e., when $t=1$) first considers contribution from itself only, then gradually introduces contribution from the second layer, and then from the third layer and so on.\n\n\n\\section{Choice of the Activation Function}\n\nIn order to implement the method, we need to obtain the explicit expressions of $\\widetilde{h}_\\sigma$ and $\\widetilde{h^2}_\\sigma$ for a given activation function $h$. For example, suppose we set $h(x)={\\operatorname{erf}}(a x)$, where $a$ is a parameter that determines the sharpness of the activation function. Note that $\\lim_{a \\rightarrow \\infty} {\\operatorname{erf}}(ax) = {\\operatorname{sign}}(x)$ and ${\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} x) \\approx \\tanh(x)$. The form of $\\widetilde{h}_\\sigma$ can be already looked up from Table \\ref{tab:Table}, which is repeated below,\n\n\n", "itemtype": "equation", "pos": 30435, "prevtext": "\n\n\nWhen $\\sigma \\rightarrow \\infty$, all the sigmoid-like activation functions listed in (\\ref{tab:Table}) become flat and their gradient vanishes ${\\widetilde{h}}_\\sigma^\\prime \\rightarrow 0$. This implies that by choosing $\\sigma$ large enough, one can find a small enough $\\epsilon$ that satisfies $\\|{\\operatorname{diag}}({\\widetilde{h}}_\\sigma^\\prime)\\| \\leq \\epsilon$. Since the contribution of each term in the above sum will be at most equal to its matrix norm, we can derive,\n\n\n", "index": 13, "text": "\\begin{equation}\n\\|\\boldsymbol{M}_t\\| \\leq \\|\\boldsymbol{I}\\| + \\epsilon \\|\\boldsymbol{V}\\| + (\\epsilon \\|\\boldsymbol{V}\\|)^2 + (\\epsilon \\|\\boldsymbol{V}\\|)^3 +  \\dots \\,. \\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\|\\boldsymbol{M}_{t}\\|\\leq\\|\\boldsymbol{I}\\|+\\epsilon\\|\\boldsymbol{V}\\|+(%&#10;\\epsilon\\|\\boldsymbol{V}\\|)^{2}+(\\epsilon\\|\\boldsymbol{V}\\|)^{3}+\\dots\\,.\" display=\"block\"><mrow><mrow><mrow><mo>\u2225</mo><msub><mi>\ud835\udc74</mi><mi>t</mi></msub><mo>\u2225</mo></mrow><mo>\u2264</mo><mrow><mrow><mo>\u2225</mo><mi>\ud835\udc70</mi><mo>\u2225</mo></mrow><mo>+</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>\ud835\udc7d</mi><mo>\u2225</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>\ud835\udc7d</mi><mo>\u2225</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>\ud835\udc7d</mi><mo>\u2225</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>3</mn></msup><mo>+</mo><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">\u2026</mi></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04114.tex", "nexttext": "\n\nIn the following, we only focus on $\\widetilde{h^2}_\\sigma$. Unfortunately, $\\widetilde{h^2}_\\sigma(x)$ lacks a closed form expression. However, observe that ${\\operatorname{erf}}^2(x) \\approx 1-e^{-\\frac{4}{\\pi} x^2}$. This approximation has a reasonably good accuracy as shown in Figure \\ref{fig:erf2}. Using this approximation, it follows that $[{\\operatorname{erf}}^2(a \\,\\Box\\, ) \\star k_\\sigma] (x) \\approx [1-e^{-\\frac{4}{\\pi} (a \\,\\Box\\, )^2}  k_\\sigma] (x)$. \n\n\\begin{eqnarray}\n\\widetilde{h^2}(x) &\\triangleq& [{\\operatorname{erf}}^2(a \\,\\Box\\, ) \\star k_\\sigma] (x) \\nonumber \\\\\n&\\approx& [1-e^{-\\frac{4}{\\pi} (a \\,\\Box\\, )^2}  k_\\sigma] (x) \\nonumber \\\\\n&=& 1- \\frac{\\sqrt{\\pi} \\,\\, e^{-\\frac{4 a^2 x^2}{\\pi + 8 a^2 \\sigma^2}}}{\\sqrt{\\pi + 8 a^2 \\sigma^2}} \\nonumber \\,.\n\\end{eqnarray}\n\n\\begin{figure}\n\\centering \\includegraphics[width=.4\\textwidth,height=.3\\textwidth]{erf2.png}\n\\caption{Blue and brown curves respectively plot ${\\operatorname{erf}}^2(x)$ and $1-e^{-\\frac{4}{\\pi} x^2}$. Due to the strong overlap, the blue curve is barely visible.}\n\\label{fig:erf2}\n\\end{figure}\n\n\\section{Preliminary Results}\n\n\nHere we present a comparison between SGD and the proposed diffusion framework. The hyperparameters in both methods are carefully searched to ensure a fair comparison. We use ${\\operatorname{erf}}$ as the activation function. The task is to learn adding two numbers, and is adapted from \\cite{RNNHard}. The network consists of has 10 hidden units, and it has two inputs and one output. One of the input units reads a sequence of 10 real numbers, and the other a sequence of 10 binary numbers. The binary numbers are zero everywhere except two random locations. The task is to add the values from the first sequence, at the two locations marked by the second sequence.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{add.png}\n\\caption{Learning to add by RNNs. Figure adapted with permission from \\cite{RNNHard}.}\n\\end{center}\n\\end{figure}\n\nWe trained the network by 1000 sequences, and generalization is computed from a test set of 100 sequences. The result is shown in the plots. The horizontal axis shows the {\\textbf{\\textit{\\color{Myblue}{generalization error}}}}, and the vertical axis shows how many {\\textbf{\\textit{\\color{Myblue}{epochs}}}} it takes to reach that generalization error. For example, with 50 batches of size 50 samples, in order to reach around error of 0.02, SGD (blue) needs about 90 epochs, while diffusion methods (red) needs about 20 epochs. \n\n\\begin{figure}\n\\noindent\\begin{center}\n\\includegraphics[width=.45\\textwidth]{mini10.png}\n\\includegraphics[width=.45\\textwidth]{mini50.png}\n\\end{center}\n\\caption{Experiments with mini batches of size 10 (left) and 50 (right).}\n\\end{figure}\n\n\n\\section{Related Works \\& Future Directions}\n\nThis work specifically studies the use of the diffusion equation for optimizing the objective function in deep learning. However, there is a growing number of techniques by others that propose new algorithms for deep learning. Using tensor decomposition techniques, \\cite{anima} offers new algorithms for deep learning with performance guarantee. A conceptually similar algorithm to ours is provided in \\cite{hazan}. However, instead of computing the convolution analytically, the latter work relies on numerical sampling. It guarantees reaching the global minimum at a proved rate for certain objective functions.\n\nThis work relies on smoothing the objective function by convolving it with the Gaussian kernel. We have previously shown that this particular form of smoothing is optimal in a certain sense, by relating Gaussian convolution to a relaxation of the convex envelope. Although connection to the convex envelope is meaningful in the context of {\\textbf{\\textit{\\color{Myblue}{nonconvex}}}} objective functions, there are side benefits in smoothing even when the objective function is convex. For example, smoothing a nonsmooth {\\textbf{\\textit{\\color{Myblue}{convex}}}} objective function by convolution can improve the convergence rate of stochastic optimization algorithms \\cite{Duchi}.\n\nAs discussed in Section \\ref{sec:noise_inject}, smoothing can be considered as means to inject noise into the training process. The idea of noise injection is already used in methods such as SGD or dropout \\cite{dropout} in order to improve learning. The key advantage of our framework for noise injection, however, is that the noise injection can be achieved in closed form and without need of sampling. In order words, we can compute the effect of infinitely many noisified objective functions in closed form. This is similar to the idea of Marginalized Denoising Autoencoders (mDA) \\cite{feisha}, where the effect of infinitely many nosified inputs is marginalized to obtain a closed form expression. However, mDA limits the form of the injected noise. Specifically, the marginalized effect is only computable in a {\\textbf{\\textit{\\color{Myblue}{linear}}}} reconstruction setup (nonlinearity is applied only after computation of the marginalized reconstruction). In addition, mDA performs noise injection layer by layer in a greedy fashion. In contrast, our framework is able to compute closed form expression for the entire deep network and allowing full nonconvexity of the associated optimization, up to reasonable approximation.\n\nDiffusion equation provides an approximate evolution toward the convex envelope. Consequently, it is not perfect: if global minimum is very narrow, diffusion can miss that minima in favor of a wider minimum whose value is slightly larger than the narrow global minimum (see Figure \\ref{fig:unstable}). This may seem a disadvantage at the first glance. However, the wider minima are in fact more stable\\footnote{By a stable minimum we mean that a small perturbation of the equilibrium resides in the basin of attraction of the same equilibrium. This is not true if the minimum is too narrow; slight perturbation may put the gradient decent into a different basin of attraction.}, which could be more desired in practice, e.g. generalizing better. In fact, a recent analysis has shown that SGD attains better generalization when the objective function is {\\textbf{\\textit{\\color{Myblue}{smoother}}}} \\cite{recht}. Note that in our framework, initializing the algorithm with larger $\\sigma$ automatically provides a smoother surrogate cost function where unstable minima disappear. Thus, it is more likely to remain in the basin of attraction of the stable minima. A thorough investigation of how smoothing the cost function in the diffusion setting may improve the generalization performance is a direction for future research.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.6\\textwidth]{unstable.png}\n\\caption{Starting from the original function at the bottom, moving upward the plots correspond to more aggressive smoothing (i.e. larger $\\sigma)$. The original function has three wide minima, and a narrow global minimum. Following the path of the minimizer from top to the bottom, it is obvious that the process misses the narrow global minimum and reaches one of the wider minima. However, among the three wide minima, it finds the lowest one.}\n\\label{fig:unstable}\n\\end{figure}\n\nA closely related work to ours is Annealed Gradient Descent \\cite{annealedPan}, where the objective landscape is also initially approximated by a smoother function and is gradually transformed to the original one. However, the unlike this work where Gaussian smoothing is theoretically motivated for nonconvex optimization  \\cite{MobahiEMMCVPR}, in \\cite{annealedPan} coarse-to-fine approximation of the objective function is based on heuristically motivated procedure. More precisely, the latter uses vector quantization methods in order to generate a code book by which the coarse cost function is approximated. Another difference between these two works is that the representation of the smoothed function in our framework is simpler, as we directly obtain a closed form expression of the objective function. that is a simpler setup than approximation by codebook generation. \n\nVery recently, the diffusion process has been proposed for learning difficult probability distributions \\cite{ganguli}. In forward time diffusion, the method converts any complex distribution into a simple distribution, e.g., Gaussian. It then learns the reverse-time of this diffusion process to define a generative\nmodel distribution. By sampling from such trained model, the authors have achieved inpainting of missing regions in natural images.\n\n\n\\section{Acknowledgment}\n\nThis research is partially funded by Shell Research. Hossein Mobahi is thankful to John W. Fisher, William T. Freeman, Yann LeCun, and Yoshua Bengio for comments and discussions and to Peter Bartlett and Fei Sha for suggesting connections to \\cite{Duchi,feisha}. Hossein Mobahi is grateful to Geoffrey Hinton, Marc'Aurelio Ranzato, and Philip Bachman for comments and Kate Saenko for discussions in earlier phase of this work.\n\n\n\n\n\n\n\n\n\\bibliography{fast_rnn_training}\n\\bibliographystyle{apalike}\n\n\\newpage~\\newpage~\n\n\\noindent {\\bf \\LARGE Appendices}\n\\bigskip\n\n\\appendix\n\n\n\\section{Diffused RNN Training Cost}\n\\label{sec:diffused_cost}\n\nDiffusing the cost function w.r.t. $\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}$ yields\\footnote{We use the fact that convolution of $(\\boldsymbol{x}^T \\boldsymbol{y})^2$ with $k_\\sigma(\\boldsymbol{x})$ is $(\\boldsymbol{x}^T \\boldsymbol{y})^2 + \\sigma^2 \\|\\boldsymbol{y}\\|^2$.},\n\n\n\\begin{eqnarray}\n\\sum_{t=1}^T & & \\| h(\\boldsymbol{n}_t)-\\boldsymbol{y}_t \\|^2 \\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, h(\\boldsymbol{m}_t ) + \\boldsymbol{b} - \\boldsymbol{n}_t \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_t + \\boldsymbol{V} h(\\boldsymbol{m}_{t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_t \\|^2\\\\\n& & \\quad + \\sigma^2 Y (1+ \\| h(\\boldsymbol{m}_t )\\|^2)  \\,+\\, \\sigma^2 H (1+ \\| \\boldsymbol{x}_t\\|^2 + \\| h(\\boldsymbol{m}_{t-1})\\|^2) \\big)\\,.\n\\end{eqnarray}\n\nSmoothing w.r.t. $\\boldsymbol{m}_t$ and $\\boldsymbol{n}_t$ leads\\footnote{We use the identity that convolution of $\\| \\boldsymbol{A} h (\\boldsymbol{x}) + \\boldsymbol{b}\\|^2$ with $k_\\sigma(\\boldsymbol{x})$ is equal to $\\| \\boldsymbol{A} \\tilde{h} (\\boldsymbol{x}) + \\boldsymbol{b}\\|^2 +  \\| \\boldsymbol{A} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h^2}}(\\boldsymbol{x} )) \\|_F^2 - \\| \\boldsymbol{A} \\, {\\operatorname{diag}}(\\widetilde{h}(\\boldsymbol{x} )) \\|_F^2$.} to,\n\n\\begin{eqnarray}\n\\sum_{t=1}^T & & \\| \\widetilde{h}(\\boldsymbol{n}_t)-\\boldsymbol{y}_t \\|^2 + \\| \\sqrt{\\widetilde{h^2}}(\\boldsymbol{n}_t) \\|^2 - \\| \\widetilde{h}(\\boldsymbol{n}_t ) \\|^2\\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, \\widetilde{h}(\\boldsymbol{m}_t ) + \\boldsymbol{b} - \\boldsymbol{n}_t \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_t + \\boldsymbol{V} \\widetilde{h}(\\boldsymbol{m}_{t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_t \\|^2\\\\\n& & \\quad + \\sigma^2 Y ( 2 + \\| \\widetilde{h}(\\boldsymbol{m}_t )\\|^2)  \\,+\\, \\sigma^2 H (2 + \\| \\boldsymbol{x}_t\\|^2 + \\| \\widetilde{h}(\\boldsymbol{m}_{t-1})\\|^2) \\\\\n& & \\quad + \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h^2}}(\\boldsymbol{m}_t )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h}(\\boldsymbol{m}_t )) \\|_F^2  \\\\\n& & \\quad + \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h^2}}(\\boldsymbol{m}_{t-1} )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h}(\\boldsymbol{m}_{t-1} )) \\|_F^2 \\big) \\,.\n\\end{eqnarray}\n\n\n\nDiscarding constants terms, i.e. those that do not depend on neither of optimization variables $\\boldsymbol{a},\\boldsymbol{b}, \\boldsymbol{U},\\boldsymbol{V}, \\boldsymbol{W}, \\boldsymbol{M} , \\boldsymbol{N}$, simplifies the diffused cost to the following,\n\n\\begin{eqnarray}\n\\sum_{t=1}^T & & \\| \\widetilde{h}(\\boldsymbol{n}_t)-\\boldsymbol{y}_t \\|^2 + \\| \\sqrt{\\widetilde{h^2}}(\\boldsymbol{n}_t) \\|^2 - \\| \\widetilde{h}(\\boldsymbol{n}_t ) \\|^2\\\\\n&+& \\lambda \\big( \\| \\boldsymbol{W} \\, \\widetilde{h}(\\boldsymbol{m}_t ) + \\boldsymbol{b} - \\boldsymbol{n}_t \\|^2 \\,+\\, \\| \\boldsymbol{U} \\boldsymbol{x}_t + \\boldsymbol{V} \\widetilde{h}(\\boldsymbol{m}_{t-1}) \\, + \\boldsymbol{a} - \\boldsymbol{m}_t \\|^2\\\\\n& & \\quad + \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h^2}}(\\boldsymbol{m}_t )) \\|_F^2 - \\| \\boldsymbol{W} \\, {\\operatorname{diag}}(\\widetilde{h}(\\boldsymbol{m}_t )) \\|_F^2 + \\sigma^2 Y \\,  \\| \\widetilde{h}(\\boldsymbol{m}_t )\\|^2 \\big) \\\\\n+ \\lambda \\sum_{t=0}^{T-1} & & \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\sqrt{\\widetilde{h^2}}(\\boldsymbol{m}_t )) \\|_F^2 - \\| \\boldsymbol{V} \\, {\\operatorname{diag}}(\\widetilde{h}(\\boldsymbol{m}_t )) \\|_F^2 + \\sigma^2 H \\, \\| \\widetilde{h}(\\boldsymbol{m}_t)\\|^2 \\,.\n\\end{eqnarray}\n\n\n\\section{Gradient of Diffused Cost}\n\\label{sec:gradient}\n\nBelow $\\odot$ denotes the element-wise product of two matrices.\n\n\\begin{eqnarray}\n\\frac{d g}{d \\boldsymbol{b}} &=& \\sum_t \\frac{\\partial \\boldsymbol{n}_t}{\\partial \\boldsymbol{b}} \\, \\frac{\\partial g}{\\partial \\boldsymbol{n}_t} \\\\\n&=& \\sum_t \\boldsymbol{I} \\Big (2 {\\widetilde{h}}^\\prime (\\boldsymbol{n}_t) \\odot (\\widetilde{h}(\\boldsymbol{n}_t)-\\boldsymbol{y}_t) + {\\widetilde{h^2}}^\\prime(\\boldsymbol{n}_t) - 2 {\\widetilde{h}}^\\prime(\\boldsymbol{n}_t ) \\odot \\widetilde{h}(\\boldsymbol{n}_t )  \\Big) \\\\\n&=& \\sum_t \\Big ({\\widetilde{h^2}}^\\prime(\\boldsymbol{n}_t) -2 {\\widetilde{h}}^\\prime (\\boldsymbol{n}_t) \\odot \\boldsymbol{y}_t \\Big) \\,.\n\\end{eqnarray}\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n\\frac{d g}{d \\boldsymbol{W}} &=& \\sum_t \\frac{\\partial g}{\\partial \\boldsymbol{W}} + \\sum_d \\frac{\\partial g}{\\partial n_t^{(d)}} \\frac{\\partial n_t^{(d)}}{\\partial \\boldsymbol{W}} \\\\\n&=& 2 \\lambda \\boldsymbol{W} {\\operatorname{diag}} \\Big( \\sum_{t=1}^T \\big(\\widetilde{h^2}(\\boldsymbol{m}_t ) \\,-\\, {\\widetilde{h}}^2(\\boldsymbol{m}_t ) \\big) \\Big)\\\\\n& & + \\sum_{t=1}^T \\Big ({\\widetilde{h^2}}^\\prime(\\boldsymbol{n}_t) -2 {\\widetilde{h}}^\\prime (\\boldsymbol{n}_t) \\odot \\boldsymbol{y}_t \\Big)\\, \\widetilde{h}(\\boldsymbol{m}_t )^T \n\\end{eqnarray}\n\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n\\boldsymbol{r}_t &\\triangleq& \\Big ({\\widetilde{h^2}}^\\prime(\\boldsymbol{n}_t) -2 {\\widetilde{h}}^\\prime (\\boldsymbol{n}_t) \\odot \\boldsymbol{y}_t \\Big)^T \\, \\Big( \\boldsymbol{W} {\\operatorname{diag}}({\\widetilde{h}}^\\prime (\\boldsymbol{m}_t))  \\Big) \\nonumber \\\\\n& & \\quad + \\lambda \\Big( \\big( {\\widetilde{h^2}}^\\prime(\\boldsymbol{m}_t) -2 {\\widetilde{h}}^\\prime(\\boldsymbol{m}_t) \\odot {\\widetilde{h}}(\\boldsymbol{m}_t) \\big)^T \\odot \\big(\\boldsymbol{1}^T (\\boldsymbol{W} \\odot \\boldsymbol{W}) + \\mathbb{I}_{t \\neq T}\\boldsymbol{1}^T (\\boldsymbol{V} \\odot \\boldsymbol{V}) \\big) \\nonumber \\\\\n& & \\quad\\quad\\quad  + 2 \\sigma^2 (\\mathbb{I}_{t \\neq T} H + Y) ({\\widetilde{h}}^\\prime(\\boldsymbol{m}_t) \\odot {\\widetilde{h}}(\\boldsymbol{m}_t)  )^T \\Big) \\,.\n\\end{eqnarray}\n\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n(\\frac{d g}{d \\boldsymbol{a}})^T &=& \\sum_{t=1}^T (\\frac{d g}{d \\boldsymbol{m}_t})^T \\, \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{a}} \\, \\\\\n&=& \\sum_{t=1}^T ((\\frac{\\partial g}{\\partial \\boldsymbol{n}_t})^T \\, \\frac{\\partial \\boldsymbol{n}_t}{\\partial \\boldsymbol{m}_t} + (\\frac{\\partial g}{\\partial \\boldsymbol{m}_t})^T) \\, \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{a}} \\, \\\\\n&=& \\sum_{t=1}^T \\boldsymbol{r}_t \\, \\boldsymbol{M}_t \\\\\n\\boldsymbol{M}_t &\\triangleq& \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{a}} = \\frac{\\partial \\boldsymbol{m}_t}{\\partial \\boldsymbol{a}} + \\frac{\\partial \\boldsymbol{m}_t}{\\partial \\boldsymbol{m}_{t-1}} \\boldsymbol{M}_{t-1} = \\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}^\\prime (\\boldsymbol{m}_{t-1}) \\big) \\boldsymbol{M}_{t-1}\\\\\n\\boldsymbol{M}_1 &\\triangleq& \\boldsymbol{I} \\,.\n\\end{eqnarray}\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n\\frac{d g}{d \\boldsymbol{V}} &=& \\frac{\\partial g}{\\partial \\boldsymbol{V}} + \\sum_{t=1}^T \\sum_d \\frac{d g}{d m_t^{(d)}} \\frac{d m_t^{(d)}}{d \\boldsymbol{V}}  \\\\\n&=& \\frac{\\partial g}{\\partial \\boldsymbol{V}} + \\sum_{t=1}^T \\sum_d ((\\frac{\\partial g}{\\partial \\boldsymbol{n}_t})^T \\, \\frac{\\partial \\boldsymbol{n}_t}{\\partial \\boldsymbol{m}_t} + (\\frac{\\partial g}{\\partial \\boldsymbol{m}_t})^T)^{(d)} \\frac{d m_t^{(d)}}{d \\boldsymbol{V}}  \\\\\n&=& 2 \\lambda \\boldsymbol{V} {\\operatorname{diag}} \\Big( \\sum_{t=0}^{T-1} \\big(\\widetilde{h^2}(\\boldsymbol{m}_t ) \\,-\\, {\\widetilde{h}}^2(\\boldsymbol{m}_t ) \\big) \\Big) + \\sum_{t=1}^T \\sum_d  \\boldsymbol{r}_t^{(d)} \\, \\boldsymbol{M}_t^{(d)}  \\\\\n\\boldsymbol{M}_t^{(d)} &\\triangleq& \\frac{d \\boldsymbol{m}_t^{(d)}}{d \\boldsymbol{V}}\\\\\n&=& \\frac{\\partial \\boldsymbol{m}_t^{(d)}}{\\partial \\boldsymbol{V}} + \\sum_{d^\\prime} \\frac{\\partial \\boldsymbol{m}_t^{(d)}}{\\partial \\boldsymbol{m}_{t-1}^{(d^\\prime)}} \\boldsymbol{M}_{t-1}^{(d^\\prime)}\\\\\n&=& \\mbox{\"Zero matrix except d'th row set to $\\widetilde{h}^T(\\boldsymbol{m}_{t-1})$\"} + \\sum_{d^\\prime} v_{d,d^\\prime}  {\\widetilde{h}}^\\prime (m_{t-1}^{(d^\\prime)}) \\boldsymbol{M}_{t-1}^{(d^\\prime)} \\\\\n\\boldsymbol{M}_1^{(d)} &\\triangleq& \\mbox{\"Zero matrix except d'th row set to $\\widetilde{h}^T(\\boldsymbol{m}_{0})$\"} \\,.\n\\end{eqnarray}\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n\\frac{d g}{d \\boldsymbol{U}} &=& \\frac{\\partial g}{\\partial \\boldsymbol{U}} + \\sum_{t=1}^T \\sum_d \\frac{d g}{d m_t^{(d)}} \\frac{d m_t^{(d)}}{d \\boldsymbol{U}}  \\\\\n&=& \\frac{\\partial g}{\\partial \\boldsymbol{U}} + \\sum_{t=1}^T \\sum_d ((\\frac{\\partial g}{\\partial \\boldsymbol{n}_t})^T \\, \\frac{\\partial \\boldsymbol{n}_t}{\\partial \\boldsymbol{m}_t} + (\\frac{\\partial g}{\\partial \\boldsymbol{m}_t})^T)^{(d)} \\frac{d m_t^{(d)}}{d \\boldsymbol{U}}  \\\\\n&=& 0 + \\sum_{t=1}^T \\sum_d \\boldsymbol{r}_t^{(d)} \\, \\Big( \\boldsymbol{P}_t^{(d)} \\Big) \\\\\n\\boldsymbol{P}_t^{(d)} &\\triangleq& \\frac{d \\boldsymbol{m}_t^{(d)}}{d \\boldsymbol{U}}\\\\\n&=& \\frac{\\partial \\boldsymbol{m}_t^{(d)}}{\\partial \\boldsymbol{U}} + \\sum_{d^\\prime} \\frac{\\partial \\boldsymbol{m}_t^{(d)}}{\\partial \\boldsymbol{m}_{t-1}^{(d^\\prime)}} \\boldsymbol{P}_{t-1}^{(d^\\prime)}\\\\\n&=& \\mbox{\"Zero matrix except d'th row set to $\\boldsymbol{x}_t^T$\"} + \\sum_{d^\\prime} v_{d,d^\\prime}  {\\widetilde{h}}^\\prime (m_{t-1}^{(d^\\prime)}) \\boldsymbol{P}_{t-1}^{(d^\\prime)} \\\\\n\\boldsymbol{P}_1^{(d)} &\\triangleq& \\mbox{\"Zero matrix except d'th row set to $\\boldsymbol{x}^T_1$\"} \\,.\n\\end{eqnarray}\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\begin{eqnarray}\n(\\frac{d g}{d \\boldsymbol{m}_0})^T &=& (\\frac{\\partial g}{\\partial \\boldsymbol{m}_0})^T + \\sum_{t=1}^T (\\frac{d g}{d \\boldsymbol{m}_t})^T \\, \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{m}_0} \\, \\\\\n&=& (\\frac{\\partial g}{\\partial \\boldsymbol{m}_0})^T + \\sum_{t=1}^T ((\\frac{\\partial g}{\\partial \\boldsymbol{n}_t})^T \\, \\frac{\\partial \\boldsymbol{n}_t}{\\partial \\boldsymbol{m}_t} + (\\frac{\\partial g}{\\partial \\boldsymbol{m}_t})^T) \\, \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{m}_0} \\, \\\\\n&=& \\lambda \\Big( \\big( {\\widetilde{h^2}}^\\prime(\\boldsymbol{m}_0) -2 {\\widetilde{h}}^\\prime(\\boldsymbol{m}_0) \\odot {\\widetilde{h}}(\\boldsymbol{m}_0) \\big)^T \\odot \\big( \\boldsymbol{1}^T (\\boldsymbol{V} \\odot \\boldsymbol{V}) \\big) \\\\\n& & \\quad\\quad\\quad  + 2 H \\sigma^2 ({\\widetilde{h}}^\\prime(\\boldsymbol{m}_0) \\odot {\\widetilde{h}}(\\boldsymbol{m}_0)  )^T \\Big) \\\\\n& & \\quad + \\sum_{t=1}^T \\boldsymbol{r}_t \\, \\Big( \\boldsymbol{Q}_t \\Big) \\\\\n\\boldsymbol{Q}_t &\\triangleq& \\frac{d \\boldsymbol{m}_t}{d \\boldsymbol{m}_0} = \\frac{\\partial \\boldsymbol{m}_t}{\\partial \\boldsymbol{m}_{t-1}} \\boldsymbol{Q}_{t-1} = \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}^\\prime (\\boldsymbol{m}_{t-1}) \\big) \\boldsymbol{Q}_{t-1}\\\\\n\\boldsymbol{Q}_0 &\\triangleq& \\boldsymbol{I} \\,.\n\\end{eqnarray}\n\n\n\\begin{center} {\\color{red}\\line(1,0){300}} \\end{center} \\vspace{-0.4in} \\begin{center} {\\color{red}\\line(1,0){250}} \\end{center}\n\n\\section{Bounding Linearization Error}\n\\label{sec:linear}\n\n\\paragraph{\\bf Proposition 1}\nAssume $n \\geq 5$, $c_f \\geq \\frac{1}{2\\pi}\\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}}$ and $\\rho^2 c_f \\frac{1}{\\sigma^2} \\leq \\epsilon$. Then if follows that $\\forall \\boldsymbol{x} \\,;\\, \\| \\boldsymbol{x} - \\boldsymbol{x}_0 \\| \\leq \\rho \\Rightarrow | g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) - g(\\boldsymbol{x})| \\leq \\epsilon$.\n\n\\begin{proof}\n\nFirst we claim that $\\frac{1}{2} \\Lambda_g \\leq  \\frac{1}{2 \\pi \\sigma^2} \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}}$. We prove this claim as below,\n\n\\begin{eqnarray}\n\\frac{1}{2}  \\Lambda_g &\\leq& \\max_{\\boldsymbol{x}} \\| \\nabla^2 g(\\boldsymbol{x})\\|_F \\\\\n&\\leq& \\max_{\\boldsymbol{x}}  \\sum_{j,k} | \\frac{d^2 g}{d x_j\\,  d x_k}(\\boldsymbol{x})| \\\\\n&\\leq&   \\sum_{j,k} \\max_{\\boldsymbol{x}} | \\frac{d^2 g}{d x_j\\,  d x_k}(\\boldsymbol{x})|\\\\\n&=& \\sum_{j,k} \\| \\frac{d^2 g}{d x_j\\,  d x_k}\\|_\\infty \\\\\n&=& \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k} \\star k_\\sigma \\|_\\infty \\\\\n\\label{eq:young}\n&\\leq& \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{p}{p-1}} \\,\\| k_\\sigma \\|_p \\\\\n&\\leq& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{p}{p-1}} \\Big) \\Big(\\int_\\mathcal{X} k^p_\\sigma(\\boldsymbol{x}) \\, d \\boldsymbol{x} \\Big)^{\\frac{1}{p}} \\\\\n&\\leq& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{p}{p-1}} \\Big) \\Big(\\frac{(2 \\pi)^{(1 - p)} \\sigma^{2(1 - p)}}{p} \\Big)^{\\frac{n}{4p}} \\\\\n&=& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{p}{p-1}} \\Big) \\Big(\\frac{(2 \\pi)^{(1 - p)} }{p} \\Big)^{\\frac{n}{4p}} \\sigma^{\\frac{n(1 - p)}{2p}}\\,,\n\\end{eqnarray}\n\nwhere (\\ref{eq:young}) is due to Young's convolution inequality and holds for any $p \\geq 1$. In particular, when $n \\geq 5$, by setting $p=\\frac{n}{n-4}$, we obtain\n\n\\begin{eqnarray}\n\\frac{1}{2}  \\Lambda_g &\\leq& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{p}{p-1}} \\Big) \\Big(\\frac{(2 \\pi)^{(1 - p)} }{p} \\Big)^{\\frac{n}{4p}} \\sigma^{\\frac{n(1 - p)}{2p}} \\\\\n\\frac{1}{2}  \\Lambda_g &=& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}} \\Big) \\frac{1}{2 \\pi \\sigma^2} \\Big(1-\\frac{4}{n} \\Big)^{\\frac{n}{4}-1} \\\\\n\\frac{1}{2}  \\Lambda_g &\\leq& \\Big( \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}} \\Big) \\frac{1}{2 \\pi \\sigma^2}  \\,.\n\\end{eqnarray}\n\nThis proves our earlier claim that $\\frac{1}{2} \\Lambda_g \\leq  \\frac{1}{2 \\pi \\sigma^2} \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}}$. Combining this with the assumption $\\frac{1}{2\\pi} \\sum_{j,k} \\| \\frac{d^2 f}{d x_j\\,  d x_k}\\|_{\\frac{n}{2}} \\leq c_f $, it follows that $\\frac{1}{2} \\Lambda_g \\leq c_f \\frac{1}{\\sigma^2}$, which implies $\\frac{1}{2} \\rho^2 \\Lambda_g \\leq \\rho^2 c_f \\frac{1}{\\sigma^2}$. The latter combined with the assumption $\\rho^2 c_f \\frac{1}{\\sigma^2} \\leq \\epsilon$ yields  $\\frac{1}{2} \\rho^2 \\Lambda_g \\leq \\epsilon$. Combining this with the Taylor's remainder theorem $| g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) - g(\\boldsymbol{x})| \\leq \\frac{1}{2} \\rho^2 \\Lambda_g$ gives $| g(\\boldsymbol{x}_0) + (\\boldsymbol{x} - \\boldsymbol{x}_0)^T \\nabla g(\\boldsymbol{x}_0) - g(\\boldsymbol{x})| \\leq \\epsilon$.\n\n{\\hfill \\ensuremath{\\Box}}\n\\end{proof}\n\n\n", "itemtype": "equation", "pos": 32605, "prevtext": "\n\nwhen $\\sigma$ is very large, and thus $\\epsilon$ is very small, we can ignore all the terms involving $\\epsilon$, which leaves us with $\\boldsymbol{M}_t\\approx \\boldsymbol{I}$. As we gradually reduce $\\sigma$, and thus increase $\\epsilon$, we can reconsider terms involving smaller exponents, while the higher order terms still remain negligible. By gradually decreasing $\\sigma$, $\\boldsymbol{M}_t$ can be approximated by $\\boldsymbol{I}$, then, $\\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\big) $, then $\\boldsymbol{I} + \\boldsymbol{V} {\\operatorname{diag}} \\big( {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\big) + \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-1}) \\, \\boldsymbol{V} {\\widetilde{h}}_\\sigma^\\prime (\\boldsymbol{m}_{t-2})$ and so on. \n\nThis is conceptually very similar to layerwise pretraining \\cite{Bengio06}, as the learning in each layer starts from considering only its immediate previous layer and then gradually switches to the full consideration by considering larger and larger number of previous layers. For example, $M_t$ at the first layer (i.e., when $t=1$) first considers contribution from itself only, then gradually introduces contribution from the second layer, and then from the third layer and so on.\n\n\n\\section{Choice of the Activation Function}\n\nIn order to implement the method, we need to obtain the explicit expressions of $\\widetilde{h}_\\sigma$ and $\\widetilde{h^2}_\\sigma$ for a given activation function $h$. For example, suppose we set $h(x)={\\operatorname{erf}}(a x)$, where $a$ is a parameter that determines the sharpness of the activation function. Note that $\\lim_{a \\rightarrow \\infty} {\\operatorname{erf}}(ax) = {\\operatorname{sign}}(x)$ and ${\\operatorname{erf}}(\\frac{\\sqrt{\\pi}}{2} x) \\approx \\tanh(x)$. The form of $\\widetilde{h}_\\sigma$ can be already looked up from Table \\ref{tab:Table}, which is repeated below,\n\n\n", "index": 15, "text": "\\begin{equation}\n\\tilde{h}(x) = {\\operatorname{erf}}(\\frac{a x}{\\sqrt{1+2 (a \\sigma)^2}}) \\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h}(x)={\\operatorname{erf}}(\\frac{ax}{\\sqrt{1+2(a\\sigma)^{2}}})\\,.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>erf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><mrow><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03c3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mfrac><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]