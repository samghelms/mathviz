[{"file": "1601.04605.tex", "nexttext": " \nwhere $R_i$ is the relevance of the subtopic at rank position $i$. When assuming subtopics are independent, i.e. $P(R_1=0,\\ldots,R_i=1) = P(R_{1}=0)\\ldots P(R_{i-1}=0) P(R_i=1) $\n\n\n\nthe expected search length for ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}} $ is\n \n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\setlength{\\pdfpagewidth}{8.5in}\n\n\\setlength{\\pdfpageheight}{11in}\n\n\n\\title{Dynamic Information Retrieval: Theoretical Framework and Application}\n\n\\numberofauthors{1}\n\\author{\n\\alignauthor\nMarc Sloan and Jun Wang\\\\\n\\affaddr{Department of Computer Science}\\\\\n\\affaddr{University College London}\\\\\n \\email{M.Sloan@cs.ucl.ac.uk, J.Wang@cs.ucl.ac.uk}\n \\alignauthor }\n\n\n\\maketitle \n\\begin{abstract}\n\nTheoretical frameworks like the \\emph{Probability Ranking Principle} and its more recent \\emph{Interactive Information Retrieval} variant have guided the development of ranking and retrieval algorithms for decades, yet they are not capable of  helping us model problems in \\emph{Dynamic Information Retrieval} which exhibit the following three properties; an observable user signal, retrieval over multiple stages and an overall search intent. In this paper a new theoretical framework for retrieval in these scenarios is proposed. We derive a general dynamic utility function for optimizing over these types of tasks, that takes into account the utility of each stage and the probability of observing user feedback. We apply our framework to experiments over TREC data in the dynamic multi page search scenario as a practical demonstration of its effectiveness and to frame the discussion of its use, its limitations and to compare it against the existing frameworks. \n\n\\end{abstract}\n\\vspace{-8pt}\n\\noindent\n\\category{H.3.3}{Information Search and Retrieval} Relevance feedback; Retrieval Models; Search process;\n\\vspace{-12pt}\n\\noindent\n  \\keywords{Dynamic IR, Interactive IR, Ranking and Retrieval Theory}\n\n\n\n\\section{Introduction}\n\nThe theoretical frameworks that underpin research in Information Retrieval (IR) are based on abstract models of user benefit. For instance, the loss function defined in the classic \\emph{Probability Ranking Principle} (\\emph{PRP})~\\cite{JD:1977:Robertson:PRP} leads to justification for the simplest and most powerful ranking rule in IR; ranking documents in decreasing order of their probability of relevance. A recent counterpart is the \\emph{Probability Ranking Principle for Interactive Information Retrieval} (\\emph{IIR-PRP})~\\cite{DBLP:journals/ir/Fuhr08}, which relaxes the independence assumption in the \\emph{PRP}'s model to take into account non-linear decision making. For example, document dependence is a key element in IR diversification that is not handled by the \\emph{PRP}~\\cite{Wang09portfoliotheory}. These models deal with traditional ad hoc query ranking and retrieval. Yet search tasks are complex and often exploratory, being comprised of multiple stages of retrieval with information needs specialized or generalized over time~\\cite{white2009exploratory}. Throughout, a user may broadcast signals of search intent that can help an IR system to improve retrieval. Examples include query reformulation in session search~\\cite{my-ijr-work} and item ratings in collaborative filtering~\\cite{Jambor:2012:UCT:2187836.2187839}. The described models are not capable of representing search tasks that operate over multiple stages nor can they incorporate user feedback. \n\n\n\nThese types of problems belong to the area of IR research known as \\emph{Dynamic Information Retrieval} (\\emph{DIR}), which we define as exhibiting three characteristics: user feedback, temporal dependency and an overall goal. In this paper we present \\emph{DIR} as a natural progression in IR research complexity; where early research concerned \\emph{static} problems such as ad hoc retrieval, which gave way to \\emph{interactive} tasks such as those incorporating relevance feedback~\\cite{Rocchio}, finally leading to \\emph{dynamic} systems where tasks such as ranking for session search are optimized~\\cite{grace-ecir}. \n\nFrom this progression we mathematically formulate a generalized framework that models the expected benefit to a user of completing a \\emph{DIR} task. This benefit is represented as a recursive utility function that is goal oriented and adaptive over time. The components of this utility function represent the three \\emph{DIR} features: the likelihood of user feedback, a probability of relevance model conditioned on this feedback and an individual stage utility function. The optimization of this recursive utility leads to an optimal policy of actions dependent on user interactions in the dynamic setting. \n\n\n\nThis utility is shown to be a form of Bellman equation~\\cite{Bellman:2003:DP:862270}, the framework an instantiation of a \\emph{Partially Observable Markov Decision Process} (\\emph{POMDP})~\\cite{Sondik:1978} and also a generalization over existing research in \\emph{DIR}~\\cite{grace-ecir,Jin:2013:IES:2488388.2488446}. The components of the utility can also be linked to the cost-benefit parameters of the \\emph{IIR-PRP} and the discount-gain functions found in session-based metrics such as \\emph{sDCG}~\\cite{Kanoulas:2011:EMS:2009916.2010056}. The structure of the utility function and its links to these areas of research give us interesting insights into the behavior of the function in \\emph{DIR} problems. These insights, such as how the quality and diversification of rankings vary over multiple stages, are supported by our experiments performed using a specific application of the \\emph{DIR} utility function over TREC data. Our experimental setting is the multi-page search scenario of choosing optimal rankings to display over several search pages for a fixed query~\\cite{Kim:2013:UPI:2541176.2505663,Jin:2013:IES:2488388.2488446}, a simplified \\emph{DIR} problem. As well as being a demonstration of the implementation of each of the components that make up the utility, practical aspects such as the computational complexity are also explored. \n\nThus, through its supporting theory (Section~\\ref{DIR}) and application (Section~\\ref{application}), we establish our dynamic utility function (Section~\\ref{opt-DIR}) as a new theoretical framework for the modeling of dynamic information retrieval problems. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Comparison of IR Frameworks}\n\\label{DIR}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[t!]\n        \\centering\n                \\caption{An example illustration of document ranking and relevance feedback using the vector space model for query $Q_1 = $ {\\tt apple}. Documents are given as points over two term frequency axes, {\\tt computer} and {\\tt fruit}, and can belong to one of three subtopics {\\tt apple fruit}, {\\tt apple logo} and {\\tt apple computer}. The distance between $Q$ and each document is inversely proportional to its relevance $r$. The documents ranked for $Q_1$ or its reformulation $Q_2$ are contained in each circular shape ${\\vec{\\mathbf{a}}}$, whose area could be thought of as the static utility ${U_S}({\\vec{\\mathbf{a}}}, {\\vec{\\mathbf{r}}})$, or ${U_D}$ the combined area of actions across stages 1 and 2.}\n        \\label{fig:dir-example}\n        \\begin{subfigure}[t]{0.238\\textwidth}\n                \\includegraphics[width=\\textwidth]{dir-example-a}\n                \\caption{Static IR: Documents within the ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ are shown to the user for query $Q_1$, but do not cover all subtopics. Optimally ranking using the \\emph{PRP} results in choosing those documents with the highest relevance.}\n                \\label{fig:dir-example-a}\n        \\end{subfigure}\n        ~ \n          \n        \\begin{subfigure}[t]{0.238\\textwidth}\n                \\includegraphics[width=\\textwidth]{dir-example-b}\n                \\caption{Interactive IR: After relevance feedback is observed (the two click observations ${o}$) on the static ranking in Fig.~\\ref{fig:dir-example-a}, $Q_1$ is modified to $Q_2$. Document relevance for $Q_2$ is now defined by ${\\tau}$ and the new interactive ranking given by documents in ${\\vec{\\mathbf{a}}}_{\\text{IIR}}$.}\n                \\label{fig:dir-example-b}\n        \\end{subfigure}\n        ~ \n          \n        \\begin{subfigure}[t]{0.238\\textwidth}\n                \\includegraphics[width=\\textwidth]{dir-example-c}\n                \\caption{Dynamic IR: Four potential rankings ${\\vec{\\mathbf{a}}}_1$, ${\\vec{\\mathbf{a}}}_2$,  ${\\vec{\\mathbf{a}}}_3$ and ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and their observation probabilities (shown as click observations with likelihood relative to size) for $Q_1$ are explored to find the optimal ranking action for both stages.}\n                \\label{fig:dir-example-c}\n        \\end{subfigure}\n                ~ \n          \n        \\begin{subfigure}[t]{0.238\\textwidth}\n                \\includegraphics[width=\\textwidth]{dir-example-d}\n                \\caption{Optimal Solution: Action ${\\vec{\\mathbf{a}}}_1$ is chosen as the optimal stage 1 ranking ${\\vec{\\mathbf{a}}}_{\\text{DIR}}$ as it diversely contains documents from all subtopics. As a result, the ranking ${\\vec{\\mathbf{a}}}_{\\text{IIR}}$ for $Q_2$ is more accurately modified after observing interactive feedback.}\n                \\label{fig:dir-example-d}\n        \\end{subfigure}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\nBefore setting up our framework for dynamic information retrieval, we consider \\emph{DIR} in the context of existing static and interactive theoretical IR frameworks in order to mathematically identify those features that distinguish it. \n\n\\subsection{Static IR Framework}\n\n\n\n\\emph{\\textbf{Definition:}} A static IR framework is one which models single user interactions, or else multiple \\emph{independent} interactions of different search intents. A typical application would be an ad hoc ranking and retrieval system.\n\nThe objective of a static system is to choose an \\textbf{action} ${a}$ (or sequence of actions ${\\vec{\\mathbf{a}}} = \\langle {a}_1,{a}_2,\\ldots \\rangle$), each of which has an associated \\textbf{probability of relevance} ${r}$ (or ${\\vec{\\mathbf{r}}}$ for a sequence of actions) that maximizes some \\textbf{static utility function} ${U_S}({a}, {r})$. The action represents a choice that can be made by the system and belongs to some action space ${\\mathcal{A}}$. For example, ${a}$ may be a query suggestion to display to a user, or ${\\vec{\\mathbf{a}}}$ the ranking order of a set of documents for retrieval. The utility function gives value to the action based on its probability of relevance by modeling the benefit of the action to the user. Utilities such as expected DCG and MAP~\\cite{WANG:2010:SAO:1835449.1835489} are examples from document retrieval, rewarding the ranking of relevant documents at high ranking positions. \n\n\\subsubsection{Probability Ranking Principle (PRP)}\n\nThe \\emph{PRP} defines ${U_S}({a}, {r})$ as a loss minimizing function across pairs of documents, which is optimized when ranking documents in decreasing order of probability of relevance (under document independence assumptions)~\\cite{JD:1977:Robertson:PRP}. Nonetheless, in instances where result diversity is important, it can be shown that \\emph{PRP} is no longer optimal \\cite{Wang09portfoliotheory}. We illustrate this with our example in Fig.~\\ref{fig:dir-example-a}. Here, we represent a simplified vector space model for ranking and retrieval using a graph over two term axes. In this case, the query is {\\tt apple}, an ambiguous term that can describe three subtopic search intents. Those documents within the ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ for the query are retrieved (analogous to ranking under the \\emph{PRP}), and as we can see, in this case only two subtopic preferences are captured. Over a population of users, those seeking information on the {\\tt apple logo} subtopic would be dissatisfied. \n\nWe can capture this probabilistically by supposing that we have two classes of users, ${\\text{{\\tt user}}_1}$ and ${\\text{{\\tt user}}_2}$, where ${\\text{{\\tt user}}_1}$ has twice as many members as ${\\text{{\\tt user}}_2}$.  Users in the ${\\text{{\\tt user}}_1}$ class are satisfied with the {\\tt apple logo} and {\\tt apple computer} subtopics, but not {\\tt apple fruit}, while those in the ${\\text{{\\tt user}}_2}$ class are only satisfied with the {\\tt apple fruit} subtopic. Our action space here is the set of subtopics which we denote $\\{{a}_1 = \\text{{\\tt apple logo}}, {a}_2 = \\text{{\\tt apple computer}}, {a}_3 = \\text{{\\tt apple fruit}}\\}$ and our goal is to choose the best ranking of subtopics. \n\nIf we set $R_{{a}_k} = 1$ if ${a}_k$ is relevant, and $r_{{a}_k} = P(R_{{a}_k} = 1)$, then we have $r_{{a}_1} = \\frac{2}{3}, r_{{a}_2} = \\frac{2}{3}$ and $r_{{a}_3} = \\frac{1}{3}$. According to the \\emph{PRP}, we should rank in decreasing order of the probability of relevance, giving us the ranking sequence ${\\vec{\\mathbf{a}}}_{\\text{PRP}} = \\langle{a}_1, {a}_2,  {a}_3\\rangle$. However, intuitively this is not optimal because users belonging to ${\\text{{\\tt user}}_2}$ have to reject two subtopics before reaching their preference~\\cite{cooper1971inadequacy}. We can explain this mathematically by studying the optimization of the diversity-encouraging metric Expected Search Length. One can also derive the same conclusion analogously using the equivalent Expected Reciprocal Rank or $k$-call at $n$ measures~\\cite{Chen:2006:LMP:1148170.1148245}. In this scenario, ${U_S}({\\vec{\\mathbf{a}}}, {\\vec{\\mathbf{r}}}) = E[L]_{{\\vec{\\mathbf{a}}}}$ which is the summation of all possible search lengths $L$ weighted by their respective probabilities, given as\n\n", "index": 1, "text": "\\begin{equation}\n  E[L]_{{\\vec{\\mathbf{a}}}} = \\sum_i \\big((i-1) P(R_1=0,\\ldots,R_{i-1}=0, R_i=1) \\big) \\nonumber \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"E[L]_{{\\vec{\\mathbf{a}}}}=\\sum_{i}\\big{(}(i-1)P(R_{1}=0,\\ldots,R_{i-1}=0,R_{i}%&#10;=1)\\big{)}\" display=\"block\"><mrow><mi>E</mi><msub><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>0</mn><mo>,</mo><msub><mi>R</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": " \nand for a diversified ranking ${\\vec{\\mathbf{a}}}_{\\text{DIV}}= \\langle {a}_1, {a}_3, {a}_2\\rangle $ the expected search length is\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \nwhere $R_i$ is the relevance of the subtopic at rank position $i$. When assuming subtopics are independent, i.e. $P(R_1=0,\\ldots,R_i=1) = P(R_{1}=0)\\ldots P(R_{i-1}=0) P(R_i=1) $\n\n\n\nthe expected search length for ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}} $ is\n \n", "index": 3, "text": "\\begin{align*}                                                                                                                                                                                                                                  &E[L]_{{\\vec{\\mathbf{a}}}_{\\text{PRP}} } = \\ 0\\!\\cdot\\!r_{{a}_1}+1\\!\\cdot\\!r_{{a}_2}(1\\!-\\! r_{{a}_1})+2\\!\\cdot\\! r_{{a}_3}(1\\!-\\!r_{{a}_2})(1\\!-\\!r_{{a}_1})\\\\\n&\\ \\ \\ \\  = \\ 0\\!\\cdot\\!(2/3) + 1\\!\\cdot\\! (2/3)(1/3) + 2\\!\\cdot\\! (1/3)(1/3)(1/3) = \\mathbf{8/27}\n\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E[L]_{{\\vec{\\mathbf{a}}}_{\\text{PRP}}}=\\ 0\\!\\cdot\\!r_{{a}_{1}}+1%&#10;\\!\\cdot\\!r_{{a}_{2}}(1\\!-\\!r_{{a}_{1}})+2\\!\\cdot\\!r_{{a}_{3}}(1\\!-\\!r_{{a}_{2}%&#10;})(1\\!-\\!r_{{a}_{1}})\" display=\"inline\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mtext>PRP</mtext></msub></msub></mrow><mo>=</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mn>\u20050</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><msub><mi>r</mi><msub><mi>a</mi><mn>1</mn></msub></msub></mrow><mo>+</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><msub><mi>r</mi><msub><mi>a</mi><mn>2</mn></msub></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">-</mo><msub><mi>r</mi><msub><mi>a</mi><mn>1</mn></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mn>2</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><msub><mi>r</mi><msub><mi>a</mi><mn>3</mn></msub></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">-</mo><msub><mi>r</mi><msub><mi>a</mi><mn>2</mn></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">-</mo><msub><mi>r</mi><msub><mi>a</mi><mn>1</mn></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ \\ \\ \\ =\\ 0\\!\\cdot\\!(2/3)+1\\!\\cdot\\!(2/3)(1/3)+2\\!\\cdot\\!(1/3)(1%&#10;/3)(1/3)=\\mathbf{8/27}\\par&#10;\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mo>=</mo><mpadded width=\"-1.7pt\"><mn>\u20050</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mpadded width=\"-1.7pt\"><mn>2</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>/</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>\ud835\udfd6</mn><mo>/</mo><mn>\ud835\udfd0\ud835\udfd5</mn></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n\n\n\n\nThus, in this case the \\emph{PRP} ranked documents have a shorter expected search path than the diversified ranking. Here, the \\emph{PRP} does lead to the optimal ranking under the independence assumption, but when we remove it this is no longer the case. To see this, we recalculate the expected search length for ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and ${\\vec{\\mathbf{a}}}_{\\text{DIV}} $ but this time without the independence assumption:  \n\n", "itemtype": "equation", "pos": 14318, "prevtext": " \nand for a diversified ranking ${\\vec{\\mathbf{a}}}_{\\text{DIV}}= \\langle {a}_1, {a}_3, {a}_2\\rangle $ the expected search length is\n\n", "index": 5, "text": "\\begin{align*}\n  E[L]_{{\\vec{\\mathbf{a}}}_{\\text{DIV}}} \\!=\\!&\\  0\\!\\cdot\\!(2/3)\\!+\\!1\\!\\cdot\\! (1/3)(1/3)\\! +\\! 2\\!\\cdot\\! (2/3)^2(1/3)  \\!=\\!\\mathbf{11/27 }\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E[L]_{{\\vec{\\mathbf{a}}}_{\\text{DIV}}}\\!=\" display=\"inline\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mtext>DIV</mtext></msub></msub></mpadded></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ 0\\!\\cdot\\!(2/3)\\!+\\!1\\!\\cdot\\!(1/3)(1/3)\\!+\\!2\\!\\cdot\\!(2/3)^{2%&#10;}(1/3)\\!=\\!\\mathbf{11/27}\" display=\"inline\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mn>0</mn></mpadded></mrow><mo rspace=\"0.8pt\">\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.8pt\">+</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.8pt\">+</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mn>2</mn></mpadded><mo rspace=\"0.8pt\">\u22c5</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"0.8pt\">=</mo><mrow><mn>\ud835\udfcf\ud835\udfcf</mn><mo>/</mo><mn>\ud835\udfd0\ud835\udfd5</mn></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n\nNow we find that the diversified ranking ${\\vec{\\mathbf{a}}}_{\\text{DIV}} $ has the shorter expected search length and is thus the optimal ranking, despite the lower probability of relevance for ${a}_3$. \n\n\\subsection{Interactive IR Framework}\n\n\\emph{\\textbf{Definition:}} An interactive IR framework extends a static framework to cover multiple stages of IR. It is \\emph{responsive} to feedback from a previous stage but does not anticipate future feedback. \n\nA \\textbf{stage} represents an interaction with the search system that is distinct from other interactions but belongs to the same search task, for example a sequence of impressions in session search. Generally, an IR system will operate over $1 \\leq t \\leq T$ stages with $T$ being potentially infinite. \n\nFurther to this, an interactive IR framework incorporates user feedback. Feedback is an \\textbf{observation} signal ${o}$ (or a sequence of observations ${\\vec{\\mathbf{o}}}$) in the space $\\mathcal{O}$, that is measurable by the search system. These signals may be \\emph{explicit} declarations of the relevance of search items (such as a movie rating), or \\emph{implicit} interpretations of user actions (such as document clicks). \n\nThe final element of this framework is the \\textbf{relevance update function} ${\\tau}$ where ${r}_{t + 1} = {\\tau}({a}_t, {r}_t, {o}_t)$. Thus, the objective function for interactive IR at stage $t + 1$ can now be represented as ${\\operatorname{argmax}}_{{a}_{t + 1} \\in {\\mathcal{A}}} {U_S} \\bigl({a}_{t + 1}, {\\tau}({a}_{t }, {r}_{t }, {o}_{t })\\bigr)$.\n\n\n\nThe relevance update function ${\\tau}$ introduces temporal dependency into the framework, without it the objective simply devolves to optimization over the static utility ${U_S}({a}, {r})$. This is also the case when finding the optimal first stage action ${a}_1$ i.e. when there are not yet any observations. In interactive IR, the optimal action is chosen at each stage as a reaction to the feedback observed in the previous stage and there is no consideration for future utility. \n\nWith these features in mind, we extend the vector space example to the interactive scenario in Fig.~\\ref{fig:dir-example-b} by introducing the Rocchio relevance feedback algorithm~\\cite{Rocchio} for interactively re-ranking documents. Here, clicked documents in the \\emph{PRP} ranked first stage are used as implicit signals of relevance to modify the user's original query $Q_1$ to $Q_2$. Document re-retrieval occurs using $Q_2$, returning documents using updated relevance scores given by ${\\tau}$, which is a function of $Q_2$ and thus the original ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$, document relevancies ${r}$ and observations ${o}$. Nonetheless, even in this interactive framework, a user interested in the {\\tt apple logo} subtopic would be dissatisfied with both the ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and ${\\vec{\\mathbf{a}}}_{\\text{IIR}}$ rankings due to a lack of documents for the relevant subtopic. \n\n\\subsubsection{Interactive Information Retrieval (IIR)}\n\nFor clarification, the area of research traditionally known as \\emph{Interactive Information Retrieval} (\\emph{IIR}) has an alternative definition to the interactive IR \\emph{framework} discussed in this paper, despite the similarity in name. \\emph{IIR} research explores the complex sequence of interactions a user may have with a search ranking within the static framework~\\cite{DBLP:journals/arist/Ruthven08}, largely motivated by the contradictory results found from conventional Cranfield style evaluation~\\cite{Cleverdon68} and observational user studies~\\cite{Hersh:2000:BUE:345508.345539}. With the exception of the \\emph{IIR-PRP} framework which we cover in more detail in Section~\\ref{sec:iir-prp}, for the remainder of this paper any reference to interactive IR instead reflects the framework defined in this paper. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Dynamic IR Theory}\n\\label{opt-DIR}\n\nA dynamic system is one which is goal-directed and adaptive to its environment. From this definition we can specify three elements that determine whether an IR system is a dynamic one:\n\\vspace{10pt}\n\\begin{description}[topsep=0.5ex]\n\\item [Feedback] An observation signal from the user.\n\\item [Temporal Dependency] Operation across multiple stages where each stage depends on the previous stage. \n\\item [Overall Goal] An objective across all stages. \n\\end{description}\n\n\n\\subsection{Dynamic IR Framework}\n\n\\emph{\\textbf{Definition:}} A dynamic IR framework extends an interactive framework by being responsive to user feedback \\emph{and} optimizing for it in advance.\n\nWe previously defined systems in the interactive IR framework as exhibiting both feedback and temporal dependency features, but they are only capable of locally optimizing for a single stage at a time. In contrast, the optimization of a dynamic system will find the optimal sequence of actions for all future interactions. A result of this is that the utility of an individual stage may be reduced so that gains can be made in the utility at a future stage. \n\nUnlike the interactive IR framework, the observation ${o}$ is unknown when evaluating the utility of future stages in the dynamic IR framework. Instead, the \\emph{expected} utility can be found by marginalizing the utility function over the space of observations ${\\mathcal{O}}$. When doing this the \\textbf{observation likelihood function} $P({o} | {a}, {r})$ must be specified. This gives the {expected} utility\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 14937, "prevtext": "\n\n\n\n\nThus, in this case the \\emph{PRP} ranked documents have a shorter expected search path than the diversified ranking. Here, the \\emph{PRP} does lead to the optimal ranking under the independence assumption, but when we remove it this is no longer the case. To see this, we recalculate the expected search length for ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and ${\\vec{\\mathbf{a}}}_{\\text{DIV}} $ but this time without the independence assumption:  \n\n", "index": 7, "text": "\\begin{align*}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        E[L]_{{\\vec{\\mathbf{a}}}_{\\text{PRP}} } =&\\ 0 \\cdot r_{{a}_1}+ 1\\cdot  P(R_{{a}_2}=1, R_{{a}_1}=0)) \\\\\n &\\ \\ \\ \\ \\ + 2\\cdot P(R_{{a}_3}=1, R_{{a}_2}=0, R_{{a}_1}=0)  \\\\\n=&\\  0\\cdot (2/3) + 1\\cdot 0 + 2\\cdot (1/3) = \\  \\mathbf{2/3}\\\\\nE[L]_{{\\vec{\\mathbf{a}}}_{\\text{DIV}}} =& \\ 0\\cdot (2/3) + 1\\cdot (1/3) + 2\\cdot 0 =\\ \\mathbf{1/3}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E[L]_{{\\vec{\\mathbf{a}}}_{\\text{PRP}}}=\" display=\"inline\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mtext>PRP</mtext></msub></msub></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ 0\\cdot r_{{a}_{1}}+1\\cdot P(R_{{a}_{2}}=1,R_{{a}_{1}}=0))\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u00a0</mi><mn>0</mn><mo>\u22c5</mo><msub><mi>r</mi><msub><mi>a</mi><mn>1</mn></msub></msub><mo>+</mo><mn>1</mn><mo>\u22c5</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><msub><mi>a</mi><mn>2</mn></msub></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>R</mi><msub><mi>a</mi><mn>1</mn></msub></msub><mo>=</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ \\ \\ \\ \\ +2\\cdot P(R_{{a}_{3}}=1,R_{{a}_{2}}=0,R_{{a}_{1}}=0)\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mo>+</mo><mn>2</mn><mo>\u22c5</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><msub><mi>a</mi><mn>3</mn></msub></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>R</mi><msub><mi>a</mi><mn>2</mn></msub></msub><mo>=</mo><mn>0</mn><mo>,</mo><msub><mi>R</mi><msub><mi>a</mi><mn>1</mn></msub></msub><mo>=</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ 0\\cdot(2/3)+1\\cdot 0+2\\cdot(1/3)=\\ \\mathbf{2/3}\" display=\"inline\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo>\u2062</mo><mn>0</mn></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>1</mn><mo>\u22c5</mo><mn>0</mn></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mn mathvariant=\"bold\">\u20052</mn><mo>/</mo><mn>\ud835\udfd1</mn></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E[L]_{{\\vec{\\mathbf{a}}}_{\\text{DIV}}}=\" display=\"inline\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mtext>DIV</mtext></msub></msub></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ 0\\cdot(2/3)+1\\cdot(1/3)+2\\cdot 0=\\ \\mathbf{1/3}\" display=\"inline\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo>\u2062</mo><mn>0</mn></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>1</mn><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u22c5</mo><mn>0</mn></mrow></mrow><mo>=</mo><mrow><mn mathvariant=\"bold\">\u20051</mn><mo>/</mo><mn>\ud835\udfd1</mn></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n\nThe observation likelihood function is represented visually in Fig.~\\ref{fig:dir-example-c}. In dynamic IR, the expected utility of potential first stage rankings (given here as ${\\vec{\\mathbf{a}}}_1$, ${\\vec{\\mathbf{a}}}_2$, ${\\vec{\\mathbf{a}}}_3$ and the optimal static ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$) are calculated by estimating which documents are likely to receive clicks and the effect this has on the utility of future stages. The \\emph{PRP} ranking is simply one among many rank actions that can be considered. \n\n\n\n\nThe final component of the \\emph{DIR} framework is the \\textbf{path discount} function ${\\omega}(t)$. When optimizing over a potentially infinite number of future stages, this helps ensure that a solution exists and also gives greater weight to earlier stage utilities.\n\nBy bringing together all of the components described so far, we can define the \\textbf{utility function for dynamic information retrieval} as \n\n", "itemtype": "equation", "pos": 21293, "prevtext": "\n\nNow we find that the diversified ranking ${\\vec{\\mathbf{a}}}_{\\text{DIV}} $ has the shorter expected search length and is thus the optimal ranking, despite the lower probability of relevance for ${a}_3$. \n\n\\subsection{Interactive IR Framework}\n\n\\emph{\\textbf{Definition:}} An interactive IR framework extends a static framework to cover multiple stages of IR. It is \\emph{responsive} to feedback from a previous stage but does not anticipate future feedback. \n\nA \\textbf{stage} represents an interaction with the search system that is distinct from other interactions but belongs to the same search task, for example a sequence of impressions in session search. Generally, an IR system will operate over $1 \\leq t \\leq T$ stages with $T$ being potentially infinite. \n\nFurther to this, an interactive IR framework incorporates user feedback. Feedback is an \\textbf{observation} signal ${o}$ (or a sequence of observations ${\\vec{\\mathbf{o}}}$) in the space $\\mathcal{O}$, that is measurable by the search system. These signals may be \\emph{explicit} declarations of the relevance of search items (such as a movie rating), or \\emph{implicit} interpretations of user actions (such as document clicks). \n\nThe final element of this framework is the \\textbf{relevance update function} ${\\tau}$ where ${r}_{t + 1} = {\\tau}({a}_t, {r}_t, {o}_t)$. Thus, the objective function for interactive IR at stage $t + 1$ can now be represented as ${\\operatorname{argmax}}_{{a}_{t + 1} \\in {\\mathcal{A}}} {U_S} \\bigl({a}_{t + 1}, {\\tau}({a}_{t }, {r}_{t }, {o}_{t })\\bigr)$.\n\n\n\nThe relevance update function ${\\tau}$ introduces temporal dependency into the framework, without it the objective simply devolves to optimization over the static utility ${U_S}({a}, {r})$. This is also the case when finding the optimal first stage action ${a}_1$ i.e. when there are not yet any observations. In interactive IR, the optimal action is chosen at each stage as a reaction to the feedback observed in the previous stage and there is no consideration for future utility. \n\nWith these features in mind, we extend the vector space example to the interactive scenario in Fig.~\\ref{fig:dir-example-b} by introducing the Rocchio relevance feedback algorithm~\\cite{Rocchio} for interactively re-ranking documents. Here, clicked documents in the \\emph{PRP} ranked first stage are used as implicit signals of relevance to modify the user's original query $Q_1$ to $Q_2$. Document re-retrieval occurs using $Q_2$, returning documents using updated relevance scores given by ${\\tau}$, which is a function of $Q_2$ and thus the original ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$, document relevancies ${r}$ and observations ${o}$. Nonetheless, even in this interactive framework, a user interested in the {\\tt apple logo} subtopic would be dissatisfied with both the ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and ${\\vec{\\mathbf{a}}}_{\\text{IIR}}$ rankings due to a lack of documents for the relevant subtopic. \n\n\\subsubsection{Interactive Information Retrieval (IIR)}\n\nFor clarification, the area of research traditionally known as \\emph{Interactive Information Retrieval} (\\emph{IIR}) has an alternative definition to the interactive IR \\emph{framework} discussed in this paper, despite the similarity in name. \\emph{IIR} research explores the complex sequence of interactions a user may have with a search ranking within the static framework~\\cite{DBLP:journals/arist/Ruthven08}, largely motivated by the contradictory results found from conventional Cranfield style evaluation~\\cite{Cleverdon68} and observational user studies~\\cite{Hersh:2000:BUE:345508.345539}. With the exception of the \\emph{IIR-PRP} framework which we cover in more detail in Section~\\ref{sec:iir-prp}, for the remainder of this paper any reference to interactive IR instead reflects the framework defined in this paper. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Dynamic IR Theory}\n\\label{opt-DIR}\n\nA dynamic system is one which is goal-directed and adaptive to its environment. From this definition we can specify three elements that determine whether an IR system is a dynamic one:\n\\vspace{10pt}\n\\begin{description}[topsep=0.5ex]\n\\item [Feedback] An observation signal from the user.\n\\item [Temporal Dependency] Operation across multiple stages where each stage depends on the previous stage. \n\\item [Overall Goal] An objective across all stages. \n\\end{description}\n\n\n\\subsection{Dynamic IR Framework}\n\n\\emph{\\textbf{Definition:}} A dynamic IR framework extends an interactive framework by being responsive to user feedback \\emph{and} optimizing for it in advance.\n\nWe previously defined systems in the interactive IR framework as exhibiting both feedback and temporal dependency features, but they are only capable of locally optimizing for a single stage at a time. In contrast, the optimization of a dynamic system will find the optimal sequence of actions for all future interactions. A result of this is that the utility of an individual stage may be reduced so that gains can be made in the utility at a future stage. \n\nUnlike the interactive IR framework, the observation ${o}$ is unknown when evaluating the utility of future stages in the dynamic IR framework. Instead, the \\emph{expected} utility can be found by marginalizing the utility function over the space of observations ${\\mathcal{O}}$. When doing this the \\textbf{observation likelihood function} $P({o} | {a}, {r})$ must be specified. This gives the {expected} utility\n\n\n\n\n\n\n\n", "index": 9, "text": "\\begin{align}\nE[{U_S}({a}, {r})] = \\sum_{{o} \\in {\\mathcal{O}}} P({o} | {a}, {r}) U_S({a}, {r}) \\label{eq:expectedutility}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E[{U_{S}}({a},{r})]=\\sum_{{o}\\in{\\mathcal{O}}}P({o}|{a},{r})U_{S%&#10;}({a},{r})\" display=\"inline\"><mrow><mi>E</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>U</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>o</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi></mrow></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>o</mi><mo stretchy=\"false\">|</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>U</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n where ${U_D}({r}_T, T) = \\max_{{a}_T \\in {\\mathcal{A}}} \\bigl[{U_S}({a}_T, {r}_T)\\bigr]$ is the static optimization of the final stage. Thus, our objective is to find, through backwards induction, the optimal sequence of actions ${\\vec{\\mathbf{a}}}^* = \\langle {a}_1, \\ldots, {a}_T \\rangle$ that maximizes the \\textbf{dynamic utility} ${U_D}$ given in  Eq.~(\\ref{eq:dynamic-utility}). To derive this utility we have simply recursively applied the dynamic utility to the expected utility from Eq.~(\\ref{eq:expectedutility}). \n \nThrough the maximization of the dynamic utility, in our example in Fig.~\\ref{fig:dir-example-d} we find the optimal action for stage 1, which is to diversify the initial ranking so that it retrieves documents belonging to all three subtopics. While this may harm the immediate retrieval utility score, overall the system improves because it can more accurately re-rank results over the subtopic preferences for all users in the next stage. \n \n\n \n\n\n\n\n\n\n\n\n\n\n \n\nWe observe that the eight elements of the \\emph{DIR} framework: ${a}$, ${r}$, ${U_S}$, $t$, ${o}$,  ${\\tau}$, $P({o} | {a}, {r})$ and ${\\omega}(t)$, are also the elements that define a \\emph{POMDP}~\\cite{Sondik:1978}, and that the dynamic utility function is its corresponding Bellman equation~\\cite{Bellman:2003:DP:862270}. Intuitively this makes sense, like a \\emph{POMDP} the dynamic IR framework finds an optimal Markovian sequence of actions to maximize a reward (here the static utility) subject to discounting (with ${\\omega}$). The state of the system (the underlying document relevance) is unknown but a belief state (the probability of relevance) is updated according to observations. The key difference from a \\emph{POMDP} is that for dynamic IR we do not define a transition probability between states as we assume that the hidden relevance of each document does not change throughout the search task. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Framework Analysis}\n\nSo far we have described the general framework for dynamic IR but have not addressed the setting of its parameters. Here we analyze each component within the context of dynamic information retrieval. \n\\vspace{-5pt}\n\n\\paragraph*{Relevance} As with any framework in information retrieval, the overall aim is to retrieve relevant information items and present them to the user. The intrinsic `relevance' of an information item is an unknown quality and the subject of most of the research in IR. In the \\emph{DIR} framework any document relevance scoring method can be used. For instance, in our application in the next section we make use of five well established relevance scoring techniques. \n\nThe relevance update function ${\\tau}$ is more difficult to define as it depends specifically on the action and observation space of the \\emph{DIR} task, for instance in Fig.~\\ref{fig:dir-example-b} it depends on the distance of the documents from $Q_2$, which itself depends on $Q_1$, ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and its clicked documents. This dependence allows ${\\tau}$ to adapt to the hidden relevance preferences of the user over the course of the search process. \n\nIt may not always be clear how to update the relevance score based on a given observation, the most straightforward setting for ${\\tau}$ can simply be to set ${r}_{{a}} = 0$ for actions already chosen by the IR system. Because ${\\tau}$ enforces the temporal dependency, it is the most important aspect in the dynamic utility because without it the utility is static.  \n\\vspace{-5pt}\n\\paragraph*{Actions} The action space is what distinguishes search tasks from one another and it is the size of this space that dictates the complexity of optimizing over the dynamic utility function. For example, the action space in query suggestion or document ranking is potentially infinite whereas the space of available advertisements in an ad selection problem may be small and finite. In our example the action space is any potential grouping of the documents in the 2D term space (four such groupings are shown in Fig.~\\ref{fig:dir-example-c}). Along with ${\\tau}$, the setting of the static utility ${U_S}$ is important for determining the desirable features of the optimal action sequence ${\\vec{\\mathbf{a}}}^*$, such as results diversification. \n\\vspace{-5pt}\n\\paragraph*{Observations} The observation space is dependent on the action space, its elements representing the user's response to system actions. Each observation must contain some signal of relevance or search intent, otherwise we would have ${\\tau}({a}, {r}, {o}) = {\\tau}({a}, {r})$ and lose the temporal dependency. In some cases the value of the observation likelihood is simply $P({o} | {a}, {r}) = {r}$, for instance in search tasks where accurate explicit relevance feedback is guaranteed. Otherwise, in most situations the observations will be click-related and thus the observation probability is the probability of click, as is the case in our example (Fig.~\\ref{fig:dir-example}).\n\\vspace{-5pt}\n\\paragraph*{Stages} Typically, the stages in a \\emph{DIR} task will represent distinct interactions occurring in a linear time order. In these cases ${\\omega}(t)$ may take a value between 0 and 1 or be set to a monotonically decreasing function that favorably weights the utility scores of immediate stages. Setting ${\\omega}(1) = 1$ and ${\\omega}(t) = 0$ for $t > 1$ gives us the static and interactive scenarios. \n\nAlternatively, a non-linear sequence of interactions (or \\emph{search path}) can be modeled as Yang and Lad did with their session-based utility function~\\cite{yangLad}. For instance in session search, a search path represents a particular sequence of documents examined by the user and the query reformulations made. For our framework, the stage $t$ may instead represent a specific search path, and so ${\\omega}(t)$ could be interpreted as the likelihood of this path rather than an explicit discount, penalizing improbable search paths and rewarding likely ones. \n\nThe time horizon $T$ dictates the number of advance stages to optimize for. A large time horizon will lead to explorative action strategies that benefit later stages. In our experiments, we set $T=2$ so that we only consider exploitative optimizations for the immediate next stage. \n\\vspace{-5pt}\n \\paragraph*{Dynamic Utility} Through the recursive evaluation of the utility function we not only learn the optimal sequence of actions to make in the dynamic system, but we also learn the optimal action for each possible observation at each stage. If we were to store these in a lookup table ahead of deployment, then the dynamic system would be immediately responsive to user feedback and able to cater to a population of users. Nonetheless, the construction and storage complexity of such a table may prove intractable. We also note that the static utility ${U_S}$ may be set as the dynamic utility function ${U_D}$ of a nested subproblem in the search task. For example, the utility of choosing an optimal r\\emph{anking of documents} may be embedded in the utility for determining an optimal \\emph{sequence of rankings} for a user in a session, which itself may be defined within the context of modeling a user's \\emph{topic preference} from search sessions in their search history. \n \n\\subsection{Links to Existing Work}\n\nBuilding on our analysis of the \\emph{DIR} framework, here we identify links between its components and other related work in \\emph{IIR} and session search. \n\n\\subsubsection{IIR-PRP}\n\\label{sec:iir-prp}\n\nThe \\emph{IIR-PRP}~\\cite{DBLP:journals/ir/Fuhr08} is a framework designed for interactive IR in the traditional sense. The objective function in \\emph{IIR-PRP} balances the costs and benefits of choosing actions within a sequence and bears some similarities to our dynamic utility function. Nonetheless, by lacking any form of user feedback or temporal dependency, we do not describe the model as interactive or dynamic, and as already discussed, within the terminology used in this paper this means that it is actually a static method. \n\nBy mapping our notation onto the \\emph{IIR-PRP} objective function, we get\n\n", "itemtype": "equation", "pos": 22379, "prevtext": "\n\nThe observation likelihood function is represented visually in Fig.~\\ref{fig:dir-example-c}. In dynamic IR, the expected utility of potential first stage rankings (given here as ${\\vec{\\mathbf{a}}}_1$, ${\\vec{\\mathbf{a}}}_2$, ${\\vec{\\mathbf{a}}}_3$ and the optimal static ranking ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$) are calculated by estimating which documents are likely to receive clicks and the effect this has on the utility of future stages. The \\emph{PRP} ranking is simply one among many rank actions that can be considered. \n\n\n\n\nThe final component of the \\emph{DIR} framework is the \\textbf{path discount} function ${\\omega}(t)$. When optimizing over a potentially infinite number of future stages, this helps ensure that a solution exists and also gives greater weight to earlier stage utilities.\n\nBy bringing together all of the components described so far, we can define the \\textbf{utility function for dynamic information retrieval} as \n\n", "index": 11, "text": "\\begin{align}\n{U_D}({r}_t, t) =& \\max_{{a}_t \\in {\\mathcal{A}}}\\bigl[{U_S}({a}_t, {r}_t) + \\nonumber \\\\\n&\\ \\ \\ {\\omega}(t)\\sum_{{o} \\in {\\mathcal{O}}}P({o} | {a}_t, {r}_t){U_D}({\\tau}({a}_t, {r}_t, {o}), t+1)\\bigr] \\label{eq:dynamic-utility}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{U_{D}}({r}_{t},t)=\" display=\"inline\"><mrow><mrow><msub><mi>U</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>t</mi></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max_{{a}_{t}\\in{\\mathcal{A}}}\\bigl{[}{U_{S}}({a}_{t},{r}_{t})+\" display=\"inline\"><mrow><munder><mi>max</mi><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></munder><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><msub><mi>U</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ \\ \\ {\\omega}(t)\\sum_{{o}\\in{\\mathcal{O}}}P({o}|{a}_{t},{r}_{t})%&#10;{U_{D}}({\\tau}({a}_{t},{r}_{t},{o}),t+1)\\bigr{]}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>\u03c9</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>o</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi></mrow></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>o</mi><mo stretchy=\"false\">|</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>r</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>U</mi><mi>D</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>r</mi><mi>t</mi></msub><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nevaluated over a sequence of $M$ actions (usually a ranking of documents). Eq.~(\\ref{eqn:iir-prp}) is a generalization of the formula originally defined, where we recognize that the cost and benefit parameters are simply a utility value, that the probability of whether a user continues searching or not is the observation likelihood, and that the cost of reaching a specific action is the path discount. \n\nFurther to this, the \\emph{IIR-PRP} defines a simple ranking rule according to this utility model, ranking documents in order of decreasing value of function $\\varrho({a}, {r})$, which balances the costs and benefits of choosing an action as well as its probability of relevance. We can define $\\varrho$ in our setting as\n\n", "itemtype": "equation", "pos": 30763, "prevtext": "\n where ${U_D}({r}_T, T) = \\max_{{a}_T \\in {\\mathcal{A}}} \\bigl[{U_S}({a}_T, {r}_T)\\bigr]$ is the static optimization of the final stage. Thus, our objective is to find, through backwards induction, the optimal sequence of actions ${\\vec{\\mathbf{a}}}^* = \\langle {a}_1, \\ldots, {a}_T \\rangle$ that maximizes the \\textbf{dynamic utility} ${U_D}$ given in  Eq.~(\\ref{eq:dynamic-utility}). To derive this utility we have simply recursively applied the dynamic utility to the expected utility from Eq.~(\\ref{eq:expectedutility}). \n \nThrough the maximization of the dynamic utility, in our example in Fig.~\\ref{fig:dir-example-d} we find the optimal action for stage 1, which is to diversify the initial ranking so that it retrieves documents belonging to all three subtopics. While this may harm the immediate retrieval utility score, overall the system improves because it can more accurately re-rank results over the subtopic preferences for all users in the next stage. \n \n\n \n\n\n\n\n\n\n\n\n\n\n \n\nWe observe that the eight elements of the \\emph{DIR} framework: ${a}$, ${r}$, ${U_S}$, $t$, ${o}$,  ${\\tau}$, $P({o} | {a}, {r})$ and ${\\omega}(t)$, are also the elements that define a \\emph{POMDP}~\\cite{Sondik:1978}, and that the dynamic utility function is its corresponding Bellman equation~\\cite{Bellman:2003:DP:862270}. Intuitively this makes sense, like a \\emph{POMDP} the dynamic IR framework finds an optimal Markovian sequence of actions to maximize a reward (here the static utility) subject to discounting (with ${\\omega}$). The state of the system (the underlying document relevance) is unknown but a belief state (the probability of relevance) is updated according to observations. The key difference from a \\emph{POMDP} is that for dynamic IR we do not define a transition probability between states as we assume that the hidden relevance of each document does not change throughout the search task. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Framework Analysis}\n\nSo far we have described the general framework for dynamic IR but have not addressed the setting of its parameters. Here we analyze each component within the context of dynamic information retrieval. \n\\vspace{-5pt}\n\n\\paragraph*{Relevance} As with any framework in information retrieval, the overall aim is to retrieve relevant information items and present them to the user. The intrinsic `relevance' of an information item is an unknown quality and the subject of most of the research in IR. In the \\emph{DIR} framework any document relevance scoring method can be used. For instance, in our application in the next section we make use of five well established relevance scoring techniques. \n\nThe relevance update function ${\\tau}$ is more difficult to define as it depends specifically on the action and observation space of the \\emph{DIR} task, for instance in Fig.~\\ref{fig:dir-example-b} it depends on the distance of the documents from $Q_2$, which itself depends on $Q_1$, ${\\vec{\\mathbf{a}}}_{\\text{PRP}}$ and its clicked documents. This dependence allows ${\\tau}$ to adapt to the hidden relevance preferences of the user over the course of the search process. \n\nIt may not always be clear how to update the relevance score based on a given observation, the most straightforward setting for ${\\tau}$ can simply be to set ${r}_{{a}} = 0$ for actions already chosen by the IR system. Because ${\\tau}$ enforces the temporal dependency, it is the most important aspect in the dynamic utility because without it the utility is static.  \n\\vspace{-5pt}\n\\paragraph*{Actions} The action space is what distinguishes search tasks from one another and it is the size of this space that dictates the complexity of optimizing over the dynamic utility function. For example, the action space in query suggestion or document ranking is potentially infinite whereas the space of available advertisements in an ad selection problem may be small and finite. In our example the action space is any potential grouping of the documents in the 2D term space (four such groupings are shown in Fig.~\\ref{fig:dir-example-c}). Along with ${\\tau}$, the setting of the static utility ${U_S}$ is important for determining the desirable features of the optimal action sequence ${\\vec{\\mathbf{a}}}^*$, such as results diversification. \n\\vspace{-5pt}\n\\paragraph*{Observations} The observation space is dependent on the action space, its elements representing the user's response to system actions. Each observation must contain some signal of relevance or search intent, otherwise we would have ${\\tau}({a}, {r}, {o}) = {\\tau}({a}, {r})$ and lose the temporal dependency. In some cases the value of the observation likelihood is simply $P({o} | {a}, {r}) = {r}$, for instance in search tasks where accurate explicit relevance feedback is guaranteed. Otherwise, in most situations the observations will be click-related and thus the observation probability is the probability of click, as is the case in our example (Fig.~\\ref{fig:dir-example}).\n\\vspace{-5pt}\n\\paragraph*{Stages} Typically, the stages in a \\emph{DIR} task will represent distinct interactions occurring in a linear time order. In these cases ${\\omega}(t)$ may take a value between 0 and 1 or be set to a monotonically decreasing function that favorably weights the utility scores of immediate stages. Setting ${\\omega}(1) = 1$ and ${\\omega}(t) = 0$ for $t > 1$ gives us the static and interactive scenarios. \n\nAlternatively, a non-linear sequence of interactions (or \\emph{search path}) can be modeled as Yang and Lad did with their session-based utility function~\\cite{yangLad}. For instance in session search, a search path represents a particular sequence of documents examined by the user and the query reformulations made. For our framework, the stage $t$ may instead represent a specific search path, and so ${\\omega}(t)$ could be interpreted as the likelihood of this path rather than an explicit discount, penalizing improbable search paths and rewarding likely ones. \n\nThe time horizon $T$ dictates the number of advance stages to optimize for. A large time horizon will lead to explorative action strategies that benefit later stages. In our experiments, we set $T=2$ so that we only consider exploitative optimizations for the immediate next stage. \n\\vspace{-5pt}\n \\paragraph*{Dynamic Utility} Through the recursive evaluation of the utility function we not only learn the optimal sequence of actions to make in the dynamic system, but we also learn the optimal action for each possible observation at each stage. If we were to store these in a lookup table ahead of deployment, then the dynamic system would be immediately responsive to user feedback and able to cater to a population of users. Nonetheless, the construction and storage complexity of such a table may prove intractable. We also note that the static utility ${U_S}$ may be set as the dynamic utility function ${U_D}$ of a nested subproblem in the search task. For example, the utility of choosing an optimal r\\emph{anking of documents} may be embedded in the utility for determining an optimal \\emph{sequence of rankings} for a user in a session, which itself may be defined within the context of modeling a user's \\emph{topic preference} from search sessions in their search history. \n \n\\subsection{Links to Existing Work}\n\nBuilding on our analysis of the \\emph{DIR} framework, here we identify links between its components and other related work in \\emph{IIR} and session search. \n\n\\subsubsection{IIR-PRP}\n\\label{sec:iir-prp}\n\nThe \\emph{IIR-PRP}~\\cite{DBLP:journals/ir/Fuhr08} is a framework designed for interactive IR in the traditional sense. The objective function in \\emph{IIR-PRP} balances the costs and benefits of choosing actions within a sequence and bears some similarities to our dynamic utility function. Nonetheless, by lacking any form of user feedback or temporal dependency, we do not describe the model as interactive or dynamic, and as already discussed, within the terminology used in this paper this means that it is actually a static method. \n\nBy mapping our notation onto the \\emph{IIR-PRP} objective function, we get\n\n", "index": 13, "text": "\\begin{align}\nU_{\\text{IIR}}({\\vec{\\mathbf{a}}}, {\\vec{\\mathbf{r}}}) =& \\sum_{i = 1}^M  \\biggl[\\prod_{j = 1}^{i - 1} (1 - {r}_j)\\biggr] \\times \\nonumber \\\\\n&\\ \\ \\ \\ \\ \\ \\  \\biggl(\\omega(i)+ {r}_i \\sum_{{o} \\in {\\vec{\\mathbf{o}}}}P( {o}| {a}_i, {r}_i){U_S}({a}_i, {r}_i)\\biggr)\\label{eqn:iir-prp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle U_{\\text{IIR}}({\\vec{\\mathbf{a}}},{\\vec{\\mathbf{r}}})=\" display=\"inline\"><mrow><mrow><msub><mi>U</mi><mtext>IIR</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{M}\\biggl{[}\\prod_{j=1}^{i-1}(1-{r}_{j})\\biggr{]}\\times\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>r</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mo>\u00d7</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\biggl{(}\\omega(i)+{r}_{i}\\sum_{{o}\\in{\\vec{\\mathbf%&#10;{o}}}}P({o}|{a}_{i},{r}_{i}){U_{S}}({a}_{i},{r}_{i})\\biggr{)}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u00a0</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mi>\u03c9</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mi>r</mi><mi>i</mi></msub><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>o</mi><mo>\u2208</mo><mover accent=\"true\"><mi>\ud835\udc28</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>o</mi><mo stretchy=\"false\">|</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>U</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nwhere the utility of adding an action ${a}_i$ to an existing sequence is countered by the path discount and the probability of not finding previous actions relevant. We implement this in our algorithm for \\emph{IIR-PRP} in Algorithm~\\ref{algo-iir-prp-mps} in the next section. \n\n \\subsubsection{Session-Based Utility}\n \n There have been recent advances in the modeling of user benefit across queries in search sessions. This is in recognition of the fact that ad-hoc retrieval often occurs over multiple queries in a session~\\cite{white2009exploratory}, with one study finding that 32\\% of search sessions consisted of at least three queries~\\cite{Jansen:2005:TCA:1059467.1059470}. A simple approach has been to extend discount-gain metrics such as DCG and Average Precision, typically associated with static retrieval, across multiple stages. Using our terminology, a discount-gain function for a single stage has the form $\\sum_{i = 1}^M  {\\omega}(i){U_S}({a}_i, {r}_i) $. For example, in the DCG metric the setting would be ${\\omega}(i) = \\frac{1}{\\log_2 (i + 1)}$ and ${U_S}({a}_i, {r}_i) = 2^{{r}_{{a}_i}} - 1$. For the session-DCG (sDCG)~\\cite{Jarvelin:2008:DCG:1793274.1793280} metric, a single layer of recursion is introduced, where \n \n", "itemtype": "equation", "pos": 31800, "prevtext": "\nevaluated over a sequence of $M$ actions (usually a ranking of documents). Eq.~(\\ref{eqn:iir-prp}) is a generalization of the formula originally defined, where we recognize that the cost and benefit parameters are simply a utility value, that the probability of whether a user continues searching or not is the observation likelihood, and that the cost of reaching a specific action is the path discount. \n\nFurther to this, the \\emph{IIR-PRP} defines a simple ranking rule according to this utility model, ranking documents in order of decreasing value of function $\\varrho({a}, {r})$, which balances the costs and benefits of choosing an action as well as its probability of relevance. We can define $\\varrho$ in our setting as\n\n", "index": 15, "text": "\\begin{align}\n\\varrho({\\vec{\\mathbf{a}}}_{1\\ldots i}, {\\vec{\\mathbf{r}}}_{1\\ldots i}, {\\omega}) = U_S({\\vec{\\mathbf{a}}}_{1\\ldots i}, {\\vec{\\mathbf{r}}}_{1\\ldots i}) - \\frac{\\omega}{r_{{a}_i}}\\prod_{j = 1}^{i - 1}(1 - r_{{a}_j}) \\label{eq:iir-prp-mps}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\varrho({\\vec{\\mathbf{a}}}_{1\\ldots i},{\\vec{\\mathbf{r}}}_{1%&#10;\\ldots i},{\\omega})=U_{S}({\\vec{\\mathbf{a}}}_{1\\ldots i},{\\vec{\\mathbf{r}}}_{1%&#10;\\ldots i})-\\frac{\\omega}{r_{{a}_{i}}}\\prod_{j=1}^{i-1}(1-r_{{a}_{j}})\" display=\"inline\"><mrow><mrow><mi>\u03f1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>U</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03c9</mi><msub><mi>r</mi><msub><mi>a</mi><mi>i</mi></msub></msub></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi><msub><mi>a</mi><mi>j</mi></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nand the discount and gain functions are set as ${\\omega}(t) = \\frac{1}{\\log_{2t}(t + 1)} $ and \n\n", "itemtype": "equation", "pos": 33308, "prevtext": "\nwhere the utility of adding an action ${a}_i$ to an existing sequence is countered by the path discount and the probability of not finding previous actions relevant. We implement this in our algorithm for \\emph{IIR-PRP} in Algorithm~\\ref{algo-iir-prp-mps} in the next section. \n\n \\subsubsection{Session-Based Utility}\n \n There have been recent advances in the modeling of user benefit across queries in search sessions. This is in recognition of the fact that ad-hoc retrieval often occurs over multiple queries in a session~\\cite{white2009exploratory}, with one study finding that 32\\% of search sessions consisted of at least three queries~\\cite{Jansen:2005:TCA:1059467.1059470}. A simple approach has been to extend discount-gain metrics such as DCG and Average Precision, typically associated with static retrieval, across multiple stages. Using our terminology, a discount-gain function for a single stage has the form $\\sum_{i = 1}^M  {\\omega}(i){U_S}({a}_i, {r}_i) $. For example, in the DCG metric the setting would be ${\\omega}(i) = \\frac{1}{\\log_2 (i + 1)}$ and ${U_S}({a}_i, {r}_i) = 2^{{r}_{{a}_i}} - 1$. For the session-DCG (sDCG)~\\cite{Jarvelin:2008:DCG:1793274.1793280} metric, a single layer of recursion is introduced, where \n \n", "index": 17, "text": "\\begin{align}\n sDCG = \\sum_{t = 1}^T  {\\omega}(t)U({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t)  \\label{eq:sDCG}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle sDCG=\\sum_{t=1}^{T}{\\omega}(t)U({\\vec{\\mathbf{a}}}_{t},{\\vec{%&#10;\\mathbf{r}}}_{t})\" display=\"inline\"><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n \n Here, the stages operate across a linear sequence of search rankings. In the session-Average Precision (sAP) metric, the path of interaction taken by the user is unknown and so the metric function marginalizes over the space of all such paths to find the expected sAP~\\cite{Kanoulas:2011:EMS:2009916.2010056}. \n \n \n \n\n\n\n\n\\section{Application}\n\\label{application}\n\n So far we have formulated a theoretical framework for dynamic IR and derived the dynamic utility function ${U_D}$ given in Eq.~(\\ref{eq:dynamic-utility}). In this section we apply this framework to the multi page search problem in  \\emph{DIR}. In doing so we demonstrate functional settings for the elements in the framework and their implementation in a workable algorithm, which gives us useful insight into the practical limitations of optimizing over ${U_D}$. We compare our algorithm against \\emph{PRP} and \\emph{IIR-PRP} based approaches in experiments using TREC data and also investigate static and interactive variants of our objective function. Through this we gain understanding of the effect that dynamic utility optimization has on the quality and diversity of rankings in multi page search. \n\n\n\\subsection{Multi Page Search Problem}\n\nThe \\emph{Multi Page Search} (MPS) scenario concerns the ranking of documents over multiple pages of search results~\\cite{Kim:2013:UPI:2541176.2505663,Jin:2013:IES:2488388.2488446}. MPS typically models exploratory search queries which are more likely to lead to multi-query sessions and multi-page searches~\\cite{white2009exploratory} (with one study finding that 27\\% of all searches occur over multiple pages~\\cite{Jansen:2006:WSW:1138797.1138813}). In this scenario documents are retrieved for a single query, ranked and then segregated into pages of $M$ documents. On each page, a user may examine and click on documents. We assume that the user will return to the results page and move onto the next page and we define a threshold of $T$ pages which the user will search over. The goal in MPS is to create rankings of relevant documents across $T$ pages. For the pages following the first, document clicks can be used to personalize search rankings, a situation analogous to our example in Fig.~\\ref{fig:dir-example}. \n\nWe chose this particular problem to apply our dynamic IR framework to as it exhibits the following beneficial features: 1) it is a \\emph{DIR} problem that is familiar and easy to define, 2) it is a simple IR scenario where we only have to consider a single query and a single set of documents, 3) we can use existing ad hoc ranking and retrieval research to find suitable implementations for the \\emph{DIR} framework components, 4) we can readily use TREC data collections and relevance judgments to evaluate our algorithms, and 5) it is naturally translatable to the \\emph{PRP} and \\emph{IIR-PRP} frameworks. A similar analysis of \\emph{DIR} in the session search scenario was also conducted by Luo et al.~\\cite{grace-ecir}\n\nIn this scenario, each page of search represents a stage in our framework, with $T$ the threshold number of pages. We nominally set $T=2$ although a larger number of pages is feasible and has been studied by Jin et al.~\\cite{Jin:2013:IES:2488388.2488446}. The action sequence ${\\vec{\\mathbf{a}}}_t = \\langle {a}_{t1}, \\ldots, {a}_{tM}\\rangle$ represents the ranking of documents for ranks 1 to $M$ on page $t$. Before we can fully implement Eq.~(\\ref{eq:dynamic-utility}), we must first define each of its functional components in the context of MPS. \n\n\n\\subsubsection{Expected DCG}\n\nThe static utility in multi page search is a measure of the quality of the ranking of documents on each page. As with ad hoc ranking and retrieval, we can evaluate this using a metric such as DCG, MAP or ERR. In the absence of relevance judgments, we can instead find the \\emph{expected} metric value which uses probabilities of relevance instead~\\cite{WANG:2010:SAO:1835449.1835489}. In our application we set the static utility as the expected DCG function, given by\n\n", "itemtype": "equation", "pos": 33528, "prevtext": "\nand the discount and gain functions are set as ${\\omega}(t) = \\frac{1}{\\log_{2t}(t + 1)} $ and \n\n", "index": 19, "text": "\\begin{align*}\nU({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t) = DCG({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t) = \\sum_{i = 1}^M\\underbrace{\\frac{1}{\\log_2(i + 1)}}_{{\\omega}(i)} \\times (\\underbrace{2^{{r}_{{a}_{ti}}} - 1}_{{U_S}({a}_{ti}, {r}_{ti})})\n \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle U({\\vec{\\mathbf{a}}}_{t},{\\vec{\\mathbf{r}}}_{t})=DCG({\\vec{%&#10;\\mathbf{a}}}_{t},{\\vec{\\mathbf{r}}}_{t})=\\sum_{i=1}^{M}\\underbrace{\\frac{1}{%&#10;\\log_{2}(i+1)}}_{{\\omega}(i)}\\times(\\underbrace{2^{{r}_{{a}_{ti}}}-1}_{{U_{S}}%&#10;({a}_{ti},{r}_{ti})})\" display=\"inline\"><mrow><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><munder><munder accentunder=\"true\"><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mi>log</mi><mn>2</mn></msub><mo movablelimits=\"false\">\u2061</mo><mrow><mo movablelimits=\"false\" stretchy=\"false\">(</mo><mrow><mi>i</mi><mo movablelimits=\"false\">+</mo><mn>1</mn></mrow><mo movablelimits=\"false\" stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></munder><mo>\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><msup><mn>2</mn><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo movablelimits=\"false\">\u2062</mo><mi>i</mi></mrow></msub></msub></msup><mo movablelimits=\"false\">-</mo><mn>1</mn></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><msub><mi>U</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>r</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\n\\\\\\\\This utility also takes into consideration the variance of the document's probability of relevance. \n\n\\subsubsection{Examination Hypothesis}\n\nIn multi page search our observations are document clicks, which we regard as an implicit signal of the relevance of a document to the user. Thus, we can utilize the clicks from previous search pages to update our probability of relevance model and personalize the document rankings for future pages. \n \n For a ranking of $M$ documents, the observation space ${\\mathcal{O}}$ in MPS for a particular page is the combination of binary click events for each document in the ranking. We denote this as the observation vector ${\\vec{\\mathbf{o}}} = \\langle {o}_1,\\ldots, {o}_M\\rangle$ where ${o} \\in \\{0, 1\\}$. We could na\\\"{\\i}vely  set the observation likelihood to the uniform distribution  $P({o} | {a}, {r}) = \\frac{1}{|{\\mathcal{O}}|}$ but eye-tracking studies tell us that this is not the case. Instead, the probability of a click occurring on a ranked document is dependent on not only its probability of relevance but also its rank position, amongst other variables~\\cite{Joachims:2007:EAI:1229179.1229181}. The probabilistic modeling of user clicks is an extensive area of IR research and in our application we use the simplest model, the \\emph{Examination Hypothesis} model~\\cite{clickmodels}.\n\n \nThis model supports the eye tracking research by inferring that the probability of a click on a document in a ranked list is equal to the product of its probability of relevance and the bias of its rank position. Thus, the probability of a sequence of clicks is given by \n \n", "itemtype": "equation", "pos": 37814, "prevtext": "\n \n Here, the stages operate across a linear sequence of search rankings. In the session-Average Precision (sAP) metric, the path of interaction taken by the user is unknown and so the metric function marginalizes over the space of all such paths to find the expected sAP~\\cite{Kanoulas:2011:EMS:2009916.2010056}. \n \n \n \n\n\n\n\n\\section{Application}\n\\label{application}\n\n So far we have formulated a theoretical framework for dynamic IR and derived the dynamic utility function ${U_D}$ given in Eq.~(\\ref{eq:dynamic-utility}). In this section we apply this framework to the multi page search problem in  \\emph{DIR}. In doing so we demonstrate functional settings for the elements in the framework and their implementation in a workable algorithm, which gives us useful insight into the practical limitations of optimizing over ${U_D}$. We compare our algorithm against \\emph{PRP} and \\emph{IIR-PRP} based approaches in experiments using TREC data and also investigate static and interactive variants of our objective function. Through this we gain understanding of the effect that dynamic utility optimization has on the quality and diversity of rankings in multi page search. \n\n\n\\subsection{Multi Page Search Problem}\n\nThe \\emph{Multi Page Search} (MPS) scenario concerns the ranking of documents over multiple pages of search results~\\cite{Kim:2013:UPI:2541176.2505663,Jin:2013:IES:2488388.2488446}. MPS typically models exploratory search queries which are more likely to lead to multi-query sessions and multi-page searches~\\cite{white2009exploratory} (with one study finding that 27\\% of all searches occur over multiple pages~\\cite{Jansen:2006:WSW:1138797.1138813}). In this scenario documents are retrieved for a single query, ranked and then segregated into pages of $M$ documents. On each page, a user may examine and click on documents. We assume that the user will return to the results page and move onto the next page and we define a threshold of $T$ pages which the user will search over. The goal in MPS is to create rankings of relevant documents across $T$ pages. For the pages following the first, document clicks can be used to personalize search rankings, a situation analogous to our example in Fig.~\\ref{fig:dir-example}. \n\nWe chose this particular problem to apply our dynamic IR framework to as it exhibits the following beneficial features: 1) it is a \\emph{DIR} problem that is familiar and easy to define, 2) it is a simple IR scenario where we only have to consider a single query and a single set of documents, 3) we can use existing ad hoc ranking and retrieval research to find suitable implementations for the \\emph{DIR} framework components, 4) we can readily use TREC data collections and relevance judgments to evaluate our algorithms, and 5) it is naturally translatable to the \\emph{PRP} and \\emph{IIR-PRP} frameworks. A similar analysis of \\emph{DIR} in the session search scenario was also conducted by Luo et al.~\\cite{grace-ecir}\n\nIn this scenario, each page of search represents a stage in our framework, with $T$ the threshold number of pages. We nominally set $T=2$ although a larger number of pages is feasible and has been studied by Jin et al.~\\cite{Jin:2013:IES:2488388.2488446}. The action sequence ${\\vec{\\mathbf{a}}}_t = \\langle {a}_{t1}, \\ldots, {a}_{tM}\\rangle$ represents the ranking of documents for ranks 1 to $M$ on page $t$. Before we can fully implement Eq.~(\\ref{eq:dynamic-utility}), we must first define each of its functional components in the context of MPS. \n\n\n\\subsubsection{Expected DCG}\n\nThe static utility in multi page search is a measure of the quality of the ranking of documents on each page. As with ad hoc ranking and retrieval, we can evaluate this using a metric such as DCG, MAP or ERR. In the absence of relevance judgments, we can instead find the \\emph{expected} metric value which uses probabilities of relevance instead~\\cite{WANG:2010:SAO:1835449.1835489}. In our application we set the static utility as the expected DCG function, given by\n\n", "index": 21, "text": "\\begin{align}\n{U_S}({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t) = \\sum_{i = i}^M \\frac{2^{{r}_{{a}_{ti}}} - 1 + 2^{{r}_{{a}_{ti}} - 1}\\log^2(2)Var[{r}_{{a}_{ti}}]}{\\log(i + 1)} \\label{eq:expected-dcg}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{U_{S}}({\\vec{\\mathbf{a}}}_{t},{\\vec{\\mathbf{r}}}_{t})=\\sum_{i=i}%&#10;^{M}\\frac{2^{{r}_{{a}_{ti}}}-1+2^{{r}_{{a}_{ti}}-1}\\log^{2}(2)Var[{r}_{{a}_{ti%&#10;}}]}{\\log(i+1)}\" display=\"inline\"><mrow><mrow><msub><mi>U</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mi>i</mi></mrow><mi>M</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msup><mn>2</mn><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub></msup><mo>-</mo><mn>1</mn></mrow><mo>+</mo><mrow><msup><mn>2</mn><mrow><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><msup><mi>log</mi><mn>2</mn></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nwhere $b_i$ is a rank bias parameter. In our implementation we set Eq.~(\\ref{eq:clickModel}) as our observation likelihood function and set $b_i = \\frac{1}{\\log (i + 1)}$ which is the discount value used in our expected DCG utility.\n\n \n\n \n\\subsubsection{Conditional Multivariate Gaussian Distribution}\n\nOnce we have a sequence of click observations for a ranking of documents, we can update the probability of relevance distribution for the remaining documents. Here, we achieve this by defining the distribution of all the probabilities of relevance for all documents in the collection as a multivariate Gaussian distribution $R \\sim \\mathcal{N}({\\vec{\\mathbf{r}}}, \\Sigma) $, where $R$ is their collective random variable, ${\\vec{\\mathbf{r}}}$ the vector of mean relevance scores and $\\Sigma$ the covariance matrix over the documents. ${\\vec{\\mathbf{r}}}$ may be set as any relevance score and $\\Sigma$ may be set using document similarity or other correlation scores~\\cite{Jin:2013:IES:2488388.2488446}. If ${\\vec{\\mathbf{r}}}$ represents a probability of relevance, then we can set the distribution as a \\emph{truncated} multivariate Gaussian bounded between 0 and 1. If it is not possible to define the distribution of a relevance score, then the distribution of the mean of multiple relevance scoring techniques can be derived, resulting in an approximately Gaussian distribution that incorporates multiple signals of relevance. It is this approach we take in our experiment, where we set ${\\vec{\\mathbf{r}}}$ and $\\Sigma$ as the means and variances of the retrieval scores from five well known techniques, with the diagonal elements from $\\Sigma$ used as variance values for our utility calculation in Eq.~(\\ref{eq:expected-dcg}). \n\nModeling the relevance distribution in this way allows us to conditionally update the probabilities of relevance ${\\vec{\\mathbf{r}}}$ based on our click observations. For a given rank action ${\\vec{\\mathbf{a}}}_t$ (which includes both clicked and non-clicked documents in the ranking), we denote the remaining non-ranked documents as $\\backslash{\\vec{\\mathbf{a}}}_t$ and partition our distribution parameters as\n\n", "itemtype": "equation", "pos": 39648, "prevtext": "\n\\\\\\\\This utility also takes into consideration the variance of the document's probability of relevance. \n\n\\subsubsection{Examination Hypothesis}\n\nIn multi page search our observations are document clicks, which we regard as an implicit signal of the relevance of a document to the user. Thus, we can utilize the clicks from previous search pages to update our probability of relevance model and personalize the document rankings for future pages. \n \n For a ranking of $M$ documents, the observation space ${\\mathcal{O}}$ in MPS for a particular page is the combination of binary click events for each document in the ranking. We denote this as the observation vector ${\\vec{\\mathbf{o}}} = \\langle {o}_1,\\ldots, {o}_M\\rangle$ where ${o} \\in \\{0, 1\\}$. We could na\\\"{\\i}vely  set the observation likelihood to the uniform distribution  $P({o} | {a}, {r}) = \\frac{1}{|{\\mathcal{O}}|}$ but eye-tracking studies tell us that this is not the case. Instead, the probability of a click occurring on a ranked document is dependent on not only its probability of relevance but also its rank position, amongst other variables~\\cite{Joachims:2007:EAI:1229179.1229181}. The probabilistic modeling of user clicks is an extensive area of IR research and in our application we use the simplest model, the \\emph{Examination Hypothesis} model~\\cite{clickmodels}.\n\n \nThis model supports the eye tracking research by inferring that the probability of a click on a document in a ranked list is equal to the product of its probability of relevance and the bias of its rank position. Thus, the probability of a sequence of clicks is given by \n \n", "index": 23, "text": "\\begin{align}\nP({\\vec{\\mathbf{o}}} | {\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t) = \\prod_{i = 1}^{M}(b_i {r}_{{a}_{ti}})^{{o}_i}(1 - b_i {r}_{{a}_{ti}})^{1 - {o}_i} \\label{eq:clickModel}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P({\\vec{\\mathbf{o}}}|{\\vec{\\mathbf{a}}}_{t},{\\vec{\\mathbf{r}}}_{%&#10;t})=\\prod_{i=1}^{M}(b_{i}{r}_{{a}_{ti}})^{{o}_{i}}(1-b_{i}{r}_{{a}_{ti}})^{1-{%&#10;o}_{i}}\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc28</mi><mo stretchy=\"false\">\u2192</mo></mover><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>i</mi></msub><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>o</mi><mi>i</mi></msub></msup><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>b</mi><mi>i</mi></msub><msub><mi>r</mi><msub><mi>a</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>-</mo><msub><mi>o</mi><mi>i</mi></msub></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nWe can then update the mean relevance scores and covariance matrix for non ranked documents using the formulae\n\n", "itemtype": "equation", "pos": 41999, "prevtext": "\nwhere $b_i$ is a rank bias parameter. In our implementation we set Eq.~(\\ref{eq:clickModel}) as our observation likelihood function and set $b_i = \\frac{1}{\\log (i + 1)}$ which is the discount value used in our expected DCG utility.\n\n \n\n \n\\subsubsection{Conditional Multivariate Gaussian Distribution}\n\nOnce we have a sequence of click observations for a ranking of documents, we can update the probability of relevance distribution for the remaining documents. Here, we achieve this by defining the distribution of all the probabilities of relevance for all documents in the collection as a multivariate Gaussian distribution $R \\sim \\mathcal{N}({\\vec{\\mathbf{r}}}, \\Sigma) $, where $R$ is their collective random variable, ${\\vec{\\mathbf{r}}}$ the vector of mean relevance scores and $\\Sigma$ the covariance matrix over the documents. ${\\vec{\\mathbf{r}}}$ may be set as any relevance score and $\\Sigma$ may be set using document similarity or other correlation scores~\\cite{Jin:2013:IES:2488388.2488446}. If ${\\vec{\\mathbf{r}}}$ represents a probability of relevance, then we can set the distribution as a \\emph{truncated} multivariate Gaussian bounded between 0 and 1. If it is not possible to define the distribution of a relevance score, then the distribution of the mean of multiple relevance scoring techniques can be derived, resulting in an approximately Gaussian distribution that incorporates multiple signals of relevance. It is this approach we take in our experiment, where we set ${\\vec{\\mathbf{r}}}$ and $\\Sigma$ as the means and variances of the retrieval scores from five well known techniques, with the diagonal elements from $\\Sigma$ used as variance values for our utility calculation in Eq.~(\\ref{eq:expected-dcg}). \n\nModeling the relevance distribution in this way allows us to conditionally update the probabilities of relevance ${\\vec{\\mathbf{r}}}$ based on our click observations. For a given rank action ${\\vec{\\mathbf{a}}}_t$ (which includes both clicked and non-clicked documents in the ranking), we denote the remaining non-ranked documents as $\\backslash{\\vec{\\mathbf{a}}}_t$ and partition our distribution parameters as\n\n", "index": 25, "text": "\\begin{align*}\n{\\vec{\\mathbf{r}}} = \\begin{bmatrix}\n       {\\vec{\\mathbf{r}}}_{\\backslash {\\vec{\\mathbf{a}}}_t}    \\\\\n       {\\vec{\\mathbf{r}}}_{{\\vec{\\mathbf{a}}}}\n     \\end{bmatrix}\\ \\ \\ \\ \n\\Sigma = \\begin{bmatrix}\n       \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t \\backslash {\\vec{\\mathbf{a}}}_t} & \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t{\\vec{\\mathbf{a}}}_t}          \\\\\n       \\Sigma_{ {\\vec{\\mathbf{a}}}_t\\backslash {\\vec{\\mathbf{a}}}_t} & \\Sigma_{ {\\vec{\\mathbf{a}}} {\\vec{\\mathbf{a}}}}\n     \\end{bmatrix}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\vec{\\mathbf{r}}}=\\begin{bmatrix}{\\vec{\\mathbf{r}}}_{\\backslash{%&#10;\\vec{\\mathbf{a}}}_{t}}\\\\&#10;{\\vec{\\mathbf{r}}}_{{\\vec{\\mathbf{a}}}}\\end{bmatrix}\\ \\ \\ \\ \\Sigma=\\begin{%&#10;bmatrix}\\Sigma_{\\backslash{\\vec{\\mathbf{a}}}_{t}\\backslash{\\vec{\\mathbf{a}}}_{%&#10;t}}&amp;\\Sigma_{\\backslash{\\vec{\\mathbf{a}}}_{t}{\\vec{\\mathbf{a}}}_{t}}\\\\&#10;\\Sigma_{{\\vec{\\mathbf{a}}}_{t}\\backslash{\\vec{\\mathbf{a}}}_{t}}&amp;\\Sigma_{{\\vec{%&#10;\\mathbf{a}}}{\\vec{\\mathbf{a}}}}\\end{bmatrix}\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi/><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mi mathvariant=\"normal\">\u03a3</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub></mtd><mtd columnalign=\"center\"><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></msub></mtd><mtd columnalign=\"center\"><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04605.tex", "nexttext": "\nand observations ${\\vec{\\mathbf{o}}}$. Thus, for given actions and observations, we can use the functions above to define a new conditional multivariate Gaussian distribution of the probability of relevance of the remaining documents, given as $\nR_{t + 1} \\sim \\mathcal{N}({\\vec{\\mathbf{r}}}_{\\backslash{\\vec{\\mathbf{a}}}_t}, \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t\\backslash {\\vec{\\mathbf{a}}}_t} | {\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{o}}})$. For the multi page search setting, we define ${\\tau}({a}, {r}, {o}) $ as the relevance update function in Eq.~(\\ref{eq:tau-func}). \n\n\\subsubsection{Geometric Discount}\n\nThe final component required for our application is the discount function ${\\omega}(t)$. In the multi page scenario, we measure the utility of a linear sequence of document rankings rather than the path-based behavior of users. As such, we adopt the simple discount used in a POMDP, setting ${\\omega} = \\lambda$ (which is effectively setting it as the geometric discount $\\omega(t) = \\lambda^{t-1}$ due to the recursion of the dynamic utility). \n\nHere, we can consider ${\\omega}(t)$ as the probability of the user visiting page $t$. When $\\lambda = 0$, we assume only the first page will be visited, and when $\\lambda = 1$ all pages are equally likely and given equal weight. The optimal setting for $\\lambda$ will vary depending on the type of searches being performed as well as the corpus and quality of results.\n\n\\subsection{DIR-MPS}\n\nNow that we have defined each of the functional components of the dynamic IR framework for the multi page search scenario, we present the DIR-MPS algorithm in Algorithm~\\ref{algo-dir-mps}. This algorithm is a direct implementation of the recursive utility function ${U_D}$ in Eq.~(\\ref{eq:dynamic-utility}) that determines the optimal sequence of document rankings to display for each page. \n\nIt is worth noting that Algorithm~\\ref{algo-dir-mps} and the described settings for the \\emph{DIR }framework elements are one such instantiation of the framework in the multi page search scenario. Our motive in this section is not to develop a state of the art new ranking technique but rather to demonstrate the application of the framework to a \\emph{DIR} problem. \n\n\n\n\\begin{algorithm}[!t]\n\\caption{The DIR-MPS Algorithm}\n\\label{algo-dir-mps}\n\\begin{algorithmic}\n\\Function{DIR-MPS}{$t, {\\vec{\\mathbf{r}}}, {\\mathcal{A}}$}\n\t\\If{$t = T + 1$} \\Return $[0, \\langle \\rangle]$ \\EndIf\n\t\\State ${\\vec{\\mathbf{a}}}_{t}^* = \\langle \\rangle; {\\vec{\\mathbf{a}}}^*_{t+1} = \\langle \\rangle$\n\t\\Loop{$\\; i \\leftarrow 1$ to $M$} \\Comment{Sequential Ranking Decision}\n\t\t\\State ${\\vec{\\mathbf{a}}} = {\\vec{\\mathbf{a}}}_{t}^*; u^* = 0$\n\t\t\\ForAll{$\\ a \\in {\\mathcal{A}}\\backslash {\\vec{\\mathbf{a}}}\\ $} \n\t\t\t\\State ${\\vec{\\mathbf{a}}}_t = \\langle {\\vec{\\mathbf{a}}}, a\\rangle$ \n\t\t\t\\State $u_t = {U_S}({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t)$ \\Comment{Eq.~(\\ref{eq:expected-dcg})}\n\t\t\t\\ForAll{${\\vec{\\mathbf{o}}} \\in {\\mathcal{O}}$} \n\t\t\t\t\\State ${\\vec{\\mathbf{r}}}_{t + 1}= \\tau({\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}, {o})$ \\Comment{Eq.~(\\ref{eq:tau-func})}\n\t\t\t\t\\State $[u_{t+1}, {\\vec{\\mathbf{a}}}_{t+1}] = \\text{DIR-MPS}( t+1,  {\\vec{\\mathbf{r}}}_{t+1}, {\\mathcal{A}} \\backslash {\\vec{\\mathbf{a}}}_t )$\n\t\t\t\t \\State $u_t = u_t + \\lambda \\cdot P({\\vec{\\mathbf{o}}}| {\\vec{\\mathbf{a}}}_t, {\\vec{\\mathbf{r}}}_t) \\cdot u_{t+1}$\\Comment{Eq.~(\\ref{eq:clickModel})}\n\t\t\t\\EndFor\n\t\t\t\\If{$u_t > u^*$} \n\t\t\t\t\\State $u^*=u_t; {\\vec{\\mathbf{a}}}_{t}^*={\\vec{\\mathbf{a}}}_{t}; {\\vec{\\mathbf{a}}}_{t+1}^* = {\\vec{\\mathbf{a}}}_{t+1}$\n\t\t\t\\EndIf \n\t\t\\EndFor\n\t\\EndLoop\n\t\\State\t\\Return $[u^*, \\langle {\\vec{\\mathbf{a}}}_{t}^*,{\\vec{\\mathbf{a}}}_{t+1}^*\\rangle]$\n\\EndFunction\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsubsection{Dynamic Utility Approximation}\n\nThe DIR-MPS algorithm features a number of approximation techniques that increase its computational efficiency as a way to counteract the inherent complexity of the \\emph{DIR} framework (discussed further in Section~\\ref{prac-limitations}). Firstly, we reduce the action space of potential rankings by employing a \\emph{Sequential Ranking Decision} policy. That is, for each page we find the optimal document to rank at each position one by one. For example, we set $M=1$ and find the document ${a}^*$ that maximizes ${U_S}({a},{r}_{{a}})$. Then we fix this document, set $M= 2$ and find the next in the sequence that maximizes ${U_S}(\\langle {a}^*, {a} \\rangle, {\\vec{\\mathbf{r}}}_{\\langle {a}^*, {a} \\rangle})$. Continuing in this fashion allows us to find an approximately optimal ranking for a single page, one document at a time, greatly reducing the computational complexity.\n\nA property of the probability distribution given in Eq.~(\\ref{eq:clickModel}) also allows us to greatly reduce the observation space. We find that this distribution follows Zipf's law, with a few of the click combinations contributing towards most of the probability mass. In fact, from our experiments we typically found that around 15\\% of the combinations contributed to 95\\% of the aggregated probability. As such, in our implementation of DIR-MPS we restrict the observation space to only the most probable click combinations that cumulatively sum to 0.95, trading off the potential 5\\% inaccuracy for speed. \n\nFinally, it can be shown that when ranking over a single stage, the expected DCG utility function is maximized when documents are ranked according to the \\emph{PRP}~\\cite{WANG:2010:SAO:1835449.1835489}. We exploit this to increase the efficiency of our algorithm by ranking the threshold page (where we no longer consider a future temporal dependency) according to the \\emph{PRP} over the conditionally updated probabilities of relevance. \n\\vspace{15pt}\n\\subsubsection{IIR-PRP-MPS}\n\nIn our experiments we directly compare DIR-MPS against rankings created from the applied \\emph{PRP} and \\emph{IIR-PRP} ranking rules. With the \\emph{Probability Ranking Principle} we can simply rank documents in decreasing order of the probability of relevance across $T$ pages. However, the \\emph{IIR-PRP} has no existing direct application to our scenario. Instead, we use our definition of the ranking function $\\varrho$ given in Eq.~(\\ref{eq:iir-prp-mps}) to create the IIR-PRP-MPS algorithm shown in Algorithm~\\ref{algo-iir-prp-mps}.\n\nHere, the sequential ranking rule is also employed to build up an optimal ranking over all pages, one document at a time, by selecting the document that has the highest $\\varrho$ value for each rank. Thus, there is some dependency on previously ranked documents, which is not possible in the \\emph{PRP}, but like the \\emph{PRP} there is no way to take into account user feedback or update the probabilities of relevance. \n\n\\begin{algorithm}[!t]\n\\caption{The IIR-PRP-MPS algorithm}\n\\label{algo-iir-prp-mps}\n\\begin{algorithmic}\n\\Function{IIR-PRP-MPS}{$M, T, \\lambda, {\\mathcal{A}}, {\\vec{\\mathbf{r}}}$}\n\t\\State ${\\vec{\\mathbf{a}}}^* = \\langle \\rangle$\n\t\\Loop{$\\; i \\leftarrow 1$ to $M \\times T$}\n\t\t\\State ${a}^* = {\\operatorname{argmax}}_{{a} \\in {\\mathcal{A}} \\backslash {\\vec{\\mathbf{a}}}^*}\\varrho(\\langle {\\vec{\\mathbf{a}}}^*, {a} \\rangle, {\\vec{\\mathbf{r}}}, \\lambda)$ \\Comment{Eq.~(\\ref{eq:iir-prp-mps})}\n\t\t\\State ${\\vec{\\mathbf{a}}}^* = \\langle {\\vec{\\mathbf{a}}}^*, {a}^* \\rangle$ \n\t\\EndLoop\n\t\\State\t\\Return ${\\vec{\\mathbf{a}}}^* $\n\\EndFunction\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\subsection{Practical Limitations}\n\\label{prac-limitations}\n\n\n\nThe general computational complexity of the optimization of $U_D$ can be shown to be PSPACE-Complete (through its connection to \\emph{POMDP}s). For small $T$ and observation and action spaces this can be reasonable, but typically these spaces may be impractically large. \n\nFor example, an IR task such as information filtering or music recommendation may operate over potentially infinite time steps. In these cases the discount factor and threshold $T$ are important. Further, the observation space may not be as well defined as that in our multi page search scenario where $|{\\mathcal{O}}| = 2^M$, for example, the space of possible reformulations for a query or 2D gaze positions in eye-tracking. Finally, the action space can be difficult to optimize over as is the case with DIR-MPS, where the sequential ranking decision reduces the size of the action space from $O({N!}/{(N - TM)!)}$ to $O(TNM - TM^2)$ for a collection of $N$ documents. Our application serves to demonstrate that such approximations may be needed when working with the \\emph{DIR} framework, especially given that the optimization of $U_D$ is not guaranteed to be tractable, and an optimal solution may not exist depending on the particular problem settings.\n\n\n\t\t\n\\subsection{Experiment}\n\nTo gain insight into our application of the dynamic IR framework in the multi page search scenario, and to compare with the other theoretical frameworks, we conducted an experiment using the WT10g, AQUAINT and ClueWeb09 datasets, the details of which are included in Table~\\ref{TBL:Corpus}. We chose these collections as they were designed for evaluating ranking and retrieval algorithms and were easily extended to the multi page problem. The WT10g dataset allowed us to test the theoretical frameworks in the standard ad hoc ranking and retrieval environment. The Robust data consists of difficult to rank ad hoc queries which we hypothesized would be more likely to require several pages of search results. The diversity track data allowed us to test our hypothesis that dynamic optimization leads to increased diversification in ad hoc retrieval. A drawback to using these datasets is that they lack interaction data, which is not needed when optimizing for probable clicks in the \\emph{DIR} framework, but important in the interactive setting. \n\n\\begin{table}[t]\n  \\centering\n  \\caption{Overview of the three TREC test collections}\n  \\setlength\\extrarowheight{0.1pt}\n  \\setlength\\tabcolsep{3pt} \n  \\begin{tabular}{| p{1.55cm} | p{1.9cm} | p{1.58cm} | p{2.25cm} | }\n  \\hline\nName & Task & \\# Docs & Topics \\\\\n\\hline  \n\\hline  \nWT10g & TREC 9 Web Track & 1,692,096 & 451-500 \\\\\n\\hline\nAQUAINT  & Robust 2005 & 1,033,461 & 50 difficult Robust 2004 topics \\\\\n\\hline\nClueWeb09 & Diversity Task 2009/10 & 503,903,810 & 1-100 (461 subtopics)\\\\\n\\hline\n  \\end{tabular}\n\\label{TBL:Corpus}\n\\end{table}\n\nOn each collection we retrieved the top 100 documents for each topic scored using each of the TF-IDF, BM25, Jelinek-Mercer, Dirichlet and Two-Stage language model retrieval methods from the Indri\\footnote{\\url{http://www.lemurproject.org/indri.php}} search engine. We pooled the documents and subsequently scored them over all the techniques. This gave us an average of 193 ranked documents per topic each with 5 relevance score values. After min-max normalization we averaged each score to give us our probability of relevance vector ${\\vec{\\mathbf{r}}}$ and covariance matrix $\\Sigma$. The dependencies in this covariance matrix reflect the level of agreement between the different retrieval methods rather than direct correlations between the documents themselves i.e. similarly ranked documents will be positively correlated with one another. Finally, we selected those documents that had the top 30 mean relevance scores. These were then used by our algorithms to create rankings for two pages of search results with ten documents on each. \n\nWe ranked these documents according to the baseline \\emph{PRP} approach and also the already described IIR-PRP-MPS and DIR-MPS algorithms. We also investigated an interactive version of DIR-MPS (called IIR-MPS) that ranks the first page of results according to the \\emph{PRP} and then optimizes a ranking for the second page of results by marginalizing over potential clicks using Eq.~(\\ref{eq:expectedutility}). We also created a static version of DIR-MPS (called S-MPS) that removes feedback from $U_D$ entirely to give us the objective function ${U_S}({\\vec{\\mathbf{a}}}_1, {\\vec{\\mathbf{r}}}) + \\lambda {U_S}({\\vec{\\mathbf{a}}}_2, {\\vec{\\mathbf{r}}})$. We also investigated `perfect click' variants of the dynamic (DIR-MPS$^C$) and interactive (IIR-MPS$^C$) algorithms, where we interpret the hidden relevance labels as clicks on the first page of results, giving us the optimal observation setting and an upper bound on performance for the second page. \n\nTo evaluate the quality of the rankings we measured MAP, NDCG and ERR for each page. For the DIR-MPS and IIR-MPS algorithms we actually generate an optimal 2nd page ranking for every click combination in our observation space, giving us different metrics scores for each. In these cases, the reported page two metric scores are averages over the page two scores for all click combination based rankings. This highlights an open area for research; the definition of evaluation metrics for \\emph{DIR} that can take into account all of the potential rankings in a dynamic system. We also measure the session-based metrics sDCG (defined in Eq.~(\\ref{eq:sDCG})) and sAP to evaluate performance over both pages, although it is worth noting that these metrics were designed for session search rather than multi page search. Finally, we also measure $\\alpha-$NDCG~\\cite{novelty-and-diversity}, Intent-Aware Precision (IA-Precision)~\\cite{Agrawal:diversifying} and Intent-Aware ERR (ERR-IA)~\\cite{ExpectedReciprocalRank} for scoring the diversity of rankings in the ClueWeb09 collection. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\setlength{\\dbltextfloatsep}{0pt}\n\n\\begin{table*}[t]\n  \\centering\n  \\caption{NDCG, MAP and ERR scores for pages 1 and 2 of the search results and sAP and sDCG over both pages. Static, interactive and dynamic algorithms are grouped. The results shown are those for the optimal value of $\\lambda$ in each collection, found by repeating the experiment for values in the range $[0,1]$. The maximum score for each metric on each page is given in boldface. A $^1$ or $^2$ indicates that the result is significantly better than the \\emph{PRP} or \\emph{IIR-PRP-MPS} baseline scores respectively using the Wilcoxon signed-rank test $(p < 0.05)$. }\n \n  {\\setlength{\\extrarowheight}{1pt}\n    \\begin{tabular}{|p{2cm}| l | c | c | c | c | c | c | c | c |}\n    \\cline{3-10}\n\\multicolumn{2}{c|}{}    & \\multicolumn{3}{c|}{Page 1} & \\multicolumn{3}{c|}{Page 2} & \\multicolumn{2}{c|}{Both Pages} \\\\\n\\hline\nCollection & Algorithm & NDCG & MAP   & ERR   & NDCG & MAP   & ERR   & sAP   & sDCG \\\\\n\\hline\n\\hline\n{\\multirow{7}{*}{\\begin{minipage}{2cm}Web Track (WT10g) $\\lambda=0.5$\\end{minipage}}} & PRP   & \\textbf{0.338} & \\textbf{0.167} & \\textbf{0.169} & 0.133 & 0.025 & 0.053 & 0.097 & 1.326 \\\\\n  & IIR-PRP-MPS & 0.330 & 0.162 & 0.162 & 0.166 & 0.041 & 0.078 & 0.101 & 1.347 \\\\\n    & S-MPS & 0.295 & 0.134 & 0.130 & 0.226 & \\textbf{0.070} & 0.242$^{12}$ & 0.095 & 1.236 \\\\\n    \\cline{2-10}\n    & IIR-MPS & \\multirow{2}[4]{*}{\\textbf{0.338}} & \\multirow{2}[4]{*}{\\textbf{0.167}} & \\multirow{2}[4]{*}{\\textbf{0.169}} & 0.125 & 0.025 & 0.103$^1$ & 0.097 & 1.291 \\\\\n& IIR-MPS$^C$ &       &       &       & 0.154 & 0.040 & 0.151$^1$ & \\textbf{0.102} & \\textbf{1.353} \\\\\n    \\cline{2-10}\n    & DIR-MPS & \\multirow{2}[4]{*}{0.235} & \\multirow{2}[4]{*}{0.091} & \\multirow{2}[4]{*}{0.092} & 0.212$^1$ & 0.054$^1$ & 0.289$^{12}$ & 0.069 & 1.022 \\\\\n    & DIR-MPS$^C$ &       &       &       & \\textbf{0.230}$^1$ & 0.059 & \\textbf{0.297}$^{12}$ & 0.072 & 1.027 \\\\\n\\hline\n\\hline\n   \\multirow{7}{*}{\\begin{minipage}{2cm}Robust (AQUAINT) $\\lambda = 0.5$\\end{minipage}} & PRP   & 0.624 & \\textbf{0.107} & \\textbf{0.398} & 0.552 & 0.061 & 0.294 & \\textbf{0.085} & 5.735 \\\\\n& IIR-PRP-MPS & \\textbf{0.629} & \\textbf{0.107} & \\textbf{0.398} & 0.514 & 0.052 & 0.288 & 0.083 & 5.680 \\\\\n    & S-MPS & 0.608 & 0.096 & 0.388 & \\textbf{0.595} & \\textbf{0.066} & \\textbf{0.887}$^{12}$ & 0.083 & \\textbf{5.749} \\\\\n    \\cline{2-10}\n   & IIR-MPS & \\multirow{2}[4]{*}{0.624} & \\multirow{2}[4]{*}{\\textbf{0.107}} & \\multirow{2}[4]{*}{\\textbf{0.398}} & 0.519 & 0.050 & 0.737$^{12}$ & 0.081 & 5.543 \\\\\n  & IIR-MPS$^C$ &       &       &       & 0.554 & 0.057 & 0.690$^{12}$ & 0.084 & 5.729 \\\\\n    \\cline{2-10}\n    & DIR-MPS & \\multirow{2}[3]{*}{0.548} & \\multirow{2}[3]{*}{0.063} & \\multirow{2}[3]{*}{0.304} & 0.575 & 0.065 & 0.656$^{12}$ & 0.065 & 4.921 \\\\\n   & DIR-MPS$^C$ &       &       &       & 0.553 & 0.058 & 0.697$^{12}$ & 0.062 & 4.909 \\\\\n    \\hline\n    \\hline\n    \\multirow{7}{*}{\\begin{minipage}{2cm}Diversity (ClueWeb09) $\\lambda = 0.8$ \\end{minipage}} & PRP   & 0.402$^2$ & \\textbf{0.049}$^2$ & 0.199 & \\textbf{0.476} & 0.052 & 0.265 & \\textbf{0.051}$^2$ & \\textbf{1.883}$^2$ \\\\\n          & IIR-PRP-MPS & 0.384 & 0.046 & 0.193 & 0.468 & 0.051 & 0.257 & 0.048 & 1.808 \\\\\n          & S-MPS & 0.388 & 0.041 & 0.193 & 0.465 & \\textbf{0.054} & 0.358$^{12}$ & 0.049 & 1.787 \\\\\n          \\cline{2-10}\n          & IIR-MPS & \\multirow{2}[4]{*}{0.402$^2$} & \\multirow{2}[4]{*}{\\textbf{0.049}$^2$} & \\multirow{2}[4]{*}{0.199} & 0.431 & 0.042 & 0.353$^{12}$ & 0.047 & 1.783 \\\\\n          & IIR-MPS$^C$ &       &       &       & 0.436 & 0.042 & 0.345$^{12}$ & 0.047 & 1.787 \\\\\n          \\cline{2-10}\n          & DIR-MPS & \\multirow{2}[4]{*}{\\textbf{0.451}$^{2}$} & \\multirow{2}[4]{*}{0.047} & \\multirow{2}[4]{*}{\\textbf{0.238}$^{2}$} & 0.445 & 0.042 & \\textbf{0.373}$^{12}$ & 0.046 & 1.859 \\\\\n          & DIR-MPS$^C$ &       &       &       & 0.426 & 0.037 & 0.356$^{12}$ & 0.044 & 1.839 \\\\\n    \\hline\n    \\end{tabular}}\n  \\label{tbl:general-results}\n\\end{table*}\n\n\nThe results of our experiments are shown in Table~\\ref{tbl:general-results}. For the Web Track and Robust collections, we observe that the 1st page losses of the dynamic techniques (when compared to the \\emph{PRP} and \\emph{IIR-PRP-MPS} baselines) are made up for by gains in the second page, significantly so on the WT10g dataset. Nonetheless, in these ad hoc ranking scenarios it is clear that the static \\emph{PRP} and \\emph{IIR-PRP} frameworks are still very effective.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n  \\centering\n  \\caption{$\\alpha$-DCG, IA-Precision and ERR-IA scores for page 1 and 2 search results from the diversity track data. The maximum score for each metric on each page is given in boldface. A $^2$ indicates that the result is significantly better than the \\emph{IIR-PRP-MPS} baseline score using the Wilcoxon signed-rank test $(p < 0.05)$. }\n  \\setlength\\tabcolsep{3pt} \n    \\begin{tabular}{|p{2.08cm}| p{0.9cm} | p{0.8cm} | p{0.8cm} | p{0.8cm} | p{0.8cm} | p{0.8cm} |}\n    \\cline{2-7}\n\\multicolumn{1}{c|}{}          & \\multicolumn{3}{c|}{Page 1} & \\multicolumn{3}{c|}{Page 2} \\\\\n    \\hline\n    Algorithm & $\\alpha$-DCG & IA-Prec & ERR-IA & $\\alpha$-DCG & IA-Prec & ERR-IA\\\\\n    \\hline\n    \\hline\n    PRP   & 0.360 & \\textbf{0.083} & 0.239 & \\textbf{0.420} & 0.085 & \\textbf{0.294} \\\\\n    IIR-PRP-MPS & 0.345 & 0.077 & 0.230 & 0.404 & 0.086 & 0.269 \\\\\n    S-MPS & 0.352 & 0.078 & 0.233 & 0.417 & \\textbf{0.089} & 0.280 \\\\\n    \\hline\n    IIR-MPS & \\multirow{2}[4]{*}{0.360} & \\multirow{2}[4]{*}{\\textbf{0.083}} & \\multirow{2}[4]{*}{0.239} & 0.377 & 0.079 & 0.243 \\\\\n    IIR-MPS$^C$ &       &       &       & 0.379 & 0.080 & 0.236 \\\\\n    \\hline\n    DIR-MPS & \\multirow{2}[4]{*}{\\textbf{0.403$^2$}} & \\multirow{2}[4]{*}{0.082} & \\multirow{2}[4]{*}{\\textbf{0.270}} & 0.400 & 0.079 & 0.264 \\\\\n    DIR-MPS$^C$  &       &       &       & 0.386 & 0.077 & 0.254 \\\\\n    \\hline\n    \\end{tabular}\n  \\label{tbl:diversity-results}\n\\end{table}\n\n\nWe see different results with the diversity track data. The metric scores for this data in Table~\\ref{tbl:general-results} were calculated using relevance judgments from all subtopics. We see the opposite relationship between page scores here, with DIR-MPS having higher scores for the 1st page and losses in the second (except for ERR which is significantly improved across both pages). We see further evidence of this with the diversity metric scores in Table~\\ref{tbl:diversity-results}, where it is clear that diversification is occurring in the first page and less so in the second. This backs up our intuition (in Fig.~\\ref{fig:dir-example-d}) that a dynamic technique will initially diversify results to improve future rankings, and also helps explain the losses in performance of the 1st page in the other datasets (which do not have subtopic relevance judgments). The reduced diversity of the 2nd page indicates that it is more tightly focused on the user's subtopic preference. \n\nWe observe that the diversity task is more suited as an application of the dynamic IR framework. This is evidenced by the optimal settings for $\\lambda$ in each collection. For the ad hoc ranking task in the WT10g and AQUAINT collections, the setting for $\\lambda$ gives greater weight to the 1st page of results, rewarding immediately effective rankings. Whereas in the diversity task, the utility of the 2nd page has a larger effect on the overall utility, encouraging diversity. \n\nFurther to this, the interactive variant scored highly with \\emph{session based metrics} on the Robust dataset, but otherwise the static techniques were optimal, even for the diversity task. This may partly be due to the application of a session-based metric to the multi page scenario and also the inability of the metric to take into account the user interaction. Finally, we also see that the `perfect click' variants generally outperform their counterparts (except over the diversity data), indicating that the 2nd page ranking can be improved when high quality clicks are observed. \n\nIn summary, by its nature the \\emph{DIR} approach to multi-page search places greater emphasis on different stages of the search task. We find that this may not be suited to all search environments i.e. ad hoc search. In such cases the static approaches can be more effective. Nonetheless, the dynamic IR framework has other desirable properties such as the diversification and personalization of results over time. \n\n\n\n\\section{Related Work}\n\n\n\nThroughout this paper we have presented the dynamic IR theory within the context of the surrounding literature, so in this section we cover those areas of the related work not already discussed.\n\n\n\nFor instance, the settings of the components in the \\emph{DIR} framework for multi page search cover a wide area of research in IR. Firstly, the examination hypothesis model used is just one of a number of probabilistic click models that could have been employed, including the click-chain model~\\cite{Guo:2009:CCM:1526709.1526712} and even a POMDP-based model~\\cite{Wang:2010:ISB:1718487.1718514}. Other path-based discount functions have been explored in the literature \\cite{White:2005:EIF:1080343.1080347} as well as other multi-stage utilities and metrics such as Time-Based Gain~\\cite{Smucker:2012:TCE:2348283.2348300}. Related work on using Markov chains to measure the utility of rankings at each time step is a potential method for evaluating \\emph{DIR} problems~\\cite{Ferrante:2014:IUM:2600428.2609637}. Further to this, the identification of the dynamic IR framework as a POMDP raises the possibility of using established techniques such as the Witness algorithm~\\cite{Littman:1994:WAS:864404} to find optimal action policies. Also, the performance of the \\emph{PRP} under results diversification is well-reported in the static frameworks quantum-PRP~\\cite{quantum-prp} and the portfolio theory of IR~\\cite{Wang09portfoliotheory}. \n\nThe concept of evaluating for retrieval utility rather than relevance was proposed by Cooper in 1973~\\cite{ASI:ASI4630240204} and is extended to all the frameworks discussed in this paper, where we aim to maximize some utility function that balances the costs and benefits of an IR system's actions. Other work in this area includes Azzopardi's~\\cite{Azzopardi:2014:MIE:2600428.2609574} work on economic models, which is itself an extension of the \\emph{IIR-PRP}, and also the work of Mostafa et al. \\cite{simulation-studies} who had a similar motivation to this work, where they defined a framework for running user simulations for information filtering, itself a \\emph{DIR} problem. \n\nOther than the \\emph{PRP} and \\emph{IIR-PRP}, the closest related works to this one are the following: The application of a POMDP to multi page search~\\cite{Jin:2013:IES:2488388.2488446}, from which many aspects of our experiments in this paper are derived, including the problem setting and the probability of relevance distribution. This paper extends their formulation to a general one applicable to other \\emph{DIR} problems and explores a different setting for the static utility, observation likelihood and discount, while also linking to static and interactive techniques. Our experimental time horizon setting of $T=2$ is based on the optimal results found in their work.\n\nFinally, the work on defining the elements of a POMDP in session search~\\cite{grace-ecir} is a close relation to this work, though focusing more on the testing of particular settings of \\emph{DIR} components in the session search scenario rather than explicitly gaining an understanding of the framework and components themselves. Nonetheless, their work contains an evaluation of algorithms that fall under the \\emph{DIR} framework including one similar to DIR-MPS. \n\n\nThis work differs from the literature in that: 1) ours is the first work to define the characteristics that distinguish dynamic IR from the other theoretical IR frameworks, 2) our utility is the generalization of many existing ranking utilities and incorporates many elements of IR research such as click models, and 3) we confirm the effectiveness of ours and the static frameworks in different scenarios in our experiments.\n\n\n\n\n\\section{Conclusion}\n\nIn this paper we have established a theoretical framework for \\emph{Dynamic Information Retrieval}. By contrasting with \\emph{static} and \\emph{interactive} frameworks, we found three characteristics that define dynamic IR systems; user feedback, temporal dependency and an overall goal. This motivated the derivation of our dynamic utility function ${U_D}$, which has its roots in the POMDP formulation. The components of this utility can be directly implemented using elements from existing research which we apply in the DIR-MPS algorithm, an example instantiation designed for the multi page search problem. Our experiments confirm that in this scenario, one of the effects of optimizing for ${U_D}$ is the diversification of search results. Otherwise, we also demonstrate that for other scenarios the \\emph{PRP} and \\emph{IIR-PRP} frameworks are still effective. \n\nLike the \\emph{PRP} and \\emph{IIR-PRP}, our framework defines certain functional parameters but does not definitively  specify how to set them. Instead, this work is a point of reference that can be used for the development of specialized models and algorithms applied to \\emph{DIR} problems. Through our application we were able to consider the limitations of the \\emph{DIR} framework, a result of which is the approximations used in the DIR-MPS algorithm. \n\nFinally, the derivation of a stationary solution to the \\emph{DIR} Markovian model is an intended future goal and would be an important result to come from this work.\n\n\n\n\n\n\n\\balance\n\\bibliographystyle{acm}\n\\begin{scriptsize}\n\\bibliography{refs}\n\\end{scriptsize}\n\n\n\n\n\\balancecolumns\n\n", "itemtype": "equation", "pos": 42633, "prevtext": "\nWe can then update the mean relevance scores and covariance matrix for non ranked documents using the formulae\n\n", "index": 27, "text": "\\begin{align}\n{\\vec{\\mathbf{r}}}_{\\backslash{\\vec{\\mathbf{a}}}_t} &= {\\vec{\\mathbf{r}}}_{\\backslash{\\vec{\\mathbf{a}}}_t} + \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t{\\vec{\\mathbf{a}}}_t}  \\Sigma_{ {\\vec{\\mathbf{a}}} {\\vec{\\mathbf{a}}}}^{-1}({\\vec{\\mathbf{o}}} -{\\vec{\\mathbf{r}}}_{{\\vec{\\mathbf{a}}}_t} ) \\label{eq:tau-func} \\\\\n\\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t\\backslash {\\vec{\\mathbf{a}}}_t} &= \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t\\backslash {\\vec{\\mathbf{a}}}_t} - \\Sigma_{\\backslash {\\vec{\\mathbf{a}}}_t{\\vec{\\mathbf{a}}}_t}  \\Sigma_{ {\\vec{\\mathbf{a}}}_t {\\vec{\\mathbf{a}}}_t}^{-1}\\Sigma_{ {\\vec{\\mathbf{a}}}_t\\backslash {\\vec{\\mathbf{a}}}_t} \\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\vec{\\mathbf{r}}}_{\\backslash{\\vec{\\mathbf{a}}}_{t}}\" display=\"inline\"><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi/><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\vec{\\mathbf{r}}}_{\\backslash{\\vec{\\mathbf{a}}}_{t}}+\\Sigma_{%&#10;\\backslash{\\vec{\\mathbf{a}}}_{t}{\\vec{\\mathbf{a}}}_{t}}\\Sigma_{{\\vec{\\mathbf{a%&#10;}}}{\\vec{\\mathbf{a}}}}^{-1}({\\vec{\\mathbf{o}}}-{\\vec{\\mathbf{r}}}_{{\\vec{%&#10;\\mathbf{a}}}_{t}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi/><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></msub><mo>+</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>\ud835\udc28</mi><mo stretchy=\"false\">\u2192</mo></mover><mo>-</mo><msub><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">\u2192</mo></mover><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Sigma_{\\backslash{\\vec{\\mathbf{a}}}_{t}\\backslash{\\vec{\\mathbf{a%&#10;}}}_{t}}\" display=\"inline\"><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\Sigma_{\\backslash{\\vec{\\mathbf{a}}}_{t}\\backslash{\\vec{\\mathbf{%&#10;a}}}_{t}}-\\Sigma_{\\backslash{\\vec{\\mathbf{a}}}_{t}{\\vec{\\mathbf{a}}}_{t}}%&#10;\\Sigma_{{\\vec{\\mathbf{a}}}_{t}{\\vec{\\mathbf{a}}}_{t}}^{-1}\\Sigma_{{\\vec{%&#10;\\mathbf{a}}}_{t}\\backslash{\\vec{\\mathbf{a}}}_{t}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub><mo>-</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi/><mo>\\</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></mrow></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub><mo>\\</mo><msub><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>t</mi></msub></mrow></msub></mrow></mrow></mrow></math>", "type": "latex"}]