[{"file": "1601.07228.tex", "nexttext": "\n\\end{definition}\n\n\n\n\nIt can be observed that each component $1\\leq i\\leq k$ of $\\bm{\\Sigma}$ is independent and identically distributed as follows.\n\\begin{numcases}{\\Pr(\\bm{\\Sigma}(i)=\\beta) =}\n1/8, & if $\\beta\\in \\{0,3\\}$,\\nonumber\\\\\n3/8, & if $\\beta\\in \\{1,2\\}$.\\nonumber\n\\end{numcases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this work we derive upper and lower bounds on $\\mathcal{C}$ for the network in Fig. \\ref{fig:fig1}.\n\n\n\\section{Upper bound on computing capacity}\n\\label{sec:upper_bd}\nBased on the relationships between the various quantities defined, we have the following.\n\\begin{IEEEeqnarray*}{rl}\n&{}H(\\bm{Z}_1^N,\\bm{Z}_2^N,N)=H(\\bm{Z}_1^N,\\bm{Z}_2^N|N)+H(N),\\\\\n{}&{}= H(N)+\\sum\\Pr(N=n)H(\\bm{Z}_1^N,\\bm{Z}_2^N|N=n),\\\\\n{}&{}\\leq H(N)+2\\log |\\mathcal{Z}|{\\rm E} N.\n\\end{IEEEeqnarray*} The last inequality above is due to the fact that $H(\\bm{Z}_1^N,\\bm{Z}_2^N|N=n)\\leq 2n\\log |\\mathcal{Z}|$ bits. Furthermore,\n\\begin{IEEEeqnarray*}{L}\nH(N)+2\\log|\\mathcal{Z}|{\\rm E} N \\\\\n\\geq H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma})+I(\\bm{Z}_1^N,\\bm{Z}_2^N,N;\\bm{\\Sigma}), \\\\\n= H(\\bm{\\Sigma})-H(\\bm{\\Sigma}|\\bm{Z}_1^N,\\bm{Z}_2^N,N)+H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma}),\\\\\n= H(\\bm{\\Sigma})+H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma}).\n\\end{IEEEeqnarray*} The last equality above is true by the zero-error criterion.\n\n\n\n\nNote that the probability mass function for $\\bm{\\Sigma}$ implies that $H(\\bm{\\Sigma})=1.8113k$ bits. Thus, we have that\n\\begin{IEEEeqnarray}{c}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma})\\leq H(N)+2\\log|\\mathcal{Z}|{\\rm E} N-1.8113k.\\IEEEeqnarraynumspace\\label{eq:put_lb_entropy}\n\\end{IEEEeqnarray}\nSection \\ref{sec:lb_entropy} derives a lower bound on the conditional entropy in the above inequality. Specifically, it is shown  that\n\\begin{IEEEeqnarray}{c}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})\\geq 0.75(-1+\\log 3)k \\;\\text{bits for large}\\; k. \\IEEEeqnarraynumspace\\label{eq:lb_entropy}\n\\end{IEEEeqnarray}\nUsing this in inequality \\eqref{eq:put_lb_entropy}, we obtain\n\\begin{IEEEeqnarray}{Rl}\n0.75(-1+\\log 3)k \\leq &H(N)+2\\log|\\mathcal{Z}|{\\rm E} N-1.8113k, \\IEEEnonumber \\\\\n\\implies \\frac{k}{{\\rm E} N \\log |\\mathcal{Z}|}\\leq &\\frac{H(N)}{{\\rm E} N \\log |\\mathcal{Z}|}+\\frac{2}{2.25}. \\label{eq:put_lb_ratio}\n\\end{IEEEeqnarray}\nFurthermore, the following claim holds (see Appendix \\ref{app:stopping_time_ratio} for a proof).\n\\begin{lemma}\nFor $k$ large enough, $H(N)/{\\rm E} N \\leq \\epsilon$, for any $\\epsilon> 0$.\n\\end{lemma}\\label{lemma:hn_by en}\nIndeed, for an arbitrary probability mass function on $\\mathbb{N}$ (set of natural numbers), this ratio can take the value $1$ when $N$ follows a geometric distribution with parameter $1/2$. However, the zero error criterion restricts the possible set of probability mass functions for the stopping time $N$ and yields the required upper bound for the ratio.\n\\begin{lemma} \\label{lemma:stopping_time}\nA valid stopping time $N$ for our network satisfies\n\n", "itemtype": "equation", "pos": 9355, "prevtext": "\n\n\n\n\n\n\n\n\\title{On Computation Rates for Arithmetic Sum}\n\n\n\n\n\n\\author{\\IEEEauthorblockN{Ardhendu Tripathy and Aditya Ramamoorthy}\n\\IEEEauthorblockA{Department of Electrical and Computer Engineering,\nIowa State University, Ames, Iowa 50011--3060\\\\\nEmail: ardhendu@iastate.edu, adityar@iastate.edu}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nFor zero-error function computation over directed acyclic networks, existing upper and lower bounds on the computation capacity are known to be loose. In this work we consider the problem of computing the arithmetic sum over a specific directed acyclic network that is not a tree. We assume the sources to be i.i.d. Bernoulli with parameter $1/2$. Even in this simple setting, we demonstrate that upper bounding the computation rate is quite nontrivial. In particular, it requires us to consider variable length network codes and relate the upper bound to equivalently lower bounding the entropy of descriptions observed by the terminal conditioned on the function value. This lower bound is obtained by further lower bounding the entropy of a so-called \\textit{clumpy distribution}. We also demonstrate an achievable scheme that uses variable length network codes and in-network compression.\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\section{Introduction}\n\n\n\n\nWe consider the problem of function computation \\cite{giridharK05, appuFKZ11, raiD12, ramamoorthyL13} using network coding \\cite{al,rm}. The setup of the problem typically is as follows. A directed acyclic graph (DAG) with capacity constraints is used to model a communication network. Certain nodes in the graph, referred to as terminals are interested in computing a target function of the data observed at some nodes (called sources) in the graph. The edges model error-free communication links and the nodes are assumed to be able to perform network coding. The objective is to find the maximum rate at which a network code can enable the terminals to compute estimates of the target function within a specified level of distortion.\n\n\n\n\n\nThe problem in its most general setting is known to be hard, and that has prompted the study of certain special cases \\cite{orlitskyR01, appuFKZ11, tripathyR14, tripathyR15}. The specific case when the network has one terminal and the function needs to be computed without any distortion has received significant attention. Under this setup, one can consider either the zero-error setting or a setting where $\\epsilon$-error (for arbitrary $\\epsilon > 0$) error is allowed. In \\cite{appuFKZ11}, the authors do not assume a joint probability distribution on the input data and focus on zero-error function computation. After describing the amount of information that needs to be transmitted for this case, the same concept is applied to cuts that separate one or more sources from the terminal in a function computation problem over a DAG. Using this approach they were able to characterize the maximum achievable computation rate and network codes that obtain it for multi-edge tree networks. The work done in \\cite{kowshikK12} approaches the function computation problem in a slightly different manner. Rather than focus on a computation rate for the entire network, they look at the rate region obtained by the vector of achievable rates over each edge in the network. They consider two scenarios: \\textit{worst case} and \\textit{average case} complexity. The worst case complexity is related to the setting in \\cite{appuFKZ11}, while the average case complexity assumes a joint probability distribution on the input data. For both scenarios, they use cut-set based arguments to characterize the rate region for tree networks. For general DAGs, cut-set based upper bounds are shown in \\cite{huangTY15} to be loose. \n\n\nIn this work we examine zero-error arithmetic sum computation over a specific DAG, but we assume a probability distribution on the input data (see Fig. \\ref{fig:fig1}). Note that for such a network there are two distinct paths from the source $s_3$ to the terminal that allows for multitude of network coding options. In \\cite{appuFKZ11}, the same network was considered in a zero-error setting, but they did not assume any distribution on the inputs. They demonstrated an upper bound on the computation rate and a matching achievable scheme. In general, the distribution of the inputs is important in defining an associated rate of computation. Indeed, if the source values are deterministic, then the actual computation does not require any information to be transmitted on the edges.\n\nIn this work, we assume that each of the source is distributed i.i.d. Bernoulli with parameter $1/2$. We demonstrate that in this setting, upper bounding the computation rate is significantly harder because of the possibility of compressing the intermediate transmissions. In addition, the presence of multiple paths for source $s_3$ allows for many ways in which the information transfer and compression can be performed. Our upper bounds on the computation rate stem from the study of the entropy of the distribution of the descriptions transmitted by nodes $s_1$ and $s_2$ conditioned on the value of the arithmetic sum. Indeed, note that the arithmetic sum of two i.i.d. Bernoulli random variables has a biased probability mass function and one can lower transmission rates by appropriately compressing these values. For zero-error compression in the single source setting, it is well recognized that variable length codes are needed. Accordingly, we consider the class of variable length network codes that allow for computation of the arithmetic sum. \n\n\\subsection{Main contribution}\n\\begin{itemize}\n\\item We consider a variable-length network code for arithmetic sum computation in the particular DAG shown in Figure \\ref{fig:fig1}. In this variable length setting, we present an upper bound and a lower bound for the computation rate. The upper bound arises from studying the entropy of the descriptions communicated by $s_1$ and $s_2$ to $t$ conditioned on the value of the sum. We show that this conditional entropy can be lower bounded by an appropriately characterized ``clumpy\" distribution. The lower bound uses variable length codes for compression.\n\n\\end{itemize}\nThis paper is organized as follows. Section \\ref{sec:setup} presents the problem formulation. Section \\ref{sec:upper_bd} uses a lower bound on the conditional entropy of the descriptions transmitted that is derived in Section \\ref{sec:lb_entropy} to give an upper bound on the computation rate. Section \\ref{sec:lower_bd} discusses an achievable scheme and Section \\ref{sec:concl} concludes the paper. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Problem formulation}\\label{sec:setup}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=2in]{fig}\n\n\n\n\\caption{A directed acyclic network with three sources and one terminal. \\label{fig:fig1}}\n\\end{figure}\n\nThe edges in Figure \\ref{fig:fig1} (later denoted by an ordered pair of vertices) have unit-capacity. Suppose that $\\mathcal{Z}$ is the alphabet used for communication, and $\\mathcal{Z}>1$. $s_1, s_2, s_3$ are the three source nodes that observe independent uniform iid sources $X_1,X_2,X_3$ respectively, each from $\\{0,1\\}$. Terminal node $ t$ wants to compute the arithmetic sum $\\Sigma=X_1+X_2+X_3, \\Sigma \\in \\{0,1,2,3\\}$. WLOG we assume that edges $(s_3,s_1), (s_3,s_2)$ forward the value of $X_3$ to $s_1,s_2$ respectively. We adapt a variable-length network code to this function computation problem. In what follows, all logarithms denoted as $\\log$ are to the base $2$ unless specified otherwise. \n\n\n\n\n\n\n\\begin{definition}\nLet $\\mathcal{Z}^\\ast$ denote the set of all finite-length sequences with alphabet $\\mathcal{Z}$. A variable-length $(k,N)$ network code for the network in Figure \\ref{fig:fig1} has the following components.\n\\begin{enumerate}\n\\item Encoding functions for the edges, $e \\in \\{(s_1,t),(s_2,t)\\}$:\n\\begin{IEEEeqnarray*}{c}\n\\phi_e:\\{0,1\\}^k\\times \\{0,1\\}^k \\rightarrow \\mathcal{Z}^\\ast\n\\end{IEEEeqnarray*}\nLet $\\bm{Z}_1:=\\phi_{(s_1,t)}(\\bm{X}_1,\\bm{X}_3), \\bm{Z}_2:=\\phi_{(s_2,t)}(\\bm{X}_2,\\bm{X}_3)$ where $\\bm{X}_j$ denotes a $k$-length random vector. $\\bm{X}_j(i)$ refers to the $i$-th component of $\\bm{X}_j$. We also let $\\bm{X}_j^n$ represent the vector $(\\bm{X}_j(1), \\dots, \\bm{X}_j(n))$.\n\\item Decoding function for the terminal $t$:\n\\begin{IEEEeqnarray*}{c}\n\\psi_t:\\mathcal{Z}^N\\times\\mathcal{Z}^N \\rightarrow \\{0,1,2,3\\}^k\n\\end{IEEEeqnarray*}\nwhere random variable $N$ is a \\textit{stopping time} with respect to the sequence $(\\bm{Z}_1(1),\\bm{Z}_2(1)), (\\bm{Z}_1(2),\\bm{Z}_2(2)), \\dots$. Thus, the indicator function $\\bm{1}_{\\{N=n\\}}$ is a function of $((\\bm{Z}_1(1),\\bm{Z}_2(1),\\hdots,(\\bm{Z}_1(n),\\bm{Z}_2(n))$.\n\\end{enumerate}\n\\end{definition}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminal $t$ estimates the $k$-length component-wise arithmetic sum (denoted as $\\bm{\\hat{\\Sigma}}$) by setting $\\bm{\\hat{\\Sigma}}:=\\psi_t(\\bm{Z}_1^N,\\bm{Z}_2^N)$.\n\n\n\n\\begin{definition}\nWe say that a $(k,N)$ network code \\textit{recovers} $\\bm{\\Sigma}$ \\textit{with zero error} if $\\Pr(\\bm{\\hat{\\Sigma}}\\neq\\bm{\\Sigma})=0,\\;\\text{for all}\\;\\bm{\\Sigma} \\in \\{0,1,2,3\\}^k$. The rate of such a network code is defined as $\\frac{k}{{\\rm E} N \\log |\\mathcal Z|}$, where ${\\rm E} N$ denotes the expected value of $N$. The capacity is\n\n", "index": 1, "text": "\\begin{equation*}\n\\mathcal{C}:=\\sup \\left\\lbrace\\frac{k}{{\\rm E} N\\log |\\mathcal{Z}|}:~\\begin{IEEEeqnarraybox*}[][c]{,t,} there is a zero-error $(k,N)$ \\\\network code that recovers $\\Sigma$.\n\\end{IEEEeqnarraybox*} \\right\\rbrace\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{C}:=\\sup\\left\\{\\frac{k}{{\\rm E}N\\log|\\mathcal{Z}|}:~{}%&#10;\\IEEEeqnarraybox*[][c]{,t,}thereisazero-error(k,N)\\\\&#10;networkcodethatrecovers\\Sigma.\\right\\}\" display=\"block\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>:=</mo><mo movablelimits=\"false\">sup</mo><mrow><mo>{</mo><mfrac><mi>k</mi><mrow><mi mathvariant=\"normal\">E</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mfrac><mo rspace=\"5.8pt\">:</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>{IEEEeqnarraybox*}</mtext></merror><mrow><mo stretchy=\"false\">[</mo><mo stretchy=\"false\">]</mo></mrow><mrow><mo stretchy=\"false\">[</mo><mi>c</mi><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mi>t</mi><mo>,</mo><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>i</mi><mi>s</mi><mi>a</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>o</mi><mo>-</mo><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><mi>n</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi mathvariant=\"normal\">\u03a3</mi><mo>.</mo><mo>}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\n\\end{lemma}\n\\begin{IEEEproof} For a given $\\bm{\\sigma} \\in \\{0,1,2,3\\}^k$ consider the set $S$ of values such that $\\Pr(N = n|\\bm{\\Sigma}=\\bm{\\sigma})>0$. By definition of stopping time, terminal $t$ can recover at most $|\\mathcal{Z}|^{2n}$ different values of $\\bm{\\hat{\\Sigma}}$, which by the zero error criterion, is the same as the value of $\\bm{\\Sigma}$. Thus, if $|S|>|\\mathcal{Z}|^{2n}$, there is a positive probability of error. Hence, we have,\n\\begin{IEEEeqnarray*}{Rl+x*}\n\\Pr(N=n)\\leq &\\sum_{\\bm{\\sigma}\\in S} \\Pr(N= n|\\bm{\\Sigma}=\\bm{\\sigma})\\Pr(\\bm{\\Sigma}=\\bm{\\sigma}),\\\\\n\\leq &|\\mathcal{Z}|^{2n}\\max_{\\sigma}\\Pr(\\bm{\\Sigma}=\\bm{\\sigma}),\\\\\n= &\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2n}. \n\\end{IEEEeqnarray*}\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lower bound on conditional entropy}\\label{sec:lb_entropy}\nIn this section we derive the lower bound on $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})$ as stated in inequality \\eqref{eq:lb_entropy}. To do this, we first note that the zero error criterion enforces a requirement that the stopped sequences $\\bm{Z}_1^N, \\bm{Z}_2^N$ must satisfy.\n\\begin{lemma}\\label{lemma:labels}\nFor a valid $(k,N)$ network code, let $\\bm{z}_1^{n_1}:=\\phi_{(s_1,t)}(\\bm{x}_1, \\bm{x}_3)$ and $\\bm{z}_1^{n_1'}:=\\phi_{(s_1,t)}(\\bm{x}_1',\\bm{x}_3)$. Similarly define $\\bm{z}_2^{n_2}$ and $\\bm{z}_2^{n_2'}$. Then,\n\\begin{itemize}\n\\item $\\bm{z}_1^{n_1} \\neq \\bm{z}_1^{n_1'}$ for all $\\bm{x}_1 \\neq \\bm{x}_1'$ with $\\bm{x}_1, \\bm{x}_1' \\in \\{0,1\\}^k$ and\n\\item $\\bm{z}_2^{n_2} \\neq \\bm{z}_2^{n_2'}$ for all $\\bm{x}_2 \\neq \\bm{x}_2'$ with $\\bm{x}_2, \\bm{x}_2' \\in \\{0,1\\}^k$.\n\\end{itemize}\n\\end{lemma}\n\\begin{IEEEproof}\nAssume otherwise and consider the two sets of inputs $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$ and $(\\bm{x}_1',\\bm{x}_2,\\bm{x}_3)$ such that $\\bm{x}_1'+\\bm{x}_2+\\bm{x}_3 \\neq \\bm{x}_1+\\bm{x}_2+\\bm{x}_3$. One can easily see that such a set of inputs exist. Then if $\\bm{z}_1^{n_1}=\\bm{z}_1^{n_1'}$, the terminal $t$ is unable to compute the arithmetic sum correctly from the corresponding stopped sequences, leading to a non zero probability of error.\n\\end{IEEEproof}\n\n\nSet $L_{x,y}:=2^{x+y}$ and $M_{x,y}:=3^{x+y}$. For a natural number $u$, let $[u]:=\\{1,2,\\hdots,u\\}$. For a vector $\\bm{v}$, index its components with a natural number $i$ and let $\\bm{v}(i)$ denote its $i$-th component.\n\\begin{claim}\nLet a particular realization $\\bm{\\sigma}$ of $\\bm{\\Sigma}$ be such that $x$ components of it equal $1$ and $y$ components of it equal $2$. For a valid $(k,N)$ network code, the conditional entropy $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ is minimized when the probability mass function $\\Pr(\\bm{Z}_1^N=\\bm{z}_1^N,\\bm{Z}_2^N=\\bm{z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ is positive for exactly $L_{x,y}$ distinct $(\\bm{z}_1^N,\\bm{z}_2^N)$ pairs.\n\\end{claim}\n\\begin{IEEEproof}\nFor a $\\bm{\\sigma}$ with $x$ 1's and $y$ 2's in it, there are $M_{x,y} ~(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$-tuples that result in that particular sum. Within these $M_{x,y}$ input tuples, there are $L_{x,y}$ different values of $\\bm{x}_3$. We can partition all input tuples into disjoint sets that have the arithmetic sum $\\bm{\\sigma}$ based on the value of $\\bm{x}_3$. The set with the most number of input tuples in it corresponds to a particular value which we denote $\\bm{\\tilde{x}}_3$. One can check that, for $i=\\{1,2,\\hdots,k\\}$\n\n", "itemtype": "equation", "pos": 12495, "prevtext": "\n\\end{definition}\n\n\n\n\nIt can be observed that each component $1\\leq i\\leq k$ of $\\bm{\\Sigma}$ is independent and identically distributed as follows.\n\\begin{numcases}{\\Pr(\\bm{\\Sigma}(i)=\\beta) =}\n1/8, & if $\\beta\\in \\{0,3\\}$,\\nonumber\\\\\n3/8, & if $\\beta\\in \\{1,2\\}$.\\nonumber\n\\end{numcases}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this work we derive upper and lower bounds on $\\mathcal{C}$ for the network in Fig. \\ref{fig:fig1}.\n\n\n\\section{Upper bound on computing capacity}\n\\label{sec:upper_bd}\nBased on the relationships between the various quantities defined, we have the following.\n\\begin{IEEEeqnarray*}{rl}\n&{}H(\\bm{Z}_1^N,\\bm{Z}_2^N,N)=H(\\bm{Z}_1^N,\\bm{Z}_2^N|N)+H(N),\\\\\n{}&{}= H(N)+\\sum\\Pr(N=n)H(\\bm{Z}_1^N,\\bm{Z}_2^N|N=n),\\\\\n{}&{}\\leq H(N)+2\\log |\\mathcal{Z}|{\\rm E} N.\n\\end{IEEEeqnarray*} The last inequality above is due to the fact that $H(\\bm{Z}_1^N,\\bm{Z}_2^N|N=n)\\leq 2n\\log |\\mathcal{Z}|$ bits. Furthermore,\n\\begin{IEEEeqnarray*}{L}\nH(N)+2\\log|\\mathcal{Z}|{\\rm E} N \\\\\n\\geq H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma})+I(\\bm{Z}_1^N,\\bm{Z}_2^N,N;\\bm{\\Sigma}), \\\\\n= H(\\bm{\\Sigma})-H(\\bm{\\Sigma}|\\bm{Z}_1^N,\\bm{Z}_2^N,N)+H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma}),\\\\\n= H(\\bm{\\Sigma})+H(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma}).\n\\end{IEEEeqnarray*} The last equality above is true by the zero-error criterion.\n\n\n\n\nNote that the probability mass function for $\\bm{\\Sigma}$ implies that $H(\\bm{\\Sigma})=1.8113k$ bits. Thus, we have that\n\\begin{IEEEeqnarray}{c}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N,N|\\bm{\\Sigma})\\leq H(N)+2\\log|\\mathcal{Z}|{\\rm E} N-1.8113k.\\IEEEeqnarraynumspace\\label{eq:put_lb_entropy}\n\\end{IEEEeqnarray}\nSection \\ref{sec:lb_entropy} derives a lower bound on the conditional entropy in the above inequality. Specifically, it is shown  that\n\\begin{IEEEeqnarray}{c}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})\\geq 0.75(-1+\\log 3)k \\;\\text{bits for large}\\; k. \\IEEEeqnarraynumspace\\label{eq:lb_entropy}\n\\end{IEEEeqnarray}\nUsing this in inequality \\eqref{eq:put_lb_entropy}, we obtain\n\\begin{IEEEeqnarray}{Rl}\n0.75(-1+\\log 3)k \\leq &H(N)+2\\log|\\mathcal{Z}|{\\rm E} N-1.8113k, \\IEEEnonumber \\\\\n\\implies \\frac{k}{{\\rm E} N \\log |\\mathcal{Z}|}\\leq &\\frac{H(N)}{{\\rm E} N \\log |\\mathcal{Z}|}+\\frac{2}{2.25}. \\label{eq:put_lb_ratio}\n\\end{IEEEeqnarray}\nFurthermore, the following claim holds (see Appendix \\ref{app:stopping_time_ratio} for a proof).\n\\begin{lemma}\nFor $k$ large enough, $H(N)/{\\rm E} N \\leq \\epsilon$, for any $\\epsilon> 0$.\n\\end{lemma}\\label{lemma:hn_by en}\nIndeed, for an arbitrary probability mass function on $\\mathbb{N}$ (set of natural numbers), this ratio can take the value $1$ when $N$ follows a geometric distribution with parameter $1/2$. However, the zero error criterion restricts the possible set of probability mass functions for the stopping time $N$ and yields the required upper bound for the ratio.\n\\begin{lemma} \\label{lemma:stopping_time}\nA valid stopping time $N$ for our network satisfies\n\n", "index": 3, "text": "\\begin{equation*}\n\\Pr(N=n)\\leq \\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2n} \\;\\text{for any}\\; n\\in \\mathbb{N}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\Pr(N=n)\\leq\\left(\\frac{3}{8}\\right)^{k}|\\mathcal{Z}|^{2n}\\;\\text{for any}\\;n%&#10;\\in\\mathbb{N}.\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>=</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><msup><mrow><mo>(</mo><mfrac><mn>3</mn><mn>8</mn></mfrac><mo>)</mo></mrow><mi>k</mi></msup><mo>\u2062</mo><mpadded width=\"+2.8pt\"><msup><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">|</mo></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>n</mi></mrow></msup></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>for any</mtext></mpadded><mo>\u2062</mo><mi>n</mi></mrow><mo>\u2208</mo><mi>\u2115</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nFrom Lemma \\ref{lemma:labels}, we know that all input tuples for a fixed $\\bm{x}_3$  must receive distinct $(\\bm{Z}_1^N,\\bm{Z}_2^N)$ labels. Thus for any $\\bm{\\sigma}$, we must have atleast $L_{x,y}$ distinct $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels as the size of the largest partition (which is the $\\bm{\\tilde{x}}_3$-partition) is $L_{x,y}$. These instantiations of the pair process $(\\bm{Z}_1^N,\\bm{Z}_2^N)$ must therefore have a positive conditional probability. Note that all the input tuples that result in a particular $\\bm{\\sigma}$ are equally likely. Hence, the conditional probability of any one particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label for a given $\\bm{\\sigma}$ has to be a multiple of $1/M_{x,y}$.\n\nIn Appendix \\ref{app:support_arg} we prove the following claim from which the result follows. Let $c > 0$ and $u^*$ be a positive integer such that $c u^* \\leq 1$. For a natural number $u \\leq u^*$, let $\\mathcal{Q}_{u}$ be the set of probability mass functions supported on $[u]$ such that for all vectors $\\bm{q} \\in \\mathcal{Q}_u$, we have that $\\bm{q}(i)\\geq c>0, ~\\forall i \\in [u]$.\n\\begin{claim}\\label{claim:entropy_support_set}\nFor some $m \\leq u^*-1$, let\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{q}_{m}:=\\arg\\min_{\\bm{q} \\in \\mathcal{Q}_m}H(\\bm{q}), \\;\\text{and}\\; \\bm{q}_{m+1}:=\\arg\\min_{\\bm{q} \\in\\mathcal{Q}_{m+1}}H(\\bm{q}).\n\\end{IEEEeqnarray*}\nThen, we have $H(\\bm{q}_m) \\leq H(\\bm{q}_{m+1})$.\n\\end{claim}\nThis follows from the fact that entropy is a concave function and attains its minimum at an extremal point of the underlying polyhedron. Thus, using any more than $L_{x,y}$ distinct labels will increase the conditional entropy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now explicitly derive the conditional entropy-minimizing distribution over $L_{x,y} ~(\\bm{z}_1^N,\\bm{z}_2^N)$ labels. Index all the $L_{x,y}$ different values of $\\bm{x}_3$ that result in a particular arithmetic sum $\\bm{\\sigma}$ by a natural number $i$ and denote them as $\\bm{x}_3^i, i \\in \\{1,2,\\hdots,L_{x,y}\\}$. Recall that the input tuples with sum $\\bm{\\sigma}$ can be partitioned into disjoint sets based on the value of $\\bm{x}_3^i$. We call the set corresponding to $\\bm{x}_3^i$, the $\\bm{x}_3^i$-partition. Let $n_i$ be the size of $\\bm{x}_3^i$-partition.\n\\begin{claim}\\label{claim:family}\nConditioned on the particular value of $\\bm{\\sigma}$, the set of all probability mass functions on $L_{x,y}$ valid $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels can be represented by a family $\\mathcal{P}$ of vectors over the reals.\n\\begin{IEEEeqnarray}{l}\n\\setlength{\\nulldelimiterspace}{0pt}\n\\mathcal{P}=\\left\\{\\begin{IEEEeqnarraybox}[\\relax][c]{l}\n\\bm{p} \\in \\mathbb{R}^{L_{x,y} \\times 1}: \\bm{p}=\\frac{1}{M_{x,y}}\\sum_{i=1}^{L_{x,y}}\\bm{e}_i, \\bm{e}_i \\in \\mathbb{R}^{L_{x,y} \\times 1}\\\\\n\\text{such that}~ \\bm{e}_i ~\\text{has}~ n_i ~\\text{1's and rest as 0 for every}~ i.\n\\end{IEEEeqnarraybox}\\right\\}\\label{eq:family}\\IEEEeqnarraynumspace\n\\end{IEEEeqnarray}\n\\end{claim}\n\\begin{IEEEproof}\nEach $\\bm{x}_3^i$-partition of the $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$ input tuples that have the arithmetic sum $\\bm{\\sigma}$ has $n_i$ elements in it.\n\nEach equally-likely input tuple for this $\\bm{\\sigma}$ is assigned a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label by the encoding functions. Thus, the conditional probability distribution $\\Pr(\\bm{Z}_1^N=\\bm{z}_1^N,\\bm{Z}_2^N=\\bm{z}_2^N|\\Sigma=\\sigma)$ is determined by how frequently the $(\\bm{z}_1^N,\\bm{z}_2^N)$ label is reassigned to different input tuples that have the arithmetic sum as $\\sigma$.\n\n\nDefine a $L_{x,y}$-length vector $\\bm{e}_i$ for each $\\bm{x}_3^i$-partition. The components of the vector $\\bm{e}_i$ denote whether a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label is assigned to an input tuple in the $\\bm{x}_3^i$-partition or not. Note that by Lemma \\ref{lemma:labels}, a $(\\bm{z}_1^N,\\bm{z}_2^N)$ label can be assigned to atmost one input tuple in a particular $\\bm{x}_3^i$-partition. Thus $\\bm{e}_i$ has $n_i$ components as $1$ and the rest as $0$. Then the frequency of occurrence of a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label among all the input tuples that result in the arithmetic sum $\\bm{\\sigma}$ can be found by considering the component-wise sum $\\sum_{i=1}^{L_{x,y}}\\bm{e}_i$. Normalizing by the total number of input tuples gives the claim.\n\n\n\n\\end{IEEEproof}\n\n\n\n\n\n\\begin{theorem}\nLet $\\bm{1}_u$ and $\\bm{0}_v$ denote the all-ones vector with $u$ components and the all-zeros vector with $v$ components respectively. Let $L_{x,y}^i:=L_{x,y}-n_i$ for all $i \\in [L_{x,y}]$. Then\n\n", "itemtype": "equation", "pos": 15985, "prevtext": "\n\\end{lemma}\n\\begin{IEEEproof} For a given $\\bm{\\sigma} \\in \\{0,1,2,3\\}^k$ consider the set $S$ of values such that $\\Pr(N = n|\\bm{\\Sigma}=\\bm{\\sigma})>0$. By definition of stopping time, terminal $t$ can recover at most $|\\mathcal{Z}|^{2n}$ different values of $\\bm{\\hat{\\Sigma}}$, which by the zero error criterion, is the same as the value of $\\bm{\\Sigma}$. Thus, if $|S|>|\\mathcal{Z}|^{2n}$, there is a positive probability of error. Hence, we have,\n\\begin{IEEEeqnarray*}{Rl+x*}\n\\Pr(N=n)\\leq &\\sum_{\\bm{\\sigma}\\in S} \\Pr(N= n|\\bm{\\Sigma}=\\bm{\\sigma})\\Pr(\\bm{\\Sigma}=\\bm{\\sigma}),\\\\\n\\leq &|\\mathcal{Z}|^{2n}\\max_{\\sigma}\\Pr(\\bm{\\Sigma}=\\bm{\\sigma}),\\\\\n= &\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2n}. \n\\end{IEEEeqnarray*}\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lower bound on conditional entropy}\\label{sec:lb_entropy}\nIn this section we derive the lower bound on $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})$ as stated in inequality \\eqref{eq:lb_entropy}. To do this, we first note that the zero error criterion enforces a requirement that the stopped sequences $\\bm{Z}_1^N, \\bm{Z}_2^N$ must satisfy.\n\\begin{lemma}\\label{lemma:labels}\nFor a valid $(k,N)$ network code, let $\\bm{z}_1^{n_1}:=\\phi_{(s_1,t)}(\\bm{x}_1, \\bm{x}_3)$ and $\\bm{z}_1^{n_1'}:=\\phi_{(s_1,t)}(\\bm{x}_1',\\bm{x}_3)$. Similarly define $\\bm{z}_2^{n_2}$ and $\\bm{z}_2^{n_2'}$. Then,\n\\begin{itemize}\n\\item $\\bm{z}_1^{n_1} \\neq \\bm{z}_1^{n_1'}$ for all $\\bm{x}_1 \\neq \\bm{x}_1'$ with $\\bm{x}_1, \\bm{x}_1' \\in \\{0,1\\}^k$ and\n\\item $\\bm{z}_2^{n_2} \\neq \\bm{z}_2^{n_2'}$ for all $\\bm{x}_2 \\neq \\bm{x}_2'$ with $\\bm{x}_2, \\bm{x}_2' \\in \\{0,1\\}^k$.\n\\end{itemize}\n\\end{lemma}\n\\begin{IEEEproof}\nAssume otherwise and consider the two sets of inputs $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$ and $(\\bm{x}_1',\\bm{x}_2,\\bm{x}_3)$ such that $\\bm{x}_1'+\\bm{x}_2+\\bm{x}_3 \\neq \\bm{x}_1+\\bm{x}_2+\\bm{x}_3$. One can easily see that such a set of inputs exist. Then if $\\bm{z}_1^{n_1}=\\bm{z}_1^{n_1'}$, the terminal $t$ is unable to compute the arithmetic sum correctly from the corresponding stopped sequences, leading to a non zero probability of error.\n\\end{IEEEproof}\n\n\nSet $L_{x,y}:=2^{x+y}$ and $M_{x,y}:=3^{x+y}$. For a natural number $u$, let $[u]:=\\{1,2,\\hdots,u\\}$. For a vector $\\bm{v}$, index its components with a natural number $i$ and let $\\bm{v}(i)$ denote its $i$-th component.\n\\begin{claim}\nLet a particular realization $\\bm{\\sigma}$ of $\\bm{\\Sigma}$ be such that $x$ components of it equal $1$ and $y$ components of it equal $2$. For a valid $(k,N)$ network code, the conditional entropy $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ is minimized when the probability mass function $\\Pr(\\bm{Z}_1^N=\\bm{z}_1^N,\\bm{Z}_2^N=\\bm{z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ is positive for exactly $L_{x,y}$ distinct $(\\bm{z}_1^N,\\bm{z}_2^N)$ pairs.\n\\end{claim}\n\\begin{IEEEproof}\nFor a $\\bm{\\sigma}$ with $x$ 1's and $y$ 2's in it, there are $M_{x,y} ~(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$-tuples that result in that particular sum. Within these $M_{x,y}$ input tuples, there are $L_{x,y}$ different values of $\\bm{x}_3$. We can partition all input tuples into disjoint sets that have the arithmetic sum $\\bm{\\sigma}$ based on the value of $\\bm{x}_3$. The set with the most number of input tuples in it corresponds to a particular value which we denote $\\bm{\\tilde{x}}_3$. One can check that, for $i=\\{1,2,\\hdots,k\\}$\n\n", "index": 5, "text": "\\begin{equation*}\n\\setlength{\\nulldelimiterspace}{0pt}\n\\bm{\\tilde{x}}_3(i)=\\left\\{\\begin{IEEEeqnarraybox}[\\relax][c]{l's}\n0,&if $\\sigma(i)=0,1$\\\\\n1,&if $\\sigma(i)=2,3.$\n\\end{IEEEeqnarraybox}\\right.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\bm{\\tilde{x}}_{3}(i)=\\left\\{\\IEEEeqnarraybox[][c]{l^{\\prime}s}0,&amp;if\\sigma(i)=%&#10;0,1\\\\&#10;1,&amp;if\\sigma(i)=2,3.\\right.\" display=\"block\"><mrow><msub><mover accent=\"true\"><mi>\ud835\udc99</mi><mo mathvariant=\"bold\" stretchy=\"false\">~</mo></mover><mn>3</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>{IEEEeqnarraybox}</mtext></merror><mrow><mo stretchy=\"false\">[</mo><mo stretchy=\"false\">]</mo></mrow><mrow><mo stretchy=\"false\">[</mo><mi>c</mi><mo stretchy=\"false\">]</mo></mrow><msup><mi>l</mi><mo>\u2032</mo></msup><mi>s</mi><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&amp;</mi><mi>i</mi><mi>f</mi><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&amp;</mi><mi>i</mi><mi>f</mi><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>.</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis a probability mass function in $\\mathcal{P}$ that minimizes the entropy on $L_{x,y}$ $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels. Moreover, any other entropy-minimizing distribution is only a permutation of $\\bm{p}^\\star$.\n\n\\end{theorem}\n\\begin{IEEEproof}\nIt will be shown in Claim \\ref{claim:extremal_point} that $\\bm{p}^\\star$ is an an extremal point of the set conv$(\\mathcal{P})$, which is the convex hull of the set $\\mathcal{P}$. Thus, it is a potential minimizer of the concave entropy function over the convex set conv$(\\mathcal{P})$. Claim \\ref{claim:convex_comb} shows that there are no other candidate minimizers, and hence\n\n", "itemtype": "equation", "pos": 20822, "prevtext": "\nFrom Lemma \\ref{lemma:labels}, we know that all input tuples for a fixed $\\bm{x}_3$  must receive distinct $(\\bm{Z}_1^N,\\bm{Z}_2^N)$ labels. Thus for any $\\bm{\\sigma}$, we must have atleast $L_{x,y}$ distinct $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels as the size of the largest partition (which is the $\\bm{\\tilde{x}}_3$-partition) is $L_{x,y}$. These instantiations of the pair process $(\\bm{Z}_1^N,\\bm{Z}_2^N)$ must therefore have a positive conditional probability. Note that all the input tuples that result in a particular $\\bm{\\sigma}$ are equally likely. Hence, the conditional probability of any one particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label for a given $\\bm{\\sigma}$ has to be a multiple of $1/M_{x,y}$.\n\nIn Appendix \\ref{app:support_arg} we prove the following claim from which the result follows. Let $c > 0$ and $u^*$ be a positive integer such that $c u^* \\leq 1$. For a natural number $u \\leq u^*$, let $\\mathcal{Q}_{u}$ be the set of probability mass functions supported on $[u]$ such that for all vectors $\\bm{q} \\in \\mathcal{Q}_u$, we have that $\\bm{q}(i)\\geq c>0, ~\\forall i \\in [u]$.\n\\begin{claim}\\label{claim:entropy_support_set}\nFor some $m \\leq u^*-1$, let\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{q}_{m}:=\\arg\\min_{\\bm{q} \\in \\mathcal{Q}_m}H(\\bm{q}), \\;\\text{and}\\; \\bm{q}_{m+1}:=\\arg\\min_{\\bm{q} \\in\\mathcal{Q}_{m+1}}H(\\bm{q}).\n\\end{IEEEeqnarray*}\nThen, we have $H(\\bm{q}_m) \\leq H(\\bm{q}_{m+1})$.\n\\end{claim}\nThis follows from the fact that entropy is a concave function and attains its minimum at an extremal point of the underlying polyhedron. Thus, using any more than $L_{x,y}$ distinct labels will increase the conditional entropy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now explicitly derive the conditional entropy-minimizing distribution over $L_{x,y} ~(\\bm{z}_1^N,\\bm{z}_2^N)$ labels. Index all the $L_{x,y}$ different values of $\\bm{x}_3$ that result in a particular arithmetic sum $\\bm{\\sigma}$ by a natural number $i$ and denote them as $\\bm{x}_3^i, i \\in \\{1,2,\\hdots,L_{x,y}\\}$. Recall that the input tuples with sum $\\bm{\\sigma}$ can be partitioned into disjoint sets based on the value of $\\bm{x}_3^i$. We call the set corresponding to $\\bm{x}_3^i$, the $\\bm{x}_3^i$-partition. Let $n_i$ be the size of $\\bm{x}_3^i$-partition.\n\\begin{claim}\\label{claim:family}\nConditioned on the particular value of $\\bm{\\sigma}$, the set of all probability mass functions on $L_{x,y}$ valid $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels can be represented by a family $\\mathcal{P}$ of vectors over the reals.\n\\begin{IEEEeqnarray}{l}\n\\setlength{\\nulldelimiterspace}{0pt}\n\\mathcal{P}=\\left\\{\\begin{IEEEeqnarraybox}[\\relax][c]{l}\n\\bm{p} \\in \\mathbb{R}^{L_{x,y} \\times 1}: \\bm{p}=\\frac{1}{M_{x,y}}\\sum_{i=1}^{L_{x,y}}\\bm{e}_i, \\bm{e}_i \\in \\mathbb{R}^{L_{x,y} \\times 1}\\\\\n\\text{such that}~ \\bm{e}_i ~\\text{has}~ n_i ~\\text{1's and rest as 0 for every}~ i.\n\\end{IEEEeqnarraybox}\\right\\}\\label{eq:family}\\IEEEeqnarraynumspace\n\\end{IEEEeqnarray}\n\\end{claim}\n\\begin{IEEEproof}\nEach $\\bm{x}_3^i$-partition of the $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3)$ input tuples that have the arithmetic sum $\\bm{\\sigma}$ has $n_i$ elements in it.\n\nEach equally-likely input tuple for this $\\bm{\\sigma}$ is assigned a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label by the encoding functions. Thus, the conditional probability distribution $\\Pr(\\bm{Z}_1^N=\\bm{z}_1^N,\\bm{Z}_2^N=\\bm{z}_2^N|\\Sigma=\\sigma)$ is determined by how frequently the $(\\bm{z}_1^N,\\bm{z}_2^N)$ label is reassigned to different input tuples that have the arithmetic sum as $\\sigma$.\n\n\nDefine a $L_{x,y}$-length vector $\\bm{e}_i$ for each $\\bm{x}_3^i$-partition. The components of the vector $\\bm{e}_i$ denote whether a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label is assigned to an input tuple in the $\\bm{x}_3^i$-partition or not. Note that by Lemma \\ref{lemma:labels}, a $(\\bm{z}_1^N,\\bm{z}_2^N)$ label can be assigned to atmost one input tuple in a particular $\\bm{x}_3^i$-partition. Thus $\\bm{e}_i$ has $n_i$ components as $1$ and the rest as $0$. Then the frequency of occurrence of a particular $(\\bm{z}_1^N,\\bm{z}_2^N)$ label among all the input tuples that result in the arithmetic sum $\\bm{\\sigma}$ can be found by considering the component-wise sum $\\sum_{i=1}^{L_{x,y}}\\bm{e}_i$. Normalizing by the total number of input tuples gives the claim.\n\n\n\n\\end{IEEEproof}\n\n\n\n\n\n\\begin{theorem}\nLet $\\bm{1}_u$ and $\\bm{0}_v$ denote the all-ones vector with $u$ components and the all-zeros vector with $v$ components respectively. Let $L_{x,y}^i:=L_{x,y}-n_i$ for all $i \\in [L_{x,y}]$. Then\n\n", "index": 7, "text": "\\begin{equation}\n\\bm{p}^\\star=\\frac{1}{M_{x,y}}\\left(\\begin{bmatrix}\n\\mathbf{1}_{n_1}\\\\\n\\mathbf{0}_{L_{x,y}^1}\n\\end{bmatrix}+\n\\begin{bmatrix}\n\\mathbf{1}_{n_2}\\\\\n\\mathbf{0}_{L_{x,y}^2}\n\\end{bmatrix}+\\hdots +\\begin{bmatrix}\n\\mathbf{1}_{n_{L_{x,y}}}\\\\\n\\mathbf{0}_{L_{x,y}^{L_{x,y}}}\n\\end{bmatrix}\\right) \\label{eq:pstar}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\bm{p}^{\\star}=\\frac{1}{M_{x,y}}\\left(\\begin{bmatrix}\\mathbf{1}_{n_{1}}\\\\&#10;\\mathbf{0}_{L_{x,y}^{1}}\\end{bmatrix}+\\begin{bmatrix}\\mathbf{1}_{n_{2}}\\\\&#10;\\mathbf{0}_{L_{x,y}^{2}}\\end{bmatrix}+\\ldots+\\begin{bmatrix}\\mathbf{1}_{n_{L_{%&#10;x,y}}}\\\\&#10;\\mathbf{0}_{L_{x,y}^{L_{x,y}}}\\end{bmatrix}\\right)\" display=\"block\"><mrow><msup><mi>\ud835\udc91</mi><mo>\u22c6</mo></msup><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>M</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mn>\ud835\udfcf</mn><msub><mi>n</mi><mn>1</mn></msub></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mn/><msubsup><mi>L</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mn>1</mn></msubsup></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>+</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mn>\ud835\udfcf</mn><msub><mi>n</mi><mn>2</mn></msub></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mn/><msubsup><mi>L</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mn>2</mn></msubsup></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>+</mo><mi mathvariant=\"normal\">\u2026</mi><mo>+</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mn>\ud835\udfcf</mn><msub><mi>n</mi><msub><mi>L</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></msub></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mn/><msubsup><mi>L</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><msub><mi>L</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></msubsup></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nWe refer to $\\bm{p}^\\star$ as the ``clumpy\" distribution.\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet ${\\bm{e}}_i^\\star=\\begin{bmatrix}\n\\mathbf{1}_{n_i} ~ \\mathbf{0}_{L_{x,y}^i}\n\\end{bmatrix}^\\intercal$ so that ${\\bm{p}}^\\star = \\frac{1}{M_{x,y}}\\sum_{i=1}^{L_{x,y}}{\\bm{e}}_i^\\star$. Let ${\\bm{e}}_i$ denote any binary vector of length $L_{x,y}$ such that it has exactly $n_i$ ones. From Claim \\ref{claim:family} any ${\\bm{p}} \\in \\mathcal{P}$ can be expressed as $\\sum_{i=1}^{L_{x,y}} {\\bm{e}}_i$ for appropriate choices of vectors ${\\bm{e}}_i, i \\in [L_{x,y}]$.\n\\begin{claim}\\label{claim:t}\nLet ${\\bm{d}} = {\\bm{p}}^\\star - {\\bm{p}}$ and let ${\\bm{d}}(i)$ represent its $i$-th component. Then, $\\sum_{i=1}^u {\\bm{d}}(i) \\geq 0$ for all $u \\in [L_{x,y}]$.\n\\end{claim}\n\\begin{IEEEproof}\nWe show this by considering ${\\bm{e}}_j^\\star - {\\bm{e}}_j$. Note that, for $1 \\leq i \\leq n_i$, we have ${\\bm{e}}_j^\\star(i) = 1 \\geq {\\bm{e}}_j(i)$. This implies that for $1 \\leq u \\leq n_i$, we have\n\n", "itemtype": "equation", "pos": 21797, "prevtext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis a probability mass function in $\\mathcal{P}$ that minimizes the entropy on $L_{x,y}$ $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels. Moreover, any other entropy-minimizing distribution is only a permutation of $\\bm{p}^\\star$.\n\n\\end{theorem}\n\\begin{IEEEproof}\nIt will be shown in Claim \\ref{claim:extremal_point} that $\\bm{p}^\\star$ is an an extremal point of the set conv$(\\mathcal{P})$, which is the convex hull of the set $\\mathcal{P}$. Thus, it is a potential minimizer of the concave entropy function over the convex set conv$(\\mathcal{P})$. Claim \\ref{claim:convex_comb} shows that there are no other candidate minimizers, and hence\n\n", "index": 9, "text": "\\begin{equation*}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})\\geq \\min_{\\bm{p} \\in \\mathcal{P}}H(\\bm{p}) \\geq \\min_{\\bm{p} \\in \\text{conv}(\\mathcal{P})}H(\\bm{p})=H(\\bm{p}^\\star).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"H(\\bm{Z}_{1}^{N},\\bm{Z}_{2}^{N}|\\bm{\\Sigma}=\\bm{\\sigma})\\geq\\min_{\\bm{p}\\in%&#10;\\mathcal{P}}H(\\bm{p})\\geq\\min_{\\bm{p}\\in\\text{conv}(\\mathcal{P})}H(\\bm{p})=H(%&#10;\\bm{p}^{\\star}).\" display=\"block\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc81</mi><mn>1</mn><mi>N</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc81</mi><mn>2</mn><mi>N</mi></msubsup><mo stretchy=\"false\">|</mo><mi>\ud835\udeba</mi><mo>=</mo><mi>\ud835\udf48</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><munder><mi>min</mi><mrow><mi>\ud835\udc91</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi></mrow></munder><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc91</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><munder><mi>min</mi><mrow><mi>\ud835\udc91</mi><mo>\u2208</mo><mrow><mtext>conv</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc91</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc91</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nOn the other hand when $n_i < u \\leq L_{x,y}$, we have\n\n", "itemtype": "equation", "pos": 22978, "prevtext": "\nWe refer to $\\bm{p}^\\star$ as the ``clumpy\" distribution.\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet ${\\bm{e}}_i^\\star=\\begin{bmatrix}\n\\mathbf{1}_{n_i} ~ \\mathbf{0}_{L_{x,y}^i}\n\\end{bmatrix}^\\intercal$ so that ${\\bm{p}}^\\star = \\frac{1}{M_{x,y}}\\sum_{i=1}^{L_{x,y}}{\\bm{e}}_i^\\star$. Let ${\\bm{e}}_i$ denote any binary vector of length $L_{x,y}$ such that it has exactly $n_i$ ones. From Claim \\ref{claim:family} any ${\\bm{p}} \\in \\mathcal{P}$ can be expressed as $\\sum_{i=1}^{L_{x,y}} {\\bm{e}}_i$ for appropriate choices of vectors ${\\bm{e}}_i, i \\in [L_{x,y}]$.\n\\begin{claim}\\label{claim:t}\nLet ${\\bm{d}} = {\\bm{p}}^\\star - {\\bm{p}}$ and let ${\\bm{d}}(i)$ represent its $i$-th component. Then, $\\sum_{i=1}^u {\\bm{d}}(i) \\geq 0$ for all $u \\in [L_{x,y}]$.\n\\end{claim}\n\\begin{IEEEproof}\nWe show this by considering ${\\bm{e}}_j^\\star - {\\bm{e}}_j$. Note that, for $1 \\leq i \\leq n_i$, we have ${\\bm{e}}_j^\\star(i) = 1 \\geq {\\bm{e}}_j(i)$. This implies that for $1 \\leq u \\leq n_i$, we have\n\n", "index": 11, "text": "\\begin{align*}\n\\sum_{i=1}^u {\\bm{e}}_j^\\star(i) - {\\bm{e}}_j(i) &\\geq 0.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{u}{\\bm{e}}_{j}^{\\star}(i)-{\\bm{e}}_{j}(i)\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover></mstyle><mrow><msubsup><mi>\ud835\udc86</mi><mi>j</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>\ud835\udc86</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq 0.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2265</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nwhere the last inequality holds because both ${\\bm{e}}_j^\\star$ and ${\\bm{e}}_j$ have $n_i$ ones. As ${\\bm{d}} = \\frac{1}{M_{x,y}}\\sum_{j=1}^{L_{x,y}} ({\\bm{e}}_j^\\star - {\\bm{e}}_j)$, the result follows.\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{corollary}\nIf $\\bm{p} \\in \\text{conv}(\\mathcal{P})$ and $\\bm{p} \\neq \\bm{p}^\\star$, then\n\\begin{itemize}\n\\item $\\bm{p}^\\star(i) > \\bm{p}(i)$  for $i=\\text{min}\\{k: \\bm{p}^\\star(k)\\neq \\bm{p}(k)\\}$, and\n\\item $\\bm{p}^\\star(j) < \\bm{p}(j)$  for $j=\\text{max}\\{k: \\bm{p}^\\star(k)\\neq \\bm{p}(k)\\}$.\n\\end{itemize}\n\\end{corollary}\n\\begin{IEEEproof}\nFor $\\bm{p} \\in \\mathcal{P}$, substitute $u=\\text{min}\\{i: \\bm{p}^\\star(i)\\neq \\bm{p}(i)\\}$ in Claim \\ref{claim:t} and note that $\\bm{d}(i)=0 ~\\forall i<u$. For the second case, note that $\\sum_{i=1}^{\\text{max}\\{j: \\bm{p}^\\star(j)\\neq \\bm{p}(j)\\}}\\bm{d}(i)=0$ and substitute $u=\\text{max}\\{j: p^\\star(j)\\neq p(j)\\}-1$ in claim \\ref{claim:t}.\n\nNow let $\\bm{p} \\in \\text{conv}(\\mathcal{P})$ such that $\\bm{p}=\\sum \\mu_l \\bm{p}_l$ where each $\\bm{p}_l \\in \\mathcal{P}$ and each $\\mu_l > 0$ with $\\sum \\mu_l=1$. Since the corollary is true for each $\\bm{p}_l$, we have that $\\bm{p}_l(i_l)<\\bm{p}^\\star(i_l)$, if $i_l$ is the first index where $\\bm{p}_l$ differs from $\\bm{p}^\\star$. Suppose $i_j:=\\min_l i_l$ is unique, then $\\bm{p}_l(i_j)=\\bm{p}^\\star(i_j) ~\\forall l \\neq j$ and $\\bm{p}_j(i_j)<\\bm{p}^\\star(i_j)$. Hence $\\bm{p}(i_j)=(1-\\mu_j)\\bm{p}^\\star(i_j)+\\mu_j \\bm{p}_j(i_j) < \\bm{p}^\\star(i_j)$. A similar argument also works when $i_j$ is not unique.\n\\end{IEEEproof}\n\n\\begin{claim}\\label{claim:extremal_point}\n$\\bm{p}^\\star$ is an extremal point of the convex set conv($\\mathcal{P}$).\n\\end{claim}\n\\begin{IEEEproof}\nSuppose that there exist $\\bm{p}_1,\\bm{p}_2 \\in \\text{conv}(\\mathcal{P})$ such that $p^\\star=(\\bm{p}_1+\\bm{p}_2)/2$ with $\\bm{p}_1 \\neq \\bm{p}_2$.\nLet $i$ be the least index such that $\\bm{p}_1(i) \\neq \\bm{p}_2(i)$. Then without loss of generality, $\\bm{p}_1(i)>\\bm{p}^\\star(i)>\\bm{p}_2(i)$. But that is a contradiction to the corollary to Claim \\ref{claim:t} and hence our claim is proved.\n\\end{IEEEproof}\nOur next claim shows that {\\it all} extremal points of conv($\\mathcal{P}$) are permutations of the distribution $\\bm{p}^\\star$ (proof appears in Appendix \\ref{app:convex_comb}).\n\\begin{claim}\\label{claim:convex_comb}\nAny $\\bm{p} \\in$ conv($\\mathcal{P}$) can be written as a convex combination of vectors which are permutations of $\\bm{p}^\\star$.\n\\end{claim}\n\n\n\n\\subsection{Entropy of clumpy distribution}\\label{subsec:clumpy_entropy}\nFor $\\bm{p}^\\star$ as defined in \\eqref{eq:pstar}, WLOG assume that $n_1 \\geq n_2 \\geq \\hdots \\geq n_{L_{x,y}}$. Recall that $n_i$ is the number of input tuples of the form $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3^i)$ that have the arithmetic sum $\\bm{\\sigma}$. Then $n_1=L_{x,y}$ as it corresponds to $\\bm{\\tilde{x}}_3$-partition. If a particular $\\bm{x}_3$ differs from $\\bm{\\tilde{x}}_3$ in any $1 \\leq u \\leq x+y$ components, then one can check that the number of input tuples with arithmetic sum $\\bm{\\sigma}$ in this particular $\\bm{x}_3$-partition is exactly $L_{x,y}/2^u$. Also, depending on which $u$ bits of $\\bm{\\tilde{x}}_3$ are flipped, there are $\\binom{x+y}{u}$ different $\\bm{x}_3$-partitions that have $L_{x,y}/2^u$ input tuples in them. Let $\\bm{A}$ denote a $L_{x,y}\\times L_{x,y}$ matrix with the $i$th column as the vector $\\bm{e}_i^\\star$ for all $i \\in [L_{x,y}]$. By the above discussion, matrix $\\bm{A}$ has a ``staircase\" structure as shown in Figure \\ref{fig:A}.\n\\begin{figure}\n\\centering\n\\includegraphics[scale=0.8]{A}\n\\caption{Staircase structure for the $\\{0,1\\}$-matrix $\\bm{A}$ defined in subsection \\ref{subsec:clumpy_entropy}. Only the $1$s are shown. Boxes denote all-ones block matrices of row and column dimension as mentioned beside their length and breadth.}\\label{fig:A}\n\\end{figure}\nEach row of $\\bm{A}$ corresponds to a particular label and the row sum indicates its frequency of occurrence. Conditioned on $\\bm{\\sigma}$, each input tuple that results in that sum is equally likely with probability $1/M_{x,y}$. Then the expression for the entropy of the clumpy distribution is given in equation \\eqref{eq:clumpy_entropy}. The first term in the RHS denotes the contribution to the entropy by labels which are repeated exactly once, the second term corresponds to labels repeated exactly twice, and so on.\n\\begin{IEEEeqnarray*}{Rl}\nH(\\bm{p}^\\star)=&-2^{x+y-1}\\frac{\\binom{x+y}{0}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}}{M_{x,y}}\\\\&-2^{x+y-2}\\frac{\\binom{x+y}{0}+\\binom{x+y}{1}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}+\\binom{x+y}{1}}{M_{x,y}}-\\hdots\\\\\n&-\\frac{\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}}{M_{x,y}}\\IEEEyesnumber \\IEEEeqnarraynumspace \\label{eq:clumpy_entropy}\n\\end{IEEEeqnarray*}\n\n\nLet the set of $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels assigned to the input tuples for a particular value of $\\bm{\\Sigma}=\\bm{\\sigma}$ be $Z_{\\bm{\\sigma}}$. Then our discussion about the clumpy distribution implies that a lower bound for the quantity $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ can be obtained by choosing $|Z_{\\bm{\\sigma}}|=L_{x,y}$ and letting these labels follow the probability mass function $\\bm{p}^\\star$.\nSince the network has to compute the arithmetic-sum of the messages, input tuples that result in a different $\\bm{\\sigma}$ must be provided a different $(\\bm{z}_1^N,\\bm{z}_2^N)$ label. Thus, for two different realizations $\\bm{\\sigma},\\bm{\\tilde{\\sigma}}$ that have the same number of 1's and 2's, we have that $Z_{\\bm{\\sigma}} \\cap Z_{\\bm{\\tilde{\\sigma}}}=\\phi$ and both $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma}),H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\tilde{\\sigma}})\\geq H(\\bm{p}^\\star)$, where $x,y$ in the definition (equation \\eqref{eq:pstar}) of $\\bm{p}^\\star$ are the number of 1's, 2's respectively in $\\bm{\\sigma}$ or $\\bm{\\tilde{\\sigma}}$.\n\n\n\n\n\n\n\n\n\n\nThus a lower bound for $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})=\\sum_{\\bm{\\sigma}}\\Pr(\\bm{\\Sigma}=\\bm{\\sigma})H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ can be found by assuming that each of the conditional pmfs are clumpy. Using this, in Appendices \\ref{app:clumpy_entropy} and \\ref{app:average_clumpy_entropy}, we show the following result.\n\\begin{lemma}\\label{lemma:limit_H_by_k}\n\n", "itemtype": "equation", "pos": 23119, "prevtext": "\nOn the other hand when $n_i < u \\leq L_{x,y}$, we have\n\n", "index": 13, "text": "\\begin{align*}\n\\sum_{i=1}^u {\\bm{e}}_j^\\star(i) - {\\bm{e}}_j(i) & = n_i - \\sum_{i=1}^z {\\bm{e}}_j(i) \\geq 0,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{u}{\\bm{e}}_{j}^{\\star}(i)-{\\bm{e}}_{j}(i)\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover></mstyle><mrow><msubsup><mi>\ud835\udc86</mi><mi>j</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>\ud835\udc86</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=n_{i}-\\sum_{i=1}^{z}{\\bm{e}}_{j}(i)\\geq 0,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>z</mi></munderover></mstyle><mrow><msub><mi>\ud835\udc86</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lower bound on computing capacity}\\label{sec:lower_bd}\nIn this section we describe a valid $(k,N)$ network code which satisfies $k/{\\rm E} N=2/2.5$ for the case when $\\mathcal{Z}=\\{0,1\\}$. A similar scheme can be easily extended to larger alphabets.\nIn fact, the scheme described here is the same scheme as the one described in \\cite{appuFKZ11}, except that no probability distribution on the inputs was used there. In what follows, all addition operations are over the real numbers.\n\nSet $k$ to be an even number. The encoder at $s_1$ computes the first $k/2$ components of the sum $\\bm{X}_1+\\bm{X}_3$. Note that the components of $\\bm{X}_1+\\bm{X}_3$ are iid distributed according to\n\\begin{numcases}{\\Pr(X_1+X_3=u)=}\n1/4,& if $u \\in \\{0,2\\}$ \\nonumber\\\\\n1/2,& otherwise, \\nonumber\n\\end{numcases} and hence $H(X_1+X_3)=1.5$ bits.\nFor an $\\epsilon >0$, we compress the sequence of first $k/2$ components of $\\bm{X}_1+\\bm{X}_3$ based on whether they belong to the weakly typical set $A_\\epsilon^{(k/2)}$ \\cite{coverthomas} or not. We can encode all the sequences in $A_\\epsilon^{(k/2)}$ by using atmost $\\lceil \\frac{k}{2}(1.5+\\epsilon)\\rceil$ bits. For encoding sequences not in $A_\\epsilon^{(k/2)}$, we don't need more than $\\lceil \\frac{k}{2}\\log 3\\rceil$ bits. We add an extra bit to indicate whether the sequence being encoded belongs to the typical set or not. Having encoded the first $k/2$ bits of $\\bm{X}_1+\\bm{X}_3$ in the above fashion, $s_1$ transmits the subsequent $k/2$ bits of $\\bm{X}_1$ in an uncoded manner.\n\nThe encoder at $s_2$ employs a similar procedure as above except that it transmits the first $k/2$ bits of $\\bm{X}_2$ in an uncoded manner and the subsequent $k/2$ bits of the component-wise sum $\\bm{X}_2+\\bm{X}_3$ using typical set coding for a typical set with the same $\\epsilon$.\n\nThe terminal is able to recover the first $k/2$ components of $\\bm{X}_1+\\bm{X_3}$ and $\\bm{X}_2$ with zero error and the last $k/2$ components of $\\bm{X}_1$ and $\\bm{X}_2+\\bm{X}_3$ with zero error. From these it can correctly compute $k$ components of the sum $\\bm{X}_1+\\bm{X}_2+\\bm{X}_3$.\n\nFor the value of the stopping time, the terminal waits for $1+k/2$ bits so as to obtain all the uncoded bits and the information about whether the coded bits belong to the typical set or not. Based on that, it waits for an appropriate number of bits so as to decode the required information without error. Let $\\bm{V}_1,\\bm{V}_2$ denote the first $k/2$ components of $\\bm{X}_1+\\bm{X}_3$ and the last $k/2$ components of $\\bm{X}_2+\\bm{X}_3$ respectively. Let $B$ denote the event $\\{\\bm{V_1}\\in A_\\epsilon^{(\\frac{k}{2})} \\cap \\bm{V}_2 \\in A_\\epsilon^{(\\frac{k}{2})}\\}$. Then the expected value of the stopping time can be evaluated as follows\n\\begin{IEEEeqnarray*}{Rl}\n{\\rm E} N =&1+\\frac{k}{2}+\\Pr(B)\\left\\lceil \\frac{k}{2}(1.5+\\epsilon)\\right\\rceil +(1-\\Pr(B))\\left\\lceil \\frac{k}{2}\\log 3 \\right\\rceil ,\\\\\n\\leq & 1+\\frac{k}{2}+\\left\\lceil \\frac{k}{2}(1.5+\\epsilon)\\right\\rceil + 2\\epsilon \\left\\lceil \\frac{k}{2}\\log 3\\right\\rceil.\n\\end{IEEEeqnarray*}\nHence, for large $k$, ${\\rm E} N \\approx \\frac{5k}{4}$ and that gives our result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions and future work}\n\\label{sec:concl}\nWe have obtained new upper and lower bounds for zero error arithmetic-sum computation using variable-length network codes for a specific network. There is still a gap between the achievable rate and the upper bound. Future work will involve trying to narrow this gap. In addition, all currently known upper bounds for function computation over DAGs are based on cutsets and are recognized to be loose. It may be fruitful to examine whether the upper bound technique used in this work that operates by lower bounding the entropy of the descriptions conditional on the function value are applicable in more general scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{IEEEabrv,../BibFiles/tip}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendices\n\\section{}\\label{app:stopping_time_ratio}\nConsider the optimization problem defined as follows, where $p_i:=\\Pr(N=i)$ for $i \\in \\mathbb{N}$ and $\\Delta=\\log e/\\epsilon$.\n\n\\begin{IEEEeqnarray}{uL}\nminimize\\hspace{15pt} & {\\rm E} N-\\frac{\\Delta}{\\log e} H(N) = \\sum p_i(i+\\Delta{{\\rm ln}\\,} p_i) \\IEEEyesnumber\\IEEEyessubnumber\\IEEEeqnarraynumspace \\label{opt:min_obj}\\\\\nsubject to:\\hspace{10pt} & p_i-\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i} \\leq 0 \\;\\text{for all}\\; i \\in \\mathbb{N}, \\IEEEyessubnumber \\label{opt:lemma1}\\\\\n& -p_i \\leq 0 \\;\\text{for all}\\; i \\in \\mathbb{N},\\IEEEnonumber*\\\\\n& \\sum p_i -1 =0.\n\\end{IEEEeqnarray}\nThe Lagrangian of the objective function \\eqref{opt:min_obj} is\n\\begin{IEEEeqnarray}{Rl}\nL(\\bm{p},\\lambda,\\bm{\\nu},\\bm{\\mu})=&\\sum p_i(i+\\Delta{{\\rm ln}\\,} p_i)+\\lambda (1-\\sum p_i)\\IEEEnonumber\\\\&-\\sum \\bm{\\nu}_i p_i + \\sum \\bm{\\mu}_i\\left(p_i-\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i}\\right).\\IEEEeqnarraynumspace \\label{eq:lagrangian}\n\\end{IEEEeqnarray}\nHere, $\\lambda, \\bm{\\nu}\\geq \\bm{0}, \\bm{\\mu}\\geq \\bm{0}$ are dual variables with the natural number subscript $i$ indexing their components. Since the Lagrangian is convex in $\\bm{p}$, minimizing it involves setting\n\\begin{IEEEeqnarray*}{Rl}\n\\frac{\\partial L}{\\partial p_i}=&i+\\Delta{{\\rm ln}\\,} p_i+\\Delta - \\lambda - \\nu_i +\\mu_i=0 ~\\text{for all}~i,\\\\\n\\implies p_i=&\\exp\\left(\\frac{\\lambda + \\nu_i - \\mu_i -i -\\Delta}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nSubstituting this back in equation \\eqref{eq:lagrangian}, we get that the dual function is\n\\begin{IEEEeqnarray*}{C}\n\\mathcal{L}(\\lambda,\\bm{\\mu},\\bm{\\nu})=\\lambda-\\sum_i B(i,\\lambda,\\mu_i,\\nu_i),\n\\end{IEEEeqnarray*}\nwhere\n\\begin{IEEEeqnarray*}{Rl}\nB(i,\\lambda,\\mu_i,\\nu_i):=&\\left[\\Delta \\exp\\left(\\frac{\\lambda + \\nu_i - \\mu_i -i -\\Delta}{\\Delta}\\right)\\right. \\\\\n&{}\\hspace{15pt}\\left.- \\mu_i \\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i}\\right].\n\\end{IEEEeqnarray*}\nWe evaluate the dual at a point in its domain to obtain a lower bound to the optimal value of equation \\eqref{opt:min_obj}. For $\\bm{\\nu}=\\bm{0}$ and\n\\begin{numcases}{\\mu_i=}\n\\lfloor ck \\rfloor -i, & if $i\\in \\{1,2,\\hdots,\\lfloor ck \\rfloor\\}$ \\nonumber\\\\\n0, & otherwise,\\nonumber\n\\end{numcases} where $c=\\log_{|\\mathcal{Z}|}\\left(\\frac{8}{3}\\right)$, the value of the dual function is\n\\begin{IEEEeqnarray*}{rL}\n&\\mathcal{L}(\\lambda)=\\lambda -\\left(\\frac{3}{8}\\right)^k\\sum_{i=1}^{\\lfloor ck \\rfloor}(\\lfloor ck\\rfloor -i)|\\mathcal{Z}|^{2i}\\\\\n&{}-\\Delta\\sum_{i=1}^{\\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -\\lfloor ck\\rfloor-\\Delta}{\\Delta}\\right)-\\Delta\\sum_{i > \\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -i-\\Delta}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nWe can separately evaluate that\n\n", "itemtype": "equation", "pos": 29583, "prevtext": "\nwhere the last inequality holds because both ${\\bm{e}}_j^\\star$ and ${\\bm{e}}_j$ have $n_i$ ones. As ${\\bm{d}} = \\frac{1}{M_{x,y}}\\sum_{j=1}^{L_{x,y}} ({\\bm{e}}_j^\\star - {\\bm{e}}_j)$, the result follows.\n\\end{IEEEproof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{corollary}\nIf $\\bm{p} \\in \\text{conv}(\\mathcal{P})$ and $\\bm{p} \\neq \\bm{p}^\\star$, then\n\\begin{itemize}\n\\item $\\bm{p}^\\star(i) > \\bm{p}(i)$  for $i=\\text{min}\\{k: \\bm{p}^\\star(k)\\neq \\bm{p}(k)\\}$, and\n\\item $\\bm{p}^\\star(j) < \\bm{p}(j)$  for $j=\\text{max}\\{k: \\bm{p}^\\star(k)\\neq \\bm{p}(k)\\}$.\n\\end{itemize}\n\\end{corollary}\n\\begin{IEEEproof}\nFor $\\bm{p} \\in \\mathcal{P}$, substitute $u=\\text{min}\\{i: \\bm{p}^\\star(i)\\neq \\bm{p}(i)\\}$ in Claim \\ref{claim:t} and note that $\\bm{d}(i)=0 ~\\forall i<u$. For the second case, note that $\\sum_{i=1}^{\\text{max}\\{j: \\bm{p}^\\star(j)\\neq \\bm{p}(j)\\}}\\bm{d}(i)=0$ and substitute $u=\\text{max}\\{j: p^\\star(j)\\neq p(j)\\}-1$ in claim \\ref{claim:t}.\n\nNow let $\\bm{p} \\in \\text{conv}(\\mathcal{P})$ such that $\\bm{p}=\\sum \\mu_l \\bm{p}_l$ where each $\\bm{p}_l \\in \\mathcal{P}$ and each $\\mu_l > 0$ with $\\sum \\mu_l=1$. Since the corollary is true for each $\\bm{p}_l$, we have that $\\bm{p}_l(i_l)<\\bm{p}^\\star(i_l)$, if $i_l$ is the first index where $\\bm{p}_l$ differs from $\\bm{p}^\\star$. Suppose $i_j:=\\min_l i_l$ is unique, then $\\bm{p}_l(i_j)=\\bm{p}^\\star(i_j) ~\\forall l \\neq j$ and $\\bm{p}_j(i_j)<\\bm{p}^\\star(i_j)$. Hence $\\bm{p}(i_j)=(1-\\mu_j)\\bm{p}^\\star(i_j)+\\mu_j \\bm{p}_j(i_j) < \\bm{p}^\\star(i_j)$. A similar argument also works when $i_j$ is not unique.\n\\end{IEEEproof}\n\n\\begin{claim}\\label{claim:extremal_point}\n$\\bm{p}^\\star$ is an extremal point of the convex set conv($\\mathcal{P}$).\n\\end{claim}\n\\begin{IEEEproof}\nSuppose that there exist $\\bm{p}_1,\\bm{p}_2 \\in \\text{conv}(\\mathcal{P})$ such that $p^\\star=(\\bm{p}_1+\\bm{p}_2)/2$ with $\\bm{p}_1 \\neq \\bm{p}_2$.\nLet $i$ be the least index such that $\\bm{p}_1(i) \\neq \\bm{p}_2(i)$. Then without loss of generality, $\\bm{p}_1(i)>\\bm{p}^\\star(i)>\\bm{p}_2(i)$. But that is a contradiction to the corollary to Claim \\ref{claim:t} and hence our claim is proved.\n\\end{IEEEproof}\nOur next claim shows that {\\it all} extremal points of conv($\\mathcal{P}$) are permutations of the distribution $\\bm{p}^\\star$ (proof appears in Appendix \\ref{app:convex_comb}).\n\\begin{claim}\\label{claim:convex_comb}\nAny $\\bm{p} \\in$ conv($\\mathcal{P}$) can be written as a convex combination of vectors which are permutations of $\\bm{p}^\\star$.\n\\end{claim}\n\n\n\n\\subsection{Entropy of clumpy distribution}\\label{subsec:clumpy_entropy}\nFor $\\bm{p}^\\star$ as defined in \\eqref{eq:pstar}, WLOG assume that $n_1 \\geq n_2 \\geq \\hdots \\geq n_{L_{x,y}}$. Recall that $n_i$ is the number of input tuples of the form $(\\bm{x}_1,\\bm{x}_2,\\bm{x}_3^i)$ that have the arithmetic sum $\\bm{\\sigma}$. Then $n_1=L_{x,y}$ as it corresponds to $\\bm{\\tilde{x}}_3$-partition. If a particular $\\bm{x}_3$ differs from $\\bm{\\tilde{x}}_3$ in any $1 \\leq u \\leq x+y$ components, then one can check that the number of input tuples with arithmetic sum $\\bm{\\sigma}$ in this particular $\\bm{x}_3$-partition is exactly $L_{x,y}/2^u$. Also, depending on which $u$ bits of $\\bm{\\tilde{x}}_3$ are flipped, there are $\\binom{x+y}{u}$ different $\\bm{x}_3$-partitions that have $L_{x,y}/2^u$ input tuples in them. Let $\\bm{A}$ denote a $L_{x,y}\\times L_{x,y}$ matrix with the $i$th column as the vector $\\bm{e}_i^\\star$ for all $i \\in [L_{x,y}]$. By the above discussion, matrix $\\bm{A}$ has a ``staircase\" structure as shown in Figure \\ref{fig:A}.\n\\begin{figure}\n\\centering\n\\includegraphics[scale=0.8]{A}\n\\caption{Staircase structure for the $\\{0,1\\}$-matrix $\\bm{A}$ defined in subsection \\ref{subsec:clumpy_entropy}. Only the $1$s are shown. Boxes denote all-ones block matrices of row and column dimension as mentioned beside their length and breadth.}\\label{fig:A}\n\\end{figure}\nEach row of $\\bm{A}$ corresponds to a particular label and the row sum indicates its frequency of occurrence. Conditioned on $\\bm{\\sigma}$, each input tuple that results in that sum is equally likely with probability $1/M_{x,y}$. Then the expression for the entropy of the clumpy distribution is given in equation \\eqref{eq:clumpy_entropy}. The first term in the RHS denotes the contribution to the entropy by labels which are repeated exactly once, the second term corresponds to labels repeated exactly twice, and so on.\n\\begin{IEEEeqnarray*}{Rl}\nH(\\bm{p}^\\star)=&-2^{x+y-1}\\frac{\\binom{x+y}{0}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}}{M_{x,y}}\\\\&-2^{x+y-2}\\frac{\\binom{x+y}{0}+\\binom{x+y}{1}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}+\\binom{x+y}{1}}{M_{x,y}}-\\hdots\\\\\n&-\\frac{\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}}{M_{x,y}}\\log\\frac{\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}}{M_{x,y}}\\IEEEyesnumber \\IEEEeqnarraynumspace \\label{eq:clumpy_entropy}\n\\end{IEEEeqnarray*}\n\n\nLet the set of $(\\bm{z}_1^N,\\bm{z}_2^N)$ labels assigned to the input tuples for a particular value of $\\bm{\\Sigma}=\\bm{\\sigma}$ be $Z_{\\bm{\\sigma}}$. Then our discussion about the clumpy distribution implies that a lower bound for the quantity $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ can be obtained by choosing $|Z_{\\bm{\\sigma}}|=L_{x,y}$ and letting these labels follow the probability mass function $\\bm{p}^\\star$.\nSince the network has to compute the arithmetic-sum of the messages, input tuples that result in a different $\\bm{\\sigma}$ must be provided a different $(\\bm{z}_1^N,\\bm{z}_2^N)$ label. Thus, for two different realizations $\\bm{\\sigma},\\bm{\\tilde{\\sigma}}$ that have the same number of 1's and 2's, we have that $Z_{\\bm{\\sigma}} \\cap Z_{\\bm{\\tilde{\\sigma}}}=\\phi$ and both $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma}),H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\tilde{\\sigma}})\\geq H(\\bm{p}^\\star)$, where $x,y$ in the definition (equation \\eqref{eq:pstar}) of $\\bm{p}^\\star$ are the number of 1's, 2's respectively in $\\bm{\\sigma}$ or $\\bm{\\tilde{\\sigma}}$.\n\n\n\n\n\n\n\n\n\n\nThus a lower bound for $H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})=\\sum_{\\bm{\\sigma}}\\Pr(\\bm{\\Sigma}=\\bm{\\sigma})H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})$ can be found by assuming that each of the conditional pmfs are clumpy. Using this, in Appendices \\ref{app:clumpy_entropy} and \\ref{app:average_clumpy_entropy}, we show the following result.\n\\begin{lemma}\\label{lemma:limit_H_by_k}\n\n", "index": 15, "text": "\\begin{align*}\n\\frac{H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\Sigma)}{k} &{}\\rightarrow 0.75(-1+\\log 3) \\;\\text{as}\\; k\\rightarrow \\infty.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{H(\\bm{Z}_{1}^{N},\\bm{Z}_{2}^{N}|\\Sigma)}{k}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc81</mi><mn>1</mn><mi>N</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc81</mi><mn>2</mn><mi>N</mi></msubsup><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>k</mi></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{}\\rightarrow 0.75(-1+\\log 3)\\;\\text{as}\\;k\\rightarrow\\infty.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><mn>0.75</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo>-</mo><mn>1</mn></mrow><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mn>3</mn></mrow></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>as</mtext></mpadded><mo>\u2062</mo><mi>k</mi></mrow><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nUsing this and expanding the geometric series, we obtain that\n\\begin{IEEEeqnarray*}{rL}\n\\mathcal{L}(\\lambda)\\geq {}&\\lambda - \\frac{|\\mathcal{Z}|^2}{(|\\mathcal{Z}|^2-1)^2}-\\Delta\\sum_{i=1}^{\\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -\\lfloor ck\\rfloor-\\Delta}{\\Delta}\\right)\\\\\n&{}-\\frac{\\Delta}{e-e^{1-1/\\Delta}}\\exp\\left(\\frac{\\lambda-\\lfloor ck \\rfloor}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nIf we choose $\\lambda = ck/2$, we can see that the value of the dual function will be positive for large $k$. Hence, for any probability mass function that satisfies Lemma \\ref{lemma:stopping_time}, we get that the value of ${\\rm E} N-\\frac{\\Delta}{\\log e} H(N) \\geq 0$, i.e., $H(N)/{\\rm E} N \\leq \\epsilon$. \\hfill {\\IEEEQEDopen}\n\n\\section{Proof of Claim \\ref{claim:entropy_support_set}}\\label{app:support_arg}\n\nWe first show that atleast one of the components of $\\bm{q}_{m+1}$ is exactly equal to $c$. Pick any $q \\in \\mathcal{Q}_{m+1}$ and arrange its components in nonincreasing order so that $\\bm{q}(m+1)$ is its smallest component. If $\\bm{q}(m+1)>c$, then we can express $\\bm{q}$ as a convex combination of two other elements of $\\mathcal{Q}_{m+1}$ as follows. Note that $\\bm{q}(1)\\geq \\bm{q}(m+1)\\geq c$.\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{q}=\\frac{1}{2}\\begin{bmatrix}\n\\bm{q}(1)+\\bm{q}(m+1)-c\\\\\n\\bm{q}(2)\\\\\n\\vdots\\\\\n\\bm{q}(m)\\\\\nc\n\\end{bmatrix}+\\frac{1}{2}\\begin{bmatrix}\n\\bm{q}(1)-\\bm{q}(m+1)+c\\\\\n\\bm{q}(2)\\\\\n\\vdots \\\\\n\\bm{q}(m)\\\\\n2\\bm{q}(m+1)-c\n\\end{bmatrix}\n\\end{IEEEeqnarray*}\nBy concavity of entropy function we then have that this $\\bm{q} \\neq \\bm{q}_{m+1}$. Thus $\\bm{q}_{m+1}$ must have atleast one component that is equal to $c$, and WLOG let $\\bm{q}_{m+1}=c$. Let $Q$ be a random variable on $[m+1]$ following the probability distribution $\\bm{q}_{m+1}$. Let $I=\\bm{1}_{\\{Q=m+1\\}}$ be the indicator function for the event $\\{Q=m+1\\}$. Then $\\Pr(I=1)=c$ and $\\Pr(I=0)=1-c$. Then we have that\n\\begin{IEEEeqnarray*}{Rl}\nH(Q,I)=&H(Q)=H(I)+H(Q|I)\\\\\n=&H(I)+\\Pr(I=0)H(Q|I=0).\n\\end{IEEEeqnarray*}\nSince $\\Pr(Q|I=0)$ is a probability distribution over $[m]$ and $\\Pr(Q=i|I=0)>\\Pr(Q=i)$ for all $i \\in [m]$, $\\Pr(Q|I=0) \\in \\mathcal{Q}_m$. Hence we have that $H(Q|I=0)\\geq H(\\bm{q}_m)$ and using this in the previous equation, we get\n\\begin{IEEEeqnarray*}{C}\nH(Q)\\geq c\\log \\frac{1}{c}+(1-c)\\log \\frac{1}{1-c}+(1-c)H(\\bm{q}_m).\n\\end{IEEEeqnarray*}\nSince $mc<1$, we have that $-\\log c>\\log m$. But $\\log m$ is the entropy of the uniform distribution over $[m]$ and hence $\\log m \\geq H(\\bm{q}_m)$. Using this in the previous equation we get\n\n", "itemtype": "equation", "pos": 36672, "prevtext": "\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Lower bound on computing capacity}\\label{sec:lower_bd}\nIn this section we describe a valid $(k,N)$ network code which satisfies $k/{\\rm E} N=2/2.5$ for the case when $\\mathcal{Z}=\\{0,1\\}$. A similar scheme can be easily extended to larger alphabets.\nIn fact, the scheme described here is the same scheme as the one described in \\cite{appuFKZ11}, except that no probability distribution on the inputs was used there. In what follows, all addition operations are over the real numbers.\n\nSet $k$ to be an even number. The encoder at $s_1$ computes the first $k/2$ components of the sum $\\bm{X}_1+\\bm{X}_3$. Note that the components of $\\bm{X}_1+\\bm{X}_3$ are iid distributed according to\n\\begin{numcases}{\\Pr(X_1+X_3=u)=}\n1/4,& if $u \\in \\{0,2\\}$ \\nonumber\\\\\n1/2,& otherwise, \\nonumber\n\\end{numcases} and hence $H(X_1+X_3)=1.5$ bits.\nFor an $\\epsilon >0$, we compress the sequence of first $k/2$ components of $\\bm{X}_1+\\bm{X}_3$ based on whether they belong to the weakly typical set $A_\\epsilon^{(k/2)}$ \\cite{coverthomas} or not. We can encode all the sequences in $A_\\epsilon^{(k/2)}$ by using atmost $\\lceil \\frac{k}{2}(1.5+\\epsilon)\\rceil$ bits. For encoding sequences not in $A_\\epsilon^{(k/2)}$, we don't need more than $\\lceil \\frac{k}{2}\\log 3\\rceil$ bits. We add an extra bit to indicate whether the sequence being encoded belongs to the typical set or not. Having encoded the first $k/2$ bits of $\\bm{X}_1+\\bm{X}_3$ in the above fashion, $s_1$ transmits the subsequent $k/2$ bits of $\\bm{X}_1$ in an uncoded manner.\n\nThe encoder at $s_2$ employs a similar procedure as above except that it transmits the first $k/2$ bits of $\\bm{X}_2$ in an uncoded manner and the subsequent $k/2$ bits of the component-wise sum $\\bm{X}_2+\\bm{X}_3$ using typical set coding for a typical set with the same $\\epsilon$.\n\nThe terminal is able to recover the first $k/2$ components of $\\bm{X}_1+\\bm{X_3}$ and $\\bm{X}_2$ with zero error and the last $k/2$ components of $\\bm{X}_1$ and $\\bm{X}_2+\\bm{X}_3$ with zero error. From these it can correctly compute $k$ components of the sum $\\bm{X}_1+\\bm{X}_2+\\bm{X}_3$.\n\nFor the value of the stopping time, the terminal waits for $1+k/2$ bits so as to obtain all the uncoded bits and the information about whether the coded bits belong to the typical set or not. Based on that, it waits for an appropriate number of bits so as to decode the required information without error. Let $\\bm{V}_1,\\bm{V}_2$ denote the first $k/2$ components of $\\bm{X}_1+\\bm{X}_3$ and the last $k/2$ components of $\\bm{X}_2+\\bm{X}_3$ respectively. Let $B$ denote the event $\\{\\bm{V_1}\\in A_\\epsilon^{(\\frac{k}{2})} \\cap \\bm{V}_2 \\in A_\\epsilon^{(\\frac{k}{2})}\\}$. Then the expected value of the stopping time can be evaluated as follows\n\\begin{IEEEeqnarray*}{Rl}\n{\\rm E} N =&1+\\frac{k}{2}+\\Pr(B)\\left\\lceil \\frac{k}{2}(1.5+\\epsilon)\\right\\rceil +(1-\\Pr(B))\\left\\lceil \\frac{k}{2}\\log 3 \\right\\rceil ,\\\\\n\\leq & 1+\\frac{k}{2}+\\left\\lceil \\frac{k}{2}(1.5+\\epsilon)\\right\\rceil + 2\\epsilon \\left\\lceil \\frac{k}{2}\\log 3\\right\\rceil.\n\\end{IEEEeqnarray*}\nHence, for large $k$, ${\\rm E} N \\approx \\frac{5k}{4}$ and that gives our result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions and future work}\n\\label{sec:concl}\nWe have obtained new upper and lower bounds for zero error arithmetic-sum computation using variable-length network codes for a specific network. There is still a gap between the achievable rate and the upper bound. Future work will involve trying to narrow this gap. In addition, all currently known upper bounds for function computation over DAGs are based on cutsets and are recognized to be loose. It may be fruitful to examine whether the upper bound technique used in this work that operates by lower bounding the entropy of the descriptions conditional on the function value are applicable in more general scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{IEEEabrv,../BibFiles/tip}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendices\n\\section{}\\label{app:stopping_time_ratio}\nConsider the optimization problem defined as follows, where $p_i:=\\Pr(N=i)$ for $i \\in \\mathbb{N}$ and $\\Delta=\\log e/\\epsilon$.\n\n\\begin{IEEEeqnarray}{uL}\nminimize\\hspace{15pt} & {\\rm E} N-\\frac{\\Delta}{\\log e} H(N) = \\sum p_i(i+\\Delta{{\\rm ln}\\,} p_i) \\IEEEyesnumber\\IEEEyessubnumber\\IEEEeqnarraynumspace \\label{opt:min_obj}\\\\\nsubject to:\\hspace{10pt} & p_i-\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i} \\leq 0 \\;\\text{for all}\\; i \\in \\mathbb{N}, \\IEEEyessubnumber \\label{opt:lemma1}\\\\\n& -p_i \\leq 0 \\;\\text{for all}\\; i \\in \\mathbb{N},\\IEEEnonumber*\\\\\n& \\sum p_i -1 =0.\n\\end{IEEEeqnarray}\nThe Lagrangian of the objective function \\eqref{opt:min_obj} is\n\\begin{IEEEeqnarray}{Rl}\nL(\\bm{p},\\lambda,\\bm{\\nu},\\bm{\\mu})=&\\sum p_i(i+\\Delta{{\\rm ln}\\,} p_i)+\\lambda (1-\\sum p_i)\\IEEEnonumber\\\\&-\\sum \\bm{\\nu}_i p_i + \\sum \\bm{\\mu}_i\\left(p_i-\\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i}\\right).\\IEEEeqnarraynumspace \\label{eq:lagrangian}\n\\end{IEEEeqnarray}\nHere, $\\lambda, \\bm{\\nu}\\geq \\bm{0}, \\bm{\\mu}\\geq \\bm{0}$ are dual variables with the natural number subscript $i$ indexing their components. Since the Lagrangian is convex in $\\bm{p}$, minimizing it involves setting\n\\begin{IEEEeqnarray*}{Rl}\n\\frac{\\partial L}{\\partial p_i}=&i+\\Delta{{\\rm ln}\\,} p_i+\\Delta - \\lambda - \\nu_i +\\mu_i=0 ~\\text{for all}~i,\\\\\n\\implies p_i=&\\exp\\left(\\frac{\\lambda + \\nu_i - \\mu_i -i -\\Delta}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nSubstituting this back in equation \\eqref{eq:lagrangian}, we get that the dual function is\n\\begin{IEEEeqnarray*}{C}\n\\mathcal{L}(\\lambda,\\bm{\\mu},\\bm{\\nu})=\\lambda-\\sum_i B(i,\\lambda,\\mu_i,\\nu_i),\n\\end{IEEEeqnarray*}\nwhere\n\\begin{IEEEeqnarray*}{Rl}\nB(i,\\lambda,\\mu_i,\\nu_i):=&\\left[\\Delta \\exp\\left(\\frac{\\lambda + \\nu_i - \\mu_i -i -\\Delta}{\\Delta}\\right)\\right. \\\\\n&{}\\hspace{15pt}\\left.- \\mu_i \\left(\\frac{3}{8}\\right)^k|\\mathcal{Z}|^{2i}\\right].\n\\end{IEEEeqnarray*}\nWe evaluate the dual at a point in its domain to obtain a lower bound to the optimal value of equation \\eqref{opt:min_obj}. For $\\bm{\\nu}=\\bm{0}$ and\n\\begin{numcases}{\\mu_i=}\n\\lfloor ck \\rfloor -i, & if $i\\in \\{1,2,\\hdots,\\lfloor ck \\rfloor\\}$ \\nonumber\\\\\n0, & otherwise,\\nonumber\n\\end{numcases} where $c=\\log_{|\\mathcal{Z}|}\\left(\\frac{8}{3}\\right)$, the value of the dual function is\n\\begin{IEEEeqnarray*}{rL}\n&\\mathcal{L}(\\lambda)=\\lambda -\\left(\\frac{3}{8}\\right)^k\\sum_{i=1}^{\\lfloor ck \\rfloor}(\\lfloor ck\\rfloor -i)|\\mathcal{Z}|^{2i}\\\\\n&{}-\\Delta\\sum_{i=1}^{\\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -\\lfloor ck\\rfloor-\\Delta}{\\Delta}\\right)-\\Delta\\sum_{i > \\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -i-\\Delta}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nWe can separately evaluate that\n\n", "index": 17, "text": "\\begin{equation*}\n\\left(\\frac{3}{8}\\right)^k\\sum_{i=1}^{\\lfloor ck \\rfloor}(\\lfloor ck\\rfloor -i)|\\mathcal{Z}|^{2i}\\leq\\frac{|\\mathcal{Z}|^2}{(|\\mathcal{Z}|^2-1)^2} ~\\text{for large}~k.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\left(\\frac{3}{8}\\right)^{k}\\sum_{i=1}^{\\lfloor ck\\rfloor}(\\lfloor ck\\rfloor-i%&#10;)|\\mathcal{Z}|^{2i}\\leq\\frac{|\\mathcal{Z}|^{2}}{(|\\mathcal{Z}|^{2}-1)^{2}}~{}%&#10;\\text{for large}~{}k.\" display=\"block\"><mrow><mrow><mrow><msup><mrow><mo>(</mo><mfrac><mn>3</mn><mn>8</mn></mfrac><mo>)</mo></mrow><mi>k</mi></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">\u230a</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>k</mi></mrow><mo stretchy=\"false\">\u230b</mo></mrow></munderover><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">\u230a</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>k</mi></mrow><mo stretchy=\"false\">\u230b</mo></mrow><mo>-</mo><mi>i</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">|</mo></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msup></mrow></mrow></mrow><mo>\u2264</mo><mrow><mpadded width=\"+3.3pt\"><mfrac><msup><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mtext>for large</mtext></mpadded><mo>\u2062</mo><mi>k</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nThe same argument can be repeated to show that $H(\\bm{q}_{m+1})\\geq H(\\bm{q}_{m}) \\geq H(\\bm{q}_{m-1})$ and so on. Hence, any probability mass function that satisfies a lower bound on the values of each of its probability masses necessarily has an equal or larger entropy if it is nonzero over a larger set. \\hfill {\\IEEEQEDopen}\n\n\\section{Proof of Claim \\ref{claim:convex_comb}}\\label{app:convex_comb}\nWe show that any $\\bm{p} \\in $ conv($\\mathcal{P}$) can be written as a convex combination of permuted $\\bm{p}^\\star$'s. Since $\\bm{p} \\in \\text{conv}(\\mathcal{P})$, we can write that $\\bm{p} =\\sum_i\\mu_i\\bm{p}_i$ where each $\\bm{p}_i \\in \\mathcal{P}$, each $\\mu_i \\geq 0$ and $\\sum_i \\mu_i =1$. One can then see that if the claim is true for each $\\bm{p}_i$ then it is also true for $\\bm{p}$. Hence we focus on $\\bm{p} \\in \\mathcal{P}$ and show that above claim holds for it. Without loss of generality, we can assume that the target vector $\\bm{p}$ is arranged in non-increasing order, otherwise we permute its components so that the largest component is the first component and the successive components are in a non-increasing order. We can then reverse the permutation for every vector in its convex combination finally to get back our original vector.\n\nAlgorithm \\ref{alg:convex_comb} returns a list of vectors, each of which can be expressed as a convex combination of permuted $\\bm{p}^\\star$'s. Using this list, we can find the convex combination of permuted $\\bm{p}^\\star$'s for any given $\\bm{p} \\in \\mathcal{P}$ arranged in non-increasing order. The notation $\\bm{p}'[i \\leftrightarrow j]$ indicates the vector $\\bm{p}'$ with its values at the $i$th and $j$th components interchanged, while all the other components remain the same.\n\\begin{algorithm}\n\\caption{Convex combination of permuted $\\bm{p}^\\star$'s for $\\bm{p}$.}\\label{alg:convex_comb}\n\\begin{algorithmic}[1]\n\\REQUIRE $\\bm{p} \\in$ conv($\\mathcal{P}$) arranged in nonincreasing order, $\\bm{p}^\\star$.\n\\ENSURE A list $L$ of vectors.\n\\STATE Initialize $\\bm{p}' \\leftarrow \\bm{p}^\\star, L \\leftarrow \\phi$. $\\bm{v}, \\lambda$ are temporary variables.\n\\WHILE{$\\bm{p}'-\\bm{p}\\neq\\bm{0}$}\n\\STATE Find the smallest indices $i,j$ such that $\\bm{p}'(i)>\\bm{p}(i)$ and $\\bm{p}'(j)<\\bm{p}(j)$.\n\\STATE Evaluate\n\n", "itemtype": "equation", "pos": 39410, "prevtext": "\nUsing this and expanding the geometric series, we obtain that\n\\begin{IEEEeqnarray*}{rL}\n\\mathcal{L}(\\lambda)\\geq {}&\\lambda - \\frac{|\\mathcal{Z}|^2}{(|\\mathcal{Z}|^2-1)^2}-\\Delta\\sum_{i=1}^{\\lfloor ck \\rfloor}\\exp\\left(\\frac{\\lambda -\\lfloor ck\\rfloor-\\Delta}{\\Delta}\\right)\\\\\n&{}-\\frac{\\Delta}{e-e^{1-1/\\Delta}}\\exp\\left(\\frac{\\lambda-\\lfloor ck \\rfloor}{\\Delta}\\right).\n\\end{IEEEeqnarray*}\nIf we choose $\\lambda = ck/2$, we can see that the value of the dual function will be positive for large $k$. Hence, for any probability mass function that satisfies Lemma \\ref{lemma:stopping_time}, we get that the value of ${\\rm E} N-\\frac{\\Delta}{\\log e} H(N) \\geq 0$, i.e., $H(N)/{\\rm E} N \\leq \\epsilon$. \\hfill {\\IEEEQEDopen}\n\n\\section{Proof of Claim \\ref{claim:entropy_support_set}}\\label{app:support_arg}\n\nWe first show that atleast one of the components of $\\bm{q}_{m+1}$ is exactly equal to $c$. Pick any $q \\in \\mathcal{Q}_{m+1}$ and arrange its components in nonincreasing order so that $\\bm{q}(m+1)$ is its smallest component. If $\\bm{q}(m+1)>c$, then we can express $\\bm{q}$ as a convex combination of two other elements of $\\mathcal{Q}_{m+1}$ as follows. Note that $\\bm{q}(1)\\geq \\bm{q}(m+1)\\geq c$.\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{q}=\\frac{1}{2}\\begin{bmatrix}\n\\bm{q}(1)+\\bm{q}(m+1)-c\\\\\n\\bm{q}(2)\\\\\n\\vdots\\\\\n\\bm{q}(m)\\\\\nc\n\\end{bmatrix}+\\frac{1}{2}\\begin{bmatrix}\n\\bm{q}(1)-\\bm{q}(m+1)+c\\\\\n\\bm{q}(2)\\\\\n\\vdots \\\\\n\\bm{q}(m)\\\\\n2\\bm{q}(m+1)-c\n\\end{bmatrix}\n\\end{IEEEeqnarray*}\nBy concavity of entropy function we then have that this $\\bm{q} \\neq \\bm{q}_{m+1}$. Thus $\\bm{q}_{m+1}$ must have atleast one component that is equal to $c$, and WLOG let $\\bm{q}_{m+1}=c$. Let $Q$ be a random variable on $[m+1]$ following the probability distribution $\\bm{q}_{m+1}$. Let $I=\\bm{1}_{\\{Q=m+1\\}}$ be the indicator function for the event $\\{Q=m+1\\}$. Then $\\Pr(I=1)=c$ and $\\Pr(I=0)=1-c$. Then we have that\n\\begin{IEEEeqnarray*}{Rl}\nH(Q,I)=&H(Q)=H(I)+H(Q|I)\\\\\n=&H(I)+\\Pr(I=0)H(Q|I=0).\n\\end{IEEEeqnarray*}\nSince $\\Pr(Q|I=0)$ is a probability distribution over $[m]$ and $\\Pr(Q=i|I=0)>\\Pr(Q=i)$ for all $i \\in [m]$, $\\Pr(Q|I=0) \\in \\mathcal{Q}_m$. Hence we have that $H(Q|I=0)\\geq H(\\bm{q}_m)$ and using this in the previous equation, we get\n\\begin{IEEEeqnarray*}{C}\nH(Q)\\geq c\\log \\frac{1}{c}+(1-c)\\log \\frac{1}{1-c}+(1-c)H(\\bm{q}_m).\n\\end{IEEEeqnarray*}\nSince $mc<1$, we have that $-\\log c>\\log m$. But $\\log m$ is the entropy of the uniform distribution over $[m]$ and hence $\\log m \\geq H(\\bm{q}_m)$. Using this in the previous equation we get\n\n", "index": 19, "text": "\\begin{equation*}\nH(Q)\\geq cH(\\bm{q}_m)+(1-c)\\log \\frac{1}{1-c}+(1-c)H(\\bm{q}_m)\\geq H(q_{m}).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"H(Q)\\geq cH(\\bm{q}_{m})+(1-c)\\log\\frac{1}{1-c}+(1-c)H(\\bm{q}_{m})\\geq H(q_{m}).\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc92</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>c</mi></mrow></mfrac></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc92</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\n\\STATE Add the vector $\\bm{v} := (1-\\lambda)\\bm{p}'+\\lambda \\bm{p}'[i\\leftrightarrow j]$ to the list $L$.\n\\STATE Update $\\bm{p}' \\leftarrow \\bm{v}$.\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\nIntuitively, the algorithm finds the difference between $\\bm{p}$ and $\\bm{p}^\\star$ and computes an intermediate vector $\\bm{v}$ that can be written as a convex combination of permuted $\\bm{p}^\\star$'s. Following this, it finds the new difference between $\\bm{v}$ and the target vector $\\bm{p}$ and repeats the previous procedure. Finally, it stops when the intermediate vector equals $\\bm{p}$. The correctness of the algorithm is ensured by the following claims.\n\\begin{claim} \\label{claim:step5}\nLet $m:=\\min \\{\\bm{p}'(i)-\\bm{p}(i), \\bm{p}(j)-\\bm{p}'(j)\\}$. Then, at step 5 in algorithm \\ref{alg:convex_comb},\n\\begin{IEEEeqnarray*}{C}\n\\bm{v}(i)=\\bm{p}'(i)-m,\\;\\text{and}\\; \\bm{v}(j)=\\bm{p}'(j)+m.\n\\end{IEEEeqnarray*} Also $\\bm{v}(k)=\\bm{p}'(k)$ for all $k \\neq i,j$.\n\\end{claim}\n\\begin{IEEEproof}\nSubstituting $\\lambda=m/(\\bm{p}'(i)-\\bm{p}'(j))$ gives the result.\n\\end{IEEEproof}\n\\begin{claim}\\label{claim:step3}\nAt step 3 in algorithm \\ref{alg:convex_comb}, $i<j$ and $\\sum_{k=1}^{u}(\\bm{p}'(k)-\\bm{p}(k))\\geq 0$ for all $u \\geq j$.\n\\end{claim}\n\\begin{IEEEproof}\nWe prove this by induction on the iteration number of the WHILE loop. Suppose the indices $i,j$ found at the $t$th iteration be $i_t,j_t$ and the $\\bm{v}$ evaluated at step 5 be denoted by $\\bm{v}_t$. Then by Claims \\ref{claim:t} and \\ref{claim:step5}, we conclude that $i_1<j_1$ and $\\sum_{k=1}^{u}(\\bm{v}_1(k)-\\bm{p}(k))\\geq 0$ for $u\\geq j_1$. As induction hypothesis, we assume that $i_t<j_t$ and $\\sum_{k=1}^{u}(\\bm{v}_t(k)-\\bm{p}(k))\\geq 0$ for $u \\geq j_t$.\n\nIf at the $t$th iteration, $m=\\bm{p}'(i_t)-\\bm{p}(i_t)$, then by Claim \\ref{claim:step5}, we get that $\\bm{v}_t(i_t)-\\bm{p}(i_t)=0$ and $\\bm{v}_t(j_t)-\\bm{p}(j_t)\\leq 0$. Also $\\bm{v}_t(k)-\\bm{p}(k)=\\bm{p}'(k)-\\bm{p}(k)$ for all $k \\neq i_t,j_t$. This implies that at the $(t+1)$th iteration, $j_{t+1}=j_t$. Furthermore, note that $\\bm{v}_{t+1}$ and $\\bm{v}_t$ differ only at the indices $i_{t+1}$ and $j_{t+1}$. For all $u\\geq j_{t+1}=j_t$\n\n", "itemtype": "equation", "pos": 41791, "prevtext": "\nThe same argument can be repeated to show that $H(\\bm{q}_{m+1})\\geq H(\\bm{q}_{m}) \\geq H(\\bm{q}_{m-1})$ and so on. Hence, any probability mass function that satisfies a lower bound on the values of each of its probability masses necessarily has an equal or larger entropy if it is nonzero over a larger set. \\hfill {\\IEEEQEDopen}\n\n\\section{Proof of Claim \\ref{claim:convex_comb}}\\label{app:convex_comb}\nWe show that any $\\bm{p} \\in $ conv($\\mathcal{P}$) can be written as a convex combination of permuted $\\bm{p}^\\star$'s. Since $\\bm{p} \\in \\text{conv}(\\mathcal{P})$, we can write that $\\bm{p} =\\sum_i\\mu_i\\bm{p}_i$ where each $\\bm{p}_i \\in \\mathcal{P}$, each $\\mu_i \\geq 0$ and $\\sum_i \\mu_i =1$. One can then see that if the claim is true for each $\\bm{p}_i$ then it is also true for $\\bm{p}$. Hence we focus on $\\bm{p} \\in \\mathcal{P}$ and show that above claim holds for it. Without loss of generality, we can assume that the target vector $\\bm{p}$ is arranged in non-increasing order, otherwise we permute its components so that the largest component is the first component and the successive components are in a non-increasing order. We can then reverse the permutation for every vector in its convex combination finally to get back our original vector.\n\nAlgorithm \\ref{alg:convex_comb} returns a list of vectors, each of which can be expressed as a convex combination of permuted $\\bm{p}^\\star$'s. Using this list, we can find the convex combination of permuted $\\bm{p}^\\star$'s for any given $\\bm{p} \\in \\mathcal{P}$ arranged in non-increasing order. The notation $\\bm{p}'[i \\leftrightarrow j]$ indicates the vector $\\bm{p}'$ with its values at the $i$th and $j$th components interchanged, while all the other components remain the same.\n\\begin{algorithm}\n\\caption{Convex combination of permuted $\\bm{p}^\\star$'s for $\\bm{p}$.}\\label{alg:convex_comb}\n\\begin{algorithmic}[1]\n\\REQUIRE $\\bm{p} \\in$ conv($\\mathcal{P}$) arranged in nonincreasing order, $\\bm{p}^\\star$.\n\\ENSURE A list $L$ of vectors.\n\\STATE Initialize $\\bm{p}' \\leftarrow \\bm{p}^\\star, L \\leftarrow \\phi$. $\\bm{v}, \\lambda$ are temporary variables.\n\\WHILE{$\\bm{p}'-\\bm{p}\\neq\\bm{0}$}\n\\STATE Find the smallest indices $i,j$ such that $\\bm{p}'(i)>\\bm{p}(i)$ and $\\bm{p}'(j)<\\bm{p}(j)$.\n\\STATE Evaluate\n\n", "index": 21, "text": "\\begin{equation*}\n\\lambda :=\\frac{\\min \\{\\bm{p}'(i)-\\bm{p}(i), \\bm{p}(j)-\\bm{p}'(j)\\}}{\\bm{p}'(i)-\\bm{p}'(j)}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\lambda:=\\frac{\\min\\{\\bm{p}^{\\prime}(i)-\\bm{p}(i),\\bm{p}(j)-\\bm{p}^{\\prime}(j)%&#10;\\}}{\\bm{p}^{\\prime}(i)-\\bm{p}^{\\prime}(j)}.\" display=\"block\"><mrow><mrow><mi>\u03bb</mi><mo>:=</mo><mfrac><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nSince $\\bm{v}_{t+1}(j_{t+1})=\\bm{v}_t(j_t)$ and $\\bm{v}_t(j_t)-\\bm{p}(j_t)\\leq 0$ it must be true that $i_{t+1}<j_{t+1}$.\n\nOn the other hand, if $m=\\bm{p}(j_t)-\\bm{p}'(j_t)$ at the $t$th iteration, then similarly $\\bm{v}_t(j_t)-\\bm{p}(j_t)=0, \\bm{v}_t(i_t)-\\bm{p}(i_t)\\geq 0$ and $\\bm{v}_t(k)-\\bm{p}(k)=\\bm{p}'(k)-\\bm{p}(k)$ for all $k \\neq i_t,j_t$. This implies that $i_{t+1}=i_t$ and $j_{t+1}>j_t$. Since $i_{t+1}<j_{t+1}$ and these are the only two indices affected at the $(t+1)$th iteration, we have that for all $u \\geq j_{t+1}$\n\n", "itemtype": "equation", "pos": 44085, "prevtext": "\n\\STATE Add the vector $\\bm{v} := (1-\\lambda)\\bm{p}'+\\lambda \\bm{p}'[i\\leftrightarrow j]$ to the list $L$.\n\\STATE Update $\\bm{p}' \\leftarrow \\bm{v}$.\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\nIntuitively, the algorithm finds the difference between $\\bm{p}$ and $\\bm{p}^\\star$ and computes an intermediate vector $\\bm{v}$ that can be written as a convex combination of permuted $\\bm{p}^\\star$'s. Following this, it finds the new difference between $\\bm{v}$ and the target vector $\\bm{p}$ and repeats the previous procedure. Finally, it stops when the intermediate vector equals $\\bm{p}$. The correctness of the algorithm is ensured by the following claims.\n\\begin{claim} \\label{claim:step5}\nLet $m:=\\min \\{\\bm{p}'(i)-\\bm{p}(i), \\bm{p}(j)-\\bm{p}'(j)\\}$. Then, at step 5 in algorithm \\ref{alg:convex_comb},\n\\begin{IEEEeqnarray*}{C}\n\\bm{v}(i)=\\bm{p}'(i)-m,\\;\\text{and}\\; \\bm{v}(j)=\\bm{p}'(j)+m.\n\\end{IEEEeqnarray*} Also $\\bm{v}(k)=\\bm{p}'(k)$ for all $k \\neq i,j$.\n\\end{claim}\n\\begin{IEEEproof}\nSubstituting $\\lambda=m/(\\bm{p}'(i)-\\bm{p}'(j))$ gives the result.\n\\end{IEEEproof}\n\\begin{claim}\\label{claim:step3}\nAt step 3 in algorithm \\ref{alg:convex_comb}, $i<j$ and $\\sum_{k=1}^{u}(\\bm{p}'(k)-\\bm{p}(k))\\geq 0$ for all $u \\geq j$.\n\\end{claim}\n\\begin{IEEEproof}\nWe prove this by induction on the iteration number of the WHILE loop. Suppose the indices $i,j$ found at the $t$th iteration be $i_t,j_t$ and the $\\bm{v}$ evaluated at step 5 be denoted by $\\bm{v}_t$. Then by Claims \\ref{claim:t} and \\ref{claim:step5}, we conclude that $i_1<j_1$ and $\\sum_{k=1}^{u}(\\bm{v}_1(k)-\\bm{p}(k))\\geq 0$ for $u\\geq j_1$. As induction hypothesis, we assume that $i_t<j_t$ and $\\sum_{k=1}^{u}(\\bm{v}_t(k)-\\bm{p}(k))\\geq 0$ for $u \\geq j_t$.\n\nIf at the $t$th iteration, $m=\\bm{p}'(i_t)-\\bm{p}(i_t)$, then by Claim \\ref{claim:step5}, we get that $\\bm{v}_t(i_t)-\\bm{p}(i_t)=0$ and $\\bm{v}_t(j_t)-\\bm{p}(j_t)\\leq 0$. Also $\\bm{v}_t(k)-\\bm{p}(k)=\\bm{p}'(k)-\\bm{p}(k)$ for all $k \\neq i_t,j_t$. This implies that at the $(t+1)$th iteration, $j_{t+1}=j_t$. Furthermore, note that $\\bm{v}_{t+1}$ and $\\bm{v}_t$ differ only at the indices $i_{t+1}$ and $j_{t+1}$. For all $u\\geq j_{t+1}=j_t$\n\n", "index": 23, "text": "\\begin{equation*}\n\\sum_{k=1}^{u}(\\bm{v}_{t+1}(k)-\\bm{p}(k))=\\sum_{k=1}^{j_t}(\\bm{v}_t(k)-\\bm{p}(k))\\geq 0.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\sum_{k=1}^{u}(\\bm{v}_{t+1}(k)-\\bm{p}(k))=\\sum_{k=1}^{j_{t}}(\\bm{v}_{t}(k)-\\bm%&#10;{p}(k))\\geq 0.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc97</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>j</mi><mi>t</mi></msub></munderover><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc97</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": " as $u \\geq j_{t+1}>j_t$. This completes the induction step and proves our claim.\n\\end{IEEEproof}\n\\begin{claim}\nAt step 4 of the algorithm, $\\lambda \\in [0,1]$.\n\\end{claim}\\label{claim:step4}\n\\begin{IEEEproof}\nBy definition of the indices $i,j$, we have that $\\bm{p}'(i)>\\bm{p}(i)$ and $\\bm{p}(j)>\\bm{p}'(j)$. From Claim \\ref{claim:step3}, we have that $i<j$. Also, by assumption, $\\bm{p}$ is arranged in nonincreasing order. This implies that $\\bm{p}(i)\\geq \\bm{p}(j)$. That gives the following string of inequalities\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nSince $\\bm{v}_{t+1}(j_{t+1})=\\bm{v}_t(j_t)$ and $\\bm{v}_t(j_t)-\\bm{p}(j_t)\\leq 0$ it must be true that $i_{t+1}<j_{t+1}$.\n\nOn the other hand, if $m=\\bm{p}(j_t)-\\bm{p}'(j_t)$ at the $t$th iteration, then similarly $\\bm{v}_t(j_t)-\\bm{p}(j_t)=0, \\bm{v}_t(i_t)-\\bm{p}(i_t)\\geq 0$ and $\\bm{v}_t(k)-\\bm{p}(k)=\\bm{p}'(k)-\\bm{p}(k)$ for all $k \\neq i_t,j_t$. This implies that $i_{t+1}=i_t$ and $j_{t+1}>j_t$. Since $i_{t+1}<j_{t+1}$ and these are the only two indices affected at the $(t+1)$th iteration, we have that for all $u \\geq j_{t+1}$\n\n", "index": 25, "text": "\\begin{equation*}\n\\sum_{k=1}^{u}(\\bm{v}_{t+1}(k)-\\bm{p}(k))=\\sum_{k=1}^{u}(\\bm{v}_t(k)-\\bm{p}(k))\\geq 0\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\sum_{k=1}^{u}(\\bm{v}_{t+1}(k)-\\bm{p}(k))=\\sum_{k=1}^{u}(\\bm{v}_{t}(k)-\\bm{p}(%&#10;k))\\geq 0\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc97</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc97</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": "\nThis implies that $\\lambda \\geq 0$. In addition it also implies that\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{p}(i)\\geq \\bm{p}'(j) \\Leftrightarrow &\\bm{p}'(i)-\\bm{p}(i)\\leq \\bm{p}'(i)-\\bm{p}'(j),\\\\\n\\bm{p}(j)\\leq \\bm{p}'(i) \\Leftrightarrow &\\bm{p}(j)-\\bm{p}'(j)\\leq \\bm{p}'(i)-\\bm{p}'(j).\n\\end{IEEEeqnarray*} These imply that $\\lambda \\leq 1$.\n\\end{IEEEproof}\nThe above claims conclude that $\\bm{v}$ is a convex combination of vectors from conv($\\mathcal{P}$). In the following claim we prove that $\\bm{v}$ finally converges to $\\bm{p}$ in a bounded number of steps.\n\\begin{claim}\nThe WHILE loop in algorithm \\ref{alg:convex_comb} terminates after a bounded number of iterations.\n\\end{claim}\n\\begin{IEEEproof}\nFrom calculations in Claim \\ref{claim:step3} we concluded that at the end of the $t$th iteration, either $\\bm{p}'(i_t)=\\bm{p}(i_t)$ and/or $\\bm{p}'(j_t)=\\bm{p}(j_t)$ based on the value of $m$. Thus $\\bm{p}'-\\bm{p}$ will have a zero element at atleast one of $i_t$ or $j_t$. Also, none of the indices for which the difference $\\bm{p}'-\\bm{p}$ is already zero are affected by the algorithm as the inequality at step 3 is strict. Thus, the number of zero elements in the vector $\\bm{p}'-\\bm{p}$ increases by atleast one in every successive iteration. Since there are a finite number of components, it finally stops when the difference is the zero vector.\n\\end{IEEEproof}\n\n\\section{}\\label{app:clumpy_entropy}\nNote that expression for entropy in equation \\eqref{eq:clumpy_entropy} can be expressed as follows\n\\begin{IEEEeqnarray}{Rl}\nH(\\bm{p}^\\star)=&\\frac{1}{M_{x,y}}\\sum_{j=1}^{x+y+1}\\lceil 2^{x+y-j}\\rceil \\left(\\sum_{s=0}^{j-1}\\right)\\binom{x+y}{s}B(x,y,j) \\IEEEeqnarraynumspace \\label{eq:lower_bd_expr_A}\n\\end{IEEEeqnarray} where\n\n", "itemtype": "equation", "pos": 45382, "prevtext": " as $u \\geq j_{t+1}>j_t$. This completes the induction step and proves our claim.\n\\end{IEEEproof}\n\\begin{claim}\nAt step 4 of the algorithm, $\\lambda \\in [0,1]$.\n\\end{claim}\\label{claim:step4}\n\\begin{IEEEproof}\nBy definition of the indices $i,j$, we have that $\\bm{p}'(i)>\\bm{p}(i)$ and $\\bm{p}(j)>\\bm{p}'(j)$. From Claim \\ref{claim:step3}, we have that $i<j$. Also, by assumption, $\\bm{p}$ is arranged in nonincreasing order. This implies that $\\bm{p}(i)\\geq \\bm{p}(j)$. That gives the following string of inequalities\n\n", "index": 27, "text": "\\begin{equation*}\n\\bm{p}'(i)>\\bm{p}(i)\\geq \\bm{p}(j)>\\bm{p}'(j).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\bm{p}^{\\prime}(i)&gt;\\bm{p}(i)\\geq\\bm{p}(j)&gt;\\bm{p}^{\\prime}(j).\" display=\"block\"><mrow><mrow><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>\ud835\udc91</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><msup><mi>\ud835\udc91</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07228.tex", "nexttext": " For a lower bound, we ignore the ceiling and simplify parts of the RHS of equation \\eqref{eq:lower_bd_expr_A} as follows.\n\n\\begin{IEEEeqnarray*}{Rl}\n&\\sum_{j=1}^{x+y+1}2^{-j}\\sum_{t=0}^{j-1}\\binom{x+y}{t}\\\\\n=&\\frac{1}{2}\\binom{x+y}{0}+\\frac{1}{2^2}\\left[\\binom{x+y}{0}+\\binom{x+y}{1}\\right]+\\hdots\\\\\n&{}+\\frac{1}{2^{x+y+1}}\\left[ \\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}\\right],\\\\\n=&\\binom{x+y}{0}\\left[\\frac{1}{2}+\\frac{1}{2^2}+\\hdots+\\frac{1}{2^{x+y+1}}\\right]\\\\\n&{}+\\:\\binom{x+y}{1}\\left[\\frac{1}{2^2}+\\hdots+\\frac{1}{2^{x+y+1}}\\right]+\\hdots \\\\\n&{}+\\binom{x+y}{x+y}\\frac{1}{2^{x+y+1}},\\\\\n=&\\binom{x+y}{0}\\left[\\frac{1}{2^0}-\\frac{1}{2^{x+y+1}}\\right]+\\binom{x+y}{1}\\left[\\frac{1}{2^1}-\\frac{1}{2^{x+y+1}}\\right]\\\\\n&{}+\\hdots+\\binom{x+y}{x+y}\\left[\\frac{1}{2^{x+y}}-\\frac{1}{2^{x+y+1}}\\right],\\\\\n=&\\binom{x+y}{0}\\frac{1}{2^0}+\\binom{x+y}{1}\\frac{1}{2^1}+\\hdots+\\binom{x+y}{x+y}\\frac{1}{2^{x+y}}\\\\\n&{}-\\left[\\binom{x+y}{0}+\\binom{x+y}{1}+\\hdots+\\binom{x+y}{x+y}\\right]\\frac{1}{2^{x+y+1}},\\\\\n=&\\left(\\frac{3}{2}\\right)^{x+y}-\\frac{1}{2}.\n\\end{IEEEeqnarray*}\nWe also have that\n\\begin{IEEEeqnarray*}{Rl}\n&\\sum_{j=1}^{x+y+1}2^{-j}\\left(\\sum_{s=0}^{j-1}\\binom{x+y}{s}\\right)\\log \\sum_{t=0}^{j-1}\\binom{x+y}{t}\\\\\n=&\\frac{1}{2}\\binom{x+y}{0}\\log\\binom{x+y}{0}\\\\\n&{}+\\frac{1}{2^2}\\left[\\binom{x+y}{0}+\\binom{x+y}{1}\\right]\\log\\left[\\binom{x+y}{0}+\\binom{x+y}{1}\\right]\\\\\n&{}+\\hdots +\\frac{1}{2^{x+y+1}}\\left[\\sum_{s=0}^{x+y}\\binom{x+y}{s}\\right]\\log \\sum_{t=0}^{x+y}\\binom{x+y}{t},\\\\\n=&\\binom{x+y}{0}\\Bigg(\\frac{1}{2}\\log\\binom{x+y}{0}+\\frac{1}{2^2}\\log\\left[\\binom{x+y}{0}+\\binom{x+y}{1}\\right]\\\\\n&{}\\hfill +\\hdots+\\frac{1}{2^{x+y+1}}\\log \\left[\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}\\right]\\Bigg)\\\\\n&{}+\\binom{x+y}{1}\\Bigg(\\frac{1}{2^2}\\log\\left[ \\binom{x+y}{0}+\\binom{x+y}{1}\\right]+\\hdots\\\\\n&{}\\hfill +\\frac{1}{2^{x+y+1}}\\log\\left[\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}\\right]\\Bigg)\\\\\n&{}+\\hdots +\\binom{x+y}{x+y}\\frac{1}{2^{x+y+1}}\\left[\\binom{x+y}{0}+\\hdots+\\binom{x+y}{x+y}\\right],\\\\\n<&\\binom{x+y}{0}\\Bigg[\\frac{1}{2}\\log 2^{x+y}+\\frac{1}{2^2}\\log 2^{x+y}\\\\\n&{}\\hspace{120pt} +\\hdots+\\frac{1}{2^{x+y+1}}\\log 2^{x+y}\\Bigg]\\\\\n&{}+\\binom{x+y}{1}\\left[\\frac{1}{2^2}\\log 2^{x+y}+\\hdots+\\frac{1}{2^{x+y+1}}\\log 2^{x+y}\\right]\\\\\n&{}+\\hdots+\\binom{x+y}{x+y}\\frac{1}{2^{x+y+1}}\\log 2^{x+y},\\\\\n=&\\binom{x+y}{0}(x+y)\\left[\\frac{1}{2^0}-\\frac{1}{2^{x+y+1}}\\right]\\\\\n&{}+\\binom{x+y}{1}(x+y)\\left[\\frac{1}{2^1}-\\frac{1}{2^{x+y+1}}\\right] +\\hdots\\\\\n&{}+\\binom{x+y}{x+y}(x+y)\\left[\\frac{1}{2^{x+y}}-\\frac{1}{2^{x+y+1}}\\right],\\\\\n=&(x+y)\\left[\\left(\\frac{3}{2}\\right)^{x+y}-\\frac{1}{2}\\right].\n\\end{IEEEeqnarray*}\n\n\n\nPutting the parts together, we get that\n\\begin{IEEEeqnarray*}{Rl}\nH(\\bm{p}^\\star)\\geq &\\frac{(x+y)L_{x,y}}{M_{x,y}}(-1+\\log 3)\\left[ \\left( \\frac{3}{2}\\right)^{x+y}-\\frac{1}{2}\\right].\n\\end{IEEEeqnarray*}\n\\section{}\\label{app:average_clumpy_entropy}\n\nWe use the bound obtained in Appendix \\ref{app:clumpy_entropy} for the entropy of the clumpy distribution to obtain the following.\n\\begin{IEEEeqnarray*}{l}\nH(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})=\\sum_{\\bm{\\sigma}}\\Pr(\\bm{\\Sigma}=\\bm{\\sigma})H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma}=\\bm{\\sigma})\\\\\n\\geq \\sum_{\\begin{matrix}\nx,y=0\\\\\nx+y \\leq k\n\\end{matrix}}^{k}\\Bigg(\\frac{k!2^{k-x-y}}{x!y!(k-x-y)!}\\left(\\frac{1}{8}\\right)^{k-x-y}\\left(\\frac{3}{8} \\right)^{x+y}\\\\\n\\hspace{60pt} \\times \\frac{(x+y)(-1+\\log 3)L_{x,y}}{M_{x,y}}\\left[\\left(\\frac{3}{2}\\right)^{x+y}-\\frac{1}{2}\\right]\\Bigg),\\\\\n\\geq \\sum_{\\begin{matrix}\nx,y=0\\\\\nx+y \\leq k\n\\end{matrix}}^{k}\\frac{k!(\\log 3-1)(x+y)}{x!y!(k-x-y)!4^k}\\left[\\left(\\frac{3}{2}\\right)^{x+y}-\\frac{1}{2}\\right],\\\\\n=\\frac{1}{4^k}\\left[\\sum_{\\begin{matrix}\nx,y=0\\\\\nx+y \\leq k\n\\end{matrix}}^{k}\\frac{k!(x+y)(\\log 3-1)}{x!y!(k-x-y)!}\\left(\\frac{3}{2}\\right)^{x+y}\\right.\\\\\n\\hfill \\left. -\\sum_{\\begin{matrix}\nx,y=0\\\\\nx+y \\leq k\n\\end{matrix}}^{k}\\frac{k!(\\log 3-1)(x+y)}{x!y!(k-x-y)!2}\\right]\\\\\n=\\frac{\\log 3-1}{4^k}\\left[2.\\frac{3}{2}.k.4^{k-1}-\\frac{1}{2}.2.k3^{k-1} \\right],\\\\\n=\\frac{\\log 3-1}{4^k}\\left[\\frac{3k}{4}4^k-\\frac{k}{3}3^k\\right],\\\\\n\\implies \\frac{H(\\bm{Z}_1^N,\\bm{Z}_2^N|\\bm{\\Sigma})}{k}\\rightarrow 0.75(\\log 3-1) ~\\text{as}~ k \\rightarrow \\infty. \n\\end{IEEEeqnarray*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThis implies that $\\lambda \\geq 0$. In addition it also implies that\n\\begin{IEEEeqnarray*}{Rl}\n\\bm{p}(i)\\geq \\bm{p}'(j) \\Leftrightarrow &\\bm{p}'(i)-\\bm{p}(i)\\leq \\bm{p}'(i)-\\bm{p}'(j),\\\\\n\\bm{p}(j)\\leq \\bm{p}'(i) \\Leftrightarrow &\\bm{p}(j)-\\bm{p}'(j)\\leq \\bm{p}'(i)-\\bm{p}'(j).\n\\end{IEEEeqnarray*} These imply that $\\lambda \\leq 1$.\n\\end{IEEEproof}\nThe above claims conclude that $\\bm{v}$ is a convex combination of vectors from conv($\\mathcal{P}$). In the following claim we prove that $\\bm{v}$ finally converges to $\\bm{p}$ in a bounded number of steps.\n\\begin{claim}\nThe WHILE loop in algorithm \\ref{alg:convex_comb} terminates after a bounded number of iterations.\n\\end{claim}\n\\begin{IEEEproof}\nFrom calculations in Claim \\ref{claim:step3} we concluded that at the end of the $t$th iteration, either $\\bm{p}'(i_t)=\\bm{p}(i_t)$ and/or $\\bm{p}'(j_t)=\\bm{p}(j_t)$ based on the value of $m$. Thus $\\bm{p}'-\\bm{p}$ will have a zero element at atleast one of $i_t$ or $j_t$. Also, none of the indices for which the difference $\\bm{p}'-\\bm{p}$ is already zero are affected by the algorithm as the inequality at step 3 is strict. Thus, the number of zero elements in the vector $\\bm{p}'-\\bm{p}$ increases by atleast one in every successive iteration. Since there are a finite number of components, it finally stops when the difference is the zero vector.\n\\end{IEEEproof}\n\n\\section{}\\label{app:clumpy_entropy}\nNote that expression for entropy in equation \\eqref{eq:clumpy_entropy} can be expressed as follows\n\\begin{IEEEeqnarray}{Rl}\nH(\\bm{p}^\\star)=&\\frac{1}{M_{x,y}}\\sum_{j=1}^{x+y+1}\\lceil 2^{x+y-j}\\rceil \\left(\\sum_{s=0}^{j-1}\\right)\\binom{x+y}{s}B(x,y,j) \\IEEEeqnarraynumspace \\label{eq:lower_bd_expr_A}\n\\end{IEEEeqnarray} where\n\n", "index": 29, "text": "\\begin{equation*}\nB(x,y,j)=(x+y)\\log 3-\\log \\sum_{t=0}^{j-1}\\binom{x+y}{t}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"B(x,y,j)=(x+y)\\log 3-\\log\\sum_{t=0}^{j-1}\\binom{x+y}{t}.\" display=\"block\"><mrow><mrow><mrow><mi>B</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mn>3</mn></mrow></mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow><mi>t</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]