[{"file": "1601.00114.tex", "nexttext": "  gives the minimum support (MS) stabilizer which minimizes the total volume of nonzero departure of the model parameters from the given prior model, \\cite{PoZh:99}.  A further modification introduced in  \\cite{PoZh:99}  uses the gradient of the model parameters in the stabilization term via $ S({\\mathbf{m}})=\\| D_{\\epsilon}(\\nabla {\\mathbf{m}}) | \\nabla {\\mathbf{m}}| \\|_2^2$, where $ D_{\\epsilon}(\\nabla {\\mathbf{m}})$ is defined by ${\\mathrm{diag}}(1/(|\\nabla{\\mathbf{m}}|^2 +\\epsilon^2)^{1/2})$, yielding the  minimum gradient support (MGS) stabilizer. This constraint minimizes the volume over which the gradient of the model parameters is nonzero, and thus yields models with sharp boundaries.  \n\n\nAnother possibility for the reconstruction of sparse models  is to use a stabilizer which minimizes the $L_{1}$-norm of the model or gradient, in this case known as the total variation (TV),  of the model parameters \\cite{FaOl:98,Far:2008,Loke:2003}.   The $L_{1}$-norm stabilizer permits occurrence of large elements in the inverted model among mostly small values and can, therefore, be used to obtain models with non-smooth properties \\cite{SunLi:2014}. Although the  $L_{1}$-norm stabilizer has favorable properties,  and yields a convex functional that can be solved by linear programming algorithms, its use for the solution of large scale problems is not feasible. Here we implement the $L_{1}$-norm stabilizer  using the approximation based on an IRLS algorithm, \\cite{BrDoEl:2009}, and extend the algorithm for the gravity inverse problem by including the  depth weighting and prior model information. Further, we use  the  weighting matrix similar to that used for the  MS stabilizer but defined by \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\\begin{summary}\nSparse inversion of the large scale gravity  problem is considered. The $L_{1}$-type stabilizer reconstructs models with sharp boundaries and blocky features,  and is implemented here using  an iteratively reweighted $L_{2}$-norm (IRLS). The resulting large scale regularized least squares problem at each iteration is solved on the projected subspace obtained using Golub-Kahan iterative bidiagonalization applied to the large scale linear system of equations. The regularization parameter for the IRLS problem is estimated using the  unbiased predictive risk estimator  (UPRE) extended for the projected problem. Further analysis leads to an improvement of the projected UPRE via analysis based on truncation of the projected spectrum. Numerical results for synthetic examples and real field data demonstrate the efficiency of the method.\n\\end{summary}\n\n\\begin{keywords}\nInverse theory; Numerical approximation and analysis; Gravity anomalies and Earth structure; Asia\n\\end{keywords}\n\n\\section{Introduction}\\label{sec:intro}\nThe gravity data inversion problem is the estimation of the unknown subsurface density and its geometry from a set of  gravity observations measured on the surface. This is a challenging problem for several reasons: Foremost of these is the non-uniqueness of the problem, there are fewer observations than the number of model parameters yielding algebraic ambiguity, but also by Gauss's theorem non-uniqueness arises due to the  physics of the problem, \\cite{LiOl:96}. Further, the data  are always contaminated with noise, which, with the ill-conditioning of the model, leads to sensitivity of the solution to the noise and to the numerical algorithm for finding the solution. Thus, the inversion of gravity data is an example of an under-determined and ill-posed problem, for which  a stable and geologically plausible solution is feasible only with the imposition of additional information on the model. Here we consider the minimization of a global \nobjective function consisting of data misfit, $\\Phi({\\mathbf{m}})$,  and stabilizing regularization, $S({\\mathbf{m}})$, terms, with relative weighting determined by regularization parameter $\\alpha$, \n\\begin{eqnarray}\\label{globalfunction}\nP^{\\alpha}({\\mathbf{m}})=\\Phi({\\mathbf{m}})+\\alpha^2 S({\\mathbf{m}}).\n\\end{eqnarray}\nData misfit $ \\Phi({\\mathbf{m}})$ measures how well an obtained model, ${\\mathbf{m}}$,  reproduces the observed data, ${\\mathbf{d}_{\\mathrm{obs}}}$. For gravity data it is standard to  assume that the noise in the data is uncorrelated and Gaussian, although it arises due to several sources such as untreated instrumental or geologic noise. Assuming that the standard deviation of the noise in the data is known, then a weighted $L_{2}$ measure of the error\nbetween the observed and the predicted data is used giving  $\\Phi({\\mathbf{m}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{m}}-{\\mathbf{d}_{\\mathrm{obs}}}) \\|_2^2$, where  $W_{{\\mathbf{d}}}$ is a diagonal  matrix approximating the inverse of the diagonal standard deviation matrix of the data.  \nWe note that throughout we use the standard definition  $\\|{\\mathbf{x}} \\|_{p}$ to be the $L_{p}$-norm of vector ${\\mathbf{x}}$, given by $\\|{\\mathbf{x}} \\|_{p}=(\\sum_{i=1}^n |x_{i}|^{p})^{\\frac{1}{p}}, p\\geq 1 $.\n\n\nThere are several choices for the stabilizer, $S({\\mathbf{m}})$, depending on the type of features one wants to see from the inverted model. A typical choice for geophysical applications  is given by $ S({\\mathbf{m}})=\\| W_{{\\mathbf{m}}}({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}}) \\| _2^2$ in which ${\\mathbf{m}_{\\mathrm{apr}}}$ may be some prior information on the model, and $W_{{\\mathbf{m}}}$  is an augmented matrix of spatially dependent weighting matrices, including potentially a depth weighting matrix and matrices that approximate low order derivative operators in each dimension, \\cite[(4)]{LiOl:96}. Then (\\ref{globalfunction}) involves two quadratic terms, and,  supposing that the null spaces of both $G$ and $W_{{\\mathbf{m}}}$ intersect only trivially, the unique minimizer  of $P^{\\alpha}({\\mathbf{m}})$ is obtained as the solution of a linear system, the \\textit{normal equations}, for \\eqref[globalfunction].  Although this type of inversion has been used successfully in much geophysical literature, models recovered in this way are characterized by smooth features, especially blurred boundaries, which are not always consistent with   real geological structures \\cite{Far:2008}. There are situations in which the sources are localized and separated by sharp, distinct interfaces, requiring alternative approaches. In the geophysical community, Last $\\&$ Kubik \\shortcite{LaKu:83} presented a compactness criterion for gravity inversion that seeks to minimize the area (or volume in 3D) of the causative body. In this case, ${\\mathbf{m}_{\\mathrm{apr}}}=0$ and $W_{{\\mathbf{m}}}({\\mathbf{m}}) ={W_{\\epsilon}}({\\mathbf{m}})={\\mathrm{diag}}(1/({\\mathbf{m}}^2+\\epsilon^2)^{1/2})$,\n for small $\\epsilon>0$, i.e. $ S({\\mathbf{m}})= \\|{W_{\\epsilon}}({\\mathbf{m}}){\\mathbf{m}}   \\|_2^2   $. Now, (\\ref{globalfunction}) is a non-linear function of ${\\mathbf{m}}$, and the model-space iteratively reweighted least square (IRLS) algorithm is used to solve the problem. At each iteration $k$, ${\\mathbf{m}}^{(k)}$ is obtained using  the  weighting matrix $ {W_{\\epsilon}}^{(k)}({\\mathbf{m}}) $ calculated using ${\\mathbf{m}}^{(k-1)}$, where for any variable the superscript $^{(k)}$ denotes that variable at iteration $k$, and here we assume ${W_{\\epsilon}}^{(1)}({\\mathbf{m}})=I$. \nIncorporating  ${\\mathbf{m}_{\\mathrm{apr}}}$ into the stabilizer, i.e.  $ S({\\mathbf{m}})= \\|{W_{\\epsilon}}({\\mathbf{m}})({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}})  \\|_2^2   $ with now $ {W_{\\epsilon}}({\\mathbf{m}})$ defined by\n\n", "index": 1, "text": "\\begin{equation}\\label{We}\n{W_{\\epsilon}}({\\mathbf{m}}) = {\\mathrm{diag}}(1/(({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}})^2+\\epsilon^2)^{1/2}), \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{W_{\\epsilon}}({\\mathbf{m}})={\\mathrm{diag}}(1/(({\\mathbf{m}}-{\\mathbf{m}_{%&#10;\\mathrm{apr}}})^{2}+\\epsilon^{2})^{1/2}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mi>\u03f5</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc26</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>diag</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc26</mi><mo>-</mo><msub><mi>\ud835\udc26</mi><mi>apr</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00114.tex", "nexttext": " where we point to the use of fourth root in the denominator rather than square root in \\eqref[We].  Our results will compare these choices.  \n\nMany techniques have been developed to estimate a suitable regularization parameter, $\\alpha$  in (\\ref{globalfunction}),  including the L-curve (LC) \\cite{Hansen:92}, and Generalized Cross Validation (GCV) \\cite{GHW:1979,Mar:1970}, and methods which assume some  knowledge of the noise level of the data, including the $ \\chi^2$-discrepancy principle \\cite{MeRe:09,RHM:2010,VRA:2014b}, the unbiased predictive risk estimator (UPRE) \\cite{Vogel:2002} and the Morozov discrepancy principle (MDP) \\cite{Morozov:66}. Our previous investigation for the gravity inverse problem, see Vatankhah et al \\shortcite{VAR:2015}, has demonstrated that the UPRE parameter-choice method provides a good estimate for the Tikhonov regularization parameter especially for high noise levels. Therefore, here, we use the UPRE for estimation of the regularization parameter $\\alpha$, extending the approach in \\cite{RVA:2015}.\n\nFor small-scale problems involving two quadratic terms the solution of   \\eqref[globalfunction]  may be found efficiently using the generalized singular value decomposition (GSVD) or singular value decomposition (SVD), dependent on the choice for ${W_{{\\mathbf{m}}}}$. In addition, these factorizations  present  regularization parameter-choice methods in computationally convenient forms \\cite{VRA:2014b}. For large-scale problems these factorizations are generally computationally impractical and an alternative approach is to find the solution  through projection of the problem to a smaller subspace, on which it may then be feasible to use the factorization for efficient estimation of the regularization parameter. Here we  use  the Golub-Kahan iterative bidiagonalization projection of the solution which is  based on the LSQR Krylov methods introduced in Paige $\\&$ Saunders \\shortcite{PaSa:1982a,PaSa:1982b}. Then,  effective parameter estimation techniques, that are useful in the context of   efficient iterative Krylov-based procedures, must be developed. Chung et al. presented  the weighted GCV  for the projected problem which requires the use of an additional solution dependent weighting parameter, \\cite{ChNaOl:2008}. Here we focus our discussion on the   UPRE in conjunction with the solution of the projected problem, additionally extending the approach introduced in \\cite{RVA:2015}.\n\nThe outline of this paper is as follows. In Section~\\ref{l1method} we review the inversion algorithm and transformation of the $L_{1}$-norm stabilizer into the standard form Tikhonov functional. Furthermore, the development of  the Tikhonov regularization functional based on the Golub-Kahan iterative bidiagonalization is given in Section~\\ref{GKB}. Section ~\\ref{parameter estimation} is devoted to discussion of parameter estimation, the derivation of the UPRE is presented in Section~\\ref{upre} with new work showing its extension for usage with the projected problem in Section~\\ref{projectedupre}. Results for synthetic examples are illustrated in Section~\\ref{synthetic}. The reconstruction of an embedded cube with high density within a homogeneous medium is  used for contrasting the algorithms using the SVD, Section~\\ref{cube:alg1} and the LSQR algorithm, Section~\\ref{cube:alg2}. These results demonstrate the need to use the truncated UPRE which is introduced and applied in Section~\\ref{cube:tupre}. The reconstruction of a more complex structure using the TUPRE is presented in Section~\\ref{multiplebodies}. These simulations are concluded with the contrast of the MS and $L_1$ stabilizers in Section~\\ref{contrastL1MS}. The approach is applied on gravity data acquired over\na hematite mine located in the southeast of Iran and the results are shown in Section~\\ref{real}. Conclusions and future work follow in Section~\\ref{conclusion}.\n\n\\section{ $L_{1}$ Inversion methodology }\\label{l1method}\nWe briefly review the well-known approach for the  standard $3$D inversion of gravity data.  The subsurface volume is discretized using a set of cubes, in which the cells size are kept fixed during the inversion, and the values of densities at the cells  are the model parameters to be determined in the inversion \\cite{BoCh:2001,LiOl:98}. Thus, we can introduce a vector ${\\mathbf{m}}$ of the unknown model parameters, here the density of each cell $\\rho_{j}$, such that ${\\mathbf{m}}=(\\rho_{1}, \\rho_{2},\\dots, \\rho_{n}) \\in {\\mathcal{R}^{n}}$ and a vector ${\\mathbf{d}_{\\mathrm{obs}}} \\in {\\mathcal{R}^{m}}$ which contains the measured data. Then, the gravity data satisfy the  rectangular underdetermined linear system\n\\begin{eqnarray}\\label{d=gm}\n{\\mathbf{d}_{\\mathrm{obs}}}= G{\\mathbf{m}}.\n\\end{eqnarray} \nPractically, ${\\mathbf{d}_{\\mathrm{obs}}}={\\mathbf{d}_{\\mathrm{exact}}}+{\\mbox{\\boldmath{$\\eta$}}} $, where ${\\mathbf{d}_{\\mathrm{exact}}}$ is the unknown exact data and ${\\mbox{\\boldmath{$\\eta$}}} \\in {\\mathcal{R}^{m}}$ represents the error in the measurements, assumed to be Gaussian and uncorrelated. The matrix $G \\in {\\mathcal{R}^{m \\times n}}, m \\ll n$, is the matrix resulting from the discretization of the forward operator which maps from the model space to the data space. Given $G$, the goal of the gravity inverse problem is to find a stable and geologically plausible density model ${\\mathbf{m}}$ that reproduces ${\\mathbf{d}_{\\mathrm{obs}}}$ at the noise level.\n\nAs discussed in section~\\ref{sec:intro}, solving problem (\\ref{d=gm}) is challenging due to the ill-posed nature of the problem and regularization is required to stabilize the solution. Furthermore, depth weighting and prior model information should be included in the formulation. Rewriting (\\ref{d=gm}) via ${\\mathbf{d}_{\\mathrm{obs}}} -G{\\mathbf{m}_{\\mathrm{apr}}}= G{\\mathbf{m}} -G{\\mathbf{m}_{\\mathrm{apr}}}$ and then \nintroducing the residual and discrepancy from the background data, using  ${\\mathbf{r}}={\\mathbf{d}_{\\mathrm{obs}}}-G{\\mathbf{m}_{\\mathrm{apr}}} $ and $ {\\mathbf{y}}={\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}}$, respectively, for which it is immediate that ${\\mathbf{r}}= G{\\mathbf{y}}$,\nwe obtain the functional for ${\\mathbf{y}}$, \n\\begin{eqnarray}\\label{globalfunction2}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {\\mathbf{y}} \\|_1,\n\\end{eqnarray}\nhere introducing the $L_{1}$ regularization term. Assuming  minimization of (\\ref{globalfunction2}) to give ${\\mathbf{y}}(\\alpha)$,  the model is updated by\n\n${\\mathbf{m}}(\\alpha)={\\mathbf{m}_{\\mathrm{apr}}} + {\\mathbf{y}}(\\alpha)$. \n\nIn the IRLS algorithm $\\|{\\mathbf{y}}\\|_1$ is approximated as follows. We first note, following for example \\cite{Voronin:2012,WoRo:07},  that \n$|y_{i}|={y_{i}^2}/{\\sqrt{y_{i}^2}}$ can be approximated by  ${y_{i}^2}/{\\sqrt{y_{i}^2 +\\epsilon^2}}$ for small and positive $\\epsilon$. Thus \n\\begin{eqnarray*}\n\\|{\\mathbf{y}} \\|_1 \\approx \\sum_{i=1}^n{ \\frac{y_{i}^2}{\\sqrt{y_{i}^2 +\\epsilon^2}}} = \\sum_{i=1}^n ({W_{{L}_1}})_{ii}^2 \\,y_i^2 = \\|{W_{{L}_1}}({\\mathbf{y}}){\\mathbf{y}}\\|^2, \\,\\, \\textrm{for}\\,\\, \\left({W_{{L}_1}}({\\mathbf{y}})\\right)_{ii}=\\frac{1}{(y_{i}^2+\\epsilon^2)^{1/4}}, \n\\end{eqnarray*}\nand (\\ref{globalfunction2}) is replaced by the approximating differentiable functional with diagonal matrix ${W_{{L}_1}}({\\mathbf{y}})$, \n\\begin{eqnarray}\\label{globalfunction3}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {W_{{L}_1}}({\\mathbf{y}}){\\mathbf{y}} \\|_2^2, \\quad {\\mathbf{y}}=({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}}).\n\\end{eqnarray}\n\n\nNow  the close relationship between the MS and $L_{1}$-norm stabilizers is clear, the difference being the fractional root, as is seen by introducing \n\\begin{eqnarray}\\label{generalnorm}\nS_p({\\mathbf{x}})=\\sum_{i=1}^n e_{p}(x_i) \\quad \\mathrm{where} \\quad e_{p}(x)= \\frac{x^2}{ (x^2+\\epsilon^2)^{\\frac{2-p}{2}} }.\n\\end{eqnarray}\nWhen $\\epsilon$ is sufficiently small, (\\ref{generalnorm}) yields the approximation of  the $L_p$ norm for $p=2$ and $p=1$,  while the case with $p=0$ corresponds to the compactness constraint used in Last $\\&$ Kubik \\shortcite{LaKu:83}. $S_0({\\mathbf{x}})$  does not meet the mathematical requirement to be regarded as a norm, and is  commonly used to denote the number of nonzero entries in ${\\mathbf{x}}$. Fig.~\\ref{fig1} demonstrates the impact of the choice of $\\epsilon$ on $e_{p}(x)$ for $\\epsilon = 1e^{-9}$ , Fig.~\\ref{1a}, and $\\epsilon=0.5$, Fig.~\\ref{1b}. For larger $p$, more weight is imposed on large  elements of ${\\mathbf{x}}$, large elements will be penalized more heavily than small elements during minimization \\cite{SunLi:2014}. Hence  $L_2$  tends to discourage the occurrence of large elements in the inverted model, yielding smooth models, while  $L_1$ and $L_0$  allow large elements leading to the recovery of data with blocky features. Further, the $L_0$-norm is non-quadratic and $e_{0}(x)$ asymptotes to one away from $0$, regardless of the magnitude of $x$. Hence the penalty on the model parameters does not depend on their relative magnitude, only on whether or not they lie above or below a threshold dependent on $\\epsilon$ \\cite{Ajo:2007}. Further,  $L_1$  does not necessarily lead to a very sparse solution and thus  $L_0$ is better for preserving sparsity. On the other hand, as compared to $L_1$,  the disadvantage of $L_0$ is the  greater dependency  on the choice of $\\epsilon$, so that it is less robust than $L_1$. \n\\begin{figure*}\n\\subfigure{\\label{1a}\\includegraphics[width=.4\\textwidth]{figure1a.pdf}}\n\\subfigure{\\label{1b}\\includegraphics[width=.4\\textwidth]{figure1b.pdf}}\n\\caption {Illustration of different norms for two values of parameter $\\epsilon$.  (a) $\\epsilon=1e{-9}$; (b) $\\epsilon=0.5$.} \\label{fig1}\n\\end{figure*}\n\nNow, incorporating diagonal matrix ${W_{\\mathrm{z}}}$, where we generally assume that this is a depth weighting matrix,  into (\\ref{globalfunction3}), see Li $\\&$ Oldenburg \\shortcite{LiOl:98}, yields\n\\begin{eqnarray}\\label{globalfunction4}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {W_{{L}_1}}({\\mathbf{y}}) {W_{\\mathrm{z}}} {\\mathbf{y}} \\|_2^2,\n\\end{eqnarray}\nwith model dependent weighting in the regularization term, $W({\\mathbf{y}})={W_{{L}_1}}({\\mathbf{y}}) {W_{\\mathrm{z}}}$. Thus the IRLS algorithm can be used to find a solution to the inverse problem as follows. \nThe normal equations for (\\ref{globalfunction4}), for given $W$, now dropping the dependence on ${\\mathbf{y}}$, with $\\tilde{G}=W_{{\\mathbf{d}}}G$ and $\\tilde{{\\mathbf{r}}}=W_{{\\mathbf{d}}}{\\mathbf{r}}$, yield\n\\begin{eqnarray*}\n{\\mathbf{y}}(\\alpha)=(\\tilde{G}^T\\tilde{G}+\\alpha^2 W^TW)^{-1}\\tilde{G}^T\\tilde{{\\mathbf{r}}}.\n\\end{eqnarray*}\nFor $\\epsilon >0$,  $W$ is invertible and  (\\ref{globalfunction4}) can be transformed to standard form using\n\\begin{eqnarray}\\nonumber\n(\\tilde{G}^T\\tilde{G}+\\alpha^2 W^TW)&=&W^T((W^T)^{-1}\\tilde{G}^T\\tilde{G}W^{-1} +\\alpha^2 I_n)W\n=W^T(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)W,\n\\end{eqnarray}\nwith $ \\tilde{\\tilde{G}}=\\tilde{G} W^{-1}$, which corresponds to  variable preconditioning of $\\tilde{G}$.  Now the standard Tikhonov functional for the left and right preconditioned system is given by \n\\begin{eqnarray}\\label{globalfunctionh}\nP^{\\alpha}({\\mathbf{h}})=\\| \\tilde{\\tilde{G}} {\\mathbf{h}}- \\tilde{{\\mathbf{r}}} \\|_2^2 + \\alpha^2 \\|{\\mathbf{h}} \\|_2^2, \\quad \\textrm{where}\\quad {\\mathbf{h}}(\\alpha)=W{\\mathbf{y}}(\\alpha).\\end{eqnarray} \nSolution\n\\begin{eqnarray}\\label{hsolution}\n{\\mathbf{h}}(\\alpha)=(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)^{-1} \\tilde{\\tilde{G}}^T \\tilde{{\\mathbf{r}}}=\n\\tilde{\\tilde{G}}(\\alpha)\\tilde{{\\mathbf{r}}},\\quad \\mathrm{where} \\quad \\tilde{\\tilde{G}}(\\alpha)=(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)^{-1} \\tilde{\\tilde{G}}^T, \n\\end{eqnarray}\nyields the model update ${\\mathbf{m}}(\\alpha)={\\mathbf{m}_{\\mathrm{apr}}}+W^{-1}{\\mathbf{h}}(\\alpha)$. For small-scale problems, the numerical solutions of  (\\ref{hsolution}) can be obtained using the SVD for  $\\tilde{\\tilde{G}}$ , see appendix \\ref{svdsolution}. Practically, however, within the context of the IRLS in which $W$, and hence $\\tilde{\\tilde{G}}$, are updated each step, the computation of the  SVD each step still represents a significant overhead for the iterative algorithm.   For the forthcoming discussion on the solution of large scale problems the overhead of the iterative update for $\\tilde{\\tilde{G}}$ is insignificant. While for clarity in Algorithm~\\ref{svdalgorithm} we directly form $\\tilde{\\tilde{G}}$, we note that  for some cases where $G$ has certain structure that may be eliminated by the pre and post multiplication even by diagonal matrices ${W_{\\mathrm{z}}}$ and  ${W_{{L}_1}}$,  it is possible and efficient to consider the matrix vector products of $\\tilde{\\tilde{G}} \\bf x$ for arbitrary $\\bf x$ without explicitly forming $\\tilde{\\tilde{G}}$, as discussed in \\cite{RVA:2015}. We note that in such cases when ${W_{\\mathrm{z}}}$ and  ${W_{{L}_1}}$ are diagonal, multiplication, inversion and storage requirements are minimal, of $\\mathcal{O}(n)$ only. \n\nApplication of the IRLS for the solution of the inverse problem requires the designation of a termination test to determine whether or not an acceptable solution has been reached.  Two criteria are chosen to terminate the algorithm; either the solution satisfies the noise level,  \n\\begin{eqnarray}\\label{noisetest}\n\\chi_{\\mathrm{Computed}}^2 =  \\| \\frac{({\\mathbf{d}_{\\mathrm{obs}}})_i -(G{\\mathbf{m}})_i}{{\\mbox{\\boldmath{$\\eta$}}}_i}\\|_2^2 \\leq m+\\sqrt{2m},\n\\end{eqnarray}\n or a maximum number of iterations, $K_{\\mathrm{max}}$, is reached. Additionally, at each step practical lower and upper  bounds on the density, $[\\rho_{\\mathrm{min}},\\rho_{\\mathrm{max}}]$, are imposed to recover reliable subsurface models. If at any iterative step a given density value falls outside the bounds, the value at that cell is projected back to the nearest constraint value.  At iteration $k$ we  use the approximation of   (\\ref{generalnorm})  given by\n\\begin{eqnarray}\\label{generalnorm2}\ne_{p}(x^{(k)},x^{(k-1)})= \\frac{(x^{(k)})^2}{ ((x^{(k-1)})^2+\\epsilon^2)^{(\\frac{2-p}{2})} },\n\\end{eqnarray}\nwhere $x^{(k)}$ denotes the unknown model parameter. \nTypically, as the iterations proceed, if $x^{(k)}$ converges, the approximation of (\\ref{generalnorm}) by (\\ref{generalnorm2}) is increasingly better. The IRLS algorithm for small scale  $L_1$ inversion  is summarized in Algorithm~\\ref{svdalgorithm}.\n\\begin{algorithm}\n\\caption{Iterative $L_1$ Inversion Algorithm}\\label{svdalgorithm}\n\\begin{algorithmic}[1]\n\\REQUIRE ${\\mathbf{d}_{\\mathrm{obs}}}$, ${\\mathbf{m}_{\\mathrm{apr}}}$, $G$, $W_{{\\mathbf{d}}}$, $\\epsilon > 0$, $\\rho_{\\mathrm{min}}$, $\\rho_{\\mathrm{max}}$, $K_{\\mathrm{max}}$\n\\STATE Calculate ${W_{\\mathrm{z}}}$, $\\tilde{G}= W_{{\\mathbf{d}}}G$, and ${\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}= W_{{\\mathbf{d}}}{\\mathbf{d}_{\\mathrm{obs}}}$\n\\STATE Initialize ${\\mathbf{m}}^{(0)}={\\mathbf{m}_{\\mathrm{apr}}}$,  ${W_{{L}_1}}^{(1)}=I_n$, $W^{(1)}={W_{\\mathrm{z}}}$\n\\STATE Calculate $\\tilde{{\\mathbf{r}}}^{(1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(0)}$, $\\tilde{\\tilde{G}}^{(1)}=\\tilde{G}(W^{(1)})^{-1} $, $k=0$\n\\WHILE {Not converged, \\eqref[noisetest] not satisfied, and $k<K_{\\mathrm{max}}$} \n\\STATE {$k=k+1$}\n\\STATE {Find the SVD: $\\tilde{\\tilde{G}}^{(k)}=U\\Sigma V^T$}\n\\STATE {Use regularization parameter estimation to find $\\alpha^{(k)}$}\n\\STATE {Set ${\\mathbf{h}}^{(k)}= \\sum_{i=1}^{m} \\frac{\\sigma_i^2}{\\sigma_i^2+(\\alpha^{(k)})^2} \\frac{{\\mathbf{u}_i} ^T\\tilde{{\\mathbf{r}}}^{(k)}}{\\sigma_i} {\\mathbf{v}_i}$}\n\\STATE {Set ${\\mathbf{m}}^{(k)}={\\mathbf{m}}^{(k-1)}+ (W^{(k)})^{-1}{\\mathbf{h}}^{(k)}$}\n\\STATE {Impose constraint conditions on ${\\mathbf{m}}^{(k)}$ to force $\\rho_{\\mathrm{min}}\\le {\\mathbf{m}}^{(k)} \\le \\rho_{\\mathrm{max}}$}\n\\STATE {Calculate the residual $\\tilde{{\\mathbf{r}}}^{(k+1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(k)}$}\n\\STATE {Set ${W_{{L}_1}}^{(k+1)} ={\\mathrm{diag}}\\left( \\left(({\\mathbf{m}}^{(k)}-{\\mathbf{m}}^{(k-1)})^2+\\epsilon^2 \\right)^{-1/4}\\right)$,  as in \\eqref[WL], and\n$W^{(k+1)}={W_{{L}_1}}^{(k+1)}{W_{\\mathrm{z}}}$}\n\\STATE {Calculate $\\tilde{\\tilde{G}}^{(k+1)}=\\tilde{G}(W^{(k+1)})^{-1} $}\n\\ENDWHILE\n\\ENSURE Solution $\\rho={\\mathbf{m}}^{(k)}$. $K=k$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Numerical solution by the Golub-Kahan bidigonalization} \\label{GKB}\nIt is not viable to use the SVD decomposition for large scale problems, rather iterative methods such as conjugate gradients (CG), or other Krylov methods, can be employed to find ${\\mathbf{h}}(\\alpha)$. In any case,  however, the problem of finding the optimal parameter $\\alpha$ is a further complication. In general determination of an optimal $\\alpha$ requires calculating ${\\mathbf{h}}(\\alpha)$ for multiple $\\alpha$. Alternatively, as suggested for example in Chung et al. \\shortcite{ChNaOl:2008} and  Kilmer $\\&$ O'Leary \\shortcite{KiOl:2001}, regularization may be imposed on a smaller space. The Golub-Kahan bidiagonalization (GKB) is applied to project the solution of the inverse problem to a smaller subspace. If we apply $t$ steps of the GKB on matrix $\\tilde{\\tilde{G}} $ with initial vector $\\tilde{{\\mathbf{r}}}$, then bidiagonal matrix $ B_t \\in \\mathcal{R}^{(t+1) \\times t}$ and matrices $H_{t+1} \\in \\mathcal{R}^{m \\times (t+1)}$, $A_t \\in \\mathcal{R}^{n \\times t}$ with orthonormal columns will be generated such that, see \\cite{Hansen:2007,KiOl:2001},\n\\begin{eqnarray*}\n\\tilde{\\tilde{G}}A_t = H_{t+1}B_t, \\quad  H_{t+1}{\\mathbf{e}_{t+1}}=\\tilde{{\\mathbf{r}}}/\\| \\tilde{{\\mathbf{r}}}\\|_2.\n\\end{eqnarray*}\nFor further discussion we explicitly use the subscript to indicate that the unit vector here is of length $(t+1)$ with a $1$  in the first entry.  The columns of $ A_t$ form an orthonormal basis for the Krylov subspace\n\\begin{eqnarray}\\label{krylov}\nK_t(\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}},\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}})=\\mathrm{span} \\lbrace \\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}},(\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}})\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}}, \\dots, (\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}})^{t-1}\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}} \\rbrace,\n\\end{eqnarray}\nand an approximate solution ${\\mathbf{h}}_t$ that lies in this Krylov subspace will have the form ${\\mathbf{h}}_t=A_t {\\mathbf{z}}_t$, ${\\mathbf{z}}_t \\in \\mathcal{R} ^{t}$. Note, we denote the quantities obtained using $t$ steps of the factorization always with subscript $t$.\nThe matrix $W^{-1}$, which acts  as a right-preconditioner for the system, must be updated at each iteration $k$ (the same happens for $\\tilde{{\\mathbf{r}}}$) and then the Krylov subspace (\\ref{krylov}) is changed at each iteration. Here the preconditioner is not used to accelerate convergence, but it is used to enforce some specific regularity condition on the solution \\cite{GaNa:2014}. \n\nWe now introduce notation that will be helpful in the discussion for estimation of the regularization parameter. Specifically, defining the residuals for the full problem and projected problems by, respectively,  \n\\begin{eqnarray}\\label{fullresidual}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t)=\\tilde{\\tilde{G}}{\\mathbf{h}}_t-\\tilde{{\\mathbf{r}}}, \\quad \\textrm{and}\\quad  \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t) = B_t {\\mathbf{z}}_t -\\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}},\n\\end{eqnarray}  \nthen\n\\begin{eqnarray*}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t)= \\tilde{\\tilde{G}} A_t {\\mathbf{z}}_t -\\tilde{{\\mathbf{r}}} =  H_{t+1} B_t {\\mathbf{z}}_t -  H_{t+1} \\|\\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}}  = H_{t+1} \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t).\n\\end{eqnarray*}\nBy the  column orthogonality of $H_{t+1}$ it is immediate that the fidelity norm is preserved under projection, $ \\|  \\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t) \\|_2^2 = \\|  \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t) \\|_2^2$, and the Tikhonov functional  (\\ref{globalfunctionh}) can be written in terms of the projected problem, \n\\begin{eqnarray}\\label{globalfunctionproj}\nP^{\\zeta}({\\mathbf{z}})= \\| B_t {\\mathbf{z}} - \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}} \\|_2^2 + \\zeta^2 \\| {\\mathbf{z}} \\|_2^2, \\quad \n\\|{\\mathbf{h}}\\|_2^2=\\|{\\mathbf{z}}\\|_2^2.\n\\end{eqnarray}\nHere regularization parameter $\\zeta$ replaces $\\alpha$ while having the  same role as $\\alpha$ but for the projected case. Since the dimensions of $B_t$ are  small as compared to the dimensions of $\\tilde{\\tilde{G}}$,  the solution of the projected problem (\\ref{globalfunctionproj}) is obtained efficiently from \n\\begin{eqnarray*}\\label{zsolution}\n{\\mathbf{z}}_t(\\zeta)= (B_t^T B_t + \\zeta^2 I_t)^{-1} B_t^T \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}} = B_t(\\zeta)  \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}},  \n\\end{eqnarray*}\nwhere $ B_t(\\zeta) = (B_t^T B_t + \\zeta^2 I_t)^{-1} B_t^T$ can be obtained using the SVD, see appendix \\ref{svdsolution}, and the update for the global solution is immediately given by  ${\\mathbf{m}}_t(\\zeta)={\\mathbf{m}_{\\mathrm{apr}}}+ W^{-1}  A_t {\\mathbf{z}}_t(\\zeta)$.\n\nThe projected solution ${\\mathbf{z}}_t(\\zeta)$  depends on both the subspace size, $t$,  and the regularization parameter, $\\zeta$.  Our focus here is not on the determination of the optimal subspace size $t_{\\mathrm{opt}}$, rather we focus on the determination of $\\zeta_{\\mathrm{opt}}$, noting that finding $t_{\\mathrm{opt}}$ is a topic of significant study see for example the discussion in e.g. \\cite{RVA:2015}. For small $t$, the singular values of $B_t$, $\\gamma_i$, approximate the largest singular values of $\\tilde{\\tilde{G}}$, however, for larger $t$ the smaller singular values of $B_t$  approximate the  smallest singular values of  $\\tilde{\\tilde{G}}$, so that there is no immediate one to one alignment of the small singular values between $\\tilde{\\tilde{G}}$ and $B_t$ with increasing $t$. Thus, it is important to choose $t$ such that the dominant singular values of $\\tilde{\\tilde{G}}$ are well approximated by those of $B_t$ effectively capturing the dominant subspace for the solution.  We will discus the effect of choosing different $t$ on the solution.  Furthermore, the regularization parameter-choice methods used in this context, also need some modification for the projected case, which is demonstrated in the next section. \n\nAlthough $H_{t+1}$ and $A_t$ have orthonormal columns in exact arithmetic, Krylov methods lose orthogonality in finite precision. This means that after a relatively low number of iterations the vectors in $H_{t+1}$ and $A_t$  are no longer orthogonal and the relationship between (\\ref{globalfunctionh}) and (\\ref{globalfunctionproj}) does not hold. Here we therefore use  reorthogonalization to maintain the column orthogonality, which is also important for replicating the dominant spectral properties of $\\tilde{\\tilde{G}}$ by $B_t$. We use  Modified Gram Schmidt (MGS),  see Hansen \\shortcite{Hansen:2007}. We summarize the steps which are needed for implementation of the projected $L_{1}$ inversion in Algorithm~\\ref{projectedalgorithm} and note that in practice one may not need to explicitly calculate $\\tilde{\\tilde{G}}$, rather, for the factorization it can be sufficient to be able to efficiently perform operations with $\\tilde{G}$, $\\tilde{G}^T$, and diagonal matrices $W$, as discussed in  section~\\ref{l1method} in relation to Algorithm~\\ref{svdalgorithm}. \n\\begin{algorithm}\n\\caption{Iterative Projected $L_1$ Inversion Algorithm}\\label{projectedalgorithm}\n\\begin{algorithmic}[1]\n\\REQUIRE ${\\mathbf{d}_{\\mathrm{obs}}}$, ${\\mathbf{m}_{\\mathrm{apr}}}$, $G$, $W_{{\\mathbf{d}}}$, $\\epsilon > 0$, $\\rho_{\\mathrm{min}}$, $\\rho_{\\mathrm{max}}$, $t$, $K_{\\mathrm{max}}$\n\n\\STATE Calculate ${W_{\\mathrm{z}}}$, $\\tilde{G}= W_{{\\mathbf{d}}}G$, and ${\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}= W_{{\\mathbf{d}}}{\\mathbf{d}_{\\mathrm{obs}}}$\n\\STATE Initialize ${\\mathbf{m}}^{(0)}={\\mathbf{m}_{\\mathrm{apr}}}$,  ${W_{{L}_1}}^{(1)}=I_n$, $W^{(1)}={W_{\\mathrm{z}}}$\n\\STATE Calculate $\\tilde{{\\mathbf{r}}}^{(1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(0)}$, $\\tilde{\\tilde{G}}^{(1)}=\\tilde{G}(W^{(1)})^{-1} $, $k=0$\n\\WHILE {Not converged, \\eqref[noisetest] not satisfied, and $k<K_{\\mathrm{max}}$} \n\\STATE {$k=k+1$}\n\\STATE {Apply GKB: $\\tilde{\\tilde{G}} ^{(k)} A_t^{(k)} = H_{t+1}^{(k)}B_t^{(k)}, \\quad  H_{t+1}^{(k)}{\\mathbf{e}_{t+1}}=\\tilde{{\\mathbf{r}}}^{(k)}/\\| \\tilde{{\\mathbf{r}}}^{(k)}\\|_2$ }\n\\STATE {Find the SVD: $B_t^{(k)}=U\\Gamma V^T$}\n\\STATE {\\label{stepzeta}Use regularization parameter estimation to find $\\zeta^{(k)}$}\n\\STATE {\\label{stephupdate}Set ${\\mathbf{z}}_t^{(k)}= \\sum_{i=1}^{t} \\frac{\\gamma_i^2}{\\gamma_i^2+(\\zeta^{(k)})^2} \\frac{{\\mathbf{u}_i} ^T(\\| \\tilde{{\\mathbf{r}}}^{(k)}\\|_2{\\mathbf{e}_{t+1}})}{\\gamma_i} {\\mathbf{v}_i}$}\n\\STATE {Set ${\\mathbf{h}}_t^{(k)}=A_t^{(k)}{\\mathbf{z}}_t^{(k)}$}\n\\STATE {Set ${\\mathbf{m}}^{(k)}={\\mathbf{m}}^{(k-1)}+ (W^{(k)})^{-1}{\\mathbf{h}}_t^{(k)}$}\n\\STATE {Impose constraint conditions on ${\\mathbf{m}}^{(k)}$ to force $\\rho_{\\mathrm{min}}\\le {\\mathbf{m}}^{(k)} \\le \\rho_{\\mathrm{max}}$}\n\\STATE {Calculate the residual $\\tilde{{\\mathbf{r}}}^{(k+1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(k)}$}\n\\STATE {\\label{stepWL}Set ${W_{{L}_1}}^{(k+1)} ={\\mathrm{diag}}\\left( \\left(({\\mathbf{m}}^{(k)}-{\\mathbf{m}}^{(k-1)})^2+\\epsilon^2 \\right)^{-1/4}\\right)$,  as in \\eqref[WL], and\n$W^{(k+1)}={W_{{L}_1}}^{(k+1)}{W_{\\mathrm{z}}}$}\n\\STATE {Calculate $\\tilde{\\tilde{G}}^{(k+1)}=\\tilde{G}(W^{(k+1)})^{-1} $}\n\\ENDWHILE\n\\ENSURE Solution $\\rho={\\mathbf{m}}^{(k)}$. $K=k$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Regularization parameter estimation}\\label{parameter estimation}\nNow we focus on determination of the regularization parameter at each step $k$, supposing that  the dimension of the subspace, $t$, is known and kept fixed during the iterations. Our previous investigations have been shown that the method of the UPRE leads to an effective estimation of the regularization parameter \\cite{RVA:2015,VAR:2015,VRA:2014b}. In order to use the method for the projected problem we briefly review the derivation of the UPRE on the full problem. \n\n\\subsection{Unbiased predictive risk estimator}\\label{upre}\nAny method which is used to determine optimal $\\alpha$ should minimize the error between the solution ${\\mathbf{h}}(\\alpha)$ and the exact solution ${\\mathbf{h}}_{\\mathrm{exact}}$. Because the exact solution is unknown, an alternative error indicator, called the predictive error, is used \\cite{Vogel:2002}\n\\begin{eqnarray}\\nonumber\n\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha)) &=& \\tilde{\\tilde{G}} {\\mathbf{h}}(\\alpha) -\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\n=  \\tilde{\\tilde{G}} \\tilde{\\tilde{G}}(\\alpha)\\tilde{{\\mathbf{r}}}-\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} \\\\\n&=& H(\\alpha)(\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+\\tilde{{\\mbox{\\boldmath{$\\eta$}}}})-\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\n=(H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+ H(\\alpha)\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}\\label{predictiveerror}\n\\end{eqnarray}\nwhere $ H(\\alpha)=\\tilde{\\tilde{G}} \\tilde{\\tilde{G}}(\\alpha)$ is the influence matrix. The predictive error is also not\ncomputable because $\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}$ is unknown, however, it can be estimated using the full residual  (\\ref{fullresidual})\n\\begin{eqnarray}\\label{fullresidual2}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha)) = \\tilde{\\tilde{G}} {\\mathbf{h}}(\\alpha) -\\tilde{{\\mathbf{r}}}\n=(H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}\n = (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+(H(\\alpha)-I_m)\\tilde{{\\mbox{\\boldmath{$\\eta$}}}} .\n\\end{eqnarray} \nFor both (\\ref{predictiveerror}) and (\\ref{fullresidual2}), the first term on the right hand side is deterministic, whereas the second is stochastic. Applying the Trace lemma, e.g. \\cite{Vogel:2002}, for both equations and using the symmetry of the influence matrix we obtain\n\\begin{eqnarray}\\label{Pexpected}\nE(\\|\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2) &=& \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\\|_2^2 + \\mathrm{trace}(H^T(\\alpha)H(\\alpha)), \\,\\,\\mathrm{and} \n\\\\E(\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2) &=& \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\\|_2^2 + \\mathrm{trace}((H(\\alpha)-I_m)^T(H(\\alpha)-I_m))\\label{Rexpected} .\n\\end{eqnarray}\nHere $E(\\|\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2)/m$ is the expected value of the predictive risk \\cite{Vogel:2002}. The first terms in the right hand sides of (\\ref{Pexpected}) and (\\ref{Rexpected}) are the same. Thus, by the linearity of the trace operator and with $E(\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2)\\approx\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2= \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}\\|_2^2 $,  the UPRE estimator of the optimal parameter is \n\\begin{eqnarray}\\label{fullupre}\n\\alpha_{\\mathrm{opt}}={\\textnormal{arg} \\min_{{\\alpha}}}\\{U(\\alpha):= \\|(H(\\alpha) -I_m)\\tilde{{\\mathbf{r}}}\\|_2^2 +2\\,\\mathrm{trace}(H(\\alpha)) - m\\}.\n\\end{eqnarray}\nTypically $\\alpha_{\\mathrm{opt}}$ is found by evaluating (\\ref{fullupre}) for a range of $\\alpha$, for example by the SVD see  appendix \\ref{svdparameter}, with the minimum found within that range of parameter values.\n\\subsection{Extending the UPRE for the projected problem}\\label{projectedupre}\nFor extending the UPRE parameter-choice method to a subspace, we first observe that given $\\tilde{{\\mathbf{r}}} = \\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} +\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$, then $\\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}} =H_{t+1}^T \\tilde{{\\mathbf{r}}}$ consists of a deterministic and stochastic part, $H_{t+1}^T \\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} + H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ , where for white noise vector $\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ and column orthogonal $H_{t+1}$, $H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ is a random vector of length $t+1$ with covariance matrix $I_t$. Thus, from the derivation of UPRE for the full problem defined by the system matrix $\\tilde{\\tilde{G}}$, right hand side $\\tilde{{\\mathbf{r}}}$ and white noise vector $\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ we immediately\nwrite down the UPRE for the projected problem with system matrix $B_t$, right hand side $ H_{t+1}^T \\tilde{{\\mathbf{r}}}$ and white noise vector $ H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$\n\\begin{eqnarray}\\label{subupre}\n\\zeta_{\\mathrm{opt}}={\\textnormal{arg} \\min_{{\\zeta}}}\\{U(\\zeta):= \\|(B(\\zeta) -I_{t+1}) \\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}}\\|_2^2 +2\\,\\mathrm{trace}(B(\\zeta)) - (t+1)\\}.\n\\end{eqnarray}\nHere $ B(\\zeta)= B_t B_t(\\zeta)$ is the influence matrix for the  subspace. As for the full problem, see appendix~\\ref{svdparameter}, the SVD of the matrix $B_t$ can be used to find $\\zeta_{\\mathrm{opt}}$. In section~\\ref{cube} we show that in some situations \\eqref[subupre], as introduced in \\cite{RVA:2015}, does not work well. Here, a modification  is introduced that does not use the entire subspace for a given $t$, but rather uses a truncated spectrum from $B_t$ for finding the regularization parameter, thus assuring that the dominant $t_{\\mathrm{trunc}}$ singular values are appropriately regularized. \n\n\\section{Synthetic examples}\\label{synthetic}\n\\subsection{Model of an embedded cube}\\label{cube}\nThe initial goal of the presented verification with simulated data is to contrast Algorithms~\\ref{svdalgorithm} and \\ref{projectedalgorithm}. \nWe use a  simple small-scale model that includes a cube with density contrast   $1$~g~cm$^{-3}$ embedded in an homogeneous background, Fig.~\\ref{2a}. Simulation data on the surface, ${\\mathbf{d}_{\\mathrm{exact}}}$, are calculated over a $ 20 \\times 20 $ regular grid with $50$ m grid spacing. To add noise to the  data, a zero mean Gaussian random matrix $\\Theta$ of size m $\\times 10$ was generated. Then, setting\n\n", "itemtype": "equation", "pos": -1, "prevtext": "  gives the minimum support (MS) stabilizer which minimizes the total volume of nonzero departure of the model parameters from the given prior model, \\cite{PoZh:99}.  A further modification introduced in  \\cite{PoZh:99}  uses the gradient of the model parameters in the stabilization term via $ S({\\mathbf{m}})=\\| D_{\\epsilon}(\\nabla {\\mathbf{m}}) | \\nabla {\\mathbf{m}}| \\|_2^2$, where $ D_{\\epsilon}(\\nabla {\\mathbf{m}})$ is defined by ${\\mathrm{diag}}(1/(|\\nabla{\\mathbf{m}}|^2 +\\epsilon^2)^{1/2})$, yielding the  minimum gradient support (MGS) stabilizer. This constraint minimizes the volume over which the gradient of the model parameters is nonzero, and thus yields models with sharp boundaries.  \n\n\nAnother possibility for the reconstruction of sparse models  is to use a stabilizer which minimizes the $L_{1}$-norm of the model or gradient, in this case known as the total variation (TV),  of the model parameters \\cite{FaOl:98,Far:2008,Loke:2003}.   The $L_{1}$-norm stabilizer permits occurrence of large elements in the inverted model among mostly small values and can, therefore, be used to obtain models with non-smooth properties \\cite{SunLi:2014}. Although the  $L_{1}$-norm stabilizer has favorable properties,  and yields a convex functional that can be solved by linear programming algorithms, its use for the solution of large scale problems is not feasible. Here we implement the $L_{1}$-norm stabilizer  using the approximation based on an IRLS algorithm, \\cite{BrDoEl:2009}, and extend the algorithm for the gravity inverse problem by including the  depth weighting and prior model information. Further, we use  the  weighting matrix similar to that used for the  MS stabilizer but defined by \n\n", "index": 3, "text": "\\begin{equation}\\label{WL}\n {W_{{L}_1}}({\\mathbf{m}}) ={\\mathrm{diag}}(1/(({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}})^2+\\epsilon^2)^{1/4}),\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{W_{{L}_{1}}}({\\mathbf{m}})={\\mathrm{diag}}(1/(({\\mathbf{m}}-{\\mathbf{m}_{%&#10;\\mathrm{apr}}})^{2}+\\epsilon^{2})^{1/4}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><msub><mi>L</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc26</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>diag</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc26</mi><mo>-</mo><msub><mi>\ud835\udc26</mi><mi>apr</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>4</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00114.tex", "nexttext": "\n for $c=1:10$, with noise parameter pairs $(\\tau_1$, $\\tau_2)$, for three choices, $N1: (0.01,0.001)$,  $N2: (0.02,0.005)$ and $N3: (0.03,0.01) $, gives  $10$ noisy right-hand side vectors for $3$ levels of noise. Fig.~\\ref{2b} shows noise-contaminated data for one right-hand side, here $c=7$, and for $N2$.\n\\begin{figure*}\n\\subfigure{\\label{2a}\\includegraphics[width=.45\\textwidth]{figure2a.pdf}}\n\\subfigure{\\label{2b}\\includegraphics[width=.45\\textwidth]{figure2b.pdf}}\n\\caption { (a) Model of the cube on an homogeneous background. The density contrast of the cube is $1$~g~cm$^{-3}$. (b) Data due to the model and contaminated with  noise $N2$.}\n\\end{figure*}\nFor the inversion  the model region of depth $500$~m, is discretized into $ 20 \\times 20 \\times 10 = 4000$ cells of size $50$m in each dimension. The background model ${\\mathbf{m}_{\\mathrm{apr}}}=\\mathbf{0}$ and parameter $\\epsilon^2=1e{-9}$ are chosen for the inversion. Realistic upper and lower density bounds $\\rho_{\\mathrm{max}}=1$~g~cm$^{-3}$ and $\\rho_{\\mathrm{min}}=0$~g~cm$^{-3}$, are specified. The iterations are terminated when, approximating \\eqref[noisetest], $\\chi_{\\mathrm{Computed}}^2 \\leq 429 $,  or $k>K_{\\mathrm{max}} =50$. \n\\subsection{Solution using Algorithm~\\ref{svdalgorithm}}\\label{cube:alg1}\nThe inversion was performed using Algorithm~\\ref{svdalgorithm} for the $3$ noise levels, and all $10$ right-hand side data vectors. The final iteration $K$, the final regularization parameter $\\alpha^{(K)}$ and the relative error of the reconstructed model \n\n", "itemtype": "equation", "pos": 39847, "prevtext": " where we point to the use of fourth root in the denominator rather than square root in \\eqref[We].  Our results will compare these choices.  \n\nMany techniques have been developed to estimate a suitable regularization parameter, $\\alpha$  in (\\ref{globalfunction}),  including the L-curve (LC) \\cite{Hansen:92}, and Generalized Cross Validation (GCV) \\cite{GHW:1979,Mar:1970}, and methods which assume some  knowledge of the noise level of the data, including the $ \\chi^2$-discrepancy principle \\cite{MeRe:09,RHM:2010,VRA:2014b}, the unbiased predictive risk estimator (UPRE) \\cite{Vogel:2002} and the Morozov discrepancy principle (MDP) \\cite{Morozov:66}. Our previous investigation for the gravity inverse problem, see Vatankhah et al \\shortcite{VAR:2015}, has demonstrated that the UPRE parameter-choice method provides a good estimate for the Tikhonov regularization parameter especially for high noise levels. Therefore, here, we use the UPRE for estimation of the regularization parameter $\\alpha$, extending the approach in \\cite{RVA:2015}.\n\nFor small-scale problems involving two quadratic terms the solution of   \\eqref[globalfunction]  may be found efficiently using the generalized singular value decomposition (GSVD) or singular value decomposition (SVD), dependent on the choice for ${W_{{\\mathbf{m}}}}$. In addition, these factorizations  present  regularization parameter-choice methods in computationally convenient forms \\cite{VRA:2014b}. For large-scale problems these factorizations are generally computationally impractical and an alternative approach is to find the solution  through projection of the problem to a smaller subspace, on which it may then be feasible to use the factorization for efficient estimation of the regularization parameter. Here we  use  the Golub-Kahan iterative bidiagonalization projection of the solution which is  based on the LSQR Krylov methods introduced in Paige $\\&$ Saunders \\shortcite{PaSa:1982a,PaSa:1982b}. Then,  effective parameter estimation techniques, that are useful in the context of   efficient iterative Krylov-based procedures, must be developed. Chung et al. presented  the weighted GCV  for the projected problem which requires the use of an additional solution dependent weighting parameter, \\cite{ChNaOl:2008}. Here we focus our discussion on the   UPRE in conjunction with the solution of the projected problem, additionally extending the approach introduced in \\cite{RVA:2015}.\n\nThe outline of this paper is as follows. In Section~\\ref{l1method} we review the inversion algorithm and transformation of the $L_{1}$-norm stabilizer into the standard form Tikhonov functional. Furthermore, the development of  the Tikhonov regularization functional based on the Golub-Kahan iterative bidiagonalization is given in Section~\\ref{GKB}. Section ~\\ref{parameter estimation} is devoted to discussion of parameter estimation, the derivation of the UPRE is presented in Section~\\ref{upre} with new work showing its extension for usage with the projected problem in Section~\\ref{projectedupre}. Results for synthetic examples are illustrated in Section~\\ref{synthetic}. The reconstruction of an embedded cube with high density within a homogeneous medium is  used for contrasting the algorithms using the SVD, Section~\\ref{cube:alg1} and the LSQR algorithm, Section~\\ref{cube:alg2}. These results demonstrate the need to use the truncated UPRE which is introduced and applied in Section~\\ref{cube:tupre}. The reconstruction of a more complex structure using the TUPRE is presented in Section~\\ref{multiplebodies}. These simulations are concluded with the contrast of the MS and $L_1$ stabilizers in Section~\\ref{contrastL1MS}. The approach is applied on gravity data acquired over\na hematite mine located in the southeast of Iran and the results are shown in Section~\\ref{real}. Conclusions and future work follow in Section~\\ref{conclusion}.\n\n\\section{ $L_{1}$ Inversion methodology }\\label{l1method}\nWe briefly review the well-known approach for the  standard $3$D inversion of gravity data.  The subsurface volume is discretized using a set of cubes, in which the cells size are kept fixed during the inversion, and the values of densities at the cells  are the model parameters to be determined in the inversion \\cite{BoCh:2001,LiOl:98}. Thus, we can introduce a vector ${\\mathbf{m}}$ of the unknown model parameters, here the density of each cell $\\rho_{j}$, such that ${\\mathbf{m}}=(\\rho_{1}, \\rho_{2},\\dots, \\rho_{n}) \\in {\\mathcal{R}^{n}}$ and a vector ${\\mathbf{d}_{\\mathrm{obs}}} \\in {\\mathcal{R}^{m}}$ which contains the measured data. Then, the gravity data satisfy the  rectangular underdetermined linear system\n\\begin{eqnarray}\\label{d=gm}\n{\\mathbf{d}_{\\mathrm{obs}}}= G{\\mathbf{m}}.\n\\end{eqnarray} \nPractically, ${\\mathbf{d}_{\\mathrm{obs}}}={\\mathbf{d}_{\\mathrm{exact}}}+{\\mbox{\\boldmath{$\\eta$}}} $, where ${\\mathbf{d}_{\\mathrm{exact}}}$ is the unknown exact data and ${\\mbox{\\boldmath{$\\eta$}}} \\in {\\mathcal{R}^{m}}$ represents the error in the measurements, assumed to be Gaussian and uncorrelated. The matrix $G \\in {\\mathcal{R}^{m \\times n}}, m \\ll n$, is the matrix resulting from the discretization of the forward operator which maps from the model space to the data space. Given $G$, the goal of the gravity inverse problem is to find a stable and geologically plausible density model ${\\mathbf{m}}$ that reproduces ${\\mathbf{d}_{\\mathrm{obs}}}$ at the noise level.\n\nAs discussed in section~\\ref{sec:intro}, solving problem (\\ref{d=gm}) is challenging due to the ill-posed nature of the problem and regularization is required to stabilize the solution. Furthermore, depth weighting and prior model information should be included in the formulation. Rewriting (\\ref{d=gm}) via ${\\mathbf{d}_{\\mathrm{obs}}} -G{\\mathbf{m}_{\\mathrm{apr}}}= G{\\mathbf{m}} -G{\\mathbf{m}_{\\mathrm{apr}}}$ and then \nintroducing the residual and discrepancy from the background data, using  ${\\mathbf{r}}={\\mathbf{d}_{\\mathrm{obs}}}-G{\\mathbf{m}_{\\mathrm{apr}}} $ and $ {\\mathbf{y}}={\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}}$, respectively, for which it is immediate that ${\\mathbf{r}}= G{\\mathbf{y}}$,\nwe obtain the functional for ${\\mathbf{y}}$, \n\\begin{eqnarray}\\label{globalfunction2}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {\\mathbf{y}} \\|_1,\n\\end{eqnarray}\nhere introducing the $L_{1}$ regularization term. Assuming  minimization of (\\ref{globalfunction2}) to give ${\\mathbf{y}}(\\alpha)$,  the model is updated by\n\n${\\mathbf{m}}(\\alpha)={\\mathbf{m}_{\\mathrm{apr}}} + {\\mathbf{y}}(\\alpha)$. \n\nIn the IRLS algorithm $\\|{\\mathbf{y}}\\|_1$ is approximated as follows. We first note, following for example \\cite{Voronin:2012,WoRo:07},  that \n$|y_{i}|={y_{i}^2}/{\\sqrt{y_{i}^2}}$ can be approximated by  ${y_{i}^2}/{\\sqrt{y_{i}^2 +\\epsilon^2}}$ for small and positive $\\epsilon$. Thus \n\\begin{eqnarray*}\n\\|{\\mathbf{y}} \\|_1 \\approx \\sum_{i=1}^n{ \\frac{y_{i}^2}{\\sqrt{y_{i}^2 +\\epsilon^2}}} = \\sum_{i=1}^n ({W_{{L}_1}})_{ii}^2 \\,y_i^2 = \\|{W_{{L}_1}}({\\mathbf{y}}){\\mathbf{y}}\\|^2, \\,\\, \\textrm{for}\\,\\, \\left({W_{{L}_1}}({\\mathbf{y}})\\right)_{ii}=\\frac{1}{(y_{i}^2+\\epsilon^2)^{1/4}}, \n\\end{eqnarray*}\nand (\\ref{globalfunction2}) is replaced by the approximating differentiable functional with diagonal matrix ${W_{{L}_1}}({\\mathbf{y}})$, \n\\begin{eqnarray}\\label{globalfunction3}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {W_{{L}_1}}({\\mathbf{y}}){\\mathbf{y}} \\|_2^2, \\quad {\\mathbf{y}}=({\\mathbf{m}}-{\\mathbf{m}_{\\mathrm{apr}}}).\n\\end{eqnarray}\n\n\nNow  the close relationship between the MS and $L_{1}$-norm stabilizers is clear, the difference being the fractional root, as is seen by introducing \n\\begin{eqnarray}\\label{generalnorm}\nS_p({\\mathbf{x}})=\\sum_{i=1}^n e_{p}(x_i) \\quad \\mathrm{where} \\quad e_{p}(x)= \\frac{x^2}{ (x^2+\\epsilon^2)^{\\frac{2-p}{2}} }.\n\\end{eqnarray}\nWhen $\\epsilon$ is sufficiently small, (\\ref{generalnorm}) yields the approximation of  the $L_p$ norm for $p=2$ and $p=1$,  while the case with $p=0$ corresponds to the compactness constraint used in Last $\\&$ Kubik \\shortcite{LaKu:83}. $S_0({\\mathbf{x}})$  does not meet the mathematical requirement to be regarded as a norm, and is  commonly used to denote the number of nonzero entries in ${\\mathbf{x}}$. Fig.~\\ref{fig1} demonstrates the impact of the choice of $\\epsilon$ on $e_{p}(x)$ for $\\epsilon = 1e^{-9}$ , Fig.~\\ref{1a}, and $\\epsilon=0.5$, Fig.~\\ref{1b}. For larger $p$, more weight is imposed on large  elements of ${\\mathbf{x}}$, large elements will be penalized more heavily than small elements during minimization \\cite{SunLi:2014}. Hence  $L_2$  tends to discourage the occurrence of large elements in the inverted model, yielding smooth models, while  $L_1$ and $L_0$  allow large elements leading to the recovery of data with blocky features. Further, the $L_0$-norm is non-quadratic and $e_{0}(x)$ asymptotes to one away from $0$, regardless of the magnitude of $x$. Hence the penalty on the model parameters does not depend on their relative magnitude, only on whether or not they lie above or below a threshold dependent on $\\epsilon$ \\cite{Ajo:2007}. Further,  $L_1$  does not necessarily lead to a very sparse solution and thus  $L_0$ is better for preserving sparsity. On the other hand, as compared to $L_1$,  the disadvantage of $L_0$ is the  greater dependency  on the choice of $\\epsilon$, so that it is less robust than $L_1$. \n\\begin{figure*}\n\\subfigure{\\label{1a}\\includegraphics[width=.4\\textwidth]{figure1a.pdf}}\n\\subfigure{\\label{1b}\\includegraphics[width=.4\\textwidth]{figure1b.pdf}}\n\\caption {Illustration of different norms for two values of parameter $\\epsilon$.  (a) $\\epsilon=1e{-9}$; (b) $\\epsilon=0.5$.} \\label{fig1}\n\\end{figure*}\n\nNow, incorporating diagonal matrix ${W_{\\mathrm{z}}}$, where we generally assume that this is a depth weighting matrix,  into (\\ref{globalfunction3}), see Li $\\&$ Oldenburg \\shortcite{LiOl:98}, yields\n\\begin{eqnarray}\\label{globalfunction4}\nP^{\\alpha}({\\mathbf{y}})=\\| W_{{\\mathbf{d}}}(G{\\mathbf{y}}-{\\mathbf{r}})  \\|_2^2 + \\alpha^2 \\| {W_{{L}_1}}({\\mathbf{y}}) {W_{\\mathrm{z}}} {\\mathbf{y}} \\|_2^2,\n\\end{eqnarray}\nwith model dependent weighting in the regularization term, $W({\\mathbf{y}})={W_{{L}_1}}({\\mathbf{y}}) {W_{\\mathrm{z}}}$. Thus the IRLS algorithm can be used to find a solution to the inverse problem as follows. \nThe normal equations for (\\ref{globalfunction4}), for given $W$, now dropping the dependence on ${\\mathbf{y}}$, with $\\tilde{G}=W_{{\\mathbf{d}}}G$ and $\\tilde{{\\mathbf{r}}}=W_{{\\mathbf{d}}}{\\mathbf{r}}$, yield\n\\begin{eqnarray*}\n{\\mathbf{y}}(\\alpha)=(\\tilde{G}^T\\tilde{G}+\\alpha^2 W^TW)^{-1}\\tilde{G}^T\\tilde{{\\mathbf{r}}}.\n\\end{eqnarray*}\nFor $\\epsilon >0$,  $W$ is invertible and  (\\ref{globalfunction4}) can be transformed to standard form using\n\\begin{eqnarray}\\nonumber\n(\\tilde{G}^T\\tilde{G}+\\alpha^2 W^TW)&=&W^T((W^T)^{-1}\\tilde{G}^T\\tilde{G}W^{-1} +\\alpha^2 I_n)W\n=W^T(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)W,\n\\end{eqnarray}\nwith $ \\tilde{\\tilde{G}}=\\tilde{G} W^{-1}$, which corresponds to  variable preconditioning of $\\tilde{G}$.  Now the standard Tikhonov functional for the left and right preconditioned system is given by \n\\begin{eqnarray}\\label{globalfunctionh}\nP^{\\alpha}({\\mathbf{h}})=\\| \\tilde{\\tilde{G}} {\\mathbf{h}}- \\tilde{{\\mathbf{r}}} \\|_2^2 + \\alpha^2 \\|{\\mathbf{h}} \\|_2^2, \\quad \\textrm{where}\\quad {\\mathbf{h}}(\\alpha)=W{\\mathbf{y}}(\\alpha).\\end{eqnarray} \nSolution\n\\begin{eqnarray}\\label{hsolution}\n{\\mathbf{h}}(\\alpha)=(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)^{-1} \\tilde{\\tilde{G}}^T \\tilde{{\\mathbf{r}}}=\n\\tilde{\\tilde{G}}(\\alpha)\\tilde{{\\mathbf{r}}},\\quad \\mathrm{where} \\quad \\tilde{\\tilde{G}}(\\alpha)=(\\tilde{\\tilde{G}}^T\\tilde{\\tilde{G}}+ \\alpha^2 I_n)^{-1} \\tilde{\\tilde{G}}^T, \n\\end{eqnarray}\nyields the model update ${\\mathbf{m}}(\\alpha)={\\mathbf{m}_{\\mathrm{apr}}}+W^{-1}{\\mathbf{h}}(\\alpha)$. For small-scale problems, the numerical solutions of  (\\ref{hsolution}) can be obtained using the SVD for  $\\tilde{\\tilde{G}}$ , see appendix \\ref{svdsolution}. Practically, however, within the context of the IRLS in which $W$, and hence $\\tilde{\\tilde{G}}$, are updated each step, the computation of the  SVD each step still represents a significant overhead for the iterative algorithm.   For the forthcoming discussion on the solution of large scale problems the overhead of the iterative update for $\\tilde{\\tilde{G}}$ is insignificant. While for clarity in Algorithm~\\ref{svdalgorithm} we directly form $\\tilde{\\tilde{G}}$, we note that  for some cases where $G$ has certain structure that may be eliminated by the pre and post multiplication even by diagonal matrices ${W_{\\mathrm{z}}}$ and  ${W_{{L}_1}}$,  it is possible and efficient to consider the matrix vector products of $\\tilde{\\tilde{G}} \\bf x$ for arbitrary $\\bf x$ without explicitly forming $\\tilde{\\tilde{G}}$, as discussed in \\cite{RVA:2015}. We note that in such cases when ${W_{\\mathrm{z}}}$ and  ${W_{{L}_1}}$ are diagonal, multiplication, inversion and storage requirements are minimal, of $\\mathcal{O}(n)$ only. \n\nApplication of the IRLS for the solution of the inverse problem requires the designation of a termination test to determine whether or not an acceptable solution has been reached.  Two criteria are chosen to terminate the algorithm; either the solution satisfies the noise level,  \n\\begin{eqnarray}\\label{noisetest}\n\\chi_{\\mathrm{Computed}}^2 =  \\| \\frac{({\\mathbf{d}_{\\mathrm{obs}}})_i -(G{\\mathbf{m}})_i}{{\\mbox{\\boldmath{$\\eta$}}}_i}\\|_2^2 \\leq m+\\sqrt{2m},\n\\end{eqnarray}\n or a maximum number of iterations, $K_{\\mathrm{max}}$, is reached. Additionally, at each step practical lower and upper  bounds on the density, $[\\rho_{\\mathrm{min}},\\rho_{\\mathrm{max}}]$, are imposed to recover reliable subsurface models. If at any iterative step a given density value falls outside the bounds, the value at that cell is projected back to the nearest constraint value.  At iteration $k$ we  use the approximation of   (\\ref{generalnorm})  given by\n\\begin{eqnarray}\\label{generalnorm2}\ne_{p}(x^{(k)},x^{(k-1)})= \\frac{(x^{(k)})^2}{ ((x^{(k-1)})^2+\\epsilon^2)^{(\\frac{2-p}{2})} },\n\\end{eqnarray}\nwhere $x^{(k)}$ denotes the unknown model parameter. \nTypically, as the iterations proceed, if $x^{(k)}$ converges, the approximation of (\\ref{generalnorm}) by (\\ref{generalnorm2}) is increasingly better. The IRLS algorithm for small scale  $L_1$ inversion  is summarized in Algorithm~\\ref{svdalgorithm}.\n\\begin{algorithm}\n\\caption{Iterative $L_1$ Inversion Algorithm}\\label{svdalgorithm}\n\\begin{algorithmic}[1]\n\\REQUIRE ${\\mathbf{d}_{\\mathrm{obs}}}$, ${\\mathbf{m}_{\\mathrm{apr}}}$, $G$, $W_{{\\mathbf{d}}}$, $\\epsilon > 0$, $\\rho_{\\mathrm{min}}$, $\\rho_{\\mathrm{max}}$, $K_{\\mathrm{max}}$\n\\STATE Calculate ${W_{\\mathrm{z}}}$, $\\tilde{G}= W_{{\\mathbf{d}}}G$, and ${\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}= W_{{\\mathbf{d}}}{\\mathbf{d}_{\\mathrm{obs}}}$\n\\STATE Initialize ${\\mathbf{m}}^{(0)}={\\mathbf{m}_{\\mathrm{apr}}}$,  ${W_{{L}_1}}^{(1)}=I_n$, $W^{(1)}={W_{\\mathrm{z}}}$\n\\STATE Calculate $\\tilde{{\\mathbf{r}}}^{(1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(0)}$, $\\tilde{\\tilde{G}}^{(1)}=\\tilde{G}(W^{(1)})^{-1} $, $k=0$\n\\WHILE {Not converged, \\eqref[noisetest] not satisfied, and $k<K_{\\mathrm{max}}$} \n\\STATE {$k=k+1$}\n\\STATE {Find the SVD: $\\tilde{\\tilde{G}}^{(k)}=U\\Sigma V^T$}\n\\STATE {Use regularization parameter estimation to find $\\alpha^{(k)}$}\n\\STATE {Set ${\\mathbf{h}}^{(k)}= \\sum_{i=1}^{m} \\frac{\\sigma_i^2}{\\sigma_i^2+(\\alpha^{(k)})^2} \\frac{{\\mathbf{u}_i} ^T\\tilde{{\\mathbf{r}}}^{(k)}}{\\sigma_i} {\\mathbf{v}_i}$}\n\\STATE {Set ${\\mathbf{m}}^{(k)}={\\mathbf{m}}^{(k-1)}+ (W^{(k)})^{-1}{\\mathbf{h}}^{(k)}$}\n\\STATE {Impose constraint conditions on ${\\mathbf{m}}^{(k)}$ to force $\\rho_{\\mathrm{min}}\\le {\\mathbf{m}}^{(k)} \\le \\rho_{\\mathrm{max}}$}\n\\STATE {Calculate the residual $\\tilde{{\\mathbf{r}}}^{(k+1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(k)}$}\n\\STATE {Set ${W_{{L}_1}}^{(k+1)} ={\\mathrm{diag}}\\left( \\left(({\\mathbf{m}}^{(k)}-{\\mathbf{m}}^{(k-1)})^2+\\epsilon^2 \\right)^{-1/4}\\right)$,  as in \\eqref[WL], and\n$W^{(k+1)}={W_{{L}_1}}^{(k+1)}{W_{\\mathrm{z}}}$}\n\\STATE {Calculate $\\tilde{\\tilde{G}}^{(k+1)}=\\tilde{G}(W^{(k+1)})^{-1} $}\n\\ENDWHILE\n\\ENSURE Solution $\\rho={\\mathbf{m}}^{(k)}$. $K=k$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Numerical solution by the Golub-Kahan bidigonalization} \\label{GKB}\nIt is not viable to use the SVD decomposition for large scale problems, rather iterative methods such as conjugate gradients (CG), or other Krylov methods, can be employed to find ${\\mathbf{h}}(\\alpha)$. In any case,  however, the problem of finding the optimal parameter $\\alpha$ is a further complication. In general determination of an optimal $\\alpha$ requires calculating ${\\mathbf{h}}(\\alpha)$ for multiple $\\alpha$. Alternatively, as suggested for example in Chung et al. \\shortcite{ChNaOl:2008} and  Kilmer $\\&$ O'Leary \\shortcite{KiOl:2001}, regularization may be imposed on a smaller space. The Golub-Kahan bidiagonalization (GKB) is applied to project the solution of the inverse problem to a smaller subspace. If we apply $t$ steps of the GKB on matrix $\\tilde{\\tilde{G}} $ with initial vector $\\tilde{{\\mathbf{r}}}$, then bidiagonal matrix $ B_t \\in \\mathcal{R}^{(t+1) \\times t}$ and matrices $H_{t+1} \\in \\mathcal{R}^{m \\times (t+1)}$, $A_t \\in \\mathcal{R}^{n \\times t}$ with orthonormal columns will be generated such that, see \\cite{Hansen:2007,KiOl:2001},\n\\begin{eqnarray*}\n\\tilde{\\tilde{G}}A_t = H_{t+1}B_t, \\quad  H_{t+1}{\\mathbf{e}_{t+1}}=\\tilde{{\\mathbf{r}}}/\\| \\tilde{{\\mathbf{r}}}\\|_2.\n\\end{eqnarray*}\nFor further discussion we explicitly use the subscript to indicate that the unit vector here is of length $(t+1)$ with a $1$  in the first entry.  The columns of $ A_t$ form an orthonormal basis for the Krylov subspace\n\\begin{eqnarray}\\label{krylov}\nK_t(\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}},\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}})=\\mathrm{span} \\lbrace \\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}},(\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}})\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}}, \\dots, (\\tilde{\\tilde{G}}^T \\tilde{\\tilde{G}})^{t-1}\\tilde{\\tilde{G}}^T\\tilde{{\\mathbf{r}}} \\rbrace,\n\\end{eqnarray}\nand an approximate solution ${\\mathbf{h}}_t$ that lies in this Krylov subspace will have the form ${\\mathbf{h}}_t=A_t {\\mathbf{z}}_t$, ${\\mathbf{z}}_t \\in \\mathcal{R} ^{t}$. Note, we denote the quantities obtained using $t$ steps of the factorization always with subscript $t$.\nThe matrix $W^{-1}$, which acts  as a right-preconditioner for the system, must be updated at each iteration $k$ (the same happens for $\\tilde{{\\mathbf{r}}}$) and then the Krylov subspace (\\ref{krylov}) is changed at each iteration. Here the preconditioner is not used to accelerate convergence, but it is used to enforce some specific regularity condition on the solution \\cite{GaNa:2014}. \n\nWe now introduce notation that will be helpful in the discussion for estimation of the regularization parameter. Specifically, defining the residuals for the full problem and projected problems by, respectively,  \n\\begin{eqnarray}\\label{fullresidual}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t)=\\tilde{\\tilde{G}}{\\mathbf{h}}_t-\\tilde{{\\mathbf{r}}}, \\quad \\textrm{and}\\quad  \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t) = B_t {\\mathbf{z}}_t -\\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}},\n\\end{eqnarray}  \nthen\n\\begin{eqnarray*}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t)= \\tilde{\\tilde{G}} A_t {\\mathbf{z}}_t -\\tilde{{\\mathbf{r}}} =  H_{t+1} B_t {\\mathbf{z}}_t -  H_{t+1} \\|\\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}}  = H_{t+1} \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t).\n\\end{eqnarray*}\nBy the  column orthogonality of $H_{t+1}$ it is immediate that the fidelity norm is preserved under projection, $ \\|  \\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}_t) \\|_2^2 = \\|  \\mathbf{R}_{\\mathrm{proj}}({\\mathbf{z}}_t) \\|_2^2$, and the Tikhonov functional  (\\ref{globalfunctionh}) can be written in terms of the projected problem, \n\\begin{eqnarray}\\label{globalfunctionproj}\nP^{\\zeta}({\\mathbf{z}})= \\| B_t {\\mathbf{z}} - \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}} \\|_2^2 + \\zeta^2 \\| {\\mathbf{z}} \\|_2^2, \\quad \n\\|{\\mathbf{h}}\\|_2^2=\\|{\\mathbf{z}}\\|_2^2.\n\\end{eqnarray}\nHere regularization parameter $\\zeta$ replaces $\\alpha$ while having the  same role as $\\alpha$ but for the projected case. Since the dimensions of $B_t$ are  small as compared to the dimensions of $\\tilde{\\tilde{G}}$,  the solution of the projected problem (\\ref{globalfunctionproj}) is obtained efficiently from \n\\begin{eqnarray*}\\label{zsolution}\n{\\mathbf{z}}_t(\\zeta)= (B_t^T B_t + \\zeta^2 I_t)^{-1} B_t^T \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}} = B_t(\\zeta)  \\| \\tilde{{\\mathbf{r}}}\\|_2 {\\mathbf{e}_{t+1}},  \n\\end{eqnarray*}\nwhere $ B_t(\\zeta) = (B_t^T B_t + \\zeta^2 I_t)^{-1} B_t^T$ can be obtained using the SVD, see appendix \\ref{svdsolution}, and the update for the global solution is immediately given by  ${\\mathbf{m}}_t(\\zeta)={\\mathbf{m}_{\\mathrm{apr}}}+ W^{-1}  A_t {\\mathbf{z}}_t(\\zeta)$.\n\nThe projected solution ${\\mathbf{z}}_t(\\zeta)$  depends on both the subspace size, $t$,  and the regularization parameter, $\\zeta$.  Our focus here is not on the determination of the optimal subspace size $t_{\\mathrm{opt}}$, rather we focus on the determination of $\\zeta_{\\mathrm{opt}}$, noting that finding $t_{\\mathrm{opt}}$ is a topic of significant study see for example the discussion in e.g. \\cite{RVA:2015}. For small $t$, the singular values of $B_t$, $\\gamma_i$, approximate the largest singular values of $\\tilde{\\tilde{G}}$, however, for larger $t$ the smaller singular values of $B_t$  approximate the  smallest singular values of  $\\tilde{\\tilde{G}}$, so that there is no immediate one to one alignment of the small singular values between $\\tilde{\\tilde{G}}$ and $B_t$ with increasing $t$. Thus, it is important to choose $t$ such that the dominant singular values of $\\tilde{\\tilde{G}}$ are well approximated by those of $B_t$ effectively capturing the dominant subspace for the solution.  We will discus the effect of choosing different $t$ on the solution.  Furthermore, the regularization parameter-choice methods used in this context, also need some modification for the projected case, which is demonstrated in the next section. \n\nAlthough $H_{t+1}$ and $A_t$ have orthonormal columns in exact arithmetic, Krylov methods lose orthogonality in finite precision. This means that after a relatively low number of iterations the vectors in $H_{t+1}$ and $A_t$  are no longer orthogonal and the relationship between (\\ref{globalfunctionh}) and (\\ref{globalfunctionproj}) does not hold. Here we therefore use  reorthogonalization to maintain the column orthogonality, which is also important for replicating the dominant spectral properties of $\\tilde{\\tilde{G}}$ by $B_t$. We use  Modified Gram Schmidt (MGS),  see Hansen \\shortcite{Hansen:2007}. We summarize the steps which are needed for implementation of the projected $L_{1}$ inversion in Algorithm~\\ref{projectedalgorithm} and note that in practice one may not need to explicitly calculate $\\tilde{\\tilde{G}}$, rather, for the factorization it can be sufficient to be able to efficiently perform operations with $\\tilde{G}$, $\\tilde{G}^T$, and diagonal matrices $W$, as discussed in  section~\\ref{l1method} in relation to Algorithm~\\ref{svdalgorithm}. \n\\begin{algorithm}\n\\caption{Iterative Projected $L_1$ Inversion Algorithm}\\label{projectedalgorithm}\n\\begin{algorithmic}[1]\n\\REQUIRE ${\\mathbf{d}_{\\mathrm{obs}}}$, ${\\mathbf{m}_{\\mathrm{apr}}}$, $G$, $W_{{\\mathbf{d}}}$, $\\epsilon > 0$, $\\rho_{\\mathrm{min}}$, $\\rho_{\\mathrm{max}}$, $t$, $K_{\\mathrm{max}}$\n\n\\STATE Calculate ${W_{\\mathrm{z}}}$, $\\tilde{G}= W_{{\\mathbf{d}}}G$, and ${\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}= W_{{\\mathbf{d}}}{\\mathbf{d}_{\\mathrm{obs}}}$\n\\STATE Initialize ${\\mathbf{m}}^{(0)}={\\mathbf{m}_{\\mathrm{apr}}}$,  ${W_{{L}_1}}^{(1)}=I_n$, $W^{(1)}={W_{\\mathrm{z}}}$\n\\STATE Calculate $\\tilde{{\\mathbf{r}}}^{(1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(0)}$, $\\tilde{\\tilde{G}}^{(1)}=\\tilde{G}(W^{(1)})^{-1} $, $k=0$\n\\WHILE {Not converged, \\eqref[noisetest] not satisfied, and $k<K_{\\mathrm{max}}$} \n\\STATE {$k=k+1$}\n\\STATE {Apply GKB: $\\tilde{\\tilde{G}} ^{(k)} A_t^{(k)} = H_{t+1}^{(k)}B_t^{(k)}, \\quad  H_{t+1}^{(k)}{\\mathbf{e}_{t+1}}=\\tilde{{\\mathbf{r}}}^{(k)}/\\| \\tilde{{\\mathbf{r}}}^{(k)}\\|_2$ }\n\\STATE {Find the SVD: $B_t^{(k)}=U\\Gamma V^T$}\n\\STATE {\\label{stepzeta}Use regularization parameter estimation to find $\\zeta^{(k)}$}\n\\STATE {\\label{stephupdate}Set ${\\mathbf{z}}_t^{(k)}= \\sum_{i=1}^{t} \\frac{\\gamma_i^2}{\\gamma_i^2+(\\zeta^{(k)})^2} \\frac{{\\mathbf{u}_i} ^T(\\| \\tilde{{\\mathbf{r}}}^{(k)}\\|_2{\\mathbf{e}_{t+1}})}{\\gamma_i} {\\mathbf{v}_i}$}\n\\STATE {Set ${\\mathbf{h}}_t^{(k)}=A_t^{(k)}{\\mathbf{z}}_t^{(k)}$}\n\\STATE {Set ${\\mathbf{m}}^{(k)}={\\mathbf{m}}^{(k-1)}+ (W^{(k)})^{-1}{\\mathbf{h}}_t^{(k)}$}\n\\STATE {Impose constraint conditions on ${\\mathbf{m}}^{(k)}$ to force $\\rho_{\\mathrm{min}}\\le {\\mathbf{m}}^{(k)} \\le \\rho_{\\mathrm{max}}$}\n\\STATE {Calculate the residual $\\tilde{{\\mathbf{r}}}^{(k+1)}={\\tilde{\\mathbf{d}}_{\\mathrm{obs}}}-\\tilde{G}{\\mathbf{m}}^{(k)}$}\n\\STATE {\\label{stepWL}Set ${W_{{L}_1}}^{(k+1)} ={\\mathrm{diag}}\\left( \\left(({\\mathbf{m}}^{(k)}-{\\mathbf{m}}^{(k-1)})^2+\\epsilon^2 \\right)^{-1/4}\\right)$,  as in \\eqref[WL], and\n$W^{(k+1)}={W_{{L}_1}}^{(k+1)}{W_{\\mathrm{z}}}$}\n\\STATE {Calculate $\\tilde{\\tilde{G}}^{(k+1)}=\\tilde{G}(W^{(k+1)})^{-1} $}\n\\ENDWHILE\n\\ENSURE Solution $\\rho={\\mathbf{m}}^{(k)}$. $K=k$.\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Regularization parameter estimation}\\label{parameter estimation}\nNow we focus on determination of the regularization parameter at each step $k$, supposing that  the dimension of the subspace, $t$, is known and kept fixed during the iterations. Our previous investigations have been shown that the method of the UPRE leads to an effective estimation of the regularization parameter \\cite{RVA:2015,VAR:2015,VRA:2014b}. In order to use the method for the projected problem we briefly review the derivation of the UPRE on the full problem. \n\n\\subsection{Unbiased predictive risk estimator}\\label{upre}\nAny method which is used to determine optimal $\\alpha$ should minimize the error between the solution ${\\mathbf{h}}(\\alpha)$ and the exact solution ${\\mathbf{h}}_{\\mathrm{exact}}$. Because the exact solution is unknown, an alternative error indicator, called the predictive error, is used \\cite{Vogel:2002}\n\\begin{eqnarray}\\nonumber\n\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha)) &=& \\tilde{\\tilde{G}} {\\mathbf{h}}(\\alpha) -\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\n=  \\tilde{\\tilde{G}} \\tilde{\\tilde{G}}(\\alpha)\\tilde{{\\mathbf{r}}}-\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} \\\\\n&=& H(\\alpha)(\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+\\tilde{{\\mbox{\\boldmath{$\\eta$}}}})-\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\n=(H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+ H(\\alpha)\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}\\label{predictiveerror}\n\\end{eqnarray}\nwhere $ H(\\alpha)=\\tilde{\\tilde{G}} \\tilde{\\tilde{G}}(\\alpha)$ is the influence matrix. The predictive error is also not\ncomputable because $\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}$ is unknown, however, it can be estimated using the full residual  (\\ref{fullresidual})\n\\begin{eqnarray}\\label{fullresidual2}\n\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha)) = \\tilde{\\tilde{G}} {\\mathbf{h}}(\\alpha) -\\tilde{{\\mathbf{r}}}\n=(H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}\n = (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}+(H(\\alpha)-I_m)\\tilde{{\\mbox{\\boldmath{$\\eta$}}}} .\n\\end{eqnarray} \nFor both (\\ref{predictiveerror}) and (\\ref{fullresidual2}), the first term on the right hand side is deterministic, whereas the second is stochastic. Applying the Trace lemma, e.g. \\cite{Vogel:2002}, for both equations and using the symmetry of the influence matrix we obtain\n\\begin{eqnarray}\\label{Pexpected}\nE(\\|\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2) &=& \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\\|_2^2 + \\mathrm{trace}(H^T(\\alpha)H(\\alpha)), \\,\\,\\mathrm{and} \n\\\\E(\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2) &=& \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}_{\\mathrm{exact}}\\|_2^2 + \\mathrm{trace}((H(\\alpha)-I_m)^T(H(\\alpha)-I_m))\\label{Rexpected} .\n\\end{eqnarray}\nHere $E(\\|\\mathbf{P}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2)/m$ is the expected value of the predictive risk \\cite{Vogel:2002}. The first terms in the right hand sides of (\\ref{Pexpected}) and (\\ref{Rexpected}) are the same. Thus, by the linearity of the trace operator and with $E(\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2)\\approx\\|\\mathbf{R}_{\\mathrm{full}}({\\mathbf{h}}(\\alpha))\\|_2^2= \\| (H(\\alpha)-I_m)\\tilde{{\\mathbf{r}}}\\|_2^2 $,  the UPRE estimator of the optimal parameter is \n\\begin{eqnarray}\\label{fullupre}\n\\alpha_{\\mathrm{opt}}={\\textnormal{arg} \\min_{{\\alpha}}}\\{U(\\alpha):= \\|(H(\\alpha) -I_m)\\tilde{{\\mathbf{r}}}\\|_2^2 +2\\,\\mathrm{trace}(H(\\alpha)) - m\\}.\n\\end{eqnarray}\nTypically $\\alpha_{\\mathrm{opt}}$ is found by evaluating (\\ref{fullupre}) for a range of $\\alpha$, for example by the SVD see  appendix \\ref{svdparameter}, with the minimum found within that range of parameter values.\n\\subsection{Extending the UPRE for the projected problem}\\label{projectedupre}\nFor extending the UPRE parameter-choice method to a subspace, we first observe that given $\\tilde{{\\mathbf{r}}} = \\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} +\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$, then $\\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}} =H_{t+1}^T \\tilde{{\\mathbf{r}}}$ consists of a deterministic and stochastic part, $H_{t+1}^T \\tilde{{\\mathbf{r}}}_{\\mathrm{exact}} + H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ , where for white noise vector $\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ and column orthogonal $H_{t+1}$, $H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ is a random vector of length $t+1$ with covariance matrix $I_t$. Thus, from the derivation of UPRE for the full problem defined by the system matrix $\\tilde{\\tilde{G}}$, right hand side $\\tilde{{\\mathbf{r}}}$ and white noise vector $\\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$ we immediately\nwrite down the UPRE for the projected problem with system matrix $B_t$, right hand side $ H_{t+1}^T \\tilde{{\\mathbf{r}}}$ and white noise vector $ H_{t+1}^T \\tilde{{\\mbox{\\boldmath{$\\eta$}}}}$\n\\begin{eqnarray}\\label{subupre}\n\\zeta_{\\mathrm{opt}}={\\textnormal{arg} \\min_{{\\zeta}}}\\{U(\\zeta):= \\|(B(\\zeta) -I_{t+1}) \\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}}\\|_2^2 +2\\,\\mathrm{trace}(B(\\zeta)) - (t+1)\\}.\n\\end{eqnarray}\nHere $ B(\\zeta)= B_t B_t(\\zeta)$ is the influence matrix for the  subspace. As for the full problem, see appendix~\\ref{svdparameter}, the SVD of the matrix $B_t$ can be used to find $\\zeta_{\\mathrm{opt}}$. In section~\\ref{cube} we show that in some situations \\eqref[subupre], as introduced in \\cite{RVA:2015}, does not work well. Here, a modification  is introduced that does not use the entire subspace for a given $t$, but rather uses a truncated spectrum from $B_t$ for finding the regularization parameter, thus assuring that the dominant $t_{\\mathrm{trunc}}$ singular values are appropriately regularized. \n\n\\section{Synthetic examples}\\label{synthetic}\n\\subsection{Model of an embedded cube}\\label{cube}\nThe initial goal of the presented verification with simulated data is to contrast Algorithms~\\ref{svdalgorithm} and \\ref{projectedalgorithm}. \nWe use a  simple small-scale model that includes a cube with density contrast   $1$~g~cm$^{-3}$ embedded in an homogeneous background, Fig.~\\ref{2a}. Simulation data on the surface, ${\\mathbf{d}_{\\mathrm{exact}}}$, are calculated over a $ 20 \\times 20 $ regular grid with $50$ m grid spacing. To add noise to the  data, a zero mean Gaussian random matrix $\\Theta$ of size m $\\times 10$ was generated. Then, setting\n\n", "index": 5, "text": "\\begin{equation}\\label{noisydata}\n{\\mathbf{d}_{\\mathrm{obs}}}^c={\\mathbf{d}_{\\mathrm{exact}}}+\\left( \\tau_1 ({\\mathbf{d}_{\\mathrm{exact}}})_i+\\tau_2 \\|{\\mathbf{d}_{\\mathrm{exact}}}\\|\\right) \\Theta^c,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{d}_{\\mathrm{obs}}}^{c}={\\mathbf{d}_{\\mathrm{exact}}}+\\left(\\tau_{1}({%&#10;\\mathbf{d}_{\\mathrm{exact}}})_{i}+\\tau_{2}\\|{\\mathbf{d}_{\\mathrm{exact}}}\\|%&#10;\\right)\\Theta^{c},\" display=\"block\"><mrow><mrow><mmultiscripts><mi>\ud835\udc1d</mi><mi>obs</mi><none/><none/><mi>c</mi></mmultiscripts><mo>=</mo><mrow><msub><mi>\ud835\udc1d</mi><mi>exact</mi></msub><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mrow><msub><mi>\u03c4</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1d</mi><mi>exact</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\u03c4</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo>\u2225</mo><msub><mi>\ud835\udc1d</mi><mi>exact</mi></msub><mo>\u2225</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0398</mi><mi>c</mi></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00114.tex", "nexttext": "  are recorded.  Table~\\ref{tab1} gives the  average and standard deviation of  $\\alpha^{(K)}$, $RE^{(K)}$ and $K$ over the $10$ samples.  It was explained by Farquharson \\shortcite{Far:2004} that it is  efficient if the inversion starts with a large value of the regularization parameter. This prohibits imposing excessive structure in the model at early iterations which would otherwise require more iterations to remove artificial structure. In this paper the method introduced by Vatankhah et.al. \\shortcite{VAR:2014a,VAR:2015} was used to determine an initial regularization parameter, $\\alpha^{(1)}$. Because the non zero singular values, $\\sigma_i$ of matrix $\\tilde{\\tilde{G}}$ are known, the initial value \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n for $c=1:10$, with noise parameter pairs $(\\tau_1$, $\\tau_2)$, for three choices, $N1: (0.01,0.001)$,  $N2: (0.02,0.005)$ and $N3: (0.03,0.01) $, gives  $10$ noisy right-hand side vectors for $3$ levels of noise. Fig.~\\ref{2b} shows noise-contaminated data for one right-hand side, here $c=7$, and for $N2$.\n\\begin{figure*}\n\\subfigure{\\label{2a}\\includegraphics[width=.45\\textwidth]{figure2a.pdf}}\n\\subfigure{\\label{2b}\\includegraphics[width=.45\\textwidth]{figure2b.pdf}}\n\\caption { (a) Model of the cube on an homogeneous background. The density contrast of the cube is $1$~g~cm$^{-3}$. (b) Data due to the model and contaminated with  noise $N2$.}\n\\end{figure*}\nFor the inversion  the model region of depth $500$~m, is discretized into $ 20 \\times 20 \\times 10 = 4000$ cells of size $50$m in each dimension. The background model ${\\mathbf{m}_{\\mathrm{apr}}}=\\mathbf{0}$ and parameter $\\epsilon^2=1e{-9}$ are chosen for the inversion. Realistic upper and lower density bounds $\\rho_{\\mathrm{max}}=1$~g~cm$^{-3}$ and $\\rho_{\\mathrm{min}}=0$~g~cm$^{-3}$, are specified. The iterations are terminated when, approximating \\eqref[noisetest], $\\chi_{\\mathrm{Computed}}^2 \\leq 429 $,  or $k>K_{\\mathrm{max}} =50$. \n\\subsection{Solution using Algorithm~\\ref{svdalgorithm}}\\label{cube:alg1}\nThe inversion was performed using Algorithm~\\ref{svdalgorithm} for the $3$ noise levels, and all $10$ right-hand side data vectors. The final iteration $K$, the final regularization parameter $\\alpha^{(K)}$ and the relative error of the reconstructed model \n\n", "index": 7, "text": "\\begin{equation}\\label{RE}\nRE^{(K)}=\\frac{\\|{\\mathbf{m}}_{\\mathrm{exact}}-{\\mathbf{m}} ^{(K)} \\|_2}{\\|{\\mathbf{m}}_{\\mathrm{exact}} \\|_2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"RE^{(K)}=\\frac{\\|{\\mathbf{m}}_{\\mathrm{exact}}-{\\mathbf{m}}^{(K)}\\|_{2}}{\\|{%&#10;\\mathbf{m}}_{\\mathrm{exact}}\\|_{2}}\" display=\"block\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><msup><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>=</mo><mfrac><msub><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc26</mi><mi>exact</mi></msub><mo>-</mo><msup><mi>\ud835\udc26</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc26</mi><mi>exact</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00114.tex", "nexttext": "\n can be selected.  For subsequent iterations the UPRE method is used to estimate $\\alpha^{(k)}$.\n\n\\begin{table}\n\n\\caption{The inversion results obtained by inverting the data from the cube using Algorithm~\\ref{svdalgorithm},  with $\\epsilon^2=1e{-9}$, average (standard deviation) over $10$ runs.}\\label{tab1}\n\\begin{tabular}{c  c  c  c  c }\n\\hline\nNoise&     $\\alpha^{(1)}$& $\\alpha^{(K)}$& $RE^{(K)}$& $K$  \\\\ \\hline\n$N1$ &  47769.1& 117.5(10.6)& 0.319(0.017)& 8.2(0.4)\\\\\n$N2$ &  48623.4& 56.2(8.5)& 0.388(0.023)& 6.1(0.6)\\\\\n$N3$ &  48886.2& 36.2(9.1)& 0.454(0.030)& 5.8(1.3)\\\\ \\hline\n\\end{tabular}\n\n\\end{table}\n\n\nTo illustrate the results, Figs.~\\ref{fig3}-\\ref{fig6} provide details for right-hand side $c=7$ and for all noise levels. Fig.~\\ref{fig3} shows the reconstructed models, indicating that a focused image of the subsurface is possible in all cases using Algorithm~\\ref{svdalgorithm}. The constructed models have sharp and distinct interfaces with the embedded medium. The progression of the data misfit $\\Phi({\\mathbf{m}})$, the regularization term $ S({\\mathbf{m}})$ and regularization parameter $\\alpha^{(k)}$ with iteration $k$ are presented in Fig.~\\ref{fig4}. $\\Phi({\\mathbf{m}})$ is initially large and decays quickly in the first few steps,  but the decay rate decreases dramatically as $k$ increases.  Fig.~\\ref{fig5} shows the progression of the relative error, \\eqref[RE], as a function of $k$. In all cases there is a dramatic decrease in the relative error for small $k$, after which the error decreases slowly. The UPRE functional for iteration $k=4$ is shown in Fig.~\\ref{fig6}. Clearly, for all cases the curves have a nicely defined minimum, which is important in the determination of the regularization parameter. We return to this when considering the projected case. \n\nTo determine the dependence  of Algorithm~\\ref{svdalgorithm} on other values of $\\epsilon^2$, we examined the results using   right-hand side data for $c=7$ for noise $N2$ with $\\epsilon ^2=0.5$ and $\\epsilon^2=1e{-15}$ with all other parameters chosen  as before. For $\\epsilon^2=1e{-15}$ the results are very close to those obtained with $\\epsilon^2=1e{-9}$, and are not presented here. For  $\\epsilon ^2=0.5$ the results are significantly different. Fig.~\\ref{3d} shows the reconstructed model, indicating a smeared-out and fuzzy image of the original model. The maximum of the obtained density is about $0.85$~g~cm$^{-3}$,  $85\\%$ of the imposed $\\rho_{\\mathrm{max}}$.  The progression of the data misfit, the regularization term and regularization parameter are presented in Fig.~\\ref{4d}, while the relative error functional and UPRE curve at iteration 4 are shown in Fig.~\\ref{5d} and Fig.~\\ref{6d}, respectively. Clearly more iterations are needed to terminate the algorithm, $K=31$, and at the final iteration $\\alpha^{(31)}=1619.2$ and $RE^{(K)}=0.563$ respectively, larger than their counterparts in the case $\\epsilon^2=1e{-9}$.\n\n\n\\begin{figure*}\n\\subfigure{\\label{3a}\\includegraphics[width=.45\\textwidth]{figure3a.pdf}}\n\\subfigure{\\label{3b}\\includegraphics[width=.45\\textwidth]{figure3b.pdf}}\n\\subfigure{\\label{3c}\\includegraphics[width=.45\\textwidth]{figure3c.pdf}}\n\\subfigure{\\label{3d}\\includegraphics[width=.45\\textwidth]{figure3d.pdf}}\n\\caption {The reconstructed model obtained by inverting the noise-contaminated right-hand side with $c=7$ using Algorithm~\\ref{svdalgorithm} for noise cases (a) $N1$; (b) $N2$;  (c) $N3$ with $\\epsilon^2=1e{-9}$  and (d) $N2$ when $\\epsilon^2=0.5$.} \\label{fig3}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{4a}\\includegraphics[width=.24\\textwidth]{figure4a.pdf}}\n\\subfigure{\\label{4b}\\includegraphics[width=.24\\textwidth]{figure4b.pdf}}\n\\subfigure{\\label{4c}\\includegraphics[width=.24\\textwidth]{figure4c.pdf}}\n\\subfigure{\\label{4d}\\includegraphics[width=.24\\textwidth]{figure4d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ using Algorithm~\\ref{svdalgorithm}. The progression of the data misfit,  $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$ for   noise cases (a) $N1$; (b) $N2$;  (c) $N3$ with $\\epsilon^2=1e{-9}$ and (d) $N2$ when $\\epsilon^2=0.5$.} \\label{fig4}\n\\end{figure*}\n\n\\begin{figure*}  \n\\subfigure{\\label{5a}\\includegraphics[width=.24\\textwidth]{figure5a.pdf}}\n\\subfigure{\\label{5b}\\includegraphics[width=.24\\textwidth]{figure5b.pdf}}\n\\subfigure{\\label{5c}\\includegraphics[width=.24\\textwidth]{figure5c.pdf}}\n\\subfigure{\\label{5d}\\includegraphics[width=.24\\textwidth]{figure5d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ using Algorithm~\\ref{svdalgorithm}. The progression of the relative error at each iteration for  noise cases (a) $N1$; (b) $N2$;  (c) $N3$ with $\\epsilon^2=1e{-9}$ and (d) $N2$ when $\\epsilon^2=0.5$.} \\label{fig5}\n\\end{figure*}\n\n\n\\begin{figure*}\n\\subfigure{\\label{6a}\\includegraphics[width=.24\\textwidth]{figure6a.pdf}}\n\\subfigure{\\label{6b}\\includegraphics[width=.24\\textwidth]{figure6b.pdf}}\n\\subfigure{\\label{6c}\\includegraphics[width=.24\\textwidth]{figure6c.pdf}}\n\\subfigure{\\label{6d}\\includegraphics[width=.24\\textwidth]{figure6d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ using Algorithm~\\ref{svdalgorithm}. The UPRE functional at iteration $4$ for   noise cases (a) $N1$; (b) $N2$;  (c) $N3$ with $\\epsilon^2=1e{-9}$ and (d) $N2$ when $\\epsilon^2=0.5$.} \\label{fig6}\n\\end{figure*}\n\\subsection{Solution using Algorithm~\\ref{projectedalgorithm}}\\label{cube:alg2}\nAlgorithm~\\ref{projectedalgorithm} is used to reconstruct the model for $4$ different values of $t$, $t=3$, $100$, $200$ and $400$, in order to examine the impact of the size of the projected subspace  on the solution and the estimated parameter $\\zeta$.  The results for $t<m$, are given in Table~\\ref{tab2new}. Here, in order to compare the algorithms,  the initial regularization parameter, $\\zeta^{(1)}$, is set to the value that would be used on the full space. Generally, for small $t$ the estimated  regularization parameter is less than the counterpart obtained for the  full case for the specific noise level. Comparing Tables~\\ref{tab1} and ~\\ref{tab2new} it is clear that with increasing $t$, the estimated $\\zeta$ increases,  reaching $\\alpha^{(K)}$ of the full space when $t=m$. For $t=3$,   the algorithm is computationally very fast and the relative error of the  reconstructed model is acceptable, but still larger than that obtained using the full model. As $t$ becomes greater than  $3$, and approaches $t=200$, the results are not satisfactory. For a sample case, $t=100$, the results are presented in Table~\\ref{tab2new}. The relative error is very large and the reconstructed model is generally not acceptable. Although the results with the least noise  are acceptable, they are still worse than the results obtained with the other selected choices for $t$. In this case, $t=100$, and for high noise levels, the algorithm usually terminates when it reaches $k=K_{\\mathrm{max}}=50$, indicating that the solution does not satisfy the noise level constraint \\eqref[noisetest]. For $t=200$ the results are again acceptable, although less satisfactory than the results obtained with the full space. With increasing $t$ the results improve, until for $t=m=400$ the results, not given here,  reproduce, as expected,  those obtained with Algorithm~\\ref{svdalgorithm}. \n\n\n\n\\begin{table}\n\\caption{The inversion results obtained by inverting the data from the cube using Algorithm~\\ref{projectedalgorithm},  with $\\epsilon^2=1e{-9}$,  and $\\zeta^{(1)} = \\alpha^{(1)}$ for the specific noise level as given in Table~\\ref{tab1}. In each case the average (standard deviation) over $10$ runs. The rows corresponding to noise cases $N1$, $N2$ and $N3$, resp.}\\label{tab2new}\n\\begin{tabular}{c  c  c  c  c  c c c c c }\n\\hline\n\\multicolumn{3}{c}{t=3}&\\multicolumn{3}{c}{t=100}&\\multicolumn{3}{c}{t=200}\\\\ \\cline{2-9}\n $\\zeta^{(K)}$& $RE^{(K)}$& $K$& $\\zeta^{(K)}$& $RE^{(K)}$& $K$ & $\\zeta^{(K)}$& $RE^{(K)}$& $K$\\\\ \\hline\n  75.5(2.9)& .427(.037)& 13.5(0.5)&  98.9(12.0)& .452(.043)& 10.0(0.7)& 102.2(11.3)& .330(.019)& 8.8(0.4)\\\\\n 46.7(14.2)& .472(.041)& 7.8(0.6)&   42.8(10.4)& 1.009(.184)& 28.1(10.9)&   43.8(7.0)& .429(.053)& 6.7(0.8)\\\\\n    25.6(4.7)& .493(.03)& 6.1(0.7)&    8.4(13.3)& 1.118(.108)& 42.6(15.6)&    27.2(6.3)& .463(.036)& 5.5(0.5)\\\\ \\hline\n\\end{tabular}\n\\end{table}\n\n\n\nTo illustrate the results, we show the reconstructed models using Algorithm~\\ref{projectedalgorithm} with different $t$ and for right-hand side $c=7$ for noise $N2$,  Fig.~\\ref{fig7}. As illustrated, the reconstructed models for $t=3$, $200$ and $400$ are acceptable, while for $t=100$ the results are completely wrong. For some right-hand sides $c$ with $t=100$, the reconstructed models may be much worse than that  shown in Fig.~\\ref{7b}. The progression of the data misfit, the regularization term and the regularization parameter with iteration $k$ are presented in Fig.~\\ref{fig8}, while  $RE^{(K)}$ is shown in  Fig.~\\ref{fig9}.  For  $t=100$ and for high noise levels, usually the estimated value for $\\zeta^{(k)}$ using  \\eqref[subupre] for $1<k<K$ is small, corresponding to under regularization and yielding  a large error in the solution. To understand why the UPRE leads to under regularization  we illustrate the UPRE curves for iteration $k=4$ in Fig.~\\ref{fig10}. It is immediate that when using  small  $t$, $U(\\zeta)$ may not have a unique  minimum, and thus the algorithm may find a  minimum at a small regularization parameter which leads to under regularization of the dominant, and more accurate,  terms in the expansion. This can cause problems for moderate $t$,  $ t<200$. On the other hand, as $t$ increases, e.g. for $t=200$ and $400$,  it appears that there is a unique minimum of $U(\\zeta)$ and the regularization parameter found is appropriate.  Unfortunately, this situation creates a conflict with the need to use $t \\ll m$ for large scale problems.   \n\n\\begin{figure*}xf\n\\subfigure{\\label{7a}\\includegraphics[width=.45\\textwidth]{figure7a.pdf}}\n\\subfigure{\\label{7b}\\includegraphics[width=.45\\textwidth]{figure7b.pdf}}\n\\subfigure{\\label{7c}\\includegraphics[width=.45\\textwidth]{figure7c.pdf}}\n\\subfigure{\\label{7d}\\includegraphics[width=.45\\textwidth]{figure7d.pdf}}\n\\caption {The reconstructed model obtained by inverting the noise-contaminated right-hand side with $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm}  with $\\epsilon^2=1e{-9}$.  (a) $t=3$; (b) $t=100$;  (c) $t=200$; and (d)  $t=400$.} \\label{fig7}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{8a}\\includegraphics[width=.24\\textwidth]{figure8a.pdf}}\n\\subfigure{\\label{8b}\\includegraphics[width=.24\\textwidth]{figure8b.pdf}}\n\\subfigure{\\label{8c}\\includegraphics[width=.24\\textwidth]{figure8c.pdf}}\n\\subfigure{\\label{8d}\\includegraphics[width=.24\\textwidth]{figure8d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm} with $\\epsilon^2=1e{-9}$. The progression of the data misfit, $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$,  for (a) $t=3$; (b) $t=100$; (c)  $t=200$; and (d)  $t=400$.} \\label{fig8}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{9a}\\includegraphics[width=.24\\textwidth]{figure9a.pdf}}\n\\subfigure{\\label{9b}\\includegraphics[width=.24\\textwidth]{figure9b.pdf}}\n\\subfigure{\\label{9c}\\includegraphics[width=.24\\textwidth]{figure9c.pdf}}\n\\subfigure{\\label{9d}\\includegraphics[width=.24\\textwidth]{figure9d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm} with $\\epsilon^2=1e{-9}$. The progression of the relative error at each iteration for (a) $t=3$; (b) $t=100$; (c)  $t=200$; and (d)  $t=400$.} \\label{fig9}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{10a}\\includegraphics[width=.24\\textwidth]{figure10a.pdf}}\n\\subfigure{\\label{10b}\\includegraphics[width=.24\\textwidth]{figure10b.pdf}}\n\\subfigure{\\label{10c}\\includegraphics[width=.24\\textwidth]{figure10c.pdf}}\n\\subfigure{\\label{10d}\\includegraphics[width=.24\\textwidth]{figure10d.pdf}}\n\\caption {Inverting the noise-contaminated right-hand side with $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm} with $\\epsilon^2=1e{-9}$. The UPRE functional at iteration $4$  for (a) $t=3$; (b) $t=100$; (c)  $t=200$; and (d)  $t=400$.} \\label{fig10}\n\\end{figure*}\n\n\\subsection{Extending the projected UPRE by spectrum truncation}\\label{cube:tupre} \nTo determine the reason for the difficulty with using $U(\\zeta)$ to find an optimal $\\zeta$ for moderate $t$, we illustrate the singular values for $B_t$ and  $\\tilde{\\tilde{G}}$   in Fig.~\\ref{fig11} for the chosen choices of $t$  for iteration  $4$ and for the case with $t=100$ the singular values at iterations $1$, $3$, $5$ and the final iteration $14$. It can be seen that for moderate $t$,  we have $\\gamma_i \\approx \\sigma_i$ only for $i=1:t^*$ for some $t^*<t$, where as seen from Figs.~\\ref{11e}-\\ref{11h}, $t^*$ is approximately preserved with the iterative steps. Now, for $i>t^*$, $\\gamma_i<< \\sigma_i$, as $B_t$ reflects the overall condition of the problem. This generates regularization parameters which are determined by the smallest $\\gamma_i$, rather than the dominant terms. Suppose, on the other hand, that we use $t$ steps of the GKB on matrix $\\tilde{\\tilde{G}}$ to obtain $B_t$, but use $t_{\\mathrm{trunc}}=\\omega \\, t$, $\\omega<1$,  singular values of $B_t$ in estimating both $\\zeta$ and $z_t$, in steps \\ref{stepzeta} and \\ref{stephupdate} of Algorithm~\\ref{projectedalgorithm}. Our  examinations, see  for example Fig.~\\ref{fig11},  suggest taking   $\\omega\\approx0.8$, with this choice consistent across all iterations.    With this choice the smallest singular values of $B_t$ are ignored in estimating $\\zeta$ and $z_t$. \n\nWe denote the approach, in which we replace steps \\ref{stepzeta} and \\ref{stephupdate} in Algorithm~\\ref{projectedalgorithm}  with estimates using $t_{\\mathrm{trunc}}$,   truncated UPRE (TUPRE).  We comment that the approach may work equally well for alternative regularization techniques, but this is not a topic of the current investigation, as is a detailed investigation for the choice of $\\omega$. For example, our investigations show that $\\omega$ may be estimated at the first iteration by examining the singular values for $B_{\\hat{t}}$ where $\\hat{t} >t$, say $\\hat{t}=1.1t$ and comparing how many of the singular values for $B_t$ are close to the first $t$ of these for $B_{\\hat{t}}$, even though for this first iteration $\\zeta^{(1)}$ is set large as in the estimation of $\\alpha^{(1)}$, using \\eqref[initalpha],  \\cite{VAR:2014a,VAR:2015}. If the relative change in the spectral value is say greater than $10\\%$ for a given $i$ then this suggests using $t_{\\mathrm{trunc}} =i$, and generally corresponds to our choice $\\omega\\approx .8$. Furthermore, we note this   is not a standard filtered truncated SVD for the solution, rather the truncation here is determined for the given projected problem and is based on the accuracy of the largest possible projected subspace for a given $t$, which is a fraction of the anticipated subspace size $t$.  To show the efficiency of TUPRE, we run the inversion algorithm for case $t=100$ for which the original results are not realistic.  The results using TUPRE are given in Table~\\ref{tab3} for noise $N1$, $N2$ and $N3$, and illustrated, for right-hand side $c=7$ for noise $N2$, in Fig.~\\ref{fig12}. Fig.~\\ref{12c} shows the  existence of a well-defined minimum for $U(\\zeta)$   at iteration $k=4$, as compared to Fig.~\\ref{10b}, although this is not preserved over all iterations.  Reconstructed models using TUPRE  for  $t=10$, $20$, $30$ and $40$ are illustrated in Fig.~\\ref{fig13}. The results indicate that the use of TUPRE yields  acceptable solutions for these moderate choices of $t$.  Although these results show that small $t$ can be used in the method, we suggest that $t > m/20$ is a suitable choice for this application. \nFor high noise levels using $\\omega\\approx0.7$ may improve the results. Here we use   $\\omega\\approx0.8$ for all cases. \n\n\n\\begin{figure*}\n\\subfigure{\\label{11a}\\includegraphics[width=.24\\textwidth]{figure11a.pdf}}\n\\subfigure{\\label{11b}\\includegraphics[width=.24\\textwidth]{figure11b.pdf}}\n\\subfigure{\\label{11c}\\includegraphics[width=.24\\textwidth]{figure11c.pdf}}\n\\subfigure{\\label{11d}\\includegraphics[width=.24\\textwidth]{figure11d.pdf}}\n\\subfigure{\\label{11e}\\includegraphics[width=.24\\textwidth]{figure11e.pdf}}\n\\subfigure{\\label{11f}\\includegraphics[width=.24\\textwidth]{figure11f.pdf}}\n\\subfigure{\\label{11g}\\includegraphics[width=.24\\textwidth]{figure11g.pdf}}\n\\subfigure{\\label{11h}\\includegraphics[width=.24\\textwidth]{figure11h.pdf}}\n\\caption {The singular values of $\\tilde{\\tilde{G}}$, blue $\\diamond$, and $B_t$, red $\\star$, at iteration $4$  using Algorithm~\\ref{projectedalgorithm} with $\\epsilon^2=1e{-9}$ for  the noise-contaminated right-hand side with $c=7$ and noise $N2$. (a) $t=3$; (b) $t=100$;   (c)   $t=200$ and $t=400$, for plots \\ref{11a}-\\ref{11d}. In \\ref{11e}-\\ref{11h} the singular values of $B_t$ for $t=100$ and the first $150$ singular values of $\\tilde{\\tilde{G}}$, for iterations $1$, $3$, $5$ and the final iteration $14$. } \\label{fig11}\n\\end{figure*}\n\n\\begin{table}\n\n\\caption{The inversion results obtained by inverting the data from the cube using Algorithm~\\ref{projectedalgorithm}, with $\\epsilon^2=1e{-9}$, using TUPRE with $t=100$, and $\\zeta^{(1)} = \\alpha^{(1)}$ for the specific noise level as given in Table~\\ref{tab1}. In each case the average (standard deviation) over $10$ runs.}\\label{tab3}\n\\begin{tabular}{c  c  c  c  c }\n\\hline\nNoise&      $\\zeta^{(K)}$& $RE^{(K)}$& $K$  \\\\ \\hline\n$N1$ &    148.0(12.5)& 0.299(0.010)& 6.7(0.7)\\\\\n$N2$ &  62.6(4.7)& 0.384(0.035)& 6.4(0.5)\\\\\n$N3$ &   33.2(4.7) & 0.445(0.034) & 6.7(1.1)\\\\ \\hline\n\\end{tabular}\n\n\\end{table}\n\n\\begin{figure*}\n\\subfigure{\\label{12a}\\includegraphics[width=.3\\textwidth]{figure12a.pdf}}\n\\subfigure{\\label{12b}\\includegraphics[width=.3\\textwidth]{figure12b.pdf}}\n\\subfigure{\\label{12c}\\includegraphics[width=.3\\textwidth]{figure12c.pdf}}\n\\subfigure{\\label{12d}\\includegraphics[width=.45\\textwidth]{figure12d.pdf}}\n\\caption {Results for right-hand side $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm}, with $\\epsilon^2=1e{-9}$ using TUPRE  when $t=100$ is chosen;  (a) The progression of the data misfit,  $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$;  (b) The progression of the relative error at each iteration;    (c) The TUPRE functional at iteration $4$;\nand (d) The reconstructed model. } \\label{fig12}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{13a}\\includegraphics[width=.45\\textwidth]{figure13a.pdf}}\n\\subfigure{\\label{13b}\\includegraphics[width=.45\\textwidth]{figure13b.pdf}}\n\\subfigure{\\label{13c}\\includegraphics[width=.45\\textwidth]{figure13c.pdf}}\n\\subfigure{\\label{13d}\\includegraphics[width=.45\\textwidth]{figure13d.pdf}}\n\\caption {The reconstructed model by inverting noise-contaminated for right-hand side $c=7$ for noise $N2$ using Algorithm~\\ref{projectedalgorithm}, with $\\epsilon^2=1e{-9}$, using TUPRE. (a) $t=10$; (b) $t=20$;  (c)  $t=30$ and (d)  $t=40$.} \\label{fig13}\n\\end{figure*}\n\n\n\\subsection{Model of multiple embedded bodies}\\label{multiplebodies}\nA model consisting of four bodies with various geometries, sizes, depths and densities is used to verify the ability and limitations of  Algorithm~\\ref{projectedalgorithm} implemented with TUPRE for the recovery of large-scale and more complex structures. Fig.~\\ref{14a} shows a perspective view of this model. The densities of the dipping dike, anomaly A, and cube B are $0.8$~g~cm$^{-3}$, and $1$~g~cm$^{-3}$ for  cubes C and D. Fig.~\\ref{fig15} shows four plane-sections of the model. The surface gravity data are calculated on a $60 \\times 60$ grid with $100$ m spacing, for a data vector of length $3600$. Noise is added to the exact data vector as in  \\eqref[noisydata] with $(\\tau_1, \\tau_2)=(.02, .002)$. Fig.~\\ref{14b} shows the noise-contaminated data. \n\nThe subsurface extends to depth $1000$~m with cells of size $100$~m in each dimension yielding the unknown model parameters to be found on $ 60 \\times 60 \\times 10 = 36000$ cells. The inversion assumes  ${\\mathbf{m}_{\\mathrm{apr}}}=\\mathbf{0}$, $\\epsilon^2=1e{-9}$ and imposes density bounds $\\rho_{\\mathrm{min}}=0$~g~cm$^{-3}$  and $ \\rho_{\\mathrm{max}}=1$~g~cm$^{-3}$. The iterations are terminated when,  approximating \\eqref[noisetest], $\\chi_{\\mathrm{Computed}}^2 \\leq 3685 $,  or $k>K_{\\mathrm{max}}=100$. The inversion is performed using Algorithm~\\ref{projectedalgorithm} but with the TUPRE  solution methods for steps \\ref{stepzeta} amd \\ref{stephupdate}. The initial regularization parameter is  $\\zeta^{(1)} = (n/m)^{3.5}(  \\gamma_1/\\mathrm{mean(\\gamma_i)})$, for $\\gamma_i$, $i=1:t $.\n\nFig.~\\ref{fig16} shows the resulting model for the inversion, when $t=200$ is chosen. The progression of the data misfit, the regularization term and the regularization parameter with iteration $k$, the TUPRE functional at the final iteration, and the progression of the relative error at each iteration are presented in Fig.~\\ref{fig17}. Convergence is reached after $11$ iterations, and $\\zeta^{(11)}=50.3$. As illustrated, the horizontal borders of the bodies are recovered and the depths to the top  are close to those of  the original model. At the intermediate depth, the shapes of the anomalies are reconstructed well, while deeper in the subsurface additional structures appear. The reconstruction of the dipping  dike is acceptable but does not completely match the original model. Using the incorrect upper density bounds for anomalies A and B, impacts the resulting model. For anomaly B the maximum density is obtained $1$~g~cm$^{-3}$, although in deeper parts it is close to the true value $0.8$~g~cm$^{-3}$. Algorithm~\\ref{projectedalgorithm} is very fast, requiring only a few minutes, dependent on the choice of  $t$. \n\\subsection{Comparison of $L_1$ and MS stabilizers}\\label{contrastL1MS}\n To compare with  results obtained using the MS stabilizer, we implement Algorithm~\\ref{projectedalgorithm} with step~\\ref{stepWL}  for finding  ${W_{{L}_1}}$  replaced by calculation of ${W_{\\epsilon}}$ as in \\eqref[We].  Equivalently we replace $p=1$ by $p=0$ in \\eqref[generalnorm]. We  use $\\epsilon^{2}=1e{-5}$. The results are presented in Figs.~\\ref{fig18} and \\ref{fig19}. Although the MS solution is more focused, the convergence of the solution to an acceptable error level takes more iterations. Here the algorithm terminates at $k=K_{\\mathrm{max}}=K_{{100}}$, indicating that the noise level condition \\eqref[noisetest] was not achieved.   We note that, although,  smaller values of $\\epsilon$ provide a more focused image, the solution is less stable.  For example, using $\\epsilon^{2}=1e^{-9}$ the reconstructed model is more focused but is less stable. In contrast, as indicated in Section~\\ref{cube}, this is not an important issue for the $L_1$ stabilizer which is less sensitive to the choice of $\\epsilon$.  Although there are advantages and disadvantages to each algorithm, neither is superior, it is clear that  the $L_1$ stabilizer offers an acceptable and efficient alternative to the standard MS approach.\n\n\n\\begin{figure*}\n\\subfigure{\\label{14a}\\includegraphics[width=.53\\textwidth]{figure14a.pdf}}\n\\subfigure{\\label{14b}\\includegraphics[width=.43\\textwidth]{figure14b.pdf}}\n\\caption {  (a) The perspective view of the model. Four different bodies embedded in an homogeneous background. Densities of  A and B are $0.8$~g~cm$^{-3}$ and  C and D are $1$~g~cm$^{-3}$; (b) The noise contaminated gravity anomaly due to the model.} \\label{fig14}\n\\end{figure*}\n\n\n\\begin{figure*}\n\\subfigure{\\label{15a}\\includegraphics[width=.45\\textwidth]{figure15a.pdf}}\n\\subfigure{\\label{15b}\\includegraphics[width=.45\\textwidth]{figure15b.pdf}}\n\\subfigure{\\label{15c}\\includegraphics[width=.45\\textwidth]{figure15c.pdf}}\n\\subfigure{\\label{15d}\\includegraphics[width=.45\\textwidth]{figure15d.pdf}}\n\\caption{ The original model is displayed in four plane-sections. The depths of the sections are: (a) $Z=100$ m; (b) $Z=300$ m; (c) $Z=400$ m and (d)  $Z=500$ m.} \\label{fig15}\n\\end{figure*}\n\n\n\\begin{figure*}\n\\subfigure{\\label{16a}\\includegraphics[width=.45\\textwidth]{figure16a.pdf}}\n\\subfigure{\\label{16b}\\includegraphics[width=.45\\textwidth]{figure16b.pdf}}\n\\subfigure{\\label{16c}\\includegraphics[width=.45\\textwidth]{figure16c.pdf}}\n\\subfigure{\\label{16d}\\includegraphics[width=.45\\textwidth]{figure16d.pdf}}\n\\caption{ For the data in Fig.~\\ref{14b}: The reconstructed model  using Algorithm~\\ref{projectedalgorithm} with $t=200$ and the $L_1$ stabilizer with $\\epsilon^2=1.e{-9}$. The depths of the sections are: (a) $Z=100$ m; (b) $Z=300$ m; (c) $Z=400$ m and (d)  $Z=500$ m.} \\label{fig16}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{17a}\\includegraphics[width=.3\\textwidth]{figure17a.pdf}}\n\\subfigure{\\label{17b}\\includegraphics[width=.3\\textwidth]{figure17b.pdf}}\n\\subfigure{\\label{17c}\\includegraphics[width=.3\\textwidth]{figure17c.pdf}}\n\\caption {For the data in Fig.~\\ref{14b} using Algorithm~\\ref{projectedalgorithm} with $t=200$ and the $L_1$ stabilizer with $\\epsilon^2=1.e{-9}$.  (a) The progression of the data misfit,  $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$; (b) The TUPRE functional at iteration $11$; and (c) The progression of the relative error at each iteration.} \\label{fig17}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{18a}\\includegraphics[width=.45\\textwidth]{figure18a.pdf}}\n\\subfigure{\\label{18b}\\includegraphics[width=.45\\textwidth]{figure18b.pdf}}\n\\subfigure{\\label{18c}\\includegraphics[width=.45\\textwidth]{figure18c.pdf}}\n\\subfigure{\\label{18d}\\includegraphics[width=.45\\textwidth]{figure18d.pdf}}\n\\caption{ For the data in Fig.~\\ref{14b} using Algorithm~\\ref{projectedalgorithm} with $t=200$ and the MS stabilizer with $\\epsilon^2=1e-5$. The depths of the sections are: (a) $Z=100$ m; (b) $Z=300$ m; (c) $Z=400$ m and (d)  $Z=500$ m.} \\label{fig18}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{19a}\\includegraphics[width=.3\\textwidth]{figure19a.pdf}}\n\\subfigure{\\label{19b}\\includegraphics[width=.3\\textwidth]{figure19b.pdf}}\n\\subfigure{\\label{19c}\\includegraphics[width=.3\\textwidth]{figure19c.pdf}}\n\\caption {For the data in Fig.~\\ref{14b} using Algorithm~\\ref{projectedalgorithm} with $t=200$ and the MS stabilizer with $\\epsilon^2=1e-5$.  (a) The progression of the data misfit,  $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$; (b) The TUPRE functional at iteration $100$; and (c) The progression of the relative error at each iteration.} \\label{fig19}\n\\end{figure*}\n\n\n\n\\section{Real data}\\label{real}\n\nTo illustrate the relevance of the approach for practical data we investigate the reconstruction of  gravity data acquired over a hematite mine. The survey area is near the village of  Gheshmeh Gaz, Kerman province, in the southeast of Iran. The gravity survey was performed by the Gravity Branch of the Institute of Geophysics, at the University of Tehran.  In this area mostly pyroclastic sediments were formed in the upper Cambrian and Devonian. The Devonian outcrops have been formed mostly from dolomitic limestone but the pyroclastic sediments of the Cambrian have been formed from rhyolite, tuff and rhyodacite, and iron oxide (Hematite) can be seen in the outcrops. Data were measured at $750$ stations spaced about $10$~m along the profile with $30$~m  between profiles. The measurements have been corrected for effects caused by variation in elevation, latitude and topography to yield the Bouguer gravity anomaly. The residual gravity anomaly has been computed using a polynomial fitting method, which was sampled every $10$~m, providing a  grid with $41 \\times 73 = 2993$ gravity measurements. To suppress effects of noise the residual anomaly is upward-continued to a height of  $2.5$~m. The resulting anomaly is shown in Fig.\\ref{fig20}. Several major areas of high gravity are  observed in the map, and are labeled as positions  $1$, $2$, $3$ and $4$. In the western part of the region, the extension of anomalies $1$ and $3$ is cut off at the boundary. At the moment, no gravity data beyond the western boundary is available.  We suppose each datum has an error with standard deviation $\\left( 0.02 ({\\mathbf{d}_{\\mathrm{obs}}})_i+ 0.003 \\|{\\mathbf{d}_{\\mathrm{obs}}}\\|\\right)$. \n\n\\begin{figure*}\n\\includegraphics[width=0.5\\textwidth]{figure20.pdf}\n\\caption{Upward Residual Anomaly.} \\label{fig20}\n\\end{figure*}\n\nFor the data inversion we use a model with cells  of width $10$~m in the eastern and northern directions. In the depth dimension the first four layers of cells have a thickness of $5$~m, while the subsequent layers increase  gradually to $10$~m. The maximum depth of the model is $100$~m. This yields a model with the $z$-coordinates: $0:5:20$,  $26$,    $33$,    $41$,    $50:10:100$. The original model is then padded in the horizontal directions with three cells with dimensions that are the same as in the original model.  A mesh  with $ 47 \\times 79 \\times 13 = 48269 $ cells results.  Based on geological information, the background density is taken to be $2.85$~g~cm$^{-3}$ and density limits $\\rho_{\\mathrm{min}}=2.5$~g~cm$^{-3}$  and $ \\rho_{\\mathrm{max}}=4$~g~cm$^{-3}$ are imposed. The TUPRE with $t=200$ and the $L_1$ stabilizer with $\\epsilon^2=1.e{-9}$ is used for the inversion by algorithm~\\ref{projectedalgorithm}. \n\nThe algorithm terminates after $19$ iterations. The progression of the data misfit, the regularization term and the regularization parameter with iteration $k$, the TUPRE functional at the final iteration, and the gravity response of the reconstructed model are shown in Fig.~\\ref{fig21}. The predicted data  provide a good fit to the observed data, see Fig.~\\ref{21c}. Cross-sections of the recovered model are shown in Fig.~\\ref{fig22}.  These are cross-sections in the northern dimension at  north=$100$~m, $210$~m, $400$~m, $500$~m, $650$~m, and  in the eastern dimension at east=$20$~m, $100$~m, $200$~m, $280$~m, $350$~m, in Figs.\\ref{22a}-\\ref{22b}, respectively. \n\nFrom  Fig.~\\ref{fig22}, three major density sources are observed beneath anomalies $1$, $2$ and $3$. The source under anomaly $4$ and some small sources in the eastern part of the area are not comparable with these major sources. The maximum extension of source $1$ in the west-east and south-north directions are approximately $250$~m and $200$~m, respectively. The depth to the surface varies between $5$-$10$~m in the east to $20$~m in the west. In the southwest, the source has a dipping form and extends to a maximum $70$~m depth. Source $2$ is approximately of horizontal dimnesions $100$~m by $100$~m, and extends from $15$-$25$~m to $50$-$60$~m in depth. Source $3$ has a  maximum extension of approximately $150$~m in the  west-east dimension and approximately $100$~m in the south-north dimension The source is quite close to the surface, approximately at depth $5$-$10$~m,  and extends to an approximate maximum of $50$~m. There are currently no data that confirm or reject the inversion results, however, based on the synthetic examples we can suppose that the inversion has yielded information that is close to the true subsurface model.\n\n\\begin{figure*}\n\\subfigure{\\label{21a}\\includegraphics[width=.3\\textwidth]{figure21a.pdf}}\n\\subfigure{\\label{21b}\\includegraphics[width=.3\\textwidth]{figure21b.pdf}}\n\\subfigure{\\label{21c}\\includegraphics[width=.3\\textwidth]{figure21c.pdf}}\n\\caption {For the real data inversion for the data in Fig.~\\ref{fig20}: (a) The progression of the data misfit,  $\\star$, the regularization term, $+$, and the regularization parameter, $\\square$, with iteration $k$; (b) The TUPRE functional at iteration $19$; and (c) The gravity response of the reconstructed model.} \\label{fig21}\n\\end{figure*}\n\n\\begin{figure*}\n\\subfigure{\\label{22a}\\includegraphics[width=.45\\textwidth]{figure22a.pdf}}\n\\subfigure{\\label{22b}\\includegraphics[width=.45\\textwidth]{figure22b.pdf}}\n\\caption{ For the data in Fig.~\\ref{fig20}: The reconstructed model  using Algorithm~\\ref{projectedalgorithm} with $t=200$ and the $L_1$ stabilizer with $\\epsilon^2=1.e{-9}$. (a) west-east cross-sections and (b) north-south cross-sections.} \\label{fig22}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\\section{conclusion}\\label{conclusion}\nWe have developed an algorithm for sparse inversion of gravity data using an $L_1$ norm stabilizer. The algorithm is developed for both small and large scale problems. Our results show that the large scale problem can be efficiently and effectively inverted using the GKB projection with regularization applied on the projected space. We demonstrated that for estimating the  regularization parameter on the  subspace, a new approach based on the truncation of the projected spectrum in conjunction with the method of unbiased predictive risk should be applied. The new method, here denoted as TUPRE, gives results using the projected subspace algorithm which are comparable with those obtained for the full space, while just requiring a small number of Golub-Kahan iterations. Then, a fast and practical algorithm for the large-scale inversion of gravity data was obtained. Numerical simulations support the use of the algorithm. Finally, the algorithm was used on gravity data from a hematite mine in Iran and the reconstructed model was shown. Future work will look further at the algorithm for other data sets, and additional justification mathematically of the truncated UPRE algorithm, \n\n\n\\begin{acknowledgments}\nRosemary Renaut acknowledges the support of  NSF grant  DMS 1418377:   ``Novel Regularization for Joint Inversion of Nonlinear Problems\". \n\\end{acknowledgments}\n\n\\begin{thebibliography}{}\n\\bibitem[\\protect\\citename{Ajo-Franklin et al. } 2007]{Ajo:2007} Ajo-Franklin, J. B., Minsley, B. J. \\& Daley, T. M., 2007. Applying compactness constraints to differential traveltime tomography, \\textit{Geophysics}, \\textbf{72(4)}, R67-R75.\n\\bibitem[\\protect\\citename{Boulanger \\& Chouteau }2001]{BoCh:2001} Boulanger, O. \\& Chouteau, M., 2001. Constraint in $3$D gravity inversion, \\textit{Geophysical prospecting}, \\textbf{49}, 265-280.\n\\bibitem[\\protect\\citename{Bruckstein et al. }2009]{BrDoEl:2009} Bruckstein, A.~M.,  Donoho, D.~L. \\& Elad, M., 2009.  From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images, \\textit{SIAM Rev.}, \\textbf{51(1)}, 34--81.\n\\bibitem[\\protect\\citename{Chung et al. }2008]{ChNaOl:2008} Chung, J., Nagy, J. \\& O'Leary, D. P., 2008. A weighted GCV method for Lanczos hybrid regularization \\textit{ETNA}, \\textbf{28}, 149-167.\n\\bibitem[\\protect\\citename{Farquharson }2008]{Far:2008} Farquharson, C. G., 2008. Constructing piecwise-constant models in  multidimensional minimum-structure inversions, \\textit{Geophysics}, \\textbf{73(1)}, K1-K9.\n\\bibitem[\\protect\\citename{Farquharson \\& Oldenburg }1998]{FaOl:98} Farquharson, C. G. \\& Oldenburg, D. W., 1998. Nonlinear inversion using general measure of data misfit and model structure, \\textit{Geophys.J.Int.}, \\textbf{134}, 213-227.\n\\bibitem[\\protect \\citename{Farquharson \\& Oldenburg }2004]{Far:2004} Farquharson, C. G. $\\&$  Oldenburg, D. W., 2004. A comparison of Automatic techniques for estimating the regularization parameter in non-linear inverse problems, \\textit{Geophys.J.Int.}, \\textbf {156}, 411-425.\n\\bibitem[\\protect\\citename{Gazzola \\& Nagy }2014]{GaNa:2014} Gazzola, S. \\& Nagy, J. G., 2014. Generalized Arnoldi-Tikhonov method for sparse reconstruction, \\textit{SIAM J. Sci. Comput.}, \\textbf{36(2)}, B225-B247.\n\\bibitem[\\protect\\citename{Golub et al. }1979]{GHW:1979} Golub, G. H., Heath, M. \\& Wahba, G., 1979. Generalized Cross Validation as a method for choosing a good ridge parameter, \\textit{Technometrics}, \\textbf{21 (2)}, 215-223.\n\\bibitem[\\protect\\citename{Golub \\& van Loan }1996]{GoLo:96} Golub, G. H. $\\&$  van Loan, C., 1996. \\textit{Matrix Computations}, (John Hopkins Press Baltimore) 3rd ed.\n\\bibitem[\\protect\\citename{Hansen }1992]{Hansen:92} Hansen, P. C., 1992. Analysis of discrete ill-posed problems by means of the L-curve, \\textit{SIAM Review}, \\textbf{34 (4)}, 561-580.\n\\bibitem[\\protect\\citename{Hansen }2007]{Hansen:2007} Hansen, P. C., 2007. \\textit{Regularization Tools: A Matlab Package for Analysis and Solution of Discrete Ill-Posed Problems Version $4.1$ for Matlab $7.3$ }, Numerical Algorithms, \\textbf{46}, 189-194.\n\\bibitem[\\protect\\citename{Kilmer \\& O'Leary }2001]{KiOl:2001} Kilmer, M. E. \\& O'Leary, D. P., 2001. Choosing regularization parameters in iterative methods for ill-posed problems, \\textit{SIAM journal on Matrix Analysis and Application}, \\textbf{22}, 1204-1221.\n\\bibitem[\\protect\\citename{Last \\& Kubik }1983]{LaKu:83} Last, B. J. \\&  Kubik, K., 1983. Compact gravity inversion, \\textit{Geophysics}, \\textbf{48}, 713-721.\n\\bibitem[\\protect\\citename{Li \\& Oldenburg }1996]{LiOl:96} Li, Y. \\& Oldenburg, D. W., 1996. 3-D inversion of magnetic data, \\textit{Geophysics}, \\textbf{61}, 394-408.\n\\bibitem[\\protect\\citename{Li \\& Oldenburg }1998]{LiOl:98} Li, Y. \\&  Oldenburg, D. W., 1998. 3-D inversion of gravity data,\\textit{Geophysics}, \\textbf{63}, 109-119.\n\\bibitem[\\protect\\citename{Loke et al. }2003]{Loke:2003} Loke, M. H., Acworth, I. \\& Dahlin, T., 2003. A comparison of smooth and blocky inversion methods in 2D electrical imaging surveys, \\textit{Exploration Geophysics}, \\textbf{34}, 182-187.\n\\bibitem[\\protect\\citename{Marquardt }1970]{Mar:1970} Marquardt, D. W., 1970. Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation, \\textit{Technometrics}, \\textbf{12(3)}, 591-612.\n\\bibitem[\\protect \\citename{Mead \\& Renaut }2009]{MeRe:09} Mead, J. L. \\&  Renaut, R. A.,  2009. A Newton root-finding algorithm for estimating the regularization parameter for solving ill-conditioned least squares problems, \\textit{Inverse Problems},  {\\bf 25}, 025002.\n\\bibitem[\\protect\\citename{Morozov }1966]{Morozov:66} Morozov, V. A., 1966. On the solution of functional equations by the method of regularization, \\textit{Sov. Math. Dokl.}, \\textbf{7}, 414-417.\n\\bibitem[\\protect\\citename{Paige \\& Saunders }1982a]{PaSa:1982a} Paige, C. C. \\& Saunders, M. A., 1982. LSQR: An algorithm for sparse linear equations and sparse least squares, \\textit{ACM Trans. Math. Software}, \\textbf{8}, 43-71.\n\\bibitem[\\protect\\citename{Paige \\& Saunders }1982b]{PaSa:1982b}  Paige, C. C. \\& Saunders, M. A., 1982.\nALGORITHM 583 LSQR: Sparse linear equations and least squares problems, \\textit{ACM Trans. Math. Software}, \\textbf{8}, 195-209.\n\\bibitem[\\protect\\citename{Portniaguine \\& Zhdanov }1999]{PoZh:99} Portniaguine, O. \\& Zhdanov, M. S., 1999. Focusing geophysical inversion images, \\textit{Geophysics}, \\textbf{64}, 874-887\n\\bibitem[\\protect\\citename{ Renaut et al. }2010]{RHM:2010} Renaut, R. A., Hnetynkov\\'{a}, I. \\& Mead, J. L., 2010. Regularization parameter estimation for large scale Tikhonov regularization using a priori information,  \\textit{Computational Statistics and Data Analysis}  \\textbf{54(12)}, 3430-3445.\n\\bibitem[\\protect\\citename{Renaut et al. }2015]{RVA:2015}Renaut, R.~A., Vatankhah, S. \\& Ardestani, V.~E., 2015. Hybrid and iteratively reweighted regularization  by unbiased predictive risk and weighted GCV for projected systems, \\url{http://arxiv.org/abs/1509.00096}, September 2015, submitted.\n\\bibitem[\\protect\\citename{Sun \\& Li }2014]{SunLi:2014} Sun, J. \\& Li, Y., 2014. Adaptive $L_{p}$ inversion for simultaneous recovery of both blocky and smooth features in geophysical model, \\textit{Geophys. J. Int}, \\textbf{197}, 882-899.\n\\bibitem[\\protect \\citename{Vatankhah et al. }2014a]{VAR:2014a} Vatankhah, S.,  Ardestani, V. E. $\\&$  Renaut, R. A., 2014a. Automatic estimation of the regularization parameter in 2-D focusing gravity inversion:  application of the method to the Safo manganese mine in northwest of Iran, \\textit{Journal Of Geophysics and Engineering}, \\textbf {11},  045001.\n\\bibitem[\\protect\\citename{Vatankhah et al. }2015]{VAR:2015} Vatankhah, S., Ardestani, V. E. \\&  Renaut, R. A., 2015. Application of the $\\chi^2$ principle and unbiased predictive risk estimator for determining the regularization parameter in $3$D focusing gravity inversion, \\textit{Geophys. J. Int.}, \\textbf{200}, 265-277. \n\\bibitem[\\protect\\citename{Vatankhah et al. }2014b]{VRA:2014b} Vatankhah, S.,  Renaut, R. A.   \\&  Ardestani, V. E., 2014b. Regularization parameter estimation for underdetermined problems by the $\\chi^2$ principle with application to $2$D focusing gravity inversion, \\textit{Inverse Problems}, \\textbf{30}, 085002.\n\\bibitem[\\protect\\citename{Vogel }2002]{Vogel:2002} Vogel, C. R., 2002. \\textit{Computational Methods for Inverse Problems}, SIAM Frontiers in Applied Mathematics, SIAM Philadelphia U.S.A.\n\\bibitem[\\protect\\citename{Voronin }2012]{Voronin:2012} Voronin, S., 2012. \\textit{Regularization of Linear Systems With Sparsity Constraints With Application to Large Scale Inverse Problems}, PhD thesis, Princeton University, U.S.A.\n\\bibitem[\\protect\\citename{Wohlberg \\& Rodriguez }2007]{WoRo:07} Wohlberg, B. \\& Rodriguez, P. 2007. An iteratively reweighted norm algorithm for minimization of total variation functionals,   \\textit{IEEE Signal Processing Letters},  {\\bf 14} 948--951.\n\\end{thebibliography}\n\n\\appendix\n\\section{solution using singular value decomposition}\\label{svdsolution}\nSuppose  $m^* = min (m,n)$ and the SVD of matrix $ \\tilde{\\tilde{G}} \\in {\\mathcal{R}^{m \\times n}} $ is given by $\\tilde{\\tilde{G}}=U \\Sigma V^T$, where the singular values are ordered $\\sigma_1 \\geq \\sigma_2\\geq \\dots \\geq \\sigma_{m^*} > 0$,  and occur\non the diagonal of $\\Sigma \\in {\\mathcal{R}^{m \\times n}}$ with $n-m$ zero columns (when $m <n $) or $m-n$ zero rows (when $m>n$), using the full definition of the SVD, \\cite{GoLo:96}. $U \\in \\mathcal{R}^{m \\times m} $, and $ V \\in \\mathcal{R}^{n \\times n}$ are orthogonal matrices with columns denoted by  ${\\mathbf{u}_i}$ and ${\\mathbf{v}_i}$. Then the solution of (\\ref{hsolution}) is given by\n\\begin{eqnarray}\\label{hsvd}\n{\\mathbf{h}}(\\alpha)= \\sum_{i=1}^{m^*} \\frac{\\sigma_i^2}{\\sigma_i^2 +\\alpha^2} \\frac{{\\mathbf{u}_i}^T \\tilde{{\\mathbf{r}}}}{\\sigma_i} {\\mathbf{v}_i}\n\\end{eqnarray}\nFor the projected problem $B_t \\in \\mathcal{R} ^{(t+1) \\times t}$, i.e. $m >n$, and the expression still applies to give  the solution of (\\ref{zsolution}) with $\\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}}$ replacing $\\tilde{{\\mathbf{r}}}$,  $\\zeta$ replacing $\\alpha$,  $\\gamma_i$ replacing $\\sigma_i$ and $m^* =t$ in (\\ref{hsvd}).\n\\section{regularization parameter estimation}\\label{svdparameter}\nThe UPRE functional for determining $\\alpha$ in the Tikhonov functional \\eqref[globalfunctionh] with system matrix  $ \\tilde{\\tilde{G}}$ is expressible using the SVD for $\\tilde{\\tilde{G}}$\n\\begin{eqnarray*}\nU(\\alpha)=\\sum_{i=1}^{m^*}  \\left( \\frac{1}{\\sigma_i^2 \\alpha^{-2} + 1} \\right)^2 \\left({\\mathbf{u}_i}^T\\tilde{{\\mathbf{r}}} \\right)^2 + 2 \\left( \\sum_{i=1}^{m^*} \\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha^2}\\right) - m.\n\\end{eqnarray*}\nIn the same way the UPRE functional for the projected problem \\eqref[globalfunctionproj] is given by\n\\begin{eqnarray*}\nU(\\zeta)=\\sum_{i=1}^{t}  \\left( \\frac{1}{\\gamma_i^2 \\zeta^{-2} + 1} \\right)^2 \\left({\\mathbf{u}_i}^T(\\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}}) \\right)^2 + \\sum_{i=t+1}^{t+1}\\left( {\\mathbf{u}_i}^T(\\| \\tilde{{\\mathbf{r}}} \\|_2 {\\mathbf{e}_{t+1}}) \\right) ^2 +2 \\left( \\sum_{i=1}^{t} \\frac{\\gamma_i^2}{\\gamma_i^2+\\zeta^2}\\right) - (t+1). \n\\end{eqnarray*}\nThen, for truncated UPRE, $t$  is  replaced by $t_{\\mathrm{trunc}} < t$ so that the terms from $t_{\\mathrm{trunc}}$ to $t$ are ignored, corresponding to dealing with these as constant with respect to the minimization of $U(\\zeta)$.\n\n", "itemtype": "equation", "pos": 42472, "prevtext": "  are recorded.  Table~\\ref{tab1} gives the  average and standard deviation of  $\\alpha^{(K)}$, $RE^{(K)}$ and $K$ over the $10$ samples.  It was explained by Farquharson \\shortcite{Far:2004} that it is  efficient if the inversion starts with a large value of the regularization parameter. This prohibits imposing excessive structure in the model at early iterations which would otherwise require more iterations to remove artificial structure. In this paper the method introduced by Vatankhah et.al. \\shortcite{VAR:2014a,VAR:2015} was used to determine an initial regularization parameter, $\\alpha^{(1)}$. Because the non zero singular values, $\\sigma_i$ of matrix $\\tilde{\\tilde{G}}$ are known, the initial value \n\n", "index": 9, "text": "\\begin{equation}\\label{initalpha}\n\\alpha^{(1)}=(n/m)^{3.5}(\\sigma_1/\\mathrm{mean(\\sigma_i)})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\alpha^{(1)}=(n/m)^{3.5}(\\sigma_{1}/\\mathrm{mean(\\sigma_{i})})\" display=\"block\"><mrow><msup><mi>\u03b1</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>/</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>3.5</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\u03c3</mi><mn>1</mn></msub><mo>/</mo><mi>mean</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c3</mi><mi mathvariant=\"normal\">i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]