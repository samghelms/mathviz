[{"file": "1601.06473.tex", "nexttext": "\nand ${\\boldsymbol{p}_A}^a$ and ${\\mathbf{R}_A}^a$ are set to zero and identity\nmatrix respectively. Only the relative poses between the assembly parts are\nused.\n\n\\section{3D Detection during Robotic Execution}\n\nThe object pose detection during robot execution is done using Kinect, Point\nCloud Library, and geometric constraints. The detection cannot be done using\nmarkers since: (1) What the robot manipulates are thousands of industrial parts,\nit is impossible to attach markers to all of them. (2) The initial configuration\nof the object is unexpectable and the markers might be occluded from time to\ntime during robotic pick-and-place.\nThe detection is also not applicable to image-based approaches: Industrial\nparts are usually mono colored and textureless, image features are not only\nhelpless but even harmful. \n\nUsing Kinect is not trivial due to its low resolution and precision. For\nexample, the objects in industrial applications are usually in clutter and it is\ndifficult to segment one object from another using Kinect's resolution. Our\nsolution is to divide the difficulty in clutter into two subproblems. First, the\nrobot considers how to pick out one object from the clutter, and place the\nobject on an open plenary surface. Second, the robot estimate the pose of the\nsingle object on the open plenary surface. The first subproblem doesn't care\nwhat the object is or the pose of the object and its only goal is to pick\nsomething out.\nIt is referred as a pick-and-place problem in contemporary literature and is\nillustrated in the left part of Fig.\\ref{twoprb}. The first subproblem is well\nstudied and interested readers may refer to \\cite{Domae14} for the solutions.\n\nThe second subproblem is to detect the pose of a single object on an\nopen plenary surface and is shown in the right part of Fig.\\ref{twoprb}. It is\nmuch easier but still requires much proess to meet the precision requirements of\nassembly. We concentrate on precision and will discuss use point cloud\nalgorithms and geometric constraints to solve the second problem.\n\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/twoproblems.png}\n  \\caption{Overcome the clutter problem by dividing the pose detection into two\n  subproblems. The first one is picking out from clutter where the system\n  doesn't care what the object is or the pose of the object and its only goal is\n  to pick something out. The problem is well studied. The second one is to\n  detect the pose of a single object on an open plenary surface. It is not\n  trivial since the precision of Kinect is bad.}\n  \\label{twoprb}\n\\end{figure}\n\n\n\n\n\n\\subsection{Rough detection using point cloud}\n\nFirst, we roughly detect the pose of the object using CVFH and CRH features.\nIn a preprocessing process before starting the detection, we put the camera to\n42 positions on a unit sphere, save the view of the camera, and\nprecompute the CVFH and CRH features of each view. This step is virtually\nperformed using PCL and is shown in the dashed blue frame of Fig.\\ref{rawpose}.\nDuring the detection, we extract the plenary surface from the point\ncloud, segment the remaining points cloud, and compute the CVFH and CRH features of\neach segmentation. Then, we match the precomputed features with the features of\neach segment and estimate the orientation of the segmentations. This step is\nshown in the dashed red frame of Fig.\\ref{rawpose}. The matched segments are\nfurther refined using ICP to ensure good matching.\nThe segmentation that has highest ICP matches and smallest outlier points will\nbe used as the output. An example is shown in the ``Raw result''\nframebox of Fig.\\ref{rawpose}.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/rawpose.png}\n  \\caption{Rawly detecting the pose of an object using model matching and CVFH\n  and CRH features. In a preprocessing process before the detection, we\n  precompute the CVFH and CRH features of 42 different views and save them as\n  the template. The preprocessing process is shown in the\n  dashed blue frame. During the detection, we segment the remaining points\n  cloud, compute the CVFH and CRH features of each segmentation, and match them\n  to the precomputed views using ICP. The best match is used as the output.}\n  \\label{rawpose}\n\\end{figure}\n\n\\subsection{Noise correction using geomteric constraints}\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/posecorrection.png}\n  \\caption{Correcting the raw result using the stable placements on\n  a plenary surface (geometric constraints). In a preprocessing process before\n  the correction, we compute the stable placements of the object on a plenary\n  surface. An example is shown in the stable placements framebox. During the\n  correction, we compute the distance between the raw results and each of the\n  stable placements, and correct the raw results using the nearest pose.}\n  \\label{posecor}\n\\end{figure}\n\nThe result of rough detection is further refined using the geometric\nconstraints.\nSince the object is on plenary surface, its stable poses are limited\n\\cite{Wan2016ral} and can be used to correct the noises of the roughly estimated\nresult. The placement planning includes two steps. First, we compute the convex\nhull of the object mesh and perform surface clustering on the convex hull. Each\ncluster is one candidate standing surface where the object may be placed on.\nSecond, we check the stability of the objects standing on these candidate\nsurfaces. The unstable placements (the placement where the projection of center\nof mass is outside the candidate surface or too near to its boundary) are\nremoved.\nAn example of the stable placements for one object is shown in the \nstable placements framebox of Fig.\\ref{posecor}.\n\nGiven the raw detection result using CVFH and CRH features, the robot computes\nits distance to the stable placements and correct it following\nAlg.\\ref{ncalg}.\n\n\\begin{algorithm}[htbp!]\n\\DontPrintSemicolon\n    \\KwData{Raw result: $\\boldsymbol{p}_r$, $\\mathbf{R}_r$;\\\\\n            ~~~~~~~~~~~Stable placements: \\{$\\mathbf{R}_p(i),~i=1,2,\\ldots$\\}\\\\\n            ~~~~~~~~~~~Table height: $h_t$}\n    \\KwResult{Corrected result: $\\boldsymbol{p}_c$, $\\mathbf{R}_c$}\n    \\SetKwFunction{size}{size}\n    \\SetKwFunction{rpyFromRot}{rpyFromRot}\n    \\SetKwFunction{rotFromRpy}{rotFromRpy}\n    $d_{min}\\leftarrow+\\infty$\\;\n    $\\mathbf{R}_{near}\\leftarrow\\mathbf{I}$\\;\n    \\For{$i\\leftarrow1$ \\KwTo $\\mathbf{R}_p$.\\size{}} {\n        $d_i\\leftarrow||\\log(\\mathbf{R}_p(i){\\mathbf{R}_p}{'})||$\\;\n        \\If{$d_i<d_{min}$} {\n            $d_{min} \\leftarrow d_i$\\;\n            $\\mathbf{R}_{near} \\leftarrow \\mathbf{R}_p(i)$\\;\n        }\n    }\n    $\\mathbf{R}_{yaw} \\leftarrow$ \\rotFromRpy{0, 0,\n    \\rpyFromRot{$\\mathbf{R}_{r}$}$.y$}\\;\n    $\\mathbf{R}_c\\leftarrow\\mathbf{R}_{yaw}\\cdot\\mathbf{R}_{near}$\\;\n    $\\boldsymbol{p}_c\\leftarrow[\\boldsymbol{p}_r.x, \\boldsymbol{p}_r.y,\n    h_t]'$\n    \\caption{Noise correction}\n    \\label{ncalg}\n\\end{algorithm}\n\nIn this pseudo code, $\\mathbf{I}$ indicates a $3\\times3$ identity matrix.\nFunctions $\\mathtt{rotFromRpy}$ and $\\mathtt{rpyFromRot}$ converts $roll$,\n$pitch$, $yaw$ angles to rotation matrix and vice verse. The distance between\ntwo rotation matrices is computed in line 4. The corrected result is updated at\nlines 11 and 12.\n\n\\section{Grasp and Motion Planning}\n\nAfter finding the poses of the parts on the plenary surface, what the robot does\nnext is to grasp the parts and assemble them. It includes two steps: A grasp\nplanning step and a motion planning step. \n\n\\subsection{Grasp planning}\nIn the grasp planning step, we set the\nobject at free space and compute the force-closured and collision-free grasps.\nEach grasp is represented using ${\\boldsymbol{g}_X}^f$=$\\{\\boldsymbol{p}_0,\n\\boldsymbol{p}_1, \\mathbf{R}\\}$ where $\\boldsymbol{p}_0$ and\n$\\boldsymbol{p}_1$ are the contact positions of the finger tips, $\\mathbf{R}$\nis the orientation of the palm. The whole set is represented by\n${\\mathbf{g}_X}^f$, which includes many ${\\boldsymbol{g}_X}^f$. Namely,\n${\\mathbf{g}_X}^f$ = $\\{{\\boldsymbol{g}_X}^f\\}$.\n\nGiven the pose of a part on the plenary surface, say ${\\boldsymbol{p}_X}^s$\nand ${\\mathbf{R}_X}^s$, the IK-feasible and collision-free grasps that the robot\ncan use to pick up the object is computed following\n\n", "itemtype": "equation", "pos": 21214, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Teaching Robots to Do Object Assembly using Multi-modal 3D\nVision}\n\n\n\n\n\n\n\\author[label1]{Weiwei Wan}\n\\author[label2]{Feng Lu}\n\\author[label1]{Zepei Wu}\n\\author[label1]{Kensuke Harada}\n\\address[label1]{National Institute of Advanced Industrial Science and\nEngineering, Japan}\n\\address[label2]{Beihang University, China}\n\n\\begin{abstract}\n\nThe motivation of this paper is to develop a smart system using multi-modal\nvision for next-generation mechanical assembly. It includes two phases where in\nthe first phase human beings teach the assembly structure to a robot and in the\nsecond phase the robot finds objects and grasps and assembles them using AI\nplanning. The crucial part of the system is the precision of 3D visual detection\nand the paper presents multi-modal approaches to meet the requirements: AR\nmarkers are used in the teaching phase since human beings can actively control\nthe process. Point cloud matching and geometric constraints are used in the\nrobot execution phase to avoid unexpected noises. Experiments are performed to\nexamine the precision and correctness of the approaches. The study is\npractical: The developed approaches are integrated with graph\nmodel-based motion planning, implemented on an industrial\nrobots and applicable to real-world scenarios.\n\n\\end{abstract}\n\n\\begin{keyword}\n\n\n\n\n\n\n3D Visual Detection \\sep Robot Manipulation \\sep Motion Planning\n\\end{keyword}\n\n\\end{frontmatter}\n\n\n\n\n\\section{Introduction}\n\\label{intro}\n\nThe motivation of this paper is to develop a smart system using multi-modal\nvision for next-generation mechanical assembly:\nA human worker assembles mechanical parts in front of a vision system;\nThe system detects the position and orientation of the assembled parts and\nlearns how to do assembly following the human worker\u0081fs demonstration; An\nindustrial robot performs assembly tasks following the data learned from human\ndemonstration; It finds the mechanical parts in its workspace, picks up them,\nand does assembly using motion planning and assembly planning.\n\n\nThe difficulty in developing the smart system is precise visual detection. Two\nproblems exist where the first one is in the human teaching phase, namely how to\nprecisely detect the position and orientation of the parts in human hands\nduring manual demonstration; The second one is in the robot execution phase,\nnamely how to precisely detect the position and orientation of the parts in\nthe workspace and perform assembly.\n\nLots of detecting and tracking studies are available in contemporary\npublication, but none of them meets the requirements of the two problems. The\napproaches used include RGB images, markers, point cloud,\nextrinsic constraints, and multi-modal solutions where the RGB images and\nmarkers are short in occlusions, the point cloud and extrinsic constraints are\nshort in partial loss and noises, and the multi-modal solutions are not clearly\nstated and are still being developed.\n\n\nThis paper solves the two problems using multi-modal vision. \nFirst, we attach AR markers to the objects for\nassembly and track them by detecting and transforming the marker positions\nduring human demonstration. We don't need to worry occlusions since the teaching\nphase is manual and is performed by human beings who are intelligent enough to\nactively avoid occlusions and show good features to vision systems. The modal\nemployed in this phase is the markers (rgb image) and the geometric relation\nbetween the markers and the object models. The tag ``AR(RGB)'' is used for\nrepresentation.\n\nSecond, we use depth cameras and match the object model to the point cloud\nobtained using depth camera to roughly estimate the object pose, and use the\ngeometric constraints from planar table surface to reinforce the precision.\nFor one thing, the robot execution phase is automatic and is not as flexible as\nhuman teaching. Markerless approaches are used to avoid occlusions. For\nthe other, the point cloud and extrinsic constraints are fused to make up the\npartial loss and noises. The assumption is when the object is placed on the\nsurface of a table, it stabilizes at a limited number of poses inherent to the\ngeometric constraints. The poses help to freeze some Degree of Freedom and\nimprove precision. The modal employed in this phase is the cloud point data\nand the geometric constraints from the plenary surface. The tag ``Depth+Geom''\nis used for representation.\n\nMoreover, we propose an improved graph model based on our previous work to\nperform assembly planning and motion planning.\n\nExperiments are performed to examine the precision and correctness of our\napproaches. We quantitatively show the advantages of ``AR(RGB)'' and\n``Depth+Geom'' in next-generation mechanical assembly and concretely demonstrate\nthe process of searching and planning using the improve graph model. We also\nintegrate the developed approaches with Kawada Nextage Robots and\nshow the applicability of them in real-world scenarios.\n\n\\section{Related Work}\n\\label{relwork}\n\n\n\nThis paper is highly related to\nstudies in 3D object detection for robotic manipulation and assembly and the\nliterature review concentrates on the perception aspect. For general studies\non robotic grasping, manipulation, and assembly, refer to \\cite{Handeybook},\n\\cite{Mason01}, and \\cite{Dogar15}.\nThe literature review emphasizes on model-based approaches since the paper is\nmotivated by next-generation mechanical assembly and is about the industrial\napplications where precision is crucial and object models are available. For\nmodel-less studies, refer to \\cite{Goldfeder10} and \\cite{Lenz14}. For\nappearance-based studies, refer to \\cite{Murase95} \\cite{Mittrapiyanuruk04}, and\n\\cite{Zickler06}.\nMoreover, learning approaches are no reviewed since they are not precise.\nRefer to \\cite{Stark10} and \\cite{Libelt10} if interested.\n\n\nWe archive and review the related work\naccording to the modals used for 3D detection, including RGB images,  markers,\npoint cloud, haptic sensing, extrinsic constraints, and multi-modal fusion.\n\n\\subsection{RGB images}\nRGB images are the most commonly used modal of robotic perception.\nUsing RGB images to solve the model-based 3D position and orientation detection\nproblem is widely known as the ``model-to-image registration problem''\n\\cite{Wunsch96} and is under the framework of POSIT (Pose from Orthography and\nScaling with Iterations) \\cite{David02}. When looking for objects in images,\nthe POSIT-based approaches match the feature descriptors by comparing the most\nrepresentative features of an image to the features of the object for\ndetection. The features could either be values computed at pixel points or\nhistograms computed on a region of pixels. Using more than three matches, the 3D\nposition and orientation of an object can be found by solving polynomial\nequations \\cite{Fischler81, DeMenthon95, Lu00}. A good material that explains\nthe matching process can be found in \\cite{Schaffalitzky02}. The paper studies\nmulti-view image matching. It is not directly related to 3D detection, but\nexplains well how to match the feature descriptors.\n\nSome of the most common choices of features include corner features\n\\cite{Harris88} applied in \\cite{Chia02} and \\cite{David02}, line features\napplied in \\cite{David03}, \\cite{Klein03} and \\cite{Marchand02}, cylinder\nfeatures applied in \\cite{Marchand02} and \\cite{Harada13}, and SIFT features\n\\cite{Lowe04} applied in \\cite{Gordon06} and \\cite{Collet09}. Especially,\n\\cite{Gordon06} stated clearly the two stages of model-based detection using\nRGB images: (1) The modeling stage where the textured 3D model is recovered from\na sequence of images; (2) The detection stage where features are extracted and\nmatched against those of the 3D models. The modeling stage is based on the\nalgorithms in \\cite{Schaffalitzky02}. The detection stage is open to different\nfeatures, different polynomial solving algorithms, and some optimizations like\nLevenberg-Marquardt \\cite{Press92} and Mean-shift \\cite{Cheng95}, etc.\n\\cite{Ramisa12} compared the different algorithms in the second stage.\n\n\\subsection{Markers}\nIn cases where the objects don't have enough features, markers are used\nto assist image-based detection. Possible marker types include: AR markers,\ncolored markers, and shape markers, etc. The well known AR libraries \\cite{Fiala05,\nWagner07} provide robust libraries and the Optitrack device provides easy-to-use\nsystems for the different marker types. However, the applications with markers\nrequire some manual work and there are limitations on marker sizes, view\ndirections, etc.\n\n\\cite{Sundareswaran98} uses circular concentric ring fiducial markers which are\nplaced at known locations on a computer case to overlay the hidden innards of\nthe case on the camera\u0081fs video feed. \\cite{Kato99} uses AR markers to locate the\ndisplay lots during virtual conference. It explains the underneath computation\nwell. \\cite{Vacchetti04} doesn\u0081ft directly use markers, but uses the offline\nmatched keyframes, which are essential the same thing, to correct online\ntracking. \\cite{Makita12} uses AR markers to recognize and cage objects.\n\\cite{Suligoj13} uses both balls (circles) and AR markers to estimate and track\nthe position of objects. More recently, \\cite{Ramirezamaro15} uses AR markers to\ntrack the position of human hands and the operating tools and uses the tracked\nmotion to teach robots. The paper shares the same assumption that human\nbeings can actively avoid occlusions. However, it doesn't need and didn't\nanalyze the precision since the goal of tracking is in task level, instead of\nthe low level trajectories.\n\n\\subsection{Point cloud}\nPoint cloud can be acquired using a structured light camera \\cite{Freedman12},\nstereovision camera \\cite{Choi12}, and LIDAR device \\cite{Bauhahn09}, etc. \nThe recent availability of low cost depth sensors and the handy Point\nCloud Library (PCL) \\cite{Rusu11} has widely disseminated 3D sensing technology in\nresearch and commercial applications. The basic flow of using point cloud for\ndetection is the same as image-based detection: The first step is to extract\nfeatures from models and objects; The second step is to match the features and\ndetect the 3D pose.\n\n\\cite{Shutz97} is one of the early studies that uses point cloud to detect the\npose of an object. It is based on the Iterative Closest Point (ICP) algorithm\n\\cite{Besl92} which iteratively minimizes the mean square distance of nearest\nneighbor points between the model and the point cloud. Following work basically\nuses the same technique, with improvements in feature extraction and matching\nalgorithms. The features used in point cloud detection are more vast than those\nin image-based detection, including local descriptors like signature of\nhistograms of orientation (SHOT) \\cite{Tombari10} and Radius-based Surface\nDescriptor (RSD) \\cite{Marton11}, and global descriptors like Clustered\nViewpoint Feature Histogram (CVFH) \\cite{Aldoma11} and Ensemble of Shape\nFunctions (ESF) \\cite{Wohlkinger11}. The matching algorithms didn\u0081ft change much,\nsticking to Random Sample Consensus (RANSAC) \\cite{Fischler81} and ICP. A\ncomplete review and usage of the features and matching algorithms can be found\nin \\cite{Aldoma12}.\n\n\\subsection{Extrinsic constraints}\n\nLike the markers, extrinsic constraints are used to assist point cloud based\ndetection. When the point cloud don't provide enough features or when the\npoint cloud are noisy and occluded, it is helpful to take into account the\nextrinsic constraints. For example, the detection pipeline in \\cite{Aldoma12}\nuses hypothesis, which is one example of extrinsic constraints, to verify the\nresult of feature matching. \\cite{Shiraki14} analyzes the functions of connected\nobject parts and uses them to refine grasps. The paper is not directly related\nto detection but is an example of improving performance using extrinsic\nconstraints caused by adjacent functional units.\n\nSome other work uses geometric constraints to reduce ambiguity of ICP.\n\\cite{Schuster10} and \\cite{Somanath09} segment 3D clutter using the geometric\nconstraints. They are not directly related to detection but are used widely as\nthe initial steps of many detection algorithms. \\cite{Savalcalvo15} clusters\npoint cloud into polygon patches and uses RANSAC with the multiple polygon\nconstraints to improve the precision of detection. \\cite{Goron12} uses table\nconstraints for segmentation and uses Hough voting to detect object poses.\n\\cite{Cheung15} uses the sliced 2D contours of 3D stable placements to reduce\nthe noises of estimation. It is like our approach but is contour-based and\nsuffers from ambiguity.\n\n\\subsection{Multi-modal fusion}\n\nMulti-modal approaches are mostly the combination or repetition of the previous\nfive modals. For example, some work fuses repeated modals to improve object\ndetection. \\cite{Taylor03} fuses colour, edge and texture cues predicted from\na textured CAD model of the tracked object to recover the 3D pose, and is open\nto additional cues.\n\nSome other work uses visual tracking to correct the noises caused by fast\nmotions and improve the precision of initial matches. The fused modals include\nthe RGB image modal and the motion modal where the later one could be either\nestimated using image sequences or third-part sensors like Global Positioning\nSystem (GPS) or gyros. \\cite{Kempter12} is one representative work which fuses\nmodel motion (model-based tracking) and model detection in RGB images to refine\nobject poses. \\cite{Klein03} fuses gyro data and line features of RGB images to\nreinforce the pose estimation for head-mounted displays. \\cite{Reitmayr06} uses\ngyro data, point descriptors, and line descriptors together to improve the\nperformance of pose estimation for outdoor applications.\n\n\\cite{Pangercic11} uses point cloud to cluster the scene and find the\nRegion-of-Interests (ROIs), and uses image modal to estimate the object pose at\nrespective ROIs. The fused modals are RGB images and Point cloud.\n\\cite{Hinterstoisser11} also combines image and depth modals. It uses the image\ngradients found on the contour of images and the surface normals found on the\nbody of point cloud to estimate object poses.\n\nTo our best knowledge, the contemporary object detection studies do not meet\nour requirements about precision. The most demanding case is robotic grasping\nand simple manipulation, which is far less strict than regrasp and assembly. We\ndevelop our own approaches in this paper by fusing different modals to deal with\nthe problems in the teaching phase and the robot execution phase respectively.\nFor one thing, we use AR markers to detect the 3D object positions and\norientations during human teaching. For the other, we use the cloud point data\nand the geometric constraints from planar table surface during robot execution.\nThe details are presented in following sections.\n\n\\section{System Overview}\n\nWe present an overview of the next-generation mechanical assembly system\nand make clear the positions of the 3D detection in this section, and describe\nin detail the ``AR(RGB)'' and ``Depth+Geom'' approaches in the sections\nfollowing it.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.2in]{fig/flow.png}\n  \\caption{The flow of the next-generation mechanical assembly. The flow is\n  composed of a human teaching phase and a robot execution phase. In the human\n  teaching phase, a human worker demonstrates assembly with marked objects in\n  front of an RGB camera. The computer detects the relationship of the assembly\n  parts. In the robot execution phase, the robot detects the parts in the\n  workspace using depth camera and geometric constraints, picks up them, and\n  performs assembly.}\n  \\label{flow}\n\\end{figure}\n\nFig.\\ref{flow} shows the flow of the next-generation mechanical assembly system.\nIt is composed of a human teaching phase and a robot execution phase. In the\nhuman teaching phase, a human worker demonstrates how to assemble mechanical\nparts in front of a vision system. The system records the relationship of the\ntwo mechanical parts and saves it as an intermediate value.\n\nIn the robot execution phase, the robot uses another vision system to find the\nmechanical parts in the workspace, picks up them, and performs assembly. The\nrelative position and orientation of the assembly parts are the intermediate\nvalues perceived by the human teaching phase. The grasping configurations of\nthe robotic hand and the motions to move the parts are computed online using\nmotion planning algorithms.\n\nThe beneficial point of this system is it doesn't need direct robot programming\nand is highly flexible. What the human worker needs to do is to attach the\nmarkers and presave the pose of the marker in the object's local coordinate\nsystem so that the vision system can compute the pose of the object from the\ndetected marker poses.\n\nThe 3D detection in the two phases are denoted by the two ``object detection''\nsub-boxes in Fig.\\ref{flow}. The one in the human teaching phase uses AR markers\nsince human beings can intentially avoid unexpected partial occlusions by human\nhands or the other parts, and as well as ensure high precision. The one in the\nmotion planning phase uses point cloud data to roughly detect the object pose,\nand uses the geometric constraints from planar table surface to correct the\nnoises and improve precision. The details will be explained in following\nsections. Before that, we list the symbols to facilitate readers.\n\n\\begin{itemize}[noitemsep, nolistsep, leftmargin=0.5in]\n\\item[${\\boldsymbol{p}_X}^s$] The position of object \\textit{X} on a planery\nsurface. We use \\textit{A} and \\textit{B} to denote the two objects and\nconsequently use ${\\boldsymbol{p}_A}^s$ and ${\\boldsymbol{p}_B}^s$ to denote\ntheir positions.\n\\item[${\\mathbf{R}_X}^s$] The orientation of object \\textit{X} on a planery\nsurface. Like ${\\boldsymbol{p}_X}^s$, \\textit{X} is to be replaced by \\textit{A}\nor \\textit{B}.\n\\item[${\\boldsymbol{p}_X}^a$] The position of object \\textit{X} in the assembled\nstructure.\n\\item[${\\mathbf{R}_X}^a$] The orientation of object \\textit{X} in the assembled\nstructure.\n\\item[${\\boldsymbol{p}_X}^p$] The pre-assembly positions of the two objects. The\nrobot will plan a motion to move the objects from the initial positions\nto the preassemble positions.\n\\item[${\\mathbf{g}_X}^f$] The force-closure grasps of object \\textit{X}. The letter $f$\nindicates the object is free, and is not in an assembled structure or laying on\nsomething.\n\\item[${\\mathbf{g}_X}^{s'}$] The force-closure grasps of object \\textit{X} on a planery\nsurface. It is associated with ${\\boldsymbol{p}_X}^s$ and ${\\mathbf{R}_X}^s$.\n\\item[${\\mathbf{g}_X}^s$] The collision-free and IK (Inverse Kinematics)\nfeasible grasps of object \\textit{X} on a planery surface. It is also associated with\n${\\boldsymbol{p}_X}^s$ and ${\\mathbf{R}_X}^s$.\n\\item[${\\mathbf{g}_X}^{a'}$] The force-closure grasps of object \\textit{X} in an\nassembled structure. It is associated with ${\\boldsymbol{p}_X}^a$ and\n${\\mathbf{R}_X}^a$.\n\\item[${\\mathbf{g}_X}^a$] The collision-free and IK (Inverse Kinematics)\nfeasible grasps of object \\textit{X} in the assembled structure. It is associated with\n${\\boldsymbol{p}_X}^a$ and ${\\mathbf{R}_X}^a$.\n\\item[${\\mathbf{g}_X}^{p'}$] The force-closure grasps of object \\textit{X} at\nthe pre-assembly positions.\n\\item[${\\mathbf{g}_X}^{p}$] The colllision-free and IK\nfeasible grasps of object \\textit{X} at the pre-assembly positions.\n\\end{itemize}\n\n\\section{3D Detection during Human Teaching}\n\nThe object detection during human teaching is done using AR markers and a RGB\ncamera. Fig.\\ref{ardetect} shows the flow of the detection and the poses of\nthe markers in the object model's local coordinate system. The markers are\nattached manually by human workers.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/ardetect.png}\n  \\caption{Object detection using AR markers. Beforehand, human worker\n  attaches the markers and presaves the pose of the marker in the object's local\n  coordinate system. During detection, vision system computes the pose of the\n  object from the detected marker poses (the three subfigures). The output is\n  the (${\\boldsymbol{p}_A}^a$, ${\\mathbf{R}_A}^a$) and (${\\boldsymbol{p}_B}^a$,\n  ${\\mathbf{R}_B}^a$).}\n  \\label{ardetect}\n\\end{figure}\n\nDuring demonstration, the worker holds the two objects and show the makers to\nthe camera. We assume the workers have enough intelligence and can expose the\nmarkers to the vision system without occlusion.\nThe detection process is conventional and can be found in many AR literature:\nGiven the positions of some features in the markers' local coordinate system,\nfind the transform matrix which converts the them into the positions on the\ncamera screen. In our implementation, the human teaching part is developed using\nUnity and the AR recognition is done using the Vuforia SDK for Unity.\n\nIn the example shown in Fig.\\ref{ardetect}, there are two objects where the\ndetected results are represented by (${\\boldsymbol{p}_A}^a$,\n${\\mathbf{R}_A}^a$) and (${\\boldsymbol{p}_B}^a$,\n${\\mathbf{R}_B}^a$). During robot execution, the (${\\boldsymbol{p}_B}^a$,\n${\\mathbf{R}_B}^a$) is set to:\n\n", "index": 1, "text": "\\begin{gather}\n    {\\boldsymbol{p}_B}^a\n    =\n    {\\boldsymbol{p}_B}^a - {\\boldsymbol{p}_A}^a, ~\n    {\\mathbf{R}_B}^{p}\n    =\n    {\\mathbf{R}_B}^{a} \\cdot ({\\mathbf{R}_A}^{a})'\n\\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\boldsymbol{p}_{B}}^{a}={\\boldsymbol{p}_{B}}^{a}-{\\boldsymbol{p}%&#10;_{A}}^{a},~{}{\\mathbf{R}_{B}}^{p}={\\mathbf{R}_{B}}^{a}\\cdot({\\mathbf{R}_{A}}^{%&#10;a})^{\\prime}\" display=\"block\"><mrow><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts><mo>-</mo><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>B</mi><none/><none/><mi>p</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts><mo>\u22c5</mo><msup><mrow><mo stretchy=\"false\">(</mo><mmultiscripts><mi>\ud835\udc11</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 29705, "prevtext": "\nand ${\\boldsymbol{p}_A}^a$ and ${\\mathbf{R}_A}^a$ are set to zero and identity\nmatrix respectively. Only the relative poses between the assembly parts are\nused.\n\n\\section{3D Detection during Robotic Execution}\n\nThe object pose detection during robot execution is done using Kinect, Point\nCloud Library, and geometric constraints. The detection cannot be done using\nmarkers since: (1) What the robot manipulates are thousands of industrial parts,\nit is impossible to attach markers to all of them. (2) The initial configuration\nof the object is unexpectable and the markers might be occluded from time to\ntime during robotic pick-and-place.\nThe detection is also not applicable to image-based approaches: Industrial\nparts are usually mono colored and textureless, image features are not only\nhelpless but even harmful. \n\nUsing Kinect is not trivial due to its low resolution and precision. For\nexample, the objects in industrial applications are usually in clutter and it is\ndifficult to segment one object from another using Kinect's resolution. Our\nsolution is to divide the difficulty in clutter into two subproblems. First, the\nrobot considers how to pick out one object from the clutter, and place the\nobject on an open plenary surface. Second, the robot estimate the pose of the\nsingle object on the open plenary surface. The first subproblem doesn't care\nwhat the object is or the pose of the object and its only goal is to pick\nsomething out.\nIt is referred as a pick-and-place problem in contemporary literature and is\nillustrated in the left part of Fig.\\ref{twoprb}. The first subproblem is well\nstudied and interested readers may refer to \\cite{Domae14} for the solutions.\n\nThe second subproblem is to detect the pose of a single object on an\nopen plenary surface and is shown in the right part of Fig.\\ref{twoprb}. It is\nmuch easier but still requires much proess to meet the precision requirements of\nassembly. We concentrate on precision and will discuss use point cloud\nalgorithms and geometric constraints to solve the second problem.\n\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/twoproblems.png}\n  \\caption{Overcome the clutter problem by dividing the pose detection into two\n  subproblems. The first one is picking out from clutter where the system\n  doesn't care what the object is or the pose of the object and its only goal is\n  to pick something out. The problem is well studied. The second one is to\n  detect the pose of a single object on an open plenary surface. It is not\n  trivial since the precision of Kinect is bad.}\n  \\label{twoprb}\n\\end{figure}\n\n\n\n\n\n\\subsection{Rough detection using point cloud}\n\nFirst, we roughly detect the pose of the object using CVFH and CRH features.\nIn a preprocessing process before starting the detection, we put the camera to\n42 positions on a unit sphere, save the view of the camera, and\nprecompute the CVFH and CRH features of each view. This step is virtually\nperformed using PCL and is shown in the dashed blue frame of Fig.\\ref{rawpose}.\nDuring the detection, we extract the plenary surface from the point\ncloud, segment the remaining points cloud, and compute the CVFH and CRH features of\neach segmentation. Then, we match the precomputed features with the features of\neach segment and estimate the orientation of the segmentations. This step is\nshown in the dashed red frame of Fig.\\ref{rawpose}. The matched segments are\nfurther refined using ICP to ensure good matching.\nThe segmentation that has highest ICP matches and smallest outlier points will\nbe used as the output. An example is shown in the ``Raw result''\nframebox of Fig.\\ref{rawpose}.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/rawpose.png}\n  \\caption{Rawly detecting the pose of an object using model matching and CVFH\n  and CRH features. In a preprocessing process before the detection, we\n  precompute the CVFH and CRH features of 42 different views and save them as\n  the template. The preprocessing process is shown in the\n  dashed blue frame. During the detection, we segment the remaining points\n  cloud, compute the CVFH and CRH features of each segmentation, and match them\n  to the precomputed views using ICP. The best match is used as the output.}\n  \\label{rawpose}\n\\end{figure}\n\n\\subsection{Noise correction using geomteric constraints}\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/posecorrection.png}\n  \\caption{Correcting the raw result using the stable placements on\n  a plenary surface (geometric constraints). In a preprocessing process before\n  the correction, we compute the stable placements of the object on a plenary\n  surface. An example is shown in the stable placements framebox. During the\n  correction, we compute the distance between the raw results and each of the\n  stable placements, and correct the raw results using the nearest pose.}\n  \\label{posecor}\n\\end{figure}\n\nThe result of rough detection is further refined using the geometric\nconstraints.\nSince the object is on plenary surface, its stable poses are limited\n\\cite{Wan2016ral} and can be used to correct the noises of the roughly estimated\nresult. The placement planning includes two steps. First, we compute the convex\nhull of the object mesh and perform surface clustering on the convex hull. Each\ncluster is one candidate standing surface where the object may be placed on.\nSecond, we check the stability of the objects standing on these candidate\nsurfaces. The unstable placements (the placement where the projection of center\nof mass is outside the candidate surface or too near to its boundary) are\nremoved.\nAn example of the stable placements for one object is shown in the \nstable placements framebox of Fig.\\ref{posecor}.\n\nGiven the raw detection result using CVFH and CRH features, the robot computes\nits distance to the stable placements and correct it following\nAlg.\\ref{ncalg}.\n\n\\begin{algorithm}[htbp!]\n\\DontPrintSemicolon\n    \\KwData{Raw result: $\\boldsymbol{p}_r$, $\\mathbf{R}_r$;\\\\\n            ~~~~~~~~~~~Stable placements: \\{$\\mathbf{R}_p(i),~i=1,2,\\ldots$\\}\\\\\n            ~~~~~~~~~~~Table height: $h_t$}\n    \\KwResult{Corrected result: $\\boldsymbol{p}_c$, $\\mathbf{R}_c$}\n    \\SetKwFunction{size}{size}\n    \\SetKwFunction{rpyFromRot}{rpyFromRot}\n    \\SetKwFunction{rotFromRpy}{rotFromRpy}\n    $d_{min}\\leftarrow+\\infty$\\;\n    $\\mathbf{R}_{near}\\leftarrow\\mathbf{I}$\\;\n    \\For{$i\\leftarrow1$ \\KwTo $\\mathbf{R}_p$.\\size{}} {\n        $d_i\\leftarrow||\\log(\\mathbf{R}_p(i){\\mathbf{R}_p}{'})||$\\;\n        \\If{$d_i<d_{min}$} {\n            $d_{min} \\leftarrow d_i$\\;\n            $\\mathbf{R}_{near} \\leftarrow \\mathbf{R}_p(i)$\\;\n        }\n    }\n    $\\mathbf{R}_{yaw} \\leftarrow$ \\rotFromRpy{0, 0,\n    \\rpyFromRot{$\\mathbf{R}_{r}$}$.y$}\\;\n    $\\mathbf{R}_c\\leftarrow\\mathbf{R}_{yaw}\\cdot\\mathbf{R}_{near}$\\;\n    $\\boldsymbol{p}_c\\leftarrow[\\boldsymbol{p}_r.x, \\boldsymbol{p}_r.y,\n    h_t]'$\n    \\caption{Noise correction}\n    \\label{ncalg}\n\\end{algorithm}\n\nIn this pseudo code, $\\mathbf{I}$ indicates a $3\\times3$ identity matrix.\nFunctions $\\mathtt{rotFromRpy}$ and $\\mathtt{rpyFromRot}$ converts $roll$,\n$pitch$, $yaw$ angles to rotation matrix and vice verse. The distance between\ntwo rotation matrices is computed in line 4. The corrected result is updated at\nlines 11 and 12.\n\n\\section{Grasp and Motion Planning}\n\nAfter finding the poses of the parts on the plenary surface, what the robot does\nnext is to grasp the parts and assemble them. It includes two steps: A grasp\nplanning step and a motion planning step. \n\n\\subsection{Grasp planning}\nIn the grasp planning step, we set the\nobject at free space and compute the force-closured and collision-free grasps.\nEach grasp is represented using ${\\boldsymbol{g}_X}^f$=$\\{\\boldsymbol{p}_0,\n\\boldsymbol{p}_1, \\mathbf{R}\\}$ where $\\boldsymbol{p}_0$ and\n$\\boldsymbol{p}_1$ are the contact positions of the finger tips, $\\mathbf{R}$\nis the orientation of the palm. The whole set is represented by\n${\\mathbf{g}_X}^f$, which includes many ${\\boldsymbol{g}_X}^f$. Namely,\n${\\mathbf{g}_X}^f$ = $\\{{\\boldsymbol{g}_X}^f\\}$.\n\nGiven the pose of a part on the plenary surface, say ${\\boldsymbol{p}_X}^s$\nand ${\\mathbf{R}_X}^s$, the IK-feasible and collision-free grasps that the robot\ncan use to pick up the object is computed following\n\n", "index": 3, "text": "\\begin{equation}\n    {\\mathbf{g}_X}^{s}\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_X}^{s'}~)\n    \\cap \n    \\mathsf{CD}~(~{\\mathbf{g}_X}^{s'},~planery~surface~)\n    \\label{transsurf1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{g}_{X}}^{s}=\\mathsf{IK}~{}(~{}{\\mathbf{g}_{X}}^{s^{\\prime}}~{})\\cap%&#10;\\mathsf{CD}~{}(~{}{\\mathbf{g}_{X}}^{s^{\\prime}},~{}planery~{}surface~{})\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>X</mi><none/><none/><mi>s</mi></mmultiscripts><mo>=</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>\ud835\udda8\ud835\uddaa</mi></mpadded><mo>\u2062</mo><mrow><mo rspace=\"5.8pt\" stretchy=\"false\">(</mo><mpadded width=\"+3.3pt\"><mmultiscripts><mi>\ud835\udc20</mi><mi>X</mi><none/><none/><msup><mi>s</mi><mo>\u2032</mo></msup></mmultiscripts></mpadded><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2229</mo><mrow><mpadded width=\"+3.3pt\"><mi>\ud835\udda2\ud835\udda3</mi></mpadded><mo>\u2062</mo><mrow><mo rspace=\"5.8pt\" stretchy=\"false\">(</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>X</mi><none/><none/><msup><mi>s</mi><mo>\u2032</mo></msup></mmultiscripts><mo rspace=\"5.8pt\">,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>y</mi></mpadded><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>e</mi></mpadded></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\n\n${\\mathbf{R}_X}^s\\cdot{\\mathbf{g}_X}^f+{\\boldsymbol{p}_X}^s$ transforms the\ngrasps at free space to the pose of the object. ${\\mathbf{g}_X}^{s'}$ denotes\nthe transformed grasp set. $\\mathsf{IK}()$ finds the IK-feasible\ngrasps from the input set. $\\mathsf{CD}()$ checks the collision between the two\ninput elements and finds the collision-free grasps. ${\\mathbf{g}_X}^{s}$\ndenotes the IK-feasible and collision-free grasps that the robot can use to pick\nup the object.\n\nLikewise, given the pose of object A in the assembled structure, say\n${\\boldsymbol{p}_A}^a$ and ${\\mathbf{R}_A}^a$, the IK-feasible and\ncollision-free grasps that the robot can use to assemble it is computed\nfollowing\n\n", "itemtype": "equation", "pos": 29902, "prevtext": "\nwhere\n\n", "index": 5, "text": "\\begin{equation}\n    {\\mathbf{g}_X}^{s'}\n    =\n    {\\mathbf{R}_X}^s\\cdot{\\mathbf{g}_X}^f+{\\boldsymbol{p}_X}^s \n    \\label{transsurf2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{g}_{X}}^{s^{\\prime}}={\\mathbf{R}_{X}}^{s}\\cdot{\\mathbf{g}_{X}}^{f}+{%&#10;\\boldsymbol{p}_{X}}^{s}\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>X</mi><none/><none/><msup><mi>s</mi><mo>\u2032</mo></msup></mmultiscripts><mo>=</mo><mrow><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>X</mi><none/><none/><mi>s</mi></mmultiscripts><mo>\u22c5</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>X</mi><none/><none/><mi>f</mi></mmultiscripts></mrow><mo>+</mo><mmultiscripts><mi>\ud835\udc91</mi><mi>X</mi><none/><none/><mi>s</mi></mmultiscripts></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 30739, "prevtext": "\n\n${\\mathbf{R}_X}^s\\cdot{\\mathbf{g}_X}^f+{\\boldsymbol{p}_X}^s$ transforms the\ngrasps at free space to the pose of the object. ${\\mathbf{g}_X}^{s'}$ denotes\nthe transformed grasp set. $\\mathsf{IK}()$ finds the IK-feasible\ngrasps from the input set. $\\mathsf{CD}()$ checks the collision between the two\ninput elements and finds the collision-free grasps. ${\\mathbf{g}_X}^{s}$\ndenotes the IK-feasible and collision-free grasps that the robot can use to pick\nup the object.\n\nLikewise, given the pose of object A in the assembled structure, say\n${\\boldsymbol{p}_A}^a$ and ${\\mathbf{R}_A}^a$, the IK-feasible and\ncollision-free grasps that the robot can use to assemble it is computed\nfollowing\n\n", "index": 7, "text": "\\begin{equation}\n    {\\mathbf{g}_A}^{a}\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_A}^{a'}~)\n    \\cap \n    \\mathsf{CD}~(~{\\mathbf{g}_A}^{a'},\\mathsf{objB}(~{\\boldsymbol{p}_B}^a,\n    {\\mathbf{R}_B}^a~)~)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{g}_{A}}^{a}=\\mathsf{IK}~{}(~{}{\\mathbf{g}_{A}}^{a^{\\prime}}~{})\\cap%&#10;\\mathsf{CD}~{}(~{}{\\mathbf{g}_{A}}^{a^{\\prime}},\\mathsf{objB}(~{}{\\boldsymbol{%&#10;p}_{B}}^{a},{\\mathbf{R}_{B}}^{a}~{})~{})\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts><mo>=</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>\ud835\udda8\ud835\uddaa</mi></mpadded><mo>\u2062</mo><mrow><mo rspace=\"5.8pt\" stretchy=\"false\">(</mo><mpadded width=\"+3.3pt\"><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><msup><mi>a</mi><mo>\u2032</mo></msup></mmultiscripts></mpadded><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2229</mo><mrow><mpadded width=\"+3.3pt\"><mi>\ud835\udda2\ud835\udda3</mi></mpadded><mo>\u2062</mo><mrow><mo rspace=\"5.8pt\" stretchy=\"false\">(</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><msup><mi>a</mi><mo>\u2032</mo></msup></mmultiscripts><mo>,</mo><mrow><mi>\ud835\uddc8\ud835\uddbb\ud835\uddc3\ud835\udda1</mi><mo>\u2062</mo><mrow><mo rspace=\"5.8pt\" stretchy=\"false\">(</mo><mmultiscripts><mi>\ud835\udc91</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts><mo>,</mo><mpadded width=\"+3.3pt\"><mmultiscripts><mi>\ud835\udc11</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts></mpadded><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\n\n${\\mathbf{R}_A}^a\\cdot{\\mathbf{g}_A}^f+{\\boldsymbol{p}_A}^a$ transforms\nthe grasps at free space to the pose of the object in the assembled structure.\n${\\mathbf{g}_X}^{a'}$ denotes the transformed grasp set. $\\mathsf{IK}()$ \nand $\\mathsf{CD}()$ are the same as Eqn(\\ref{transsurf1}).\n$\\mathsf{objB}(~{\\boldsymbol{p}_B}^a, {\\mathbf{R}_B}^a~)$ indicates the mesh\nmodel of object B at pose ${\\boldsymbol{p}_B}^a, {\\mathbf{R}_B}^a$. ${\\mathbf{g}_A}^{a}$\ndenotes the IK-feasible and collision-free grasps that the robot can use to\nassemble the object.\n\n\\subsection{Motion planning}\nIn the motion planning step, we build a graph using the elements in\n${\\mathbf{g}_X}^{s}$ and ${\\mathbf{g}_X}^{a}$, search the grasp to find\nhigh-level keyframes, and perform Transition-based Rapidly-Exploring Random\nTree (Transition-RRT) \\cite{Jaillet08} motion planning between the keyframes to\nfind assemble motions.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/motionplanning.png}\n  \\caption{The flow of the motion planning. Given initial and goal poses of an\n  object (left images in the upperleft and lowerleft frameboxes), we search its\n  available initial and goal grasps and use the common grasps and IK to get the\n  initial and goal configurations of the robot arm.\n  Then, we do motion planning repeatedly between the initial and goal\n  configurations to find a solution to the desired task.\n  }\n  \\label{mflow}\n\\end{figure}\n\nFig.\\ref{mflow} shows the flow. The object \\textit{X} in this graph is a wooden\nblock shown in the left image of the upper-left frame box. The image also shows\nthe pose of this object on the plenary surface, ${\\boldsymbol{p}_X}^s$ and\n${\\mathbf{R}_X}^s$. When the object is assembled in the structure, its pose\n${\\boldsymbol{p}_X}^a$ and ${\\mathbf{R}_S}^a$ is shown in the left image of the\nbottom-left frame box.\nThe grasps associated with the poses are shown in the right images of the two\nframe boxes. They are rendered using the colored hand model. Green,\nblue, and red denote IK-feasible and collision free,\nIK-infeasible, and collided grasps respectively.\nWe build a graph to find the common grasps and employ Transition-RRT to find the\nmotion between the initial configuration and goal configuration.\n\nIn practice, the flow in Fig.\\ref{mflow} doesn't work since the goal\nconfiguration is in the assembled structure and is in the narrow passages or on\nthe boundaries of the configuration space. The motion planning problem is a\nnarrow-passage \\cite{Wan2008} or peg-in-hole problem \\cite{Yun2008} which is not\nsolvable. We overcome the difficulty by adding a pre-assemble configuration:\nFor the two objects A and B, we retract them from the structure following the\napproaching direction $\\boldsymbol{v}^a$ of the two objects and get the\npre-assemble poses ${\\boldsymbol{p}_A}^p$, ${\\mathbf{R}_A}^p$, and\n${\\boldsymbol{p}_B}^p$, ${\\mathbf{R}_B}^p$ where\n\n", "itemtype": "equation", "pos": 30956, "prevtext": "\nwhere\n\n", "index": 9, "text": "\\begin{equation}\n    {\\mathbf{g}_A}^{a'} =\n    {\\mathbf{R}_A}^a\\cdot{\\mathbf{g}_A}^f+{\\boldsymbol{p}_A}^a \\\\\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{g}_{A}}^{a^{\\prime}}={\\mathbf{R}_{A}}^{a}\\cdot{\\mathbf{g}_{A}}^{f}+{%&#10;\\boldsymbol{p}_{A}}^{a}\\\\&#10;\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><msup><mi>a</mi><mo>\u2032</mo></msup></mmultiscripts><mo>=</mo><mrow><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts><mo>\u22c5</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mi>f</mi></mmultiscripts></mrow><mo>+</mo><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\n\nThe grasps associated with the retracted poses are\n\n\\begin{eqnarray}\n    {\\mathbf{g}_A}^p\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_A}^{p'}~),~\\mathrm{where}~\n    {\\mathbf{g}_A}^{p'}\n    =\n    {\\mathbf{g}_A}^a+0.5\\boldsymbol{v}^a\n    \\label{retg1}\\\\\n    {\\mathbf{g}_B}^p\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_B}^{p'}~),~\\mathrm{where}~\n    {\\mathbf{g}_B}^{p'}\n    =\n    {\\mathbf{g}_B}^a-0.5\\boldsymbol{v}^a\n    \\label{retg2}\n\\end{eqnarray}\n\nNote that the poses in Eqn(\\ref{ret1}-\\ref{retg2}) are in the local coordinate\nof object A where ${\\boldsymbol{p}_A}^a$ is a zero vector and\n${\\boldsymbol{R}_A}^a$ is an identity matrix. Given the pose of object A in\nworld coordinate, ${\\boldsymbol{p}_A}^{a(g)}$ and ${\\boldsymbol{R}_A}^{a(g)}$,\nthe grasps in the world coordinate are computed using\n\n\n", "itemtype": "equation", "pos": 33981, "prevtext": "\n\n${\\mathbf{R}_A}^a\\cdot{\\mathbf{g}_A}^f+{\\boldsymbol{p}_A}^a$ transforms\nthe grasps at free space to the pose of the object in the assembled structure.\n${\\mathbf{g}_X}^{a'}$ denotes the transformed grasp set. $\\mathsf{IK}()$ \nand $\\mathsf{CD}()$ are the same as Eqn(\\ref{transsurf1}).\n$\\mathsf{objB}(~{\\boldsymbol{p}_B}^a, {\\mathbf{R}_B}^a~)$ indicates the mesh\nmodel of object B at pose ${\\boldsymbol{p}_B}^a, {\\mathbf{R}_B}^a$. ${\\mathbf{g}_A}^{a}$\ndenotes the IK-feasible and collision-free grasps that the robot can use to\nassemble the object.\n\n\\subsection{Motion planning}\nIn the motion planning step, we build a graph using the elements in\n${\\mathbf{g}_X}^{s}$ and ${\\mathbf{g}_X}^{a}$, search the grasp to find\nhigh-level keyframes, and perform Transition-based Rapidly-Exploring Random\nTree (Transition-RRT) \\cite{Jaillet08} motion planning between the keyframes to\nfind assemble motions.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.4in]{fig/motionplanning.png}\n  \\caption{The flow of the motion planning. Given initial and goal poses of an\n  object (left images in the upperleft and lowerleft frameboxes), we search its\n  available initial and goal grasps and use the common grasps and IK to get the\n  initial and goal configurations of the robot arm.\n  Then, we do motion planning repeatedly between the initial and goal\n  configurations to find a solution to the desired task.\n  }\n  \\label{mflow}\n\\end{figure}\n\nFig.\\ref{mflow} shows the flow. The object \\textit{X} in this graph is a wooden\nblock shown in the left image of the upper-left frame box. The image also shows\nthe pose of this object on the plenary surface, ${\\boldsymbol{p}_X}^s$ and\n${\\mathbf{R}_X}^s$. When the object is assembled in the structure, its pose\n${\\boldsymbol{p}_X}^a$ and ${\\mathbf{R}_S}^a$ is shown in the left image of the\nbottom-left frame box.\nThe grasps associated with the poses are shown in the right images of the two\nframe boxes. They are rendered using the colored hand model. Green,\nblue, and red denote IK-feasible and collision free,\nIK-infeasible, and collided grasps respectively.\nWe build a graph to find the common grasps and employ Transition-RRT to find the\nmotion between the initial configuration and goal configuration.\n\nIn practice, the flow in Fig.\\ref{mflow} doesn't work since the goal\nconfiguration is in the assembled structure and is in the narrow passages or on\nthe boundaries of the configuration space. The motion planning problem is a\nnarrow-passage \\cite{Wan2008} or peg-in-hole problem \\cite{Yun2008} which is not\nsolvable. We overcome the difficulty by adding a pre-assemble configuration:\nFor the two objects A and B, we retract them from the structure following the\napproaching direction $\\boldsymbol{v}^a$ of the two objects and get the\npre-assemble poses ${\\boldsymbol{p}_A}^p$, ${\\mathbf{R}_A}^p$, and\n${\\boldsymbol{p}_B}^p$, ${\\mathbf{R}_B}^p$ where\n\n", "index": 11, "text": "\\begin{gather}\n    {\\boldsymbol{p}_A}^p\n    =\n    {\\boldsymbol{p}_A}^a+0.5\\boldsymbol{v}^a,~\n    {\\mathbf{R}_A}^{p}\n    =\n    {\\mathbf{R}_A}^{a}\n    \\label{ret1}\\\\\n    {\\boldsymbol{p}_B}^p\n    =\n    {\\boldsymbol{p}_B}^a-0.5\\boldsymbol{v}^a,~\n    {\\mathbf{R}_B}^{p}\n    =\n    {\\mathbf{R}_B}^{a}\n    \\label{ret2}\n\\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\boldsymbol{p}_{A}}^{p}={\\boldsymbol{p}_{A}}^{a}+0.5\\boldsymbol{%&#10;v}^{a},~{}{\\mathbf{R}_{A}}^{p}={\\mathbf{R}_{A}}^{a}\" display=\"block\"><mrow><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mi>p</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts><mo>+</mo><mrow><mn>0.5</mn><mo>\u2062</mo><msup><mi>\ud835\udc97</mi><mi>a</mi></msup></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>A</mi><none/><none/><mi>p</mi></mmultiscripts><mo>=</mo><mmultiscripts><mi>\ud835\udc11</mi><mi>A</mi><none/><none/><mi>a</mi></mmultiscripts></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\boldsymbol{p}_{B}}^{p}={\\boldsymbol{p}_{B}}^{a}-0.5\\boldsymbol{%&#10;v}^{a},~{}{\\mathbf{R}_{B}}^{p}={\\mathbf{R}_{B}}^{a}\" display=\"block\"><mrow><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>B</mi><none/><none/><mi>p</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts><mo>-</mo><mrow><mn>0.5</mn><mo>\u2062</mo><msup><mi>\ud835\udc97</mi><mi>a</mi></msup></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mmultiscripts><mi>\ud835\udc11</mi><mi>B</mi><none/><none/><mi>p</mi></mmultiscripts><mo>=</mo><mmultiscripts><mi>\ud835\udc11</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06473.tex", "nexttext": "\n\nThe moton planning is then to find a motion between one initial configuration in \n${\\mathbf{g}_X}^s$ to a goal configuration in ${\\mathbf{g}_X}^p(g)$ where $X$\nis either $A$ or $B$. There is no motion between ${\\mathbf{g}_A}^p(g)$ and\n${\\mathbf{g}_A}^a(g)$ since they are equal to each other. The motion between\n${\\mathbf{g}_B}^p(g)$ and ${\\mathbf{g}_B}^a(g)$ is hard coded along\n$\\boldsymbol{v}^a$.\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width = 3.2in]{fig/graph.png}\n  \\caption{The grasp graph built using ${\\mathbf{g}_X}^{s}$ and\n  ${\\mathbf{g}_X}^{p(g)}$. It has three layers where the top layer encodes the\n  grasps associated with the initial configuration, the middle layer encodes\n  the grasps associated with placements on planery sufaces, and the bottom\n  layer encodes the grasps associated with the assemble pose. The left image\n  shows one ${\\mathbf{g}_X}^p(g)$ (the virtual grasp illustrated in cyan).\n  It corresponds to a node in the bottom layer. The subimages in the frame box\n  illustrate the placements (yellow) and their associated grasps (green).}\n  \\label{graph}\n\\end{figure}\n\nWhich initial and goal configuration to use is decided by building and searching\na grasp graph which is built using ${\\mathbf{g}_X}^{s}$ and\n${\\mathbf{g}_X}^{p(g)}$, and is shown in the frame box of Fig.\\ref{graph}. The\ngraph is basically the same as \\cite{Wan2016ral}, but has three layers. The top\nlayer has only one circle and is mapped to the initial configuration. The bottom\nlayer also has only one circle and is mapped to the goal configuration. The\nmiddle layers are composed of several circles where each of them maps a\npossible placement on a plenary surface.\nEach node of the circles represents a grasp: The ones in the upper layers are\nfrom ${\\mathbf{g}_X}^s$, and the ones in the bottom layers are from\n${\\mathbf{g}_X}^{p(g)})$. The ones from the middle layers are the grasps\nassociated with the placements. The orientations of the placements are evenly\nsampled on line. Their positions are fixed to the initial position\n${\\mathbf{p}_A}^s$.\nIf the circles share some grasps (grasps with the same $\\boldsymbol{p}_0$,\n$\\boldsymbol{p}_1$, $\\mathbf{R}$ values in the object's local coordinate\nsystem), we connect them at the correspondent nodes. We search the graph to find\nthe initial and goal configurations and a sequence of high-level keyframes, and\nperform motion planning between the keyframes to perform desired tasks. An\nexemplary result will be shown in the experiment section.\n\n\\section{Experiments and Analysis}\n\nWe performed experiments to examine the precision of the developed approaches,\nanalyze the process of grasp and motion planning,\nand demonstrate the applicability of the study using a Kawada Nextage Robot.\nThe camera used in the human teaching phase is a logicool HD Webcam C615. The\ncomputer system is a Lenovo Thinkpad E550 laptop (Processor: Intel Core i5-5200\n2.20GHz Clock, Memory: 4G 1600MHz DDR3).\nThe depth sensor used in the robotic execution phase is Kinect. The computer\nsystem used to compute the grasps and motions is a Dell T7910 workstation\n(Processor: Intel Xeon E5-2630 v3 with 8CHT, 20MB Cache, and 2.4GHz Clock,\nMemory: 32G 2133MHz DDR4).\n\n\\subsection{Precision of the object detection in human teaching}\n\nFirst, we examine the precision of object detection during human teaching.\nWe use two sets of assembly parts and examine the precision of five assembly\nstructures for each set. Moreover, for each assembly structure, we examine the\nvalues of at five different orientations to make the results confident.\n\nThe two sets and ten structures (five for each) are shown in the first row of\nFig.\\ref{exphumandemo}. Each structure is posed at five different orientations\nto examine the precision. The five data rows under the subimage row in\nFig.\\ref{exphumandemo} are the result at the orientations. Each grid of the\ndata rows is shown in the form $\\Delta d (\\Delta r, \\Delta p, \\Delta y)$ where\n$\\Delta d$ is the difference in the measured\n$|{\\boldsymbol{p}_A}^{a}-{\\boldsymbol{p}_B}^{a}|$ and the actual value. $(\\Delta\nr, \\Delta p, \\Delta y)$ are the difference in the measured $roll$, $pitch$, and\n$yaw$ angles. The last row of the figure shows the average detection error of\neach structure in the form $|\\Delta d| (|\\Delta r|, |\\Delta p|, |\\Delta y|)$\nwhere $|\\cdot|$ indicates the absolute value. The metrics are $millimeter$\n($mm$) for distance and $degree$ ($^\\circ$) for orientation. The precision in\nposition is less than 1$mm$ and the precision in orientation is less than\n2$^\\circ$ on average.\n\n\n\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width = 7.2in]{fig/exp1.png}\n  \\caption{Results of the object detected during human teaching. The image\n  row shows the structure to be assembled. Each structure is posed at five\n  different orientations to examine the precision and the detected error\n  in distance and orientation are shown in the five data rows\n  below. Each grid of the data rows is shown in the form $\\Delta d (\\Delta r,\n  \\Delta p, \\Delta y)$ where $\\Delta d$ is the difference in the measured\n  $|{\\boldsymbol{p}_A}^{a}-{\\boldsymbol{p}_B}^{a}|$ and the actual value.\n  $(\\Delta r, \\Delta p, \\Delta y)$ are the difference in the measured roll,\n  pitch, and yaw angles. The last row is the average detection error. The\n  metrics are $millimeter$ for distance and $degree$ for orientation.}\n  \\label{exphumandemo}\n\\end{figure*}\n\n\n\\subsection{Precision of the object detection in robotic execution}\n\nThen, we examine the pecision of the object detection in the robot execution\nphase. Three objects with eight placements are used during the process. They are\nshown in the top row of Fig.\\ref{exprobotexe}. The plenary surface is set in\nfront of the robot and is divided into four quarters. We place each placement\ninto each quoter to get the average values. There are five\ndata rows divided by dashed or solid lines in Fig.\\ref{exprobotexe} where the\nfirst four of them show the individual detection precision at each quoter and\nthe last one shows the average detection precision. The detection precision is\nthe difference between the detected value and the groundtruth. Since we know the\nexact model of the object and the height of the table, the groundtruth is know\nbeforehand. The average detection precision in the last row are the\nmean of the absolute difference.\n\nInside each data grid there are four triples where the upper two are the\nroughly detected position and orientation and the lower two are the corrected\nvalues. The roughly detected results are marked with red shadows and the\ncorrected results are marked in green. The three values of the position\ntriples are the $x$, $y$, $z$ coordinates, and their metrics are\n$millimeters$ ($mm$). The three values of the orientation triples are the\n$roll$, $pitch$, $yaw$ angles and their metrics are $degree$ ($^\\circ$).\n\n\n\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width = 7.2in]{fig/exp2.png}\n  \\caption{Results of the object detected during robotic execution. The figure\n  includes three subfigure rows and five data rows where the first data row\n  show the target object pose, the second and third subfigure rows plot some\n  examples of the roughly detected poses and the corrected poses. The five data\n  rows are divided by dashed or solid lines where the first four of them show\n  the individual detection precision at four different positions and the last\n  one shows the average detection precision. Each data grid of the data rows\n  include four triples where the upper two (under red shadow) are the roughly\n  detected position and orientation and the lower two (under green shadow) are\n  the corrected values. The three values of the position triples are the $x$,\n  $y$, $z$ coordinates, and their metrics are $millimeters$ ($mm$). The three\n  values of the orientation triples are the $roll$, $pitch$, $yaw$ angles and\n  their metrics are $degree$ ($^\\circ$). The maximum values of each data\n  element are marked with colored frameboxes.}\n  \\label{exprobotexe}\n\\end{figure*}\n\n\nThe results show that the maximum errors of rough position detection are\n-1.1$mm$, 3.3$mm$, and 2.3$mm$ along $x$, $y$, and $z$ axis. They are marked\nwith red boxes in Fig.\\ref{exprobotexe}. After correction, the maximum position\nerrors change to -1.1$mm$, 3.6$mm$, 0.0$mm$ respectively. They are marked with\ngreen boxes. The maximum errors of rough orientation detection are\n-26.3$^\\circ$, 26.1$^\\circ$, and -19.5$^\\circ$ in $roll$, $pitch$, and $yaw$\nangles. They are marked with red boxes. The errors change to $0.0$, $0.0$, and\n-$21.4$ after correction. The correction using geometric constraints completely\ncorrects the erros along $z$ axis and in $roll$ and $pitch$ angles. It might\nslightly increase the errors along $x$ and $y$ and in $yaw$ but the values are\nignorable. The average performance can be found from the data under thee dual\nsolid line. The performance is good enough for robotic assembly.\n\nIn addition, the second and third subfigure rows of Fig.\\ref{exprobotexe} plot\nsome examples of the roughly detected poses and the corrected poses.\nReaders may compare them with the data rows.\n\n\\subsection{Simulation and real-world results}\n\nFig.\\ref{expcorr} shows the correspondence between the paths found by the\nsearching algorithms and the configurations of the robot and the object. The\nstructure to be assembled in this task is the first one (upper-left one) shown in\nFig.\\ref{exphumandemo}. We do motion planning along the paths repeatedly to\nperform the desired tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width = 7.05in]{fig/expcorr_one.png}\n  \\caption{The snapshots of assembling the structure shown in\n   Fig.\\ref{exphumandemo}. It is divided into two step with the first step shown\n   in (1)-(4) and the second step shown in (5)-(8). In the first step, the robot\n   picks up object A and transfers it to the goal pose. In the second step,\n   the robot finds object B on the table and assembles it to object A.\n   The subfigures (1')-(8') shows correspondent path edges and nodes on the\n   three-layer graph.}\n  \\label{expcorr}\n\\end{figure*}\n\n\nThe assembly process is divided into two step with each step corresponds to one\nassembly part. In the first step, the robot finds object A on the table and\nmoves it to a goal pose using the three-layer graph. The\nsubfigures (1)-(4) of Fig.\\ref{expcorr} shows this step. In\nFig.\\ref{expcorr}(1), the robot computes the grasps associated with the initial\npose and goal pose of object A. The associated grasps are rendered in green,\nblue, and red colors like Fig.\\ref{flow}. They corresponds to the top and bottom\nlayer of the grasp shown in Fig.\\ref{expcorr}(1'). In\nFig.\\ref{expcorr}(2), the robot chooses one feasible (IK-feasible and\ncollision-free) grasp from the associated grasps and does motion planning to\npick up the object. The selected grasp corresponds to one node in the top layer\nof the graph, which is marked with red color in Fig.\\ref{expcorr}(2'). In\nFig.\\ref{expcorr}(3), the robot picks up object A and transfers it to the goal\npose using a second motion planning. This corresponds to an edge in\nFig.\\ref{expcorr}(3') which connects the node in one circle to the node in\nanother. The edge directly connects to the goal in this example and there is no\nintermediate placements. After that, the robot moves its arm back at\nFig.\\ref{expcorr}(4), which corresponds to a node in the bottom layer of the\ngraph shown in Fig.\\ref{expcorr}(4').\n\nIn the second step, the robot finds object B on the table and\nassembles it to object A. The subfigures (5)-(8) Fig.\\ref{expcorr}(b) show it.\nIn Fig.\\ref{expcorr}(5), the robot computes the grasps associated with the\ninitial pose and goal pose of object B. They are rendered in\ngreen, blue, and red colors like Fig.\\ref{expcorr}(1) and are correspondent\nto the top and bottom layer of the grasp shown in Fig.\\ref{expcorr}(5'). In\nFig.\\ref{expcorr}(6), the robot chooses one feasible grasp and does motion\nplanning to pick up the object. The selected grasp corresponds to the marked\nnode in Fig.\\ref{expcorr}(6'). In Fig.\\ref{expcorr}(7), the robot picks\nup object B and assembles it to the goal pose using a second motion planning\nwhich corresponds to an edge in Fig.\\ref{expcorr}(7'). Finally, the robot\nmoves its arm back at Fig.\\ref{expcorr}(8) and (8').\n\nThe subfigures (1'')-(8'') in the third row show how the robot executes the\nplanned motion. They correspond to (1)-(8) and (1')-(8') in the first two rows.\n\n\\section{Conclusions and Future Work}\n\nWe presented precise 3D visual detection approaches in this paper to meet the\nrequirements of a smart mechanical assembly system. In the human teaching phase\nof the system where human beings control the operation and can actively avoid\nocclusion, we use AR markers and compute the pose of the object by detecting\nthe markers' poses. In the robot execution phase where occlusions happen\nunexpectedly, we use point cloud matching to find a raw pose and use extrinsic\nconstraints to correct the noises. We examine the precision of the approaches\nin the experiment part and demonstrate that the precision fulfills assembly\ntasks using a graph model and an industrial robot.\n\nThe future work will be on the manipulation and assembly aspect. The current\nresult is position-based assembly. It will be extended to force-based assembly\ntasks like inserting, snapping, etc., in the future.\n\n\\bibliographystyle{elsarticle-num}\n\\bibliography{references}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 35090, "prevtext": "\n\nThe grasps associated with the retracted poses are\n\n\\begin{eqnarray}\n    {\\mathbf{g}_A}^p\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_A}^{p'}~),~\\mathrm{where}~\n    {\\mathbf{g}_A}^{p'}\n    =\n    {\\mathbf{g}_A}^a+0.5\\boldsymbol{v}^a\n    \\label{retg1}\\\\\n    {\\mathbf{g}_B}^p\n    =\n    \\mathsf{IK}~(~{\\mathbf{g}_B}^{p'}~),~\\mathrm{where}~\n    {\\mathbf{g}_B}^{p'}\n    =\n    {\\mathbf{g}_B}^a-0.5\\boldsymbol{v}^a\n    \\label{retg2}\n\\end{eqnarray}\n\nNote that the poses in Eqn(\\ref{ret1}-\\ref{retg2}) are in the local coordinate\nof object A where ${\\boldsymbol{p}_A}^a$ is a zero vector and\n${\\boldsymbol{R}_A}^a$ is an identity matrix. Given the pose of object A in\nworld coordinate, ${\\boldsymbol{p}_A}^{a(g)}$ and ${\\boldsymbol{R}_A}^{a(g)}$,\nthe grasps in the world coordinate are computed using\n\n\n", "index": 13, "text": "\\begin{gather}\n    {\\mathbf{g}_A}^{a(g)}={\\mathbf{g}_A}^{p(g)}\\\\\n    {\\mathbf{g}_B}^{a(g)}=\n    {\\boldsymbol{p}_A}^{a(g)}+{\\boldsymbol{R}_A}^{a(g)}\\cdot{\\mathbf{g}_B}^a\\\\\n    {\\mathbf{g}_A}^{p(g)}=\n    {\\boldsymbol{p}_A}^{a(g)}+{\\boldsymbol{R}_A}^{a(g)}\\cdot{\\mathbf{g}_A}^p\\\\\n    {\\mathbf{g}_B}^{p(g)}=\n    {\\boldsymbol{p}_A}^{a(g)}+{\\boldsymbol{R}_A}^{a(g)}\\cdot{\\mathbf{g}_B}^p\n\\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{g}_{A}}^{a(g)}={\\mathbf{g}_{A}}^{p(g)}\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>=</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{g}_{B}}^{a(g)}={\\boldsymbol{p}_{A}}^{a(g)}+{\\boldsymbol{%&#10;R}_{A}}^{a(g)}\\cdot{\\mathbf{g}_{B}}^{a}\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>B</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>+</mo><mrow><mmultiscripts><mi>\ud835\udc79</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>\u22c5</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>B</mi><none/><none/><mi>a</mi></mmultiscripts></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{g}_{A}}^{p(g)}={\\boldsymbol{p}_{A}}^{a(g)}+{\\boldsymbol{%&#10;R}_{A}}^{a(g)}\\cdot{\\mathbf{g}_{A}}^{p}\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>+</mo><mrow><mmultiscripts><mi>\ud835\udc79</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>\u22c5</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>A</mi><none/><none/><mi>p</mi></mmultiscripts></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{g}_{B}}^{p(g)}={\\boldsymbol{p}_{A}}^{a(g)}+{\\boldsymbol{%&#10;R}_{A}}^{a(g)}\\cdot{\\mathbf{g}_{B}}^{p}\" display=\"block\"><mrow><mmultiscripts><mi>\ud835\udc20</mi><mi>B</mi><none/><none/><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>\ud835\udc91</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>+</mo><mrow><mmultiscripts><mi>\ud835\udc79</mi><mi>A</mi><none/><none/><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mmultiscripts><mo>\u22c5</mo><mmultiscripts><mi>\ud835\udc20</mi><mi>B</mi><none/><none/><mi>p</mi></mmultiscripts></mrow></mrow></mrow></math>", "type": "latex"}]