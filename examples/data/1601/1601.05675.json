[{"file": "1601.05675.tex", "nexttext": "\n\nwhere $I_{{\\mathcal{S}}}\\in{\\mathbb{R}}^{n\\times n}$ is the identity matrix with zeros\ncorresponding to the nodes not in ${\\mathcal{S}}$ and $\\gamma$ is a\nregularization parameter. The solution can be computed in closed form as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ {\\mathbf{y}}_{\\mathcal{S}}$, where\n${\\mathbf{y}}_{\\mathcal{S}} = I_{\\mathcal{S}} {\\mathbf{y}} \\in {\\mathbb{R}}^n$.\nThe singularity of the Laplacian may lead to unstable behavior with\ndrastically different results for small perturbations to the dataset. For this\nreason, we focus on the {\\textsc{Stable-HFS}\\xspace} algorithm proposed\nin~\\cite{belkin2004regularization} where an additional regularization term is\nintroduced to restrict the space of admissible hypotheses to the space\n${\\mathcal{F}} = \\left\\{{\\mathbf{f}} : \\langle{\\mathbf{f}},{\\mathbf{1}}\\rangle = 0\\right\\}$ of functions\northogonal to null space of $L_{\\mathcal{G}}$ (i.e., centered functions). This\nrestriction can be easily enforced by introducing an additional regularization\nterm $\\frac{\\mu}{l} {\\mathbf{f}}^\\top {\\mathbf{1}}$ in Eq.~\\ref{eq:hfs.original}. As shown\nin~\\cite{belkin2004regularization}, in order to guarantee that the resulting\n${\\widehat{{{\\mathbf{f}}}}}$ actually belongs to ${\\mathcal{F}}$, it is sufficient to set the\nregularization parameter to\n$\\mu = ((\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+{\\mathbf{y}}_S)^{\\mathsf{T}} {\\mathbf{1}} / ((\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ {\\mathbf{1}})^{\\mathsf{T}} {\\mathbf{1}}$,\n\nand compute the solution as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ ({\\mathbf{y}}_{\\mathcal{S}} - \\mu {\\mathbf{1}})$.\nFurthermore, it can be shown that if we center the vector of labels\n${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}} = {\\mathbf{y}}_{\\mathcal{S}} - {\\overline{{{\\mathbf{y}}}}}_{\\mathcal{S}}$, with\n${\\overline{{{\\mathbf{y}}}}} = \\frac{1}{l}{\\mathbf{y}}_{\\mathcal{S}}^{\\mathsf{T}} {\\mathbf{1}}$, then the solution of {\\textsc{Stable-HFS}\\xspace}\ncan be rewritten as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ ({\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}} - \\mu {\\mathbf{1}}) = \\big(P_{\\mathcal{F}} (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})\\big)^+ {\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$,\nwhere $P_{\\mathcal{F}} = L_{\\mathcal{G}} L_{\\mathcal{G}}^+$ is the projection matrix on the $n-1$\ndimensional space ${\\mathcal{F}}$. Indeed, since the Laplacian of any graph ${\\mathcal{G}}$\nhas a null space equal to the one vector ${\\mathbf{1}}$, then $P_{\\mathcal{F}}$ is\ninvariant w.r.t.\\@ the specific graph ${\\mathcal{G}}$ used to defined it. While {\\textsc{Stable-HFS}\\xspace}\nis more stable and thus more suited for theoretical analysis, its\ntime and space requirements remain ${\\mathcal{O}}(mn)$ and ${\\mathcal{O}}(m)$,\nand cannot be applied to graph with a large number of edges.\n\n\n\n\\vspace{-0.05in}\n\\section{Spectral Sparsification for Graph-Based SSL}\n\\vspace{-0.05in}\n\n\\begin{figure}\n\\vspace{-0.2in}\n\\begin{minipage}[t]{1.0\\linewidth}\n\\small \\par\\medskip\\noindent \\framebox[\\columnwidth]{ \\begin{minipage}{0.98\\columnwidth} {\\par\\smallskip{ \\begin{algorithmic} \\begin{small}     \\INPUT Graph ${\\mathcal{G}} = ({\\mathcal{X}}, {\\mathcal{E}})$, labels ${\\mathbf{y}}_{\\mathcal{S}}$, accuracy ${\\varepsilon}$     \\OUTPUT Solution ${\\widehat{{{\\mathbf{f}}}}}$, sparsified graph ${\\mathcal{H}}$     \\STATE Let $\\alpha = 1/(1-{\\varepsilon})$ and $N = \\alpha^2 n \\log^2(n)/{\\varepsilon}^2$     \\STATE Partition ${\\mathcal{E}}$ in $\\tau = \\lceil m / N\\rceil$ blocks $\\Delta_1,\\ldots,\\Delta_\\tau$     \\STATE Initialize ${\\mathcal{H}} = \\emptyset$     \\FOR{$t=1,\\ldots,\\tau$}         \\STATE Load $\\Delta_t$ in memory         \\STATE Compute ${\\mathcal{H}}_t = \\textsc{sparsify}({\\mathcal{H}}_{t-1}, \\Delta_t, N, \\alpha)$     \\ENDFOR     \\STATE Center the labels ${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$     \\STATE Compute ${\\widetilde{{{\\mathbf{f}}}}}$ with {\\textsc{Stable-HFS}\\xspace} with ${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$ using a suitable SDD solver \\end{small} \\end{algorithmic} }\\par\\smallskip} \\end{minipage} } \\par\\medskip \n\\vspace{-0.1in}\n\\caption{\\small{\\textsc{Sparse-HFS}\\xspace}\\vspace{-0.1in}}\\label{alg:sparse_ssl}\n\\end{minipage}\n\\end{figure}\n\n\n\n\nIn this section we introduce a novel variant of {\\textsc{HFS}\\xspace}, called \\textit{{\\textsc{Sparse-HFS}\\xspace}}, where spectral graph sparsification techniques are integrated into {\\textsc{Stable-HFS}\\xspace}, drastically reducing the time and memory requirements without compromising the resulting accuracy.\n\n\\textbf{Spectral sparsification.} \nA graph sparsifier receives as input a graph\n${\\mathcal{G}}$ and it returns a graph ${\\mathcal{H}}$ on the same set of nodes ${\\mathcal{X}}$ but with much fewer edges. Among different techniques~\\cite{Batson:2013:SSG:2492007.2492029},\nspectral sparsification methods provide the stronger guarantees on the accuracy of the resulting graph.\n\n\\begin{definition}\\label{def:eps-sparsifier}\nA $ 1 \\pm {\\varepsilon}$ spectral sparsifier of ${\\mathcal{G}}$ is a graph ${\\mathcal{H}} \\subseteq {\\mathcal{G}}$ such that for all ${\\mathbf{x}}\\in{\\mathbb{R}}^n$\n\n\n", "itemtype": "equation", "pos": 13397, "prevtext": "\n\\maketitle\n\n\n\\begin{abstract}\nWhile the \\textit{harmonic function solution} performs well in many semi-supervised learning (SSL) tasks, it is known to scale poorly with the number of samples.\nRecent successful and scalable methods, such as the eigenfunction method~\\cite{fergus2009semi-supervised} focus on efficiently approximating the whole spectrum of the graph Laplacian constructed from the data. This is in contrast to various subsampling and quantization methods proposed in the past, which may fail in preserving the graph spectra. However, the impact of the approximation of the spectrum on the final generalization error is either unknown~\\cite{fergus2009semi-supervised}, or requires strong assumptions on the data~\\cite{yang2012simple}.  In this paper, we introduce {\\textsc{Sparse-HFS}\\xspace}, an efficient edge-sparsification algorithm for SSL. By constructing an edge-sparse and spectrally similar graph, we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of {\\textsc{Sparse-HFS}\\xspace}. As a result, we obtain a theoretically-grounded approximation scheme for graph-based SSL that also empirically matches the performance of known large-scale methods.\n\\end{abstract}\n\n\n\\vspace{-0.05in}\n\\section{Introduction}\n\\vspace{-0.05in}\nIn many classification and regression tasks, obtaining labels for large\ndatasets is expensive. When the number of labeled samples is too small,\ntraditional supervised learning algorithms fail in learning accurate\npredictors. \\emph{Semi-supervised learning} (SSL)\n\\cite{chapelle2010semi-supervised,zhu2008semi-supervised} effectively\ndeals with this problem by integrating the labeled samples with\nan additional set of unlabeled samples, which are\nabundant and readily available in many applications (e.g., set of images collected on web\nsites~\\cite{fergus2009semi-supervised}). The intuition behind SSL is that\nunlabeled data may reveal the underlying structure of the problem (e.g., a\nmanifold) that could be exploited to compensate for the limited number of\nlabels and improve the prediction accuracy.\nAmong different SSL settings, in this paper we focus on the case where data\nare embedded in a \\emph{graph}.\n\nThe graph is expected to effectively represent the geometry of data and\ngraph-based SSL~\\cite{zhu2003semi-supervised,belkin2006manifold,subramanya2014graph} methods\nleverage the intuition that nodes that are similar according to the graph are\nmore likely to be labeled similarly.\n\nA popular approach is the \\emph{harmonic function solution} ({\\textsc{HFS}\\xspace})\n\\cite{zhu2003semi-supervised,belkin2004regularization,fergus2009semi-supervised},\nwhose objective is to find a solution where each node's value is the weighted\naverage of its neighbors. Computing the {\\textsc{HFS}\\xspace} solution requires solving a\nLaplacian regularized least-squares problem.\nWhile the resulting solution is\nboth empirically effective~\\cite{fergus2009semi-supervised} and enjoys strong\nperformance guarantees~\\cite{belkin2006manifold,cortes2008stability},\nsolving \\emph{exactly} the least-squares problem on a graph with $n$ nodes\namounts to ${\\mathcal{O}}(n^3)$ time and ${\\mathcal{O}}(n^2)$ space complexity. Using a general iterative\nsolver on a graph with $m$ edges to obtain a comparable solution requires\nonly ${\\mathcal{O}}(mn)$ time and ${\\mathcal{O}}(m)$ space,\nbut this is still practically unfeasible in many applications of\ninterest. In fact, many graphs have naturally a large number of edges, so that\neven if the $n$ nodes could fit in memory, storing $m$ edges largely exceeds\nthe memory capacity. For instance, Facebook's graph of\nrelationships~\\cite{ching2015one} counts about $n=1.39e9$ users connected by a\ntrillion ($m=1e12$) edges. While $n$ is still\nin the order of the memory capacity, the edges cannot be stored in a single\ncomputer.\nA similar issue is faced when\nthe graph is built starting from a dataset, for instance using\na $k$-nn graph. In this case $m=kn$ edges are created, and sometimes a large\n$k$ is necessary to obtain good performance~\\cite{saluja2014graph},\nor artificially adding neighbours can improve the stability of the\nmethod \\cite{Gleich2015robustifying}.\nIn such problems, a direct application of {\\textsc{HFS}\\xspace} is not possible and thus some\nform of approximation or graph sketching is required. A straightforward\napproach is to distribute the graph over multiple machines in a cluster and\nresort to an iterative solver for the solution of the least-squares\nproblem~\\cite{ching2015one}. Distributed algorithms require infrastructure and\ncareful engineering in order to deal with communication issues\n\\cite{cai2014comparison}. But even assuming that these problems are\nsatisfactorily dealt with, all known iterative solvers that\nhave provably fast convergence\ndo not have known distributed implementations, as they assume\nrandom, constant time access to all the edges in the graph. Thus one would have to\nresort to distributed implementations of simpler but much slower\nmethods, in effect trading-off space for a significant reduction in overall\nefficiency.\n\n\nMore principled\nmethods try to address the memory bottleneck by directly manipulating the\nstructure of the graph to reduce its size.\nThese include \\emph{subsampling} the\nnodes of the original graph, \\emph{quantization}, approaches related to\n\\emph{manifold learning}, and various \\emph{approximation} strategies.\n\nThe most straightforward way to reduce the complexity in graph-based method is\nto subsample the nodes to create a smaller, \\emph{backbone graph} of\nrepresentative vertices, or \\emph{landmarks}~\\cite{talwalkar2008large-scale}.\nNystr\\\"{o}m sampling methods~\\cite{kumar2012sampling} randomly select $s$\nnodes from the original graph and compute $q$ eigenvectors of the smaller\ngraph, which can be later used to solve the HFS regularized problem. It can be\nshown~\\cite{kumar2012sampling} that the reconstructed Laplacian is accurate in\n$\\ell_2$-norm and thus only its largest eigenvalue is preserved.\nUnfortunately, the {\\textsc{HFS}\\xspace} solution does not depend only on the largest\neigenvalues, both because the largest eigenvectors are the ones most penalized\nby {\\textsc{HFS}\\xspace}'s regularizer (\\cite{belkin2006manifold}) and because theoretical\nanalysis shows that preserving the smallest eigenvalue is important for generalization\nbounds~(\\cite{belkin2004regularization}).\n\nAs a result, subsampling methods can completely fail when the sampled nodes\ncompromise the spectral structure of the\ngraph~\\cite{fergus2009semi-supervised}. Although alternative techniques have\nbeen developed over years (see e.g.,\n\\cite{jebara2009graph,yu2005blockwise,zhu2005harmonic,garcke2005semi-supervised,tsang2006large-scale,liu2010large}),\nthis drawback is common to all backbone graph methods.\n\nMotivated by this observation, other approaches focus on computing a more\naccurate approximation of the spectrum of the Laplacian. Fergus \\textit{et\n    al.}~\\cite{fergus2009semi-supervised} build on the observation that when\nthe number of unlabeled samples$n$ tends to infinity, the eigenvectors of the\nLaplacian tend to the eigenfunctions of the sampling distribution ${\\mathcal{P}}$.\nThus instead of approximating eigenvectors in ${\\mathbb{R}}^n$, they first compute\nempirical\neigenfunctions of the estimated sampling distribution (defined on the $d$-dimensional\nfeature space) obtained by assuming that~${\\mathcal{P}}$ is factorized\nand by using a histogram estimation over $b$ bins over each dimension\nseparately. While the method scales to the order of million nodes, it still\nrequires $d$ and $b$ to be small to be efficient. Furthermore, no theoretical analysis is\navailable, and the method  may return poor approximations whenever the\nsampling distribution is not factorized.\nMotivated by the empirical success of~\\cite{fergus2009semi-supervised},\nJi \\textit{et al.}~\\cite{yang2012simple} proposed a similar algorithm,\n{\\textsc{Simple-HFS}\\xspace}, for which they provide theoretical guarantees.\nHowever, in order to prove bounds on the generalization error, they need to\nassume several strong and hard to verify assumptions, such as a sufficiently\nlarge eigengap. On the contrary, the guarantees for our method work for any graph.\n\n\\paragraph{Our contribution}\nIn this paper, we focus on reducing the space complexity of graph-based SSL while matching the smallest possible computational complexity of $\\Omega(m)$ up to logarithmic factors\\footnote{While the computational complexity of exact {\\textsc{HFS}\\xspace} is ${\\mathcal{O}}(mn)$, many approximated methods can significantly reduce it. Nonetheless, any method that requires reading all the edges once has at least $\\Omega(m)$ time complexity.} and providing strong guarantees about the quality of the solution.\nIn particular, we introduce a novel approach which employs efficient\nspectral graph sparsification techniques~\\cite{kelner_spectral_2013} to\nincrementally process the original graph. This method, coupled with dedicated\nsolvers for symmetric diagonally dominant (SDD)\nsystems~\\cite{koutis2011a-nearly-m}, allows to find an approximate {\\textsc{HFS}\\xspace}\nsolution without storing the whole graph in memory and to control the\ncomputational complexity as $n$ grows. In fact, we show that\nour proposed method, called {\\textsc{Sparse-HFS}\\xspace}, requires only fixed ${\\mathcal{O}}(n \\log^2(n))$\nspace to run, and allows to compute solutions to large {\\textsc{HFS}\\xspace} problems in memory.\nFor example, in the experimental section we show that the sparsifier can\nachieve an accuracy comparable to the full graph, using one order\nof magnitude less edges. With a careful choice of the frequency of\nresparsification, the proposed method does not\nincrease significantly the running time. Given a minimum amortized cost of\n$\\Omega(1)$ per edge, necessary to examine each edge at least once, our algorithm only increases\nthis cost to ${\\mathcal{O}}(\\log^3(n))$.\nFurthermore, using the approximation properties of spectral sparsifiers and\nresults from algorithmic stability theory~\\cite{bousquet_stability_2002,\n    cortes2008stability} we provide theoretical guarantees for the\ngeneralization error for {\\textsc{Sparse-HFS}\\xspace}, showing that the performance is\nasymptotically the same as the exact solution of {\\textsc{HFS}\\xspace}. Finally, we report\nempirical results on both synthetic and real data showing that {\\textsc{Sparse-HFS}\\xspace} is\ncompetitive with subsampling and the {\\textsc{EigFun}\\xspace} method\nin~\\cite{fergus2009semi-supervised}.\n\n\n\n\\vspace{-0.05in}\n\\section{Graph-Based Semi-Supervised Learning}\n\\vspace{-0.05in}\n\n\\textbf{Notation.}\nWe denote with lowercase letter $a$ a scalar, with bold lowercase letter ${\\mathbf{a}}$\na vector and with uppercase letter $A$ a matrix.\n\nWe consider the problem of regression\nin the semi-supervised setting, where a large set of $n$ points\n${\\mathcal{X}} = ({\\mathbf{x}}_1,\\ldots,{\\mathbf{x}}_n) \\subset {\\mathbb{R}}^d$ is drawn from a distribution ${\\mathcal{P}}$\nand labels $\\{y_i\\}_{i=1}^l$ are provided only for a small (random) subset\n${\\mathcal{S}} \\subset {\\mathcal{X}}$ of $l$ points. Graph-based SSL builds on the observation\nthat ${\\mathcal{P}}$ is often far from being uniform and it may display a specific\nstructure that could be exploited to ``propagate'' the labels to similar\nunlabeled points. Building on this intuition, graph-based SSL algorithms\nconsider the case when the points in ${\\mathcal{X}}$ are embedded into an undirected\nweighted graph ${\\mathcal{G}} = ({\\mathcal{X}},{\\mathcal{E}})$ with $|{\\mathcal{E}}| = m$ edges. Associated\nwith each edge $e_{i,j}\\in {\\mathcal{E}}$ there is a weight~$a_{e_{i,j}}$ measuring\nthe ``distance'' between ${\\mathbf{x}}_i$ and${\\mathbf{x}}_j$\\footnote{Notice that ${\\mathcal{G}}$ can be either constructed from the data (e.g.,\n    building a $k$-nn graph using the exponential distance\n    $a_{e_{i,j}} = \\exp(- ||x_i - x_j||_2 / \\sigma^2)$) or it can be provided\n    directly as input (e.g., in social networks).}.\n\nA graph-based SSL algorithm receives as input ${\\mathcal{G}}$ and the labels of the\nnodes in ${\\mathcal{S}}$ and it returns a function ${\\mathbf{f}} : {\\mathcal{X}} \\rightarrow {\\mathbb{R}}$ that\npredicts the label for all nodes in~${\\mathcal{X}}$. The objective is to minimize the\nprediction error over the set~$\\mathcal{T}$ of $u = n-l$ unlabeled nodes. In\nthe following we denote by ${\\mathbf{y}}\\in{\\mathbb{R}}^n$ the full vector of labels.\n\n\n\\textbf{{\\textsc{Stable-HFS}\\xspace}.}\n{\\textsc{HFS}\\xspace} directly exploits the structure embedded in ${\\mathcal{G}}$ to learn functions that\nare smooth over the graph, thus predicting similar labels for similar nodes.\nGiven the weighted adjacency matrix $A_{{\\mathcal{G}}}$ and the degree matrix $D_{{\\mathcal{G}}}$,\nthe Laplacian of ${\\mathcal{G}}$ is defined as $L_{\\mathcal{G}} = D_{\\mathcal{G}} - A_{\\mathcal{G}}$. The Laplacian\n$L_{{\\mathcal{G}}}$ is semi-definite positive (SDP) with ${\\operatorname{Ker}}(L_{\\mathcal{G}})={\\mathbf{1}}$.\nFurthermore, we assume that ${\\mathcal{G}}$ is connected and thus has only one\neigenvalue at $0$. Let $L_{{\\mathcal{G}}}^+$ be the pseudoinverse of $L_{{\\mathcal{G}}}$, and\n$L_{{\\mathcal{G}}}^{-1/2} = (L_{{\\mathcal{G}}}^+)^{1/2}$. The {\\textsc{HFS}\\xspace}\nmethod~\\cite{zhu2003semi-supervised} can be formulated as\nthe Laplacian-regularized least-squares problem\n\n\n", "index": 1, "text": "\\begin{align}\\label{eq:hfs.original}\n    {\\widehat{{{\\mathbf{f}}}}} &= {\\operatorname{arg\\,min}}_{{\\mathbf{f}} \\in {\\mathbb{R}}^n}  \\tfrac{1}{l} ({\\mathbf{f}} - {\\mathbf{y}})^{\\mathsf{T}} I_{{\\mathcal{S}}} ({\\mathbf{f}}-{\\mathbf{y}}) + \\gamma {\\mathbf{f}}^{\\mathsf{T}} L_{\\mathcal{G}} {\\mathbf{f}},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\widehat{{{\\mathbf{f}}}}}\" display=\"inline\"><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{arg\\,min}}_{{\\mathbf{f}}\\in{\\mathbb{R}}^{n}}%&#10;\\tfrac{1}{l}({\\mathbf{f}}-{\\mathbf{y}})^{\\mathsf{T}}I_{{\\mathcal{S}}}({\\mathbf%&#10;{f}}-{\\mathbf{y}})+\\gamma{\\mathbf{f}}^{\\mathsf{T}}L_{\\mathcal{G}}{\\mathbf{f}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>n</mi></msup></mrow></msub><mo>\u2061</mo><mfrac><mn>1</mn><mi>l</mi></mfrac></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1f</mi><mo>-</mo><mi>\ud835\udc32</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\ud835\uddb3</mi></msup><mo>\u2062</mo><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1f</mi><mo>-</mo><mi>\ud835\udc32</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mi>\ud835\udc1f</mi><mi>\ud835\uddb3</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></msub><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\n\\end{definition}\n\nThe key idea~\\cite{spielman2011graph} is that to construct a sparse graph ${\\mathcal{H}}$, it is sufficient to randomly select $m' = {\\mathcal{O}}(n \\log(n)/{\\varepsilon}^2)$ edges from ${\\mathcal{G}}$ with a probability proportional to their effective resistance and add them to the new graph with suitable weights.\nWhile storing the sparsified graph requires only ${\\mathcal{O}}(m')$ space, it still suffers from major limitations: \\textit{(i)} the naive computation of the effective resistances needs ${\\mathcal{O}}(m n \\log(n))$ time\\footnote{A completely naive method would solve $n$ linear problems, each costing ${\\mathcal{O}}(mn)$. Using random projections we can solve only $\\log(n)$ problems with only a small constant multiplicative error \\cite{koutis2012improved}.}, \\textit{(ii)} it requires ${\\mathcal{O}}(m)$ space to store the initial graph ${\\mathcal{G}}$, and \\textit{(iii)} computing the {\\textsc{HFS}\\xspace} solution on ${\\mathcal{H}}$ in a naive way still has a cost of ${\\mathcal{O}}(m'n)$. For this reason, we employ more sophisticated solutions and integrate the recent spectral sparsification technique for the semi-streaming setting in~\\cite{kelner_spectral_2013} and the efficient solver for SDD systems in~\\cite{koutis2011a-nearly-m}. The resulting algorithm is illustrated in Fig.~\\ref{alg:sparse_ssl}.\n\n\\begin{figure}\n\\small \\par\\medskip\\noindent \\framebox[\\columnwidth]{ \\begin{minipage}{0.98\\columnwidth} {\\par\\smallskip{ \\begin{algorithmic} \\begin{small} \\INPUT A sparsifier ${\\mathcal{H}}$, block $\\Delta$, number of edges $N$, effective resistance accuracy $\\alpha$     \\OUTPUT A sparsifier ${\\mathcal{H}}'$,  probabilities $\\{{\\widetilde{{p}}}'_e: e \\in {\\mathcal{H}}'\\}$.     \\STATE Compute estimates of ${\\widetilde{{R}}}'_e$ for any edge in ${\\mathcal{H}}+\\Delta$ such that $1/\\alpha \\leq {\\widetilde{{R}}}'_e/R'_e \\leq \\alpha  $     with an SDD solver \\cite{koutis2011a-nearly-m}     \\STATE Compute probabilities ${\\widetilde{{p}}}'_e = (a_e {\\widetilde{{R}}}'_e)/(\\alpha(n-1))$ and weights $w_e = a_e/(N{\\widetilde{{p}}}'_e)$     \\STATE For all edges $e\\in{\\mathcal{H}}$ compute ${\\widetilde{{p}}}'_e \\leftarrow \\min\\{{\\widetilde{{p}}}_e,{\\widetilde{{p}}}'_e\\}$ and initialize ${\\mathcal{H}}' = \\emptyset$     \\FOR{all edges $e \\in {\\mathcal{H}}$}     \\STATE Add edge $e$ to ${\\mathcal{H}}'$ with weight $w_e$ with prob.~${\\widetilde{{p}}}'_e/{\\widetilde{{p}}}_e$     \\ENDFOR     \\FOR{all edges $e \\in \\Delta$}         \\FOR{$ i = 1 $ to $N$}         \\STATE Add edge $e$ to ${\\mathcal{H}}'$ with weight $w_e$ with prob.~${\\widetilde{{p}}}'_e$         \\ENDFOR     \\ENDFOR     \\end{small} \\end{algorithmic} }\\par\\smallskip} \\end{minipage} } \\par\\medskip \n\\vspace{-0.1in}\n\\caption{\\small Kelner-Levin Sparsification algorithm~\\cite{kelner_spectral_2013}\\vspace{-0.1in}}\\label{alg:kl_resparsify}\n\\end{figure}\n\nWe first introduce additional notation. Given two graphs ${\\mathcal{G}}$ and ${\\mathcal{G}}'$ over the same set of nodes ${\\mathcal{X}}$, we denote by ${\\mathcal{G}}+{\\mathcal{G}}'$ the graph obtained by summing the weights on the edges of ${\\mathcal{G}}'$ to ${\\mathcal{G}}$. \nFor any node $i=1,\\ldots,n$, we denote with $\\chi_i\\in{\\mathbb{R}}^n$ the indicator vector so that $\\chi_i - \\chi_j$ is the ``edge'' vector. The effective resistance of an edge $e_{i,j}$ in a graph ${\\mathcal{G}}$ is equal to $R_{e_{i,j}} = (\\chi_i - \\chi_j)^{\\mathsf{T}} L_{\\mathcal{G}}^+ (\\chi_i - \\chi_j)$. The key intuition behind our {\\textsc{Sparse-HFS}\\xspace} is that processing  the graph incrementally allows to dramatically reduce the memory requirements and keep low time complexity at the same time. Let ${\\varepsilon}$ be the (spectral) accuracy desired for the final sparsified graph ${\\mathcal{H}}$, {\\textsc{Sparse-HFS}\\xspace} first partitions the set of edges ${\\mathcal{E}}$ of the original graph ${\\mathcal{G}}$ into $\\tau$ blocks $(\\Delta_1,\\ldots,\\Delta_\\tau)$ of size $N = \\alpha^2 n\\log^2(n) / {\\varepsilon}^2$, with $\\alpha = 1/(1-{\\varepsilon})$. While the original graph with $m$ edges is too large to fit in memory, each of these blocks has a number of edges which is nearly linear in the number of nodes and can be easily managed. {\\textsc{Sparse-HFS}\\xspace} processes blocks over iterations. Starting with an empty graph ${\\mathcal{H}}_0$, at each iteration $t$ a new block~$\\Delta_t$ is loaded and the intermediate sparsifier~${\\mathcal{H}}_{t-1}$ is updated using the routine \\textsc{sparsify}, which is guaranteed to return a $(1\\pm{\\varepsilon})$-sparsifier of size $N$ for the graph~${\\mathcal{H}}_{t-1}+\\Delta_t$. After all the blocks are processed, a sparsifier ${\\mathcal{H}}$ is returned and the {\\textsc{Stable-HFS}\\xspace} solution can be computed. Since~${\\mathcal{H}}$ is very sparse (i.e., it only contains $N = {\\mathcal{O}}(n\\log^2(n))$ edges), it is now possible to use efficient solvers for linear sparse systems and drastically reduce the computational complexity of solving {\\textsc{Stable-HFS}\\xspace} from ${\\mathcal{O}}(Nn)$ down to ${\\mathcal{O}}(N\\log(n))$. The routine \\textsc{sparsify} can be implemented using different spectral sparsification techniques developed for the streaming setting, here we rely on the method proposed in~\\cite{kelner_spectral_2013}. The effective resistance is computed for all edges in the current sparsifier and the new block using random projections and an efficient solver for SDD systems \\cite{koutis2011a-nearly-m}. This step takes ${\\mathcal{O}}(N\\log n)$ time and it returns $\\alpha$-accurate estimates ${\\widetilde{{R}}}'_e$ of the effective resistance for the $2N$ nodes in ${\\mathcal{H}}_{t-1}$ and $\\Delta_t$. If $\\alpha = 1/(1-{\\varepsilon})$ and the input graph ${\\mathcal{H}}_{t-1}$ is a $(1\\pm{\\varepsilon})$-sparsifier, then sampling $N$ edges proportionally to ${\\widetilde{{R}}}'_e$ is guaranteed to generate a $(1\\pm{\\varepsilon})$-sparsifier for the full graph ${\\mathcal{G}}_t = \\sum_{s=1}^t \\Delta_s$ up to iteration $t$. More details on this process are provided in Fig.~\\ref{alg:kl_resparsify} and in~\\cite{kelner_spectral_2013}. The resulting process has a space complexity ${\\mathcal{O}}(N)$ and a time complexity that never exceeds ${\\mathcal{O}}(N\\log(n))$ in sparsifying each block and computing the final solution (see next section for more precise statements on time and space complexity). \n\n\n\n\n\n\\vspace{-0.05in}\n\\section{Theoretical Analysis}\n\\vspace{-0.05in}\n\nWe first report the time and space complexity of {\\textsc{Sparse-HFS}\\xspace}. This result follows from the properties of the sparsifier in~\\cite{kelner_spectral_2013} and the SDD solver in~\\cite{koutis2011a-nearly-m} and thus we do not report its proof.\n\n\\begin{lemma}\\label{lem:sparse.complexity}\n    Let ${\\varepsilon}>0$ be the desired accuracy and $\\delta>0$ the probability of failure. For any connected graph ${\\mathcal{G}}=({\\mathcal{X}},{\\mathcal{E}})$ with $n$ nodes, $m$ edges, eigenvalues $0 = \\lambda_1({\\mathcal{G}}) < \\lambda_2({\\mathcal{G}}) \\leq \\ldots \\leq \\lambda_n({\\mathcal{G}})$, and any partitioning of~${\\mathcal{E}}$ into $\\tau$ blocks, {\\textsc{Sparse-HFS}\\xspace} returns a graph ${\\mathcal{H}}$ such that for any $i=1,\\ldots,n$\n\n\n", "itemtype": "equation", "pos": 18889, "prevtext": "\n\nwhere $I_{{\\mathcal{S}}}\\in{\\mathbb{R}}^{n\\times n}$ is the identity matrix with zeros\ncorresponding to the nodes not in ${\\mathcal{S}}$ and $\\gamma$ is a\nregularization parameter. The solution can be computed in closed form as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ {\\mathbf{y}}_{\\mathcal{S}}$, where\n${\\mathbf{y}}_{\\mathcal{S}} = I_{\\mathcal{S}} {\\mathbf{y}} \\in {\\mathbb{R}}^n$.\nThe singularity of the Laplacian may lead to unstable behavior with\ndrastically different results for small perturbations to the dataset. For this\nreason, we focus on the {\\textsc{Stable-HFS}\\xspace} algorithm proposed\nin~\\cite{belkin2004regularization} where an additional regularization term is\nintroduced to restrict the space of admissible hypotheses to the space\n${\\mathcal{F}} = \\left\\{{\\mathbf{f}} : \\langle{\\mathbf{f}},{\\mathbf{1}}\\rangle = 0\\right\\}$ of functions\northogonal to null space of $L_{\\mathcal{G}}$ (i.e., centered functions). This\nrestriction can be easily enforced by introducing an additional regularization\nterm $\\frac{\\mu}{l} {\\mathbf{f}}^\\top {\\mathbf{1}}$ in Eq.~\\ref{eq:hfs.original}. As shown\nin~\\cite{belkin2004regularization}, in order to guarantee that the resulting\n${\\widehat{{{\\mathbf{f}}}}}$ actually belongs to ${\\mathcal{F}}$, it is sufficient to set the\nregularization parameter to\n$\\mu = ((\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+{\\mathbf{y}}_S)^{\\mathsf{T}} {\\mathbf{1}} / ((\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ {\\mathbf{1}})^{\\mathsf{T}} {\\mathbf{1}}$,\n\nand compute the solution as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ ({\\mathbf{y}}_{\\mathcal{S}} - \\mu {\\mathbf{1}})$.\nFurthermore, it can be shown that if we center the vector of labels\n${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}} = {\\mathbf{y}}_{\\mathcal{S}} - {\\overline{{{\\mathbf{y}}}}}_{\\mathcal{S}}$, with\n${\\overline{{{\\mathbf{y}}}}} = \\frac{1}{l}{\\mathbf{y}}_{\\mathcal{S}}^{\\mathsf{T}} {\\mathbf{1}}$, then the solution of {\\textsc{Stable-HFS}\\xspace}\ncan be rewritten as\n${\\widehat{{{\\mathbf{f}}}}} = (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})^+ ({\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}} - \\mu {\\mathbf{1}}) = \\big(P_{\\mathcal{F}} (\\gamma l L_{\\mathcal{G}} + I_{\\mathcal{S}})\\big)^+ {\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$,\nwhere $P_{\\mathcal{F}} = L_{\\mathcal{G}} L_{\\mathcal{G}}^+$ is the projection matrix on the $n-1$\ndimensional space ${\\mathcal{F}}$. Indeed, since the Laplacian of any graph ${\\mathcal{G}}$\nhas a null space equal to the one vector ${\\mathbf{1}}$, then $P_{\\mathcal{F}}$ is\ninvariant w.r.t.\\@ the specific graph ${\\mathcal{G}}$ used to defined it. While {\\textsc{Stable-HFS}\\xspace}\nis more stable and thus more suited for theoretical analysis, its\ntime and space requirements remain ${\\mathcal{O}}(mn)$ and ${\\mathcal{O}}(m)$,\nand cannot be applied to graph with a large number of edges.\n\n\n\n\\vspace{-0.05in}\n\\section{Spectral Sparsification for Graph-Based SSL}\n\\vspace{-0.05in}\n\n\\begin{figure}\n\\vspace{-0.2in}\n\\begin{minipage}[t]{1.0\\linewidth}\n\\small \\par\\medskip\\noindent \\framebox[\\columnwidth]{ \\begin{minipage}{0.98\\columnwidth} {\\par\\smallskip{ \\begin{algorithmic} \\begin{small}     \\INPUT Graph ${\\mathcal{G}} = ({\\mathcal{X}}, {\\mathcal{E}})$, labels ${\\mathbf{y}}_{\\mathcal{S}}$, accuracy ${\\varepsilon}$     \\OUTPUT Solution ${\\widehat{{{\\mathbf{f}}}}}$, sparsified graph ${\\mathcal{H}}$     \\STATE Let $\\alpha = 1/(1-{\\varepsilon})$ and $N = \\alpha^2 n \\log^2(n)/{\\varepsilon}^2$     \\STATE Partition ${\\mathcal{E}}$ in $\\tau = \\lceil m / N\\rceil$ blocks $\\Delta_1,\\ldots,\\Delta_\\tau$     \\STATE Initialize ${\\mathcal{H}} = \\emptyset$     \\FOR{$t=1,\\ldots,\\tau$}         \\STATE Load $\\Delta_t$ in memory         \\STATE Compute ${\\mathcal{H}}_t = \\textsc{sparsify}({\\mathcal{H}}_{t-1}, \\Delta_t, N, \\alpha)$     \\ENDFOR     \\STATE Center the labels ${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$     \\STATE Compute ${\\widetilde{{{\\mathbf{f}}}}}$ with {\\textsc{Stable-HFS}\\xspace} with ${\\widetilde{{{\\mathbf{y}}}}}_{\\mathcal{S}}$ using a suitable SDD solver \\end{small} \\end{algorithmic} }\\par\\smallskip} \\end{minipage} } \\par\\medskip \n\\vspace{-0.1in}\n\\caption{\\small{\\textsc{Sparse-HFS}\\xspace}\\vspace{-0.1in}}\\label{alg:sparse_ssl}\n\\end{minipage}\n\\end{figure}\n\n\n\n\nIn this section we introduce a novel variant of {\\textsc{HFS}\\xspace}, called \\textit{{\\textsc{Sparse-HFS}\\xspace}}, where spectral graph sparsification techniques are integrated into {\\textsc{Stable-HFS}\\xspace}, drastically reducing the time and memory requirements without compromising the resulting accuracy.\n\n\\textbf{Spectral sparsification.} \nA graph sparsifier receives as input a graph\n${\\mathcal{G}}$ and it returns a graph ${\\mathcal{H}}$ on the same set of nodes ${\\mathcal{X}}$ but with much fewer edges. Among different techniques~\\cite{Batson:2013:SSG:2492007.2492029},\nspectral sparsification methods provide the stronger guarantees on the accuracy of the resulting graph.\n\n\\begin{definition}\\label{def:eps-sparsifier}\nA $ 1 \\pm {\\varepsilon}$ spectral sparsifier of ${\\mathcal{G}}$ is a graph ${\\mathcal{H}} \\subseteq {\\mathcal{G}}$ such that for all ${\\mathbf{x}}\\in{\\mathbb{R}}^n$\n\n\n", "index": 3, "text": "\\begin{align*}\n(1-{\\varepsilon}){\\mathbf{x}}^{\\mathsf{T}} L_{{\\mathcal{G}}} {\\mathbf{x}} \\leq {\\mathbf{x}}^{\\mathsf{T}} L_{\\mathcal{H}} {\\mathbf{x}} \\leq (1 + {\\varepsilon}) {\\mathbf{x}}^{\\mathsf{T}} L_{{\\mathcal{G}}} {\\mathbf{x}}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1-{\\varepsilon}){\\mathbf{x}}^{\\mathsf{T}}L_{{\\mathcal{G}}}{%&#10;\\mathbf{x}}\\leq{\\mathbf{x}}^{\\mathsf{T}}L_{\\mathcal{H}}{\\mathbf{x}}\\leq(1+{%&#10;\\varepsilon}){\\mathbf{x}}^{\\mathsf{T}}L_{{\\mathcal{G}}}{\\mathbf{x}}.\" display=\"inline\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc31</mi><mi>\ud835\uddb3</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></msub><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>\u2264</mo><mrow><msup><mi>\ud835\udc31</mi><mi>\ud835\uddb3</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></msub><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc31</mi><mi>\ud835\uddb3</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></msub><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwith prob.~$1-\\delta$ (w.r.t.\\@ the random estimation of the effective resistance and the sampling of edges in the \\textsc{sparsify} routine). Furthermore, let $N = \\alpha^2 n\\log^2(n)/{\\varepsilon}^2$ for $\\alpha = 1/(1-{\\varepsilon})$ and $\\tau = m / N$, then with prob.~$1-\\delta$ {\\textsc{Sparse-HFS}\\xspace} has an amortized time per edge of ${\\mathcal{O}}(\\log^3(n))$ and it requires ${\\mathcal{O}}(N)$ memory.\\footnote{In all these big-${\\mathcal{O}}$ expressions we hide multiplicative constants independent from the graph and terms $\\log(1/\\delta)$ which depends on the high-probability nature of the statements.}\n\\end{lemma}\n\n\nThe previous lemma shows the dramatic improvement of {\\textsc{Sparse-HFS}\\xspace} w.r.t.~{\\textsc{Stable-HFS}\\xspace} in terms of both time and space complexity. In fact, while solving {\\textsc{Stable-HFS}\\xspace} in a naive way can take up to ${\\mathcal{O}}(m)$ space and ${\\mathcal{O}}(mn)$ time, {\\textsc{Sparse-HFS}\\xspace} drops these requirements down to ${\\mathcal{O}}(n \\log^2(n)/{\\varepsilon}^2)$ space and ${\\mathcal{O}}(m \\log^3(n))$ time, which allows scaling {\\textsc{Stable-HFS}\\xspace} to graphs orders of magnitude bigger. These improvements have only a limited impact on the spectrum of~${\\mathcal{G}}$ and all its eigenvalues are approximated up to a $(1\\pm{\\varepsilon})$ factor. Moreover, all of the sparsification guarantees hold w.h.p.\\@ for any graph, regardless of how it is generated, its original spectra, and more importantly regardless of the exact order in which the edges are assigned to the blocks. Finally, we notice that the choice of the number of blocks as $m/N$ is crucial to guarantee a logarithmic amortized time, since each iteration takes ${\\mathcal{O}}(N\\log^3(n))$ time. As discussed in Sect.~\\ref{s:conclusions}, this property allows to directly apply {\\textsc{Sparse-HFS}\\xspace} in online learning settings where edges arrive in a stream and intermediate solutions have to be computed incrementally.\n\n\nIn the following, we show that, unlike other heuristics, the space complexity improvements obtained with sparsification come with guarantees and do not degrade the actual learning performance of {\\textsc{HFS}\\xspace}. The analysis of SSL algorithms is built around the algorithm stability theory~\\cite{bousquet_stability_2002}, which is extensively used to analyse transductive learning algorithms~\\cite{el-yaniv_stable_2006,cortes2008stability}. \nWe first remind the definition of algorithmic stability.\n\n\\begin{definition}\\label{def:beta-stability}\n    Let $\\mathcal{L}$ be a transductive learning algorithm. We denote by ${\\mathbf{f}}$ and ${\\mathbf{f}}'$ the functions obtained by running $\\mathcal{L}$ on datasets ${\\mathcal{X}} = ({\\mathcal{S}}, {\\mathcal{T}})$  and ${\\mathcal{X}} = ({\\mathcal{S}}', {\\mathcal{T}}')$ respectively. $\\mathcal{L}$ is uniformly $\\beta$-stable w.r.t. the squared loss if there exists $\\beta \\geq 0$ such that for any two partitions $({\\mathcal{S}}, {\\mathcal{T}})$ and $({\\mathcal{S}}', {\\mathcal{T}}')$ that differ by exactly one training (and test) point and for all ${\\mathbf{x}} \\in {\\mathcal{X}}$,\n\n\n", "itemtype": "equation", "pos": 26316, "prevtext": "\n\n\\end{definition}\n\nThe key idea~\\cite{spielman2011graph} is that to construct a sparse graph ${\\mathcal{H}}$, it is sufficient to randomly select $m' = {\\mathcal{O}}(n \\log(n)/{\\varepsilon}^2)$ edges from ${\\mathcal{G}}$ with a probability proportional to their effective resistance and add them to the new graph with suitable weights.\nWhile storing the sparsified graph requires only ${\\mathcal{O}}(m')$ space, it still suffers from major limitations: \\textit{(i)} the naive computation of the effective resistances needs ${\\mathcal{O}}(m n \\log(n))$ time\\footnote{A completely naive method would solve $n$ linear problems, each costing ${\\mathcal{O}}(mn)$. Using random projections we can solve only $\\log(n)$ problems with only a small constant multiplicative error \\cite{koutis2012improved}.}, \\textit{(ii)} it requires ${\\mathcal{O}}(m)$ space to store the initial graph ${\\mathcal{G}}$, and \\textit{(iii)} computing the {\\textsc{HFS}\\xspace} solution on ${\\mathcal{H}}$ in a naive way still has a cost of ${\\mathcal{O}}(m'n)$. For this reason, we employ more sophisticated solutions and integrate the recent spectral sparsification technique for the semi-streaming setting in~\\cite{kelner_spectral_2013} and the efficient solver for SDD systems in~\\cite{koutis2011a-nearly-m}. The resulting algorithm is illustrated in Fig.~\\ref{alg:sparse_ssl}.\n\n\\begin{figure}\n\\small \\par\\medskip\\noindent \\framebox[\\columnwidth]{ \\begin{minipage}{0.98\\columnwidth} {\\par\\smallskip{ \\begin{algorithmic} \\begin{small} \\INPUT A sparsifier ${\\mathcal{H}}$, block $\\Delta$, number of edges $N$, effective resistance accuracy $\\alpha$     \\OUTPUT A sparsifier ${\\mathcal{H}}'$,  probabilities $\\{{\\widetilde{{p}}}'_e: e \\in {\\mathcal{H}}'\\}$.     \\STATE Compute estimates of ${\\widetilde{{R}}}'_e$ for any edge in ${\\mathcal{H}}+\\Delta$ such that $1/\\alpha \\leq {\\widetilde{{R}}}'_e/R'_e \\leq \\alpha  $     with an SDD solver \\cite{koutis2011a-nearly-m}     \\STATE Compute probabilities ${\\widetilde{{p}}}'_e = (a_e {\\widetilde{{R}}}'_e)/(\\alpha(n-1))$ and weights $w_e = a_e/(N{\\widetilde{{p}}}'_e)$     \\STATE For all edges $e\\in{\\mathcal{H}}$ compute ${\\widetilde{{p}}}'_e \\leftarrow \\min\\{{\\widetilde{{p}}}_e,{\\widetilde{{p}}}'_e\\}$ and initialize ${\\mathcal{H}}' = \\emptyset$     \\FOR{all edges $e \\in {\\mathcal{H}}$}     \\STATE Add edge $e$ to ${\\mathcal{H}}'$ with weight $w_e$ with prob.~${\\widetilde{{p}}}'_e/{\\widetilde{{p}}}_e$     \\ENDFOR     \\FOR{all edges $e \\in \\Delta$}         \\FOR{$ i = 1 $ to $N$}         \\STATE Add edge $e$ to ${\\mathcal{H}}'$ with weight $w_e$ with prob.~${\\widetilde{{p}}}'_e$         \\ENDFOR     \\ENDFOR     \\end{small} \\end{algorithmic} }\\par\\smallskip} \\end{minipage} } \\par\\medskip \n\\vspace{-0.1in}\n\\caption{\\small Kelner-Levin Sparsification algorithm~\\cite{kelner_spectral_2013}\\vspace{-0.1in}}\\label{alg:kl_resparsify}\n\\end{figure}\n\nWe first introduce additional notation. Given two graphs ${\\mathcal{G}}$ and ${\\mathcal{G}}'$ over the same set of nodes ${\\mathcal{X}}$, we denote by ${\\mathcal{G}}+{\\mathcal{G}}'$ the graph obtained by summing the weights on the edges of ${\\mathcal{G}}'$ to ${\\mathcal{G}}$. \nFor any node $i=1,\\ldots,n$, we denote with $\\chi_i\\in{\\mathbb{R}}^n$ the indicator vector so that $\\chi_i - \\chi_j$ is the ``edge'' vector. The effective resistance of an edge $e_{i,j}$ in a graph ${\\mathcal{G}}$ is equal to $R_{e_{i,j}} = (\\chi_i - \\chi_j)^{\\mathsf{T}} L_{\\mathcal{G}}^+ (\\chi_i - \\chi_j)$. The key intuition behind our {\\textsc{Sparse-HFS}\\xspace} is that processing  the graph incrementally allows to dramatically reduce the memory requirements and keep low time complexity at the same time. Let ${\\varepsilon}$ be the (spectral) accuracy desired for the final sparsified graph ${\\mathcal{H}}$, {\\textsc{Sparse-HFS}\\xspace} first partitions the set of edges ${\\mathcal{E}}$ of the original graph ${\\mathcal{G}}$ into $\\tau$ blocks $(\\Delta_1,\\ldots,\\Delta_\\tau)$ of size $N = \\alpha^2 n\\log^2(n) / {\\varepsilon}^2$, with $\\alpha = 1/(1-{\\varepsilon})$. While the original graph with $m$ edges is too large to fit in memory, each of these blocks has a number of edges which is nearly linear in the number of nodes and can be easily managed. {\\textsc{Sparse-HFS}\\xspace} processes blocks over iterations. Starting with an empty graph ${\\mathcal{H}}_0$, at each iteration $t$ a new block~$\\Delta_t$ is loaded and the intermediate sparsifier~${\\mathcal{H}}_{t-1}$ is updated using the routine \\textsc{sparsify}, which is guaranteed to return a $(1\\pm{\\varepsilon})$-sparsifier of size $N$ for the graph~${\\mathcal{H}}_{t-1}+\\Delta_t$. After all the blocks are processed, a sparsifier ${\\mathcal{H}}$ is returned and the {\\textsc{Stable-HFS}\\xspace} solution can be computed. Since~${\\mathcal{H}}$ is very sparse (i.e., it only contains $N = {\\mathcal{O}}(n\\log^2(n))$ edges), it is now possible to use efficient solvers for linear sparse systems and drastically reduce the computational complexity of solving {\\textsc{Stable-HFS}\\xspace} from ${\\mathcal{O}}(Nn)$ down to ${\\mathcal{O}}(N\\log(n))$. The routine \\textsc{sparsify} can be implemented using different spectral sparsification techniques developed for the streaming setting, here we rely on the method proposed in~\\cite{kelner_spectral_2013}. The effective resistance is computed for all edges in the current sparsifier and the new block using random projections and an efficient solver for SDD systems \\cite{koutis2011a-nearly-m}. This step takes ${\\mathcal{O}}(N\\log n)$ time and it returns $\\alpha$-accurate estimates ${\\widetilde{{R}}}'_e$ of the effective resistance for the $2N$ nodes in ${\\mathcal{H}}_{t-1}$ and $\\Delta_t$. If $\\alpha = 1/(1-{\\varepsilon})$ and the input graph ${\\mathcal{H}}_{t-1}$ is a $(1\\pm{\\varepsilon})$-sparsifier, then sampling $N$ edges proportionally to ${\\widetilde{{R}}}'_e$ is guaranteed to generate a $(1\\pm{\\varepsilon})$-sparsifier for the full graph ${\\mathcal{G}}_t = \\sum_{s=1}^t \\Delta_s$ up to iteration $t$. More details on this process are provided in Fig.~\\ref{alg:kl_resparsify} and in~\\cite{kelner_spectral_2013}. The resulting process has a space complexity ${\\mathcal{O}}(N)$ and a time complexity that never exceeds ${\\mathcal{O}}(N\\log(n))$ in sparsifying each block and computing the final solution (see next section for more precise statements on time and space complexity). \n\n\n\n\n\n\\vspace{-0.05in}\n\\section{Theoretical Analysis}\n\\vspace{-0.05in}\n\nWe first report the time and space complexity of {\\textsc{Sparse-HFS}\\xspace}. This result follows from the properties of the sparsifier in~\\cite{kelner_spectral_2013} and the SDD solver in~\\cite{koutis2011a-nearly-m} and thus we do not report its proof.\n\n\\begin{lemma}\\label{lem:sparse.complexity}\n    Let ${\\varepsilon}>0$ be the desired accuracy and $\\delta>0$ the probability of failure. For any connected graph ${\\mathcal{G}}=({\\mathcal{X}},{\\mathcal{E}})$ with $n$ nodes, $m$ edges, eigenvalues $0 = \\lambda_1({\\mathcal{G}}) < \\lambda_2({\\mathcal{G}}) \\leq \\ldots \\leq \\lambda_n({\\mathcal{G}})$, and any partitioning of~${\\mathcal{E}}$ into $\\tau$ blocks, {\\textsc{Sparse-HFS}\\xspace} returns a graph ${\\mathcal{H}}$ such that for any $i=1,\\ldots,n$\n\n\n", "index": 5, "text": "\\begin{align}\\label{eq:spectrum.sparsifier}\n(1-{\\varepsilon}) \\lambda_i({\\mathcal{G}}) \\leq \\lambda_i({\\mathcal{H}}) \\leq (1+{\\varepsilon})\\lambda_i({\\mathcal{G}}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1-{\\varepsilon})\\lambda_{i}({\\mathcal{G}})\\leq\\lambda_{i}({%&#10;\\mathcal{H}})\\leq(1+{\\varepsilon})\\lambda_{i}({\\mathcal{G}}),\" display=\"inline\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\n\\end{definition}\n\\noindent Define the empirical error as ${\\widehat{{R}}}({\\mathbf{f}}) = \\tfrac{1}{l} \\sum_{i=1}^{l} ({\\mathbf{f}}(x_i) - {\\mathbf{y}}(x_i))^2$ and the generalization error as $R({\\mathbf{f}}) = \\tfrac{1}{u}\\sum_{i=1}^{u} ({\\mathbf{f}}(x_i) - {\\mathbf{y}}(x_i))^2$.\n\\begin{theorem}\\label{thm:sparse-ssl-generalization}\n    Let ${\\mathcal{G}}$ be a fixed (connected) graph with $n$ nodes ${\\mathcal{X}}$ and $m$ edges ${\\mathcal{E}}$ and eigenvalues $0 = \\lambda_1({\\mathcal{G}}) < \\lambda_2({\\mathcal{G}}) \\leq \\ldots \\leq \\lambda_n({\\mathcal{G}})$. Let ${\\mathbf{y}}\\in{\\mathbb{R}}^n$ be the labels of the nodes in ${\\mathcal{G}}$ with $|{\\mathbf{y}}(x)| \\leq M$ and ${\\mathcal{F}}$ be the set of centered functions such that $|{\\mathbf{f}}(x) - {\\mathbf{y}}(x)| \\leq c$. Let ${\\mathcal{S}}\\subset {\\mathcal{X}}$ be a random subset of labeled nodes. If the corresponding labels ${\\widetilde{{{\\mathbf{y}}}}}_S$ are centered and {\\textsc{Sparse-HFS}\\xspace} is run with parameter ${\\varepsilon}$, then w.p.~at least $1 - \\delta$ (w.r.t. the random generation of the sparsifier ${\\mathcal{H}}$ and the random subset of labeled points ${\\mathcal{S}}$) the resulting function~${\\widetilde{{{\\mathbf{f}}}}}$ satisfies,\n\n\n", "itemtype": "equation", "pos": 29618, "prevtext": "\n\nwith prob.~$1-\\delta$ (w.r.t.\\@ the random estimation of the effective resistance and the sampling of edges in the \\textsc{sparsify} routine). Furthermore, let $N = \\alpha^2 n\\log^2(n)/{\\varepsilon}^2$ for $\\alpha = 1/(1-{\\varepsilon})$ and $\\tau = m / N$, then with prob.~$1-\\delta$ {\\textsc{Sparse-HFS}\\xspace} has an amortized time per edge of ${\\mathcal{O}}(\\log^3(n))$ and it requires ${\\mathcal{O}}(N)$ memory.\\footnote{In all these big-${\\mathcal{O}}$ expressions we hide multiplicative constants independent from the graph and terms $\\log(1/\\delta)$ which depends on the high-probability nature of the statements.}\n\\end{lemma}\n\n\nThe previous lemma shows the dramatic improvement of {\\textsc{Sparse-HFS}\\xspace} w.r.t.~{\\textsc{Stable-HFS}\\xspace} in terms of both time and space complexity. In fact, while solving {\\textsc{Stable-HFS}\\xspace} in a naive way can take up to ${\\mathcal{O}}(m)$ space and ${\\mathcal{O}}(mn)$ time, {\\textsc{Sparse-HFS}\\xspace} drops these requirements down to ${\\mathcal{O}}(n \\log^2(n)/{\\varepsilon}^2)$ space and ${\\mathcal{O}}(m \\log^3(n))$ time, which allows scaling {\\textsc{Stable-HFS}\\xspace} to graphs orders of magnitude bigger. These improvements have only a limited impact on the spectrum of~${\\mathcal{G}}$ and all its eigenvalues are approximated up to a $(1\\pm{\\varepsilon})$ factor. Moreover, all of the sparsification guarantees hold w.h.p.\\@ for any graph, regardless of how it is generated, its original spectra, and more importantly regardless of the exact order in which the edges are assigned to the blocks. Finally, we notice that the choice of the number of blocks as $m/N$ is crucial to guarantee a logarithmic amortized time, since each iteration takes ${\\mathcal{O}}(N\\log^3(n))$ time. As discussed in Sect.~\\ref{s:conclusions}, this property allows to directly apply {\\textsc{Sparse-HFS}\\xspace} in online learning settings where edges arrive in a stream and intermediate solutions have to be computed incrementally.\n\n\nIn the following, we show that, unlike other heuristics, the space complexity improvements obtained with sparsification come with guarantees and do not degrade the actual learning performance of {\\textsc{HFS}\\xspace}. The analysis of SSL algorithms is built around the algorithm stability theory~\\cite{bousquet_stability_2002}, which is extensively used to analyse transductive learning algorithms~\\cite{el-yaniv_stable_2006,cortes2008stability}. \nWe first remind the definition of algorithmic stability.\n\n\\begin{definition}\\label{def:beta-stability}\n    Let $\\mathcal{L}$ be a transductive learning algorithm. We denote by ${\\mathbf{f}}$ and ${\\mathbf{f}}'$ the functions obtained by running $\\mathcal{L}$ on datasets ${\\mathcal{X}} = ({\\mathcal{S}}, {\\mathcal{T}})$  and ${\\mathcal{X}} = ({\\mathcal{S}}', {\\mathcal{T}}')$ respectively. $\\mathcal{L}$ is uniformly $\\beta$-stable w.r.t. the squared loss if there exists $\\beta \\geq 0$ such that for any two partitions $({\\mathcal{S}}, {\\mathcal{T}})$ and $({\\mathcal{S}}', {\\mathcal{T}}')$ that differ by exactly one training (and test) point and for all ${\\mathbf{x}} \\in {\\mathcal{X}}$,\n\n\n", "index": 7, "text": "\\begin{align*}\n|({\\mathbf{f}}({\\mathbf{x}}) - {\\mathbf{y}}({\\mathbf{x}}))^2 - ({\\mathbf{f}}'({\\mathbf{x}}) - {\\mathbf{y}}({\\mathbf{x}}))^2| \\leq \\beta.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|({\\mathbf{f}}({\\mathbf{x}})-{\\mathbf{y}}({\\mathbf{x}}))^{2}-({%&#10;\\mathbf{f}}^{\\prime}({\\mathbf{x}})-{\\mathbf{y}}({\\mathbf{x}}))^{2}|\\leq\\beta.\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc1f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc1f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>\u03b2</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere ${\\widehat{{{\\mathbf{f}}}}}$ is the solution of exact {\\textsc{Stable-HFS}\\xspace} on ${\\mathcal{G}}$,\n\n\n", "itemtype": "equation", "pos": 31017, "prevtext": "\n\n\\end{definition}\n\\noindent Define the empirical error as ${\\widehat{{R}}}({\\mathbf{f}}) = \\tfrac{1}{l} \\sum_{i=1}^{l} ({\\mathbf{f}}(x_i) - {\\mathbf{y}}(x_i))^2$ and the generalization error as $R({\\mathbf{f}}) = \\tfrac{1}{u}\\sum_{i=1}^{u} ({\\mathbf{f}}(x_i) - {\\mathbf{y}}(x_i))^2$.\n\\begin{theorem}\\label{thm:sparse-ssl-generalization}\n    Let ${\\mathcal{G}}$ be a fixed (connected) graph with $n$ nodes ${\\mathcal{X}}$ and $m$ edges ${\\mathcal{E}}$ and eigenvalues $0 = \\lambda_1({\\mathcal{G}}) < \\lambda_2({\\mathcal{G}}) \\leq \\ldots \\leq \\lambda_n({\\mathcal{G}})$. Let ${\\mathbf{y}}\\in{\\mathbb{R}}^n$ be the labels of the nodes in ${\\mathcal{G}}$ with $|{\\mathbf{y}}(x)| \\leq M$ and ${\\mathcal{F}}$ be the set of centered functions such that $|{\\mathbf{f}}(x) - {\\mathbf{y}}(x)| \\leq c$. Let ${\\mathcal{S}}\\subset {\\mathcal{X}}$ be a random subset of labeled nodes. If the corresponding labels ${\\widetilde{{{\\mathbf{y}}}}}_S$ are centered and {\\textsc{Sparse-HFS}\\xspace} is run with parameter ${\\varepsilon}$, then w.p.~at least $1 - \\delta$ (w.r.t. the random generation of the sparsifier ${\\mathcal{H}}$ and the random subset of labeled points ${\\mathcal{S}}$) the resulting function~${\\widetilde{{{\\mathbf{f}}}}}$ satisfies,\n\n\n", "index": 9, "text": "\\begin{align}\\label{eq:bound}\n    R({\\widetilde{{{\\mathbf{f}}}}}) \\leq \\widehat{R}({\\widehat{{{\\mathbf{f}}}}}) + &\\frac{ l^2 \\gamma^2 \\lambda_n({\\mathcal{G}})^2 M^2 \\varepsilon^2 }{(l \\gamma (1 - \\varepsilon)\\lambda_{2}({\\mathcal{G}}) - 1)^4}+ \\beta +\\nonumber\\\\\n    &\\left(2\\beta + \\frac{c^2(l+u)}{lu}\\right)\\sqrt{\\frac{\\pi(l,u)\\ln\\frac{1}{\\delta}}{2}},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R({\\widetilde{{{\\mathbf{f}}}}})\\leq\\widehat{R}({\\widehat{{{%&#10;\\mathbf{f}}}}})+\" display=\"inline\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{l^{2}\\gamma^{2}\\lambda_{n}({\\mathcal{G}})^{2}M^{2}%&#10;\\varepsilon^{2}}{(l\\gamma(1-\\varepsilon)\\lambda_{2}({\\mathcal{G}})-1)^{4}}+\\beta+\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>l</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>n</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b5</mi><mn>2</mn></msup></mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>4</mn></msup></mfrac></mstyle><mo>+</mo><mrow><mi>\u03b2</mi><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left(2\\beta+\\frac{c^{2}(l+u)}{lu}\\right)\\sqrt{\\frac{\\pi(l,u)\\ln%&#10;\\frac{1}{\\delta}}{2}},\" display=\"inline\"><mrow><mrow><mrow><mo>(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>c</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>l</mi><mo>+</mo><mi>u</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>u</mi></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mo>\u2062</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mfrac><mn>1</mn><mi>\u03b4</mi></mfrac></mrow></mrow><mn>2</mn></mfrac></mstyle></msqrt></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\n\\end{theorem}\nTheorem~\\ref{thm:sparse-ssl-generalization} shows how approximating ${\\mathcal{G}}$ with ${\\mathcal{H}}$ impacts the generalization error as the number of labeled samples $l$ increases. If we compare the bound to the exact case ($\\varepsilon = 0$), we see that for any fixed $\\varepsilon$ the rate of convergence is not affected by the sparsification. The first term in Eq.~\\ref{eq:bound} is of order ${\\mathcal{O}}(\\varepsilon^2/l^2(1-\\varepsilon)^4)$ and it is the additive error w.r.t.\\ the empirical error $R({\\widehat{{{\\mathbf{f}}}}})$ of the {\\textsc{Stable-HFS}\\xspace} solution. For any constant value of~$\\varepsilon$, this term scales as $1/l^2$ and thus it is dominated by the second term in the stability $\\beta$. The $\\beta$ term itself preserves the same order of convergence as for the exact case up to a constant term of order $1/(1-\\varepsilon)$. In conclusion, for any fixed value of the $\\varepsilon$, {\\textsc{Sparse-HFS}\\xspace} preserves the same convergence rate as the exact {\\textsc{Stable-HFS}\\xspace} w.r.t.\\@ the number of labeled and unlabeled points. This means that ${\\varepsilon}$ can be arbitrarily chosen to trade off accuracy and space complexity (in Lemma~\\ref{lem:sparse.complexity}) depending on the problem constraints. Furthermore running time does not depend on this trade-off, because less frequent resparsifications will balance the increased block size.\n\n\\begin{proof}\n\n\\textbf{Step 1 (generalization of stable algorithms).}\nLet $\\beta$ be the stability of {\\textsc{Sparse-HFS}\\xspace}, then using the result in~\\cite{cortes2008stability}, we have that with probability at least $1 - \\delta$ (w.r.t.\\@ the randomness of the labeled set ${\\mathcal{S}}$) the solution ${\\widetilde{{{\\mathbf{f}}}}}$ returned by the {\\textsc{Sparse-HFS}\\xspace} satisfies\n\n\n", "itemtype": "equation", "pos": 31495, "prevtext": "\n\nwhere ${\\widehat{{{\\mathbf{f}}}}}$ is the solution of exact {\\textsc{Stable-HFS}\\xspace} on ${\\mathcal{G}}$,\n\n\n", "index": 11, "text": "\\begin{align*}\n    &\\pi(l,u) = \\frac{lu}{l+u-0.5} \\frac{2\\max\\{l,u\\}}{2\\max\\{l,u\\}-1}, \\enspace \\text{ and }\\\\\n    &\\beta \\leq \\frac{1.5 M\\sqrt{l}}{(l \\gamma (1-\\varepsilon)\\lambda_{2}({\\mathcal{G}})-1)^{2}} + \\frac{4M}{l \\gamma (1 - \\varepsilon)\\lambda_{2}({\\mathcal{G}})-1}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\pi(l,u)=\\frac{lu}{l+u-0.5}\\frac{2\\max\\{l,u\\}}{2\\max\\{l,u\\}-1},%&#10;\\@math@espace\\text{ and }\" display=\"inline\"><mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>l</mi><mo>\u2062</mo><mi>u</mi></mrow><mrow><mrow><mi>l</mi><mo>+</mo><mi>u</mi></mrow><mo>-</mo><mn>0.5</mn></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>l</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>l</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>-</mo><mn>1</mn></mrow></mfrac></mstyle></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\@math@espace</mtext></merror><mo>\u2062</mo><mtext>\u00a0and</mtext></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\beta\\leq\\frac{1.5M\\sqrt{l}}{(l\\gamma(1-\\varepsilon)\\lambda_{2}({%&#10;\\mathcal{G}})-1)^{2}}+\\frac{4M}{l\\gamma(1-\\varepsilon)\\lambda_{2}({\\mathcal{G}%&#10;})-1}.\" display=\"inline\"><mrow><mrow><mi>\u03b2</mi><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1.5</mn><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><msqrt><mi>l</mi></msqrt></mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>4</mn><mo>\u2062</mo><mi>M</mi></mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nIn order to obtain the final result and study how much the sparsification may affect the performance of {\\textsc{Stable-HFS}\\xspace}, we first derive an upper bound on the stability of {\\textsc{Sparse-HFS}\\xspace} and then relate its empirical error to the one of {\\textsc{Stable-HFS}\\xspace}.\n\n\\textbf{Step 2 (stability).}\nThe bound on the stability follows similar steps as in the analysis of {\\textsc{Stable-HFS}\\xspace} in~\\cite{belkin2004regularization} integrated with the properties of streaming spectral sparsifiers in~\\cite{kelner_spectral_2013} reported in Lemma~\\ref{lem:sparse.complexity}.\nLet ${\\mathcal{S}}$ and ${\\mathcal{S}}'$ be two labeled sets only differing by one element and ${\\widetilde{{{\\mathbf{f}}}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'$ be the solutions obtained by running {\\textsc{Sparse-HFS}\\xspace} on ${\\mathcal{S}}$ and ${\\mathcal{S}}'$ respectively. Without loss of generality, we assume that $I_{{\\mathcal{S}}}(l,l) = 1$ and $I_{{\\mathcal{S}}}(l+1,l+1) = 0$, and the opposite for $I_{{\\mathcal{S}}'}$. The original proof in~\\cite{cortes2008stability} showed that the stability $\\beta$ can be bounded as $\\beta\\leq{\\Vert {{\\widetilde{{{\\mathbf{f}}}}} - {\\widetilde{{{\\mathbf{f}}}}}'} \\Vert}$. In the following we show that the difference between the solutions ${\\widetilde{{{\\mathbf{f}}}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'$, and thus the stability of the algorithm, is~strictly related to eigenvalues of the sparse graph${\\mathcal{H}}$. Let $A=P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}})$ and $B=P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}'})$, we remind that if the labels are centered, the solutions of {\\textsc{Sparse-HFS}\\xspace} can be conveniently written as ${\\widetilde{{{\\mathbf{f}}}}}=A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'=B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}$. As a result, the difference between the solutions can be written as\n\n\n", "itemtype": "equation", "pos": 33598, "prevtext": "\n\n\\end{theorem}\nTheorem~\\ref{thm:sparse-ssl-generalization} shows how approximating ${\\mathcal{G}}$ with ${\\mathcal{H}}$ impacts the generalization error as the number of labeled samples $l$ increases. If we compare the bound to the exact case ($\\varepsilon = 0$), we see that for any fixed $\\varepsilon$ the rate of convergence is not affected by the sparsification. The first term in Eq.~\\ref{eq:bound} is of order ${\\mathcal{O}}(\\varepsilon^2/l^2(1-\\varepsilon)^4)$ and it is the additive error w.r.t.\\ the empirical error $R({\\widehat{{{\\mathbf{f}}}}})$ of the {\\textsc{Stable-HFS}\\xspace} solution. For any constant value of~$\\varepsilon$, this term scales as $1/l^2$ and thus it is dominated by the second term in the stability $\\beta$. The $\\beta$ term itself preserves the same order of convergence as for the exact case up to a constant term of order $1/(1-\\varepsilon)$. In conclusion, for any fixed value of the $\\varepsilon$, {\\textsc{Sparse-HFS}\\xspace} preserves the same convergence rate as the exact {\\textsc{Stable-HFS}\\xspace} w.r.t.\\@ the number of labeled and unlabeled points. This means that ${\\varepsilon}$ can be arbitrarily chosen to trade off accuracy and space complexity (in Lemma~\\ref{lem:sparse.complexity}) depending on the problem constraints. Furthermore running time does not depend on this trade-off, because less frequent resparsifications will balance the increased block size.\n\n\\begin{proof}\n\n\\textbf{Step 1 (generalization of stable algorithms).}\nLet $\\beta$ be the stability of {\\textsc{Sparse-HFS}\\xspace}, then using the result in~\\cite{cortes2008stability}, we have that with probability at least $1 - \\delta$ (w.r.t.\\@ the randomness of the labeled set ${\\mathcal{S}}$) the solution ${\\widetilde{{{\\mathbf{f}}}}}$ returned by the {\\textsc{Sparse-HFS}\\xspace} satisfies\n\n\n", "index": 13, "text": "\\begin{align*}\nR({\\widetilde{{{\\mathbf{f}}}}}) \\leq \\widehat{R}({\\widetilde{{{\\mathbf{f}}}}}) + \\beta + \\Big(2\\beta + \\frac{c^2(l+u)}{lu}\\Big)\\sqrt{\\frac{\\pi(l,u)\\log(1/\\delta)}{2}}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R({\\widetilde{{{\\mathbf{f}}}}})\\leq\\widehat{R}({\\widetilde{{{%&#10;\\mathbf{f}}}}})+\\beta+\\Big{(}2\\beta+\\frac{c^{2}(l+u)}{lu}\\Big{)}\\sqrt{\\frac{%&#10;\\pi(l,u)\\log(1/\\delta)}{2}}.\" display=\"inline\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>\u03b2</mi><mo>+</mo><mrow><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>c</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>l</mi><mo>+</mo><mi>u</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>u</mi></mrow></mfrac></mstyle></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>\u2062</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mn>2</mn></mfrac></mstyle></msqrt></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\n\nLet consider any vector ${\\mathbf{f}} \\in {\\mathcal{F}}$, since the null space of a Laplacian $L_{{\\mathcal{H}}}$ is the one vector ${\\mathbf{1}}$ and $P_{\\mathcal{F}} = L_{{\\mathcal{H}}}L_{{\\mathcal{H}}}^+$, then $P_{\\mathcal{F}} {\\mathbf{f}} = {\\mathbf{f}}$. Thus we have\n\n\n", "itemtype": "equation", "pos": 35775, "prevtext": "\n\nIn order to obtain the final result and study how much the sparsification may affect the performance of {\\textsc{Stable-HFS}\\xspace}, we first derive an upper bound on the stability of {\\textsc{Sparse-HFS}\\xspace} and then relate its empirical error to the one of {\\textsc{Stable-HFS}\\xspace}.\n\n\\textbf{Step 2 (stability).}\nThe bound on the stability follows similar steps as in the analysis of {\\textsc{Stable-HFS}\\xspace} in~\\cite{belkin2004regularization} integrated with the properties of streaming spectral sparsifiers in~\\cite{kelner_spectral_2013} reported in Lemma~\\ref{lem:sparse.complexity}.\nLet ${\\mathcal{S}}$ and ${\\mathcal{S}}'$ be two labeled sets only differing by one element and ${\\widetilde{{{\\mathbf{f}}}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'$ be the solutions obtained by running {\\textsc{Sparse-HFS}\\xspace} on ${\\mathcal{S}}$ and ${\\mathcal{S}}'$ respectively. Without loss of generality, we assume that $I_{{\\mathcal{S}}}(l,l) = 1$ and $I_{{\\mathcal{S}}}(l+1,l+1) = 0$, and the opposite for $I_{{\\mathcal{S}}'}$. The original proof in~\\cite{cortes2008stability} showed that the stability $\\beta$ can be bounded as $\\beta\\leq{\\Vert {{\\widetilde{{{\\mathbf{f}}}}} - {\\widetilde{{{\\mathbf{f}}}}}'} \\Vert}$. In the following we show that the difference between the solutions ${\\widetilde{{{\\mathbf{f}}}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'$, and thus the stability of the algorithm, is~strictly related to eigenvalues of the sparse graph${\\mathcal{H}}$. Let $A=P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}})$ and $B=P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}'})$, we remind that if the labels are centered, the solutions of {\\textsc{Sparse-HFS}\\xspace} can be conveniently written as ${\\widetilde{{{\\mathbf{f}}}}}=A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}$ and ${\\widetilde{{{\\mathbf{f}}}}}'=B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}$. As a result, the difference between the solutions can be written as\n\n\n", "index": 15, "text": "\\begin{align}\\label{eq:stability.step1}\n    \\Vert{\\widetilde{{{\\mathbf{f}}}}}-{\\widetilde{{{\\mathbf{f}}}}}'\\Vert&=\\Vert A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\\\\n    &\\leq \\Vert A^{-1}({\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'})\\Vert+\\Vert A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}-B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert.\\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|{\\widetilde{{{\\mathbf{f}}}}}-{\\widetilde{{{\\mathbf{f}}}}}^{%&#10;\\prime}\\|\" display=\"inline\"><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo>-</mo><msup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo>\u2032</mo></msup></mrow><mo>\u2225</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\|A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-B^{-1}{%&#10;\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}\\|\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo>-</mo><mrow><msup><mi>B</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow></mrow><mo>\u2225</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\|A^{-1}({\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{%&#10;\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}})\\|+\\|A^{-1}{\\widetilde{{{%&#10;\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}-B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{%&#10;\\mathcal{S}}^{\\prime}}\\|.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mo>\u2225</mo><mrow><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mo>+</mo><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo>-</mo><mrow><msup><mi>B</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow></mrow><mo>\u2225</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere $(1)$ follows from the triangle inequality \nand $(2)$ follows from the fact that $\\Vert P_{{\\mathcal{F}}}I_{{\\mathcal{S}}}{\\mathbf{f}}\\Vert \\leq \\Vert{\\mathbf{f}}\\Vert$ since the largest eigenvalue of the project matrix $P_{\\mathcal{F}}$ is one and the norm of ${\\mathbf{f}}$ restricted on ${\\mathcal{S}}$ is smaller than the norm of ${\\mathbf{f}}$. Finally (3) follows from the fact that $\\Vert P_{\\mathcal{F}} L_{\\mathcal{H}} {\\mathbf{f}} \\Vert = \\Vert L_{\\mathcal{H}} L_{\\mathcal{H}}^+ L_{\\mathcal{H}} {\\mathbf{f}} \\Vert = \\Vert L_{\\mathcal{H}} {\\mathbf{f}}\\Vert$ and since ${\\mathbf{f}}$ is orthogonal to the null space of $L_{\\mathcal{H}}$ then $\\Vert L_{\\mathcal{H}} {\\mathbf{f}}\\Vert \\geq \\lambda_2({\\mathcal{H}}) \\Vert{\\mathbf{f}}\\Vert$, where $\\lambda_2({\\mathcal{H}})$ is the smallest non-zero eigenvalue of $L_{\\mathcal{H}}$. At this point we can exploit the spectral guarantees of the sparsified graph $L_{\\mathcal{H}}$ and from Lemma~\\ref{lem:sparse.complexity}, we have that $\\lambda_2({\\mathcal{H}}) \\geq (1-\\varepsilon)\\lambda_2({\\mathcal{G}})$. \nAs a result, we have an upper-bound on the spectral radius of the inverse operator $(P_{{\\mathcal{F}}}(l\\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}}))^{-1}$ and thus\n\n\n", "itemtype": "equation", "pos": 36539, "prevtext": "\n\n\nLet consider any vector ${\\mathbf{f}} \\in {\\mathcal{F}}$, since the null space of a Laplacian $L_{{\\mathcal{H}}}$ is the one vector ${\\mathbf{1}}$ and $P_{\\mathcal{F}} = L_{{\\mathcal{H}}}L_{{\\mathcal{H}}}^+$, then $P_{\\mathcal{F}} {\\mathbf{f}} = {\\mathbf{f}}$. Thus we have\n\n\n", "index": 17, "text": "\\begin{align}\n    &\\Vert P_{{\\mathcal{F}}}(l\\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}}){\\mathbf{f}}\\Vert\\stackrel{(1)}{\\geq}\\Vert P_{{\\mathcal{F}}}l\\gamma L_{{\\mathcal{H}}}{\\mathbf{f}}\\Vert \\!-\\! \\Vert P_{{\\mathcal{F}}}I_{{\\mathcal{S}}}{\\mathbf{f}}\\Vert\\nonumber\\\\\n    &\\stackrel{(2)}{\\geq} \\Vert P_{{\\mathcal{F}}}l\\gamma L_{{\\mathcal{H}}}{\\mathbf{f}}\\Vert \\!-\\! \\Vert{\\mathbf{f}}\\Vert \\stackrel{(3)}{\\geq}(l \\gamma \\lambda_{1}({\\mathcal{H}})\\!-\\!1)\\Vert {\\mathbf{f}}\\Vert \\label{eq:lower.bound}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|P_{{\\mathcal{F}}}(l\\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}}){%&#10;\\mathbf{f}}\\|\\stackrel{(1)}{\\geq}\\|P_{{\\mathcal{F}}}l\\gamma L_{{\\mathcal{H}}}{%&#10;\\mathbf{f}}\\|\\!-\\!\\|P_{{\\mathcal{F}}}I_{{\\mathcal{S}}}{\\mathbf{f}}\\|\" display=\"inline\"><mrow><mrow><mo>\u2225</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></msub></mrow><mo>+</mo><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow><mo>\u2225</mo></mrow><mover><mo movablelimits=\"false\">\u2265</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">1</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><mrow><mo>\u2225</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></msub><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow><mo rspace=\"0.8pt\">\u2225</mo></mrow><mo rspace=\"0.8pt\">-</mo><mrow><mo>\u2225</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>\u2062</mo><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow><mo>\u2225</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\stackrel{(2)}{\\geq}\\|P_{{\\mathcal{F}}}l\\gamma L_{{\\mathcal{H}}}{%&#10;\\mathbf{f}}\\|\\!-\\!\\|{\\mathbf{f}}\\|\\stackrel{(3)}{\\geq}(l\\gamma\\lambda_{1}({%&#10;\\mathcal{H}})\\!-\\!1)\\|{\\mathbf{f}}\\|\" display=\"inline\"><mrow><mi/><mover><mo movablelimits=\"false\">\u2265</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">2</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><mrow><mo>\u2225</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></msub><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow><mo rspace=\"0.8pt\">\u2225</mo></mrow><mo rspace=\"0.8pt\">-</mo><mrow><mo>\u2225</mo><mi>\ud835\udc1f</mi><mo>\u2225</mo></mrow></mrow><mover><mo movablelimits=\"false\">\u2265</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">3</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.8pt\">-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>\ud835\udc1f</mi><mo>\u2225</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere the first step follows from Eq.~\\ref{eq:lower.bound} since both ${\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}$ and ${\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}$ are centered and thus $({\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'})\\in{\\mathcal{F}}$, and the second step is obtained by bounding $\\Vert{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\leq \\Vert{\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'}\\Vert + \\Vert{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert \\leq 4M$. The second term in Eq.~\\ref{eq:stability.step1} can be bounded as\n\n\n", "itemtype": "equation", "pos": 38282, "prevtext": "\n\nwhere $(1)$ follows from the triangle inequality \nand $(2)$ follows from the fact that $\\Vert P_{{\\mathcal{F}}}I_{{\\mathcal{S}}}{\\mathbf{f}}\\Vert \\leq \\Vert{\\mathbf{f}}\\Vert$ since the largest eigenvalue of the project matrix $P_{\\mathcal{F}}$ is one and the norm of ${\\mathbf{f}}$ restricted on ${\\mathcal{S}}$ is smaller than the norm of ${\\mathbf{f}}$. Finally (3) follows from the fact that $\\Vert P_{\\mathcal{F}} L_{\\mathcal{H}} {\\mathbf{f}} \\Vert = \\Vert L_{\\mathcal{H}} L_{\\mathcal{H}}^+ L_{\\mathcal{H}} {\\mathbf{f}} \\Vert = \\Vert L_{\\mathcal{H}} {\\mathbf{f}}\\Vert$ and since ${\\mathbf{f}}$ is orthogonal to the null space of $L_{\\mathcal{H}}$ then $\\Vert L_{\\mathcal{H}} {\\mathbf{f}}\\Vert \\geq \\lambda_2({\\mathcal{H}}) \\Vert{\\mathbf{f}}\\Vert$, where $\\lambda_2({\\mathcal{H}})$ is the smallest non-zero eigenvalue of $L_{\\mathcal{H}}$. At this point we can exploit the spectral guarantees of the sparsified graph $L_{\\mathcal{H}}$ and from Lemma~\\ref{lem:sparse.complexity}, we have that $\\lambda_2({\\mathcal{H}}) \\geq (1-\\varepsilon)\\lambda_2({\\mathcal{G}})$. \nAs a result, we have an upper-bound on the spectral radius of the inverse operator $(P_{{\\mathcal{F}}}(l\\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}}))^{-1}$ and thus\n\n\n", "index": 19, "text": "\\begin{align*}\n    \\Vert A^{-1}({\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'})\\Vert&\\leq \\frac{\\Vert{\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'}\\Vert}{l\\gamma (1 - \\varepsilon)\\lambda_{1}({\\mathcal{G}})-1} \\\\\n    &\\leq \\frac{4M}{l\\gamma (1 - \\varepsilon)\\lambda_{1}({\\mathcal{G}})-1},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|A^{-1}({\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}%&#10;^{\\prime}})\\|\" display=\"inline\"><mrow><mo>\u2225</mo><mrow><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc32</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>-</mo><msub><mi>\ud835\udc32</mi><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{\\|{\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{%&#10;S}}^{\\prime}}\\|}{l\\gamma(1-\\varepsilon)\\lambda_{1}({\\mathcal{G}})-1}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc32</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>-</mo><msub><mi>\ud835\udc32</mi><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo>\u2225</mo></mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{4M}{l\\gamma(1-\\varepsilon)\\lambda_{1}({\\mathcal{G}})-1},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>4</mn><mo>\u2062</mo><mi>M</mi></mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere we used $\\Vert{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\leq \\Vert{\\mathbf{y}}_{{\\mathcal{S}}'}\\Vert + \\Vert{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert \\leq 2M\\sqrt{l}$, $\\Vert P_{{\\mathcal{F}}}(I_{{\\mathcal{S}}}-I_{{\\mathcal{S}}'})\\Vert \\leq \\sqrt{2} < 1.5$ and we applied Eq.~\\ref{eq:lower.bound} twice.\nPutting it all together we obtain the final bound reported in the statement.\n\n\\textbf{Step 3 (empirical error).} The other element effected by the sparsification is the empirical error ${\\widehat{{R}}}({\\widetilde{{{\\mathbf{f}}}}})$. Let ${\\widetilde{{A}}} = P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}})$, ${\\widehat{{A}}} = P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{G}}}+I_{{\\mathcal{S}}})$, then \n\n\n", "itemtype": "equation", "pos": 39276, "prevtext": "\n\nwhere the first step follows from Eq.~\\ref{eq:lower.bound} since both ${\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}$ and ${\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}$ are centered and thus $({\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'})\\in{\\mathcal{F}}$, and the second step is obtained by bounding $\\Vert{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\leq \\Vert{\\mathbf{y}}_{{\\mathcal{S}}}-{\\mathbf{y}}_{{\\mathcal{S}}'}\\Vert + \\Vert{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}-{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert \\leq 4M$. The second term in Eq.~\\ref{eq:stability.step1} can be bounded as\n\n\n", "index": 21, "text": "\\begin{align*}\n    &\\Vert A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}-B^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert=\\Vert B^{-1}(B-A)A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\\\\n&=\\Vert B^{-1}P_{{\\mathcal{F}}}(I_{{\\mathcal{S}}}-I_{{\\mathcal{S}}'})A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\leq\\frac{1.5 M\\sqrt{l}}{(l \\gamma (1-\\varepsilon)\\lambda_{1}({\\mathcal{G}})-1)^{2}},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}-B^{%&#10;-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}\\|=\\|B^{-1}(B-A)A^{-1}%&#10;{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}\\|\" display=\"inline\"><mrow><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo>-</mo><mrow><msup><mi>B</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow></mrow><mo>\u2225</mo></mrow><mo>=</mo><mrow><mo>\u2225</mo><mrow><msup><mi>B</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>B</mi><mo>-</mo><mi>A</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo>\u2225</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\|B^{-1}P_{{\\mathcal{F}}}(I_{{\\mathcal{S}}}-I_{{\\mathcal{S}}^{%&#10;\\prime}})A^{-1}{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}^{\\prime}}\\|\\leq%&#10;\\frac{1.5M\\sqrt{l}}{(l\\gamma(1-\\varepsilon)\\lambda_{1}({\\mathcal{G}})-1)^{2}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>\u2225</mo><mrow><msup><mi>B</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>-</mo><msub><mi>I</mi><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></msub></mrow><mo>\u2225</mo></mrow><mo>\u2264</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1.5</mn><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><msqrt><mi>l</mi></msqrt></mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere in the last step we applied Eq.~\\ref{eq:lower.bound} on both ${\\widehat{{A}}}^{-1}$ and ${\\widetilde{{A}}}^{-1}$. We are left with ${\\Vert {{\\widehat{{A}}} -{\\widetilde{{A}}}} \\Vert}^2 = {\\Vert {P_{{\\mathcal{F}}} l \\gamma (L_{{\\mathcal{G}}} - L_{{\\mathcal{H}}})} \\Vert}^2$. We first recall that $P_{\\mathcal{F}} = L_{\\mathcal{G}}^+ L_{\\mathcal{G}} = L_{{\\mathcal{G}}}^{-1/2}L_{{\\mathcal{G}}}L_{{\\mathcal{G}}}^{-1/2}$ (and equivalently with ${\\mathcal{G}}$ replaced by ${\\mathcal{H}}$) and we introduce ${\\widetilde{{P}}}_{{\\mathcal{F}}} = L_{{\\mathcal{G}}}^{-1/2}L_{{\\mathcal{H}}}L_{{\\mathcal{G}}}^{-1/2}$. We have\n\n\n", "itemtype": "equation", "pos": 40455, "prevtext": "\n\nwhere we used $\\Vert{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert\\leq \\Vert{\\mathbf{y}}_{{\\mathcal{S}}'}\\Vert + \\Vert{\\overline{{{\\mathbf{y}}}}}_{{\\mathcal{S}}'}\\Vert \\leq 2M\\sqrt{l}$, $\\Vert P_{{\\mathcal{F}}}(I_{{\\mathcal{S}}}-I_{{\\mathcal{S}}'})\\Vert \\leq \\sqrt{2} < 1.5$ and we applied Eq.~\\ref{eq:lower.bound} twice.\nPutting it all together we obtain the final bound reported in the statement.\n\n\\textbf{Step 3 (empirical error).} The other element effected by the sparsification is the empirical error ${\\widehat{{R}}}({\\widetilde{{{\\mathbf{f}}}}})$. Let ${\\widetilde{{A}}} = P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{H}}}+I_{{\\mathcal{S}}})$, ${\\widehat{{A}}} = P_{{\\mathcal{F}}}(l \\gamma L_{{\\mathcal{G}}}+I_{{\\mathcal{S}}})$, then \n\n\n", "index": 23, "text": "\\begin{align*}\n    {\\widehat{{R}}}({\\widetilde{{{\\mathbf{f}}}}}) &= \\frac{1}{l} {\\Vert {I_{\\mathcal{S}} {\\widetilde{{{\\mathbf{f}}}}} -I_{\\mathcal{S}} {\\widehat{{{\\mathbf{f}}}}} +I_{\\mathcal{S}} {\\widehat{{{\\mathbf{f}}}}}  - {\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}} \\Vert}^2\\nonumber\\\\\n&\\leq \\tfrac{1}{l} {\\Vert {I_{\\mathcal{S}} {\\widehat{{{\\mathbf{f}}}}}  - {\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}} \\Vert}^2 + \\tfrac{1}{l} {\\Vert {I_{\\mathcal{S}} {\\widetilde{{{\\mathbf{f}}}}} -I_{\\mathcal{S}} {\\widehat{{{\\mathbf{f}}}}} } \\Vert}^2 \\\\\n&\\leq {\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}}) + \\tfrac{1}{l} {\\Vert {I_{\\mathcal{S}}({\\widetilde{{A}}}^{-1} -{\\widehat{{A}}}^{-1}) {\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}} } \\Vert}^2\\nonumber\\\\\n&\\leq {\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}}) + \\tfrac{1}{l} {\\Vert {{\\widehat{{A}}}^{-1}({\\widehat{{A}}} -{\\widetilde{{A}}}){\\widetilde{{A}}}^{-1} {\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}} } \\Vert}^2\\\\\n&\\leq {\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}}) + \\frac{1}{l}\\frac{lM^2}{(l \\gamma (1 - \\varepsilon)\\lambda_{1}({\\mathcal{G}}) - 1)^4} {\\Vert {{\\widehat{{A}}} -{\\widetilde{{A}}}} \\Vert}^2,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\widehat{{R}}}({\\widetilde{{{\\mathbf{f}}}}})\" display=\"inline\"><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{l}{\\|{I_{\\mathcal{S}}{\\widetilde{{{\\mathbf{f}}}}}-I_{%&#10;\\mathcal{S}}{\\widehat{{{\\mathbf{f}}}}}+I_{\\mathcal{S}}{\\widehat{{{\\mathbf{f}}}%&#10;}}-{\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>l</mi></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mrow><mrow><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover></mrow><mo>-</mo><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover></mrow></mrow><mo>+</mo><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover></mrow></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\tfrac{1}{l}{\\|{I_{\\mathcal{S}}{\\widehat{{{\\mathbf{f}}}}}-{%&#10;\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}}\\|}^{2}+\\tfrac{1}{l}{\\|{I_{%&#10;\\mathcal{S}}{\\widetilde{{{\\mathbf{f}}}}}-I_{\\mathcal{S}}{\\widehat{{{\\mathbf{f}%&#10;}}}}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mfrac><mn>1</mn><mi>l</mi></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>l</mi></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>~</mo></mover></mrow><mo>-</mo><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}})+\\tfrac{1}{l}{\\|{I%&#10;_{\\mathcal{S}}({\\widetilde{{A}}}^{-1}-{\\widehat{{A}}}^{-1}){\\widetilde{{{%&#10;\\mathbf{y}}}}}_{{\\mathcal{S}}}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>l</mi></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msub><mi>I</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mover accent=\"true\"><mi>A</mi><mo>~</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>-</mo><msup><mover accent=\"true\"><mi>A</mi><mo>^</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}})+\\tfrac{1}{l}{\\|{{%&#10;\\widehat{{A}}}^{-1}({\\widehat{{A}}}-{\\widetilde{{A}}}){\\widetilde{{A}}}^{-1}{%&#10;\\widetilde{{{\\mathbf{y}}}}}_{{\\mathcal{S}}}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>l</mi></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msup><mover accent=\"true\"><mi>A</mi><mo>^</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>A</mi><mo>^</mo></mover><mo>-</mo><mover accent=\"true\"><mi>A</mi><mo>~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>A</mi><mo>~</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc32</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\widehat{{R}}}({\\widehat{{{\\mathbf{f}}}}})+\\frac{1}{l}\\frac{%&#10;lM^{2}}{(l\\gamma(1-\\varepsilon)\\lambda_{1}({\\mathcal{G}})-1)^{4}}{\\|{{\\widehat%&#10;{{A}}}-{\\widetilde{{A}}}}\\|}^{2},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mover accent=\"true\"><mi>R</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo>^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>l</mi></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>l</mi><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>4</mn></msup></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>A</mi><mo>^</mo></mover><mo>-</mo><mover accent=\"true\"><mi>A</mi><mo>~</mo></mover></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05675.tex", "nexttext": "\n\nwhere in $(1)$ we use $P_{{\\mathcal{F}}} L_{\\mathcal{G}} = L_{\\mathcal{G}}$ and $P_{{\\mathcal{F}}} L_{\\mathcal{H}} = L_{\\mathcal{H}}$, in $(2)$ we rewrite $L_{\\mathcal{G}} = L_{\\mathcal{G}}^{1/2}L_{\\mathcal{G}}^{-1/2}L_{\\mathcal{G}} L_{\\mathcal{G}}^{-1/2}L_{\\mathcal{G}}^{1/2} = L_{\\mathcal{G}}^{1/2} P_{\\mathcal{F}} L_{\\mathcal{G}}^{1/2}$ and $L_{\\mathcal{G}} = L_{\\mathcal{G}}^{1/2}L_{\\mathcal{G}}^{-1/2}L_{\\mathcal{H}} L_{\\mathcal{G}}^{-1/2}L_{\\mathcal{G}}^{1/2} = L_{\\mathcal{G}}^{1/2} {\\widetilde{{P}}}_{\\mathcal{F}} L_{\\mathcal{G}}^{1/2}$, in $(3)$ we split the norm and use the fact that the spectral norm of $L_{\\mathcal{G}}$ corresponds to its largest eigenvalue $\\lambda_n({\\mathcal{G}})$, while in $(4)$ we use the fact that Def.~\\ref{def:eps-sparsifier} implies that $(1-{\\varepsilon}) P_{{\\mathcal{F}}} \\preceq {\\widetilde{{P}}}_{{\\mathcal{F}}} \\preceq (1 + {\\varepsilon}) P_{{\\mathcal{F}}}$ and thus the largest eigenvalue of $P_{{\\mathcal{F}}} - {\\widetilde{{P}}}_{{\\mathcal{F}}}$ is $\\varepsilon^2 \\Vert P_{\\mathcal{F}}\\Vert \\leq {\\varepsilon}^2$.\nThe final statement follows by combining the three steps above.\n\\end{proof}\n\n\n\n\n\\vspace{-0.1in}\n\\section{Experiments}\\label{s:experiments}\n\n\nIn this section we evaluate the empirical accuracy of {\\textsc{Sparse-HFS}\\xspace} compared to other baselines for large-scale SSL on both synthetic and real datasets.\n\n\n\\begin{figure*}[t]\n    \\vspace{-0.3cm}\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|}\n    & Guarant. & Space & Preprocessing Time & Solving Time\\\\\n    \\hline\n    \\vspace{-0.3cm}& & & & \\\\\n    {\\textsc{Sparse-HFS}\\xspace} & \\includegraphics[height=0.3cm]{guarantees_yes} & $N = {\\mathcal{O}}(n \\log^2(n)) $& ${\\mathcal{O}}(m \\log^3(n)) $& ${\\mathcal{O}}(N \\log(n)) = {\\mathcal{O}}(n \\log^3(n))$\\\\\n    {\\textsc{Stable-HFS}\\xspace} & \\includegraphics[height=0.3cm]{guarantees_yes}& {$\\textcolor{red}{\\boldsymbol{{{\\mathcal{O}}(m)}}}$}& ${\\mathcal{O}}(m) $& ${\\mathcal{O}}(m n)$\\\\\n    {\\textsc{Simple-HFS}\\xspace} &\\includegraphics[height=0.3cm]{guarantees_kinda} & {$\\textcolor{red}{\\boldsymbol{{{\\mathcal{O}}(m)}}}$}& ${\\mathcal{O}}(mq) $ & ${\\mathcal{O}}(q^4)$\\\\\n    {\\textsc{EigFun}\\xspace} & \\includegraphics[height=0.3cm]{guarantees_no}& ${\\mathcal{O}}(nd + nq + b^2) $& ${\\mathcal{O}}(qb^3  + db^3) $& ${\\mathcal{O}}(q^3 + nq)$\\\\\n    {\\textsc{SubSampling}\\xspace} & \\includegraphics[height=0.3cm]{guarantees_no}& ${\\mathcal{O}}(sk) $& ${\\mathcal{O}}(m) $& ${\\mathcal{O}}(s^2 k + n)$\n    \\end{tabular}\n    \\caption{{\\small \\textbf{Guarantees and Computational complexities.}\n    Bold text indicates unfeasible time or space complexity.\n    \\protect \\includegraphics[height=0.2cm]{guarantees_no} Guarantees unavailable.\n     \\protect \\includegraphics[height=0.2cm]{guarantees_kinda} {\\textsc{Simple-HFS}\\xspace}'s guarantees require assumptions on the graph ${\\mathcal{G}}$. \\vspace{-0.3cm}}\\label{fig:comparison-table}}\n\\end{figure*}\n\n\\textbf{Synthetic data.}\nThe objective of this first experiment is to show that the sparsification method is effective in reducing the number of edges in the graph and that preserving the full spectrum of ${\\mathcal{G}}$ retains the accuracy of the exact {\\textsc{HFS}\\xspace} solution.\nWe evaluate the algorithms on the ${\\mathbb{R}}^2$ data distributed as in\nFig.~\\ref{fig:fibsyn}\\subref{fig:gen-data}, which is designed so that a large\nnumber of neighbours is needed to achieve a good accuracy. The dataset is\ncomposed of $n = 12100$ points, where the two upper clusters belong to one\nclass and the two lower to the other. We build an unweighted, $k$-nn graph\n${\\mathcal{G}}$ for $k = {100,\\ldots,12000}$.\nAfter constructing the graph, we randomly select two points from the uppermost\nand two from lowermost cluster as our labeled set ${\\mathcal{S}}$. We then run\n{\\textsc{Sparse-HFS}\\xspace} with ${\\varepsilon} = 0.8$ to compute ${\\mathcal{H}}$ and~${\\widetilde{{{\\mathbf{f}}}}}$, and run\n(exact) {\\textsc{Stable-HFS}\\xspace}\non ${\\mathcal{G}}$ to compute~${\\widehat{{{\\mathbf{f}}}}}$, both with $\\gamma = 1$.\nFig.~\\ref{fig:fibsyn}\\subref{fig:gen-error} reports the accuracy of the two\nalgorithms. Both algorithms fail to recover a good solution until\n$k \\approx 4000$. This is due to the fact that until a certain threshold, each cluster remains separated and the labels\ncannot propagate. Beyond this threshold, {\\textsc{Stable-HFS}\\xspace} is very\naccurate, while, as $k$ increases again, the graph becomes almost full, masking\nthe actual structure of the data and thus loosing performance again. We notice\nthat the accuracy of {\\textsc{Stable-HFS}\\xspace} and {\\textsc{Sparse-HFS}\\xspace} is never significantly different,\nand, quite importantly, they match around the value of $k = 4500$\nthat provides the best performance. This is\nin line with the theoretical analysis that shows that the contribution due to\nthe sparsification error has the same order of magnitude as the other elements\nin the bound.\nFurthermore, in Fig.~\\ref{fig:fibsyn}\\subref{fig:edge-ratio} we report the ratio of the number of edges in the sparsifier~${\\mathcal{H}}$ and ${\\mathcal{G}}$. \nThis quantity is always smaller than one and it constantly decreases since the number of edges in ${\\mathcal{H}}$ is constant, while the size ${\\mathcal{G}}$ increases linearly with the number of neighbors (i.e., $|{\\mathcal{H}}|/|{\\mathcal{G}}| = {\\mathcal{O}}(1/k)$).\nWe notice that for the optimal $k$ the sparsifier contains less than 10\\% of the edges of the original graph but it achieves almost the same accuracy.\n\n\\begin{figure}[t]\n\\begin{tabular}{m{0.3\\columnwidth}m{0.7\\columnwidth}}\n        \\sidesubfloat[]{\\includegraphics[height=6cm]{./data.pdf} \\label{fig:gen-data}}&\n        \\sidesubfloat[]{\\includegraphics[height=3cm]{./toy_result.pdf}\\label{fig:gen-error}}\\newline\n        \\sidesubfloat[]{\\includegraphics[height=3cm]{./edge_ratio_plot.pdf}\\label{fig:edge-ratio}}\n    \\end{tabular}\n\\caption{\\small \\protect\\subref{fig:gen-data} The dataset of the synthetic experiment, \\protect\\subref{fig:gen-error} Accuracy of {\\textsc{Stable-HFS}\\xspace} and {\\textsc{Sparse-HFS}\\xspace}, \\protect\\subref{fig:edge-ratio} ratio of the number of edges $|{\\mathcal{H}}|/|{\\mathcal{G}}|$.\\vspace{-0.25in}}\n\\label{fig:fibsyn}\n\\end{figure}\n\n\n\\begin{figure}[t]\n\\begin{center}\n    \\includegraphics[width=\\columnwidth]{cost_plot_combined}\n\\end{center}\n\\vspace{-0.05in}\n\\caption{\\small Accuracy vs complexity on the TREC 2007 SPAM Corpus for different number of labels. \\textit{Legend:} \\allowbreak \\protect\\includegraphics[height=0.2cm]{legend_sparsehfs} {\\textsc{Sparse-HFS}\\xspace}, \\protect\\includegraphics[height=0.2cm]{legend_fergus} {\\textsc{EigFun}\\xspace}, \\protect\\includegraphics[height=0.2cm]{legend_subsamplehfs} {\\textsc{SubSampling}\\xspace}, \\protect\\raisebox{0.2em}{\\protect\\includegraphics[width=0.5cm]{legend_onenn}} {\\textsc{1-NN}\\xspace} \\vspace{-0.5cm}\\label{fig:gen-error-emp}}\n\\end{figure}\n\n\n\n\\textbf{Spam-filtering dataset.} We now evaluate the performance of our\nalgorithm on the TREC 2007 Public Spam Corpus\\footnote{\\scriptsize\\url{http://plg.uwaterloo.ca/~gvcormac/treccorpus07/}},\nthat contains $n = 75419$ raw emails labeled as either \\textit{SPAM} or\n\\textit{HAM}. The emails are provided as raw text and we applied standard NLP\ntechniques to extract features vectors from it. In particular, we computed\nTF-IDF scores for each of the emails, with some additional cleaning in the\nform of a stop word list, simple stemming and dropping the 1\\% most common and\nmost rare words. We ended up with~$d = 68697$ features, each representing a\nword present in some of the emails. From these features we proceeded to build\na graph ${\\mathcal{G}}$ where given two emails ${\\mathbf{x}}_i,{\\mathbf{x}}_j$, the weight is computed as\n$a_{ij} = \\exp(-\\Vert {\\mathbf{x}}_i - {\\mathbf{x}}_j\\Vert /2\\sigma^2)$, with $\\sigma^2=3$. We\nconsider the transductive setting, where the graph is fixed and known, but\nonly a small random subset of $l = \\{20,100,1000\\}$ labels is revealed to the\nalgorithm. As a performance measure, we consider the prediction accuracy over\nthe whole dataset. We compare our method to several baselines. The most basic\nsupervised baseline is {\\textsc{1-NN}\\xspace}, which connects each node to the closest labeled node. The\n{\\textsc{SubSampling}\\xspace} algorithm selects uniformly $s$ nodes out of~$n$, computes the {\\textsc{HFS}\\xspace}\nsolution on the induced subgraph of ${\\mathcal{G}}$ and assigns to each node outside of the subset the same label as the closest node in the subset.\n{\\textsc{SubSampling}\\xspace}'s complexity depends on the size\n$s$ of the subgraph and the number $k$ of neighbors retained when building the~$k$-nn subgraph. The eigenfunction ({\\textsc{EigFun}\\xspace})\nalgorithm~\\cite{fergus2009semi-supervised} tries to sidestep the\ncomputational complexity of finding an {\\textsc{HFS}\\xspace} solution on ${\\mathcal{G}}$,\nby directly approximating the distribution that created the graph.\nStarting directly from the samples, each of the $d$ feature's density is separately approximated\nusing histograms with $b$ bins.\nFrom the histograms, $q$ empirical eigenfunctions (vectors in ${\\mathbb{R}}^n$) are extracted and used\nto compute the final solution.\nWe did not include {\\textsc{Stable-HFS}\\xspace} and {\\textsc{Simple-HFS}\\xspace} in the comparison\nbecause their ${\\mathcal{O}}(m)$ space complexity made them unfeasible for this dataset.\nIn Fig.~\\ref{fig:gen-error-emp}, we\nreport the accuracy of each method against the time and space complexity,\nwhere each separate point corresponds to a different choice in metaparameters (e.g.~$k,q,s$).\nFor {\\textsc{EigFun}\\xspace}, we use the same $b = 50$ as in the original implementation, but\nwe varied $q$ from 10 to 2000. For {\\textsc{SubSampling}\\xspace}, $s=15000$ and $k$ varies from 100 to\n10000. We run {\\textsc{Sparse-HFS}\\xspace} on ${\\mathcal{G}}$ setting $\\varepsilon = 0.9$, and\nchanging the size $m$ of the input graph by changing the number of\nneighbours $k$ from 1000 to 7500.\nSince the actual running time and memory\noccupation are highly dependent on the implementation (e.g., {\\textsc{EigFun}\\xspace} is\nimplemented in Matlab, while {\\textsc{Sparse-HFS}\\xspace} is Matlab/C), the\ncomplexities are computed using their theoretical form (e.g.,\n${\\mathcal{O}}(m \\log^3(n))$ for {\\textsc{Sparse-HFS}\\xspace})\nwith the values actually used in the experiment (e.g.,~$m = nk$ for a $k$-nn graph).\nAll the complexities are reported in Fig.~\\ref{fig:comparison-table}.\nThe only exception is the number of edges in the sparsifier $N$ used in the space complexity of {\\textsc{Sparse-HFS}\\xspace}.\nSince this is a random quantity that holds only w.h.p.~and that is\nindependent from implementation details, we measured it empirically and\nused it for the complexities.\nFor all methods we notice that the performance increases as the space\ncomplexity gets larger, until a peak is reached, while additional space\ninduces the algorithms to overfit\nand reduces accuracy.\nFor {\\textsc{EigFun}\\xspace} this means that a large number of eigenfunctions is necessary\nto accurately model the high dimensional distribution. And as theory predicts,\n{\\textsc{SubSampling}\\xspace}'s uniform sampling is not efficient to approximate the graph spectra,\nand a large subset of the nodes is required for good performance.\n{\\textsc{Sparse-HFS}\\xspace}'s accuracy also increases as the input graph gets richer, but unlike\nthe other methods the space complexity does not change much. This is\nbecause the sparsifier is oblivious to the structure of the graph,\nand even if {\\textsc{Sparse-HFS}\\xspace} reaches its optimum performance for $k=3000$,\nthe sparsifier contains roughly the same number of edges present as\n$k=1000$, and only 5\\% of the edges present in the input graph.\nAlthough preliminary, this\nexperiment shows that the theoretical properties of {\\textsc{Sparse-HFS}\\xspace} translate into\nan effective practical algorithm which is competitive with state-of-the-art\nmethods for large-scale SSL.\n\n\n\\vspace{-0.15in}\n\\section{Conclusions and Future Work}\\label{s:conclusions}\n\\vspace{-0.15in}\n\nWe introduced {\\textsc{Sparse-HFS}\\xspace}, an algorithm that combines\nsparsification methods and efficient solvers for SDD\nsystems to find approximate {\\textsc{HFS}\\xspace} solutions\nusing only ${\\mathcal{O}}(n\\log^2(n))$ space instead of ${\\mathcal{O}}(m)$.\nFurthermore, we show that the ${\\mathcal{O}}(m\\log^3(n))$ time complexity of the methods\nis only a ${\\operatorname{polylog}}$ term away from the smallest possible complexity  $\\Omega(m)$.\nFinally, we provide a bound on the generalization error that\nshows that the sparsification does not affect the asymptotic convergence rate\nof {\\textsc{HFS}\\xspace}. As such, the accuracy parameter ${\\varepsilon}$ can be freely chosen to\nmeet the desired trade-off between accuracy and space complexity.\nIn this paper we relied on the sparsifier in~\\cite{kelner_spectral_2013}\nto guarantee a fixed space requirement, and\nthe solver in~\\cite{koutis2011a-nearly-m} to efficiently compute\nthe effective resistances. Both are straightforward to scale and parallelize \\cite{blelloch2010hierarchical},\nand the bottleneck in practice reduces to finding a fast sparse matrix-vector\nmultiplication implementation\nfor which many off-the-shelf solutions exist. We also remark that {\\textsc{Sparse-HFS}\\xspace} could easily\naccommodate any improved version of these algorithms and their properties\nwould directly translate into the performance of {\\textsc{Sparse-HFS}\\xspace}.\nIn particular \\cite{koutis2011a-nearly-m} already mentions how finding\nan appropriate spanning tree (a low-stretch tree) and using it\nas the backbone of the sparsifier allows to reduce the space\nrequirements of the sparsifier.\nAlthough this technique could lower the space complexity to\n${\\mathcal{O}}(n \\log(n))$, it is not clear how to find such a tree incrementally.\nAn interesting feature of {\\textsc{Sparse-HFS}\\xspace} is that it could be easily employed in\nonline learning problems where edges arrive in a stream and intermediate\nsolutions have to be computed over time.\nSince {\\textsc{Sparse-HFS}\\xspace} has a\n${\\mathcal{O}}(\\log^3(n))$ amortized time per edge, it could compute intermediate\nsolutions every $N$ edges\nwithout compromising its overall time complexity.\nThe fully dynamic setting, where edges can be both inserted and removed,\nis an important extension where our approach could be further investigated,\nespecially because it has been observed in several domains that graphs\nbecome denser as they evolve over time \\cite{leskovec2005graphs}.\nWhile sparsifiers have been developed for this setting (see\ne.g.,~\\cite{kapralov_single_2014}), current solutions would require\n${\\mathcal{O}}(n^2 {\\operatorname{polylog}}(n))$ time to compute the {\\textsc{HFS}\\xspace} solution, thus making it\nunfeasible to repeat this computation many times over the stream. Extending\nsparsification techniques to the fully dynamic setting in a computationally\nefficient manner is  an open problem.\n\n\\begin{thebibliography}{10}\n\n\\bibitem{Batson:2013:SSG:2492007.2492029}\nJoshua Batson, Daniel~A. Spielman, Nikhil Srivastava, and Shang-Hua Teng.\n\\newblock Spectral sparsification of graphs: Theory and algorithms.\n\\newblock {\\em Commun. ACM}, 56(8):87--94, August 2013.\n\n\\bibitem{belkin2004regularization}\nMikhail Belkin, Irina Matveeva, and Partha Niyogi.\n\\newblock {Regularization and Semi-Supervised Learning on Large Graphs}.\n\\newblock In {\\em Proceedings of COLT}, 2004.\n\n\\bibitem{belkin2006manifold}\nMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.\n\\newblock {Manifold Regularization: A Geometric Framework for Learning from\n  Labeled and Unlabeled Examples}.\n\\newblock {\\em Journal of Machine Learning Research}, 7:2399--2434, 2006.\n\n\\bibitem{blelloch2010hierarchical}\nGuy~E Blelloch, Ioannis Koutis, Gary~L Miller, and Kanat Tangwongsan.\n\\newblock Hierarchical diagonal blocking and precision reduction applied to\n  combinatorial multigrid.\n\\newblock In {\\em High Performance Computing, Networking, Storage and Analysis\n  (SC), 2010 International Conference for}, pages 1--12. IEEE, 2010.\n\n\\bibitem{bousquet_stability_2002}\nOlivier Bousquet and Andr{\\'e} Elisseeff.\n\\newblock Stability and generalization.\n\\newblock {\\em The Journal of Machine Learning Research}, 2:499--526, 2002.\n\n\\bibitem{cai2014comparison}\nZhuhua Cai, Zekai~J Gao, Shangyu Luo, Luis~L Perez, Zografoula Vagena, and\n  Christopher Jermaine.\n\\newblock A comparison of platforms for implementing and running very large\n  scale machine learning algorithms.\n\\newblock In {\\em Proceedings of the 2014 ACM SIGMOD international conference\n  on Management of data}, pages 1371--1382. ACM, 2014.\n\n\\bibitem{chapelle2010semi-supervised}\nOlivier Chapelle, Bernhard Schlkopf, and Alexander Zien.\n\\newblock {\\em Semi-Supervised Learning}.\n\\newblock The MIT Press, 1st edition, 2010.\n\n\\bibitem{ching2015one}\nAvery Ching, Sergey Edunov, Maja Kabiljo, Dionysios Logothetis, and Sambavi\n  Muthukrishnan.\n\\newblock One trillion edges: graph processing at facebook-scale.\n\\newblock {\\em Proceedings of the VLDB Endowment}, 8(12):1804--1815, 2015.\n\n\\bibitem{cortes2008stability}\nCorinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi.\n\\newblock Stability of transductive regression algorithms.\n\\newblock In {\\em Proceedings of ICML}, pages 176--183. ACM, 2008.\n\n\\bibitem{el-yaniv_stable_2006}\nRan El-Yaniv and Dmitry Pechyony.\n\\newblock Stable transductive learning.\n\\newblock In {\\em Proceedings of COLT}, pages 35--49. Springer, 2006.\n\n\\bibitem{fergus2009semi-supervised}\nRob Fergus, Yair Weiss, and Antonio Torralba.\n\\newblock {Semi-Supervised Learning in Gigantic Image Collections}.\n\\newblock In {\\em Proceedings of NIPS}, pages 522--530, 2009.\n\n\\bibitem{garcke2005semi-supervised}\nJochen Garcke and Michael Griebel.\n\\newblock Semi-supervised learning with sparse grids.\n\\newblock In {\\em Proc. of the 22nd ICML Workshop on Learning with Partially\n  Classified Training Data}, 2005.\n\n\\bibitem{Gleich2015robustifying}\nDavid~F. Gleich and Michael~W. Mahoney.\n\\newblock Using local spectral methods to robustify graph-based learning\n  algorithms.\n\\newblock In {\\em Proceedings of the 21th ACM SIGKDD International Conference\n  on Knowledge Discovery and Data Mining}, KDD '15, pages 359--368, New York,\n  NY, USA, 2015. ACM.\n\n\\bibitem{jebara2009graph}\nTony Jebara, Jun Wang, and Shih-Fu Chang.\n\\newblock Graph construction and b-matching for semi-supervised learning.\n\\newblock In {\\em Proceedings of the 26th Annual International Conference on\n  Machine Learning}, pages 441--448. ACM, 2009.\n\n\\bibitem{yang2012simple}\nMing Ji, Tianbao Yang, Binbin Lin, Rong Jin, and Jiawei Han.\n\\newblock {A Simple Algorithm for Semi-supervised Learning with Improved\n  Generalization Error Bound}.\n\\newblock In {\\em Proceedings of ICML}, June 2012.\n\n\\bibitem{kapralov_single_2014}\nMichael Kapralov, Yin~Tat Lee, Christopher Musco, and Aaron Sidford.\n\\newblock Single pass spectral sparsification in dynamic streams.\n\\newblock In {\\em Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual\n  Symposium on}, pages 561--570. IEEE, 2014.\n\n\\bibitem{kelner_spectral_2013}\nJonathan~A. Kelner and Alex Levin.\n\\newblock Spectral {Sparsification} in the {Semi}-streaming {Setting}.\n\\newblock {\\em Theory of Computing Systems}, 53(2):243--262, August 2013.\n\n\\bibitem{koutis2012improved}\nIoannis Koutis, Alex Levin, and Richard Peng.\n\\newblock Improved spectral sparsification and numerical algorithms for sdd\n  matrices.\n\\newblock In {\\em STACS'12 (29th Symposium on Theoretical Aspects of Computer\n  Science)}, volume~14, pages 266--277. LIPIcs, 2012.\n\n\\bibitem{koutis2011a-nearly-m}\nIoannis Koutis, Gary~L. Miller, and Richard Peng.\n\\newblock A nearly-m log n time solver for {SDD} linear systems.\n\\newblock In {\\em {IEEE} 52nd Annual Symposium on Foundations of Computer\n  Science, {FOCS}}, pages 590--598, 2011.\n\n\\bibitem{kumar2012sampling}\nSanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.\n\\newblock Sampling methods for the {N}ystr\\\"{o}m method.\n\\newblock {\\em J. Mach. Learn. Res.}, 13(1):981--1006, April 2012.\n\n\\bibitem{leskovec2005graphs}\nJure Leskovec, Jon Kleinberg, and Christos Faloutsos.\n\\newblock Graphs over time: densification laws, shrinking diameters and\n  possible explanations.\n\\newblock In {\\em Proceedings of the eleventh ACM SIGKDD international\n  conference on Knowledge discovery in data mining}, pages 177--187. ACM, 2005.\n\n\\bibitem{liu2010large}\nWei Liu, Junfeng He, and Shih-Fu Chang.\n\\newblock Large graph construction for scalable semi-supervised learning.\n\\newblock In {\\em Proceedings of the 27th international conference on machine\n  learning (ICML-10)}, pages 679--686, 2010.\n\n\\bibitem{saluja2014graph}\nAvneesh Saluja, Hany Hassan, Kristina Toutanova, and Chris Quirk.\n\\newblock Graph-based semi-supervised learning of translation models from\n  monolingual data.\n\\newblock In {\\em Proceedings of the 52nd Annual Meeting of the Association for\n  Computational Linguistics (ACL), Baltimore, Maryland, June}, 2014.\n\n\\bibitem{spielman2011graph}\nDaniel~A. Spielman and Nikhil Srivastava.\n\\newblock Graph sparsification by effective resistances.\n\\newblock {\\em SIAM Journal on Computing}, 40(6):1913--1926, 2011.\n\n\\bibitem{subramanya2014graph}\nAmarnag Subramanya and Partha~Pratim Talukdar.\n\\newblock Graph-based semi-supervised learning.\n\\newblock {\\em Synthesis Lectures on Artificial Intelligence and Machine\n  Learning}, 8(4):1--125, 2014.\n\n\\bibitem{talwalkar2008large-scale}\nAmeet Talwalkar, Sanjiv Kumar, and Henry~A. Rowley.\n\\newblock Large-scale manifold learning.\n\\newblock In {\\em Computer Vision and Pattern Recognition (CVPR)}, 2008.\n\n\\bibitem{tsang2006large-scale}\nIvor~W. Tsang and James~T. Kwok.\n\\newblock Large-scale sparsified manifold regularization.\n\\newblock In Bernhard Sch{\\\"o}lkopf, John~C. Platt, and Thomas Hoffman,\n  editors, {\\em NIPS}, pages 1401--1408. MIT Press, 2006.\n\n\\bibitem{yu2005blockwise}\nKai Yu and Shipeng Yu.\n\\newblock Blockwise supervised inference on large graphs.\n\\newblock In {\\em Proc. of the 22nd ICML Workshop on Learning}, 2005.\n\n\\bibitem{zhu2008semi-supervised}\nXiaojin Zhu.\n\\newblock {Semi-Supervised Learning Literature Survey}.\n\\newblock Technical Report 1530, U. of Wisconsin-Madison, 2008.\n\n\\bibitem{zhu2003semi-supervised}\nXiaojin Zhu, Zoubin Ghahramani, and John Lafferty.\n\\newblock {Semi-Supervised Learning Using Gaussian Fields and Harmonic\n  Functions}.\n\\newblock In {\\em Proceedings of ICML}, pages 912--919, 2003.\n\n\\bibitem{zhu2005harmonic}\nXiaojin Zhu and John~D. Lafferty.\n\\newblock Harmonic mixtures: combining mixture models and graph-based methods\n  for inductive and scalable semi-supervised learning.\n\\newblock In {\\em Proceedings of ICML}, pages 1052--1059, 2005.\n\n\\end{thebibliography}\n\n\\bibliographystyle{plain}\n\n\n", "itemtype": "equation", "pos": 42238, "prevtext": "\n\nwhere in the last step we applied Eq.~\\ref{eq:lower.bound} on both ${\\widehat{{A}}}^{-1}$ and ${\\widetilde{{A}}}^{-1}$. We are left with ${\\Vert {{\\widehat{{A}}} -{\\widetilde{{A}}}} \\Vert}^2 = {\\Vert {P_{{\\mathcal{F}}} l \\gamma (L_{{\\mathcal{G}}} - L_{{\\mathcal{H}}})} \\Vert}^2$. We first recall that $P_{\\mathcal{F}} = L_{\\mathcal{G}}^+ L_{\\mathcal{G}} = L_{{\\mathcal{G}}}^{-1/2}L_{{\\mathcal{G}}}L_{{\\mathcal{G}}}^{-1/2}$ (and equivalently with ${\\mathcal{G}}$ replaced by ${\\mathcal{H}}$) and we introduce ${\\widetilde{{P}}}_{{\\mathcal{F}}} = L_{{\\mathcal{G}}}^{-1/2}L_{{\\mathcal{H}}}L_{{\\mathcal{G}}}^{-1/2}$. We have\n\n\n", "index": 25, "text": "\\begin{align*}\n    {\\Vert {{\\widehat{{A}}} -{\\widetilde{{A}}}} \\Vert}^2 &\\stackrel{(1)}{=} l^2 \\gamma^2 {\\Vert { L_{{\\mathcal{G}}} - L_{{\\mathcal{H}}}} \\Vert}^2\\\\\n    &\\stackrel{(2)}{=} l^2 \\gamma^2 {\\Vert {L_{{\\mathcal{G}}}^{1/2}(P_{{\\mathcal{F}}} - {\\widetilde{{P}}}_{{\\mathcal{F}}})L_{{\\mathcal{G}}}^{1/2}} \\Vert}^2 \\\\\n&\\stackrel{(3)}{\\leq} l^2 \\gamma^2 \\lambda_n({\\mathcal{G}})^2 {\\Vert {P_{{\\mathcal{F}}} - {\\widetilde{{P}}}_{{\\mathcal{F}}}} \\Vert}^2 \\stackrel{(4)}{\\leq} l^2 \\gamma^2 \\lambda_n^2 \\varepsilon^2,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\|{{\\widehat{{A}}}-{\\widetilde{{A}}}}\\|}^{2}\" display=\"inline\"><msup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>A</mi><mo>^</mo></mover><mo>-</mo><mover accent=\"true\"><mi>A</mi><mo>~</mo></mover></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\stackrel{(1)}{=}l^{2}\\gamma^{2}{\\|{L_{{\\mathcal{G}}}-L_{{%&#10;\\mathcal{H}}}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mover><mo movablelimits=\"false\">=</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">1</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><msup><mi>l</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></msub><mo>-</mo><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\stackrel{(2)}{=}l^{2}\\gamma^{2}{\\|{L_{{\\mathcal{G}}}^{1/2}(P_{{%&#10;\\mathcal{F}}}-{\\widetilde{{P}}}_{{\\mathcal{F}}})L_{{\\mathcal{G}}}^{1/2}}\\|}^{2}\" display=\"inline\"><mrow><mi/><mover><mo movablelimits=\"false\">=</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">2</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><msup><mi>l</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msubsup><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mi>P</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\stackrel{(3)}{\\leq}l^{2}\\gamma^{2}\\lambda_{n}({\\mathcal{G}})^{2}%&#10;{\\|{P_{{\\mathcal{F}}}-{\\widetilde{{P}}}_{{\\mathcal{F}}}}\\|}^{2}\\stackrel{(4)}{%&#10;\\leq}l^{2}\\gamma^{2}\\lambda_{n}^{2}\\varepsilon^{2},\" display=\"inline\"><mrow><mrow><mi/><mover><mo movablelimits=\"false\">\u2264</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">3</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><msup><mi>l</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>n</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mi>P</mi><mo>~</mo></mover><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mover><mo movablelimits=\"false\">\u2264</mo><mrow><mo maxsize=\"142%\" minsize=\"142%\">(</mo><mn mathsize=\"142%\">4</mn><mo maxsize=\"142%\" minsize=\"142%\">)</mo></mrow></mover><mrow><msup><mi>l</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03b3</mi><mn>2</mn></msup><mo>\u2062</mo><msubsup><mi>\u03bb</mi><mi>n</mi><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>\u03b5</mi><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]