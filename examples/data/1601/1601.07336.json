[{"file": "1601.07336.tex", "nexttext": "\nOnce the problem is solved, the class of given test sample can be found as the class that best represents it using the corresponding training data in class-wise way. That is, let $\\mathbf{c}_i^*$ be a vector whose only nonzero entries are the entries in $\\mathbf{c}_i$ that are associated with class $i$, we can adopt the following rule to determine $\\mathbf{x}$ as class $j$ that has the minimum residual.\n\n", "itemtype": "equation", "pos": 12959, "prevtext": "\n\\title{Neighborhood Preserved Sparse Representation for Robust Classification on Symmetric Positive Definite Matrices}\n\\author{Ming Yin, ~Shengli Xie, ~Yi Guo, ~Junbin Gao and Yun Zhang \n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem Ming Yin, Shengli Xie and Yun Zhang are with School of Automation, Guangdong University of Technology, Guangzhou, 510006, China.\nE-mail: yiming@gdut.edu.cn, shlxie@gdut.edu.cn, zy@gdut.edu.cn \\protect\n\\IEEEcompsocthanksitem Yi Guo is Commonwealth Scientific and Industrial Research Organisation, North Ryde, NSW 1670, Australia. E-mail: yi.guo@csiro.au\\protect\n\\IEEEcompsocthanksitem Junbin Gao is with The University of Sydney Business School, The University of Sydney, Camperdown, NSW 2006, Australia.\nE-mail: junbin.gao@sydney.edu.au.}\n\\thanks{Manuscript received xxx; revised xxx.}}\n\n\n\n\\maketitle\n\\begin{abstract}\nDue to its promising classification performance, sparse representation based classification(SRC) algorithm has attracted great attention in the past few years. However, the existing SRC type methods apply only to vector data in Euclidean space. As such, there is still no satisfactory approach to conduct classification task for symmetric positive definite (SPD) matrices which is very useful in computer vision. To address this problem, in this paper, a neighborhood preserved kernel SRC method is proposed on SPD manifolds. Specifically, by embedding the SPD matrices into a Reproducing Kernel Hilbert Space (RKHS), the proposed method can perform classification on SPD manifolds through an appropriate Log-Euclidean kernel. Through exploiting the geodesic distance between SPD matrices, our method can effectively characterize the intrinsic local Riemannian geometry within data so as to well unravel the underlying sub-manifold structure. Despite its simplicity, experimental results on several famous database demonstrate that the proposed method achieves better classification results than the state-of-the-art approaches.\n\\end{abstract}\n\\begin{IEEEkeywords}\nsparse representation based classification, ~Riemannian manifold, ~multi-manifold, ~intrinsic geometry, ~geodesic distance.\n\\end{IEEEkeywords}\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\nIn the past a few years, inspired by advances in $\\ell_0$-norm and $\\ell_1$-norm techniques, sparse representation{\\cite}{DonohoEladTemlyakov2006} has been widely applied in computer vision, such as image segmentation{\\cite}{YangWrightMaSastry2008}, image deblurring{\\cite}{YinGaoTienCai2014} and face recognition{\\cite}{WrightYangGaneshSastryMa2009}. It is worth noting that Wright \\emph{et al}. proposed a sparse representation based classification(SRC) {\\cite}{WrightYangGaneshSastryMa2009} to classify facial images, which is the first time to exploit the discriminative nature of sparse represeor face recognition.\nIn fact, facial images have a high dimensionality, which usually lie on a low-dimensional subspace or sub-manifold. Thus, Yang \\emph{et al}. {\\cite}{YangChuZhangXuYang2013} proposed a novel dimensionality reduction method that adopts SRC as a criterion to steer the design of a feature extraction method. In addition, many high-dimensional data in real world may be better modeled by nonlinear manifolds. To overcome the nonlinear obstruction, some researches suggest to map these data into a kernel feature space by using some nonlinear mapping, and then SRC is performed in this new feature space by utilizing kernel trick {\\cite}{ZhangZhouChangLiuYanWangLi2012}{\\cite}{YinLiuJinYang2012}.\n\nHowever, most of the above work mainly focuses on the problem associated with vector-valued data. The higher-order signals like images (2D, 3D or higher) have to be dealt with primarily by vectorizing them and applying any of the available vector techniques. As a result, such type vector features cannot efficiently characterize the high-dimensional data in computer vision, machine learning and medical image analysis{\\cite}{YinGaoGuo2015}. Concretely, in traditional sparse representation based classification, the sparsity representation for each query image is attained by a dictionary composed of all gallery data across all classes in a \\emph{linear combination} way. Recent advance{\\cite}{TuzelPorikliMeer2008} suggests that encoding images through symmetric positive definite (SPD) matrices and then interpreting such matrices as points on Riemannian manifolds can lead to promising classification performance. For instance, the human facial images are regarded as samples from a nonlinear sub-manifold{\\cite}{WangShanChenGao2008}. Unfortunately, the \\emph{linear combination} is not applicable to this case where data may be better modeled by nonlinear manifolds {\\cite}{JayasumanaHartleySalzmannLiHarandi2013}{\\cite}{HarandiHartleyLovellSanderson2014}. In other words, the direct applications of linear combination model to matrix-valued data will result in the comprised performance as inaccurate representation. Consequently, the traditional SRC is also no longer available to classification on SPD matrices as points on Riemannian manifolds.\n\nTo address this problem, a few solutions have been recently proposed to generalize sparse coding problems to Riemannian manifolds, such as {\\cite}{CherianSra2014}{\\cite}{HoXieVemuri2013}{\\cite}{SivalingamBoleyMorellasPapanikolopoulos2014}. The most common approach is to calculate the tangent space to the manifold at the mean of the data points so as to obtain a Euclidean approximation of the manifold{\\cite}{TuzelPorikliMeer2008}. Inspired by this idea, Ho \\emph{et al}. {\\cite}{HoXieVemuri2013} firstly proposed a nonlinear generalization of sparse coding to handle the non-linearity of Riemannian manifolds, via flattening a SPD manifold using a fixed tangent space. In order to further measure the representation error effectively, in {\\cite}{SivalingamBoleyMorellasPapanikolopoulos2014}, a tensor sparse coding framework was proposed for positive definite matrices based on the log-determinant divergence (Burg loss). Instead of using extrinsic similarity measures as work{\\cite}{SivalingamBoleyMorellasPapanikolopoulos2014}, the authors{\\cite}{CherianSra2014} proposed to use the intrinsic Riemannian distance on the manifold of SPD matrices. Although locally flattening Riemannian manifolds via tangent spaces can handle their non-linearity, it inevitably leads to very demanding computation due to switching back and forth between tangent spaces and the manifold{\\cite}{HarandiHartleyLovellSanderson2014}. Furthermore, linear reconstruction of SPD matrices is not as natural as in Euclidean space and this may incur errors {\\cite}{LiWangZuoZhang2013}. On the other line, to address this nonlinear problems via LRR, a nonlinear LRR model is proposed to extend the traditional LRR from Euclidean space to Stiefel manifold \\cite{YinGaoGuo2015}, SPD manifold \\cite{FuGaoHongTien2015} and abstract Grassmann manifold {\\cite}{WangHuGaoSunYin2015} respectively. Low-rank representation based method, however, often suffers from high computational complexities as the nuclear norm regularized optimizing. From this view point, sparse representation based method can readily reduce the computational complexities greatly due to only solving $\\ell_0$-norm optimization problems rather than nuclear-norm ones.\n\nThe existing sparse representation methods on SPD matrices can been shown to be effective for classification{\\cite}{HarandiHartleyLovellSanderson2014}{\\cite}{LiWangZuoZhang2013}, however, there still remain questions about classification in the multiple sub-manifolds setting{\\cite}{WangSlavakisLerman2015} with sparse representation. As SPD matrices are often that low-dimensional data embedded in high-dimensional non-Euclidean spaces, their underlying sub-manifolds are geodesic and referred to Riemannian multi-manifolds. Let $X$ be a SPD matrix and hence a point on $\\mathcal{S}_d^+$, it can be assumed residing on the tubular neighborhood of some unknown geodesic sub-manifold $\\mathcal{M}_k (1 \\leq k \\leq K)$, of a Riemannian manifold. As for this issue, sparse representation based classification {\\cite}{HarandiHartleyLovellSanderson2014}{\\cite}{LiWangZuoZhang2013} has not been sufficiently explored yet. Another reason, may not trivial, the sparse coding coefficients may vary a lot even for similar query samples in classification task as the mechanism of $\\ell_1$-minimization. As a result, the unsatisfied recognition rate will be achieved. Motivated by these observations, in this paper, we propose a neighborhood preserved sparse representation for robust classification on SPD matrices. When encoding the query sample, we aim to use the training samples lying in its vicinity as the training samples and the query sample may reside in the same sub-manifold leading to a better classification performance. Despite its simplicity, the proposed method performs well for classification task. Specifically, to thoroughly exploit the intrinsic geometry among data on Riemannian manifold, a neighborhood preserved prior induced from the geodesic distance, besides the sparsity, is imposed on the sparse coefficients so that the similar query data produce similar sparse codes.\n\nThe main contributions in our paper are summarized below.\n\\begin{enumerate}\n\\item To our best knowledge, it is the first attempts to formulate the local consistency into the sparse coding paradigm over a Riemannian manifold via embedding them into RKHS. It is significantly different from the work in {\\cite}{ZhangZhouChangLiuYanWangLi2012}{\\cite}{YinLiuJinYang2012} as the latter did not consider the Riemannian geometry structure within data.\n\\item To efficiently measure the neighborhood between data points on Riemannian manifold, we compare the two geodesic distance under Stein metric and Log-Euclidean metric, respectively. To our best knowledge, this is one of the first attempts, from the weighted structured perspective, to compare the benefit from this two metrics for analyzing SPD matrices.\n\\item We apply our proposed methods to several classification tasks where the data are depicted as region covariance matrices.\n\\end{enumerate}\n\nThe remainder of this paper is organized as follows. In Section\n\\ref{SectionII}, we give a brief review on the related works. Section\n\\ref{SectionIII} is dedicated to introducing our novel neighborhood preserved kernel SRC, termed as NPKSRC. Section \\ref{SectionIV} presents experimental results on image classification tasks. Finally, Section \\ref{SectionV} concludes our paper and also provides the directions for future improvements.\n\n\\section{Related Work}\\label{SectionII}\nBefore we introduce our model, in this section, we briefly review the recent development of sparse representation based classification methods{\\cite}{WrightYangGaneshSastryMa2009}{\\cite}{HoXieVemuri2013}\nand the analysis of Riemannian geometry of SPD manifold{\\cite}{PennecFillardAyache2006}. For convenience, Table \\ref{Notation} gives the notation used throughout this paper.\n\n\n\\begin{table}\n\\begin{center}\n\\caption{Notation used in this paper}\n\\begin{tabular}{ll}\n\\hline\n\\textbf{Notation}&  \\textbf{Description} \\\\\n\\hline\n$X$& data matrix \\\\\n$\\mathcal{X}$& 3D matrix or 3-order tensor \\\\\n$\\textbf{x}$& column vector\\\\\n${x}_i$& the element at position $i$ of vector $\\textbf{x}$\\\\\n$X_{ij}$& the $(i,j)$-th entry of matrix $X$\\\\\n$\\| \\textbf{x} \\|_1$ & $\\ell_1$ norm of  $\\textbf{x} $\\\\\n$\\|\\textbf{x} \\|_2$&  $\\ell_2$ norm of  $\\textbf{x} $\\\\\n$T$&  transpose operator\\\\\n$ \\left\\| \\cdot \\right\\|_F $&  matrix Frobenius norm defined as $\\left\\| X \\right\\|_F^2 = \\sum\\limits_{i } {\\sum\\limits_{j} {\\left| {X_{ij} } \\right|^2 }}$ \\\\\n$\\|X\\|_*$& nuclear norm of $X$ defined by the sum of its singular values\\\\\n$\\textrm{tr}( \\cdot )$& matrix trace operator\\\\\n$\\mathcal{S}_d^+$& space of $d \\times d$ SPD matrices\\\\\nLog map & principal matrix logarithm \\\\\n$\\mathbf{log}_X(\\cdot)$& Log map from SPD manifold to a tangent space at $X$\\\\\n$T_X \\mathcal{S}_d^+$& tangent space at a point $X$ on $\\mathcal{S}_d^+$, \\\\\n& which is a vector space including the tangent vectors\\\\\n& of all possible curves passing over $X$. \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\\label{Notation}\n\n\\subsection{Classification via Sparse Representation}\nSparse representation based classification(SRC) has been well-known as its robustness to face recognition{\\cite}{WrightYangGaneshSastryMa2009}. Suppose that there exist $n$ classes and $m_i$ training data for each class $i$. We denote by $Y_i$ the collection of training data in the $i$-th class and $Y= [Y_1, Y_2,..., Y_n]\\in \\mathcal{R}^{d \\times N}, N= \\sum_{i=1}^n{ m_i }$ by the collection of all training data over all  classes. Given a test sample $\\mathbf{x} \\in \\mathcal{R}^d$, which belongs to one of the $n$ classes, the goal of SRC is to find out the class to which $\\mathbf{x}$ belongs, by seeking its sparsest representation over all training data.\n\nConcretely, the SRC solves the following optimization problem.\n", "index": 1, "text": "\n\\[\n\\mathop{\\min}\\limits_{\\mathbf{c}} ~\\frac1{2}\\|\\mathbf{x} - Y\\mathbf{c} \\|_2^2 + \\lambda \\| \\mathbf{c} \\|_1.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathop{\\min}\\limits_{\\mathbf{c}}~{}\\frac{1}{2}\\|\\mathbf{x}-Y\\mathbf{c}\\|_{2}^%&#10;{2}+\\lambda\\|\\mathbf{c}\\|_{1}.\" display=\"block\"><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder></mpadded><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><mrow><mi>Y</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1c</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\n\\subsection{Riemannian Geometry on SPD Matrices}\nIn general, SPD matrices lie on a non-flat Riemannian manifold, whose structure is suitably characterized by the geodesic distance induced by Riemannian metric. That is, a natural way to measure closeness of data on a Riemannian manifold is geodesics, eg. curves analogous to straight lines in $\\mathds{R}^n$. For any two data points on a manifold, geodesic distance is the length of the shortest curve on the manifold connecting them. For this reason, there are, currently, two popular distance measures in $\\mathcal{S}_d^+$. One is the affine invariant Riemannian metric (AIRM) and the other is Log-Euclidean metric.\n\nAs one of true metrics of geodesic distance, AIRM is probably the most widely used Riemannian metric defined as follows{\\cite}{PennecFillardAyache2006}. Given $X \\in \\mathcal{S}_d^+$, the AIRM of two tangent vectors $\\mathbf{v}, \\mathbf{w} \\in T_X \\mathcal{S}_d^+$ is defined as\n\n", "itemtype": "equation", "pos": 13481, "prevtext": "\nOnce the problem is solved, the class of given test sample can be found as the class that best represents it using the corresponding training data in class-wise way. That is, let $\\mathbf{c}_i^*$ be a vector whose only nonzero entries are the entries in $\\mathbf{c}_i$ that are associated with class $i$, we can adopt the following rule to determine $\\mathbf{x}$ as class $j$ that has the minimum residual.\n\n", "index": 3, "text": "\\begin{align}\n\\text{label}(\\mathbf{x})= \\mathop{\\arg\\min}\\limits_{j} ~\\frac1{2}\\|\\mathbf{x} - Y_j\\mathbf{c}_j^* \\|_2^2.\\label{rule}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{label}(\\mathbf{x})=\\mathop{\\arg\\min}\\limits_{j}~{}\\frac{1}{%&#10;2}\\|\\mathbf{x}-Y_{j}\\mathbf{c}_{j}^{*}\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><mrow><mtext>label</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder><mrow><mi>arg</mi><mo movablelimits=\"false\">\u2061</mo><mi>min</mi></mrow><mi>j</mi></munder></mpadded><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><mrow><msub><mi>Y</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc1c</mi><mi>j</mi><mo>*</mo></msubsup></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nThe geodesic distance between points $X, Y \\in \\mathcal{S}_d^+$  induced from AIRM is then\n\n", "itemtype": "equation", "pos": 14574, "prevtext": "\n\n\\subsection{Riemannian Geometry on SPD Matrices}\nIn general, SPD matrices lie on a non-flat Riemannian manifold, whose structure is suitably characterized by the geodesic distance induced by Riemannian metric. That is, a natural way to measure closeness of data on a Riemannian manifold is geodesics, eg. curves analogous to straight lines in $\\mathds{R}^n$. For any two data points on a manifold, geodesic distance is the length of the shortest curve on the manifold connecting them. For this reason, there are, currently, two popular distance measures in $\\mathcal{S}_d^+$. One is the affine invariant Riemannian metric (AIRM) and the other is Log-Euclidean metric.\n\nAs one of true metrics of geodesic distance, AIRM is probably the most widely used Riemannian metric defined as follows{\\cite}{PennecFillardAyache2006}. Given $X \\in \\mathcal{S}_d^+$, the AIRM of two tangent vectors $\\mathbf{v}, \\mathbf{w} \\in T_X \\mathcal{S}_d^+$ is defined as\n\n", "index": 5, "text": "\\begin{align*}\n&\\langle \\mathbf{v}, \\mathbf{w}  \\rangle = \\langle X^{-1/2}\\mathbf{v} X^{-1/2}, X^{-1/2} \\mathbf{w} X^{-1/2} \\rangle \\\\\n&= \\textrm{tr} ( X^{-1} \\mathbf{v}  X^{-1} \\mathbf{w} ).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\mathbf{v},\\mathbf{w}\\rangle=\\langle X^{-1/2}\\mathbf{v}X^{%&#10;-1/2},X^{-1/2}\\mathbf{w}X^{-1/2}\\rangle\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\ud835\udc2f</mi><mo>,</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>\ud835\udc2f</mi><mo>\u2062</mo><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo>,</mo><mrow><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>\ud835\udc30</mi><mo>\u2062</mo><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\textrm{tr}(X^{-1}\\mathbf{v}X^{-1}\\mathbf{w}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc2f</mi><mo>\u2062</mo><msup><mi>X</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\nHowever, the above distance induced by AIRM is computationally intensive resulting in a significant numerical burden. To overcome this drawback of AIRM, Log-Euclidean metric is defined on the Lie group of SPD matrices corresponding to a Euclidean metric in the logarithmic domain. Specifically, the distance under Log-Euclidean metric is denoted by,\n\n", "itemtype": "equation", "pos": 14870, "prevtext": "\nThe geodesic distance between points $X, Y \\in \\mathcal{S}_d^+$  induced from AIRM is then\n\n", "index": 7, "text": "\\begin{align}\n&\\delta_g (X, Y) = \\|\\mathbf{log} (X^{-1/2}Y X^{-1/2}) \\|_F . \\label{gAIRM}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\delta_{g}(X,Y)=\\|\\mathbf{log}(X^{-1/2}YX^{-1/2})\\|_{F}.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>\u03b4</mi><mi>g</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>Y</mi><mo>\u2062</mo><msup><mi>X</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\n\\subsection{Spare Representation on SPD Matrices}\nSince SPD matrices belong to a Lie group which is a Riemannian manifold {\\cite}{ArsignyFillardPennecAyache2007}, it cripples many methods that rely on linear reconstruction. Generally, there are two methods to deal with the non-linearity of Riemannian manifolds. One is to locally flatten the manifold to tangent spaces{\\cite}{TuzelPorikliMeer2008}. The underlying idea is to exploit the geometry of the manifold directly. The other is to map the data into a feature space usually a Hilbert space {\\cite}{JayasumanaHartleySalzmannLiHarandi2013}. Precisely, it is to project the data into RKHS through kernel mapping {\\cite}{HarandiSandersonHartleyLovell2012}. Both of these methods are seeking a transformation so that the linearity re-emerges.\n\nA typical example of the former method is the one in {\\cite}{HoXieVemuri2013}. Let $X$ be a SPD matrix and hence a point on $\\mathcal{S}_d^+$. $\\mathcal{D} = \\{D_1, D_2, ..., D_N \\}, D_i \\in \\mathcal{S}_d^+$ is a dictionary. An optimization problem for sparse coding of  $X$ on a manifold $\\mathcal{M}$ is formulated as follows\n\n", "itemtype": "equation", "pos": 15323, "prevtext": "\n\nHowever, the above distance induced by AIRM is computationally intensive resulting in a significant numerical burden. To overcome this drawback of AIRM, Log-Euclidean metric is defined on the Lie group of SPD matrices corresponding to a Euclidean metric in the logarithmic domain. Specifically, the distance under Log-Euclidean metric is denoted by,\n\n", "index": 9, "text": "\\begin{align}\n\\delta_l(X,Y) =  \\|\\mathbf{log}(X)- \\mathbf{log}(Y)\\|_F. \\label{gLE}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\delta_{l}(X,Y)=\\|\\mathbf{log}(X)-\\mathbf{log}(Y)\\|_{F}.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>\u03b4</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathbf{log}_X(\\cdot)$ denotes Log map from SPD manifold to a tangent space at $X$,  $\\mathbf{c} = [c_1, c_2,..., c_N]$ is the sparse vector and $\\|\\cdot\\|_X$ is the norm associated with $T_X \\mathcal{S}_d^+$. Because $\\mathbf{log}_X(X)=\\textbf{0}$, the second term in Eq.\\eqref{SRManifold} is essentially the error of linearly reconstructing $\\mathbf{log}_X(X)$ by others on the tangent space of $X$. \n\nAlthough locally flattening Riemannian manifolds via tangent spaces{\\cite}{HoXieVemuri2013} can handle their non-linearity, it inevitably leads to very demanding computation due to\nswitching back and forth between tangent spaces and the manifold. Furthermore, linear reconstruction of SPD matrices is not as natural as in Euclidean space and this may incur errors. Thus, the kernel-based sparse coding on SPD matrices is proposed as follows{\\cite}{HarandiSalzmann2015}.\n\n", "itemtype": "equation", "pos": 16543, "prevtext": "\n\n\\subsection{Spare Representation on SPD Matrices}\nSince SPD matrices belong to a Lie group which is a Riemannian manifold {\\cite}{ArsignyFillardPennecAyache2007}, it cripples many methods that rely on linear reconstruction. Generally, there are two methods to deal with the non-linearity of Riemannian manifolds. One is to locally flatten the manifold to tangent spaces{\\cite}{TuzelPorikliMeer2008}. The underlying idea is to exploit the geometry of the manifold directly. The other is to map the data into a feature space usually a Hilbert space {\\cite}{JayasumanaHartleySalzmannLiHarandi2013}. Precisely, it is to project the data into RKHS through kernel mapping {\\cite}{HarandiSandersonHartleyLovell2012}. Both of these methods are seeking a transformation so that the linearity re-emerges.\n\nA typical example of the former method is the one in {\\cite}{HoXieVemuri2013}. Let $X$ be a SPD matrix and hence a point on $\\mathcal{S}_d^+$. $\\mathcal{D} = \\{D_1, D_2, ..., D_N \\}, D_i \\in \\mathcal{S}_d^+$ is a dictionary. An optimization problem for sparse coding of  $X$ on a manifold $\\mathcal{M}$ is formulated as follows\n\n", "index": 11, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}} ~\\|\\mathbf{w}\\|_1 + \\lambda \\left\\| \\sum\\limits_{i = 1}^N {c}_{i}\\mathbf{log}_{X}(D_i)  \\right\\|_{X}^2, ~\\textrm{ s.t. } \\sum\\limits_{i = 1}^N c_{i}=1,\\label{SRManifold}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}~{}\\|\\mathbf{w}\\|_{1}+\\lambda%&#10;\\left\\|\\sum\\limits_{i=1}^{N}{c}_{i}\\mathbf{log}_{X}(D_{i})\\right\\|_{X}^{2},~{}%&#10;\\textrm{ s.t. }\\sum\\limits_{i=1}^{N}c_{i}=1,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder></mpadded><msub><mrow><mo>\u2225</mo><mi>\ud835\udc30</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>X</mi><mn>2</mn></msubsup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>c</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\phi( \\cdot )$ denotes a feature mapping function that projects SPD matrices into RKHS such that $\\langle \\phi(X),\\phi(Y) \\rangle = \\kappa(X,Y)$ where $\\kappa(X,Y)$ is a positive definite (PD) kernel.\n\n\\section{Kernel Sparse Representation on SPD Matrices via Neighborhood Preserved}\\label{SectionIII}\nHowever, the constraint of $\\ell_1$-norm sparsity is beneficial to classification task, a test input might be reconstructed by training images, i.e., codewords, which are far away from the test sample{\\cite}{WangYangYuLvHuangGong2010}. As a consequence, the SRC type methods will produce unsatisfying classification results. In addition, data of SPD matrices are often modeled as a union of low-dimensional sub-manifolds{\\cite}{WangSlavakisLerman2015}. Under this context, classification algorithms aim at partitioning data based on the underlying low-dimensional  non-Euclidean spaces. Therefore, the neighborhood of each data on Riemannian manifold can be fit by a geodesic sub-manifold model.\n\nMotivated by the above issues, in this section, we propose a neighborhood preserved kernel sparse representation based classification (termed as NPKSRC) algorithm on SPD matrices, by considering the structure within data points. The formulation can be written as following.\n\n", "itemtype": "equation", "pos": 17654, "prevtext": "\nwhere $\\mathbf{log}_X(\\cdot)$ denotes Log map from SPD manifold to a tangent space at $X$,  $\\mathbf{c} = [c_1, c_2,..., c_N]$ is the sparse vector and $\\|\\cdot\\|_X$ is the norm associated with $T_X \\mathcal{S}_d^+$. Because $\\mathbf{log}_X(X)=\\textbf{0}$, the second term in Eq.\\eqref{SRManifold} is essentially the error of linearly reconstructing $\\mathbf{log}_X(X)$ by others on the tangent space of $X$. \n\nAlthough locally flattening Riemannian manifolds via tangent spaces{\\cite}{HoXieVemuri2013} can handle their non-linearity, it inevitably leads to very demanding computation due to\nswitching back and forth between tangent spaces and the manifold. Furthermore, linear reconstruction of SPD matrices is not as natural as in Euclidean space and this may incur errors. Thus, the kernel-based sparse coding on SPD matrices is proposed as follows{\\cite}{HarandiSalzmann2015}.\n\n", "index": 13, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}} ~\\|\\mathbf{c}\\|_1 + \\lambda \\left\\| \\phi(X)-  \\sum\\limits_{i = 1}^N {c}_{i}\\phi(D_i)  \\right\\|_F^2, ~\\textrm{ s.t. } \\sum\\limits_{i = 1}^N c_{i}=1,\\label{KSRManifold}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}~{}\\|\\mathbf{c}\\|_{1}+\\lambda%&#10;\\left\\|\\phi(X)-\\sum\\limits_{i=1}^{N}{c}_{i}\\phi(D_{i})\\right\\|_{F}^{2},~{}%&#10;\\textrm{ s.t. }\\sum\\limits_{i=1}^{N}c_{i}=1,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder></mpadded><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1c</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>c</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\odot$ means element-wise multiplication and $\\mathbf{w}$ is a vector imposing restriction on the structure of the solution. Similar to the prior work{\\cite}{HarandiSalzmann2015}{\\cite}{HoXieVemuri2013}, the affine constraint is applied to our model too. Furthermore, by introducing structure constraint, i.e., $\\mathbf{w}$, we actually enforce a smaller weight on the samples belonging to the same sub-manifold with the test input, and vice versa. To some extent, the entries of $\\mathbf{w}$ are denoting the affinity between the test input and the training data. Then, how to choose a informative entries of $\\mathbf{w}$ is a key factor of success for the subsequent classification tasks.\n\n\\subsection{Analysis of the Weight Matrix}\nThe structure of data are often determined by using pairwise distance between data points {\\cite}{WangYangYuLvHuangGong2010}. Moreover, manifold learning (neighborhood preservation model) has been widely used for dimension reduction by learning and embedding local consistency\nof original data into a low-dimensional representation{\\cite}{RoweisSaul2000}{\\cite}{TenenbaumSilvaLangford2000}.\nFor simplicity, we assume only there exist a two-class data underlying geodesic sub-manifolds $\\mathcal{S}_1$ and $\\mathcal{S}_2$, respectively. Given a test input $X \\in \\mathcal{S}_d^+$, in general, there is a larger probability to assign it to that class determined by points lying on the sub-manifolds $\\mathcal{S}_1$ if the nearby points of $X$ is that points located on $\\mathcal{S}_1$. From this intuition, we can use the affinity, i.e., geodesic distance, between test input and training data to compute the weight. Concretely, $\\mathbf{w}$ is constructed in terms of the geodesic distance of $X$ from every training sample(a subset of $\\mathcal{D}$). As such, a locally smooth sparse code vector is achieved where the sparsity is a result of the neighborhood preserving since the training samples far away from $X$ do not contribute to its reconstruction. Therefore, in this paper, we utilize the geodesic distance, under Log-Euclidean metric, between a test input and training samples as the weight, illustrated as following.\n", "itemtype": "equation", "pos": 19161, "prevtext": "\nwhere $\\phi( \\cdot )$ denotes a feature mapping function that projects SPD matrices into RKHS such that $\\langle \\phi(X),\\phi(Y) \\rangle = \\kappa(X,Y)$ where $\\kappa(X,Y)$ is a positive definite (PD) kernel.\n\n\\section{Kernel Sparse Representation on SPD Matrices via Neighborhood Preserved}\\label{SectionIII}\nHowever, the constraint of $\\ell_1$-norm sparsity is beneficial to classification task, a test input might be reconstructed by training images, i.e., codewords, which are far away from the test sample{\\cite}{WangYangYuLvHuangGong2010}. As a consequence, the SRC type methods will produce unsatisfying classification results. In addition, data of SPD matrices are often modeled as a union of low-dimensional sub-manifolds{\\cite}{WangSlavakisLerman2015}. Under this context, classification algorithms aim at partitioning data based on the underlying low-dimensional  non-Euclidean spaces. Therefore, the neighborhood of each data on Riemannian manifold can be fit by a geodesic sub-manifold model.\n\nMotivated by the above issues, in this section, we propose a neighborhood preserved kernel sparse representation based classification (termed as NPKSRC) algorithm on SPD matrices, by considering the structure within data points. The formulation can be written as following.\n\n", "index": 15, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}} ~\\|\\mathbf{w} \\odot \\mathbf{c}\\|_1 + \\lambda \\left\\| \\phi(X)-  \\sum\\limits_{i = 1}^N {c}_{i}\\phi(D_i)  \\right\\|_F^2, ~\\textrm{ s.t. } \\sum\\limits_{i = 1}^N c_{i}=1,\\label{NPKSRC}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}~{}\\|\\mathbf{w}\\odot\\mathbf{c}\\|%&#10;_{1}+\\lambda\\left\\|\\phi(X)-\\sum\\limits_{i=1}^{N}{c}_{i}\\phi(D_{i})\\right\\|_{F}%&#10;^{2},~{}\\textrm{ s.t. }\\sum\\limits_{i=1}^{N}c_{i}=1,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder></mpadded><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc30</mi><mo>\u2299</mo><mi>\ud835\udc1c</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>c</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\n\\subsection{The Proposed Method}\nGiven the training data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N]$ on SPD manifold, the corresponding kernel sparse representation algorithm is formulated as following.\n\n", "itemtype": "equation", "pos": 21568, "prevtext": "\nwhere $\\odot$ means element-wise multiplication and $\\mathbf{w}$ is a vector imposing restriction on the structure of the solution. Similar to the prior work{\\cite}{HarandiSalzmann2015}{\\cite}{HoXieVemuri2013}, the affine constraint is applied to our model too. Furthermore, by introducing structure constraint, i.e., $\\mathbf{w}$, we actually enforce a smaller weight on the samples belonging to the same sub-manifold with the test input, and vice versa. To some extent, the entries of $\\mathbf{w}$ are denoting the affinity between the test input and the training data. Then, how to choose a informative entries of $\\mathbf{w}$ is a key factor of success for the subsequent classification tasks.\n\n\\subsection{Analysis of the Weight Matrix}\nThe structure of data are often determined by using pairwise distance between data points {\\cite}{WangYangYuLvHuangGong2010}. Moreover, manifold learning (neighborhood preservation model) has been widely used for dimension reduction by learning and embedding local consistency\nof original data into a low-dimensional representation{\\cite}{RoweisSaul2000}{\\cite}{TenenbaumSilvaLangford2000}.\nFor simplicity, we assume only there exist a two-class data underlying geodesic sub-manifolds $\\mathcal{S}_1$ and $\\mathcal{S}_2$, respectively. Given a test input $X \\in \\mathcal{S}_d^+$, in general, there is a larger probability to assign it to that class determined by points lying on the sub-manifolds $\\mathcal{S}_1$ if the nearby points of $X$ is that points located on $\\mathcal{S}_1$. From this intuition, we can use the affinity, i.e., geodesic distance, between test input and training data to compute the weight. Concretely, $\\mathbf{w}$ is constructed in terms of the geodesic distance of $X$ from every training sample(a subset of $\\mathcal{D}$). As such, a locally smooth sparse code vector is achieved where the sparsity is a result of the neighborhood preserving since the training samples far away from $X$ do not contribute to its reconstruction. Therefore, in this paper, we utilize the geodesic distance, under Log-Euclidean metric, between a test input and training samples as the weight, illustrated as following.\n", "index": 17, "text": "\n\\[ {w}_{i} = \\delta_l(Y_i,X)=  \\|\\mathbf{log}(Y_i)- \\mathbf{log}(X)\\|_F.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{w}_{i}=\\delta_{l}(Y_{i},X)=\\|\\mathbf{log}(Y_{i})-\\mathbf{log}(X)\\|_{F}.\" display=\"block\"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>\u03b4</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc25\ud835\udc28\ud835\udc20</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\nThrough expanding the $\\ell_2$-norm term and some algebra manipulations, we will consider the following problem that has a same solution to problem \\ref{NPKSRC2}. For clarity and completeness, the detailed derivation of the problem \\ref{NPKSRC2} can be found in the appendix.\n\n", "itemtype": "equation", "pos": 21839, "prevtext": "\n\n\\subsection{The Proposed Method}\nGiven the training data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N]$ on SPD manifold, the corresponding kernel sparse representation algorithm is formulated as following.\n\n", "index": 19, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}} ~ \\|\\text{diag}(\\mathbf{w})\\mathbf{c}\\|_1 + \\frac{\\lambda}{2} \\left\\| \\phi(X)-  \\sum\\limits_{i = 1}^N {c}_{i}\\phi(Y_i)  \\right\\|_2^2, ~\\textrm{ s.t. } \\sum\\limits_{i = 1}^N c_{i}=1,\\label{NPKSRC2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}~{}\\|\\text{diag}(\\mathbf{w})%&#10;\\mathbf{c}\\|_{1}+\\frac{\\lambda}{2}\\left\\|\\phi(X)-\\sum\\limits_{i=1}^{N}{c}_{i}%&#10;\\phi(Y_{i})\\right\\|_{2}^{2},~{}\\textrm{ s.t. }\\sum\\limits_{i=1}^{N}c_{i}=1,\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder></mpadded><msub><mrow><mo>\u2225</mo><mrow><mtext>diag</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>c</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathbf{\\bar{x}}= \\Sigma^{-1/2}U^T \\kappa(X, \\mathcal{Y})$ and $\\bar{D} = \\Sigma^{-1/2}U^T$, given the \\text{SVD} of $\\kappa(\\mathcal{Y}, \\mathcal{Y})$ is $U\\Sigma U^T$.\n\nHere, we adopt Log-Euclidean Gaussian kernel{\\cite}{LiWangZuoZhang2013} to transform the SPD matrices into RKHS such that the linear combination will make sense. In contrast, the Log-Euclidean kernel can well characterize the true geodesic distance between SPD matrices instead. Specifically, a Log-Euclidean Gaussian kernel is defined by $ \\kappa_g(X,Y) = \\textrm{exp}\\{-\\gamma \\|\\mathbf{log}(X)- \\mathbf{log}(Y)\\|_F^2  \\}$, which is a \\textit{p.d.} kernel for any  $\\gamma > 0$.\n\n\\subsection{Optimization}\\label{opt}\nTo solve the problem\\eqref{NPKSRC2}, we apply the well-known alternating direction method of multipliers (ADMM){\\cite}{BoydParikhChuPeleatoEckstein2011} here. Before directly using ADMM, we should decouple the variables in the problem \\eqref{NPKSRC2} firstly. Let $W = \\text{diag}(\\mathbf{w})$ and introduce a variable $\\mathbf{a}= W\\mathbf{c}$. Then,\n\n", "itemtype": "equation", "pos": 22373, "prevtext": "\n\nThrough expanding the $\\ell_2$-norm term and some algebra manipulations, we will consider the following problem that has a same solution to problem \\ref{NPKSRC2}. For clarity and completeness, the detailed derivation of the problem \\ref{NPKSRC2} can be found in the appendix.\n\n", "index": 21, "text": "\\begin{align*}\n\\mathop{\\min}\\limits_{\\mathbf{c}}  \\|\\text{diag}(\\mathbf{w}) \\mathbf{c}\\|_1+ \\frac{\\lambda}{2}\n\\| \\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c}\\|_2^2, ~\\textrm{ s.t. } \\sum\\limits_{i = 1}^N c_{i}=1.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}\\|\\text{diag}(\\mathbf{w})\\mathbf%&#10;{c}\\|_{1}+\\frac{\\lambda}{2}\\|\\mathbf{\\bar{x}}-\\bar{D}\\mathbf{c}\\|_{2}^{2},~{}%&#10;\\textrm{ s.t. }\\sum\\limits_{i=1}^{N}c_{i}=1.\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><msub><mrow><mo>\u2225</mo><mrow><mtext>diag</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mtext>\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>c</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathbf{1} \\in \\mathcal{R}^N$ is a column vector whose entries are all ones.\n\nThe above problem is not convex in both, however, it is convex in a variable for fixed another unrelated one. Hence, the augmented Lagrangian function of problem \\eqref{NPKSRC3} can be written as follows.\n\n", "itemtype": "equation", "pos": 23640, "prevtext": "\nwhere $\\mathbf{\\bar{x}}= \\Sigma^{-1/2}U^T \\kappa(X, \\mathcal{Y})$ and $\\bar{D} = \\Sigma^{-1/2}U^T$, given the \\text{SVD} of $\\kappa(\\mathcal{Y}, \\mathcal{Y})$ is $U\\Sigma U^T$.\n\nHere, we adopt Log-Euclidean Gaussian kernel{\\cite}{LiWangZuoZhang2013} to transform the SPD matrices into RKHS such that the linear combination will make sense. In contrast, the Log-Euclidean kernel can well characterize the true geodesic distance between SPD matrices instead. Specifically, a Log-Euclidean Gaussian kernel is defined by $ \\kappa_g(X,Y) = \\textrm{exp}\\{-\\gamma \\|\\mathbf{log}(X)- \\mathbf{log}(Y)\\|_F^2  \\}$, which is a \\textit{p.d.} kernel for any  $\\gamma > 0$.\n\n\\subsection{Optimization}\\label{opt}\nTo solve the problem\\eqref{NPKSRC2}, we apply the well-known alternating direction method of multipliers (ADMM){\\cite}{BoydParikhChuPeleatoEckstein2011} here. Before directly using ADMM, we should decouple the variables in the problem \\eqref{NPKSRC2} firstly. Let $W = \\text{diag}(\\mathbf{w})$ and introduce a variable $\\mathbf{a}= W\\mathbf{c}$. Then,\n\n", "index": 23, "text": "\\begin{align}\n&\\mathop{\\min}\\limits_{\\mathbf{a},\\mathbf{c}}  \\|\\mathbf{a}\\|_1+ \\frac{\\lambda}{2}\n\\| \\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c}\\|_2^2, \\notag \\\\\n& \\quad\\textrm{s.t.,} ~\\mathbf{a}= W\\mathbf{c}, ~{\\textbf{c}}^T\\textbf{1} = 1. \\label{NPKSRC3}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{a},\\mathbf{c}}\\|\\mathbf{a}\\|_{1}+%&#10;\\frac{\\lambda}{2}\\|\\mathbf{\\bar{x}}-\\bar{D}\\mathbf{c}\\|_{2}^{2},\" display=\"inline\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mrow><mi>\ud835\udc1a</mi><mo>,</mo><mi>\ud835\udc1c</mi></mrow></munder><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1a</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad\\textrm{s.t.,}~{}\\mathbf{a}=W\\mathbf{c},~{}{\\textbf{c}}^{T}%&#10;\\textbf{1}=1.\" display=\"inline\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mtext>s.t.,</mtext></mpadded><mo>\u2062</mo><mi>\ud835\udc1a</mi></mrow><mo>=</mo><mrow><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><msup><mtext>\ud835\udc1c</mtext><mi>T</mi></msup><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>=</mo><mn>1</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\nThus, we optimize the problem by alternatively fixing other unrelated variables as follows.\n\\begin{enumerate}\n\\item Update $\\mathbf{c}$,\\\\\n\n", "itemtype": "equation", "pos": 24191, "prevtext": "\nwhere $\\mathbf{1} \\in \\mathcal{R}^N$ is a column vector whose entries are all ones.\n\nThe above problem is not convex in both, however, it is convex in a variable for fixed another unrelated one. Hence, the augmented Lagrangian function of problem \\eqref{NPKSRC3} can be written as follows.\n\n", "index": 25, "text": "\\begin{align}\n\\mathcal{L}(\\mathbf{a},\\mathbf{c}) = \\mathop{\\min}\\limits_{\\mathbf{a},\\mathbf{c}} \\|\\mathbf{a}\\|_1+  \\frac{\\lambda}{2}\n\\| \\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c}\\|_2^2 \\notag \\\\ +\\frac{\\mu}{2} (\\| \\mathbf{a}- W\\mathbf{c}\\|_2^2 + ({\\textbf{c}}^T\\textbf{1}- 1)^2) \\notag \\\\\n+ \\Delta^T (\\mathbf{a}- W\\mathbf{c})+ \\delta({\\textbf{c}}^T\\textbf{1}- 1). \\label{NPKSRC4}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{L}(\\mathbf{a},\\mathbf{c})=\\mathop{\\min}\\limits_{\\mathbf{%&#10;a},\\mathbf{c}}\\|\\mathbf{a}\\|_{1}+\\frac{\\lambda}{2}\\|\\mathbf{\\bar{x}}-\\bar{D}%&#10;\\mathbf{c}\\|_{2}^{2}\" display=\"inline\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1a</mi><mo>,</mo><mi>\ud835\udc1c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mrow><mi>\ud835\udc1a</mi><mo>,</mo><mi>\ud835\udc1c</mi></mrow></munder><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1a</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{\\mu}{2}(\\|\\mathbf{a}-W\\mathbf{c}\\|_{2}^{2}+({\\textbf{c}}^{%&#10;T}\\textbf{1}-1)^{2})\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mtext>\ud835\udc1c</mtext><mi>T</mi></msup><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\Delta^{T}(\\mathbf{a}-W\\mathbf{c})+\\delta({\\textbf{c}}^{T}%&#10;\\textbf{1}-1).\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><msup><mi mathvariant=\"normal\">\u0394</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mtext>\ud835\udc1c</mtext><mi>T</mi></msup><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nSetting the derivative w.r.t. $\\mathbf{c}$ to be zero gives the following.\n\n", "itemtype": "equation", "pos": 24717, "prevtext": "\n\nThus, we optimize the problem by alternatively fixing other unrelated variables as follows.\n\\begin{enumerate}\n\\item Update $\\mathbf{c}$,\\\\\n\n", "index": 27, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}}  \\frac{\\lambda}{2}\n\\| \\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c}\\|_2^2+\\frac{\\mu}{2}(\\| \\mathbf{a}- W\\mathbf{c}\\|_2^2 + ({\\textbf{c}}^T\\textbf{1}- 1)^2) \\notag \\\\\n+ \\Delta^T (\\mathbf{a}- W\\mathbf{c})+ \\delta({\\textbf{c}}^T\\textbf{1}- 1). \\label{updateC}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}\\frac{\\lambda}{2}\\|\\mathbf{\\bar{%&#10;x}}-\\bar{D}\\mathbf{c}\\|_{2}^{2}+\\frac{\\mu}{2}(\\|\\mathbf{a}-W\\mathbf{c}\\|_{2}^{%&#10;2}+({\\textbf{c}}^{T}\\textbf{1}-1)^{2})\" display=\"inline\"><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mtext>\ud835\udc1c</mtext><mi>T</mi></msup><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\Delta^{T}(\\mathbf{a}-W\\mathbf{c})+\\delta({\\textbf{c}}^{T}%&#10;\\textbf{1}-1).\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><msup><mi mathvariant=\"normal\">\u0394</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mtext>\ud835\udc1c</mtext><mi>T</mi></msup><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathds{1}_{N \\times N}$ is the matrix of size ${N \\times N}$ with all ones.\n\nThen, \\\\\n\n", "itemtype": "equation", "pos": 25100, "prevtext": "\nSetting the derivative w.r.t. $\\mathbf{c}$ to be zero gives the following.\n\n", "index": 29, "text": "\\begin{align*}\n0 = -\\lambda\\bar{D}^T(\\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c})+ \\mu W^T(W\\textbf{c}- \\textbf{a})\\\\\n+\\mu \\mathds{1}_{N \\times N}\\mathbf{c}- \\mu \\mathbf{1}\n-W^T\\Delta + \\delta \\mathbf{1}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 0=-\\lambda\\bar{D}^{T}(\\mathbf{\\bar{x}}-\\bar{D}\\mathbf{c})+\\mu W^%&#10;{T}(W\\textbf{c}-\\textbf{a})\" display=\"inline\"><mrow><mn>0</mn><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><msup><mi>W</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>W</mi><mo>\u2062</mo><mtext>\ud835\udc1c</mtext></mrow><mo>-</mo><mtext>\ud835\udc1a</mtext></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\mu\\mathds{1}_{N\\times N}\\mathbf{c}-\\mu\\mathbf{1}-W^{T}\\Delta+%&#10;\\delta\\mathbf{1}.\" display=\"inline\"><mrow><mrow><mrow><mrow><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><msub><mn>\ud835\udfd9</mn><mrow><mi>N</mi><mo>\u00d7</mo><mi>N</mi></mrow></msub><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>-</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo>-</mo><mrow><msup><mi>W</mi><mi>T</mi></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow></mrow><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\n\\item Update $\\mathbf{a}$, \\\\ \n\n", "itemtype": "equation", "pos": 25405, "prevtext": "\nwhere $\\mathds{1}_{N \\times N}$ is the matrix of size ${N \\times N}$ with all ones.\n\nThen, \\\\\n\n", "index": 31, "text": "\\begin{align}\n&\\mathbf{c}_k=(\\lambda\\bar{D}^T\\bar{D}+ \\mu W^TW + \\mu \\mathds{1}_{N \\times N})^{-1}(\\lambda\\bar{D}^T\\mathbf{\\bar{x}} \\notag \\\\\n&  + \\mu W \\textbf{a} + (\\mu -\\delta )\\mathbf{1} + W^T\\Delta).\\label{Update_c}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{c}_{k}=(\\lambda\\bar{D}^{T}\\bar{D}+\\mu W^{T}W+\\mu\\mathds{1%&#10;}_{N\\times N})^{-1}(\\lambda\\bar{D}^{T}\\mathbf{\\bar{x}}\" display=\"inline\"><mrow><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><msup><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>T</mi></msup><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>+</mo><mi>\u03bc</mi><msup><mi>W</mi><mi>T</mi></msup><mi>W</mi><mo>+</mo><mi>\u03bc</mi><msub><mn>\ud835\udfd9</mn><mrow><mi>N</mi><mo>\u00d7</mo><mi>N</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><msup><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>T</mi></msup><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\mu W\\textbf{a}+(\\mu-\\delta)\\mathbf{1}+W^{T}\\Delta).\" display=\"inline\"><mrow><mo>+</mo><mi>\u03bc</mi><mi>W</mi><mtext>\ud835\udc1a</mtext><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bc</mi><mo>-</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow><mn>\ud835\udfcf</mn><mo>+</mo><msup><mi>W</mi><mi>T</mi></msup><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\nThat is, \\\\\n\n", "itemtype": "equation", "pos": 25670, "prevtext": "\n\n\\item Update $\\mathbf{a}$, \\\\ \n\n", "index": 33, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{a}} \\|\\mathbf{a}\\|_1+ \\frac{\\mu}{2}\\| \\mathbf{a}- W\\mathbf{c}_k\\|_2^2 +\\Delta^T (\\mathbf{a}- W\\mathbf{c}_k). \\label{updateA}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{a}}\\|\\mathbf{a}\\|_{1}+\\frac{\\mu}{2}%&#10;\\|\\mathbf{a}-W\\mathbf{c}_{k}\\|_{2}^{2}+\\Delta^{T}(\\mathbf{a}-W\\mathbf{c}_{k}).\" display=\"inline\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1a</mi></munder><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1a</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msup><mi mathvariant=\"normal\">\u0394</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nThe above problem has the following closed-form solution given by shrinkage operator{\\cite}{LinChenMa2009}. That is,\n\n", "itemtype": "equation", "pos": 25864, "prevtext": "\n\nThat is, \\\\\n\n", "index": 35, "text": "\\begin{align*}\n\\mathop{\\min}\\limits_{\\mathbf{a}} \\|\\mathbf{a}\\|_1+ \\frac{\\mu}{2} \\| \\mathbf{a}- (W\\mathbf{c}_k- \\frac{\\Delta}{\\mu}) \\|_2^2.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{a}}\\|\\mathbf{a}\\|_{1}+\\frac{\\mu}{2}%&#10;\\|\\mathbf{a}-(W\\mathbf{c}_{k}-\\frac{\\Delta}{\\mu})\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1a</mi></munder><msub><mrow><mo>\u2225</mo><mi>\ud835\udc1a</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc1a</mi><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>W</mi><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub></mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>\u03bc</mi></mfrac></mstyle></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathcal{S}_{\\eta}(\\cdot)$ is a shrinkage operator acting on each element of the given matrix, and is defined as $\\mathcal{S}_{\\eta}(v) = \\textrm{sgn}(v)\\textrm{max}(|v|- \\eta, 0) $.\n\n\\item Update $\\Delta$ and $\\delta$.\n\n", "itemtype": "equation", "pos": 26134, "prevtext": "\nThe above problem has the following closed-form solution given by shrinkage operator{\\cite}{LinChenMa2009}. That is,\n\n", "index": 37, "text": "\\begin{align}\n\\mathbf{a}_k = \\mathcal{S}_{\\frac1{\\mu}}(W\\mathbf{c}_k- \\frac{\\Delta}{\\mu})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{a}_{k}=\\mathcal{S}_{\\frac{1}{\\mu}}(W\\mathbf{c}_{k}-\\frac{%&#10;\\Delta}{\\mu})\" display=\"inline\"><mrow><msub><mi>\ud835\udc1a</mi><mi>k</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mfrac><mn>1</mn><mi>\u03bc</mi></mfrac></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>W</mi><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub></mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>\u03bc</mi></mfrac></mstyle></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\n\\end{enumerate}\nThese iterative steps will be terminated when $\\|\\textbf{c}_k- \\textbf{c}_{k-1}\\|_{\\infty} \\leq \\epsilon$ and $\\| {\\textbf{c}_k}^T\\textbf{1}- 1\\|_{\\infty}  \\leq \\epsilon $ are satisfied.\n\n\\subsection{Classification}\nOnce the new representation of the test input is obtained, the decision rule \\eqref{rule} is applied to determine its class finally.\nThe detailed procedure of classification using neighborhood preserved kernel sparse representation is described in Algorithm\\ref{Alg1}.\n\\begin{algorithm}\n\\caption{Classification Using Neighborhood Preserved Kernel Sparse Representation on SPD matrices}\n\\SetKwData{Index}{Index}\n\\KwIn{Training data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N], Y_i \\in \\mathcal{S}_d^+$ sorted according to the label of each data point; A test sample $X \\in \\mathcal{S}_d^+$; $\\lambda$ and $\\mu$.}\n\\BlankLine\n\\textbf{Steps:}\n\\begin{enumerate}\n\\item Construct the weight vector $\\mathbf{w}$ by calculating the geodesic distance between $X$ and $\\mathcal{Y}$ in terms of equation \\eqref{gLE}.\n\\item Solve \\eqref{NPKSRC2} by ADMM explained in Section \\ref{opt},\nand obtain the optimal solution $ \\mathbf{c}^* $.\n\\item Compute the residuals of the test sample $X$ over all classes and assign its label finally.\n\\end{enumerate}\n\\KwOut{the label of test sample.}\n\\label{Alg1}\n\\end{algorithm}\n\n\\subsection{Complexity Analysis and Convergence}\nAs for the computational cost of the proposed algorithm, it is mainly determined by the steps in ADMM. The total complexity of NPKSRC is, as a function of the number of data points, $\\mathcal{O}(N^3+t N^2)$ where $t$ is the total number of iterations. The soft thresholding to update the sparse matrix $\\textrm{C}$ in each step is relatively cheaper, much less than $\\mathcal{O}(N^2)$.  For updating $\\mathbf{c}$ we can pre-compute the Cholesky decomposition of $(\\lambda\\bar{D}^T\\bar{D}+ \\mu W^TW + \\mu \\mathds{1}_{N \\times N})^{-1}$ at the cost of less than $\\mathcal{O}(\\frac12 N^3)$, then compute new $\\mathbf{c}$ by using \\eqref{Update_c} which has a complexity of $\\mathcal{O}(N^2)$ in general.\n\nThe above proposed ADMM iterative procedure to the augmented Lagrangian problem \\eqref{NPKSRC4} satisfies the general condition for the convergence theorem in {\\cite}{LinLiuLi2015}.\n\n\\section{Experimental Results} \\label{SectionIV}\nIn this section, we present several experimental results to demonstrate the effectiveness of NPKSRC. To comprehensively evaluate the performance of NPKSRC, we tested it on texture images, human faces and  pedestrian re-identification. Some sample images from test databases are shown in Figure \\ref{fig1}.\n\\begin{figure}[ht]\n\\centering\n  \\subfigure[]{\\includegraphics[width=0.23 \\textwidth]{FERET_sample.pdf}}\n   \\subfigure[]{\\includegraphics[width=0.12 \\textwidth]{Brodatz_sample.pdf}}\n   \\caption{Samples on the FERET (a) and Brodatz (b) database. }\\label{fig1}\n\\end{figure}\n\nWe compare our proposed method with five state-of-the-art methods in terms of recognition accuracy.\n\\begin{enumerate}\n\\item  Sparse representation classification(SRC){\\cite}{WrightYangGaneshSastryMa2009} ;\n\\item  Gabor feature-based sparse representation in Euclidean space (GSRC){\\cite}{YangZhangShiuZhang2013} ;\n\\item  Classification using Riemannian sparse representation based on Riemannian distance  (RSRC){\\cite}{CherianSra2014};\n\\item  Classification using Riemnnian sparse representation based on Stein kernel (RSRS){\\cite}{HarandiHartleyLovellSanderson2014};\n\\item  Log-Euclidean Gaussian kernel sparse representation based classification (LogE-GkSRC) {\\cite}{LiWangZuoZhang2013}.\n\\end{enumerate}\n\n\\subsection{Texture Classification}\nFirstly, we used Brodatz texture database to conduct classification task.\nIn this dataset, it includes 5-texture (`5c', `5m', `5v', `5v2',`5v3'), 10-texture (`10', `10v') and 16-texture (`16c', `16v') mosaics. Before using the proposed method, we downsampled each image into 256 $\\times$ 256 and then split into 64 regions of size 32 $\\times$ 32. To obtain their Region Covariance Matrices (RCM), a feature vector $f(x,y)$ for any pixel $I(x,y)$ is extracted, e.g., $f(x,y) = (I(x,y), |\\frac{\\partial I}{\\partial x}|, |\\frac{\\partial I}{\\partial y}|, |\\frac{\\partial^2 I}{\\partial x^2}|, |\\frac{\\partial^2 I}{\\partial y^2}|)$. Then, each region can be depicted by a 5 $\\times$ 5 covariance descriptor. As for the obtained RCM, there are 64 covariance matrices in each class. We randomly selected 5 from each class as training samples and the rest as the query samples. That means almost 8\\% samples are selected as training data and the rest for testing classification. To achieve a stable result, the reported classification rate is averaged over 20 trials.\n\nHow to construct the weight vector $\\mathbf{w}$ is not a trivial work in our method. Here, to better characterize the local geometry within data, we test the two kinds of distance by different metrics, i.e., Stein metric and Log-Euclidean metric. Brodatz-`16v' is selected as test dataset for classification task, which includes 16 classes. The classification results are shown in Table \\ref{Tab2}. As can be seen, the geodesic distance under Log-Euclidean metric can better characterize the manifold structure of data by achieving a better classification rate.\n\\begin{table}\n\\caption{classification results in terms of accuracy (\\%) on Brodatz-`16v' with different geodesic distance.}\n\\begin{center}\n\\begin{tabular}{|c|c|c| }\n\\hline\n$Metric$ & Stein metric & LogE metric  \\\\\n\\hline\nAccuracy &78.5  &79.34 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\label{Tab2}\n\\end{table}\n\nTo efficiently determine the parameters in our method, in Fig.\\ref{fig2},we report the recognition accuracy on Brodatz-`16c' with varying parameters $\\lambda$ and $\\gamma$, respectively. From the figs., we can set $\\lambda =0.09$ and $\\gamma =0.05$ for the best recognition result.\n\nBy applying the different methods, we presented the classification results in Table \\ref{Tab3}.  As well, the tuned parameters are reported for the results achieved by other methods. The bold numbers highlight the best results. From the results, we can observe that the proposed approach outperforms other methods in most cases while, on average, RSRS achieved the second best performance based on Stein kernel. This can be interpreted by the Stein distance may better suit some subset of Brodatz dataset, i.e., `5v2' and`5v3'. As for SRC, it conducts classification as a baseline due to the lack of consideration of the intrinsic geometry structure within data.\n\n\\begin{figure}[ht]\n\\centering\n  \\subfigure[]{\\includegraphics[width=0.45 \\textwidth]{lambda_Brodatz.pdf}}\n   \\subfigure[]{\\includegraphics[width=0.45 \\textwidth]{gamma_Brodatz.pdf}}\n   \\caption{Classification rate on Brodatz-16c  vs. parameter $\\lambda$ and $\\gamma$. }\\label{fig2}\n\\end{figure}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on Brodatz dataset.}\\label{Tab3}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & `5c'& `5m'& `5v'& `5v2'&`5v3' &`16v' &`16c' &`10v' &`10' & avg. \\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009} (0.001)&20.00 &20.00 &20.00 &20.00 &20.00 & 6.25 &44.07 &10.00 &20.00 &20.04 \\\\\n\nRSRC{\\cite}{CherianSra2014}(0.01) &97.97 &50.51 &83.05 &83.05 &71.19 &61.65 &74.05\t&85.08 &92.88 &77.71\\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(10.0) &98.31& 89.15 & 83.05 &\\textbf{87.12} &\\textbf{88.14}& 72.46 & 83.79 & 88.31 & 94.24  &87.17\\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &97.29\t&94.92 &83.73 &86.10 &86.78 &73.20 &80.08 &90.34\t&94.58 &84.55 \\\\\nNPKSRC(0.09,0.05) &\\textbf{98.31} &\\textbf{98.98} &\\textbf{84.41}\t&84.07 &87.12 &\\textbf{79.34} &\\textbf{88.35}&\\textbf{91.02} &\\textbf{98.31} &\\textbf{89.26} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\n\\subsection{Face Recognition}\nNext, we selected the ``b\" subset of FERET database to further evaluate the classification performance, in which it covers 1400 images with the size of 80 $\\times$ 80 from 200 subjects (about 7 each). This subset consists of the images, under different expression and illumination conditions, marked by `ba', `bd', `be', `bf', `bg', `bj', and `bk'. Specifically, training images include neutral expression `ba', smiling expression `bj', and illumination changes `bk', while test samples involve face images of varying pose angle such as `bd'+25$^{\\circ}$, `be'+15$^{\\circ}$, `bf'-15$^{\\circ}$, and `bg'-25$^{\\circ}$.\n\nTo represent a facial image, similar to the work {\\cite}{YangZhangShiuZhang2013}, we created a 43$\\times$43 region covariance matrix, i.e., a specific SPD matrix, which is composed of intensity value, spatial coordinates, 40 Gabor filters at 8 orientations and 5 scales. The down-sampling factor in Gabor filtering is applied too. For SRC, the Gabor features are firstly vectorized and the common SRC classifier is applied. The classification results achieved by other methods are reported in Table \\ref{Tab4} and Fig.\\ref{fig3}. The tuned parameters are presented in the table too. For NPKSRC, the $\\lambda $ and $\\gamma $ are set 0.9 and 0.02, respectively. From the table, we can see our proposed method achieves pleasing recognition performance compared to others. This is owed to the consideration of locality structure between data by using Riemannian metric.\n\\begin{figure}[ht]\n\\centering\n{\\includegraphics[width=0.45 \\textwidth]{Rec_FERET.pdf}}\n   \\caption{Comparison of Recognition rate (\\%) on FERET dataset. }\\label{fig3}\n\\end{figure}\n\n\\begin{table}\n\\caption{Classification results in terms of accuracy (\\%) on FERET dataset.}\\label{Tab4}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c| }\n\\hline\n$Dataset$ & `bg' &`bf' &`be' &`bd' & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.01)&61.00 &96.50 &95.50 &55.50 &77.13\\\\\nGSRC\\cite{YangZhangShiuZhang2013}(0.001) & 79.00 & 97.00 & 93.50 & 77.00 & 86.60 \\\\\nRSRC{\\cite}{CherianSra2014} (0.01) & 79.50 &97.00 &96.50 &68.50 &85.38  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(10.0) &86.00 &97.50  &96.50  &79.50  &89.90   \\\\\nNPKSRC (0.9, 0.02) &\\textbf{93.00} &\\textbf{99.50} &\\textbf{99.00} &\\textbf{92.00} &\\textbf{95.88} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\subsection{Pedestrian Re-identification}\nFinally, we conduct the person re-identification task by our proposed method and compare with other methods. Here, we used the modified ETHZ dataset{\\cite}{SchwartzDavis2009}, illustrated in Fig.\\ref{fig4}. The original ETHZ includes 3 Sequences, in which Sequence 1 contains 83 pedestrians (4,857 images), Sequence 2 contains 35 pedestrians (1,936 images), and Sequence 3 contains 28 pedestrians (1,762 images). To facilitate the subsequent processing, we first down-sampled all images to 64$\\times$32 pixels following the work{\\cite}{HarandiHartleyLovellSanderson2014}. To prepare the covariance descriptors, the following features are utilized: the position of pixel, the color information from RGB channels, the gradient and Laplacian information from the corresponding color part, respectively. That is, each region can be depicted by a 17$\\times$17 covariance matrix. To constructing the training samples, 10 images are randomly selected from each subject while the rest are used for testing. For fairly comparison, we adopt five splits for  each sequence to test the classification performance.\n\nThe recognition results are presented in Tables\\ref{Tab5}-\\ref{Tab7}. We tuned the parameters for each method to achieve the best results and reported them in tables. And the best results for each test Seq. are highlighted in bold numbers as usual. For the methods using kernel trick, the second parameter in the brackets denotes the kernel parameter. As can be seen, the proposed NPKSRC achieves the best score for each sequence in average sense. While for LogE-GkSRC, it obtains the second best results in terms of classification rate thanks to the use of Log-Euclidean metric. To explain this observation, it may owe to considering the weight structure within data. As for RSRS, it applies the Stein kernel inferior to the methods using Log-Euclidean Gaussian one.\n\nFurthermore, to clearly show the advantage of our method, we plot a recognition rate vs. each split for seq.1 in Fig.\\ref{fig5}. As the curves for seq.2 and seq.3 are similar to that of seq.1, we do not repeatedly present here.\n\n\\begin{figure}[ht]\n\\centering\n {\\includegraphics[width=0.20 \\textwidth]{ETHZ_p1.pdf}}  \n {\\includegraphics[width=0.20 \\textwidth]{ETHZ_p2.pdf}}\n   \\caption{Samples from the ETHZ dataset{\\cite}{HarandiSandersonHartleyLovell2012}. }\\label{fig4}\n\\end{figure}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.1 dataset.}\\label{Tab5}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &87.97 &87.06 &88.36\t&89.39\t&90.04\t&88.56 \\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &77.10 &79.82\t&79.17 &80.08 &79.95 &79.22  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) & 89.78 &89.39\t&91.72 &92.63 &91.98 &91.10    \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &90.69\t&88.87\t&\\textbf{92.11}\t&91.98 &92.88 &91.31 \\\\\nNPKSRC (0.001,0.001) &\\textbf{92.11} &\\textbf{90.30} &91.98 &\\textbf{92.88} &\\textbf{92.76}\t&\\textbf{92.00}  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.2 dataset.}\\label{Tab6}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &83.74 &85.28 &85.89\t&86.81 &86.50 &85.64  \\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &84.66 &81.60 &82.52 &86.20 &83.44 &83.68\\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) &90.49 &88.96 &\\textbf{89.88} &90.80 &\\textbf{91.41} &90.31 \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &91.41 &89.88 &88.65 &92.94 &90.80 &90.74 \\\\\nNPKSRC (0.001,0.001) &\\textbf{91.72} &\\textbf{89.88} &88.35 &\\textbf{93.25} & 90.49 &\\textbf{90.74} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.3 dataset.}\\label{Tab7}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &95.92 &95.10 &90.61 &93.88 &93.06 &93.71\\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &92.65 &92.65 &91.02 &91.02 &91.84 &91.84  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) &98.78\t&97.55\t&\\textbf{98.37}\t&96.73 &98.37 &97.96  \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &\\textbf{99.59}\t&98.37 &97.55 &95.92 &98.37 &97.96  \\\\\nNPKSRC (0.001,0.001) &99.18\t&\\textbf{99.59} &97.14 &\\textbf{97.14} &\\textbf{98.37} &\\textbf{98.28}   \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{figure}[ht]\n\\centering\n {\\includegraphics[width=0.50 \\textwidth]{ETHZ_Seq1.pdf}}  \n   \\caption{Recognition results from the ETHZ dataset. }\\label{fig5}\n\\end{figure}\n\n\\section{Conclusion}\\label{SectionV}\nIn this paper, a novel robust classification algorithm, termed as neighborhood preserved sparse representation, is proposed for SPD matrices by fully exploiting the Riemannian geometry structure within data. Specifically, the local consistency constraint, formulated by the geodesic distance under Log-Euclidean metric, is imposed onto the sparse coding paradigm over a Riemannian manifold. Experimental results show that the proposed method can provide better classification solutions than the state-of-the-art approaches thanks to incorporating Riemannian geometry structure.\n\nAlthough our proposed method achieved promising performance in terms of recognition rate, there still exists some open issues deserving to study. One prompt direction may be how to devise a better weighted constraint such that the beneficial discriminant representations can be achieved.\n\n\\section{Acknowledgement}\nThe Project was supported in part by the Guangdong Natural Science Foundation under Grant (No. 2014A030313511) and in part by the Scientific Research Foundation for the Returned Overseas\nChinese Scholars, State Education Ministry, China.\n\n\\section*{Appendix}\nGiven a least-squares problem as following,\n\n", "itemtype": "equation", "pos": 26463, "prevtext": "\nwhere $\\mathcal{S}_{\\eta}(\\cdot)$ is a shrinkage operator acting on each element of the given matrix, and is defined as $\\mathcal{S}_{\\eta}(v) = \\textrm{sgn}(v)\\textrm{max}(|v|- \\eta, 0) $.\n\n\\item Update $\\Delta$ and $\\delta$.\n\n", "index": 39, "text": "\\begin{align}\n\\Delta_{k} = \\Delta_{k-1} + \\mu (\\mathbf{a}_k- W\\mathbf{c}_k).\\notag \\\\\n\\delta_k = \\delta_{k-1} + \\mu ({\\textbf{c}_k}^T\\textbf{1}- 1).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta_{k}=\\Delta_{k-1}+\\mu(\\mathbf{a}_{k}-W\\mathbf{c}_{k}).\" display=\"inline\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mi>k</mi></msub><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc1a</mi><mi>k</mi></msub><mo>-</mo><mrow><mi>W</mi><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\delta_{k}=\\delta_{k-1}+\\mu({\\textbf{c}_{k}}^{T}\\textbf{1}-1).\" display=\"inline\"><mrow><mrow><msub><mi>\u03b4</mi><mi>k</mi></msub><mo>=</mo><mrow><msub><mi>\u03b4</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mmultiscripts><mtext>\ud835\udc1c</mtext><mi>k</mi><none/><none/><mi>T</mi></mmultiscripts><mo>\u2062</mo><mtext>\ud835\udfcf</mtext></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N]$ and $X$ are on SPD manifold $\\mathcal{S}_d^+$. This problem can be rewritten as a least-squares problem on Euclidean space. That is,\n\n", "itemtype": "equation", "pos": 42928, "prevtext": "\n\\end{enumerate}\nThese iterative steps will be terminated when $\\|\\textbf{c}_k- \\textbf{c}_{k-1}\\|_{\\infty} \\leq \\epsilon$ and $\\| {\\textbf{c}_k}^T\\textbf{1}- 1\\|_{\\infty}  \\leq \\epsilon $ are satisfied.\n\n\\subsection{Classification}\nOnce the new representation of the test input is obtained, the decision rule \\eqref{rule} is applied to determine its class finally.\nThe detailed procedure of classification using neighborhood preserved kernel sparse representation is described in Algorithm\\ref{Alg1}.\n\\begin{algorithm}\n\\caption{Classification Using Neighborhood Preserved Kernel Sparse Representation on SPD matrices}\n\\SetKwData{Index}{Index}\n\\KwIn{Training data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N], Y_i \\in \\mathcal{S}_d^+$ sorted according to the label of each data point; A test sample $X \\in \\mathcal{S}_d^+$; $\\lambda$ and $\\mu$.}\n\\BlankLine\n\\textbf{Steps:}\n\\begin{enumerate}\n\\item Construct the weight vector $\\mathbf{w}$ by calculating the geodesic distance between $X$ and $\\mathcal{Y}$ in terms of equation \\eqref{gLE}.\n\\item Solve \\eqref{NPKSRC2} by ADMM explained in Section \\ref{opt},\nand obtain the optimal solution $ \\mathbf{c}^* $.\n\\item Compute the residuals of the test sample $X$ over all classes and assign its label finally.\n\\end{enumerate}\n\\KwOut{the label of test sample.}\n\\label{Alg1}\n\\end{algorithm}\n\n\\subsection{Complexity Analysis and Convergence}\nAs for the computational cost of the proposed algorithm, it is mainly determined by the steps in ADMM. The total complexity of NPKSRC is, as a function of the number of data points, $\\mathcal{O}(N^3+t N^2)$ where $t$ is the total number of iterations. The soft thresholding to update the sparse matrix $\\textrm{C}$ in each step is relatively cheaper, much less than $\\mathcal{O}(N^2)$.  For updating $\\mathbf{c}$ we can pre-compute the Cholesky decomposition of $(\\lambda\\bar{D}^T\\bar{D}+ \\mu W^TW + \\mu \\mathds{1}_{N \\times N})^{-1}$ at the cost of less than $\\mathcal{O}(\\frac12 N^3)$, then compute new $\\mathbf{c}$ by using \\eqref{Update_c} which has a complexity of $\\mathcal{O}(N^2)$ in general.\n\nThe above proposed ADMM iterative procedure to the augmented Lagrangian problem \\eqref{NPKSRC4} satisfies the general condition for the convergence theorem in {\\cite}{LinLiuLi2015}.\n\n\\section{Experimental Results} \\label{SectionIV}\nIn this section, we present several experimental results to demonstrate the effectiveness of NPKSRC. To comprehensively evaluate the performance of NPKSRC, we tested it on texture images, human faces and  pedestrian re-identification. Some sample images from test databases are shown in Figure \\ref{fig1}.\n\\begin{figure}[ht]\n\\centering\n  \\subfigure[]{\\includegraphics[width=0.23 \\textwidth]{FERET_sample.pdf}}\n   \\subfigure[]{\\includegraphics[width=0.12 \\textwidth]{Brodatz_sample.pdf}}\n   \\caption{Samples on the FERET (a) and Brodatz (b) database. }\\label{fig1}\n\\end{figure}\n\nWe compare our proposed method with five state-of-the-art methods in terms of recognition accuracy.\n\\begin{enumerate}\n\\item  Sparse representation classification(SRC){\\cite}{WrightYangGaneshSastryMa2009} ;\n\\item  Gabor feature-based sparse representation in Euclidean space (GSRC){\\cite}{YangZhangShiuZhang2013} ;\n\\item  Classification using Riemannian sparse representation based on Riemannian distance  (RSRC){\\cite}{CherianSra2014};\n\\item  Classification using Riemnnian sparse representation based on Stein kernel (RSRS){\\cite}{HarandiHartleyLovellSanderson2014};\n\\item  Log-Euclidean Gaussian kernel sparse representation based classification (LogE-GkSRC) {\\cite}{LiWangZuoZhang2013}.\n\\end{enumerate}\n\n\\subsection{Texture Classification}\nFirstly, we used Brodatz texture database to conduct classification task.\nIn this dataset, it includes 5-texture (`5c', `5m', `5v', `5v2',`5v3'), 10-texture (`10', `10v') and 16-texture (`16c', `16v') mosaics. Before using the proposed method, we downsampled each image into 256 $\\times$ 256 and then split into 64 regions of size 32 $\\times$ 32. To obtain their Region Covariance Matrices (RCM), a feature vector $f(x,y)$ for any pixel $I(x,y)$ is extracted, e.g., $f(x,y) = (I(x,y), |\\frac{\\partial I}{\\partial x}|, |\\frac{\\partial I}{\\partial y}|, |\\frac{\\partial^2 I}{\\partial x^2}|, |\\frac{\\partial^2 I}{\\partial y^2}|)$. Then, each region can be depicted by a 5 $\\times$ 5 covariance descriptor. As for the obtained RCM, there are 64 covariance matrices in each class. We randomly selected 5 from each class as training samples and the rest as the query samples. That means almost 8\\% samples are selected as training data and the rest for testing classification. To achieve a stable result, the reported classification rate is averaged over 20 trials.\n\nHow to construct the weight vector $\\mathbf{w}$ is not a trivial work in our method. Here, to better characterize the local geometry within data, we test the two kinds of distance by different metrics, i.e., Stein metric and Log-Euclidean metric. Brodatz-`16v' is selected as test dataset for classification task, which includes 16 classes. The classification results are shown in Table \\ref{Tab2}. As can be seen, the geodesic distance under Log-Euclidean metric can better characterize the manifold structure of data by achieving a better classification rate.\n\\begin{table}\n\\caption{classification results in terms of accuracy (\\%) on Brodatz-`16v' with different geodesic distance.}\n\\begin{center}\n\\begin{tabular}{|c|c|c| }\n\\hline\n$Metric$ & Stein metric & LogE metric  \\\\\n\\hline\nAccuracy &78.5  &79.34 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\label{Tab2}\n\\end{table}\n\nTo efficiently determine the parameters in our method, in Fig.\\ref{fig2},we report the recognition accuracy on Brodatz-`16c' with varying parameters $\\lambda$ and $\\gamma$, respectively. From the figs., we can set $\\lambda =0.09$ and $\\gamma =0.05$ for the best recognition result.\n\nBy applying the different methods, we presented the classification results in Table \\ref{Tab3}.  As well, the tuned parameters are reported for the results achieved by other methods. The bold numbers highlight the best results. From the results, we can observe that the proposed approach outperforms other methods in most cases while, on average, RSRS achieved the second best performance based on Stein kernel. This can be interpreted by the Stein distance may better suit some subset of Brodatz dataset, i.e., `5v2' and`5v3'. As for SRC, it conducts classification as a baseline due to the lack of consideration of the intrinsic geometry structure within data.\n\n\\begin{figure}[ht]\n\\centering\n  \\subfigure[]{\\includegraphics[width=0.45 \\textwidth]{lambda_Brodatz.pdf}}\n   \\subfigure[]{\\includegraphics[width=0.45 \\textwidth]{gamma_Brodatz.pdf}}\n   \\caption{Classification rate on Brodatz-16c  vs. parameter $\\lambda$ and $\\gamma$. }\\label{fig2}\n\\end{figure}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on Brodatz dataset.}\\label{Tab3}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & `5c'& `5m'& `5v'& `5v2'&`5v3' &`16v' &`16c' &`10v' &`10' & avg. \\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009} (0.001)&20.00 &20.00 &20.00 &20.00 &20.00 & 6.25 &44.07 &10.00 &20.00 &20.04 \\\\\n\nRSRC{\\cite}{CherianSra2014}(0.01) &97.97 &50.51 &83.05 &83.05 &71.19 &61.65 &74.05\t&85.08 &92.88 &77.71\\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(10.0) &98.31& 89.15 & 83.05 &\\textbf{87.12} &\\textbf{88.14}& 72.46 & 83.79 & 88.31 & 94.24  &87.17\\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &97.29\t&94.92 &83.73 &86.10 &86.78 &73.20 &80.08 &90.34\t&94.58 &84.55 \\\\\nNPKSRC(0.09,0.05) &\\textbf{98.31} &\\textbf{98.98} &\\textbf{84.41}\t&84.07 &87.12 &\\textbf{79.34} &\\textbf{88.35}&\\textbf{91.02} &\\textbf{98.31} &\\textbf{89.26} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\n\\subsection{Face Recognition}\nNext, we selected the ``b\" subset of FERET database to further evaluate the classification performance, in which it covers 1400 images with the size of 80 $\\times$ 80 from 200 subjects (about 7 each). This subset consists of the images, under different expression and illumination conditions, marked by `ba', `bd', `be', `bf', `bg', `bj', and `bk'. Specifically, training images include neutral expression `ba', smiling expression `bj', and illumination changes `bk', while test samples involve face images of varying pose angle such as `bd'+25$^{\\circ}$, `be'+15$^{\\circ}$, `bf'-15$^{\\circ}$, and `bg'-25$^{\\circ}$.\n\nTo represent a facial image, similar to the work {\\cite}{YangZhangShiuZhang2013}, we created a 43$\\times$43 region covariance matrix, i.e., a specific SPD matrix, which is composed of intensity value, spatial coordinates, 40 Gabor filters at 8 orientations and 5 scales. The down-sampling factor in Gabor filtering is applied too. For SRC, the Gabor features are firstly vectorized and the common SRC classifier is applied. The classification results achieved by other methods are reported in Table \\ref{Tab4} and Fig.\\ref{fig3}. The tuned parameters are presented in the table too. For NPKSRC, the $\\lambda $ and $\\gamma $ are set 0.9 and 0.02, respectively. From the table, we can see our proposed method achieves pleasing recognition performance compared to others. This is owed to the consideration of locality structure between data by using Riemannian metric.\n\\begin{figure}[ht]\n\\centering\n{\\includegraphics[width=0.45 \\textwidth]{Rec_FERET.pdf}}\n   \\caption{Comparison of Recognition rate (\\%) on FERET dataset. }\\label{fig3}\n\\end{figure}\n\n\\begin{table}\n\\caption{Classification results in terms of accuracy (\\%) on FERET dataset.}\\label{Tab4}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c| }\n\\hline\n$Dataset$ & `bg' &`bf' &`be' &`bd' & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.01)&61.00 &96.50 &95.50 &55.50 &77.13\\\\\nGSRC\\cite{YangZhangShiuZhang2013}(0.001) & 79.00 & 97.00 & 93.50 & 77.00 & 86.60 \\\\\nRSRC{\\cite}{CherianSra2014} (0.01) & 79.50 &97.00 &96.50 &68.50 &85.38  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(10.0) &86.00 &97.50  &96.50  &79.50  &89.90   \\\\\nNPKSRC (0.9, 0.02) &\\textbf{93.00} &\\textbf{99.50} &\\textbf{99.00} &\\textbf{92.00} &\\textbf{95.88} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\subsection{Pedestrian Re-identification}\nFinally, we conduct the person re-identification task by our proposed method and compare with other methods. Here, we used the modified ETHZ dataset{\\cite}{SchwartzDavis2009}, illustrated in Fig.\\ref{fig4}. The original ETHZ includes 3 Sequences, in which Sequence 1 contains 83 pedestrians (4,857 images), Sequence 2 contains 35 pedestrians (1,936 images), and Sequence 3 contains 28 pedestrians (1,762 images). To facilitate the subsequent processing, we first down-sampled all images to 64$\\times$32 pixels following the work{\\cite}{HarandiHartleyLovellSanderson2014}. To prepare the covariance descriptors, the following features are utilized: the position of pixel, the color information from RGB channels, the gradient and Laplacian information from the corresponding color part, respectively. That is, each region can be depicted by a 17$\\times$17 covariance matrix. To constructing the training samples, 10 images are randomly selected from each subject while the rest are used for testing. For fairly comparison, we adopt five splits for  each sequence to test the classification performance.\n\nThe recognition results are presented in Tables\\ref{Tab5}-\\ref{Tab7}. We tuned the parameters for each method to achieve the best results and reported them in tables. And the best results for each test Seq. are highlighted in bold numbers as usual. For the methods using kernel trick, the second parameter in the brackets denotes the kernel parameter. As can be seen, the proposed NPKSRC achieves the best score for each sequence in average sense. While for LogE-GkSRC, it obtains the second best results in terms of classification rate thanks to the use of Log-Euclidean metric. To explain this observation, it may owe to considering the weight structure within data. As for RSRS, it applies the Stein kernel inferior to the methods using Log-Euclidean Gaussian one.\n\nFurthermore, to clearly show the advantage of our method, we plot a recognition rate vs. each split for seq.1 in Fig.\\ref{fig5}. As the curves for seq.2 and seq.3 are similar to that of seq.1, we do not repeatedly present here.\n\n\\begin{figure}[ht]\n\\centering\n {\\includegraphics[width=0.20 \\textwidth]{ETHZ_p1.pdf}}  \n {\\includegraphics[width=0.20 \\textwidth]{ETHZ_p2.pdf}}\n   \\caption{Samples from the ETHZ dataset{\\cite}{HarandiSandersonHartleyLovell2012}. }\\label{fig4}\n\\end{figure}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.1 dataset.}\\label{Tab5}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &87.97 &87.06 &88.36\t&89.39\t&90.04\t&88.56 \\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &77.10 &79.82\t&79.17 &80.08 &79.95 &79.22  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) & 89.78 &89.39\t&91.72 &92.63 &91.98 &91.10    \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &90.69\t&88.87\t&\\textbf{92.11}\t&91.98 &92.88 &91.31 \\\\\nNPKSRC (0.001,0.001) &\\textbf{92.11} &\\textbf{90.30} &91.98 &\\textbf{92.88} &\\textbf{92.76}\t&\\textbf{92.00}  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.2 dataset.}\\label{Tab6}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &83.74 &85.28 &85.89\t&86.81 &86.50 &85.64  \\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &84.66 &81.60 &82.52 &86.20 &83.44 &83.68\\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) &90.49 &88.96 &\\textbf{89.88} &90.80 &\\textbf{91.41} &90.31 \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &91.41 &89.88 &88.65 &92.94 &90.80 &90.74 \\\\\nNPKSRC (0.001,0.001) &\\textbf{91.72} &\\textbf{89.88} &88.35 &\\textbf{93.25} & 90.49 &\\textbf{90.74} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{table*}\n\\caption{Classification results in terms of accuracy (\\%) on ETHZ Seq.3 dataset.}\\label{Tab7}\n\\begin{center}\n\\begin{tabular}{|l|c|c|c|c|c|c| }\n\\hline\n$Dataset$ & $s_1$ & $s_2$ & $s_3$ &$s_4$ & $s_5$ & avg.\\\\\n\\hline\\hline\nSRC{\\cite}{WrightYangGaneshSastryMa2009}(0.001) &95.92 &95.10 &90.61 &93.88 &93.06 &93.71\\\\\nRSRC{\\cite}{CherianSra2014}(0.01) &92.65 &92.65 &91.02 &91.02 &91.84 &91.84  \\\\\nRSRS{\\cite}{HarandiHartleyLovellSanderson2014}(0.01,10) &98.78\t&97.55\t&\\textbf{98.37}\t&96.73 &98.37 &97.96  \\\\\nLogE-GkSRC{\\cite}{LiWangZuoZhang2013}(0.001,0.02)  &\\textbf{99.59}\t&98.37 &97.55 &95.92 &98.37 &97.96  \\\\\nNPKSRC (0.001,0.001) &99.18\t&\\textbf{99.59} &97.14 &\\textbf{97.14} &\\textbf{98.37} &\\textbf{98.28}   \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\n\\begin{figure}[ht]\n\\centering\n {\\includegraphics[width=0.50 \\textwidth]{ETHZ_Seq1.pdf}}  \n   \\caption{Recognition results from the ETHZ dataset. }\\label{fig5}\n\\end{figure}\n\n\\section{Conclusion}\\label{SectionV}\nIn this paper, a novel robust classification algorithm, termed as neighborhood preserved sparse representation, is proposed for SPD matrices by fully exploiting the Riemannian geometry structure within data. Specifically, the local consistency constraint, formulated by the geodesic distance under Log-Euclidean metric, is imposed onto the sparse coding paradigm over a Riemannian manifold. Experimental results show that the proposed method can provide better classification solutions than the state-of-the-art approaches thanks to incorporating Riemannian geometry structure.\n\nAlthough our proposed method achieved promising performance in terms of recognition rate, there still exists some open issues deserving to study. One prompt direction may be how to devise a better weighted constraint such that the beneficial discriminant representations can be achieved.\n\n\\section{Acknowledgement}\nThe Project was supported in part by the Guangdong Natural Science Foundation under Grant (No. 2014A030313511) and in part by the Scientific Research Foundation for the Returned Overseas\nChinese Scholars, State Education Ministry, China.\n\n\\section*{Appendix}\nGiven a least-squares problem as following,\n\n", "index": 41, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}} \\left\\| \\phi(X)-  \\sum\\limits_{i = 1}^N {c}_{i}\\phi(Y_i)  \\right\\|_2^2.\\label{ProbSPD}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}\\left\\|\\phi(X)-\\sum\\limits_{i=1}%&#10;^{N}{c}_{i}\\phi(Y_{i})\\right\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nwhere $\\mathbf{\\bar{x}}= \\Sigma^{-1/2}U^T \\kappa(X, \\mathcal{Y})$ and $\\bar{D} = \\Sigma^{-1/2}U^T$, given the \\text{SVD} of $\\kappa(\\mathcal{Y}, \\mathcal{Y})$ is $U\\Sigma U^T, UU^T = \\textbf{I}$.\n\n\\begin{proof} By expanding the $\\ell_2$-norm term in problem \\eqref{ProbSPD}, we have the following formulation,\n\n", "itemtype": "equation", "pos": 43253, "prevtext": "\nwhere data $\\mathcal{Y}= [Y_1,Y_2,..., Y_N]$ and $X$ are on SPD manifold $\\mathcal{S}_d^+$. This problem can be rewritten as a least-squares problem on Euclidean space. That is,\n\n", "index": 43, "text": "\\begin{align}\n\\mathop{\\min}\\limits_{\\mathbf{c}}  \\|\\mathbf{\\bar{x}}- \\bar{D} \\mathbf{c}\\|_2^2.\\label{ProbSPD2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}\\|\\mathbf{\\bar{x}}-\\bar{D}%&#10;\\mathbf{c}\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>-</mo><mrow><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07336.tex", "nexttext": "\nLet $\\mathbf{\\bar{x}}= \\Sigma^{-1/2}U^T \\kappa(X, \\mathcal{Y})$ and $\\bar{D} = \\Sigma^{-1/2}U^T$, then the formulation \\eqref{ProbSPD2} is recognized.\n\\end{proof}\n\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem{ArsignyFillardPennecAyache2007}\nVincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache.\n\\newblock Geometric means in a novel vector space structure on symmetric\n  positive-definite matrices.\n\\newblock {\\em {SIAM} Journal on Matrix Analysis and Applications},\n  29(1):328--347, 2007.\n\n\\bibitem{BoydParikhChuPeleatoEckstein2011}\nS.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.\n\\newblock {\\em Distributed Optimization and Statistical Learning via the\n  Alternating Direction Method of Multipliers}, volume~3.\n\\newblock Foundations and Trends in Machine Learning, 2011.\n\n\\bibitem{CherianSra2014}\nAnoop Cherian and Suvrit Sra.\n\\newblock Riemannian sparse coding for positive definite matrices.\n\\newblock In {\\em Proceedings of ECCV}, volume 8691, pages 299--314. Springer\n  International Publishing, 2014.\n\n\\bibitem{DonohoEladTemlyakov2006}\nD.L. Donoho, M.~Elad, and V.~Temlyakov.\n\\newblock Stable recovery of sparse overcomplete representations in the\n  presence of noise.\n\\newblock {\\em IEEE Transactions on Information Theory}, 52(1):6--18, 2006.\n\n\\bibitem{FuGaoHongTien2015}\nYifan Fu, Junbin Gao, Xia Hong, and David Tien.\n\\newblock Low rank representation on {R}iemannian manifold of symmetric\n  positive deffinite matrices.\n\\newblock In {\\em Proceedings of SDM}, 2015, DOI:10.1137/1.9781611974010.36.\n\n\\bibitem{HarandiSalzmann2015}\nM.~Harandi and M.~Salzmann.\n\\newblock Riemannian coding and dictionary learning: Kernels to the rescue.\n\\newblock In {\\em Proceedings of CVPR}, pages 3926--3935, 2015.\n\n\\bibitem{HarandiSandersonHartleyLovell2012}\nM~Harandi, C~Sanderson, R~Hartley, and B~Lovell.\n\\newblock Sparse coding and dictionary learning for symmetric positive definite\n  matrices: A kernel approach.\n\\newblock In {\\em Proceedings of ECCV}, pages 216--229, 2012.\n\n\\bibitem{HarandiHartleyLovellSanderson2014}\nMehrtash~Tafazzoli Harandi, Richard Hartley, Brian~C. Lovell, and Conrad\n  Sanderson.\n\\newblock Sparse coding on symmetric positive definite manifolds using bregman\n  divergences.\n\\newblock {\\em IEEE Transactions on Neural Networks and Learning Systems},\n  2015, DOI:10.1109/TNNLS.2014.2387383.\n\n\\bibitem{HoXieVemuri2013}\nJeffrey Ho, Yuchen Xie, and Baba~C. Vemuri.\n\\newblock On a nonlinear generalization of sparse coding and dictionary\n  learning.\n\\newblock In {\\em Proceedings of ICML}, volume~28, pages 1480--1488, 2013.\n\n\\bibitem{JayasumanaHartleySalzmannLiHarandi2013}\nSadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and\n  Mehrtash~Tafazzoli Harandi.\n\\newblock Kernel methods on the {R}iemannian manifold of symmetric positive\n  definite matrices.\n\\newblock In {\\em Proceedings of CVPR}, pages 73--80, June 2013.\n\n\\bibitem{LiWangZuoZhang2013}\nPeihua Li, Qilong Wang, Wangmeng Zuo, and Lei Zhang.\n\\newblock Log-{E}uclidean kernels for sparse representation and dictionary\n  learning.\n\\newblock In {\\em Proceedings of ICCV}, pages 1601--1608, Dec 2013.\n\n\\bibitem{LinLiuLi2015}\nZ.~Lin, R.~Liu, and H.~Li.\n\\newblock Linearized alternating direction method with parallel splitting and\n  adaptive penalty for separable convex programs in machine learning.\n\\newblock {\\em Machine Learning}, 99(2):287--325, 2015.\n\n\\bibitem{LinChenMa2009}\nZhouchen Lin, Minming Chen, and Yi~Ma.\n\\newblock The augmented lagrange multiplier method for exact recovery of\n  corrupted low-rank matrices.\n\\newblock Technical report, UIUC Technical Report UILU-ENG-09-2215, 2009.\n\n\\bibitem{PennecFillardAyache2006}\nXavier Pennec, Pierre Fillard, and Nicholas Ayache.\n\\newblock A {R}iemannian framework for tensor computing.\n\\newblock {\\em International Journal Of Computer Vision}, 66:41--66, 2006.\n\n\\bibitem{RoweisSaul2000}\nS.~T. Roweis and L.~K. Saul.\n\\newblock Nonlinear dimensionality reduction by locally linear embedding.\n\\newblock {\\em Science}, 290:2323--2326, 2000.\n\n\\bibitem{SchwartzDavis2009}\nW.R. Schwartz and L.S. Davis.\n\\newblock Learning discriminative appearance-based models using partial least\n  squares.\n\\newblock In {\\em Proceedings of {SIBGRAPI}}, pages 322--329, 2009.\n\n\\bibitem{SivalingamBoleyMorellasPapanikolopoulos2014}\nRavishankar Sivalingam, Daniel Boley, Vassilios Morellas, and Nikolaos\n  Papanikolopoulos.\n\\newblock Tensor sparse coding for positive definite matrices.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  36(3):592--605, 2014.\n\n\\bibitem{TenenbaumSilvaLangford2000}\nJ.~B. Tenenbaum, V.~d.~Silva, and J.~C. Langford.\n\\newblock A global geometric framework for nonlinear dimensionality reduction.\n\\newblock {\\em Science}, 290:2319--2323, 2000.\n\n\\bibitem{TuzelPorikliMeer2008}\nO.~Tuzel, F.~Porikli, and P.~Meer.\n\\newblock Pedestrian detection via classification on {R}iemannian manifolds.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  30(10):1713--1727, Oct 2008.\n\n\\bibitem{WangHuGaoSunYin2015}\nB.Y. Wang, Y.L. Hu, J.~Gao, Y.F. Sun, and B.C. Yin.\n\\newblock Low rank representation on {G}rassmann manifolds: An extrinsic\n  perspective.\n\\newblock {\\em arXiv preprint arXiv:1504.01807}.\n\n\\bibitem{WangYangYuLvHuangGong2010}\nJinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, and Yihong Gong.\n\\newblock Locality-constrained linear coding for image classification.\n\\newblock In {\\em Proceedings of CVPR}, 2010.\n\n\\bibitem{WangShanChenGao2008}\nRuiping Wang, Shiguang Shan, Xilin Chen, and Wen Gao.\n\\newblock Manifold-manifold distance with application to face recognition based\n  on image set.\n\\newblock In {\\em Proceedings of CVPR}, pages 1--8, June 2008.\n\n\\bibitem{WangSlavakisLerman2015}\nXu~Wang, Konstantinos Slavakis, and Gilad Lerman.\n\\newblock Multi-manifold modeling in non-euclidean spaces.\n\\newblock In {\\em Proceedings of {AISTATS} 2015, San Diego, California, USA,\n  May 9-12, 2015}, 2015.\n\n\\bibitem{WrightYangGaneshSastryMa2009}\nJohn Wright, Allen~Y. Yang, Arvind Ganesh, S.~Shankar Sastry, and Yi~Ma.\n\\newblock Robust face recognition via sparse representation.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machince\n  Intelligence}, 31(2):210--227, February 2009.\n\n\\bibitem{YangWrightMaSastry2008}\nAllen~Y. Yang, John Wright, Yi~Ma, and Shankar Sastry.\n\\newblock Unsupervised segmentation of natural images via lossy data\n  compression.\n\\newblock {\\em Computer Vision and Image Understanding}, 110(2):212--225, 2008.\n\n\\bibitem{YangChuZhangXuYang2013}\nJian Yang, Delin Chu, Lei Zhang, Yong Xu, and Jingyu Yang.\n\\newblock Sparse representation classifier steered discriminative projection\n  with applications to face recognition.\n\\newblock {\\em IEEE Transactions on Neural Networks and Learning Systems},\n  24(7):1023--1035, July 2013.\n\n\\bibitem{YangZhangShiuZhang2013}\nMeng Yang, Lei Zhang, Simon~C.K. Shiu, and David Zhang.\n\\newblock Gabor feature based robust representation and classification for face\n  recognition with {G}abor occlusion dictionary.\n\\newblock {\\em Pattern Recognition}, 46(7):1865 -- 1878, 2013.\n\n\\bibitem{YinLiuJinYang2012}\nJun Yin, Zhonghua Liu, Zhong Jin, and Wankou Yang.\n\\newblock Kernel sparse representation based classification.\n\\newblock {\\em Neurocomputing}, 77(1):120 -- 128, 2012.\n\n\\bibitem{YinGaoGuo2015}\nMing Yin, Junbin Gao, and Yi~Guo.\n\\newblock Nonlinear low-rank representation on {S}tiefel manifolds.\n\\newblock {\\em Electronics Letters}, 51(10):749--751, 2015.\n\n\\bibitem{YinGaoTienCai2014}\nMing Yin, Junbin Gao, David Tien, and Shuting Cai.\n\\newblock Blind image deblurring via coupled sparse representation.\n\\newblock {\\em Journal of Visual Communication and Image Representation},\n  25(5):814--821, July 2014.\n\n\\bibitem{ZhangZhouChangLiuYanWangLi2012}\nLi~Zhang, Wei-Da Zhou, Pei-Chann Chang, Jing Liu, Zhe Yan, Ting Wang, and\n  Fan-Zhang Li.\n\\newblock Kernel sparse representation-based classifier.\n\\newblock {\\em IEEE Transactions on Signal Processing}, 60(4):1684--1695, 2012.\n\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 43686, "prevtext": "\nwhere $\\mathbf{\\bar{x}}= \\Sigma^{-1/2}U^T \\kappa(X, \\mathcal{Y})$ and $\\bar{D} = \\Sigma^{-1/2}U^T$, given the \\text{SVD} of $\\kappa(\\mathcal{Y}, \\mathcal{Y})$ is $U\\Sigma U^T, UU^T = \\textbf{I}$.\n\n\\begin{proof} By expanding the $\\ell_2$-norm term in problem \\eqref{ProbSPD}, we have the following formulation,\n\n", "index": 45, "text": "\\begin{align}\n&\\mathop{\\min}\\limits_{\\mathbf{c}}  \\left\\| \\phi(X)-  \\sum\\limits_{i = 1}^N {c}_{i}\\phi(Y_i)  \\right\\|_2^2 \\notag \\\\\n&= \\mathop{\\min}\\limits_{\\mathbf{c}}  \\mathbf{c}^T \\kappa(\\mathcal{Y}, \\mathcal{Y})\\mathbf{c} - 2 \\mathbf{c}^T \\kappa(X, \\mathcal{Y}) + f(X) \\notag  \\\\\n&= \\mathop{\\min}\\limits_{\\mathbf{c}}  \\mathbf{c}^TU\\Sigma U^T \\mathbf{c}- 2\\mathbf{c}^TU\\Sigma^{-1/2}\\Sigma^{1/2} U^T \\kappa(X, \\mathcal{Y}) \\notag  \\\\ &+\\kappa(X, \\mathcal{Y})^T U\\Sigma^{-1/2}\\Sigma^{-1/2} U^T \\kappa(X, \\mathcal{Y}) \\notag  \\\\\n&=\\mathop{\\min}\\limits_{\\mathbf{c}} \\| \\Sigma^{-1/2} U^T \\kappa(X, \\mathcal{Y})- \\Sigma^{-1/2} U^T \\mathbf{c} \\|_2^2.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathop{\\min}\\limits_{\\mathbf{c}}\\left\\|\\phi(X)-\\sum\\limits_{i=1}%&#10;^{N}{c}_{i}\\phi(Y_{i})\\right\\|_{2}^{2}\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathop{\\min}\\limits_{\\mathbf{c}}\\mathbf{c}^{T}\\kappa(\\mathcal{Y%&#10;},\\mathcal{Y})\\mathbf{c}-2\\mathbf{c}^{T}\\kappa(X,\\mathcal{Y})+f(X)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><mrow><msup><mi>\ud835\udc1c</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\ud835\udc1c</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathop{\\min}\\limits_{\\mathbf{c}}\\mathbf{c}^{T}U\\Sigma U^{T}%&#10;\\mathbf{c}-2\\mathbf{c}^{T}U\\Sigma^{-1/2}\\Sigma^{1/2}U^{T}\\kappa(X,\\mathcal{Y})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><mrow><msup><mi>\ud835\udc1c</mi><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\ud835\udc1c</mi><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\kappa(X,\\mathcal{Y})^{T}U\\Sigma^{-1/2}\\Sigma^{-1/2}U^{T}\\kappa(%&#10;X,\\mathcal{Y})\" display=\"inline\"><mrow><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathop{\\min}\\limits_{\\mathbf{c}}\\|\\Sigma^{-1/2}U^{T}\\kappa(X,%&#10;\\mathcal{Y})-\\Sigma^{-1/2}U^{T}\\mathbf{c}\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><munder><mo movablelimits=\"false\">min</mo><mi>\ud835\udc1c</mi></munder><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msup><mi mathvariant=\"normal\">\u03a3</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]