[{"file": "1601.01422.tex", "nexttext": "\n\nwith ${\\bm{\\Phi}}:{\\mathbb{S}}_{++}^{n}\\to{\\mathbb{R}}^{n\\times m}$ for some $m\\in{\\mathbb{N}}$.\nIf ${\\bm{\\Phi}}({\\bm{X}}):={\\operatorname{logm}}({\\bm{X}})$, where ${\\operatorname{logm}}({\\bm{X}})$ takes the principal matrix logarithm of a strictly positive definite matrix ${\\bm{X}}$, the \nLog-Frobenius distance~\\cite{arsigny2006log} is obtained. Setting ${\\bm{\\Phi}}({\\bm{X}}):={\\bm{X}}^{p}$ gives the Power-Frobenius distance~\\cite{JayasumanaCVPR13}, while ${\\bm{\\Phi}}({\\bm{X}}):=\\text{chol}({\\bm{X}})$, where $\\text{chol}:{\\mathbb{S}}^{n}_{+}\\to{\\mathbb{R}}^{n\\times n}$ produces the Cholesky decomposition of ${\\bm{X}}$ such that ${\\bm{X}} = \\text{chol}({\\bm{X}})\\text{chol}({\\bm{X}})^\\top$, yields the Cholesky-Frobenius distance~\\cite{Dryden09a}. These metrics are pre-defined before the employment of machine learning algorithms, and are not adaptive to the data to be analyzed. Meanwhile, for categorization of vectorial data, supervised learning for fitting metrics to the task has been proven to significantly increase the performance of the distance-based classifier~\\cite{DavKulJaiSraDhi07a, KatNag10a,RelNagKat16a}.\n\nIn this paper, we introduce a parametric distance measure between covariance descriptors and present novel metric learning algorithms to determine the parameters of the distance measure function. The learning problem is formulated as the Bregman projection onto the intersections of half-spaces. This kind of problem can be solved by the Dykstra algorithm~\\cite{Censor98,Dykstra83}, which chooses a single half-space in a cyclic order and projects a current solution to the half-space. We developed an efficient technique for projection onto a single half-space. Furthermore, we empirically found that selecting the half-space stochastically, rather than in a cyclic order, dramatically increases the speed of converging to an optimal solution.\n\n\\subsection{Related work}\n\nTo the best of our knowledge, Vemulapalli et al. (2015)~\\cite{vemulapalli2015riemannian}\nwere the first to introduce the supervised metric\nlearning approach for covariance descriptors. They vectorized the matrix logarithms of the covariance descriptors to apply existing metric learning methods to the vectorizations of\nmatrices. The dimensionality of the vectorizations is\n$n(n+1)/2$ when the size of the covariance matrices are\n$n\\times n$. Thus, the size of the Mahalanobis matrix is\n$n(n+1)/2\\times n(n+1)/2$, which is computationally\nprohibitive when $n$ is large.\n\nOur approach is an extension of the distance measure of Huang et al.~\\cite{ZhiwuHuang15a}, which is based on the Log-Euclidean metric, with their loss function being a special case of our formulation. They also adopted the cyclic Dykstra algorithm for learning the Mahalanobis-like matrix. However, they misused the Woodbury matrix inversion formula when deriving the projection onto a single half-space, therefore, their algorithm has no theoretical guarantee of converging to the optimal solution. In this paper, their update rule is corrected by presenting a new technique that projects a current solution to a single half-space within $O(n^{3})$ computational time.\n\nYger and Sugiyama~\\cite{yger2015supervised} devised a different formulation of metric learning. They introduced the congruent transform~\\cite{Bhatia-book09} and measures distances between the transformations of covariance\ndescriptors. An objective function based on the kernel target alignment~\\cite{Cristianini01kta} is employed to determine the transformation parameters. Compared to their algorithm, our algorithm has the capability to monitor the upper bound of the objective gap, i.e. the difference between the current objective and the minimum. This implies that the resultant solution is ensured to be $\\epsilon$-suboptimal if the algorithm\u00e2\u0080\u0099s convergence criterion is set such that the objective gap upper bound is less than a very small number $\\epsilon$. Since Yger and Sugiyama~\\cite{yger2015supervised}  employed a gradient method for learning the congruent transform, there is no way to know the objective gap.\n\n\\subsection{Contributions}\n\nOur contributions of this paper can be\nsummarized as follows.\n\n\\begin{itemize}\n  \n  \\item For metric learning on positive semidefinite cone, we developed a new algorithm based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iterate, and runs at $O(n^{3})$ time.\n\n\\item\nWe present an upper-bound for the objective gap which provides a stopping criterion and ensures the optimality of the solution.\n\n\\item\n  We empirically found that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution.\n\n\\item\nWe show that our approach yields promising experimental results on pattern recognition tasks.\n\n\\end{itemize}\n\n\\subsection{Notation}\n\n\nWe denote vectors by bold-faced lower-case letters and\nmatrices by bold-faced upper-case letters.\n\nEntries of vectors and matrices are not bold-faced.\n\nThe transposition of a matrix ${\\bm{A}}$ is denoted by ${\\bm{A}}^{\\top}$,\nand the inverse of ${\\bm{A}}$ is by ${\\bm{A}}^{-1}$.\n\nThe $n\\times n$ identity matrix is denoted by ${\\bm{I}}_{n}$. \nThe subscript is often omitted. \n\nThe $m\\times n$ zero matrix is denoted by ${\\bm{O}}_{m\\times n}$. \nThe subscript is often omitted. \n\n\n\nThe $n$-dimensional vector all of whose entries are one is denoted by ${\\bm{1}}_{n}$.\n\nWe use ${\\mathbb{R}}$ and ${\\mathbb{N}}$ to denote the set of real and natural numbers,\n${\\mathbb{R}}^{n}$ and ${\\mathbb{N}}^{n}$ to denote the set of $n$-dimensional real and natural vectors,\nand ${\\mathbb{R}}^{m\\times n}$ to denote the set of $m\\times n$ real matrices.\n\nFor any $n\\in{\\mathbb{N}}$, we use ${\\mathbb{N}}_{n}$ to denote the set of natural numbers less than or equal to $n$.\n\n\n\n\n\n\n\n\nLet us define \n${\\mathbb{R}}_{+}:=\\{ x\\in{\\mathbb{R}} \\,|\\, x\\ge 0 \\}$,\n${\\mathbb{R}}_{++}:=\\{ x\\in{\\mathbb{R}} \\,|\\, x> 0 \\}$,  \n${\\mathbb{R}}_{+}^{n}:=\\{ x\\in{\\mathbb{R}}^{n} \\,|\\, {\\bm{x}}\\ge {\\bm{0}}_{p} \\}$, and \n${\\mathbb{R}}_{++}^{n}:=\\{ x\\in{\\mathbb{R}}^{n} \\,|\\, {\\bm{x}}> {\\bm{0}}_{p} \\}$. \n\nThe relational operator $\\succ$ denotes the generalized inequality\nassociated with the strictly positive definite cone.\n\n\nWe use\n${\\mathbb{S}}^{n}$ to denote the set of symmetric $n\\times n$ matrices.\n${\\mathbb{S}}_{+}^{n}$ to denote the set of symmetric positive semi-definite $n\\times n$ matrices,\nand\n${\\mathbb{S}}_{++}^{n}$ to denote the set of symmetric strictly positive definite $n\\times n$ matrices.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor any ${\\bm{x}} = \\left[x_{1},\\dots,x_{n}\\right]^\\top\\in{\\mathbb{R}}^{n}$, \n$\\text{diag}({\\bm{x}})$ is defined as an $n\\times n$ \ndiagonal matrix whose diagonal entries are \n$x_{1},\\dots,x_{n}$. \n\nFor any $n\\times n$ square matrix ${\\bm{X}}$, its trace is denoted by \n$\\text{tr}({\\bm{X}})$. \n\nFor any ${\\bm{x}},{\\bm{y}}\\in{\\mathbb{R}}^{n}$, define $\\left<{\\bm{x}},{\\bm{y}}\\right> := \\sum_{i=1}^{n}x_{i}y_{i}$\nwhere $x_{i}$ and $y_{i}$ is the $i$-th entry of ${\\bm{x}}$ and ${\\bm{y}}$, respectively. \n\nFor any ${\\bm{X}},{\\bm{Y}}\\in{\\mathbb{R}}^{m\\times n}$, \ndefine $\\left<{\\bm{X}},{\\bm{Y}}\\right> := \\sum_{i=1}^{m}\\sum_{j=1}^{n}X_{i,j}Y_{i,j}$\nwhere $X_{i,j}$ and $Y_{i,j}$ is the $(i,j)$-th entry of ${\\bm{X}}$ and ${\\bm{Y}}$, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n${\\mathbb{O}}_{n}$ is used to denote the set of $n\\times n$ orthonormal\nmatrices, i.e.\n${\\mathbb{O}}_{n} := \\{{\\bm{A}} \\in{\\mathbb{R}}^{n\\times n} \\,|\\, {\\bm{A}}^\\top {\\bm{A}} = {\\bm{I}}_{n}\\}.$ \n\n\n\n\n\n\n\n\n\\section{Our Metric Learning Problem}\n\n\\subsection{Parametric distance measure on ${\\mathbb{S}}_{+}^{n}$}\n\nWe introduce the following distance measure for covariance descriptors ${\\bm{X}}_{1},{\\bm{X}}_{2}\\in{\\mathbb{S}}_{+}^{n}$:\n\n\n", "itemtype": "equation", "pos": 2830, "prevtext": "\n\n\\begin{center}\n{\\Large\n  Stochastic Dykstra Algorithms for Metric Learning\n  on Positive Semi-Definite Cone}\n\\\\\n\\vspace{1cm}\n       {\\large\n         Tomoki Matsuzawa${}^{\\dagger}$, \n         Raissa Relator${}^{\\diamond}$,\n         Jun Sese${}^{\\diamond}$, \n         Tsuyoshi Kato${}^{\\dagger,\\ddagger,*}$}\n\\\\\n\\vspace{1cm}\n\\begin{tabular}{lp{0.7\\textwidth}}\n${}^\\dagger$ & \nFaculty of Science and Engineering, Gunma University, \nKiryu-shi, Gunma, 326--0338, Japan.  \n\\\\\n${}^\\ddagger$ &\nCenter for Informational Biology, Ochanomizu University, \nBunkyo-ku, Tokyo,\n112--8610, Japan. n\n\\\\\n${}^\\diamond$ &\nBRD, AIST, Koto-ku, Tokyo, 135--0064, Japan.\n\\\\\n\\end{tabular}\n\\end{center}\n\n\\nocite{KatTsuAsa05a,RelNagKat16a}\n\n\\begin{abstract}\n  \nRecently, covariance descriptors have received much attention as powerful representations of set of points. In this research, we present a new metric learning algorithm for covariance descriptors based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iteration, and runs at $O(n^3)$ time. We empirically demonstrate that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution. Furthermore, we show that our approach yields promising experimental results on pattern recognition tasks.\n\n\\end{abstract}\n\\section{Introduction}\n\nLearning with example objects characterized by a set of several points, instead of a single point, in a feature space is an important task in the computer vision and pattern recognition community. In the case of visual categorization of still images, many local image descriptors such as SIFT~\\cite{lowe2004distinctive} are extracted from an input image to form a single vector such as a Bag-of-Visual-Words vector or a Fisher Vector~\\cite{MatRelTak15a,perronnin2010improving}. For image set classification, a surge of methods have been developed in the last decade, and probabilistic models\n\\cite{shakhnarovich2002face} or kernels~\\cite{RelHirItoKat14a} are introduced to describe the image set. Alternative descriptors are the covariance descriptors, which have received much attention as a powerful representation of a set of points.\n\nThe performance of categorizing covariance descriptors depends on the metric that is used to measure the distances between them.\nTo compare covariance descriptors, a variety of distance measures such as affine invariant Riemannian metric~\\cite{pennec2006riemannian}, Stein metric~\\cite{sra2012a}, J-divergence~\\cite{wang2004affine}, Frobenius distance~\\cite{JayasumanaCVPR13}, and Log-Frobenius distance~\\cite{arsigny2006log}, have been discussed in existing literature. Some of them are designed from their geometrical properties, but some are not. Many of these distance measures are expressed in the form\n\n\n", "index": 1, "text": "\\begin{align*}\nD_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2}) \n:=\n\\left\\|{\\bm{\\Phi}}({\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right\\|^{2}_{\\text{F}},    \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2}):=\\left\\|{\\bm{\\Phi}}({%&#10;\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right\\|^{2}_{\\text{F}},\" display=\"inline\"><mrow><mrow><mrow><msub><mi>D</mi><mi>\ud835\udebd</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udc7f</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mtext>F</mtext><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhere ${\\bm{W}}\\in{\\mathbb{S}}_{+}^{n}$ is the parameter of this distance measure function. If ${\\bm{W}}$ is strictly positive definite and  ${\\bm{\\Phi}}$ is bijective, then this distance measure $D_{{\\bm{\\Phi}}}(\\cdot,\\cdot;{\\bm{W}}):{\\mathbb{S}}_{+}^{n}\\times {\\mathbb{S}}_{+}^{n}\\to{\\mathbb{R}}$ is a metric because all of the following conditions are satisfied:\n(i) non-negativity:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) \\ge 0; $\n\n(ii) identity of indiscernibles:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = 0 \\text{ iff } {\\bm{X}}_{1}={\\bm{X}}_{2};$\n\n(iii) symmetry:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = D_{{\\bm{\\Phi}}}({\\bm{X}}_{2},{\\bm{X}}_{1};{\\bm{W}});$\n\n(iv) triangle inequality:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{3};{\\bm{W}}) \\le\nD_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) + D_{{\\bm{\\Phi}}}({\\bm{X}}_{2},{\\bm{X}}_{3};{\\bm{W}}).$\n\nIf the parameter matrix ${\\bm{W}}$ is singular, $D_{{\\bm{\\Phi}}}(\\cdot,\\cdot;{\\bm{W}})$ is a pseudometric, and the identity of indiscernibles is changed to the following property:  \nFor any ${\\bm{X}}_{1}\\in{\\mathbb{S}}_{++}^{n}$, $D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{1};{\\bm{W}}) = 0$ holds, while $D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = 0$ occurs for some non-identical positive semi-definite matrices ${\\bm{X}}_{1}$ and ${\\bm{X}}_{2}$.  \n\n\\subsection{Formulations of the learning problems}\n\nTo determine the value of the parameter matrix~${\\bm{W}}$, we pose a constrained optimization problem based on the idea of ITML~\\cite{DavKulJaiSraDhi07a}. We now consider a multi-class categorization problem. Let $n_{c}$ be the number of classes, and the class labels are represented by natural numbers in ${\\mathbb{N}}_{n_{c}}$. Suppose weare given\n\n", "itemtype": "equation", "pos": 10731, "prevtext": "\n\nwith ${\\bm{\\Phi}}:{\\mathbb{S}}_{++}^{n}\\to{\\mathbb{R}}^{n\\times m}$ for some $m\\in{\\mathbb{N}}$.\nIf ${\\bm{\\Phi}}({\\bm{X}}):={\\operatorname{logm}}({\\bm{X}})$, where ${\\operatorname{logm}}({\\bm{X}})$ takes the principal matrix logarithm of a strictly positive definite matrix ${\\bm{X}}$, the \nLog-Frobenius distance~\\cite{arsigny2006log} is obtained. Setting ${\\bm{\\Phi}}({\\bm{X}}):={\\bm{X}}^{p}$ gives the Power-Frobenius distance~\\cite{JayasumanaCVPR13}, while ${\\bm{\\Phi}}({\\bm{X}}):=\\text{chol}({\\bm{X}})$, where $\\text{chol}:{\\mathbb{S}}^{n}_{+}\\to{\\mathbb{R}}^{n\\times n}$ produces the Cholesky decomposition of ${\\bm{X}}$ such that ${\\bm{X}} = \\text{chol}({\\bm{X}})\\text{chol}({\\bm{X}})^\\top$, yields the Cholesky-Frobenius distance~\\cite{Dryden09a}. These metrics are pre-defined before the employment of machine learning algorithms, and are not adaptive to the data to be analyzed. Meanwhile, for categorization of vectorial data, supervised learning for fitting metrics to the task has been proven to significantly increase the performance of the distance-based classifier~\\cite{DavKulJaiSraDhi07a, KatNag10a,RelNagKat16a}.\n\nIn this paper, we introduce a parametric distance measure between covariance descriptors and present novel metric learning algorithms to determine the parameters of the distance measure function. The learning problem is formulated as the Bregman projection onto the intersections of half-spaces. This kind of problem can be solved by the Dykstra algorithm~\\cite{Censor98,Dykstra83}, which chooses a single half-space in a cyclic order and projects a current solution to the half-space. We developed an efficient technique for projection onto a single half-space. Furthermore, we empirically found that selecting the half-space stochastically, rather than in a cyclic order, dramatically increases the speed of converging to an optimal solution.\n\n\\subsection{Related work}\n\nTo the best of our knowledge, Vemulapalli et al. (2015)~\\cite{vemulapalli2015riemannian}\nwere the first to introduce the supervised metric\nlearning approach for covariance descriptors. They vectorized the matrix logarithms of the covariance descriptors to apply existing metric learning methods to the vectorizations of\nmatrices. The dimensionality of the vectorizations is\n$n(n+1)/2$ when the size of the covariance matrices are\n$n\\times n$. Thus, the size of the Mahalanobis matrix is\n$n(n+1)/2\\times n(n+1)/2$, which is computationally\nprohibitive when $n$ is large.\n\nOur approach is an extension of the distance measure of Huang et al.~\\cite{ZhiwuHuang15a}, which is based on the Log-Euclidean metric, with their loss function being a special case of our formulation. They also adopted the cyclic Dykstra algorithm for learning the Mahalanobis-like matrix. However, they misused the Woodbury matrix inversion formula when deriving the projection onto a single half-space, therefore, their algorithm has no theoretical guarantee of converging to the optimal solution. In this paper, their update rule is corrected by presenting a new technique that projects a current solution to a single half-space within $O(n^{3})$ computational time.\n\nYger and Sugiyama~\\cite{yger2015supervised} devised a different formulation of metric learning. They introduced the congruent transform~\\cite{Bhatia-book09} and measures distances between the transformations of covariance\ndescriptors. An objective function based on the kernel target alignment~\\cite{Cristianini01kta} is employed to determine the transformation parameters. Compared to their algorithm, our algorithm has the capability to monitor the upper bound of the objective gap, i.e. the difference between the current objective and the minimum. This implies that the resultant solution is ensured to be $\\epsilon$-suboptimal if the algorithm\u00e2\u0080\u0099s convergence criterion is set such that the objective gap upper bound is less than a very small number $\\epsilon$. Since Yger and Sugiyama~\\cite{yger2015supervised}  employed a gradient method for learning the congruent transform, there is no way to know the objective gap.\n\n\\subsection{Contributions}\n\nOur contributions of this paper can be\nsummarized as follows.\n\n\\begin{itemize}\n  \n  \\item For metric learning on positive semidefinite cone, we developed a new algorithm based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iterate, and runs at $O(n^{3})$ time.\n\n\\item\nWe present an upper-bound for the objective gap which provides a stopping criterion and ensures the optimality of the solution.\n\n\\item\n  We empirically found that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution.\n\n\\item\nWe show that our approach yields promising experimental results on pattern recognition tasks.\n\n\\end{itemize}\n\n\\subsection{Notation}\n\n\nWe denote vectors by bold-faced lower-case letters and\nmatrices by bold-faced upper-case letters.\n\nEntries of vectors and matrices are not bold-faced.\n\nThe transposition of a matrix ${\\bm{A}}$ is denoted by ${\\bm{A}}^{\\top}$,\nand the inverse of ${\\bm{A}}$ is by ${\\bm{A}}^{-1}$.\n\nThe $n\\times n$ identity matrix is denoted by ${\\bm{I}}_{n}$. \nThe subscript is often omitted. \n\nThe $m\\times n$ zero matrix is denoted by ${\\bm{O}}_{m\\times n}$. \nThe subscript is often omitted. \n\n\n\nThe $n$-dimensional vector all of whose entries are one is denoted by ${\\bm{1}}_{n}$.\n\nWe use ${\\mathbb{R}}$ and ${\\mathbb{N}}$ to denote the set of real and natural numbers,\n${\\mathbb{R}}^{n}$ and ${\\mathbb{N}}^{n}$ to denote the set of $n$-dimensional real and natural vectors,\nand ${\\mathbb{R}}^{m\\times n}$ to denote the set of $m\\times n$ real matrices.\n\nFor any $n\\in{\\mathbb{N}}$, we use ${\\mathbb{N}}_{n}$ to denote the set of natural numbers less than or equal to $n$.\n\n\n\n\n\n\n\n\nLet us define \n${\\mathbb{R}}_{+}:=\\{ x\\in{\\mathbb{R}} \\,|\\, x\\ge 0 \\}$,\n${\\mathbb{R}}_{++}:=\\{ x\\in{\\mathbb{R}} \\,|\\, x> 0 \\}$,  \n${\\mathbb{R}}_{+}^{n}:=\\{ x\\in{\\mathbb{R}}^{n} \\,|\\, {\\bm{x}}\\ge {\\bm{0}}_{p} \\}$, and \n${\\mathbb{R}}_{++}^{n}:=\\{ x\\in{\\mathbb{R}}^{n} \\,|\\, {\\bm{x}}> {\\bm{0}}_{p} \\}$. \n\nThe relational operator $\\succ$ denotes the generalized inequality\nassociated with the strictly positive definite cone.\n\n\nWe use\n${\\mathbb{S}}^{n}$ to denote the set of symmetric $n\\times n$ matrices.\n${\\mathbb{S}}_{+}^{n}$ to denote the set of symmetric positive semi-definite $n\\times n$ matrices,\nand\n${\\mathbb{S}}_{++}^{n}$ to denote the set of symmetric strictly positive definite $n\\times n$ matrices.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor any ${\\bm{x}} = \\left[x_{1},\\dots,x_{n}\\right]^\\top\\in{\\mathbb{R}}^{n}$, \n$\\text{diag}({\\bm{x}})$ is defined as an $n\\times n$ \ndiagonal matrix whose diagonal entries are \n$x_{1},\\dots,x_{n}$. \n\nFor any $n\\times n$ square matrix ${\\bm{X}}$, its trace is denoted by \n$\\text{tr}({\\bm{X}})$. \n\nFor any ${\\bm{x}},{\\bm{y}}\\in{\\mathbb{R}}^{n}$, define $\\left<{\\bm{x}},{\\bm{y}}\\right> := \\sum_{i=1}^{n}x_{i}y_{i}$\nwhere $x_{i}$ and $y_{i}$ is the $i$-th entry of ${\\bm{x}}$ and ${\\bm{y}}$, respectively. \n\nFor any ${\\bm{X}},{\\bm{Y}}\\in{\\mathbb{R}}^{m\\times n}$, \ndefine $\\left<{\\bm{X}},{\\bm{Y}}\\right> := \\sum_{i=1}^{m}\\sum_{j=1}^{n}X_{i,j}Y_{i,j}$\nwhere $X_{i,j}$ and $Y_{i,j}$ is the $(i,j)$-th entry of ${\\bm{X}}$ and ${\\bm{Y}}$, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n${\\mathbb{O}}_{n}$ is used to denote the set of $n\\times n$ orthonormal\nmatrices, i.e.\n${\\mathbb{O}}_{n} := \\{{\\bm{A}} \\in{\\mathbb{R}}^{n\\times n} \\,|\\, {\\bm{A}}^\\top {\\bm{A}} = {\\bm{I}}_{n}\\}.$ \n\n\n\n\n\n\n\n\n\\section{Our Metric Learning Problem}\n\n\\subsection{Parametric distance measure on ${\\mathbb{S}}_{+}^{n}$}\n\nWe introduce the following distance measure for covariance descriptors ${\\bm{X}}_{1},{\\bm{X}}_{2}\\in{\\mathbb{S}}_{+}^{n}$:\n\n\n", "index": 3, "text": "\\begin{align*}\nD_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) \n:=\n\\left<{\\bm{W}},\\left({\\bm{\\Phi}}({\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right)\n\\left({\\bm{\\Phi}}({\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right)^\\top\n\\right>,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}):=\\left&lt;{\\bm{%&#10;W}},\\left({\\bm{\\Phi}}({\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right)\\left({\\bm%&#10;{\\Phi}}({\\bm{X}}_{1})-{\\bm{\\Phi}}({\\bm{X}}_{2})\\right)^{\\top}\\right&gt;,\" display=\"inline\"><mrow><mrow><mrow><msub><mi>D</mi><mi>\ud835\udebd</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udc7f</mi><mn>2</mn></msub><mo>;</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mo>\u27e8</mo><mi>\ud835\udc7e</mi><mo>,</mo><mrow><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u22a4</mo></msup></mrow><mo>\u27e9</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nas a training dataset, where ${\\bm{X}}_{i}$ is\nthe covariance descriptor of the $i$-th example, and\n$\\omega_{i}$ is its class label. From the $\\ell$ examples,\n$K$ pairs\n$(i_{1},j_{1}),\\dots,(i_{K},j_{K})\\in{\\mathbb{N}}_{\\ell}\\times{\\mathbb{N}}_{\\ell}$\nare picked to give, to each pair, the\nfollowing constraint:\n\n\n", "itemtype": "equation", "pos": 12735, "prevtext": "\n\nwhere ${\\bm{W}}\\in{\\mathbb{S}}_{+}^{n}$ is the parameter of this distance measure function. If ${\\bm{W}}$ is strictly positive definite and  ${\\bm{\\Phi}}$ is bijective, then this distance measure $D_{{\\bm{\\Phi}}}(\\cdot,\\cdot;{\\bm{W}}):{\\mathbb{S}}_{+}^{n}\\times {\\mathbb{S}}_{+}^{n}\\to{\\mathbb{R}}$ is a metric because all of the following conditions are satisfied:\n(i) non-negativity:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) \\ge 0; $\n\n(ii) identity of indiscernibles:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = 0 \\text{ iff } {\\bm{X}}_{1}={\\bm{X}}_{2};$\n\n(iii) symmetry:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = D_{{\\bm{\\Phi}}}({\\bm{X}}_{2},{\\bm{X}}_{1};{\\bm{W}});$\n\n(iv) triangle inequality:\n$D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{3};{\\bm{W}}) \\le\nD_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) + D_{{\\bm{\\Phi}}}({\\bm{X}}_{2},{\\bm{X}}_{3};{\\bm{W}}).$\n\nIf the parameter matrix ${\\bm{W}}$ is singular, $D_{{\\bm{\\Phi}}}(\\cdot,\\cdot;{\\bm{W}})$ is a pseudometric, and the identity of indiscernibles is changed to the following property:  \nFor any ${\\bm{X}}_{1}\\in{\\mathbb{S}}_{++}^{n}$, $D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{1};{\\bm{W}}) = 0$ holds, while $D_{{\\bm{\\Phi}}}({\\bm{X}}_{1},{\\bm{X}}_{2};{\\bm{W}}) = 0$ occurs for some non-identical positive semi-definite matrices ${\\bm{X}}_{1}$ and ${\\bm{X}}_{2}$.  \n\n\\subsection{Formulations of the learning problems}\n\nTo determine the value of the parameter matrix~${\\bm{W}}$, we pose a constrained optimization problem based on the idea of ITML~\\cite{DavKulJaiSraDhi07a}. We now consider a multi-class categorization problem. Let $n_{c}$ be the number of classes, and the class labels are represented by natural numbers in ${\\mathbb{N}}_{n_{c}}$. Suppose weare given\n\n", "index": 5, "text": "\\begin{align*}\n  ({\\bm{X}}_{1},\\omega_{1}),\\dots,({\\bm{X}}_{\\ell},\\omega_{\\ell})\\in{\\mathbb{S}}_{+}^{n}\\times{\\mathbb{N}}_{n_{c}}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\bm{X}}_{1},\\omega_{1}),\\dots,({\\bm{X}}_{\\ell},\\omega_{\\ell})%&#10;\\in{\\mathbb{S}}_{+}^{n}\\times{\\mathbb{N}}_{n_{c}}\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c9</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>,</mo><msub><mi>\u03c9</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mrow><msubsup><mi>\ud835\udd4a</mi><mo>+</mo><mi>n</mi></msubsup><mo>\u00d7</mo><msub><mi>\u2115</mi><msub><mi>n</mi><mi>c</mi></msub></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhere, when $\\xi_{k}=1$,\nthe two constants~$b_{\\text{ub}}$ and $b_{\\text{lb}}$,\nrespectively, are the upper-bound of the distances between any two examples in the same class and the lower-bound of the distances between any two examples in different classes.\nNow let us define for $k\\in{\\mathbb{N}}_{K}$, \n\n\n", "itemtype": "equation", "pos": 13192, "prevtext": "\n\nas a training dataset, where ${\\bm{X}}_{i}$ is\nthe covariance descriptor of the $i$-th example, and\n$\\omega_{i}$ is its class label. From the $\\ell$ examples,\n$K$ pairs\n$(i_{1},j_{1}),\\dots,(i_{K},j_{K})\\in{\\mathbb{N}}_{\\ell}\\times{\\mathbb{N}}_{\\ell}$\nare picked to give, to each pair, the\nfollowing constraint:\n\n\n", "index": 7, "text": "\\begin{align}\\label{eq:con01}\nD_{{\\bm{\\Phi}}}({\\bm{X}}_{i_{k}},{\\bm{X}}_{j_{k}};{\\bm{W}}) \n\\begin{cases}\n\\le b_{\\text{ub}}\\xi_{k},\n&\\qquad \\text{if $\\omega_{i_{k}}=\\omega_{j_{k}}$}, \n\\\\\n\\ge b_{\\text{lb}}\\xi_{k},\n&\\qquad \\text{if $\\omega_{i_{k}}\\ne \\omega_{j_{k}}$},  \n\\end{cases}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D_{{\\bm{\\Phi}}}({\\bm{X}}_{i_{k}},{\\bm{X}}_{j_{k}};{\\bm{W}})%&#10;\\begin{cases}\\leq b_{\\text{ub}}\\xi_{k},&amp;\\qquad\\text{if $\\omega_{i_{k}}=\\omega_%&#10;{j_{k}}$},\\\\&#10;\\geq b_{\\text{lb}}\\xi_{k},&amp;\\qquad\\text{if $\\omega_{i_{k}}\\neq\\omega_{j_{k}}$},%&#10;\\end{cases}\" display=\"inline\"><mrow><msub><mi>D</mi><mi>\ud835\udebd</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>,</mo><msub><mi>\ud835\udc7f</mi><msub><mi>j</mi><mi>k</mi></msub></msub><mo>;</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><msub><mi>b</mi><mtext>ub</mtext></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>=</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2265</mo><mrow><msub><mi>b</mi><mtext>lb</mtext></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>\u2260</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nUnder the constraint \\eqref{eq:con01}, we wish to find ${\\bm{W}}$ and $\\xi_{k}$ such that ${\\bm{W}}$ is not much deviated from the identity matrix and $\\xi_{k}$ is close to one. From this motivation, we pose the following problem:\n\n\n", "itemtype": "equation", "pos": 13791, "prevtext": "\n\nwhere, when $\\xi_{k}=1$,\nthe two constants~$b_{\\text{ub}}$ and $b_{\\text{lb}}$,\nrespectively, are the upper-bound of the distances between any two examples in the same class and the lower-bound of the distances between any two examples in different classes.\nNow let us define for $k\\in{\\mathbb{N}}_{K}$, \n\n\n", "index": 9, "text": "\\begin{align*}\n  y_{k} :=\n  \\begin{cases}\n    +1, \\qquad \\text{if }\\omega_{i_{k}}=\\omega_{j_{k}},\n    \\\\\n    -1, \\qquad \\text{if }\\omega_{i_{k}}\\ne\\omega_{j_{k}},\n  \\end{cases}\n  \\qquad\n  \\text{and}\n  \\qquad\n  b_{k} :=\n  \\begin{cases}\n    b_{\\text{ub}}, \\qquad \\text{if }\\omega_{i_{k}}=\\omega_{j_{k}},\n    \\\\\n    b_{\\text{lb}}, \\qquad \\text{if }\\omega_{i_{k}}\\ne\\omega_{j_{k}}.\n  \\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle y_{k}:=\\begin{cases}+1,\\qquad\\text{if }\\omega_{i_{k}}=\\omega_{j_%&#10;{k}},\\\\&#10;-1,\\qquad\\text{if }\\omega_{i_{k}}\\neq\\omega_{j_{k}},\\end{cases}\\qquad\\text{and%&#10;}\\qquad b_{k}:=\\begin{cases}b_{\\text{ub}},\\qquad\\text{if }\\omega_{i_{k}}=%&#10;\\omega_{j_{k}},\\\\&#10;b_{\\text{lb}},\\qquad\\text{if }\\omega_{i_{k}}\\neq\\omega_{j_{k}}.\\end{cases}\" display=\"inline\"><mrow><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>:=</mo><mrow><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mo>+</mo><mn>1</mn></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub></mrow></mrow><mo>=</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow><mo>,</mo></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mo>-</mo><mn>1</mn></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub></mrow></mrow><mo>\u2260</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow><mo>,</mo></mrow></mtd><mtd/></mtr></mtable></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>and</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mi>b</mi><mi>k</mi></msub><mo>:=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mi>b</mi><mtext>ub</mtext></msub><mo rspace=\"22.5pt\">,</mo><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub></mrow></mrow><mo>=</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow><mo>,</mo></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mi>b</mi><mtext>lb</mtext></msub><mo rspace=\"22.5pt\">,</mo><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03c9</mi><msub><mi>i</mi><mi>k</mi></msub></msub></mrow></mrow><mo>\u2260</mo><msub><mi>\u03c9</mi><msub><mi>j</mi><mi>k</mi></msub></msub></mrow><mo>.</mo></mrow></mtd><mtd/></mtr></mtable></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhere\n$\\text{BD}_{\\varphi}(\\cdot,\\cdot):({\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K})\\times ({\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K})\\to{\\mathbb{R}}_{+}$\nis the \\emph{Bregman divergence}~\\cite{KatTakOma13a}. \nOnly if $({\\bm{W}},{\\bm{\\xi}})=({\\bm{I}},{\\bm{1}})$ will the divergence\n$\\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}}))$ become zero,\nand the value of divergence becomes larger if\n$({\\bm{W}},{\\bm{\\xi}})$ is more deviated from $({\\bm{I}},{\\bm{1}})$.\nThe definition of the Bregman divergence contains a seed function $\\varphi: {\\mathbb{S}}_{++}^{n}\\times {\\mathbb{R}}_{++}^{K}\\to {\\mathbb{R}}$ which is assumed to be continuously differentiable and strictly convex.  For some $\\varphi$, the Bregman divergence is defined as\n\n\n", "itemtype": "equation", "pos": 14429, "prevtext": "\n\nUnder the constraint \\eqref{eq:con01}, we wish to find ${\\bm{W}}$ and $\\xi_{k}$ such that ${\\bm{W}}$ is not much deviated from the identity matrix and $\\xi_{k}$ is close to one. From this motivation, we pose the following problem:\n\n\n", "index": 11, "text": "\\begin{align}\\label{eq:soft-covitml-breg}\n\\text{min }\\quad&\n\\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}})) , \n\\qquad\n\\text{wrt }\\quad {\\bm{W}}\\in{\\mathbb{S}}_{++}^{n}, \\quad\n{\\bm{\\xi}} = \\left[\\xi_{1},\\dots,\\xi_{K}\\right]^\\top \\in{\\mathbb{R}}_{++}^{K}, \n\\\\\n\\text{subject to }\\quad&\n\\forall k\\in{\\mathbb{N}}_{K}, \\quad\ny_{k} D_{{\\bm{\\Phi}}}({\\bm{X}}_{i_{k}},{\\bm{X}}_{j_{k}};{\\bm{W}}) \n\\le y_{k} b_{k}\\xi_{k}, \n\\nonumber \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}})),%&#10;\\qquad\\text{wrt }\\quad{\\bm{W}}\\in{\\mathbb{S}}_{++}^{n},\\quad{\\bm{\\xi}}=\\left[%&#10;\\xi_{1},\\dots,\\xi_{K}\\right]^{\\top}\\in{\\mathbb{R}}_{++}^{K},\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><msub><mtext>BD</mtext><mi>\u03c6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo>,</mo><mi>\ud835\udf43</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc70</mi><mo>,</mo><mn>\ud835\udfcf</mn><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mtext>wrt\u00a0</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>\ud835\udc7e</mi></mrow><mo>\u2208</mo><msubsup><mi>\ud835\udd4a</mi><mrow><mi/><mo>+</mo><mo>+</mo></mrow><mi>n</mi></msubsup></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>\ud835\udf43</mi><mo>=</mo><msup><mrow><mo>[</mo><msub><mi>\u03be</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\u03be</mi><mi>K</mi></msub><mo>]</mo></mrow><mo>\u22a4</mo></msup><mo>\u2208</mo><msubsup><mi>\u211d</mi><mrow><mi/><mo>+</mo><mo>+</mo></mrow><mi>K</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall k\\in{\\mathbb{N}}_{K},\\quad y_{k}D_{{\\bm{\\Phi}}}({\\bm{X}}_%&#10;{i_{k}},{\\bm{X}}_{j_{k}};{\\bm{W}})\\leq y_{k}b_{k}\\xi_{k},\" display=\"inline\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><mi>k</mi></mrow><mo>\u2208</mo><msub><mi>\u2115</mi><mi>K</mi></msub></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>D</mi><mi>\ud835\udebd</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>,</mo><msub><mi>\ud835\udc7f</mi><msub><mi>j</mi><mi>k</mi></msub></msub><mo>;</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>b</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nfor\n${\\bm{\\Theta}},{\\bm{\\Theta}}_{0}\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$.\nThis implies that the quantities of the deviations of\nthe solution $({\\bm{W}},{\\bm{\\xi}})$ from $({\\bm{I}},{\\bm{1}})$ depend on\nthe definition of the seed function. In this study,\nthe seed function is assumed to be the sum of two terms:\n\n\n", "itemtype": "equation", "pos": 15641, "prevtext": "\n\nwhere\n$\\text{BD}_{\\varphi}(\\cdot,\\cdot):({\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K})\\times ({\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K})\\to{\\mathbb{R}}_{+}$\nis the \\emph{Bregman divergence}~\\cite{KatTakOma13a}. \nOnly if $({\\bm{W}},{\\bm{\\xi}})=({\\bm{I}},{\\bm{1}})$ will the divergence\n$\\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}}))$ become zero,\nand the value of divergence becomes larger if\n$({\\bm{W}},{\\bm{\\xi}})$ is more deviated from $({\\bm{I}},{\\bm{1}})$.\nThe definition of the Bregman divergence contains a seed function $\\varphi: {\\mathbb{S}}_{++}^{n}\\times {\\mathbb{R}}_{++}^{K}\\to {\\mathbb{R}}$ which is assumed to be continuously differentiable and strictly convex.  For some $\\varphi$, the Bregman divergence is defined as\n\n\n", "index": 13, "text": "\\begin{align*}\n\\text{BD}_{\\varphi}({\\bm{\\Theta}},{\\bm{\\Theta}}_{0})\n= \\varphi({\\bm{\\Theta}}) - \\varphi({\\bm{\\Theta}}_{0}) - \n\\left< \\nabla \\varphi({\\bm{\\Theta}}_{0}), {\\bm{\\Theta}}-{\\bm{\\Theta}}_{0} \\right>, \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{BD}_{\\varphi}({\\bm{\\Theta}},{\\bm{\\Theta}}_{0})=\\varphi({\\bm%&#10;{\\Theta}})-\\varphi({\\bm{\\Theta}}_{0})-\\left&lt;\\nabla\\varphi({\\bm{\\Theta}}_{0}),{%&#10;\\bm{\\Theta}}-{\\bm{\\Theta}}_{0}\\right&gt;,\" display=\"inline\"><mrow><mrow><mrow><msub><mtext>BD</mtext><mi>\u03c6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>,</mo><msub><mi>\ud835\udeaf</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udeaf</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mo>\u27e8</mo><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>\u03c6</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udeaf</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>\ud835\udeaf</mi><mo>-</mo><msub><mi>\ud835\udeaf</mi><mn>0</mn></msub></mrow><mo>\u27e9</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhere $c_{k}$ is a positive constant. \nThe first term $\\varphi_{\\text{r}}:{\\mathbb{S}}_{++}^{n}\\to{\\mathbb{R}}$ in the definition of the seed function is defined by $\\varphi_{\\text{r}}({\\bm{W}}):=- \\text{logdet}({\\bm{W}})$.As for the definition of the second term$\\varphi_{\\text{l}}:{\\mathbb{R}}_{++}\\to{\\mathbb{R}}$, we considered the following three functions:\n\n\n", "itemtype": "equation", "pos": 16192, "prevtext": "\n\nfor\n${\\bm{\\Theta}},{\\bm{\\Theta}}_{0}\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$.\nThis implies that the quantities of the deviations of\nthe solution $({\\bm{W}},{\\bm{\\xi}})$ from $({\\bm{I}},{\\bm{1}})$ depend on\nthe definition of the seed function. In this study,\nthe seed function is assumed to be the sum of two terms:\n\n\n", "index": 15, "text": "\\begin{align*}\n  \\varphi( {\\bm{W}}, {\\bm{\\xi}} )\n  := \\varphi_{\\text{r}}({\\bm{W}}) +\n  \\sum_{k=1}^{K}c_{k}\\varphi_{\\text{l}}(\\xi_{k}), \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\varphi({\\bm{W}},{\\bm{\\xi}}):=\\varphi_{\\text{r}}({\\bm{W}})+\\sum_{%&#10;k=1}^{K}c_{k}\\varphi_{\\text{l}}(\\xi_{k}),\" display=\"inline\"><mrow><mrow><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo>,</mo><mi>\ud835\udf43</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><msub><mi>\u03c6</mi><mtext>r</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>c</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThe Bregman divergences generated from three seed functions~$\\varphi_{\\text{is}}$, $\\varphi_{\\text{l2}}$, and $\\varphi_{\\text{e}}$, respectively, are referred to as \\emph{Itakura-Saito Bregman Divergence} (ISBD), \\emph{L2 Bregman Divergence} (L2BD), and \\emph{Relative Entropy Bregman Divergence} (REBD), where ISBD is equal to the objective function employed by Huang et al.~\\cite{ZhiwuHuang15a}.\n\\section{Stochastic Variants of Dykstra Algorithm}\n\nWe introduce the Dykstra algorithm~\\cite{Censor98,Dykstra83}\nto solve the optimization problem~\\eqref{eq:soft-covitml-breg}.\nThe original Dykstra algorithm~\\cite{Dykstra83}\nwas developed as a computational method that finds the\nEuclidean projection from a point onto the intersection of\nconvex sets. Censor \\& Reich~\\cite{Censor98} extended the algorithm\nto finding the Bregman projection from a point ${\\bm{x}}_{0}$ to a set ${\\mathcal{C}}$,\ndefined by\n\n\n", "itemtype": "equation", "pos": 16706, "prevtext": "\n\nwhere $c_{k}$ is a positive constant. \nThe first term $\\varphi_{\\text{r}}:{\\mathbb{S}}_{++}^{n}\\to{\\mathbb{R}}$ in the definition of the seed function is defined by $\\varphi_{\\text{r}}({\\bm{W}}):=- \\text{logdet}({\\bm{W}})$.As for the definition of the second term$\\varphi_{\\text{l}}:{\\mathbb{R}}_{++}\\to{\\mathbb{R}}$, we considered the following three functions:\n\n\n", "index": 17, "text": "\\begin{align*}\n\\varphi_{\\text{is}}(\\xi_{k}) &:= -\\log(\\xi_{k}), &\n\\varphi_{\\text{l2}}(\\xi_{k}) &:= \\frac{1}{2}\\xi_{k}^{2}, &\n\\varphi_{\\text{e}}(\\xi_{k}) &:= (\\log \\xi_{k} - 1)\\xi_{k}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\varphi_{\\text{is}}(\\xi_{k})\" display=\"inline\"><mrow><msub><mi>\u03c6</mi><mtext>is</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:=-\\log(\\xi_{k}),\" display=\"inline\"><mrow><mrow><mi/><mo>:=</mo><mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\varphi_{\\text{l2}}(\\xi_{k})\" display=\"inline\"><mrow><msub><mi>\u03c6</mi><mtext>l2</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle:=\\frac{1}{2}\\xi_{k}^{2},\" display=\"inline\"><mrow><mrow><mi/><mo>:=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mi>\u03be</mi><mi>k</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\varphi_{\\text{e}}(\\xi_{k})\" display=\"inline\"><mrow><msub><mi>\u03c6</mi><mtext>e</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m6\" class=\"ltx_Math\" alttext=\"\\displaystyle:=(\\log\\xi_{k}-1)\\xi_{k}.\" display=\"inline\"><mrow><mrow><mi/><mo>:=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nIn available literature related to stochastic gradient\ndescent methods and the\nvariants~\\cite{Bottou10a-sgd,Johnson13a-svrg,Roux12a-sag,Shalev-Shwartz11a-pegasos}\nthat minimize the regularized loss averaged\nover a set of examples, it is empirically\nshown that, rather than picking an example in a cyclic order, example selection in a stochastic order dramatically speeds up the\nconvergence to the optimal solution. Alternatively, some\nliterature reported that at the beginning of every epoch in\nthe gradient method, random permutation of the order of\nexamples also accelerates the convergence~\\cite{defazio2014finito}.\n\nMotivated by these facts, this study proposes the use of\nstochastic orders for selection of convex set components in\nthe Dykstra algorithm. We term the stochastic version of the\nDykstra algorithm as the \\emph{stochastic Dykstra algorithm}. In\nour case, every convex set component is one of $K$\nhalf-spaces, as will be described in a later discussion. There are, then, three ways to\nselect half-spaces:\n\n\\begin{itemize}\n\\item \\textbf{Cyclic}:\n  Pick a half-space in a cyclic order at each iteration.\n\\item \\textbf{Rand}:\n  Pick a half-space randomly at each iteration.\n\\item \\textbf{Perm}:\n  Permute the order of $K$ half-spaces randomly at the\n  beginning of each epoch.\n\\end{itemize}\n\nHereinafter, we assume to employ the ``Rand'' option,\nalthough replacing this option with one of the remaining two is\nstraightforward.\n\nIf every convex set component is a half-space, and the\n$k$-th convex set component~${\\mathcal{C}}_{k}$ is expressed as\n\n\n", "itemtype": "equation", "pos": 17809, "prevtext": "\n\nThe Bregman divergences generated from three seed functions~$\\varphi_{\\text{is}}$, $\\varphi_{\\text{l2}}$, and $\\varphi_{\\text{e}}$, respectively, are referred to as \\emph{Itakura-Saito Bregman Divergence} (ISBD), \\emph{L2 Bregman Divergence} (L2BD), and \\emph{Relative Entropy Bregman Divergence} (REBD), where ISBD is equal to the objective function employed by Huang et al.~\\cite{ZhiwuHuang15a}.\n\\section{Stochastic Variants of Dykstra Algorithm}\n\nWe introduce the Dykstra algorithm~\\cite{Censor98,Dykstra83}\nto solve the optimization problem~\\eqref{eq:soft-covitml-breg}.\nThe original Dykstra algorithm~\\cite{Dykstra83}\nwas developed as a computational method that finds the\nEuclidean projection from a point onto the intersection of\nconvex sets. Censor \\& Reich~\\cite{Censor98} extended the algorithm\nto finding the Bregman projection from a point ${\\bm{x}}_{0}$ to a set ${\\mathcal{C}}$,\ndefined by\n\n\n", "index": 19, "text": "\\begin{align*}\n{\\mathop{\\textrm{argmin}}\\limits}_{{\\bm{x}}\\in{\\mathcal{C}}}\\text{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}_{0}). \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathop{\\textrm{argmin}}\\limits}_{{\\bm{x}}\\in{\\mathcal{C}}}\\text%&#10;{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}_{0}).\" display=\"inline\"><mrow><mrow><munder><mtext>argmin</mtext><mrow><mi>\ud835\udc99</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi></mrow></munder><mrow><msub><mtext>BD</mtext><mi>\u03c6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nthen computing the Bregman projection from a point ${\\bm{x}}_{0}$\nto its boundary $\\text{bd}({\\mathcal{C}}_{k})$ is equivalent to\nsolving the following saddle point problem:\n\n\n", "itemtype": "equation", "pos": 19505, "prevtext": "\n\nIn available literature related to stochastic gradient\ndescent methods and the\nvariants~\\cite{Bottou10a-sgd,Johnson13a-svrg,Roux12a-sag,Shalev-Shwartz11a-pegasos}\nthat minimize the regularized loss averaged\nover a set of examples, it is empirically\nshown that, rather than picking an example in a cyclic order, example selection in a stochastic order dramatically speeds up the\nconvergence to the optimal solution. Alternatively, some\nliterature reported that at the beginning of every epoch in\nthe gradient method, random permutation of the order of\nexamples also accelerates the convergence~\\cite{defazio2014finito}.\n\nMotivated by these facts, this study proposes the use of\nstochastic orders for selection of convex set components in\nthe Dykstra algorithm. We term the stochastic version of the\nDykstra algorithm as the \\emph{stochastic Dykstra algorithm}. In\nour case, every convex set component is one of $K$\nhalf-spaces, as will be described in a later discussion. There are, then, three ways to\nselect half-spaces:\n\n\\begin{itemize}\n\\item \\textbf{Cyclic}:\n  Pick a half-space in a cyclic order at each iteration.\n\\item \\textbf{Rand}:\n  Pick a half-space randomly at each iteration.\n\\item \\textbf{Perm}:\n  Permute the order of $K$ half-spaces randomly at the\n  beginning of each epoch.\n\\end{itemize}\n\nHereinafter, we assume to employ the ``Rand'' option,\nalthough replacing this option with one of the remaining two is\nstraightforward.\n\nIf every convex set component is a half-space, and the\n$k$-th convex set component~${\\mathcal{C}}_{k}$ is expressed as\n\n\n", "index": 21, "text": "\\begin{align*}\n  {\\mathcal{C}}_{k} := \\left\\{ {\\bm{x}} \\,|\\, \\left<{\\bm{a}}_{k},{\\bm{x}}\\right> \\le b_{k} \\right\\}, \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{C}}_{k}:=\\left\\{{\\bm{x}}\\,|\\,\\left&lt;{\\bm{a}}_{k},{\\bm{x}%&#10;}\\right&gt;\\leq b_{k}\\right\\},\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>k</mi></msub><mo>:=</mo><mrow><mo>{</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc99</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc82</mi><mi>k</mi></msub><mo>,</mo><mi>\ud835\udc99</mi><mo>\u27e9</mo></mrow><mo>\u2264</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThis fact enables us to rewrite the Dykstra algorithm\nwith Rand option for finding the Bregman projection\nfrom a point ${\\bm{x}}_{0}$ to the intersection of\n${\\mathcal{C}}_{1},\\dots,{\\mathcal{C}}_{K}$, as described in Algorithm~\\ref{algo:dykstra},\nwhere $\\varphi^{*}$ is the convex conjugate of the seed function $\\varphi$. \n\n\\begin{algorithm}[th!]\n\\caption{\nStochastic Dykstra Algorithm. \n\\label{algo:dykstra}}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{begin}\n\\STATE $\\forall k\\in{\\mathbb{N}}_{K}:\\; \\alpha_{k} := 0;$\n\\FOR{$t = 1, 2, \\dots$}\n\\STATE Pick $k$ randomly from $\\{1,\\dots,K\\}$;  \n\\STATE Solve the following saddle point problem\nand let $\\delta_{t-1/2}$ be the solution of $\\delta$: \n\n\n", "itemtype": "equation", "pos": 19811, "prevtext": "\n\nthen computing the Bregman projection from a point ${\\bm{x}}_{0}$\nto its boundary $\\text{bd}({\\mathcal{C}}_{k})$ is equivalent to\nsolving the following saddle point problem:\n\n\n", "index": 23, "text": "\\begin{align*}\n\\max_{\\delta} \\min_{{\\bm{x}}}\n\\text{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}_{0}) + \\delta ( \\left<{\\bm{a}}_{k},{\\bm{x}}\\right> - b_{k} ). \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max_{\\delta}\\min_{{\\bm{x}}}\\text{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}%&#10;_{0})+\\delta(\\left&lt;{\\bm{a}}_{k},{\\bm{x}}\\right&gt;-b_{k}).\" display=\"inline\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>\u03b4</mi></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>\ud835\udc99</mi></munder><mo>\u2061</mo><msub><mtext>BD</mtext><mi>\u03c6</mi></msub></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><msub><mi>\ud835\udc99</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc82</mi><mi>k</mi></msub><mo>,</mo><mi>\ud835\udc99</mi><mo>\u27e9</mo></mrow><mo>-</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\n\\STATE \n$\\delta_{t} := \\max( \\delta_{t-1/2}, -\\alpha_{k} ); $\n$\\alpha_{k} := \\alpha_{k} + \\delta_{t}; $\n\\STATE \n${\\bm{x}}_{t} = \\nabla\\varphi^{*}(\\nabla\\varphi({\\bm{x}}_{t-1}) - \\delta_{t}{\\bm{a}}_{k})$; \n\\ENDFOR\n\\STATE \\textbf{end.}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\section{Efficient Projection Technique}\n\nWe now show that\nsolving the optimization problem~\\eqref{eq:soft-covitml-breg}\nis equivalent to finding a Bregman projection from a point\n$({\\bm{I}},{\\bm{1}})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$\nonto the intersection of multiple half-spaces.\n\nLet ${\\bm{A}}_{k}$ be a positive semidefinite matrix expressed as \n\n\n", "itemtype": "equation", "pos": 20663, "prevtext": "\n\nThis fact enables us to rewrite the Dykstra algorithm\nwith Rand option for finding the Bregman projection\nfrom a point ${\\bm{x}}_{0}$ to the intersection of\n${\\mathcal{C}}_{1},\\dots,{\\mathcal{C}}_{K}$, as described in Algorithm~\\ref{algo:dykstra},\nwhere $\\varphi^{*}$ is the convex conjugate of the seed function $\\varphi$. \n\n\\begin{algorithm}[th!]\n\\caption{\nStochastic Dykstra Algorithm. \n\\label{algo:dykstra}}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{begin}\n\\STATE $\\forall k\\in{\\mathbb{N}}_{K}:\\; \\alpha_{k} := 0;$\n\\FOR{$t = 1, 2, \\dots$}\n\\STATE Pick $k$ randomly from $\\{1,\\dots,K\\}$;  \n\\STATE Solve the following saddle point problem\nand let $\\delta_{t-1/2}$ be the solution of $\\delta$: \n\n\n", "index": 25, "text": "\\begin{align}\\label{eq:saddleprob-general}\n\\max_{\\delta} \\min_{{\\bm{x}}}\n\\text{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}_{t-1}) + \\delta_{t} ( \\left<{\\bm{a}}_{k},{\\bm{x}}\\right> - b_{k} );   \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max_{\\delta}\\min_{{\\bm{x}}}\\text{BD}_{\\varphi}({\\bm{x}},{\\bm{x}}%&#10;_{t-1})+\\delta_{t}(\\left&lt;{\\bm{a}}_{k},{\\bm{x}}\\right&gt;-b_{k});\" display=\"inline\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>\u03b4</mi></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>\ud835\udc99</mi></munder><mo>\u2061</mo><msub><mtext>BD</mtext><mi>\u03c6</mi></msub></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc99</mi><mo>,</mo><msub><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b4</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc82</mi><mi>k</mi></msub><mo>,</mo><mi>\ud835\udc99</mi><mo>\u27e9</mo></mrow><mo>-</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nfor $k\\in{\\mathbb{N}}_{K}$, to define a half-space\n\n\n", "itemtype": "equation", "pos": 21498, "prevtext": "\n\n\\STATE \n$\\delta_{t} := \\max( \\delta_{t-1/2}, -\\alpha_{k} ); $\n$\\alpha_{k} := \\alpha_{k} + \\delta_{t}; $\n\\STATE \n${\\bm{x}}_{t} = \\nabla\\varphi^{*}(\\nabla\\varphi({\\bm{x}}_{t-1}) - \\delta_{t}{\\bm{a}}_{k})$; \n\\ENDFOR\n\\STATE \\textbf{end.}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\section{Efficient Projection Technique}\n\nWe now show that\nsolving the optimization problem~\\eqref{eq:soft-covitml-breg}\nis equivalent to finding a Bregman projection from a point\n$({\\bm{I}},{\\bm{1}})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$\nonto the intersection of multiple half-spaces.\n\nLet ${\\bm{A}}_{k}$ be a positive semidefinite matrix expressed as \n\n\n", "index": 27, "text": "\\begin{align*}\n{\\bm{A}}_{k} := \n\\left({\\bm{\\Phi}}({\\bm{X}}_{i_{k}})-{\\bm{\\Phi}}({\\bm{X}}_{j_{k}})\\right)\n\\left({\\bm{\\Phi}}({\\bm{X}}_{i_{k}})-{\\bm{\\Phi}}({\\bm{X}}_{j_{k}})\\right)^{\\top}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{A}}_{k}:=\\left({\\bm{\\Phi}}({\\bm{X}}_{i_{k}})-{\\bm{\\Phi}}({%&#10;\\bm{X}}_{j_{k}})\\right)\\left({\\bm{\\Phi}}({\\bm{X}}_{i_{k}})-{\\bm{\\Phi}}({\\bm{X}%&#10;}_{j_{k}})\\right)^{\\top}\" display=\"inline\"><mrow><msub><mi>\ud835\udc68</mi><mi>k</mi></msub><mo>:=</mo><mrow><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>j</mi><mi>k</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7f</mi><msub><mi>j</mi><mi>k</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u22a4</mo></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThen, it can be seen that the intersection of $K$ half-spaces\n\n\n", "itemtype": "equation", "pos": 21749, "prevtext": "\n\nfor $k\\in{\\mathbb{N}}_{K}$, to define a half-space\n\n\n", "index": 29, "text": "\\begin{align*}\n{\\mathcal{C}}_{k} :=\n\\left\\{\n({\\bm{W}},{\\bm{\\xi}})\\in{\\mathbb{S}}_{++}^{n}\\times {\\mathbb{R}}_{++}^{K}\\,|\\,\ny_{k}\\left<{\\bm{A}}_{k},{\\bm{W}}\\right> - y_{k}b_{k}\\xi_{k} \\le 0\n\\right\\}. \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{C}}_{k}:=\\left\\{({\\bm{W}},{\\bm{\\xi}})\\in{\\mathbb{S}}_{+%&#10;+}^{n}\\times{\\mathbb{R}}_{++}^{K}\\,|\\,y_{k}\\left&lt;{\\bm{A}}_{k},{\\bm{W}}\\right&gt;-%&#10;y_{k}b_{k}\\xi_{k}\\leq 0\\right\\}.\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>k</mi></msub><mo>:=</mo><mrow><mo>{</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo>,</mo><mi>\ud835\udf43</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><msubsup><mi>\ud835\udd4a</mi><mrow><mi/><mo>+</mo><mo>+</mo></mrow><mi>n</mi></msubsup><mo>\u00d7</mo><mpadded width=\"+1.7pt\"><msubsup><mi>\u211d</mi><mrow><mi/><mo>+</mo><mo>+</mo></mrow><mi>K</mi></msubsup></mpadded></mrow></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mrow><mrow><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub><mo>,</mo><mi>\ud835\udc7e</mi><mo>\u27e9</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>b</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mi>k</mi></msub></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nis the feasible region of \nthe optimization problem~\\eqref{eq:soft-covitml-breg}.\nThis implies that the Dykstra algorithm can be applied to\nsolve problem~\\eqref{eq:soft-covitml-breg}. \n\nNext we present an efficient technique that projects\n$({\\bm{W}}_{t-1},{\\bm{\\xi}}_{t-1})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$\nonto the $k$-th half-space ${\\mathcal{C}}_{k}$, \nwhere $({\\bm{W}}_{t-1},{\\bm{\\xi}}_{t-1})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$ is \nthe model parameter after the $(t-1)$-th iteration.  \nLet $\\xi_{k,t-1}$ be the $k$-th entry in the vector~${\\bm{\\xi}}_{t-1}$.\nThe value of the function $J_{t}:{\\mathbb{R}}\\to{\\mathbb{R}}$ defined by\n\n\n", "itemtype": "equation", "pos": 22026, "prevtext": "\n\nThen, it can be seen that the intersection of $K$ half-spaces\n\n\n", "index": 31, "text": "\\begin{align*}\n  \\bigcap_{k=1}^{K}{\\mathcal{C}}_{k}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\bigcap_{k=1}^{K}{\\mathcal{C}}_{k}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c2</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>k</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nis zero at the solution $\\delta_{t-1/2}$\nof the saddle point problem~\\eqref{eq:saddleprob-general}.\nThe solution $\\delta$ must satisfy\nthe strictly positive definiteness:\n\n", "itemtype": "equation", "pos": 22764, "prevtext": "\n\nis the feasible region of \nthe optimization problem~\\eqref{eq:soft-covitml-breg}.\nThis implies that the Dykstra algorithm can be applied to\nsolve problem~\\eqref{eq:soft-covitml-breg}. \n\nNext we present an efficient technique that projects\n$({\\bm{W}}_{t-1},{\\bm{\\xi}}_{t-1})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$\nonto the $k$-th half-space ${\\mathcal{C}}_{k}$, \nwhere $({\\bm{W}}_{t-1},{\\bm{\\xi}}_{t-1})\\in{\\mathbb{S}}_{++}^{n}\\times{\\mathbb{R}}_{++}^{K}$ is \nthe model parameter after the $(t-1)$-th iteration.  \nLet $\\xi_{k,t-1}$ be the $k$-th entry in the vector~${\\bm{\\xi}}_{t-1}$.\nThe value of the function $J_{t}:{\\mathbb{R}}\\to{\\mathbb{R}}$ defined by\n\n\n", "index": 33, "text": "\\begin{align*}\n  J_{t}(\\delta) :=\n  \\left< {\\bm{A}}_{k}, ({\\bm{W}}_{t-1}^{-1}+\\delta y_{k}{\\bm{A}}_{k})^{-1}\\right>\n  - b_{k}\n  \\nabla\\varphi^{*}_{\\text{l}}\n  (\\nabla\\varphi_{\\text{l}}(\\xi_{t-1}) + \\delta y_{k}b_{k}/c_{k}), \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle J_{t}(\\delta):=\\left&lt;{\\bm{A}}_{k},({\\bm{W}}_{t-1}^{-1}+\\delta y_%&#10;{k}{\\bm{A}}_{k})^{-1}\\right&gt;-b_{k}\\nabla\\varphi^{*}_{\\text{l}}(\\nabla\\varphi_{%&#10;\\text{l}}(\\xi_{t-1})+\\delta y_{k}b_{k}/c_{k}),\" display=\"inline\"><mrow><mrow><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub><mo>,</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u27e9</mo></mrow><mo>-</mo><mrow><msub><mi>b</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><msubsup><mi>\u03c6</mi><mtext>l</mtext><mo>*</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo>/</mo><msub><mi>c</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nand the feasibility of the slack variables: \n\n\n", "itemtype": "equation", "pos": 23174, "prevtext": "\n\nis zero at the solution $\\delta_{t-1/2}$\nof the saddle point problem~\\eqref{eq:saddleprob-general}.\nThe solution $\\delta$ must satisfy\nthe strictly positive definiteness:\n\n", "index": 35, "text": "\\begin{align}\\label{eq:W-gt-zero}\n  ({\\bm{Y}}(\\delta))^{-1} := {\\bm{W}}_{t-1}^{-1}+\\delta y_{k}{\\bm{A}}_{k} \\succ {\\bm{O}},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\bm{Y}}(\\delta))^{-1}:={\\bm{W}}_{t-1}^{-1}+\\delta y_{k}{\\bm{A}}%&#10;_{k}\\succ{\\bm{O}},\" display=\"inline\"><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>:=</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub></mrow></mrow><mo>\u227b</mo><mi>\ud835\udc76</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThere is no closed-form solution found for this projection\nproblem.  Hence, some numerical method such as the Newton-Raphson\nmethod is necessary for solving the nonlinear system $J_{t}(\\delta)=0$.\nIf one tries to compute the value of $J_{t}(\\delta)$ na\\\"{i}vely,\nit will require an $O(n^{3})$ computational cost because \n$J_{t}(\\cdot)$ involves computation of the inverse of\nan $n\\times n$ matrix. \nIf we suppose the numerical method assesses the value of\nthe scalar-valued function $J_{t}(\\cdot)$ $L$ times, the\nna\\\"{i}ve approach will take $O(Ln^{3})$ computational time\nto find the solution of the nonlinear system $J_{t}(\\delta)=0$.\nFurthermore, the positive definiteness condition in~\\eqref{eq:W-gt-zero}\nand the feasibility condition in~\\eqref{eq:xi-in-ridom} must be checked.\n\nWe will show the following two claims:\n\n\\begin{itemize}\n\\item The solution of the\nsystem $J_{t}(\\delta)=0$ satisfying \\eqref{eq:W-gt-zero}\nand \\eqref{eq:xi-in-ridom} can be\ncomputed within $O(n^{3}+Ln)$ time, where $L$ is\nthe number of times a numerical method assesses the value of\n$J_{t}(\\cdot)$.\n\\\\\n\\item The solution exists and it is unique.\n\\end{itemize}\n\nHereinafter, we assume ${\\bm{A}}_{k}$ is strictly positive definite.\nBy setting\n${\\bm{A}}_{k} \\leftarrow {\\bm{A}}_{k} + \\epsilon {\\bm{I}}$, with $\\epsilon$ as a small positive\nconstant, it is easy to satisfy this assumption.\nSince $L \\in O(n^{2})$ in a typical setting, we can say that\neach update can be done in $O(n^{3})$ computation. \n\nWe define ${\\bm{A}}^{1/2}_{k}$, ${\\bm{U}}$, ${\\bm{D}}$, and ${\\bm{d}}$ as follows. \nLet ${\\bm{A}}_{k}^{1/2}\\in{\\mathbb{S}}_{++}^{n}$ \nsuch that ${\\bm{A}}_{k}^{1/2}{\\bm{A}}_{k}^{1/2}={\\bm{A}}_{k}$, and \ndenote by ${\\bm{A}}_{k}^{-1/2}\\in{\\mathbb{S}}_{++}^{n}$ the inverse of ${\\bm{A}}_{k}^{1/2}$.\nIntroduce an orthonormal matrix ${\\bm{U}}\\in{\\mathbb{O}}_{n}$ and \na diagonal matrix ${\\bm{D}}={\\operatorname{diag}}(\\{d_{1},\\dots,d_{n}\\})$ that \nrepresent a spectral decomposition \n${\\bm{U}}{\\bm{D}}{\\bm{U}}^{\\top} = {\\bm{A}}^{-1/2}{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}$,\nwith $d_{1}\\ge\\dots\\ge d_{n}$. \nThen, we have\n\n\n", "itemtype": "equation", "pos": 23356, "prevtext": "\nand the feasibility of the slack variables: \n\n\n", "index": 37, "text": "\\begin{align}\\label{eq:xi-in-ridom}\n  \\exists \\, \\xi_{k,t-1/2} \\quad\\text{s.t.}\\quad\n  \\nabla\\varphi_{\\text{l}}(\\xi_{k,t-1/2}) =\n  \\nabla\\varphi_{\\text{l}}(\\xi_{k,t-1}) - \\delta y_{k}b_{k}/c_{k} . \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\exists\\,\\xi_{k,t-1/2}\\quad\\text{s.t.}\\quad\\nabla\\varphi_{\\text{l%&#10;}}(\\xi_{k,t-1/2})=\\nabla\\varphi_{\\text{l}}(\\xi_{k,t-1})-\\delta y_{k}b_{k}/c_{k}.\" display=\"inline\"><mrow><mrow><mrow><mrow><mo rspace=\"4.2pt\">\u2203</mo><msub><mi>\u03be</mi><mrow><mi>k</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></mrow></msub></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mrow><mi>k</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mrow><mi>k</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo>/</mo><msub><mi>c</mi><mi>k</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhich allows us to rewrite the first term of $J_{t}(\\delta)$ as \n\n\n", "itemtype": "equation", "pos": 25663, "prevtext": "\n\nThere is no closed-form solution found for this projection\nproblem.  Hence, some numerical method such as the Newton-Raphson\nmethod is necessary for solving the nonlinear system $J_{t}(\\delta)=0$.\nIf one tries to compute the value of $J_{t}(\\delta)$ na\\\"{i}vely,\nit will require an $O(n^{3})$ computational cost because \n$J_{t}(\\cdot)$ involves computation of the inverse of\nan $n\\times n$ matrix. \nIf we suppose the numerical method assesses the value of\nthe scalar-valued function $J_{t}(\\cdot)$ $L$ times, the\nna\\\"{i}ve approach will take $O(Ln^{3})$ computational time\nto find the solution of the nonlinear system $J_{t}(\\delta)=0$.\nFurthermore, the positive definiteness condition in~\\eqref{eq:W-gt-zero}\nand the feasibility condition in~\\eqref{eq:xi-in-ridom} must be checked.\n\nWe will show the following two claims:\n\n\\begin{itemize}\n\\item The solution of the\nsystem $J_{t}(\\delta)=0$ satisfying \\eqref{eq:W-gt-zero}\nand \\eqref{eq:xi-in-ridom} can be\ncomputed within $O(n^{3}+Ln)$ time, where $L$ is\nthe number of times a numerical method assesses the value of\n$J_{t}(\\cdot)$.\n\\\\\n\\item The solution exists and it is unique.\n\\end{itemize}\n\nHereinafter, we assume ${\\bm{A}}_{k}$ is strictly positive definite.\nBy setting\n${\\bm{A}}_{k} \\leftarrow {\\bm{A}}_{k} + \\epsilon {\\bm{I}}$, with $\\epsilon$ as a small positive\nconstant, it is easy to satisfy this assumption.\nSince $L \\in O(n^{2})$ in a typical setting, we can say that\neach update can be done in $O(n^{3})$ computation. \n\nWe define ${\\bm{A}}^{1/2}_{k}$, ${\\bm{U}}$, ${\\bm{D}}$, and ${\\bm{d}}$ as follows. \nLet ${\\bm{A}}_{k}^{1/2}\\in{\\mathbb{S}}_{++}^{n}$ \nsuch that ${\\bm{A}}_{k}^{1/2}{\\bm{A}}_{k}^{1/2}={\\bm{A}}_{k}$, and \ndenote by ${\\bm{A}}_{k}^{-1/2}\\in{\\mathbb{S}}_{++}^{n}$ the inverse of ${\\bm{A}}_{k}^{1/2}$.\nIntroduce an orthonormal matrix ${\\bm{U}}\\in{\\mathbb{O}}_{n}$ and \na diagonal matrix ${\\bm{D}}={\\operatorname{diag}}(\\{d_{1},\\dots,d_{n}\\})$ that \nrepresent a spectral decomposition \n${\\bm{U}}{\\bm{D}}{\\bm{U}}^{\\top} = {\\bm{A}}^{-1/2}{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}$,\nwith $d_{1}\\ge\\dots\\ge d_{n}$. \nThen, we have\n\n\n", "index": 39, "text": "\\begin{align}\\label{eq:Y-diagnolized}\n{\\bm{Y}}(\\delta) = \n{\\bm{A}}_{k}^{-1/2} {\\bm{U}}\n({\\bm{D}} + y_{k} \\delta {\\bm{I}})^{-1} {\\bm{U}}^{\\top} {\\bm{A}}_{k}^{-1/2},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{Y}}(\\delta)={\\bm{A}}_{k}^{-1/2}{\\bm{U}}({\\bm{D}}+y_{k}\\delta%&#10;{\\bm{I}})^{-1}{\\bm{U}}^{\\top}{\\bm{A}}_{k}^{-1/2},\" display=\"inline\"><mrow><mrow><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc68</mi><mi>k</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><mi>\ud835\udc7c</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc6b</mi><mo>+</mo><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mi>\ud835\udc70</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc7c</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc68</mi><mi>k</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nAssessment of $J_{t}(\\delta)$ can be\ndone within $O(n)$ computational cost after $d_{1},\\dots,d_{n}$ are obtained.\nTo get the $n$ scalars $d_{1},\\dots,d_{n}$,\nwe need to find ${\\bm{A}}_k^{-1/2}$ and the\nspectral decomposition of ${\\bm{A}}^{-1/2}_k{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}_k$, \neach of which requires $O(n^{3})$ computation. \nThe $n\\times n$ matrix ${\\bm{A}}^{-1/2}_{k}$ can be computed\nin the pre-process of the Dykstra algorithm, while \nthe spectral decomposition of ${\\bm{A}}^{-1/2}_k{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}_k$\nis done once before invoking some numerical method to\nsolve the nonlinear system $J_{t}(\\delta)=0$. These support the first claim.\n\nEquation~\\eqref{eq:Y-diagnolized} suggests that \nthe set of $\\delta$ satisfying \\eqref{eq:W-gt-zero}\nis given by\n\n\n", "itemtype": "equation", "pos": 25906, "prevtext": "\n\nwhich allows us to rewrite the first term of $J_{t}(\\delta)$ as \n\n\n", "index": 41, "text": "\\begin{align}\\label{eq:dotprod-linear-comp}\n\\left<{\\bm{A}}_k,\n({\\bm{W}}_{t-1}^{-1} + \\delta y_{k} {\\bm{A}}_k)^{-1}\n\\right> \n=\n\\sum_{i=1}^{n}\\frac{1}{d_{i}+y_{k}\\delta}. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left&lt;{\\bm{A}}_{k},({\\bm{W}}_{t-1}^{-1}+\\delta y_{k}{\\bm{A}}_{k})%&#10;^{-1}\\right&gt;=\\sum_{i=1}^{n}\\frac{1}{d_{i}+y_{k}\\delta}.\" display=\"inline\"><mrow><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub><mo>,</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc68</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u27e9</mo></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\u03b4</mi></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThe set of $\\delta$ satisfying \\eqref{eq:xi-in-ridom} is given\nas follows.  In the case of using ISBD,\n$\\delta$ ensuring \\eqref{eq:xi-in-ridom} is in\nthe interval\n\n\n", "itemtype": "equation", "pos": 26869, "prevtext": "\n\nAssessment of $J_{t}(\\delta)$ can be\ndone within $O(n)$ computational cost after $d_{1},\\dots,d_{n}$ are obtained.\nTo get the $n$ scalars $d_{1},\\dots,d_{n}$,\nwe need to find ${\\bm{A}}_k^{-1/2}$ and the\nspectral decomposition of ${\\bm{A}}^{-1/2}_k{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}_k$, \neach of which requires $O(n^{3})$ computation. \nThe $n\\times n$ matrix ${\\bm{A}}^{-1/2}_{k}$ can be computed\nin the pre-process of the Dykstra algorithm, while \nthe spectral decomposition of ${\\bm{A}}^{-1/2}_k{\\bm{W}}_{t-1}^{-1}{\\bm{A}}^{-1/2}_k$\nis done once before invoking some numerical method to\nsolve the nonlinear system $J_{t}(\\delta)=0$. These support the first claim.\n\nEquation~\\eqref{eq:Y-diagnolized} suggests that \nthe set of $\\delta$ satisfying \\eqref{eq:W-gt-zero}\nis given by\n\n\n", "index": 43, "text": "\\begin{align}\\label{eq:int-nlrsys-for-upd-alph}\nI_{\\text{r},t} := \n\\begin{cases}\n( -d_{n},+\\infty ), &\\qquad \\text{for $y_{k}=+1$}, \n\\\\\n( -\\infty, d_{n} ), &\\qquad \\text{for $y_{k}=-1$}. \n\\end{cases}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I_{\\text{r},t}:=\\begin{cases}(-d_{n},+\\infty),&amp;\\qquad\\text{for $%&#10;y_{k}=+1$},\\\\&#10;(-\\infty,d_{n}),&amp;\\qquad\\text{for $y_{k}=-1$}.\\end{cases}\" display=\"inline\"><mrow><msub><mi>I</mi><mrow><mtext>r</mtext><mo>,</mo><mi>t</mi></mrow></msub><mo>:=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><mo>,</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nwhere \n\n", "itemtype": "equation", "pos": 27246, "prevtext": "\n\nThe set of $\\delta$ satisfying \\eqref{eq:xi-in-ridom} is given\nas follows.  In the case of using ISBD,\n$\\delta$ ensuring \\eqref{eq:xi-in-ridom} is in\nthe interval\n\n\n", "index": 45, "text": "\\begin{align*}\nI_{\\text{is},t} := \n\\begin{cases}\n( -\\infty, \\delta_{\\text{b}} ), &\\qquad \\text{for $y_{k}=+1$}, \n\\\\\n( -\\delta_{\\text{b}}, +\\infty ), &\\qquad \\text{for $y_{k}=-1$}, \n\\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I_{\\text{is},t}:=\\begin{cases}(-\\infty,\\delta_{\\text{b}}),&amp;%&#10;\\qquad\\text{for $y_{k}=+1$},\\\\&#10;(-\\delta_{\\text{b}},+\\infty),&amp;\\qquad\\text{for $y_{k}=-1$},\\end{cases}\" display=\"inline\"><mrow><msub><mi>I</mi><mrow><mtext>is</mtext><mo>,</mo><mi>t</mi></mrow></msub><mo>:=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo>,</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub></mrow><mo>,</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nIn the case of using L2BD and REBD, there exists $\\xi_{t-1/2}$ even if\n$\\nabla\\varphi_{\\text{l}}(\\xi_{t-1}) - \\delta y_{k}b_{k}/c_{k}$ takes any value.\n\nHence, if ISBD is employed, the solution $\\delta_{t-1/2}$ can be searched\nfrom the interval\n\n", "itemtype": "equation", "pos": 27459, "prevtext": "\nwhere \n\n", "index": 47, "text": "\\begin{align*}\n  \\delta_{\\text{b}} := \\frac{c_{k}}{b_{k}\\xi_{k,t-1}}. \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\delta_{\\text{b}}:=\\frac{c_{k}}{b_{k}\\xi_{k,t-1}}.\" display=\"inline\"><mrow><mrow><msub><mi>\u03b4</mi><mtext>b</mtext></msub><mo>:=</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>c</mi><mi>k</mi></msub><mrow><msub><mi>b</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mrow><mi>k</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mfrac></mstyle></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nIf L2BD or REBD is employed, \nthe solution $\\delta_{t-1/2}$ can be searched from $I_{\\text{r},t}$.\nIn the reminder of this section, we shall use the notation $I_{t}$ to\ndenote the interval for $\\delta$ satisfying \\eqref{eq:W-gt-zero}\nand \\eqref{eq:xi-in-ridom} simultaneously.\n\nWe now show the uniqueness of the solution. The gradient of $J_{t}:{\\mathbb{R}}\\to{\\mathbb{R}}$\nis expressed as\n\n\n", "itemtype": "equation", "pos": 27788, "prevtext": "\nIn the case of using L2BD and REBD, there exists $\\xi_{t-1/2}$ even if\n$\\nabla\\varphi_{\\text{l}}(\\xi_{t-1}) - \\delta y_{k}b_{k}/c_{k}$ takes any value.\n\nHence, if ISBD is employed, the solution $\\delta_{t-1/2}$ can be searched\nfrom the interval\n\n", "index": 49, "text": "\\begin{align*}\n  I_{\\text{r},t}\\cap I_{\\text{is},t} =\n\\begin{cases}\n( -d_{n}, \\delta_{\\text{b}} ), &\\qquad \\text{for $y_{k}=+1$}, \n\\\\\n( -\\delta_{\\text{b}}, +d_{n} ), &\\qquad \\text{for $y_{k}=-1$}. \n\\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I_{\\text{r},t}\\cap I_{\\text{is},t}=\\begin{cases}(-d_{n},\\delta_{%&#10;\\text{b}}),&amp;\\qquad\\text{for $y_{k}=+1$},\\\\&#10;(-\\delta_{\\text{b}},+d_{n}),&amp;\\qquad\\text{for $y_{k}=-1$}.\\end{cases}\" display=\"inline\"><mrow><mrow><msub><mi>I</mi><mrow><mtext>r</mtext><mo>,</mo><mi>t</mi></mrow></msub><mo>\u2229</mo><msub><mi>I</mi><mrow><mtext>is</mtext><mo>,</mo><mi>t</mi></mrow></msub></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><mo>,</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub></mrow><mo>,</mo><mrow><mo>+</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mpadded lspace=\"20pt\" width=\"+20pt\"><mrow><mtext>for\u00a0</mtext><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></mrow></mpadded><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nfor $\\delta\\in I_{t}$.  We first consider the case that \n$y_{k}=+1$. Clearly, the first term is negative. \nThe second term is non-positive because any convex conjugate function\nis convex. Therefore, we have $\\nabla J_{t}(\\delta) < 0$.\nIn the case of $y_{k}=-1$, \nwe get $\\nabla J_{t}(\\delta) > 0$ from a similar derivation. \nThese observations imply that the solution is unique\nif a solution exists.  The existence of the solution can be\nestablished by showing that the curve $J_{t}(\\delta)$ crosses\nthe horizontal axis.\n\nWe consider the cases of using ISBD and using either \nL2BD or REBD separately.\n\nFor the ISBD case, we have\n\n", "itemtype": "equation", "pos": 28402, "prevtext": "\nIf L2BD or REBD is employed, \nthe solution $\\delta_{t-1/2}$ can be searched from $I_{\\text{r},t}$.\nIn the reminder of this section, we shall use the notation $I_{t}$ to\ndenote the interval for $\\delta$ satisfying \\eqref{eq:W-gt-zero}\nand \\eqref{eq:xi-in-ridom} simultaneously.\n\nWe now show the uniqueness of the solution. The gradient of $J_{t}:{\\mathbb{R}}\\to{\\mathbb{R}}$\nis expressed as\n\n\n", "index": 51, "text": "\\begin{align*}\n  \\nabla J_{t}(\\delta)\n  =\n  -\\sum_{i=1}^{n}\\frac{y_{k}}{(d_{i}+y_{k}\\delta)^{2}}\n  -\\frac{y_{k}b_{k}^{2}}{c_{k}}\n  \\nabla^{2}\\varphi^{*}_{\\text{l}}\n  (\\nabla\\varphi_{\\text{l}}(\\xi_{t-1}) - \\delta y_{k}b_{k}/c_{k}), \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\nabla J_{t}(\\delta)=-\\sum_{i=1}^{n}\\frac{y_{k}}{(d_{i}+y_{k}%&#10;\\delta)^{2}}-\\frac{y_{k}b_{k}^{2}}{c_{k}}\\nabla^{2}\\varphi^{*}_{\\text{l}}(%&#10;\\nabla\\varphi_{\\text{l}}(\\xi_{t-1})-\\delta y_{k}b_{k}/c_{k}),\" display=\"inline\"><mrow><mrow><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>J</mi><mi>t</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><msub><mi>y</mi><mi>k</mi></msub><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\u03b4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>b</mi><mi>k</mi><mn>2</mn></msubsup></mrow><msub><mi>c</mi><mi>k</mi></msub></mfrac></mstyle><mo>\u2062</mo><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><msubsup><mi>\u03c6</mi><mtext>l</mtext><mo>*</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo>/</mo><msub><mi>c</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nif $y_{k}=+1$, and\n\n", "itemtype": "equation", "pos": 29277, "prevtext": "\n\nfor $\\delta\\in I_{t}$.  We first consider the case that \n$y_{k}=+1$. Clearly, the first term is negative. \nThe second term is non-positive because any convex conjugate function\nis convex. Therefore, we have $\\nabla J_{t}(\\delta) < 0$.\nIn the case of $y_{k}=-1$, \nwe get $\\nabla J_{t}(\\delta) > 0$ from a similar derivation. \nThese observations imply that the solution is unique\nif a solution exists.  The existence of the solution can be\nestablished by showing that the curve $J_{t}(\\delta)$ crosses\nthe horizontal axis.\n\nWe consider the cases of using ISBD and using either \nL2BD or REBD separately.\n\nFor the ISBD case, we have\n\n", "index": 53, "text": "\\begin{align*}\n \\lim_{\\delta\\searrow -d_{n}}J_{t}(\\delta) &= +\\infty,\n &\n \\lim_{\\delta\\nearrow \\delta_{\\text{b}}}J_{t}(\\delta) &= -\\infty,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\searrow-d_{n}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2198</mo><mrow><mo>-</mo><msub><mi>d</mi><mi>n</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=+\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\nearrow\\delta_{\\text{b}}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2197</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nif $y_{k}=-1$.\n\nOn the other hand, when using either L2BD or REBD with $y_{k}=+1$ we get\n\n", "itemtype": "equation", "pos": 29448, "prevtext": "\nif $y_{k}=+1$, and\n\n", "index": 55, "text": "\\begin{align*}\n \\lim_{\\delta\\searrow -\\delta_{\\text{b}}}J_{t}(\\delta) &= -\\infty,\n &\n \\lim_{\\delta\\nearrow d_{n}}J_{t}(\\delta) &= +\\infty, \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\searrow-\\delta_{\\text{b}}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2198</mo><mrow><mo>-</mo><msub><mi>\u03b4</mi><mtext>b</mtext></msub></mrow></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\nearrow d_{n}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2197</mo><msub><mi>d</mi><mi>n</mi></msub></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle=+\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\nwhile we obtain \n\n", "itemtype": "equation", "pos": 29690, "prevtext": "\nif $y_{k}=-1$.\n\nOn the other hand, when using either L2BD or REBD with $y_{k}=+1$ we get\n\n", "index": 57, "text": "\\begin{align*}\n \\lim_{\\delta\\searrow -d_{n}}J_{t}(\\delta) &= +\\infty,\n &\n \\lim_{\\delta\\to +\\infty}J_{t}(\\delta) &= -\\infty,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\searrow-d_{n}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2198</mo><mrow><mo>-</mo><msub><mi>d</mi><mi>n</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=+\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\to+\\infty}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2192</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhen $y_{k}=-1$. Hence, we conclude that\n\n\n", "itemtype": "equation", "pos": 29844, "prevtext": "\nwhile we obtain \n\n", "index": 59, "text": "\\begin{align*}\n \\lim_{\\delta\\to -\\infty}J_{t}(\\delta) &= -\\infty,\n &\n \\lim_{\\delta\\nearrow d_{n}}J_{t}(\\delta) &= +\\infty, \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\to-\\infty}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2192</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{\\delta\\nearrow d_{n}}J_{t}(\\delta)\" display=\"inline\"><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b4</mi><mo>\u2197</mo><msub><mi>d</mi><mi>n</mi></msub></mrow></munder><mo>\u2061</mo><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle=+\\infty,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\n\n\\subsection{Stopping Criterion}\n\n\n\nHere we discuss how to determine if the solution is already optimal and when to terminate the algorithm. \nWhile running the algorithm,\n$({\\bm{W}}_{t},{\\bm{\\xi}}_{t})$ may be infeasible to the primal\nproblem. Denote the index set of the violated constraints by\n\n${\\mathcal{I}}_{\\text{vio}} := \\{ k\\in{\\mathbb{N}}_{K}\\,|\\, ({\\bm{W}}_{t},{\\bm{\\xi}}_{t})\\not\\in{\\mathcal{C}}_{k} \\}$ \n\nand let us define $\\bar{{\\bm{\\xi}}}_{t}\\in{\\mathbb{R}}_{++}^{K}$ so that\nthe $k$-th entry is given by\n$\\bar{\\xi}_{h,t} := \\frac{1}{b_{h}}\\left<{\\bm{W}}_{t},{\\bm{A}}_{h}\\right>$\nfor $h\\in{\\mathcal{I}}_{\\text{vio}}$ and\n$\\bar{\\xi}_{h,t} := \\xi_{h,t}$ for $h\\not\\in{\\mathcal{I}}_{\\text{vio}}$.\nNote that $({\\bm{W}}_{t},\\bar{{\\bm{\\xi}}}_{t})$ is a feasible solution,\nand $\\bar{{\\bm{\\xi}}}_{t}={\\bm{\\xi}}_{t}$ when $({\\bm{W}}_{t},{\\bm{\\xi}}_{t})$ is feasible. \nThe objective gap after iteration $t$\nis bounded as follows:\n\n\n", "itemtype": "equation", "pos": 30024, "prevtext": "\n\nwhen $y_{k}=-1$. Hence, we conclude that\n\n\n", "index": 61, "text": "\\begin{align*}\n \\exists ! \\,\\delta\\in I_{t} \\qquad \\text{s.t.} \\quad\n J_{t}(\\delta) = 0. \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\exists!\\,\\delta\\in I_{t}\\qquad\\text{s.t.}\\quad J_{t}(\\delta)=0.\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mo>\u2203</mo><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow><mo>\u2061</mo><mi>\u03b4</mi></mrow><mo>\u2208</mo><mrow><msub><mi>I</mi><mi>t</mi></msub><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>s.t.</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><msub><mi>J</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nwhere we have defined \n\n\n", "itemtype": "equation", "pos": 31063, "prevtext": "\n\n\n\\subsection{Stopping Criterion}\n\n\n\nHere we discuss how to determine if the solution is already optimal and when to terminate the algorithm. \nWhile running the algorithm,\n$({\\bm{W}}_{t},{\\bm{\\xi}}_{t})$ may be infeasible to the primal\nproblem. Denote the index set of the violated constraints by\n\n${\\mathcal{I}}_{\\text{vio}} := \\{ k\\in{\\mathbb{N}}_{K}\\,|\\, ({\\bm{W}}_{t},{\\bm{\\xi}}_{t})\\not\\in{\\mathcal{C}}_{k} \\}$ \n\nand let us define $\\bar{{\\bm{\\xi}}}_{t}\\in{\\mathbb{R}}_{++}^{K}$ so that\nthe $k$-th entry is given by\n$\\bar{\\xi}_{h,t} := \\frac{1}{b_{h}}\\left<{\\bm{W}}_{t},{\\bm{A}}_{h}\\right>$\nfor $h\\in{\\mathcal{I}}_{\\text{vio}}$ and\n$\\bar{\\xi}_{h,t} := \\xi_{h,t}$ for $h\\not\\in{\\mathcal{I}}_{\\text{vio}}$.\nNote that $({\\bm{W}}_{t},\\bar{{\\bm{\\xi}}}_{t})$ is a feasible solution,\nand $\\bar{{\\bm{\\xi}}}_{t}={\\bm{\\xi}}_{t}$ when $({\\bm{W}}_{t},{\\bm{\\xi}}_{t})$ is feasible. \nThe objective gap after iteration $t$\nis bounded as follows:\n\n\n", "index": 63, "text": "\\begin{align*}\n  \\text{BD}_{\\varphi}(({\\bm{W}}_{t},\\bar{{\\bm{\\xi}}}_{t}),({\\bm{I}},{\\bm{1}})) - \\text{BD}_{\\star}\n  \\le\n  \\sum_{h\\in{\\mathcal{I}}_{\\text{vio}}}\n  c_{h}\n  \\left(\n  \\varphi_{\\text{l}}(\\bar{\\xi}_{h,t})-\\varphi_{\\text{l}}({\\xi}_{h,t})\n  -\\nabla\\varphi_{\\text{l}}(1)(\\bar{\\xi}_{h,t}-{\\xi}_{h,t})\n  \\right)\n  -\n  \\sum_{h=1}^{K}\\alpha_{h}y_{h}\n  \\left(\\left<{\\bm{A}}_{h},{\\bm{W}}_{t}\\right> - b_{h}\\xi_{h,t}\\right), \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{BD}_{\\varphi}(({\\bm{W}}_{t},\\bar{{\\bm{\\xi}}}_{t}),({\\bm{I}}%&#10;,{\\bm{1}}))-\\text{BD}_{\\star}\\leq\\sum_{h\\in{\\mathcal{I}}_{\\text{vio}}}c_{h}%&#10;\\left(\\varphi_{\\text{l}}(\\bar{\\xi}_{h,t})-\\varphi_{\\text{l}}({\\xi}_{h,t})-%&#10;\\nabla\\varphi_{\\text{l}}(1)(\\bar{\\xi}_{h,t}-{\\xi}_{h,t})\\right)-\\sum_{h=1}^{K}%&#10;\\alpha_{h}y_{h}\\left(\\left&lt;{\\bm{A}}_{h},{\\bm{W}}_{t}\\right&gt;-b_{h}\\xi_{h,t}%&#10;\\right),\" display=\"inline\"><mrow><mrow><mrow><mrow><msub><mtext>BD</mtext><mi>\u03c6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc7e</mi><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\ud835\udf43</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc70</mi><mo>,</mo><mn>\ud835\udfcf</mn><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mtext>BD</mtext><mo>\u22c6</mo></msub></mrow><mo>\u2264</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>h</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi><mtext>vio</mtext></msub></mrow></munder></mstyle><mrow><msub><mi>c</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>\u03c6</mi><mtext>l</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03be</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c6</mi><mtext>l</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi>\u03c6</mi><mtext>l</mtext></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03be</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>-</mo><msub><mi>\u03be</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>\u03b1</mi><mi>h</mi></msub><mo>\u2062</mo><msub><mi>y</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>\u27e8</mo><msub><mi>\ud835\udc68</mi><mi>h</mi></msub><mo>,</mo><msub><mi>\ud835\udc7e</mi><mi>t</mi></msub><mo>\u27e9</mo></mrow><mo>-</mo><mrow><msub><mi>b</mi><mi>h</mi></msub><mo>\u2062</mo><msub><mi>\u03be</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01422.tex", "nexttext": "\n\nThen this upper-bound of the objective gap can be used for the stopping\ncriterion of the Dykstra algorithm.   \n\\section{Numerical Experiments}\n\nWe conducted experiments to assess the convergence speed of\nour optimization algorithms and the generalization\nperformance for pattern recognition.\n\n\\begin{figure*}[t!]\n\\begin{center}\n\\begin{tabular}{l}\n\\includegraphics[width=1.0\\textwidth]{001-logdet.primal2.eps}\n\\end{tabular}\n\\end{center}\n\\caption{Convergence behavior of the algorithms using different settings. \\label{fig:logdet.primal}}\n\\end{figure*}\n\n\\subsection{Convergence behavior of optimization algorithms}\n\nWe examined our algorithms for assessment of convergence\nspeed. We generated datasets artificially as follows. $K=50$\nmatrices ${\\bm{F}}_{k}\\in{\\mathbb{R}}^{n\\times n}$ are generated in which\neach entry is drawn from the uniform distribution in the\ninterval $[-0.5,0.5]$. Then, we set\n${\\bm{A}}_{k}:={\\bm{F}}_{k}{\\bm{F}}_{k}^\\top$. The values of the variables\n$y_{k}$ are randomly chosen from $\\{\\pm 1\\}$ with same\nprobabilities. We set ${\\bm{b}}={\\bm{1}}$ and ${\\bm{c}}={\\bm{1}}/(\\lambda K)$. We\nexhaustively tested Cyclic, Perm, and Rand with the settings\nof $\\lambda=10^{-2},10^{-3},10^{-4}$ and $n=10,50,100$.\n\nFigure~\\ref{fig:logdet.primal}\ndemonstrates the convergence behavior of the\ncyclic Dykstra algorithm and the two stochastic Dykstra\nalgorithm with various $\\lambda$ and $n$. Here, one epoch is\ncalled $K$ times projection onto a single half-space.\nISBD is employed as the objective function for learning the metric~${\\bm{W}}$. \nThe\nobjective gap is defined as the difference between the\ncurrent objective value and the minimum. In most of the\nsettings, the two stochastic Dykstra algorithms converged\nfaster than the cyclic algorithm. Especially when\n$\\lambda=10^{-4}$, the cyclic algorithm was too slow to use\nit in practice.\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Generalization performance for pattern recognition}\n\nWe used the Brodatz texture dataset~\\cite{randen1999filtering} containing 111\ndifferent texture images to examine the generalization\nperformance for texture classification. Each image has a size of\n$640\\times 640$ and gray-scaled. Images\nwere individually divided into four sub-images of equal size. One of\nthe four sub-images was picked randomly and used for\ntesting, and the rest of the images were used for training.\n\nFor each training image and each testing image, covariance\ndescriptors of randomly chosen $50$ were extracted from\n$128\\times 128$ patches. The covariance matrices are of\nfive-dimensional feature vectors\n$\\left[I,|I_{x}|,|I_{y}|,|I_{xx}|,|I_{yy}|\\right]^\\top$.\nThen, $11,100(=111\\times 2 \\times 50)$ covariance\ndescriptors are obtained for training and testing,\nrespectively. For evaluation of generalized performance,\n$k$-nearest neighbor classifier is used, where the number of\nthe nearest neighbors is set to three. We set $K=100\\times\nn_{c}$, $b_{\\text{ub}}=0.05$, and $b_{\\text{lb}}=0.95$.\n\nWe also examined the generalization performance for generic\nvisual categorization using the ETH-80 dataset~\\cite{leibe2003analyzing} containing\n$n_{c}=8$ classes. Each class has $10$ objects, each of which includes $41$\ncolored images. For every\nobject, $20$ images are randomly chosen and used for\ntraining, and the rest of images are used for testing.\n\nOne covariance matrix is obtained from each image. Eight\nfeatures $\\left[x, y, R, G, B,\n|I_{x}|,|I_{y}|,|I_{xx}|,|I_{yy}|\\right]^\\top$ are obtained\nfrom each pixel in an image.\n\nWe tried four types of ${\\bm{\\Phi}}$:\n\n\\textbf{Id}: ${\\bm{\\Phi}}({\\bm{X}})={\\bm{X}}$,\n\\textbf{Log}: ${\\bm{\\Phi}}({\\bm{X}})={\\operatorname{logm}}({\\bm{X}})$,\n\\textbf{Sqrt}: ${\\bm{\\Phi}}({\\bm{X}})={\\bm{X}}^{1/2}$,\n\\textbf{Chol}: ${\\bm{\\Phi}}({\\bm{X}})=\\text{chol}({\\bm{X}})$. \nThe parameter ${\\bm{W}}$ is determined\nby the metric learning algorithms with\nISBD, L2BD, and REBD, to be compared with ${\\bm{W}}={\\bm{I}}$ we denote as \\textbf{Eye}. \nNote that $D_{{\\bm{\\Phi}}}(\\cdot,\\cdot;{\\bm{I}})=D_{{\\bm{\\Phi}}}(\\cdot,\\cdot)$. \nFigure~\\ref{fig:accbars} gives the accuracy bar plots for the two\nmulti-class classification problems. Whichever ${\\bm{\\Phi}}$ is\nused, supervised metric learning improved the generalization\nperformances both for texture classification and for generic\nvisual categorization. For texture classification, the\nCholesky decomposition-based mapping $\\text{chol}(\\cdot)$\nachieved the best accuracy, while the matrix logarithm-based mapping\n$\\text{logm}(\\cdot)$ obtained the highest accuracy for generic image\ncategorization.\n\n\n\\begin{figure*}[t!]\n\\begin{center}\n  \\begin{tabular}{ll}\n    (a) Brodatz texture dataset\n    & (b) ETH-80  dataset\n    \\\\\n    \\includegraphics[width=0.4\\textwidth]{002-brodatz_acc.eps}\n    &\n    \\includegraphics[width=0.4\\textwidth]{003-eth80_acc.eps}\n\\end{tabular}\n\\end{center}\n\\caption{Generalized performances for pattern recognition. \\label{fig:accbars}}\n\\end{figure*}\n\n\\section{Conclusions}\n\nIn this paper, we have devised several objective functions\nfor metric learning on positive semidefinite cone, all of\nwhich can be minimized by the Dykstra algorithm. We have\nintroduced a new technique that performs each update\nefficiently when the Dykstra algorithm is applied to the\nmetric learning problems. We have empirically demonstrated\nthat the stochastic versions of the Dykstra algorithm are\nmuch faster than the original algorithm.\n\n\n\\section*{Acknowledgment}\n\nThis work was supported by JSPS KAKENHI Grant Number\n26249075, 40401236. \nThe last author would like to thank Dr. Zhiwu Huang for fruitful discussions.\n \n\n\\bibliographystyle{plain} \n\\begin{thebibliography}{10}\n\n\\bibitem{arsigny2006log}\nVincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache.\n\\newblock Log-euclidean metrics for fast and simple calculus on diffusion\n  tensors.\n\\newblock {\\em Magnetic resonance in medicine}, 56(2):411--421, 2006.\n\n\\bibitem{Bhatia-book09}\nRajendra Bhatia.\n\\newblock {\\em Positive Definite Matrices}.\n\\newblock Princeton University Press, 2009.\n\n\\bibitem{Bottou10a-sgd}\nL\\'{e}on Bottou.\n\\newblock Large-scale machine learning with stochastic gradient descent.\n\\newblock In Yves Lechevallier and Gilbert Saporta, editors, {\\em Proceedings\n  of the 19th International Conference on Computational Statistics\n  (COMPSTAT'2010)}, pages 177--187, Paris, France, August 2010. Springer.\n\n\\bibitem{Censor98}\nYair Censor and Simeon Reich.\n\\newblock The dykstra algorithm with bregman projections.\n\\newblock {\\em Communications in Applied Analysis}, 2:407--419, 1998.\n\n\\bibitem{Cristianini01kta}\nNello Cristianini, John Shawe-Taylor, Andr\u00c3\u00a9 Elisseeff, and Jaz~S. Kandola.\n\\newblock On kernel-target alignment.\n\\newblock In Thomas~G. Dietterich, Suzanna Becker, and Zoubin Ghahramani,\n  editors, {\\em NIPS}, pages 367--373. MIT Press, 2001.\n\n\\bibitem{DavKulJaiSraDhi07a}\nJason~V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit~S. Dhillon.\n\\newblock Information-theoretic metric learning.\n\\newblock In {\\em Proceedings of the 24th international conference on Machine\n  learning}, pages 209--216. ACM, 2007.\n\n\\bibitem{defazio2014finito}\nAaron~J Defazio, Tib{\\'e}rio~S Caetano, and Justin Domke.\n\\newblock Finito: A faster, permutable incremental gradient method for big data\n  problems.\n\\newblock {\\em arXiv preprint arXiv:1407.2710}, 2014.\n\n\\bibitem{Dryden09a}\nIan~L Dryden, Alexey Koloydenko, and Diwei Zhou.\n\\newblock Non-euclidean statistics for covariance matrices, with applications\n  to diffusion tensor imaging.\n\\newblock {\\em The Annals of Applied Statistics}, pages 1102--1123, 2009.\n\n\\bibitem{Dykstra83}\nRichard~L. Dykstra.\n\\newblock An algorithm for restricted least squares regression.\n\\newblock {\\em Journal of the American Statistical Association},\n  78(384):837--842, December 1983.\n\n\\bibitem{ZhiwuHuang15a}\nZhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen.\n\\newblock Log-euclidean metric learning on symmetric positive definite manifold\n  with application to image set classification.\n\\newblock In {\\em Proceedings of the 32nd International Conference on Machine\n  Learning, {ICML} 2015}, pages 720--729, 2015.\n\n\\bibitem{JayasumanaCVPR13}\nSadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and\n  Mehrtash~Tafazzoli Harandi.\n\\newblock Kernel methods on the riemannian manifold of symmetric positive\n  definite matrices.\n\\newblock In {\\em CVPR}, pages 73--80. IEEE, 2013.\n\n\\bibitem{Johnson13a-svrg}\nRie Johnson and Tong Zhang.\n\\newblock Accelerating stochastic gradient descent using predictive variance\n  reduction.\n\\newblock In {\\em Advances in Neural Information Processing Systems 26:\n  Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United\n  States.}, pages 315--323, 2013.\n\n\\bibitem{KatNag10a}\nTsuyoshi Kato and Nozomi Nagano.\n\\newblock Metric learning for enzyme active-site search.\n\\newblock {\\em Bioinformatics}, 26(21):2698--2704, November 2010.\n\n\\bibitem{KatTakOma13a}\nTsuyoshi Kato, Wataru Takei, and Shinichiro Omachi.\n\\newblock A discriminative metric learning algorithm for face recognition.\n\\newblock {\\em IPSJ Transactions on Computer Vision and Applications},\n  5:85--89, 2013.\n\n\\bibitem{KatTsuAsa05a}\nTsuyoshi Kato, Koji Tsuda, and Kiyoshi Asai.\n\\newblock Selective integration of multiple biological data for supervised\n  network inference.\n\\newblock {\\em Bioinformatics}, 21:2488--2495, May 2005.\n\n\\bibitem{leibe2003analyzing}\nBastian Leibe and Bernt Schiele.\n\\newblock Analyzing appearance and contour based methods for object\n  categorization.\n\\newblock In {\\em Computer Vision and Pattern Recognition, 2003. Proceedings.\n  2003 IEEE Computer Society Conference on}, volume~2, pages II--409. IEEE,\n  2003.\n\n\\bibitem{lowe2004distinctive}\nDavid~G Lowe.\n\\newblock Distinctive image features from scale-invariant keypoints.\n\\newblock {\\em International journal of computer vision}, 60(2):91--110, 2004.\n\n\\bibitem{MatRelTak15a}\nTomoki Matsuzawa, Raissa Relator, Wataru Takei, Shinichiro Omachi, and Tsuyoshi\n  Kato.\n\\newblock Mahalanobis encodings for visual categorization.\n\\newblock {\\em IPSJ Transactions on Computer Vision and Applications},\n  7:69--73, July 2015.\n\\newblock doi: 10.2197/ipsjtcva.7.1.\n\n\\bibitem{pennec2006riemannian}\nXavier Pennec, Pierre Fillard, and Nicholas Ayache.\n\\newblock A riemannian framework for tensor computing.\n\\newblock {\\em International Journal of Computer Vision}, 66(1):41--66, 2006.\n\n\\bibitem{perronnin2010improving}\nFlorent Perronnin, Jorge Sanchez, and Thomas Mensink.\n\\newblock Improving the fisher kernel for large-scale image classification.\n\\newblock In {\\em Computer Vision--ECCV 2010}, pages 143--156. Springer, 2010.\n\n\\bibitem{randen1999filtering}\nTrygve Randen and John~Hakon Husoy.\n\\newblock Filtering for texture classification: A comparative study.\n\\newblock {\\em Pattern Analysis and Machine Intelligence, IEEE Transactions\n  on}, 21(4):291--310, 1999.\n\n\\bibitem{RelHirItoKat14a}\nRaissa Relator, Yoshihiro Hirohashi, Eisuke Ito, and Tsuyoshi Kato.\n\\newblock Mean polynomial kernel and its application to vector sequence\n  recognition.\n\\newblock {\\em IEICE Transactions on Information and Systems},\n  E97-D(7):1855--1863, July 2014.\n\n\\bibitem{RelNagKat16a}\nRaissa Relator, Nozomi Nagano, and Tsuyoshi Kato.\n\\newblock Using bregmann divergence regularized machine for comparison of\n  molecular local structures.\n\\newblock {\\em IEICE Transactions on Information \\& Systems}, E99-D(1):--, Jan\n  2016.\n\n\\bibitem{Roux12a-sag}\nNicolas~L. Roux, Mark Schmidt, and Francis~R. Bach.\n\\newblock A stochastic gradient method with an exponential convergence \\_rate\n  for finite training sets.\n\\newblock In F.~Pereira, C.J.C. Burges, L.~Bottou, and K.Q. Weinberger,\n  editors, {\\em Advances in Neural Information Processing Systems 25}, pages\n  2663--2671. Curran Associates, Inc., 2012.\n\n\\bibitem{shakhnarovich2002face}\nGregory Shakhnarovich, John~W Fisher, and Trevor Darrell.\n\\newblock Face recognition from long-term observations.\n\\newblock In {\\em ECCV 2002}, pages 851--865. Springer Berlin Heidelberg, 2002.\n\n\\bibitem{Shalev-Shwartz11a-pegasos}\nShai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.\n\\newblock {Pegasos:} primal estimated sub-gradient solver for {SVM}.\n\\newblock {\\em Math. Program.}, 127(1):3--30, 2011.\n\n\\bibitem{sra2012a}\nSuvrit Sra.\n\\newblock A new metric on the manifold of kernel matrices with application to\n  matrix geometric means.\n\\newblock In {\\em Advances in Neural Information Processing Systems}, pages\n  144--152, 2012.\n\n\\bibitem{vemulapalli2015riemannian}\nRaviteja Vemulapalli and David~W Jacobs.\n\\newblock Riemannian metric learning for symmetric positive definite matrices.\n\\newblock {\\em arXiv preprint arXiv:1501.02393}, 2015.\n\n\\bibitem{wang2004affine}\nZhizhou Wang and Baba~C Vemuri.\n\\newblock An affine invariant tensor dissimilarity measure and its applications\n  to tensor-valued image segmentation.\n\\newblock In {\\em Computer Vision and Pattern Recognition, 2004. CVPR 2004.\n  Proceedings of the 2004 IEEE Computer Society Conference on}, volume~1, pages\n  I--228. IEEE, 2004.\n\n\\bibitem{yger2015supervised}\nFlorian Yger and Masashi Sugiyama.\n\\newblock Supervised logeuclidean metric learning for symmetric positive\n  definite matrices.\n\\newblock {\\em arXiv preprint arXiv:1502.03505}, 2015.\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 31527, "prevtext": "\n\nwhere we have defined \n\n\n", "index": 65, "text": "\\begin{align*}\n  \\text{BD}_{\\star} := \\min_{({\\bm{W}},{\\bm{\\xi}})\\in\\bigcap_{h}{\\mathcal{C}}_{h}}\n  \\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}})). \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{BD}_{\\star}:=\\min_{({\\bm{W}},{\\bm{\\xi}})\\in\\bigcap_{h}{%&#10;\\mathcal{C}}_{h}}\\text{BD}_{\\varphi}(({\\bm{W}},{\\bm{\\xi}}),({\\bm{I}},{\\bm{1}})).\" display=\"inline\"><mrow><mrow><msub><mtext>BD</mtext><mo>\u22c6</mo></msub><mo>:=</mo><mrow><mrow><munder><mi>min</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo>,</mo><mi>\ud835\udf43</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mstyle displaystyle=\"false\"><msub><mo largeop=\"true\" mathsize=\"160%\" stretchy=\"false\" symmetric=\"true\">\u22c2</mo><mi>h</mi></msub></mstyle><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>h</mi></msub></mrow></mrow></munder><mo>\u2061</mo><msub><mtext>BD</mtext><mi>\u03c6</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7e</mi><mo>,</mo><mi>\ud835\udf43</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc70</mi><mo>,</mo><mn>\ud835\udfcf</mn><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]