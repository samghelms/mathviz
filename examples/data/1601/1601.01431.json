[{"file": "1601.01431.tex", "nexttext": "\nwhere $\\mathbf M_k\\in\\mathbb{R}^{p\\times q}$ is the mean matrix, $\\pi_k$s satisfy $\\pi_k>0$ and $\\sum_{k=1}^K \\pi_k=1$, and $\\mathbf{L}_k \\in\\mathbb{R}^{p\\times r}$ and $\\mathbf{R}_k\\in\\mathbb{R}^{q\\times c}$ are the row and column loading matrices with $r\\leq p,c\\leq q$. Note that $\\mathbf M_k$, $\\mathbf L_k$ and $\\mathbf R_k$ are associated with each component of mixture model, respectively. $\\mathbf{B}_n^{(k)}\\in\\mathbb{R}^{r\\times c}$ is the latent variable core of $\\mathbf{X}_n$ associated with $k$-th matrix-variate Gaussian component \\cite[Sec 3.3]{Timm2002} with $\\sigma_k^2$ as residual variance.\n\nLike \\cite{Bishop2006}, we introduce a $K$-dimensional binary random variable $\\mathbf z$ having a 1-of-$K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0. That is, $z_k\\in\\{0,1\\}$ and $\\sum_{k=1}^Kz_k=1$.\nThe distribution of $\\mathbf z$ is defined by\n", "itemtype": "equation", "pos": 8666, "prevtext": "\n\n\n\\title{Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis}\n\n\\author{Fujiao Ju$^1$, Yanfeng Sun$^1$, Junbin Gao$^2$, Simeng Liu$^1$ and Yongli Hu$^1$\\\\\n{\\small $^1$College of Metropolitan Transportation,\nBeijing University of Technology, Beijing 100124, China}\\\\\n{\\small School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW 2795, Australia}\\\\\n{\\tt\\small \\{jufujiao2013,smliu\\}@emails.bjut.edu.cn; \\{yfsun,huyongli\\}@bjut.edu.cn; jbgao@csu.edu.au}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n}\n\n\\maketitle\n\n\n\n\\begin{abstract}\nThe probabilistic principal component analysis (PPCA) is built upon a global linear mapping, with which it is insufficient to model complex data variation. This paper proposes a mixture of bilateral-projection probabilistic principal component analysis model (mixB2DPPCA) on 2D data. With multi-components in the mixture, this model can be seen as a `soft' cluster algorithm and has capability of modeling data with complex structures. A Bayesian inference scheme has been proposed based on the variational EM (Expectation-Maximization) approach for learning model parameters.   Experiments on some publicly available databases show that the performance of mixB2DPPCA has been largely improved, resulting in more accurate reconstruction errors and recognition rates than the existing PCA-based algorithms. \n\\end{abstract}\n\n\n\\section{Introduction}\n\nPrinciple Component Analysis (PCA) \\cite{Bishop2006} is one of  popular dimensionality reduction methods widely used in image analysis \\cite{HoyerHyvarinen2000, KeSukthankar2004}, pattern recognition \\cite{HeYanHuNiyogiZhang2005, LuPlataniotisVenetsanopoulosLi2006} and machine learning \\cite{KriegelKrogerSchubert2008} for data analysis. It can be  derived under algebraic framework. However, algebraic models don't have flexibility of providing confidence information of the model when dealing with noisy data. This is due to the absence of an associated probability density or generative model in algebraic framework. \n\nTo compensate the algebraic PCA drawbacks, Tipping and Bishop \\cite{TippingBishop1999} firstly proposed a probabilistic PCA model, called PPCA. Under the probabilistic framework, PPCA takes advantage of Bayesian learning and inference by combining the likelihood with appropriate priors. As a result, the observed data are regarded as random variables, generated from a set of latent random variables which follow the Gaussian distribution of zero mean and identity covariance, with additive noises following a Gaussian distribution with zero mean and an isotropic covariance.\n\nUnder such a probabilistic learning framework, the model parameters in PPCA can be easily solved by the maximum likelihood estimation (MLE). Much progress has been made based on PCA and PPCA in the last couple of decades \\cite{ArchambeauDelannayVerleysen2006,Gao2007}. \n\nPPCA and standard PCA methods can be interpreted in many ways, one of which assumes that the observed high-dimensional data are generated from their low-dimensional factors through a linear model with the corruption of Gaussian noise. So those algorithms essentially use a linear model for representing the entire data in a low dimensional subspace. It may be insufficient to model data with large variation caused by, for example, pose, expression and lighting in face recognition. Thus the application scope of PPCA and PCA-based methods is necessarily somewhat limited by its global linearity assumption. An alternative improving paradigm is to model the complex manifold with a mixture of local linear PPCA sub-models. Thus the single PCA model could be extended to a mixture of such sub-models.\n\nA number of `mixture of PPCA' have been proposed in literature. The first work was done by Ghahramani and Hinton \\cite{GhahramaniHinton1996}. They presented an exact Expectation-Maximization (EM) algorithm for fitting the parameters of the mixture of factor analyzers. By constraining the error covariance to be a diagonal matrix whose elements are usually equal, the mixture of factor analyzers became the mixture of PPCA \\cite{TippingBishop1999a}. Bishop and Tipping \\cite{BishopTipping1998} extended the mixture of PPCA model to achieve a hierarchical mixture model. Su and Dy \\cite{SuDy2004} introduced an automated hierarchical mixture of PPCA algorithm, which utilizes the integrated classification likelihood as a criterion for splitting and stopping the addition of hierarchical levels. Kim \\emph{et al}. \\cite{KimKimBang2003} proposed a fast and sub-optimal selection method of model order such as the number of mixture components and the number of PCA bases for the PCA mixture model, consisting of a combination of many PCAs. In addition, under the assumption of the Student-$t$ distribution, the related research includes the mixture model of Student-$t$ components \\cite{PeelMcLachlan2000}, which actually is a generalized mixture of Gaussian model without considering subspace structures, and more recent work such as the robust subspace mixture model \\cite{RidderFranc2003}, in which both the likelihood and the latent variables were supposed to follow the Student-$t$ distribution and the EM algorithm was applied to the model. In 2005, Archambeau \\cite{Archambeau2005} discussed the robust models in the context of finite mixture models, and a similar work for the mixture of the robust Laplacians was presented in \\cite{GaoXu2007}. These mixture models are important as it enables one to model nonlinear relationships by aligning a collection of such local models.\n\nThe aforementioned models are concerned with vectorial data. \nIn order to apply these methods to 2D data, a typical workaround way is to vectorize 2D data. Vectorizing 2D data not only results in very high-dimensional data, causing the problem of the curse of dimensionality \\cite{XieYanKwokHuang2008}, but also ignores valuable information on the spatial relationship among 2D data. Instead of using vectorization, PCA approaches for two-dimensional data (2DPCA) have been proposed \\cite{WangWang2013,YangZhangFrangiYang2004,YuBiYe2008}, \nto generally extract features of 2D data under the assumption of Gaussian noises. Ju \\emph{et al}. \\cite{JuSunGaoHuYin2015} proposed a probabilistic 2DPCA model to deal with outlier noises by using Laplacian distribution. This model benefits outlier detection. Wang \\emph{et al}. \\cite{WangChenHuLuo2008} extended the probabilistic 2DPCA to a mixture of local probabilistic 2DPCA models (MP2DPCA). MP2DPCA offers a tempting prospect of being able to model data with complex variation.\n\n\nMP2DPCA model regards each row vector of the 2D data as a observed sample and used all rows to train the mixture model, resulting in mean vectors from the mixture model. This is essentially a unilateral projection based scheme, where only one side multiplication is taken into account. The unilateral scheme usually preserves the correlation information among the row/column vectors of the images and more parameters are needed to well represent an image. To tackle these problems, a bilateral-projection scheme is favored. In this study, our intention is propose a mixture of bilateral-projection-based probabilistic 2DPCA (mixB2DPPCA) model. Different from MP2DPCA, we regard each 2D images as observed samples in their natural shape and reduce 2D dimensionality directly. The mixB2DPPCA has two major advantages: 1) The model makes use of structured information of 2D data and can be easily extended for high order tensorial data. All the algorithm derivations remain without major difficulties. 2) mix2DPPCA carries over all the advantages of the mixture of PPCA.\n\nThe remainder of the paper is organized as follows. In Section \\ref{Sec:II}, the mixture of bilateral-projection two-dimensional probabilistic PCA model is introduced. The variational approximation approach for solving the model is presented in Section \\ref{Sec:III}. In Section \\ref{Sec:IV}, some experimental results are conducted to evaluate the performance of the proposed model. Finally, conclusions are summarized in Section \\ref{Sec:V}.\n\n\n\\section{Mixture of Bilateral-Projection 2DPPCA Model (mixB2DPPCA)}\\label{Sec:II}\n\nIn this section, we introduce the mixture of bilateral-projection probabilistic 2DPCA model. For the purpose, we introduce several notations. Let $\\mathcal X=\\{\\mathbf{X}_1,\\mathbf{X}_2,...,\\mathbf{X}_N\\}$ be $N$ independent and identical random samples with values in $\\mathbb{R}^{p\\times q}$. For $n=1,...,N$, we suppose that sample $\\mathbf X_n$ is generated independently from a mixture of $K$ underlying components with unknown probabilities $\\pi_1,\\pi_2,...,\\pi_K$,\n\n", "index": 1, "text": "\\begin{align}\np(\\mathbf X_n|\\mathbf B_n)= \\sum_{k=1}^K\\pi_k\\mathcal N(\\mathbf L_k\\mathbf B_n^{(k)}\\mathbf R_k^T+\\mathbf M_k,\\sigma_k\\mathbf I,\\sigma_k\\mathbf I)\\label{model}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(\\mathbf{X}_{n}|\\mathbf{B}_{n})=\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N%&#10;}(\\mathbf{L}_{k}\\mathbf{B}_{n}^{(k)}\\mathbf{R}_{k}^{T}+\\mathbf{M}_{k},\\sigma_{%&#10;k}\\mathbf{I},\\sigma_{k}\\mathbf{I})\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc01</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><msub><mi>\u03c0</mi><mi>k</mi></msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhich can be written as\n", "itemtype": "equation", "pos": 9771, "prevtext": "\nwhere $\\mathbf M_k\\in\\mathbb{R}^{p\\times q}$ is the mean matrix, $\\pi_k$s satisfy $\\pi_k>0$ and $\\sum_{k=1}^K \\pi_k=1$, and $\\mathbf{L}_k \\in\\mathbb{R}^{p\\times r}$ and $\\mathbf{R}_k\\in\\mathbb{R}^{q\\times c}$ are the row and column loading matrices with $r\\leq p,c\\leq q$. Note that $\\mathbf M_k$, $\\mathbf L_k$ and $\\mathbf R_k$ are associated with each component of mixture model, respectively. $\\mathbf{B}_n^{(k)}\\in\\mathbb{R}^{r\\times c}$ is the latent variable core of $\\mathbf{X}_n$ associated with $k$-th matrix-variate Gaussian component \\cite[Sec 3.3]{Timm2002} with $\\sigma_k^2$ as residual variance.\n\nLike \\cite{Bishop2006}, we introduce a $K$-dimensional binary random variable $\\mathbf z$ having a 1-of-$K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0. That is, $z_k\\in\\{0,1\\}$ and $\\sum_{k=1}^Kz_k=1$.\nThe distribution of $\\mathbf z$ is defined by\n", "index": 3, "text": "\n\\[\np(z_k=1):=\\pi_k,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"p(z_{k}=1):=\\pi_{k},\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>k</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>:=</mo><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\nThus the conditional distribution of $\\mathbf X_n$ given a particular value for $\\mathbf z_n$ and $\\mathbf B_n^{(k)}$ is the matrix-variate Gaussian\n\n", "itemtype": "equation", "pos": 9818, "prevtext": "\nwhich can be written as\n", "index": 5, "text": "\n\\[\np(\\mathbf z)=\\prod_{k=1}^K\\pi_k^{z_{k}}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{z})=\\prod_{k=1}^{K}\\pi_{k}^{z_{k}}.\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mi>\u03c0</mi><mi>k</mi><msub><mi>z</mi><mi>k</mi></msub></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nGenerally we have\n", "itemtype": "equation", "pos": 10016, "prevtext": "\n\nThus the conditional distribution of $\\mathbf X_n$ given a particular value for $\\mathbf z_n$ and $\\mathbf B_n^{(k)}$ is the matrix-variate Gaussian\n\n", "index": 7, "text": "\\begin{align*}\np(\\mathbf X_n|z_{nk}=1,\\mathbf B_n^{(k)})= \\mathcal N(\\mathbf X_n|\\mathbf L_k\\mathbf B_n^{(k)}\\mathbf R_k^T+\\mathbf M_k,\\sigma_k\\mathbf I,\\sigma_k\\mathbf I).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(\\mathbf{X}_{n}|z_{nk}=1,\\mathbf{B}_{n}^{(k)})=\\mathcal{N}(%&#10;\\mathbf{X}_{n}|\\mathbf{L}_{k}\\mathbf{B}_{n}^{(k)}\\mathbf{R}_{k}^{T}+\\mathbf{M}%&#10;_{k},\\sigma_{k}\\mathbf{I},\\sigma_{k}\\mathbf{I}).\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nIn this model setting, the parameters are $\\Theta = \\{\\pi_k,\\mathbf M_k,\\mathbf L_k,\\mathbf R_k,\\sigma^2_k\\} (k=1,..,K)$, and the latent variables are $\\mathbf z_n$ and $\\mathbf B_n^{(k)}(n=1,...,N)$.\n\n\n\nTo develop a generative Bayesian model, we define a matrix-variate Gaussian prior $p(\\mathbf B_n^{(k)})$ over the latent variable with zero-mean unit-covariance, defined as\n\n", "itemtype": "equation", "pos": 10219, "prevtext": "\nGenerally we have\n", "index": 9, "text": "\n\\[\np(\\mathbf X_n|\\mathbf z_n,\\mathbf B_n^{(k)})= \\prod_{k=1}^K\\mathcal N(\\mathbf X_n|\\mathbf L_k\\mathbf B_n^{(k)}\\mathbf R_k^T+\\mathbf M_k,\\sigma_k\\mathbf I,\\sigma_k\\mathbf I)^{z_{nk}}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{X}_{n}|\\mathbf{z}_{n},\\mathbf{B}_{n}^{(k)})=\\prod_{k=1}^{K}\\mathcal{%&#10;N}(\\mathbf{X}_{n}|\\mathbf{L}_{k}\\mathbf{B}_{n}^{(k)}\\mathbf{R}_{k}^{T}+\\mathbf%&#10;{M}_{k},\\sigma_{k}\\mathbf{I},\\sigma_{k}\\mathbf{I})^{z_{nk}}\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc33</mi><mi>n</mi></msub><mo>,</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>+</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo>,</mo><msub><mi>\u03c3</mi><mi>k</mi></msub><mi>\ud835\udc08</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>z</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></msup></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\nHence the joint log-likelihood of the observed  data set for such a mixture model is:\n\n", "itemtype": "equation", "pos": 10785, "prevtext": "\nIn this model setting, the parameters are $\\Theta = \\{\\pi_k,\\mathbf M_k,\\mathbf L_k,\\mathbf R_k,\\sigma^2_k\\} (k=1,..,K)$, and the latent variables are $\\mathbf z_n$ and $\\mathbf B_n^{(k)}(n=1,...,N)$.\n\n\n\nTo develop a generative Bayesian model, we define a matrix-variate Gaussian prior $p(\\mathbf B_n^{(k)})$ over the latent variable with zero-mean unit-covariance, defined as\n\n", "index": 11, "text": "\\begin{equation*}\np(\\mathbf B_n^{(k)})=\\mathcal{N}(0,\\mathbf{I}_r,\\mathbf{I}_c)=\\bigg(\\frac{1}{2\\pi}\\bigg)^{\\frac{rc}{2}}\\cdot\\exp\\{-\\frac{1}{2}\\mathbf{tr}(\\mathbf B_n^{(k)T}\\mathbf B_n^{(k)})\\}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{B}_{n}^{(k)})=\\mathcal{N}(0,\\mathbf{I}_{r},\\mathbf{I}_{c})=\\bigg{(}%&#10;\\frac{1}{2\\pi}\\bigg{)}^{\\frac{rc}{2}}\\cdot\\exp\\{-\\frac{1}{2}\\mathbf{tr}(%&#10;\\mathbf{B}_{n}^{(k)T}\\mathbf{B}_{n}^{(k)})\\}\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msub><mi>\ud835\udc08</mi><mi>r</mi></msub><mo>,</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mfrac><mrow><mi>r</mi><mo>\u2062</mo><mi>c</mi></mrow><mn>2</mn></mfrac></msup><mo>\u22c5</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mi>\ud835\udc2d\ud835\udc2b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\n\n\\section{Variational Approximation for mixB2DPPCA Model}\\label{Sec:III}\nWe employ the Expectation Maximization (EM) algorithm to solve for model parameters $\\Theta$. To maximize the log-likelihood of mixB2DPPCA, we take the expectation of $\\mathcal L$ with respect to the posterior distribution of both $\\mathbf B_n^{(k)}$ and $z_{nk}$, i.e.,\n\n", "itemtype": "equation", "pos": 11083, "prevtext": "\n\nHence the joint log-likelihood of the observed  data set for such a mixture model is:\n\n", "index": 13, "text": "\\begin{align*}\n\\mathcal L = \\sum_{n=1}^N\\sum_{k=1}^Kz_{nk}\\ln\\{\\pi_kp(\\mathbf X_n,\\mathbf B_n^{(k)})\\}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{L}=\\sum_{n=1}^{N}\\sum_{k=1}^{K}z_{nk}\\ln\\{\\pi_{k}p(%&#10;\\mathbf{X}_{n},\\mathbf{B}_{n}^{(k)})\\}.\" display=\"inline\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>z</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>,</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $\\langle \\cdot\\rangle$ denotes the expectation.\n\nIn E-step, we update Q-distributions of all hidden variables $\\mathbf B_n^{(k)}$ and $z_{n,k}$ with the current fixed parameter values for $\\Theta$. In M-step, maximizing the function $\\langle\\mathcal L\\rangle$ with respect to the model parameters $\\Theta$, we can obtain `new' values for these parameters.\n\n\\subsection{\\textbf{Variational E-step}}\n\n\\subsubsection{Update the Posterior Distribution of $z_{nk}$}\nSuppose $\\gamma_{nk}:=\\langle z_{nk}\\rangle$ and it is actually the posterior probability of $k$-mixture generating data point $\\mathbf X_n$. By using the same strategy for the mixture Gaussian model \\cite{Bishop2006}, we can obtain\n\n", "itemtype": "equation", "pos": 11545, "prevtext": "\n\n\n\\section{Variational Approximation for mixB2DPPCA Model}\\label{Sec:III}\nWe employ the Expectation Maximization (EM) algorithm to solve for model parameters $\\Theta$. To maximize the log-likelihood of mixB2DPPCA, we take the expectation of $\\mathcal L$ with respect to the posterior distribution of both $\\mathbf B_n^{(k)}$ and $z_{nk}$, i.e.,\n\n", "index": 15, "text": "\\begin{align}\n\\langle\\mathcal L\\rangle &= \\sum_{n=1}^N\\sum_{k=1}^K\\langle z_{nk}\\rangle\\{\\ln \\pi_k-\\frac{pq}{2}\\ln\\sigma^2_{k}-\\frac{1}{2}\\text{tr}(\\langle\\mathbf B_n^{(k)T}\\mathbf B_n^{(k)}\\rangle)\\nonumber\\\\\n&-\\frac{1}{2\\sigma^2_k}\\text{tr}(\\mathbf X_n-\\mathbf M_k)^T(\\mathbf X_n-\\mathbf M_k)\\nonumber\\\\\n&+\\frac{1}{\\sigma^2}\\text{tr}((\\mathbf X_n-\\mathbf M_k)^T\\mathbf L_k \\langle\\mathbf B^{(k)}_n\\rangle\\mathbf R_k^T)\\nonumber\\\\\n&-\\frac{1}{2\\sigma^2}\\text{tr}(\\langle\\mathbf B^{(k)T}_n\\mathbf L_k^T\\mathbf L_k \\mathbf B^{(k)}_n\\rangle\\mathbf R_k^T\\mathbf R_k)\\label{L-fuction} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\mathcal{L}\\rangle\" display=\"inline\"><mrow><mo stretchy=\"false\">\u27e8</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">\u27e9</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{n=1}^{N}\\sum_{k=1}^{K}\\langle z_{nk}\\rangle\\{\\ln\\pi_{k}-%&#10;\\frac{pq}{2}\\ln\\sigma^{2}_{k}-\\frac{1}{2}\\text{tr}(\\langle\\mathbf{B}_{n}^{(k)T%&#10;}\\mathbf{B}_{n}^{(k)}\\rangle)\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>z</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mrow><mo stretchy=\"false\">{</mo><mi>ln</mi><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow><mn>2</mn></mfrac></mstyle><mi>ln</mi><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mtext>tr</mtext><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{2\\sigma^{2}_{k}}\\text{tr}(\\mathbf{X}_{n}-\\mathbf{M}_{k}%&#10;)^{T}(\\mathbf{X}_{n}-\\mathbf{M}_{k})\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{\\sigma^{2}}\\text{tr}((\\mathbf{X}_{n}-\\mathbf{M}_{k})^{T%&#10;}\\mathbf{L}_{k}\\langle\\mathbf{B}^{(k)}_{n}\\rangle\\mathbf{R}_{k}^{T})\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msup><mi>\u03c3</mi><mn>2</mn></msup></mfrac></mstyle><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{2\\sigma^{2}}\\text{tr}(\\langle\\mathbf{B}^{(k)T}_{n}%&#10;\\mathbf{L}_{k}^{T}\\mathbf{L}_{k}\\mathbf{B}^{(k)}_{n}\\rangle\\mathbf{R}_{k}^{T}%&#10;\\mathbf{R}_{k})\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $p(\\mathbf X_n|k)$ is the $k$-the component, representing the marginal distribution for the observed data $\\mathbf X_n$ over the latent variable. In our case, the marginal distribution of $\\mathbf X_n$ is obtained by integrating out the latent variable $\\mathbf B_n^{(k)}$:\n", "itemtype": "equation", "pos": 12838, "prevtext": "\nwhere $\\langle \\cdot\\rangle$ denotes the expectation.\n\nIn E-step, we update Q-distributions of all hidden variables $\\mathbf B_n^{(k)}$ and $z_{n,k}$ with the current fixed parameter values for $\\Theta$. In M-step, maximizing the function $\\langle\\mathcal L\\rangle$ with respect to the model parameters $\\Theta$, we can obtain `new' values for these parameters.\n\n\\subsection{\\textbf{Variational E-step}}\n\n\\subsubsection{Update the Posterior Distribution of $z_{nk}$}\nSuppose $\\gamma_{nk}:=\\langle z_{nk}\\rangle$ and it is actually the posterior probability of $k$-mixture generating data point $\\mathbf X_n$. By using the same strategy for the mixture Gaussian model \\cite{Bishop2006}, we can obtain\n\n", "index": 17, "text": "\\begin{align}\n\\gamma_{nk} = \\frac{\\pi_kp(\\mathbf X_n|k)}{p(\\mathbf X_n)}\\label{gamma},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\gamma_{nk}=\\frac{\\pi_{k}p(\\mathbf{X}_{n}|k)}{p(\\mathbf{X}_{n})},\" display=\"inline\"><mrow><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c0</mi><mi>k</mi></msub><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\nDifferent from the vectorial PPCA, we note that the marginal distribution of the observed data $\\mathbf X_n$ is in general no longer a matrix-variate Gaussian. Thus it is difficult to work with $p(\\mathbf X_n|k)$ directly. Let $\\mathbf {x}_n:=\\text{vec}(\\mathbf X_n)$, now we can work with $p(\\mathbf x_n|k)$ instead of $p(\\mathbf X_n|k)$. Fortunately, the marginal distribution of $\\mathbf x_n$ is a multivariate Gaussian distribution when taking the special matrix-variate Gaussian prior $\\mathbf B_n^{(k)}\\sim\\mathcal N(0,\\mathbf I,\\mathbf I)$. Let $\\mathbf m_k = \\text{vec}(\\mathbf M_k)$\n, we can obtain\n", "itemtype": "equation", "pos": 13216, "prevtext": "\nwhere $p(\\mathbf X_n|k)$ is the $k$-the component, representing the marginal distribution for the observed data $\\mathbf X_n$ over the latent variable. In our case, the marginal distribution of $\\mathbf X_n$ is obtained by integrating out the latent variable $\\mathbf B_n^{(k)}$:\n", "index": 19, "text": "\n\\[\np(\\mathbf X_n|k) = \\int p(\\mathbf X_n|\\mathbf B_n^{(k)})p(\\mathbf B_n^{(k)})d\\mathbf B_n^{(k)}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{X}_{n}|k)=\\int p(\\mathbf{X}_{n}|\\mathbf{B}_{n}^{(k)})p(\\mathbf{B}_{n%&#10;}^{(k)})d\\mathbf{B}_{n}^{(k)}.\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere the observation covariance model is specified by $\\mathbf C_k = (\\mathbf R_k\\mathbf R_k^T)\\otimes(\\mathbf L_k\\mathbf L_k^T)+\\sigma_k^2\\mathbf I$. We refer readers to \\cite{Bishop2006,Timm2002} for more details. Then the denominator in (\\ref{gamma})\nbecomes\n\n", "itemtype": "equation", "pos": 13927, "prevtext": "\n\nDifferent from the vectorial PPCA, we note that the marginal distribution of the observed data $\\mathbf X_n$ is in general no longer a matrix-variate Gaussian. Thus it is difficult to work with $p(\\mathbf X_n|k)$ directly. Let $\\mathbf {x}_n:=\\text{vec}(\\mathbf X_n)$, now we can work with $p(\\mathbf x_n|k)$ instead of $p(\\mathbf X_n|k)$. Fortunately, the marginal distribution of $\\mathbf x_n$ is a multivariate Gaussian distribution when taking the special matrix-variate Gaussian prior $\\mathbf B_n^{(k)}\\sim\\mathcal N(0,\\mathbf I,\\mathbf I)$. Let $\\mathbf m_k = \\text{vec}(\\mathbf M_k)$\n, we can obtain\n", "index": 21, "text": "\n\\[\np(\\mathbf x_n|k) \\sim \\mathcal N(\\mathbf m_k,\\mathbf C_k)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{x}_{n}|k)\\sim\\mathcal{N}(\\mathbf{m}_{k},\\mathbf{C}_{k})\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc26</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\ud835\udc02</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\nAfter getting $\\gamma_{nk}$, we update the estimated mean matrices $\\mathbf M_k$'s and mixing proportions $\\pi_k$'s, respectively, by\n\n", "itemtype": "equation", "pos": 14255, "prevtext": "\nwhere the observation covariance model is specified by $\\mathbf C_k = (\\mathbf R_k\\mathbf R_k^T)\\otimes(\\mathbf L_k\\mathbf L_k^T)+\\sigma_k^2\\mathbf I$. We refer readers to \\cite{Bishop2006,Timm2002} for more details. Then the denominator in (\\ref{gamma})\nbecomes\n\n", "index": 23, "text": "\\begin{align*}\np(\\mathbf x_n) = \\sum_{k=1}^K\\pi_k p(\\mathbf x_n|k)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(\\mathbf{x}_{n})=\\sum_{k=1}^{K}\\pi_{k}p(\\mathbf{x}_{n}|k)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><msub><mi>\u03c0</mi><mi>k</mi></msub><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\n\\subsubsection{Update the Posterior Distribution of $\\mathbf B_n^{(k)}$}\n In computing the posterior distribution of $\\mathbf B_n^{(k)}$, we encounter a difficulty that\n\nthe posteriori distribution of $\\mathbf B_n^{(k)}$ given $\\mathbf X_n$\n", "itemtype": "equation", "pos": 14470, "prevtext": "\n\nAfter getting $\\gamma_{nk}$, we update the estimated mean matrices $\\mathbf M_k$'s and mixing proportions $\\pi_k$'s, respectively, by\n\n", "index": 25, "text": "\\begin{align}\n\\pi_k = \\frac{1}{N}\\sum_{n=1}^N\\gamma_{nk}\\quad \\text{and} \\quad\\mathbf M_k = \\frac{\\sum_{n=1}^N \\gamma_{nk}\\mathbf X_n}{\\sum_{n=1}^N\\gamma_{nk}}\\label{updatePi}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\pi_{k}=\\frac{1}{N}\\sum_{n=1}^{N}\\gamma_{nk}\\quad\\text{and}\\quad%&#10;\\mathbf{M}_{k}=\\frac{\\sum_{n=1}^{N}\\gamma_{nk}\\mathbf{X}_{n}}{\\sum_{n=1}^{N}%&#10;\\gamma_{nk}}\" display=\"inline\"><mrow><mrow><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>and</mtext></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mfrac></mstyle></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nis also in general not a matrix-variate Gaussian. To get a tractable posterior in the variational EM, we restrict the approximated variational distribution to be a matrix-variate Gaussian $\\mathcal{N}(\\mathbf{B}_n^{(k)}\\,|\\,\\mathbf{Q}_n^{(k)}, \\mathbf{T}_n^{(k)},\\mathbf{S}_n^{(k)})$ to approximate the true posterior with the mean $\\mathbf{Q}_n^{(k)}$ in size $r\\times c$ and covariances $\\mathbf{T}_n^{(k)}\\succ 0$ of size $r\\times r$  and $\\mathbf{S}_n^{(k)} \\succ 0$ of size $c\\times c$, respectively. For mixB2DPPCA model, it follows as a natural extension of a single 2DPPCA. So the parameters $\\mathbf Q_n^{(k)}$, $\\mathbf T_n^{(k)}$ and $\\mathbf S_n^{(k)}$ can be estimated through the maximization of a single likelihood function. Particularly, the derived formulas for estimating these parameters are given by, see more details in  \\cite{YuBiYe2008},\n\n", "itemtype": "equation", "pos": 14899, "prevtext": "\n\n\\subsubsection{Update the Posterior Distribution of $\\mathbf B_n^{(k)}$}\n In computing the posterior distribution of $\\mathbf B_n^{(k)}$, we encounter a difficulty that\n\nthe posteriori distribution of $\\mathbf B_n^{(k)}$ given $\\mathbf X_n$\n", "index": 27, "text": "\n\\[\np(\\mathbf B_n^{(k)}|\\mathbf X_n,\\mathbf L_k,\\mathbf R_k,\\sigma^2)\\propto p(\\mathbf X_n|\\mathbf B_n^{(k)}, \\mathbf L_k,\\mathbf R_k,\\sigma^2)p(\\mathbf B_n^{(k)})\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{B}_{n}^{(k)}|\\mathbf{X}_{n},\\mathbf{L}_{k},\\mathbf{R}_{k},\\sigma^{2}%&#10;)\\propto p(\\mathbf{X}_{n}|\\mathbf{B}_{n}^{(k)},\\mathbf{L}_{k},\\mathbf{R}_{k},%&#10;\\sigma^{2})p(\\mathbf{B}_{n}^{(k)})\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 15927, "prevtext": "\nis also in general not a matrix-variate Gaussian. To get a tractable posterior in the variational EM, we restrict the approximated variational distribution to be a matrix-variate Gaussian $\\mathcal{N}(\\mathbf{B}_n^{(k)}\\,|\\,\\mathbf{Q}_n^{(k)}, \\mathbf{T}_n^{(k)},\\mathbf{S}_n^{(k)})$ to approximate the true posterior with the mean $\\mathbf{Q}_n^{(k)}$ in size $r\\times c$ and covariances $\\mathbf{T}_n^{(k)}\\succ 0$ of size $r\\times r$  and $\\mathbf{S}_n^{(k)} \\succ 0$ of size $c\\times c$, respectively. For mixB2DPPCA model, it follows as a natural extension of a single 2DPPCA. So the parameters $\\mathbf Q_n^{(k)}$, $\\mathbf T_n^{(k)}$ and $\\mathbf S_n^{(k)}$ can be estimated through the maximization of a single likelihood function. Particularly, the derived formulas for estimating these parameters are given by, see more details in  \\cite{YuBiYe2008},\n\n", "index": 29, "text": "\\begin{align*}\n\\mathbf T_n^{(k)}=c\\sigma_k^2[\\text{tr}(\\mathbf R^T_k\\mathbf R_k\\mathbf S_n^{(k)})\\mathbf L_k^T\\mathbf L_k+\\sigma_k^2\\text{tr}(\\mathbf S_n^{(k)})\\mathbf I_r]^{-1}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{T}_{n}^{(k)}=c\\sigma_{k}^{2}[\\text{tr}(\\mathbf{R}^{T}_{k}%&#10;\\mathbf{R}_{k}\\mathbf{S}_{n}^{(k)})\\mathbf{L}_{k}^{T}\\mathbf{L}_{k}+\\sigma_{k}%&#10;^{2}\\text{tr}(\\mathbf{S}_{n}^{(k)})\\mathbf{I}_{r}]^{-1}\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mi>c</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>r</mi></msub></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nand each $\\mathbf Q_n^{(k)}$ needs to satisfy\n", "itemtype": "equation", "pos": 16118, "prevtext": "\n\n", "index": 31, "text": "\\begin{align*}\n\\mathbf S_n^{(k)}=r\\sigma_k^2[\\text{tr}(\\mathbf L_k^T\\mathbf L_k\\mathbf T_n^{(k)})\\mathbf R^T_k\\mathbf R_k+\\sigma_k^2\\text{tr}(\\mathbf T_n^{(k)})\\mathbf I_c]^{-1}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{S}_{n}^{(k)}=r\\sigma_{k}^{2}[\\text{tr}(\\mathbf{L}_{k}^{T}%&#10;\\mathbf{L}_{k}\\mathbf{T}_{n}^{(k)})\\mathbf{R}^{T}_{k}\\mathbf{R}_{k}+\\sigma_{k}%&#10;^{2}\\text{tr}(\\mathbf{T}_{n}^{(k)})\\mathbf{I}_{c}]^{-1}\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mi>r</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nTo solve this we need to make a vectorization on both sides and solve a linear equation\n\n", "itemtype": "equation", "pos": 16354, "prevtext": "\nand each $\\mathbf Q_n^{(k)}$ needs to satisfy\n", "index": 33, "text": "\n\\[\n\\mathbf L^T_k\\mathbf L_k\\mathbf Q_n^{(k)}\\mathbf R_k^T\\mathbf R_k+\\sigma_k^2\\mathbf Q_n^{(k)}=\\mathbf L^T_k(\\mathbf X_n-\\mathbf M_k)\\mathbf R_k.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{L}^{T}_{k}\\mathbf{L}_{k}\\mathbf{Q}_{n}^{(k)}\\mathbf{R}_{k}^{T}\\mathbf{%&#10;R}_{k}+\\sigma_{k}^{2}\\mathbf{Q}_{n}^{(k)}=\\mathbf{L}^{T}_{k}(\\mathbf{X}_{n}-%&#10;\\mathbf{M}_{k})\\mathbf{R}_{k}.\" display=\"block\"><mrow><mrow><mrow><mrow><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwith respect to $\\text{vec}(\\mathbf Q_n^{(k)})$, where\n\n", "itemtype": "equation", "pos": 16594, "prevtext": "\nTo solve this we need to make a vectorization on both sides and solve a linear equation\n\n", "index": 35, "text": "\\begin{align}\n(\\mathbf R_k^T\\mathbf R_k\\otimes\\mathbf L^T_k\\mathbf L_k+\\sigma_k\\mathbf I\\otimes \\sigma_k\\mathbf I)\\text{vec}(\\mathbf Q^{(k)}_n)=\\mathbf y^{(k)}_n\\label{updateQ}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{R}_{k}^{T}\\mathbf{R}_{k}\\otimes\\mathbf{L}^{T}_{k}\\mathbf%&#10;{L}_{k}+\\sigma_{k}\\mathbf{I}\\otimes\\sigma_{k}\\mathbf{I})\\text{vec}(\\mathbf{Q}^%&#10;{(k)}_{n})=\\mathbf{y}^{(k)}_{n}\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mrow><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo>\u2297</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup></mrow><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mrow><mrow><msub><mi>\u03c3</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow><mo>\u2297</mo><msub><mi>\u03c3</mi><mi>k</mi></msub></mrow><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msubsup><mi>\ud835\udc32</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nthen reshape $\\text{vec}(\\mathbf Q_n^{(k)})$ back to get $\\mathbf Q_n^{(k)}$.\n\nAs we assume the approximated posterior distribution of $\\mathbf B_n^{(k)}$ is matrix-variate Gaussian, so we can get $\\langle\\mathbf B_n^{(k)}\\rangle = \\mathbf Q_n^{(k)}$  and the following second-order expectations:\n\n", "itemtype": "equation", "pos": 16838, "prevtext": "\nwith respect to $\\text{vec}(\\mathbf Q_n^{(k)})$, where\n\n", "index": 37, "text": "\\begin{align*}\n\\mathbf y^{(k)}_n=\\text{vec}(\\mathbf L^T_k(\\mathbf X_n-\\mathbf M_k)\\mathbf R_k)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{y}^{(k)}_{n}=\\text{vec}(\\mathbf{L}^{T}_{k}(\\mathbf{X}_{n}%&#10;-\\mathbf{M}_{k})\\mathbf{R}_{k})\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc32</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\n\\subsection{\\textbf{Variational M-step}}\nIn the M-step, we fix all the distributions over the hidden variables and gather all the terms containing parameters $\\mathbf L_k$, $\\mathbf R_k$ and $\\sigma_k^2$ in (\\ref{L-fuction}) to\nmaximize them respectively. It turns out that:\n\n", "itemtype": "equation", "pos": 17243, "prevtext": "\nthen reshape $\\text{vec}(\\mathbf Q_n^{(k)})$ back to get $\\mathbf Q_n^{(k)}$.\n\nAs we assume the approximated posterior distribution of $\\mathbf B_n^{(k)}$ is matrix-variate Gaussian, so we can get $\\langle\\mathbf B_n^{(k)}\\rangle = \\mathbf Q_n^{(k)}$  and the following second-order expectations:\n\n", "index": 39, "text": "\\begin{align}\n&\\langle\\mathbf B_n^{(k)T}\\mathbf B_n^{(k)}\\rangle = \\mathbf Q_n^{(k)T}\\mathbf Q_n^{(k)}+\\mathbf S_n^{(k)}\\text{tr}(\\mathbf T_n^{(k)})\\label{updateBB}\\\\\n&\\langle\\mathbf B^{(k)T}_n\\mathbf L_k^T\\mathbf L_k \\mathbf B^{(k)}_n\\rangle=\\mathbf Q^{(k)T}_n\\mathbf L_k^T\\mathbf L_k \\mathbf Q^{(k)}_n+\\mathbf S_n^{(k)}\\text{tr}(\\mathbf T_n^{(k)}\\mathbf L_k^T\\mathbf L_k)\\label{updateBLLB}\n\\\\\n&\\langle\\mathbf B^{(k)}_n\\mathbf R_k^T\\mathbf R_k\\mathbf B^{(k)T}_n\\rangle=\\mathbf Q^{(k)}_n\\mathbf R_k^T\\mathbf R_k\\mathbf Q^{(k)T}_n+\\mathbf T_n^{(k)}\\text{tr}(\\mathbf S_n^{(k)}\\mathbf R_k^T\\mathbf R_k)\\label{updateBRRB}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\mathbf{B}_{n}^{(k)T}\\mathbf{B}_{n}^{(k)}\\rangle=\\mathbf{Q%&#10;}_{n}^{(k)T}\\mathbf{Q}_{n}^{(k)}+\\mathbf{S}_{n}^{(k)}\\text{tr}(\\mathbf{T}_{n}^%&#10;{(k)})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\mathbf{B}^{(k)T}_{n}\\mathbf{L}_{k}^{T}\\mathbf{L}_{k}%&#10;\\mathbf{B}^{(k)}_{n}\\rangle=\\mathbf{Q}^{(k)T}_{n}\\mathbf{L}_{k}^{T}\\mathbf{L}_%&#10;{k}\\mathbf{Q}^{(k)}_{n}+\\mathbf{S}_{n}^{(k)}\\text{tr}(\\mathbf{T}_{n}^{(k)}%&#10;\\mathbf{L}_{k}^{T}\\mathbf{L}_{k})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\mathbf{B}^{(k)}_{n}\\mathbf{R}_{k}^{T}\\mathbf{R}_{k}%&#10;\\mathbf{B}^{(k)T}_{n}\\rangle=\\mathbf{Q}^{(k)}_{n}\\mathbf{R}_{k}^{T}\\mathbf{R}_%&#10;{k}\\mathbf{Q}^{(k)T}_{n}+\\mathbf{T}_{n}^{(k)}\\text{tr}(\\mathbf{S}_{n}^{(k)}%&#10;\\mathbf{R}_{k}^{T}\\mathbf{R}_{k})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc13</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc12</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 18149, "prevtext": "\n\n\\subsection{\\textbf{Variational M-step}}\nIn the M-step, we fix all the distributions over the hidden variables and gather all the terms containing parameters $\\mathbf L_k$, $\\mathbf R_k$ and $\\sigma_k^2$ in (\\ref{L-fuction}) to\nmaximize them respectively. It turns out that:\n\n", "index": 41, "text": "\\begin{align}\n\\mathbf L_k=&[\\sum_{n=1}^N\\gamma_{nk}(\\mathbf X_n-\\mathbf M_k)\\mathbf R_k\\langle\\mathbf B_n^{(k)}\\rangle^T]\\nonumber\\\\\n&\\times[\\sum_{n=1}^N\\gamma_{nk}\\langle \\mathbf B_n^{(k)}\\mathbf R^T_k\\mathbf R_k\\mathbf B_n^{(k)T}\\rangle]^{-1}\\label{updateL}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{L}_{k}=\" display=\"inline\"><mrow><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle[\\sum_{n=1}^{N}\\gamma_{nk}(\\mathbf{X}_{n}-\\mathbf{M}_{k})\\mathbf{%&#10;R}_{k}\\langle\\mathbf{B}_{n}^{(k)}\\rangle^{T}]\" display=\"inline\"><mrow><mo stretchy=\"false\">[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\times[\\sum_{n=1}^{N}\\gamma_{nk}\\langle\\mathbf{B}_{n}^{(k)}%&#10;\\mathbf{R}^{T}_{k}\\mathbf{R}_{k}\\mathbf{B}_{n}^{(k)T}\\rangle]^{-1}\" display=\"inline\"><mrow><mi/><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 18421, "prevtext": "\n\n", "index": 43, "text": "\\begin{align}\n\\mathbf R_k=&[\\sum_{n=1}^N\\gamma_{nk}(\\mathbf X_n-\\mathbf M_k)^T\\mathbf L_k\\langle\\mathbf B_n^{(k)}\\rangle]\\nonumber\\\\\n&\\times[\\sum_{n=1}^N\\gamma_{nk}\\mathbf \\langle \\mathbf B_n^{(k)T}\\mathbf L^T_k\\mathbf L_k\\mathbf B_n^{(k)}\\rangle]^{-1}\\label{updateR}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{R}_{k}=\" display=\"inline\"><mrow><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle[\\sum_{n=1}^{N}\\gamma_{nk}(\\mathbf{X}_{n}-\\mathbf{M}_{k})^{T}%&#10;\\mathbf{L}_{k}\\langle\\mathbf{B}_{n}^{(k)}\\rangle]\" display=\"inline\"><mrow><mo stretchy=\"false\">[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\times[\\sum_{n=1}^{N}\\gamma_{nk}\\mathbf{\\langle}\\mathbf{B}_{n}^{(%&#10;k)T}\\mathbf{L}^{T}_{k}\\mathbf{L}_{k}\\mathbf{B}_{n}^{(k)}\\rangle]^{-1}\" display=\"inline\"><mrow><mi/><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $N_k=\\sum_n\\gamma_{nk}$.\n\nThe overall variational EM algorithm is to alternate between E-step and M-step.\n The final variational EM algorithm is summarized\nin Algorithm \\ref{alg1}.\n\\begin{algorithm}\n\n\\caption{Variational EM algorithm for mixB2DPPCA.}\\label{alg1}\n\\begin{algorithmic}[1]\n  \\REQUIRE Training set $\\mathcal{X} = \\{\\mathbf X_n\\}_{n=1}^N$; Initialize all of model parameters $\\Theta$ and covariance matrices $\\mathbf T_n^{(k)}$ and $\\mathbf S_n^{(k)}$, $n=1,...,N$ and $k=1,...,K$.\n \\FOR {$t=1$ to $T$}\n    \n    \\STATE \\textbf{Variational E-step:}\n    \\begin{itemize}\n             \\item Iterate the mean matrix $\\mathbf Q_n^{(k)}$ based on (\\ref{updateQ}) and update the second-order expectations based on (\\ref{updateBB}), (\\ref{updateBLLB}) and (\\ref{updateBRRB}).\\\\\n           \\end{itemize}\n       \\begin{itemize}\n           \\item  Update each $\\gamma_{nk}$, mixing proportions $\\pi_k$ and mean matrices $\\mathbf M_k$ based on (\\ref{gamma}) and  (\\ref{updatePi}).\n           \\end{itemize}\n\n     \\STATE \\textbf{Variational M-step:}\n            \\begin{itemize}\n           \\item   Maximize objective function  $\\langle\\mathcal L\\rangle$ with respect to each elements $\\mathbf L_k$, $\\mathbf R_k$ and $\\sigma_k^2$ based on (\\ref{updateL}), (\\ref{updateR}) and (\\ref{updateSigma}).\n           \\end{itemize}\n \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\nDefine the average reconstruction error\n\n", "itemtype": "equation", "pos": 18705, "prevtext": "\nand\n\n", "index": 45, "text": "\\begin{align}\n\\sigma^2_k &= \\frac{1}{pqN_k}\\{\\sum_{n=1}^N\\gamma_{nk}\\text{tr}(\\mathbf X_n-\\mathbf M_k)^T\n(\\mathbf X_n-\\mathbf M_k)\\nonumber\\\\\n&-2\\sum_{n=1}^N\\gamma_{nk}\\text{tr}(\\mathbf R_k\\langle\\mathbf B^{(k)}_n\\rangle^T\\mathbf L_k^T(\\mathbf X_n-\\mathbf M_k))\\nonumber\\\\\n&+\\sum_{n=1}^N\\gamma_{nk}\\text{tr}(\\langle\\mathbf B^{(k)T}_n\\mathbf L_k^T\\mathbf L_k\\mathbf B^{(k)}_n\\rangle\\mathbf R_k^T\\mathbf R_k)\\}\\label{updateSigma}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sigma^{2}_{k}\" display=\"inline\"><msubsup><mi>\u03c3</mi><mi>k</mi><mn>2</mn></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{pqN_{k}}\\{\\sum_{n=1}^{N}\\gamma_{nk}\\text{tr}(\\mathbf{X}%&#10;_{n}-\\mathbf{M}_{k})^{T}(\\mathbf{X}_{n}-\\mathbf{M}_{k})\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi><mo>\u2062</mo><msub><mi>N</mi><mi>k</mi></msub></mrow></mfrac></mstyle><mrow><mo stretchy=\"false\">{</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mtext>tr</mtext><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-2\\sum_{n=1}^{N}\\gamma_{nk}\\text{tr}(\\mathbf{R}_{k}\\langle\\mathbf%&#10;{B}^{(k)}_{n}\\rangle^{T}\\mathbf{L}_{k}^{T}(\\mathbf{X}_{n}-\\mathbf{M}_{k}))\" display=\"inline\"><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{n=1}^{N}\\gamma_{nk}\\text{tr}(\\langle\\mathbf{B}^{(k)T}_{n}%&#10;\\mathbf{L}_{k}^{T}\\mathbf{L}_{k}\\mathbf{B}^{(k)}_{n}\\rangle\\mathbf{R}_{k}^{T}%&#10;\\mathbf{R}_{k})\\}\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mtext>tr</mtext><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msubsup><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub><msubsup><mi>\ud835\udc01</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">\u27e9</mo></mrow><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><msub><mi>\ud835\udc11</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $\\widehat{\\mathbf X}_n = \\mathbf L_{k'}\\mathbf B^{(k')}_n \\mathbf R^T_{k'} + \\mathbf M_{k'}$ with $k'=\\text{arg}\\max_k\\{\\gamma_{nk}\\}$ the reconstructed image.\n\nAlgorithm \\ref{alg1} may terminate either a given maximum iterative number $T$ is achieved or the following condition is satisfied,\n\n", "itemtype": "equation", "pos": 20551, "prevtext": "\nwhere $N_k=\\sum_n\\gamma_{nk}$.\n\nThe overall variational EM algorithm is to alternate between E-step and M-step.\n The final variational EM algorithm is summarized\nin Algorithm \\ref{alg1}.\n\\begin{algorithm}\n\n\\caption{Variational EM algorithm for mixB2DPPCA.}\\label{alg1}\n\\begin{algorithmic}[1]\n  \\REQUIRE Training set $\\mathcal{X} = \\{\\mathbf X_n\\}_{n=1}^N$; Initialize all of model parameters $\\Theta$ and covariance matrices $\\mathbf T_n^{(k)}$ and $\\mathbf S_n^{(k)}$, $n=1,...,N$ and $k=1,...,K$.\n \\FOR {$t=1$ to $T$}\n    \n    \\STATE \\textbf{Variational E-step:}\n    \\begin{itemize}\n             \\item Iterate the mean matrix $\\mathbf Q_n^{(k)}$ based on (\\ref{updateQ}) and update the second-order expectations based on (\\ref{updateBB}), (\\ref{updateBLLB}) and (\\ref{updateBRRB}).\\\\\n           \\end{itemize}\n       \\begin{itemize}\n           \\item  Update each $\\gamma_{nk}$, mixing proportions $\\pi_k$ and mean matrices $\\mathbf M_k$ based on (\\ref{gamma}) and  (\\ref{updatePi}).\n           \\end{itemize}\n\n     \\STATE \\textbf{Variational M-step:}\n            \\begin{itemize}\n           \\item   Maximize objective function  $\\langle\\mathcal L\\rangle$ with respect to each elements $\\mathbf L_k$, $\\mathbf R_k$ and $\\sigma_k^2$ based on (\\ref{updateL}), (\\ref{updateR}) and (\\ref{updateSigma}).\n           \\end{itemize}\n \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\nDefine the average reconstruction error\n\n", "index": 47, "text": "\\begin{align}\n\\mathbf e(t) =\\sqrt{\\frac{\\sum_{n=1}^N\\|\\mathbf X_{n}-\\widehat{\\mathbf X}_{n}^{(t)}\\|_F^2}{N}}\\label{comput_error}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{e}(t)=\\sqrt{\\frac{\\sum_{n=1}^{N}\\|\\mathbf{X}_{n}-\\widehat%&#10;{\\mathbf{X}}_{n}^{(t)}\\|_{F}^{2}}{N}}\" display=\"inline\"><mrow><mrow><mi>\ud835\udc1e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo>-</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc17</mi><mo>^</mo></mover><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mi>N</mi></mfrac></mstyle></msqrt></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $\\epsilon$ is a given error tolerance.\n\n\\subsection{The Reduced-Dimensionality Representation for a New Sample}\n\nIn order to obtain the reduced-dimensionality representation for a given sample, we should solve for the latent variable cores. From the probabilistic perspective, the posterior mean $\\mathbf Q_{new}^{(k)}:=\\langle\\mathbf B_{new}^{(k)}|\\mathbf X_{new}\\rangle$ can be seen as the reduced-dimensionality representation, which is a $r\\times c$ feature matrix and given by solving a linear equation\n\n", "itemtype": "equation", "pos": 20991, "prevtext": "\nwhere $\\widehat{\\mathbf X}_n = \\mathbf L_{k'}\\mathbf B^{(k')}_n \\mathbf R^T_{k'} + \\mathbf M_{k'}$ with $k'=\\text{arg}\\max_k\\{\\gamma_{nk}\\}$ the reconstructed image.\n\nAlgorithm \\ref{alg1} may terminate either a given maximum iterative number $T$ is achieved or the following condition is satisfied,\n\n", "index": 49, "text": "\\begin{align}\n|\\mathbf e(t)-\\mathbf e(t+1)|\\leq \\epsilon \\label{epsilon}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|\\mathbf{e}(t)-\\mathbf{e}(t+1)|\\leq\\epsilon\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>\ud835\udc1e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc1e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>\u03f5</mi></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwith respect to $\\text{vec}(\\mathbf Q_{new}^{(k)})$, where\n\n", "itemtype": "equation", "pos": 21590, "prevtext": "\nwhere $\\epsilon$ is a given error tolerance.\n\n\\subsection{The Reduced-Dimensionality Representation for a New Sample}\n\nIn order to obtain the reduced-dimensionality representation for a given sample, we should solve for the latent variable cores. From the probabilistic perspective, the posterior mean $\\mathbf Q_{new}^{(k)}:=\\langle\\mathbf B_{new}^{(k)}|\\mathbf X_{new}\\rangle$ can be seen as the reduced-dimensionality representation, which is a $r\\times c$ feature matrix and given by solving a linear equation\n\n", "index": 51, "text": "\\begin{align*}\n(\\mathbf R_k^T\\mathbf R_k\\otimes\\mathbf L^T_k\\mathbf L_k+\\sigma_k\\mathbf I\\otimes \\sigma_k\\mathbf I)\\text{vec}(\\mathbf Q^{(k)}_{new})=\\mathbf y_{new}^{(k)}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{R}_{k}^{T}\\mathbf{R}_{k}\\otimes\\mathbf{L}^{T}_{k}\\mathbf%&#10;{L}_{k}+\\sigma_{k}\\mathbf{I}\\otimes\\sigma_{k}\\mathbf{I})\\text{vec}(\\mathbf{Q}^%&#10;{(k)}_{new})=\\mathbf{y}_{new}^{(k)}\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mrow><msubsup><mi>\ud835\udc11</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo>\u2297</mo><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup></mrow><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><mrow><mrow><msub><mi>\u03c3</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow><mo>\u2297</mo><msub><mi>\u03c3</mi><mi>k</mi></msub></mrow><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc10</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msubsup><mi>\ud835\udc32</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nthen reshape $\\text{vec}(\\mathbf Q_{new}^{(k)})$ back to get $\\mathbf Q_{new}^{(k)}$. As the same time, we can compute the corresponding $\\gamma_{new,k}$, i.e., the posterior probability of $k$-th component generating the new sample, given by\n", "itemtype": "equation", "pos": 21833, "prevtext": "\nwith respect to $\\text{vec}(\\mathbf Q_{new}^{(k)})$, where\n\n", "index": 53, "text": "\\begin{align*}\n\\mathbf y_{new}^{(k)}=\\text{vec}(\\mathbf L^T_k(\\mathbf X_{new}-\\mathbf M_k)\\mathbf R_k)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{y}_{new}^{(k)}=\\text{vec}(\\mathbf{L}^{T}_{k}(\\mathbf{X}_{%&#10;new}-\\mathbf{M}_{k})\\mathbf{R}_{k})\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc32</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc0b</mi><mi>k</mi><mi>T</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc17</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo>-</mo><msub><mi>\ud835\udc0c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\n\nWe find the largest $\\gamma_{new,k}$ ($k=1,...,K$) from which the most appropriate local 2DPPCA model can be identified for the new sample. That is, a natural choice is to assign the new sample to a cluster with the largest posterior probability.\n\n  \\section{Experimental Results and Analysis}\\label{Sec:IV}\nIn this section, we conduct several experiments on some public databases to assess the proposed mixB2DPPCA model. These experiments are designed to evaluate the performance of the proposed mix2DPPCA in reconstruction and recognition by comparing with existing models and algorithms. \n\nThe relevant PCA algorithms that can be fairly compared against our proposed mixB2DPPCA are GLRAM (Generalized Low Rank Approximations of Matrices) \\cite{Ye2005}, PSOPCA (Probabilistic Second-Order PCA) \\cite{YuBiYe2011}, mixture of PPCA \\cite{TippingBishop1999a} with the code from {\\small\\url{http://www.science.uva.nl/\\~jverbeek}}. \nBecause the zero-noise PSOPCA model and GLRAM have the same stationary point \\cite{YuBiYe2011}, we only compare with GLRAM.\n\\subsection{Data Preparation and Experiment Setting}\\label{sectionIVA}\nAll of the experiments are conducted on the following four public available datasets:\n\\begin{itemize}\n  \\item A subset of handwritten digits images from the MNIST database ({\\small\\url{http://yann.lecun.com/exdb/mnist}}).\n  \\item The Yale face database ({\\small\\url{http://vision.ucsd.edu/content/yale-face-database}}).\n  \\item The AR face database ({\\small\\url{http://rvl1.ecn.purdue.edu/aleix/aleix\\_face\\_DB.html}}).\n    \\item The FERET face database ({\\small\\url{http://www.itl.nist.gov/iad/humanid/feret/feret\\_master.html}}).\n\\end{itemize}\n\nThe subset of handwritten digits images is selected from MNIST database, which contains 1000 digital images with 100 images of each digit. All images are in grayscale and have a uniform size of $28\\times 28$ pixels.\n\nThe Yale face database contains 15 individuals, with 11 images for each individual. The images were captured under different illumination and expression conditions. \nThe images are all $100\\times 100$ pixels with 256 grey levels. In the experiments, we randomly select 6 images of each person as the training samples, and use the remaining images to form the testing sample set. All images are scaled to a resolution of $64\\times 64$ pixels.\n\nThe AR face database contains over 4,000 color images corresponding to 126 subjects. There are variations of facial expressions, illumination conditions, and occlusions (sun glasses and scarf) with each person. Each individual consists of 26 frontal view images taken in two sessions (separated by 2 weeks), where each session has 13 images. Figure \\ref{ARdatabase} shows the 26 images of one subject. In the experiments, we select 30 subjects (15 man and 15 women), and only use the non-occluded 14 images (i.e., the first seven face images of each row in Figure \\ref{ARdatabase}). The first seven of each subject are used for training and the last seven for testing. All images are cropped and resized to $50\\times 40$ pixels.\n\nFERET database includes 1400 images of 200 different subjects, with\n7 images per subject. In the experiments, we select 50 subjects randomly. Five images of each subject are used for training and the remained images are used for testing. All images are cropped and resized to $32\\times 32$ pixels.\n\n\\begin{figure*}\n\\begin{center}\n   {\\includegraphics[width=175mm,height=45mm]{ARDatabase}}\n\\end{center}\n\\caption{Twenty-six face examples of one subject from AR database. The first row is from the first session, and the second row images are from the second session.}\\label{ARdatabase}\n\\end{figure*}\n\nIn experiments, the initial mixing proportions are set to $\\pi_k = 1/K$ and the initial loading matrices $\\mathbf L_k$ and $\\mathbf R_k$ are given randomly. Besides, we choose randomly $K$ samples as mean matrices $\\mathbf M_k$ of the mixture gaussian model and set all $\\sigma_k^2=1$.\n\\subsection{Reconstruction Performance}\nIn this section, we test reconstruction error of the proposed mixB2DPPCA model (\\ref{model}). Applying the proposed model, all digital images can be softly grouped into $K$ clusters,  each of which is modelled by a local B2DPPCA. From all the trained $\\gamma_{nk}$, the most appropriate local B2DPPCA for a given sample can be found. Then we use the most appropriate local B2DPPCA to reconstruct the initial digit image, that is:\n", "itemtype": "equation", "pos": 22191, "prevtext": "\nthen reshape $\\text{vec}(\\mathbf Q_{new}^{(k)})$ back to get $\\mathbf Q_{new}^{(k)}$. As the same time, we can compute the corresponding $\\gamma_{new,k}$, i.e., the posterior probability of $k$-th component generating the new sample, given by\n", "index": 55, "text": "\n\\[\n\\gamma_{new,k} = \\frac{p(\\mathbf X_{new}|k)\\pi_k}{p(\\mathbf X_{new})}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\gamma_{new,k}=\\frac{p(\\mathbf{X}_{new}|k)\\pi_{k}}{p(\\mathbf{X}_{new})}\" display=\"block\"><mrow><msub><mi>\u03b3</mi><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow><mo>,</mo><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c0</mi><mi>k</mi></msub></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $k^{'}$ represents the $k^{'}$-th local B2DPPCA which most appropriate to the sample $\\mathbf X_n$. After obtaining all reconstructed digit images $\\widehat{\\mathbf X}_n$, we can using the equation (\\ref{comput_error}) to compute the average reconstruction error.\n\nNext we compare the reconstruction error of different algorithms on three databases. \nIn all algorithms, we set the iterative number is  $T = 50$ and the reduced dimension is $r=c=4$.\n\\subsubsection{Reconstruction Error on Digit Image Set}\nWe use the given digital image subset in Section \\ref{sectionIVA}  as training set. In this phase, we compare the reconstruction error of the training set.\n\\begin{figure*}\n\\begin{center}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_2}}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_5}}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_10}}\n\\end{center}\n\\caption{Average reconstruction error versus iteration number with the components number $K=2$, $K=5$ and $K=10$ from the left to right.}\\label{err_digit}\n\\end{figure*}\n\nFigure \\ref{err_digit} shows the average reconstruction error of the relevant algorithms. From left to right, the component number is $K=2$, $K=5$ and $K=10$ respectively. Firstly, from these three sub-figures, we can see that the reconstruction error of GLRAM algorithm has no change. This is because GLRAM has no relationship with $K$. Besides, GLRAM works by iteratively computing the leading eigenvectors of the left and right one-sided sample covariance matrices. Thus GLRAM convergent in five steps and the change of reconstruction error is not obvious in the figure. Secondly, fixing the same number of reduced dimension, the performance of our proposed mixB2DPPCA is better than GLRAM. From the view of compression, decoded images from our algorithm have higher quality for the compression ratio of $49:1$. It illustrates that mixB2DPPCA can correctly identify data according to clusters. When $K$ becomes larger, the mixB2DPPCA outperform  the mixture of PPCA in terms of reconstruction errors.\n\\begin{figure}\n\\begin{center}\n   {\\includegraphics[width=60mm,height=60mm]{cons_digit}}\n\\end{center}\n\\caption{Original images and reconstructed images: The first row shows four original digital images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively.}\\label{cons_digit}\n\\end{figure}\n\nThe reconstructed images of different methods are shown in Fig. \\ref{cons_digit} with $K = 10$. The first row shows three original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively. It can be found that the proposed mixB2DPPCA has better reconstruction outcomes, while the results of other two methods show a litter degradation.\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n      \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=4$} &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n     2   & 0.6133 & 0.5760$\\pm$0.0401 & 0.6400$\\pm$0.0267& 0.6237$\\pm$0.0219 &\\textbf{0.6720}$\\pm$\\textbf{0.0210} &0.6519$\\pm$0.0124&0.6693$\\pm$0.0250   \\\\\n\\hline\n      4       & 0.7067&0.6376$\\pm$0.0222  & 0.7173$\\pm$0.0197& 0.6642$\\pm$0.0245 &0.7146$\\pm$0.0180&0.6613$\\pm$0.0201 &\\textbf{0.7200}$\\pm$\\textbf{0.0089}  \\\\\n\\hline\n       6       & 0.7200& 0.6480$\\pm$0.0289& 0.7200$\\pm$0.0154& 0.6506$\\pm$0.0186& 0.7187$\\pm$0.0203&0.6480$\\pm$0.0283&\\textbf{0.7320}$\\pm$\\textbf{0.0160}  \\\\\n \\hline\n       8       &0.7200 & 0.6786$\\pm$0.0117& 0.7240$\\pm$0.0227&0.6560$\\pm$0.0265  &0.7187$\\pm$0.0262 &0.6640$\\pm$0.0233& \\textbf{0.7347}$\\pm$\\textbf{0.0160} \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the Yale database}\\label{Table_Yale}\n\\end{table*}\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n     \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$} &\\multicolumn{2}{c|}{$K=10$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n      4       & 0.5714& 0.5328$\\pm$0.0220 &0.6671$\\pm$0.0333 & 0.5595$\\pm$0.0214 & 0.7000$\\pm$0.0371&0.5752$\\pm$0.0297& \\textbf{0.7244}$\\pm$\\textbf{0.0381}  \\\\\n\\hline\n       6       & 0.6857& 0.6252$\\pm$0.0242&0.7867$\\pm$0.0138 & 0.6343$\\pm$0.0236 & \\textbf{0.8017}$\\pm$\\textbf{0.0291}&0.6613$\\pm$0.0182&0.7576$\\pm$0.0366   \\\\\n \\hline\n       8       &0.7190 & 0.7004$\\pm$0.0190& 0.8116$\\pm$0.0231 & 0.7100$\\pm$0.0246 &0.8211$\\pm$0.0246 &0.7133$\\pm$0.0222& \\textbf{0.8357}$\\pm$\\textbf{0.0237}  \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the AR database}\\label{Table_AR}\n\\end{table*}\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n     \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$} &\\multicolumn{2}{c|}{$K=10$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n      4       & 0.5000& 0.4620$\\pm$0.0315 &0.6070$\\pm$0.0427 & 0.4690$\\pm$0.0470 & \\textbf{0.6210}$\\pm$\\textbf{0.0326}&0.4840$\\pm$0.0316& 0.5900$\\pm$0.0429  \\\\\n\\hline\n       6       & 0.5300& 0.5140$\\pm$0.0206&\\textbf{0.6733}$\\pm$\\textbf{0.0541} & 0.5350$\\pm$0.0283 & 0.6467$\\pm$0.0343&0.5320$\\pm$0.0297&0.6644$\\pm$0.0328   \\\\\n \\hline\n       8       &0.5400 & 0.5440$\\pm$0.0298& 0.6900$\\pm$0.0458 & 0.5610$\\pm$0.0159 &\\textbf{0.6945}$\\pm$\\textbf{0.0526} &0.5580$\\pm$0.0187& 0.6770$\\pm$0.0593  \\\\\n     \\hline\n     10      &0.5500 & 0.5910$\\pm$0.0460& 0.6890$\\pm$0.0455 & 0.5720$\\pm$0.0364 &0.6960$\\pm$0.0599 &0.5970$\\pm$0.0336& \\textbf{0.7100}$\\pm$\\textbf{0.0573}  \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the FERET database}\\label{Table_FERET}\n\\end{table*}\n\\subsubsection{Reconstruction Error on Yale and AR Databases}\nIn this experiment, we compare the reconstruction error on Yale and AR databases.\n\\begin{figure}\n\\begin{center}\n   \\subfloat[]{\\includegraphics[width=41mm,height=36mm]{err_Yale}}\n   \\subfloat[]{\\includegraphics[width=41mm,height=36mm]{err_AR}}\n\\end{center}\n\\caption{Average reconstruction error versus iteration number with the components number $K=5$ on Yale database (a) and AR database (b).}\\label{err_Yale}\n\\end{figure}\nFigure \\ref{err_Yale} shows the average reconstruction error of all the algorithms: (a) on the Yale database and (b) on the AR database. The component number is $K=5$ and the reduced dimensionality is $(r,c) =(4,4)$. It is obvious that the reconstruction error of mixB2DPPCA on testing set has reduced greatly than other algorithms.\n\nFigure \\ref{Cons_Yale} shows some reconstructed images of different algorithms on Yale database. The first row is four original images. The second, third, and fourth rows are the corresponding images reconstructed by mixture of PPCA, GLRAM and mixB2DPPCA. It can be shown that the results of our algorithm have better visual effect than that of GLRAM. Besides we can also see that although the face images reconstructed by mixture of PPCA are relatively clear, they don't match the same original images visually. The reconstructed images on AR database are shown in Figure \\ref{Cons_AR}. The first row shows five original images in the test set and the last three rows are the reconstructed images from three models.\n\\begin{figure}\n\\begin{center}   {\\includegraphics[width=65mm,height=65mm]{Cons_Yale}}\n\\end{center}\n\\caption{Original images (in the Yale database) and reconstructed images: The first row is original images. The second, third and fourth rows are the reconstructed images by mixture PPCA, GLRAM and mixB2DPPCA, repectively.}\\label{Cons_Yale}\n\\end{figure}\n\\begin{figure}\n\\begin{center}\n   {\\includegraphics[width=80mm,height=68mm]{Cons_AR}}\n\\end{center}\n\\caption{Original images (in the AR database) and reconstructed images: The first row is the original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture PPCA and mixB2DPPCA, repectively.}\\label{Cons_AR}\n\\end{figure}\n\nFrom the reconstruction experiments, we can conclude that the mixB2DPPCA generally outperforms global linear 2DPCA algorithms in terms of reconstruction errors. It demonstrates that the classification of training set in advanced is important for the performance of feature extraction.\n\\subsection{Recognition Performance}\nIn this section, we compare the recognition performances of GLRAM, mixture of PPCA and mixB2DPCA on Yale, AR and FERET face databases. These algorithms can be used for extracting features of facial images from the training samples, respectively, and then a nearest neighbor classifier (1-NN) is used to find the most-similar face from the training samples for a querying face. In our experiments, the distance measure between two sets of feature matrices $\\mathbf B_{n_1}$ and $\\mathbf B_{n_2}$, is defined as\n", "itemtype": "equation", "pos": 26684, "prevtext": "\n\nWe find the largest $\\gamma_{new,k}$ ($k=1,...,K$) from which the most appropriate local 2DPPCA model can be identified for the new sample. That is, a natural choice is to assign the new sample to a cluster with the largest posterior probability.\n\n  \\section{Experimental Results and Analysis}\\label{Sec:IV}\nIn this section, we conduct several experiments on some public databases to assess the proposed mixB2DPPCA model. These experiments are designed to evaluate the performance of the proposed mix2DPPCA in reconstruction and recognition by comparing with existing models and algorithms. \n\nThe relevant PCA algorithms that can be fairly compared against our proposed mixB2DPPCA are GLRAM (Generalized Low Rank Approximations of Matrices) \\cite{Ye2005}, PSOPCA (Probabilistic Second-Order PCA) \\cite{YuBiYe2011}, mixture of PPCA \\cite{TippingBishop1999a} with the code from {\\small\\url{http://www.science.uva.nl/\\~jverbeek}}. \nBecause the zero-noise PSOPCA model and GLRAM have the same stationary point \\cite{YuBiYe2011}, we only compare with GLRAM.\n\\subsection{Data Preparation and Experiment Setting}\\label{sectionIVA}\nAll of the experiments are conducted on the following four public available datasets:\n\\begin{itemize}\n  \\item A subset of handwritten digits images from the MNIST database ({\\small\\url{http://yann.lecun.com/exdb/mnist}}).\n  \\item The Yale face database ({\\small\\url{http://vision.ucsd.edu/content/yale-face-database}}).\n  \\item The AR face database ({\\small\\url{http://rvl1.ecn.purdue.edu/aleix/aleix\\_face\\_DB.html}}).\n    \\item The FERET face database ({\\small\\url{http://www.itl.nist.gov/iad/humanid/feret/feret\\_master.html}}).\n\\end{itemize}\n\nThe subset of handwritten digits images is selected from MNIST database, which contains 1000 digital images with 100 images of each digit. All images are in grayscale and have a uniform size of $28\\times 28$ pixels.\n\nThe Yale face database contains 15 individuals, with 11 images for each individual. The images were captured under different illumination and expression conditions. \nThe images are all $100\\times 100$ pixels with 256 grey levels. In the experiments, we randomly select 6 images of each person as the training samples, and use the remaining images to form the testing sample set. All images are scaled to a resolution of $64\\times 64$ pixels.\n\nThe AR face database contains over 4,000 color images corresponding to 126 subjects. There are variations of facial expressions, illumination conditions, and occlusions (sun glasses and scarf) with each person. Each individual consists of 26 frontal view images taken in two sessions (separated by 2 weeks), where each session has 13 images. Figure \\ref{ARdatabase} shows the 26 images of one subject. In the experiments, we select 30 subjects (15 man and 15 women), and only use the non-occluded 14 images (i.e., the first seven face images of each row in Figure \\ref{ARdatabase}). The first seven of each subject are used for training and the last seven for testing. All images are cropped and resized to $50\\times 40$ pixels.\n\nFERET database includes 1400 images of 200 different subjects, with\n7 images per subject. In the experiments, we select 50 subjects randomly. Five images of each subject are used for training and the remained images are used for testing. All images are cropped and resized to $32\\times 32$ pixels.\n\n\\begin{figure*}\n\\begin{center}\n   {\\includegraphics[width=175mm,height=45mm]{ARDatabase}}\n\\end{center}\n\\caption{Twenty-six face examples of one subject from AR database. The first row is from the first session, and the second row images are from the second session.}\\label{ARdatabase}\n\\end{figure*}\n\nIn experiments, the initial mixing proportions are set to $\\pi_k = 1/K$ and the initial loading matrices $\\mathbf L_k$ and $\\mathbf R_k$ are given randomly. Besides, we choose randomly $K$ samples as mean matrices $\\mathbf M_k$ of the mixture gaussian model and set all $\\sigma_k^2=1$.\n\\subsection{Reconstruction Performance}\nIn this section, we test reconstruction error of the proposed mixB2DPPCA model (\\ref{model}). Applying the proposed model, all digital images can be softly grouped into $K$ clusters,  each of which is modelled by a local B2DPPCA. From all the trained $\\gamma_{nk}$, the most appropriate local B2DPPCA for a given sample can be found. Then we use the most appropriate local B2DPPCA to reconstruct the initial digit image, that is:\n", "index": 57, "text": "\n\\[\n\\widehat{\\mathbf X}_n =  \\mathbf L_{k'}*\\mathbf{Q}_n^{(k')}*\\mathbf R_{k'}^T + \\mathbf{M}_{k'}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\widehat{\\mathbf{X}}_{n}=\\mathbf{L}_{k^{\\prime}}*\\mathbf{Q}_{n}^{(k^{\\prime})}%&#10;*\\mathbf{R}_{k^{\\prime}}^{T}+\\mathbf{M}_{k^{\\prime}}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\ud835\udc17</mi><mo>^</mo></mover><mi>n</mi></msub><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc0b</mi><msup><mi>k</mi><mo>\u2032</mo></msup></msub><mo>*</mo><msubsup><mi>\ud835\udc10</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>k</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>*</mo><msubsup><mi>\ud835\udc11</mi><msup><mi>k</mi><mo>\u2032</mo></msup><mi>T</mi></msubsup></mrow><mo>+</mo><msub><mi>\ud835\udc0c</mi><msup><mi>k</mi><mo>\u2032</mo></msup></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01431.tex", "nexttext": "\nwhere $\\mathbf B_{n}=[\\mathbf B_{n}^{(1)},\\mathbf B_{n}^{(2)},...,\\mathbf B_{n}^{(K)}]$ represents the combination of $K$ latent variable cores related with $n$-th sample\\footnote{A more accurate way is to use $\\gamma_{n_1k}\\gamma_{n_2k}$ to weight the individual distance.}. In all algorithms, we set maximum iteration number is 50 and $\\epsilon$ in (\\ref{epsilon}) is 1E-3. We repeat the procedure 10 times, and the mean values and relevant variances are reported in Tables \\ref{Table_Yale} to \\ref{Table_FERET}.\n\nTable \\ref{Table_Yale} shows the recognition rates of three feature extraction algorithms: GLRAM, mixture of PPCA and mixB2DPPCA training on Yale database. The mean values and relevant variances are reported for the cases of the reduced dimension $(r,c)=(2,2)$, $(4,4)$, $(6,6)$ and $(8,8)$. For the mixture of PPCA and mixB2DPPCA, we also computed the recognition rates for the different component number $K$ ($K=4, 6, 8$), shown in Table \\ref{Table_Yale}. Firstly, from the table we can see that the recognition rates of the mixture of PPCA and mixB2DPPCA have a little fluctuation compared with GLRAM. This may be caused by the uncertainty of probability. Secondly, compared with GLRAM, the mean recognition rates of mixB2DPPCA algorithm have obviously improved. The bold figures are the best results in the comparison.\n\nTable \\ref{Table_AR} shows the recognition rates of the above three algorithms training on AR database. The reduced dimensions are $(r,c)=(4,4)$, $(6,6)$ and $(8,8)$ and component numbers are $K=6, 8, 10$, respectively. From the table we can see that the mean recognition rates of mixB2DPPCA algorithm have better improvement over the other two algorithms. \n\nTable \\ref{Table_FERET} shows the recognition rates on FERET database. The reduced dimensions are $(r,c)=(4,4)$, $(6,6)$, $(8,8)$ and $(10,10)$, and the component numbers are $K=6, 8, 10$,  respectively. In this case, both the mixture of PPCA and the proposed mixB2DPPCA produce slightly larger variances, however the mean recognition rates have risen greatly. GLRAM is relatively more robust.\n\n\\section{Conclusions}\\label{Sec:V}\nIn this paper, we proposed a mixture of bilateral-projection probabilistic PCA model for feature extraction and dimensionality reduction for 2D data. Different from the standard PCA which is a global dimension reduction model, this model employs the mixture of matrix-variate Gaussian to model local linear sub-models. All the parameters in the resulting probabilistic model can be estimated through the maximization of the likelihood function. The new model not only makes good use of spatial (structural) information of 2D data but also can softly group data into a given number of clusters. The performance of feature extraction of the proposed method generally outperforms other existing 2D  algorithms in terms of reconstruction error and recognition rate. The approach used in this paper can be readily extended to higher order tensorial data and other non-Gaussian noise models can also be integrated into the model such.\n\n\n\n{\\small\n\\begin{thebibliography}{1}\n\n\\bibitem{Archambeau2005}\nC. Archambeau. ``Probabilistic models in noisy environments\nand their application to a visual prosthesis for the blind''. In \\emph{Unpublished doctoral dissertation, Universit\u00a8\u00a6 Catholique de\nLouvain, Belgium}, 2005.\n\n\\bibitem{ArchambeauDelannayVerleysen2006}\nC. Archambeau, N. Delannay, and M. Verleysen. ``Robust probabilistic projections''. In \\emph{International Conference on Machine Learning}, pages 33--40, 2006.\n\n\\bibitem{Bishop2006}\n C. M. Bishop. \\emph{Pattern Recognition and Machine Learning}. Information Science and Statistics. Springer, 2006.\n\n\\bibitem{BishopTipping1998}\nC. M. Bishop and M. E. Tipping. ``A hierarchical latent variable model for data visualization''. \\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 20(3):281--293, 1998.\n\n\\bibitem{Gao2007}\nJ. Gao. ``Robust L1 principal component analysis and its Bayesian variational inference''. \\emph{Neural Computation}, 20(2):555--572, 2007.\n\n\\bibitem{GaoXu2007}\nJ. Gao and R. Xu. ``Mixture of the robust L1 distributions and its applications''. In \\emph{Lecture Notes in Artificial Intelligence}, volume 4830, pages 26--35, 2007.\n\n\\bibitem{GhahramaniHinton1996}\n Z. Ghahramani and G. E. Hinton. ``The EM algorithm for mixtures of factor analyzers''. In \\emph{Technical Report CRG-TR-96-1, University of Toronto}, 1996.\n\n\\bibitem{HeYanHuNiyogiZhang2005}\nX. He, S. Yan, Y. Hu, P. Niyogi, and H. Zhang. ``Face recognition using Laplacianfaces''. \\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 27(3):328--340, 2005.\n\n\\bibitem{HoyerHyvarinen2000}\nP. Hoyer and A. Hyv$\\ddot{\\text{a}}$rinen. ``Independent componentanalysis applied to feature extraction from colour and stereo images''. \\emph{Network: Computation in Neural Systems}, 11(3):191--210, 2000.\n\n\\bibitem{JuSunGaoHuYin2015}\nF. Ju, S. Sun, J. Gao, Y. Hu, and B. Yin. ``Image outlier detection and feature extraction via L1-norm based 2D probabilistic PCA''. \\emph{IEEE Transactions on Image Processing}, 24(12):4834--4846, 2015.\n\n\\bibitem{KeSukthankar2004}\nY. Ke and R. Sukthankar. ``PCA-SIFT: A more distinctive representation for local image descriptors''. In \\emph{Computer Vision and Pattern Recognition}, pages 506--513, 2004.\n\n\\bibitem{KimKimBang2003}\nH. C. Kim, D. Kim, and S. Y. Bang. ``An efficient model order selection for PCA mixture model''. \\emph{Pattern Recognition Letters}, 24(9):1385--1393, 2003.\n\n\\bibitem{KriegelKrogerSchubert2008}\nH. Kriegel, P. Kr$\\ddot{\\text{o}}$ger, and E. Schubert. ``A general framework for increasing the robustness of PCA-based correlation clustering algorithms''. In \\emph{Scientific and Statistical Database Management. Springer Berlin Heidelberg}, pages 418--435, 2008.\n\n\\bibitem{LuPlataniotisVenetsanopoulosLi2006}\nJ. Lu, K. Plataniotis, A. Venetsanopoulos, and S. Li. ``Ensemble-based discriminant learning with boosting for face recognition''. \\emph{IEEE Transactions on Neural Network}, 17(1):166--178, 2006.\n\n\\bibitem{PeelMcLachlan2000}\nD. Peel and G. McLachlan. ``Robust mixture modelling using the t distribution''. \\emph{Statistics and Computing}, 10:339\u00a8C348, 2000.\n\n\\bibitem{RidderFranc2003}\nD. Ridder and V. Franc. ``Robust subspace mixture models using t-distributions''. In \\emph{Proceedings of the 14th British Machine Vision Conference}, pages 1--10, 2003.\n\n\\bibitem{SuDy2004}\nT. Su and J. G. Dy. ``Automated hierarchical mixtures of probabilistic principal component analyzers''. In \\emph{International Conference on Machine learning, ACM}, 2004.\n\n\\bibitem{Timm2002}\nN. H. Timm. \\emph{Applied Multivariate Analysis}. Springer, 2002.\n\n\\bibitem{TippingBishop1999}\nM. Tipping and C. Bishop. ``Probabilistic principal component analysis''. \\emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, 61(3):611--622, 1999.\n\n\\bibitem{TippingBishop1999a}\nM. E. Tipping and C. M. Bishop. ``Mixtures of probabilistic principal component analyzers''. \\emph{Neural computation}, 11(2):443--482, 1999.\n\n\\bibitem{WangChenHuLuo2008}\nH. Wang, S. Chen, Z. Hu, and B. Luo. ``Probabilistic two dimensional principal component analysis and its mixture model for face recognition''. \\emph{Neural Computing and Applications}, 17(5-6):541--547, 2008.\n\n\\bibitem{WangWang2013}\nH. Wang and J. Wang. ``2DPCA with L1-norm for simultaneously robust and sparse modelling''. \\emph{Neural Network}, 46(10):190--198, 2013.\n\n\\bibitem{XieYanKwokHuang2008}\nX. Xie, S. Yan, J. Kwok, and T. Huang. ``Matrix-variate factor analysis and its applications''. \\emph{IEEE Transactions on Neural Networks}, 19(10):1821--1826, 2008.\n\n\\bibitem{YangZhangFrangiYang2004}\nJ. Yang, D. Zhang, A. F. Frangi, and J. Yang. ``Two dimensional PCA: A new approach to appearance-based face representation and recognition''. \\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 26(1):131--137, 2004.\n\n\\bibitem{Ye2005}\nJ. Ye. ``Generalized low rank approximations of matrices''. \\emph{Machine Learning}, 61(1-3):167--191, 2005.\n\n\\bibitem{YuBiYe2008}\nS. Yu, J. Bi, and J. Ye. ``Matrix-variate factor analysis and its applications''. In \\emph{KDD Workshop Data Mining Using Matrix and Tensors}, pages 1--7, 2008.\n\n\\bibitem{YuBiYe2011}\nS. Yu, J. Bi, and J. Ye. ``Matrix-variate and higher-order probabilistic projections''. \\emph{Data Mining and Knowledge Discovery}, 22(3):372--392, 2011.\n\n\\end{thebibliography}\n}\n\n\n", "itemtype": "equation", "pos": 36001, "prevtext": "\nwhere $k^{'}$ represents the $k^{'}$-th local B2DPPCA which most appropriate to the sample $\\mathbf X_n$. After obtaining all reconstructed digit images $\\widehat{\\mathbf X}_n$, we can using the equation (\\ref{comput_error}) to compute the average reconstruction error.\n\nNext we compare the reconstruction error of different algorithms on three databases. \nIn all algorithms, we set the iterative number is  $T = 50$ and the reduced dimension is $r=c=4$.\n\\subsubsection{Reconstruction Error on Digit Image Set}\nWe use the given digital image subset in Section \\ref{sectionIVA}  as training set. In this phase, we compare the reconstruction error of the training set.\n\\begin{figure*}\n\\begin{center}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_2}}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_5}}\n   {\\includegraphics[width=50mm,height=40mm]{err_digit_10}}\n\\end{center}\n\\caption{Average reconstruction error versus iteration number with the components number $K=2$, $K=5$ and $K=10$ from the left to right.}\\label{err_digit}\n\\end{figure*}\n\nFigure \\ref{err_digit} shows the average reconstruction error of the relevant algorithms. From left to right, the component number is $K=2$, $K=5$ and $K=10$ respectively. Firstly, from these three sub-figures, we can see that the reconstruction error of GLRAM algorithm has no change. This is because GLRAM has no relationship with $K$. Besides, GLRAM works by iteratively computing the leading eigenvectors of the left and right one-sided sample covariance matrices. Thus GLRAM convergent in five steps and the change of reconstruction error is not obvious in the figure. Secondly, fixing the same number of reduced dimension, the performance of our proposed mixB2DPPCA is better than GLRAM. From the view of compression, decoded images from our algorithm have higher quality for the compression ratio of $49:1$. It illustrates that mixB2DPPCA can correctly identify data according to clusters. When $K$ becomes larger, the mixB2DPPCA outperform  the mixture of PPCA in terms of reconstruction errors.\n\\begin{figure}\n\\begin{center}\n   {\\includegraphics[width=60mm,height=60mm]{cons_digit}}\n\\end{center}\n\\caption{Original images and reconstructed images: The first row shows four original digital images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively.}\\label{cons_digit}\n\\end{figure}\n\nThe reconstructed images of different methods are shown in Fig. \\ref{cons_digit} with $K = 10$. The first row shows three original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture of PPCA and mixB2DPPCA, respectively. It can be found that the proposed mixB2DPPCA has better reconstruction outcomes, while the results of other two methods show a litter degradation.\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n      \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=4$} &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n     2   & 0.6133 & 0.5760$\\pm$0.0401 & 0.6400$\\pm$0.0267& 0.6237$\\pm$0.0219 &\\textbf{0.6720}$\\pm$\\textbf{0.0210} &0.6519$\\pm$0.0124&0.6693$\\pm$0.0250   \\\\\n\\hline\n      4       & 0.7067&0.6376$\\pm$0.0222  & 0.7173$\\pm$0.0197& 0.6642$\\pm$0.0245 &0.7146$\\pm$0.0180&0.6613$\\pm$0.0201 &\\textbf{0.7200}$\\pm$\\textbf{0.0089}  \\\\\n\\hline\n       6       & 0.7200& 0.6480$\\pm$0.0289& 0.7200$\\pm$0.0154& 0.6506$\\pm$0.0186& 0.7187$\\pm$0.0203&0.6480$\\pm$0.0283&\\textbf{0.7320}$\\pm$\\textbf{0.0160}  \\\\\n \\hline\n       8       &0.7200 & 0.6786$\\pm$0.0117& 0.7240$\\pm$0.0227&0.6560$\\pm$0.0265  &0.7187$\\pm$0.0262 &0.6640$\\pm$0.0233& \\textbf{0.7347}$\\pm$\\textbf{0.0160} \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the Yale database}\\label{Table_Yale}\n\\end{table*}\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n     \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$} &\\multicolumn{2}{c|}{$K=10$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n      4       & 0.5714& 0.5328$\\pm$0.0220 &0.6671$\\pm$0.0333 & 0.5595$\\pm$0.0214 & 0.7000$\\pm$0.0371&0.5752$\\pm$0.0297& \\textbf{0.7244}$\\pm$\\textbf{0.0381}  \\\\\n\\hline\n       6       & 0.6857& 0.6252$\\pm$0.0242&0.7867$\\pm$0.0138 & 0.6343$\\pm$0.0236 & \\textbf{0.8017}$\\pm$\\textbf{0.0291}&0.6613$\\pm$0.0182&0.7576$\\pm$0.0366   \\\\\n \\hline\n       8       &0.7190 & 0.7004$\\pm$0.0190& 0.8116$\\pm$0.0231 & 0.7100$\\pm$0.0246 &0.8211$\\pm$0.0246 &0.7133$\\pm$0.0222& \\textbf{0.8357}$\\pm$\\textbf{0.0237}  \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the AR database}\\label{Table_AR}\n\\end{table*}\n\\begin{table*}\n  \\centering\n  \\footnotesize\n   \\begin{tabular}{|c|c|r|c|r|c|r|c|}\n      \\hline\n     \\multirow{2}*{$r,c$}  &\\multirow{2}*{GLRAM}   &\\multicolumn{2}{c|}{$K=6$} &\\multicolumn{2}{c|}{$K=8$} &\\multicolumn{2}{c|}{$K=10$}  \\\\\n \\cline{3-8}\n     & &  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA&  mixPPCA& mixB2DPPCA \\\\\n     \\hline\n      4       & 0.5000& 0.4620$\\pm$0.0315 &0.6070$\\pm$0.0427 & 0.4690$\\pm$0.0470 & \\textbf{0.6210}$\\pm$\\textbf{0.0326}&0.4840$\\pm$0.0316& 0.5900$\\pm$0.0429  \\\\\n\\hline\n       6       & 0.5300& 0.5140$\\pm$0.0206&\\textbf{0.6733}$\\pm$\\textbf{0.0541} & 0.5350$\\pm$0.0283 & 0.6467$\\pm$0.0343&0.5320$\\pm$0.0297&0.6644$\\pm$0.0328   \\\\\n \\hline\n       8       &0.5400 & 0.5440$\\pm$0.0298& 0.6900$\\pm$0.0458 & 0.5610$\\pm$0.0159 &\\textbf{0.6945}$\\pm$\\textbf{0.0526} &0.5580$\\pm$0.0187& 0.6770$\\pm$0.0593  \\\\\n     \\hline\n     10      &0.5500 & 0.5910$\\pm$0.0460& 0.6890$\\pm$0.0455 & 0.5720$\\pm$0.0364 &0.6960$\\pm$0.0599 &0.5970$\\pm$0.0336& \\textbf{0.7100}$\\pm$\\textbf{0.0573}  \\\\\n     \\hline\n   \\end{tabular}\n  \\caption{Recognition accuracy of GLRAM, mixture of PPCA and mixB2DPPCA training on the FERET database}\\label{Table_FERET}\n\\end{table*}\n\\subsubsection{Reconstruction Error on Yale and AR Databases}\nIn this experiment, we compare the reconstruction error on Yale and AR databases.\n\\begin{figure}\n\\begin{center}\n   \\subfloat[]{\\includegraphics[width=41mm,height=36mm]{err_Yale}}\n   \\subfloat[]{\\includegraphics[width=41mm,height=36mm]{err_AR}}\n\\end{center}\n\\caption{Average reconstruction error versus iteration number with the components number $K=5$ on Yale database (a) and AR database (b).}\\label{err_Yale}\n\\end{figure}\nFigure \\ref{err_Yale} shows the average reconstruction error of all the algorithms: (a) on the Yale database and (b) on the AR database. The component number is $K=5$ and the reduced dimensionality is $(r,c) =(4,4)$. It is obvious that the reconstruction error of mixB2DPPCA on testing set has reduced greatly than other algorithms.\n\nFigure \\ref{Cons_Yale} shows some reconstructed images of different algorithms on Yale database. The first row is four original images. The second, third, and fourth rows are the corresponding images reconstructed by mixture of PPCA, GLRAM and mixB2DPPCA. It can be shown that the results of our algorithm have better visual effect than that of GLRAM. Besides we can also see that although the face images reconstructed by mixture of PPCA are relatively clear, they don't match the same original images visually. The reconstructed images on AR database are shown in Figure \\ref{Cons_AR}. The first row shows five original images in the test set and the last three rows are the reconstructed images from three models.\n\\begin{figure}\n\\begin{center}   {\\includegraphics[width=65mm,height=65mm]{Cons_Yale}}\n\\end{center}\n\\caption{Original images (in the Yale database) and reconstructed images: The first row is original images. The second, third and fourth rows are the reconstructed images by mixture PPCA, GLRAM and mixB2DPPCA, repectively.}\\label{Cons_Yale}\n\\end{figure}\n\\begin{figure}\n\\begin{center}\n   {\\includegraphics[width=80mm,height=68mm]{Cons_AR}}\n\\end{center}\n\\caption{Original images (in the AR database) and reconstructed images: The first row is the original images. The second, third and fourth rows are the reconstructed images by GLRAM, mixture PPCA and mixB2DPPCA, repectively.}\\label{Cons_AR}\n\\end{figure}\n\nFrom the reconstruction experiments, we can conclude that the mixB2DPPCA generally outperforms global linear 2DPCA algorithms in terms of reconstruction errors. It demonstrates that the classification of training set in advanced is important for the performance of feature extraction.\n\\subsection{Recognition Performance}\nIn this section, we compare the recognition performances of GLRAM, mixture of PPCA and mixB2DPCA on Yale, AR and FERET face databases. These algorithms can be used for extracting features of facial images from the training samples, respectively, and then a nearest neighbor classifier (1-NN) is used to find the most-similar face from the training samples for a querying face. In our experiments, the distance measure between two sets of feature matrices $\\mathbf B_{n_1}$ and $\\mathbf B_{n_2}$, is defined as\n", "index": 59, "text": "\n\\[\n\\text{dist} = \\sum^K_{k=1} \\|\\mathbf B^{(k)}_{n_1}-\\mathbf B^{(k)}_{n_2}\\|_F.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\text{dist}=\\sum^{K}_{k=1}\\|\\mathbf{B}^{(k)}_{n_{1}}-\\mathbf{B}^{(k)}_{n_{2}}%&#10;\\|_{F}.\" display=\"block\"><mrow><mrow><mtext>dist</mtext><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>\ud835\udc01</mi><msub><mi>n</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><msubsup><mi>\ud835\udc01</mi><msub><mi>n</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]