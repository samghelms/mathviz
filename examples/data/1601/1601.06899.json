[{"file": "1601.06899.tex", "nexttext": "\nwhere the collection of non-zero elements' positions in ${\\bf x}$, ${\\rm supp}({\\bf x})$, is defined as $\\mathcal{T}=\\{i\\mid  {\\bf x}_i\\neq 0,  i\\in [1:n]\\}$, with cardinality $|\\mathcal{T}|=k$. Unfortunately, computational complexity for solving this problem is NP-hard, implying that, in practice, it is computationally infeasible to obtain the optimal solution when $n$ is very large. There exists many practical algorithms that perfectly  reconstruct the sparse signal with polynomial time computational complexity, provided that a measurement matrix has a good incoherence property. Greedy sparse signal recovery algorithms \\cite{TroppGilbert2007,Needell2010_CoSaMP,Dai2009_SP,Namyoon2014} became popular due to the computational efficiency in implementing these algorithms.\n\nIn practice, obtaining the measurement vector ${\\bf y}$ with infinite precision is infeasible. This is because, in many image sensors or communication systems, signal acquisition is performed by using analog-to-digital converters (ADCs) that quantizes each measurement to a predefined value with a finite number of bits. This quantization process makes difficulty in recovering sparse signals, as it might give rise to significant measurement errors, especially when the number of quantization bits is small. Numerous sparse signal recovery algorithms with quantized measurements in \\cite{Sun_Goyal_2009,Dai_2009,BoufounosBaraniuk2008,BIHT2013,BP_onebit,Haupt_Baraniuk_2011} were proposed to overcome the impact of the quantization errors. In particular, under the premise that each measurement is quantized with just \\textit{one-bit} (i.e., an extreme case of quantization errors), a compressive sensing problem was introduced in \\cite{BoufounosBaraniuk2008}. For given ${\\bf \\Phi}$ and ${\\bf x}$, the measurements are obtained using their signs as\n\n", "itemtype": "equation", "pos": 2534, "prevtext": "\n\n\\sloppy\n\n\n\\title{Coded Compressive Sensing:\\\\A Compute-and-Recover Approach}\n\n\n\n\\author{\n  \\IEEEauthorblockN{Namyoon Lee and Song-Nam Hong\\\\}\n\\thanks{N. Lee  is with Intel Labs,\n2200 Mission College Blvd, Santa Clara, CA 95054, USA (e-mail:namyoon.lee@gmail.com). S.-N. Hong  is with Ericsson Silicon Valley Research Lab. 300 Holger Way,\nSan Jose, CA 95134 (e-mail:sunny7955@gmail.com) }\n}\n\n\n\n\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we propose \\textit{coded compressive sensing} that recovers an $n$-dimensional integer sparse signal vector from a noisy and quantized measurement vector whose dimension $m$ is far-fewer than $n$. The core idea of coded compressive sensing is to construct a linear sensing matrix whose columns consist of lattice codes. We present a two-stage decoding method named \\textit{compute-and-recover} to detect the sparse signal from the noisy and quantized measurements. In the first stage, we transform such measurements into noiseless finite-field measurements using the linearity of lattice codewords. In the second stage, syndrome decoding is applied over the finite-field to reconstruct the sparse signal vector. A sufficient condition of a perfect recovery is derived. Our theoretical result demonstrates an interplay among the quantization level $p$, the sparsity level $k$, the signal dimension $n$, and the number of measurements $m$ for the perfect recovery. Considering 1-bit compressive sensing as a special case, we show that the proposed algorithm empirically outperforms an existing greedy recovery algorithm.\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\\vspace{-0.4cm}\n\\section{Introduction}\nCompressive sensing (CS)\\cite{ CandesRombergTao2006,Donoho} is a promising technique that recovers a high-dimensional signal represented by a few non-zero elements using far-fewer measurements than the signal dimension. This technique has immense applications ranging from image compression to sensing systems requiring lower power consumption. The mathematical heart of CS is to solve a under-determined linear system of equations by harnessing an inherent sparse structure in the signal.\n\nLet ${\\bf x}\\in\\mathbb{R}^n$ and ${\\bf \\Phi}\\in\\mathbb{R}^{m\\times n}$ be a real-valued spare signal vector and a compressive sensing matrix that linearly projects a high-dimensional signal in $\\mathbb{R}^n$ to a low-dimensional signal in $\\mathbb{R}^m$ where $m<n$, respectively. Formally, the noiseless CS problem is to reconstruct sparse signal vector ${\\bf x}$ by solving the following $\\ell_0$-minimization problem:\n\n", "index": 1, "text": "\\begin{align}\n\\min \\|{\\bf x}\\|_0~~ {\\rm subject}~{\\rm to}~~ {\\bf y}={\\bf \\Phi}{\\bf x}, \\label{eq:L0_CS}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\|{\\bf x}\\|_{0}~{}~{}{\\rm subject}~{}{\\rm to}~{}~{}{\\bf y}={%&#10;\\bf\\Phi}{\\bf x},\" display=\"inline\"><mrow><mrow><mrow><mi>min</mi><mo>\u2062</mo><mpadded width=\"+6.6pt\"><msub><mrow><mo>\u2225</mo><mi>\ud835\udc31</mi><mo>\u2225</mo></mrow><mn>0</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>to</mi></mpadded><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow><mo>=</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere the measurement vector is in the Boolean cube, i.e., ${\\bf y}\\in \\{-1,1\\}^m$. It was shown that, with the one-bit measurements, sparse signal vectors with unit-norm can be recovered with high probability by convex optimization techniques \\cite{BP_onebit} or iterative greedy algorithms \\cite{BIHT2013}.\n\n\nIn this paper, we study a generalized compressive sensing problem in which each measurement is quantized with $p$ levels where $p$ ($\\geq 2$) is a prime number. We also consider a quantized source signal, i.e., the non-zero elements of a sparse signal are chosen from a set of integer values, i.e., ${\\bf x}\\in \\mathbb{Z}^n_p$. Such setting can be found in  many applications. For instance, in a random access wireless system, active users among all users in the system send quadrature amplitude modulated symbols (i.e., $p$-level quantized signals) to a receiver, and it detects the active users' signals using a $p$-level ADC.\n\n\n\nA fundamental question we ask in this paper is: what is the sufficient condition for the perfect recovery of the integer sparse signal with $p$-level per measurement in the presence of Gaussian noise? To shed light on the answer to this question, we develop a new sparse signal recovery framework, which is referred to as ``\\textit{coded compressive sensing}.'' The core idea of coded compressive sensing is to exploit both source and channel coding techniques in information theory. The proposed scheme consists of two cascade encoding and decoding phases. The first phase of encoding is the \\textit{compression phase}, in which a high dimensional sparse signal vector in ${\\mathbb Z}_p^n$ is compressed to a low dimensional signal vector using a parity check matrix of a maximum distance separable (MDS) linear code. The second phase is the \\textit{dictionary coding phase}. In this phase, each dictionary vector (each column vector of the parity check matrix) is encoded to a coded dictionary vector by exploiting a (near) capacity-achieving lattice code for a Gaussian channel. We propose  a two-stage decoding method called ``\\textit{compute-and-recover}.'' In the first stage of decoding, a linear combination of the encoded dictionary vectors corresponding the non-zero elements in ${\\bf x}\\in\\mathbb{Z}^n_p$ is decoded. We call this as the dictionary equation decoding stage that produces a noise-free measurement vector. Once the dictionary equation is perfectly decoded, in the second stage of decoding, we apply syndrome decoding to the equivalent finite field representation of the dictionary equation for the sparse signal recovery. Using the proposed scheme, we derive a lower bound of the number of measurements for the perfect recovery as a function of important system parameters: the quantization level $p$, the sparsity level $k$, the signal dimension $n$, and the number of measurements $m$. Considering $p=2$ as a special case, we compare the proposed scheme with existing algorithms developed for the one-bit compressive sensing problem \\cite{BoufounosBaraniuk2008}. Numerical results show that the proposed scheme outperforms than binary iterative hard thresholding (BITH) \\cite{BIHT2013} in a low signal-to-noise ratio (SNR) regime.\n\n\\vspace{-0.5cm}\n\\section{Coded Compressive Sensing Problem}\nIn this section, we present a coded compressive sensing framework for an integer sparse signal recovery in the presence Gaussian noise.\n\n\n\n\n\\vspace{-0.4cm}\n\\subsection{Signal Model}\nWe are interested in a sparse signal detection problem from a compressed measurement in the presence of noise. Let ${\\bf x}=[x_1,x_2,\\ldots,x_n]^{\\top} \\in \\mathbb{Z}_p^{n\\times 1}$ be an unknown sparse signal vector whose sparsity level is equal to $k$, i.e., $\\|{\\bf x}\\|_0=k \\ll n$. The measurement equation of quantized compressed sensing is given by\n\n", "itemtype": "equation", "pos": 4481, "prevtext": "\nwhere the collection of non-zero elements' positions in ${\\bf x}$, ${\\rm supp}({\\bf x})$, is defined as $\\mathcal{T}=\\{i\\mid  {\\bf x}_i\\neq 0,  i\\in [1:n]\\}$, with cardinality $|\\mathcal{T}|=k$. Unfortunately, computational complexity for solving this problem is NP-hard, implying that, in practice, it is computationally infeasible to obtain the optimal solution when $n$ is very large. There exists many practical algorithms that perfectly  reconstruct the sparse signal with polynomial time computational complexity, provided that a measurement matrix has a good incoherence property. Greedy sparse signal recovery algorithms \\cite{TroppGilbert2007,Needell2010_CoSaMP,Dai2009_SP,Namyoon2014} became popular due to the computational efficiency in implementing these algorithms.\n\nIn practice, obtaining the measurement vector ${\\bf y}$ with infinite precision is infeasible. This is because, in many image sensors or communication systems, signal acquisition is performed by using analog-to-digital converters (ADCs) that quantizes each measurement to a predefined value with a finite number of bits. This quantization process makes difficulty in recovering sparse signals, as it might give rise to significant measurement errors, especially when the number of quantization bits is small. Numerous sparse signal recovery algorithms with quantized measurements in \\cite{Sun_Goyal_2009,Dai_2009,BoufounosBaraniuk2008,BIHT2013,BP_onebit,Haupt_Baraniuk_2011} were proposed to overcome the impact of the quantization errors. In particular, under the premise that each measurement is quantized with just \\textit{one-bit} (i.e., an extreme case of quantization errors), a compressive sensing problem was introduced in \\cite{BoufounosBaraniuk2008}. For given ${\\bf \\Phi}$ and ${\\bf x}$, the measurements are obtained using their signs as\n\n", "index": 3, "text": "\\begin{align}\n{\\bf y} ={\\rm sign}\\left({\\bf \\Phi}{\\bf x}\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf y}={\\rm sign}\\left({\\bf\\Phi}{\\bf x}\\right),\" display=\"inline\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere $S_{p}$ denotes the $p$-level {\\em scalar} quantizer that applied component-wise, and ${\\bf y}\\in \\mathbb{R}^{m\\times 1}=[y_1,\\ldots,y_m]^{\\top}$ and ${\\bf n}\\in \\mathbb{R}^{m\\times 1}=[n_1,\\ldots,n_m]^{\\top}$ denote the measurement and noise vector, respectively. All entries of the noise vector are assumed to be independent and identically distributed (IID) Gaussian random variables with zero mean and variance $\\sigma^2/m$, i.e., $n_{i} \\sim \\mathcal{N}\\left(0,\\frac{\\sigma^2}{m}\\right)$ for all $i$.\n\nOur objective is to reliably estimate the unknown sparse signal vector ${\\bf x}$ given ${\\bf y}$ in the presence of Gaussian noise ${\\bf n}$, by appropriately constructing a linear measurement matrix ${\\bf \\Phi}$ and $p$-level scalar quantizer $S_{p}$. We define a sparse signal recovery decoder $\\mathcal{D}:\\mathbb{R}^m \\rightarrow \\mathbb{Z}_p^n$, which maps the measurement vector ${\\bf y}$ to an estimate ${\\bf \\hat x}=\\mathcal{D}({\\bf y})$ of the original sparse signal vector ${\\bf x}$. It is said that the average probability of error is at most $\\epsilon>0$ if $\\mathbb{P}[{\\bf \\hat x}\\neq {\\bf x}] \\leq \\epsilon$.\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.5cm}\n\\subsection{Sensing Matrix Construction}\nA linear encoding function is represented by a sensing matrix ${\\bf \\Phi}\\in\\mathbb{R}^{m\\times n}$, which linearly maps the $n$-dimensional sparse vector to an $m$-dimensional output vector, where $m\\ll n$. We construct the sensing matrix ${\\bf \\Phi}$ using the proposed idea, which is referred to as \\textit{dictionary coding.}\n\n\n\\subsubsection{Dictionary Basis Vector Selection} \nLet ${\\bf \\tilde H}\\in\\mathbb{F}^{{\\tilde m}_1\\times n}_q$ be a parity check matrix of a $q$-ary $[n,b]$ MDS code, where\n$q$ is a prime power $p^s$ for any positive integer $s\\in\\mathbb{Z}^+$. In this paper we focus on a $q$-ary $[n,b]$ Reed-Solomon (RS) code with field size constraint $q\\geq n$. Thus, the parity check matrix of the RS code has ${\\tilde m}_1$ full-rank where ${\\tilde m}_1=n-b \\in \\mathbb{Z}^{+}$. The $\\ell$th column vector of ${\\bf \\tilde H}$ is denoted by ${\\bf \\tilde h}_{\\ell}\\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$ where $\\ell\\in [1:n]$. We define the one-to-one mapping $h:\\mathbb{F}_{p^s}^{{\\tilde m}_1} \\rightarrow \\mathbb{F}_{p}^{{\\tilde m}_1s}$ that maps each element of $\\mathbb{F}_{p^s}$ into an $s$-length word in $\\mathbb{F}_{p}$. For instance, when $p=2$, it is possible to express an element of $\\mathbb{F}_{2^s}$ as a binary vector of length $s$. Using this mapping, we can transform each dictionary vector ${\\bf \\tilde h}_{\\ell} \\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$ into ${\\bf h}_{\\ell} = h({\\bf \\tilde h}_{\\ell})\\in \\mathbb{F}_{p}^{{m}_1}$ where $m_1={\\tilde m}_1 s$. The transformed column vector ${\\bf h}_{\\ell}$ is referred to as the $\\ell$th dictionary basis vector.\n\n\n\n\\subsubsection{Dictionary Coding via a Lattice Code}\nDictionary coding is to map a dictionary basis vector in ${\\mathbb F}^{m_1}_p$ into a lattice point in $\\mathbb{R}^m$ using lattice encoding where $m\\geq m_1$.\n\nWe commence by providing a brief background for a lattice construction. Let $\\mathbb{Z}$ be the ring of Gaussian integers and $p$ be a Gaussian prime. Let us denote the addition over $\\mathbb{F}_{p}$ by $\\bigoplus$, and let $g:\\mathbb{F}_{p} \\rightarrow \\mathbb{R}$ be the natural mapping of $\\mathbb{F}_{p} $ onto $\\{a:a\\in \\mathbb{Z}_p\\}\\subset\\mathbb{R}$. We recall the nested lattice code construction given in [14]. Let $\\Lambda = \\{ {\\boldsymbol \\lambda} = {\\bf T}{\\bf z} : {\\bf z} \\in \\mathbb{Z}^m\\}$ be a lattice in ${\\mathbb R}^m$, with full-rank generator matrix ${\\bf T}\\in {\\mathbb R}^{m\\times m}$. Let $\\mathcal{C} = \\{{\\bf c} = {\\bf w}{\\bf G} : {\\bf w} \\in \\mathbb{F}_{p}^{m_1} \\}$ denote a linear code over $\\mathbb{F}_{p}$ with block length $m$ and dimension $m_1$, with generator matrix ${\\bf G}\\in \\mathbb{F}_p^{m_1\\times m}$ where $m\\geq m_1$. The lattice $\\Lambda_1$ is defined through ``\\textit{construction A}'' (see \\cite{Nazer} and references therein) as\n\n", "itemtype": "equation", "pos": 8356, "prevtext": "\nwhere the measurement vector is in the Boolean cube, i.e., ${\\bf y}\\in \\{-1,1\\}^m$. It was shown that, with the one-bit measurements, sparse signal vectors with unit-norm can be recovered with high probability by convex optimization techniques \\cite{BP_onebit} or iterative greedy algorithms \\cite{BIHT2013}.\n\n\nIn this paper, we study a generalized compressive sensing problem in which each measurement is quantized with $p$ levels where $p$ ($\\geq 2$) is a prime number. We also consider a quantized source signal, i.e., the non-zero elements of a sparse signal are chosen from a set of integer values, i.e., ${\\bf x}\\in \\mathbb{Z}^n_p$. Such setting can be found in  many applications. For instance, in a random access wireless system, active users among all users in the system send quadrature amplitude modulated symbols (i.e., $p$-level quantized signals) to a receiver, and it detects the active users' signals using a $p$-level ADC.\n\n\n\nA fundamental question we ask in this paper is: what is the sufficient condition for the perfect recovery of the integer sparse signal with $p$-level per measurement in the presence of Gaussian noise? To shed light on the answer to this question, we develop a new sparse signal recovery framework, which is referred to as ``\\textit{coded compressive sensing}.'' The core idea of coded compressive sensing is to exploit both source and channel coding techniques in information theory. The proposed scheme consists of two cascade encoding and decoding phases. The first phase of encoding is the \\textit{compression phase}, in which a high dimensional sparse signal vector in ${\\mathbb Z}_p^n$ is compressed to a low dimensional signal vector using a parity check matrix of a maximum distance separable (MDS) linear code. The second phase is the \\textit{dictionary coding phase}. In this phase, each dictionary vector (each column vector of the parity check matrix) is encoded to a coded dictionary vector by exploiting a (near) capacity-achieving lattice code for a Gaussian channel. We propose  a two-stage decoding method called ``\\textit{compute-and-recover}.'' In the first stage of decoding, a linear combination of the encoded dictionary vectors corresponding the non-zero elements in ${\\bf x}\\in\\mathbb{Z}^n_p$ is decoded. We call this as the dictionary equation decoding stage that produces a noise-free measurement vector. Once the dictionary equation is perfectly decoded, in the second stage of decoding, we apply syndrome decoding to the equivalent finite field representation of the dictionary equation for the sparse signal recovery. Using the proposed scheme, we derive a lower bound of the number of measurements for the perfect recovery as a function of important system parameters: the quantization level $p$, the sparsity level $k$, the signal dimension $n$, and the number of measurements $m$. Considering $p=2$ as a special case, we compare the proposed scheme with existing algorithms developed for the one-bit compressive sensing problem \\cite{BoufounosBaraniuk2008}. Numerical results show that the proposed scheme outperforms than binary iterative hard thresholding (BITH) \\cite{BIHT2013} in a low signal-to-noise ratio (SNR) regime.\n\n\\vspace{-0.5cm}\n\\section{Coded Compressive Sensing Problem}\nIn this section, we present a coded compressive sensing framework for an integer sparse signal recovery in the presence Gaussian noise.\n\n\n\n\n\\vspace{-0.4cm}\n\\subsection{Signal Model}\nWe are interested in a sparse signal detection problem from a compressed measurement in the presence of noise. Let ${\\bf x}=[x_1,x_2,\\ldots,x_n]^{\\top} \\in \\mathbb{Z}_p^{n\\times 1}$ be an unknown sparse signal vector whose sparsity level is equal to $k$, i.e., $\\|{\\bf x}\\|_0=k \\ll n$. The measurement equation of quantized compressed sensing is given by\n\n", "index": 5, "text": "\\begin{align}\n{\\bf y}=S_{p}\\left({\\bf \\Phi}{\\bf x} +{\\bf n}\\right), \\label{eq:system_eq}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf y}=S_{p}\\left({\\bf\\Phi}{\\bf x}+{\\bf n}\\right),\" display=\"inline\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><msub><mi>S</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>+</mo><mi>\ud835\udc27</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere $g(\\mathcal{C})$ is the image of $\\mathcal{C}$ under the mapping function $g$. It follows that $\\Lambda \\subseteq \\Lambda_1 \\subseteq p^{-1}\\Lambda$ is a chain of nested lattices, such that $\\left|\\frac{\\Lambda_1}{\\Lambda}\\right| = p^{m_1}$ and $\\left|\\frac{ p^{-1}\\Lambda}{\\Lambda_1}\\right| = p^{(m-m_1)}$.\n\nFor a lattice $\\Lambda$ and ${\\bf r} \\in \\mathbb{R}^m$, we define the lattice quantizer ${\\rm Q}_\\Lambda({\\bf r})=\\arg \\min_{{\\bf r}\\in \\Lambda}\\|{\\bf r}-{\\boldsymbol \\lambda}\\|_2$, the Voronoi region $\\mathcal{V}_{\\Lambda}=\\{{\\bf r}\\in \\mathbb{R}^m :{\\rm Q}_\\Lambda({\\bf r})={\\bf 0}\\}$ and $[{\\bf r}]\\mod \\Lambda ={\\bf r}-{\\rm Q}_\\Lambda({\\bf r})$. For $\\Lambda$ and $\\Lambda_1$ given above, we define\nthe lattice code $\\mathcal{L}=\\Lambda_1 \\cup \\mathcal{V}_{\\Lambda}$ with rate $R = \\frac{1}{m} \\log |\\mathcal{L}| = \\frac{m_1}{m}\\log(p)$.\n\n\n\nConstruction A provides an encoding function that maps a dictionary basis vector ${\\bf h}_{\\ell} \\in \\mathbb{F}_{p}^{m_1} $ into a codeword in $\\mathcal{L}$. Notice that the set $p^{-1}g(\\mathcal{C}){\\bf T}$ is a system of coset representatives of the cosets of $\\Lambda$ in $\\Lambda_1$. Hence, the encoding function $f : \\mathbb{F}^{m_1}_p \\rightarrow \\mathcal{L}$ is defined by\n\n", "itemtype": "equation", "pos": 12447, "prevtext": "\nwhere $S_{p}$ denotes the $p$-level {\\em scalar} quantizer that applied component-wise, and ${\\bf y}\\in \\mathbb{R}^{m\\times 1}=[y_1,\\ldots,y_m]^{\\top}$ and ${\\bf n}\\in \\mathbb{R}^{m\\times 1}=[n_1,\\ldots,n_m]^{\\top}$ denote the measurement and noise vector, respectively. All entries of the noise vector are assumed to be independent and identically distributed (IID) Gaussian random variables with zero mean and variance $\\sigma^2/m$, i.e., $n_{i} \\sim \\mathcal{N}\\left(0,\\frac{\\sigma^2}{m}\\right)$ for all $i$.\n\nOur objective is to reliably estimate the unknown sparse signal vector ${\\bf x}$ given ${\\bf y}$ in the presence of Gaussian noise ${\\bf n}$, by appropriately constructing a linear measurement matrix ${\\bf \\Phi}$ and $p$-level scalar quantizer $S_{p}$. We define a sparse signal recovery decoder $\\mathcal{D}:\\mathbb{R}^m \\rightarrow \\mathbb{Z}_p^n$, which maps the measurement vector ${\\bf y}$ to an estimate ${\\bf \\hat x}=\\mathcal{D}({\\bf y})$ of the original sparse signal vector ${\\bf x}$. It is said that the average probability of error is at most $\\epsilon>0$ if $\\mathbb{P}[{\\bf \\hat x}\\neq {\\bf x}] \\leq \\epsilon$.\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.5cm}\n\\subsection{Sensing Matrix Construction}\nA linear encoding function is represented by a sensing matrix ${\\bf \\Phi}\\in\\mathbb{R}^{m\\times n}$, which linearly maps the $n$-dimensional sparse vector to an $m$-dimensional output vector, where $m\\ll n$. We construct the sensing matrix ${\\bf \\Phi}$ using the proposed idea, which is referred to as \\textit{dictionary coding.}\n\n\n\\subsubsection{Dictionary Basis Vector Selection} \nLet ${\\bf \\tilde H}\\in\\mathbb{F}^{{\\tilde m}_1\\times n}_q$ be a parity check matrix of a $q$-ary $[n,b]$ MDS code, where\n$q$ is a prime power $p^s$ for any positive integer $s\\in\\mathbb{Z}^+$. In this paper we focus on a $q$-ary $[n,b]$ Reed-Solomon (RS) code with field size constraint $q\\geq n$. Thus, the parity check matrix of the RS code has ${\\tilde m}_1$ full-rank where ${\\tilde m}_1=n-b \\in \\mathbb{Z}^{+}$. The $\\ell$th column vector of ${\\bf \\tilde H}$ is denoted by ${\\bf \\tilde h}_{\\ell}\\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$ where $\\ell\\in [1:n]$. We define the one-to-one mapping $h:\\mathbb{F}_{p^s}^{{\\tilde m}_1} \\rightarrow \\mathbb{F}_{p}^{{\\tilde m}_1s}$ that maps each element of $\\mathbb{F}_{p^s}$ into an $s$-length word in $\\mathbb{F}_{p}$. For instance, when $p=2$, it is possible to express an element of $\\mathbb{F}_{2^s}$ as a binary vector of length $s$. Using this mapping, we can transform each dictionary vector ${\\bf \\tilde h}_{\\ell} \\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$ into ${\\bf h}_{\\ell} = h({\\bf \\tilde h}_{\\ell})\\in \\mathbb{F}_{p}^{{m}_1}$ where $m_1={\\tilde m}_1 s$. The transformed column vector ${\\bf h}_{\\ell}$ is referred to as the $\\ell$th dictionary basis vector.\n\n\n\n\\subsubsection{Dictionary Coding via a Lattice Code}\nDictionary coding is to map a dictionary basis vector in ${\\mathbb F}^{m_1}_p$ into a lattice point in $\\mathbb{R}^m$ using lattice encoding where $m\\geq m_1$.\n\nWe commence by providing a brief background for a lattice construction. Let $\\mathbb{Z}$ be the ring of Gaussian integers and $p$ be a Gaussian prime. Let us denote the addition over $\\mathbb{F}_{p}$ by $\\bigoplus$, and let $g:\\mathbb{F}_{p} \\rightarrow \\mathbb{R}$ be the natural mapping of $\\mathbb{F}_{p} $ onto $\\{a:a\\in \\mathbb{Z}_p\\}\\subset\\mathbb{R}$. We recall the nested lattice code construction given in [14]. Let $\\Lambda = \\{ {\\boldsymbol \\lambda} = {\\bf T}{\\bf z} : {\\bf z} \\in \\mathbb{Z}^m\\}$ be a lattice in ${\\mathbb R}^m$, with full-rank generator matrix ${\\bf T}\\in {\\mathbb R}^{m\\times m}$. Let $\\mathcal{C} = \\{{\\bf c} = {\\bf w}{\\bf G} : {\\bf w} \\in \\mathbb{F}_{p}^{m_1} \\}$ denote a linear code over $\\mathbb{F}_{p}$ with block length $m$ and dimension $m_1$, with generator matrix ${\\bf G}\\in \\mathbb{F}_p^{m_1\\times m}$ where $m\\geq m_1$. The lattice $\\Lambda_1$ is defined through ``\\textit{construction A}'' (see \\cite{Nazer} and references therein) as\n\n", "index": 7, "text": "\\begin{align}\n\\Lambda_1 =p^{-1}g(\\mathcal{C}){\\bf T} + \\Lambda,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Lambda_{1}=p^{-1}g(\\mathcal{C}){\\bf T}+\\Lambda,\" display=\"inline\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub><mo>=</mo><mrow><mrow><msup><mi>p</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc13</mi></mrow><mo>+</mo><mi mathvariant=\"normal\">\u039b</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "where\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $g(\\mathcal{C})$ is the image of $\\mathcal{C}$ under the mapping function $g$. It follows that $\\Lambda \\subseteq \\Lambda_1 \\subseteq p^{-1}\\Lambda$ is a chain of nested lattices, such that $\\left|\\frac{\\Lambda_1}{\\Lambda}\\right| = p^{m_1}$ and $\\left|\\frac{ p^{-1}\\Lambda}{\\Lambda_1}\\right| = p^{(m-m_1)}$.\n\nFor a lattice $\\Lambda$ and ${\\bf r} \\in \\mathbb{R}^m$, we define the lattice quantizer ${\\rm Q}_\\Lambda({\\bf r})=\\arg \\min_{{\\bf r}\\in \\Lambda}\\|{\\bf r}-{\\boldsymbol \\lambda}\\|_2$, the Voronoi region $\\mathcal{V}_{\\Lambda}=\\{{\\bf r}\\in \\mathbb{R}^m :{\\rm Q}_\\Lambda({\\bf r})={\\bf 0}\\}$ and $[{\\bf r}]\\mod \\Lambda ={\\bf r}-{\\rm Q}_\\Lambda({\\bf r})$. For $\\Lambda$ and $\\Lambda_1$ given above, we define\nthe lattice code $\\mathcal{L}=\\Lambda_1 \\cup \\mathcal{V}_{\\Lambda}$ with rate $R = \\frac{1}{m} \\log |\\mathcal{L}| = \\frac{m_1}{m}\\log(p)$.\n\n\n\nConstruction A provides an encoding function that maps a dictionary basis vector ${\\bf h}_{\\ell} \\in \\mathbb{F}_{p}^{m_1} $ into a codeword in $\\mathcal{L}$. Notice that the set $p^{-1}g(\\mathcal{C}){\\bf T}$ is a system of coset representatives of the cosets of $\\Lambda$ in $\\Lambda_1$. Hence, the encoding function $f : \\mathbb{F}^{m_1}_p \\rightarrow \\mathcal{L}$ is defined by\n\n", "index": 9, "text": "\\begin{equation}\nf({\\bf h}_{\\ell}) = \\left[p^{-1}g({\\bf c}_{\\ell}){\\bf T}\\right] \\mod \\Lambda,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"f({\\bf h}_{\\ell})=\\left[p^{-1}g({\\bf c}_{\\ell}){\\bf T}\\right]\\mod\\Lambda,\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc21</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mrow><msup><mi>p</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1c</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc13</mi></mrow><mo>]</mo></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mi mathvariant=\"normal\">\u039b</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nConsequently, the $\\ell$th codeword vector ${\\bf t}_{\\ell}$ is produced by the encoding function\n\n", "itemtype": "equation", "pos": 13878, "prevtext": "where\n\n", "index": 11, "text": "\\begin{equation}\n({\\bf c}_{\\ell})^{\\top} = ({\\bf h}_{\\ell})^{\\top}{\\bf G}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"({\\bf c}_{\\ell})^{\\top}=({\\bf h}_{\\ell})^{\\top}{\\bf G}.\" display=\"block\"><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1c</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc21</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc06</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere each dictionary vector is chosen from lattice codewords in the nested lattice codebook $\\mathcal{L}$, i.e.,\n${\\bf t}_{\\ell} \\in \\mathcal{L}$. Using this construction method, we have a linear sensing matrix consisting of $n$ column vectors as\n\n", "itemtype": "equation", "pos": 14065, "prevtext": "\nConsequently, the $\\ell$th codeword vector ${\\bf t}_{\\ell}$ is produced by the encoding function\n\n", "index": 13, "text": "\\begin{align}\n{\\bf t}_{\\ell}=f({\\bf h}_{\\ell}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf t}_{\\ell}=f({\\bf h}_{\\ell}),\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc2d</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc21</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nThe average power of each codeword is assumed to be\n\n", "itemtype": "equation", "pos": 14373, "prevtext": "\nwhere each dictionary vector is chosen from lattice codewords in the nested lattice codebook $\\mathcal{L}$, i.e.,\n${\\bf t}_{\\ell} \\in \\mathcal{L}$. Using this construction method, we have a linear sensing matrix consisting of $n$ column vectors as\n\n", "index": 15, "text": "\\begin{align}\n{\\bf \\Phi} =\\left[{\\bf t}_1, {\\bf t}_2, \\ldots, {\\bf t}_n\\right].\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf\\Phi}=\\left[{\\bf t}_{1},{\\bf t}_{2},\\ldots,{\\bf t}_{n}\\right].\" display=\"inline\"><mrow><mrow><mi>\ud835\udebd</mi><mo>=</mo><mrow><mo>[</mo><msub><mi>\ud835\udc2d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\ud835\udc2d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc2d</mi><mi>n</mi></msub><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nFinally, in this paper, we choose the shaping lattice $\\Lambda$ as a {\\em cubic} lattice, namely ${\\bf T} = \\tau{\\bf I}$, which enables that a lattice decoding is implemented by a scalar quantizer (see \\cite{Hong_Caire2011} for more details). Here, $\\tau$ is chosen to satisfy the power constraint in (\\ref{eq:power_const}) as $\\tau = \\sqrt{8} \\mbox{ for } p=2 \\mbox{ and } \\tau=\\sqrt{12} \\mbox{ for } p \\geq 3$. Then, the element-wise SNR is defined as\n\n", "itemtype": "equation", "pos": 14517, "prevtext": "\nThe average power of each codeword is assumed to be\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:power_const}\n\\frac{1}{m}\\mathbb{E}[\\|{\\bf t}_{\\ell}\\|_2^2]\\leq 1.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{m}\\mathbb{E}[\\|{\\bf t}_{\\ell}\\|_{2}^{2}]\\leq 1.\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>\ud835\udc2d</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\n\n\n\n\\subsection{Proposed Scalar Quantizer}\n\nWe propose a $p$-level scalar quantizer called {\\em sawtooth transform} as depicted in Fig.~\\ref{fig:1}, which can be implemented by the modulo operation followed by the scalar quantization as\n\n", "itemtype": "equation", "pos": 15078, "prevtext": "\nFinally, in this paper, we choose the shaping lattice $\\Lambda$ as a {\\em cubic} lattice, namely ${\\bf T} = \\tau{\\bf I}$, which enables that a lattice decoding is implemented by a scalar quantizer (see \\cite{Hong_Caire2011} for more details). Here, $\\tau$ is chosen to satisfy the power constraint in (\\ref{eq:power_const}) as $\\tau = \\sqrt{8} \\mbox{ for } p=2 \\mbox{ and } \\tau=\\sqrt{12} \\mbox{ for } p \\geq 3$. Then, the element-wise SNR is defined as\n\n", "index": 19, "text": "\\begin{align}\n{\\rm SNR} = \\frac{\\tau^2}{\\sigma^2/m}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\rm SNR}=\\frac{\\tau^{2}}{\\sigma^{2}/m}.\" display=\"inline\"><mrow><mrow><mi>SNR</mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>\u03c4</mi><mn>2</mn></msup><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>/</mo><mi>m</mi></mrow></mfrac></mstyle></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\n\n\n\n\n\\section{Main Result}\n\nIn this section, we characterize the sufficient condition for the exact recovery of an integer sparse signal vector. The following theorem is the main result of this paper.\n\n\n\\begin{theorem}\\label{th1}\n\n\nThe proposed coded compressive sensing method perfectly reconstructs the sparse signal vector ${\\bf x}\\in \\mathbb{Z}^{n}_p$ with $\\|{\\bf x}\\|_0\\leq k$, with vanishing error probability for large enough $n$, provided that\n\n", "itemtype": "equation", "pos": 15379, "prevtext": "\n\n\n\n\\subsection{Proposed Scalar Quantizer}\n\nWe propose a $p$-level scalar quantizer called {\\em sawtooth transform} as depicted in Fig.~\\ref{fig:1}, which can be implemented by the modulo operation followed by the scalar quantization as\n\n", "index": 21, "text": "\\begin{equation}\\label{eq:quantizer}\nS_{p}(\\cdot) = \\left[Q_{(\\tau/p)\\mathbb{Z}}(\\cdot)\\right] \\mod \\tau\\mathbb{Z}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"S_{p}(\\cdot)=\\left[Q_{(\\tau/p)\\mathbb{Z}}(\\cdot)\\right]\\mod\\tau\\mathbb{Z}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mrow><msub><mi>Q</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>/</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u2124</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mi>\u2124</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "where $H_p(\\cdot)$ represents a $p$-ary entropy function and $Z$ denotes an effective quantized noise obtained from the $p$-level quantizer as\n$Z = g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left( N \\right)\\right] \\mod p\\mathbb{Z} \\right)$,\n\n\n\nwhere $N\\sim \\mathcal{N}\\left(0,\\frac{p^2}{{\\rm SNR}}\\right)$.\n\\end{theorem}\n\n\n\\begin{proof}\n\nThe proof of this theorem is based on the proposed two-stage decoding method called ``\\textit{compute-and-recover}''. In the first stage, we decode an integer linear combination of coded dictionary vectors by removing noise, which essentially yields a finite-field sparse signal recovery problem. In the second stage, we apply syndrome decoding over the finite-field to reconstruct the sparse signal vector.\n\n\n\n\n\n\\vspace{-0.28cm}\n\\subsection{Step 1: Computation of Dictionary Equation}\nIn this stage, we decode a noise-free measurement vector ${\\bf \\tilde y} \\in \\mathbb{F}^{m_1}_p$ from ${\\bf y} \\in \\mathbb{R}^{m}$ using the key property of a lattice code. Recall that dictionary vector is a lattice code; thereby, any integer-linear combination of lattice codewords is again a lattice codeword\n\\cite{Nazer}. Thus we have that $[\\sum_{\\ell\\in \\mathcal{T}}{\\bf t}_{\\ell}x_{\\ell}] \\mod \\Lambda \\in \\mathcal{L} $ due to ${\\bf x} \\in \\mathbb{Z}_p^{n \\times 1}$. We will first exploit this fact to decode a noise-free measurement vector.\n\nLetting $\\mathcal{T}$ be the support set of ${\\bf x}$, the noisy measurement vector with the $p$-level quantizer is given by\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\n\n\n\\section{Main Result}\n\nIn this section, we characterize the sufficient condition for the exact recovery of an integer sparse signal vector. The following theorem is the main result of this paper.\n\n\n\\begin{theorem}\\label{th1}\n\n\nThe proposed coded compressive sensing method perfectly reconstructs the sparse signal vector ${\\bf x}\\in \\mathbb{Z}^{n}_p$ with $\\|{\\bf x}\\|_0\\leq k$, with vanishing error probability for large enough $n$, provided that\n\n", "index": 23, "text": "\\begin{align}\nm\\geq \\frac{2k\\log_p{n}}{1- H_p(Z)},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m\\geq\\frac{2k\\log_{p}{n}}{1-H_{p}(Z)},\" display=\"inline\"><mrow><mrow><mi>m</mi><mo>\u2265</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><msub><mi>log</mi><mi>p</mi></msub><mo>\u2061</mo><mi>n</mi></mrow></mrow><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>H</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": " where the second equality follows from (\\ref{eq:quantizer}).\n\n\nWe transform this noisy and quantized measurement into a noiseless finite-field measurement as follows. From the quantized sequence ${\\bf y}$, we produce the sequence ${\\bf \\hat y} \\in \\mathbb{F}^{m}_p$ with components\n\n", "itemtype": "equation", "pos": -1, "prevtext": "where $H_p(\\cdot)$ represents a $p$-ary entropy function and $Z$ denotes an effective quantized noise obtained from the $p$-level quantizer as\n$Z = g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left( N \\right)\\right] \\mod p\\mathbb{Z} \\right)$,\n\n\n\nwhere $N\\sim \\mathcal{N}\\left(0,\\frac{p^2}{{\\rm SNR}}\\right)$.\n\\end{theorem}\n\n\n\\begin{proof}\n\nThe proof of this theorem is based on the proposed two-stage decoding method called ``\\textit{compute-and-recover}''. In the first stage, we decode an integer linear combination of coded dictionary vectors by removing noise, which essentially yields a finite-field sparse signal recovery problem. In the second stage, we apply syndrome decoding over the finite-field to reconstruct the sparse signal vector.\n\n\n\n\n\n\\vspace{-0.28cm}\n\\subsection{Step 1: Computation of Dictionary Equation}\nIn this stage, we decode a noise-free measurement vector ${\\bf \\tilde y} \\in \\mathbb{F}^{m_1}_p$ from ${\\bf y} \\in \\mathbb{R}^{m}$ using the key property of a lattice code. Recall that dictionary vector is a lattice code; thereby, any integer-linear combination of lattice codewords is again a lattice codeword\n\\cite{Nazer}. Thus we have that $[\\sum_{\\ell\\in \\mathcal{T}}{\\bf t}_{\\ell}x_{\\ell}] \\mod \\Lambda \\in \\mathcal{L} $ due to ${\\bf x} \\in \\mathbb{Z}_p^{n \\times 1}$. We will first exploit this fact to decode a noise-free measurement vector.\n\nLetting $\\mathcal{T}$ be the support set of ${\\bf x}$, the noisy measurement vector with the $p$-level quantizer is given by\n\n", "index": 25, "text": "\\begin{align}\n{\\bf y}&=S_{p}\\left(\\sum_{\\ell\\in \\mathcal{T}}{\\bf t}_{\\ell}x_{\\ell} +{\\bf n}\\right) \\nonumber \\\\\n&=\\left[Q_{(\\tau/p)\\mathbb{Z}}\\left(\\sum_{\\ell\\in \\mathcal{T}}{\\bf t}_{\\ell}x_{\\ell} +{\\bf n}\\right)\\right] \\mod \\tau\\mathbb{Z},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf y}\" display=\"inline\"><mi>\ud835\udc32</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=S_{p}\\left(\\sum_{\\ell\\in\\mathcal{T}}{\\bf t}_{\\ell}x_{\\ell}+{\\bf n%&#10;}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>S</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder></mstyle><mrow><msub><mi>\ud835\udc2d</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi mathvariant=\"normal\">\u2113</mi></msub></mrow></mrow><mo>+</mo><mi>\ud835\udc27</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left[Q_{(\\tau/p)\\mathbb{Z}}\\left(\\sum_{\\ell\\in\\mathcal{T}}{\\bf t%&#10;}_{\\ell}x_{\\ell}+{\\bf n}\\right)\\right]\\mod\\tau\\mathbb{Z},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>[</mo><mrow><msub><mi>Q</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>/</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u2124</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder></mstyle><mrow><msub><mi>\ud835\udc2d</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi mathvariant=\"normal\">\u2113</mi></msub></mrow></mrow><mo>+</mo><mi>\ud835\udc27</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mi>\u2124</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": " for $i=1,\\ldots,m$. Since $\\frac{p}{\\tau}t_{\\ell,i} \\in \\mathbb{Z}$ by construction, and using the obvious identity $Q_{\\mathbb{Z}}(v+\\zeta) = v+Q_{\\mathbb{Z}}(\\zeta)$ with $v \\in \\mathbb{Z}$ and $\\zeta \\in \\mathcal{R}$, we arrive at\n\n", "itemtype": "equation", "pos": -1, "prevtext": " where the second equality follows from (\\ref{eq:quantizer}).\n\n\nWe transform this noisy and quantized measurement into a noiseless finite-field measurement as follows. From the quantized sequence ${\\bf y}$, we produce the sequence ${\\bf \\hat y} \\in \\mathbb{F}^{m}_p$ with components\n\n", "index": 27, "text": "\\begin{align}\n{\\hat y}_{i} &= g^{-1}\\left(\\frac{p}{\\tau}{\\bf y} \\right)\\nonumber \\\\\n&=g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left(\\frac{p}{\\tau}\\left(\\sum_{\\ell \\in \\mathcal{T}}t_{\\ell,i} + n_{i} \\right)   \\right)   \\right] \\mod p\\mathbb{Z}\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\hat{y}}_{i}\" display=\"inline\"><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=g^{-1}\\left(\\frac{p}{\\tau}{\\bf y}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msup><mi>g</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>p</mi><mi>\u03c4</mi></mfrac></mstyle><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left(\\frac{p}{\\tau}\\left(\\sum_{%&#10;\\ell\\in\\mathcal{T}}t_{\\ell,i}+n_{i}\\right)\\right)\\right]\\mod p\\mathbb{Z}\\right),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mi>g</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><msub><mi>Q</mi><mi>\u2124</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>p</mi><mi>\u03c4</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder></mstyle><msub><mi>t</mi><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><msub><mi>n</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>\u2124</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": " where the elements of the discrete additive noise vector ${\\bf z}$ are given by\n\n", "itemtype": "equation", "pos": -1, "prevtext": " for $i=1,\\ldots,m$. Since $\\frac{p}{\\tau}t_{\\ell,i} \\in \\mathbb{Z}$ by construction, and using the obvious identity $Q_{\\mathbb{Z}}(v+\\zeta) = v+Q_{\\mathbb{Z}}(\\zeta)$ with $v \\in \\mathbb{Z}$ and $\\zeta \\in \\mathcal{R}$, we arrive at\n\n", "index": 29, "text": "\\begin{equation}\n{\\bf \\hat y} = \\left(\\bigoplus_{\\ell \\in \\mathcal{T}}{\\bf c}_{\\ell}g^{-1}(x_{\\ell}) \\right) \\oplus {\\bf z},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"{\\bf\\hat{y}}=\\left(\\bigoplus_{\\ell\\in\\mathcal{T}}{\\bf c}_{\\ell}g^{-1}(x_{\\ell}%&#10;)\\right)\\oplus{\\bf z},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><mo>(</mo><mrow><munder><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2295</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder><mrow><msub><mi>\ud835\udc1c</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2062</mo><msup><mi>g</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2295</mo><mi>\ud835\udc33</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "for $i=1,\\ldots,m$.\nSince, by linearity, ${\\bf v}=\\bigoplus_{\\ell \\in \\mathcal{T}}{\\bf c}_{\\ell}g^{-1}(x_{\\ell})$ is a codeword of $\\mathcal{C}$, the above channel can be considered as a point-to-point channel with discrete additive noise over $\\mathbb{F}_{p}$. Then, we can reliably decode ${\\bf v}$ if\n\n", "itemtype": "equation", "pos": -1, "prevtext": " where the elements of the discrete additive noise vector ${\\bf z}$ are given by\n\n", "index": 31, "text": "\\begin{equation}\nz_{i} = g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left(\\frac{p}{\\tau}n_{i}\\right)\\right] \\mod p\\mathbb{Z}\\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"z_{i}=g^{-1}\\left(\\left[Q_{\\mathbb{Z}}\\left(\\frac{p}{\\tau}n_{i}\\right)\\right]%&#10;\\mod p\\mathbb{Z}\\right),\" display=\"block\"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msup><mi>g</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><msub><mi>Q</mi><mi>\u2124</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mi>p</mi><mi>\u03c4</mi></mfrac><mo>\u2062</mo><msub><mi>n</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo lspace=\"2.5pt\" rspace=\"2.5pt\">mod</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>\u2124</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": " This is an immediate consequence of the well-known fact that linear codes achieve the capacity of symmetric discrete memoryless channel\\cite{Dobrushin}. From this result, we can obtain that the sufficient condition for the perfect recovery of the noise-free measurement vector is\n\n", "itemtype": "equation", "pos": -1, "prevtext": "for $i=1,\\ldots,m$.\nSince, by linearity, ${\\bf v}=\\bigoplus_{\\ell \\in \\mathcal{T}}{\\bf c}_{\\ell}g^{-1}(x_{\\ell})$ is a codeword of $\\mathcal{C}$, the above channel can be considered as a point-to-point channel with discrete additive noise over $\\mathbb{F}_{p}$. Then, we can reliably decode ${\\bf v}$ if\n\n", "index": 33, "text": "\\begin{equation}\n\\frac{m_1}{m}\\leq 1 - H_p(Z),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\frac{m_{1}}{m}\\leq 1-H_{p}(Z),\" display=\"block\"><mrow><mrow><mfrac><msub><mi>m</mi><mn>1</mn></msub><mi>m</mi></mfrac><mo>\u2264</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>H</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\n\n\n\\vspace{-0.3cm}\n\\subsection{Step 2: Recovery via Syndrome Decoding}\n\nRecall that, in the first stage, the decoder has recovered the dictionary equation, i.e., ${\\bf v}=\\bigoplus_{\\ell \\in \\mathcal{T}}{\\bf c}_{\\ell}{\\tilde x}_{\\ell}$ where ${\\tilde x}_{\\ell} = g^{-1}(x_{\\ell})$. Using the linearity of code $\\mathcal{C}$, we have:\n\n", "itemtype": "equation", "pos": 19539, "prevtext": " This is an immediate consequence of the well-known fact that linear codes achieve the capacity of symmetric discrete memoryless channel\\cite{Dobrushin}. From this result, we can obtain that the sufficient condition for the perfect recovery of the noise-free measurement vector is\n\n", "index": 35, "text": "\\begin{align}\nm\\geq \\frac{m_1}{1 - H_p(Z)}. \\label{eq:lattice_rate}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m\\geq\\frac{m_{1}}{1-H_{p}(Z)}.\" display=\"inline\"><mrow><mrow><mi>m</mi><mo>\u2265</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>m</mi><mn>1</mn></msub><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>H</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere ${\\bf \\tilde y}$ represents the effective measurement vector in $\\mathbb{F}^{m_1}_p$. As a result, the measurement equation can be equivalently rewritten in a matrix form over $\\mathbb{F}_{p}$ as\n\n", "itemtype": "equation", "pos": 19952, "prevtext": "\n\n\n\\vspace{-0.3cm}\n\\subsection{Step 2: Recovery via Syndrome Decoding}\n\nRecall that, in the first stage, the decoder has recovered the dictionary equation, i.e., ${\\bf v}=\\bigoplus_{\\ell \\in \\mathcal{T}}{\\bf c}_{\\ell}{\\tilde x}_{\\ell}$ where ${\\tilde x}_{\\ell} = g^{-1}(x_{\\ell})$. Using the linearity of code $\\mathcal{C}$, we have:\n\n", "index": 37, "text": "\\begin{align}\n{\\bf \\tilde y}=\\bigoplus_{\\ell\\in \\mathcal{T}}{\\bf h}_{\\ell}{\\tilde x}_{\\ell},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf\\tilde{y}}=\\bigoplus_{\\ell\\in\\mathcal{T}}{\\bf h}_{\\ell}{%&#10;\\tilde{x}}_{\\ell},\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2295</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder></mstyle><mrow><msub><mi>\ud835\udc21</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mi mathvariant=\"normal\">\u2113</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere ${\\bf H}\\in\\mathbb{F}_p^{m_1\\times n}$ denotes the effective sensing matrix whose column vectors are selected from dictionary basis vectors ${\\bf h}_{\\ell}\\in\\mathbb{F}^{m_1\\times 1}_p$.\n\n\n\nWe would like to recover ${\\bf x} = g({\\bf \\tilde x})\\in \\mathbb{Z}_p^n$ from the effective measurement vector ${\\bf \\tilde y}={\\bf H}{\\bf \\tilde x}\\in \\mathbb{F}_p^{m_1}$ in a noiseless setting and using one-to-one mapping $g(\\cdot)$. Unlike the sparse recovery algorithm in a finite field in \\cite{Draper}, we apply a syndrome decoding method \\cite{Ancheta,Vishwanath2013}. Syndrome decoding harnesses the fact that there is a bijection mapping between a sparse signal (error) vector ${\\bf x}$ and the effective measurement (syndrome) vector ${\\bf \\tilde y}$, provided that ${\\bf x}$ contains at most $\\lfloor\\frac{d_{\\rm min}}{2} \\rfloor$ non-zero entries, i.e, $k\\leq \\lfloor\\frac{d_{\\rm min}}{2} \\rfloor$. Recall that, in our construction, the $\\ell$th dictionary vector ${\\bf h}_{\\ell} \\in \\mathbb{F}_p^{m}$ in ${\\bf H}$ was generated from the mapping $h: \\mathbb{F}_{p^s}^{{\\tilde m}_1}\\rightarrow \\mathbb{F}_{p}^{m_1}$ where $m_1={\\tilde m}_1s$, i.e., ${\\bf h}_{\\ell}=h({\\bf \\tilde h}_{\\ell})$. Since $h$ is bijection, applying the inverse mapping function ${\\bf \\bar y}=h^{-1}({\\bf \\tilde y})\\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$, we obtain the resultant measurement equation over $\\mathbb{F}_{p^s}$ as\n\n", "itemtype": "equation", "pos": 20259, "prevtext": "\nwhere ${\\bf \\tilde y}$ represents the effective measurement vector in $\\mathbb{F}^{m_1}_p$. As a result, the measurement equation can be equivalently rewritten in a matrix form over $\\mathbb{F}_{p}$ as\n\n", "index": 39, "text": "\\begin{align}\n{\\bf \\tilde y} ={\\bf H}{\\bf \\tilde x}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf\\tilde{y}}={\\bf H}{\\bf\\tilde{x}}.\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mi>\ud835\udc07</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere the second equality follows from $\\mathbb{F}_p^n \\subset \\mathbb{F}_{p^s}^n$ and the last equality is due to the one-to-one mapping between ${\\bf H}$ and ${\\bf \\tilde H}$ by $h(\\cdot)$. Since ${\\bf \\tilde H}$ was selected from the parity-check matrix of the $p^s$-ary $[n,b]$-RS code whose minimum distance, $d_{\\rm min}$ achieves a singleton bound, i.e., $d_{\\rm min}= n-b+1={\\tilde m}_1+1$. As a result, the syndrome decoding method allows us to recover the sparse signal perfectly, provided that\n\n", "itemtype": "equation", "pos": 21731, "prevtext": "\nwhere ${\\bf H}\\in\\mathbb{F}_p^{m_1\\times n}$ denotes the effective sensing matrix whose column vectors are selected from dictionary basis vectors ${\\bf h}_{\\ell}\\in\\mathbb{F}^{m_1\\times 1}_p$.\n\n\n\nWe would like to recover ${\\bf x} = g({\\bf \\tilde x})\\in \\mathbb{Z}_p^n$ from the effective measurement vector ${\\bf \\tilde y}={\\bf H}{\\bf \\tilde x}\\in \\mathbb{F}_p^{m_1}$ in a noiseless setting and using one-to-one mapping $g(\\cdot)$. Unlike the sparse recovery algorithm in a finite field in \\cite{Draper}, we apply a syndrome decoding method \\cite{Ancheta,Vishwanath2013}. Syndrome decoding harnesses the fact that there is a bijection mapping between a sparse signal (error) vector ${\\bf x}$ and the effective measurement (syndrome) vector ${\\bf \\tilde y}$, provided that ${\\bf x}$ contains at most $\\lfloor\\frac{d_{\\rm min}}{2} \\rfloor$ non-zero entries, i.e, $k\\leq \\lfloor\\frac{d_{\\rm min}}{2} \\rfloor$. Recall that, in our construction, the $\\ell$th dictionary vector ${\\bf h}_{\\ell} \\in \\mathbb{F}_p^{m}$ in ${\\bf H}$ was generated from the mapping $h: \\mathbb{F}_{p^s}^{{\\tilde m}_1}\\rightarrow \\mathbb{F}_{p}^{m_1}$ where $m_1={\\tilde m}_1s$, i.e., ${\\bf h}_{\\ell}=h({\\bf \\tilde h}_{\\ell})$. Since $h$ is bijection, applying the inverse mapping function ${\\bf \\bar y}=h^{-1}({\\bf \\tilde y})\\in \\mathbb{F}_{p^s}^{{\\tilde m}_1}$, we obtain the resultant measurement equation over $\\mathbb{F}_{p^s}$ as\n\n", "index": 41, "text": "\\begin{align}\n{\\bf \\bar y} &=h^{-1}\\left({\\bf H}{\\bf \\tilde x}\\right) =h^{-1}\\left({\\bf H}\\right){\\bf \\tilde x}= {\\bf \\tilde  H}{\\bf \\tilde x},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf\\bar{y}}\" display=\"inline\"><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">\u00af</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h^{-1}\\left({\\bf H}{\\bf\\tilde{x}}\\right)=h^{-1}\\left({\\bf H}%&#10;\\right){\\bf\\tilde{x}}={\\bf\\tilde{H}}{\\bf\\tilde{x}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mi>h</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\ud835\udc07</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>h</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mi>\ud835\udc07</mi><mo>)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>=</mo><mrow><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nPutting two inequalities in \\eqref{eq:lattice_rate} and \\eqref{eq:syndrome_decoding} together and using the fact $m_1={\\tilde m}_1s$ and $s= \\log_{p}(n)$, the number of required measurements for the sparse signal recover in the presence of Gaussian noise boils down to\n\n", "itemtype": "equation", "pos": 22392, "prevtext": "\nwhere the second equality follows from $\\mathbb{F}_p^n \\subset \\mathbb{F}_{p^s}^n$ and the last equality is due to the one-to-one mapping between ${\\bf H}$ and ${\\bf \\tilde H}$ by $h(\\cdot)$. Since ${\\bf \\tilde H}$ was selected from the parity-check matrix of the $p^s$-ary $[n,b]$-RS code whose minimum distance, $d_{\\rm min}$ achieves a singleton bound, i.e., $d_{\\rm min}= n-b+1={\\tilde m}_1+1$. As a result, the syndrome decoding method allows us to recover the sparse signal perfectly, provided that\n\n", "index": 43, "text": "\\begin{align}\nk\\leq \\left\\lfloor\\frac{\\tilde{m}_1+1}{2} \\right\\rfloor. \\label{eq:syndrome_decoding}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle k\\leq\\left\\lfloor\\frac{\\tilde{m}_{1}+1}{2}\\right\\rfloor.\" display=\"inline\"><mrow><mrow><mi>k</mi><mo>\u2264</mo><mrow><mo>\u230a</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>m</mi><mo stretchy=\"false\">~</mo></mover><mn>1</mn></msub><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mstyle><mo>\u230b</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\\textbf{Remark 1 (Decoding complexity)}:\nThe proposed two-stage decoding method can be  implemented with a polynomial time computational complexity. In the first stage, the lattice equation can be efficiently decoded with the $p$-level scalar quantizer in \\cite{Hong_Caire2011} and the successive decoding algorithm of the polar code \\cite{Arikan}, which essentially uses $\\mathcal{O}(m\\log(m))$ operations. Syndrome decoding used in the second stage can be implemented with polynomial time computational complexity algorithms such as Berlekamp-Massey algorithm, which requires $\\mathcal{O}(nk)$ operations in $\\mathbb{F}_{p^s}$. Considering $\\mathbb{F}_{p^s}$ is a vector space over $\\mathbb{F}_{p}$, this amount corresponds to $\\mathcal{O}(nks^2)$ operations in $\\mathbb{F}_{p}$. Since $n>m$ and $s= \\log_{p}(n)$,  the overall computational complexity of the proposed method is at most $\\mathcal{O}\\left(nk\\log^2 (n)\\right)$ operations for recovery.\n\n\\textbf{Remark 2 (Universality of the measurement matrix)}:\nThe proposed coded compressive sensing method is universal, as it is possible to recover all $k$ sparse signals using a fixed sensing matrix ${\\bf \\Phi}$. This universality is practically important, because one may needs to randomly  construct a new measurement matrix ${\\bf \\Phi}$ for each signal. Some existing one-bit compressive sensing algorithms \\cite{BIHT2013, Haupt_Baraniuk_2011,BP_onebit\n} do not hold the universality property.\n\n\\textbf{Remark 3 (Non-integer sparse signal case)}:\nOne potential concern for our integer sparse signal setting  is that a sparse signal can have real value components in some applications. This concern can be resolved by exploiting an integer-forcing technique in which ${\\bf x}\\in\\mathbb{R}^n$ is quantized into an integer vector ${\\bf  x}_{{\\rm int}}\\in\\mathbb{Z}^n$ and interpreting the residual ${\\bf \\Phi}({\\bf x}-{\\bf  x}_{{\\rm int}})$ as additional noise. Then, the effective measurements are obtained as\n\n", "itemtype": "equation", "pos": 22773, "prevtext": "\nPutting two inequalities in \\eqref{eq:lattice_rate} and \\eqref{eq:syndrome_decoding} together and using the fact $m_1={\\tilde m}_1s$ and $s= \\log_{p}(n)$, the number of required measurements for the sparse signal recover in the presence of Gaussian noise boils down to\n\n", "index": 45, "text": "\\begin{align}\nm\\geq \\frac{2k\\log_p{n}}{1 - H_p(Z)},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m\\geq\\frac{2k\\log_{p}{n}}{1-H_{p}(Z)},\" display=\"inline\"><mrow><mrow><mi>m</mi><mo>\u2265</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><msub><mi>log</mi><mi>p</mi></msub><mo>\u2061</mo><mi>n</mi></mrow></mrow><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>H</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\nwhere ${\\bf \\tilde n}={\\bf n}+{\\bf \\Phi}({\\bf x}-{\\bf  x}_{{\\rm int}})$ denotes effective noise. Utilizing this modified equation, we are able to apply the proposed coded compressive sensing method to estimate the integer approximation ${\\bf  x}_{{\\rm int}}$. Assuming the non-zero values in ${\\bf x}$ are bounded as  $|{\\bf x}_i| \\leq U$ for some $U\\in\\mathbb{R}^{+}$, we conjecture that the proposed scheme guarantees to recover the sparse signal with a bounded estimation error $\\|{\\bf x}-{\\bf  x}_{{\\rm int}}\\|_2^2\\leq k\\frac{U^2}{p^2}$ with an increased number of measurements than that in Theorem 1. The rigorous proof of this conjecture will be provided in our journal version \\cite{Lee_Hong_2016}.\n\n\n\n\\textbf{Remark 4 (Noiseless one-bit compressive sensing)}: One interesting scenario is that when a one-bit quantizer and a binary signal are used. In the case of noise-free, the number of required measurements for the perfect recovery is lower bounded by\n\n", "itemtype": "equation", "pos": 24844, "prevtext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\\textbf{Remark 1 (Decoding complexity)}:\nThe proposed two-stage decoding method can be  implemented with a polynomial time computational complexity. In the first stage, the lattice equation can be efficiently decoded with the $p$-level scalar quantizer in \\cite{Hong_Caire2011} and the successive decoding algorithm of the polar code \\cite{Arikan}, which essentially uses $\\mathcal{O}(m\\log(m))$ operations. Syndrome decoding used in the second stage can be implemented with polynomial time computational complexity algorithms such as Berlekamp-Massey algorithm, which requires $\\mathcal{O}(nk)$ operations in $\\mathbb{F}_{p^s}$. Considering $\\mathbb{F}_{p^s}$ is a vector space over $\\mathbb{F}_{p}$, this amount corresponds to $\\mathcal{O}(nks^2)$ operations in $\\mathbb{F}_{p}$. Since $n>m$ and $s= \\log_{p}(n)$,  the overall computational complexity of the proposed method is at most $\\mathcal{O}\\left(nk\\log^2 (n)\\right)$ operations for recovery.\n\n\\textbf{Remark 2 (Universality of the measurement matrix)}:\nThe proposed coded compressive sensing method is universal, as it is possible to recover all $k$ sparse signals using a fixed sensing matrix ${\\bf \\Phi}$. This universality is practically important, because one may needs to randomly  construct a new measurement matrix ${\\bf \\Phi}$ for each signal. Some existing one-bit compressive sensing algorithms \\cite{BIHT2013, Haupt_Baraniuk_2011,BP_onebit\n} do not hold the universality property.\n\n\\textbf{Remark 3 (Non-integer sparse signal case)}:\nOne potential concern for our integer sparse signal setting  is that a sparse signal can have real value components in some applications. This concern can be resolved by exploiting an integer-forcing technique in which ${\\bf x}\\in\\mathbb{R}^n$ is quantized into an integer vector ${\\bf  x}_{{\\rm int}}\\in\\mathbb{Z}^n$ and interpreting the residual ${\\bf \\Phi}({\\bf x}-{\\bf  x}_{{\\rm int}})$ as additional noise. Then, the effective measurements are obtained as\n\n", "index": 47, "text": "\\begin{align}\n{\\bf y}=S_{p}\\left({\\bf \\Phi}{\\bf  x}_{{\\rm int}} +{\\bf \\tilde n}\\right), \\label{eq:system_eq2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf y}=S_{p}\\left({\\bf\\Phi}{\\bf x}_{{\\rm int}}+{\\bf\\tilde{n}}%&#10;\\right),\" display=\"inline\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><msub><mi>S</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>int</mi></msub></mrow><mo>+</mo><mover accent=\"true\"><mi>\ud835\udc27</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06899.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering \\vspace{-0.3cm}\n\\includegraphics[width=3.4in]{Fig1.pdf} \\vspace{-0.2cm}\\caption{ The proposed coded compressive sensing framework for the binary sparse signal vector ${\\bf x} \\in \\{0,1\\}^n$ with one-bit and noisy measurements.} \\label{fig:1} \\vspace{-0.5cm}\n\\end{figure}\n\n\n\n \\vspace{-0.2cm}\n\\section{Numerical Example }\nIn this section, we provide the signal recovery performance of the proposed coded compressive sensing method for $p=2$, i.e., one-bit compressive sensing, by numerical experiments.\n\nTo test the proposed algorithm, $k$-sparse binary vector ${\\bf x}\\in \\mathbb{Z}_2^{511}$ is generated in which the non-zero positions of ${\\bf x}$ is uniformly distributed between 1 and 511. A fixed binary sensing matrix ${\\bf \\Phi}\\in \\mathbb{F}_2^{180 \\times 511}$ is designed by the concatenation of compression matrix ${\\bf H}\\in \\mathbb{F}_2^{90 \\times 511}$ and the generator matrix ${\\bf G}^T\\in \\mathbb{F}_2^{180 \\times 90}$ of polar code (which is completely determined by the rate-one Arikan's kernal matrix and the information set \\cite{Arikan}), as illustrated in Fig. \\ref{fig:1}. In particular, the binary compression matrix ${\\bf H}$ is obtained from ${\\bf \\tilde H}$ that is the parity check matrix of ${\\rm GF}(2^9)$-ary $[511, 501]$ RS code with the minimum distance of $10$. Therefore, it is perfectly able to perform syndrome decoding up to the sparsity level of 5 in a noiseless case. In addition, we pick the binary polar generator matrix ${\\bf G}^T\\in \\mathbb{F}_2^{180\\times 90}$  of code rate $\\frac{1}{2}$. We evaluate the perfect recovery probability, i.e., $\\mathbb{E}[{\\bf 1}\\left({\\bf x}={\\bf \\hat x}\\right)]$ of the sparse signal  in the presence of noise with variance $\\sigma^2$ when the proposed algorithm is applied.\n\nWe compare our coded compressive sensing algorithm with the following two well-known one-bit compressive sensing algorithms with some modification for a binary signal.\n\\begin{itemize}\n\\item Convex optimization: a variant of the $\\ell_1$-minimization method proposed in \\cite{BP_onebit} for a binary sparse signal, which is summarized in Table \\ref{table1};\n\\!\\!\\item Binary iterative hard thresholding (BIHT): a heuristic algorithm in \\cite{BIHT2013} with some modifications for the binary signal recovery as in step 3) and 4) of Table \\ref{table1}.\n\\end{itemize}\nFor the two modified reference algorithms, we use a Gaussian sensing matrix ${\\bf \\Phi}_{{\\rm G}}\\in \\mathbb{R}^{180 \\times 511}$ whose elements are drawn from IID Gaussian distribution $\\mathcal{N}(0,\\frac{1}{m})$. For each setting of $m$, $n$, $k$, and $\\sigma^2$, we perform the recovery experiment for 500 independent trials, and compute the average of perfect recovery rate.\n\n\n\\begin{table}\n\\center\n\\caption{A Convex Optimization Algorithm for Binary Sparse Signal}\\vspace{-0.2cm}\n\\begin{tabular}{|l|}\n\\hline\\hline\n1)  Initialization:\\\\\n\\hspace{5mm} Given $k$, $m$, $n$, $\\sigma^2$, ${\\bf \\Phi}_{{\\rm G}}$, ${\\bf y}$, and $\\mathcal{T}:=\\{\\emptyset\\}$\\\\\n\\hline\n2) Find $\\mathbf{\\hat x}$ solving the following convex optimization problem:\\\\\n\\hspace{30mm}$\\min \\|{\\bf x}\\|_1$ \\\\\n\\hspace{15mm}${\\rm subject}~{\\rm to}~~ ({\\bf \\Phi}_{{\\rm G}}{\\bf x})_i {\\bf y}_i \\geq 0 ~~{\\rm for}~~ i\\in\\{1,\\ldots,m\\},$\\\\\n\\hspace{30mm}$  \\sum_{i=1}^m({\\bf \\Phi}_{{\\rm G}}{\\bf x})_i {\\bf y}_i=m$,\\\\\n\\hspace{30mm}$  {\\bf x}\\geq {\\bf 0}$.\\\\\n3) Select the $k$ largest index in $\\mathbf{\\hat x}$:\\\\\n\\hspace{10mm}  ${\\mathcal T}=: \\arg\\max_{|{\\mathcal T}|=K} \\left\\{|{\\bf \\hat x}|\\right\\}$.\\\\\n4) Binary signal assignment in ${\\mathcal T}$:\\\\\n\\hspace{10mm}  ${\\bf \\hat x}_{\\mathcal T}=: {\\bf 1}$ and ${\\bf \\hat x}_{{\\mathcal T}^c}=: {\\bf 0}$.\\\\\n\\hline\\hline\n\\end{tabular}\\label{table1}\n\\end{table}\n\n\n\n\\begin{figure}\n\\centering \\vspace{-0.2cm}\n\\includegraphics[width=3.1in]{Fig2.pdf} \\vspace{-0.4cm}\\caption{ Coded one-bit compressive sensing for the binary  sparse signal vector ${\\bf x} \\in \\{0,1\\}^n$.} \\label{fig:2} \\vspace{-0.4cm}\n\\end{figure}\n\n\nFig. \\ref{fig:2} plots the perfect recovery probability versus SNR for each algorithm, when $n=511$, $m = 180$, and $k=5$. As can be seen in Fig. \\ref{fig:2}, the proposed method outperforms BIHT significantly in terms of the perfect signal recovery performance. Specifically, BIHT is not capable of recovering the signal with high probability until SNR=12 dB, because there are a lot of sign flips in the measurements due to noise. Whereas the proposed algorithm is robust to noise; thereby it recovers the signal with probability one when SNR is 6 dB above. The convex optimization approach provides a better performance than the other algorithms; yet, it requires the computational complexity order of $\\mathcal{O}(m^2n^3)$, which is much higher than that of the proposed one.\n\n\n\\vspace{-0.1cm}\n \\section{Conclusion}\nIn this paper, we proposed a novel compressive sensing framework with noisy and quantized measurements for integer sparse signals. With this framework we derived the sufficient condition of the perfect recovery as a function of important system parameters. Considering one-bit compressive sensing as a special case, we demonstrated that the proposed algorithm empirically outperforms  the existing greedy recovery algorithm.\n\n\n\\begin{thebibliography}{1}\n\n\n\n\n\n\n\n\n\\bibitem{CandesRombergTao2006}\nE. J. Cand$\\grave{\\rm e}$s, J. Romberg, and T. Tao, \\newblock ``Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information,''\n\\newblock {\\em IEEE Trans. Inf. Theory,} vol. 52, no. 2, pp. 489-509, Feb. 2006.\n\n\n\n\n\n\n\n\\bibitem{Donoho}\nD. L. Donoho, M. Elad, and V. M. Temlyakov, \\newblock ``Stable recovery of sparse overcomplete representations in the presence of noise,''\n\\newblock {\\em IEEE Trans. Inf. Theory,} vol. 52, no. 1, pp. 6-18, Jan. 2006.\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibitem{TroppGilbert2007}\nJ. A. Tropp and A. C. Gilbert, \\newblock ``Signal recovery from random measurements via orthogonal matching pursuit,'' \\newblock {\\em IEEE Trans. Inf. Theory,}  vol. 53, no. 12, pp. 4655-4666, Dec. 2007.\n\n \\bibitem{Needell2010_CoSaMP}\nD. Needell and J. A. Tropp, \\newblock ``CoSaMP: iterative signal recovery from incomplete and inaccurate samples,''\n\\newblock {\\em Commun. ACM,} vol. 53, no. 12,\npp. 93-100, Dec. 2010.\n\n\\bibitem{Dai2009_SP}\nW. Dai and O. Milenkovic, \\newblock ``Subspace pursuit for compressive sensing signal reconstruction,''\n\\newblock {\\em IEEE Trans. Inf. Theory,} vol. 55, no. 5, pp. 2230-2249, May 2009.\n\n\\bibitem{Namyoon2014}\nN. Lee, \\newblock ``MAP support detection for greedy sparse signal recovery algorithms in compressive sensing,''\n\\newblock {\\em Submitted to IEEE Trans. Signal Processing,} Aug. 2015 (Available at:http://arxiv.org/abs/1508.00964).\n\n\n\\bibitem{CandesRombergTao2006_2}\nE. J. Cand$\\grave{\\rm e}$s, J. Romberg, and and T. Tao,\n\\newblock ``Stable signal recovery for incomplete and inaccurate measurements,''\n\\newblock {\\em Commun. Pure Appl. Math.,} vol. 59, pp. 1207-1223, Aug. 2006.\n\n\n\\bibitem{Nazer}\nB. Nazer and M. Gastpar,\n\\newblock ``Compute-and-forward: Harnessing interference through structured codes,''\n\\newblock {\\em IEEE Trans. Inform. Theory,} vol. 57, pp. 6463-6486, Oct. 2011.\n\n\n\n\n\n\n\\bibitem{Draper}\nS. C. Draper and S. Malekpour,\\newblock ``Compressed sensing over finite fields,''\n\\newblock {\\em in Proc. IEEE ISIT 2013,}  Jul. 2009.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibitem{Sun_Goyal_2009}\nZ. Sun and V. K. Goyal, \\newblock ``Quantization for compressed sensing reconstruction,''\n\\newblock {\\em in SAMPTA\u00e2\u0080\u009909, International Conference on Sampling Theory and Applications,}  2009.\n\n\\bibitem{Dai_2009}\nW. Dai, H. V. Pham, and O. Milenkovic, \\newblock ``Distortion-rate functions for quantized compressive sensing,''\n\\newblock {\\em IEEE Information Theory Workshop on Networking and Information Theory,} pp. 171-175,  June 2009.\n\n\n\n\n\n\n\n\\bibitem{BoufounosBaraniuk2008}\nP. T. Boufounos and R. G. Baraniuk, \\newblock ``1-bit compressive sensing,''\n\\newblock {\\em in Proc. of Conference on Information Science and Systems (CISS),} Mar. 2008.\n\n\n\\bibitem{BP_onebit}\nY. Plan and R. Vershynin, \\newblock ``1-bit compressed sensing and sparse logistic regression: A convex programming,'' \\newblock {\\em IEEE Trans. Inform. Theory,} vol. 59, no.1, pp. 482-494, Jan. 2013.\n\n\\bibitem{BIHT2013}\nL. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, \\newblock ``Robust 1- bit compressive sensing via binary stable embeddings of sparse vectors,''\n\\newblock {\\em IEEE Trans. Inform. Theory,} vol. 59, no.4, pp. 2082 - 2102, April 2013.\n\n\n\\bibitem{Haupt_Baraniuk_2011}\nJ. Haupt and R. Baraniuk, \\newblock ``Robust support recovery using sparse compressive sensing matrices,'' \\newblock {\\em in Proc. 45th Annual Conf. on Information Sciences and Systems,} pp. 1-6, Mar. 2011.\n\n\\bibitem{Hong_Caire2011}\nS-N. Hong and G. Caire, \\newblock ``Compute-and-forward strategies for cooperative distributed antenna systems,''\\newblock {\\em IEEE Trans. Inform. Theory,} vol. 59, pp. 5227-5243, Aug. 2013.\n\n\n\\bibitem{Dobrushin} R. L. Dobrushin, ''Asymptotic optimality of group and systematic codes for some channels,\" {\\em Theory of Probability and its\nApplications}, vol. 8, pp. 47-59, 1963.\n\n\n\\bibitem{Ancheta}\nT. Ancheta,\n\\newblock ``Syndrome source coding and its universal generalization,''\n\\newblock {\\em IEEE Trans. Inform. Theory,} vol.22, no.4, pp.432-436, Jul. 1976.\n\n\\bibitem{Vishwanath2013}\nA. K. Das and S. Vishwanath, \\newblock ``On finite alphabet compressive sensing,''\\newblock {\\em in Proc. IEEE Int. Conf. Acoust. Speech Signal Process (ICASSP),}   pp. 5890-5894, Mar. 2013.\n\n\n\n\\bibitem{Arikan}\nE. Arikan, \\newblock ``Channel polarization: A method for constructing capacity achieving codes for symmetric binary-input memoryless channels,''\\newblock {\\em IEEE Trans. Inform. Theory,} vol. 55, pp. 3051-3073, Jul. 2009.\n\n\n\n\n\n\n\\bibitem{Lee_Hong_2016}\nN. Lee and S-N. Hong, \\newblock ``Coded compressive sensing,''\\newblock {\\em in preparation,} 2016.\n\n\n\n\n\\end{thebibliography}\n\n\n\n\n\n", "itemtype": "equation", "pos": 25930, "prevtext": "\nwhere ${\\bf \\tilde n}={\\bf n}+{\\bf \\Phi}({\\bf x}-{\\bf  x}_{{\\rm int}})$ denotes effective noise. Utilizing this modified equation, we are able to apply the proposed coded compressive sensing method to estimate the integer approximation ${\\bf  x}_{{\\rm int}}$. Assuming the non-zero values in ${\\bf x}$ are bounded as  $|{\\bf x}_i| \\leq U$ for some $U\\in\\mathbb{R}^{+}$, we conjecture that the proposed scheme guarantees to recover the sparse signal with a bounded estimation error $\\|{\\bf x}-{\\bf  x}_{{\\rm int}}\\|_2^2\\leq k\\frac{U^2}{p^2}$ with an increased number of measurements than that in Theorem 1. The rigorous proof of this conjecture will be provided in our journal version \\cite{Lee_Hong_2016}.\n\n\n\n\\textbf{Remark 4 (Noiseless one-bit compressive sensing)}: One interesting scenario is that when a one-bit quantizer and a binary signal are used. In the case of noise-free, the number of required measurements for the perfect recovery is lower bounded by\n\n", "index": 49, "text": "\\begin{align}\nm\\geq 2k\\log_2{n}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m\\geq 2k\\log_{2}{n}.\" display=\"inline\"><mrow><mrow><mi>m</mi><mo>\u2265</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mi>n</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]