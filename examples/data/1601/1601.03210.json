[{"file": "1601.03210.tex", "nexttext": "\nwhere $n$ is the number of vertices of the tree and $\\left< k^2 \\right>$ is the mean of the squared degrees of its vertices \\cite{Ferrer2014c}. $C_{max} = 0$ if only if the tree is a star tree \\cite{Ferrer2013d}.\n$C$ denotes the number of crossings of the linear arrangement of a graph in general while  \n$C_{true}$ denotes the number of crossings of the syntactic dependencies of a real sentence. The {\\textbf{{relative number of crossings}}} is $\\bar{C} = C/C_{max}$ or \n$\\bar{C}_{true} = C_{true}/C_{max}$ \\cite{Ferrer2014c}.\n\n$C$ can be expressed as a sum over $Q$, i.e. \n\n", "itemtype": "equation", "pos": 22628, "prevtext": "\n\n\\newtoggle{arxiv}\n\\toggletrue{arxiv}\n\n\\iftoggle{arxiv}{\n\n  \n  \\thispagestyle{plain}\n  \\pagestyle{plain} \n  \\addtolength\\footskip{3em}\n  \\cfoot{\\phantom{x}\\\\\\thepage}\n\n}{\n\n}\n\n\n\n\n\\title{The scarcity of crossing dependencies: a direct outcome of a specific constraint?}\n\n\\iftoggle{arxiv}{\n\n\\author{Carlos G\\'omez-Rodr\\'iguez\\\\\n  LyS Research Group \\\\\n  Departamento de Computaci\\'on \\\\\n  Universidade da Coru\\~{n}a \\\\\n  Campus de Elvi\\~{n}a, s/n \\\\\n  15071 A Coru\\~{n}a, Spain \\\\\n  {\\tt carlos.gomez@udc.es} \\\\\\And\n  Ramon Ferrer-i-Cancho \\\\\n  Complexity and Quantitative Linguistics Lab \\\\\n  LARCA Research Group \\\\\n  Departament de Ci\\`encies de la Computaci\\'o \\\\\n  Universitat Polit\\`ecnica de Catalunya (UPC) \\\\\n  Campus Nord, Edifici Omega\\\\\n  08034 Barcelona, Catalonia, Spain \\\\\n  {\\tt rferrericancho@cs.upc.edu} \\\\}\n\n\n\n\n\n\n\n}{\n\n\\author{First Author \\\\\n  Affiliation / Address line 1 \\\\\n  Affiliation / Address line 2 \\\\\n  Affiliation / Address line 3 \\\\\n  {\\tt email@domain} \\\\\\And\n  Second Author \\\\\n  Affiliation / Address line 1 \\\\\n  Affiliation / Address line 2 \\\\\n  Affiliation / Address line 3 \\\\\n  {\\tt email@domain} \\\\}\n\n}\n\n\\maketitle\n\n\\begin{abstract}\nCrossing syntactic dependencies have been observed to be infrequent in natural language, to the point that \n\nsome syntactic theories and formalisms disregard them entirely.\n\n\nThis leads to the question of whether the scarcity of crossings in languages arises from an independent and specific constraint on crossings.\nWe provide statistical evidence suggesting that this is not the case, as the proportion of dependency crossings in a wide range of natural language treebanks can be accurately estimated by a simple predictor based on the local probability that two dependencies cross given their lengths. \n\nThe relative error of this predictor never exceeds $5\\%$ on average, whereas a baseline predictor assuming a random ordering of the words of a sentence incurs a relative error that is at least 6 times greater.\nOur results suggest that the low frequency of crossings in natural languages is neither originated by hidden knowledge of language nor by the undesirability of crossings {\\em per se}, but as a mere side effect of the principle of dependency length minimization.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\label{introduction_section}\n\n\n\nIt is well known that {\\textbf{{crossing dependencies}}}, those that cross each other when drawn above the words of a sentence, are relatively uncommon in natural language \\cite{lecerf60,hays64}.\nTheoretical accounts of languages like English or Japanese often assume\nthat crossing dependencies are nonexistent or negligible, and disallow them in their descriptions of syntax \\cite{sleator93,hudson07,Tanaka97,KyotoCorpus}. In other languages, like Dutch or German, crossing dependencies have been shown to be necessary to describe certain syntactic phenomena \\cite{shieber1985eac,Bresnan:87}, but they still make up only a small percentage of the dependencies found in treebanks \\cite{Hav07,HamleDT}.\n\nThe issue of the presence and frequency of crossing dependencies in the syntax of natural languages has received considerable attention in the computational linguistics community, as supporting them makes parsing computationally harder. Exact inference for parsing models that are restricted to {\\textbf{{projective}}} dependency structures (i.e., those without crossing dependencies or a covered root node\\footnote{Dependency structures that have no crossing dependencies are called {\\textbf{{planar}}} \\cite{kuhlmann06}, and are a proper superset of projective structures, as they can include analyses with covered root nodes. However, in practice, structures that are planar but not projective are infrequent in treebanks \\cite{GomNiv2013}. Furthermore, if an artificial root node is added at the beginning or end of the sentence, as is common practice in dependency parsing \\cite{BalNiv13}, then planarity and projectivity become equivalent.}) can be solved in polynomial time \\cite{Eisner96,GomCarWei08}. In contrast, in the case of general non-projective dependency trees, the problem is only tractable under strong independence assumptions \\cite{McDonald07}.\n\n\nFor this reason, many dependency parsers are restricted to projective trees, obtaining good accuracy in treebanks of languages like English, Chinese or Japanese, where crossing dependencies do not appear or are extremely rare \\cite{CheMan2014,Iwatate2008}. In languages and corpora where they appear more frequently, one option is to resort to approximate inference \\cite{Martins2013,Choi2013}. However, the fact that even in such corpora crossing dependencies are still infrequent has enabled an alternative: the so-called mildly non-projective parsers, which can do exact inference supporting very restricted patterns of crossing dependencies that, nevertheless, have been shown to have a high empirical coverage in treebanks \\cite{gomez2011cl,CohGomSat2011,Pitler2014}.\n\nIn this context, a question naturally arises: what is the reason for the low frequency of crossing dependencies, consistently observed across languages? Answering this question would deepen our understanding of the conditions under which non-projectivity arises, helping us design better formalisms and algorithms to handle it in syntactic parsing and generation. The traditional answer consists of postulating that there is some kind of grammatical ban on non-projectivity \\cite{sleator93,Tanaka97,KyotoCorpus,Starosta03,Lee04,hudson07}.\nHowever, this position fails to explain many linguistic phenomena such as topicalization, wh-movement, scrambling or extraposition \\cite{Levy2012a,Versley2014}.\n\nAnother option is to assume that crossing dependencies can be grammatical, but only if they follow certain patterns or hard constraints. However, while some mildly non-projective classes of dependency structures have a very good empirical coverage \\cite{kuhlmann06,GomNiv2013}, these proposals still face counterexamples that fall outside the restrictions \\cite{chen2010unavoidable,Bhat2012,ChenMain2014}.\n\n\nFrom the perspective of theoretical linguistics, the grammatical ban on non-projectivity can be interpreted \n\\begin{itemize}\n\\item\nAs a ban set independently from performance considerations, e.g., requiring some hidden parameter to be turned. In this case the ban can be seen as avoidable (e.g., it depends on whether the parameter is on or off for each given language). \n\\item\nAs a consequence of performance constraints associated directly to crossing dependencies. The ban would be inevitable if the cognitive pressures were strong enough but then it would not be properly a ban (a norm added on top of human cognition) but rather a side-effect of cognitive constraints. This view is challenged by psychological and graph theoretic research indicating that crossing dependencies can be easier to process ({\\newcite}{deVries2012a} and {\\newcite}{Ferrer2014f} and references therein).\n\\end{itemize}  \nSome researchers have adopted an apparently neutral position concerning the nature of the ban but assume that the low frequency of crossings derives from an independent and specific constraint on crossings: explicitly when postulating a principle of minimization of crossings \\cite{Liu2008a} or implicitly in a large body of research on dependency length minimization that takes for granted that syntactic dependencies should not cross \\cite{Liu2008a,Park2009a,GildeaTemperley10,Futrell2015a,Gulordava2015}. \n\n\n\nIf it turned out that non-crossing dependencies can be explained as a side-effect of some cognitive pressure that is not directly associated to crossings (e.g., dependency length minimization), could all these views be regarded as really neutral regarding the nature of the ban?  \n\nIn this article, we explore a simpler hypothesis: that in order to explain the scarcity of crossing dependencies in language, it is not necessary to assume any underlying rule or principle of human languages that is responsible directly for this fact (including the possibility of some cognitive cost associated directly to crossings). \n\nInstead, the low frequency of crossings naturally arises, indirectly, from the actual length of dependencies, which are constrained by a well-known  psychological principle: dependency length minimization (see {\\newcite}{Ferrer2013e} or {\\newcite}{Tily2010a} for a review). \n\n\nTo this end, we provide statistical evidence that the amount of dependency crossings in a wide range of treebanks can be predicted with small error by a simple estimator based exclusively on dependency length information and information on which edges can potentially cross. The estimator consistently delivers good predictions of the number of crossings, \n\nin two different collections of dependency treebanks with diverse annotation criteria. We will argue that this is the best explanation for the low frequency of crossings when both psychological plausibility and parsimony at all levels (from a model of crossings to a general theory of language) are required. \n\n\n\nThe remainder of the article is organized as follows. Section \\ref{predictors_section} discusses various ways in which the crossings of a sentence could be predicted. \nSection \\ref{crossing_theory_section} presents the predictor of crossings chosen for this article and its theoretical background. The dependency trees used to test the predictor are presented in Section \\ref{methods_section}. Section \\ref{results_section} shows the results of the predictions, and Section \\ref{discussion_section} discusses some implications for computational linguistics and linguistic theory.    \n\n\n\n\\section{Possible predictors}\n\n\\label{predictors_section}\n\nHere we will examine various possibilities to predict the number of dependency crossings in a sentence. \nThe problem of the origins of non-crossing dependencies can be recast as problem of modeling: we want to find the best model\nfor predicting the number of crossings in a sentence. According to modern model selection, the best model is the one that has the best trade-off between quality of fit (predictive power) and parsimony \\cite{Burnham2002a}. However, we will be more ambitious, involving further requirements: \n\\begin{itemize}\n\\item\nThe model must be psychologically realistic. A model that assumes orderings of words that are hard to produce by the human brain should be penalized with respect to one that is based on orderings that real speakers produce (or can rather easily produce). We are not only simply concerned about predicting the low number of crossings of a sentence but also understanding why that number is that low. Hiding the problem under the carpet of grammar or an ad-hoc principle of projectivity does not help.\n\\item  \nIts assumptions must be valid. The predictions of a model may be compatible with real data and even be of high quality but its assumptions may not be supported by real data.  \n\\item \nWe are not only concerned about the best model in a local sense but one that leads a general theory of word order or even a comprehensive theory of language that is compact. A real scientific theory is more than a collection of disconnected ideas or models \\cite{Bunge2001a_French}.\nModels that lead to an unnecessarily fat general theory when integrated into it should also be penalized. \nModels that exploit assumptions from successful models in other domains should be favored. For instance, a model that allows one to understand not only the scarcity of crossings but also why adjectives tend to be placed after the subject in SOV languages is preferable to one that requires an independent solution to explain the placement of adjectives \\cite{Ferrer2014f}. \n\\end{itemize}\n\nIn what follows, we will use $C$ to refer to the number of dependency crossings in the parse of a sentence (i.e., the number of pairs of syntactic dependencies that cross). Our goal is, therefore, to find a suitable predictor for $C$. For this purpose we will use the notion of the {\\textbf{{length}}} of a dependency, which is defined as the linear distance between the words that are connected: adjacent words are at distance 1, words separated by one word are at distance 2, and so on \\cite{Ferrer2004b}. The sum of the lengths of all dependencies in a sentence will be denoted by $D$. A {\\textbf{{star tree}}} is a tree with a vertex of maximum degree and $C = 0$ \\cite{Ferrer2013d}.\n\n\\subsection{Minimization of crossings}\n\nA principle of minimization of crossings \\cite{Liu2008a} leads to a simple deterministic predictor: $C = 0$, reflecting a grammatical ban on non-projectivity \\cite{sleator93,Tanaka97,KyotoCorpus,Starosta03,Lee04,hudson07}.\n\nThis predictor is problematic for various reasons:\n\\begin{itemize}\n\\item\nConcerning the validity of its assumptions, the model assumes that $C = 0$ independently from $D$, while $C$ and $D$ are positively correlated in many languages \\cite{Ferrer2015c}. \n\\item\nConcerning the accuracy of its predictions, this model fails to explain many linguistic phenomena like topicalization, wh-movement, scrambling or extraposition \\cite{Levy2012a,Versley2014}. In many languages, sentences with $C > 0$ are found and \nthen the likelihood of the model is minimum, which indicates that the model is among the worst possible models for crossing dependencies according to modern model selection \\cite{Burnham2002a} because its likelihood is zero. \nBesides, this predictor gives $C=0$ regardless of the value of $D$, failing to account for the positive correlation between $C$ and $D$ \\cite{Ferrer2015c}.    \n\\item\nIts psychological validity is unclear. If the model is interpreted as arising from processing difficulties inherent to crossing dependencies \\cite{Levy2012a} or computational tractability (as reviewed in Section \\ref{introduction_section}) then it is challenged by psychological and graph theoretic research indicating that sentences with $C >0$ can be easier to process than sentences with $C= 0$ (see {\\newcite}{deVries2012a}, {\\newcite}{Ferrer2014f}, {\\newcite}{Chung1978a} and references therein). Another problem is how a language generation process could warrant that $C=0$. If $C=0$ is determined before the sentence is produced, how is it possible that sentence production does not introduce (many) crossings? Crossing theory indicates that a star tree is needed to keep a low number of crossings \\cite{Ferrer2014f}. \nIf $C=0$ is determined while the sentence is produced (linearized), how are crossings avoided on the fly as real language production is not a batch process \\cite{Christiansen2015a}? It looks simpler to consider that non-crossing dependencies are a side-effect of a principle of dependency length minimization \\cite{Ferrer2006d,Ferrer2014c,Ferrer2014f}.\n\\item\nConcerning the compactness of the whole theory, the model $C = 0$ leads to a fatter theory of language because the scarcity of crossings and also the positive correlation between $D$ and $C$ could be explained to a large extent recycling the highly predictive principle of dependency length minimization \\cite{Ferrer2013e}, as we will see below. \n\\end{itemize}\n\n\nAnother option is to assume that crossing dependencies can be grammatical, but only if they follow certain patterns or hard constraints. However, while some mildly non-projective classes of dependency structures have a very good empirical coverage \\cite{kuhlmann06,GomNiv2013}, these proposals still face counterexamples that fall outside the restrictions \\cite{chen2010unavoidable,Bhat2012,ChenMain2014}.\nOne possibility is to relax the simple deterministic predictor above so that on average $C = \\alpha(n)$, where $\\alpha$ is a constant for sentences of a given length $n$ \\cite{Ferrer2015c}. \nThis allows one to capture the variation in the number of crossings across languages but is problematic for the reasons of the case $\\alpha(n) = 0$ that we have examined above. Further arguments can be found in Section 4.3 of {\\newcite}{Ferrer2014f}. \n\n\\subsection{Minimum linear arrangement}\n\nA minimum linear arrangement of a sentence is an ordering of the words of the sentence that minimizes the sum of dependency lengths.\n\nOne may predict the assumed number of crossings by calculating the minimum linear arrangements of a sentence \\cite{Ferrer2006d}. A possible predictor could be the mean number of crossings over all those arrangements. \n\n\nThe predictive power of the model is supported by the fact that solving the minimum linear arrangement problem reduces crossings to practically zero  \\cite{Ferrer2006d}, as in many languages. A potential problem of this model is that it has never been checked whether it predicts the actual number of crossings of real sentences, as far as we know.  \n\nPerhaps the major challenge for this predictor is the validity of the assumption of a minimum linear arrangement because:\n\\begin{itemize}\n\\item\nThe actual value of $D$ in real sentences is located between the minimum and that of a random ordering of vertices \\cite{Ferrer2004b,Ferrer2013c}. The ratio $\\Gamma = D/D_{min}$ (where $D_{min}$ is the minimum value of $D$) is greater than 1.2 in Romanian for sufficiently long sentences \\cite{Ferrer2004b} and a similar lower bound on language efficiency has been found in English across centuries \\cite{Tily2010a}. \n\\item\nIt may not be valid also for theoretical reasons: word order is a multiconstraint satisfaction problem where the principle of dependency length minimization is in conflict with  other word order constraints \\cite{Ferrer2014a,Futrell2015a}.\nThus, a model based on minimum linear arrangements is not that simple: it has to explain why dependency length minimization dominates fully over other principles or provide evidence that the distortion caused by other principles can be neglected. Below we will present a model that does not have this problem because it works on true dependency lengths, which are expected to be determined by the interplay between dependency length minimization and other principles.  \n\\item\nThe full minimization of $D$ is cognitively unrealistic, as it is incompatible with the predictions of the now-or-never bottleneck \\cite{Christiansen2015a}. As for the latter, notice that the minimization of $D$ implies that the whole sentence must be available as input for some minimum linear arrangement algorithm, whereas actual language generation and processing is intrinsically online and heavily constrained by our fleeting memory \\cite{Christiansen2015a}.\n\\end{itemize}\n\n\n\\subsection{Random linear arrangement}\n\nIf the minimum linear arrangement is too restrictive, one could consider the opposite:  \npredicting the number of crossings assuming a random ordering of the words of the sentence \\cite{Ferrer2014f}. However, this is cognitively unrealistic: even in languages with high word order flexibility, word order is constrained \\cite{wals-81,Lester2015a}.\nThe assumption that the ordering of sentences is arbitrary (unconstrained) is easily rejected by the fact that dependency lengths are below chance in real languages \\cite{Ferrer2004b,Ferrer2013c,Futrell2015a}. \nFurthermore, a random linear arrangement cannot explain the low numbers of crossings observed in real sentences, e.g., a constant low number of crossings requires a star tree \\cite{Ferrer2014f}. Thus, this predictor is only useful as a random baseline for other predictors. Here we will compare it against a better predictor that is introduced next. \n  \n\\subsection{Random linear arrangement with some knowledge about dependency lengths}\n\nA stronger predictor can be built by focusing on the set of pairs of edges that may potentially cross and basing predictions on the actual length of the edges under the assumption of a random arrangement of the four vertices that are potentially involved in an edge crossing \\cite{Ferrer2014c}. So far, its predictive power is supported by its capacity to predict the actual number of crossings in random trees with an error of about $5\\%$ \\cite{Ferrer2014c}.\nA crucial goal of the present article is to test the accuracy of its predictions on real sentences. \n\nConcerning assumptions, this model is simpler than the model based on minimum linear arrangements: this model does not assume an unrealistic ordering of the elements of the sentence but the true ordering. This predictor does not assume the principle of dependency length minimization but in case that it predicts the actual number of crossings of real sentences accurately, it success can be easily interpreted as a consequence of the principle of dependency length minimization because actual dependency lengths are below chance (below $n/3$ approximately), a domain where the probability theory behind this model indicates that a reduction of dependency length yields a reduction in the probability that two edges cross \\cite{Ferrer2014f,Ferrer2014c}. This predictor is fully compatible with the positive correlation between $D$ and $C$ \\cite{Ferrer2015c}, in contrast with the deterministic predictor ($C=0$) and its generalization. \n\nIts psychological validity is greater than that of the minimum linear arrangement predictor because it can base its prediction on information from sentences that have actually been produced by a human brain. Contrary to the minimum linear arrangement predictor, this model bases its prediction on true dependency lengths instead of ideal values of $D$.\nHowever, it can be argued that a fundamental assumption of the model, namely that edges are arranged linearly at random, is not supported empirically, for the reasons provided against the random linear arrangement predictor. This is a totally fair criticism, but for this reason this model should be regarded as a null hypothesis rather than as a full realistic model. However, modeling requires a compromise between quality of fit, adequacy and parsimony. If this null hypothesis model provides predictions of sufficient quality on real sentences, do we really need to worry about providing a more realistic but also more complicated model? In the worst case, this predictor is an inevitable baseline for an alternative model. \n  \n\n\n\n\n\n\\section{Crossing theory}\n\n\\label{crossing_theory_section}\n\nHere we provide a quick overview of a crossing theory developed in a series of articles \\cite{Ferrer2013b,Ferrer2013d,Ferrer2014c,Ferrer2014f}. A central concept of crossing theory is $Q$, the set of pairs of edges of a graph that can potentially cross when their vertices are arranged linearly in some arbitrary order (edges sharing a vertex cannot cross). The cardinality of $Q$ is $C_{max}$, the {\\textbf{{potential number of crossings}}}. In a tree, one has \n\n", "index": 1, "text": "\\begin{equation}\nC_{max} = n(n - 1 - \\left< k^2 \\right>)/2,\n\\label{potential_number_of_crossings_equation}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"C_{max}=n(n-1-\\left&lt;k^{2}\\right&gt;)/2,\" display=\"block\"><mrow><mrow><msub><mi>C</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>=</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn><mo>-</mo><mrow><mo>\u27e8</mo><msup><mi>k</mi><mn>2</mn></msup><mo>\u27e9</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mn>2</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03210.tex", "nexttext": " \nwhere $C(e_1, e_2)$ is an indicator variable, $C(e_1, e_2)$ = 1 if the edges $e_1$ and $e_2$ cross and $C(e_1, e_2) = 0$ otherwise.  \nThe simplest prediction about $C$ than can be made departs from the null hypothesis that the vertices are arranged linearly at random (all possible orderings are equally likely). Following Eq. \\ref{number_of_crossings_equation}, the expected number of crossings under that null hypothesis turns out to be \n\\begin{eqnarray}\nE_0[C] & = & \\sum_{(e_1, e_2) \\in Q} E[C(e_1, e_2)]\\\\\n       & = & \\sum_{(e_1, e_2) \\in Q} p(C(e_1, e_2) = 1), \n\\label{expected_number_of_crossings_equation}\n\\end{eqnarray}\nwhere $p(C(e_1, e_2) = 1)$ is the probability that the edges $e_1$ and $e_2$ cross knowing that they belong to $Q$. \nUnder that null hypothesis, the probability that two edges of $Q$ cross is constant, i.e. $p(C(e_1, e_2) = 1) = 1/3$, yielding \\cite{Ferrer2013d} $E_0[C] = C_{max}/3$.\n\n\nThe prediction offered by $E_0[C]$ can be improved by introducing knowledge about the length of the dependencies (edges of length 1 or $n - 1$ are not crossable). The predictor \n$E_2[C]$ is obtained when $p(C(e_1, e_2) = 1)$ is replaced by $p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$, the probability that two arbitrary edges of $Q$ cross, knowing that their lengths are $d(e_1)$ and $d(e_2)$, in Eq. \\ref{expected_number_of_crossings_equation}, yielding \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $n$ is the number of vertices of the tree and $\\left< k^2 \\right>$ is the mean of the squared degrees of its vertices \\cite{Ferrer2014c}. $C_{max} = 0$ if only if the tree is a star tree \\cite{Ferrer2013d}.\n$C$ denotes the number of crossings of the linear arrangement of a graph in general while  \n$C_{true}$ denotes the number of crossings of the syntactic dependencies of a real sentence. The {\\textbf{{relative number of crossings}}} is $\\bar{C} = C/C_{max}$ or \n$\\bar{C}_{true} = C_{true}/C_{max}$ \\cite{Ferrer2014c}.\n\n$C$ can be expressed as a sum over $Q$, i.e. \n\n", "index": 3, "text": "\\begin{equation}\nC = \\sum_{(e_1, e_2) \\in Q} C(e_1, e_2),\n\\label{number_of_crossings_equation}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"C=\\sum_{(e_{1},e_{2})\\in Q}C(e_{1},e_{2}),\" display=\"block\"><mrow><mrow><mi>C</mi><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>Q</mi></mrow></munder><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03210.tex", "nexttext": "\n$p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$ depends only on $n$, $d(e_1)$ and $d(e_2)$ (see {\\newcite}{Ferrer2014c} for further details about $p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$). \n\nAlthough $E_0[C]$ and $E_2[C]$ are predictors of $C$ that have the same mathematical structure (they are sums of probabilities over pairs of edges of $Q$), $E_0[C]$ is a true expectation while $E_2[C]$ is not. \n\nThe {\\textbf{{relative error}}} of a predictor is defined as \\cite{Ferrer2014c}\n\n", "itemtype": "equation", "pos": 24804, "prevtext": " \nwhere $C(e_1, e_2)$ is an indicator variable, $C(e_1, e_2)$ = 1 if the edges $e_1$ and $e_2$ cross and $C(e_1, e_2) = 0$ otherwise.  \nThe simplest prediction about $C$ than can be made departs from the null hypothesis that the vertices are arranged linearly at random (all possible orderings are equally likely). Following Eq. \\ref{number_of_crossings_equation}, the expected number of crossings under that null hypothesis turns out to be \n\\begin{eqnarray}\nE_0[C] & = & \\sum_{(e_1, e_2) \\in Q} E[C(e_1, e_2)]\\\\\n       & = & \\sum_{(e_1, e_2) \\in Q} p(C(e_1, e_2) = 1), \n\\label{expected_number_of_crossings_equation}\n\\end{eqnarray}\nwhere $p(C(e_1, e_2) = 1)$ is the probability that the edges $e_1$ and $e_2$ cross knowing that they belong to $Q$. \nUnder that null hypothesis, the probability that two edges of $Q$ cross is constant, i.e. $p(C(e_1, e_2) = 1) = 1/3$, yielding \\cite{Ferrer2013d} $E_0[C] = C_{max}/3$.\n\n\nThe prediction offered by $E_0[C]$ can be improved by introducing knowledge about the length of the dependencies (edges of length 1 or $n - 1$ are not crossable). The predictor \n$E_2[C]$ is obtained when $p(C(e_1, e_2) = 1)$ is replaced by $p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$, the probability that two arbitrary edges of $Q$ cross, knowing that their lengths are $d(e_1)$ and $d(e_2)$, in Eq. \\ref{expected_number_of_crossings_equation}, yielding \n\n", "index": 5, "text": "\\begin{equation}\nE_2[C] = \\sum_{(e_1, e_2) \\in Q} p(C(e_1, e_2) = 1 | d(e_1), d(e_2)),\n\\label{predicted_number_of_crossings_equation}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"E_{2}[C]=\\sum_{(e_{1},e_{2})\\in Q}p(C(e_{1},e_{2})=1|d(e_{1}),d(e_{2})),\" display=\"block\"><mrow><msub><mi>E</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">[</mo><mi>C</mi><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>Q</mi></mrow></munder><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03210.tex", "nexttext": "\n$\\Delta_0$ will be used as a baseline for $\\Delta_2$. Interestingly,\n$\\Delta_0$ converges to $1/3$ for sufficiently long sentences when $C_{true}$ is small \\cite{Ferrer2014c}. \n \n\n\n\n\n\n   \n\n\n\n\n\n \n \n\n \n \n\n\n\n\\section{Resources and methods}\n\n\\label{methods_section}\n\nWe considered the corpora in version 2.0 of the HamleDT collection of treebanks \\cite{HamleDTJournal,HamledTStanford}. This collection is a harmonization of existing treebanks for 30 different languages into two well-known annotation styles: Prague dependencies \\cite{PDT20} and Universal Stanford dependencies \\cite{UniversalStanford}. Therefore, using this collection allows us to evaluate our predictions of crossings both across a wide range of languages and two popular annotation schemes. The latter is interesting because observations like the number of dependency crossings present in treebank sentences do not only depend on the properties of languages themselves, but also on annotation criteria ({\\newcite}{Ferrer2015c} list some examples of how annotation criteria may affect $C$).\n\nEach of the syntactic dependency structures in the treebanks was preprocessed by removing nodes corresponding to punctuation tokens, as is standard in research related to dependency length (e.g. {\\newcite}{Ferrer2004b} and {\\newcite}{Futrell2015a}), which is only concerned with dependencies between actual words. To preserve the syntactic structure of the rest of the nodes, non-punctuation nodes that had a punctuation node as their head were attached as dependents of their nearest non-punctuation ancestor. Null elements, which appear in the Bengali, Hindi and Telugu corpora, were also subject to the same treatment as punctuation.\n\nAfter this preprocessing, a syntactic dependency structure was included in our analyses if (1) it defined a tree and (2) the tree was not a star tree. \n\nThe reason for (1) is that our predictors of the number of crossings assume a tree structure \\cite{Ferrer2013d,Ferrer2014c}. \nThe reason for (2) is that crossings are impossible in a star tree \\cite{Ferrer2013b}. Condition (2) implies that the syntactic dependency structure has at least four vertices (otherwise all the possible trees are star trees). By excluding star trees we are discarding trees where the prediction cannot fail.  \nAn additional reason for excluding star trees is that their relative number of crossings, $C/C_{max}$, is not defined because\n$C = C_{max} = 0$.\n\nTable \\ref{relative_table} shows the number of sentences in the original treebanks and the number of sentences actually included in our analyses, after filtering by the criteria (1) and (2) above. The average number of crossings per sentence does not exceed 1 for most of the treebanks. \n \n\n\n\\section{Results}\n\n\\label{results_section}\n\n\nTable \\ref{relative_table} shows that the average $\\Delta_2$, the relative error of the predictor $E_2[C]$,\nis small: it does not exceed $5\\%$. Thus, the average $\\Delta_2$ is at least 6 times smaller than the baseline $\\Delta_0 \\approx 30\\%$. \nThe averages presented in Table \\ref{relative_table} have been produced mixing measurements from sentences of different lengths. This is potentially problematic because the results might be heavily determined by the distribution of sentence lengths \\cite{Ferrer2013c}. \nTo control for sentence length, sentences were grouped by length and the average $\\Delta_2$ was computed for the sentences within each group. Table \\ref{relative_grouping_by_sentence_length_table} summarizes the statistical properties over the average $\\Delta_2$ of each group. Interestingly, the average over group averages of $\\Delta_2$ decreases with respect to the previous analysis: it does not exceed $4.3\\%$. Thus, the average $\\Delta_2$ is at least 7 times smaller than the baseline error, again $\\Delta_0 \\approx 30\\%$. The minimum size of a group is one sentence; the qualitative results are very similar if the minimum size is set to 2.\n    \n\n\\newcommand{\\specialcell}[2][c]{   \\begin{tabular}[#1]{@{}c@{}}#2\\end{tabular}}\n\n\\begin{table*}\n\\caption{\\label{relative_table} Summary of results for each treebank: number of sentences before and after filtering, average values of $C_{true}$ and $\\Delta_0$, and average, median and standard deviation of the relative error $\\Delta_2$, over the trees of each treebank. \nRomanian (Prague) is the only treebank with no crossing dependencies.}\n\\begin{scriptsize}\n\\begin{center}\n\\begin{tabular}{lrrrrrrrr} \\hline\nTreebank & \\#Sent & \\specialcell{\\#Sent\\\\ \\footnotesize{(filtered)}} &\n\\specialcell{$C_{true}$\\\\ \\footnotesize{(avg.)}} & \\specialcell{$\\Delta_0$\\\\ \\footnotesize{(avg.)}} & \\specialcell{$\\Delta_2$\\\\ \\footnotesize{(avg.)}} & \\specialcell{$\\Delta_2$\\\\ \\footnotesize{(median)}} & \\specialcell{$\\Delta_2$\\\\ \\footnotesize{(st. dev.)}} \\\\ \\hline\n\\multicolumn{2}{l}{\\emph{Stanford annotation}} & \\phantom{x} & \\phantom{x} & \\phantom{x} & \\phantom{x} & \\phantom{x} \\\\ \\hline\nArabic & 7547 & 2280 & 0.9807 & 0.328 & 0.019 & 0.016 & 0.021 \\\\ \nBasque & 11225 & 9072 & 0.1391 & 0.330 & 0.028 & 0.022 & 0.033 \\\\ \nBengali & 1129 & 678 & 0.1062 & 0.321 & 0.027 & 0.000 & 0.051 \\\\ \nBulgarian & 13221 & 12119 & 0.3598 & 0.326 & 0.045 & 0.039 & 0.042 \\\\ \nCatalan & 14924 & 14520 & 0.6419 & 0.331 & 0.034 & 0.029 & 0.023 \\\\ \nCzech & 87913 & 74843 & 0.5277 & 0.326 & 0.040 & 0.035 & 0.035 \\\\ \nDanish & 5512 & 4894 & 0.6800 & 0.324 & 0.047 & 0.040 & 0.038 \\\\ \nDutch & 13735 & 10974 & 1.3980 & 0.311 & 0.046 & 0.041 & 0.051 \\\\ \nEnglish & 18791 & 18275 & 0.5241 & 0.330 & 0.049 & 0.043 & 0.031 \\\\ \nEstonian & 1315 & 851 & 0.0376 & 0.331 & 0.016 & 0.000 & 0.037 \\\\ \nFinnish & 4307 & 4078 & 0.3183 & 0.326 & 0.034 & 0.028 & 0.038 \\\\ \nGerman & 38020 & 33492 & 0.7826 & 0.325 & 0.050 & 0.046 & 0.036 \\\\ \nGreek & 2902 & 2584 & 0.6540 & 0.330 & 0.039 & 0.033 & 0.028 \\\\ \nAnc. Greek & 21173 & 18713 & 3.2621 & 0.244 & 0.030 & 0.027 & 0.058 \\\\ \nHindi & 13274 & 12417 & 0.3043 & 0.332 & 0.027 & 0.025 & 0.017 \\\\ \nHungarian & 6424 & 6103 & 0.9720 & 0.326 & 0.036 & 0.031 & 0.033 \\\\ \nItalian & 3359 & 2502 & 0.4153 & 0.329 & 0.035 & 0.029 & 0.032 \\\\ \nJapanese & 17753 & 4614 & 0.1641 & 0.326 & 0.024 & 0.019 & 0.032 \\\\ \nLatin & 3473 & 3036 & 2.1785 & 0.282 & 0.034 & 0.031 & 0.046 \\\\ \nPersian & 12455 & 11579 & 0.5914 & 0.326 & 0.027 & 0.023 & 0.031 \\\\ \nPortuguese & 9359 & 8621 & 0.6336 & 0.328 & 0.039 & 0.033 & 0.032 \\\\ \nRomanian & 4042 & 3145 & 0.1021 & 0.331 & 0.028 & 0.021 & 0.036 \\\\ \nRussian & 34895 & 31581 & 0.4171 & 0.326 & 0.038 & 0.032 & 0.035 \\\\ \nSlovak & 57408 & 47727 & 0.4559 & 0.324 & 0.044 & 0.036 & 0.044 \\\\ \nSlovenian & 1936 & 1719 & 0.7749 & 0.322 & 0.047 & 0.039 & 0.046 \\\\ \nSpanish & 15984 & 15354 & 0.6218 & 0.331 & 0.034 & 0.029 & 0.024 \\\\ \nSwedish & 11431 & 10714 & 0.4871 & 0.328 & 0.043 & 0.039 & 0.034 \\\\ \nTamil & 600 & 584 & 0.0240 & 0.333 & 0.026 & 0.022 & 0.025 \\\\ \nTelugu & 1450 & 429 & 0.0140 & 0.322 & 0.016 & 0.000 & 0.045 \\\\ \nTurkish & 5935 & 3862 & 0.0984 & 0.330 & 0.031 & 0.025 & 0.038 \\\\\n\\hline\n\\multicolumn{2}{l}{\\emph{Prague annotation}} & \\phantom{x} & \\phantom{x} & \\phantom{x} & \\phantom{x} & \\phantom{x} \\\\ \\hline\nArabic & 7547 & 2248 & 0.0881 & 0.333 & 0.013 & 0.010 & 0.016 \\\\ \nBasque & 11225 & 8717 & 0.1252 & 0.330 & 0.026 & 0.021 & 0.029 \\\\ \nBengali & 1129 & 651 & 0.1244 & 0.320 & 0.025 & 0.000 & 0.052 \\\\ \nBulgarian & 13221 & 11947 & 0.1248 & 0.329 & 0.023 & 0.017 & 0.029 \\\\ \nCatalan & 14924 & 14556 & 0.0873 & 0.333 & 0.020 & 0.017 & 0.016 \\\\ \nCzech & 87913 & 70023 & 0.3729 & 0.327 & 0.031 & 0.025 & 0.031 \\\\ \nDanish & 5512 & 4840 & 0.1643 & 0.331 & 0.027 & 0.022 & 0.027 \\\\ \nDutch & 13735 & 11131 & 0.9898 & 0.315 & 0.034 & 0.027 & 0.041 \\\\ \nEnglish & 18791 & 18369 & 0.1072 & 0.333 & 0.034 & 0.029 & 0.024 \\\\  \nEstonian & 1315 & 843 & 0.0130 & 0.332 & 0.013 & 0.000 & 0.031 \\\\ \nFinnish & 4307 & 4011 & 0.1279 & 0.330 & 0.028 & 0.024 & 0.031 \\\\ \nGerman & 38020 & 32443 & 0.7230 & 0.326 & 0.043 & 0.039 & 0.033 \\\\ \nGreek & 2902 & 2543 & 0.2057 & 0.332 & 0.030 & 0.024 & 0.023 \\\\ \nAnc. Greek & 21173 & 16237 & 3.3528 & 0.243 & 0.025 & 0.020 & 0.058 \\\\ \nHindi & 13274 & 12334 & 0.3875 & 0.330 & 0.015 & 0.012 & 0.015 \\\\ \nHungarian & 6424 & 5047 & 0.8675 & 0.326 & 0.034 & 0.030 & 0.032 \\\\ \nItalian & 3359 & 2398 & 0.0621 & 0.333 & 0.020 & 0.014 & 0.024 \\\\ \nJapanese & 17753 & 4792 & 0.0002 & 0.333 & 0.006 & 0.000 & 0.013 \\\\ \nLatin & 3473 & 2833 & 1.8503 & 0.286 & 0.036 & 0.032 & 0.047 \\\\ \nPersian & 12455 & 11632 & 0.4024 & 0.329 & 0.030 & 0.024 & 0.033 \\\\ \nPortuguese & 9359 & 8596 & 0.2465 & 0.331 & 0.021 & 0.016 & 0.021 \\\\ \nRomanian & 4042 & 3193 & 0.0000 & 0.333 & 0.015 & 0.005 & 0.026 \\\\ \nRussian & 34895 & 31900 & 0.1570 & 0.330 & 0.027 & 0.021 & 0.028 \\\\ \nSlovak & 57408 & 44297 & 0.2688 & 0.326 & 0.034 & 0.026 & 0.039 \\\\ \nSlovenian & 1936 & 1581 & 0.3125 & 0.327 & 0.035 & 0.027 & 0.038 \\\\ \nSpanish & 15984 & 15424 & 0.1105 & 0.333 & 0.020 & 0.016 & 0.017 \\\\\nSwedish & 11431 & 10207 & 0.1946 & 0.332 & 0.034 & 0.029 & 0.029 \\\\ \nTamil & 600 & 585 & 0.0137 & 0.333 & 0.023 & 0.019 & 0.023 \\\\ \nTelugu & 1450 & 373 & 0.0080 & 0.325 & 0.014 & 0.000 & 0.043 \\\\ \nTurkish & 5935 & 3518 & 0.1373 & 0.327 & 0.015 & 0.000 & 0.026 \\\\ \n\\hline \n\\end{tabular}\n\\end{center}\n\\end{scriptsize}\n\\end{table*}\n\n\n\\begin{table*}\n\\caption{\\label{relative_grouping_by_sentence_length_table} \nSummary of results for each treebank: number of distinct sentence lengths, \naverage $\\Delta_0$ and, average, median and standard deviation of the average values of $\\Delta_2$ over the groups of sentences with the same length.}\n\\begin{scriptsize}\n\\begin{center}\n\\begin{tabular}{lrrrrrr} \\hline\nTreebank & \\#Lengths & \\specialcell{$\\Delta_0$ \\\\ \\footnotesize{(avg.)}} & \\specialcell{$\\Delta_2$ \\\\ \\footnotesize{(avg.)}} & \\specialcell{$\\Delta_2$ \\\\ \\footnotesize{(median)}} & \\specialcell{$\\Delta_2$ \\\\ \\footnotesize{(st. dev.)}} \\\\ \\hline\n\\multicolumn{2}{l}{\\emph{Stanford annotation}} & \\phantom{x} & \\phantom{x} & \\phantom{x}  \\\\ \\hline\nArabic & 109 & 0.331 & 0.014 & 0.013 & 0.006 \\\\ \nBasque & 35 & 0.331 & 0.026 & 0.022 & 0.017 \\\\ \nBengali & 18 & 0.322 & 0.034 & 0.028 & 0.026 \\\\ \nBulgarian & 64 & 0.330 & 0.029 & 0.027 & 0.012 \\\\ \nCatalan & 98 & 0.332 & 0.023 & 0.021 & 0.008 \\\\ \nCzech & 88 & 0.330 & 0.024 & 0.022 & 0.010 \\\\ \nDanish & 66 & 0.328 & 0.031 & 0.029 & 0.013 \\\\ \nDutch & 54 & 0.319 & 0.037 & 0.035 & 0.019 \\\\ \nEnglish & 74 & 0.331 & 0.033 & 0.031 & 0.013 \\\\ \nEstonian & 25 & 0.331 & 0.036 & 0.032 & 0.020 \\\\  \nFinnish & 41 & 0.329 & 0.028 & 0.025 & 0.016 \\\\ \nGerman & 85 & 0.328 & 0.033 & 0.032 & 0.012 \\\\ \nGreek & 75 & 0.331 & 0.027 & 0.024 & 0.010 \\\\ \nAnc. Greek & 66 & 0.293 & 0.025 & 0.024 & 0.019 \\\\ \nHindi & 69 & 0.332 & 0.020 & 0.018 & 0.008 \\\\ \nHungarian & 65 & 0.328 & 0.027 & 0.025 & 0.014 \\\\ \nItalian & 69 & 0.331 & 0.024 & 0.022 & 0.010 \\\\ \nJapanese & 44 & 0.330 & 0.021 & 0.020 & 0.010 \\\\ \nLatin & 59 & 0.309 & 0.031 & 0.029 & 0.018 \\\\ \nPersian & 93 & 0.329 & 0.023 & 0.021 & 0.009 \\\\\nPortuguese & 88 & 0.331 & 0.024 & 0.023 & 0.009 \\\\ \nRomanian & 46 & 0.332 & 0.023 & 0.021 & 0.011 \\\\ \nRussian & 80 & 0.330 & 0.024 & 0.022 & 0.010 \\\\ \nSlovak & 92 & 0.330 & 0.024 & 0.022 & 0.010 \\\\ \nSlovenian & 57 & 0.326 & 0.034 & 0.032 & 0.016 \\\\ \nSpanish & 95 & 0.332 & 0.023 & 0.022 & 0.009 \\\\ \nSwedish & 74 & 0.329 & 0.028 & 0.026 & 0.011 \\\\ \nTamil & 40 & 0.333 & 0.023 & 0.020 & 0.011 \\\\ \nTelugu & 10 & 0.330 & 0.043 & 0.030 & 0.030 \\\\ \nTurkish & 51 & 0.332 & 0.030 & 0.027 & 0.013 \\\\\n\\hline\n\\multicolumn{2}{l}{\\emph{Prague annotation}} & \\phantom{x} & \\phantom{x} & \\phantom{x} \\\\ \\hline\nArabic & 109 & 0.333 & 0.010 & 0.008 & 0.005 \\\\ \nBasque & 35 & 0.331 & 0.024 & 0.021 & 0.015 \\\\ \nBengali & 17 & 0.321 & 0.034 & 0.027 & 0.030 \\\\ \nBulgarian & 63 & 0.332 & 0.016 & 0.014 & 0.009 \\\\ \nCatalan & 98 & 0.333 & 0.014 & 0.012 & 0.006 \\\\ \nCzech & 87 & 0.331 & 0.019 & 0.017 & 0.009 \\\\ \nDanish & 66 & 0.332 & 0.019 & 0.017 & 0.010 \\\\\nDutch & 54 & 0.323 & 0.027 & 0.024 & 0.016 \\\\ \nEnglish & 75 & 0.333 & 0.023 & 0.021 & 0.010 \\\\ \nEstonian & 25 & 0.332 & 0.032 & 0.028 & 0.019 \\\\ \nFinnish & 41 & 0.331 & 0.024 & 0.021 & 0.013 \\\\ \nGerman & 85 & 0.329 & 0.029 & 0.027 & 0.011 \\\\ \nGreek & 74 & 0.333 & 0.021 & 0.019 & 0.008 \\\\ \nAnc. Greek & 65 & 0.292 & 0.021 & 0.021 & 0.020 \\\\ \nHindi & 69 & 0.331 & 0.012 & 0.010 & 0.007 \\\\ \nHungarian & 65 & 0.329 & 0.026 & 0.023 & 0.013 \\\\ \nItalian & 68 & 0.333 & 0.014 & 0.012 & 0.008 \\\\ \nJapanese & 44 & 0.333 & 0.008 & 0.006 & 0.007 \\\\ \nLatin & 59 & 0.313 & 0.031 & 0.030 & 0.017 \\\\ \nPersian & 93 & 0.331 & 0.022 & 0.021 & 0.009 \\\\ \nPortuguese & 88 & 0.332 & 0.013 & 0.012 & 0.006 \\\\ \nRomanian & 46 & 0.333 & 0.012 & 0.010 & 0.008 \\\\ \nRussian & 80 & 0.332 & 0.017 & 0.016 & 0.008 \\\\ \nSlovak & 87 & 0.331 & 0.020 & 0.018 & 0.010 \\\\ \nSlovenian & 50 & 0.329 & 0.027 & 0.024 & 0.014 \\\\ \nSpanish & 95 & 0.333 & 0.014 & 0.013 & 0.006 \\\\ \nSwedish & 73 & 0.331 & 0.022 & 0.021 & 0.010 \\\\ \nTamil & 40 & 0.333 & 0.018 & 0.016 & 0.010 \\\\ \nTelugu & 10 & 0.331 & 0.037 & 0.029 & 0.033 \\\\ \nTurkish & 49 & 0.331 & 0.015 & 0.013 & 0.010 \\\\ \n\\hline \n\\end{tabular}\n\\end{center}\n\\end{scriptsize}\n\\end{table*}\n\n\n\\section{Discussion}\n\n\\label{discussion_section}\n\nWe have shown that $E_2[C]$ predicts $C_{true}$ with small error, much better than the baseline. \nThe positive results are not surprising given the previous success of $E_2[C]$ predicting crossings on uniformly random trees, where $\\Delta_2$ is about $5\\%$, i.e. about 6 times smaller than the baseline $\\Delta_0$, for sufficiently long sentences \\cite{Ferrer2014c}.  \nIt is also worth noting that $E_2[C]$ behaves well even in the treebanks with the lowest proportion of crossings, where one could argue that grammar would impose the heaviest constraints against crossings. For example, it achieves a particularly low relative error in the Romanian and Japanese Prague treebanks although they contain no or almost no crossings (Table \\ref{relative_table}).\n\n\nIt could be argued that the good predictions of $E_2[C]$ are not surprising at all \nbecause the syntactic dependency structures that we have analyzed could be the result of some sophisticated apparatus: a complex language faculty or external grammatical knowledge which could have produced, indirectly, a distribution of dependency lengths and vertex degrees that is favorable for $E_2[C]$. \n\nThen the input with which the predictor yields good predictions, e.g., dependency lengths, would be an indirect result of that complex device.\nHowever, $E_2[C]$ does not require such a device: $E_2[C]$ also makes accurate predictions on uniformly random trees with a small number of crossings \\cite{Ferrer2014c}. Therefore, the need of external grammatical knowledge to explain the origins of non-crossing dependencies is seriously challenged. \n\nThe high precision of $E_2[C]$ suggests that the actual number of crossings in sentences might be a side effect of the dependency lengths, which are in turn constrained by a general principle of dependency length minimization (see {\\newcite}{Ferrer2013e} for a review of the empirical and theoretical backup of that principle).  \nA ban of crossings by grammar (e.g., {\\newcite}{hudson07,Tanaka97}),\na principle of minimization of crossings \\cite{Liu2008a} or a competence-plus \\cite{Hurford2012_Chapter3} limiting the number of crossings, may not be necessary to explain the low frequency of crossings in world languages.\n\n\n\n\nIn spite of the arguments in favor of a model predicting crossings based on dependency lengths reviewed and expanded in this article,\nit looks difficult to rule out \nsome principle of minimization of crossings or projectivity constraint. The reason is the positive correlation between crossings and dependency lengths that has been unveiled by this article and previous research combining both theory and experiment (see {\\newcite}{Ferrer2014c}, {\\newcite}{Ferrer2014f}, {\\newcite}{Ferrer2015c} and references therein). The question is: what is the causal force for the scarcity of crossings: (a) a principle of minimization of crossings that explains why dependency lengths are short or (b) a principle of dependency length minimization that explains the scarcity of crossings? \\cite{Ferrer2014f}. A temporary solution to this dilemma is straightforward if we are seriously concerned about the construction of a general theory of language that is not only highly predictive but also parsimonious: a theory of language based on (b) is more parsimonious than one based on (a) \\cite{Ferrer2014f}. \n\nOur results also have interesting implications for the field of parsing, and related tasks like machine translation \\cite{gomez2009optimal}, where algorithms and formalisms face a trade-off between coverage of crossing dependencies and computational cost. If the low frequency of crossing dependencies arises naturally from dependency length minimization, this suggests that the adequacy of the so-called mildly non-projective parsers and formalisms, which support restricted forms of non-projectivity, should be evaluated solely on their empirical coverage in natural language treebanks in relation to their computational efficiency, rather than the conformance to some concrete set of grammatical rules or principles governing which kinds of crossing dependencies should be allowed.\n\nThis is relevant because the latter has typically been taken into account in the literature: some examples are the limitations like constant growth in the classic characterization of mild context-sensitivity \\cite{joshi85,Kallmeyer2010}; or the popularity of well-nested dependency structures with gap degree at most $1$ \\cite{kuhlmann06}, probably the best-known class of mildly non-projective structures (see for example {\\newcite}{Pitler2012}) over alternatives like mildly ill-nested dependency structures of gap degree at most $1$, which have a larger coverage with the exact same parsing complexity \\cite{gomez2011cl}. On the other hand, as an example of the purely empirical approach, {\\newcite}{Gildea2010} shows how the optimal factorization of LCFRS rules in terms of time complexity does not necessarily arise from observing restrictions like well-nestedness or even minimum fan-out (gap degree).\n\n\nThe good empirical coverage of mildly non-projective classes of structures defined by such diverse restrictions as well-nestedness \\cite{kuhlmann06}, 1-endpoint-crossing \\cite{Pitler2013,Pitler2014}, 2-planarity \\cite{GomNiv2013}, parsability with the basic Attardi transitions \\cite{attardi06,CohGomSat2011} or even more so mildly ill-nestedness \\cite{gomez2011cl} or 1-ill-nestedness \\cite{maier:lichte:11} may be explained simply because of their flexibility, not because the particular restrictions on crossings they introduce necessarily match real underlying restrictions in a direct fashion. Our findings suggest that realistic constraints on dependency lengths could help to define, maybe probabilistically, the mild non-projectivity that is needed to describe and parse real languages with an affordable cost.\n\n\n\n\n\n\\iftoggle{arxiv}{\n\n\\section{Acknowledgments}\n\n\n\n\nRFC is funded by the grants 2014SGR 890 (MACDA) from AGAUR (Generalitat de Catalunya) and also\nthe APCOM project (TIN2014-57226-P) from MINECO (Ministerio de Economia y Competitividad).\nCGR is partially funded by the TELEPARES-UDC project (FFI2014-51978-C2-2-R) from MINECO and an Oportunius program grant from Xunta de Galicia.\n\nWe thank Morten Christiansen for helpful discussions, Wolfgang Maier for comments on an earlier version of this manuscript, and Dan Zeman for help with data conversion.\n\n\n\n\n\n\n\n}{\n\n\n}\n\n\n\n\n\n\\bibliographystyle{fullname}\n\\bibliography{main,twoplanaracl,Ramon}\n\n\n", "itemtype": "equation", "pos": 25423, "prevtext": "\n$p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$ depends only on $n$, $d(e_1)$ and $d(e_2)$ (see {\\newcite}{Ferrer2014c} for further details about $p(C(e_1, e_2) = 1 | d(e_1), d(e_2))$). \n\nAlthough $E_0[C]$ and $E_2[C]$ are predictors of $C$ that have the same mathematical structure (they are sums of probabilities over pairs of edges of $Q$), $E_0[C]$ is a true expectation while $E_2[C]$ is not. \n\nThe {\\textbf{{relative error}}} of a predictor is defined as \\cite{Ferrer2014c}\n\n", "index": 7, "text": "\\begin{equation}\n\\Delta_x = E_x\\left[\\bar{C}\\right] - \\bar{C}_{true} = (E_x[C]-C_{true})/C_{max}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\Delta_{x}=E_{x}\\left[\\bar{C}\\right]-\\bar{C}_{true}=(E_{x}[C]-C_{true})/C_{max}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mi>x</mi></msub><mo>=</mo><mrow><mrow><msub><mi>E</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mover accent=\"true\"><mi>C</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>]</mo></mrow></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>C</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi></mrow></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>E</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>C</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><msub><mi>C</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>C</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]