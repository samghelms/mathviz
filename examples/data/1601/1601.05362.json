[{"file": "1601.05362.tex", "nexttext": "\nwhere $A$ is a non-Hermitian but symmetric matrix, i.e, $A \\neq A^H$ and $A = A^T$. \nLinear systems of this form arise frequently in electromagnetic scattering applications, \nfor example in monostatic radar cross-section calculation, where each right-hand side \ntypically corresponds to an incident wave illuminating the target at a given angle \nof incidence~\\cite{ISDLG,CDGS05}.\n\nRoughly speaking, computational techniques for solving linear systems on modern \ncomputers can be divided into the class of direct and of iterative methods. Block \niterative Krylov subspace methods are particularly designed for solving efficiently \nlinear systems with multiple RHSs~(cf. \\cite{JZJZA,MHGB}). Block algorithms require \none or more matrix product operations of the form $AV$, with $V\\in \\mathbb{C}^{n\\times \np}$ an arbitrary rectangular matrix, per iteration step. Thus they can solve the \ntypical memory bottlenecks of direct methods. However, most of them, such as the \nBlock Bi-Conjugate Gradient (bl\\_bicg)~\\cite{DPOL}, Block Bi-Conjugate Residual \n(bl\\_bicr)~\\cite{JZJZA}, Block BiCGSTAB (bl\\_bicgstab)~\\cite{AGKJHS}, Block BiCRSTAB \n(bl\\_bicrstab)~\\cite{JZJZA}, Block QMR (bl\\_qmr)~\\cite{RWFMM}, Block IDR($s$) \n(bl\\_idr($s$)) \\cite{blidrs} and Block GMRES (bl\\_gmres)~\\cite{BVED} methods, do \nnot naturally exploit any symmetry of $A$.\n\nMethods that can exploit the symmetry of $A$ are typically of (quasi) minimal residual type \n(i.e. bl\\_sqmr)~\\cite{RWFMM}. Tadano and Sakurai recently proposed the Block COCG \n(bl\\_cocg) \\cite{HTTSA} method, which can be regarded as a natural extension of the \nCOCG \\cite{HAVVJBM} algorithm for solving linear systems (\\ref{eq1.1}). Both these \ntwo methods need one operation $AV$ per iteration step. In this paper we revisit \nthe Block COCG method, presenting a more systematic derivation than the one presented \n\\cite{HTTSA}, and we introduce a new Block solver~(bl\\_cocr) that can be seen as an \nextension of the COCR algorithm proposed in~\\cite{TSSLZ}. The numerical stability \nof the bl\\_cocg and the bl\\_cocr methods are enhanced by the residual orthonormalization \ntechnique~\\cite{AADR}.\n\n\nThe paper is organized as follows. In Section~\\ref{author_mini4:sec:2x} we present the general framework for the development of the bl\\_cocg and the bl\\_cocr solvers. In Section 3 we study their numerical stability properties and then we show how  to improve their convergence by employing the residual orthonormalization technique. In Section 3, we report on extensive numerical experiments to illustrate the effectiveness of the two new iterative methods in computational electromagnetics. Finally, some conclusions arising from this work are presented in Section 4.\n\n\\section{The Block COCG and Block COCR methods}\n\\label{author_mini4:sec:2x}\nLet $X^{m + 1}\\in\\mathbb{C}^{n\\times p}$ be the $(m + 1)$th approximate solution of linear systems (\\ref{eq1.1}) satisfying the following condition\n\n", "itemtype": "equation", "pos": 1772, "prevtext": "\n\n\\title{Block variants of the COCG and COCR methods for solving complex symmetric linear\nsystems with multiple right-hand sides}\n\n\n\\titlerunning{Block versions of COCG and COCR}\n\n\\author{Xian-Ming Gu\\inst{1,2} \\and Bruno Carpentieri\\inst{3} \\and Ting-Zhu Huang\\inst{1}\n\\and Jing Meng\\inst{4}}\n\n\n\\authorrunning{X.-M. Gu, B. Carpentieri, T.-Z. Huang, and J. Meng}\n\n\\institute{\n\nSchool of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu\n611731, P.R. China, {\\tt guxianming@live.cn}, {\\tt tingzhuhuang@126.com}\n\\and\nInstitute of Mathematics and Computing Science, University of Groningen,\nNijenborgh 9, P.O. Box 407, 9700 AK Groningen, The Netherlands\n\\and\nSchool of Science and Technology, Nottingham Trent University, Clifton Campus, Nottingham, NG11 8NS,\nUnited Kingdom, {\\tt bcarpentieri@gmail.com}\n\\and\nSchool of Mathematics and Statistics, Taishan University, Taian, 271021, P.R. China,\n{\\tt mengmeng-erni@163.com}\n}\n\n\\maketitle\n\n\\begin{abstract}\nIn the present study, we establish two new block variants of the Conjugate Orthogonal \nConjugate Gradient (COCG) and the Conjugate $A$-Orthogonal Conjugate Residual (COCR) \nKrylov subspace methods for solving complex symmetric linear systems with multiple \nright hand sides. The proposed Block iterative solvers can fully exploit the complex \nsymmetry property of coefficient matrix of the linear system. We report on extensive \nnumerical experiments to show the favourable convergence properties of our newly \ndeveloped Block algorithms for solving realistic electromagnetic simulations.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{author_mini4:sec:2}\nIn this paper we are interested in the efficient solution of linear systems\nwith multiple right-hand sides (RHSs) of the form\n\n", "index": 1, "text": "\\begin{equation}\nAX = B,\\quad\\ A\\in \\mathbb{C}^{n\\times n},\\ \\ X,B\\in \\mathbb{C}^{n\\times p},\\ p\\ll n,\n\\label{eq1.1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"AX=B,\\quad\\ A\\in\\mathbb{C}^{n\\times n},\\ \\ X,B\\in\\mathbb{C}^{n\\times p},\\ p\\ll&#10;n,\" display=\"block\"><mrow><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>=</mo><mi>B</mi></mrow><mo rspace=\"17.5pt\">,</mo><mrow><mrow><mi>A</mi><mo>\u2208</mo><mrow><msup><mi>\u2102</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup><mo rspace=\"12.5pt\">,</mo><mi>X</mi></mrow></mrow><mo>,</mo><mrow><mrow><mi>B</mi><mo>\u2208</mo><msup><mi>\u2102</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>p</mi><mo>\u226a</mo><mi>n</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05362.tex", "nexttext": "\nwhere $R_0 = B - AX_0$ is an initial residual and $\\mathcal{K}^{\\diamond}_{m+1}(A; R_0)$ is the block\nKrylov subspace~\\cite{MHGB} defined as\n\n", "itemtype": "equation", "pos": 4834, "prevtext": "\nwhere $A$ is a non-Hermitian but symmetric matrix, i.e, $A \\neq A^H$ and $A = A^T$. \nLinear systems of this form arise frequently in electromagnetic scattering applications, \nfor example in monostatic radar cross-section calculation, where each right-hand side \ntypically corresponds to an incident wave illuminating the target at a given angle \nof incidence~\\cite{ISDLG,CDGS05}.\n\nRoughly speaking, computational techniques for solving linear systems on modern \ncomputers can be divided into the class of direct and of iterative methods. Block \niterative Krylov subspace methods are particularly designed for solving efficiently \nlinear systems with multiple RHSs~(cf. \\cite{JZJZA,MHGB}). Block algorithms require \none or more matrix product operations of the form $AV$, with $V\\in \\mathbb{C}^{n\\times \np}$ an arbitrary rectangular matrix, per iteration step. Thus they can solve the \ntypical memory bottlenecks of direct methods. However, most of them, such as the \nBlock Bi-Conjugate Gradient (bl\\_bicg)~\\cite{DPOL}, Block Bi-Conjugate Residual \n(bl\\_bicr)~\\cite{JZJZA}, Block BiCGSTAB (bl\\_bicgstab)~\\cite{AGKJHS}, Block BiCRSTAB \n(bl\\_bicrstab)~\\cite{JZJZA}, Block QMR (bl\\_qmr)~\\cite{RWFMM}, Block IDR($s$) \n(bl\\_idr($s$)) \\cite{blidrs} and Block GMRES (bl\\_gmres)~\\cite{BVED} methods, do \nnot naturally exploit any symmetry of $A$.\n\nMethods that can exploit the symmetry of $A$ are typically of (quasi) minimal residual type \n(i.e. bl\\_sqmr)~\\cite{RWFMM}. Tadano and Sakurai recently proposed the Block COCG \n(bl\\_cocg) \\cite{HTTSA} method, which can be regarded as a natural extension of the \nCOCG \\cite{HAVVJBM} algorithm for solving linear systems (\\ref{eq1.1}). Both these \ntwo methods need one operation $AV$ per iteration step. In this paper we revisit \nthe Block COCG method, presenting a more systematic derivation than the one presented \n\\cite{HTTSA}, and we introduce a new Block solver~(bl\\_cocr) that can be seen as an \nextension of the COCR algorithm proposed in~\\cite{TSSLZ}. The numerical stability \nof the bl\\_cocg and the bl\\_cocr methods are enhanced by the residual orthonormalization \ntechnique~\\cite{AADR}.\n\n\nThe paper is organized as follows. In Section~\\ref{author_mini4:sec:2x} we present the general framework for the development of the bl\\_cocg and the bl\\_cocr solvers. In Section 3 we study their numerical stability properties and then we show how  to improve their convergence by employing the residual orthonormalization technique. In Section 3, we report on extensive numerical experiments to illustrate the effectiveness of the two new iterative methods in computational electromagnetics. Finally, some conclusions arising from this work are presented in Section 4.\n\n\\section{The Block COCG and Block COCR methods}\n\\label{author_mini4:sec:2x}\nLet $X^{m + 1}\\in\\mathbb{C}^{n\\times p}$ be the $(m + 1)$th approximate solution of linear systems (\\ref{eq1.1}) satisfying the following condition\n\n", "index": 3, "text": "\\begin{equation}\nX_{m + 1} = X_0 + Z_{m + 1},\\quad Z_{m +1}\\in \\mathcal{K}^{\\diamond}_{m+1}(A; R_0),\n\\label{eq1.2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"X_{m+1}=X_{0}+Z_{m+1},\\quad Z_{m+1}\\in\\mathcal{K}^{\\diamond}_{m+1}(A;R_{0}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>X</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>X</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Z</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><msub><mi>Z</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2208</mo><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo>\u22c4</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>;</mo><msub><mi>R</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05362.tex", "nexttext": "\nCompared with conventional Krylov subspace methods, where ${\\bm x}^{(j)}_{m + 1} - {\\bm x}^{(j)}_0\n\\in\\mathcal{K}_{m + 1}(A, {\\bm r}^{(j)}_0)$, note that block Krylov methods can\nsearch the approximate solutions into larger spaces, and thus they may require less\niterations to converge to a given accuracy. In the next section we introduce the framework for the development of the Block COCG and the Block COCR methods.\n\n\n\\subsection{Derivation of the Block COCG and Block COCR methods}\n\\label{author_mini4:subsec:2}\nAccording to Eqs. (\\ref{eq1.2})--(\\ref{eq1.3}), the $(m + 1)$th residual $R_{m+1} =\nB - AX_{m +1}$ of the Block COCG method \\cite{HTTSA} and the Block COCR method is\ncomputed by the following recurrence relations,\n\\begin{eqnarray}\nR_0 = P_0 = B - AX_0 \\in \\mathcal{K}^{\\diamond}_1(A; R_0), \\nonumber\\\\\nR_{m+1} = R_m - AP_m\\alpha_m\\in \\mathcal{K}^{\\diamond}_{m + 2}(A; R_0),\\nonumber\\\\\nP_{m + 1} = R_{m + 1} + P_m \\beta_m \\in \\mathcal{K}^{\\diamond}_{m + 2}(A;\nR_0).\n\\label{author_mini4:eq:01x}\n\\end{eqnarray}\nHere, $P_{m+1} \\in \\mathbb{C}^{n\\times p}, \\alpha_m, \\beta_m\\in \\mathbb{C}^{p\\times\np}$. The $(m + 1)$th approximate solution $X_{m+1}$ is updated through the recurrence\nrelation\n\n", "itemtype": "equation", "pos": 5105, "prevtext": "\nwhere $R_0 = B - AX_0$ is an initial residual and $\\mathcal{K}^{\\diamond}_{m+1}(A; R_0)$ is the block\nKrylov subspace~\\cite{MHGB} defined as\n\n", "index": 5, "text": "\\begin{equation}\n\\mathcal{K}^{\\diamond}_{m+1}(A; R_0) = \\Big\\{\\sum^{m}_{j = 0}A^j R_0\\gamma_j\\mid \\gamma_j \\in\n\\mathbb{C}^{p\\times p}~(j = 0,1,\\ldots,m)\\Big\\}.\n\\label{eq1.3}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{K}^{\\diamond}_{m+1}(A;R_{0})=\\Big{\\{}\\sum^{m}_{j=0}A^{j}R_{0}\\gamma_{%&#10;j}\\mid\\gamma_{j}\\in\\mathbb{C}^{p\\times p}~{}(j=0,1,\\ldots,m)\\Big{\\}}.\" display=\"block\"><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo>\u22c4</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>;</mo><msub><mi>R</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">{</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>m</mi></munderover><msup><mi>A</mi><mi>j</mi></msup><msub><mi>R</mi><mn>0</mn></msub><msub><mi>\u03b3</mi><mi>j</mi></msub><mo>\u2223</mo><msub><mi>\u03b3</mi><mi>j</mi></msub><mo>\u2208</mo><mpadded width=\"+3.3pt\"><msup><mi>\u2102</mi><mrow><mi>p</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mpadded><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"160%\" minsize=\"160%\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05362.tex", "nexttext": "\nSimilarly to the framework introduced in \\cite{XMGMC}, different formulae for the\n$p\\times p$ matrices $\\alpha_m,\\beta_m~(m = 0,1,\\ldots)$ in the recurrences\n(\\ref{author_mini4:eq:01x})--(\\ref{author_mini4:eq:01xx}) lead to different iterative\nalgorithms. Denoting by $\\mathcal{L}$ the \\textit{block constraints subspace}, these matrices\n$\\alpha_m,\\beta_m$ are determined by imposing the orthogonality conditions\n\n", "itemtype": "equation", "pos": 6498, "prevtext": "\nCompared with conventional Krylov subspace methods, where ${\\bm x}^{(j)}_{m + 1} - {\\bm x}^{(j)}_0\n\\in\\mathcal{K}_{m + 1}(A, {\\bm r}^{(j)}_0)$, note that block Krylov methods can\nsearch the approximate solutions into larger spaces, and thus they may require less\niterations to converge to a given accuracy. In the next section we introduce the framework for the development of the Block COCG and the Block COCR methods.\n\n\n\\subsection{Derivation of the Block COCG and Block COCR methods}\n\\label{author_mini4:subsec:2}\nAccording to Eqs. (\\ref{eq1.2})--(\\ref{eq1.3}), the $(m + 1)$th residual $R_{m+1} =\nB - AX_{m +1}$ of the Block COCG method \\cite{HTTSA} and the Block COCR method is\ncomputed by the following recurrence relations,\n\\begin{eqnarray}\nR_0 = P_0 = B - AX_0 \\in \\mathcal{K}^{\\diamond}_1(A; R_0), \\nonumber\\\\\nR_{m+1} = R_m - AP_m\\alpha_m\\in \\mathcal{K}^{\\diamond}_{m + 2}(A; R_0),\\nonumber\\\\\nP_{m + 1} = R_{m + 1} + P_m \\beta_m \\in \\mathcal{K}^{\\diamond}_{m + 2}(A;\nR_0).\n\\label{author_mini4:eq:01x}\n\\end{eqnarray}\nHere, $P_{m+1} \\in \\mathbb{C}^{n\\times p}, \\alpha_m, \\beta_m\\in \\mathbb{C}^{p\\times\np}$. The $(m + 1)$th approximate solution $X_{m+1}$ is updated through the recurrence\nrelation\n\n", "index": 7, "text": "\\begin{equation}\nX_{m+1} = X_m + P_m \\alpha_m.\n\\label{author_mini4:eq:01xx}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"X_{m+1}=X_{m}+P_{m}\\alpha_{m}.\" display=\"block\"><mrow><mrow><msub><mi>X</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>X</mi><mi>m</mi></msub><mo>+</mo><mrow><msub><mi>P</mi><mi>m</mi></msub><mo>\u2062</mo><msub><mi>\u03b1</mi><mi>m</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05362.tex", "nexttext": "\n\nThe Block COCG and the Block COCR methods correspond to the choices $\\mathcal{L}=\\mathcal{K}^{\\diamond}_m(\\bar{A}; \\bar{R}_0)$\nand $\\mathcal{L}=\\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$, respectively. In Table~\\ref{author_mini4:tab:1}, the\nconjugate orthogonality conditions imposed to determine $\\alpha_m$ and $\\beta_m$ are summarized for the sake of clarity.\n\\begin{table}\n\\centering\n\\caption{Orthogonality conditions imposed to determine $p\\times p$ matrices $\\alpha_m,\\beta_m$}\n\\label{author_mini4:tab:1}\n\\begin{tabular}{p{2cm}p{3cm}p{3cm}}\n\\hline\\noalign{\\smallskip}\nMatrix             & Block COCG                             & Blcok COCR   \\\\\n$\\alpha_m,\\beta_m$ & $R_m \\perp \\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$  & $R_m\n\\perp \\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ \\\\\n                   & $AP_m \\perp \\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ & $AP_m\n                   \\perp \\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ \\\\\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n\\end{tabular}\n\\end{table}\n\nWe show the complete Block COCR algorithm in Algorithm \\ref{alg1x}. We use\nthe notation $\\|\\cdot\\|_F$ for the Frobenius norm of a matrix, and $\\epsilon$ is a sufficiently\nsmall user-defined value. We see that the Block COCR method requires two matrix products\n$AP_{m + 1}$, $AR_{m + 1}$ at each iteration step. While the product $AR_{m + 1}$ is computed by explicit matrix multiplication, the product $AP_{m + 1}$ is computed by the recurrence relation at line 9, to reduce the computational complexity.\nNote that the Block COCG and the Block COCR methods can be derived from the Block BiCG and the Block BiCR methods, respectively, by choosing the initial auxiliary residual $\\hat{R}_0 = \\bar{R}_0$ and removing some redundant computations; we refer to the recent work~\\cite{XMGMC} for similar discussions about the derivation of conventional non-block Krylov subspace methods for complex symmetric linear systems with single RHS.\n\n\\begin{algorithm}\n\\caption{The Block COCR method}\n\\begin{algorithmic}[1]\n  \\STATE $X_0\\in \\mathbb{C}^{n\\times p}$ is an initial guess, $R_0 = B - AX_0$,\n  \\STATE Set $P_0 = R_0$, $U_0 = V_0 = AR_0$,\n  \\FOR{$m = 0,1,\\ldots$, until $\\|R_m\\|_F/\\|R_0\\|_F \\leq \\epsilon$}\n  \\STATE Solve $(U^{T}_mU_m)\\alpha_m = R^{T}_m V_m$ for $\\alpha_m$,\n  \\STATE $X_{m + 1} = X_m + P_m \\alpha_m$,\n  \\STATE $R_{m + 1} = R_m - U_m \\alpha_m$ and $V_{m+1} = AR_{m+1}$,\n  \\STATE Solve $(R^{T}_mV_m)\\beta_m = R^{T}_{m + 1} V_{m + 1}$ for $\\beta_m$,\n  \\STATE $P_{m + 1} = R_{m + 1} + P_m \\beta_m$,\n  \\STATE $U_{m + 1} = V_{m + 1} + U_m \\beta_m$,\n  \\ENDFOR\n\\end{algorithmic}\n\\label{alg1x}\n\\end{algorithm}\n\n\n\n\\subsection{Improving the numerical stability of the Block COCG and Block COCR methods by residual orthonormalization}\nOne known problem with Block Krylov subspace methods is that the residual norms  may not converge when the number $p$ of right-hand sides\nis large, mainly due to numerical instabilities, see e.g.~\\cite{AADR}. These instabilities often arise because of the loss of linear independence among the column vectors of the $n\\times p$ matrices that appear in the methods, such as $R_{m}$ and $P_m$. Motivated by this concern, in this section we propose to use the residual orthonormalization technique to enhance the numerical stability\nof the Block COCG and Block COCR algorithms. This efficient technique was introduced in~\\cite{AADR} in the context of the Block CG method \\cite{DPOL}.\n\nLet the Block residual $R_m$ be factored as $R_m = Q_m\\xi_m$ by conventional QR\nfactorization\\footnote{For our practical implementation, we use MATLAB qr-function\n``\\texttt{qr}($W$,0)\" for a given matrix $W\\in\\mathbb{C}^{n\\times p}$.}, with $Q^{H}_m Q_m = I_p$. Here $I_p$ denotes the identity\nmatrix of order $p$ and $\\xi_m\\in \\mathbb{C}^{p\\times p}$. From (\\ref{author_mini4:eq:01x}), the following equation can be obtained\n\n", "itemtype": "equation", "pos": 7002, "prevtext": "\nSimilarly to the framework introduced in \\cite{XMGMC}, different formulae for the\n$p\\times p$ matrices $\\alpha_m,\\beta_m~(m = 0,1,\\ldots)$ in the recurrences\n(\\ref{author_mini4:eq:01x})--(\\ref{author_mini4:eq:01xx}) lead to different iterative\nalgorithms. Denoting by $\\mathcal{L}$ the \\textit{block constraints subspace}, these matrices\n$\\alpha_m,\\beta_m$ are determined by imposing the orthogonality conditions\n\n", "index": 9, "text": "\\begin{equation}\nR_m \\perp \\mathcal{L}\\quad\\ \\mathrm{and}\\quad\\ AP_m \\perp \\mathcal{L}.\n\\label{author_mini4:eq:01y}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"R_{m}\\perp\\mathcal{L}\\quad\\ \\mathrm{and}\\quad\\ AP_{m}\\perp\\mathcal{L}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>R</mi><mi>m</mi></msub><mo>\u27c2</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mi>and</mi></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>P</mi><mi>m</mi></msub></mrow><mo>\u27c2</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05362.tex", "nexttext": "\nHere, $\\tau_{m +1} \\equiv \\xi_{m + 1}\\xi_{m - 1}$, $\\alpha'_k \\equiv \\xi_m \\alpha_m\n\\xi_{m - 1}$, and $S_m = P_m\\xi_{m - 1}$. In the new Algorithms~\\ref{alg2x}-\\ref{alg3x}, \nthe matrix $\\beta'_m$ is defined as $\\alpha'_m\\equiv \\xi_m \\beta_m \\xi^{-1}_{m + 1}$. \nThe residual norm is monitored by $\\|\\xi_m\\|_F$ instead of $\\|R_m\\|_F$, since the \nFrobenius norm of $R_m$ satisfies $\\|R_m\\|_F = \\|\\xi_m\\|_F$. Note that the QR decomposition \nis calculated at each iteration. However, the numerical results shown in the next \nsection indicate that the extra cost is amortized by the improved robustness of the \ntwo Block solvers.\n\n\n\\begin{algorithm}[t]\n\\caption{Algorithm of the Block COCG method with residual orthonormalization (bl\\_cocg\\_rq)}\n\\begin{algorithmic}[1]\n  \\STATE $X_0\\in \\mathbb{C}^{n\\times p}$ is an initial guess, and compute $Q_0\\xi_0 = B - AX_0$,\n  \\STATE Set $S_0 = Q_0$,\n  \\FOR{$m = 0,1,\\ldots$, until $\\|\\xi_m\\|_F/\\|B\\|_F \\leq \\epsilon$}\n  \\STATE Solve $(S^{T}_m A S_m)\\alpha'_m = Q^{T}_m Q_m$ for $\\alpha'_m$,\n  \\STATE $X_{m + 1} = X_m + S_m \\alpha'_m\\xi_m$,\n  \\STATE $Q_{m + 1}\\tau_{m + 1} = Q_m - AS_m \\alpha'_m$ and $\\xi_{m+1} = \\tau_{m+1}\\xi_m$,\n  \\STATE Solve $(Q^{T}_mQ_m)\\beta'_m = \\tau^{T}_{m + 1} Q^{T}_{m + 1}Q_{m + 1}$ for $\\beta'_m$,\n  \\STATE $S_{m + 1} = Q_{m + 1} + S_m \\beta'_m$,\n  \\ENDFOR\n\\end{algorithmic}\n\\label{alg2x}\n\\end{algorithm}\n\n\\begin{algorithm}\n\\caption{Algorithm of the Block COCR method with residual orthonormalization (bl\\_cocr\\_rq)}\n\\begin{algorithmic}[1]\n  \\STATE $X_0\\in \\mathbb{C}^{n\\times p}$ is an initial guess, and compute $Q_0\\xi_0 = B - AX_0$,\n  \\STATE Set $S_0 = Q_0$ and $U_0 = V_0 =  AQ_0$,\n  \\FOR{$m = 0,1,\\ldots$, until $\\|\\xi_m\\|_F/\\|B\\|_F \\leq \\epsilon$}\n  \\STATE Solve $(U^{T}_mU_m)\\alpha'_m = Q^{T}_m U_m$ for $\\alpha'_m$,\n  \\STATE $X_{m + 1} = X_m + P_m \\alpha'_m$\n  \\STATE $Q_{m + 1}\\tau_{m + 1} = Q_m - U_m \\alpha'_m$ and $\\xi_{m+1} = \\tau_{m+1}\\xi_m$,\n  \\STATE Compute $V_{m + 1} = AQ_{m + 1}$,\n  \\STATE Solve $(Q^{T}_mV_m)\\beta_m = \\tau^{T}_{m + 1} Q^{T}_{m + 1}V_{m + 1}$ for $\\beta'_m$,\n  \\STATE $S_{m + 1} = Q_{m + 1} + S_m \\beta'_m$,\n  \\STATE $U_{m + 1} = V_{m + 1} + U_m \\beta'_m$,\n  \\ENDFOR\n\\end{algorithmic}\n\\label{alg3x}\n\\end{algorithm}\n\n\n\n\n\\section{Numerical experiments}\n\\label{author_mini4:subsec:3}\nIn this section, we carry out some numerical experiments to show the potential\neffectiveness of the proposed iterative solution strategies in computational electromagnetics. We compare the bl\\_cocg,\nbl\\_cocg\\_rq, bl\\_cocr, bl\\_cocr\\_rq methods against other popular block Krylov subspace\nmethods such as bl\\_qmr, bl\\_bicgstab, bl\\_bicrstab, bl\\_idr($s$) (selecting matrix $P =\nrand(n,sp)$, see~\\cite{blidrs}) and restarted bl\\_gmres(m).\nWe use the value $m = 80$ for the restart in bl\\_gmres(m). The experiments have\nbeen carried out in double precision floating point arithmetic with MATLAB 2014a\n(64 bit) on PC-Intel(R) Core(TM) i5-3470 CPU 3.20 GHz, 8 GB of RAM.\n\nThe different Block algorithms are compared in terms of number of iterations, denoted as\n\\textit{Iters} in the tables, and $\\log_{10}$ of the final true relative residual\nnorm defined as $\\log_{10}(\\|B - AX_{\\mathrm{final}}\\|_F/\\|B\\|_F)$, denoted as\n\\textit{TRR}. The iterative solution is started choosing $X_0 = O\\in\\mathbb{C}^{n\\times p}$ \nas initial guess. The stopping criterion in our runs is the reduction of the norm \nof the initial Block residual by eight orders of magnitude, i.e., $\\|R_m\\|_F/\\|B\\|_F \n\\leq Tol = 10^{-10}$. The right-hand side $B$ is computed by the MATLAB function \n{\\texttt{rand}}. In the tables, the symbol ``$\\dag$\" indicates no convergence \nwithin $n$ iterations, or $n/m$ cycles for the bl\\_gmres($m$) method.\n\nThe first test problems are three matrices extracted from the Matrix Market\ncollection\\footnote{\\url{http://math.nist.gov/MatrixMarket/matrices.html}},\narising from modeling acoustic scattering problems. They are denoted as~\\texttt{young1c, young2c}, \nand {\\tt young3c}. The results of our experiments are presented in Table~\\ref{tab2}. \nThe symbol ${}^{*}$ used for the bl\\_bicgstab, bl\\_idr(4), and bl\\_bicrstav methods \nindicate that these three methods require no less than two matrix products $AV$ per \niteration step. The symbol ${}^{**}$ refers to the number of outer iterations in the \nBlock GMRES($m$) method, when it can achieve convergence; refer to \\cite{HXZGW} for details. This notation is used throughout this section.\n\n\\begin{table}[t]\\footnotesize\\tabcolsep=4.3pt\n\\begin{center}\n\\caption{The numerical results of different iterative solvers for the first example.}\n\\begin{tabular}{cccccccccc}\n\\hline Method &\\multicolumn{3}{c}{\\texttt{young2c} ($p = 10$)}&\\multicolumn{3}{c}{\n\\texttt{young3c} ($p = 8$)}&\\multicolumn{3}{c}{\\texttt{young1c} ($p = 8$)}\\\\\n[-2pt]\\cmidrule(r{0.5em}){2-4} \\cmidrule(l{0.5em}r{0.5em}){5-7}\\cmidrule(l{0.5em}){8-10}\n\\[-11pt]\n            &$Iters$     &{\\it TRR} &CPU   &$Iters$ &{\\it TRR} &CPU    &$Iters$   &{\\it TRR} &CPU    \\\\\n\\hline\nbl\\_cocg      &238       &-10.03    &0.17  &$\\dag$  &$\\dag$    &$\\dag$ &329       &-10.16    &0.16 \\\\\nbl\\_cocg\\_rq  &142       &-10.14    &0.13  &151     &-10.00    &0.09   &177       &-10.29    &0.12 \\\\\nbl\\_cocr      &201       &-10.07    &0.15  &145     &-9.95     &0.04   &221       &-10.07    &0.12 \\\\\nbl\\_cocr\\_rq  &138       &-10.18    &0.13  &146     &-10.03    &0.05   &180       &-10.18    &0.13 \\\\\nbl\\_sqmr      &154       &-9.87     &0.29  &131     &-10.39    &0.09   &188       &-9.88     &0.25 \\\\\nbl\\_bicgstab  &395$^{*}$ &-10.09    &0.41  &$\\dag$  &$\\dag$    &$\\dag$ &433$^{*}$ &-10.04    &0.35 \\\\\nbl\\_bicrstab  &356$^{*}$ &-9.96     &0.46  &$\\dag$  &$\\dag$    &$\\dag$ &417$^{*}$ &-9.71     &0.44 \\\\\nbl\\_idr(4)    &269$^{*}$ &-8.57     &0.28  &$\\dag$  &$\\dag$    &$\\dag$ &334$^{*}$ &-10.10    &0.27 \\\\\nbl\\_gmres(m)  &3$^{**}$  &-10.08    &24.5  &$\\dag$  &$\\dag$    &$\\dag$ &$\\dag$    &$\\dag$    &$\\dag$ \\\\\n\\hline\n\\end{tabular}\n\\label{tab2}\n\\end{center}\n\\end{table}\n\nTable \\ref{tab2} shows the results with nine different Block Krylov solvers. Although the bl\\_cocg and bl\\_cocr methods required more $Iters$, they are\nmore competitive than the bl\\_sqmr method in terms of CPU time and \\textit{TRR} (except the case of \\texttt{young3c}). Bl\\_cocr method is more robust than  bl\\_cocg in terms of $Iters$, CPU time and \\textit{TRR}. The bl\\_cocg\\_rq and bl\\_cocr\\_rq variant are very efficient in terms of \\textit{TRR} and CPU time. The bl\\_bicgstab, bl\\_bicrstab, bl\\_idr(4), and bl\\_gmres($m$) methods cannot solve the test problem (\\texttt{young3c}), while bl\\_cocg and bl\\_cocr converge rapidly. Due to the long iterative recurrence, the bl\\_gmres($m$) method is typically expensive.\n\nIn the second experiment we consider three dense matrices arising from monostatic radar cross-section calculation; they are denoted as \\texttt{sphere2430}, \\texttt{parallelepipede}, {\\texttt{cube1800}}. These problems are available from our GitHub repository\\footnote{\\url{https://github.com/Hsien-Ming-Ku/Test_matrices/tree/master/Example2}}, and we choose $p = 8$. Although rather small, the selected dense problems are representative of realistic radar-cross-section calculation~\\cite{CDGS05}. Larger problems would require a Fortran or C implementation of the solvers and will be considered in a separate study.\nNumerical results for each test problem are summarized in Table~\\ref{tab3}.\n\n\n\\begin{table}[t]\\footnotesize\\tabcolsep=4.3pt\n\\begin{center}\n\\caption{The numerical results of different iterative solvers for Example 1.}\n\\begin{tabular}{cccccccccc}\n\\hline Method &\\multicolumn{3}{c}{\\texttt{sphere2430}}&\\multicolumn{3}{c}{\\texttt{parallelepipede}}\n&\\multicolumn{3}{c}{\\texttt{cube1800}}\\\\\n[-2pt]\\cmidrule(r{0.5em}){2-4} \\cmidrule(l{0.5em}r{0.5em}){5-7}\\cmidrule(l{0.5em}){8-10}\n\\[-11pt]\n            &$Iters$     &{\\it TRR} &CPU   &$Iters$   &{\\it TRR} &CPU    &$Iters$   &{\\it TRR} &CPU    \\\\\n\\hline\nbl\\_cocg      &189       &-10.07    &4.16  &176       &-10.02    &2.40   &174       &-10.21    &1.94 \\\\\nbl\\_cocg\\_rq  &169       &-10.00    &3.77  &156       &-10.13    &2.13   &156       &-10.08    &1.74 \\\\\nbl\\_cocr      &186       &-10.03    &4.12  &174       &-10.02    &2.35   &169       &-10.00    &1.84 \\\\\nbl\\_cocr\\_rq  &166       &-10.05    &3.77  &152       &-10.15    &2.11   &151       &-10.09    &1.73 \\\\\nbl\\_sqmr      &172       &-9.84     &4.15  &161       &-9.91     &2.42   &159       &-9.97     &2.11 \\\\\nbl\\_bicgstab  &379$^{*}$ &-10.04    &16.5  &370$^{*}$ &-10.04    &9.94   &396$^{*}$ &-10.29    &8.42 \\\\\nbl\\_bicrstab  &392$^{*}$ &-9.57     &17.3  &355$^{*}$ &-9.85     &9.98   &303$^{*}$ &-8.38     &6.70 \\\\\nbl\\_idr(4)    &409$^{*}$ &-9.64     &22.1  &474$^{*}$ &-10.11    &16.5   &334$^{*}$ &-9.43     &10.2 \\\\\nbl\\_gmres(m)  &2$^{**}$  &-10.07    &38.2  &2$^{**}$  &-10.04    &33.3   &2$^{**}$  &-10.09    &22.1  \\\\\n\\hline\n\\end{tabular}\n\\label{tab3}\n\\end{center}\n\\end{table}\n\nTable \\ref{tab3} displays the results with again nine different Block Krylov solvers. We can see that the bl\\_sqmr method requires less \\textit{Iters} to converge compared to the bl\\_cocg and bl\\_cocr methods. However, it is more expensive in terms of CPU time except on the \\texttt{sphere2430} problem. Besides, the true residual norms produced by the bl\\_sqmr method are larger than those of both bl\\_cocg and bl\\_cocr. Furthermore, bl\\_cocg\\_rq and bl\\_cocr\\_rq are the most effective and promising solvers in terms of \\textit{Iters} and CPU time. Specifically, the bl\\_cocr\\_rq method is slightly more efficient than the bl\\_cocg\\_rq method in terms of \\textit{TRR}.\n\n\n\\section{Conclusions}\n\\label{author_mini4:subsec:4}\nIn this paper, a framework for constructing new Block iterative Krylov subspace \nmethods is presented. Two new matrix solvers that can exploit the symmetry of $A$ \nfor solving complex symmetric non-Hermitian linear systems~(\\ref{eq1.1}) are introduced. \nStabilization techniques based on residual orthonormalization strategy are discussed \nfor both methods. The numerical experiments show that the solvers can be viable \nalternative to standard Krylov subspace methods for solving  complex symmetric linear systems \nwith multiple RHSs efficiently. Obviously, for solving realistic electromagnetic \nproblems they both need to be combinated with suitable preconditioners that reflect \nthe symmetry of $A$; we refer the reader to, e.g.,~\\cite{CABO12,PLRRSC,CDGM01B} \nfor some related studies.\n\n\n\\ifx\\undefined\\bysame\n\\fi\n\\begin{thebibliography}{99}\n\\parskip1.0ex\n\n\\bibitem{ISDLG}\n{\\sc I.~S. Duff, L. Giraud, J. Langou, and E. Martin}, {\\em Using spectral low\nrank preconditioners for large electromagnetic calculations}, Int. J. Numer.\nMeth. Engng. {\\bf 62} 2005, 416--434.\n\n\\bibitem{CDGS05}\n{\\sc B.~Carpentieri, I.S. Duff, L.~Giraud and G.~Sylvand}, {\\em Combining fast \nmultipole techniques and an approximate inverse preconditioner for large \nelectromagnetism calculations}, SIAM J. Sci. Comput. {\\bf 27} (2005), 774--792.\n\n\\bibitem{JZJZA}\n{\\sc J. Zhang and J. Zhao}, {\\em A novel class of block methods based on the block\n$AA^T$-Lanczos bi-orthogonalization process for matrix equations}, Int. J. Comput.\nMath. {\\bf 90} (2013), 341--359.\n\n\\bibitem{MHGB}\n{\\sc M.~H. Gutknecht}, {\\em Block Krylov space methods for linear systems with\nmultiple right-hand sides: an introduction}, Modern Mathematical Models, Methods\nand Algorithms for Real World Systems ({\\sc A.~H. Siddiqi, I.~S. Duff, and O.\nChristensen}, eds.), Anamaya Publishers, New Delhi, India, 2006, pp.~420--447.\n\n\\bibitem{DPOL}\n{\\sc D.~P. O'Leary}, {\\em The block conjugate gradient algorithm and related\nmethods}, Linear Algebra Appl. {\\bf 29} (1980), 293--322.\n\n\\bibitem{AGKJHS}\n{\\sc A.~el~Guennouni, K.~Jbilou, and H.~Sadok}, {\\em A block version of BiCGSTAB\nfor linear systems with multiple right-hand sides}, Electron. Trans. Numer. Anal.\n{\\bf 16} (2003), 129--142.\n\n\\bibitem{RWFMM}\n{\\sc R.~W. Freund and M. Malhotra}, {\\em A block QMR algorithm for non-Hermitian\nlinear systems with multiple right-hand sides}, Linear Algebra Appl. {\\bf 254}\n(1997), 119--157.\n\n\\bibitem{blidrs}\n{\\sc L. Du, T. Sogabe, B. Yu, Y. Yamamoto, and S.-L. Zhang}, {\\em A block IDR($s$)\nmethod for nonsymmetric linear systems with multiple right-hand sides}, J.\nComput. Appl. Math. {\\bf 235} (2011), 4095--4106.\n\n\\bibitem{BVED}\n{\\sc B. Vital}, {\\em Etude de quelques m\\'{e}thodes de r\\'{e}solution de probl\\'{e}mes\nlin\\'{e}aires de grande taille sur multiprocesseur}, Ph.D. Thesis, Universit\\'{e} de\nRennes I, Rennes, 1990.\n\n\\bibitem{HTTSA}\n{\\sc H. Tadano and T. Sakurai}, {\\em A block Krylov subspace method for the contour\nintegral method and its application to molecular orbital computations}, IPSJ Trans.\nAdv. Comput. Syst. {\\bf 2} (2009), 10--18. (in Japanese)\n\n\\bibitem{HAVVJBM}\n{\\sc H.~A.~Van der Vorst and J.~B.~M. Melissen}, {\\em A Petrov-Galerkin type\nmethod for solving $A{\\bm x} = {\\bm b}$, where $A$ is symmetric complex},\nIEEE Trans. Mag. {\\bf 26} (1990), 706--708.\n\n\\bibitem{TSSLZ}\n{\\sc T.~Sogabe and S.-L.~Zhang}, {\\em A COCR method for solving complex symmetric\nlinear systems}, J. Comput. Appl. Math. {\\bf 199} (2007), 297--303.\n\n\\bibitem{AADR}\n{\\sc A.~A. Dubrulle}, {\\em Retooling the method of block conjugate gradients},\nElectron. Trans. Numer. Anal. {\\bf 12} (2001), 216--233.\n\n\\bibitem{XMGMC}\n{\\sc X.-M. Gu, M. Clemens, T.-Z. Huang, and L. Li}, {\\em The SCBiCG class of\nalgorithms for complex symmetric linear systems with applications in several\nelectromagnetic model problems}, Comput. Phys. Commun. {\\bf 191} (2015), 52--64.\n\n\\bibitem{HXZGW}\n{\\sc H.-X. Zhong, G. Wu, and G. Chen}, {\\em A flexible and adaptive simpler\nblock GMRES with deflated restarting for linear systems with multiple right-hand\nsides}, J. Comput. Appl. Math. {\\bf 282} (2015), 139--156.\n\n\\bibitem{CABO12}\n{\\sc B.~Carpentieri and M.~Bollh{\\\"o}fer}, {\\em Symmetric inverse-based multilevel \n{ILU} preconditioning for solving dense complex non-Hermitian systems in \nelectromagnetics}, Prog. Electromagn. Res. (PIER) {\\bf 128} (2012), 55--74.\n\n\\bibitem{PLRRSC}\n{\\sc P.~L. Rui, R.~S.Chen, Z.~H. Fan, and D.~Z. Ding}, {\\em Multi-step spectral\npreconditioner for fast monostatic radar cross-section calculation}, Electron.\nLett. {\\bf 43} (2007), 422--423.\n\n\\bibitem{CDGM01B}\n{\\sc B.~Carpentieri, I.S.~Duff , L.~Giraud and M.~Magolu monga Made}, {\\em Sparse \nsymmetric preconditioners for dense linear systems in electromagnetism}, \nNumer. Linear Algebra Appl. {\\bf 11} (2004), 753--771.\n\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 11021, "prevtext": "\n\nThe Block COCG and the Block COCR methods correspond to the choices $\\mathcal{L}=\\mathcal{K}^{\\diamond}_m(\\bar{A}; \\bar{R}_0)$\nand $\\mathcal{L}=\\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$, respectively. In Table~\\ref{author_mini4:tab:1}, the\nconjugate orthogonality conditions imposed to determine $\\alpha_m$ and $\\beta_m$ are summarized for the sake of clarity.\n\\begin{table}\n\\centering\n\\caption{Orthogonality conditions imposed to determine $p\\times p$ matrices $\\alpha_m,\\beta_m$}\n\\label{author_mini4:tab:1}\n\\begin{tabular}{p{2cm}p{3cm}p{3cm}}\n\\hline\\noalign{\\smallskip}\nMatrix             & Block COCG                             & Blcok COCR   \\\\\n$\\alpha_m,\\beta_m$ & $R_m \\perp \\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$  & $R_m\n\\perp \\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ \\\\\n                   & $AP_m \\perp \\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ & $AP_m\n                   \\perp \\bar{A}\\mathcal{K}^{\\diamond}_m(\\bar{A};\\bar{R}_0)$ \\\\\n\\noalign{\\smallskip}\\hline\\noalign{\\smallskip}\n\\end{tabular}\n\\end{table}\n\nWe show the complete Block COCR algorithm in Algorithm \\ref{alg1x}. We use\nthe notation $\\|\\cdot\\|_F$ for the Frobenius norm of a matrix, and $\\epsilon$ is a sufficiently\nsmall user-defined value. We see that the Block COCR method requires two matrix products\n$AP_{m + 1}$, $AR_{m + 1}$ at each iteration step. While the product $AR_{m + 1}$ is computed by explicit matrix multiplication, the product $AP_{m + 1}$ is computed by the recurrence relation at line 9, to reduce the computational complexity.\nNote that the Block COCG and the Block COCR methods can be derived from the Block BiCG and the Block BiCR methods, respectively, by choosing the initial auxiliary residual $\\hat{R}_0 = \\bar{R}_0$ and removing some redundant computations; we refer to the recent work~\\cite{XMGMC} for similar discussions about the derivation of conventional non-block Krylov subspace methods for complex symmetric linear systems with single RHS.\n\n\\begin{algorithm}\n\\caption{The Block COCR method}\n\\begin{algorithmic}[1]\n  \\STATE $X_0\\in \\mathbb{C}^{n\\times p}$ is an initial guess, $R_0 = B - AX_0$,\n  \\STATE Set $P_0 = R_0$, $U_0 = V_0 = AR_0$,\n  \\FOR{$m = 0,1,\\ldots$, until $\\|R_m\\|_F/\\|R_0\\|_F \\leq \\epsilon$}\n  \\STATE Solve $(U^{T}_mU_m)\\alpha_m = R^{T}_m V_m$ for $\\alpha_m$,\n  \\STATE $X_{m + 1} = X_m + P_m \\alpha_m$,\n  \\STATE $R_{m + 1} = R_m - U_m \\alpha_m$ and $V_{m+1} = AR_{m+1}$,\n  \\STATE Solve $(R^{T}_mV_m)\\beta_m = R^{T}_{m + 1} V_{m + 1}$ for $\\beta_m$,\n  \\STATE $P_{m + 1} = R_{m + 1} + P_m \\beta_m$,\n  \\STATE $U_{m + 1} = V_{m + 1} + U_m \\beta_m$,\n  \\ENDFOR\n\\end{algorithmic}\n\\label{alg1x}\n\\end{algorithm}\n\n\n\n\\subsection{Improving the numerical stability of the Block COCG and Block COCR methods by residual orthonormalization}\nOne known problem with Block Krylov subspace methods is that the residual norms  may not converge when the number $p$ of right-hand sides\nis large, mainly due to numerical instabilities, see e.g.~\\cite{AADR}. These instabilities often arise because of the loss of linear independence among the column vectors of the $n\\times p$ matrices that appear in the methods, such as $R_{m}$ and $P_m$. Motivated by this concern, in this section we propose to use the residual orthonormalization technique to enhance the numerical stability\nof the Block COCG and Block COCR algorithms. This efficient technique was introduced in~\\cite{AADR} in the context of the Block CG method \\cite{DPOL}.\n\nLet the Block residual $R_m$ be factored as $R_m = Q_m\\xi_m$ by conventional QR\nfactorization\\footnote{For our practical implementation, we use MATLAB qr-function\n``\\texttt{qr}($W$,0)\" for a given matrix $W\\in\\mathbb{C}^{n\\times p}$.}, with $Q^{H}_m Q_m = I_p$. Here $I_p$ denotes the identity\nmatrix of order $p$ and $\\xi_m\\in \\mathbb{C}^{p\\times p}$. From (\\ref{author_mini4:eq:01x}), the following equation can be obtained\n\n", "index": 11, "text": "\\begin{equation}\nQ_{m +1}\\tau_{m + 1} = Q_m - AS_m \\alpha'_k.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"Q_{m+1}\\tau_{m+1}=Q_{m}-AS_{m}\\alpha^{\\prime}_{k}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>Q</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><msub><mi>\u03c4</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><msub><mi>Q</mi><mi>m</mi></msub><mo>-</mo><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>S</mi><mi>m</mi></msub><mo>\u2062</mo><msubsup><mi>\u03b1</mi><mi>k</mi><mo>\u2032</mo></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]