[{"file": "1601.04920.tex", "nexttext": "\nIt implies that $f_0$ is Lipschitz continuous: $|f_0(z)-f_0(z')| \\leq \n\\epsilon^{-1} |z - z'|$, for $(z,z') \\in \\Phi(\\Omega)^2$. \nIn a classification problem, $f(x) \\neq f(x')$ means that $x$ and $x '$ are\nnot in the same class. The Lipschitz separation condition (\\ref{minsdf01}) \nbecomes a margin condition specifying a minimum distance across classes:\n\n", "itemtype": "equation", "pos": 8123, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nDeep convolutional networks provide state of the art\nclassifications and regressions results\nover many high-dimensional problems.\nWe review their architecture, which scatters data with \na cascade of linear filter weights and non-linearities. \nA mathematical framework is introduced to analyze \ntheir properties.\nComputations of invariants involve \nmultiscale contractions, the linearization of hierarchical symmetries,\nand sparse separations.\nApplications are discussed. \\\\\n\\end{abstract}\n\n\\section{Introduction}\n\nSupervised learning is a high-dimensional interpolation problem.\nWe approximate a function\n$f(x)$ from $q$ training samples $\\{x^i, f(x^i)\\}_{i \\leq q}$, \nwhere $x$ is a data vector of very high dimension $d$.\nThis dimension is often\nlarger than $10^6$, for images or other large size signals. \nDeep convolutional neural networks \nhave recently obtained remarkable experimental results \\cite{nature}.  \nThey give state of the art performances for image\nclassification with thousands of complex classes \\cite{Krizhevsky},\nspeech recognition \\cite{speech}, bio-medical applications \\cite{Leung},\nnatural language understanding \\cite{language}, and in many other domains. \nThey are also\nstudied as neuro-physiological models of vision \\cite{poggio}.\n\n\nMultilayer neural networks are computational learning architectures\nwhich propagate the input data across a sequence of linear operators\nand simple non-linearities.\nThe properties of shallow networks, with one hidden layer, are well understood\nas decompositions in\nfamilies of ridge functions \\cite{ridglet}. However, these approaches do not \nextend to networks with more layers.\nDeep convolutional neural networks, introduced by Le Cun \\cite{LeCun}, \nare implemented with linear convolutions followed by non-linearities,\nover typically more than $5$ layers.\nThese complex programmable machines, \ndefined by potentially billions of filter weights, \nbring us to a different mathematical world. \n\nMany researchers have pointed out that deep convolution\nnetworks are computing progressively more powerful invariants as \ndepth increases \\cite{poggio,nature}, but relations with networks weights\nand non-linearities are complex. This paper aims at clarifying important\nprinciples which govern the properties \nof such networks, but their\narchitecture and weights may differ with applications. \nWe show that computations of invariants involve \nmultiscale contractions, the linearization of hierarchical symmetries,\nand sparse separations. This conceptual basis \nis only a first step towards a full mathematical understanding of \nconvolutional network properties.\n\nIn high dimension, $x$ has a considerable number of parameters, which\nis a dimensionality curse.\nSampling uniformly a volume of dimension $d$ requires a number of samples\nwhich grows exponentially with $d$.\nIn most applications, the number $q$ of training samples rather grows linearly\nwith $d$. It is possible to approximate $f(x)$ with so few samples,\nonly if $f$ has some strong regularity properties allowing to ultimately\nreduce the dimension of the estimation. \nAny learning algorithm, including deep convolutional networks,\nthus relies on an underlying assumption of regularity. Specifying the \nnature of this regularity is one of the core mathematical problem.\n\nOne can try to circumvent the curse of dimensionality\nby reducing the variability or the dimension of $x$, without sacrificing the ability to approximate \n$f(x)$. This is done by defining a new variable $\\Phi(x)$ where\n$\\Phi$ is a {\\it contractive} operator which reduces \nthe range of variations of $x$, while still {\\it separating} different values\nof $f$: $\\Phi(x) \\neq \\Phi(x')$ if $f(x) \\neq f(x')$. This\nseparation-contraction trade-off needs to be adjusted to the properties of $f$.\n\nLinearization is a strategy used in machine learning to \nreduce the dimension with a linear\nprojector. A low-dimensional linear projection of $x$ can separate the values of\n$f$ if this function remains constant in the direction of \na high-dimensional linear space. \nThis is rarely the case, but one can try to find $\\Phi(x)$ which\nlinearizes high-dimensional domains where $f(x)$ remains constant.\nThe dimension is then reduced by applying a low-dimensional\nlinear projector on $\\Phi(x)$. Finding such a $\\Phi$ is the\ndream of kernel learning algorithms, explained in Section \\ref{kernel}.\n\nDeep neural networks are more conservative. \nThey progressively \ncontract the space and linearize transformations along which\n$f$ remains nearly constant, to preserve separation.\nSuch directions are defined by linear operators which belong to\ngroups of local symmetries, introduced in Section \\ref{symmdiffsec}.\nTo understand the difficulty to linearize the action of high-dimensional \ngroups of operators, we begin with the groups of translations and \ndiffeomorphisms, which deform signals. \nThey capture essential mathematical properties that are extended\nto general deep network symmetries, in Section \\ref{grnsdfssec}.\n\nTo linearize diffeomorphisms and \npreserve separability, Section \\ref{wavelet-sec} shows that we must\nseparate the variations of $x$ at different scales, \nwith a wavelet transform. This is implemented with\nmultiscale filter convolutions, which are building blocks\nof deep convolution filtering. General deep network architectures\nare introduced in Section \\ref{DeepNet}. They iterate\non linear operators which filter and\nlinearly combine different channels in each network layer, \nfollowed by contractive non-linearities.\n\nTo understand how non-linear contractions interact with linear operators,\nSection \\ref{scatnsfoisdnf} begins with simpler networks which\ndo not recombine channels in each layer.\nIt defines a non-linear scattering transform, introduced in \\cite{mallat-math},\nwhere wavelets have a separation and linearization role.\nThe resulting \ncontraction, linearization and separability properties are\nreviewed. We shall see that sparsity is important for separation. \n\n\nSection \\ref{grnsdfssec} extends these ideas to a more general class of\ndeep convolutional networks.\nChannel combinations provide the flexibility needed to extend\ntranslations to larger groups of local symmetries adapted\nto $f$. The network is structured by\nfactorizing groups of symmetries, in which case all linear operators\nare generalized convolutions.\nComputations are ultimately performed with filter weights, which are\nlearned. Their relation with groups of symmetries is\nexplained. A major issue is to preserve a separation margin\nacross classification frontiers. Deep convolutional networks have the ability\nto do so, by separating network fibers \nwhich are progressively more invariant and specialized. \nThis can give rise to invariant grandmother type neurons observed in deep networks\n\\cite{grandmother}.\nThe paper studies architectures as opposed to \ncomputational learning of network weights,\nwhich is an outstanding optimization issue \\cite{nature}.\n\n{\\bf Notations} $\\|z\\|$ is a Euclidean norm if $z$ is a vector in a Euclidean\nspace. If $z$ is a function in ${\\bf L^2}$ then $\\|z\\|^2 = \\int |z(u)|^2 du$.\nIf $z = \\{z_k \\}_k$\nis a sequence of vectors or functions then $\\|z\\|^2 = \\sum_k \\|z_k \\|^2$. \n\n\\section{Linearization, Projection  and Separability}\n\\label{kernel}\n\nSupervised learning computes an\napproximation $\\tilde f(x)$ of a function $f(x)$ \nfrom  $q$ training samples $\\{x^i ,  f(x^i) \\}_{i\\leq q}$,  \nfor $x = (x(1),...,x(d))\\in \\Omega$.\nThe domain $\\Omega$ is a high dimensional open \nsubset of ${\\mathbb R}^d$, not a low-dimensional manifold.\nIn a regression problem, $f(x)$ takes its values in ${\\mathbb R}$, whereas\nin classification its values are class indices.\n\n\\paragraph{Separation} Ideally, we would like to reduce the dimension of $x$ by computing\na low dimensional vector $\\Phi(x)$ such that \none can write $f(x) = f_0 (\\Phi(x))$. It is equivalent to \nimpose that if $f(x) \\neq f(x')$ then\n$\\Phi(x) \\neq \\Phi(x')$. We then say that $\\Phi$ {\\it separates} $f$.\nFor regression problems, to guarantee that $f_0$ is regular, we further\nimpose that the separation is Lipschitz:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{minsdf01}\n\\exists \\epsilon>0~~ \\forall (x,x') \\in \\Omega^2 ~~,~~\n\\|\\Phi(x) - \\Phi(x') \\| \\geq \\epsilon\\, |f(x) - f(x')|~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\exists\\epsilon&gt;0~{}~{}\\forall(x,x^{\\prime})\\in\\Omega^{2}~{}~{},~{}~{}\\|\\Phi(x%&#10;)-\\Phi(x^{\\prime})\\|\\geq\\epsilon\\,|f(x)-f(x^{\\prime})|~{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2203</mo><mi>\u03f5</mi></mrow><mo>&gt;</mo><mrow><mpadded width=\"+6.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2208</mo><mpadded width=\"+6.6pt\"><msup><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msup></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mo>\u2225</mo><mrow><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mo>\u2265</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03f5</mi></mpadded><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nWe can try to find a linear\nprojection of $x$ in some space ${\\bf V}$ of lower dimension $k$,\nwhich separates $f$. It requires that\n$f(x) = f(x + z)$ for all $z \\in {\\bf V}^\\perp$, where ${\\bf V}^\\perp$ is the orthogonal\ncomplement of ${\\bf V}$ in ${\\mathbb R}^d$, of dimension $d-k$.\nIn most cases, the final dimension $k$ can not be much smaller than $d$.\n\n\\paragraph{Linearization} An alternative strategy is to\nlinearize the variations of $f$ with a first change of variable\n$\\Phi(x) = \\{ \\phi_k (x) \\}_{k \\leq d'}$ of dimension $d'$ potentially\nmuch larger than the dimension $d$ of $x$. We can then optimize\na low-dimensional linear projection along directions where $f$ is constant.\nWe say that $\\Phi$ {\\it separates $f$ linearly} if $f(x)$ is well approximated\nby a one-dimensional projection:\n\n", "itemtype": "equation", "pos": 8639, "prevtext": "\nIt implies that $f_0$ is Lipschitz continuous: $|f_0(z)-f_0(z')| \\leq \n\\epsilon^{-1} |z - z'|$, for $(z,z') \\in \\Phi(\\Omega)^2$. \nIn a classification problem, $f(x) \\neq f(x')$ means that $x$ and $x '$ are\nnot in the same class. The Lipschitz separation condition (\\ref{minsdf01}) \nbecomes a margin condition specifying a minimum distance across classes:\n\n", "index": 3, "text": "\\begin{equation}\n\\label{minsdf010}\n\\exists \\epsilon>0~~ \\forall (x,x') \\in \\Omega^2 ~~,~~\n\\|\\Phi(x) - \\Phi(x') \\| \\geq \\epsilon\\, ~~\\mbox{if}~~f(x) \\neq f(x')~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\exists\\epsilon&gt;0~{}~{}\\forall(x,x^{\\prime})\\in\\Omega^{2}~{}~{},~{}~{}\\|\\Phi(x%&#10;)-\\Phi(x^{\\prime})\\|\\geq\\epsilon\\,~{}~{}\\mbox{if}~{}~{}f(x)\\neq f(x^{\\prime})~%&#10;{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2203</mo><mi>\u03f5</mi></mrow><mo>&gt;</mo><mrow><mpadded width=\"+6.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2208</mo><mpadded width=\"+6.6pt\"><msup><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msup></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mo>\u2225</mo><mrow><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mo>\u2265</mo><mrow><mpadded width=\"+8.3pt\"><mi>\u03f5</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe regression vector $w$ is optimized by\nminimizing a loss on the training data, which needs to be\nregularized if $d' > q$, for example\nby an $\\bf l^p$ norm of $w$ with a regularization constant $\\lambda$:\n\n", "itemtype": "equation", "pos": 9617, "prevtext": "\nWe can try to find a linear\nprojection of $x$ in some space ${\\bf V}$ of lower dimension $k$,\nwhich separates $f$. It requires that\n$f(x) = f(x + z)$ for all $z \\in {\\bf V}^\\perp$, where ${\\bf V}^\\perp$ is the orthogonal\ncomplement of ${\\bf V}$ in ${\\mathbb R}^d$, of dimension $d-k$.\nIn most cases, the final dimension $k$ can not be much smaller than $d$.\n\n\\paragraph{Linearization} An alternative strategy is to\nlinearize the variations of $f$ with a first change of variable\n$\\Phi(x) = \\{ \\phi_k (x) \\}_{k \\leq d'}$ of dimension $d'$ potentially\nmuch larger than the dimension $d$ of $x$. We can then optimize\na low-dimensional linear projection along directions where $f$ is constant.\nWe say that $\\Phi$ {\\it separates $f$ linearly} if $f(x)$ is well approximated\nby a one-dimensional projection:\n\n", "index": 5, "text": "\\begin{equation}\n\\label{minsdf0}\n\\tilde f(x) = {\\langle} \\Phi(x) \\, , w {\\rangle} = \\sum_{k=1}^{d'} w_k \\, \\phi_k (x) ~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{f}(x)={\\langle}\\Phi(x)\\,,w{\\rangle}=\\sum_{k=1}^{d^{\\prime}}w_{k}\\,\\phi_%&#10;{k}(x)~{}.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>w</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>d</mi><mo>\u2032</mo></msup></munderover><mrow><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msub><mi>\u03d5</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nSparse regressions are obtained with $p \\leq 1$, whereas $p=2$ defines\nkernel regressions \\cite{Friedman}. \n\nClassification problems are addressed similarly, by approximating the\nfrontiers between classes. For example, a classification with $Q$ classes\ncan be reduced to $Q-1$ ``one versus all'' binary classifications.\nEach binary classification is specified by an\n$f(x)$ equal to $1$ or $-1$ in each class. \nWe approximate $f(x)$ by \n$\\tilde f(x) = {\\rm sign}({\\langle} \\Phi(x) , w {\\rangle})$, where $w$ minimizes the\ntraining error (\\ref{minsdf}).\n\n\n\n\n\\section{Invariants, Symmetries and Diffeomorphisms}\n\\label{symmdiffsec}\n\nWe now study strategies to compute a change of variables $\\Phi$ which linearizes\n$f$. Deep convolutional networks operate layer per layer\nand linearize $f$ progressively, as depth increases.\nClassification and regression problems are addressed similarly by considering\nthe level sets of $f$, defined by\n$\\Omega_t = \\{x ~:~f(x) = t \\}$ if $f$ is continuous.\nFor classification, each level set is a particular class.\nLinear separability means that one can find $w$ such\nthat $f(x) \\approx {\\langle} \\Phi(x) , w {\\rangle}$.\nIf $x \\in \\Omega_t$ then ${\\langle} \\Phi(x) , w {\\rangle} \\approx t$, so all $\\Omega_t$\nare mapped by $\\Phi$ in different hyperplanes\northogonal to some $w$. The change of variable linearizes the level sets of $f$.\n\n\\paragraph{Symmetries} To linearize level sets, we need to find directions along which $f(x)$\ndoes not vary locally, and then linearize these directions in order to map\nthem in a linear space. It is tempting to try to do this with some\nlocal data analysis along $x$.\nThis is not possible because the training set includes few \nclose neighbors in high dimension. \nWe thus consider simultaneously\nall points $x \\in \\Omega$ and look for \ncommon directions along which $f(x)$ does not vary.\nThis is where groups of symmetries come in. \nTranslations and diffeomorphisms will illustrate the\ndifficulty to linearize high dimensional symmetries, \nand provide a first mathematical ground\nto analyze convolution networks architectures. \n\nWe look for invertible operators \nwhich preserve the value of $f$.\nThe action of an operator $g$ on $x$ is written $g.x$. A global\nsymmetry is an invertible and often non-linear \noperator $g$ from $\\Omega$ to $\\Omega$,\nsuch that $f(g.x) = f(x)$ for all $x \\in \\Omega$.\nIf $g_1$ and $g_2$ are global symmetries then $g_1.g_2$ is also\na global symmetry, so products define groups of symmetries. \nGlobal symmetries are usually \nhard to find. We shall first concentrate on local symmetries.\nWe suppose that there is a\nmetric $|g|_G$ which measures the distance between $g \\in G$ and the identity.\nA function $f$ is locally invariant to the action of $G$ if\n\n", "itemtype": "equation", "pos": 9960, "prevtext": "\nThe regression vector $w$ is optimized by\nminimizing a loss on the training data, which needs to be\nregularized if $d' > q$, for example\nby an $\\bf l^p$ norm of $w$ with a regularization constant $\\lambda$:\n\n", "index": 7, "text": "\\begin{equation}\n\\label{minsdf}\n\\sum_{i=1}^q {\\rm loss}(f(x^i) - \\tilde f(x^i)) + \\lambda\\, \\sum_{k = 1}^{d'}\n|w_k|^p~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{q}{\\rm loss}(f(x^{i})-\\tilde{f}(x^{i}))+\\lambda\\,\\sum_{k=1}^{d^{%&#10;\\prime}}|w_{k}|^{p}~{}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><mrow><mi>loss</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03bb</mi></mpadded><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>d</mi><mo>\u2032</mo></msup></munderover><mpadded width=\"+3.3pt\"><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nWe then say that $G$ is a \ngroup of local symmetries of $f$. The constant $C_x$ is the local\nrange of symmetries which preserve $f$. \nSince $\\Omega$ is a continuous subset of ${\\mathbb R}^d$, we consider groups of operators\nwhich transport vectors in $\\Omega$ with a continuous parameter. \nThey are called Lie groups if the group \nhas a differential structure.\n\n\\paragraph{Translations and diffeomorphisms}\nLet us interpolate the $d$ samples of $x$ and define\n$x(u)$ for all  $u \\in {\\mathbb R}^n$, with $n=1,2,3$ respectively for time-series, \nimages and volumetric data. The translation group $G = {\\mathbb R}^n$ is an example of Lie group.\nThe action of  $g \\in G = {\\mathbb R}^n$ over $x \\in \\Omega$ is\n$g.x(u) = x(u-g)$. The distance $|g|_G$ between $g$ and the identity\nis the Euclidean norm of $g \\in {\\mathbb R}^n$. The function\n$f$ is locally invariant to translations if sufficiently small\ntranslations of $x$ do not change $f(x)$.\nDeep convolutional networks compute\nconvolutions, because they assume that translations are local symmetries of $f$.\nThe dimension of a group $G$ is the number of\ngenerators which define all group elements by products. \nFor $G = {\\mathbb R}^n$ it is equal to $n$. \n\n\nTranslations are not powerful symmetries because\nthey are defined by only $n$ variables, and $n=2$ for images.\nMany image classification problems are also locally invariant to small deformations,\nwhich provide much stronger constraints.\nIt means that $f$ is locally\ninvariant to diffeomorphisms $G = {\\rm {Diff}}({\\mathbb R}^n)$, which transform $x(u)$ with a\ndifferential warping of $u \\in {\\mathbb R}^n$. We do not know in advance what is the\nlocal range of diffeomorphism symmetries.\nFor example, to classify images $x$ of hand-written digits, \ncertain deformations of $x$ will\npreserve a digit class but modify the class of another digit.\nWe shall linearize small diffeomorphims $g$. \nIn a space where local symmetries are linearized, we can\nfind global symmetries by\noptimizing linear projectors\nwhich preserve the values of $f(x)$, and thus reduce dimensionality.\n\nLocal symmetries are linearized by finding a change of variable $\\Phi(x)$\nwhich locally linearizes the action of $g \\in G$.\nWe say that $\\Phi$ is Lipschitz continuous if\n\n", "itemtype": "equation", "pos": 12843, "prevtext": "\nSparse regressions are obtained with $p \\leq 1$, whereas $p=2$ defines\nkernel regressions \\cite{Friedman}. \n\nClassification problems are addressed similarly, by approximating the\nfrontiers between classes. For example, a classification with $Q$ classes\ncan be reduced to $Q-1$ ``one versus all'' binary classifications.\nEach binary classification is specified by an\n$f(x)$ equal to $1$ or $-1$ in each class. \nWe approximate $f(x)$ by \n$\\tilde f(x) = {\\rm sign}({\\langle} \\Phi(x) , w {\\rangle})$, where $w$ minimizes the\ntraining error (\\ref{minsdf}).\n\n\n\n\n\\section{Invariants, Symmetries and Diffeomorphisms}\n\\label{symmdiffsec}\n\nWe now study strategies to compute a change of variables $\\Phi$ which linearizes\n$f$. Deep convolutional networks operate layer per layer\nand linearize $f$ progressively, as depth increases.\nClassification and regression problems are addressed similarly by considering\nthe level sets of $f$, defined by\n$\\Omega_t = \\{x ~:~f(x) = t \\}$ if $f$ is continuous.\nFor classification, each level set is a particular class.\nLinear separability means that one can find $w$ such\nthat $f(x) \\approx {\\langle} \\Phi(x) , w {\\rangle}$.\nIf $x \\in \\Omega_t$ then ${\\langle} \\Phi(x) , w {\\rangle} \\approx t$, so all $\\Omega_t$\nare mapped by $\\Phi$ in different hyperplanes\northogonal to some $w$. The change of variable linearizes the level sets of $f$.\n\n\\paragraph{Symmetries} To linearize level sets, we need to find directions along which $f(x)$\ndoes not vary locally, and then linearize these directions in order to map\nthem in a linear space. It is tempting to try to do this with some\nlocal data analysis along $x$.\nThis is not possible because the training set includes few \nclose neighbors in high dimension. \nWe thus consider simultaneously\nall points $x \\in \\Omega$ and look for \ncommon directions along which $f(x)$ does not vary.\nThis is where groups of symmetries come in. \nTranslations and diffeomorphisms will illustrate the\ndifficulty to linearize high dimensional symmetries, \nand provide a first mathematical ground\nto analyze convolution networks architectures. \n\nWe look for invertible operators \nwhich preserve the value of $f$.\nThe action of an operator $g$ on $x$ is written $g.x$. A global\nsymmetry is an invertible and often non-linear \noperator $g$ from $\\Omega$ to $\\Omega$,\nsuch that $f(g.x) = f(x)$ for all $x \\in \\Omega$.\nIf $g_1$ and $g_2$ are global symmetries then $g_1.g_2$ is also\na global symmetry, so products define groups of symmetries. \nGlobal symmetries are usually \nhard to find. We shall first concentrate on local symmetries.\nWe suppose that there is a\nmetric $|g|_G$ which measures the distance between $g \\in G$ and the identity.\nA function $f$ is locally invariant to the action of $G$ if\n\n", "index": 9, "text": "\\begin{equation}\n\\label{localsym}\n\\forall x \\in \\Omega~~,~~\\exists C_x > 0~~,~~\\forall g \\in G~~\\mbox{with}~~\n|g|_G < C_x~~,~~f(g.x) = f(x)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\forall x\\in\\Omega~{}~{},~{}~{}\\exists C_{x}&gt;0~{}~{},~{}~{}\\forall g\\in G~{}~{%&#10;}\\mbox{with}~{}~{}|g|_{G}&lt;C_{x}~{}~{},~{}~{}f(g.x)=f(x)~{}.\" display=\"block\"><mrow><mo>\u2200</mo><mi>x</mi><mo>\u2208</mo><mpadded width=\"+6.6pt\"><mi mathvariant=\"normal\">\u03a9</mi></mpadded><mo rspace=\"9.1pt\">,</mo><mo>\u2203</mo><msub><mi>C</mi><mi>x</mi></msub><mo>&gt;</mo><mpadded width=\"+6.6pt\"><mn>0</mn></mpadded><mo rspace=\"9.1pt\">,</mo><mo>\u2200</mo><mi>g</mi><mo>\u2208</mo><mpadded width=\"+6.6pt\"><mi>G</mi></mpadded><mpadded width=\"+6.6pt\"><mtext>with</mtext></mpadded><mo stretchy=\"false\">|</mo><mi>g</mi><msub><mo stretchy=\"false\">|</mo><mi>G</mi></msub><mo>&lt;</mo><mpadded width=\"+6.6pt\"><msub><mi>C</mi><mi>x</mi></msub></mpadded><mo rspace=\"9.1pt\">,</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe norm $\\|x\\|$ is just a normalization factor often set to $1$.\nThe Radon-Nikodim property proves that the map that transforms $g$ into\n$\\Phi(g.x)$  is almost everywhere differentiable in the sense of Gateaux.\nIf $|g|_G$ is small then  $\\Phi(x) - \\Phi(g.x)$\nis closely approximated by a bounded linear operator of $g$,\nwhich is the G\\^ateaux derivative. Locally, it thus nearly remains in \na linear space.\n\nLipschitz continuity over diffeomorphisms is defined relatively to a metric,\nwhich is now defined. A small diffeomorphism acting on $x(u)$ \ncan be written as a translation of $u$ by a $g(u)$:\n\n", "itemtype": "equation", "pos": 15250, "prevtext": "\nWe then say that $G$ is a \ngroup of local symmetries of $f$. The constant $C_x$ is the local\nrange of symmetries which preserve $f$. \nSince $\\Omega$ is a continuous subset of ${\\mathbb R}^d$, we consider groups of operators\nwhich transport vectors in $\\Omega$ with a continuous parameter. \nThey are called Lie groups if the group \nhas a differential structure.\n\n\\paragraph{Translations and diffeomorphisms}\nLet us interpolate the $d$ samples of $x$ and define\n$x(u)$ for all  $u \\in {\\mathbb R}^n$, with $n=1,2,3$ respectively for time-series, \nimages and volumetric data. The translation group $G = {\\mathbb R}^n$ is an example of Lie group.\nThe action of  $g \\in G = {\\mathbb R}^n$ over $x \\in \\Omega$ is\n$g.x(u) = x(u-g)$. The distance $|g|_G$ between $g$ and the identity\nis the Euclidean norm of $g \\in {\\mathbb R}^n$. The function\n$f$ is locally invariant to translations if sufficiently small\ntranslations of $x$ do not change $f(x)$.\nDeep convolutional networks compute\nconvolutions, because they assume that translations are local symmetries of $f$.\nThe dimension of a group $G$ is the number of\ngenerators which define all group elements by products. \nFor $G = {\\mathbb R}^n$ it is equal to $n$. \n\n\nTranslations are not powerful symmetries because\nthey are defined by only $n$ variables, and $n=2$ for images.\nMany image classification problems are also locally invariant to small deformations,\nwhich provide much stronger constraints.\nIt means that $f$ is locally\ninvariant to diffeomorphisms $G = {\\rm {Diff}}({\\mathbb R}^n)$, which transform $x(u)$ with a\ndifferential warping of $u \\in {\\mathbb R}^n$. We do not know in advance what is the\nlocal range of diffeomorphism symmetries.\nFor example, to classify images $x$ of hand-written digits, \ncertain deformations of $x$ will\npreserve a digit class but modify the class of another digit.\nWe shall linearize small diffeomorphims $g$. \nIn a space where local symmetries are linearized, we can\nfind global symmetries by\noptimizing linear projectors\nwhich preserve the values of $f(x)$, and thus reduce dimensionality.\n\nLocal symmetries are linearized by finding a change of variable $\\Phi(x)$\nwhich locally linearizes the action of $g \\in G$.\nWe say that $\\Phi$ is Lipschitz continuous if\n\n", "index": 11, "text": "\\begin{equation}\n\\label{addLipsch3}\n\\exists C > 0~,~\\forall (x,g) \\in \\Omega \\times G~~,~~\n\\|\\Phi(g.x) - \\Phi(x) \\| \\leq C\\, |g|_G\\, \\|x\\|~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\exists C&gt;0~{},~{}\\forall(x,g)\\in\\Omega\\times G~{}~{},~{}~{}\\|\\Phi(g.x)-\\Phi(x%&#10;)\\|\\leq C\\,|g|_{G}\\,\\|x\\|~{}.\" display=\"block\"><mrow><mo>\u2203</mo><mi>C</mi><mo>&gt;</mo><mpadded width=\"+3.3pt\"><mn>0</mn></mpadded><mo rspace=\"5.8pt\">,</mo><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi><mo>\u00d7</mo><mpadded width=\"+6.6pt\"><mi>G</mi></mpadded><mo rspace=\"9.1pt\">,</mo><mo>\u2225</mo><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2225</mo><mo>\u2264</mo><mpadded width=\"+1.7pt\"><mi>C</mi></mpadded><mo stretchy=\"false\">|</mo><mi>g</mi><mpadded width=\"+1.7pt\"><msub><mo stretchy=\"false\">|</mo><mi>G</mi></msub></mpadded><mo>\u2225</mo><mi>x</mi><mo rspace=\"5.8pt\">\u2225</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThis diffeomorphism translates points by at most $\\|g\\|_\\infty = \\sup_{u \\in {\\mathbb R}^n} |g(u)|$. \nLet $|\\nabla g(u)|$ be the matrix  norm of the Jacobian matrix of $g$ at $u$.\nSmall diffeomorphisms correspond to \n$\\|\\nabla g \\|_\\infty = \\sup_u |\\nabla g(u)| < 1$. \nApplying a diffeomorphism $g$ transforms two points $(u_1,u_2)$ into\n$(u_1-g(u_1),u_2-g(u_2))$. Their distance is thus multiplied by a\nscale factor, which is bounded above and below by\n$1 \\pm \\|\\nabla g \\|_\\infty$.\nThe distance of this diffeomorphism to the identity is defined by:\n\n", "itemtype": "equation", "pos": 16007, "prevtext": "\nThe norm $\\|x\\|$ is just a normalization factor often set to $1$.\nThe Radon-Nikodim property proves that the map that transforms $g$ into\n$\\Phi(g.x)$  is almost everywhere differentiable in the sense of Gateaux.\nIf $|g|_G$ is small then  $\\Phi(x) - \\Phi(g.x)$\nis closely approximated by a bounded linear operator of $g$,\nwhich is the G\\^ateaux derivative. Locally, it thus nearly remains in \na linear space.\n\nLipschitz continuity over diffeomorphisms is defined relatively to a metric,\nwhich is now defined. A small diffeomorphism acting on $x(u)$ \ncan be written as a translation of $u$ by a $g(u)$:\n\n", "index": 13, "text": "\\begin{equation}\n\\label{diffaction}\ng.x (u) = x(u - g(u))~~ \\mbox{with}~~g \\in {\\bf C^1} ({\\mathbb R}^n)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"g.x(u)=x(u-g(u))~{}~{}\\mbox{with}~{}~{}g\\in{\\bf C^{1}}({\\mathbb{R}}^{n})~{}.\" display=\"block\"><mrow><mrow><mi>g</mi><mo>.</mo><mrow><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"9.1pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mtext>with</mtext></mpadded><mo>\u2062</mo><mi>g</mi></mrow><mo>\u2208</mo><mrow><msup><mi>\ud835\udc02</mi><mn>\ud835\udfcf</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u211d</mi><mi>n</mi></msup><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe factor $2^J$ is a local translation invariance scale. It gives\nthe range of translations over which small diffeomorphisms are\nlinearized. For $J = \\infty$ the metric is globally invariant\nto translations.\n\n\\section{Contractions and Scale Separation with Wavelets}\n\\label{wavelet-sec}\n\nDeep convolutional networks can linearize the action of very complex non-linear\ntransformations in high dimensions, such as inserting glasses in images of \nfaces \\cite{faces}. A transformation of $x \\in \\Omega$ \nis a transport of $x$ in $\\Omega$. \nTo understand how to linearize any such transport, we shall begin\nwith translations and diffeomorphisms.\nDeep network architectures are covariant to translations, because all\nlinear operators are implemented with convolutions. \nTo compute invariants to translations and linearize diffeomorphisms,\nwe need to separate scales and apply a non-linearity. \nThis is implemented with a cascade of filters computing\na wavelet transform, and a pointwise contractive non-linearity. \nSection \\ref{grnsdfssec} extends these tools to general group actions.\n\n\\paragraph{Averaging} A linear operator\ncan compute local invariants to the action of the translation group $G$,\nby averaging $x$ along the orbit $\\{g.x\\}_{g \\in G}$,\nwhich are translations of $x$. This is done with a convolution by an\naveraging kernel $\\phi_J (u) = 2^{-nJ} \\phi(2^{-J} u)$ \nof size $2^J$, with $\\int \\phi(u)\\,du = 1$:\n\n", "itemtype": "equation", "pos": 16680, "prevtext": "\nThis diffeomorphism translates points by at most $\\|g\\|_\\infty = \\sup_{u \\in {\\mathbb R}^n} |g(u)|$. \nLet $|\\nabla g(u)|$ be the matrix  norm of the Jacobian matrix of $g$ at $u$.\nSmall diffeomorphisms correspond to \n$\\|\\nabla g \\|_\\infty = \\sup_u |\\nabla g(u)| < 1$. \nApplying a diffeomorphism $g$ transforms two points $(u_1,u_2)$ into\n$(u_1-g(u_1),u_2-g(u_2))$. Their distance is thus multiplied by a\nscale factor, which is bounded above and below by\n$1 \\pm \\|\\nabla g \\|_\\infty$.\nThe distance of this diffeomorphism to the identity is defined by:\n\n", "index": 15, "text": "\\begin{equation}\n\\label{diffmetrs}\n|g|_{{\\rm {Diff}}} = 2^{-J}\\, \\|g\\|_\\infty + \\|\\nabla g \\|_\\infty~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"|g|_{{\\rm{Diff}}}=2^{-J}\\,\\|g\\|_{\\infty}+\\|\\nabla g\\|_{\\infty}~{}.\" display=\"block\"><mrow><mrow><msub><mrow><mo stretchy=\"false\">|</mo><mi>g</mi><mo stretchy=\"false\">|</mo></mrow><mi>Diff</mi></msub><mo>=</mo><mrow><mrow><mpadded width=\"+1.7pt\"><msup><mn>2</mn><mrow><mo>-</mo><mi>J</mi></mrow></msup></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>g</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mo>+</mo><mpadded width=\"+3.3pt\"><msub><mrow><mo>\u2225</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nOne can verify \\cite{mallat-math} that this averaging is \nLipschitz continuous to diffeomorphisms for all $x \\in {\\bf L^2}({\\mathbb R}^n)$,\nover a translation range $2^J$.\nHowever, it eliminates the variations of $x$ above the frequency $2^{-J}$.\nIf  $J = \\infty$ then $\\Phi_\\infty x = \\int x(u)\\, du$,\nwhich eliminates nearly all information.\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[width=12cm]{Figure2w.png}\n\\caption{Wavelet transform of\nan image $x(u)$, computed with a cascade of convolutions \nwith filters over $J = 4$ scales and $K=4$\norientations. The low-pass and $K=4$ band-pass filters are shown on the\nfirst arrows.}\n\\label{figure2}\n\\end{figure*}\n\n\n\\paragraph{Wavelet transform} A diffeomorphism acts as a local translation and scaling of the variable\n$u$. If we let aside translations for now, to linearize \nsmall diffeomorphism we must linearize this scaling action.\nThis is done by separating the variations of $x$ at different scales \nwith wavelets. We define $K$ wavelets $\\psi_k (u)$ for $u \\in {\\mathbb R}^n$.\nThey are regular functions with a fast decay and\na zero average $\\int \\psi_k (u)\\, du = 0$. \nThese $K$ wavelets are dilated by $2^j$:\n$\\psi_{j,k} (u) = 2^{-j n} \\psi_k (2^{-j} u)$. \nA wavelet transform computes the local average of \n$x$ at a scale $2^J$, and variations \nat scales $2^j \\geq 2^J$ with wavelet convolutions:\n\n", "itemtype": "equation", "pos": 18216, "prevtext": "\nThe factor $2^J$ is a local translation invariance scale. It gives\nthe range of translations over which small diffeomorphisms are\nlinearized. For $J = \\infty$ the metric is globally invariant\nto translations.\n\n\\section{Contractions and Scale Separation with Wavelets}\n\\label{wavelet-sec}\n\nDeep convolutional networks can linearize the action of very complex non-linear\ntransformations in high dimensions, such as inserting glasses in images of \nfaces \\cite{faces}. A transformation of $x \\in \\Omega$ \nis a transport of $x$ in $\\Omega$. \nTo understand how to linearize any such transport, we shall begin\nwith translations and diffeomorphisms.\nDeep network architectures are covariant to translations, because all\nlinear operators are implemented with convolutions. \nTo compute invariants to translations and linearize diffeomorphisms,\nwe need to separate scales and apply a non-linearity. \nThis is implemented with a cascade of filters computing\na wavelet transform, and a pointwise contractive non-linearity. \nSection \\ref{grnsdfssec} extends these tools to general group actions.\n\n\\paragraph{Averaging} A linear operator\ncan compute local invariants to the action of the translation group $G$,\nby averaging $x$ along the orbit $\\{g.x\\}_{g \\in G}$,\nwhich are translations of $x$. This is done with a convolution by an\naveraging kernel $\\phi_J (u) = 2^{-nJ} \\phi(2^{-J} u)$ \nof size $2^J$, with $\\int \\phi(u)\\,du = 1$:\n\n", "index": 17, "text": "\\begin{equation}\n\\label{andv8sdfsdfs9}\n\\Phi_J x(u) = x \\star \\phi_J (u) ~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{J}x(u)=x\\star\\phi_{J}(u)~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03d5</mi><mi>J</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe parameter $u$ is \nsampled on a grid such that intermediate sample values can be recovered\nby linear interpolations. \nThe wavelets $\\psi_k$ are chosen so that ${\\cal W}$ is a contractive and invertible operator,\nand in order to obtain a sparse representation.\nThis means that $x \\star \\psi_{j,k}(u)$ is mostly zero besides few high\namplitude coefficients corresponding to variations of $x(u)$ \nwhich ``match'' $\\psi_k$ at the scale $2^j$.\nThis sparsity plays an important role in non-linear contractions.\n\nFor audio signals, $n=1$, sparse representations are usually\nobtained with at least $K = 12$ intermediate frequencies\nwithin each octave $2^j$, which are similar to half-tone musical notes. This is done\nby choosing a wavelet $\\psi(u)$ having a frequency bandwidth\nof less than $1/12$ octave and \n$\\psi_{k} (u) = 2^{k/K} \\psi (2^{-k/K} u)$ for $1 \\leq k \\leq K$.\nFor images, $n = 2$, we must discriminate image variations along\ndifferent spatial orientation. It is obtained by separating angles\n$\\pi k / K$, with an oriented wavelet which is rotated\n$\\psi_{k} (u) = \\psi(r_k^{-1} u)$. Intermediate rotated wavelets are approximated\nby linear interpolations of these $K$ wavelets.\nFigure \\ref{figure2} shows the wavelet transform of an image,\nwith $J = 4$ scales and $K = 4$ angles, where $x \\star \\psi_{j,k} (u)$ is\nsubsampled at intervals $2^j$. It has few \nlarge amplitude coefficients shown in white. \n\n\n\\paragraph{Filter bank} \nWavelet transforms can be computed with a fast multiscale cascade of\nfilters, which is at the core of deep network architectures.\nAt each scale $2^j$, we define a low-pass filter $w_{j,0}$ which increases\nthe averaging scale from $2^{j-1}$ to $2^j$,\nand band-pass filters $w_{j,k}$ which compute each wavelet:\n\n", "itemtype": "equation", "pos": 19661, "prevtext": "\nOne can verify \\cite{mallat-math} that this averaging is \nLipschitz continuous to diffeomorphisms for all $x \\in {\\bf L^2}({\\mathbb R}^n)$,\nover a translation range $2^J$.\nHowever, it eliminates the variations of $x$ above the frequency $2^{-J}$.\nIf  $J = \\infty$ then $\\Phi_\\infty x = \\int x(u)\\, du$,\nwhich eliminates nearly all information.\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[width=12cm]{Figure2w.png}\n\\caption{Wavelet transform of\nan image $x(u)$, computed with a cascade of convolutions \nwith filters over $J = 4$ scales and $K=4$\norientations. The low-pass and $K=4$ band-pass filters are shown on the\nfirst arrows.}\n\\label{figure2}\n\\end{figure*}\n\n\n\\paragraph{Wavelet transform} A diffeomorphism acts as a local translation and scaling of the variable\n$u$. If we let aside translations for now, to linearize \nsmall diffeomorphism we must linearize this scaling action.\nThis is done by separating the variations of $x$ at different scales \nwith wavelets. We define $K$ wavelets $\\psi_k (u)$ for $u \\in {\\mathbb R}^n$.\nThey are regular functions with a fast decay and\na zero average $\\int \\psi_k (u)\\, du = 0$. \nThese $K$ wavelets are dilated by $2^j$:\n$\\psi_{j,k} (u) = 2^{-j n} \\psi_k (2^{-j} u)$. \nA wavelet transform computes the local average of \n$x$ at a scale $2^J$, and variations \nat scales $2^j \\geq 2^J$ with wavelet convolutions:\n\n", "index": 19, "text": "\\begin{equation}\n\\label{wave1}\n{\\cal W} x = \\{ x \\star \\phi_J(u) \\, , \\, x \\star \\psi_{j,k}\n(u)\\}_{j \\leq J,  1 \\leq k \\leq K}~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\cal W}x=\\{x\\star\\phi_{J}(u)\\,,\\,x\\star\\psi_{j,k}(u)\\}_{j\\leq J,1\\leq k\\leq K%&#10;}~{}.\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>=</mo><mpadded width=\"+3.3pt\"><msub><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03d5</mi><mi>J</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mrow><mi>j</mi><mo>\u2264</mo><mi>J</mi></mrow><mo>,</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>k</mi><mo>\u2264</mo><mi>K</mi></mrow></mrow></msub></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nLet us write $x_{j} (u,0) = x \\star \\phi_j(u)$ and\n $x_{j} (u,k) = x \\star \\psi_{j,k}(u)$ for $k \\neq 0$.\nIt results from (\\ref{nsdf8sdf})  that for $0 < j \\leq J$\nand all $1 \\leq k \\leq K$:\n\n", "itemtype": "equation", "pos": 21555, "prevtext": "\nThe parameter $u$ is \nsampled on a grid such that intermediate sample values can be recovered\nby linear interpolations. \nThe wavelets $\\psi_k$ are chosen so that ${\\cal W}$ is a contractive and invertible operator,\nand in order to obtain a sparse representation.\nThis means that $x \\star \\psi_{j,k}(u)$ is mostly zero besides few high\namplitude coefficients corresponding to variations of $x(u)$ \nwhich ``match'' $\\psi_k$ at the scale $2^j$.\nThis sparsity plays an important role in non-linear contractions.\n\nFor audio signals, $n=1$, sparse representations are usually\nobtained with at least $K = 12$ intermediate frequencies\nwithin each octave $2^j$, which are similar to half-tone musical notes. This is done\nby choosing a wavelet $\\psi(u)$ having a frequency bandwidth\nof less than $1/12$ octave and \n$\\psi_{k} (u) = 2^{k/K} \\psi (2^{-k/K} u)$ for $1 \\leq k \\leq K$.\nFor images, $n = 2$, we must discriminate image variations along\ndifferent spatial orientation. It is obtained by separating angles\n$\\pi k / K$, with an oriented wavelet which is rotated\n$\\psi_{k} (u) = \\psi(r_k^{-1} u)$. Intermediate rotated wavelets are approximated\nby linear interpolations of these $K$ wavelets.\nFigure \\ref{figure2} shows the wavelet transform of an image,\nwith $J = 4$ scales and $K = 4$ angles, where $x \\star \\psi_{j,k} (u)$ is\nsubsampled at intervals $2^j$. It has few \nlarge amplitude coefficients shown in white. \n\n\n\\paragraph{Filter bank} \nWavelet transforms can be computed with a fast multiscale cascade of\nfilters, which is at the core of deep network architectures.\nAt each scale $2^j$, we define a low-pass filter $w_{j,0}$ which increases\nthe averaging scale from $2^{j-1}$ to $2^j$,\nand band-pass filters $w_{j,k}$ which compute each wavelet:\n\n", "index": 21, "text": "\\begin{equation}\n\\label{nsdf8sdf}\n\\phi_{j} = w_{j,0} \\star  \\phi_{j-1} ~~\\mbox{and}~~\n\\psi_{j,k} = w_{j,k} \\star \\phi_{j-1}~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\phi_{j}=w_{j,0}\\star\\phi_{j-1}~{}~{}\\mbox{and}~{}~{}\\psi_{j,k}=w_{j,k}\\star%&#10;\\phi_{j-1}~{}.\" display=\"block\"><mrow><mrow><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>=</mo><mrow><mrow><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mn>0</mn></mrow></msub><mo>\u22c6</mo><mpadded width=\"+6.6pt\"><msub><mi>\u03d5</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mpadded></mrow><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mtext>and</mtext></mpadded><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mrow><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>\u22c6</mo><mpadded width=\"+3.3pt\"><msub><mi>\u03d5</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThese convolutions  may be subsampled by $2$ along $u$, in which case\n$x_j (u,k)$ is sampled at intervals $2^j$ along $u$.\n\n\n\\paragraph{Phase removal} Wavelet coefficients $x_j(u,k) = x \\star \\psi_{j,k}(u)$ \noscillate at a scale $2^j$. \nTranslations of $x$ smaller than $2^j$ modifies the complex phase of\n$x_j (u,k)$ if the wavelet is complex or its sign if it is real. \nBecause of these oscillations, averaging $x_j$ with $\\phi_J$ outputs a zero signal.\nIt is necessary to apply a non-linearity which removes oscillations.\nA modulus $\\rho(\\alpha) = |\\alpha|$ computes such a positive envelop.\nAveraging\n$\\rho(x \\star \\psi_{j,k} (u))$ by $\\phi_J$ outputs non-zero coefficients\nwhich are locally invariant at a scale $2^J$:\n\n", "itemtype": "equation", "pos": 21887, "prevtext": "\nLet us write $x_{j} (u,0) = x \\star \\phi_j(u)$ and\n $x_{j} (u,k) = x \\star \\psi_{j,k}(u)$ for $k \\neq 0$.\nIt results from (\\ref{nsdf8sdf})  that for $0 < j \\leq J$\nand all $1 \\leq k \\leq K$:\n\n", "index": 23, "text": "\\begin{equation}\n\\label{fionsdfsdsf}\nx_{j} (u,k) = x_{j-1}(\\cdot,0) \\star w_{j,k} (u)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"x_{j}(u,k)=x_{j-1}(\\cdot,0)\\star w_{j,k}(u)~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nReplacing the modulus by\na rectifier $\\rho (\\alpha) = \\max(0 , \\alpha)$ gives nearly the same result, up \nto a factor $2$. \nOne can prove \\cite{mallat-math} that this representation is \nLipschitz continuous to actions of diffeomorphisms \nover $x \\in {\\bf L^2}({\\mathbb R}^n)$, and thus satisfies\n(\\ref{addLipsch3}) for the metric (\\ref{diffmetrs}).\nIndeed, the\nwavelet coefficients of $x$  deformed by $g$ can be written as the wavelet\ncoefficients of $x$ with deformed wavelets. Small deformations \nproduce small modifications of wavelets in ${\\bf L^2}({\\mathbb R}^n)$, because they are localized\nand regular. The resulting modifications of wavelet coefficients is of the\norder of the diffeomorphism metric $|g|_{{\\rm {Diff}}}$.\n\n\\paragraph{Contractions} A modulus and a rectifier are contractive non-linear pointwise operators:\n\n", "itemtype": "equation", "pos": 22714, "prevtext": "\nThese convolutions  may be subsampled by $2$ along $u$, in which case\n$x_j (u,k)$ is sampled at intervals $2^j$ along $u$.\n\n\n\\paragraph{Phase removal} Wavelet coefficients $x_j(u,k) = x \\star \\psi_{j,k}(u)$ \noscillate at a scale $2^j$. \nTranslations of $x$ smaller than $2^j$ modifies the complex phase of\n$x_j (u,k)$ if the wavelet is complex or its sign if it is real. \nBecause of these oscillations, averaging $x_j$ with $\\phi_J$ outputs a zero signal.\nIt is necessary to apply a non-linearity which removes oscillations.\nA modulus $\\rho(\\alpha) = |\\alpha|$ computes such a positive envelop.\nAveraging\n$\\rho(x \\star \\psi_{j,k} (u))$ by $\\phi_J$ outputs non-zero coefficients\nwhich are locally invariant at a scale $2^J$:\n\n", "index": 25, "text": "\\begin{equation}\n\\label{sdf}\n\\Phi_J x(u,j,k) = \\rho(x \\star \\psi_{j,k}) \\star \\phi_J (u)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{J}x(u,j,k)=\\rho(x\\star\\psi_{j,k})\\star\\phi_{J}(u)~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03d5</mi><mi>J</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nHowever, if $\\alpha = 0$ or $\\alpha' = 0$ then this inequality is an equality.\nReplacing $\\alpha$ and $\\alpha'$ by \n$x \\star \\psi_{j,k} (u)$ and $x' \\star \\psi_{j,k} (u)$ shows that distances\nare much less reduced if \n$x \\star \\psi_{j,k} (u)$ is sparse. Such contractions \ndo not reduce as\nmuch the distance between sparse signals and other signals. \nThis is illustrated by reconstruction examples in Section \\ref{scatnsfoisdnf}.\n\n\\paragraph{Scale separation limitations} \nThe local multiscale invariants in (\\ref{sdf}) have dominated\npattern classification applications for music, speech and images, until\n$2010$. It is called\n{\\it Mel-spectrum} for audio  \\cite{shamma} and SIFT type \nfeature vectors \\cite{SIFT} in images.\nTheir limitations comes from the loss of information produced by \nthe averaging by $\\phi_J$ in (\\ref{sdf}). To reduce this loss,\nthey are computed at short time scales $2^J \\leq 50 ms$ in audio signals, \nor over small image patches $2^{2J} = 16^2$ pixels.\nAs a consequence, they do not capture large scale structures, which are\nimportant for classification and regression problems. To build a rich set\nof local invariants at a large scale $2^J$,\nit is not sufficient to separate scales with wavelets, we must also \ncapture scale interactions.\n\nA similar issue appears\nin physics to characterize the interactions of complex systems.\nMultiscale separations are used to reduce the parametrization\nof classical many body systems, for example with multipole methods \n\\cite{Greengard}. However, it does not apply to\ncomplex interactions, as in quantum systems. \nInteractions across scales, between small and larger structures, must\nbe taken into account. Capturing these interactions with low-dimensional models\nis a major challenge. We shall see that deep neural networks and\nscattering transforms provide high order coefficients which partly characterize\nmultiscale interactions.\n\n\n\\section{Deep Convolutional Neural Network Architectures}\n\\label{DeepNet}\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[natwidth=2280,natheight=1450,width=12cm]{Figure10e.png}\n\\caption{A convolution network iteratively computes each layer $x_{j}$ by \ntransforming the previous layer $x_{j-1}$, with a linear operator $W_j$ and a pointwise non-linearity $\\rho$. }\n\\label{figure1}\n\\end{figure*}\n\nDeep convolutional networks are computational architectures introduced\nby Le Cun \\cite{LeCun}, providing remarkable regression and \nclassification results in high dimension \\cite{nature,Krizhevsky,speech}.\nWe describe these architectures illustrated by Figure \\ref{figure1}.\nThey iterate over linear operators $W_j$ including convolutions,\nand predefined pointwise non-linearities.\n\nA convolutional network takes in input a signal $x(u)$, which is here an image. \nAn internal network layer $x_j (u,k_j)$ at a depth $j$\nis indexed by the same translation variable $u$,\nusually subsampled, and a channel index $k_j$. \nA layer $x_{j}$ is computed from $x_{j-1}$ by applying a linear operator $W_j$\nfollowed by a pointwise non-linearity $\\rho$:\n", "itemtype": "equation", "pos": 23650, "prevtext": "\nReplacing the modulus by\na rectifier $\\rho (\\alpha) = \\max(0 , \\alpha)$ gives nearly the same result, up \nto a factor $2$. \nOne can prove \\cite{mallat-math} that this representation is \nLipschitz continuous to actions of diffeomorphisms \nover $x \\in {\\bf L^2}({\\mathbb R}^n)$, and thus satisfies\n(\\ref{addLipsch3}) for the metric (\\ref{diffmetrs}).\nIndeed, the\nwavelet coefficients of $x$  deformed by $g$ can be written as the wavelet\ncoefficients of $x$ with deformed wavelets. Small deformations \nproduce small modifications of wavelets in ${\\bf L^2}({\\mathbb R}^n)$, because they are localized\nand regular. The resulting modifications of wavelet coefficients is of the\norder of the diffeomorphism metric $|g|_{{\\rm {Diff}}}$.\n\n\\paragraph{Contractions} A modulus and a rectifier are contractive non-linear pointwise operators:\n\n", "index": 27, "text": "\\begin{equation}\n\\label{contractions}\n|\\rho(\\alpha) - \\rho(\\alpha')| \\leq |\\alpha - \\alpha'|. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"|\\rho(\\alpha)-\\rho(\\alpha^{\\prime})|\\leq|\\alpha-\\alpha^{\\prime}|.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03b1</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>\u03b1</mi><mo>-</mo><msup><mi>\u03b1</mi><mo>\u2032</mo></msup></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe non-linearity $\\rho$ \ntransforms each coefficient $\\alpha$ of the array $W_j x_{j-1}$, and satisfies\nthe contraction condition (\\ref{contractions}).\nA usual choice is the rectifier\n$\\rho(\\alpha) = \\max(\\alpha, 0)$ for $\\alpha \\in {\\mathbb R}$, but it can\nalso be a sigmoid, or a \nmodulus $\\rho(\\alpha) = |\\alpha|$ where $\\alpha$ may be complex. \n\nSince most classification and regression functions $f(x)$\nare invariant or covariant to translations, \nthe architecture imposes that $W_j$ is covariant to translations. The\noutput is translated if the input is translated. Since $W_j$ is linear,\nit can thus be written as a sum of convolutions:\n\n", "itemtype": "equation", "pos": 26796, "prevtext": "\nHowever, if $\\alpha = 0$ or $\\alpha' = 0$ then this inequality is an equality.\nReplacing $\\alpha$ and $\\alpha'$ by \n$x \\star \\psi_{j,k} (u)$ and $x' \\star \\psi_{j,k} (u)$ shows that distances\nare much less reduced if \n$x \\star \\psi_{j,k} (u)$ is sparse. Such contractions \ndo not reduce as\nmuch the distance between sparse signals and other signals. \nThis is illustrated by reconstruction examples in Section \\ref{scatnsfoisdnf}.\n\n\\paragraph{Scale separation limitations} \nThe local multiscale invariants in (\\ref{sdf}) have dominated\npattern classification applications for music, speech and images, until\n$2010$. It is called\n{\\it Mel-spectrum} for audio  \\cite{shamma} and SIFT type \nfeature vectors \\cite{SIFT} in images.\nTheir limitations comes from the loss of information produced by \nthe averaging by $\\phi_J$ in (\\ref{sdf}). To reduce this loss,\nthey are computed at short time scales $2^J \\leq 50 ms$ in audio signals, \nor over small image patches $2^{2J} = 16^2$ pixels.\nAs a consequence, they do not capture large scale structures, which are\nimportant for classification and regression problems. To build a rich set\nof local invariants at a large scale $2^J$,\nit is not sufficient to separate scales with wavelets, we must also \ncapture scale interactions.\n\nA similar issue appears\nin physics to characterize the interactions of complex systems.\nMultiscale separations are used to reduce the parametrization\nof classical many body systems, for example with multipole methods \n\\cite{Greengard}. However, it does not apply to\ncomplex interactions, as in quantum systems. \nInteractions across scales, between small and larger structures, must\nbe taken into account. Capturing these interactions with low-dimensional models\nis a major challenge. We shall see that deep neural networks and\nscattering transforms provide high order coefficients which partly characterize\nmultiscale interactions.\n\n\n\\section{Deep Convolutional Neural Network Architectures}\n\\label{DeepNet}\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[natwidth=2280,natheight=1450,width=12cm]{Figure10e.png}\n\\caption{A convolution network iteratively computes each layer $x_{j}$ by \ntransforming the previous layer $x_{j-1}$, with a linear operator $W_j$ and a pointwise non-linearity $\\rho$. }\n\\label{figure1}\n\\end{figure*}\n\nDeep convolutional networks are computational architectures introduced\nby Le Cun \\cite{LeCun}, providing remarkable regression and \nclassification results in high dimension \\cite{nature,Krizhevsky,speech}.\nWe describe these architectures illustrated by Figure \\ref{figure1}.\nThey iterate over linear operators $W_j$ including convolutions,\nand predefined pointwise non-linearities.\n\nA convolutional network takes in input a signal $x(u)$, which is here an image. \nAn internal network layer $x_j (u,k_j)$ at a depth $j$\nis indexed by the same translation variable $u$,\nusually subsampled, and a channel index $k_j$. \nA layer $x_{j}$ is computed from $x_{j-1}$ by applying a linear operator $W_j$\nfollowed by a pointwise non-linearity $\\rho$:\n", "index": 29, "text": "\n\\[\nx_{j}  = \\rho W_j x_{j-1} ~.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"x_{j}=\\rho W_{j}x_{j-1}~{}.\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><msub><mi>W</mi><mi>j</mi></msub><mo>\u2062</mo><mpadded width=\"+3.3pt\"><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe variable $u$ is usually subsampled. For a fixed $j$, all\nfilters $w_{j,k_{j}}(u,k)$ have the same support width along $u$,\ntypically smaller than $10$. \n\nThe operators $\\rho\\, W_j$ propagates the input signal $x_0 = x$ \nuntil the last layer $x_J$. \nThis cascade of spatial convolutions defines translation covariant\noperators of progressively\nwider supports as the depth $j$ increases. Each $x_j (u,k_j)$\nis a non-linear function of $x(v)$, for $v$ in a square centered at $u$,\nwhose width $\\Delta_j$ does not depend upon $k_j$. The width\n$\\Delta_j$ is the spatial scale of a layer $j$. \nIt is equal to $2^j\\, \\Delta$ if all filters $w_{j,k_{j}}$ have a\nwidth $\\Delta$ and the convolutions (\\ref{filtering}) are \nsubsampled by $2$.\n\n\nNeural networks include many side tricks. \nThey sometimes normalize the amplitude of\n$x_j (v,k)$, by dividing it by the norm of all coefficients \n$x_j (v,k)$ for $v$ in a neighborhood of $u$. This eliminates\nmultiplicative amplitude variabilities. Instead of subsampling \n(\\ref{filtering}) on a regular grid, a max pooling \nmay select the largest coefficients over each sampling cell.\nCoefficients may also be modified by subtracting a constant adapted\nto each coefficient. When applying a rectifier $\\rho$, this constant\nacts as a soft threshold, which increases sparsity.\nIt is usually observed that inside network coefficients\n$x_j (u,k_j)$ have a sparse activation. \n\nThe deep network output \n$x_J = \\Phi_J (x)$ is provided to a classifier, \nusually composed of fully connected neural network layers\n\\cite{nature}. Supervised deep learning algorithms\noptimize the filter values $w_{j,k_{j}}(u,k)$ in order\nto minimize the average classification or regression error on the training\nsamples $\\{x^i\\, , \\, f(x^i) \\}_{i \\leq q}$. \nThere can be more than $10^8$ variables in a network \\cite{nature}. \nThe filter update is done with a back-propagation algorithm,\nwhich may be computed with a stochastic gradient descent, with\nregularization procedures such as dropout.\nThis high-dimensional optimization is non-convex, but\ndespite the presence of many local minima, the regularized\nstochastic gradient descent converges to a local minimum providing good\naccuracy on test examples \\cite{benarous}. The rectifier non-linearity\n$\\rho$ is usually preferred\nbecause the resulting optimization has a better convergence.\nIt however requires a large number of training examples. \nSeveral hundreds of examples per class are usually\nneeded to reach a good performance.\n\nInstabilities have been observed in some network architectures\n\\cite{adversarial}, where additions of small perturbations on\n$x$ can produce large variations of $x_J$. It happens when the norms of\nthe matrices $W_j$ are larger than $1$, and hence amplified\nwhen cascaded. However, deep network also have a strong form of stability\nillustrated by transfer learning \\cite{nature}. A deep network\nlayer $x_J$ optimized on particular training databasis, can \napproximate different classification functions,\nif the final classification layers are trained on a new databasis. \nThis means that it has learned stable structures, which can be\ntransferred across similar learning problems. \n\n\n\\section{Scattering on the Translation Group}\n\\label{scatnsfoisdnf}\n\nA deep network alternates linear operators $W_j$ and \ncontractive non-linearities $\\rho$. To analyze the properties\nof this cascade, we begin with a simpler architecture, where $W_j$ does not\ncombine multiple convolutions across channels in each layer.\nWe show that such network coefficients are obtained through convolutions\nwith a reduced number of equivalent wavelet filters. It defines\na scattering transform \\cite{mallat-math} whose contraction and\nlinearization properties are reviewed.\nVariance reduction and loss of information are studied with reconstructions of\nstationary processes.\n\n\\paragraph{No channel combination} \nSuppose that $x_{j}(u,k_{j})$ is computed by convolving \na single channel $x_{j-1} (u,k_{j-1})$  along $u$:\n\n", "itemtype": "equation", "pos": 27477, "prevtext": "\nThe non-linearity $\\rho$ \ntransforms each coefficient $\\alpha$ of the array $W_j x_{j-1}$, and satisfies\nthe contraction condition (\\ref{contractions}).\nA usual choice is the rectifier\n$\\rho(\\alpha) = \\max(\\alpha, 0)$ for $\\alpha \\in {\\mathbb R}$, but it can\nalso be a sigmoid, or a \nmodulus $\\rho(\\alpha) = |\\alpha|$ where $\\alpha$ may be complex. \n\nSince most classification and regression functions $f(x)$\nare invariant or covariant to translations, \nthe architecture imposes that $W_j$ is covariant to translations. The\noutput is translated if the input is translated. Since $W_j$ is linear,\nit can thus be written as a sum of convolutions:\n\n", "index": 31, "text": "\\begin{equation}\n\\label{filtering}\n[W_j x_{j-1}] (u,k_j) =  \n\\sum_{k} \\sum_{v} x_{j-1}(v,k)\\, w_{j, k_j} (u-v,k) =\n\\sum_{k} [x_{j-1}(\\cdot,k) \\star w_{j, k_j} (\\cdot,k)] (u) ~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"[W_{j}x_{j-1}](u,k_{j})=\\sum_{k}\\sum_{v}x_{j-1}(v,k)\\,w_{j,k_{j}}(u-v,k)=\\sum_%&#10;{k}[x_{j-1}(\\cdot,k)\\star w_{j,k_{j}}(\\cdot,k)](u)~{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>W</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><msub><mi>k</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>v</mi></munder><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo>,</mo><mi>k</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><msub><mi>k</mi><mi>j</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mi>v</mi></mrow><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><msub><mi>k</mi><mi>j</mi></msub></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nIt corresponds to a deep network filtering (\\ref{filtering}), where\nfilters do not combine several channels.\nIterating on $j$ defines a convolution tree, as opposed to a full network. \nIt results from  (\\ref{nsdfs}) that\n\n", "itemtype": "equation", "pos": 31656, "prevtext": "\nThe variable $u$ is usually subsampled. For a fixed $j$, all\nfilters $w_{j,k_{j}}(u,k)$ have the same support width along $u$,\ntypically smaller than $10$. \n\nThe operators $\\rho\\, W_j$ propagates the input signal $x_0 = x$ \nuntil the last layer $x_J$. \nThis cascade of spatial convolutions defines translation covariant\noperators of progressively\nwider supports as the depth $j$ increases. Each $x_j (u,k_j)$\nis a non-linear function of $x(v)$, for $v$ in a square centered at $u$,\nwhose width $\\Delta_j$ does not depend upon $k_j$. The width\n$\\Delta_j$ is the spatial scale of a layer $j$. \nIt is equal to $2^j\\, \\Delta$ if all filters $w_{j,k_{j}}$ have a\nwidth $\\Delta$ and the convolutions (\\ref{filtering}) are \nsubsampled by $2$.\n\n\nNeural networks include many side tricks. \nThey sometimes normalize the amplitude of\n$x_j (v,k)$, by dividing it by the norm of all coefficients \n$x_j (v,k)$ for $v$ in a neighborhood of $u$. This eliminates\nmultiplicative amplitude variabilities. Instead of subsampling \n(\\ref{filtering}) on a regular grid, a max pooling \nmay select the largest coefficients over each sampling cell.\nCoefficients may also be modified by subtracting a constant adapted\nto each coefficient. When applying a rectifier $\\rho$, this constant\nacts as a soft threshold, which increases sparsity.\nIt is usually observed that inside network coefficients\n$x_j (u,k_j)$ have a sparse activation. \n\nThe deep network output \n$x_J = \\Phi_J (x)$ is provided to a classifier, \nusually composed of fully connected neural network layers\n\\cite{nature}. Supervised deep learning algorithms\noptimize the filter values $w_{j,k_{j}}(u,k)$ in order\nto minimize the average classification or regression error on the training\nsamples $\\{x^i\\, , \\, f(x^i) \\}_{i \\leq q}$. \nThere can be more than $10^8$ variables in a network \\cite{nature}. \nThe filter update is done with a back-propagation algorithm,\nwhich may be computed with a stochastic gradient descent, with\nregularization procedures such as dropout.\nThis high-dimensional optimization is non-convex, but\ndespite the presence of many local minima, the regularized\nstochastic gradient descent converges to a local minimum providing good\naccuracy on test examples \\cite{benarous}. The rectifier non-linearity\n$\\rho$ is usually preferred\nbecause the resulting optimization has a better convergence.\nIt however requires a large number of training examples. \nSeveral hundreds of examples per class are usually\nneeded to reach a good performance.\n\nInstabilities have been observed in some network architectures\n\\cite{adversarial}, where additions of small perturbations on\n$x$ can produce large variations of $x_J$. It happens when the norms of\nthe matrices $W_j$ are larger than $1$, and hence amplified\nwhen cascaded. However, deep network also have a strong form of stability\nillustrated by transfer learning \\cite{nature}. A deep network\nlayer $x_J$ optimized on particular training databasis, can \napproximate different classification functions,\nif the final classification layers are trained on a new databasis. \nThis means that it has learned stable structures, which can be\ntransferred across similar learning problems. \n\n\n\\section{Scattering on the Translation Group}\n\\label{scatnsfoisdnf}\n\nA deep network alternates linear operators $W_j$ and \ncontractive non-linearities $\\rho$. To analyze the properties\nof this cascade, we begin with a simpler architecture, where $W_j$ does not\ncombine multiple convolutions across channels in each layer.\nWe show that such network coefficients are obtained through convolutions\nwith a reduced number of equivalent wavelet filters. It defines\na scattering transform \\cite{mallat-math} whose contraction and\nlinearization properties are reviewed.\nVariance reduction and loss of information are studied with reconstructions of\nstationary processes.\n\n\\paragraph{No channel combination} \nSuppose that $x_{j}(u,k_{j})$ is computed by convolving \na single channel $x_{j-1} (u,k_{j-1})$  along $u$:\n\n", "index": 33, "text": "\\begin{equation}\n\\label{nsdfs}\nx_{j} (u, k_{j}) = \\rho\\Big(x_{j-1}(\\cdot,k_{j-1}) \\star w_{j,{h}} (u)\\Big)~~\\mbox{with}~~k_{j} = (k_{j-1}, {h})~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"x_{j}(u,k_{j})=\\rho\\Big{(}x_{j-1}(\\cdot,k_{j-1})\\star w_{j,{h}}(u)\\Big{)}~{}~{%&#10;}\\mbox{with}~{}~{}k_{j}=(k_{j-1},{h})~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><msub><mi>k</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo>,</mo><msub><mi>k</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\" rspace=\"9.1pt\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mtext>with</mtext></mpadded><mo>\u2062</mo><msub><mi>k</mi><mi>j</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>h</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nIf $\\rho$ is a\nrectifier $\\rho(\\alpha) = \\max(\\alpha,0)$\nor a modulus $\\rho(\\alpha) = |\\alpha|$ then \n$\\rho(\\alpha) = \\alpha$ if $\\alpha \\geq 0$. \nWe can thus remove this non-linearity at the output of an averaging\nfilter $w_{j,{h}}$. Indeed\nthis averaging filter is applied\nto positive coefficients and thus computes positive coefficients,\nwhich are not affected by $\\rho$.  \nOn the contrary, if $w_{j,{h}}$ is a band-pass\nfilter then the convolution with $x_{j-1}(\\cdot,k_{j-1})$\nhas alternating signs or a complex\nphase which varies. The non-linearity \n$\\rho$ removes the sign or the phase, which has a strong contraction\neffect. \n\n\\paragraph{Equivalent wavelet filter} Let $m$ be the number of band-pass filters \n$\\{ w_{j_n,{h}_{j_n}} \\}_{1 \\leq n \\leq m}$ in the convolution\ncascade (\\ref{cascasdnfsd}). All other filters are thus\nlow-pass filters. If we remove \n$\\rho$ after each low-pass filter, \nwe get $m$ equivalent band-pass filters:\n\n", "itemtype": "equation", "pos": 32038, "prevtext": "\nIt corresponds to a deep network filtering (\\ref{filtering}), where\nfilters do not combine several channels.\nIterating on $j$ defines a convolution tree, as opposed to a full network. \nIt results from  (\\ref{nsdfs}) that\n\n", "index": 35, "text": "\\begin{equation}\n\\label{cascasdnfsd}\nx_J (u,k_J) = \\rho(\\rho(\\rho(\\rho(x \\star w_{1,{h}_1})\n\\star ... )\\star w_{J-1,{h}_{J-1}}) \\star w_{J,{h}_J}) ~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"x_{J}(u,k_{J})=\\rho(\\rho(\\rho(\\rho(x\\star w_{1,{h}_{1}})\\star...)\\star w_{J-1,%&#10;{h}_{J-1}})\\star w_{J,{h}_{J}})~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>J</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><msub><mi>k</mi><mi>J</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>,</mo><msub><mi>h</mi><mn>1</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>w</mi><mrow><mrow><mi>J</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>h</mi><mrow><mi>J</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>w</mi><mrow><mi>J</mi><mo>,</mo><msub><mi>h</mi><mi>J</mi></msub></mrow></msub></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe cascade of $J$ convolutions (\\ref{cascasdnfsd}) is reduced\nto $m$ convolutions with these equivalent filters\n\n", "itemtype": "equation", "pos": 33148, "prevtext": "\nIf $\\rho$ is a\nrectifier $\\rho(\\alpha) = \\max(\\alpha,0)$\nor a modulus $\\rho(\\alpha) = |\\alpha|$ then \n$\\rho(\\alpha) = \\alpha$ if $\\alpha \\geq 0$. \nWe can thus remove this non-linearity at the output of an averaging\nfilter $w_{j,{h}}$. Indeed\nthis averaging filter is applied\nto positive coefficients and thus computes positive coefficients,\nwhich are not affected by $\\rho$.  \nOn the contrary, if $w_{j,{h}}$ is a band-pass\nfilter then the convolution with $x_{j-1}(\\cdot,k_{j-1})$\nhas alternating signs or a complex\nphase which varies. The non-linearity \n$\\rho$ removes the sign or the phase, which has a strong contraction\neffect. \n\n\\paragraph{Equivalent wavelet filter} Let $m$ be the number of band-pass filters \n$\\{ w_{j_n,{h}_{j_n}} \\}_{1 \\leq n \\leq m}$ in the convolution\ncascade (\\ref{cascasdnfsd}). All other filters are thus\nlow-pass filters. If we remove \n$\\rho$ after each low-pass filter, \nwe get $m$ equivalent band-pass filters:\n\n", "index": 37, "text": "\\begin{equation}\n\\label{eqndf08sdfsd}\n\\psi_{j_n,k_n} (u) = w_{j_{n-1}+1,{h}_{j_{n-1}+1}} \\star ... \n \\star w_{j_n , {h}_{j_n}} (u)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\psi_{j_{n},k_{n}}(u)=w_{j_{n-1}+1,{h}_{j_{n-1}+1}}\\star...\\star w_{j_{n},{h}_%&#10;{j_{n}}}(u)~{}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mi>n</mi></msub><mo>,</mo><msub><mi>k</mi><mi>n</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>w</mi><mrow><mrow><msub><mi>j</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>h</mi><mrow><msub><mi>j</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>\u22c6</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u22c6</mo><msub><mi>w</mi><mrow><msub><mi>j</mi><mi>n</mi></msub><mo>,</mo><msub><mi>h</mi><msub><mi>j</mi><mi>n</mi></msub></msub></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nwith $0 < j_1 < j_2 <...<j_{m-1} < J$.\nIf the final filter $w_{J,{h}_J}$ at the depth $J$ is a low-pass filter then\n$\\psi_{J,k_J} = \\phi_J$ is an equivalent low-pass filter. In this case,\nthe last non-linearity $\\rho$ can also be removed, which gives\n\n", "itemtype": "equation", "pos": 33409, "prevtext": "\nThe cascade of $J$ convolutions (\\ref{cascasdnfsd}) is reduced\nto $m$ convolutions with these equivalent filters\n\n", "index": 39, "text": "\\begin{equation}\n\\label{deandfs}\nx_J (u,k_J) = \n\\rho(\\rho(...\\rho(\\rho(x \\star \\psi_{j_1,k_1}) \\star \\psi_{j_2,k_2})... \\star \n\\psi_{j_{m-1},k_{m-1}}) \\star \n\\psi_{J,k_J} (u))~,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"x_{J}(u,k_{J})=\\rho(\\rho(...\\rho(\\rho(x\\star\\psi_{j_{1},k_{1}})\\star\\psi_{j_{2%&#10;},k_{2}})...\\star\\psi_{j_{m-1},k_{m-1}})\\star\\psi_{J,k_{J}}(u))~{},\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>J</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><msub><mi>k</mi><mi>J</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>2</mn></msub><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><mi>J</mi><mo>,</mo><msub><mi>k</mi><mi>J</mi></msub></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe operator $\\Phi_J x = x_J$ is a wavelet scattering transform,\nintroduced in \\cite{mallat-math}.\nChanging the network filters $w_{j,{h}}$ modifies the equivalent band-pass\nfilters $\\psi_{j,k}$.\nAs in the fast wavelet transform algorithm (\\ref{fionsdfsdsf}),\nif $w_{j,{h}}$ is a rotation of a dilated filter $w_j$ then\n$\\psi_{j,{h}}$ is a dilation and rotation of\na single mother wavelet $\\psi$.\n\n\\paragraph{Scattering order} The order $m = 1$ coefficients \n$x_J(u,k_J) = \\rho( x \\star \\psi_{j_1,k_1}) \\star \\phi_J (u)$ are \nthe wavelet coefficient computed in (\\ref{sdf}). \nThe loss of information due to averaging\nis now compensated by higher order coefficient.\nFor $m = 2$, \n$\\rho(\\rho(x \\star \\psi_{j_1,k_1}) \\star  \\psi_{j_2,k_2}) \\star \\phi_J$ \nare complementary invariants. They measure interactions\nbetween variations of $x$ at a scale $2^{j_1}$, within \na distance $2^{j_2}$, and along orientation or frequency bands\ndefined by $k_1$ and $k_2$. \nThese are scale interaction coefficients, missing from first order coefficients.\nBecause $\\rho$ is strongly contracting,\norder $m$ coefficients have an amplitude\nwhich decrease quickly as $m$ increases \\cite{mallat-math,waldspurger}.\nFor images\nand audio signals, the energy of scattering coefficients becomes negligible\nfor $m \\geq 3$. Let us emphasize that\nthe convolution network depth is $J$, whereas $m$ is the number of effective\nnon-linearity of an output coefficient.\n\n\\paragraph{Diffeomorphism continuity} Section \\ref{wavelet-sec} explains that a wavelet transform defines \nrepresentations which are Lipschitz continuous to actions of diffeomorphisms.\nScattering coefficients up to the order $m$ are computed by applying\n$m$ wavelet transforms. One can prove \\cite{mallat-math} that it thus\ndefines a representation which is Lipschitz continuous to the \nthe action of diffeomorphisms. There exists $C > 0$ such that\n", "itemtype": "equation", "pos": 33853, "prevtext": "\nwith $0 < j_1 < j_2 <...<j_{m-1} < J$.\nIf the final filter $w_{J,{h}_J}$ at the depth $J$ is a low-pass filter then\n$\\psi_{J,k_J} = \\phi_J$ is an equivalent low-pass filter. In this case,\nthe last non-linearity $\\rho$ can also be removed, which gives\n\n", "index": 41, "text": "\\begin{equation}\n\\label{deandfs2}\nx_J(u,k_J) = \\rho(\\rho(...\\rho(\\rho(x \\star \\psi_{j_1,k_1}) \\star \\psi_{j_2,k_2})... \\star \n\\psi_{j_{m-1},k_{m-1}}) \n\\star \\phi_J (u)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"x_{J}(u,k_{J})=\\rho(\\rho(...\\rho(\\rho(x\\star\\psi_{j_{1},k_{1}})\\star\\psi_{j_{2%&#10;},k_{2}})...\\star\\psi_{j_{m-1},k_{m-1}})\\star\\phi_{J}(u)~{}.\" display=\"block\"><mrow><msub><mi>x</mi><mi>J</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><msub><mi>k</mi><mi>J</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2026</mi><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>2</mn></msub><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2026</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c6</mo><msub><mi>\u03d5</mi><mi>J</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nplus a Hessian term which is neglected. This result is proved\nin \\cite{mallat-math} for $\\rho(\\alpha) = |\\alpha|$, but it remains valid for\nany contractive pointwise operator such as rectifiers\n$\\rho(\\alpha) = \\max(\\alpha,0)$. \nIt relies\non commutation properties of wavelet transforms and diffeomorphisms. \nIt shows that the action of small diffeomorphisms is linearized over\nscattering coefficients. \n\n\\paragraph{Classification} Scattering vectors are restricted to coefficients of order $m \\leq 2$,\nbecause their amplitude is negligible beyond.\nA translation scattering $\\Phi_J x$ is well adapted to\nclassification problems where the main source of intra-class variability\nare due to translations, to small deformations, or to ergodic\nstationary processes. \nFor example, intra-class variabilities of \nhand-written digit images are essentially due to translations and deformations. \nOn the MNIST digit data basis \\cite{Bruna},\napplying a linear classifier to scattering coefficients $\\Phi_J x$ \ngives state \nof the art classification errors.\nMusic or speech classification over short time intervals\nof $100ms$ can be modeled by ergodic stationary processes.\nGood music and speech classification results are then\nobtained with a scattering transform \\cite{Anden}.\nImage texture classification are also problems where intra class variability\ncan be modeled by ergodic stationary processes.\nScattering transforms give state of the art\nresults over a wide range of image texture databases\n\\cite{Bruna,sifre}, compared to other descriptors including\npower spectrum moments. \nSoftwares can be retrieved at\n{\\it www.di.ens.fr/data/software}.\n\n\\paragraph{Stationary processes} To analyze the information loss,\nwe now study the reconstruction of $x$ from its scattering coefficients, in a\nstochastic framework where $x$ is a stationary process. \nThis will raise variance and\nseparation issues, where sparsity plays a role. \nIt also demonstrates the importance\nof second order scale interaction terms, to capture non-Gaussian geometric\nproperties of ergodic stationary processes. \nLet us consider scattering coefficients of order $m$\n\n", "itemtype": "equation", "pos": 35919, "prevtext": "\nThe operator $\\Phi_J x = x_J$ is a wavelet scattering transform,\nintroduced in \\cite{mallat-math}.\nChanging the network filters $w_{j,{h}}$ modifies the equivalent band-pass\nfilters $\\psi_{j,k}$.\nAs in the fast wavelet transform algorithm (\\ref{fionsdfsdsf}),\nif $w_{j,{h}}$ is a rotation of a dilated filter $w_j$ then\n$\\psi_{j,{h}}$ is a dilation and rotation of\na single mother wavelet $\\psi$.\n\n\\paragraph{Scattering order} The order $m = 1$ coefficients \n$x_J(u,k_J) = \\rho( x \\star \\psi_{j_1,k_1}) \\star \\phi_J (u)$ are \nthe wavelet coefficient computed in (\\ref{sdf}). \nThe loss of information due to averaging\nis now compensated by higher order coefficient.\nFor $m = 2$, \n$\\rho(\\rho(x \\star \\psi_{j_1,k_1}) \\star  \\psi_{j_2,k_2}) \\star \\phi_J$ \nare complementary invariants. They measure interactions\nbetween variations of $x$ at a scale $2^{j_1}$, within \na distance $2^{j_2}$, and along orientation or frequency bands\ndefined by $k_1$ and $k_2$. \nThese are scale interaction coefficients, missing from first order coefficients.\nBecause $\\rho$ is strongly contracting,\norder $m$ coefficients have an amplitude\nwhich decrease quickly as $m$ increases \\cite{mallat-math,waldspurger}.\nFor images\nand audio signals, the energy of scattering coefficients becomes negligible\nfor $m \\geq 3$. Let us emphasize that\nthe convolution network depth is $J$, whereas $m$ is the number of effective\nnon-linearity of an output coefficient.\n\n\\paragraph{Diffeomorphism continuity} Section \\ref{wavelet-sec} explains that a wavelet transform defines \nrepresentations which are Lipschitz continuous to actions of diffeomorphisms.\nScattering coefficients up to the order $m$ are computed by applying\n$m$ wavelet transforms. One can prove \\cite{mallat-math} that it thus\ndefines a representation which is Lipschitz continuous to the \nthe action of diffeomorphisms. There exists $C > 0$ such that\n", "index": 43, "text": "\n\\[\n\\forall (g,x) \\in {\\rm {Diff}}({\\mathbb R}^n) \\times {\\bf L^2}({\\mathbb R}^n)~~,~~\n\\|\\Phi_J (g.x) - \\Phi_J x \\| \\leq C\\, m\\,\\Big(2^{-J} \\|g\\|_\\infty + \\|\\nabla g \\|_\\infty\\Big)\\, \\|x\\|~,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\forall(g,x)\\in{\\rm{Diff}}({\\mathbb{R}}^{n})\\times{\\bf L^{2}}({\\mathbb{R}}^{n}%&#10;)~{}~{},~{}~{}\\|\\Phi_{J}(g.x)-\\Phi_{J}x\\|\\leq C\\,m\\,\\Big{(}2^{-J}\\|g\\|_{\\infty%&#10;}+\\|\\nabla g\\|_{\\infty}\\Big{)}\\,\\|x\\|~{},\" display=\"block\"><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>Diff</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u211d</mi><mi>n</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><msup><mi>\ud835\udc0b</mi><mn>\ud835\udfd0</mn></msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u211d</mi><mi>n</mi></msup><mo rspace=\"9.1pt\" stretchy=\"false\">)</mo></mrow><mo rspace=\"9.1pt\">,</mo><mo>\u2225</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mi>x</mi><mo>\u2225</mo><mo>\u2264</mo><mpadded width=\"+1.7pt\"><mi>C</mi></mpadded><mpadded width=\"+1.7pt\"><mi>m</mi></mpadded><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><msup><mn>2</mn><mrow><mo>-</mo><mi>J</mi></mrow></msup><mo>\u2225</mo><mi>g</mi><msub><mo>\u2225</mo><mi mathvariant=\"normal\">\u221e</mi></msub><mo>+</mo><mo>\u2225</mo><mo>\u2207</mo><mi>g</mi><msub><mo>\u2225</mo><mi mathvariant=\"normal\">\u221e</mi></msub><mo maxsize=\"160%\" minsize=\"160%\" rspace=\"4.2pt\">)</mo></mrow><mo>\u2225</mo><mi>x</mi><mo rspace=\"5.8pt\">\u2225</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nwith $\\int \\phi_J (u) du = 1$. \nIf $x$ is a stationary process then \n$\\rho(...\\rho(x \\star \\psi_{j_1,k_1})... \\star \\psi_{j_{m},k_{m}})$\nremains stationary because convolutions and pointwise operators\npreserve stationarity. The spatial averaging by $\\phi_J$ provides a non-biased\nestimator of the expected value of $\\Phi_J x(u,k)$, \nwhich is a scattering moment:\n\n", "itemtype": "equation", "pos": 38239, "prevtext": "\nplus a Hessian term which is neglected. This result is proved\nin \\cite{mallat-math} for $\\rho(\\alpha) = |\\alpha|$, but it remains valid for\nany contractive pointwise operator such as rectifiers\n$\\rho(\\alpha) = \\max(\\alpha,0)$. \nIt relies\non commutation properties of wavelet transforms and diffeomorphisms. \nIt shows that the action of small diffeomorphisms is linearized over\nscattering coefficients. \n\n\\paragraph{Classification} Scattering vectors are restricted to coefficients of order $m \\leq 2$,\nbecause their amplitude is negligible beyond.\nA translation scattering $\\Phi_J x$ is well adapted to\nclassification problems where the main source of intra-class variability\nare due to translations, to small deformations, or to ergodic\nstationary processes. \nFor example, intra-class variabilities of \nhand-written digit images are essentially due to translations and deformations. \nOn the MNIST digit data basis \\cite{Bruna},\napplying a linear classifier to scattering coefficients $\\Phi_J x$ \ngives state \nof the art classification errors.\nMusic or speech classification over short time intervals\nof $100ms$ can be modeled by ergodic stationary processes.\nGood music and speech classification results are then\nobtained with a scattering transform \\cite{Anden}.\nImage texture classification are also problems where intra class variability\ncan be modeled by ergodic stationary processes.\nScattering transforms give state of the art\nresults over a wide range of image texture databases\n\\cite{Bruna,sifre}, compared to other descriptors including\npower spectrum moments. \nSoftwares can be retrieved at\n{\\it www.di.ens.fr/data/software}.\n\n\\paragraph{Stationary processes} To analyze the information loss,\nwe now study the reconstruction of $x$ from its scattering coefficients, in a\nstochastic framework where $x$ is a stationary process. \nThis will raise variance and\nseparation issues, where sparsity plays a role. \nIt also demonstrates the importance\nof second order scale interaction terms, to capture non-Gaussian geometric\nproperties of ergodic stationary processes. \nLet us consider scattering coefficients of order $m$\n\n", "index": 45, "text": "\\begin{equation}\n\\label{estnasfs}\n\\Phi_J x(u,k) = \\rho(...\\rho(\\rho(x \\star \\psi_{j_1,k_1})\\star \\psi_{j_2,k_2})\n... \\star \n\\psi_{j_{m},k_{m}}) \n\\star \\phi_J (u)~,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{J}x(u,k)=\\rho(...\\rho(\\rho(x\\star\\psi_{j_{1},k_{1}})\\star\\psi_{j_{2},k_{%&#10;2}})...\\star\\psi_{j_{m},k_{m}})\\star\\phi_{J}(u)~{},\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>2</mn></msub><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mi>m</mi></msub><mo>,</mo><msub><mi>k</mi><mi>m</mi></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03d5</mi><mi>J</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nIf $x$ is a slow mixing process, which is a weak ergodicity assumption, then \nthe estimation variance $\\sigma_J^2 = \\|\\Phi_J x - {\\mathbb E}(\\Phi_J x)\\|^2$\nconverges to zero \\cite{brunastat} when $J$ goes to $\\infty$. \nIndeed, $\\Phi_J$ is computed by iterating on\ncontractive operators, which average an ergodic stationary process $x$ \nover progressively larger scales. \nOne can prove that scattering moments characterize complex\nmultiscale properties of fractals and multifractal processes, such \nas Brownian motions, Levi processes or Mandelbrot cascades\n\\cite{brunaAnals}.  \n\n\n\n\\begin{figure*}\n\\begin{tabular}{lllll}\n\\includegraphics[width=2.5cm]{example_14.png}&\n\\includegraphics[width=2.5cm]{example_23.png}&\n\\includegraphics[width=2.5cm]{example_1.png}&\n\\includegraphics[width=2.5cm]{example_13.png}&\n\\includegraphics[width=2.5cm]{example_6.png}\\\\\n\\includegraphics[width=2.5cm]{reconstr_14.png}&\n\\includegraphics[width=2.5cm]{reconstr_23.png}&\n\\includegraphics[width=2.5cm]{reconstr_1.png}&\n &\\\\\n\\includegraphics[width=2.5cm]{reconstr_14-2.png}&\n\\includegraphics[width=2.5cm]{reconstr_23-2.png}&\n\\includegraphics[width=2.5cm]{reconstr_1-2.png}&\n\\includegraphics[width=2.5cm]{reconstr2_13.png}&\n\\includegraphics[width=2.5cm]{reconstr_6.png}\n\\end{tabular}\n\\caption{First row: original images. Second row: realization of\na Gaussian process with same second covariance moments.\nThird row: reconstructions from \nfirst and second order scattering coefficients.}\n\\label{textures}\n\\end{figure*}\n\n\\paragraph{Inverse scattering and sparsity} \nScattering transforms are generally not invertible but \ngiven $\\Phi_J (x)$ one can can compute vectors $\\tilde x$ such that\n$\\|\\Phi_J (x) - \\Phi_J (\\tilde x)\\| \\leq \\sigma_J$. \nWe initialize $\\tilde x_0$ as a Gaussian white noise realization,\nand iteratively update $\\tilde x_n$ by reducing\n$\\|\\Phi_J (x) - \\Phi_J (\\tilde x_n)\\|$ with a gradient descent algorithm, \nuntil it reaches $\\sigma_J$ \\cite{brunastat}. Since $\\Phi_J (x)$ is not\nconvex, there is no guaranteed convergence, but numerical reconstructions\nconverge up to a sufficient precision. The recovered $\\tilde x$\nis a stationary process having nearly the same scattering moments as $x$,\nwhose properties are similar to a maximum entropy process for fixed\nscattering moments \\cite{brunastat}. \n\n\nFigure \\ref{textures} shows several examples of images $x$ with\n$N^2$ pixels. \nThe first three images are realizations of ergodic stationary textures.\nThe second row gives realizations of stationary Gaussian \nprocesses having the same $N^2$\nsecond order covariance moments as the top textures.\nThe third column shows the vorticity field of a two-dimensional turbulent fluid. \nThe Gaussian realization is thus a Kolmogorov type model, which does not \nrestore the filament geometry.\nThe third row gives reconstructions from scattering coefficients, limited to\norder $m \\leq 2$.\nThe scattering vector is computed at the maximum scale $2^J = N$, with\nwavelets having $K = 8$ different orientations. It \nis thus completely invariant to translations.\nThe dimension of\n$\\Phi_{J} x$ is about $(K \\log_2 N)^2/2 \\ll N^2$.\nScattering moments \nrestore better texture geometries than the Gaussian models \nobtained with $N^2$ covariance moments. This geometry is mostly captured\nby second order scattering coefficients,\nproviding scale interaction terms. Indeed, \nfirst order scattering moments can only \nreconstruct images which are\nsimilar to realizations of Gaussian processes.\nFirst and second order scattering moments also provide good models\nof ergodic audio textures \\cite{brunastat}.\n\nThe fourth image has very sparse wavelet coefficients. In this case the image\nis nearly perfectly restored by its scattering coefficients, up to a random translation. The reconstruction is centered for comparison. \nSection \\ref{wavelet-sec} explains that if wavelet coefficients are\nsparse then a rectifier or an absolute value contractions $\\rho$ \ndoes not contract as much distances with other signals. Indeed, \n$|\\rho(\\alpha) - \\rho(\\alpha')| = |\\alpha - \\alpha'|$ if $\\alpha=0$ or\n$\\alpha'=0$. Inverting a scattering transform is a non-linear inverse\nproblem, which requires to recover a lost phase information. \nSparsity has an important role on such phase recovery problems\n\\cite{waldspurger}. Translating randomly the last motorcycle image defines\na non-ergodic stationary process, whose wavelet coefficients are not as sparse. \nAs a result, the reconstruction from a\nrandom initialization is very different, and does not \npreserve patterns which are important for most classification tasks.\nThis is not surprising since there is much less scattering coefficients than\nimage pixels. If we reduce $2^J$ so that the number of \nscattering coefficients reaches the number of pixels then the reconstruction is of good quality, but there is little variance reduction.\n\n\nConcentrating on the translation group is not so effective to reduce variance\nwhen the process is not translation ergodic. Applying wavelet filters \ncan destroy important structures which are not sparse over wavelets. \nNext section addresses both issues. \nImpressive texture synthesis results have been obtained with \ndeep convolutional networks trained on image data bases\n\\cite{bethdge}, but with much more output coefficients. \nNumerical reconstructions \\cite{fergus} also show that one\ncan also recover complex patterns, such as\nbirds, airplanes, cars, dogs, ships,\nif the network is trained to recognize the corresponding image classes.\nThe network keeps some form of memory of important\nclassification patterns.\n\n\n\\section{Multiscale Hierarchical Convolutional Networks}\n\\label{grnsdfssec}\n\nScattering transforms on the translation group are restricted\ndeep convolutional network architectures, which suffer from\nvariance issues and loss of information. We shall explain why \nchannel combinations provide the flexibility needed to avoid some of these \nlimitations. We analyze a general class of\nconvolutional network architectures by extending the tools previously introduced. \nContractions and invariants to translations are\nreplaced by contractions along groups of local symmetries adapted\nto $f$, which are defined by parallel transports in each network layer. \nThe network is structured by\nfactorizing groups of symmetries, as depth increases. It\nimplies that all linear operators can be written as generalized convolutions\nacross multiple channels. To preserve the classification margin, wavelets must\nalso be replaced by adapted filter weights, which separate discriminative patterns in multiple\nnetwork fibers.\n\n\\paragraph{Separation margin}\nNetwork layers $x_{j} = \\rho W_{j} x_{j-1}$ are computed with\noperators $\\rho W_j$ which contract and separate components of $x_j$.\nWe shall see that $W_j$ also needs to prepare\n$x_j$ for the next transformation $W_{j+1}$, \nso consecutive operators $W_j$ and $W_{j+1}$ are strongly dependant.\nEach $W_{j}$ is a contractive linear operator, \n$\\|W_{j} z\\| \\leq \\|z\\|$ to reduce the space volume, and \navoid instabilities when cascading such operators \\cite{adversarial}.\nA layer $x_{j-1}$ must separate $f$ so that we can write\n$f(x) = f_{j-1} (x_{j-1})$ for some function $f_{j-1}(z)$.\nTo simplify explanations, we \nconcentrate on classification, where \nseparation is an $\\epsilon > 0$ \nmargin condition:\n\n", "itemtype": "equation", "pos": 38781, "prevtext": "\nwith $\\int \\phi_J (u) du = 1$. \nIf $x$ is a stationary process then \n$\\rho(...\\rho(x \\star \\psi_{j_1,k_1})... \\star \\psi_{j_{m},k_{m}})$\nremains stationary because convolutions and pointwise operators\npreserve stationarity. The spatial averaging by $\\phi_J$ provides a non-biased\nestimator of the expected value of $\\Phi_J x(u,k)$, \nwhich is a scattering moment:\n\n", "index": 47, "text": "\\begin{equation}\n\\label{nnsdf}\n{\\mathbb E} (\\Phi_J x(u,k)) = {\\mathbb E} \\Big(\n\\rho(...\\rho(\\rho(x \\star \\psi_{j_1,k_1})\\star \\psi_{j_2,k_2})\n... \\star \n\\psi_{j_{m},k_{m}})  \\Big) ~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}(\\Phi_{J}x(u,k))={\\mathbb{E}}\\Big{(}\\rho(...\\rho(\\rho(x\\star\\psi_{%&#10;j_{1},k_{1}})\\star\\psi_{j_{2},k_{2}})...\\star\\psi_{j_{m},k_{m}})\\Big{)}~{}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>J</mi></msub><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mn>2</mn></msub><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi></mrow><mo>\u22c6</mo><msub><mi>\u03c8</mi><mrow><msub><mi>j</mi><mi>m</mi></msub><mo>,</mo><msub><mi>k</mi><mi>m</mi></msub></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\" rspace=\"5.8pt\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe next layer $x_{j} = \\rho W_{j} x_{j-1}$ lives in a contracted space\nbut it must also satisfy\n\n", "itemtype": "equation", "pos": 46285, "prevtext": "\nIf $x$ is a slow mixing process, which is a weak ergodicity assumption, then \nthe estimation variance $\\sigma_J^2 = \\|\\Phi_J x - {\\mathbb E}(\\Phi_J x)\\|^2$\nconverges to zero \\cite{brunastat} when $J$ goes to $\\infty$. \nIndeed, $\\Phi_J$ is computed by iterating on\ncontractive operators, which average an ergodic stationary process $x$ \nover progressively larger scales. \nOne can prove that scattering moments characterize complex\nmultiscale properties of fractals and multifractal processes, such \nas Brownian motions, Levi processes or Mandelbrot cascades\n\\cite{brunaAnals}.  \n\n\n\n\\begin{figure*}\n\\begin{tabular}{lllll}\n\\includegraphics[width=2.5cm]{example_14.png}&\n\\includegraphics[width=2.5cm]{example_23.png}&\n\\includegraphics[width=2.5cm]{example_1.png}&\n\\includegraphics[width=2.5cm]{example_13.png}&\n\\includegraphics[width=2.5cm]{example_6.png}\\\\\n\\includegraphics[width=2.5cm]{reconstr_14.png}&\n\\includegraphics[width=2.5cm]{reconstr_23.png}&\n\\includegraphics[width=2.5cm]{reconstr_1.png}&\n &\\\\\n\\includegraphics[width=2.5cm]{reconstr_14-2.png}&\n\\includegraphics[width=2.5cm]{reconstr_23-2.png}&\n\\includegraphics[width=2.5cm]{reconstr_1-2.png}&\n\\includegraphics[width=2.5cm]{reconstr2_13.png}&\n\\includegraphics[width=2.5cm]{reconstr_6.png}\n\\end{tabular}\n\\caption{First row: original images. Second row: realization of\na Gaussian process with same second covariance moments.\nThird row: reconstructions from \nfirst and second order scattering coefficients.}\n\\label{textures}\n\\end{figure*}\n\n\\paragraph{Inverse scattering and sparsity} \nScattering transforms are generally not invertible but \ngiven $\\Phi_J (x)$ one can can compute vectors $\\tilde x$ such that\n$\\|\\Phi_J (x) - \\Phi_J (\\tilde x)\\| \\leq \\sigma_J$. \nWe initialize $\\tilde x_0$ as a Gaussian white noise realization,\nand iteratively update $\\tilde x_n$ by reducing\n$\\|\\Phi_J (x) - \\Phi_J (\\tilde x_n)\\|$ with a gradient descent algorithm, \nuntil it reaches $\\sigma_J$ \\cite{brunastat}. Since $\\Phi_J (x)$ is not\nconvex, there is no guaranteed convergence, but numerical reconstructions\nconverge up to a sufficient precision. The recovered $\\tilde x$\nis a stationary process having nearly the same scattering moments as $x$,\nwhose properties are similar to a maximum entropy process for fixed\nscattering moments \\cite{brunastat}. \n\n\nFigure \\ref{textures} shows several examples of images $x$ with\n$N^2$ pixels. \nThe first three images are realizations of ergodic stationary textures.\nThe second row gives realizations of stationary Gaussian \nprocesses having the same $N^2$\nsecond order covariance moments as the top textures.\nThe third column shows the vorticity field of a two-dimensional turbulent fluid. \nThe Gaussian realization is thus a Kolmogorov type model, which does not \nrestore the filament geometry.\nThe third row gives reconstructions from scattering coefficients, limited to\norder $m \\leq 2$.\nThe scattering vector is computed at the maximum scale $2^J = N$, with\nwavelets having $K = 8$ different orientations. It \nis thus completely invariant to translations.\nThe dimension of\n$\\Phi_{J} x$ is about $(K \\log_2 N)^2/2 \\ll N^2$.\nScattering moments \nrestore better texture geometries than the Gaussian models \nobtained with $N^2$ covariance moments. This geometry is mostly captured\nby second order scattering coefficients,\nproviding scale interaction terms. Indeed, \nfirst order scattering moments can only \nreconstruct images which are\nsimilar to realizations of Gaussian processes.\nFirst and second order scattering moments also provide good models\nof ergodic audio textures \\cite{brunastat}.\n\nThe fourth image has very sparse wavelet coefficients. In this case the image\nis nearly perfectly restored by its scattering coefficients, up to a random translation. The reconstruction is centered for comparison. \nSection \\ref{wavelet-sec} explains that if wavelet coefficients are\nsparse then a rectifier or an absolute value contractions $\\rho$ \ndoes not contract as much distances with other signals. Indeed, \n$|\\rho(\\alpha) - \\rho(\\alpha')| = |\\alpha - \\alpha'|$ if $\\alpha=0$ or\n$\\alpha'=0$. Inverting a scattering transform is a non-linear inverse\nproblem, which requires to recover a lost phase information. \nSparsity has an important role on such phase recovery problems\n\\cite{waldspurger}. Translating randomly the last motorcycle image defines\na non-ergodic stationary process, whose wavelet coefficients are not as sparse. \nAs a result, the reconstruction from a\nrandom initialization is very different, and does not \npreserve patterns which are important for most classification tasks.\nThis is not surprising since there is much less scattering coefficients than\nimage pixels. If we reduce $2^J$ so that the number of \nscattering coefficients reaches the number of pixels then the reconstruction is of good quality, but there is little variance reduction.\n\n\nConcentrating on the translation group is not so effective to reduce variance\nwhen the process is not translation ergodic. Applying wavelet filters \ncan destroy important structures which are not sparse over wavelets. \nNext section addresses both issues. \nImpressive texture synthesis results have been obtained with \ndeep convolutional networks trained on image data bases\n\\cite{bethdge}, but with much more output coefficients. \nNumerical reconstructions \\cite{fergus} also show that one\ncan also recover complex patterns, such as\nbirds, airplanes, cars, dogs, ships,\nif the network is trained to recognize the corresponding image classes.\nThe network keeps some form of memory of important\nclassification patterns.\n\n\n\\section{Multiscale Hierarchical Convolutional Networks}\n\\label{grnsdfssec}\n\nScattering transforms on the translation group are restricted\ndeep convolutional network architectures, which suffer from\nvariance issues and loss of information. We shall explain why \nchannel combinations provide the flexibility needed to avoid some of these \nlimitations. We analyze a general class of\nconvolutional network architectures by extending the tools previously introduced. \nContractions and invariants to translations are\nreplaced by contractions along groups of local symmetries adapted\nto $f$, which are defined by parallel transports in each network layer. \nThe network is structured by\nfactorizing groups of symmetries, as depth increases. It\nimplies that all linear operators can be written as generalized convolutions\nacross multiple channels. To preserve the classification margin, wavelets must\nalso be replaced by adapted filter weights, which separate discriminative patterns in multiple\nnetwork fibers.\n\n\\paragraph{Separation margin}\nNetwork layers $x_{j} = \\rho W_{j} x_{j-1}$ are computed with\noperators $\\rho W_j$ which contract and separate components of $x_j$.\nWe shall see that $W_j$ also needs to prepare\n$x_j$ for the next transformation $W_{j+1}$, \nso consecutive operators $W_j$ and $W_{j+1}$ are strongly dependant.\nEach $W_{j}$ is a contractive linear operator, \n$\\|W_{j} z\\| \\leq \\|z\\|$ to reduce the space volume, and \navoid instabilities when cascading such operators \\cite{adversarial}.\nA layer $x_{j-1}$ must separate $f$ so that we can write\n$f(x) = f_{j-1} (x_{j-1})$ for some function $f_{j-1}(z)$.\nTo simplify explanations, we \nconcentrate on classification, where \nseparation is an $\\epsilon > 0$ \nmargin condition:\n\n", "index": 49, "text": "\\begin{equation}\n\\label{sdnfsdfsdfe0}\n\\forall (x,x') \\in \\Omega^2~~,~~\n\\| x_{j-1} -  x'_{j-1}  \\| \\geq \\epsilon ~~\n\\mbox{if $f(x) \\neq f(x')$}~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\forall(x,x^{\\prime})\\in\\Omega^{2}~{}~{},~{}~{}\\|x_{j-1}-x^{\\prime}_{j-1}\\|%&#10;\\geq\\epsilon~{}~{}\\mbox{if $f(x)\\neq f(x^{\\prime})$}~{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mpadded width=\"+6.6pt\"><msup><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msup></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mo>\u2225</mo><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo>\u2032</mo></msubsup></mrow><mo>\u2225</mo></mrow><mo>\u2265</mo><mrow><mpadded width=\"+6.6pt\"><mi>\u03f5</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mrow><mtext>if\u00a0</mtext><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe operator $W_{j}$ computes a linear projection\nwhich preserves this margin condition,\nbut the resulting dimension reduction is limited.\nWe can further contract the space non-linearly with $\\rho$. To preserve the\nmargin, it must reduce distances along non-linear displacements which transform \nany $x_{j-1}$ into an $x'_{j-1}$ which is in the same class.\n\n\\paragraph{Parallel transport}\nDisplacements which preserve classes are defined by local symmetries (\\ref{localsym}),\nwhich are transformations $\\bar g$ such that \n$f_{j-1}(x_{j-1}) = f_{j-1}(\\bar g.x_{j-1})$.\nTo define a local invariant to \na group of transformations $G$,\nwe must process the orbit\n$\\{ \\bar g. x_{j-1} \\}_{g \\in G}$. \nHowever, $W_j$ is applied to $x_{j-1}$ not on the non-linear transformations\n$\\bar g.x_{j-1}$ of $x_{j-1}$. The key idea is that\na deep network can proceed in two steps. \nLet us write $x_{j} (u,k_j) = x_j(v)$ with $v \\in P_{j}$. \nFirst, $\\rho W_j$ computes an approximate mapping of such an\norbit $\\{ \\bar g. x_{j-1} \\}_{\\bar g \\in G}$ into\na parallel transport in $P_j$, which moves coefficients of $x_j$.  \nThen $W_{j+1}$ applied to $x_j$ is filtering the orbits of this parallel transport.\nA parallel transport is defined by operators\n$g \\in G_j$ acting on $v \\in P_j$, and we write\n", "itemtype": "equation", "pos": 46542, "prevtext": "\nThe next layer $x_{j} = \\rho W_{j} x_{j-1}$ lives in a contracted space\nbut it must also satisfy\n\n", "index": 51, "text": "\\begin{equation}\n\\label{sdnfsdfsdfe0987sd}\n\\forall (x,x') \\in \\Omega^2~~,~~\n\\|\\rho W_{j} x_{j-1} - \\rho W_{j} x'_{j-1}  \\| \\geq \\epsilon \\, ~~\n\\mbox{if $f (x) \\neq f (x')$}~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\forall(x,x^{\\prime})\\in\\Omega^{2}~{}~{},~{}~{}\\|\\rho W_{j}x_{j-1}-\\rho W_{j}x%&#10;^{\\prime}_{j-1}\\|\\geq\\epsilon\\,~{}~{}\\mbox{if $f(x)\\neq f(x^{\\prime})$}~{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mpadded width=\"+6.6pt\"><msup><mi mathvariant=\"normal\">\u03a9</mi><mn>2</mn></msup></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mo>\u2225</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><msub><mi>W</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>-</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><msub><mi>W</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo>\u2032</mo></msubsup></mrow></mrow><mo>\u2225</mo></mrow><mo>\u2265</mo><mrow><mpadded width=\"+8.3pt\"><mi>\u03f5</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mrow><mtext>if\u00a0</mtext><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe operator $W_j$ is defined \nso that $G_j$ is a group of local symmetries: $f_j (g.x_j) = f_j (x_j)$ \nfor small $|g|_{G_j}$. This is obtained if a transport of $x_j = W_j x_{j-1}$\nby $g \\in G_j$ corresponds to the action of a\nlocal symmetry $\\bar g$ of $f_{j-1}$ on $x_{j-1}$:\n\n", "itemtype": "equation", "pos": 48011, "prevtext": "\nThe operator $W_{j}$ computes a linear projection\nwhich preserves this margin condition,\nbut the resulting dimension reduction is limited.\nWe can further contract the space non-linearly with $\\rho$. To preserve the\nmargin, it must reduce distances along non-linear displacements which transform \nany $x_{j-1}$ into an $x'_{j-1}$ which is in the same class.\n\n\\paragraph{Parallel transport}\nDisplacements which preserve classes are defined by local symmetries (\\ref{localsym}),\nwhich are transformations $\\bar g$ such that \n$f_{j-1}(x_{j-1}) = f_{j-1}(\\bar g.x_{j-1})$.\nTo define a local invariant to \na group of transformations $G$,\nwe must process the orbit\n$\\{ \\bar g. x_{j-1} \\}_{g \\in G}$. \nHowever, $W_j$ is applied to $x_{j-1}$ not on the non-linear transformations\n$\\bar g.x_{j-1}$ of $x_{j-1}$. The key idea is that\na deep network can proceed in two steps. \nLet us write $x_{j} (u,k_j) = x_j(v)$ with $v \\in P_{j}$. \nFirst, $\\rho W_j$ computes an approximate mapping of such an\norbit $\\{ \\bar g. x_{j-1} \\}_{\\bar g \\in G}$ into\na parallel transport in $P_j$, which moves coefficients of $x_j$.  \nThen $W_{j+1}$ applied to $x_j$ is filtering the orbits of this parallel transport.\nA parallel transport is defined by operators\n$g \\in G_j$ acting on $v \\in P_j$, and we write\n", "index": 53, "text": "\n\\[\n\\forall (g,v) \\in G_j \\times P_j~~,~~g.x_j (v) = x_j (g.v)~.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\forall(g,v)\\in G_{j}\\times P_{j}~{}~{},~{}~{}g.x_{j}(v)=x_{j}(g.v)~{}.\" display=\"block\"><mrow><mo>\u2200</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>G</mi><mi>j</mi></msub><mo>\u00d7</mo><mpadded width=\"+6.6pt\"><msub><mi>P</mi><mi>j</mi></msub></mpadded><mo rspace=\"9.1pt\">,</mo><mi>g</mi><mo>.</mo><msub><mi>x</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi>x</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>v</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nBy definition $f_j (x_j) = f_{j-1} (x_{j-1}) = f(x)$. \nSince $f_{j-1} (\\bar g . x_{j-1}) = f_{j-1} (x_{j-1})$ it results \nfrom (\\ref{Onsdf09sdf}) that\n$f_j (g.x_j) = f_j (x_j)$. \n\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[width=6cm]{FigureFlow4.png}\n\\caption{A multiscale hierarchical networks computes convolutions along the fibers of\na parallel transport. It is defined by a group $G_j$ of symmetries acting on\nthe index set $P_j$ of a layer $x_j$. \nFilter weights are transported along fibers.}\n\\label{figure4}\n\\end{figure*}\n\nThe index space $P_{j}$ is called\na $G_{j}$-principal fiber bundle in differential geometry \\cite{Petitot},\nillustrated by Figure \\ref{figure4}. \nThe orbits of $G_j$ in $P_j$ are fibers,\nindexed by the equivalence classes $B_{j} = P_{j} / G_{j}$.\nThey are\nglobally invariant to the action of $G_j$, and play an important role\nto separate $f$. Each fiber is indexing a continuous Lie group, \nbut it is sampled along $G_j$ at \nintervals such that values of $x_j$ can be interpolated in between.\nAs in the translation case, \nthese sampling intervals depend upon the local invariance of $x_j$, which \nincreases with $j$.\n\n\\paragraph{Hierarchical symmetries} In a hierarchical convolution network, we further impose that\nlocal symmetry groups are growing with depth, and can be factorized:\n\n", "itemtype": "equation", "pos": 48358, "prevtext": "\nThe operator $W_j$ is defined \nso that $G_j$ is a group of local symmetries: $f_j (g.x_j) = f_j (x_j)$ \nfor small $|g|_{G_j}$. This is obtained if a transport of $x_j = W_j x_{j-1}$\nby $g \\in G_j$ corresponds to the action of a\nlocal symmetry $\\bar g$ of $f_{j-1}$ on $x_{j-1}$:\n\n", "index": 55, "text": "\\begin{equation}\n\\label{Onsdf09sdf}\ng. [\\rho W_j x_{j-1}]  = \\rho W_j  [\\bar g.x_{j-1}]~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"g.[\\rho W_{j}x_{j-1}]=\\rho W_{j}[\\bar{g}.x_{j-1}]~{}.\" display=\"block\"><mrow><mi>g</mi><mo>.</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c1</mi><msub><mi>W</mi><mi>j</mi></msub><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><mi>\u03c1</mi><msub><mi>W</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">[</mo><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>.</mo><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo rspace=\"5.8pt\" stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nThe hierarchy begins for $j = 0$ by the translation group $G_0 = {\\mathbb R}^n$,\nwhich acts on $x(u)$ through the spatial variable $u \\in {\\mathbb R}^n$. \nThe condition (\\ref{enbmdosdfbsd}) \nis not necessarily satisfied by general deep networks,\nbesides $j = 0$ for translations. \nIt is used\nby joint scattering transforms \\cite{sifre,andenLostanlen} \nand has been proposed for unsupervised convolution network learning \\cite{szlam}.\nProposition \\ref{props1} proves that this hierarchical embedding\nimplies that each $W_{j}$ is a convolution on $G_{j-1}$. \n\n\n\\begin{proposition}\n\\label{props1}\nThe group embedding (\\ref{enbmdosdfbsd}) implies that $x_j$ can be indexed by\n$(g,{h},b) \\in G_{j-1} \\times H_j \\times B_j$ and there exists\n$w_{j,{h}.b} \\in {\\mathbb C}^{P_{j-1}}$ such that\n\n", "itemtype": "equation", "pos": 49779, "prevtext": "\nBy definition $f_j (x_j) = f_{j-1} (x_{j-1}) = f(x)$. \nSince $f_{j-1} (\\bar g . x_{j-1}) = f_{j-1} (x_{j-1})$ it results \nfrom (\\ref{Onsdf09sdf}) that\n$f_j (g.x_j) = f_j (x_j)$. \n\n\n\n\n\\begin{figure*}\n\\center\n\\includegraphics[width=6cm]{FigureFlow4.png}\n\\caption{A multiscale hierarchical networks computes convolutions along the fibers of\na parallel transport. It is defined by a group $G_j$ of symmetries acting on\nthe index set $P_j$ of a layer $x_j$. \nFilter weights are transported along fibers.}\n\\label{figure4}\n\\end{figure*}\n\nThe index space $P_{j}$ is called\na $G_{j}$-principal fiber bundle in differential geometry \\cite{Petitot},\nillustrated by Figure \\ref{figure4}. \nThe orbits of $G_j$ in $P_j$ are fibers,\nindexed by the equivalence classes $B_{j} = P_{j} / G_{j}$.\nThey are\nglobally invariant to the action of $G_j$, and play an important role\nto separate $f$. Each fiber is indexing a continuous Lie group, \nbut it is sampled along $G_j$ at \nintervals such that values of $x_j$ can be interpolated in between.\nAs in the translation case, \nthese sampling intervals depend upon the local invariance of $x_j$, which \nincreases with $j$.\n\n\\paragraph{Hierarchical symmetries} In a hierarchical convolution network, we further impose that\nlocal symmetry groups are growing with depth, and can be factorized:\n\n", "index": 57, "text": "\\begin{equation}\n\\label{enbmdosdfbsd}\n\\forall j \\geq 0~~,~~G_{j} = G_{j-1} \\rtimes H_{j}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\forall j\\geq 0~{}~{},~{}~{}G_{j}=G_{j-1}\\rtimes H_{j}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><mi>j</mi></mrow><mo>\u2265</mo><mpadded width=\"+6.6pt\"><mn>0</mn></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><msub><mi>G</mi><mi>j</mi></msub><mo>=</mo><mrow><msub><mi>G</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u22ca</mo><msub><mi>H</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nwhere ${h}.b$ transports $b \\in B_j$ by ${h} \\in H_j$ in $P_j$.\n\\end{proposition}\n\n\\begin{proof}\nWe write\n$x_{j} = \\rho W_{j} x_{j-1}$ as inner products with row vectors:\n\n", "itemtype": "equation", "pos": 50669, "prevtext": "\nThe hierarchy begins for $j = 0$ by the translation group $G_0 = {\\mathbb R}^n$,\nwhich acts on $x(u)$ through the spatial variable $u \\in {\\mathbb R}^n$. \nThe condition (\\ref{enbmdosdfbsd}) \nis not necessarily satisfied by general deep networks,\nbesides $j = 0$ for translations. \nIt is used\nby joint scattering transforms \\cite{sifre,andenLostanlen} \nand has been proposed for unsupervised convolution network learning \\cite{szlam}.\nProposition \\ref{props1} proves that this hierarchical embedding\nimplies that each $W_{j}$ is a convolution on $G_{j-1}$. \n\n\n\\begin{proposition}\n\\label{props1}\nThe group embedding (\\ref{enbmdosdfbsd}) implies that $x_j$ can be indexed by\n$(g,{h},b) \\in G_{j-1} \\times H_j \\times B_j$ and there exists\n$w_{j,{h}.b} \\in {\\mathbb C}^{P_{j-1}}$ such that\n\n", "index": 59, "text": "\\begin{equation}\n\\label{Consdfoushdf}\nx_{j} (g,{h},b) = \\rho \\Big( \\sum_{v' \\in P_{j-1}} x_{j-1} (v') \\, w_{j,{h}.b} (g^{-1}. v')\n\\Big) = \n\\rho \\Big( x_{j-1} \\star^{j-1}  w_{j,{h}.b} (g) \\Big) ~, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"x_{j}(g,{h},b)=\\rho\\Big{(}\\sum_{v^{\\prime}\\in P_{j-1}}x_{j-1}(v^{\\prime})\\,w_{%&#10;j,{h}.b}(g^{-1}.v^{\\prime})\\Big{)}=\\rho\\Big{(}x_{j-1}\\star^{j-1}w_{j,{h}.b}(g)%&#10;\\Big{)}~{},\" display=\"block\"><mrow><msub><mi>x</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>,</mo><mi>h</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c1</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>v</mi><mo>\u2032</mo></msup><mo>\u2208</mo><msub><mi>P</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></munder><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>v</mi><mo>\u2032</mo></msup><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><msub><mi>w</mi><mrow><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow><mo>.</mo><mi>b</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>g</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>.</mo><msup><mi>v</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>=</mo><mi>\u03c1</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><msup><mo>\u22c6</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msup><msub><mi>w</mi><mrow><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow><mo>.</mo><mi>b</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"160%\" minsize=\"160%\" rspace=\"5.8pt\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04920.tex", "nexttext": "\nIf $\\bar g \\in G_j$ then \n$\\bar g. x_{j} (v) = x_j (\\bar g . v) = \\rho ( {\\langle}  x_{j-1} \\,,\\, w_{j,\\bar g.v} {\\rangle}$.\nOne can write $w_{j,v} =  w_{j,\\bar g. b}$ with $\\bar g \\in G_j$\nand $b \\in B_j = P_j / G_j$. \nIf $G_{j} = G_{j-1} \\rtimes H_j$ then\n$\\bar g \\in G_j$ can be decomposed \ninto $\\bar g = (g, {h}) \\in G_{j-1} \\rtimes H_j$, where\n$g.x_j = \\rho ({\\langle} g.x_{j-1},w_{j,b} {\\rangle})$. \nBut $g. x_{j-1}(v') = x_{j-1} (g. v')$ so with a change of variable \nwe get\n$w_{j,g.b} (v') = w_{j,b}(g^{-1}. v')$. \nHence $w_{j, \\bar g.b} (v') = w_{j,(g,l).b}(v) = {h}.w_{j,{h}.b}(g^{-1}. v')$.\nInserting this filter expression in (\\ref{definHj}) proves (\\ref{Consdfoushdf}).\n\\end{proof}\n\n\nThis proposition proves that $W_j$ is a convolution along the fibers\nof $G_{j-1}$ in $P_{j-1}$.\nEach $w_{j,{h}.b}$ is a transformation of an elementary\nfilter $w_{j,b}$ by a group of local symmetries ${h} \\in H_j$ so that \n$f_j (x_j (g,{h},b))$ remains\nconstant when $x_j$ is locally transported along ${h}$. \nWe give below several examples\nof groups $H_j$ and filters $w_{j,{h}.b}$. \nHowever, learning algorithms compute\nfilters directly, with no prior knowledge on the group $H_j$. \nThe filters $w_{j,{h}.b}$ can be optimized so that variations of\n$x_j (g,{h},b)$ along ${h}$ captures a large variance of\n$x_{j-1}$ within each class. Indeed, this variance is then reduced by\nthe next $\\rho W_{j+1}$.\nThe generators of $H_j$ can\nbe interpreted as {\\it principal symmetry generators}, by analogy \nwith the principal directions of a PCA.\n\n\n\\paragraph{Generalized scattering} The scattering convolution along translations\n(\\ref{nsdfs}) is replaced in\n(\\ref{Consdfoushdf}) by a convolution along $G_{j-1}$, which \ncombines different layer channels. Results for translations can \nessentially be extended to the general case.\nIf $w_{j,{h}.b}$ is an averaging filter then it computes positive\ncoefficients, so the non-linearity $\\rho$\ncan be removed. \nIf each filter $w_{j,{h}.b}$ has a support in\na single fiber indexed by $b$, as in Figure \\ref{figure4},\nthen $B_{j-1} \\subset B_j$.\nIt defines a  generalized scattering transform, which is \na structured multiscale hierarchical convolutional network such that\n$G_{j-1} \\rtimes H_j = G_j$ and $B_{j-1} \\subset B_j$.\nIf $j=0$ then $G_0 = P_0 = {\\mathbb R}^n$ so $B_0$ is reduced to $1$ fiber. \n\n\nAs in the translation case, we need to linearize small deformations in\n${\\rm {Diff}}(G_{j-1})$, which include much more local symmetries than\nthe low-dimensional group $G_{j-1}$.\nA small diffeomorphism $g \\in {\\rm {Diff}}(G_{j-1})$ \nis a non-parallel transport along the fibers of $G_{j-1}$ in $P_{j-1}$, \nwhich is a perturbation of a parallel\ntransport. It modifies distances between pairs of points\nin $P_{j-1}$ by scaling factors. \nTo linearize\nsuch diffeomorphisms, we must use localized filters whose supports have\ndifferent scales. Scale parameters are typically\ndifferent along the different generators\nof $G_{j-1} = {\\mathbb R}^n \\rtimes H_1 \\rtimes ...\\rtimes H_{j-1}$. \nFilters can be constructed with wavelets dilated at different scales,\nalong the generators of each group $H_k$ for $1 \\leq k \\leq j$.\nLinear dimension reduction mostly results from this\nfiltering.\nVariations at fine scales may be eliminated, so that $x_j (g,{h},b)$\ncan be coarsely sampled along $g$.\n\n\n\\paragraph{Rigid movements} For small $j$, the local symmetry groups $H_{j}$ may be\nassociated to linear or non-linear physical phenomena such as rotations,\nscaling, colored illuminations or pitch frequency shifts. \nLet $SO(n)$ be the group of rotations.\nRigid movements\n$SE(n) = {\\mathbb R}^n \\rtimes SO(n)$ is a non-commutative group,\nwhich often includes local symmetries.\nFor images, $n = 2$, this group becomes a transport in\n$P_1$ with $H_1 = SO(n)$ which rotates\na wavelet filter $w_{1,{h}} (u) = w_1 (r_{{h}}^{-1} u)$.\nSuch filters are often observed in the first layer of deep convolutional\nnetworks \\cite{fergus}. \nThey map the action of \n$\\bar g = (v,r_k) \\in SE(n)$ on $x$ to\na parallel transport of $(u,{h}) \\in P_1$ defined \nfor $g \\in G_1 = {\\mathbb R}^2 \\times SO(n)$ by $g.(u,{h}) = (v+r_k u,{h}+k)$.\nSmall diffeomorphisms in ${\\rm {Diff}}(G_j)$\ncorrespond to \ndeformations along translations and rotations, which are sources of local\nsymmetries. \nA roto-translation scattering  \\cite{sifre,Oyallon}\nlinearizes them with  wavelet filters along \ntranslations and rotations, with $G_j = SE(n)$ for all $j > 1$. \nThis roto-translation scattering can efficiently\nregress physical functionals which are often invariant to\nrigid movements, and Lipschitz continuous to deformations. \nFor example, quantum molecular energies $f(x)$ \nare well estimated by sparse regressions over such scattering\nrepresentations  \\cite{Hirn}.\n\n\\paragraph{Audio pitch} Pitch frequency shift is a more complex example of a non-linear symmetry \nfor audio signals. Two different musical notes of a same instrument have\na pitch shift. Their harmonic frequencies are multiplied by\na factor $2^{h}$, but it is not a dilation because\nthe note duration is not changed. \nWith narrow band-pass filters  $w_{1,{h}}(u) = w_1 (2^{-{h}} u)$,\na pitch shift is approximatively mapped to\na translation along ${h} \\in H_1 = {\\mathbb R}$ of $\\rho(x \\star w_{1,{h}} (u))$,\nwith no modification along the time $u$. \nThe action of $g = (v,k) \\in G_1 = {\\mathbb R} \\times {\\mathbb R} = {\\mathbb R}^2$ over\n$(u,{h}) \\in P_1$ \nis thus a two-dimensional translation $g.(u,{h}) = (u+v,{h}+k)$.\nA pitch shift also comes with deformations along time and log-frequencies, \nwhich define a much larger class of symmetries in ${\\rm {Diff}}(G_1)$.\nTwo-dimensional wavelets along $(u,{h})$ can\nlinearize these small time and log-frequency deformations.\nThese define a joint time-frequency scattering applied to \nspeech and music classifications \\cite{andenLostanlen}.\nSuch transformations were first proposed as neurophysiological\nmodels of audition \\cite{shamma}. \n\n\n\\paragraph{Manifolds of patterns} The group $H_{j}$ is associated to complex transformations when $j$ increases.\nIt needs to capture large transformations between different\npatterns in a same class, for example chairs of different styles. Let us consider\ntraining samples $\\{x^i \\}_i$ of a same class.\nThe iterated network contractions\ntransform them into vectors\n$\\{x^i_{j-1} \\}_i$ which are much closer. \nTheir distances define weighted graphs which sample \nunderlying continuous manifolds in\nthe space. Such manifolds clearly appear in \\cite{Aubry}, \nfor high-level patterns\nsuch as chairs or cars, together\nwith poses and colors. \nAs opposed to manifold learning,\ndeep network filters result from\na global optimization which can be\ncomputed in high dimension. The principal symmetry generators of $H_j$\nis associated to common transformations over all\nmanifolds of examples $x^i_{j-1}$, which preserve the class\nwhile capturing large intra-class variance.\nThey are approximatively mapped to a parallel transport in $x_j$ by\nthe filters $w_{j,{h}.b}$.\nThe diffeomorphisms in  ${\\rm {Diff}}(G_j)$ are non-parallel transports\ncorresponding to high-dimensional displacements on the manifolds of $x_{j-1}$.\nLinearizing ${\\rm {Diff}}(G_j)$ is equivalent\nto partially flatten simultaneously all these manifolds, which may\nexplain why manifolds are progressively\nmore regular as the network depth increases \\cite{Aubry}, but it involves\nopen mathematical questions.\n\n\\paragraph{Sparse support vectors} We have up to now been concentrated on the reduction of the data variability through contractions.\nWe now explain why the classification margin can be preserved\nthanks to the existence of multiple fibers $B_j$ in $P_j$,\nby adapting filters instead of using standard wavelets.\nThe fibers indexed by $b \\in B_j$ are separation instruments, which\nincrease dimensionality to avoid reducing the classification margin. \nThey prevent from collapsing \nvectors in different classes, which have a distance $\\|x_{j-1} -  x_{j-1}'\\|$ \nclose to the minimum margin $\\epsilon$. These vectors are close\nto classification frontiers. They are called {\\it multiscale support vectors},\nby analogy with support vector machines.\nTo avoid further contracting their distance, \nthey can be separated along different fibers indexed by $b$. \nThe separation is achieved by filters $w_{j,{h}.b}$, which transform\n$x_{j-1}$ and $x'_{j-1}$ into \n$x_{j}(g,{h},b)$ and\n$x_j'(g,{h},b)$ having sparse supports on different fibers $b$. \nThe next contraction $\\rho W_{j+1}$ reduces distances along fibers indexed\nby $(g,{h}) \\in G_j$, but not across\n$b \\in B_j$, which preserves distances.\nThe contraction increases with $j$ so the number of support vectors close\nto frontiers also increases,\nwhich implies that more fibers are needed to separate them. \n\n\nWhen $j$ increases, \nthe size of $x_j$ is a balance between the dimension reduction along fibers,\nby subsampling $g \\in G_j$, and an increasing number of fibers $B_j$ which\nencode progressively more support vectors.\nCoefficients in these fibers become more specialized and invariants,\nas the grandmother neurons observed in deep layers of\nconvolutional networks \\cite{grandmother}. They have a strong response to \nparticular patterns and are invariant to a large class of transformations.\nIn this model, the choice of filters\n$w_{j,{h}.b}$ are adapted to produce sparse representations\nof multiscale support vectors. They provide a sparse distributed\ncode, defining invariant pattern memorisation. This\nmemorisation is numerically observed in \ndeep network reconstructions \\cite{fergus}, which\ncan restore complex patterns within each class. \nLet us emphasize that groups and fibers are mathematical ghost behind filters,\nwhich are never computed. The learning optimization is directly performed on\nfilters, which carry the trade-off between contractions to reduce the data variability\nand separation to preserve classification margin.\n\n\n\\section{Conclusion}\n\nThis paper provides a mathematical framework to analyze\ncontraction and separation properties of deep convolutional networks.\nIn this model, network filters are guiding non-linear contractions,\nto reduce the data variability in directions of local symmetries. \nThe classification margin can be controlled by sparse separations along network fibers.\nNetwork fibers combine invariances along groups of symmetries\nand distributed pattern representations,   \nwhich could be sufficiently stable to explain transfer learning \nof deep networks \\cite{nature}. However, this is only a framework.\nWe need\ncomplexity measures, approximation theorems in spaces of high-dimensional functions,\nand guaranteed convergence of filter optimization, \nto fully understand the mathematics of these convolution networks. \n\nBesides learning, there are striking similarities between these\nmultiscale mathematical tools and the treatment of \nsymmetries in particle and statistical physics \\cite{Glinsky}. \nOne can expect a rich cross fertilization between high-dimensional\nlearning and physics, through the development of a common mathematical\nlanguage.\n\n\\paragraph{ Acknowledgements} I would like to thank Carmine Emanuele Cella, Ivan Dokmaninc, Sira Ferradans,  Edouard Oyallon and Ir\\`ene Waldspurger for their helpful comments and suggestions.\n\n\\paragraph{Funding} This work was supported by the ERC grant InvariantClass 320959.\n\n\n\n\n\n\n\n\\begin{thebibliography}{9}\n\n\\bibitem{grandmother}\nAgrawal A, Girshick R, Malik J, 2014, \\textit{Analyzing the Performance of Multilayer Neural Networks for Object Recognition}, Proc. of ECCV.\n\n\n\\bibitem{Anden} And\\`en J, Mallat S. 2014 \\textit{Deep Scattering Spectrum},\nIEEE Trans. on Signal Processing, \\textbf{62}.\n\n\\bibitem{andenLostanlen} And\\`en J, Lostanlen V, Mallat S. 2015, \\textit{Joint time-frequency scattering for audio classification}, Proc. of Machine Learn. for Signal Proc., Boston.\n\n\\bibitem{poggio} Anselmi F, Leibo J, Rosasco L, Mutch J, Tacchetti A, Poggio T. 2013 \\textit{Unsupervised Learning of Invariant Representations in Hierarchical Architectures} {arXiv:1311.4158}.\n\n\\bibitem{Aubry} Aubry M, Russell B. 2015 \\textit{Understanding deep features with computer-generated imagery}, arXiv:1506.01151.\n\n\n\\bibitem{Bruna} Bruna J, Mallat S. 2013, \\textit{Invariant Scattering Convolution Networks}, IEEE Trans. on PAMI \\textbf{35}.\n\n\n\n\\bibitem{brunaAnals} Bruna J, Mallat S, Bacry E, Muzy JF. 2015 \n\\textit{Intermittent process analysis with scattering moments} \nAnnals of Stats. \n\\textbf{43}.\n\n\\bibitem{brunastat} Bruna J, Mallat S. 2015 \n\\textit{Stochastic scattering models}\nsubmit. IEEE Trans. Info. Theory.  \n\n\\bibitem{szlam}\nBruna J., Szlam A., Le Cun Y., 2014,\n\\textit{Learning Stable Group Invariant Representations with Convolutional Networks}, ICLR 2014.\n\n\n\n\\bibitem{ridglet} Cand\\`es E, Donoho D. 1999, Ridglets: a key to high-dimensional intermittency ? \\textit{Phil. Trans. Roy. S. A} \\textbf{357}.\n\n\n\n\n\\bibitem{Greengard} Carrier J, Greengard L, Rokhlin V. 1988\n\\textit{A Fast Adaptive Multipole Algorithm for Particle Simulations},\nSIAM J. Sci. Stat. Comput. \\textbf{9}.\n\n\\bibitem{benarous}\nChoromanska A, Henaff M, Mathieu M, Ben ArousG, Le Cun Y. 2014,\n\\textit{The loss surfaces of multilayer networks}, arXiv:1412.0233.\n\n\n\\bibitem{fergus}\nDenton E, Chintala S, Szlam A, Fergus R. 2015 \\textit{Deep generative image models using a Laplacian pyramid of adversarial networks}, NIPS 2015.\n\n\\bibitem{bethdge} Gatys LA, Ecker AS, Bethge M. 2015 \n\\textit{Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks},  \n{arXiv:1505.07376}.\n\n\\bibitem{Glinsky} Glinsky M 2011 \\textit{A new perspective on renormalization: the scattering transformation},  {arXiv:1106.4369}.\n\n\n\n\n\n\\bibitem{Friedman} Hastie T, Tibshirani R, Friedman J. 2009 \\textit{The elements of statistical learning}, Springer Series in Statistics.\n\n\n\\bibitem{speech} Hinton G, Li D, Yu D, Dahl G, Mohamed A, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath T, Kingsbury B. 2012 \\textit{Deep neural networks for acoustic modeling in speech recognition} IEEE Signal Processing Magazine, \\textbf{29}, 82-97. \n\n\n\\bibitem{Hirn}\nHirn M, Poilvert N, Mallat S. 2015 \\textit{Quantum energy regression using scattering transforms} {arXiv:1502.02077}.\n\n\n\n\\bibitem{Krizhevsky} Krizhevsky A, Sutskever I, Hinton G. 2012 \\textit{ImageNet classification with deep convolutional neural networks}, In Proc. of NIPS, p. 1090-1098, 2012. \n\n\n\\bibitem{LeCun} Le Cun Y, Boser B, Denker J, Henderson D, Howard R, Hubbard W, Jackelt L. 1990 \\textit{Handwritten digit recognition with a back-propagation network}, In Proc. of NIPS. \\textbf{3}.\n\n\\bibitem{nature} Le Cun Y, Bengio Y, Hinton G. 2015 \\textit{Deep learning}, \nNature, \\textbf{521}.\n\n\n\\bibitem{Leung} Leung MK, Xiong HY, Lee LJ, Frey BJ. 2014 \\textit{Deep learning of the tissue regulated splicing code}, Bioinformatics, \\textbf{30}.\n\n\n\n\\bibitem{SIFT} Lowe DG. 2004 \\textit{SIFT: Scale Invariant Feature Transform}, J. of Computer Vision, \\textbf{60}. \n\n\\bibitem{mallat-math} Mallat S. 2012, \\textit{Group Invariant Scattering}, \nComm. in Pure and Applied Mathematics, \\textbf{65}.\n\n\\bibitem{shamma} Mesgarani M, Slaney M, Shamma S. 2006 \\textit{Discrimination of speech from nonspeech based on multiscale spectro-temporal modulations} \nIEEE Trans. Audio, Speech, Lang. Process., \\textbf{14}.\n\n\\bibitem{Petitot} J. Petitot, 2008 \\textit{Neurog\\'em\\'etrie de la vision}, \\'Editions de l'\\'Ecole Polytechnique.\n\n\\bibitem{Oyallon} Oyallon E, Mallat S. 2015 \\textit{Deep roto-translation scattering for object classification}, Proc. of CVPR.\n\n\\bibitem{faces} Radford A, Metz L,\nChintala S. 2016 \\textit{Unsupervised representation learning with deep\nconvolutional generative adversarial networks,} ICLR 2016.\n\n\\bibitem{sifre} Sifre L, Mallat S. 2013 \\textit{Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination}, In Proc. of CVPR.\n\n\\bibitem{language} Sutskever I, Vinyals O, Le QV. 2015 \\textit{Sequence to sequence learning with neural networks} In Proc. of NIPS, \\textbf{27}.\n\n\n\\bibitem{adversarial}\nSzegedy C, Erhan D, Zaremba W, Sutskever I, \nGoodfellow I, Bruna J, Fergus R. 2014,\n\\textit{Intriguing properties of neural networks}, In Proc. of ICLR.\n\n\n\\bibitem{waldspurger} Waldspurger I. 2015 \\textit{Wavelet transform modulus: phase retrieval and scattering}, Ph.D Ecole Normale Sup\\`erieure.\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 51052, "prevtext": "\nwhere ${h}.b$ transports $b \\in B_j$ by ${h} \\in H_j$ in $P_j$.\n\\end{proposition}\n\n\\begin{proof}\nWe write\n$x_{j} = \\rho W_{j} x_{j-1}$ as inner products with row vectors:\n\n", "index": 61, "text": "\\begin{equation}\n\\label{definHj}\n\\forall v \\in P_j~~,~~x_{j} (v) = \\rho \\Big( \\sum_{v' \\in P_{j-1}} x_{j-1} (v') \\, w_{j,v} (v')\n\\Big) = \\rho \\Big( {\\langle} x_{j-1} \\,,\\, w_{j,v} {\\rangle} \\Big)~.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\forall v\\in P_{j}~{}~{},~{}~{}x_{j}(v)=\\rho\\Big{(}\\sum_{v^{\\prime}\\in P_{j-1}%&#10;}x_{j-1}(v^{\\prime})\\,w_{j,v}(v^{\\prime})\\Big{)}=\\rho\\Big{(}{\\langle}x_{j-1}\\,%&#10;,\\,w_{j,v}{\\rangle}\\Big{)}~{}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><mi>v</mi></mrow><mo>\u2208</mo><mpadded width=\"+6.6pt\"><msub><mi>P</mi><mi>j</mi></msub></mpadded></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>v</mi><mo>\u2032</mo></msup><mo>\u2208</mo><msub><mi>P</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></munder><mrow><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>v</mi><mo>\u2032</mo></msup><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mi>v</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>v</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mpadded><mo rspace=\"4.2pt\">,</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>,</mo><mi>v</mi></mrow></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo maxsize=\"160%\" minsize=\"160%\" rspace=\"5.8pt\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]