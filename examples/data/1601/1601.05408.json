[{"file": "1601.05408.tex", "nexttext": "\n\\noindent where, $\\boldsymbol\\mu(t_0)$ represents the beginning position at time $t_0$, and the sum in (\\ref{eq:mut_basic}) accumulates a sequence of discrete steps (i.e., displacement vectors) up to time $t_i$.  If we consider the individual displacement vectors (i.e., $\\sigma \\boldsymbol\\varepsilon(t_j)\\equiv \\boldsymbol\\mu(t_j)-\\boldsymbol\\mu(t_{j-1})$) to be independent multivariate Gaussian such that $\\boldsymbol\\varepsilon(t_j)\\sim \\text{N}(\\mathbf{0},\\mathbf{I})$ (i.e., white noise), then we arrive at a continuous-time stochastic integral representation for $\\boldsymbol\\mu(t_i)$ by letting the time between positions ($\\Delta t$) approach zero such that \n\n", "itemtype": "equation", "pos": 8621, "prevtext": "\n\\maketitle\n\\section*{Abstract}\nAdvances in satellite-based data collection techniques have served as a catalyst for new statistical methodology to analyze these data. In wildlife ecological studies, satellite-based data and methodology have provided a wealth of information about animal space use and the investigation of individual-based animal-environment relationships.  With the technology for data collection improving dramatically over time, we are left with massive archives of historical animal telemetry data of varying quality. While many contemporary statistical approaches for inferring movement behavior are specified in discrete time, we develop a flexible continuous-time stochastic differential equation framework that is amenable to reduced-rank second-order covariance parameterizations. We demonstrate how the associated first-order basis functions can be constructed to mimic behavioral characteristics in realistic trajectory processes using telemetry data from mule deer and mountain lion individuals in western North America. Our approach is parallelizable and provides inference for heterogeneous trajectories using nonstationary spatial modeling techniques that are feasible for large telemetry data sets.\n\n\\section{Introduction}\nAdvancements in satellite data collection techniques have stimulated the development of dynamic statistical models for individual-based movement processes (\\citealt{Kays:15}).  Individual-based statistical movement models have been used in a variety of recent applications including: vehicle (e.g., \\citealt{Gloaguen:15}), cellular phone (e.g., \\citealt{Calabrese:11}), and wildlife (e.g., \\citealt{Hooten:10a}) tracking.   In particular, new inferential tools are crucial for improving the understanding of wildlife behavior and the response of individual animals to changing landscapes and environmental conditions.   Modern telemetry technology allows for remote data collection via ``on board'' devices (e.g., often using satellite-based observations of geographic position) and has provided massive repositories of information (e.g., \\citealt{WikelskiKays:15}).      \n\nDespite numerous improvements to data collection methodology for remote tracking of individual animals, several remaining features of contemporary telemetry data must be addressed when obtaining statistical inference.  For example, all forms of remotely collected telemetry data (i.e., measured geographic locations or positions) are susceptible to measurement error that can depend on the device, satellite system, terrain, land cover, weather, and behavior.  Recent advances have led to improved data modeling techniques that properly incorporate (and sometimes estimate) the uncertainty associated with telemetry measurement error (e.g., \\citealt{Brost:15}; \\citealt{Buderman:16}; \\citealt{McClintock:15}).  \n\nIrregular temporal measurement is another important feature to consider when modeling telemetry data.  Telemetry devices are often programmed (i.e., duty cycled) to record position data at a pre-specified set of times.  However, the frequency and regularity of these times are not consistent across studies.  Furthermore, despite the deterministic programming of satellite telemetry devices, missing data can occur stochastically due to instrumental difficulties as well as environmental and behavioral influences (e.g., terrain and weather).         \n\nDynamic statistical models for animal movement that formally incorporate measurement error typically assume a hierarchical structure (\\citealt{Berliner:96}) where $\\mathbf{s}(t_i)$ are the measured positions at time $t_i$ (for observation $i=1,\\ldots,n$) and depend on the true positions $\\boldsymbol\\mu(t_i)$, which arise as a dynamical process.  Various process models have been proposed for the true underlying individual positions $\\boldsymbol\\mu(t_i)$, depending on the desired form of inference.  Primarily, statistical models for position processes have fallen into three main categories: 1.) point process models (e.g., \\citealt{Johnson:08a}; \\citealt{Forester:09}; \\citealt{Johnson:13}; \\citealt{Brost:15}), 2.) discrete-time dynamic models (e.g., \\citealt{Morales:04}; \\citealt{Jonsen:05}; \\citealt{McClintock:12}), and 3.) continuous-time dynamic models (e.g., \\citealt{DunnGipson:77}; \\citealt{Blackwell:97}; \\citealt{Brillinger:01}; \\citealt{Johnson:08b}; \\citealt{Brillinger:10}).  While there have been similar advancements in each of these classes of movement models, we focus on the continuous-time formulations in what follows.  \n\nDespite the popularity of point process models and discrete-time dynamic models, both present computational difficulties that prohibit widespread use for all but the simplest forms.  Point process models require numerical integration to calculate the likelihood (\\citealt{Cressie:93}; \\citealt{BermanTurner:92}; \\citealt{WartonShepherd:10}; \\citealt{Aarts:12}).  Discrete-time dynamic models often lack a joint model specification and require iterative calculation (\\citealt{Morales:04}; \\citealt{Jonsen:05}; \\citealt{McClintock:12}).  Furthermore, discrete-time models provide inference relative to the scale of temporal discretization and must contain some mechanism to reconcile the times at which data are available with those of the latent discrete-time process (\\citealt{McClintock:12}).\n            \nDiscrete-time dynamic models for telemetry data are popular because they are heuristically straightforward to understand.  They have a long history of use and there is a large body of existing literature associated with modeling discrete time series (e.g., \\citealt{Anderson-SprecherLedolter:91}).  Discrete-time models can also be extended to incorporate change-points and hidden Markov processes that allow for time-varying changes in the dynamics, hence better accommodating heterogeneous animal behavior through time (e.g., \\citealt{Morales:04}).    \n\nGiven the attractive properties of discrete-time formulations for model building and the continuous-time nature of the true underlying trajectory, we present a general framework for constructing continuous-time models for animal movement based on limiting processes involving discrete-time models.  Our approach provides an intuitive connection to previously existing continuous-time stochastic process models for telemetry data (e.g., \\citealt{DunnGipson:77}; \\citealt{Blackwell:97}; \\citealt{Brillinger:01}; \\citealt{Johnson:08b}; \\citealt{Brillinger:10}).  Borrowing techniques commonly used in spatial and spatio-temporal statistics (\\citealt{CressieWikle:11}), we show how continuous-time models based on first-order (i.e., mean) dynamic structure can be implemented using equivalent second-order (i.e., covariance) specifications.  Second-order model specifications for animal movement applications have appeared only recently in the literature (e.g., \\citealt{Fleming:14}; \\citealt{Fleming:16}).  \n\nWe present a natural basis function approach to constructing appropriate covariance models for movement processes.  Our basis function specification is amenable to rank reduction and thus computationally feasible to implement for large telemetry data sets.  We also demonstrate how the choice of basis function corresponds to an explicit representation of animal cognitive processes which may involve memory and perception.  Finally, to allow for heterogeneous dynamics in movement, we induce a non-stationary continuous-time temporal process by appropriately warping the temporal domain (i.e., temporal deformation, \\citealt{SampsonGuttorp:92}).  Our warping approach allows for temporal clustering of movement behavior in continuous time similar to popular state-switching approaches in discrete-time models (e.g., \\citealt{Morales:04}; \\citealt{Hanks:11}; \\citealt{McClintock:12}).  We also describe a parallelization strategy that can reduce required computational time substantially.    \n\n\\section{Methods}\n\\subsection{Continuous-Time Stochastic Trajectory Models}\nWe focus on process models for continuous-time trajectories (e.g., individual animal movement) and begin by describing dynamic model specifications for trajectories in multiple dimensions.  We return to measurement error models for observed trajectories in the next Section.  \n\nMany hierarchical statistical models involving continuous-time processes have relied on direct connections to Eulerian differential equations (e.g., \\citealt{CangelosiHooten:09}).  In contrast, we begin with a discrete-time representation and describe the trajectory of a moving particle in terms of its position $\\boldsymbol\\mu(t_i)$, at time $t_i$, as     \n\n", "index": 1, "text": "\\begin{equation}\n  \\boldsymbol\\mu(t_i) = \\boldsymbol\\mu(t_0) + \\sum_{j=1}^i (\\boldsymbol\\mu(t_j)-\\boldsymbol\\mu(t_{j-1})) \\; ,\n  \\label{eq:mut_basic}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\mu}(t_{i})=\\boldsymbol{\\mu}(t_{0})+\\sum_{j=1}^{i}(\\boldsymbol{\\mu%&#10;}(t_{j})-\\boldsymbol{\\mu}(t_{j-1}))\\;,\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $\\sigma$ controls the magnitude of displacement and $\\mathbf{b}(t_i)$ is standard multivariate Brownian motion (i.e., a multivariate Weiner process).  In the limiting trajectory model (\\ref{eq:mut_brownian1}), the number of displacement steps in the sum increases as $\\Delta t \\rightarrow 0$, resulting in an infinite series that is often written as an integral     \n\n", "itemtype": "equation", "pos": 9455, "prevtext": "\n\\noindent where, $\\boldsymbol\\mu(t_0)$ represents the beginning position at time $t_0$, and the sum in (\\ref{eq:mut_basic}) accumulates a sequence of discrete steps (i.e., displacement vectors) up to time $t_i$.  If we consider the individual displacement vectors (i.e., $\\sigma \\boldsymbol\\varepsilon(t_j)\\equiv \\boldsymbol\\mu(t_j)-\\boldsymbol\\mu(t_{j-1})$) to be independent multivariate Gaussian such that $\\boldsymbol\\varepsilon(t_j)\\sim \\text{N}(\\mathbf{0},\\mathbf{I})$ (i.e., white noise), then we arrive at a continuous-time stochastic integral representation for $\\boldsymbol\\mu(t_i)$ by letting the time between positions ($\\Delta t$) approach zero such that \n\n", "index": 3, "text": "\\begin{align}\n  \\boldsymbol\\mu(t_i) &= \\boldsymbol\\mu(t_0) + \\sigma \\lim_{\\Delta t \\rightarrow 0}\\sum_{j=1}^i \\boldsymbol\\varepsilon(t_j) \\label{eq:mut_brownian1} \\\\ \n  &= \\boldsymbol\\mu(t_0) + \\sigma \\mathbf{b}(t_i) \\; ,\n  \\label{eq:mut_brownian2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\boldsymbol{\\mu}(t_{i})\" display=\"inline\"><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\boldsymbol{\\mu}(t_{0})+\\sigma\\lim_{\\Delta t\\rightarrow 0}\\sum_{%&#10;j=1}^{i}\\boldsymbol{\\varepsilon}(t_{j})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>\u2192</mo><mn>0</mn></mrow></munder><mo>\u2061</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover></mstyle><mrow><mi>\ud835\udf3a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\boldsymbol{\\mu}(t_{0})+\\sigma\\mathbf{b}(t_{i})\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent using Ito notation (\\citealt{Protter:04}). \n\nWhile the Brownian motion process model for an individual animal trajectory in (\\ref{eq:bt_ito}) is a stochastic integral equation (SIE), it is also common to see it expressed as a stochastic differential equation (SDE) by differentiating both sides of (\\ref{eq:mut_brownian1}).  Using Ito calculus, the SDE is often written as   \n\n", "itemtype": "equation", "pos": 10099, "prevtext": "\n\\noindent where $\\sigma$ controls the magnitude of displacement and $\\mathbf{b}(t_i)$ is standard multivariate Brownian motion (i.e., a multivariate Weiner process).  In the limiting trajectory model (\\ref{eq:mut_brownian1}), the number of displacement steps in the sum increases as $\\Delta t \\rightarrow 0$, resulting in an infinite series that is often written as an integral     \n\n", "index": 5, "text": "\\begin{equation}\n  \\mathbf{b}(t_i) = \\int_{t_0}^{t_i} d\\mathbf{b}(\\tau)  \\; , \n  \\label{eq:bt_ito}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{b}(t_{i})=\\int_{t_{0}}^{t_{i}}d\\mathbf{b}(\\tau)\\;,\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>i</mi></msub></msubsup><mrow><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc1b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent which is similar to the continuous-time models for trajectories described by \\cite{DunnGipson:77}, \\cite{Blackwell:97}, \\cite{Brillinger:01}, and \\cite{Preisler:04}, for example.   \n\n\\subsection{Smoothness in Trajectory Models}\nStochastic trajectory models based on Brownian motion (e.g., \\ref{eq:mut_brownian2}) are inherently noisy (Figure~\\ref{fig:bm_vs_dj}, left panels) as continuous-time processes.  \\cite{Johnson:08b} presented a stochastic trajectory model for the velocity associated with animal movement which is integrated over time to yield a smoother position process (Figure~\\ref{fig:bm_vs_dj}, right panels).  \n\\begin{figure}[htp]\n  \\centering\n  \\includegraphics[width=5in, angle=0]{bm_vs_dj.jpg}\n  \\caption{Left panels: Two-dimensional Brownian motion process, $\\mathbf{b}(t)$.  Right panels:  Two-dimensional integrated Brownian motion, $\\boldsymbol\\eta(t)$.  Processes are displayed marginally in longitude and latitude.}\n  \\label{fig:bm_vs_dj}\n\\end{figure}\n\nWe show that the velocity model of \\cite{Johnson:08b} fits into a larger class of stochastic trajectory models by reparameterizing the simple Brownian SIE (\\ref{eq:mut_brownian2}).  Recall that Brownian motion ($\\mathbf{b}(t)$), at time $t$, can be expressed as an integral of white noise.  Thus, if we integrate Brownian motion itself, with respect to time, we have \n\n", "itemtype": "equation", "pos": 10599, "prevtext": "\n\\noindent using Ito notation (\\citealt{Protter:04}). \n\nWhile the Brownian motion process model for an individual animal trajectory in (\\ref{eq:bt_ito}) is a stochastic integral equation (SIE), it is also common to see it expressed as a stochastic differential equation (SDE) by differentiating both sides of (\\ref{eq:mut_brownian1}).  Using Ito calculus, the SDE is often written as   \n\n", "index": 7, "text": "\\begin{align}\n  d\\boldsymbol\\mu(t_i) &= \\boldsymbol\\mu(t_0)dt + \\sigma d\\mathbf{b}(t_i) \\\\\n  &= \\boldsymbol\\mu(t_0)dt + \\sigma \\boldsymbol\\varepsilon(t_i) \\;,  \n  \\label{eq:sde_brownian}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle d\\boldsymbol{\\mu}(t_{i})\" display=\"inline\"><mrow><mi>d</mi><mo>\u2062</mo><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\boldsymbol{\\mu}(t_{0})dt+\\sigma d\\mathbf{b}(t_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>+</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\boldsymbol{\\mu}(t_{0})dt+\\sigma\\boldsymbol{\\varepsilon}(t_{i})\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>+</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udf3a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $\\boldsymbol\\eta(t)$ is a version of the integrated stochastic process proposed by \\cite{Johnson:08b}. This integrated Brownian motion model (\\ref{eq:intBM}) can be likened to that of \\cite{Johnson:08b} by substituting $\\boldsymbol\\eta(t)$ into the position process (\\ref{eq:mut_brownian2}) to yield $\\boldsymbol\\mu(t) = \\boldsymbol\\mu(t_0) + \\boldsymbol\\eta(t)$.  \\cite{Jonsen:05} set an earlier precedent for modeling velocity, but strictly in a discrete-time framework.  \n\nThe approach proposed by \\cite{Johnson:08b} suggests a more general framework for modeling movement that can be obtained by reparameterizing the velocity model using the $2\\times 2$ matrix $\\mathbf{H}(t,\\tau)$ with diagonal elements equal to the function \n\n", "itemtype": "equation", "pos": 12153, "prevtext": "\n\\noindent which is similar to the continuous-time models for trajectories described by \\cite{DunnGipson:77}, \\cite{Blackwell:97}, \\cite{Brillinger:01}, and \\cite{Preisler:04}, for example.   \n\n\\subsection{Smoothness in Trajectory Models}\nStochastic trajectory models based on Brownian motion (e.g., \\ref{eq:mut_brownian2}) are inherently noisy (Figure~\\ref{fig:bm_vs_dj}, left panels) as continuous-time processes.  \\cite{Johnson:08b} presented a stochastic trajectory model for the velocity associated with animal movement which is integrated over time to yield a smoother position process (Figure~\\ref{fig:bm_vs_dj}, right panels).  \n\\begin{figure}[htp]\n  \\centering\n  \\includegraphics[width=5in, angle=0]{bm_vs_dj.jpg}\n  \\caption{Left panels: Two-dimensional Brownian motion process, $\\mathbf{b}(t)$.  Right panels:  Two-dimensional integrated Brownian motion, $\\boldsymbol\\eta(t)$.  Processes are displayed marginally in longitude and latitude.}\n  \\label{fig:bm_vs_dj}\n\\end{figure}\n\nWe show that the velocity model of \\cite{Johnson:08b} fits into a larger class of stochastic trajectory models by reparameterizing the simple Brownian SIE (\\ref{eq:mut_brownian2}).  Recall that Brownian motion ($\\mathbf{b}(t)$), at time $t$, can be expressed as an integral of white noise.  Thus, if we integrate Brownian motion itself, with respect to time, we have \n\n", "index": 9, "text": "\\begin{equation}\n  \\boldsymbol\\eta(t) = \\int_{t_0}^t \\sigma \\mathbf{b}(\\tau) d\\tau  \\; ,  \n  \\label{eq:intBM}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\eta}(t)=\\int_{t_{0}}^{t}\\sigma\\mathbf{b}(\\tau)d\\tau\\;,\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udf3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><mi>t</mi></msubsup><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mi>\u03c4</mi></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $t_n$ is the last time at which data are observed, and off-diagonal elements equal zero.  Substituting $\\mathbf{H}(t,\\tau)$ into (\\ref{eq:intBM}), the velocity-based Brownian motion model appears as the convolution \n\n", "itemtype": "equation", "pos": 13026, "prevtext": "\n\\noindent where $\\boldsymbol\\eta(t)$ is a version of the integrated stochastic process proposed by \\cite{Johnson:08b}. This integrated Brownian motion model (\\ref{eq:intBM}) can be likened to that of \\cite{Johnson:08b} by substituting $\\boldsymbol\\eta(t)$ into the position process (\\ref{eq:mut_brownian2}) to yield $\\boldsymbol\\mu(t) = \\boldsymbol\\mu(t_0) + \\boldsymbol\\eta(t)$.  \\cite{Jonsen:05} set an earlier precedent for modeling velocity, but strictly in a discrete-time framework.  \n\nThe approach proposed by \\cite{Johnson:08b} suggests a more general framework for modeling movement that can be obtained by reparameterizing the velocity model using the $2\\times 2$ matrix $\\mathbf{H}(t,\\tau)$ with diagonal elements equal to the function \n\n", "index": 11, "text": "\\begin{equation}\n  h(t,\\tau) = \n  \\begin{cases}\n    1  &\\mbox{if } t_0 < \\tau \\leq t \\\\\n    0  &\\mbox{if } t < \\tau \\leq t_n  \n  \\end{cases} \\; , \n  \\label{eq:hJohnson}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"h(t,\\tau)=\\begin{cases}1&amp;\\mbox{if }t_{0}&lt;\\tau\\leq t\\\\&#10;0&amp;\\mbox{if }t&lt;\\tau\\leq t_{n}\\end{cases}\\;,\" display=\"block\"><mrow><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+2.8pt\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>t</mi><mn>0</mn></msub></mrow><mo>&lt;</mo><mi>\u03c4</mi><mo>\u2264</mo><mi>t</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>t</mi></mrow><mo>&lt;</mo><mi>\u03c4</mi><mo>\u2264</mo><msub><mi>t</mi><mi>n</mi></msub></mrow></mtd></mtr></mtable></mrow></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent The convolution in (\\ref{eq:intBMconv}) is the key feature in a more general class of stochastic process models for animal movement trajectories.  For example, if $h(t,\\tau)$ is a continuous function such that $t_0 \\leq t \\leq t_n$, $t_0 \\leq \\tau \\leq t_n$ and with finite positive integral $0 < \\int_{t_0}^{t_n} h(t,\\tau) d\\tau < \\infty$, then a new general class of continuous-time animal movement models arises.  We refer to this class of models as ``functional movement models'' (FMMs). \n\nThe ability to specify continuous-time movement models as a convolution (\\ref{eq:intBMconv}) has two major advantages.  First, it clearly identifies the connections among animal movement models and similar models used in spatial statistics and time series.  Second, for the same reasons convolution specifications are popular in spatial statistics and time series, we show that FMMs share similar advantageous properties.   \n\nIn Appendix A (Supplementary Material), we show that the FMM in (\\ref{eq:intBMconv}) can be rewritten as:  \n\n", "itemtype": "equation", "pos": 13442, "prevtext": "\n\\noindent where $t_n$ is the last time at which data are observed, and off-diagonal elements equal zero.  Substituting $\\mathbf{H}(t,\\tau)$ into (\\ref{eq:intBM}), the velocity-based Brownian motion model appears as the convolution \n\n", "index": 13, "text": "\\begin{equation}\n  \\boldsymbol\\eta(t) = \\int_{t_0}^{t_n} \\sigma \\mathbf{H}(t,\\tau) \\mathbf{b}(\\tau) d\\tau  \\; .  \n  \\label{eq:intBMconv}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\eta}(t)=\\int_{t_{0}}^{t_{n}}\\sigma\\mathbf{H}(t,\\tau)\\mathbf{b}(%&#10;\\tau)d\\tau\\;.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udf3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc07</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mi>\u03c4</mi></mpadded></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $\\tilde{\\mathbf{H}}(t,\\tau)$ has diagonal elements  \n\n", "itemtype": "equation", "pos": 14632, "prevtext": "\n\\noindent The convolution in (\\ref{eq:intBMconv}) is the key feature in a more general class of stochastic process models for animal movement trajectories.  For example, if $h(t,\\tau)$ is a continuous function such that $t_0 \\leq t \\leq t_n$, $t_0 \\leq \\tau \\leq t_n$ and with finite positive integral $0 < \\int_{t_0}^{t_n} h(t,\\tau) d\\tau < \\infty$, then a new general class of continuous-time animal movement models arises.  We refer to this class of models as ``functional movement models'' (FMMs). \n\nThe ability to specify continuous-time movement models as a convolution (\\ref{eq:intBMconv}) has two major advantages.  First, it clearly identifies the connections among animal movement models and similar models used in spatial statistics and time series.  Second, for the same reasons convolution specifications are popular in spatial statistics and time series, we show that FMMs share similar advantageous properties.   \n\nIn Appendix A (Supplementary Material), we show that the FMM in (\\ref{eq:intBMconv}) can be rewritten as:  \n\n", "index": 15, "text": "\\begin{equation}\n  \\boldsymbol\\eta(t) = \\int_{t_0}^{t_n} \\sigma \\tilde{\\mathbf{H}}(t,\\tau)  \\boldsymbol\\varepsilon(\\tau) d\\tau \\; , \n  \\label{eq:intHtilda} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\eta}(t)=\\int_{t_{0}}^{t_{n}}\\sigma\\tilde{\\mathbf{H}}(t,\\tau)%&#10;\\boldsymbol{\\varepsilon}(\\tau)d\\tau\\;,\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udf3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup><mrow><mi>\u03c3</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf3a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mi>\u03c4</mi></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\nReturning now to the advantages of this FMM approach, we can see that (\\ref{eq:intHtilda}) has the same form described in spatial statistics as a ``process convolution'' (or kernel convolution; e.g., \\citealt{BarryVerHoef:96}; \\citealt{Higdon:98}; \\citealt{Lee:05}; \\citealt{Calder:07}).  The process convolution has been instrumental in many fields, but especially in spatial statistics for allowing both complicated and efficient representations of covariance structure.  \n\nThere are three main computational advantages to using the convolution perspective in continuous-time stochastic trajectory models.  First, it is clear from (\\ref{eq:intHtilda}) that we need not simulate Brownian motion, rather we can operate on it implicitly by transforming the matrix function $\\mathbf{H}(t,\\tau)$ to $\\tilde{\\mathbf{H}}(t,\\tau)$ via integration and convolving $\\sigma\\tilde{\\mathbf{H}}(t,\\tau)$ with white noise directly.  The convolution with white noise resembles that used in covariance models for spatial processes (e.g., \\citealt{BarryVerHoef:96}; \\citealt{Higdon:98}).  \n\nAssume the Gaussian kernel as the function $h(t,\\tau)$, for example.  The Gaussian kernel is one of the most commonly used functions in kernel convolution methods.  Normalizing the Gaussian kernel so that it integrates to one for $t_0<\\tau < t_n$, results in a truncated normal probability density function (pdf) for the kernel such that $h(t,\\tau)\\equiv \\text{TN}(\\tau,t,\\phi^2)_{t_0}^{t_n}$.  We can then convert it to the required function $\\tilde{h}(t,\\tau)$ using (\\ref{eq:htilda}) and arrive at a numerical solution for the new kernel function by subtracting the truncated normal cumulative distribution function (cdf) from one, a trivial calculation in most statistical software.  With respect to the time domain, the kernel $\\tilde{h}(t,\\tau)$ appears different than most kernels used in time series or spatial statistics (Figure~\\ref{fig:hforms}, column b, row 5).  Rather than being unimodal and symmetric, it has a sigmoidal shape equal to one at $t=t_0$ and nonlinearly decreasing to zero at $t=t_n$ resembling an I-spline (\\citealt{Ramsay:88}).  In effect, this new kernel is accumulating the white noise up to near time $t$ and including a discounted amount of white noise ahead of time $t$.     \n\\begin{figure}[htp]\n  \\centering\n  \\includegraphics[height=7in, angle=0]{hforms_med.pdf}\n  \\caption{Example kernels $h(t,\\tau)$ (column a) and resulting integrated kernels $\\tilde{h}(t,\\tau)$ (column b). The first row results in the regular Brownian motion, row two is equivalent to that used by \\cite{Johnson:08b}, rows three through five are more common in time series and spatial statistics. The vertical gray line indicates time $t$ for the particular kernel shown; in this case $t=0.5$.}\n  \\label{fig:hforms}\n\\end{figure}\n\nThe options for kernel functions are limitless and will result in different stochastic process models for animal movement.  In fact, we have already seen that this FMM class of movement models is general enough to include that proposed by \\cite{Johnson:08b}.  The FMM class also includes the original unsmoothed Brownian motion process if we let $h(t,\\tau)$ be a point mass function at $\\tau=t$ and zero elsewhere (Figure~\\ref{fig:hforms}).  The point mass kernel function can also be achieved by taking the limit as $\\phi\\rightarrow0$ of our Gaussian kernel and results in an integrated kernel function of   \n\n", "itemtype": "equation", "pos": 14873, "prevtext": "\n\\noindent where $\\tilde{\\mathbf{H}}(t,\\tau)$ has diagonal elements  \n\n", "index": 17, "text": "\\begin{equation}\n  \\tilde{h}(t,\\tau) = \\int_{\\tau}^{t_n} h(t,\\tilde\\tau) d\\tilde\\tau  \\; .\n  \\label{eq:htilda}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h}(t,\\tau)=\\int_{\\tau}^{t_{n}}h(t,\\tilde{\\tau})d\\tilde{\\tau}\\;.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>\u03c4</mi><msub><mi>t</mi><mi>n</mi></msub></msubsup><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover></mpadded></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent We can imagine this integrated kernel summing up all of the past velocities to obtain the current position. The steep drop at $\\tau=t$ is what induces roughness in the original Brownian motion process (Figure~\\ref{fig:hforms}, column b, row 1).  Whereas, when we use a non-pointmass function for $h(t,\\tau)$, we arrive at a smoother stochastic process model for movement.   \n\nWe highlight a few different kernel functions to examine their implications for animal movement behavior.  In doing so, it is simplest to interpret the $h(t,\\tau)$ and $\\tilde{h}(t,\\tau)$ functions directly.  For example, using the direct integration of velocity as proposed by \\cite{Johnson:08b} results in the integrated kernel ($\\tilde{h}(t,\\tau)$) on the second row of column b in Figure~\\ref{fig:hforms}.  In this case, the individual's position simply accumulates its past steps which are noisy themselves in direction and length but have some general momentum.  The ``tail up'' kernel shown in the third row of Figure~\\ref{fig:hforms} models the current position based on past steps that decay linearly with time (we borrow the tail up and tail down terminology from \\citealt{VerHoefPeterson:10}).  In this case, the individual's position is more strongly a function of recent steps than steps in the distant past.  The opposite is true with the ``tail down'' model shown in row 4 of Figure~\\ref{fig:hforms}, where only future steps influence position.  Heuristically, we interpret the resulting movement as perception driven.  That is, the individual may have an awareness of a distant destination that affects their movement.  Finally, the Gaussian kernel discussed earlier and shown in the bottom row of Figure~\\ref{fig:hforms} indicates a symmetric mixture of previous and future velocities suggesting an equal perception of former and future events by the individual.       \n\n\\subsection{Functional movement models and covariance}\nUsing the basic FMM (\\ref{eq:intBMconv}) presented in the previous Section, we can choose from a large set of possible smoothing kernels ($h(t,\\tau)$) for Brownian motion and arrive at the appropriate form for the integrated kernel ($\\tilde{h}(t,\\tau)$) that is convolved with white noise.  We can then use $\\tilde{h}(t,\\tau)$ directly to construct the proper covariance function for the joint process.  In fact, for a one-dimensional movement process $\\eta(t)$, the covariance function can be calculated (e.g., \\citealt{PaciorekSchervish:06}) as the convolution of the kernels \n\n", "itemtype": "equation", "pos": 18422, "prevtext": "\nReturning now to the advantages of this FMM approach, we can see that (\\ref{eq:intHtilda}) has the same form described in spatial statistics as a ``process convolution'' (or kernel convolution; e.g., \\citealt{BarryVerHoef:96}; \\citealt{Higdon:98}; \\citealt{Lee:05}; \\citealt{Calder:07}).  The process convolution has been instrumental in many fields, but especially in spatial statistics for allowing both complicated and efficient representations of covariance structure.  \n\nThere are three main computational advantages to using the convolution perspective in continuous-time stochastic trajectory models.  First, it is clear from (\\ref{eq:intHtilda}) that we need not simulate Brownian motion, rather we can operate on it implicitly by transforming the matrix function $\\mathbf{H}(t,\\tau)$ to $\\tilde{\\mathbf{H}}(t,\\tau)$ via integration and convolving $\\sigma\\tilde{\\mathbf{H}}(t,\\tau)$ with white noise directly.  The convolution with white noise resembles that used in covariance models for spatial processes (e.g., \\citealt{BarryVerHoef:96}; \\citealt{Higdon:98}).  \n\nAssume the Gaussian kernel as the function $h(t,\\tau)$, for example.  The Gaussian kernel is one of the most commonly used functions in kernel convolution methods.  Normalizing the Gaussian kernel so that it integrates to one for $t_0<\\tau < t_n$, results in a truncated normal probability density function (pdf) for the kernel such that $h(t,\\tau)\\equiv \\text{TN}(\\tau,t,\\phi^2)_{t_0}^{t_n}$.  We can then convert it to the required function $\\tilde{h}(t,\\tau)$ using (\\ref{eq:htilda}) and arrive at a numerical solution for the new kernel function by subtracting the truncated normal cumulative distribution function (cdf) from one, a trivial calculation in most statistical software.  With respect to the time domain, the kernel $\\tilde{h}(t,\\tau)$ appears different than most kernels used in time series or spatial statistics (Figure~\\ref{fig:hforms}, column b, row 5).  Rather than being unimodal and symmetric, it has a sigmoidal shape equal to one at $t=t_0$ and nonlinearly decreasing to zero at $t=t_n$ resembling an I-spline (\\citealt{Ramsay:88}).  In effect, this new kernel is accumulating the white noise up to near time $t$ and including a discounted amount of white noise ahead of time $t$.     \n\\begin{figure}[htp]\n  \\centering\n  \\includegraphics[height=7in, angle=0]{hforms_med.pdf}\n  \\caption{Example kernels $h(t,\\tau)$ (column a) and resulting integrated kernels $\\tilde{h}(t,\\tau)$ (column b). The first row results in the regular Brownian motion, row two is equivalent to that used by \\cite{Johnson:08b}, rows three through five are more common in time series and spatial statistics. The vertical gray line indicates time $t$ for the particular kernel shown; in this case $t=0.5$.}\n  \\label{fig:hforms}\n\\end{figure}\n\nThe options for kernel functions are limitless and will result in different stochastic process models for animal movement.  In fact, we have already seen that this FMM class of movement models is general enough to include that proposed by \\cite{Johnson:08b}.  The FMM class also includes the original unsmoothed Brownian motion process if we let $h(t,\\tau)$ be a point mass function at $\\tau=t$ and zero elsewhere (Figure~\\ref{fig:hforms}).  The point mass kernel function can also be achieved by taking the limit as $\\phi\\rightarrow0$ of our Gaussian kernel and results in an integrated kernel function of   \n\n", "index": 19, "text": "\\begin{equation}\n  \\tilde{h}(t,\\tau) = \n  \\begin{cases}\n    1  &\\mbox{if } t_0 < \\tau \\leq t \\\\\n    0  &\\mbox{if } t < \\tau \\leq t_n  \n  \\end{cases} \\; . \n  \\label{eq:htildaBM}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h}(t,\\tau)=\\begin{cases}1&amp;\\mbox{if }t_{0}&lt;\\tau\\leq t\\\\&#10;0&amp;\\mbox{if }t&lt;\\tau\\leq t_{n}\\end{cases}\\;.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+2.8pt\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>t</mi><mn>0</mn></msub></mrow><mo>&lt;</mo><mi>\u03c4</mi><mo>\u2264</mo><mi>t</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>t</mi></mrow><mo>&lt;</mo><mi>\u03c4</mi><mo>\u2264</mo><msub><mi>t</mi><mi>n</mi></msub></mrow></mtd></mtr></mtable></mrow></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent for any two times, say $t_1$ and $t_2$.  \n\nOne benefit of the covariance function (\\ref{eq:BMconvcov}) is that, for a finite subset of $n$ times $\\{t_1, \\ldots, t_n\\}$ and process $\\boldsymbol\\eta \\equiv (\\eta_1,\\ldots,\\eta_n)'$, the joint probability model can be expressed as    \n\n", "itemtype": "equation", "pos": 21122, "prevtext": "\n\\noindent We can imagine this integrated kernel summing up all of the past velocities to obtain the current position. The steep drop at $\\tau=t$ is what induces roughness in the original Brownian motion process (Figure~\\ref{fig:hforms}, column b, row 1).  Whereas, when we use a non-pointmass function for $h(t,\\tau)$, we arrive at a smoother stochastic process model for movement.   \n\nWe highlight a few different kernel functions to examine their implications for animal movement behavior.  In doing so, it is simplest to interpret the $h(t,\\tau)$ and $\\tilde{h}(t,\\tau)$ functions directly.  For example, using the direct integration of velocity as proposed by \\cite{Johnson:08b} results in the integrated kernel ($\\tilde{h}(t,\\tau)$) on the second row of column b in Figure~\\ref{fig:hforms}.  In this case, the individual's position simply accumulates its past steps which are noisy themselves in direction and length but have some general momentum.  The ``tail up'' kernel shown in the third row of Figure~\\ref{fig:hforms} models the current position based on past steps that decay linearly with time (we borrow the tail up and tail down terminology from \\citealt{VerHoefPeterson:10}).  In this case, the individual's position is more strongly a function of recent steps than steps in the distant past.  The opposite is true with the ``tail down'' model shown in row 4 of Figure~\\ref{fig:hforms}, where only future steps influence position.  Heuristically, we interpret the resulting movement as perception driven.  That is, the individual may have an awareness of a distant destination that affects their movement.  Finally, the Gaussian kernel discussed earlier and shown in the bottom row of Figure~\\ref{fig:hforms} indicates a symmetric mixture of previous and future velocities suggesting an equal perception of former and future events by the individual.       \n\n\\subsection{Functional movement models and covariance}\nUsing the basic FMM (\\ref{eq:intBMconv}) presented in the previous Section, we can choose from a large set of possible smoothing kernels ($h(t,\\tau)$) for Brownian motion and arrive at the appropriate form for the integrated kernel ($\\tilde{h}(t,\\tau)$) that is convolved with white noise.  We can then use $\\tilde{h}(t,\\tau)$ directly to construct the proper covariance function for the joint process.  In fact, for a one-dimensional movement process $\\eta(t)$, the covariance function can be calculated (e.g., \\citealt{PaciorekSchervish:06}) as the convolution of the kernels \n\n", "index": 21, "text": "\\begin{equation}\n  \\text{cov}(\\eta(t_1),\\eta(t_2))=\\int_{t_0}^{t_n} \\tilde{h}(t_1,\\tau)\\tilde{h}(t_2,\\tau)d\\tau \\; ,\n  \\label{eq:BMconvcov}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\text{cov}(\\eta(t_{1}),\\eta(t_{2}))=\\int_{t_{0}}^{t_{n}}\\tilde{h}(t_{1},\\tau)%&#10;\\tilde{h}(t_{2},\\tau)d\\tau\\;,\" display=\"block\"><mrow><mrow><mrow><mtext>cov</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mi>\u03c4</mi></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $\\mathbf{0}$ is an $n\\times 1$ vector of zeros and $\\tilde{\\mathbf{H}}$ is a matrix of basis functions with the $i^{\\text{th}}$ row equal to $\\tilde{h}(t_i,\\tau)$ for all $\\tau$.  This procedure for constructing the covariance matrix and defining a correlated process is similar to that recommended in spatial statistics (e.g., \\citealt{PaciorekSchervish:06}; \\citealt{VerHoefPeterson:10}).  For simplicity, the resulting model for the joint one-dimensional position process in this FMM for smooth Brownian motion is   \n\n", "itemtype": "equation", "pos": 21569, "prevtext": "\n\\noindent for any two times, say $t_1$ and $t_2$.  \n\nOne benefit of the covariance function (\\ref{eq:BMconvcov}) is that, for a finite subset of $n$ times $\\{t_1, \\ldots, t_n\\}$ and process $\\boldsymbol\\eta \\equiv (\\eta_1,\\ldots,\\eta_n)'$, the joint probability model can be expressed as    \n\n", "index": 23, "text": "\\begin{equation}\n  \\boldsymbol\\eta\\sim\\text{N}(\\mathbf{0},\\sigma^2\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}') \\; ,\n  \\label{eq:BMjoint}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\eta}\\sim\\text{N}(\\mathbf{0},\\sigma^{2}\\tilde{\\mathbf{H}}\\tilde{%&#10;\\mathbf{H}}^{\\prime})\\;,\" display=\"block\"><mrow><mrow><mi>\ud835\udf3c</mi><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn/><mo>,</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\nDespite the fact that it is often more intuitive to model the process from the first-moment (i.e., mean dynamical structure) rather than the second-moment (\\citealt{WikleHooten:10}), the joint form of (\\ref{eq:BMjointmu}) with dependence imposed through the correlation matrix $\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}'$ can be useful computationally (\\citealt{Sampson:10}).  When the integral in (\\ref{eq:BMconvcov}) cannot be used to analytically compute the necessary covariance matrix, we can still use the outer product of the matrices explicitly (i.e., $\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}'$).  However, the true covariance requires the number of columns of $\\tilde{\\mathbf{H}}$ to approach infinity, which, under approximation, can lead to computational difficulties.  \\cite{Higdon:02} suggested a finite process convolution as an approximation.  In the finite approximation, the number of columns of $\\tilde{\\mathbf{H}}$ could be reduced to say, $m$ columns.  This rank reduction implies that there are $m$ knots in the temporal domain that anchor the basis functions (i.e., kernels) and thus only $m$ white noise terms are required so that $\\boldsymbol\\eta \\approx \\tilde{\\mathbf{H}}\\boldsymbol\\varepsilon$, where $\\tilde{\\mathbf{H}}$ is an $n\\times m$ matrix and $\\boldsymbol\\varepsilon\\equiv (\\varepsilon(t_1),\\ldots,\\varepsilon(t_j),\\ldots,\\varepsilon(t_m))'$ is an $m\\times 1$ vector.  The use of a finite approximation to the convolution is also sometimes referred to as a reduced-rank method (\\citealt{Wikle:10}).  Rank reduction can improve computational efficiency substantially and has become very popular in spatial and spatio-temporal statistics for large data sets (\\citealt{NychkaSaltzman:98}; \\citealt{Nychka:02}; \\citealt{Banerjee:08};  \\citealt{CressieWikle:11}).          \n\nTo illustrate how kernel functions relate to covariance thus far, we have simplified the movement process so that it is one-dimensional in space.  The same approach generalizes to higher dimensions.  In the more typical two-dimensional case, we stack the vectors in each dimension to form a single $2n\\times 1$ vector $\\boldsymbol\\eta$.  Then the joint model can be written as \n\n", "itemtype": "equation", "pos": 22251, "prevtext": "\n\\noindent where $\\mathbf{0}$ is an $n\\times 1$ vector of zeros and $\\tilde{\\mathbf{H}}$ is a matrix of basis functions with the $i^{\\text{th}}$ row equal to $\\tilde{h}(t_i,\\tau)$ for all $\\tau$.  This procedure for constructing the covariance matrix and defining a correlated process is similar to that recommended in spatial statistics (e.g., \\citealt{PaciorekSchervish:06}; \\citealt{VerHoefPeterson:10}).  For simplicity, the resulting model for the joint one-dimensional position process in this FMM for smooth Brownian motion is   \n\n", "index": 25, "text": "\\begin{equation}\n  \\boldsymbol\\mu\\sim\\text{N}(\\mu(0)\\mathbf{1},\\sigma^2\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}') \\; . \n  \\label{eq:BMjointmu}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\mu}\\sim\\text{N}(\\mu(0)\\mathbf{1},\\sigma^{2}\\tilde{\\mathbf{H}}%&#10;\\tilde{\\mathbf{H}}^{\\prime})\\;.\" display=\"block\"><mrow><mrow><mi>\ud835\udf41</mi><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo>,</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where $\\mathbf{I}$ is a $2\\times 2$ identity matrix (assuming that $\\tilde{\\mathbf{H}}$ is the appropriate set of basis functions for both directions; i.e., latitude and longitude).  \n\n\\subsection{Heterogeneous Dynamics}\nDiscrete-time trajectory models for animal movement (e.g., \\citealt{Morales:04}) are commonly specified with time-varying dynamics.  In spatial statistics, heterogeneity in dependence structure is often treated as nonstationarity. A variety of approaches have been suggested for modeling nonstationary continuous processes (e.g., \\citealt{SampsonGuttorp:92}; \\citealt{Higdon:02}).  We describe a temporal warping approach to modeling nonstationary movement.  \n\nOur warping approach (also known as ``deformation'') was described in a continuous spatial modeling context by \\cite{SampsonGuttorp:92} and later extended (e.g., \\citealt{Damian:01}; \\citealt{SchmidtOHagan:03}).  In the warping approach, we map the times $\\mathbf{t}\\equiv (t_1,\\ldots,t_n)'$ to a new set of times $\\mathbf{w}\\equiv(w_1,\\ldots,w_n)'$ using a smooth function so that no ``folding'' occurs (i.e., transformed times remain ordinal).  A model-based method to perform the warping allows $\\mathbf{w}$ to arise as a correlated random field anchored by $\\mathbf{t}$.  For example, a simple additive warping can be induced through the Gaussian process model $\\mathbf{w} \\sim \\text{N}(\\mathbf{t},\\boldsymbol\\Sigma_w)$, where a smooth covariance function, such as the Gaussian $\\Sigma_{w,ij} \\equiv \\sigma_w^2 \\exp(-(t_i - t_j)^2/\\phi_w^2)$, is chosen.\n\nWhen fitting the FMM (\\ref{eq:BMjointmu}), we construct the basis functions so that they depend on the warped times.  For example, using a Gaussian kernel, we now have \n\n", "itemtype": "equation", "pos": 24581, "prevtext": "\n\nDespite the fact that it is often more intuitive to model the process from the first-moment (i.e., mean dynamical structure) rather than the second-moment (\\citealt{WikleHooten:10}), the joint form of (\\ref{eq:BMjointmu}) with dependence imposed through the correlation matrix $\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}'$ can be useful computationally (\\citealt{Sampson:10}).  When the integral in (\\ref{eq:BMconvcov}) cannot be used to analytically compute the necessary covariance matrix, we can still use the outer product of the matrices explicitly (i.e., $\\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}'$).  However, the true covariance requires the number of columns of $\\tilde{\\mathbf{H}}$ to approach infinity, which, under approximation, can lead to computational difficulties.  \\cite{Higdon:02} suggested a finite process convolution as an approximation.  In the finite approximation, the number of columns of $\\tilde{\\mathbf{H}}$ could be reduced to say, $m$ columns.  This rank reduction implies that there are $m$ knots in the temporal domain that anchor the basis functions (i.e., kernels) and thus only $m$ white noise terms are required so that $\\boldsymbol\\eta \\approx \\tilde{\\mathbf{H}}\\boldsymbol\\varepsilon$, where $\\tilde{\\mathbf{H}}$ is an $n\\times m$ matrix and $\\boldsymbol\\varepsilon\\equiv (\\varepsilon(t_1),\\ldots,\\varepsilon(t_j),\\ldots,\\varepsilon(t_m))'$ is an $m\\times 1$ vector.  The use of a finite approximation to the convolution is also sometimes referred to as a reduced-rank method (\\citealt{Wikle:10}).  Rank reduction can improve computational efficiency substantially and has become very popular in spatial and spatio-temporal statistics for large data sets (\\citealt{NychkaSaltzman:98}; \\citealt{Nychka:02}; \\citealt{Banerjee:08};  \\citealt{CressieWikle:11}).          \n\nTo illustrate how kernel functions relate to covariance thus far, we have simplified the movement process so that it is one-dimensional in space.  The same approach generalizes to higher dimensions.  In the more typical two-dimensional case, we stack the vectors in each dimension to form a single $2n\\times 1$ vector $\\boldsymbol\\eta$.  Then the joint model can be written as \n\n", "index": 27, "text": "\\begin{equation}\n  \\boldsymbol\\eta\\sim\\text{N}(\\mathbf{0},\\sigma^2(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}}\\tilde{\\mathbf{H}}')) \\; ,\n  \\label{eq:BMjointMV}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{\\eta}\\sim\\text{N}(\\mathbf{0},\\sigma^{2}(\\mathbf{I}\\otimes\\tilde{%&#10;\\mathbf{H}}\\tilde{\\mathbf{H}}^{\\prime}))\\;,\" display=\"block\"><mrow><mrow><mi>\ud835\udf3c</mi><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn/><mo>,</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent Notice that the kernel $h(t,\\tau)$ only depends on a single range parameter $\\phi$ that does not vary in time itself because the heterogeneity enters through the norm in the warped temporal domain $(w(t)-\\tau)^2$.  Thus, the warp field $\\mathbf{w}$ is a latent process to be estimated in the model and can provide inference associated with changes in movement dynamics.  For example, the derivative $dw(t)/dt$ of the warp field provides insight about changes in the individual's velocity over time.  When $dw(t)/dt$ is large, the warp field expands the temporal domain to allow for higher speeds and and less tortuous paths.  Expanding the temporal domain leads to behavior indicative of long-distance migration or nomadicism.  By contrast, when the $dw(t)/dt$ is small, the temporal domain compresses, causing the individual to slow and exhibit sharper turning angles.  Temporal compression leads to behavior more typical of a central place forager, or an individual performing routine activities within its home range (e.g., foraging). \n\n\n\\subsection{Full Model Specification}\nAssuming Gaussian measurement error associated with the telemetry observations and a full-rank position process $\\boldsymbol\\mu$, we can specify the full model using matrix notation in a hierarchical Bayesian framework   \n\n", "itemtype": "equation", "pos": 26469, "prevtext": "\n\\noindent where $\\mathbf{I}$ is a $2\\times 2$ identity matrix (assuming that $\\tilde{\\mathbf{H}}$ is the appropriate set of basis functions for both directions; i.e., latitude and longitude).  \n\n\\subsection{Heterogeneous Dynamics}\nDiscrete-time trajectory models for animal movement (e.g., \\citealt{Morales:04}) are commonly specified with time-varying dynamics.  In spatial statistics, heterogeneity in dependence structure is often treated as nonstationarity. A variety of approaches have been suggested for modeling nonstationary continuous processes (e.g., \\citealt{SampsonGuttorp:92}; \\citealt{Higdon:02}).  We describe a temporal warping approach to modeling nonstationary movement.  \n\nOur warping approach (also known as ``deformation'') was described in a continuous spatial modeling context by \\cite{SampsonGuttorp:92} and later extended (e.g., \\citealt{Damian:01}; \\citealt{SchmidtOHagan:03}).  In the warping approach, we map the times $\\mathbf{t}\\equiv (t_1,\\ldots,t_n)'$ to a new set of times $\\mathbf{w}\\equiv(w_1,\\ldots,w_n)'$ using a smooth function so that no ``folding'' occurs (i.e., transformed times remain ordinal).  A model-based method to perform the warping allows $\\mathbf{w}$ to arise as a correlated random field anchored by $\\mathbf{t}$.  For example, a simple additive warping can be induced through the Gaussian process model $\\mathbf{w} \\sim \\text{N}(\\mathbf{t},\\boldsymbol\\Sigma_w)$, where a smooth covariance function, such as the Gaussian $\\Sigma_{w,ij} \\equiv \\sigma_w^2 \\exp(-(t_i - t_j)^2/\\phi_w^2)$, is chosen.\n\nWhen fitting the FMM (\\ref{eq:BMjointmu}), we construct the basis functions so that they depend on the warped times.  For example, using a Gaussian kernel, we now have \n\n", "index": 29, "text": "\\begin{equation}\n  h(t,\\tau) \\propto \\exp \\left(-\\frac{(w(t)-\\tau)^2}{\\phi^2} \\right) \\; . \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"h(t,\\tau)\\propto\\exp\\left(-\\frac{(w(t)-\\tau)^{2}}{\\phi^{2}}\\right)\\;.\" display=\"block\"><mrow><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msup><mi>\u03d5</mi><mn>2</mn></msup></mfrac></mrow><mo rspace=\"5.3pt\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where, $\\mathbf{K}$ is a matrix that maps the data to the underlying process at the appropriate times and the error covariance matrix could be left general or simplified as $\\boldsymbol\\Sigma_s\\equiv \\sigma^2_s\\mathbf{I}$.  In the model formulation that allows for temporal heterogeneity, recall that the matrix of basis functions depends on the range parameter $\\phi$ as well as the warped time process $\\mathbf{w}$, thus we must augment the model specification by letting $\\mathbf{w} \\sim \\text{N}(\\mathbf{t},\\boldsymbol\\Sigma_w)$.        \n\nIn Gaussian state-space models, conjugate priors for $\\sigma^2_s$ and $\\sigma^2$ are inverse gamma, where strong prior information helps to separate the measurement error variance $\\sigma^2_s$.  Alternatively, it may be of interest to use a different prior for the latent process variance (\\citealt{Gelman:06}).  A variety of options are available for the temporal range parameter $\\phi$, however a discrete uniform prior $\\phi \\sim \\text{DiscUnif}(\\boldsymbol\\Phi)$ is advantageous computationally (\\citealt{DiggleRibeiro:02}).  \n\n\\subsection{Model Implementation}\nThe full model described in the previous Section can be implemented using an MCMC algorithm to sample from all full-conditional distributions sequentially.  However, such an algorithm would be prohibitively slow for all but the smallest telemetry data sets.  Thus, we provide a model reparameterization that results in several computational improvements.  Our approach involves three critical features: 1.) Discrete uniform prior for the range parameter, 2.) integrated likelihood, and 3.) a Dirichlet process inspired mixture model to accommodate temporal heterogeneity.  We describe each of these steps in what follows.\n\nWhen specifying the prior $\\phi \\sim \\text{DiscUnif}(\\boldsymbol\\Phi)$, for each value of $\\phi$ in the support $\\boldsymbol\\Phi$, the entire matrix of basis functions $\\tilde{\\mathbf{H}}$ can be precomputed and accessed as needed in a Markov Chain Monte Carlo (MCMC) algorithm.  The discrete uniform prior permits an MCMC algorithm that requires only minimal matrix calculations (\\citealt{DiggleRibeiro:02}).   \n\nWe used Rao-Blackwellization to derive an integrated likelihood that results in a more stable MCMC algorithm and allows us to avoid sampling the latent process directly.  The integrated likelihood provides better mixing for covariance parameters and the latent process can be recovered \\emph{post hoc} using saved MCMC samples and composition sampling in a secondary algorithm.  This approach is a common computational strategy used in spatial statistics (e.g., \\citealt{Finley:15}). In our case, multivariate normal properties allow us to integrate out $\\boldsymbol\\varepsilon$ from our hierarchical model resulting in the integrated model formulation  \n\n", "itemtype": "equation", "pos": 27887, "prevtext": "\n\\noindent Notice that the kernel $h(t,\\tau)$ only depends on a single range parameter $\\phi$ that does not vary in time itself because the heterogeneity enters through the norm in the warped temporal domain $(w(t)-\\tau)^2$.  Thus, the warp field $\\mathbf{w}$ is a latent process to be estimated in the model and can provide inference associated with changes in movement dynamics.  For example, the derivative $dw(t)/dt$ of the warp field provides insight about changes in the individual's velocity over time.  When $dw(t)/dt$ is large, the warp field expands the temporal domain to allow for higher speeds and and less tortuous paths.  Expanding the temporal domain leads to behavior indicative of long-distance migration or nomadicism.  By contrast, when the $dw(t)/dt$ is small, the temporal domain compresses, causing the individual to slow and exhibit sharper turning angles.  Temporal compression leads to behavior more typical of a central place forager, or an individual performing routine activities within its home range (e.g., foraging). \n\n\n\\subsection{Full Model Specification}\nAssuming Gaussian measurement error associated with the telemetry observations and a full-rank position process $\\boldsymbol\\mu$, we can specify the full model using matrix notation in a hierarchical Bayesian framework   \n\n", "index": 31, "text": "\\begin{align}\n  \\mathbf{s} &\\sim \\text{N}(\\mathbf{K}\\boldsymbol\\mu,\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s) \\label{eq:datamod} \\\\\n  \\boldsymbol\\mu &= \\boldsymbol\\mu(0)\\otimes \\mathbf{1} + (\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})\\boldsymbol\\varepsilon \\\\ \n  \\boldsymbol\\varepsilon &\\sim \\text{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}) \\; , \\label{eq:epsmod} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{s}\" display=\"inline\"><mi>\ud835\udc2c</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sim\\text{N}(\\mathbf{K}\\boldsymbol{\\mu},\\mathbf{I}\\otimes%&#10;\\boldsymbol{\\Sigma}_{s})\" display=\"inline\"><mrow><mi/><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mi>\ud835\udf41</mi></mrow><mo>,</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\boldsymbol{\\mu}\" display=\"inline\"><mi>\ud835\udf41</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\boldsymbol{\\mu}(0)\\otimes\\mathbf{1}+(\\mathbf{I}\\otimes\\tilde{%&#10;\\mathbf{H}})\\boldsymbol{\\varepsilon}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf3a</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\boldsymbol{\\varepsilon}\" display=\"inline\"><mi>\ud835\udf3a</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sim\\text{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn/><mo>,</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent The resulting combined covariance matrix in (\\ref{eq:integratedmodel}) includes both the measurement error variance and the process covariance and has the form $\\mathbf{A}+\\mathbf{BCD}$, which can be inverted efficiently using the Sherman-Morrison-Woodbury identity (Appendix B, Supplementary Material).  In addition to the precalculation of terms involving the matrix of basis functions $\\tilde{\\mathbf{H}}$, fast matrix calculations are essential for fitting the FMM efficiently to real data sets.     \n\nThe two steps described above will facilitate the fitting of a temporally homogeneous model that is conditioned on a known warping ($\\mathbf{w}$).  However, because the optimal warping is unknown and occurs in a nonlinear covariance function for the data, we would need to sample it using Metropolis-Hastings within the broader MCMC algorithm for fitting the model.  The resulting computational requirements are prohibitively large.  Thus, we propose a Dirchlet process approach that relies on an infinite mixture model framework.  In practice, the Dirichlet process model can be implemented using a large, but finite, set of potential temporal warpings, say $\\mathbf{w}_j$ for $j=1,\\ldots,J$.  We construct the mixture model using models ${\\cal M}_j$ and latent indicator variables $z_j$ as\n\n", "itemtype": "equation", "pos": 31057, "prevtext": "\n\\noindent where, $\\mathbf{K}$ is a matrix that maps the data to the underlying process at the appropriate times and the error covariance matrix could be left general or simplified as $\\boldsymbol\\Sigma_s\\equiv \\sigma^2_s\\mathbf{I}$.  In the model formulation that allows for temporal heterogeneity, recall that the matrix of basis functions depends on the range parameter $\\phi$ as well as the warped time process $\\mathbf{w}$, thus we must augment the model specification by letting $\\mathbf{w} \\sim \\text{N}(\\mathbf{t},\\boldsymbol\\Sigma_w)$.        \n\nIn Gaussian state-space models, conjugate priors for $\\sigma^2_s$ and $\\sigma^2$ are inverse gamma, where strong prior information helps to separate the measurement error variance $\\sigma^2_s$.  Alternatively, it may be of interest to use a different prior for the latent process variance (\\citealt{Gelman:06}).  A variety of options are available for the temporal range parameter $\\phi$, however a discrete uniform prior $\\phi \\sim \\text{DiscUnif}(\\boldsymbol\\Phi)$ is advantageous computationally (\\citealt{DiggleRibeiro:02}).  \n\n\\subsection{Model Implementation}\nThe full model described in the previous Section can be implemented using an MCMC algorithm to sample from all full-conditional distributions sequentially.  However, such an algorithm would be prohibitively slow for all but the smallest telemetry data sets.  Thus, we provide a model reparameterization that results in several computational improvements.  Our approach involves three critical features: 1.) Discrete uniform prior for the range parameter, 2.) integrated likelihood, and 3.) a Dirichlet process inspired mixture model to accommodate temporal heterogeneity.  We describe each of these steps in what follows.\n\nWhen specifying the prior $\\phi \\sim \\text{DiscUnif}(\\boldsymbol\\Phi)$, for each value of $\\phi$ in the support $\\boldsymbol\\Phi$, the entire matrix of basis functions $\\tilde{\\mathbf{H}}$ can be precomputed and accessed as needed in a Markov Chain Monte Carlo (MCMC) algorithm.  The discrete uniform prior permits an MCMC algorithm that requires only minimal matrix calculations (\\citealt{DiggleRibeiro:02}).   \n\nWe used Rao-Blackwellization to derive an integrated likelihood that results in a more stable MCMC algorithm and allows us to avoid sampling the latent process directly.  The integrated likelihood provides better mixing for covariance parameters and the latent process can be recovered \\emph{post hoc} using saved MCMC samples and composition sampling in a secondary algorithm.  This approach is a common computational strategy used in spatial statistics (e.g., \\citealt{Finley:15}). In our case, multivariate normal properties allow us to integrate out $\\boldsymbol\\varepsilon$ from our hierarchical model resulting in the integrated model formulation  \n\n", "index": 33, "text": "\\begin{equation}\n  \\mathbf{s} \\sim \\text{N}(\\mathbf{K}(\\boldsymbol\\mu(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 \\mathbf{K}(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'\\mathbf{K}') \\;.\n  \\label{eq:integratedmodel}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{s}\\sim\\text{N}(\\mathbf{K}(\\boldsymbol{\\mu}(0)\\otimes\\mathbf{1}),%&#10;\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}\\mathbf{K}(\\mathbf{I}%&#10;\\otimes\\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})^{\\prime}%&#10;\\mathbf{K}^{\\prime})\\;.\" display=\"block\"><mrow><mrow><mi>\ud835\udc2c</mi><mo>\u223c</mo><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc0a</mi><mo>\u2032</mo></msup></mrow></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where each correlation matrix is defined as $\\mathbf{R}\\equiv(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'$ (with implicit dependence on the appropriate range parameter and temporal warping).  The indicator variables $\\mathbf{z}\\equiv(z_1,z_2,\\ldots,z_J)'$ are multinomial ($\\mathbf{z} \\sim \\text{MN}(1,\\mathbf{p})$) with $\\mathbf{p}$ serving as prior model probabilities.  Using the mixture model specification in (\\ref{eq:mixturemodel}), and conditioning on a large set of potential warpings, the resulting MCMC algorithm resembles a reversible-jump MCMC (RJMCMC; \\citealt{Green:95}) where the indicator variables $z_j$ are conjugate.  The marginal posterior mean $E(z_j | \\mathbf{s})$ corresponds to the posterior model probability for model ${\\cal M}_j$.    \n\nAn alternative computational strategy involves the two-stage RJMCMC approach of \\cite{BarkerLink:13} to find the posterior model probability $P({\\cal M}_j | \\mathbf{s})$ and model averaged posterior inference (Appendix C, Supplementary Material).  The advantage of the RJMCMC approach of \\cite{BarkerLink:13} is that each model can be fit independently (and in parallel), and a secondary MCMC algorithm can be used to sample from the averaged posterior distributions.   Parallel model fits provide appreciable gains in computational efficiency.  Finally, after fitting each of the $J$ models, and applying Bayesian model averaging via RJMCMC, we can use a tertiary algorithm to obtain realizations of the latent position process $\\boldsymbol\\mu(t)$ for any time of interest $t$.  In the case of the models we propose, where the number of parameters are balanced, the secondary and tertiary algorithms could also be parallelized, but are usually fast enough to be implemented sequentially.   \n\nIn simulation, our method for fitting the FMM to data was approximately an order of magnitude faster than conventional Bayesian computational methods.  We demonstrate that our Bayesian FMM recovers model parameters for simulated data sets with varying amounts of data missingness and temporal heterogeneity in Appendix D (Supplementary Material).  In what follows, we apply the temporally heterogeneous FMM to satellite telemetry data corresponding to the movement of large mammals in western North America and describe the implications for our proposed methods.  \n\n\\section{Data Analysis: Animal Movement}\nWe apply our temporally heterogeneous FMM to satellite telemetry data sets consisting of recorded mule deer (\\emph{Odocoileus hemionus}) and mountain lion (\\emph{Puma concolor}) positions in Southeastern Utah and Northern Colorado, USA (originally analyzed for other purposes by \\cite{Hooten:10a} and \\cite{Hanks:15}).  Mule deer and mountain lion are important species in the western USA that often exhibit seasonally varying movement behavior.  Many mule deer individuals split their time between winter and summer ranges and migrate from one to the other in the spring and autumn.  Mountain lion movement behavior also varies temporally, with individuals roaming regionally and often returning to locations where prey has been captured.  In both mule deer and mountain lion individuals, there is a clear heterogeneity in movement dynamics.  Our FMM approach can accommodate such movement heterogeneity by treating it as temporal nonstationarity.  \n\nFor the mule deer example, we focus on a 5 day period during autumn that spans the migratory behavior of an individual.  For the mountain lion, we focus on a 33 day period in the summer.  These data (Figure~\\ref{fig:MD_ML_path}) were obtained using a global positioning system (GPS) telemetry device affixed to the individual and are comprised of 226 measured geographic positions spaced approximately 30 minutes apart for the mule deer individual and 221 observations spaced approximately 3 hours apart for the mountain lion individual.  For analysis and plotting purposes, we have spatially and temporally standardized the data (i.e., subtracted the sample mean and divided by the pooled sample standard deviation). \n\nTo implement an FMM for the mule deer and mountain lion data using the specification presented in the previous Section (\\ref{eq:integratedmodel}), we relied on the truncated Gaussian kernel for $h(t,\\tau)$.  Following \\cite{DiggleRibeiro:02}, we reparameterized the covariance matrix in the integrated model (\\ref{eq:integratedmodel}) such that  \n\n", "itemtype": "equation", "pos": 32653, "prevtext": "\n\\noindent The resulting combined covariance matrix in (\\ref{eq:integratedmodel}) includes both the measurement error variance and the process covariance and has the form $\\mathbf{A}+\\mathbf{BCD}$, which can be inverted efficiently using the Sherman-Morrison-Woodbury identity (Appendix B, Supplementary Material).  In addition to the precalculation of terms involving the matrix of basis functions $\\tilde{\\mathbf{H}}$, fast matrix calculations are essential for fitting the FMM efficiently to real data sets.     \n\nThe two steps described above will facilitate the fitting of a temporally homogeneous model that is conditioned on a known warping ($\\mathbf{w}$).  However, because the optimal warping is unknown and occurs in a nonlinear covariance function for the data, we would need to sample it using Metropolis-Hastings within the broader MCMC algorithm for fitting the model.  The resulting computational requirements are prohibitively large.  Thus, we propose a Dirchlet process approach that relies on an infinite mixture model framework.  In practice, the Dirichlet process model can be implemented using a large, but finite, set of potential temporal warpings, say $\\mathbf{w}_j$ for $j=1,\\ldots,J$.  We construct the mixture model using models ${\\cal M}_j$ and latent indicator variables $z_j$ as\n\n", "index": 35, "text": "\\begin{equation}\n  \\mathbf{s} \\sim \n  \\begin{cases}\n    \\text{N}(\\mathbf{K}(\\boldsymbol\\mu(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 \\mathbf{K}\\mathbf{R}(\\phi,\\mathbf{w}_1)\\mathbf{K}') &\\mbox{ ,  }  z_1 = 1  \\\\\n    \\text{N}(\\mathbf{K}(\\boldsymbol\\mu(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 \\mathbf{K}\\mathbf{R}(\\phi,\\mathbf{w}_2)\\mathbf{K}') &\\mbox{ ,  }  z_2 = 1 \\\\ \n   \\vdots  \\\\\n    \\text{N}(\\mathbf{K}(\\boldsymbol\\mu(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 \\mathbf{K}\\mathbf{R}(\\phi,\\mathbf{w}_J)\\mathbf{K}') &\\mbox{ ,  }  z_J = 1 \\\\ \n  \\end{cases}\n  \\label{eq:mixturemodel}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{s}\\sim\\begin{cases}\\text{N}(\\mathbf{K}(\\boldsymbol{\\mu}(0)\\otimes%&#10;\\mathbf{1}),\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}\\mathbf{K}%&#10;\\mathbf{R}(\\phi,\\mathbf{w}_{1})\\mathbf{K}^{\\prime})&amp;\\mbox{ , }z_{1}=1\\\\&#10;\\text{N}(\\mathbf{K}(\\boldsymbol{\\mu}(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes%&#10;\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}\\mathbf{K}\\mathbf{R}(\\phi,\\mathbf{w}_{2})%&#10;\\mathbf{K}^{\\prime})&amp;\\mbox{ , }z_{2}=1\\\\&#10;\\vdots\\\\&#10;\\text{N}(\\mathbf{K}(\\boldsymbol{\\mu}(0)\\otimes\\mathbf{1}),\\mathbf{I}\\otimes%&#10;\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}\\mathbf{K}\\mathbf{R}(\\phi,\\mathbf{w}_{J})%&#10;\\mathbf{K}^{\\prime})&amp;\\mbox{ , }z_{J}=1\\\\&#10;\\end{cases}\" display=\"block\"><mrow><mi>\ud835\udc2c</mi><mo>\u223c</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc0a\ud835\udc11</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03d5</mi><mo>,</mo><msub><mi>\ud835\udc30</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc0a</mi><mo>\u2032</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0,\u00a0</mtext><mo>\u2062</mo><msub><mi>z</mi><mn>1</mn></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc0a\ud835\udc11</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03d5</mi><mo>,</mo><msub><mi>\ud835\udc30</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc0a</mi><mo>\u2032</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0,\u00a0</mtext><mo>\u2062</mo><msub><mi>z</mi><mn>2</mn></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udf41</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc0a\ud835\udc11</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03d5</mi><mo>,</mo><msub><mi>\ud835\udc30</mi><mi>J</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc0a</mi><mo>\u2032</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0,\u00a0</mtext><mo>\u2062</mo><msub><mi>z</mi><mi>J</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where, $\\sigma^2_{\\mu/s}=\\sigma^2/\\sigma^2_s$ and $\\tilde{\\mathbf{H}}(\\phi)$ is a function of the temporal range parameter $\\phi$.  For the standardized data, we used a discrete uniform prior for $\\phi$ on 100 evenly spaced values from $0.001$ to $0.1$, a uniform prior for $\\sigma_{\\mu/s}$ on support $(0,20)$, and an informative inverse gamma prior for $\\sigma^2_s \\sim \\text{IG}(12,0.01)$ representing strong prior information that GPS telemetry error is often less than 150 m (recall that the data have been scaled).   \n\nIn implementing the FMM, we sampled 1000 warp fields using a Latin hypercube design from a Gaussian process with Gaussian covariance constrained to avoid temporal folding (i.e., variance parameter equal to 0.1 and range parameter equal to 0.2).  Trace plots for model parameters indicated excellent MCMC mixing and convergence, based on 10000 MCMC samples for each model fit.  Each model fit required only 3 minutes on a 16-core workstation with 3 Ghz processors and 64 GB of memory.  The total time required to fit the model for each of the warps in parallel was approximately 3 hours, which is substantially less than the 50 hours it would require to compute in sequence.  The secondary and tertiary algorithms to perform the BMA and posterior predictive sampling required only 4 additional minutes.          \n\nThe resulting inference is summarized in Figure~\\ref{fig:MD_ML_path}.  The top row of panels in Figure~\\ref{fig:MD_ML_path} presents the data and estimated latent process for the mule deer and mountain lion individuals in two dimensional geographic space. The gray lines are posterior predictive realizations of the latent process $\\boldsymbol\\mu(t)$.  The middle two rows of panels in Figure~\\ref{fig:MD_ML_path} display the marginal latent process in longitude and latitude.  The bottom row of panels in Figure~\\ref{fig:MD_ML_path} shows the inferred derivative of the temporal warp at the posterior mean of the position process.      \n\\begin{figure}[htp]\n  \\centering\n  \\vspace{-.9in}\n  \\includegraphics[height=6in, angle=0]{path_plot_vert_MD_ML.jpg}\n  \\caption{Posterior predicted mule deer (left column) and mountain lion (right column) paths $\\boldsymbol\\mu(t)$ using BMA.  Top panels:  Paths in two-dimensional geographic space.  Middle panels: Marginal paths for longitude and latitude. Position uncertainty is represented as posterior realizations in the top panel and 95\\% marginal credible intervals in the middle two panels. Bottom panel:  Warp derivative obtained via BMA.}\n  \\label{fig:MD_ML_path}\n\\end{figure}\n\nThe mule deer individual migrated during the time interval 0.3--0.6 (Figure~\\ref{fig:MD_ML_path}).  The migratory movement behavior caused the BMA warp derivative to be positive, indicating that a temporal expansion is needed to accommodate the trajectory during that time period.  In contrast, the warp derivative was negative during the time interval 0.6--0.75 suggesting a temporal contraction was necessary to capture the different movement behavior for the individual directly after migration.  \n\nThe warp derivative for the mountain lion individual (Figure~\\ref{fig:MD_ML_path}) suggests a temporal contraction at the beginning and end of the temporal domain (i.e., $t<0.2$ and $t>0.8$) with a temporal expansion between (i.e., $0.2 < t < 0.8$).   The FMM inference confirms that the mountain lion individual occupies a location with prey before traversing a large loop, only to return to the same location a few weeks later.  \n\n\\section{Discussion}\nThe rapidly expanding literature on statistical models for animal movement has provided a wealth of useful tools for analyzing telemetry data.  In particular, individual-based models for trajectories have become a popular means for obtaining inference concerning the patterns and processes involved in animal movement.  Despite the development of approaches for modeling animal trajectories in continuous-time, discrete-time movement models have dominated the recent literature.  Heuristically, discrete-time animal movement models (e.g., \\citealt{Morales:04}; \\citealt{Jonsen:05}; \\citealt{McClintock:12}) are straightforward and easy to understand.  They also provide a direct means to incorporate time-varying heterogeneity.  In particular, temporal clustering of animal movement trajectories has proven useful for inferring animal behavior and for better understanding the spatial patterning of behavior.  However, movement processes are necessarily continuous in time, but traditional continuous-time trajectory models have been oversimplified or computationally challenging to implement for large datasets (\\citealt{McClintock:14}).    \n\nWe presented a continuous-time stochastic trajectory model that retains the natural intuition of discrete-time models, is feasible to implement for large data sets, accommodates time-varying heterogeneity in movement, and is general enough to allow for cognitive realism (e.g., memory and perception).  We showed that existing stochastic process models for movement can be generalized in such a way that facilitates a basis function representation of the dynamical process.  Basis function representations are commonly used in spatial and spatio-temporal statistics, semi-parametric modeling, and functional data analysis.  We show how the same machinery used in spatial statistics to model complicated dependence structures in data can also be used in animal movement modeling.  For example, temporal heterogeneity can be expressed as non-stationarity in a second-order Gaussian process model for individual trajectories using existing methods.  \n\nIn previous implementations of both continuous- and discrete-time animal movement models, computation has been a bottleneck for inference based on statistical modeling.  Computationally feasible model structures were often over-simplified and lacked appropriate realism.  More complicated models have historically required custom software written in compiled languages that do not facilitate user modifications.  We combined multiple computational approaches to develop a strategy for fitting hierarchical stochastic process models to telemetry data.  Our method exploits the substantially improved storage and multicore processing capabilities of modern computers so that the models we presented herein can be fit on a standard laptop computer. \n\nWe showed that our modeling framework appropriately accommodates temporally heterogeneous structure in realistic animal trajectories as well as providing a natural way to handle irregular fix rates common in telemetry data.  Most discrete-time trajectory models for telemetry data rely on either imputation or a linear interpolation to accommodate irregular acquisition times.  For mule deer and mountain lion individuals, we showed that our model is able to accommodate mechanistic forms of temporal heterogeneity due to natural seasonality and environmental cues, providing inference concerning where and when changes in movement behavior occur.  \n\nThe basis function approach we present is both familiar and further generalizable.  For example, various model selection techniques could be applied to assess whether certain basis function specifications based on memory or perception yield models with better predictive ability (\\citealt{HootenHobbs:15}).  Furthermore, because the computational approach we presented is feasible for large data sets, the individual-level models could be nested within a population-level framework to provide inference for groups of individuals simultaneously.  Both of these extensions are the subject of ongoing research. \n\nWhile our analysis utilized highly accurate GPS telemetry data, more complicated measurement processes could be accounted for by modifying the data component of the model to accommodate non-elliptically distributed errors for example (e.g., \\citealt{Buderman:16}; \\citealt{McClintock:15}).  Additionally, our study system was somewhat devoid of hard boundaries to movement, but future research to formally incorporate physical constraints to the true position process would be useful (e.g., \\citealt{Tracey:05}; \\citealt{Brost:15}).  Statistically rigorous methods for accommodating constraints in multivariate continuous-time dynamic processes have been proposed in other fields (e.g., \\citealt{CangelosiHooten:09}), but are still nascent in statistical approaches for telemetry data.   \n\n\\section*{Acknowledgements}\nThe authors thank Leslie McFarlane, Frances Buderman, Henry Scharf, Jay Ver Hoef, and Bill Link for help for insightful discussions and feedback on the data and research.  Funding for this research was provided by NOAA (RWO 103) and CPW (TO 1304).  Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. government.\n\n\\baselineskip=12pt\n\\bibliographystyle{./fullnat.bst}\n\\bibliography{animalmovement.bib}\n\n\\section*{Appendix A}\nUsing the previously specified definitions for variables and Ito calculus, we see that the process can be rewritten as:  \n\n", "itemtype": "equation", "pos": 37760, "prevtext": "\n\\noindent where each correlation matrix is defined as $\\mathbf{R}\\equiv(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'$ (with implicit dependence on the appropriate range parameter and temporal warping).  The indicator variables $\\mathbf{z}\\equiv(z_1,z_2,\\ldots,z_J)'$ are multinomial ($\\mathbf{z} \\sim \\text{MN}(1,\\mathbf{p})$) with $\\mathbf{p}$ serving as prior model probabilities.  Using the mixture model specification in (\\ref{eq:mixturemodel}), and conditioning on a large set of potential warpings, the resulting MCMC algorithm resembles a reversible-jump MCMC (RJMCMC; \\citealt{Green:95}) where the indicator variables $z_j$ are conjugate.  The marginal posterior mean $E(z_j | \\mathbf{s})$ corresponds to the posterior model probability for model ${\\cal M}_j$.    \n\nAn alternative computational strategy involves the two-stage RJMCMC approach of \\cite{BarkerLink:13} to find the posterior model probability $P({\\cal M}_j | \\mathbf{s})$ and model averaged posterior inference (Appendix C, Supplementary Material).  The advantage of the RJMCMC approach of \\cite{BarkerLink:13} is that each model can be fit independently (and in parallel), and a secondary MCMC algorithm can be used to sample from the averaged posterior distributions.   Parallel model fits provide appreciable gains in computational efficiency.  Finally, after fitting each of the $J$ models, and applying Bayesian model averaging via RJMCMC, we can use a tertiary algorithm to obtain realizations of the latent position process $\\boldsymbol\\mu(t)$ for any time of interest $t$.  In the case of the models we propose, where the number of parameters are balanced, the secondary and tertiary algorithms could also be parallelized, but are usually fast enough to be implemented sequentially.   \n\nIn simulation, our method for fitting the FMM to data was approximately an order of magnitude faster than conventional Bayesian computational methods.  We demonstrate that our Bayesian FMM recovers model parameters for simulated data sets with varying amounts of data missingness and temporal heterogeneity in Appendix D (Supplementary Material).  In what follows, we apply the temporally heterogeneous FMM to satellite telemetry data corresponding to the movement of large mammals in western North America and describe the implications for our proposed methods.  \n\n\\section{Data Analysis: Animal Movement}\nWe apply our temporally heterogeneous FMM to satellite telemetry data sets consisting of recorded mule deer (\\emph{Odocoileus hemionus}) and mountain lion (\\emph{Puma concolor}) positions in Southeastern Utah and Northern Colorado, USA (originally analyzed for other purposes by \\cite{Hooten:10a} and \\cite{Hanks:15}).  Mule deer and mountain lion are important species in the western USA that often exhibit seasonally varying movement behavior.  Many mule deer individuals split their time between winter and summer ranges and migrate from one to the other in the spring and autumn.  Mountain lion movement behavior also varies temporally, with individuals roaming regionally and often returning to locations where prey has been captured.  In both mule deer and mountain lion individuals, there is a clear heterogeneity in movement dynamics.  Our FMM approach can accommodate such movement heterogeneity by treating it as temporal nonstationarity.  \n\nFor the mule deer example, we focus on a 5 day period during autumn that spans the migratory behavior of an individual.  For the mountain lion, we focus on a 33 day period in the summer.  These data (Figure~\\ref{fig:MD_ML_path}) were obtained using a global positioning system (GPS) telemetry device affixed to the individual and are comprised of 226 measured geographic positions spaced approximately 30 minutes apart for the mule deer individual and 221 observations spaced approximately 3 hours apart for the mountain lion individual.  For analysis and plotting purposes, we have spatially and temporally standardized the data (i.e., subtracted the sample mean and divided by the pooled sample standard deviation). \n\nTo implement an FMM for the mule deer and mountain lion data using the specification presented in the previous Section (\\ref{eq:integratedmodel}), we relied on the truncated Gaussian kernel for $h(t,\\tau)$.  Following \\cite{DiggleRibeiro:02}, we reparameterized the covariance matrix in the integrated model (\\ref{eq:integratedmodel}) such that  \n\n", "index": 37, "text": "\\begin{equation}\n  \\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 (\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})' = \\sigma^2_s (\\mathbf{I} + \\sigma^2_{\\mu/s}(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'),  \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}(\\mathbf{I}\\otimes\\tilde{%&#10;\\mathbf{H}})(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})^{\\prime}=\\sigma^{2}_{s}(%&#10;\\mathbf{I}+\\sigma^{2}_{\\mu/s}(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})(\\mathbf{I}%&#10;\\otimes\\tilde{\\mathbf{H}})^{\\prime}),\" display=\"block\"><mrow><mrow><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\u03c3</mi><mi>s</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>+</mo><mrow><msubsup><mi>\u03c3</mi><mrow><mi>\u03bc</mi><mo>/</mo><mi>s</mi></mrow><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n\\noindent where a step-by-step description for the above is as follows:\n\\begin{enumerate}   \n  \\item (\\ref{eq:intstep1}): Begin with the convolution model. \n  \\item (\\ref{eq:intstep2}): Write the Brownian term $\\mathbf{b}(\\tau)$ in its integral form.  \n  \\item (\\ref{eq:intstep3}): Move the function $\\mathbf{H}(t,\\tau)$ inside both integrals.  Note that: $t_0<\\tilde{\\tau}<\\tau$ and $t_0<\\tau<t_n$.  \n  \\item (\\ref{eq:intstep4}): Switch the order of integration, paying careful attention to the limits of integration.  That is, $\\tilde{\\tau}<\\tau < t_n$ and $t_0 <\\tilde{\\tau}<t_n$.  \n  \\item (\\ref{eq:intstep5}): Define $\\tilde{\\mathbf{H}}(t,\\tilde{\\tau})\\equiv\\int_{\\tilde{\\tau}}^{t_n} \\mathbf{H}(t,\\tau) d\\tau$.    \n  \\item (\\ref{eq:intstep6}): The expression can now be written as a convolution of $\\sigma\\tilde{\\mathbf{H}}(t,\\tau)$ and white noise $\\boldsymbol\\varepsilon(\\tau)$.    \n\\end{enumerate}   \n\n\\section*{Appendix B}\nThe inverse of the covariance in the integrated FMM can be obtained by\n\n", "itemtype": "equation", "pos": 47107, "prevtext": "\n\\noindent where, $\\sigma^2_{\\mu/s}=\\sigma^2/\\sigma^2_s$ and $\\tilde{\\mathbf{H}}(\\phi)$ is a function of the temporal range parameter $\\phi$.  For the standardized data, we used a discrete uniform prior for $\\phi$ on 100 evenly spaced values from $0.001$ to $0.1$, a uniform prior for $\\sigma_{\\mu/s}$ on support $(0,20)$, and an informative inverse gamma prior for $\\sigma^2_s \\sim \\text{IG}(12,0.01)$ representing strong prior information that GPS telemetry error is often less than 150 m (recall that the data have been scaled).   \n\nIn implementing the FMM, we sampled 1000 warp fields using a Latin hypercube design from a Gaussian process with Gaussian covariance constrained to avoid temporal folding (i.e., variance parameter equal to 0.1 and range parameter equal to 0.2).  Trace plots for model parameters indicated excellent MCMC mixing and convergence, based on 10000 MCMC samples for each model fit.  Each model fit required only 3 minutes on a 16-core workstation with 3 Ghz processors and 64 GB of memory.  The total time required to fit the model for each of the warps in parallel was approximately 3 hours, which is substantially less than the 50 hours it would require to compute in sequence.  The secondary and tertiary algorithms to perform the BMA and posterior predictive sampling required only 4 additional minutes.          \n\nThe resulting inference is summarized in Figure~\\ref{fig:MD_ML_path}.  The top row of panels in Figure~\\ref{fig:MD_ML_path} presents the data and estimated latent process for the mule deer and mountain lion individuals in two dimensional geographic space. The gray lines are posterior predictive realizations of the latent process $\\boldsymbol\\mu(t)$.  The middle two rows of panels in Figure~\\ref{fig:MD_ML_path} display the marginal latent process in longitude and latitude.  The bottom row of panels in Figure~\\ref{fig:MD_ML_path} shows the inferred derivative of the temporal warp at the posterior mean of the position process.      \n\\begin{figure}[htp]\n  \\centering\n  \\vspace{-.9in}\n  \\includegraphics[height=6in, angle=0]{path_plot_vert_MD_ML.jpg}\n  \\caption{Posterior predicted mule deer (left column) and mountain lion (right column) paths $\\boldsymbol\\mu(t)$ using BMA.  Top panels:  Paths in two-dimensional geographic space.  Middle panels: Marginal paths for longitude and latitude. Position uncertainty is represented as posterior realizations in the top panel and 95\\% marginal credible intervals in the middle two panels. Bottom panel:  Warp derivative obtained via BMA.}\n  \\label{fig:MD_ML_path}\n\\end{figure}\n\nThe mule deer individual migrated during the time interval 0.3--0.6 (Figure~\\ref{fig:MD_ML_path}).  The migratory movement behavior caused the BMA warp derivative to be positive, indicating that a temporal expansion is needed to accommodate the trajectory during that time period.  In contrast, the warp derivative was negative during the time interval 0.6--0.75 suggesting a temporal contraction was necessary to capture the different movement behavior for the individual directly after migration.  \n\nThe warp derivative for the mountain lion individual (Figure~\\ref{fig:MD_ML_path}) suggests a temporal contraction at the beginning and end of the temporal domain (i.e., $t<0.2$ and $t>0.8$) with a temporal expansion between (i.e., $0.2 < t < 0.8$).   The FMM inference confirms that the mountain lion individual occupies a location with prey before traversing a large loop, only to return to the same location a few weeks later.  \n\n\\section{Discussion}\nThe rapidly expanding literature on statistical models for animal movement has provided a wealth of useful tools for analyzing telemetry data.  In particular, individual-based models for trajectories have become a popular means for obtaining inference concerning the patterns and processes involved in animal movement.  Despite the development of approaches for modeling animal trajectories in continuous-time, discrete-time movement models have dominated the recent literature.  Heuristically, discrete-time animal movement models (e.g., \\citealt{Morales:04}; \\citealt{Jonsen:05}; \\citealt{McClintock:12}) are straightforward and easy to understand.  They also provide a direct means to incorporate time-varying heterogeneity.  In particular, temporal clustering of animal movement trajectories has proven useful for inferring animal behavior and for better understanding the spatial patterning of behavior.  However, movement processes are necessarily continuous in time, but traditional continuous-time trajectory models have been oversimplified or computationally challenging to implement for large datasets (\\citealt{McClintock:14}).    \n\nWe presented a continuous-time stochastic trajectory model that retains the natural intuition of discrete-time models, is feasible to implement for large data sets, accommodates time-varying heterogeneity in movement, and is general enough to allow for cognitive realism (e.g., memory and perception).  We showed that existing stochastic process models for movement can be generalized in such a way that facilitates a basis function representation of the dynamical process.  Basis function representations are commonly used in spatial and spatio-temporal statistics, semi-parametric modeling, and functional data analysis.  We show how the same machinery used in spatial statistics to model complicated dependence structures in data can also be used in animal movement modeling.  For example, temporal heterogeneity can be expressed as non-stationarity in a second-order Gaussian process model for individual trajectories using existing methods.  \n\nIn previous implementations of both continuous- and discrete-time animal movement models, computation has been a bottleneck for inference based on statistical modeling.  Computationally feasible model structures were often over-simplified and lacked appropriate realism.  More complicated models have historically required custom software written in compiled languages that do not facilitate user modifications.  We combined multiple computational approaches to develop a strategy for fitting hierarchical stochastic process models to telemetry data.  Our method exploits the substantially improved storage and multicore processing capabilities of modern computers so that the models we presented herein can be fit on a standard laptop computer. \n\nWe showed that our modeling framework appropriately accommodates temporally heterogeneous structure in realistic animal trajectories as well as providing a natural way to handle irregular fix rates common in telemetry data.  Most discrete-time trajectory models for telemetry data rely on either imputation or a linear interpolation to accommodate irregular acquisition times.  For mule deer and mountain lion individuals, we showed that our model is able to accommodate mechanistic forms of temporal heterogeneity due to natural seasonality and environmental cues, providing inference concerning where and when changes in movement behavior occur.  \n\nThe basis function approach we present is both familiar and further generalizable.  For example, various model selection techniques could be applied to assess whether certain basis function specifications based on memory or perception yield models with better predictive ability (\\citealt{HootenHobbs:15}).  Furthermore, because the computational approach we presented is feasible for large data sets, the individual-level models could be nested within a population-level framework to provide inference for groups of individuals simultaneously.  Both of these extensions are the subject of ongoing research. \n\nWhile our analysis utilized highly accurate GPS telemetry data, more complicated measurement processes could be accounted for by modifying the data component of the model to accommodate non-elliptically distributed errors for example (e.g., \\citealt{Buderman:16}; \\citealt{McClintock:15}).  Additionally, our study system was somewhat devoid of hard boundaries to movement, but future research to formally incorporate physical constraints to the true position process would be useful (e.g., \\citealt{Tracey:05}; \\citealt{Brost:15}).  Statistically rigorous methods for accommodating constraints in multivariate continuous-time dynamic processes have been proposed in other fields (e.g., \\citealt{CangelosiHooten:09}), but are still nascent in statistical approaches for telemetry data.   \n\n\\section*{Acknowledgements}\nThe authors thank Leslie McFarlane, Frances Buderman, Henry Scharf, Jay Ver Hoef, and Bill Link for help for insightful discussions and feedback on the data and research.  Funding for this research was provided by NOAA (RWO 103) and CPW (TO 1304).  Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. government.\n\n\\baselineskip=12pt\n\\bibliographystyle{./fullnat.bst}\n\\bibliography{animalmovement.bib}\n\n\\section*{Appendix A}\nUsing the previously specified definitions for variables and Ito calculus, we see that the process can be rewritten as:  \n\n", "index": 39, "text": "\\begin{align}\n  \\boldsymbol\\eta(t) &= \\int_{t_0}^{t_n} \\sigma\\mathbf{H}(t,\\tau) \\mathbf{b}(\\tau) d\\tau \\label{eq:intstep1}\\\\\n  &= \\int_{t_0}^{t_n} \\sigma \\mathbf{H}(t,\\tau) \\int_{t_0}^\\tau d\\mathbf{b}(\\tilde{\\tau}) d\\tau \\label{eq:intstep2}\\\\\\\n  &= \\int_{t_0}^{t_n} \\int_{t_0}^\\tau \\sigma \\mathbf{H}(t,\\tau) d\\mathbf{b}(\\tilde{\\tau}) d\\tau \\label{eq:intstep3}\\\\\n  &= \\int_{t_0}^{t_n} \\int_{\\tilde{\\tau}}^{t_n} \\sigma \\mathbf{H}(t,\\tau) d\\tau d\\mathbf{b}(\\tilde{\\tau}) \\label{eq:intstep4}\\\\\n  &= \\int_{t_0}^{t_n} \\sigma \\tilde{\\mathbf{H}}(t,\\tilde{\\tau}) d\\mathbf{b}(\\tilde{\\tau}) \\label{eq:intstep5}\\\\\n  &= \\int_{t_0}^{t_n} \\sigma \\tilde{\\mathbf{H}}(t,\\tau)  \\boldsymbol\\varepsilon(\\tau) d\\tau \\; , \\label{eq:intstep6} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\boldsymbol{\\eta}(t)\" display=\"inline\"><mrow><mi>\ud835\udf3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\sigma\\mathbf{H}(t,\\tau)\\mathbf{b}(\\tau)d\\tau\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc07</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03c4</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\sigma\\mathbf{H}(t,\\tau)\\int_{t_{0}}^{\\tau}d%&#10;\\mathbf{b}(\\tilde{\\tau})d\\tau\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc07</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><mi>\u03c4</mi></msubsup></mstyle><mrow><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc1b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03c4</mi></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\int_{t_{0}}^{\\tau}\\sigma\\mathbf{H}(t,\\tau)d%&#10;\\mathbf{b}(\\tilde{\\tau})d\\tau\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><mi>\u03c4</mi></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc07</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc1b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03c4</mi></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\int_{\\tilde{\\tau}}^{t_{n}}\\sigma\\mathbf{H}(%&#10;t,\\tau)d\\tau d\\mathbf{b}(\\tilde{\\tau})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>\ud835\udc07</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03c4</mi></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc1b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\sigma\\tilde{\\mathbf{H}}(t,\\tilde{\\tau})d%&#10;\\mathbf{b}(\\tilde{\\tau})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc1b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{t_{0}}^{t_{n}}\\sigma\\tilde{\\mathbf{H}}(t,\\tau)\\boldsymbol{%&#10;\\varepsilon}(\\tau)d\\tau\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mi>n</mi></msub></msubsup></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf3a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+2.8pt\"><mi>\u03c4</mi></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\nusing the Sherman-Morrison-Woodbury identity.  \n\n\\section*{Appendix C}\nTo implement the RJMCMC approach of \\cite{BarkerLink:13}, we use the following algorithm:\n\\begin{enumerate} \n  \\item Set initial MCMC iteration index to $k=1$.\n  \\item Choose initial model ${\\cal M}_j$. \n  \\item Select $\\boldsymbol\\theta \\equiv (\\phi, \\sigma_s^2, \\sigma^2)'$ from the $k^{\\text{th}}$ iteration of the MCMC output from fitting model ${\\cal M}_j$. \n  \\item Compute the full-conditional model probability \\\\\n  \n", "itemtype": "equation", "pos": 48842, "prevtext": "\n\\noindent where a step-by-step description for the above is as follows:\n\\begin{enumerate}   \n  \\item (\\ref{eq:intstep1}): Begin with the convolution model. \n  \\item (\\ref{eq:intstep2}): Write the Brownian term $\\mathbf{b}(\\tau)$ in its integral form.  \n  \\item (\\ref{eq:intstep3}): Move the function $\\mathbf{H}(t,\\tau)$ inside both integrals.  Note that: $t_0<\\tilde{\\tau}<\\tau$ and $t_0<\\tau<t_n$.  \n  \\item (\\ref{eq:intstep4}): Switch the order of integration, paying careful attention to the limits of integration.  That is, $\\tilde{\\tau}<\\tau < t_n$ and $t_0 <\\tilde{\\tau}<t_n$.  \n  \\item (\\ref{eq:intstep5}): Define $\\tilde{\\mathbf{H}}(t,\\tilde{\\tau})\\equiv\\int_{\\tilde{\\tau}}^{t_n} \\mathbf{H}(t,\\tau) d\\tau$.    \n  \\item (\\ref{eq:intstep6}): The expression can now be written as a convolution of $\\sigma\\tilde{\\mathbf{H}}(t,\\tau)$ and white noise $\\boldsymbol\\varepsilon(\\tau)$.    \n\\end{enumerate}   \n\n\\section*{Appendix B}\nThe inverse of the covariance in the integrated FMM can be obtained by\n\n", "index": 41, "text": "\\begin{align}\n  (\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 (\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})')^{-1}&= \n  (\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s)^{-1} - \\sigma^2(\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s)^{-1}(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}}) \\notag \\\\\n  &\\times (\\mathbf{I}+(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'(\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s)^{-1}(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}}))^{-1} \\notag \\\\\n  &\\times (\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'(\\mathbf{I}\\otimes \\boldsymbol\\Sigma_s)^{-1} \\notag \\; ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s}+\\sigma^{2}(\\mathbf{I}%&#10;\\otimes\\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})^{\\prime})^{-1}\" display=\"inline\"><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s})^{-1}-\\sigma^{2}(%&#10;\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s})^{-1}(\\mathbf{I}\\otimes\\tilde{\\mathbf%&#10;{H}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>-</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\times(\\mathbf{I}+(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})^{\\prime}(%&#10;\\mathbf{I}\\otimes\\boldsymbol{\\Sigma}_{s})^{-1}(\\mathbf{I}\\otimes\\tilde{\\mathbf%&#10;{H}}))^{-1}\" display=\"inline\"><mrow><mi/><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\times(\\mathbf{I}\\otimes\\tilde{\\mathbf{H}})^{\\prime}(\\mathbf{I}%&#10;\\otimes\\boldsymbol{\\Sigma}_{s})^{-1}\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>\u00d7</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><mover accent=\"true\"><mi>\ud835\udc07</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><mpadded width=\"+2.8pt\"><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>\u2297</mo><msub><mi>\ud835\udeba</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mpadded></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n  for each model $j=1,\\ldots,J$.\n  \\item Sample ${\\cal M}_j$ from a categorical distribution with probabilities $P({\\cal M}_1 | \\cdot)$, $P({\\cal M}_2 | \\cdot)$, $\\ldots$ , $P({\\cal M}_J | \\cdot)$. \n  \\item If inference for the position process is desired, sample $\\boldsymbol\\mu$ from its full-conditional distribution $[\\boldsymbol\\mu | \\cdot]$, where the full-conditional depends on the current values for the parameters $\\boldsymbol\\theta$ and the data $\\mathbf{s}$.  The full-conditional for $\\boldsymbol\\mu$ is multivariate Gaussian:\n  \n", "itemtype": "equation", "pos": 49918, "prevtext": "\nusing the Sherman-Morrison-Woodbury identity.  \n\n\\section*{Appendix C}\nTo implement the RJMCMC approach of \\cite{BarkerLink:13}, we use the following algorithm:\n\\begin{enumerate} \n  \\item Set initial MCMC iteration index to $k=1$.\n  \\item Choose initial model ${\\cal M}_j$. \n  \\item Select $\\boldsymbol\\theta \\equiv (\\phi, \\sigma_s^2, \\sigma^2)'$ from the $k^{\\text{th}}$ iteration of the MCMC output from fitting model ${\\cal M}_j$. \n  \\item Compute the full-conditional model probability \\\\\n  \n", "index": 43, "text": "\\begin{equation}\n    P({\\cal M}_j | \\cdot) = \\frac{[\\mathbf{s}|\\boldsymbol\\theta]_j  p_j}{\\sum_{l=1}^J [\\mathbf{s}|\\boldsymbol\\theta]_l p_l} \\;,  \n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"P({\\cal M}_{j}|\\cdot)=\\frac{[\\mathbf{s}|\\boldsymbol{\\theta}]_{j}p_{j}}{\\sum_{l%&#10;=1}^{J}[\\mathbf{s}|\\boldsymbol{\\theta}]_{l}p_{l}}\\;,\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mpadded width=\"+2.8pt\"><mfrac><mrow><msub><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc2c</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">]</mo></mrow><mi>j</mi></msub><msub><mi>p</mi><mi>j</mi></msub></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></msubsup><msub><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc2c</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">]</mo></mrow><mi>l</mi></msub><msub><mi>p</mi><mi>l</mi></msub></mrow></mfrac></mpadded><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05408.tex", "nexttext": "\n  where,  $\\boldsymbol\\Sigma_{\\mu}\\equiv\\sigma^2(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'$ and $\\boldsymbol\\Sigma_{\\mu|\\cdot}\\equiv(\\mathbf{K}'\\boldsymbol\\Sigma_s^{-1}\\mathbf{K}+\\boldsymbol\\Sigma_\\mu^{-1})^{-1}$.\n  \\item Increment $k=k+1$ and go to step 3.\n\\end{enumerate}\n\n\\section*{Appendix D}\nIn this Section, we show the results of two examples using simulated data.  In the first example, we simulated a stationary stochastic movement process and, in the second example, we simulated a nonstationary stochastic movement process.  For consistency with the example involving the mule deer GPS data, we used the same model specification as described in Section 3 in both simulated examples.  We used the truncated Gaussian kernel for $h(t,\\tau)$ and the reparameterized covariance matrix \n\\begin{displaymath}\n  \\mathbf{I}\\otimes \\boldsymbol\\Sigma_s + \\sigma^2 (\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})' = \\sigma^2_s (\\mathbf{I} + \\sigma^2_{\\mu/s}(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})(\\mathbf{I}\\otimes \\tilde{\\mathbf{H}})'),\n\\end{displaymath}\n\\noindent where, $\\sigma^2_{\\mu/s}=\\sigma^2/\\sigma^2_s$ and $\\tilde{\\mathbf{H}}(\\phi)$ is a function of the temporal range parameter $\\phi$.  For the simulated data, we used a discrete uniform prior for $\\phi$ on 100 evenly spaced values from $0.001$ to $0.1$, a uniform prior for $\\sigma_{\\mu/s}$ on support $(0,20)$, and an informative inverse gamma prior for $\\sigma^2_s \\sim \\text{IG}(12,0.01)$ representing the same strong prior information available for GPS telemetry error similar to that in the mule deer data.  \n \nIn the first example, we simulated 300 telemetry observations (to mimic the types of data obtained in many telemetry studies) based on the parameter values $\\sigma^2_{s}=0.001$, $\\sigma^2_{\\mu/s}=0.01/\\sigma^2_s$, and $\\phi=0.005$. The marginal posterior distributions and trace plots for the first example are shown in Appendix Figure~\\ref{fig:traceplots}.  The true values were captured well in this simulation example.  We also observed similar inference in other simulations and, for a range of sample sizes, posterior inference was similarly acceptable.  Empirically, we found that the model demonstrated good asympotic properties, providing less biased and more precise inference with larger sample sizes.  This empirical consistency mirrors results from similar continuous process models using basis function constructions in spatial statistics.         \n\\begin{figure}[htp]\n  \\centering\n  \\includegraphics[width=5.5in, angle=0]{trace_plots_H_1.jpg}\n  \\caption{Left panels:  MCMC histograms for model parameters (i.e., approximate marginal distributions); simulation truth shown as vertical black line.  Right panels:  Trace plots exhibiting adequate convergence and mixing.}\n  \\label{fig:traceplots}\n\\end{figure}\n\nThe mean posterior predictive position process and associated uncertainty are shown in Appendix Figure~\\ref{fig:predex1}.  While it is well known that Gaussian process models are excellent at prediction, the Bayesian version of the model provides a straightforward mechanism for obtaining uncertainty estimates for these complicated continuous-time trajectory processes.\n\\begin{figure}[htp]\n  \\centering\n  \\vspace{-.5in}\n  \\includegraphics[height=7in, angle=0]{path_plot_vert_H_1.jpg}\n  \\caption{Top panel:  Two-dimensional position process and data (predicted in solid, truth in dashed).  Uncertainty associated with the predictive distribution is shown as overlaid predictive realizations of the position process in gray.  Bottom panels:  One-dimensional position process and data (predicted in solid, truth in dashed).  The uncertainty is shown as pointwise 95\\% credible intervals for the posterior predictive distribution of the position process.}\n  \\label{fig:predex1}\n\\end{figure}\n\nIn the second example, we allowed for heterogeneous dynamics in the continuous-time movement process by simulating a temporal warp field $w(t)$ as described in Section 2.4 (Appendix Figure~\\ref{fig:predex2}).  We then simulated the continuous-time movement process and data using the same parameters as in the first simulation example described above.  In implementing the FMM, we sampled 1000 warp fields using a Latin hypercube design from a Gaussian process with Gaussian covariance constrained to avoid folding.     \n\\begin{figure}[htp]\n  \\centering\n  \\vspace{-.85in}\n  \\includegraphics[height=7in, angle=0]{path_plot_vert_H_2.jpg}\n  \\caption{Top panel:  Two-dimensional position process and data (predicted in solid, truth in dashed).  Uncertainty associated with the predictive distribution is shown as overlaid predictive realizations of the position process in gray.  Middle panels:  One-dimensional position process and data (predicted in solid, truth in dashed).  The uncertainty is shown as pointwise 95\\% credible intervals for the posterior predictive distribution of the position process.  Bottom panel:  True warp deriviative and warp derivative obtained via BMA.  Sample warps show in gray are weighted by their posterior model probabilities (warps with posterior probability $<0.1$ not shown).}\n  \\label{fig:predex2}\n\\end{figure}\nAs shown in Appendix Figure~\\ref{fig:predex2}, the simulated process contains visibly smoother dynamics during the time period 0.4 -- 0.7 and less smooth elsewhere in the temporal domain (except for near the end, after time 0.9).  \n\nUsing the procedure described in Appendix C of this Supplementary Material, we fit the FMM based on each of the sampled warp fields and then performed BMA to obtain a final posterior predicted position process and associated warp derivative (Appendix Figure~\\ref{fig:predex2}).  The bottom panel of Appendix Figure~\\ref{fig:predex2} indicates that the BMA procedure, using only 1000 sampled warps, was able to recover the general pattern of nonstationarity in the process (i.e., smoothness in the time interval 0.4--0.7 and after time 0.9).  The posterior predicted path distribution resulting from the BMA indicates that the FMM recovers the important characteristics of the true position process, despite the temporal gaps in the data.  We observed similar performance over a range of other simulation scenarios as well.       \n\n\n\n", "itemtype": "equation", "pos": 50624, "prevtext": "\n  for each model $j=1,\\ldots,J$.\n  \\item Sample ${\\cal M}_j$ from a categorical distribution with probabilities $P({\\cal M}_1 | \\cdot)$, $P({\\cal M}_2 | \\cdot)$, $\\ldots$ , $P({\\cal M}_J | \\cdot)$. \n  \\item If inference for the position process is desired, sample $\\boldsymbol\\mu$ from its full-conditional distribution $[\\boldsymbol\\mu | \\cdot]$, where the full-conditional depends on the current values for the parameters $\\boldsymbol\\theta$ and the data $\\mathbf{s}$.  The full-conditional for $\\boldsymbol\\mu$ is multivariate Gaussian:\n  \n", "index": 45, "text": "\\begin{equation}\n    [\\boldsymbol\\mu | \\cdot] = \\mathbf{N}(\\boldsymbol\\Sigma_{\\mu|\\cdot}(\\mathbf{K}'\\boldsymbol\\Sigma_s^{-1}\\mathbf{s}+\\boldsymbol\\Sigma_\\mu^{-1}(\\boldsymbol\\mu(0)\\otimes \\mathbf{1})),\\boldsymbol\\Sigma_{\\mu|\\cdot}) \\; , \n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"[\\boldsymbol{\\mu}|\\cdot]=\\mathbf{N}(\\boldsymbol{\\Sigma}_{\\mu|\\cdot}(\\mathbf{K}%&#10;^{\\prime}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{s}+\\boldsymbol{\\Sigma}_{\\mu}^{-1}%&#10;(\\boldsymbol{\\mu}(0)\\otimes\\mathbf{1})),\\boldsymbol{\\Sigma}_{\\mu|\\cdot})\\;,\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udf41</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><mi>\ud835\udc0d</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udeba</mi><mrow><mi>\u03bc</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc0a</mi><mo>\u2032</mo></msup><msubsup><mi>\ud835\udeba</mi><mi>s</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi>\ud835\udc2c</mi><mo>+</mo><msubsup><mi>\ud835\udeba</mi><mi>\u03bc</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf41</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2297</mo><mn>\ud835\udfcf</mn><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msub><mi>\ud835\udeba</mi><mrow><mi>\u03bc</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo></mrow></msub><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}]