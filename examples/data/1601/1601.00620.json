[{"file": "1601.00620.tex", "nexttext": "\nwhere $D=\\Sigma_i{S_i}$, $SD^{-1}$ is the column-stochastic transition\nmatrix of the graph; and $\\mathbf{r}$, the \\emph{seed vector}, is a uniform distribution over the\nlabeled training instances of each class (here the facts from KB); and\n$\\alpha$ is the restart probability.  (In the experiments we use $\\alpha=0.1$.)\nThe vector $\\mathbf{v}$ can be interpreted as the probability\ndistribution of a random walk on the graph, where at each step there is\na probability $\\alpha$ to ``teleport'' to a random\nnode with distribution $\\mathbf{r}$. MRW performs one such computation of a vector\n$\\mathbf{v}_c$ for each class $c$,\nthen assigns each instance $i$ to the class $c$ with highest score, i.e.\nit predicts for $i$ the label $c=\\textrm{argmax}_c \\mathbf{v}_c(i)$.\n\nMRW can be viewed as simply computing one personalized PageRank vector for each class, where each vector is computed using a personalization vector that is uniform over the seeds, and finally assigning to each node the class associated with its highest-scoring vector.  MRW's final scores depend on centrality of nodes, as well as proximity to the seeds, and in this respect MRW  differs from other label propagation methods (e.g., \\cite{ZhuICML2003}): in particular, \\emph{it will not assign identical scores to all seed examples}.  Hence MRW will weight up seeds that are well-connected to other seeds, and weight down seeds that are in``outlying\" sections of the graph.\nThe MRW implementation we use is based on ProPPR \\cite{wang2013programming}.\n\n\\subsection{Classification}\n\nOne could imagine using the output of MRW to extend a KB directly. However, the process described above cannot be used conveniently to label new documents as they appear.  Since this is also frequently a goal, we use the MRW output to train a classifier, which can be then used to classify the entity mentions (singleton lists) and coordinate lists in any new document, as well as those not reached ones in the above graph.\n\n\n\\bing{\nIn the previous LP step, the seed labels are propagated to the other unlabeled nodes\nin the bipartite graph. One limitation is that if there is no path between a list and\nany seed node, MRW cannot walk to this list so that it fails to label it.\nAnother limitation of LP step is that if an unseen list is given, i.e.\nnot included in the graph, it needs to add the new list into the graph\nand redo the MRW propagation to label it\\bing{, which is too expensive for labeling a single list}.\nThe third limitation is that it does not consider that context in the sentences where the lists are originated.\nTo overcome these limitations, we develop a list classification method with the high confident lists\nof those types in the SSL step as training data.\n\n\nWe use five types of features in classification\nand they are extracted from a list or the sentence containing the list.\n \\emph{List-BOW feature} captures the words in the list considering that the list content is important to determine its label. If some of its words\nappear in the training lists, the classification model can explore this clue.\n\\emph{Prefix/suffix feature} encodes the fixed-length prefixes and suffixes of the words in the list.\nThis feature relax the previous feature.\nPrefixes and suffixes of\nterminologies provide helpful hints, such as the suffix ``ime'' for cefuroxime and ceftazidime.\n \\emph{Sentence-BOW feature} is composed of the other words of the sentence from which the list is extracted.\nNote that sentence-BOW features\ndo not have overlapping with list-BOW features and they can be distinguished\nby appending different feature-type tokens to them.\nThe words from the source\nsentence can provide useful information.\nFor instance, the words ``emergency''\nand ``help'' in passage 2 in Table \\ref{tab:meloxicam} can increase the confidence to the list ``chest pain, weakness, ...'' as symptom.\n \\emph{List-context feature} is collected from fixed-length contexts\nof the list. For the list ``aspirin, ibuprofen, ketoprofen,\nor naproxen'' in passages 3 in Table \\ref{tab:meloxicam}, if the context length is 2,\nthese features include ``leftTok=as'', ``leftTok=such'', ``left1gram=at'',  ``left2gram=such\\_as'', etc.\n\\emph{Dependency feature}, is generated from the dependency tree of a sentence.\nThese features include directly depended verb of the list, such as\n``contain'' in passages 3 in Table \\ref{tab:meloxicam}, modifiers of the depended verb, the dependency path of the list to the depended verb, such as ``vp\\_OBJ'', and\nother verbs on the path. }\n\nWe use the same feature generator for both mentions and lists.  Shallow features include: tokens in the NPs, and character prefixes/suffixes of these tokens; tokens from the sentence containing the NP; and tokens and bigrams from a window around the NPs.  From the dependency parsing, we also find the verb which is the closest ancestor of the head of the NP, all modifiers of this verb, and the path to this verb.  For a list, the dependency features are computed relative to the head of the list.\n\nWe used an SVM classifier \\cite{CC01a} and discard singleton features, and also the most frequent 5\\% of all features (as a stop-wording variant).\n\n\n\nWe train a binary classifier on the top N lists (including mentions and coordinate lists) of each relation, as scored by MRW. A linear kernel and defaults for all other parameters are used.\nIf a new list or mention is not classified as positive by all binary classifiers, it is predicted as ``other''.\n\n\n\n\n\\subsection{Parameter Tuning}\n\n\\wc{not quite clear how the freebase holdouts are used here....}\nTwo important parameters in DIEBOLDS are the seed number for label propagation\nwith MRW and the top N number for generating training examples of SVM.\nHere we describe our method for tuning them.\nThe evaluation data is generated with a validating set of facts.\nSpecifically, these facts are used as seeds for MRW and the\ntop 200 lists (singleton and coordinate lists)\nof each relation, as scored by MRW, are collected. We regard these lists as pseudo-labeled\nexamples to test the performance of trained classifiers in DIEBOLDS.\nTheir feature vectors are generated in the same way as above.\nWe refer to total available seeds for DIEBOLDS as development set,\nno overlapping with the validating set here.\n\nThe effect of top N number when using 100\\% of development seeds is given in Figure \\ref{fig:top_seed100}.\nAs we expected, too few examples or too many examples are not effective for training\nan accurate classifier. The reason is that, if the examples are too few, they\nare not adequate to train a classifier with good generalization capability.\nOne the other hand, if N is too large, the quality of the involved examples\ncannot be guaranteed, which also degrades the accuracy of the trained classifier.\nA good aspect can be observed from Figure \\ref{fig:top_seed100} is that the classifier's\nperformance is quite stable in a large N range, from 1,200 to 5,000. It indicates that\nDIEBOLDS is quite robust and its classification performance is not very sensitive to this parameter.\n\nWe also try different ratios of the development set as seeds\nof label propagation in DIEBOLDS.\nThe results are given in Figure \\ref{fig:seed_top1200}.\nWhen the seed number is mall, say 20\\%, the trained classifier is\nnot effective. As the seed number increasing, F1 value gets improved.\nThe values of 80\\% and 100\\% are quite similar, and it shows a certain number of seeds will almost\nachieve the best result, and the marginal improvement with more seeds\nis limited. It is because the label propagation with MRW helps collect\nsufficiently good training examples with less number of seeds, (i.e. 80\\%).\n\n\n\n\\bing{F1 values in Figures \\ref{fig:seed_top1200} and Figure \\ref{fig:top_seed100} are much higher\nthan those in Table \\ref{t:prf}, because the testing examples here are from the top\nranked ones from MRW and they are easier to predict. This way of tuning parameters\nmight not be the optimal solution, nevertheless it is a workable way because to label\nenough pages for tuning is a labor-intensive work.}\n\n\n\n\\begin{figure}\n\\centerline{\\includegraphics[width=0.44\\textwidth]{./fig/top_seed100.pdf}}\n\n\\caption{Effect of top N number, for generating training examples of SVM, on classification performance. }\n\\label{fig:top_seed100}\n\\end{figure}\n\n\n\\begin{figure}\n\\centerline{\\includegraphics[width=0.44\\textwidth]{./fig/seed_top1200.pdf}}\n\n\\caption{Effect of the seed number, for label propagation with MRW, on classification performance. }\n\\label{fig:seed_top1200}\n\\end{figure}\n\n\n\n\\bing{\nIn our implementation, the used classification model is support vector machine (SVM) {\\it ct: {{xxx}}}.\nBefore training the SVM model, the features are first filtered with simple rules. If a feature only appear\nonce among the training examples, it is filtered. After that, we remove the\ntop frequent features by a certain percentage for different features.\nSpecifically, for list-BOW, prefix/suffix, sentence-BOW, list-context, and\ndependency features are 0\\%, 5\\%, 10\\%, 10\\%, and 10\\%.\nWe do not remove the frequent list-BOW features, it is because the most frequent ones\nare usually the instances of some type, such as ``vomiting'' and ``nausea''.\nA larger fraction from the top of sentence-BOW, list-context, and\ndependency features are removed, because these features are very noisy, containing\nstopwords, measurement units, etc.\nLibSVM  with a linear kernel is employed for training, and the other parameters are defaulted. }\n\n\n\\section{Experiments}\n\n\n\\subsection{Evaluation Datasets \\footnote{We released some data at\nhttp://www.wcohen.com.}}\nFor the first dataset, we manually labeled 10 pages from WikiDisease corpus and\n10 pages from DailyMed corpus. The annotated text fragments\nare those NPs that are object values of those 8 relations, with the\ndrug or disease described by the corresponding document as the relation\nsubject. In total, we collected 436 triple facts for disease domain\nand 320 triples facts for drug domain.\nA pipeline's task is to extract the objects of the\nrelations in a given document.\n\nFor the second dataset, we employ questions in the training dataset of BioASQ3-Task\nB \\footnote{http://participants-area.bioasq.org/general\\_information/Task3b/}\nto examine the ability of DIEBOLDS output on answering those questions.\nThis dataset contains four types of questions: yes/no questions, factoid questions,\nlist questions, and summary questions \\cite{bioasq}. We focus on\nfactoid and list questions because these questions require a particular entity\nname (e.g., of a disease, drug, or gene), or a list of them as an answer.\nWe only keep the questions that are related to the relations in this paper, and finally we get 58 questions, including 37 factoid questions and 21 list questions.\nEach natural language question is translated to a structured database query,\nwhich can be evaluated on any KB. For instance, the answer of query(treatsDisease, daonil, Y)\nis expected to be a disease name, i.e. ``diabetes mellitus''.\n\n\\subsection{Baselines}\nThe first two baselines are distant-supervision-based.\nA DS baseline attempts to classify\neach NP in its input corpus into one of the interested\nrelation types or ``other'' with the training seeds as distance\nsupervision. Each sentence in the corpus is processed with the\nsame preprocessing pipeline to detect NPs. Then,\nthese NPs are labeled with the training seeds.\nThe features are defined and extracted in the\nsame way as we did for DIEBOLDS, and binary\nclassifiers are trained with the same method.\nThe first DS baseline only generates labeled\nexamples from the target corpus, and it is named \\textbf{DS1}.\nWhile the second DS baseline uses both target corpus and structured\ncorpus, and it is named \\textbf{DS2}. The third baseline applies list structure\ninto DS1, and it first preforms label propagation with MRW on the\nbipartite graph of target corpus.\nThen binary classifiers are trained with the top N lists\nscored by MRW in the same way. This baseline is\nnamed \\textbf{DS+L}.\n\n\n\n\\subsection{Variants of DIEBOLDS}\nWe also investigate different variants of DIEBOLDS. The first variant removes S-edges\nand N-edges from the graph of DIEBOLDS when propagating labels,\nand it is named DIEBOLDS-SN. By removing S-edges and N-edgess respectively,\nwe have two more variants, named DIEBOLDS-S and DIEBOLDS-N.\nWe use the same way to tune the parameters for these variants and also the baseline DS+L.\nSpecifically, all baselines and variants employ 100\\% of the training seeds, and\ntop N values are determined as the ones achieved the best performance on the tuning examples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Experimental Settings}\n\n\\wc{is this the total number of seeds or total number of matches?}\n\nWe extracted triples of these 8 target relations from Freebase. Specifically,\nif the subject of a triple matches with a drug or disease name\nin our target corpora and its object value also appear in that document,\nit is extracted as a seed.\nFor disease domain, we get 1,524, 1,976, 593, 674, and 99\ntriples for treatments, symptoms, risk\\_factors, causes, and\nprevention\\_factors, respectively. For drug domain, we get 2,973, 229, and 243\ntriples for used\\_to\\_treat, conditions\\_his\\_may\\_prevent, and\nside\\_effects, respectively. These triples are split into development\nset and validating set in the ratio of 9:1. The development set is used as seed of MRW,\nand the validating set is used to validate different parameters.\n\nNote that we did not use the seeds that only match with the structured corpora, because we aim\nat examining the effect of using structured corpora on the extraction of target corpus\nand excluding such seeds will avoid the bias because of more seeds.\nWe report the average performance of 3 runs, and each\nrun has its own randomly generated development set and validating set\n to avoid the bias of seed sampling,\n\n\n\n\n\n\n\\subsection{Results on Labeled Pages}\nDS+L and DIEBOLDS variants can classify both NPs and coordinate lists.\nAfter that, lists are broken into items, i.e. NPs, for evaluation.\nWe evaluate the performance of different pipelines\nfrom IR perspective, with a subject (i.e., document name)\nand a relation together as a query, and extracted NPs as retrieval results.\nThus, we have 50 and 30 queries for disease domain and drug domain, respectively.\nThe predicted probability by the binary classifiers serves as the ranking score inside each query.\n\n\n\n\n\\begin{table}[!t]\n\\center{\n\\begin{tabular}{@{}c@{~}|@{~}c@{~}@{~}c@{~}@{~}c@{~}|@{~}c@{~}@{~}c@{~}@{~}c@{}}\n  \\hline\n &  & Disease &  &  & Drug & \\\\\n\\cline{2-7}\n& P &    R   & F1 & P & R & F1 \\\\\n\\hline\n  \\hline\nDS1 & 0.117 &  0.350 &  0.175 & 0.020 & 0.268 & 0.037 \\\\\nDS2 & 0.115 &  0.361 &  0.174 & 0.018 & 0.254 & 0.034 \\\\\nDS+L & 0.122 &  0.380 &  0.184 & 0.031 & 0.432 & 0.057 \\\\\nFreebase & 0.202 &  0.037 &  0.062 & 0.318 & 0.022 & 0.041 \\\\\n\\hline\nDIEBOLDS-SN & 0.128 &  0.374 &  0.191 & 0.045 & 0.451 & 0.082 \\\\\nDIEBOLDS-S & 0.136 &  \\textbf{0.382} &  0.198 & 0.048 & \\textbf{0.480} & 0.088 \\\\\nDIEBOLDS-N & 0.131 &  0.372 &  0.194 & 0.047 & 0.419 & 0.085 \\\\\n\\hline\nDIEBOLDS & \\textbf{0.143} &  0.372 &  \\textbf{0.209} & \\textbf{0.050} & 0.435 & \\textbf{0.090} \\\\\n\\hline\n\\end{tabular}\n\n}\n\\vspace{-0.2cm}\n\\caption{Comparison between baselines and DIEBOLDS on extraction results of the labeled pages.\\label{t:prf}}\n\\vspace{-0.4cm}\n\\end{table}\n\nThe results evaluated by precision, recall and F1 measure are given in Table \\ref{t:prf}.\nDIEBOLDS and its variants outperform the baselines in all metrics.\nDIEBOLDS is the most effective pipeline in both domains, and its\nimprovement over DS1, pure distant supervision, on F1 is about 20\\%\nin disease domain, and more than 100\\% in drug domain.\n\nThe precision of DIEBOLDS is consistently better than its variants. Presumably, this is true because\nas more linking information is added into the graph, the top-scored\nlists or NPs in MRW are becoming less noisy.\nDIEBOLDS-S achieves the best recall values in both domains. By removing the S-edges\nfrom DIEBOLDS, N-edges become more important in the graph\nand MRW walks to more diverse NPs and lists. Thus, the trained classifier\nhas better generalization capability and achieves better recall values.\nOn the other hand, its precision is affected.\nDIEBOLDS-SN outperforms DS+L under most metrics in both domains.\nBoth of them explore list information in label propagation, but the\ndifference is that DIEBOLDS-SN employs a merged graph of target corpus and\nstructured corpus. Thus, the lists from structured corpus enhances the transduction capability\nof the graph.\n\n\nThe performance order of different pipelines is very stable. The more\ninformation of list and document structure is used, the better the performance is.\nIt shows that these types of information are all value-added and\ncombining them is a workable way to get better results. Without using the structured corpus, DS+L still achieves encouraging improvements over DS1\nlist is a useful resource by itself.\n One interesting\nobservation is that although DS2 also uses the distantly labeled\nexamples in the structured corpus, its performance is similar or even worse\nthan DS1. It shows that simply adding some examples from another corpus is not an effective approach to upgrade the performance.\nThe results in drug domain are much lower than those of disease domain.\nThe main reason is that documents of DailyMed are usually quite long, and\ntoo much description of different aspects of a drug overwhelms the\ntargeted facts.\n\nWe also employ Freebase as a comparison system, and use its facts as the system output.\nIt is not unexpected to observe very low recall values in both domains, since the coverage\nof Freebase on specific domains such as biomedical domain is limited.\nSpecifically, Freebase only contains 16 disease triples and 7 drug triples of those annotated ones.\nHowever, the precision values are also not quite high, especially for disease domain.\nThe reason is two-fold. First, our labeled pages do not contain all facts of those relations,\nbut Freebase does contain some facts of those missing ones from the labeled pages.\nThe second reason is that Freebase is not noise-free, and some facts in it are actually wrong.\n\n\nThe precision-recall curves are given in Figures \\ref{fig:pr-curve-disease}\nand \\ref{fig:pr-curve-drug}. We adopt the 11-point curve which is a graph plotting\nthe interpolated precision of an IR system at 11 standard recall levels \\cite{manning2008introduction}.\nIn general, the top ranked results are of reasonable accuracy. For the top results,\nthe average precision values of DIEBOLDS are about 0.5 and 0.35.\nDIEBOLDS and its variants are better than the baselines.\n\n\\begin{figure}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{./fig/disease_PR_curve.pdf}}\n\n\\caption{Precision-recall curve of disease domain. }\n\\label{fig:pr-curve-disease}\n\\end{figure}\n\n\n\n\\subsection{Results on BioASQ Questions}\n\nTo answer some queries in BioASQ dataset, the facts from different relations need to\nbe combined. For example, to answer query(treatsDiseaseWithSideEffect, X , epilepsy,\nspina\\_bifida), the triples of used\\_to\\_treat and side\\_effect\\_of are combined in:\\\\\n\\begin{small}\n\\indent query(treatsDiseaseWithSideEffect, Drug, Disease, Effect)\\\\\n\\indent\\indent :- used\\_to\\_treat(Drug, Disease), side\\_effect\\_of(Effect, Drug)\n\\end{small}\nWe define such rules together with the triples as input of ProPPR \\footnote{https://github.com/TeamCohen/ProPPR},\nto answer these queries.\n\n\n\nWe compare the triples of Freebase and DIEBOLDS pipeline.\n(Output of DIEBOLDS only comes from the target corpora.)\nThe evaluation metrics are\nMean Average Precision (MAP) and Mean Reciprocal Rank (MRR), commonly used for question answering, as well as Recall. The results are given in Table \\ref{t:bioasq_results}.\nof Freebase. It shows the higher scored triples from DIEBOLDS have reasonably good accuracy. On the other hand, Freebase does not have\nThe recall value of DIEBOLDS is about 80\\% higher than that of Freebase. It shows that DIEBOLDS returns richer knowledge.\n\\bing{\nFor both DIEBOLDS and Freebase, micro-recall value is higher than\nmacro-recall value. This is because they cover the answers of list questions better, which\nbenefits micro-recall value. The macro-recall value of DIEBOLDS is four times the value\nof Freebase. The reason is that DIEBOLDS outputs more diverse facts to answer more questions.\n\n\nWe may extract knowledge from structured corpora as well,\nso as to better meet the general goal of knowledge collection.\nWe did not do it here since our goal is to\ninvestigate the use of structured corpora to help extract knowledge from target corpora.}\n\n\n\\begin{figure}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{./fig/drug_PR_curve.pdf}}\n\n\\caption{Precision-recall curve of drug domain. }\n\\label{fig:pr-curve-drug}\n\\end{figure}\n\n\n\\begin{table}[!t]\n\\center{\n\\begin{tabular}{c|ccc}\n  \\hline\n & MRR & MAP & Recall \\\\\n  \\hline\n  \\hline\nDIEBOLDS  & 0.094  & 0.092 & 0.195 \\\\\nFreebase  & 0.025  & 0.025 & 0.109 \\\\\n  \\hline\n\\end{tabular}\n\n\n}\n\\caption{Results on BioASQ questions.\\label{t:bioasq_results}}\n\\vspace{-0.2cm}\n\\end{table}\n\n\n\\section{Related Work}\n\n\nDistant supervision was initially employed by \\citeauthor{CravenISMB99}\n\\shortcite{CravenISMB99} in the biomedical domain under a\ndifferent terminology, i.e. \\emph{weakly labeled data}.\nThen, it attracted attentions from the IE community.\n\n\\citeauthor{mintz2009distant} \\shortcite{mintz2009distant}\nemployed Freebase to label sentences containing\na pair of entities that participate in a known Freebase relation, and\naggregated the features from different sentences for this relation.\n\\citeauthor{wu2007autonomously} \\shortcite{wu2007autonomously,wu2010open}\nalso proposed an entity-centric corpus oriented method, and\nemployed infoboxes to label their corresponding Wikipedia articles.\nDIEBOLDS employs Freebase to label entity-centric documents of particular domains.\n\nTo tolerate the noise in distantly-labeled examples, \\citeauthor{riedel2010modeling}\n\\shortcite{riedel2010modeling} assumed that at least one of the relation mentions\nin each ``bag'' of mentions sharing a pair of argument\nentities which bears a relation, expresses the target relation, instead of\ntaking all of them as correct examples.\nMultiR \\cite{hoffmann2011knowledge} and Multi-Instance Multi-Label Learning (MIML)\n\\cite{Surdeanu:2012:MML:2390948.2391003} further improve it\nto support multiple relations expressed by different\nsentences in a bag. Different from them, before feeding the noisy examples into a learner,\nDIEBOLDS improves the quality of training data with a bootstrapping step, which propagates\nthe labels in an appropriate graph. The benefit of this step is two-fold. First,\nit distills the distantly-labeled examples by propagating labels through those coupling edges,\nand downweights the noisy ones. Second, the propagation will\nwalk to other good examples that are not distantly labeled with the seeds.\nIn the classic bootstrapping learning \\cite{riloff99Learning,agichtein00snowball,DBLP:conf/acl/BunescuM07},\nsmall number of seed instances are used to\nextract, from a large corpus, new patterns, which are used to extract more\ninstances. Then new instances are used to extract more patterns, in an iterative\nfashion.\nDIEBOLDS departs from earlier bootstrapping uses in combining label\npropagation with a standard classification learner,\nso that it can improve the quality of distant examples and collect new examples\nsimultaneously.\n\n\\section{Conclusions and Future Work}\n\nWe explored an alternative approach to distant supervision\nby detection of lists in text and utilization of document structure to\novercome the weakness of distant supervision becaseu of\nnoisy training data. It uses distant supervision\nand label propagation to find mentions\nthat can be confidently labeled, and uses them\nto train classifiers to label more entity mentions.\nThe experimental results show that this approach\nconsistently and significantly outperforms naive\ndistant-supervision approaches.\n\nFor future work, one direction is to build more comprehensive\ngraph by integrating corpora from highly related domains.\nAnother worthwhile direction is to suppress the false positives,\nwhich will significantly upgrade the overall performance.\nAnother approach that might be able to upgrade the performance is to use an\nannotated validating page set, instead of using 10\\% Freebase seeds\nto automatically generate testing examples, for tuning parameters.\nDIEBOLDS-SN outperforms DS+L by using the additional list information from the structured corpus.\nThis reminds us that using more list information of other corpora, which could be\ngeneral corpora and much larger than the target corpus, might be a worthwhile approach\nto try for enhancing the extraction on the target corpus.\nOne might want to directly classify the drug-NP pairs on the left side\nof the graph, instead of lists and mentions. This approach aggregates different\nmention occurrences of the same NP, falling in the macro-reading\nparadigm~\\cite{Mitchell:2009:PSW:1693684.1693754}, and it might also be\na good direction to explore.\n\n\n\\section{Acknowledgments}\nThis work was funded by a grant from Baidu USA and by the NSF under research grant IIS-1250956.\n\n\n\n\n\\begin{thebibliography}{}\n\n\\bibitem[\\protect\\citeauthoryear{Agichtein and\n  Gravano}{2000}]{agichtein00snowball}\nAgichtein, E., and Gravano, L.\n\\newblock 2000.\n\\newblock Snowball: Extracting relations from large plain-text collections.\n\\newblock In {\\em Proceedings of the Fifth {ACM} International Conference on\n  Digital Libraries}, 85--94.\n\n\\bibitem[\\protect\\citeauthoryear{Bing \\bgroup et al\\mbox.\\egroup\n  }{2015}]{bing-EtAl:2015:EMNLP}\nBing, L.; Chaudhari, S.; Wang~C, R.; and Cohen, W.~W.\n\\newblock 2015.\n\\newblock Improving distant supervision for information extraction using label\n  propagation through lists.\n\\newblock In {\\em EMNLP},  524--529.\n\n\\bibitem[\\protect\\citeauthoryear{Bunescu and\n  Mooney}{2007}]{DBLP:conf/acl/BunescuM07}\nBunescu, R.~C., and Mooney, R.~J.\n\\newblock 2007.\n\\newblock Learning to extract relations from the web using minimal supervision.\n\\newblock In {\\em ACL}.\n\n\\bibitem[\\protect\\citeauthoryear{Chang and Lin}{2001}]{CC01a}\nChang, C.-C., and Lin, C.-J.\n\\newblock 2001.\n\\newblock {\\em {LIBSVM}: a library for support vector machines}.\n\\newblock Software available at {http://www.csie.ntu.edu.tw/\\~{}cjlin/libsvm}.\n\n\\bibitem[\\protect\\citeauthoryear{Cohen, Ravikumar, and\n  Fienberg}{2003}]{CohenIIWeb2003}\nCohen, W.~W.; Ravikumar, P.; and Fienberg, S.~E.\n\\newblock 2003.\n\\newblock A comparison of string distance metrics for name-matching tasks.\n\\newblock In {\\em IIWeb-03}.\n\n\\bibitem[\\protect\\citeauthoryear{Craven and Kumlien}{1999}]{CravenISMB99}\nCraven, M., and Kumlien, J.\n\\newblock 1999.\n\\newblock Constructing biological knowledge bases by extracting information\n  from text sources.\n\\newblock In {\\em ISMB-99},  77--86.\n\\bibitem[\\protect\\citeauthoryear{Haveliwala \\bgroup et al\\mbox.\\egroup\n  }{2003}]{Haveliwala03ananalytical}\nHaveliwala, T.; Kamvar, S.; Kamvar, A.; and Jeh, G.\n\\newblock 2003.\n\\newblock An analytical comparison of approaches to personalizing pagerank.\n\\newblock Technical report, Stanford University.\n\n\\bibitem[\\protect\\citeauthoryear{Hoffmann \\bgroup et al\\mbox.\\egroup\n  }{2011}]{hoffmann2011knowledge}\nHoffmann, R.; Zhang, C.; Ling, X.; Zettlemoyer, L.; and Weld, D.~S.\n\\newblock 2011.\n\\newblock Knowledge-based weak supervision for information extraction of\n  overlapping relations.\n\\newblock In {\\em ACL},  541--550.\n\n\\bibitem[\\protect\\citeauthoryear{Lin and Cohen}{2010}]{DBLP:conf/asunam/LinC10}\nLin, F., and Cohen, W.~W.\n\\newblock 2010.\n\\newblock Semi-supervised classification of network data using very few labels.\n\\newblock In {\\em ASONAM},  192--199.\n\n\\bibitem[\\protect\\citeauthoryear{Manning, Raghavan, and\n  Sch{\\\"u}tze}{2008}]{manning2008introduction}\nManning, C.~D.; Raghavan, P.; and Sch{\\\"u}tze, H.\n\\newblock 2008.\n\\newblock {\\em Introduction to information retrieval}, volume~1.\n\\newblock Cambridge university press.\n\n\\bibitem[\\protect\\citeauthoryear{Mintz \\bgroup et al\\mbox.\\egroup\n  }{2009}]{mintz2009distant}\nMintz, M.; Bills, S.; Snow, R.; and Jurafsky, D.\n\\newblock 2009.\n\\newblock Distant supervision for relation extraction without labeled data.\n\\newblock In {\\em ACL},  1003--1011.\n\n\\bibitem[\\protect\\citeauthoryear{Mitchell \\bgroup et al\\mbox.\\egroup\n  }{2009}]{Mitchell:2009:PSW:1693684.1693754}\nMitchell, T.~M.; Betteridge, J.; Carlson, A.; Hruschka, E.; and Wang, R.\n\\newblock 2009.\n\\newblock Populating the semantic web by macro-reading internet text.\n\\newblock In {\\em ISWC},  998--1002.\n\n\\bibitem[\\protect\\citeauthoryear{Movshovitz-Attias and\n  Cohen}{2012}]{Movshovitz-Attias:2012:BBO:2391123.2391126}\nMovshovitz-Attias, D., and Cohen, W.~W.\n\\newblock 2012.\n\\newblock Bootstrapping biomedical ontologies for scientific text using nell.\n\\newblock In {\\em BioNLP},  11--19.\n\n\\bibitem[\\protect\\citeauthoryear{Riedel, Yao, and\n  McCallum}{2010}]{riedel2010modeling}\nRiedel, S.; Yao, L.; and McCallum, A.\n\\newblock 2010.\n\\newblock Modeling relations and their mentions without labeled text.\n\\newblock In {\\em ECML PKDD}.\n\\newblock  148--163.\n\n\\bibitem[\\protect\\citeauthoryear{Riloff and Jones}{1999}]{riloff99Learning}\nRiloff, E., and Jones, R.\n\\newblock 1999.\n\\newblock {Learning Dictionaries for Information Extraction by Multi-level\n  Boot-strapping}.\n\\newblock In {\\em AAAI/IAAI},  1044--1049.\n\n\\bibitem[\\protect\\citeauthoryear{Sagae and Tsujii}{2007}]{sagae:2007b}\nSagae, K., and Tsujii, J.\n\\newblock 2007.\n\\newblock Dependency parsing and domain adaptation with lr models and parser\n  ensembles.\n\\newblock In {\\em EMNLP-CoNLL},\n  1044--1050.\n\n\\bibitem[\\protect\\citeauthoryear{Surdeanu \\bgroup et al\\mbox.\\egroup\n  }{2012}]{Surdeanu:2012:MML:2390948.2391003}\nSurdeanu, M.; Tibshirani, J.; Nallapati, R.; and Manning, C.~D.\n\\newblock 2012.\n\\newblock Multi-instance multi-label learning for relation extraction.\n\\newblock In {\\em EMNLP-CoNLL}, 455--465.\n\n\\bibitem[\\protect\\citeauthoryear{Tong, Faloutsos, and\n  Pan}{2006}]{DBLP:conf/icdm/TongFP06}\nTong, H.; Faloutsos, C.; and Pan, J.-Y.\n\\newblock 2006.\n\\newblock Fast random walk with restart and its applications.\n\\newblock In {\\em ICDM},  613--622.\n\n\\bibitem[\\protect\\citeauthoryear{Tsatsaronis \\bgroup et al\\mbox.\\egroup\n  }{2015}]{bioasq}\nTsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.; Zschunke, M.;\n  Alvers, M.; Weissenborn, D.; Krithara, A.; Petridis, S.; Polychronopoulos,\n  D.; Almirantis, Y.; Pavlopoulos, J.; Baskiotis, N.; Gallinari, P.; Arti{\\`{e}}res,\n  T.; Ngomo, A.-C.; Heino, N.; Gaussier, E.; Barrio-Alvers, L.; Schroeder, M.;\n  Androutsopoulos, I.; and Paliouras, G.\n\\newblock 2015.\n\\newblock An overview of the bioasq large-scale biomedical semantic indexing\n  and question answering competition.\n\\newblock {\\em BMC Bioinformatics} 16(1).\n\n\\bibitem[\\protect\\citeauthoryear{Wang, Mazaitis, and\n  Cohen}{2013}]{wang2013programming}\nWang, W.~Y.; Mazaitis, K.; and Cohen, W.~W.\n\\newblock 2013.\n\\newblock Programming with personalized pagerank: a locally groundable\n  first-order probabilistic logic.\n\\newblock In {\\em CIKM},  2129--2138.\n\n\\bibitem[\\protect\\citeauthoryear{West \\bgroup et al\\mbox.\\egroup\n  }{2014}]{west2014knowledge}\nWest, R.; Gabrilovich, E.; Murphy, K.; Sun, S.; Gupta, R.; and Lin, D.\n\\newblock 2014.\n\\newblock Knowledge base completion via search-based question answering.\n\\newblock In {\\em WWW},  515--526.\n\n\\bibitem[\\protect\\citeauthoryear{Wu and Weld}{2007}]{wu2007autonomously}\nWu, F., and Weld, D.~S.\n\\newblock 2007.\n\\newblock Autonomously semantifying wikipedia.\n\\newblock In {\\em CIKM},  41--50.\n\n\\bibitem[\\protect\\citeauthoryear{Wu and Weld}{2010}]{wu2010open}\nWu, F., and Weld, D.~S.\n\\newblock 2010.\n\\newblock Open information extraction using wikipedia.\n\\newblock In {\\em ACL},  118--127.\n\n\\bibitem[\\protect\\citeauthoryear{Zhu, Ghahramani, and\n  Lafferty}{2003}]{ZhuICML2003}\nZhu, X.; Ghahramani, Z.; and Lafferty, J.\n\\newblock 2003.\n\\newblock Semi-supervised learning using {Gaussian} fields and harmonic\n  functions.\n\\newblock In {\\em ICML-03}.\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 17494, "prevtext": "\n\\author{Lidong Bing$^{\\S}$ \\ \\ Mingyang Ling$^{\\S}$ \\ \\ Richard C. Wang$^{\\natural}$ \\ \\ William W. Cohen$^{\\S}$ \\\\\n$^{\\S}$Carnegie Mellon University, Pittsburgh, PA 15213\\\\\n$^{\\natural}$US Development Center,\nBaidu USA, Sunnyvale, CA 94089\\\\\n{ $^{\\S}$\\{lbing@cs, mingyanl@andrew, wcohen@cs\\}.cmu.edu} \\\\ {$^{\\natural}$richardwang@baidu.com}}\n\\maketitle\n\n\\begin{abstract}\nDistant labeling for information extraction (IE) suffers from noisy training data.\nWe describe a way of reducing the noise associated with distant IE by identifying\ncoupling constraints between potential instance labels.  As one example of coupling,\nitems in a list are likely to have the same label.\nA second example of coupling comes from analysis of document structure: in some corpora,\nsections can be identified such that items in the same section are likely to have\nthe same label.  Such sections do not exist in all corpora, but we show that\naugmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task.\n\\end{abstract}\n\n\\section{Introduction}\n\nIn distantly-supervised information extraction (IE), a knowledge base\n(KB) of relation or concept instances is used to train an IE system.\nFor instance, a set of facts like\n\\textit{ad\\-verse\\-Effect\\-Of(melox\\-icam, stom\\-ach\\-Bleed\\-ing)},\n\\textit{int\\-eracts\\-With(melox\\-icam, ibu\\-prof\\-en)}, might be matched against a corpus, and the\nmatching sentences then used to generate training data consisting\nof labeled entity mentions.  For instance, matching the KB above might\nlead to labeling passage \\ref{lab:bleeding} from\nTable~\\ref{tab:meloxicam} as support for the fact\n\\textit{ad\\-verse\\-Effect\\-Of(melox\\-icam, stom\\-ach\\-Bleed\\-ing)}.\n\nA weakness of distant supervision is that it produces noisy training data, when matching errors occur.  E.g., consider using distant learning to classify noun phrases (NPs) into types, like \\textit{drug} or \\textit{symptom}; matching a polysemous term like \\textit{weakness} could lead to incorrectly-labeled mention examples.  Hence distant\nsupervision is often coupled with learning methods that allow for this\nsort of noise by introducing latent variables\nfor each entity mention (e.g., \\cite{hoffmann2011knowledge,riedel2010modeling,Surdeanu:2012:MML:2390948.2391003});\nby carefully selecting the entity mentions\nfrom contexts likely to include specific KB facts \\cite{wu2010open};\n   or by careful filtering of\nthe KB strings used as seeds \\cite{Movshovitz-Attias:2012:BBO:2391123.2391126}.\n\n\\iffalse Hoffmann, Raphael, et al. \"Knowledge-based weak supervision for information extraction of overlapping relations.\" Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011.\n\nSurdeanu, Mihai, et al. \"Multi-instance multi-label learning for relation extraction.\" Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012.\n\nbionell Movshovitz-Attias, Dana, and William W. Cohen. \"Bootstrapping biomedical ontologies for scientific text using nell.\" Proceedings of the 2012 Workshop on Biomedical Natural Language Processing. Association for Computational Linguistics, 2012.\n\\fi\n\n\\begin{table}[t]\n\\hrule\n\\begin{enumerate}\n\\item \\label{lab:bleeding}  ``Avoid drinking alcohol.  It may increase your risk of stomach bleeding.''\n\\item \\label{lab:adverse} ``Get emergency medical help if you have chest pain, weakness, shortness of breath, slurred speech, or problems with vision or balance.''\n\\item \\label{lab:drug} ``Check the label to see if a medicine contains an NSAID (non-steroidal anti-inflammatory drug) such as aspirin, ibuprofen, ketoprofen, or naproxen.''\n\\end{enumerate}\n\\hrule\n\\caption{Passages from a page describing the drug melox\\-icam.} \\label{tab:meloxicam}\n\n\\end{table}\n\n\n\\begin{figure}[t]\n\\centerline{\\includegraphics[width=0.48\\textwidth]{./fig/side_effect_meloxicam.png}}\n\n\\caption{A structured document in WebMD describing the drug meloxicam.  All documents in this corpora have the same 7 sections.}\n\\label{fig:side_effect_meloxicam}\n\\end{figure}\n\n\nWe describe a way of reducing the noise associated with distant IE by identifying coupling\nconstraints between potential instance labels.  As one example of coupling, NPs in a conjunctive list\nare likely to have the same category, a fact used in prior work \\cite{bing-EtAl:2015:EMNLP} to propagate NP categories from unambiguous NPs (such as \\textit{chest pain} in passage \\ref{lab:adverse}) to ambiguous ones (e.g., the mention \\textit{weakness} in the same passage). Bing et al. used\npropagation methods \\cite{ZhuICML2003,DBLP:conf/asunam/LinC10} to\nexploit this intuition, by propagating the low-confidence labels associated\nwith distance supervision matches through an appropriate graph.\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=0.64\\textwidth]{./fig/arch.pdf}}\n\n\\caption{Architecture of DIEBOLDS}\n\\label{fig:arch}\n\\end{figure*}\n\n\nIn this paper we adapt this coupling to extracting relations, rather than NP categories.  We also explore additional types of coupling, derived from analysis of document structure.  In particular, in some corpora, sections can be identified that correspond fairly accurately to relation arguments. For example, Figure~\\ref{fig:side_effect_meloxicam} shows part of a small but well-structured corpus (discussed below) which contains sections labeled ``Side Effects\".  This document structure cannot be used to directly derive training data (there are many NPs of many types, such as ``doctor\" or ``physicial\", even in a ``Side Effects\" section),\nnevertheless, we will show that coupling schemes can be derived and used to improve distantly-supervised IE, even when the test corpus does not contain well-structured sections.\n\n\n\\section{\nDIEBOLDS: \\textit{\\textbf{D}}istant \\textit{\\textbf{IE}} by \\textit{\\textbf{BO}}otstrapping using \\textit{\\textbf{L}}ist and \\textit{\\textbf{D}}ocument \\textit{\\textbf{S}}tructure}\n\nHere we describe a pipelined system called DIEBOLDS.  DIEBOLDS parses two corpora: a large \\textit{target corpus} and a smaller \\textit{structured corpus}, and also perfoms some document analysis on the structured corpus.  It then extracts NP chunks, together with features that describe each NP mention, as well as \\textit{coupling information} of various types.  In particular, DIEBOLDS derives edges that define \\textit{list coupling}, \\textit{section coupling}, and \\textit{neighbor coupling}.  DIEBOLDS then creates an appropriate graph, and uses distant supervision, in combination with a\nlabel-propagation method, to find mentions that can be confidently\nlabeled. From this pseudo-labeled data, it uses ordinary classifier\nlearners to classify NP mentions by relation types, where the relation indicates the relationship of an NP mention to the entity that is the subject of the document containing the mention.  Extensive experiments are conducted on two corpora, for diseases and drugs, and the results show that this approach significantly improves over a classical distant-supervision approach.  The architecture of the system is shown in Figure~\\ref{fig:arch}.\n\n\\subsection{Knowledge Base and Corpora}\nDistantly-supervised IE is often used to extend an incomplete KB.  Even large curated KBs\nare often incomplete: e.g., a recent work showed\nthat more than 57\\% of the nine commonly used attribute/relation values are missing for the\ntop 100k most frequent PERSON entities in Freebase \\cite{west2014knowledge}.\nWe consider extending the coverage of Freebase in the medical domain, which is currently fairly limited:\ne.g., a Freebase snapshot from April 2014 has (after filtering noise with simple rules such as length greater than 60 characters and containing comma) only 4,605 instances in ``Disease or Medical Condition`` type and 4,383 instances in ``Drug'' type, whereas \\texttt{dailymed.nlm.nih.gov} contains data on over 74k drugs, and \\texttt{malacards.org} lists nearly 10k diseases.\n\nWe focus on extracting instances for 8 relations, defined in Freebase, of drugs and diseases.\nThe targeted drug relations include used\\_to\\_treat, conditions\\_this\\_may\\_prevent, and side\\_effects. The targeted disease relations include treatments, symptoms, risk\\_factors, causes, and prevention\\_factors.\n\n\nOur target drug corpus, called DailyMed, is downloaded from\n\\texttt{dailymed.nlm.nih.gov} which contains 28,590 XML documents,\neach of which describes a drug that can be legally prescribed in the United States.\nOur target disease corpus, called WikiDisease, is extracted from a Wikipedia dump of May 2015 and it contains 8,596 disease articles.\nLarge amount of this information in our corpora is in free text. DailyMed includes information about treated diseases, adverse effects, drug ingredients, etc. WikiDisease includes information about causes, treatments, symptoms, etc.\n\n\nOur corpora are ``entity centric\", i.e., each document discusses a single drug or disease. Relation extraction is to predict the type of an entity mention and its relation with the document subject. For instance, the mention \\textit{chest pain} is an instance of side\\_effects of the drug meloxicam in Table~\\ref{tab:meloxicam}.\n\nThe structured drug corpus, called WebMD, is collected from \\texttt{www.webmd.com}, and each drug page has 7 sections, such as Uses, Side Effects, Precautions, etc. WebMD contains 2,096 pages.\nThe structured disease corpus, called MayoClinic, is collected from \\texttt{www.mayoclinic.org}. The sections of MayoClinic pages include Symptoms, Causes, Risk Factors, Treatments and Drugs, Prevention, etc. MayoClinic contains 1,117 pages.\nThese sections discuss the important aspects of drugs and diseases, and Freebase has corresponding relations to capture such aspects.\n\n\n\\bing{\n\\begin{table}[!t]\n\\center{\n\\small{\n\\begin{tabular}{@{}c@{~}|@{~}c@{~}@{~}c@{~}|@{~}c@{~}@{~}c@{}}\n  \\hline\n & MayoClinic & WikiDisease & WebMD & DailyMed \\\\\n & (Bootstrap)  & (Target)  & (Bootstrap)  & (Target)  \\\\\n  \\hline\n  \\hline\nDocuments & 000  & 000&000& 000\\\\\nUnique sentences & 000  & 000&000& 000\\\\\nNPs & 000  & 000&000& 000\\\\\nLists & 000  & 000&000& 000\\\\\n  \\hline\n\\end{tabular}\n\n}\n}\n\\caption{Target corpora and bootstrap corpora.\\label{t:data}}\n\\end{table}\n}\n\n\\subsection{Propagation Graph}\n\n\\subsubsection{List Extraction and Graph with List Edges}\nWe use the GDep parser~\\cite{sagae:2007b}, a dependency parser trained on the GENIA Treebank, to parse the corpora. We use a simple POS-tag based noun-phrase (NP) chunker, and extract a list for each coordinating conjunction that modifies a nominal. For each NP we extract features (described below); and for each identified coordinate-term list, we extract its items.\n\n\\iffalse\n\\begin{table}[t]\n\\centering\n\\begin{small}\n\\begin{tabular}{c|@{~}c@{~}@{~}c@{~}@{~}c@{~}@{~}c@{~}@{~}c}\n\\hline\nToken  &         &   NE   &    &  Parent   &  Dep  \\\\\nIndex  &  Token  &   Tab  &  POS  &   Token   &   Label\\\\\n \\hline\n20  &  produces   &  I-VP  &  VB   &  18  &  VMOD \\\\\n21  &  diarrhea    &  B-NP  &  NN   &  26  &  NMOD  \\\\\n22  &    ,  &  O  &  ,  &   26  &  P \\\\\n23  &  nausea    &  B-NP  &  NN   &  26  &  NMOD \\\\\n24  &  ,  &  O  &  ,  &   26  &  P \\\\\n25  &  and   &  O  &  CC   &  26  &  NMOD \\\\\n26  &  vomiting   &  B-NP  &  NN  &  20  &  OBJ \\\\\n\\hline\n\\bing{\nToken  &    &    &  NE   &    &  Parent   &  Dep  \\\\\nIndex  &  Token  &  Stem  &   Tab  &  POS  &   Token   &   Label\\\\\n \\hline\n20  &  produces  &  produce  &  I-VP  &  VB   &  18  &  VMOD \\\\\n21  &  diarrhea  &  diarrhea  &  B-NP  &  NN   &  26  &  NMOD  \\\\\n22  &  ,  &  ,  &  O  &  ,  &   26  &  P \\\\\n23  &  nausea  &  nausea  &  B-NP  &  NN   &  26  &  NMOD \\\\\n24  &  ,  &  ,  &  O  &  ,  &   26  &  P \\\\\n25  &  and  &  and  &  O  &  CC   &  26  &  NMOD \\\\\n26  &  vomiting  &  vomiting  &  B-NP  &  NN  &  20  &  OBJ \\\\\n\\hline\n24  &  e.g.  &  e.g.  &  B-NP  &  FW  &  41  &  NMOD \\\\\n25  &  ,  &  ,  &  O  &  ,  &   41  &  P \\\\\n26  &  pneumonia  &  pneumonia  &  B-NP  &  NN  &  29  &  NMOD \\\\\n27  &  ,  &  ,  &  O  &  ,  &   29  &  P \\\\\n28  &  systemic  &  systemic  &  B-NP  &  JJ  &  29  &  NMOD \\\\\n29  &  infection  &  infection  &  I-NP  &  NN  & 41  &  NMOD \\\\\n30  &  ,  &  ,  &  B-NP  &  ,  &   32  &  P \\\\\n31  &  etc.  &  etc.  &  I-NP  &  FW  &   32  &  DEP \\\\\n  \\hline}\n\\end{tabular}\n\\caption{\\label{t:gdep_example}A parsed text fragment.}\n\\end{small}\n\\end{table}\n\\fi\n\nThe extracted lists and their items, as well as entity mentions and their corresponding NPs, are used to create bipartite graph. One set of vertices correspond to entity mentions, where each mention is encoded as a pair, consisting of the subject entity for the document, paired with the string corresponding to the NP itself.\nThe other set of vertex identifiers are for the lists.\nA mention not inside a list is regarded as a singleton list that contains only one item.\nIf an NP is contained by a list, an edge between the NP vertex and the list vertex is included in the graph.\nWe refer to such edges as list edges (L-edges for short).\nAn example bipartite graph is given in Figure \\ref{fig:graph_section_link} (ignore for now the dashed and dotted links). There are 9 instances of side\\_effects relations from three lists and four mentions extracted from two drugs.\n\n\\subsubsection{Section Edges}\n\nFor each subject entity, there are only a few pages (typically one) that discuss that entity.  Hence a graph containing only list edges is not well-connected: generally the edges only link vertices from a single document. To improve the connectivity, we augment the graph of a target corpus with a graph derived from the  structured corpus, thus building an augmented graph.\n\nFirstly, a bipartite graph for a structured corpus is constructed. In addition to lists, we employ the section information of the structured documents: specifically, edges are added between the drug-NP pairs (or disease-NP pairs) as exemplified by the dotted and dashed links in Figure \\ref{fig:graph_section_link}.\nWe add a dashed blue edge for two drug-NP pairs if their NP strings match and the two NPs come from the same section of two documents. In Figure \\ref{fig:graph_section_link}, two such edges are added because of the same section ``Side Effects'' in two drug documents, i.e., meloxicam and fluphenazine.\nThe intuition is that if an NP appears in the same section of different documents, the occurrences are very likely to have the same relation with the corresponding document subjects. Also, if two NPs appear in the same section of a document, they might have the same relation\nwith the document subject. For instance, both ``vomiting'' and ``stomach\\_upset'' appear in ``Side Effects'' section of meloxicam, it is reasonable to infer they might have the same relation label.\nWe refer to those edges as section edges (S-edges).\n\nOur target corpora have thousands of different section titles, many of which are not related in any way to the relations being extracted, so we do not add S-edges for their sections.\nNow we augment the bipartite graph of a target corpus with the graph for a corresponding structured corpus.\n\n\\begin{figure}\n\\centerline{\\includegraphics[width=0.42\\textwidth]{./fig/graph_section_link.png}}\n\n\\caption{An example of label propagation graph. }\n\\label{fig:graph_section_link}\n\\end{figure}\n\n\n\n\\subsubsection{Neighbor Edges}\n\nWe might want to further link the drug-NP (or disease-NP) pairs of a target corpus\nand a structured corpus with similarity-based edges. We add a weighted edge for\ntwo drug-NP pairs if their NP mentions are similar, where the weight\nis in $(0, 1]$ and calculated with TFIDF-weighted BOW of contexts for the NPs. The context contains all\nwords, excluding NP itself, from sentences containing the NP. We weight these edges with the cosine similarity of the two BOW objects, after TFIDF weighting. (Note that\nthat the weight of other edges is 1.)\nSuch weighted edges capture the intuition that if two NPs have similar\ncontexts, they are very likely to have the same relation label.\nWe refer to such edge as near-neighbor edges (N-edges).\n\nObviously, if both drug names and NP strings in two drug-NP pairs\nmatch, they are merged as the same node in the augmented graph.\nFor NP string and drug/disease name matching, we employ SecondString with\nSoftTFIDF as distance metric \\cite{CohenIIWeb2003} and the match threshold is 0.8.\nIt is also used for all baselines compared in the Experiments section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Label Propagation}\nConsidering the nature of those added edges, it seems plausible to use label propagation on the above graph to propagate relation types from seed drug-NP (disease-NP) pairs with known relation types, e.g., those matching the triples in KB, to more pairs and lists across the graph.\n\n\nThis can be viewed as semi-supervised learning (SSL) of the pairs that may denote a relation (e.g., ``used\\_to\\_treat'' or ``side\\_effects'').\nWe adopt an existing multi-class label propagation method,\nnamely, MultiRankWalk (MRW) \\cite{DBLP:conf/asunam/LinC10},\nto handle our task, which is a graph-based SSL related to personalized\nPageRank (PPR) \\cite{Haveliwala03ananalytical} (aka random walk with restart  \\cite{DBLP:conf/icdm/TongFP06}).\nGiven a graph represented by matrix $S$,\na vector probability\ndistribution over the nodes $\\mathbf{v}$ is found that satisfies the equation\n\n", "index": 1, "text": "$$\\mathbf{v}=\\alpha\\mathbf{r}+(1-\\alpha)SD^{-1}\\mathbf{v}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{v}=\\alpha\\mathbf{r}+(1-\\alpha)SD^{-1}\\mathbf{v}\" display=\"block\"><mrow><mi>\ud835\udc2f</mi><mo>=</mo><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\ud835\udc2b</mi></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><msup><mi>D</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc2f</mi></mrow></mrow></mrow></math>", "type": "latex"}]