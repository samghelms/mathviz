[{"file": "1601.02197.tex", "nexttext": "\nwhere $X$ submits the Gauss distribution $N(\\mu,\\sigma^2)$, $x$ is a variable, and $\\pi$ and $e$ are constants. According to \\cite{duan2013differential}, in a certain band, DE is equivalent to the logarithmic power spectral density for a fixed length EEG sequence.\n\n\n\nBecause many evidences show that the lateralization between the left and right hemisphere is associated with emotions \\cite{davidson1992anterior}, we investigate asymmetry features. We compute differential asymmetry (DASM) and rational asymmetry (RASM) features as the differences and ratios between the DE features of 27 pairs of hemispheric asymmetry electrodes (Fp1-Fp2, F7-F8, F3-F4, FT7-FT8, FC3-FC4, T7-T8, P7-P8, C3-C4, TP7-TP8, CP3-CP4, P3-P4, O1-O2, AF3-AF4, F5-F6, F7-F8, FC5-FC6, FC1-FC2, C5-C6, C1-C2, CP5-CP6, CP1-CP2, P5-P6, P1-P2, PO7-PO8, PO5-PO6, PO3-PO4, and CB1-CB2) \\cite{duan2013differential}. DASM and RASM can be expressed, respectively, as\n\n", "itemtype": "equation", "pos": 25394, "prevtext": "\n\n\n\n\\title{Identifying Stable Patterns over Time for Emotion Recognition from EEG}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Wei-Long~Zheng,\n        Jia-Yi~Zhu,\n        and~Bao-Liang~Lu*,~\\IEEEmembership{Senior~Member,~IEEE}\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem Wei-Long Zheng, Jia-Yi Zhu and Bao-Liang Lu are with the Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University and Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, 800 Dong Chuan Road, Shanghai 200240, China.\\protect\\\\\n\n\nE-mail: \\{weilonglive,zhujiayi1991\\}@gmail.com, bllu@sjtu.edu.cn.\\hfil\\break *Corresponding author}\n\n\\thanks{}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\nIn this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. To validate the efficiency of the machine learning algorithms used in this study, we systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset for this study. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotion than negative one in beta and gamma bands; the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites; and for negative emotion, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition system shows that the neural patterns are relatively stable within and between sessions.\n\n\n\n\n\n\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\\begin{keywords}\nAffective Computing, Affective Brain-Computer Interaction, Emotion Recognition, EEG, Stable EEG Patterns, Machine Learning\n\\end{keywords}}\n\n\n\n\\maketitle\n\n\n\n\n\n\n\n\n\n\\IEEEdisplaynotcompsoctitleabstractindextext\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\section{Introduction}\n\n\\IEEEPARstart{E}{MOTION} plays an important role in human communication and decision making. Although it seems natural to us with emotions in our daily life, we have little knowledge on the mechanisms of emotional function of the brain and modeling human emotion. In recent years, the research on emotion recognition from EEG has attracted great interest from a vast amount of interdisciplinary fields from psychology to engineering, including basic studies on emotion theories and applications to affective Brain-Computer Interaction (aBCI) \\cite{muhl2014survey}, which enhances the BCI systems with the ability to detect, process, and respond to users affective states using physiological signals.\n\n\n \n\n\n\nAlthough many progresses in theories, methods and experiments that support affective computing have been made in the past several years, the problem of detecting and modeling human emotions in aBCI remains largely unexplored \\cite{muhl2014survey}. Emotion recognition is critical because computers can never respond to users' emotional states without recognizing human emotions. However, emotion recognition from EEG is very challenging due to the fuzzy boundaries and individual different variations of emotion. In addition, we can't obtain the `ground truth' of human emotions in theory, that is, the true label of EEG corresponding different emotional states. The reason is that emotion is considered as a function of time, context, space, language, culture, and races \\cite{kim2008emotion}.\n\nMany previous studies focus on subject-dependent and subject-independent patterns and evaluations for emotion recognition. However, the stable patterns and performance of models over time are not fully exploited, which are very important for real world applications. Stable EEG patterns are considered as neural activities such as critical brain areas and critical frequency bands that share commonality across individuals and sessions under different emotional states. Although task-related EEG is sensitive to change due to different cognitive states and environmental variables \\cite{mcevoy2000test}, we intuitively think that the stable patterns for specific tasks should exhibit consistency among repeated sessions of the same subjects. In this paper, we focus on the following issues of EEG-based emotion recognition: What is the capability of EEG signals for discrimination of different emotions? Are there any stable EEG patterns of neural oscillations or brain regions for representing emotions? What is the performance of the models based on machine learning approach from day to day?\n\n\nThe main contributions of this paper to emotion recognition from EEG can be summarized as follows: 1) We have developed a novel emotion EEG dataset as a subset of SEED (SJTU Emotion EEG Dataset), that will be publicly available for research to evaluate stable patterns across subjects and sessions. To the best of our knowledge, there is no available public EEG dataset for the analysis of stability of neural patterns regarding emotion recognition. 2) We carry out a systematic comparison and a qualitative evaluation of different feature extraction, feature selection, feature smoothing and pattern classification methods on a public available EEG dataset, DEAP, and our own dataset, SEED. 3) We adopt discriminative Graph regularized Extreme Learning Machine (GELM) to identify the stable patterns over time and evaluate the stability of our emotion recognition model with cross-session schemes. 4) Our experiment results reveal that neural signatures for three emotions (positive, neutral and negative) do exist and the EEG patterns at critical frequency bands and brain regions are relatively stable within and between sessions.\n\n\nThe layout of the paper is as follows. In Section 2, we give a brief overview of related work on emotion recognition from EEG, as well as the findings of stable patterns for different emotions. A systematic description of brain signal analysis methods and classification procedure for feature extraction, dimensionality reduction and classifiers is given in Section 3. Section 4 presents the motivation and rationale for our emotion experimental setting. A detailed explanation of all the materials and protocol we used is described. A systematic evaluation on different methods is conducted on the DEAP dataset and our SEED dataset. We use time-frequency analysis to find the neural signatures and stable patterns for different emotions, and evaluate the stability of our emotion recognition models over time. In Section 5, we make a conclusion about our work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table*}\n\\caption{Various studies on emotion classification using EEG and the best performance reported in each study}\\label{tab:table1}\n\\begin{center}\n\\begin{tabular}{|c|p{3em}|p{3em}|L{5.cm}|L{2.5cm}|L{2.4cm}|p{1cm}|}\n\\hline\n\\hline\nStudy     & Stimuli    & \\#Chan. & Method Description                                                                            & Emotion states                             & Accuracy              & Pattern Study                                      \\\\ \\hline\n\\cite{bos2006eeg}  & IAPS,\\newline IADS & 3         & Power of alpha and beta, then PCA, 5 subjects, classification with FDA                        & Valence and arousal                        & Valence: 92.3\\%, arousal: 92.3\\%            &$\\times$                                       \\\\ \\hline\n\\cite{heraz2007predicting}  & IAPS       & 2         & Amplitudes of four frequency bands, 17 subjects, evaluated KNN, Bagging                       & Valence (12), arousal (12) and dominance (12)             & Valence: 74\\%, arousal: 74\\%, and dominance: 75\\%           &$\\times$\\\\ \\hline\n\\cite{murugappan2010classification}  & Video      & 62        & Wavelet features of alpha, beta and gamma, 20 subjects, classification with KNN and LDA       & disgust, happy,\\newline surprise, fear and neutral & 83.26\\%          &$\\times$                                          \\\\ \\hline\n\\cite{lin2010eeg}  & Music      & 24        & Power spectral density and asymmetry features of five frequency bands, 26 subjects, evaluated SVM                & Joy, anger, sadness, and pleasure          & 82.29\\%                          &$\\surd$                          \\\\ \\hline\n\\cite{brown2011towards}  & IAPS       & 8         & Spectral power features, 11 subjects, KNN                                                     & Positive, negative and neutral             & 85\\%                                  &$\\times$                     \\\\ \\hline\n\\cite{petrantonakis2011novel}  & IAPS       & 4         & Asymmetry index of alpha and beta power, 16 subjects, SVM                                     & Four quadrants of the valence-arousal space                           & 94.4\\% (subject-dependent), 62.58\\% (subject-independent)                     &$\\times$                                \\\\ \\hline\n\\cite{koelstra2012deap}  & Video      & 32        & Spectral power features of five frequency bands, 32 subjects, Gaussian naive Bayes classifier & Valence (2), arousal (2) and liking (2)          & Valence: 57.6\\%, arousal: 62\\% and liking: 55.4\\%  &$\\times$\\\\ \\hline\n\\cite{hadjidimitriou2012toward}  & Music      & 14        & Time-frequency (TF) analysis, 9 subjects, KNN, QDA and SVM                                    & Like and dislike                           & 86.52\\%                    &$\\surd$                                \\\\ \\hline\n\\cite{soleymani2012multimodal}  & Video      & 32        & Power spectral density features of five frequency bands, modality fusion with eye track, 24 subjects, SVM        & Valence (3) and arousal (3)                    & Valence: 68.5\\%, arousal: 76.4\\%     &$\\times$             \\\\ \\hline\n\\cite{wang2013emotional}  & Video      & 62        & Power spectrum features, wavelet features, nonlinear dynamical features, 6 subjects, SVM      & Positive and negative                      & 87.53\\%                              &$\\surd$                      \\\\ \\hline\n\\cite{jenke2014feature}  & IAPS      & 64        & Higher Order Crossings, Higher Order Spectra and Hilbert-Huang Spectrum features, 16 subjects, QDA        & Happy, curious, angry, sad, quiet                    & 36.8\\%     &$\\surd$             \\\\ \\hline\n\\hline\n\\end{tabular}\n\\begin{quote}\nIAPS and IADS denotes the International Affective Picture System and the International Affective Digital Sounds, respectively. The numbers given in parenthesis denote the numbers of categories for each dimension. Pattern study denotes revealing neural activities (critical brain areas and critical frequency bands) that share commonality across subjects or sessions. Classifiers include $K$ Nearest Neighbors (KNN), Fisher's Discriminant Analysis (FDA), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Bagging, and Support Vector Machine (SVM).\n\\end{quote}\n\n\\end{center}\n\\end{table*}\n\n\\section{Related Work}\n\\subsection{Emotion Recognition Methods}\nIn the field of affective computing, a vast amount of studies has been conducted toward emotion recognition based on different signals. Many efforts have been made to recognize emotions using external emotion expression, such as facial expression \\cite{zeng2009survey}, speech \\cite{schuller2005meta}, and gestures \\cite{d2009automatic}. However, sometimes the emotional states remain internal and cannot be detected by external expression. Considering extreme cases where people do not say anything but actually they are angry, even smile during negative emotional states due to the social masking \\cite{ekman1989argument}. In these cases, the external emotional expression can be controlled subjectively and these external cues for emotion recognition may be inadequate.\n\nEmotion is an experience that is associated with a particular pattern of physiological activity including central nervous system and autonomic nervous system. Contrary to audiovisual expression, it is more objective and straightforward to recognize emotions using physiological activities. These physiological activities can be recorded by noninvasive sensors, mostly as electrical signals. These measures include skin conductivity, electrocardiogram, electromyogram, electrooculogram, and EEG. A detailed review of emotion recognition methods can be found in \\cite{calvo2010affect}.\n\nWith the fast development of micro-nano technologies and embedded systems, it is now conceivable to port aBCI systems from laboratory to real-world environments. Many advanced dry electrodes and embedded systems are developed to handle the wearability, portability, and practical use of these systems in real world applications \\cite{lin2011novel, grozea2011bristle}. Various studies in affective computing community try to build computational models to estimate emotional states based on EEG features. In short, a brief summary of emotion recognition using EEG is presented in Table~\\ref{tab:table1}. These studies show the efficiency and feasibility of building computational models of emotion recognition using EEG. In these studies, the stimuli used in emotion recognition experiments contains still images, music and videos and emotions evaluated in most studies are discrete.\n\n\n\n\n\n\n\n\n\\subsection{EEG Patterns Associated with Emotions}\nOne of the goals in affective neuroscience is to examine whether patterns of brain activity for specific emotions exist, and whether these patterns are to some extent common across individuals. Various studies have examined the neural correlations of emotions. It seems that there don't exist any processing modules for specific emotion. However, there may exist neural signatures of specific emotion, as a distributed pattern of brain activity \\cite{kassam2013identifying}. Mauss and Robinson \\cite{mauss2009measures} proposed that emotional state is likely to involve circuits rather than any brain region considered in isolation. To AC researchers, to identify neural patterns that are both common across subjects and stable across sessions can provide valuable information for emotion recognition from EEG.\n\nCortical activity in response to emotional cues was related to the lateralization effect. Muller \\emph{et al.} \\cite{muller1999processing} reported increased gamma (30-50Hz) power for negative valence over left temporal region. Davidson \\emph{et al.} {\\cite{davidson1982asymmetrical,davidson1992anterior}} showed that frontal EEG asymmetry is hypothesized to relate to approach and withdrawal emotions, with heightened approach tendencies reflected in left frontal activity and heightened withdrawal tendencies reflected in relative right-frontal activity. Nie \\emph{et al.} \\cite{nie2011eeg} reported that the subject-independent features associated with positive and negative emotions are mainly in the right occipital lobe and parietal lobe for the alpha band, the central site for beta band, and the left frontal lobe and right temporal lobe for gamma band. Balconi \\emph{et al.} \\cite{balconi2009appetitive} found that frequency band modulations are effected by valence and arousal rating, with an increased response for high arousing and negative or positive stimuli in comparison with low arousing and neutral stimuli.\n\nFor EEG-based emotion recognition, subject-dependent and subject-independent schemes are always used for evaluating the performance of emotion recognition systems. As shown in Table~\\ref{tab:table1}, some findings of activated patterns such as critical channels and oscillations associated with different emotions have been proposed. However, a major limitation is that they extract activated patterns only across subject but do not consider the time factor.\n\nThe studies of internal consistency and test-retest stability of EEG can be dated back to many years ago \\cite{salinsky1991test,mcevoy2000test,gudmundsson2007reliability}, especially for clinical applications \\cite{allen2004stability}. McEvoy \\emph{et al.} \\cite{mcevoy2000test} proposed that under appropriate conditions, task-related EEG has sufficient retest reliability for use in assessing clinical changes. However, these previous studies investigated the stability of EEG features under different conditions, for example, a working memory task \\cite{mcevoy2000test}. Moreover, in these studies, stability and reliability are often quantified using statistical parameters such as intraclass correlation coefficients \\cite{gudmundsson2007reliability}, instead of the performance of pattern classifiers.\n\nSo far, a few preliminary studies on stability and reliability of neural patterns for emotion recognition have been conducted. Lan \\emph{et al.} \\cite{lan2014stability} presented a pilot study of stability of features in emotion recognition algorithms. However, in their stability assessment, the same features derived from the same channel from the same emotion class of the same subject were grouped together to compute the correlation coefficients. Furthermore, their experiments were conducted on a small group of subjects with 14-channel EEG signals. They investigated the stability of each feature instead of neural patterns that we focus on in this paper. Up to now, there is no systematic evaluation about the stability for activated patterns over time in previous studies. The performance of emotion recognition systems over time is still an unsolved problem for developing real-world application systems. Therefore, our major aim in this paper is to investigate the stable EEG patterns over time using time frequency analysis and machine learning approaches. Here, we want to emphasize that we do not study neural patterns under emotion regulation \\cite{gross2009handbook}, but for specific emotional states during different times.\n\nTo investigate various critical problems of emotion recognition from EEG, we face a serious lack of publicly available emotional EEG datasets. To the best of our knowledge, the only publicly available emotional EEG datasets are MAHNOB HCI \\cite{soleymani2012multimodal} and DEAP \\cite{koelstra2012deap}. The first one includes EEG, physiological signals, eye gaze, audio, and facial expressions of 30 people when watching 20 emotional videos. The DEAP dataset includes the EEG and peripheral physiological signals of 32 participants when watching 40 one-minute music videos. It also contains participants' rate of each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. However, these datasets do not contain EEG data from different sessions for the same subject, which can not be used for investigating the stable patterns over time. Since there is no available published EEG dataset for the analysis of stability of neural patterns for emotion recognition, we develop a new emotional EEG dataset for this study as a subset of SEED\\footnote{\\url{http://bcmi.sjtu.edu.cn/~seed/}}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Emotion Experiment Design}\n\nIn order to investigate neural signatures of different emotions and stable patterns over time, we design a new emotion experiment to collect EEG data, which is different from other existing publicly available datasets. In our experiments, the same subject performs the emotion experiments three times with an interval of one week or longer. We choose film clips as emotion elicitation materials. These emotional films contain both scene and audio, which can expose subjects to more real-life scenarios and elicit strong subjective and physiological changes.\n\nIn our emotion experiments, Chinese film clips are used considering that native culture factors may affect elicitation in emotion experiments \\cite{zhengmultimodal}. In the preliminary study, we selected a pool of emotional film clips from famous Chinese films manually. Twenty participants were asked to assess their emotions when watching the selected film clips by scores (1-5) and keywords (positive, neutral and negative). The criteria for selecting film clips are as follows: (a) the length of the whole experiment should not be too long in case it will make subjects visual fatigue; (b) the videos should be understood without explanation; and (c) the videos should elicit a single desired target emotion. At last, 15 Chinese film clips for positive, neutral and negative emotions were chosen from the pool of materials, which received 3 or higher sores of mean ratings from the participants. Each emotion has five film clips in one experiment. The duration of each film clip is about 4 minutes. Each film clip is well edited to create coherent emotion eliciting. The details of the film clips used in the experiments are listed in Table~\\ref{tab:table2}.\n\n\\begin{table}[h]\\normalsize\n\\caption{Details of film clips used in our emotion experiment}\\label{tab:table2}\n\n\\begin{center}\n\\begin{tabular}{p{1em}p{3.1em}p{12.5em}p{1.8em}}\n\\hline\n\\hline\nNo. & Labels & Film clips sources   & \\#clips      \\\\ \\hline\n1   & negative      & Tangshan Earthquake        &2\\\\ \\hline\n2   & negative      & Back to 1942               &3        \\\\ \\hline\n3   & positive      & Lost in Thailand           &2\\\\ \\hline\n4   & positive      & Flirting Scholar           &1\\\\ \\hline\n5   & positive      & Just Another Pandora's Box &2\\\\ \\hline\n6   & neutral       & World Heritage in China    &5\\\\ \\hline\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nFifteen subjects (7 males and 8 females; mean: 23.27, std: 2.37) who are different from those in the preliminary study, participate in the experiments. In order to investigate neural signatures and stable patterns across sessions and individuals, each subject is required to perform the experiments for three sessions. The time interval between two sessions is one week or longer. All participants are native Chinese students from Shanghai Jiao Tong University with self-reported normal or corrected-to-normal vision and normal hearing. Before the experiments, the participants are informed about the experiment and instructed to sit comfortably, watch the forthcoming movie clips attentively without diverting their attention from the screen, and refrain as much as possible from overt movements.\n\nFacial videos and EEG data are recorded simultaneously. EEG is recorded using an ESI NeuroScan System\\footnote{http://www.neuroscan.com/} at a sampling rate of 1000 Hz from 62-channel active AgCl electrode cap according to the international 10-20 system. The layout of EEG electrodes on the cap is shown in Fig. \\ref{fig:figure2}. The impendence of each electrode has to be less than 5 k$\\Omega$. The frontal face videos are recorded from the camera mounted in front of the subjects. Facial videos are encoded into AVI format with the frame rate of 30 frames per second and the resolution of $160\\times120$.\n \n\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=2.in]{asm}}\n\\caption{The EEG cap layout for 62 channels}\\label{fig:figure2}\n\\end{figure}\n\nThere are totally 15 trials for each experiment. There is a 15s hint before each clips and 10s feedback after each clip. For the feedback, participants are told to report their emotional reactions to each film clip by completing the questionnaire immediately after watching each clip. The questions are following Philippot \\cite{philippot1993inducing}: (1) what they had actually felt in response to viewing the film clip; (2) how they felt at the specific time they were watching the film clips; (3) have they watched this movie before; (4) have they understood the film clips. They also rate the intensity of subjective emotional arousal using a 5-point scale according to what they actually felt during the task \\cite{schaefer2010assessing}. Fig.~\\ref{fig:figure3} shows the detailed protocol. For EEG signal processing, the raw EEG data are first downsampled to 200Hz sampling rate. In order to filter the noise and remove the artifacts, the EEG data are then processed with a bandpass filter between 0.5Hz to 70Hz.\n\n\n\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=3.2in]{protocol}}\n\\caption{The protocol used in our emotion experiment}\\label{fig:figure3}\n\\end{figure}\n\n\\section{Methodology}\n\\subsection{Feature Extraction}\nFrom our previous work\\cite{duan2013differential,zhengeeg}, we have found that the following six different features and electrode combinations are efficient for EEG-based emotion recognition: power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), asymmetry (ASM) and differential caudality (DCAU) features from EEG. As a result, we use these six different features in this study. According to five frequency bands: delta (1-3Hz); theta (4-7Hz); alpha (8-13Hz); beta (14-30Hz); and gamma (31-50Hz), we compute the traditional PSD features using Short Time Fourier Transform (STFT) with a 1s long window and no overlapping Hanning window. The differential entropy feature is defined as follows \\cite{duan2013differential},\n\n", "index": 1, "text": "\\begin{equation}\n\\begin{split}\nh(X)=&-\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\frac{(x-\\mu)^2}{2\\sigma^2}}\\log{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}}\\\\\n&\\exp{\\frac{(x-\\mu)^2}{2\\sigma^2}}dx=\\frac{1}{2}\\log{2\\pi e\\sigma^2},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle h(X)=&amp;\\displaystyle-\\int_{-\\infty}^{\\infty}\\frac{1}%&#10;{\\sqrt{2\\pi\\sigma^{2}}}\\exp{\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\\log{\\frac{1}{%&#10;\\sqrt{2\\pi\\sigma^{2}}}}\\\\&#10;&amp;\\displaystyle\\exp{\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx=\\frac{1}{2}\\log{2\\pi e%&#10;\\sigma^{2}},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></msqrt></mfrac></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 26584, "prevtext": "\nwhere $X$ submits the Gauss distribution $N(\\mu,\\sigma^2)$, $x$ is a variable, and $\\pi$ and $e$ are constants. According to \\cite{duan2013differential}, in a certain band, DE is equivalent to the logarithmic power spectral density for a fixed length EEG sequence.\n\n\n\nBecause many evidences show that the lateralization between the left and right hemisphere is associated with emotions \\cite{davidson1992anterior}, we investigate asymmetry features. We compute differential asymmetry (DASM) and rational asymmetry (RASM) features as the differences and ratios between the DE features of 27 pairs of hemispheric asymmetry electrodes (Fp1-Fp2, F7-F8, F3-F4, FT7-FT8, FC3-FC4, T7-T8, P7-P8, C3-C4, TP7-TP8, CP3-CP4, P3-P4, O1-O2, AF3-AF4, F5-F6, F7-F8, FC5-FC6, FC1-FC2, C5-C6, C1-C2, CP5-CP6, CP1-CP2, P5-P6, P1-P2, PO7-PO8, PO5-PO6, PO3-PO4, and CB1-CB2) \\cite{duan2013differential}. DASM and RASM can be expressed, respectively, as\n\n", "index": 3, "text": "\\begin{equation}\nDASM=DE(X_{left})-DE(X_{right})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"DASM=DE(X_{left})-DE(X_{right})\" display=\"block\"><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>M</mi></mrow><mo>=</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>r</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nASM features are the direct concatenation of DASM and RASM features for comparison. In the literature, the patterns of spectral differences along frontal and posterior brain regions have also been explored \\cite{lin2014fusion}. To characterize the spectral-band asymmetry in respect of caudality (in frontal-posterior direction), we define DCAU features as the differences between DE features of 23 pairs of frontal-posterior electrodes (FT7-TP7, FC5-CP5, FC3-CP3, FC1-CP1, FCZ-CPZ, FC2-CP2, FC4-CP4, FC6-CP6, FT8-TP8, F7-P7, F5-P5, F3-P3, F1-P1, FZ-PZ, F2-P2, F4-P4, F6-P6, F8-P8, FP1-O1, FP2-O2, FPZ-OZ, AF3-CB1, and AF4-CB2). DCAU is defined as\n\n", "itemtype": "equation", "pos": 26652, "prevtext": "\nand\n\n", "index": 5, "text": "\\begin{equation}\nRASM=DE(X_{left})/DE(X_{right}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"RASM=DE(X_{left})/DE(X_{right}).\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>M</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>D</mi></mrow><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>r</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nThe dimensions of PSD, DE, DASM, RASM, ASM and DCAU are 310 (62 electrodes $\\times$ 5 bands), 310 (62 electrodes $\\times$ 5 bands), 135 (27 electrode pairs $\\times$ 5 bands), 135 (27 electrode pairs $\\times$ 5 bands), 270 (54 electrode pairs $\\times$ 5 bands), and 115 (23 electrode pairs $\\times$ 5 bands), respectively.\n\n\\subsection{Feature Smoothing}\nMost of the existing approaches to emotion recognition from EEG may be suboptimal because they map EEG signals to static discrete emotional states and do not take temporal dynamics of the emotional state into account. However, in general, emotion should not be considered as a discrete psychophysiological variable \\cite{fontaine2007world}. Here, we assume that the emotional state is defined in a continuous space and emotional states change gradually. Our approach focuses on tracking the change of the emotional state over time from EEG. In our approach, we introduce the dynamic characteristics of emotional changes into emotion recognition and investigate how observed EEG is generated from a hidden emotional state. We apply the linear dynamic system (LDS) approach to filter out components which are not associated with emotional states \\cite{shi2010off,duan2012eeg}. For comparison, we also evaluate the performance of conventional moving average method.\n\nTo make use of the time dependency of emotion changes and further reduce the influence of emotion-unrelated EEG, we introduce the LDS approach to smooth features. A linear dynamic system can be expressed as follows,\n\n", "itemtype": "equation", "pos": 27365, "prevtext": "\nASM features are the direct concatenation of DASM and RASM features for comparison. In the literature, the patterns of spectral differences along frontal and posterior brain regions have also been explored \\cite{lin2014fusion}. To characterize the spectral-band asymmetry in respect of caudality (in frontal-posterior direction), we define DCAU features as the differences between DE features of 23 pairs of frontal-posterior electrodes (FT7-TP7, FC5-CP5, FC3-CP3, FC1-CP1, FCZ-CPZ, FC2-CP2, FC4-CP4, FC6-CP6, FT8-TP8, F7-P7, F5-P5, F3-P3, F1-P1, FZ-PZ, F2-P2, F4-P4, F6-P6, F8-P8, FP1-O1, FP2-O2, FPZ-OZ, AF3-CB1, and AF4-CB2). DCAU is defined as\n\n", "index": 7, "text": "\\begin{equation}\nDCAU = DE(X_{frontal})-DE(X_{posterior}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"DCAU=DE(X_{frontal})-DE(X_{posterior}).\" display=\"block\"><mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>U</mi></mrow><mo>=</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 28973, "prevtext": "\nThe dimensions of PSD, DE, DASM, RASM, ASM and DCAU are 310 (62 electrodes $\\times$ 5 bands), 310 (62 electrodes $\\times$ 5 bands), 135 (27 electrode pairs $\\times$ 5 bands), 135 (27 electrode pairs $\\times$ 5 bands), 270 (54 electrode pairs $\\times$ 5 bands), and 115 (23 electrode pairs $\\times$ 5 bands), respectively.\n\n\\subsection{Feature Smoothing}\nMost of the existing approaches to emotion recognition from EEG may be suboptimal because they map EEG signals to static discrete emotional states and do not take temporal dynamics of the emotional state into account. However, in general, emotion should not be considered as a discrete psychophysiological variable \\cite{fontaine2007world}. Here, we assume that the emotional state is defined in a continuous space and emotional states change gradually. Our approach focuses on tracking the change of the emotional state over time from EEG. In our approach, we introduce the dynamic characteristics of emotional changes into emotion recognition and investigate how observed EEG is generated from a hidden emotional state. We apply the linear dynamic system (LDS) approach to filter out components which are not associated with emotional states \\cite{shi2010off,duan2012eeg}. For comparison, we also evaluate the performance of conventional moving average method.\n\nTo make use of the time dependency of emotion changes and further reduce the influence of emotion-unrelated EEG, we introduce the LDS approach to smooth features. A linear dynamic system can be expressed as follows,\n\n", "index": 9, "text": "\\begin{equation}\nx_t=z_t+w_t,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"x_{t}=z_{t}+w_{t},\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>+</mo><msub><mi>w</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $x_t$ denotes observed variables, $z_t$ denotes the hidden emotion variables, $A$ is a transition matrix, $w_t$ is a Gaussian noise with mean $\\bar w$ and variable $Q$, and $v_t$ is a Gaussian noise with mean $\\bar v$ and variable $R$. These equations can also be expressed in an equivalent form in terms of Gaussian conditional distributions,\n\n\n", "itemtype": "equation", "pos": 29018, "prevtext": "\n\n", "index": 11, "text": "\\begin{equation}\nz_t=Az_{t-1}+v_t,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"z_{t}=Az_{t-1}+v_{t},\" display=\"block\"><mrow><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>v</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 29419, "prevtext": "\nwhere $x_t$ denotes observed variables, $z_t$ denotes the hidden emotion variables, $A$ is a transition matrix, $w_t$ is a Gaussian noise with mean $\\bar w$ and variable $Q$, and $v_t$ is a Gaussian noise with mean $\\bar v$ and variable $R$. These equations can also be expressed in an equivalent form in terms of Gaussian conditional distributions,\n\n\n", "index": 13, "text": "\\begin{equation}\np(x_t|z_t)=N(x_t|z_t+\\bar w,Q),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"p(x_{t}|z_{t})=N(x_{t}|z_{t}+\\bar{w},Q),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>t</mi></msub><mo>+</mo><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>,</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nThe initial state is assumed to be\n\n", "itemtype": "equation", "pos": 29487, "prevtext": "\nand\n\n", "index": 15, "text": "\\begin{equation}\np(z_t|z_{t-1})=N(z_t|Az_{t-1}+\\bar v,R).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"p(z_{t}|z_{t-1})=N(z_{t}|Az_{t-1}+\\bar{v},R).\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><mi>A</mi><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mover accent=\"true\"><mi>v</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nThe above model is parameterized by $\\theta=\\{A,Q,R,\\bar w,\\bar v,\\pi_0,S_0\\}$. $\\theta$ can be determined using maximum likelihood through the EM algorithm \\cite{shumway2010time} based on the observation sequence ${x_t}$. To inference the latent states ${z_t}$ from the observation sequence ${x_t}$, the marginal distribution, $p(z_t|X)$, must be calculated. The latent state can be expressed as\n\n", "itemtype": "equation", "pos": 29595, "prevtext": "\nThe initial state is assumed to be\n\n", "index": 17, "text": "\\begin{equation}\np(z_1)=N(z_1|\\pi_0,S_0).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"p(z_{1})=N(z_{1}|\\pi_{0},S_{0}).\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">|</mo><msub><mi>\u03c0</mi><mn>0</mn></msub><mo>,</mo><msub><mi>S</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $E$ means expectation. This marginal distribution can be achieved by using the messages propagation method \\cite{shumway2010time}. We use cross validation to estimate the prior parameters.\n\n\\subsection{Dimensionality Reduction}\nWe compute the initial EEG features based on the signal analysis. However, the features extracted may be uncorrelated with emotion states, and lead to performance degradation of classifiers. What's more, the high dimensionality of features may make classifiers suffer from the `curse of dimensionality' \\cite{jain2000statistical}. For real-world applications, dimensionality reduction could help to increase the speed and stability of the classifier. Hence, in this study, we compare two popular approaches: principal component analysis (PCA) algorithm and minimal redundancy maximal relevance (MRMR) algorithm \\cite{peng2005feature}.\n\nPrincipal component analysis (PCA) algorithm uses orthogonal transformation to project high-dimension data to a low-dimension space with a minimal loss of information. This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\nAlthough PCA can reduce the feature dimension, it cannot preserve the original domain information such as channel and frequency after the transformation. Hence, we also choose the minimal redundancy maximal relevance (MRMR) algorithm to select a feature subset from an initial feature set. The MRMR algorithm uses mutual information as relevance measure with max-dependency criterion and minimal redundancy criterion. Max-Relevance is to search features satisfying (\\ref{maxeq}) with the mean value of all mutual information values between individual feature $x_d$ and class $c$:\n\n", "itemtype": "equation", "pos": 30049, "prevtext": "\nThe above model is parameterized by $\\theta=\\{A,Q,R,\\bar w,\\bar v,\\pi_0,S_0\\}$. $\\theta$ can be determined using maximum likelihood through the EM algorithm \\cite{shumway2010time} based on the observation sequence ${x_t}$. To inference the latent states ${z_t}$ from the observation sequence ${x_t}$, the marginal distribution, $p(z_t|X)$, must be calculated. The latent state can be expressed as\n\n", "index": 19, "text": "\\begin{equation}\nz_t=E(z_t|X),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"z_{t}=E(z_{t}|X),\" display=\"block\"><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $S$ represents the feature subset to select. When two features highly depend on each other, the respective class-discriminative power would not change much if one of them is removed. Therefore, the following minimal redundancy condition can be added to select mutually exclusive features,\n\n", "itemtype": "equation", "pos": 31973, "prevtext": "\nwhere $E$ means expectation. This marginal distribution can be achieved by using the messages propagation method \\cite{shumway2010time}. We use cross validation to estimate the prior parameters.\n\n\\subsection{Dimensionality Reduction}\nWe compute the initial EEG features based on the signal analysis. However, the features extracted may be uncorrelated with emotion states, and lead to performance degradation of classifiers. What's more, the high dimensionality of features may make classifiers suffer from the `curse of dimensionality' \\cite{jain2000statistical}. For real-world applications, dimensionality reduction could help to increase the speed and stability of the classifier. Hence, in this study, we compare two popular approaches: principal component analysis (PCA) algorithm and minimal redundancy maximal relevance (MRMR) algorithm \\cite{peng2005feature}.\n\nPrincipal component analysis (PCA) algorithm uses orthogonal transformation to project high-dimension data to a low-dimension space with a minimal loss of information. This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\nAlthough PCA can reduce the feature dimension, it cannot preserve the original domain information such as channel and frequency after the transformation. Hence, we also choose the minimal redundancy maximal relevance (MRMR) algorithm to select a feature subset from an initial feature set. The MRMR algorithm uses mutual information as relevance measure with max-dependency criterion and minimal redundancy criterion. Max-Relevance is to search features satisfying (\\ref{maxeq}) with the mean value of all mutual information values between individual feature $x_d$ and class $c$:\n\n", "index": 21, "text": "\\begin{equation}\\label{maxeq}\n\\max D(S,c), D=\\frac{1}{|S|}\\sum_{x_d\\in{S}}{I(x_d;c)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\max D(S,c),D=\\frac{1}{|S|}\\sum_{x_{d}\\in{S}}{I(x_{d};c)},\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi>max</mi><mo>\u2061</mo><mi>D</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>D</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>d</mi></msub><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>d</mi></msub><mo>;</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nThe criterion, combining the above two constraints, is minimal-redundancy-maximal-relevance (MRMR), which can be expressed as\n\n", "itemtype": "equation", "pos": 32369, "prevtext": "\nwhere $S$ represents the feature subset to select. When two features highly depend on each other, the respective class-discriminative power would not change much if one of them is removed. Therefore, the following minimal redundancy condition can be added to select mutually exclusive features,\n\n", "index": 23, "text": "\\begin{equation}\n\\min R(S), R=\\frac{1}{|S|^2}\\sum_{x_{di},x_{dj}\\in{S}}{I(x_{di},x_{dj})}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\min R(S),R=\\frac{1}{|S|^{2}}\\sum_{x_{di},x_{dj}\\in{S}}{I(x_{di},x_{dj})}.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi>min</mi><mo>\u2061</mo><mi>R</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>R</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msup><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msub><mi>x</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nIn practice, an incremental search method is used to find the near optimal $K$ features.\n\n\\subsection{Classification}\nThe extracted features are further fed to three conventional pattern classifiers, i.e., \\emph{k} nearest neighbors (KNN) \\cite{cover1967nearest}, logistic regression (LR) \\cite{hosmer2004applied}, and support vector machine (SVM) \\cite{vapnik2000nature} and a newly developed pattern classifier, discriminative Graph regularized Extreme Learning Machine (GELM) \\cite{ypeng}, to build emotion recognition systems. For the KNN classifier, the Euclidean distance is selected as the distance metric and the number of nearest neighbors is set to 5 using cross validation. For LR, the parameters are computed by maximal likelihood estimation. We use LIBLINEAR software \\cite{fan2008liblinear} to build the SVM classifier with linear kernel. The soft margin parameter is selected using cross-validation.\n\nExtreme Learning Machine (ELM) is a single hidden layer feed forward neural network (SLFN) \\cite{huang2006extreme}, while learning with local consistency of data has drawn much attention to improve the performance of the existing machine learning models in recent years. Peng \\textit{et al.} \\cite{ypeng} proposed a discriminative Graph regularized Extreme Learning Machine (GELM) based on the idea that similar samples should share similar properties. GELM obtains much better performance in comparison with other models for face recognition \\cite{ypeng} and emotion classification \\cite{zhengeeg}.\n\nGiven a training data set,\n\n", "itemtype": "equation", "pos": 32601, "prevtext": "\nThe criterion, combining the above two constraints, is minimal-redundancy-maximal-relevance (MRMR), which can be expressed as\n\n", "index": 25, "text": "\\begin{equation}\n\\max \\varphi(D,R), \\varphi=D-R.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\max\\varphi(D,R),\\varphi=D-R.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi>max</mi><mo>\u2061</mo><mi>\u03c6</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>\u03c6</mi></mrow><mo>=</mo><mrow><mi>D</mi><mo>-</mo><mi>R</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $x_{i}=(x_{i1},x_{i2},\\cdots, x_{id})^{T}$ and $t_{i}=(t_{i1},t_{i2},\\cdot \\cdot \\cdot, x_{im})^{T}$. In GELM, the adjacent $W$ is defined as follows,\n\n\n", "itemtype": "equation", "pos": 34209, "prevtext": "\nIn practice, an incremental search method is used to find the near optimal $K$ features.\n\n\\subsection{Classification}\nThe extracted features are further fed to three conventional pattern classifiers, i.e., \\emph{k} nearest neighbors (KNN) \\cite{cover1967nearest}, logistic regression (LR) \\cite{hosmer2004applied}, and support vector machine (SVM) \\cite{vapnik2000nature} and a newly developed pattern classifier, discriminative Graph regularized Extreme Learning Machine (GELM) \\cite{ypeng}, to build emotion recognition systems. For the KNN classifier, the Euclidean distance is selected as the distance metric and the number of nearest neighbors is set to 5 using cross validation. For LR, the parameters are computed by maximal likelihood estimation. We use LIBLINEAR software \\cite{fan2008liblinear} to build the SVM classifier with linear kernel. The soft margin parameter is selected using cross-validation.\n\nExtreme Learning Machine (ELM) is a single hidden layer feed forward neural network (SLFN) \\cite{huang2006extreme}, while learning with local consistency of data has drawn much attention to improve the performance of the existing machine learning models in recent years. Peng \\textit{et al.} \\cite{ypeng} proposed a discriminative Graph regularized Extreme Learning Machine (GELM) based on the idea that similar samples should share similar properties. GELM obtains much better performance in comparison with other models for face recognition \\cite{ypeng} and emotion classification \\cite{zhengeeg}.\n\nGiven a training data set,\n\n", "index": 27, "text": "\\begin{equation}\nL=\\{(x_{i},t_{i})|x_{i}\\in R^{d},t_{i}\\in R^{m}\\},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"L=\\{(x_{i},t_{i})|x_{i}\\in R^{d},t_{i}\\in R^{m}\\},\" display=\"block\"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><mo>,</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>\u2208</mo><msup><mi>R</mi><mi>m</mi></msup></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $h_{i}=(g_{1}(x_{i}),\\cdots,g_{K}(x_{i}))^{T}$ and $h_{j}=(g_{1}(x_{j}),\\cdots,g_{K}(x_{j}))^{T}$ are hidden layer outputs for two input samples $x_{i}$ and $x_{j}$ . Then we can compute the graph Laplacian $L=D-W$ , where $D$ is a diagonal matrix  and each of the entries in $D$ is the column sums of $W$. Therefore, GELM can incorporate two regularization terms into conventional ELM model. The objective function of GELM is defined as follows,\n\n\n", "itemtype": "equation", "pos": 34450, "prevtext": "\nwhere $x_{i}=(x_{i1},x_{i2},\\cdots, x_{id})^{T}$ and $t_{i}=(t_{i1},t_{i2},\\cdot \\cdot \\cdot, x_{im})^{T}$. In GELM, the adjacent $W$ is defined as follows,\n\n\n", "index": 29, "text": "\\begin{equation}\nx_i=\\left\\{ \\begin{array}{ll} 1/N_{t}, & \\textrm{if $h_{i}$ and $h_{j}$ belong to the $t$th class}\\\\\n0, & \\textrm{otherwise;}\\end{array}\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"x_{i}=\\left\\{\\begin{array}[]{ll}1/N_{t},&amp;\\textrm{if $h_{i}$ and $h_{j}$ belong%&#10; to the $t$th class}\\\\&#10;0,&amp;\\textrm{otherwise;}\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>N</mi><mi>t</mi></msub></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><msub><mi>h</mi><mi>i</mi></msub><mtext>\u00a0and\u00a0</mtext><msub><mi>h</mi><mi>j</mi></msub><mtext>\u00a0belong to the\u00a0</mtext><mi>t</mi><mtext>th class</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise;</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\nwhere $Tr(H\\beta L \\beta^{T}H^{T})$ is the graph regularization term, $||\\beta||^{2}$ is the $l_{2}$-norm regularization term, and $\\lambda_{1}$ and $\\lambda_{2}$ are regularization parameters to balance two terms.\n\nBy setting the differentiate of the objective function (\\ref{10}) with respect to $\\beta$ as zero, we have\n\n\n", "itemtype": "equation", "pos": 35080, "prevtext": "\nwhere $h_{i}=(g_{1}(x_{i}),\\cdots,g_{K}(x_{i}))^{T}$ and $h_{j}=(g_{1}(x_{j}),\\cdots,g_{K}(x_{j}))^{T}$ are hidden layer outputs for two input samples $x_{i}$ and $x_{j}$ . Then we can compute the graph Laplacian $L=D-W$ , where $D$ is a diagonal matrix  and each of the entries in $D$ is the column sums of $W$. Therefore, GELM can incorporate two regularization terms into conventional ELM model. The objective function of GELM is defined as follows,\n\n\n", "index": 31, "text": "\\begin{equation}\\label{10}\n\\min_{\\beta}||H\\beta-T||_{2}^{2}+\\lambda_{1}Tr(H\\beta L \\beta^{T}H^{T})+\\lambda_{2}||\\beta||_{2}^{2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\beta}||H\\beta-T||_{2}^{2}+\\lambda_{1}Tr(H\\beta L\\beta^{T}H^{T})+\\lambda%&#10;_{2}||\\beta||_{2}^{2}\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mi>\u03b2</mi></munder><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>H</mi><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>-</mo><mi>T</mi></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>H</mi><mo>\u2062</mo><mi>\u03b2</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>\u03b2</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\u03b2</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02197.tex", "nexttext": "\n\nIn GELM, the constraint imposed on output weights enforces the outputs of samples from the same class to be similar. The constraint can be formulated as a regularization term to the objective function of basic ELM, which also makes the output weight matrix calculated directly.\n\n\n\n\\section{Experiment Results}\n\\subsection{Experiment Results on DEAP Data}\nIn this section, to validate the efficiency of the machine learning algorithms used in this study, we first evaluate these algorithms with the publicly available emotion dataset, the DEAP dataset \\footnote{http://www.eecs.qmul.ac.uk/mmv/datasets/deap/index.html} \\cite{koelstra2012deap} and compare the performance of our models with other methods used in the existing studies on the same emotion EEG dataset.\n\nThe DEAP dataset consists of EEG and peripheral physiological signals of 32 participants as each watched 40 excerpts of one-minute duration music videos. The EEG signals were recorded from 32 active electrodes (channels) according to the international 10-20 system, whereas peripheral physiological signals (8 channels) include galvanic skin response, skin temperature, blood volume pressure, respiration rate, electromyogram and electrooculogram (horizontal and vertical). The participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. More details about the DEAP dataset are given in \\cite{koelstra2012deap}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!b]\n\\centering\n\\centerline{\\includegraphics[width=3.5in]{VAspace.pdf}}\n\\caption{The rating distribution of DEAP on the arousal-valence plane (VA plane) for the four conditions (LALV, HALV, LAHV, and HAHV)}\\label{fig:figure5}\n\\end{figure}\n\nIn this experiment, we adopt an emotion representation model based on the valence-arousal model. Each dimension has value ranging from 1 to 9. Further, we segment the four quadrants of the valence-arousal (VA) space according to the ratings. LALV, HALV, LAHV, and HAHV denote low arousal/low valence, high arousal/low valence, low arousal/high valence, and high arousal/high valence, respectively. Considering the fuzzy boundary of emotions and the variations of participants' ratings possibly associated with individual difference in rating scale, we add an gap to segment the quadrants of VA space to ensure the correct ratings of participants' true self-elicitation emotion and discard the EEG data whose ratings of arousal and valence are between 4.8 and 5.2. The numbers of instances for LALV, HALV, LAHV, and HAHV are 12474, 16128, 10962 and 21420, respectively. The rating distribution of DEAP on the arousal-valence plane (VA plane) for the four conditions is shown in Fig. \\ref{fig:figure5}. We can see that the ratings are distributed approximately uniformly \\cite{koelstra2012deap}. We label the EEG data according to the participants' ratings of valence and arousal.\n\n\nWe firstly extracted the PSD, DE, DASM, RASM, ASM and DCAU features of the 32-channel EEG data. The original EEG data that we got from DEAP dataset are preprocessed with a downsample to 128 Hz and a bandpass frequency filter from 4.0-45.0Hz and EOG artifacts are removed. So we extract the features in the four frequency bands: theta: 4-7Hz, alpha: 8-13Hz, beta: 14-30Hz, and gamma: 31-45Hz. The features are further smoothed with the linear dynamic system approach. Then we select SVM and GELM as classifiers. In this study, we use SVM classifier with linear kernel and the number of hidden layer neurons for GELM is fixed as 10 times of the dimensions of input. To use the entire data set for training and testing the classifiers, a 5-fold cross-validation scheme is adopted. All experiments here are performed with 5-fold cross-validation and classification performance is evaluated through the classification accuracy rate.\n\n\n\n\n\n\n\n\\begin{table}[h]\n\\caption{The mean accuracy rates (\\%) of SVM and GELM classifiers for different features obtained from separate and total frequency bands.}\\label{tab:table4}\n\\begin{center}\n\\begin{tabular}{p{2.2em}lp{2.em}llllll}\n\\hline\n\\hline\nFeature               & Classifier & Theta & Alpha & Beta  & Gamma & Total \\\\ \\hline\n\\multirow{2}{*}{PSD}  & SVM        & 32.86 & 33.49 & 33.73 & 31.99 & \\textbf{36.19} \\\\\n                      & GELM       & \\textbf{61.78} & 61.14 & 61.77 & 61.56 & 61.46 \\\\ \\hline\n\\multirow{2}{*}{DE}   & SVM        & 44.31 & 41.59 & 43.54 & 42.74 & \\textbf{47.57} \\\\\n                      & GELM       & 61.45 & 61.65 & 62.17 & 61.45 & \\textbf{69.67} \\\\ \\hline\n\\multirow{2}{*}{DASM} & SVM        & \\textbf{43.18} & 42.72 & 42.07 & 41.24 & 40.70 \\\\\n                      & GELM       & \\textbf{57.86} & 57.02 & 56.08 & 56.48 & 52.54 \\\\ \\hline\n\\multirow{2}{*}{RASM} & SVM        & \\textbf{54.34} & 52.54 & 53.12 & 52.76 & 51.83 \\\\\n                      & GELM       & 46.66 & 44.52 & 44.88 & 45.56 & \\textbf{52.70} \\\\ \\hline\n\\multirow{2}{*}{ASM}  & SVM        & \\textbf{45.03} & 44.03 & 43.59 & 44.03 & 40.76 \\\\\n                      & GELM       & \\textbf{56.16} & 54.30 & 54.40 & 54.73 & 51.82 \\\\ \\hline\n\\multirow{2}{*}{DCAU} & SVM        & \\textbf{41.51} & 41.05 & 41.27 & 40.00 & 40.85 \\\\\n                      & GELM       & 57.47 & 56.58 & \\textbf{58.25} & \\textbf{58.25} & 55.26 \\\\ \\hline\n                      \\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable \\ref{tab:table4} shows the mean accuracy rates of SVM and GELM classifiers for different features obtained from various frequency bands (theta, alpha, beta and gamma) and total frequency bands. It should be noted that `Total' in Table \\ref{tab:table4} represents the direct concatenation of all features from four frequency bands. Since the EEG data of DEAP are preprocessed with a bandpass frequency filter from 4.0-45.0Hz, the results of delta frequency bands are not included. The average accuracies (\\%) are 61.46, 69.67, 52.54, 52.70, 51.82 and 55.26 for PSD, DE, DASM, RASM, ASM and DCAU features from the total frequency bands, respectively. The best accuracy of GELM classifier is 69.67\\% using DE features of total frequency bands while the best accuracy of SVM classifier is 54.34\\%. We also evaluate the performance of KNN, logistic regression and SVM with RBF kernel on DEAP, which achieve the accuracies (\\%) and standard deviations (\\%) of 35.50/14.50, 40.86/16.87, and 39.21/15.87, respectively, using the DE features of total frequency bands. We perform one way analysis of variance (ANOVA) to study the statistical significance. The DE features outperform the PSD features significantly ($p<0.01$) and for classifiers, GELM performs much better than SVM ($p<0.01$). As we can also see from Table \\ref{tab:table4}, the diversity of classification accuracy for different frequency bands is not significant for the DEAP dataset ($p>0.95$). The results here do not show specific frequency bands for the quadrants of VA space. We can also see that the DCAU features achieve comparable accuracies. These results indicate that there exists some kind asymmetry which has discriminative information for the four affect elicitation conditions (LALV, HALV, LAHV, and HAHV), as discussed in Section 2.2.\n\n\\begin{table}[h]\\small\n\\caption{Comparison of various studies using EEG in the DEAP dataset. Our method achieves the best performance among these approaches. }\\label{tab:table5}\n\\begin{center}\n\\begin{tabular}{p{1.5cm}|p{6cm}}\n\\hline\n\\hline\nStudy                    & Results                                                                          \\\\ \\hline\nChung \\emph{et al.} \\cite{chung2012affective}    & 66.6\\%, 66.4\\% for valence and arousal (2 classes), 53.4\\%, 51.0\\% for valence and arousal (3 classes) with all 32 participants.  \\\\ \\hline\nKoelstra \\emph{et al.} \\cite{koelstra2012deap} & 62.0\\%, 57.6\\% for valence and arousal (2 classes) with all 32 participants.             \\\\ \\hline\nLiu \\emph{et al.} \\cite{liu2013real}      & 63.04\\% for arousal-dominance recognition (4 classes) with selected 10 participants.                     \\\\ \\hline\nZhang \\emph{et al.} \\cite{zhang2013ontology}    & 75.19 \\% and 81.74 \\% on valence and arousal (2 classes) with selected eight participants. \\\\ \\hline\nOur method               & 69.67\\% for quadrants of VA space (4 classes) with all 32 participants.                                 \\\\ \\hline\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nThe recognition accuracy comparison of various systems using EEG signals in DEAP dataset is presented in Table \\ref{tab:table5}. Here single modality signal (EEG) is used rather than in a combined modality fusion way. Chung \\emph{et al.} \\cite{chung2012affective} defined a weighted-log-posterior function for the Bayes classifier and evaluated the method with DEAP dataset. Their accuracies for valence and arousal classification are 66.6\\% and 66.4\\% for two classes and 53.4\\% and 51.0\\% for three classes, respectively. Koelstra \\emph{et al.} \\cite{koelstra2012deap} developed the DEAP dataset and they obtained an average accuracy of 62.0\\%, 57.6\\% for valence and arousal (2 classes), respectively. Liu \\emph{et al.} \\cite{liu2013real} proposed a real-time fractal dimension (FD) based valence level recognition algorithm from EEG signals and got a mean accuracy of 63.04\\% for arousal-dominance recognition (4 classes) with selected 10 participants. Zhang \\emph{et al.} \\cite{zhang2013ontology} described an ontological model for representation and integration of EEG data and their model reached an average recognition ratio of 75.19\\% on valence and 81.74\\% on arousal for eight participants. Although their accuracies were relatively high, the categories of each dimension are only two and these results were achieved with a subset of the original dataset. In contrast, our method achieves an average accuracy of 69.67\\% on the same data set for quadrants of VA space (LALV, HALV, LAHV, and HAHV) with PSD features from theta frequency band for all 32 participants.\n\n\\subsection{Experiment Results on SEED data}\n\nIn this section, we present the results of our approaches evaluated with our SEED dataset. One very important difference between SEED and DEAP is that SEED contains three sessions at the time interval of one week or longer for the same subject.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Performance of Emotion Recognition Models}\nWe first compare six different features, namely PSD, DE, DASM, RASM, ASM and DCAU, from total frequency bands. Here we use GELM as classifier and the number of hidden layer neurons is fixed as 10 times of the dimensions of input. We adopt a 5-fold cross-validation scheme. From Table \\ref{tab:tablenew}, we can see that DE features have the highest accuracy and the lowest standard deviation than traditional PSD features, which imply that DE features are more suitable for EEG-based emotion recognition than other five different features. For the asymmetry features, although they have fewer dimensions than PSD features, they can achieve significant better performance than PSD, which means that brain processing about positive, neutral and negative emotion has asymmetrical characteristics.\n\n\\begin{table}[!h]\n\\caption{The means and standard deviations of accuracies in percentage (\\%) for PSD, DE, DASM, RASM, ASM and DCAU features from total frequency bands.}\\label{tab:tablenew}\n\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\hline\n\\hline\nFeature & PSD         & DE            & DASM & RASM & ASM & DCAU            \\\\ \\hline\nMean   & 72.75 & \\textbf{91.07} & 86.76 &86.98 &85.27 &89.95\\\\ \\hline\nStd.   & 11.85 & \\textbf{7.54}  & 8.43 &9.09 &9.16 &6.87 \\\\ \\hline\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!b]\n\n\\centerline{\\includegraphics[width=3.4in]{differentFea_new}}\n\\caption{The average accuracies of GELM using different features obtained from five frequency bands and in a fusion method.}\\label{fig:figure7}\n\\end{figure}\n\n\\begin{figure}[!b]\n\\centering\n\\centerline{\\includegraphics[width=3.2in]{Time-frequecy-analysis-v4}}\n\\caption{The spectrogram of one electrode position T7 in one experiment. As different emotions elicited, we can see that the spectrogram has different patterns.}\\label{fig:figure8}\n\n\\end{figure}\n\nWe also evaluate the performance of two different feature smoothing algorithms. Here, we compare the linear dynamic system (LDS) approach with the conventional moving average algorithm. The size of moving windows is five in this study.  The means and standard deviations of accuracies in percentage (\\%) for without smoothing, moving average, and the LDS approach are 70.82/9.17, 76.07/8.86 and 91.07/7.54, respectively. We can see that the LDS approach significantly outperforms the moving average method ($p<0.01$), which achieves 14.41\\% higher for accuracy. And the results also demonstrate that feature smoothing plays a significant role in EEG-based emotion recognition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compare the performance of four different classifiers, KNN, Logistic Regression, SVM and GELM. In this evaluation, DE features of 310 dimensions were used as the inputs of classifiers. The parameter $K$ of KNN was fixed constant five. For LR and linear SVM, grid search with cross-validation was used to tune the parameters. The mean accuracies and standard deviations in percentage (\\%) of KNN, LR, SVM with RBF kernel, SVM with linear kernel and GELM are 70.43/12.73, 84.08/8.77, 78.21/9.72, 83.26/9.08 and 91.07/7.54, respectively. From the above results, we can see that GELM outperforms other classifiers with higher accuracies and lower standard deviations, which imply that GELM is more suited for EEG-based emotion recognition. In GELM, the constraint imposed on output weights enforces the output of samples from the same class to be similar.\n\n\n\\subsubsection{Neural Signatures and Stable Patterns}\n\n\n\n\\begin{figure*}[!htbp]\n\\centering\n\\centerline{\\includegraphics[width=6.8in]{Time-frequecy-analysis-v3}}\n\\caption{The average spectrogram over subjects for each session at some electrodes, which shows the stable neural patterns over time in temporal lobes and high frequency bands. (Red color indicates high amplitude.)}\\label{fig:figure8_new}\n\n\\end{figure*}\n\n\\begin{figure*}[!htbp]\n\\centering\n\\centerline{\\includegraphics[width=6.8in]{pattern}}\n\\caption{The average neural patterns over all subjects and sessions for different emotions, which shows that neural signatures associated with positive, neutral and negative emotions do exist. The lateral temporal areas activate more for positive emotion than negative one in beta and gamma bands. While the neural patterns of neutral emotion are similar to that of negative emotion, which both have less activation in temporal areas, the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites. The negative emotion patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites.}\\label{fig:figure9}\n\\end{figure*}\n\nFig. \\ref{fig:figure7} presents the average accuracies of GELM classifier with the six different features extracted from five frequency bands (Delta, Theta, Alpha, Beta and Gamma) and the direct concatenation of these five frequency bands. The results in Fig. \\ref{fig:figure7} indicate that features obtained from gamma and beta frequency bands perform better than other frequency bands, which imply that beta and gamma oscillation of brain activity are more related with the processing of these three emotional states than other frequency oscillation as described in \\cite{li2009emotion,guntekin2010event,martini2012dynamics}.\n\nFig. \\ref{fig:figure8} shows the spectrogram of one electrode position T7 in a experiment and Fig. \\ref{fig:figure8_new} shows the average spectrogram over subjects for each session at some electrodes (FPZ, FT7, F7, FT8, T7, C3, CZ, C4, T8, P7, PZ, P8 and OZ). As we can see from Figs. \\ref{fig:figure8} and \\ref{fig:figure8_new}, the spectrograms have different patterns for different elicited emotions. The dynamics of higher frequency oscillations are more related to positive/negative emotions, especially for the temporal lobes. Moreover, the neural patterns over time are relatively stable for each session. To see the neural patterns associated with emotion processing, we project the DE features to the scalp to find temporal dynamics of frequency oscillations and stable patterns across subjects.\n\nFig. \\ref{fig:figure9} depicts the average neural patterns for positive, neutral, and negative emotion. The results demonstrate that neural signatures associated with positive, neutral, and negative emotions do exist. The lateral temporal areas activate more for positive emotion than negative one in beta and gamma bands, while the energy of the prefrontal area enhances for negative emotion over positive one in beta and gamma bands. While the neural patterns of neutral emotion are similar to negative emotion, which both have less activation in temporal areas, the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites. For negative emotion, the neural patterns have significant higher delta responses at parietal and occipital sites and significant higher gamma responses at prefrontal sites. The existing studies \\cite{ray1985eeg}, \\cite{klimesch1998induced} have shown that EEG alpha activity reflects attentional processing and beta activity reflects emotional and cognitive processes. When participants watched neutral stimuli, they tended to be more relaxed and less attentional, which evoked alpha responses. And when processing positive emotion processing, the energy of beta and gamma response enhanced. Our results are consistent with the findings of the existing work \\cite{hadjidimitriou2012toward,jenke2014feature,ray1985eeg}.\n\n\\subsubsection{Dimensionality Reduction}\n\nAs discussed above, the brain activities of emotion processing have critical frequency bands and brain areas, which imply that there must be a low-dimension manifold structure for emotion related EEG signals. Therefore, we investigate how the dimension of features will affect the performance of emotion recognition. Here, we compare two dimensionality reduction algorithms, the principle component analysis (PCA) algorithm and the minimal redundancy maximal relevance (MRMR) algorithm, with DE features of 310 dimensions as inputs and GELM as a classifier.\n\n\n\n\\begin{figure}[!b]\n\\centering\n\\centerline{\\includegraphics[width=3in]{PCA_MRMR_v3}}\n\\caption{The results of dimensionality reduction using PCA and MRMR}\\label{fig:figure10}\n\\end{figure}\n\nFig. \\ref{fig:figure10} shows the results of dimensionality reduction. We find that dimensionality reduction does not affect the performance of our model very much. For PCA algorithm, when the dimension is reduced to 210, the accuracy drops from 91.07\\% to 88.46\\% and then reached a local maximum value 89.57\\% at dimension of 160. For MRMR algorithms, the accuracies vary slightly with lower dimension features. \n\nFrom the performance comparison between PCA and MRMR, we see that it is better to apply MRMR algorithm in EEG-based emotion recognition. Because MRMR algorithm finds the best emotion-relevant and minimal redundancy features. It also preserves original domain information such as channel and frequency bands, which have most discriminative information for emotion recognition after the transformation. This discovery helps us to reduce the computations of features and the complexity of the computational models.\n\nFig. \\ref{fig:figure11} presents the distribution of the top 20 subject-independent features selected by correlation coefficient. The top 20 features were selected from alpha frequency bands at electrode locations (FT8), beta frequency bands at electrode locations (AF4, F6, F8, FT7, FC5, FC6, FT8, T7, TP7) and gamma frequency band at electrode locations (FP2, AF4, F4, F6, F8, FT7, FC5, FC6, T7, C5). These selected features are mostly from beta and gamma frequency bands and at lateral temporal and frontal brain areas, which is consistent with the above findings of time-frequency analysis.\n\n\\begin{figure}[!h]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{beta_gamma-v2}}\n\\begin{flushleft}\n\\footnotesize{One electrode location ``FT8\" is from alpha frequency band}\n\\end{flushleft}\n\\caption{Distribution of the top 20 subject-independent features selected by correlation coefficient.}\\label{fig:figure11}\n\n\n\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Stability of Emotion Recognition Model over Time}\n\nIt should be noted that SEED consists of 15 participants and each one performed the experiments three times. The interval of two sessions is one week or longer. This is the novelty of SEED we developed in comparison with the existing emotion EEG datasets. By using SEED, we evaluate whether the performance of our emotion recognition model is stable with the passage of time. We split the data from different sessions of one participant to training data and testing data and trained the model with GELM. The features employed here are the DE features extracted from total frequency bands after LDS smoothing. \n\n\\begin{table*}[!htbp] \\normalsize\n\\caption{The classification accuracies (\\%) of training and test data from different sessions using GELM.}\\label{tab:table8}\n\n\\begin{center}\n\\begin{tabular}{|c|l|c|c|c||c|c|c|c|c}\n\\hline\n\\hline\n\\multirow{2}{*}{Subject} & \\multirow{2}{*}{Train} & \\multicolumn{3}{c||}{Test} & \\multicolumn{1}{c|}{\\multirow{2}{*}{Subject}} & \\multicolumn{1}{c|}{\\multirow{2}{*}{Train}} & \\multicolumn{3}{c|}{Test}                                                               \\\\ \\cline{3-5} \\cline{8-10}\n                         &                        & 1st   & 2nd & 3rd  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{c|}{}                       & \\multicolumn{1}{c|}{1st}  & \\multicolumn{1}{c|}{2nd} & \\multicolumn{1}{c|}{3rd}  \\\\ \\hline\n\\multirow{3}{*}{\\#1}     & 1st                  & \\textbf{91.62}   & 60.26  & 60.26  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#2}}     & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{100.00}} & \\multicolumn{1}{c|}{68.28}  & \\multicolumn{1}{c|}{60.84}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 68.28   & \\textbf{80.2}   & 68.28  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{\\textbf{85.12}}  & \\multicolumn{1}{c|}{71.68}  & \\multicolumn{1}{c|}{81.65}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 68.28   & 52.53  & \\textbf{92.56}  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{85.12}  & \\multicolumn{1}{c|}{80.42}  & \\multicolumn{1}{c|}{\\textbf{90.82}}  \\\\ \\hline\n\\multirow{3}{*}{\\#3}     & 1st                  & 95.95   & \\textbf{100.00} & 75.51  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#4}}     & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{100.00}} & \\multicolumn{1}{c|}{68.28}  & \\multicolumn{1}{c|}{68.28}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 76.95   & \\textbf{97.04}  & 82.95  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{83.02}  & \\multicolumn{1}{c|}{\\textbf{100.00}} & \\multicolumn{1}{c|}{91.69}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 80.20   & \\textbf{88.08}  & 68.93  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{76.81}  & \\multicolumn{1}{c|}{\\textbf{100.00}} & \\multicolumn{1}{c|}{\\textbf{100.00}} \\\\ \\hline\n\\multirow{3}{*}{\\#5}     & 1st                  & \\textbf{75.94}   & 59.61  & 61.05  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#6}}     & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{79.70}}  & \\multicolumn{1}{c|}{71.60}  & \\multicolumn{1}{c|}{53.83}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & \\textbf{80.78}   & 75.00  & 69.65  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{67.92}  & \\multicolumn{1}{c|}{\\textbf{80.78}}  & \\multicolumn{1}{c|}{55.71}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 56.50   & 54.48  & \\textbf{98.12}  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{70.30}  & \\multicolumn{1}{c|}{71.75}  & \\multicolumn{1}{c|}{\\textbf{86.27}}  \\\\ \\hline\n\\multirow{3}{*}{\\#7}     & 1st                  & \\textbf{90.39}   & 66.69  & 66.47  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#8}}     & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{75.07}}  & \\multicolumn{1}{c|}{67.77}  & \\multicolumn{1}{c|}{51.01}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 67.49   & \\textbf{95.81}  & 59.83  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{72.83}  & \\multicolumn{1}{c|}{\\textbf{91.33}}  & \\multicolumn{1}{c|}{45.95}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & \\textbf{83.60}   & 81.79  & 74.93  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{59.61}  & \\multicolumn{1}{c|}{\\textbf{75.94}}  & \\multicolumn{1}{c|}{73.05}  \\\\ \\hline\n\\multirow{3}{*}{\\#9}     & 1st                  & \\textbf{91.98}   & 80.56  & 78.47  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#10}}    & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{85.12}}  & \\multicolumn{1}{c|}{70.38}  & \\multicolumn{1}{c|}{69.29}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 81.36   & \\textbf{100.00} & 95.95  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{60.12}  & \\multicolumn{1}{c|}{86.05}  & \\multicolumn{1}{c|}{\\textbf{87.14}}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 93.42   & \\textbf{95.52}  & 93.42  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{87.07}  & \\multicolumn{1}{c|}{83.02}  & \\multicolumn{1}{c|}{\\textbf{95.74}}  \\\\ \\hline\n\\multirow{3}{*}{\\#11}    & 1st                  & \\textbf{96.24}   & 67.99  & 76.01  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#12}}    & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{86.78}}  & \\multicolumn{1}{c|}{74.86}  & \\multicolumn{1}{c|}{63.29}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 77.89   & 85.33  & \\textbf{95.59}  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{84.39}  & \\multicolumn{1}{c|}{\\textbf{91.62}}  & \\multicolumn{1}{c|}{68.06}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 65.39   & 66.04  & \\textbf{100.00} & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{75.58}  & \\multicolumn{1}{c|}{\\textbf{83.45}}  & \\multicolumn{1}{c|}{73.48}  \\\\ \\hline\n\\multirow{3}{*}{\\#13}    & 1st                  & \\textbf{93.71}   & 76.88  & 92.63  & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\#14}}    & \\multicolumn{1}{l|}{1st}                  & \\multicolumn{1}{c|}{\\textbf{100.00}} & \\multicolumn{1}{c|}{86.27}  & \\multicolumn{1}{c|}{64.02}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 2nd                 & 70.38   & 90.75  & \\textbf{94.00}  & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{2nd}                 & \\multicolumn{1}{c|}{76.23}  & \\multicolumn{1}{c|}{\\textbf{86.42}}  & \\multicolumn{1}{c|}{77.67}  \\\\ \\cline{2-5} \\cline{7-10}\n                         & 3rd                  & 84.10   & 87.21  & \\textbf{100.00} & \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{l|}{3rd}                  & \\multicolumn{1}{c|}{\\textbf{91.69}}  & \\multicolumn{1}{c|}{82.15}  & \\multicolumn{1}{c|}{89.31}  \\\\ \\hline\n\\multirow{3}{*}{\\#15}    & 1st                  & \\textbf{100.00}  & 68.79  & 67.34  & \\multicolumn{5}{c}{\\multirow{3}{*}{}}                                                                                                                                                 \\\\ \\cline{2-5}\n                         & 2nd                 & 85.12   & \\textbf{91.26}  & 75.14  & \\multicolumn{5}{c}{}                                                                                                                                                                  \\\\ \\cline{2-5}\n                         & 3rd                  & 66.47   & 70.16  & \\textbf{80.35}  & \\multicolumn{5}{c}{}                                                                                                                                                                  \\\\ \\cline{1-5}\n\\end{tabular}\n\\end{center}\n\n\\footnotesize{'1st', '2nd', and '3rd' mean the data obtained from the first, second, and third experiments of one participant, respectively.}\n\\end{table*}\n\n\n\n\\begin{table}[!h]\\normalsize\n\\caption{The average accuracies (\\%) of our emotion model across sessions.}\\label{tab:table9}\n\n\\begin{center}\n\\begin{tabular}{|c|l|c|c|c|}\n\\hline\n\\hline\n\\multirow{2}{*}{Stats.} & \\multirow{2}{*}{Train} & \\multicolumn{3}{c|}{Test} \\\\ \\cline{3-5}\n                       &                        & First  & Second  & Third  \\\\ \\hline\n\\multirow{3}{*}{Mean}  & First                  & \\textbf{90.83}  & 72.55   & 67.22  \\\\ \\cline{2-5}\n                       & Second                 & 75.86  & \\textbf{88.22}   & 76.62  \\\\ \\cline{2-5}\n                       & Third                  & 76.28  & 78.17   & \\textbf{87.80}  \\\\ \\hline\n\\multirow{3}{*}{Std.}  & First                  & \\textbf{8.64}   & 10.29   & 10.42  \\\\ \\cline{2-5}\n                       & Second                 & \\textbf{7.71}   & 8.59    & 15.34  \\\\ \\cline{2-5}\n                       & Third                  & 11.47  & 13.41   & \\textbf{10.97}  \\\\ \\hline\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nThe results are presented in Tables \\ref{tab:table8} and \\ref{tab:table9}. From the mean accuracies and standard deviations, we find that the accuracies with training set and test set from the same sessions are much higher than those from different sessions. The performance of the emotion recognition model is better with train data and test data obtained from sessions performed in nearer time. A comparative mean classification accuracy of 79.28\\% is achieved using our emotion recognition model with training and testing datasets from different sessions. This result implies that the relation between the variation of emotional states and the EEG signal is stable for one person in a period of time. With the passage of time, the performance of the model may become worse. So the adaption of the computational model should be further studied as future work.\n\n\n\n\nNow we consider the situation of cross-subject and examine the subject-independent emotion recognition model. Here, we employ a leave-one-out cross validation to investigate the classification performance in a subject-independent approach and use linear SVM classifier with DE features from five frequency bands as inputs. The average accuracy and standard deviation with subject-independent features reach 60.93\\% and 13.95\\%, respectively. These results indicate that the subject-independent features are relatively stable and it is possible to build a common emotion recognition model. But on the other hand, the factors of individual difference should be considered to build a more robust affective computing model.\n\nWe have investigated how stable our emotion recognition model both across subjects and sessions, and we find that the performance of the model across subjects and sessions are worse than that on single experiment. In general, we want to train the model on the EEG data from a set of subjects or sessions and make inference on the new data from other unseen subjects or sessions. However, it is technically difficult due to individual differences across subjects with the inherent variability of the EEG measurements such as environmental variables \\cite{olivetti2014meg}. Although different emotions share some commonalities of neural patterns as we report above, they still contains some individual differences for different subjects and different sessions, which may lead to the changes of underlying probability distribution from subject to subject or from session to session. This is why the average accuracy of the classifiers trained and tested on each individual subject or session is much higher than that of a classifier trained on a set of subjects or sessions and tested on other subjects or sessions. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions and Future Work}\nIn this paper, we have systematically evaluated the performance of different popular feature extraction, feature selection, feature smoothing and pattern classification methods for emotion recognition on both our SEED dataset and the public DEAP dataset. From the experimental results, we have found that GELM with the differential entropy features outperforms other methods. For DEAP dataset, the best average classification accuracy of 69.67 percent for quadrants of VA space with 32 participants is obtained using 5-fold cross-validations. For our SEED dataset, the best average classification accuracy of 91.07 percent from 45 experiments is obtained using 5-fold cross-validations. The comparative classification accuracies achieved show the reliability and superior performance of our machine learning methods in comparison with the existing approaches. We have utilized these methods to investigate the stability of neural patterns over time.\n\n\n\nOn our SEED dataset, an average classification accuracy of 79.28\\% is achieved with training and testing datasets from different sessions. The experimental results indicate that neural signatures and stable EEG patterns associated with positive, neutral and negative emotions do exist. We have found that the lateral temporal areas activate more for positive emotion than negative one in beta and gamma bands; the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites; and the negative emotion patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The experiment results also indicate that the stable EEG patterns across sessions exhibit consistency among repeated EEG measurements of the same subject.\n\nIn this study, we investigate the stable neural patterns of three emotions, positive, neutral and negative. For the future work, more categories of emotions will be studied and the generalization of our proposed approach extended to more categories of emotions will be evaluated. Moreover, different factors such as gender, age, and race should be considered. To make automatical emotion recognition models adaptable, the factors like individual difference and temporal evolution should be considered. One possible way to dealing with these problems is to adopt transfer learning techniques \\cite{pan2010survey,samek2013transferring,zheng2015transfer}.\n\n\n\n\n\n\n\n\n\\ifCLASSOPTIONcompsoc\n  \n  \\section*{Acknowledgments}\n\\else\n  \n  \\section*{Acknowledgment}\n\\fi\n\n\nThe authors wish to thank all the participants in the emotion experiments and thank the Center for Brain-Like Computing and Machine Intelligence for providing the platform for EEG experiments. This work was supported in part by the grants from the National Natural Science Foundation of China (Grant No. 61272248), the National Basic Research Program of China (Grant No. 2013CB329401), and the Science and Technology Commission of Shanghai Municipality (Grant No.13511500200).\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{bare_jrnl_compsoc}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 35547, "prevtext": "\nwhere $Tr(H\\beta L \\beta^{T}H^{T})$ is the graph regularization term, $||\\beta||^{2}$ is the $l_{2}$-norm regularization term, and $\\lambda_{1}$ and $\\lambda_{2}$ are regularization parameters to balance two terms.\n\nBy setting the differentiate of the objective function (\\ref{10}) with respect to $\\beta$ as zero, we have\n\n\n", "index": 33, "text": "\\begin{equation}\\label{11}\n\\beta = (HH^{T}+\\lambda_{1}HLH^{T}+\\lambda_{2}I)^{-1}HT\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\beta=(HH^{T}+\\lambda_{1}HLH^{T}+\\lambda_{2}I)^{-1}HT\" display=\"block\"><mrow><mi>\u03b2</mi><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>H</mi><mo>\u2062</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi>I</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>T</mi></mrow></mrow></math>", "type": "latex"}]