[{"file": "1601.01073.tex", "nexttext": "\nand ${\\mathbf{{E}}}_x \\in {\\mathbb{R}}^{|V_x| \\times d}$ is an embedding matrix containing row\nvectors of the source symbols.  The reverse RNN in an opposite direction,\nresulting in $\\left\\{ {\\overleftarrow}{{\\mathbf{{h}}}}_1,\n\\ldots, {\\overleftarrow}{{\\mathbf{{h}}}}_{T_x}\\right\\}$, where\n", "itemtype": "equation", "pos": 7889, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\n    We propose multi-way, multilingual neural machine translation. The proposed\n    approach enables a single neural translation model to translate between\n    multiple languages, with a number of parameters that grows only linearly\n    with the number of languages. This is made possible by having a single\n    attention mechanism that is shared across all language pairs. We train the\n    proposed multi-way, multilingual model on ten language pairs from WMT\u00e2\u0080\u009915\n    simultaneously and observe clear performance improvements over models\n    trained on only one language pair. In particular, we observe that the\n    proposed model significantly improves the translation quality of\n    low-resource language pairs.\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\\end{abstract}\n\n\\section{Introduction}\n\n\\paragraph{Neural Machine Translation}\n\nIt has been shown that a deep (recurrent) neural network can successfully learn\na complex mapping between variable-length input and output sequences on its own.\nSome of the earlier successes in this task have, for instance, been handwriting\nrecognition~\\cite{bottou1997global,graves2009novel} and speech\nrecognition~\\cite{graves2006connectionist,chorowski2015attention}. More\nrecently, a general framework of encoder-decoder networks has been found to be\neffective at learning this kind of sequence-to-sequence mapping by using two\nrecurrent neural networks \\cite{cho2014learning,sutskever2014sequence}.\n\nA basic encoder-decoder network consists of two recurrent networks. The first\nnetwork, called an encoder, maps an input sequence of variable length into a\npoint in a continuous vector space, resulting in a fixed-dimensional context\nvector. The other recurrent neural network, called a decoder, then generates a\ntarget sequence again of variable length starting from the context vector. This\napproach however has been found to be inefficient in \\cite{Cho2014a} when\nhandling long sentences, due to the difficulty in learning a complex mapping\nbetween an arbitrary long sentence and a single fixed-dimensional vector. \n\nIn \\cite{bahdanau2014neural}, a remedy to this issue was proposed by\nincorporating an {\\em attention mechanism} to the basic encoder-decoder network.\nThe attention mechanism in the encoder-decoder network frees the network from\nhaving to map a sequence of arbitrary length to a single, fixed-dimensional\nvector. Since this attention mechanism was introduced to the encoder-decoder\nnetwork for machine translation, neural machine translation, which is purely\nbased on neural networks to perform full end-to-end translation, has become\ncompetitive with the existing phrase-based statistical machine translation in\nmany language pairs\n\\cite{jean2015WMT,Gulcehre-Orhan-et-al-2015,luong2015effective}.\n\n\n\n\n\n\n\n\n\n\\paragraph{Multilingual Neural Machine Translation}\n\nExisting machine translation systems, mostly based on a phrase-based system or\nits variants, work by directly mapping a symbol or a subsequence of symbols in a\nsource language to its corresponding symbol or subsequence in a target\nlanguage. This kind of mapping is strictly specific to a given language {\\em\npair}, and it is not trivial to extend this mapping to work on multiple pairs of\nlanguages.\n\nA system based on neural machine translation, on the other hand, can be\ndecomposed into two modules. The encoder maps a source sentence into a\ncontinuous representation, either a fixed-dimensional vector in the case of the\nbasic encoder-decoder network or a set of vectors in the case of attention-based\nencoder-decoder network. The decoder then generates a target translation based\non this source representation. This makes it possible conceptually to build a\nsystem that maps a source sentence in any language to a common continuous\nrepresentation space and decodes the representation into any of the target\nlanguages, allowing us to make a {\\em multilingual machine translation} system.\n\n\nThis possibility is straightforward to implement and has been validated in the case of basic\nencoder-decoder networks \\cite{luong2015multi}. It is however not so, in the case\nof the attention-based encoder-decoder network, as the attention mechanism, or\noriginally called the alignment function in \\cite{bahdanau2014neural}, is\nconceptually language pair-specific. In \\cite{dong2015multi}, the authors\ncleverly avoided this issue of language pair-specific attention mechanism by\nconsidering only a one-to-many translation, where each target language decoder\nembedded its own attention mechanism. Also, we notice that both of these works\nhave only evaluated their models on relatively small-scale tasks, making it\ndifficult to assess whether multilingual neural machine translation can scale\nbeyond low-resource language translation.\n\n\\paragraph{Multi-Way, Multilingual Neural Machine Translation}\n\nIn this paper, we first step back from the currently available multilingual\nneural translation systems proposed in \\cite{luong2015multi,dong2015multi} and\nask the question of whether the attention mechanism can be shared across\nmultiple language pairs. As an answer to this question, we propose an\nattention-based encoder-decoder network that admits a shared attention mechanism\nwith multiple encoders and decoders. We use this model for all the experiments,\nwhich suggests that it is indeed possible to share an attention\nmechanism across multiple language pairs.\n\n\n\n\n\n\n\n\n\n\nThe next question we ask is the following: in which scenario would the proposed multi-way,\nmultilingual neural translation have an advantage over the existing, single-pair\nmodel? Specifically, we consider a case of the translation between a\nlow-resource language pair. The experiments show that the proposed multi-way,\nmultilingual model generalizes better than the single-pair translation model,\nwhen the amount of available parallel corpus is small. Furthermore, we validate\nthat this is not only due to the increased amount of target-side, monolingual\ncorpus. \n\nFinally, we train a single model with the proposed architecture on all the\nlanguage pairs from the WMT'15; English, French, Czech, German,\nRussian and Finnish. \n\nThe experiments show that it is indeed possible to train a single\nattention-based network to perform multi-way translation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Background: Attention-based Neural Machine Translation}\n\\label{sec2}\nThe attention-based neural machine translation was proposed in\n\\cite{bahdanau2014neural}. It was motivated from the observation in\n\\cite{Cho2014a} that a basic encoder-decoder translation model from\n\\cite{cho2014learning,sutskever2014sequence} suffers from translating a long\nsource sentence efficiently. This is largely due to the fact that the encoder of\nthis basic approach needs to compress a whole source sentence into a single\nvector. Here we describe the attention-based\nneural machine translation.\n\nNeural machine translation aims at building a single neural network that takes\nas input a source sequence $X = \\left( x_1, \\ldots, x_{T_x} \\right)$ and generates a\ncorresponding translation $Y = \\left( y_1, \\ldots, y_{T_y} \\right)$.  Each symbol in\nboth source and target sentences, $x_t$ or $y_t$, is an integer index of the\nsymbol in a vocabulary. \n\nThe encoder of the attention-based model encodes a source sentence into\na set of context vectors $C = \\left\\{ {\\mathbf{{h}}}_1, {\\mathbf{{h}}}_2, \\ldots, {\\mathbf{{h}}}_{T_x} \\right\\}$,\nwhose size varies w.r.t. the length of the source sentence.  This\ncontext set is constructed by a bidirectional recurrent neural network (RNN)\nwhich consists of a forward RNN and reverse RNN. The forward RNN reads the\nsource sentence from the first token until the last one, resulting in the\nforward context vectors $\\left\\{ {\\overrightarrow}{{\\mathbf{{h}}}}_1, \\ldots, {\\overrightarrow}{{\\mathbf{{h}}}}_{T_x}\\right\\}$,\nwhere\n\\vspace{-5px}\n", "index": 1, "text": "\n\\[\n    {\\overrightarrow}{{\\mathbf{{h}}}}_t = {\\overrightarrow}{\\Psi}_{\\text{enc}}\\left( {\\overrightarrow}{{\\mathbf{{h}}}}_{t-1},\n    {\\mathbf{{E}}}_x\\left[x_t\\right] \\right),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\overrightarrow{}}{{\\mathbf{{h}}}}_{t}={\\overrightarrow{}}{\\Psi}_{\\text{enc}}%&#10;\\left({\\overrightarrow{}}{{\\mathbf{{h}}}}_{t-1},{\\mathbf{{E}}}_{x}\\left[x_{t}%&#10;\\right]\\right),\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi/><mo>\u2192</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>=</mo><mrow><mover accent=\"true\"><mi/><mo>\u2192</mo></mover><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mtext>enc</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mover accent=\"true\"><mi/><mo>\u2192</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><mrow><msub><mi>\ud835\udc04</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>x</mi><mi>t</mi></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\n\n${\\overrightarrow}{\\Psi}_{\\text{enc}}$ and ${\\overleftarrow}{\\Psi}_{\\text{enc}}$ are recurrent activation\nfunctions such as long short-term memory units (LSTM, \\cite{hochreiter1997long})\nor gated recurrent units (GRU, \\cite{cho2014learning}). \nAt each position in the source sentence, the forward and reverse context vectors\nare concatenated to form a full context vector, i.e., \n\\vspace{-0.25cm}\n\n", "itemtype": "equation", "pos": 8357, "prevtext": "\nand ${\\mathbf{{E}}}_x \\in {\\mathbb{R}}^{|V_x| \\times d}$ is an embedding matrix containing row\nvectors of the source symbols.  The reverse RNN in an opposite direction,\nresulting in $\\left\\{ {\\overleftarrow}{{\\mathbf{{h}}}}_1,\n\\ldots, {\\overleftarrow}{{\\mathbf{{h}}}}_{T_x}\\right\\}$, where\n", "index": 3, "text": "\n\\[\n    {\\overleftarrow}{{\\mathbf{{h}}}}_t = {\\overleftarrow}{\\Psi}_{\\text{enc}}\\left( {\\overleftarrow}{{\\mathbf{{h}}}}_{t+1},\n    {\\mathbf{{E}}}_x\\left[x_t\\right] \\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\overleftarrow{}}{{\\mathbf{{h}}}}_{t}={\\overleftarrow{}}{\\Psi}_{\\text{enc}}%&#10;\\left({\\overleftarrow{}}{{\\mathbf{{h}}}}_{t+1},{\\mathbf{{E}}}_{x}\\left[x_{t}%&#10;\\right]\\right).\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>=</mo><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mtext>enc</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><mrow><msub><mi>\ud835\udc04</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>x</mi><mi>t</mi></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\n\nThe decoder, which is implemented as an RNN as well, generates one symbol at a\ntime, the translation of the source sentence, based on the context set returned by\nthe encoder. At each time step $t$ in the decoder, a time-dependent context\nvector ${\\mathbf{{c}}}_t$ is computed based on the previous hidden state of the decoder\n${\\mathbf{{z}}}_{t-1}$, the previously decoded symbol $\\tilde{y}_{t-1}$ and the whole\ncontext set $C$. \n\nThis starts by computing the relevance score of each context vector as\n\n", "itemtype": "equation", "pos": 8931, "prevtext": "\n\n${\\overrightarrow}{\\Psi}_{\\text{enc}}$ and ${\\overleftarrow}{\\Psi}_{\\text{enc}}$ are recurrent activation\nfunctions such as long short-term memory units (LSTM, \\cite{hochreiter1997long})\nor gated recurrent units (GRU, \\cite{cho2014learning}). \nAt each position in the source sentence, the forward and reverse context vectors\nare concatenated to form a full context vector, i.e., \n\\vspace{-0.25cm}\n\n", "index": 5, "text": "\\begin{align}\n    \\label{eq:context}\n    {\\mathbf{{h}}}_t = \\left[ {\\overrightarrow}{{\\mathbf{{h}}}}_t; {\\overleftarrow}{{\\mathbf{{h}}}}_t \\right].\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{h}}}_{t}=\\left[{\\overrightarrow{}}{{\\mathbf{{h}}}}_{t};%&#10;{\\overleftarrow{}}{{\\mathbf{{h}}}}_{t}\\right].\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc21</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mrow><mover accent=\"true\"><mi/><mo>\u2192</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>;</mo><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nfor all $i=1,\\ldots, T_x$. ${\\text{f}}_{\\text{score}}$ can be implemented in various\nways \\cite{luong2015effective}, but in this work, we use a simple single-layer\nfeedforward network. This relevance score measures how relevant the $i$-th\ncontext vector of the source sentence is in deciding the next symbol in the\ntranslation.  These relevance scores are further normalized:\n\n", "itemtype": "equation", "pos": 9594, "prevtext": "\n\nThe decoder, which is implemented as an RNN as well, generates one symbol at a\ntime, the translation of the source sentence, based on the context set returned by\nthe encoder. At each time step $t$ in the decoder, a time-dependent context\nvector ${\\mathbf{{c}}}_t$ is computed based on the previous hidden state of the decoder\n${\\mathbf{{z}}}_{t-1}$, the previously decoded symbol $\\tilde{y}_{t-1}$ and the whole\ncontext set $C$. \n\nThis starts by computing the relevance score of each context vector as\n\n", "index": 7, "text": "\\begin{align}\n    \\label{eq:score}\n    e_{t, i} = f_{\\text{score}}({\\mathbf{{h}}}_i, {\\mathbf{{z}}}_{t-1}, {\\mathbf{{E}}}_y\\left[\n    \\tilde{y}_{t-1}\\right]),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle e_{t,i}=f_{\\text{score}}({\\mathbf{{h}}}_{i},{\\mathbf{{z}}}_{t-1}%&#10;,{\\mathbf{{E}}}_{y}\\left[\\tilde{y}_{t-1}\\right]),\" display=\"inline\"><mrow><mrow><msub><mi>e</mi><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>score</mtext></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc21</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><msub><mi>\ud835\udc04</mi><mi>y</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nand we call $\\alpha_{t, i}$ the attention weight.\n\nThe time-dependent context vector ${\\mathbf{{c}}}_t$ is then the weighted sum of the\ncontext vectors with their weights being the attention weights from above:\n\n", "itemtype": "equation", "pos": 10141, "prevtext": "\nfor all $i=1,\\ldots, T_x$. ${\\text{f}}_{\\text{score}}$ can be implemented in various\nways \\cite{luong2015effective}, but in this work, we use a simple single-layer\nfeedforward network. This relevance score measures how relevant the $i$-th\ncontext vector of the source sentence is in deciding the next symbol in the\ntranslation.  These relevance scores are further normalized:\n\n", "index": 9, "text": "\\begin{align}\n\t\\label{eq:normalize}\n    \\alpha_{t, i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T_x} \\exp(e_{t, j})},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\alpha_{t,i}=\\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T_{x}}\\exp(e_{t,j})},\" display=\"inline\"><mrow><mrow><msub><mi>\u03b1</mi><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></msubsup><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\n\nWith this time-dependent context vector ${\\mathbf{{c}}}_t$, the previous hidden state\n${\\mathbf{{z}}}_{t-1}$ and the previously decoded symbol $\\tilde{y}_{t-1}$, the decoder's\nhidden state is updated by\n\n", "itemtype": "equation", "pos": 10475, "prevtext": "\nand we call $\\alpha_{t, i}$ the attention weight.\n\nThe time-dependent context vector ${\\mathbf{{c}}}_t$ is then the weighted sum of the\ncontext vectors with their weights being the attention weights from above:\n\n", "index": 11, "text": "\\begin{align}\n\t\\label{eq:td_context}\n    {\\mathbf{{c}}}_t = \\sum_{i=1}^{T_x} \\alpha_{t, i} {\\mathbf{{h}}}_i.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{c}}}_{t}=\\sum_{i=1}^{T_{x}}\\alpha_{t,i}{\\mathbf{{h}}}_{%&#10;i}.\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover></mstyle><mrow><msub><mi>\u03b1</mi><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhere $\\Psi_{\\text{dec}}$ is a recurrent activation function.\n\nThe initial hidden state ${\\mathbf{{z}}}_0$ of the decoder is initialized based on the\nlast hidden state of the reverse RNN:\n\\vspace{-5px}\n\n", "itemtype": "equation", "pos": 10800, "prevtext": "\n\nWith this time-dependent context vector ${\\mathbf{{c}}}_t$, the previous hidden state\n${\\mathbf{{z}}}_{t-1}$ and the previously decoded symbol $\\tilde{y}_{t-1}$, the decoder's\nhidden state is updated by\n\n", "index": 13, "text": "\\begin{align}\n\t\\label{eq:decoder}\n    {\\mathbf{{z}}}_t = \\Psi_{\\text{dec}}\\left( {\\mathbf{{z}}}_{t-1}, {\\mathbf{{E}}}_y\\left[ \\tilde{y}_{t-1}\n    \\right], {\\mathbf{{c}}}_t \\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{z}}}_{t}=\\Psi_{\\text{dec}}\\left({\\mathbf{{z}}}_{t-1},{%&#10;\\mathbf{{E}}}_{y}\\left[\\tilde{y}_{t-1}\\right],{\\mathbf{{c}}}_{t}\\right),\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mtext>dec</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><msub><mi>\ud835\udc04</mi><mi>y</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo>,</mo><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\n\n\n\n\n\n\\noindent where $f_{\\text{init}}$ is a feedforward network with one or two hidden layers.\n\nThe probability distribution for the next target symbol is computed by\n\n", "itemtype": "equation", "pos": 11195, "prevtext": "\nwhere $\\Psi_{\\text{dec}}$ is a recurrent activation function.\n\nThe initial hidden state ${\\mathbf{{z}}}_0$ of the decoder is initialized based on the\nlast hidden state of the reverse RNN:\n\\vspace{-5px}\n\n", "index": 15, "text": "\\begin{align}\n    \\label{eq:init}\n{\\mathbf{{z}}}_0 = f_{\\text{init}}\\left( {\\overleftarrow}{{\\mathbf{{h}}}}_{T_x} \\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{z}}}_{0}=f_{\\text{init}}\\left({\\overleftarrow{}}{{%&#10;\\mathbf{{h}}}}_{T_{x}}\\right),\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc33</mi><mn>0</mn></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>init</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><msub><mi>T</mi><mi>x</mi></msub></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhere $g_k$ is a parametric function that returns the unnormalized probability\nfor the next target symbol being $k$.\n\nTraining this attention-based model is done by maximizing the conditional log-likelihood\n\n", "itemtype": "equation", "pos": 11497, "prevtext": "\n\n\n\n\n\n\\noindent where $f_{\\text{init}}$ is a feedforward network with one or two hidden layers.\n\nThe probability distribution for the next target symbol is computed by\n\n", "index": 17, "text": "\\begin{align}\n    \\label{eq:target_prob}\n    p(y_t = k|\\tilde{y}_{< t}, X) \\propto\n    e^{g_k({\\mathbf{{z}}}_t, {\\mathbf{{c}}}_t, {\\mathbf{{E}}}\\left[ \\tilde{y}_{t-1} \\right])},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(y_{t}=k|\\tilde{y}_{&lt;t},X)\\propto e^{g_{k}({\\mathbf{{z}}}_{t},{%&#10;\\mathbf{{c}}}_{t},{\\mathbf{{E}}}\\left[\\tilde{y}_{t-1}\\right])},\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>k</mi><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><msup><mi>e</mi><mrow><msub><mi>g</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo>,</mo><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhere the log probability inside the inner summation is from\nEq.~\\eqref{eq:target_prob}. It is important to note that the ground-truth target\nsymbols $y_t^{(n)}$ are used during training. The entire model is differentiable,\nand the gradient of the log-likelihood function with respect to all the\nparameters ${\\boldsymbol{{\\theta}}}$ can be computed efficiently by backpropagation. This makes it\nstraightforward to use stochastic gradient descent or its variants to train the\nwhole model jointly to maximize the translation performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Multi-Way, Multilingual Translation}\n\nIn this section, we discuss issues and our solutions in extending the\nconventional {\\em single-pair} attention-based neural machine translation into\n{\\em multi-way, multilingual} model.\n\n\\paragraph{Problem Definition}\n\nWe assume $N > 1$ source languages $\\left\\{ X^1, X^2, \\ldots, X^N \\right\\}$ and\n$M > 1$ target languages $\\left\\{ Y^1, Y^2, \\ldots, Y^M\\right\\}$ , and the\navailability of $L \\leq M \\times N$ {\\em bilingual} parallel corpora $\\left\\{\nD_1, \\ldots, D_L \\right\\}$, each of which is a set of sentence pairs of one\nsource and one target languages. We use $s(D_l)$ and $t(D_l)$ to indicate the\nsource and target languages of the $l$-th parallel corpus.\n\nFor each parallel corpus $l$, we can directly use the log-likelihood function\nfrom Eq.~\\eqref{eq:target_prob} to define a pair-specific log-likelihood\n${\\mathcal{L}}^{s(D_l), t(D_l)}$.  Then, the goal of multi-way, multilingual neural\nmachine translation is to build a model that maximizes the joint log-likelihood\nfunction\n$\n{{\\mathcal{L}}({\\boldsymbol{{\\theta}}}) = \\frac{1}{L} \\sum_{l=1}^L {\\mathcal{L}}^{s(D_l), t(D_l)}({\\boldsymbol{{\\theta}}}).}\n    $\nOnce the training is over, the model can do translation from any of the source\nlanguages to any of the target languages\nincluded in the parallel corpora.\n\n\\subsection{Existing Approaches}\n\n\n\n\n\\paragraph{Neural Machine Translation without Attention}\n\nIn \\cite{luong2015multi}, the authors extended the basic encoder-decoder network\nfor multitask neural machine translation. As they extended the basic\nencoder-decoder network, their model effectively becomes a set of encoders and\ndecoders, where each of the encoder projects a source sentence into a common\nvector space. The point in the common space is then decoded into different\nlanguages.\n\n\n\n\n\nThe major difference between \\cite{luong2015multi} and our work is that we\nextend the attention-based encoder-decoder instead of the basic model.\nThis is an important contribution, as the attention-based neural machine\ntranslation has become {\\em de facto} standard in neural translation literatures\nrecently\n\\cite{jean2014using,jean2015WMT,luong2015effective,sennrich2015neural,sennrich2015improving},\nby opposition to the basic encoder-decoder. \n\nThere are two minor differences as well. First, they do not consider\nmultilinguality in depth. The authors of \\cite{luong2015multi} tried only a\nsingle language pair, English and German, in their model. Second, they only\nreport translation perplexity, which is not a widely used\nmetric for measuring translation quality. To more easily compare with\nother machine translation approaches it would be important to evaluate\nmetrics such as BLEU, which counts the number of matched\n$n$-grams between the generated and reference translations.\n\n\\paragraph{One-to-Many Neural Machine Translation}\n\nThe authors of \\cite{dong2015multi} earlier proposed a multilingual translation\nmodel based on the {\\em attention-based neural machine translation}. \n\nUnlike this paper, they only tried it on one-to-many translation, similarly to\nearlier work by \\cite{collobert2011natural} where one-to-many natural language\nprocessing was done. In this setting, it is trivial to extend the single-pair\nattention-based model into multilingual translation by simply having a single\nencoder for a source language and pairs of a decoder and attention mechanism\n(Eq.~\\eqref{eq:score}) for each target language. We will shortly discuss  more\non why, with the attention mechanism,\none-to-many translation is trivial, while multi-way translation is not.\n\n\n\\subsection{Challenges}\n\nA quick look at neural machine translation seems to suggest a straightforward\npath toward incorporating multiple languages in both source and target sides. As\ndescribed earlier already in the introduction, the basic idea is simple. We\nassign a separate encoder to each source language and a separate decoder to each\ntarget language. The encoder will project a source sentence in its own language\ninto a common, language-agnostic space, from which the decoder will generate a\ntranslation in its own language. \n\nUnlike training multiple single-pair neural translation models, in this case,\nthe encoders and decoders are shared across multiple pairs. This is\ncomputationally beneficial, as the number of parameters grows only linearly with\nrespect to the number of languages ($O(L)$), in contrary to training separate\nsingle-pair models, in which case the number of parameters grows quadratically\n($O(L^2)$.) \n\n\n\n\n\nThe attention mechanism, which was initially called\na soft-alignment model in \\cite{bahdanau2014neural}, aligns a (potentially\nnon-contiguous) source phrase to a target word. This alignment process is\nlargely specific to a language pair, and it is not clear whether an alignment\nmechanism for one language pair can also work for another pair.\n\n\n\nThe most naive solution to this issue is to have $O(L^2)$ attention mechanisms\nthat are {\\em not} shared across multiple language pairs. Each attention\nmechanism takes care of a single pair of source and target languages. This is\nthe approach employed in \\cite{dong2015multi}, where each decoder had its own\nattention mechanism.\n\n\nThere are two issues with this naive approach. First, unlike what has been hoped\ninitially with multilingual neural machine translation, the number of parameters\nagain grows quadratically w.r.t. the number of languages. Second and\nmore importantly, having separate attention mechanisms makes it less\nlikely for the model to fully benefit from having multiple tasks \\cite{caruana1997multitask},\nespecially for transfer learning towards resource-poor languages.\n\n\nIn short, the major challenge in building a multi-way, multilingual neural\nmachine translation is in avoiding independent (i.e., quadratically many) attention\nmechanisms. There are two questions behind this challenge.\nThe first one is whether it is even possible to share a single attention\nmechanism across multiple language pairs. \n\n\nThe second question immediately follows: how can we build a\nneural translation model to share a single attention mechanism for all the\nlanguage pairs in consideration?\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.8\\columnwidth]{shared_att_small.png}\n\\caption{One step of the proposed multi-way. multilingual Neural Machine\n    Translation model, for the $n$-th encoder and the $m$-th decoder at time\nstep $t$. See Sec.~\\ref{sec:mlnmt} for details.}\n\n\n\n\n\\label{fig:shared_att}\n\n\n    \\vspace{-4mm}\n\\end{figure}\n\n\\section{Multi-Way, Multilingual Model}\n\\label{sec:mlnmt}\n\nWe describe in this section a proposed {\\em multi-way, multilingual \nattention-based neural machine translation}.  The proposed model consists of $N$\nencoders $\\{ \\Psi_{\\text{enc}}^n \\}_{n=1}^N$ (see Eq.~\\eqref{eq:context}), $M$\ndecoders $\\{ (\\Psi_{\\text{dec}}^m, g^m, f_{\\text{init}}^m) \\}_{m=1}^M$ (see\nEqs.~\\eqref{eq:decoder}--\\eqref{eq:target_prob}) and a shared attention\nmechanism $f_{\\text{score}}$ (see Eq.~\\eqref{eq:score} in the single language pair case).\n\n\\paragraph{Encoders}\n\nSimilarly to \\cite{luong2015effective}, we have one encoder per source language,\nmeaning that a single encoder is shared for translating the language to multiple\ntarget languages. In order to handle different source languages better, we may\nuse for each source language a different type of encoder, for instance, of\ndifferent size (in terms of the number of recurrent units) or of different\narchitecture (convolutional instead of recurrent.) This allows\nus to efficiently incorporate varying types of languages in the proposed\nmultilingual translation model.\n\nThis however implies that the dimensionality of the context vectors in\nEq.~\\eqref{eq:context} may differ across source languages. Therefore, we add to\nthe original bidirectional encoder from Sec.~\\ref{sec2}, a linear transformation\nlayer consisting of a weight matrix ${\\mathbf{{W}}}^n_{\\text{adp}}$ and a bias vector\n${\\mathbf{{b}}}^n_{\\text{adp}}$, which is used to project each context vector into a\ncommon dimensional space:\n\\vspace*{-3px}\n\n", "itemtype": "equation", "pos": 11894, "prevtext": "\nwhere $g_k$ is a parametric function that returns the unnormalized probability\nfor the next target symbol being $k$.\n\nTraining this attention-based model is done by maximizing the conditional log-likelihood\n\n", "index": 19, "text": "\\begin{align*}\n    {\\mathcal{L}}({\\boldsymbol{{\\theta}}}) = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_y} \\log p(y_t = y_t^{(n)} |\n    y_{<t}^{(n)}, X^{(n)}),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{L}}({\\boldsymbol{{\\theta}}})=\\frac{1}{N}\\sum_{n=1}^{N}%&#10;\\sum_{t=1}^{T_{y}}\\log p(y_{t}=y_{t}^{(n)}|y_{&lt;t}^{(n)},X^{(n)}),\" display=\"inline\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>y</mi></msub></munderover></mstyle><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msup><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhere ${\\mathbf{{W}}}^n_{\\text{adp}} \\in {\\mathbb{R}}^{d \\times (\\text{dim}{{\\overrightarrow}{{\\mathbf{{h}}}}_t} +\n\\text{dim}{{\\overleftarrow}{{\\mathbf{{h}}}}_t})}$ and ${\\mathbf{{b}}}_{\\text{adp}}^n \\in {\\mathbb{R}}^d$.\n\nIn addition, each encoder exposes two transformation functions\n$\\phi_{\\text{att}}^n$ and $\\phi_{\\text{init}}^n$. The first transformer\n$\\phi_{\\text{att}}^n$ transforms a context vector to be compatible with a shared\nattention mechanism:\n\\vspace*{-3px}\n\n", "itemtype": "equation", "pos": 20709, "prevtext": "\nwhere the log probability inside the inner summation is from\nEq.~\\eqref{eq:target_prob}. It is important to note that the ground-truth target\nsymbols $y_t^{(n)}$ are used during training. The entire model is differentiable,\nand the gradient of the log-likelihood function with respect to all the\nparameters ${\\boldsymbol{{\\theta}}}$ can be computed efficiently by backpropagation. This makes it\nstraightforward to use stochastic gradient descent or its variants to train the\nwhole model jointly to maximize the translation performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Multi-Way, Multilingual Translation}\n\nIn this section, we discuss issues and our solutions in extending the\nconventional {\\em single-pair} attention-based neural machine translation into\n{\\em multi-way, multilingual} model.\n\n\\paragraph{Problem Definition}\n\nWe assume $N > 1$ source languages $\\left\\{ X^1, X^2, \\ldots, X^N \\right\\}$ and\n$M > 1$ target languages $\\left\\{ Y^1, Y^2, \\ldots, Y^M\\right\\}$ , and the\navailability of $L \\leq M \\times N$ {\\em bilingual} parallel corpora $\\left\\{\nD_1, \\ldots, D_L \\right\\}$, each of which is a set of sentence pairs of one\nsource and one target languages. We use $s(D_l)$ and $t(D_l)$ to indicate the\nsource and target languages of the $l$-th parallel corpus.\n\nFor each parallel corpus $l$, we can directly use the log-likelihood function\nfrom Eq.~\\eqref{eq:target_prob} to define a pair-specific log-likelihood\n${\\mathcal{L}}^{s(D_l), t(D_l)}$.  Then, the goal of multi-way, multilingual neural\nmachine translation is to build a model that maximizes the joint log-likelihood\nfunction\n$\n{{\\mathcal{L}}({\\boldsymbol{{\\theta}}}) = \\frac{1}{L} \\sum_{l=1}^L {\\mathcal{L}}^{s(D_l), t(D_l)}({\\boldsymbol{{\\theta}}}).}\n    $\nOnce the training is over, the model can do translation from any of the source\nlanguages to any of the target languages\nincluded in the parallel corpora.\n\n\\subsection{Existing Approaches}\n\n\n\n\n\\paragraph{Neural Machine Translation without Attention}\n\nIn \\cite{luong2015multi}, the authors extended the basic encoder-decoder network\nfor multitask neural machine translation. As they extended the basic\nencoder-decoder network, their model effectively becomes a set of encoders and\ndecoders, where each of the encoder projects a source sentence into a common\nvector space. The point in the common space is then decoded into different\nlanguages.\n\n\n\n\n\nThe major difference between \\cite{luong2015multi} and our work is that we\nextend the attention-based encoder-decoder instead of the basic model.\nThis is an important contribution, as the attention-based neural machine\ntranslation has become {\\em de facto} standard in neural translation literatures\nrecently\n\\cite{jean2014using,jean2015WMT,luong2015effective,sennrich2015neural,sennrich2015improving},\nby opposition to the basic encoder-decoder. \n\nThere are two minor differences as well. First, they do not consider\nmultilinguality in depth. The authors of \\cite{luong2015multi} tried only a\nsingle language pair, English and German, in their model. Second, they only\nreport translation perplexity, which is not a widely used\nmetric for measuring translation quality. To more easily compare with\nother machine translation approaches it would be important to evaluate\nmetrics such as BLEU, which counts the number of matched\n$n$-grams between the generated and reference translations.\n\n\\paragraph{One-to-Many Neural Machine Translation}\n\nThe authors of \\cite{dong2015multi} earlier proposed a multilingual translation\nmodel based on the {\\em attention-based neural machine translation}. \n\nUnlike this paper, they only tried it on one-to-many translation, similarly to\nearlier work by \\cite{collobert2011natural} where one-to-many natural language\nprocessing was done. In this setting, it is trivial to extend the single-pair\nattention-based model into multilingual translation by simply having a single\nencoder for a source language and pairs of a decoder and attention mechanism\n(Eq.~\\eqref{eq:score}) for each target language. We will shortly discuss  more\non why, with the attention mechanism,\none-to-many translation is trivial, while multi-way translation is not.\n\n\n\\subsection{Challenges}\n\nA quick look at neural machine translation seems to suggest a straightforward\npath toward incorporating multiple languages in both source and target sides. As\ndescribed earlier already in the introduction, the basic idea is simple. We\nassign a separate encoder to each source language and a separate decoder to each\ntarget language. The encoder will project a source sentence in its own language\ninto a common, language-agnostic space, from which the decoder will generate a\ntranslation in its own language. \n\nUnlike training multiple single-pair neural translation models, in this case,\nthe encoders and decoders are shared across multiple pairs. This is\ncomputationally beneficial, as the number of parameters grows only linearly with\nrespect to the number of languages ($O(L)$), in contrary to training separate\nsingle-pair models, in which case the number of parameters grows quadratically\n($O(L^2)$.) \n\n\n\n\n\nThe attention mechanism, which was initially called\na soft-alignment model in \\cite{bahdanau2014neural}, aligns a (potentially\nnon-contiguous) source phrase to a target word. This alignment process is\nlargely specific to a language pair, and it is not clear whether an alignment\nmechanism for one language pair can also work for another pair.\n\n\n\nThe most naive solution to this issue is to have $O(L^2)$ attention mechanisms\nthat are {\\em not} shared across multiple language pairs. Each attention\nmechanism takes care of a single pair of source and target languages. This is\nthe approach employed in \\cite{dong2015multi}, where each decoder had its own\nattention mechanism.\n\n\nThere are two issues with this naive approach. First, unlike what has been hoped\ninitially with multilingual neural machine translation, the number of parameters\nagain grows quadratically w.r.t. the number of languages. Second and\nmore importantly, having separate attention mechanisms makes it less\nlikely for the model to fully benefit from having multiple tasks \\cite{caruana1997multitask},\nespecially for transfer learning towards resource-poor languages.\n\n\nIn short, the major challenge in building a multi-way, multilingual neural\nmachine translation is in avoiding independent (i.e., quadratically many) attention\nmechanisms. There are two questions behind this challenge.\nThe first one is whether it is even possible to share a single attention\nmechanism across multiple language pairs. \n\n\nThe second question immediately follows: how can we build a\nneural translation model to share a single attention mechanism for all the\nlanguage pairs in consideration?\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.8\\columnwidth]{shared_att_small.png}\n\\caption{One step of the proposed multi-way. multilingual Neural Machine\n    Translation model, for the $n$-th encoder and the $m$-th decoder at time\nstep $t$. See Sec.~\\ref{sec:mlnmt} for details.}\n\n\n\n\n\\label{fig:shared_att}\n\n\n    \\vspace{-4mm}\n\\end{figure}\n\n\\section{Multi-Way, Multilingual Model}\n\\label{sec:mlnmt}\n\nWe describe in this section a proposed {\\em multi-way, multilingual \nattention-based neural machine translation}.  The proposed model consists of $N$\nencoders $\\{ \\Psi_{\\text{enc}}^n \\}_{n=1}^N$ (see Eq.~\\eqref{eq:context}), $M$\ndecoders $\\{ (\\Psi_{\\text{dec}}^m, g^m, f_{\\text{init}}^m) \\}_{m=1}^M$ (see\nEqs.~\\eqref{eq:decoder}--\\eqref{eq:target_prob}) and a shared attention\nmechanism $f_{\\text{score}}$ (see Eq.~\\eqref{eq:score} in the single language pair case).\n\n\\paragraph{Encoders}\n\nSimilarly to \\cite{luong2015effective}, we have one encoder per source language,\nmeaning that a single encoder is shared for translating the language to multiple\ntarget languages. In order to handle different source languages better, we may\nuse for each source language a different type of encoder, for instance, of\ndifferent size (in terms of the number of recurrent units) or of different\narchitecture (convolutional instead of recurrent.) This allows\nus to efficiently incorporate varying types of languages in the proposed\nmultilingual translation model.\n\nThis however implies that the dimensionality of the context vectors in\nEq.~\\eqref{eq:context} may differ across source languages. Therefore, we add to\nthe original bidirectional encoder from Sec.~\\ref{sec2}, a linear transformation\nlayer consisting of a weight matrix ${\\mathbf{{W}}}^n_{\\text{adp}}$ and a bias vector\n${\\mathbf{{b}}}^n_{\\text{adp}}$, which is used to project each context vector into a\ncommon dimensional space:\n\\vspace*{-3px}\n\n", "index": 21, "text": "\\begin{align}\n    \\label{eq:ml_context}\n    {\\mathbf{{h}}}^n_t = {\\mathbf{{W}}}^n_{\\text{adp}} \\left[ {\\overrightarrow}{{\\mathbf{{h}}}}_t; {\\overleftarrow}{{\\mathbf{{h}}}}_t \\right] +\n    {\\mathbf{{b}}}_{\\text{adp}}^n,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{h}}}^{n}_{t}={\\mathbf{{W}}}^{n}_{\\text{adp}}\\left[{%&#10;\\overrightarrow{}}{{\\mathbf{{h}}}}_{t};{\\overleftarrow{}}{{\\mathbf{{h}}}}_{t}%&#10;\\right]+{\\mathbf{{b}}}_{\\text{adp}}^{n},\" display=\"inline\"><mrow><mrow><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>n</mi></msubsup><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc16</mi><mtext>adp</mtext><mi>n</mi></msubsup><mo>\u2062</mo><mrow><mo>[</mo><mrow><mover accent=\"true\"><mi/><mo>\u2192</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>;</mo><mrow><mover accent=\"true\"><mi/><mo>\u2190</mo></mover><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>]</mo></mrow></mrow><mo>+</mo><msubsup><mi>\ud835\udc1b</mi><mtext>adp</mtext><mi>n</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nThis transformer can be implemented as any type of parametric function, and in\nthis paper, we simply apply an element-wise $\\tanh$ to ${\\mathbf{{h}}}_t^n$. \n\nThe second transformer $\\phi_{\\text{init}}^n$ transforms the first\n\ncontext vector\n${\\mathbf{{h}}}_1^n$ to be compatible with the initializer of the decoder's hidden\nstate (see Eq.~\\eqref{eq:init}): \n\\vspace*{-3px}\n\n", "itemtype": "equation", "pos": 21414, "prevtext": "\nwhere ${\\mathbf{{W}}}^n_{\\text{adp}} \\in {\\mathbb{R}}^{d \\times (\\text{dim}{{\\overrightarrow}{{\\mathbf{{h}}}}_t} +\n\\text{dim}{{\\overleftarrow}{{\\mathbf{{h}}}}_t})}$ and ${\\mathbf{{b}}}_{\\text{adp}}^n \\in {\\mathbb{R}}^d$.\n\nIn addition, each encoder exposes two transformation functions\n$\\phi_{\\text{att}}^n$ and $\\phi_{\\text{init}}^n$. The first transformer\n$\\phi_{\\text{att}}^n$ transforms a context vector to be compatible with a shared\nattention mechanism:\n\\vspace*{-3px}\n\n", "index": 23, "text": "\\begin{align}\n    \\label{eq:enc_att}\n    \\tilde{{\\mathbf{{h}}}}_t^n = \\phi_{\\text{att}}^n({\\mathbf{{h}}}_t^n).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{{\\mathbf{{h}}}}_{t}^{n}=\\phi_{\\text{att}}^{n}({\\mathbf{{h}%&#10;}}_{t}^{n}).\" display=\"inline\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc21</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi><mi>n</mi></msubsup><mo>=</mo><mrow><msubsup><mi>\u03d5</mi><mtext>att</mtext><mi>n</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nSimilarly to $\\phi_{\\text{att}}^n$, it can be implemented as any type of\nparametric function. In this paper, we use a feedforward network with a single\nhidden layer and share one network $\\phi_{\\text{init}}$ for all encoder-decoder pairs, like attention mechanism.\n\n\\begin{table}[t]\n    \\small\n    \\centering\n    \\begin{tabular}{c || c c| c}\n        & \\multicolumn{2}{c|}{\\# Symbols} & \\# Sentence \\\\\n        & \\# En   & Other& Pairs \\\\\n        \\hline\n        \\hline\nEn-Fr & 1.022b & 2.213b & 38.85m \\\\  \nEn-Cs & 186.57m & 185.58m & 12.12m   \\\\ \nEn-Ru & 50.62m & 55.76m & 2.32m   \\\\ \nEn-De & 111.77m & 117.41m & 4.15m    \\\\ \nEn-Fi & 52.76m & 43.67m & 2.03m  \\\\ \n    \\end{tabular}\n    \\caption{Statistics of the parallel corpora from WMT'15. Symbols are\n        BPE-based sub-words.}\n    \\label{tab:stats}\n\n\n    \\vspace{-4mm}\n\\end{table}\n\n\\paragraph{Decoders}\n\nWe first start with an initialization of the decoder's hidden state. Each\ndecoder has its own parametric function $\\varphi_{\\text{init}}^m$ that maps the\nlast context vector $\\hat{{\\mathbf{{h}}}}_{T_x}^n$ of the source encoder from\nEq.~\\eqref{eq:enc_init} into the initial hidden state:\n\n", "itemtype": "equation", "pos": 21910, "prevtext": "\nThis transformer can be implemented as any type of parametric function, and in\nthis paper, we simply apply an element-wise $\\tanh$ to ${\\mathbf{{h}}}_t^n$. \n\nThe second transformer $\\phi_{\\text{init}}^n$ transforms the first\n\ncontext vector\n${\\mathbf{{h}}}_1^n$ to be compatible with the initializer of the decoder's hidden\nstate (see Eq.~\\eqref{eq:init}): \n\\vspace*{-3px}\n\n", "index": 25, "text": "\\begin{align}\n    \\label{eq:enc_init}\n    \\hat{{\\mathbf{{h}}}}_1^n = \\phi_{\\text{init}}^n({\\mathbf{{h}}}_1^n).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathbf{{h}}}}_{1}^{n}=\\phi_{\\text{init}}^{n}({\\mathbf{{h}}%&#10;}_{1}^{n}).\" display=\"inline\"><mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc21</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn><mi>n</mi></msubsup><mo>=</mo><mrow><msubsup><mi>\u03d5</mi><mtext>init</mtext><mi>n</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mn>1</mn><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\n$\\varphi_{\\text{init}}^m$ can be any parametric function, and in this paper, we\nused a feedforward network with a single $\\tanh$ hidden layer.\n\nEach decoder exposes a parametric function $\\varphi_{\\text{att}}^m$ that\ntransforms its hidden state and the previously decoded symbol to be compatible\nwith a shared attention mechanism. This transformer is a parametric function\nthat takes as input the previous hidden state ${\\mathbf{{z}}}_{t-1}^m$ and the previous\nsymbol $\\tilde{y}_{t-1}^m$ and returns a vector for the attention mechanism:\n\n", "itemtype": "equation", "pos": 23180, "prevtext": "\nSimilarly to $\\phi_{\\text{att}}^n$, it can be implemented as any type of\nparametric function. In this paper, we use a feedforward network with a single\nhidden layer and share one network $\\phi_{\\text{init}}$ for all encoder-decoder pairs, like attention mechanism.\n\n\\begin{table}[t]\n    \\small\n    \\centering\n    \\begin{tabular}{c || c c| c}\n        & \\multicolumn{2}{c|}{\\# Symbols} & \\# Sentence \\\\\n        & \\# En   & Other& Pairs \\\\\n        \\hline\n        \\hline\nEn-Fr & 1.022b & 2.213b & 38.85m \\\\  \nEn-Cs & 186.57m & 185.58m & 12.12m   \\\\ \nEn-Ru & 50.62m & 55.76m & 2.32m   \\\\ \nEn-De & 111.77m & 117.41m & 4.15m    \\\\ \nEn-Fi & 52.76m & 43.67m & 2.03m  \\\\ \n    \\end{tabular}\n    \\caption{Statistics of the parallel corpora from WMT'15. Symbols are\n        BPE-based sub-words.}\n    \\label{tab:stats}\n\n\n    \\vspace{-4mm}\n\\end{table}\n\n\\paragraph{Decoders}\n\nWe first start with an initialization of the decoder's hidden state. Each\ndecoder has its own parametric function $\\varphi_{\\text{init}}^m$ that maps the\nlast context vector $\\hat{{\\mathbf{{h}}}}_{T_x}^n$ of the source encoder from\nEq.~\\eqref{eq:enc_init} into the initial hidden state:\n\n", "index": 27, "text": "\\begin{align*}\n    {\\mathbf{{z}}}_0^m = \\varphi_{\\text{init}}^m(\\hat{{\\mathbf{{h}}}}_{T_x}^n) =\n    \\varphi_{\\text{init}}^m(\\phi_{\\text{init}}^n({\\mathbf{{h}}}_{1}^n))\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{z}}}_{0}^{m}=\\varphi_{\\text{init}}^{m}(\\hat{{\\mathbf{{h%&#10;}}}}_{T_{x}}^{n})=\\varphi_{\\text{init}}^{m}(\\phi_{\\text{init}}^{n}({\\mathbf{{h%&#10;}}}_{1}^{n}))\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc33</mi><mn>0</mn><mi>m</mi></msubsup><mo>=</mo><mrow><msubsup><mi>\u03c6</mi><mtext>init</mtext><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc21</mi><mo stretchy=\"false\">^</mo></mover><msub><mi>T</mi><mi>x</mi></msub><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\u03c6</mi><mtext>init</mtext><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03d5</mi><mtext>init</mtext><mi>n</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mn>1</mn><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhich replaces ${\\mathbf{{z}}}_{t-1}$ in Eq.~\\ref{eq:score}.\nIn this paper, we use a feedforward network with a single $\\tanh$ hidden layer\nfor each $\\varphi_{\\text{att}}^m$.\n\nGiven the previous hidden state ${\\mathbf{{z}}}_{t-1}^m$, previously decoded symbol\n$\\tilde{y}_{t-1}^m$ and the time-dependent context vector ${\\mathbf{{c}}}_t^m$, which we\nwill discuss shortly, the decoder updates its hidden state:\n\n", "itemtype": "equation", "pos": 23899, "prevtext": "\n$\\varphi_{\\text{init}}^m$ can be any parametric function, and in this paper, we\nused a feedforward network with a single $\\tanh$ hidden layer.\n\nEach decoder exposes a parametric function $\\varphi_{\\text{att}}^m$ that\ntransforms its hidden state and the previously decoded symbol to be compatible\nwith a shared attention mechanism. This transformer is a parametric function\nthat takes as input the previous hidden state ${\\mathbf{{z}}}_{t-1}^m$ and the previous\nsymbol $\\tilde{y}_{t-1}^m$ and returns a vector for the attention mechanism:\n\n", "index": 29, "text": "\\begin{align}\n    \\label{eq:dec_att}\n    \\tilde{{\\mathbf{{z}}}}_{t-1}^m = \\varphi_{\\text{att}}^m\\left( {\\mathbf{{z}}}_{t-1}^m, {\\mathbf{{E}}}_y^m \\left[\n            \\tilde{y}_{t-1}^m\n    \\right]\\right)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{{\\mathbf{{z}}}}_{t-1}^{m}=\\varphi_{\\text{att}}^{m}\\left({%&#10;\\mathbf{{z}}}_{t-1}^{m},{\\mathbf{{E}}}_{y}^{m}\\left[\\tilde{y}_{t-1}^{m}\\right]\\right)\" display=\"inline\"><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc33</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>=</mo><mrow><msubsup><mi>\u03c6</mi><mtext>att</mtext><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>,</mo><mrow><msubsup><mi>\ud835\udc04</mi><mi>y</mi><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nwhere $f_{\\text{adp}}^m$ affine-transforms the time-dependent context vector to be of\nthe same dimensionality as the decoder. We share a single\naffine-transformation layer $f_{adp}^m$, for all the decoders in this paper. \n\nOnce the hidden state is updated, the probability distribution over the next\nsymbol is computed exactly as for the pair-specific model (see\nEq.~\\eqref{eq:target_prob}.)\n\n\n\\paragraph{Attention Mechanism}\n\nUnlike the encoders and decoders of which there is an instance for each language,\nthere is only a single attention mechanism,\n\nshared across all the language pairs. \n\n\nThis shared mechanism uses the {\\em\nattention-specific} vectors $\\tilde{{\\mathbf{{h}}}}_t^n$ and $\\tilde{{\\mathbf{{z}}}}_{t-1}^m$ from the\nencoder and decoder, respectively. \n\nThe relevance score of each context vector ${\\mathbf{{h}}}_t^n$ is computed based on the\ndecoder's previous hidden state ${\\mathbf{{z}}}_{t-1}^m$ and previous symbol\n$\\tilde{y}^m_{t-1}$:\n\n", "itemtype": "equation", "pos": 24522, "prevtext": "\nwhich replaces ${\\mathbf{{z}}}_{t-1}$ in Eq.~\\ref{eq:score}.\nIn this paper, we use a feedforward network with a single $\\tanh$ hidden layer\nfor each $\\varphi_{\\text{att}}^m$.\n\nGiven the previous hidden state ${\\mathbf{{z}}}_{t-1}^m$, previously decoded symbol\n$\\tilde{y}_{t-1}^m$ and the time-dependent context vector ${\\mathbf{{c}}}_t^m$, which we\nwill discuss shortly, the decoder updates its hidden state:\n\n", "index": 31, "text": "\\begin{align*}\n    {\\mathbf{{z}}}_t = \\Psi_{\\text{dec}}\\left( {\\mathbf{{z}}}_{t-1}^m, {\\mathbf{{E}}}_y^m\\left[ \\tilde{y}^m_{t-1}\n    \\right], f_{\\text{adp}}^m({\\mathbf{{c}}}_t^m) \\right),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{z}}}_{t}=\\Psi_{\\text{dec}}\\left({\\mathbf{{z}}}_{t-1}^{m%&#10;},{\\mathbf{{E}}}_{y}^{m}\\left[\\tilde{y}^{m}_{t-1}\\right],f_{\\text{adp}}^{m}({%&#10;\\mathbf{{c}}}_{t}^{m})\\right),\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mtext>dec</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>,</mo><mrow><msubsup><mi>\ud835\udc04</mi><mi>y</mi><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>]</mo></mrow></mrow><mo>,</mo><mrow><msubsup><mi>f</mi><mtext>adp</mtext><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc1c</mi><mi>t</mi><mi>m</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01073.tex", "nexttext": "\nThese scores are normalized according to Eq.~\\eqref{eq:normalize} to become the\nattention weights $\\alpha_{t, i}^{m, n}$.\n\nWith these attention weights, the time-dependent context vector is computed as\nthe weighted sum of the {\\em original} context vectors:\n$\n    {\\mathbf{{c}}}_t^{m, n} = \\sum_{i=1}^{T_x} \\alpha_{t, i}^{m, n} {\\mathbf{{h}}}_{i}^n.\n    $\n\nSee Fig.~\\ref{fig:shared_att} for the illustration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n    \\small\n    \\centering\n    \\begin{tabular}{c c c c c }\n        & Size & Single & Single+DF & Multi \\\\\n        \\hline\n        \\hline\n        \n        \\multirow{4}{*}{\\rotatebox[origin=c]{90}{En$\\to$Fi}} \n        & 100k & 5.06/3.96 & 4.98/3.99  & 6.2/{\\bf 5.17}   \\\\\n        & 200k & 7.1/6.16 & 7.21/6.17  & 8.84/{\\bf 7.53}   \\\\\n        & 400k & 9.11/7.85 & 9.31/8.18  & 11.09/{\\bf 9.98}   \\\\\n        & 800k & 11.08/9.96 & 11.59/10.15 & 12.73/{\\bf 11.28}  \\\\\n        \\hline\n        \n        \\multirow{4}{*}{\\rotatebox[origin=c]{90}{De$\\to$En}} \n        & 210k  &   14.27/13.2  & 14.65/13.88 & 16.96/{\\bf 16.26} \\\\ \n        & 420k  &   18.32/17.32 & 18.51/17.62 & 19.81/{\\bf 19.63} \\\\ \n        & 840k  &   21/19.93 & 21.69/20.75 & 22.17/{\\bf 21.93} \\\\ \n        & 1.68m & 23.38/23.01 & 23.33/22.86 & 23.86/{\\bf 23.52} \\\\ \n        \\hline\n        \n        \\multirow{4}{*}{\\rotatebox[origin=c]{90}{En$\\to$De}} \n        & 210k  & 11.44/11.57 & 11.71/11.16 & 12.63/{\\bf 12.68} \\\\\n        & 420k  & 14.28/14.25 & 14.88/15.05 & 15.01/{\\bf 15.67} \\\\\n        & 840k  & 17.09/17.44 & 17.21/17.88 & 17.33/{\\bf 18.14} \\\\\n        & 1.68m & 19.09/19.6  & 19.36/20.13 & 19.23/{\\bf 20.59} \\\\\n    \\end{tabular}\n    \\caption{BLEU scores where the target pair's\n        parallel corpus is constrained to be 5\\%, 10\\%, 20\\% and 40\\% of the\n        original size. We report the BLEU scores on the development and test\n        sets (separated by /) by the single-pair model (Single), the\n        single-pair model with monolingual corpus (Single+DF) and the proposed\n    multi-way, multilingual model (Multi).  \n    \n}\n\\label{tab:control}\n\n\n    \\vspace{-4mm}\n\\end{table}\n\n\n\\section{Experiment Settings}\n\n\\subsection{Datasets}\n\nWe evaluate the proposed multi-way, multilingual translation model on all the\npairs available from WMT'15--English (En) $\\leftrightarrow$ French (Fr), Czech\n(Cs), German (De), Russian (Ru) and Finnish (Fi)--, totalling ten directed\npairs. For each pair, we concatenate all the available parallel corpora from\nWMT'15 and use it as a training set. We use newstest-2013 as a development set\nand newstest-2015 as a test set, in all the pairs other than\nFi-En. In the case of Fi-En, we use newsdev-2015 and newstest-2015 as a\ndevelopment set and test set, respectively. \n\n\\paragraph{Data Preprocessing}\n\nEach training corpus is tokenized using the tokenizer script from the Moses decoder.\n\n\n\n\nThe tokenized training corpus is cleaned following the procedure in\n\\cite{jean2015WMT}. Instead of using space-separated tokens, or words, we use\nsub-word units extracted by byte pair encoding, as recently proposed in\n\\cite{sennrich2015neural}. For each and every language, we include 30k sub-word\nsymbols in a vocabulary. See Table~\\ref{tab:stats} for the statistics of the\nfinal, preprocessed training corpora.\n\n\\paragraph{Evaluation Metric}\n\nWe mainly use BLEU as an evaluation metric using the multi-bleu script from\nMoses.\n\n\n\n\nBLEU is computed on the tokenized text after merging the BPE-based sub-word\nsymbols. We further look at the average log-probability assigned to reference\ntranslations by the trained model as an additional evaluation metric, as a way\nto measure the model's density estimation performance free from any error caused\nby approximate decoding. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Two Scenarios}\n\n\\paragraph{Low-Resource Translation}\nFirst, we investigate the effect of\nthe proposed multi-way, multilingual model on low-resource language-pair\ntranslation. Among the five languages from WMT'15, we choose En, De and Fi as\nsource languages, and En and De as target languages. We control the amount of\nthe parallel corpus of each pair out of three to be 5\\%, 10\\%, 20\\% and 40\\% of\nthe original corpus. In other words, we train four models with different sizes\nof parallel corpus for each language pair (En-De, De-En, Fi-En.) \n\nAs a baseline, we train a single-pair model for each multi-way, multilingual\nmodel. We further finetune the single-pair model to incorporate the target-side\nmonolingual corpus consisting of all the target side text from the other\nlanguage pairs (e.g., when a single-pair model was trained on Fi-En, the\ntarget-side monolingual corpus consists of the target sides from De-En.) This is\ndone by the recently proposed deep fusion \\cite{Gulcehre-Orhan-et-al-2015}. The\nlatter is included to tell whether any improvement from the multilingual model\nis simply due to the increased amount of target-side monolingual corpus.\n\n\\paragraph{Large-scale Translation}\n\nWe train one multi-way, multilingual model that has five\nencoders and five decoders, corresponding to the five languages from WMT'15; En,\nFr, De, Cs, Ru, Fi $\\to$ En, Fr, De, Cs, Ru, Fi. We use the full corpora for all\nof them.\n\n\n\\subsection{Model  Architecture} \nEach symbol, either source or target, is projected on a 620-dimensional space.\nThe encoder is a bidirectional recurrent neural network with 1,000 gated\nrecurrent units (GRU) in each direction, and the decoder is a recurrent neural network with\nalso 1,000 GRU's. The decoder's output function $g_k$ from\nEq.~\\eqref{eq:target_prob} is a feedforward network with 1,000 $\\tanh$ hidden\nunits. The dimensionalities of the context vector ${\\mathbf{{h}}}_t^n$ in\nEq.~\\eqref{eq:ml_context}, the attention-specific context vector\n$\\tilde{{\\mathbf{{h}}}}_t^n$ in Eq.~\\eqref{eq:enc_att} and the attention-specific decoder\nhidden state $\\tilde{{\\mathbf{{h}}}}_{t-1}^m$ in Eq.~\\eqref{eq:dec_att} are all set to\n1,200.\n\nWe use the same type of encoder for every source language, and the same type of\ndecoder for every target language.  The only difference between the single-pair\nmodels and the proposed multilingual ones is the numbers of encoders $N$ and\ndecoders $M$. We leave those multilingual translation specific components, such\nas the ones in Eqs.~\\eqref{eq:ml_context}--\\eqref{eq:dec_att}, in the\nsingle-pair models in order to keep the number of shared parameters constant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table*}[ht]\n    \\small\n    \\centering\n    \\begin{minipage}{\\textwidth}\n        \\centering\n        \\begin{tabular}{c | c c || c c |  c c |  c  c |  c c |  c c}\n            \\multicolumn{3}{r||}{} \n            & \\multicolumn{2}{c|}{Fr (39m)} \n            & \\multicolumn{2}{c|}{Cs (12m)} \n            & \\multicolumn{2}{c|}{De (4.2m)} \n            & \\multicolumn{2}{c|}{Ru (2.3m)} \n            & \\multicolumn{2}{c}{Fi (2m)}  \\\\\n            \\cline{4-13}\n            \n            \n            \n            \n            \n            \n            \n            \\multicolumn{3}{r||}{Dir} \n            & $\\to$ En & En $\\to$ \n            & $\\to$ En & En $\\to$ \n            & $\\to$ En & En $\\to$ \n            & $\\to$ En & En $\\to$ \n            & $\\to$ En & En $\\to$  \\\\\n            \\hline\n            \\hline\n            \n            \n            \\multirow{4}{*}{\\rotatebox[origin=c]{90}{(a) BLEU}}\n            &\n            \\multirow{2}{*}{\\rotatebox[origin=c]{90}{Dev}}\n            & Single & 27.22 & 26.91 & 21.24 & 15.9 & 24.13 & 20.49 & 21.04 & 18.06 & 13.15 & 9.59 \\\\\n            &\n            & Multi & 26.09 & 25.04 & 21.23 & 14.42 & 23.66 & 19.17 & 21.48 & 17.89 & 12.97 & 8.92 \\\\\n            \n            \n            \n            \n            \\cline{2-13}\n            &\n            \\multirow{2}{*}{\\rotatebox[origin=c]{90}{Test}}\n            & Single & 27.94 & {\\bf 29.7} & 20.32 & {\\bf 13.84} & 24 &\n            {\\bf 21.75} & 22.44 & {\\bf 19.54} & 12.24 & {\\bf 9.23} \\\\\n            &\n            & Multi & {\\bf 28.06} & 27.88 & {\\bf 20.57} & 13.29 & {\\bf 24.20} & 20.59 & {\\bf 23.44}& 19.39 & {\\bf 12.61} & 8.98 \\\\\n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \\hline\n            \\hline\n            \\multirow{4}{*}{\\rotatebox[origin=c]{90}{(b) LL}}\n            &\n            \\multirow{2}{*}{\\rotatebox[origin=c]{90}{Dev}}\n            & Single                & -50.53          & -53.38         & -60.69                               & -69.56         & -54.76          & -61.21 & -60.19          & -65.81         & -88.44          & -91.75         \\\\\n            &\n            & Multi                 & -50.6          & -56.55         & -54.46                               & -70.76         & -54.14          & -62.34 & -54.09          & -63.75         & -74.84          & -88.02         \\\\ \n            \n            \n            \\cline{2-13}\n            \n            \n            &\n            \\multirow{2}{*}{\\rotatebox[origin=c]{90}{Test}}\n            & Single & -43.34 & {\\bf -45.07} & -60.03 & {\\bf -64.34} & -57.81 & {\\bf -59.55} & -60.65 & -60.29 & -88.66 & -94.23 \\\\\n            & \n            & Multi  & {\\bf -42.22} & -46.29 & {\\bf -54.66} & -64.80 & {\\bf -53.85} & -60.23 & {\\bf -54.49} & {\\bf -58.63} & {\\bf -71.26} & {\\bf -88.09} \\\\\n            \n            \n            \n        \\end{tabular}\n    \\end{minipage}\n\n    \\caption{(a) BLEU scores and (b) average log-probabilities for all the five\n        languages from WMT'15. \n        \n        \n        \n        \n                 \n    }\n    \\label{tab:all}\n\n    \\vspace{-4mm}\n\\end{table*}\n\n\\subsection{Training}\n\\label{sec:training}\n\n\\paragraph{Basic Settings}\nWe train each model using stochastic gradient descent (SGD) with Adam\n\\cite{kingma2014adam} as an adaptive learning rate algorithm. We use the initial\nlearning rate of $2\\cdot 10^{-4}$ and leave all the other hyperparameters as\nsuggested in \\cite{kingma2014adam}. Each SGD update is computed using a\nminibatch of 80 examples, unless the model is parallelized over two GPUs, in\nwhich case we use a minibatch of 60 examples. We only use sentences of length up\nto 50 symbols. We clip the norm of the gradient to be no more than $1$\n\\cite{pascanu2012difficulty}.  All training runs are early-stopped based on BLEU\non the development set. As we observed in preliminary experiments better scores on the \ndevelopment set when finetuning the shared\nparameters and output layers of the decoders in the case of multilingual models,\nwe do this for all the multilingual models. During\nfinetuning, we clip the norm of the gradient to be no more than $5$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{Schedule} \n\nAs we have access only to bilingual corpora, each sentence pair updates only a\nsubset of the parameters. Excessive updates based on a single language pair may\nbias the model away from the other pairs. To avoid it, we cycle through all the\nlanguage pairs, one pair at a time, in Fi$\\leftrightarrows$En, De$\\leftrightarrows$En, Fr$\\leftrightarrows$En, Cs$\\leftrightarrows$En, Ru$\\leftrightarrows$En order.\\footnote{$\\leftrightarrows$ indicates simultaneous updates on two GPUs.} \n\n\n\n\n\n\n\n\\paragraph{Model Parallelism}\n\nThe size of the multilingual model grows linearly w.r.t. the number of\nlanguages. We observed that a single model that handles five source and five\ntarget languages does not fit in a single GPU \n\nduring training. We address this by \ndistributing computational paths according to different translation pairs over\nmultiple GPUs.\n\n\n\n\nThe shared parameters, mainly related to the attention mechanism, is duplicated\non both GPUs. \n\n\nThe implementation was based on the work in \\cite{DingWMT14}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Results and Analysis}\n\n\\paragraph{Low-Resource Translation}\n\nIt is clear from Table~\\ref{tab:control} that the proposed model (Multi)\noutperforms the single-pair one (Single) in all the cases. This is true even\nwhen the single-pair model is strengthened with a target-side monolingual corpus\n(Single+DF). This suggests that the benefit of generalization from having\nmultiple languages goes beyond that of simply having more target-side\nmonolingual corpus. The performance gap grows as the size of target parallel\ncorpus decreases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{Large-Scale Translation}\n\nIn Table~\\ref{tab:all}, we observe that the proposed multilingual model\noutperforms or is comparable to the single-pair models for the majority of the\nall ten pairs/directions considered. This happens in terms of both BLEU and\naverage log-probability. This is encouraging, considering that there are twice\nmore parameters in the whole set of single-pair models than in the\nmultilingual model.\n\nIt is worthwhile to notice that the benefit is more apparent when the model\ntranslates from a foreign language to English. This may be due to the fact that\nall of the parallel corpora include English as either a source or target\nlanguage, leading to a better parameter estimation of the English decoder. In\nthe future, a strategy of using a pseudo-parallel corpus to increase the amount\nof training examples for the decoders of other languages\n\\cite{sennrich2015improving} should be investigated to confirm this conjecture.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\vspace{-5px}\nIn this paper, we proposed multi-way, multilingual attention-based neural\nmachine translation. The proposed approach allows us to build a single neural\nnetwork that can handle multiple source and target languages simultaneously.\nThe proposed model is a step forward from the recent works on multilingual\nneural translation, in the sense that we support attention mechanism, compared\nto \\cite{luong2015multi} and multi-way translation, compared to\n\\cite{dong2015multi}. Furthermore, we evaluate the proposed model on large-scale\nexperiments, using the full set of parallel corpora from WMT'15.\n\nWe empirically evaluate the proposed model in large-scale experiments using all\nfive languages from WMT'15 with the full set of parallel corpora and also in the\nsettings with artificially controlled amount of the target parallel corpus. In\nboth of the settings, we observed the benefits of the proposed multilingual\nneural translation model over having a set of single-pair models. The\nimprovement was especially clear in the cases of translating low-resource\nlanguage pairs.\n\nWe observed the larger improvements when translating to\nEnglish. We conjecture that this is due to a higher availability of English in\nmost parallel corpora, leading to a better parameter estimation of the English\ndecoder. More research on this phenomenon in the future will result in further\nimprovements from using the proposed model.  Also, all the other techniques\nproposed recently, such as ensembling and large vocabulary tricks, need to be\ntried together with the proposed multilingual model to improve the translation\nquality even further. Finally, an interesting future work is to use the proposed\nmodel to translate between a language pair not included in a set of training\ncorpus.\n\n\n\\section*{Acknowledgments}\n\nThe authors would like to thank the developers of Theano \\cite{bergstra2010,Bastien2012} and Blocks \\cite{MerrienboerBDSW15}. We acknowledge the support of the following organizations for research funding and computing support: NSERC, Samsung, IBM, Calcul Qu\\'ebec, Compute Canada, the Canada Research Chairs, CIFAR and TUBITAK-2214a.\n\n\n\n\n\\bibliography{mlnmt}\n\\bibliographystyle{naaclhlt2016}\n\n\n\n", "itemtype": "equation", "pos": 25681, "prevtext": "\nwhere $f_{\\text{adp}}^m$ affine-transforms the time-dependent context vector to be of\nthe same dimensionality as the decoder. We share a single\naffine-transformation layer $f_{adp}^m$, for all the decoders in this paper. \n\nOnce the hidden state is updated, the probability distribution over the next\nsymbol is computed exactly as for the pair-specific model (see\nEq.~\\eqref{eq:target_prob}.)\n\n\n\\paragraph{Attention Mechanism}\n\nUnlike the encoders and decoders of which there is an instance for each language,\nthere is only a single attention mechanism,\n\nshared across all the language pairs. \n\n\nThis shared mechanism uses the {\\em\nattention-specific} vectors $\\tilde{{\\mathbf{{h}}}}_t^n$ and $\\tilde{{\\mathbf{{z}}}}_{t-1}^m$ from the\nencoder and decoder, respectively. \n\nThe relevance score of each context vector ${\\mathbf{{h}}}_t^n$ is computed based on the\ndecoder's previous hidden state ${\\mathbf{{z}}}_{t-1}^m$ and previous symbol\n$\\tilde{y}^m_{t-1}$:\n\n", "index": 33, "text": "\\begin{align*}\n    e_{t, i}^{m, n} =& f_{\\text{score}}\\left(\n        \\tilde{{\\mathbf{{h}}}}_t^n, \\tilde{{\\mathbf{{z}}}}_{t-1}^m, \\tilde{y}^m_{t-1}\n    \\right) \n    \n    \n    \n    \n    \n\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle e_{t,i}^{m,n}=\" display=\"inline\"><mrow><msubsup><mi>e</mi><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msubsup><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{\\text{score}}\\left(\\tilde{{\\mathbf{{h}}}}_{t}^{n},\\tilde{{%&#10;\\mathbf{{z}}}}_{t-1}^{m},\\tilde{y}^{m}_{t-1}\\right)\\par&#10;\\par&#10;\\par&#10;\\par&#10;\\par&#10;\\par&#10;\" display=\"inline\"><mrow><msub><mi>f</mi><mtext>score</mtext></msub><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc21</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi><mi>n</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc33</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math>", "type": "latex"}]