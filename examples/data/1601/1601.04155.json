[{"file": "1601.04155.tex", "nexttext": "\n$KL(N_1, N_2) $ = 0 if and only if the two distributions are exactly the same, and increases while $N_2$ diverges from $N_1$.\n\nWhen training BDN to predict rating distributions, we replace the default softmax loss with the loss function (\\ref{KL}). The outputs of the global average pooling from the high-level synthesis network remains to be a vector $\\in R^{2 \\times 1}$. But different from the binary prediction task where the output denotes a Bernoulli distribution over [0, 1] labels, the two elements in the output here denote the predicted mean and variance, respectively. They could thus be arbitrary real values falling within the rating scale.\n\n\n\n\n\\begin{figure*}[htbp]\n\\centering\n\\begin{minipage}{0.40\\textwidth}\n\\centering \\subfigure[] {\n\\includegraphics[width=\\textwidth]{highquality.png}\n}\\end{minipage}\n\\begin{minipage}{0.40\\textwidth}\n\\centering \\subfigure[] {\n\\includegraphics[width=\\textwidth]{lowquality.png}\n}\\end{minipage}\n\\caption{Examples of BDN classification results: (a) high-quality images; (b) low-quality images. ($\\delta$ = 0)}\n\\label{image}\n\\end{figure*}\n\n\\section{Study Label-Preserving Transformations}\n\n When training deep networks, the most common approach to reduce overfitting is to artificially enlarge the dataset using label-preserving transformations \\cite{bishop2006pattern}. In \\cite{imagenet}, image translations and horizontal reflections are generated, while the intensities of the RGB channels are altered, both of which apparently will not change the object class labels. Other alternatives, such as random noise, rotations, warping and scaling, are also widely adopted by the latest deep-learning based object recognition. However, there have been little study put on identifying label-preserving transformations for image aesthetics assessment, e.g., those that will not significantly alter the human aesthetics judgements, considering the rating-based labels are very subjective. In \\cite{rapid}, motivated by their need to create fixed-size inputs, the authors created randomly-cropped local regions from training images, which could be empirically treated as data augmentation.\n \n \n\n\n \\begin{table}[t]\n  \\scriptsize\n\\begin{center}\n\\caption{The subjective evaluation survey on the aesthetics influences of various transformations ($s$ denotes a random number)}\n\\label{trans}\n\\begin{tabular}{|c|c||c|}\n\\hline\nTransformation & Description & LP factor   \\\\\n\\hline\nReflection & Flipping the image horizontally   &0.99 \\\\\n\\hline\nRandom scaling & Scale the image proportionally by $s \\in$ [0.9, 1.1]  & 0.94 \\\\\n\\hline\nSmall noise & Add a Gaussian noise $\\in$ $N(0, 5)$ & 0.87 \\\\\n\\hline\nLarge noise & Add a Gaussian noise $\\in$ $N(0, 30)$ & 0.63 \\\\\n\\hline\nAlter RGB & Perturbed the intensities of the RGB channels \\cite{imagenet} &  0.10 \\\\\n\\hline\nRotation & Randomly-parameterized affine transformation & 0.26 \\\\\n\\hline\nSqueezing & Change the aspect ratio by $s \\in$ [0.8, 1.2] & 0.55 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nWe make the first exploration to identify whether a certain transformation will preserve the binary aesthetics rating, i.e., high quality versus low quality, by conducting a \\textbf{subjective evaluation survey} among over 50 participants. We select 20 high-quality ($\\delta$ = 1) images from the AVA dataset (since low-quality images are unlikely to become more aesthetically pleasing after some simple/random transformations). Each image is processed by all different kinds of transformations in Table \\ref{trans}. For each time, a participant is shown with a set of image pairs originated from the same image, but processed with different transformations (including the groundtruth). For each pair, the participant needs to decide which one is better in terms of aesthetics quality. The image pairs are drawn randomly, and the image winning this pairwise comparison will be compared again in the next round, until the best one is selected. \n\nWe fit a Bradley-Terry \\cite{bradley1952rank} model to estimate the subjective scores for each method so that they can be ranked. With groundtruth set as score 1, each transformation will receive a score between [0, 1]. We define the score as the \\textit{label-preserving} (\\textbf{LP}) factor of a transformation; a larger LP factor denotes a smaller impact on image aesthetics. According to Table \\ref{trans}, \\textit{reflection} and \\textit{random scaling} receive high LR factors; the small noise seems to marginally affect the aesthetics feelings, while all the remaining will significantly degrade human aesthetics perceptions. We therefore adopt reflection, random scaling, and small noise as our default data augmentation approaches, unless otherwise specified.\n\n\n\n\\section{Experiment}\n\n\\subsection{Settings}\n\nWe implement our models based on the cuda-convnet package \\cite{imagenet}. The ReLU nonlinearity as well as dropout is applied. The batch size is fixed as 128. Since BDN is fully convolutional, there is no need to normalize the input size. Experiments run on a workstation with 12 Intel Xeon 2.67GHz CPUs and 1 GTX680 GPU. Training one pathway takes roughly 4-5 hours. The fine-tuning of the entire BDN model typically takes about one day. For $I_H$, $I_S$, and $I_V$, we downsample them  to 1/4 of the original size to improve training efficiency. We find the performance hardly affected, which is understandable as the human perceptions of those features are insensitive to scale changes.\n\nThe adjustment of learning rates in such a hierarchical model calls for special attentions. We first train the 14 parallel pathways, with the identical learning rates: $\\eta$ = 0.05 for unsupervised pre-training and 0.01 for supervised tuning, both of which are not annealed throughout training. We then train the high-level synthesis network on top of them and fine-tune the entire BDN. For the pathway part, its learning rate $\\eta'$ starts from 0.001; for the high-level part, the learning rate $\\rho$ starts from 0.01. When the training curve reaches a plateau, we first try dividing $\\rho$ by 10; and further try dividing $\\rho$ by 10 if the training/validation error still does not decrease.\n\nFor binary prediction, we follow RAPID \\cite{rapid} to quantize images' mean ratings into binary values. Images with mean ratings smaller than $5 - \\delta$ are labeled as ``low-quality'', while those with mean ratings larger than $5 + \\delta$ are referred to as ``high-quality''. For rating distribution prediction, we do not quantize the image ratings.\n\n\n\\subsection{Binary Rating Prediction}\n\nWe compare BDN with the state-of-the-art RAPID model for binary aesthetics rating prediction.\nBenefiting from our fully-convolutional architecture, the BDN model has a much lower parameter capacity than RAPID that relies on fully-connected layers. In addition, we manually design three baseline networks, all with exactly the same parameter capacity as BDN:\n\\begin{itemize}\n\\item \\textbf{Baseline fully-convolutional network (BFCN)} first binds the conv1 -- conv3 layers of 14 pathways horizontally, constituting a three-layer fully convolutional network, each layer owning 64 $\\times$ 14 = 896 filter channels. The attribute learning part is trained in a unsupervised way, and then concatenated with the high-level synthesis network, to be jointly supervised-tuned. BFCN does not utilize style annotations.\n\\item \\textbf{BDN without parallel pathways (BDN-WP)} utilize style annotations in an entangled fashion. Its only difference with BFCN lies in that, the training of the attribute learning part is supervised by a composite label $\\in R^{28 \\times 1}$, which binds 14 individual labels altogether.\n\n\\item \\textbf{BDN without data augmentations (BDN-WA)} denotes BDN without the three data augmentations applied (reflection, random scaling, and small noise).\n\\end{itemize}\nWe train the above five models for binary rating prediction, with both $\\delta$ = 0 and $\\delta$ = 1. The overall accuracies are compared in Table \\ref{overall}. \\footnote{The accuracies of RAPID are from the RDCNN results in Table 3 \\cite{rapid}}It appears that BFCN performs significantly worse than others, due to the absence of style attribute information. While RAPID, BDN-WP and BDN all utilize style annotations for supervision, BDN outperform the other two in both cases with remarkable margins. By comparing BDN-WP with BDN, we observe that the biologically-inspired parallel pathway architecture in BDN facilitate the learning. Such a specific architecture avoids overly large all-in-one models (such as BDN-WP), but instead have more effective, dedicated sub-models. In BDN, style annotations serve as powerful priors, to enforce BDN focus on extracting features highly correlated to aesthetics judgements. The BDN is jointly tuned from end to end, which is different from RAPID whose style column only acts as a static regularization. We also notice a gain of nearly 3\\% of BDN over BDN-WA, which verifies the effectiveness of our proposed augmentation approaches.\n\n\\begin{table}[t]\n\\begin{center}\n\\caption{The overall accuracy comparison of different methods for binary rating prediction.}\n\\label{overall}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n& RAPID & BFCN & BDN-WP & BDN-WA & BDN   \\\\\n\\hline\n$\\delta$ = 0 & 74.46\\%  & 70.20\\% & 73.54\\% & 74.03\\% & 76.80\\% \\\\\n\\hline\n$\\delta$ = 1 & 73.70\\% & 68.10\\% &  72.23\\% & 73.72\\% & 76.04\\% \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\\begin{figure}[bpht]\n\\centering\n\\begin{minipage}{0.40\\textwidth}\n\\centering {\n\\includegraphics[width=\\textwidth]{failure.png}\n}\\end{minipage}\n\\caption{Examples of incorrectly classified images ($\\delta$ = 0). }\n\\label{failure}\n\\end{figure}\n\n\nTo qualitatively analyze the results, we display eight images correctly classified by BDN to be high-quality when $\\delta$ = 0, in Fig. \\ref{image} (a), and eight correctly classified low-quality images in in Fig. \\ref{image} (b). The images ranked high in terms of aesthetics typically present salient foreground objects, low depth of field, proper composition, and color harmony. In contrast, low-quality images are at least defected in one aspect. For example, the top left image has no focused foreground object, while the bottom right one suffers from a messy layout. For the top right ``girl'' portrait in Fig \\ref{image} (b), we looked at its original comments on DPChallenge.com, and found that people rated it low because of the noticeable detail loss caused by noise reduction post-processing, as well as the unnatural ``plastic-like'' lights on her hair.\n\nEven more interestingly, Fig. \\ref{failure} lists two \\textbf{failure} examples of BDN. The left image in Fig. \\ref{failure} depicts a waving glowstick captured by time-lapse photography. The image itself has no appealing composition or colors, and is thus identified by BDN to be low-quality. However, the DPChallenge raters/commenters were amazed by the angel shape and rated it very favorably due to the creative idea. The right image, in contrast, is a high-quality portrait, on which DBN confidently agrees.  However, it was associated with the ``Rectangular'' challenge topic on DPChallenge, and was rated low because this targeted theme was overshadowed by the woman. The failure examples manifest the huge subjectivity and sensitivity of human aesthetics judgement.\n\n\n\\begin{figure}[bpht]\n\\centering\n\\begin{minipage}{0.40\\textwidth}\n\\centering {\n\\includegraphics[width=\\textwidth]{variance.png}\n}\\end{minipage}\n\\caption{Examples of high-variance images, correctly predicted by BDN. Those images have nonconventional styles or subjects.}\n\\label{var}\n\\end{figure}\n\n\n\\subsection{Rating Distribution Prediction}\n\\begin{table}[t]\n\\begin{center}\n\\caption{The average KL divergence comparison of different methods for rating distribution prediction.}\n\\label{KL}\n\\begin{tabular}{|c|c|c|}\n\\hline\n BDN & BDN-soft-D & BDN-KL-D   \\\\\n\\hline\n0.1743  & 0.2338 & 0.2052 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nTo our best knowledge, among all state-of-the-art models working on latest large-scale datasets, BDN is the only one accounting for rating distribution prediction. We use the binary prediction BDN as the initialization, and re-train only the high-level synthesis network with the loss defined in Eqn. (\\ref{KL}). We then compare the predicted distributions with the groundtruth of the AVA testing set. We also include two more BDN variants as baselines in this task:\n\\begin{itemize}\n\\item \\textbf{BDN with the softmax loss for rating distribution vectors (BDN-soft-D)} makes the only architecture change by modifying the global average pooling of the high-level network to be 10-channel. Its output is compared to the raw rating distribution under the conventional softmax loss (i.e., cross entropy).\n\\item \\textbf{BDN with the KL loss for rating distribution vectors (BDN-KL-D)} replaces the softmax loss in BDN-soft-D, with the general KL loss (i.e., relative entropy) \\cite{bishop2006pattern}. It remains to work with the raw rating distribution.\n\\end{itemize}\nAs compared in Table \\ref{KL}, KL-based loss function tends to perform better than the softmax function for this specific task. It is important to notice that BDN further reduces the KL divergence compared to BDN-KL-D. While the raw ratings can be noisy due to both the coarse rating grid and the limited rating number, we are able to obtain a more robust estimation of the underlying rating distribution, with the aid of the strong Gaussian prior from the AVA study \\cite{AVA}.\n\n\n\nVery notably, we observe that for more than 96\\% of the AVA testing images, the differences between their groundtruth mean values and estimates by BDN are less than 1. We further binarize the estimated and groundtruth mean values, to re-evaluate the results in the context of binary rating prediction. The overall accuracies are improved to 78.08\\% ($\\delta$ = 0), and 77.27\\% ($\\delta$ = 1). It verifies the benefits to jointly predict the means and standard deviations, built upon the AVA observation that they are correlated.\n\n\n\n\n\n\n\n\nFig. \\ref{var} visualizes images that are correctly predicted by BDN to have large variances. It is intuitive that images with a high variance seem more likely to be edgy or subject to interpretation. Taking the top right image for example, the comments it received indicate that while many voters found the photo striking (e.g. ``nice macro''``good idea''), others found it rude (e.g. ``it frightens me''``too close for comfort'').\n\n\n\n\n\n\n\n\\section{Conclusion}\n\nIn this paper, we get inspired by the knowledge abstracted from the human visual perception and neuroaesthetics, and formulate the Brain-Inspired Deep Networks (BDN). The biological inspired, task-specific architecture of BDN leads to superior performances, compared to other state-of-the-art models with the same or higher parameter capacity. Since it has been observed in Fig. \\ref{failure} and \\ref{var} that emotions and contexts could alter the aesthetics judgment, our immediate next work is to take the two factors into account for a more comprehensive framework.\n\n\n\n\n\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{egbib}\n}\n\n\n", "itemtype": "equation", "pos": 22497, "prevtext": "\n\n\n\\title{Brain-Inspired Deep Networks for Image Aesthetics Assessment }\n\n\\author{Zhangyang Wang, Florin Dolcos, Diane Beck, Shiyu Chang, and Thomas S. Huang\n \\\\\nBeckman Institute, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA\\\\\n{\\tt\\small \\{zwang119, fdolcos, dmbeck, chang87, t-huang1\\}@illinois.edu}\n\n\n\n\n\n}\n\\maketitle\n\n\n\n\n\\begin{abstract}\nImage aesthetics assessment has been challenging due to its subjective nature. Being extensively inspired by the scientific advances in the human visual perception and neuroaesthetics, we design the Brain-Inspired Deep Networks (BDN) for this task. BDN first learns attributes through the parallel supervised pathways, on a variety of selected feature dimensions. A high-level synthesis network is trained to associate and transform those attributes into the overall aesthetics rating. We then extend BDN to predicting the distribution of human ratings, since aesthetics ratings often vary somewhat from observer to observer. Another highlight is our first-of-its-kind study of label-preserving transformations in the context of aesthetics assessment, which leads to effective data augmentation approaches. Experimental results on the AVA dataset show that our biological inspired, task-specific BDN model leads to significantly improved performances, compared to other state-of-the-art models with the same or higher parameter capacity.\n\\end{abstract}\n\n\n\n\\section{Introduction}\n\n\n\nAutomated assessment or rating of pictorial aesthetics has many applications, such as in an image retrieval system or a picture editing software \\cite{cheng2010learning}. Compared to many typical machine vision problems, the aesthetics assessment is even more challenging, due to the loose and highly subjective nature of aesthetics, and the seemingly inherent semantic gap between low-level computable features and high-level human-oriented semantics. Though aesthetics influences many human judgments, our understanding of what makes an image aesthetically pleasing is still limited. Contrary to semantics, an aesthetics response is usually very subjective and difficult to gauge even among human beings.\n\nExisting research has predominantly focused on constructing hand-crafted features that are empirically related to aesthetics. Those features are designed under the guidance of photography and psychological rules, such as rule-of-thirds composition, depth of field (DOF), and colorfulness \\cite{ECCV06, ke2006design}. With the images being represented by these hand-crafted features, aesthetic classification or regression models can be trained on datasets consisting of images associated with human aesthetic ratings. However, the effectiveness of hand-crafted features is only empirical, due to the vagueness of certain photographic or psychologic rules. Recently, Lu et.al. \\cite{rapid} proposed the \\textit{Rating Pictorial Aesthetics using Deep Learning} (\\textbf{RAPID}) model, with impressive accuracies on the \\textit{Aesthetic Visual Analysis} (\\textbf{AVA}) dataset \\cite{AVA}. They have not yet studied more precise predictions, such as finer-grain ratings or rating distributions \\cite{distribution}.\n\n\n\n\n \\begin{figure*}[htbp]\n\\centering\n\\begin{minipage}{0.95\\textwidth}\n\\centering \\subfigure[] {\n\\includegraphics[width=\\textwidth]{BDN.pdf}\n}\\end{minipage}\n\n\\caption{The Brain-Inspired Deep Networks (BDN) architecture. The input image is first processed by parallel pathways, each of which learns an attribute along a selected feature dimension independently. Except for the first three simplest features (\\textit{hue, saturation, value}), all parallel pathways take the form of fully-convolutional networks, supervised by individual labels; their hidden layer activations are utilized as learned attributes. Those attributes are then associated and transformed into the overall aesthetic rating, by a high-level synthesis network. In addition to the binary rating prediction, we also extend BDN to predicting the distribution of human ratings.}\n\\label{DBN}\n\\end{figure*}\n\nFurthermore, the study of the cognitive and neural underpinnings of aesthetic appreciation by means of neuroimaging techniques yields some promise for understanding human aesthetics \\cite{cela2011neural}. Although the results of these studies have been somewhat divergent, a hierarchical set of core mechanisms involved in aesthetic preference have been identified \\cite{chatterjee2011neuroaesthetics}. Whereas deep learning is well known to be inspired by and analogous to brain mechanisms \\cite{bengio2009learning}, there is hardly any work providing the synergy between the neuroaesthetics knowledge and the advances of learning-based aesthetics assessment models.\n\nIn this work, we develop a novel deep-learning based image aesthetics assessment model, called \\textbf{Brain-Inspired Deep Networks (BDN)}. BDN clearly distinguishes itself from prior models, for its unique architecture inspired by the hierarchical information processing phases from the neuroaesthetics study \\cite{cela2011neural}. We introduce a specific architecture, called \\textit{parallel supervised pathways}, to learn multiple attributes along a variety of selected feature dimensions. Those attributes are then associated and transformed into the overall aesthetic rating, by a \\textit{high-level synthesis network}. We extend BDN to predicting the distribution of human ratings, since aesthetics ratings often vary somewhat from observer to observer. Our technical contribution also includes the study of label-preserving transformations in the context of aesthetics assessment, which could be referred to as effective data augmentation approaches. We examine the BDN model on the large-scale AVA dataset \\cite{AVA}, for both binary rating prediction and rating distribution prediction tasks, and confirms its superiority over a few competitive methods with the same or higher parameter capacity.\n\n\n\n\\subsection{Related Work}\n\nLarge and reliable datasets, consisting of images and corresponding human ratings, are the essential foundation for the development of machine assessment models. Several Web photo resources have taken advantage of crowdsourcing contributions, such as Flickr and DPChallenge.com \\cite{AVA}. The AVA dataset is a large-scale collection of images and meta-data derived from DPChallenge.com. It contains over 250,000 images with aesthetic ratings from 1 to 10, and a 14,079 subset with binary style labels (e.g., rule of thirds, motion blur, and complementary colors), making automatic feature learning using deep learning approaches possible. In this paper, we focus on AVA as our research subject.\n\nDatta et.al. \\cite{ECCV06} first casted the image aesthetics assessment problem as a classification or regression problem. A given image is mapped to an aesthetic rating, which is usually collected from multiple subject raters. The rating is normally quantized with discrete values. The earliest work \\cite{ECCV06, ke2006design} extracted various handcrafted features, including low-level image statistics such as distributions of edges and color histograms, and high-level photographic rules such as the rule of thirds. A part of subsequent efforts, such as \\cite{MM10, CVPR11, ICCV11}, focus on improving the quality of those features. Generic image features \\cite{general}, such as SIFT and Fisher Vector \\cite{lowe2004distinctive}, have also been applied to predict aesthetics. However, empirical features cannot accurately and exhaustively represent the aesthetic properties.\n\n\n\n\n\n\n\n\n\n\nThe human brain transforms and synthesizes a torrent of complex and ambiguous sensory information into coherent thought and decisions. Most atheistic assessment methods adopt simple linear classifiers to categorize the input features, which is obviously oversimplified. Deep networks \\cite{bengio2009learning} attempt to emulate the underlying complex neural mechanisms of human perception, and display the ability to describe image content from the primitive level (low-level features) to the abstract level (high-level features). They are composed of multiple non-linear transformations to yield more abstract and descriptive embedding representations. The RAPID model \\cite{rapid} is among the first to apply deep convolutional neural networks (CNN) \\cite{imagenet} to the aesthetics rating prediction, where the features are automatically learned. They further improved the model by exploring style annotations \\cite{AVA} associated with images. In fact, even the hidden activations from a generic CNN architecture prove to work reasonably well for aesthetics features \\cite{ustc}.\n\n\n\n\nMost current work treat aesthetics assessment as a conventional classification problem: each photo's user ratings are transformed into a ordinal scalar rating (by averaging, etc.), which is taken as its label. For example, RAPID \\cite{rapid} simply divided all samples as aesthetic or unaesthetic, and trained a binary classification model. However, it is common for different users to rate visual subjects inconsistently or even oppositely due to the subjective problem nature \\cite{ke2006design}. Since human aesthetic assessment depends on multiple dimensions such as composition, colorfulness, or even emotion \\cite{Jiebo}, it is difficult for individuals to reliably convert their experiences to a single rating, resulting in noisy estimates of real aesthetic responses. In \\cite{distribution}, Wu et.al. first proposed to represent each photo's rating as a distribution vector over basic ratings, constituting a structural regression problem. Gao et.al. \\cite{gao2015multiple} formulated the aesthetic assessment as a multi-label task, where multiple aesthetic attributes were predicted jointly via bayesian networks.\n\n\n\\section{Biological Inspirations}\n\n\\subsection{Summary of Scientistic Advances}\n\nRecent advances in neuroaesthetics imply that the human perception of aesthetics is a highly complicated and systematic process. Multiple parallel processing strategies, involving over a dozen retinal ganglion cell types, can be found in the retina. Each ganglion cell type tiles the retina to focus on one specific kind of feature, and provide a complete representation across the entire visual field \\cite{nassi2009parallel}. Retinal ganglion cells project in parallel from the retina, through the lateral geniculate nucleus of the thalamus to the primary visual cortex. Primary visual cortex receives parallel inputs from the thalamus and uses modularity, defined spatially and by cell-type specific connectivity, to recombine these inputs into new parallel outputs. Beyond primary visual cortex, separate but interacting dorsal and ventral streams perform distinct computations on similar visual information to support distinct behavioural goals \\cite{functional}. The integration of visual information is then achieved progressively. Independent groups of cells with different functions are brought into temporary association, by a so-called ``binding'' mechanism \\cite{cela2011neural}, for the final decision-making.\n\nMore specifically, from the retina to the prefrontal cortex, the human visual processing system will first conduct a very rapid holistic image analysis \\cite{treisman1980feature, itti1998model, tsotsos2011computational}. The divergence comes at a later stage, in how the low-level visual features are further processed through parallel pathways \\cite{field2007information} before being utilized. The pathway can be characterized by a hierarchical architecture, in which neurons in higher areas code for progressively more complex representations by pooling information from lower areas. For example, there is evidence \\cite{rousselet2004parallel} that neurons in V1 code for relatively simple features such as local contours and colors, whereas neurons in TE fire in response to more abstractive features, that encode the scene's gist and/or saliency information and act as a holistic signature of the input.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent \\textbf{Key Notations:} For the consistency of terms, we use \\textit{feature dimension} to denote a prominent visual property, that is relevant to aesthetics judgement. We define an \\textit{attribute} as the learned abstracted, holistic feature representation over a specific feature dimension. We define a \\textit{pathway} as the processing mechanism from a raw visual input to an attribute.\n\n\\subsection{Principled Design Insights}\n\nSo far, the computational model of deep learning has been closely related to a class of theories of brain development \\cite{elman1998rethinking}. For example, the design of CNNs follows the discovery of general human vision mechanisms \\cite{functional}, indicating the usefulness of ideas borrowed from neurobiological processes. However, all current ``deep'' models remain extremely simple compared to the vastness and complexity of biological information processing. It is demonstrated that a single neuron is probably more complex than an entire CNN \\cite{stoodley2009functional}, not to mention our lack of knowledge in the cells' electrochemical properties and inter-neuron interactions.\n\nWe argue that it is neither impractical nor necessary to design a model, that exactly reproduce the full perception process in the human brain, take the typical example of man being able to fly without the complexity and fluidity of flapping wings. Starting from the the underlying neuroscience principles \\cite{prospects, nassi2009parallel}, we conclude the following simplified, but important insights that inspire our model:\n\\begin{itemize}\n\\item The human brain works as a multi-leveled system.\n\\item For the visual sensory input, a variety of relevant feature dimensions are first targeted.\n\\item A set of parallel pathways abstract the visual input. Each pathway processes the input into an attribute on a specific feature dimension.\n\\item The high-level association and synthesis transforms all attributes into an aesthetics decision.\n\\end{itemize}\nStep 2 and 3 are derived from the existing neuroaesthetics advances, that aesthetics judgments evidently involve multiple pathways, which could connect from related perception tasks \\cite{cela2011neural, chatterjee2011neuroaesthetics, prospects}. On the other hand, many feature dimensions, such as color, shape, and composition, have already been discovered to be crucial for aesthetics. A bold yet rational assumption is thus made by us, that the attribute learning for aesthetics tasks could be decomposed onto those pre-known feature dimensions and processed in parallel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Brain-Inspired Deep Networks}\n\nThe architecture of Brain-Inspired Deep Networks (BDN) is depicted in Fig. \\ref{DBN}, which is divided in two stages based on the above insights. In brief, we first learn attributes through parallel (supervised) pathways, over the selected feature dimensions. We then correlate them with the overall aesthetics ratings in the high-level synthesis network.\n\n\n\\subsection{Attribute Learning via Parallel Pathways}\n\n\\subsubsection{Selecting Feature Dimensions}\n\nWe first select feature dimensions that are discovered to be highly related to aesthetics assessment. Despite the lack of firm rules, certain visual features are believed to please humans more than others \\cite{ECCV06}. We take advantage of those photographically or psychologically inspired feature dimensions as priors, and force BDN to ``focus'' on them.\n\nThe previous work \\cite{ECCV06, ke2006design} has identified a set of aesthetically discriminative features, among which the light exposure, saturation and hue play indispensable roles. We assume the RGB data of each image is converted to HSV color space, producing two-dimensional matrices $I_H$, $I_S$, and $I_V$, each of the same size as the original image. Furthermore, many photographic style features influence human's aesthetic judgements. \\cite{ECCV06} proposed six sets of photographic styles, including the rule of thirds composition, textures, shapes, and shallow depth-of-field (DOF). The AVA dataset comes with a more enriched variety of \\textit{style annotations}, as listed in Table \\ref{style}, which are leveraged by us. \\footnote{The 14 photographic styles are chosen specifically on the AVA datasets. We do not think they represent all aesthetics-related visual information, and  plan to have more photographic styles annotated.}\n\n\n\n \\begin{table}[t]\n\\small\n\\begin{center}\n\\caption{The 14 style attribute annotations in the AVA dataset}\n\\label{style}\n\\begin{tabular}{|c|c||c|c|}\n\\hline\nStyle & Number & Style & Number \\\\\n\\hline\n\\hline\nComplementary Colors & 949 & Duotones & 1, 301\\\\\n\\hline\nHigh Dynamic Range & 396 &  Image Grain & 840 \\\\\n\\hline\nLight on White & 1,199 &  Long Exposure & 845 \\\\\n\\hline\nMacro & 1,698 & Motion Blur & 609 \\\\\n\\hline\nNegative Image & 959 &  Rule of Thirds & 1,031\\\\\n\\hline\nShallow DOF & 710 & Silhouettes & 1,389 \\\\\n\\hline\nSoft Focus & 1,479 & Vanishing Point & 674\\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Parallel Supervised Pathways}\n\nAmong the 17 feature dimensions, the simplest three, $I_H$, $I_S$, and $I_V$ are immediately obtained from the input. However, the remaining 14 style feature dimensions are not qualitatively well-defined; their attributes are not straightforward to be extracted. \n\n\\begin{figure}[tbpht]\n\\centering\n\\begin{minipage}{0.45\\textwidth}\n\\centering {\n\\includegraphics[width=\\textwidth]{fcnn.png}\n}\\end{minipage}\n\\caption{The architecture of one supervised pathway, in the form of FCNN. }\n\\label{fcnn}\n\\end{figure}\nFor each style category as a feature dimension, we create binary \\textit{individual labels}, by labelling images with the style annotation as ``1'' and otherwise ``0'', which follows many previous work \\cite{AVA, CVPR11}. In \\cite{rapid}, the authors utilized style labels to train a style classier column in their network, which serves as a static regularization and is not jointly tuned with the overall prediction.\n\nInstead, we design a special architecture, called \\textit{parallel supervised pathways}. Each pathway is modeled with a \\textit{fully convolutional neural network} (FCNN), as in Fig. \\ref{fcnn}. It takes an image as the input, and outputs image's individual label along this feature dimension. All pathways are learned in parallel without intervening with each other. The choice of FCNN is motivated by the spatial locality-preserving characteristics of human brain's low-level visual perception \\cite{functional}.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each feature dimension, the number of labeled samples is limited, as can be seen from Table \\ref{style}. In view of that, we pre-train the first two layers in Fig. \\ref{fcnn}, using all images from the AVA dataset, in a unsupervised way. We construct a 4-layer Stacked Convolutional Auto Encoder (SCAE): its first 2 layers follows the same topology as the conv1 and conv2 layers, and the last 2 layers are mirror-symmetrical deconvolutional layers \\cite{zeiler2011adaptive}. After SCAE is trained, the first two layers are applied to initialize the conv1 and conv2 layers of all 14 FCNN pathways. The strategy is based on the common belief that the lower layers of CNNs learn general-purpose features, such as edges and contours, which could be adapted for extensive high-level tasks \\cite{decaf}.\n\nAfter the initialization of the first two layers, for each pathway, we concatenate the conv3 and conv4 layers, and further conduct supervised training using individual labels. The conv4 layer always has the same channel number with the corresponding style classes (here 2 for all). It is followed by the global average pooling \\cite{nin} step, to be correlated with the binary labels. In the end, the conv3 layer activations of each pathway are utilized as attributes \\cite{decaf}.\n\n\n\n\n\n\n\n\n\n\\subsection{Training The High-Level Synthesis Network}\n\n\nFinally, we simulates brain's high-level association and synthesis, using a larger FCNN. Its architecture resembles Fig. \\ref{fcnn}, except that the first three convolutional layers each have 128 channels instead of 64. The high-level synthesis network takes the attributes from all parallel pathways as inputs, and outputs the overall aesthetics rating.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Predicting The Distribution Representation}\n\nMost existing studies \\cite{ECCV06} apply a scalar value to represent the predicted aesthetics quality, which appears insufficient to capture the true subjective nature. For example, two images with the equal mean score could have very different deviations among raters. Typically. an image with a large rating variance is more likely to be edgy or subject to interpretation. \\cite{rapid} assigned images with binary aesthetics labels,  i.e., high quality and low quality, by thresholding their mean ratings, which provided less informative supervision due to the large intra-class variation. \\cite{distribution} suggested to represent the ratings as a distribution on pre-defined ordinal basic ratings. However, such a structural label could be very noisy, due to the coarse grid of basic ratings, the limited sample size (number of ratings) per image, and the lack of shifting robustness of their $L_2$-based loss.\n\nThe previous study of the AVA datasets \\cite{AVA} reveals two important facts:\n\\begin{itemize}\n\\item For all images, the standard deviation of an image's ratings is a function of its mean rating. Especially,  images with ``moderate'' ratings  tend to have a lower variance than images with ``extreme'' ratings. It inspires us that the estimations of mean ratings and standard deviations may be jointly performed, which can potentially mutually reinforce each other.\n\\item For each image, the distribution of its ratings from different raters is largely Gaussian. According to \\cite{AVA}, Gaussian functions perform adequately good approximations to fit the rating distributions of 99.77\\% AVA images. Besides, those non-Gaussian distributions tend to be highly-skewed, occurring at the low and high extremes of the rating scale, where their mean ratings could be predicted with higher confidences.\n\\end{itemize}\nTo this end, we propose to explicitly model one image's rating distribution as Gaussian, and jointly predict its mean and standard deviation. Assuming the underlaying distribution $N_1(\\mu_1, \\sigma_1)$ and the predicted distribution $N_2(\\mu_2, \\sigma_2)$, their difference could be calculated by the classical Kullback-Leibler (KL) divergence \\cite{bishop2006pattern}:\n\n", "index": 1, "text": "\\begin{equation}\n\\begin{array}{l}\\label{KL}\nKL(N_1, N_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\mu_2^2} - \\frac{1}{2}\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}KL(N_{1},N_{2})=\\log\\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{%&#10;\\sigma_{1}^{2}+(\\mu_{1}-\\mu_{2})^{2}}{2\\mu_{2}^{2}}-\\frac{1}{2}\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>K</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>N</mi><mn>1</mn></msub><mo>,</mo><msub><mi>N</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>log</mi><mo>\u2061</mo><mfrac><msub><mi>\u03c3</mi><mn>2</mn></msub><msub><mi>\u03c3</mi><mn>1</mn></msub></mfrac></mrow><mo>+</mo><mfrac><mrow><msubsup><mi>\u03c3</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>-</mo><msub><mi>\u03bc</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03bc</mi><mn>2</mn><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}]