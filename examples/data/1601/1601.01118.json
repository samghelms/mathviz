[{"file": "1601.01118.tex", "nexttext": "\nprovided that neither $m(x,y)$ nor $m^*(x)$ is zero. Notice that  $R(x,y)=1$ iff $y\\in SOL^*(x)$. Let $\\gamma>1$ be a constant indicating an upper bound of the performance ratio.\nWith this constant $\\gamma$, we say that $P$ is\n{\\em polynomial-time $\\gamma$-approximable} if there exists a  polynomial-time deterministic Turing machine $M$  such that, for any instance $x\\in I$, if $SOL(x)\\neq{\\mathrm{\\O}}$, then $M(x)\\in SOL(x)$ and $R(x,M(x))\\leq \\gamma$; otherwise, $M(x)$ outputs ``no solution'' (or a symbol $\\bot$); in addition,\nthe values\\footnote{The polynomial-time computability of the value $m(x,M(x))$ is trivial; however, the computability requirement for this value is quite important for the log-space computability and the NC$^{1}$ computability.} $m(x,M(x))$ must be computed in polynomial time from inputs $x$.\nSuch a machine is referred to as a {\\em $\\gamma$-approximate algorithm}. The $\\gamma$-approximability clearly implies that the set  $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ belongs to ${\\mathrm{P}}$.\nThe notation ${\\mathrm{APXP}}_{{\\cal D}}$ denotes a class consisting of problems $P$ in   class ${\\cal D}$ of optimization problems such that, for a certain fixed constant  $\\gamma>1$, $P$ is polynomial-time $\\gamma$-approximable. Notice that ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ is conventionally expressed as ${\\mathrm{APX}}$ (see, {\\textrm{e.g.},\\hspace*{2mm}} \\cite{ACG+03}).\n\nLikewise, we define three extra notions of ``log-space $\\gamma$-approximation'' \\cite{Tan07}, ``${\\mathrm{NC}^{ {i} }}$ $\\gamma$-approximation,''  and ``${\\mathrm{AC}^{ {i} }}$ $\\gamma$-approximation'' by replacing ``polynomial-time Turing machine'' in the above definition with ``logarithmic-space (auxiliary) Turing machine,'' ``uniform family of ${\\mathrm{NC}^{ {i} }}$-circuits,'' and ``uniform family of ${\\mathrm{AC}^{ {i} }}$-circuits,'' respectively, for every index $i\\in{\\mathbb{N}}$.\nWe then introduce the notations of ${\\mathrm{APXL}}_{{\\cal D}}$, ${\\mathrm{APXNC}^{ {i} }}_{{\\cal D}}$, and ${\\mathrm{APXAC}^{ {i} }}_{{\\cal D}}$ using ``log-space $\\gamma$-approximation,'' ``${\\mathrm{NC}^{ {i} }}$ $\\gamma$-approximation,'' and ``${\\mathrm{AC}^{ {i} }}$ $\\gamma$-approximation,''  respectively. It follows that ${\\mathrm{APXAC}^{ {0} }}_{{\\cal D}} \\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\cal D}} \\subseteq {\\mathrm{APXL}}_{{\\cal D}} \\subseteq {\\mathrm{APXP}}_{{\\cal D}}$ for any reasonable optimization/approximation class ${\\cal D}$.\n\n\n\\paragraph{PTAS, LSAS, NC$^{i}$AS, and AC$^{i}$AS.}\nA deterministic Turing machine $M$ is called a {\\em polynomial-time approximation scheme} (or a PTAS) if, for any ``fixed constant'' $r\\in{\\mathbb{Q}}^{>1}$, there exists a polynomial $p_r(n)$ such that, for every admissible instance $x\\in I$, if $SOL(x)\\neq{\\mathrm{\\O}}$, then $M$ takes $(x,r)$ as its input and outputs an $r$-approximate solution of $x$ in time at most $p_r(|x|)$; otherwise, $M(x)$ outputs ``no solution'' (or a symbol $\\bot$).\nExamples of such polynomial $p_r(n)$ are   ${\\lceil {\\frac{r}{r-1}} \\rceil}n^3$ and $n^{{\\lceil {1/(r-1)} \\rceil}}$.\nAny approximation scheme is also a $\\gamma$-approximate algorithm for any chosen constant $\\gamma>1$. The approximation class $\\mathrm{PTAS}_{{\\mathrm{NPO}}}$ denotes a collection of all NPO problems that admit PTAS's. In a similar manner, we can define a notion of {\\em logarithmic-space approximation scheme} (or LSAS) and the associated approximation class $\\mathrm{LSAS}_{{\\mathrm{NLO}}}$ by replacing ``polynomial time'' and ``polynomial'' with ``logarithmic space'' and ``logarithmic function,'' respectively.\n\nThe definitions of ${\\mathrm{NC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ and ${\\mathrm{AC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ are given essentially in the same way with a slight technical complication on uniformity condition. ${\\mathrm{NC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{AC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$) can be introduced using circuits of size $p_r(n)$ and depth $\\ell_r(n)$ with bounded (resp., unbounded) fan-in gates, where $p_r(n)$ is a polynomial and $\\ell_r(n)$ is a logarithmic function as long as $r$ is treated as a fixed constant. Here, the uniformity requires $\\mathrm{DTIME}(\\ell'_r(n))$ for another logarithmic function $\\ell'_r$ with $r$ being treated as a constant.  \n{\\medskip}\n\nWe have so far given 14 classes of optimization problems, which we shall discuss in details in the subsequent sections. Given an arbitrary nonempty class ${\\cal D}$ of optimization problems, it holds that  ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\cal D}}\\subseteq {\\mathrm{LO}}_{{\\cal D}} \\subseteq {\\mathrm{PO}}_{{\\cal D}}$ and  ${\\mathrm{APXNC}^{ {1} }}_{{\\cal D}}\\subseteq {\\mathrm{APXL}}_{{\\cal D}}\\subseteq {\\mathrm{APXP}}_{{\\cal D}}$. It also follows that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\cal D}} \\subseteq {\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\cal D}} \\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\cal D}}$, ${\\mathrm{LO}}_{{\\cal D}} \\subseteq {\\mathrm{LSAS}}_{{\\cal D}}  \\subseteq {\\mathrm{APXL}}_{{\\cal D}}$, and ${\\mathrm{PO}}_{{\\cal D}} \\subseteq {\\mathrm{PTAS}}_{{\\cal D}} \\subseteq {\\mathrm{APXP}}_{{\\cal D}}$.\nWhen ${\\cal D}={\\mathrm{NLO}}$, in particular, three classes ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$, ${\\mathrm{PTAS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$ coincide with ${\\mathrm{NLO}}$. Since the proof of this fact is short, we include it here.\n\n\\begin{lemma}\\label{PO=APXP=NLO}\n${\\mathrm{APXP}}_{{\\mathrm{NLO}}} = {\\mathrm{PTAS}}_{{\\mathrm{NLO}}} = {\\mathrm{PO}}_{{\\mathrm{NLO}}} = {\\mathrm{NLO}}$.\n\\end{lemma}\n\n\\begin{proof}\nNote that ${\\mathrm{PO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{PTAS}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{APXP}}_{{\\mathrm{NLO}}}$. First, we claim that ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{NLO}}$. By the definition of ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$, all problems in ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$ must be ${\\mathrm{NLO}}$ problems, and hence they are in ${\\mathrm{NLO}}$.\n\nNext, we show that ${\\mathrm{NLO}}\\subseteq {\\mathrm{PO}}_{{\\mathrm{NLO}}}$.\nLet $P=(I,SOL,m,goal)$ be any problem in ${\\mathrm{NLO}}$.\nHere, we consider only the case of $goal=\\text{\\sc max}$ because the case of {\\sc min} is analogous. We want to show that $P$ belongs to ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$. Let $x$ be any instance in $I$. Consider the following algorithm on input $x$.\nHere, we define $D=\\{(x,y)\\in I\\circ SOL \\mid \\exists z\\in SOL(x)\\,[ z \\geq y \\wedge m(x,z)\\geq m(x,y)]\\}$, where the notation $\\geq$ used for strings $x$ and $y$ is the lexicographic ordering. Note that $D\\in{\\mathrm{NL}}\\subseteq{\\mathrm{P}}$. Now, we can use a binary search technique using $D$ to find a maximal solution $y\\in SOL^*(x)$ in polynomial time.\nTherefore, we conclude that ${\\mathrm{NLO}} \\subseteq {\\mathrm{PO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{PTAS}}_{{\\mathrm{NLO}}} \\subseteq {\\mathrm{APXP}}_{{\\mathrm{NLO}}} \\subseteq {\\mathrm{NLO}}$. This implies the lemma.\n\\end{proof}\n\n\n\nTaking a slightly different approach toward a study on ${\\mathrm{NPO}}$ problems, Krentel \\cite{Kre88} introduced a class ${\\mathrm{OptP}}$ of optimization functions. Let ${\\mathrm{MaxP}}$ (resp., ${\\mathrm{MinP}}$) denote the class of all functions from $\\Sigma_1^*$ to $\\Sigma_2^*$, each of which satisfies the following property: there exists a polynomial-time nondeterministic Turing machine $M$ such that, for every input $x\\in\\Sigma_1^*$, $f(x)$ denotes the maximal (resp., minimal) string (in the lexicographic order) generated by $M$ on $x$ \\cite{KST89}, where $\\Sigma_1$ and $\\Sigma_2^*$ are alphabets.\nThe class ${\\mathrm{OptP}}$ is simply defined as ${\\mathrm{MaxP}}\\cup {\\mathrm{MinP}}$. We further define ${\\mathrm{OptL}}$ in a similar way but using log-space nondeterministic Turing machines.\nNotice that \\`{A}lvarez and Jenner \\cite{AJ93} originally defined ${\\mathrm{OptL}}$ as the set of {\\em only} maximization problems and that we need to pay a special attention to their results whenever we apply them in our setting.\n\n\n\\subsection{Approximation-Preserving Reductions}\\label{sec:AP-reducibility}\n\nTo compare the computational complexity of two optimization problems, we wish to use three types of reductions between those two problems. We follow well-studied reductions, known as {\\em approximation-preserving (AP) reductions} and {\\em exact (EX) reductions}.\nGiven two optimization problems $P=(I_1,SOL_1,m_1,goal)$ and $Q=(I_2,SOL_2,m_2,goal)$, $P$ is {\\em  polynomial-time AP-reducible} (or more conveniently, {\\em APP-reducible}) to $Q$, denoted $P{\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}} Q$, if there are two functions $f$ and $g$ and a constant $c\\geq1$ such that the following {\\em APP-condition} is satisfied:\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\\item for any instance $x\\in I_1$ and any $r\\in{\\mathbb{Q}}^{>1}$, it holds that $f(x,r)\\in I_2$,\n\n\\item for any $x\\in I_1$ and any $r\\in{\\mathbb{Q}}^{>1}$, if $SOL_1(x)\\neq{\\mathrm{\\O}}$ then $SOL_2(f(x,r))\\neq{\\mathrm{\\O}}$,\n\n\\item for any $x\\in I_1$, any $r\\in{\\mathbb{Q}}^{>1}$, and any $y\\in SOL_2(f(x,r))$, it holds that $g(x,y,r)\\in SOL_1(x)$,\n\n\\item $f(x,r)$ is computed by a deterministic Turing machine and $g(x,y,r)$ is computed by an auxiliary Turing machine, both of which run in time polynomial in $(|x|,|y|)$ for any $(x,y)\\in I\\circ SOL$ and any number  $r\\in{\\mathbb{Q}}^{>1}$, and\n\n\\item for any $x\\in I_1$, any $r\\in{\\mathbb{Q}}^{>1}$, and any $y\\in SOL_2(f(x,r))$, $R_2(f(x,r),y) \\leq r$ implies $R_1(x,g(x,y,r))\\leq 1+c(r-1)$, where $R_1$ and $R_2$ respectively express the performance ratios for $P_1$ and $P_2$.\n\\end{itemize}\nNotice that the above APP-condition makes us concentrate only on instances of $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ and that, for other instances $x$, we might possibly set the value $f(x,r)$ arbitrarily (as long as $x\\in I_1$ iff $f(x,r)\\in I_2$).\nWhen this APP-condition holds, we also say that $P$ {\\em APP-reduces}\nto $Q$. The triplet $(f,g,c)$ is called a {\\em polynomial-time AP-reduction} (or an {\\em APP-reduction}) from $P$ to $Q$. For more details, refer to, e.g., \\cite{ACG+03}.\n\nTo discuss optimization problems within ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$,\nwe further need to introduce another type of reduction $(f,g)$, in which $g$ ``exactly'' transforms in polynomial time an optimal solution for $Q$ to another optimal solution for $P$ so that ``$Q\\in {\\mathrm{PO}}_{{\\mathrm{NPO}}}$'' directly implies ``$P\\in {\\mathrm{PO}}_{{\\mathrm{NPO}}}$.'' We write $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{P}}} Q$ when the following {\\em EX-condition} holds:\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\\item for any instance $x\\in I_1$, it holds that $f(x)\\in I_2$,\n\n\\item for any $x\\in I_1$, if $SOL_1(x)\\neq{\\mathrm{\\O}}$ then $SOL_2(f(x))\\neq{\\mathrm{\\O}}$,\n\n\\item for any $x\\in I_1$ and any $y\\in SOL_2(f(x))$, it holds that $g(x,y)\\in SOL_1(x)$,\n\n\\item $f(x)$ is computed by deterministic Turing machine and $g(x,y)$ is computed by an auxiliary Turing machine, both of which run in time polynomial in $(|x|,|y|)$, and\n\n\\item for any $x\\in I_1$ and any $y\\in SOL_2(f(x))$, $R_2(f(x),y) =1$ implies $R_1(x,g(x,y)) =1$, where $R_1$ and $R_2$ respectively express the performance ratios for $P_1$ and $P_2$.\n\\end{itemize}\nThe above pair $(f,g)$ is called a {\\em polynomial-time EX-reduction} (or an {\\em EXP-reduction}) from $P$ to $Q$.\n\nIt is quite useful to introduce a notion that combines both  ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}}$ and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{P}}}$. Let us define the notion of {\\em polynomial-time strong AP-reduction} (strong APP-reduction or sAPP-reduction), denoted ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{P}}}$, obtained from ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}}$ by allowing $r$ (used in the above definition of APP-reduction) to be chosen from ${\\mathbb{Q}}^{\\geq1}$ (instead of ${\\mathbb{Q}}^{>1}$).\n\nNext, we weaken the behaviors of polynomial-time (strong) APP-reductions by modifying the ``polynomial-time'' requirement imposed on the aforementioned definition of (strong) APP-condition.  When we replace ``polynomial-time'' by ``logarithmic-space,''  ``uniform family of ${\\mathrm{NC}^{ {1} }}$-circuits,'' and  ``uniform family of ${\\mathrm{AC}^{ {0} }}$-circuits,''  we respectively obtain the corresponding notions of {\\em (strong) APL-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$),  {\\em (strong) APNC$^1$-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$), and {\\em (strong) APAC$^0$-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{AC}^{ {0} }}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$). Notice that the notion of error-preserving reduction (or E-reduction), which was used in \\cite{Tan07}, essentially matches sAPL-reduction. Likewise, we define {\\em EXL-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$), {\\em EXNC$^{1}$-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$), and {\\em EXAC$^{0}$-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{AC}^{ {0} }}}$) from EXP-reduction.\n\nThe following two lemmas are immediate from the definition of sAP-reductions and we omit their proofs.\n\n\\begin{lemma}\\label{reduction-transitive}\nFor any two reduction type $e_1,e_2\\in\\{{\\mathrm{P}},{\\mathrm{L}},{\\mathrm{NC}^{ {1} }},{\\mathrm{AC}^{ {0} }}\\}$, if $e_1\\subseteq e_2$ (seen as complexity classes), then $P_1{\\leq_{\\mathrm{sAP}}}^{e_1}P_2$ implies $P_1{\\leq_{\\mathrm{sAP}}}^{e_2}P_2$. The same statement holds for ${\\leq_{\\mathrm{AP}}}^{e}$ and ${\\leq_{\\mathrm{EX}}}^{e}$.\n\\end{lemma}\n\n\\begin{lemma}\nFor any reduction type $e\\in\\{{\\mathrm{P}},{\\mathrm{L}},{\\mathrm{NC}^{ {1} }},{\\mathrm{AC}^{ {0} }}\\}$, $P_1{\\leq_{\\mathrm{sAP}}}^{e} P_2$ implies both $P_1{\\leq_{\\mathrm{AP}}}^{e} P_2$ and $P_1{\\leq_{\\mathrm{EX}}}^{c} P_2$.\n\\end{lemma}\n\nIn the next lemma, we shall present a useful property, called a {\\em downward closure property}, for ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reductions. A similar property holds also for ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions.\n\n\\begin{lemma}\\label{downward-property}\n[downward closure property]\nLet $P$ and $Q$ be any two optimization problems in ${\\mathrm{NLO}}$.\n\\begin{enumerate}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{APXNC}^{ {1} }},{\\mathrm{NC}^{ {1} }\\mathrm{AS}}\\}$. If $P{\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$, where ${\\mathrm{NLO}}_{{\\mathrm{NLO}}}$ is understood as ${\\mathrm{NLO}}$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{LO}},{\\mathrm{NC}^{ {1} }\\mathrm{O}}\\}$. If $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$.\n\\end{enumerate}\n\\end{lemma}\n\nAn immediate consequence of Lemma \\ref{downward-property} is the following corollary. In comparison, by setting  ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{LO}}\\}$, for any $P,Q\\in {\\mathrm{NLO}}$, if $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$ \\cite{Tan07}.\n\n\\begin{corollary}\nLet ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{LO}}\\}$. For any $P,Q\\in {\\mathrm{NLO}}$, if $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$, where ${\\mathrm{NLO}}_{{\\mathrm{NLO}}}$ is ${\\mathrm{NLO}}$.\n\\end{corollary}\n\nHere, we shall briefly give the proof of Lemma \\ref{downward-property}.\n\n\\begin{proofof}{Lemma \\ref{downward-property}}\nTake any two optimization problems $P=(I_1,SOL_1,m_1,goal_1)$ and $Q=(I_2,SOL_2,m_2,goal_2)$ in ${\\mathrm{NLO}}$. In what follows, we shall prove only the case of ${\\cal D}={\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, because the other cases can be similarly\ntreated.\n\n(1)  Assume that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}} Q$ via an APNC$^{1}$-reduction $(f,g,c)$ and that $Q$ is in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. Given any constant $r'>1$, let $C_{r'}$ be an  NC$^{1}$ $r'$-approximate algorithm solving $Q$.\nTo show that $P\\in{\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, it suffices to construct, for each constant $r>1$, an appropriate NC$^{1}$ circuit, say, $D_r$ that finds $r$-approximate solutions for $P$.\n\nGiven a constant $r>1$, let us define $r'=1+(r-1)/c >1$ and consider $C_{r'}$.  Since $C_{r'}$ is an NC$^{1}$ $r'$-approximate algorithm, it follows that the performance ratio $R_2$ for $C_{r'}$ satisfies $R_2(z,C_{r'}(z))\\leq r'$ for any $z\\in I_2$.\nNext, we define the desired algorithm $N_{r}$ as follows: on input $x\\in I_1$, compute simultaneously $z= f(x,r)$ and $y = C_{r'}(z)$ and then output $g(x,y,r')$. Since $R_2(z,C_{r'}(z))\\leq r'$, it follows by the definition of ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$ that $R_1(x,N_{r}(x)) = R_1(x,g(x,C_{r'}(z),r'))\\leq 1+c(r'-1)=r$. Hence, $N_r$ is an $r$-approximate algorithm for $P$.\n\nWe still need to show that $N_{r}$ can be realized by an NC$^{1}$-circuit.\nFor this purpose, we prepare an NC$^{1}$ circuit $C_f$ that, on input $(x,e)\\in \\Sigma^*\\times 1\\{0,1\\}^*$,  outputs the $rep(e)$-th bit of $f(x,r)$. Notice that  $|f(x,r)|$ is polynomially bounded.\nMoreover, let $C_g$ denote an NC$^{1}$ circuit computing $g$.\nWe construct an NC$^{1}$-circuit $M'$ that, on input $(x,e)\\in \\Sigma^*\\times 1\\{0,1\\}^*$, computes the $rep(e)$-th bit of $C_{r'}(f(x,r'))$. During this procedure, whenever $C_{r'}$ tries to access the $j$th bit of $f(x,r')$, we run $C_f$ on $(x,bin(j))$.\nThe desired algorithm $N_{r}$ is executed as follows. We first run $C_g$ using the first and third input tapes for $x$ and $r'$ and leaving the second tape blank. Whenever $C_g$ tries to access the $i$th bit $y_i$ of $y=C_{r'}(f(x,r'))$, we run $M'$ on $(x,bin(i))$. It is not difficult to show that this procedure can be implemented on an appropriate NC$^{1}$-circuit.\n\n(2) Assume that $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}} Q$ via an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g)$ with  $Q\\in{\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.  Since $Q\\in{\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$,\nthere exists an NC$^{1}$ circuit $M$ for which  $M(x)\\in SOL_{2}(x)$ and $R_2(x,M(x))=1$ for any $x\\in I_2$. To show that $P$ is in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$, let us consider the following algorithm $N$. On input $x$, compute $y=M(f(x))$ and output $w=g(x,y)$. For a similar reason to (1), $N$ can be implemented by a certain NC$^{1}$ circuit. Since $R_1(x,N(x))=R_1(x,g(x,y))$, by the definition of an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction, we obtain $R_2(f(x),M(f(x))) = 1$. Therefore, $N$ exactly solves $P$.\n\\end{proofof}\n\nOur AP-, EX-, and sAP-reductions can help us identify the most difficult problems in a given optimization/approximation class.\nSuch problems are generally called ``complete problems,'' which have played a crucial role in understanding the structural features of\noptimization and approximation classes.\n\nFormally, let $\\leq$ be any reduction discussed in this section, and let  ${\\cal D}$ be any class of optimization problems. An optimization problem $P$ is called {\\em $\\leq$-hard} for ${\\cal D}$ if, for every problem $Q$ in ${\\cal D}$,  $Q\\leq P$ holds. Moreover, $P$ is said to be {\\em $\\leq$-complete} for ${\\cal D}$ if $P$ is in ${\\cal D}$ and it is $\\leq$-hard for ${\\cal D}$.\nThis completeness will be a central subject in Sections \\ref{sec:completeness}--\\ref{sec:PBP}.\n\n\n\n\n\\section{General Complete Problems}\\label{sec:completeness}\n\nComplete problems represent a certain structure of a given optimization or approximation class and they provide useful insights into specific features of the class. To develop a coherent theory of NLO problems, it is essential to study such complete problems. In the subsequent subsections, we shall present numerous complete problems for various optimization and approximation classes.\n\n\n\\subsection{Why APNC$^{1}$- and EXNC$^{1}$-Reductions?}\\label{sec:why-NC1}\n\nTo discuss complete problems for refined optimization and approximation classes under certain reductions, it is crucial to choose reasonable types of reductions. By Lemma \\ref{reduction-transitive}, for example, any ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problem for an optimization/approximation class ${\\cal D}$ is also ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$-complete, but the converse may not be true in general. In what follows, we briefly argue that the  ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reductions are so powerful that all problems in  ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ respectively become reducible to similar problems residing even in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ and ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{proposition}\\label{characterization}\n\\begin{enumerate}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item ${\\mathrm{APXL}}_{{\\mathrm{NLO}}} = \\{P\\in{\\mathrm{NLO}}\\mid \\exists Q\\in {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}} \\cap{\\mathrm{PBO}} \\,[ P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}} Q]\\}$.\n\n\\item ${\\mathrm{LO}}_{{\\mathrm{NLO}}} = \\{P\\in{\\mathrm{NLO}}\\mid \\exists Q\\in {\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}} \\cap{\\mathrm{PBO}} \\,[ P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}} Q]\\}$.\n\\end{enumerate}\n\\end{proposition}\n\n\\begin{proof}\n(1) This claim is split into two opposite containments.\n\n($\\supseteq$) Let $P\\in {\\mathrm{NLO}}$ and $Q\\in {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$, and assume that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$. Notice that $Q$ also belongs to ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\nLemma \\ref{downward-property}(1) therefore implies that $P\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\n($\\subseteq$) Since  ${\\mathrm{APXL}}_{{\\mathrm{NLO}}} = {\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}\\cup {\\mathrm{APXL}}_{{\\mathrm{MinNL}}}$, we first consider the case of ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. Take any maximization problem $P=(I_1,SOL_2,m_1,\\text{\\sc max})$ in  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.  We want to define a new maximization problem $Q = (I_2,SOL_2,m_2,\\text{\\sc max})$ and show that $Q\\in{\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$ and $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$.\n\nSince $P\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, there exists a constant $e\\in{\\mathbb{Q}}^{>1}$ and a log-space deterministic Turing machine $M$ that produces $e$-approximate solutions of $P$; namely, the performance ratio $R_1$ of $M$'s outcome for $P$ satisfies $R_1(x,M(x))\\leq e$ for every $x\\in (I_1\\circ SOL_1)^{\\exists}$.\nFirst, we set $I_2$ to be composed of all instances of the form $(x,M(x))$ for  $x\\in I_1$. Notice that, whenever $SOL_1(x)={\\mathrm{\\O}}$, $M$ outputs the designated symbol $\\bot$. Since $M$ uses only log space, $I_2\\in{\\mathrm{L}}$ follows. Next, we define $SOL_2(x,y) = SOL_1(x)$ and $m_2((x,y),z)= m_1(x,z)$ for any $x\\in I_1$ and $y,z\\in SOL_1(x)$. By those definitions, $Q$ is a problem in ${\\mathrm{NLO}}$.\n\nLet us consider an ${\\mathrm{AC}^{ {0} }}$-circuit that outputs $y$ on admissible instance   $(x,y)$ in $(I_2\\circ SOL_2)^{\\exists}$. For any $(x,y)\\in (I_2\\circ SOL_2)^{\\exists}$, it follows that $R_2((x,y),C(x,y)) = \\frac{m_2^*(x,y)}{m_1((x,y),C(x,y))} =  \\frac{m_1^*(x)}{m_1(x,y)} = R_1(x,M(x))\\leq e$, where $R_2$ means the performance ratio for $Q$.\nHence, $C(x,y)$ is an $e$-approximate solution of $Q$. Thus, $Q$ belongs to ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\nNext, we want to show that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}} Q$ via $(f,g,1)$. Take any number $r\\in{\\mathbb{Q}}^{\\geq1}$ and define $f(x,r)=(x,M(x))$ and $g((x,y),z,r)=z$ for $x\\in I_1$ and $y,z\\in SOL_1(x)$. It follows that $R_2(f(x,r),z) = \\frac{m_1^*(x)}{m_1(x,z)} = \\frac{m_1^*(x)}{m_1(x,g((x,y),z,r))} =R_1(x,g((x,y),z,r))$. Since  $f$ is in ${\\mathrm{FL}}$ and $g$ is in ${\\mathrm{FAC}^{ {0} }}$, $P$ indeed  sAPL-reduces to $Q$. Because $P$ is arbitrary, we conclude that every maximization problem in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-reducible to $Q$.\n\nIn a similar fashion, we can show that every minimization problem $P'$ in ${\\mathrm{APXL}}_{{\\mathrm{MinNL}}}$ can be reduced to a certain minimization problem $Q'$ in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\n(2) This claim can be proven in a similar way to (1).\n\n($\\supseteq$) Take two optimization problems  $P\\in{\\mathrm{NLO}}$ and $Q\\in{\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Assume that $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}Q$ via $(f,g)$. Lemma \\ref{downward-property}(2) then ensures that $P$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\n($\\subseteq$) We begin with the case of ${\\mathrm{LO}}_{{\\mathrm{MaxNL}}}$.\nLet $P=(I_1,SOL_1,m_1,\\text{\\sc max})$ be an arbitrary problem in ${\\mathrm{LO}}_{{\\mathrm{MaxNL}}}$ and take a deterministic Turing machine $M$ that solves $P$ using log space.\nWe intend to construct another problem $Q = (I_2,SOL_2,m_2,\\text{\\sc max})$ so that  $P$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reducible to $Q$. For this desired problem $Q$, we set $I_2= I_1\\circ SOL_1$ and $SOL_2(x,y)=\\{y\\}$ for any $(x,y)\\in I_2$. The measure function $m_2$ is defined as $m_2((x,y),z)=2$ if $y=z$, and $1$ otherwise. Obviously, $m_2$ is polynomially bounded and is in ${\\mathrm{FAC}^{ {0} }}$.\nFor a particular input $(x,M(x))$, since $m_2((x,M(x)),M(x)) = m_2^*(x,M(x))$, we obtain $M(x)\\in SOL_2^*(x,M(x))$. Thus, it follows that $Q\\in{\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Finally, we define a reduction $(f,g)$ as\n$f(x)=(x,M(x))$ and $g(x,z)=M(x)$ for any $z\\in SOL_2(f(x))$. Note that $R_2(f(x),z) =1$ implies $z=M(x)$, and thus the performance ratio $R_1(x,g(x,z))$ for $P$ satisfies $R_1(x,g(x,M(x))) = R_1(x,M(x)) =1$. Therefore, $(f,g)$ ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reduces $P$ to $Q$.\n\\end{proof}\n\nProposition \\ref{characterization} suggests that  the notions of ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-completeness do not capture the essential difficulty of the optimization complexity class ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Therefore, in what follows, we intend to use weaker types of reductions. In particular, we limit our interest within  ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions.\n\n\n\n\nAs a quick example of ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problems, let us consider the {\\em minimum weighted $s$-$t$ cut problem} ({\\sc Min Weight-st-Cut}), which is to find an $s$-$t$ cut of a given weighted directed graph so  that the {\\em (weighted) capacity}  of the cut (i.e., the total weight of edges from $S_0$ to $S_1$)  is minimized, where an {\\em $s$-$t$ cut} for two distinct vertices $s,t\\in V$ is a partition $(S_0,S_1)$ of the vertices for which $s\\in S_0$ and $t\\in S_1$.\nWe represent this cut $(S_0,S_1)$ by an assignment $\\sigma$ from $V$ to $\\{0,1\\}$ satisfying the following condition: for every $v\\in V$ and every $i\\in\\{0,1\\}$, $\\sigma(v)=i$ iff $v\\in S_i$.\n\n{\\medskip}\n{\\sc Minimum Weighted s-t Cut Problem} ({\\sc Min Weight-st-Cut}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\\item {\\sc instance:} a directed graph $G=(V,E)$, two distinguished vertices $s,t\\in V$, where $s$ is a {\\em source} and $t$ is a {\\em sink} (or a {\\em target}), and an edge weight function $c:E\\to{\\mathbb{N}}^{+}$.\n\n\\item {\\sc Solution:} an $s$-$t$ cut $(S_0,S_1)$, specified by an assignment $\\sigma:V\\to\\{0,1\\}$ as described above.\n\n\\item {\\sc Measure:} the {\\em (weighted) capacity} of the $s$-$t$ cut (i.e., $\\sum_{(v,w)\\in E\\wedge v\\in S_0\\wedge w\\in S_1}c(v,w)$).\n\\end{itemize}\n\nNote that the capacity of any $s$-$t$ cut is at most $\\max_{e\\in E}\\{c(e)\\}|E|$. It is possible to prove that {\\sc Min Weight-st-Cut} is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\n\\begin{proposition}\\label{min-st-cut-is-po}\n{\\sc Min Weight-st-Cut} is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\\end{proposition}\n\nThe proof of Proposition \\ref{min-st-cut-is-po} can be obtained by an appropriate modification of the ${\\mathrm{P}}$-completeness proof of Goldschlager {\\textrm{et al.}}~\\cite{GSS82} for the ``decision version'' of the {\\em maximum $s$-$t$ flow problem}.\nThe proof of Proposition \\ref{min-st-cut-is-po} is placed in Appendix for readability. The proposition will be used in Section \\ref{sec:complexity-OP}.\n\n\n\\subsection{Complete Problems Concerning Path Weight}\\label{sec:general-complete}\n\nWe have seen in Section \\ref{sec:why-NC1} the importance of ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions for discussing the computational complexity of our refined optimization problems.\nIn this and the next subsections under those special reductions, we shall present a few complete problems for various optimization and approximation classes.\n\nThere are two categories of NLO problems to distinguish in our course of studying  the complexity of NLO problems. The first category contains NLO problems $(I,SOL,m,goal)$ for which the set $(I\\circ SOL)^{\\exists}$ ($=\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$) belongs to ${\\mathrm{NL}}$ but may not fall into ${\\mathrm{L}}$ unless ${\\mathrm{L}} = {\\mathrm{NL}}$. The second category, in contrast, requires the set $(I\\circ SOL)^{\\exists}$ to be in ${\\mathrm{L}}$. Many of the optimization problems of the first category are unlikely to fall into ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ or ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\nFirst, we shall look into an optimization analogue of\nthe well-known {\\em directed $s$-$t$ connectivity problem} (also known as the {\\em graph accessibility problem} and the {\\em graph reachability problem} in the past literature), denoted by $\\mathrm{DSTCON}$, in which, for any  directed graph $G=(V,E)$ and two vertices $s,t\\in V$, we are asked to determine whether there is a path from $s$ to $t$ in $G$.\nEarlier, Jones \\cite{Jon75} showed that $\\mathrm{DSTCON}$ is ${\\mathrm{NL}}$-complete under $\\leq_{m}^{{\\mathrm{L}}}$ (log-space many-one) reductions. These reductions can be replaced by appropriate $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-reductions, and thus  $\\mathrm{DSTCON}$ becomes $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NL}}$.\nLet us consider a series of problems associated with minimum path weights of  graphs. First, recall the minimum path weight problem ({\\sc Min Path-Weight}) introduced in Section \\ref{sec:intro}.\n\n{\\medskip}\n{\\sc Minimum Path Weight Problem} ({\\sc Min Path-Weight}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a directed graph $G=(V,E)$, two distinguished vertices $s,t\\in V$, and a (vertex) weight function $w:V\\to{\\mathbb{N}}$.\n\n\\item {\\sc Solution:} a path ${\\cal S}=(v_1,v_2,\\ldots,v_k)$ from $s$ to $t$ (i.e., $s=v_1$ and $t=v_k$).\n\n\\item {\\sc Measure:} ``biased'' path weight $w(S) = \\max\\{1,rep(bin(w(v_1))bin(w(v_2))\\cdots bin(w(v_k)))\\}$.\n\\end{itemize}\n\nIn the above definition, we generally do not demand that $s$ is a {\\em source} (i.e., a node of indegree $0$) and $t$ is a {\\em sink} (i.e., a node of outdegree $0$)  although such a restriction does not change the completeness of the problem.\n\nHere, we need to remark that the choice of our measure function for {\\sc Min Path-Weight} is quite artificial.\nAs a quick example, if ${\\cal S}=(v_1,v_2,v_3,v_4)$ with $w(v_1)=3$, $w(v_2)=0$,  $w(v_3)=2$, and $w(v_4)=4$, then $w({\\cal S}) = rep(1110100)$ since $bin(0)=\\lambda$ (the empty string).\nIt is important to note that we use the {\\em biased path weight} instead of a {\\em standard path weight} defined as $\\sum_{i\\in[k]}w(v_i)$. This comes from the fact that,  because log-space computation cannot store super-logarithmically many bits, it cannot sum up all super-logarithmically large weights of vertices. However, if we set all vertices of a given input graph have weights of exactly  $1$, then {\\sc Min Path-Weight} is essentially identical to a problem of finding the ``shortest'' $s$-$t$ path in the graph.\n\nIn comparison, we also define a polynomially-bounded form of {\\sc Min Path-Weight} simply by demanding that $1\\leq w(v)\\leq|V|$ for all $v\\in V$ and by changing $w({\\cal S})$ to the {\\em total path weight} $w'({\\cal S})=\\sum_{i=1}^{k}w(v_i)$. Notice that $w'({\\cal S})\\leq k|V|$. For our later reference in Section \\ref{sec:PBP}, we call this modified problem the {\\em minimum bounded path weight problem} ({\\sc Min BPath-Weight}) to emphasize the polynomially-boundedness of the problem.\n\nHereafter, we shall prove that {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{MinNL}}$.\n\n\\begin{theorem}\\label{Min-Path-complete}\n$\\text{\\sc Min Path-Weight}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{MinNL}}$.\n\\end{theorem}\n\nFor the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hardness part of Theorem \\ref{Min-Path-complete}, we want to introduce a useful notion of {\\em configuration graph} of a log-space auxiliary Turing machine $M$ on a given input $x$ together with any possible auxiliary input $y$,  which describes an entire computation tree of $M$ working on $x$ and $y$.\nThis is a weighted directed graph, which will be used in later proofs, establishing the hardness of target optimization problems; however, in those proofs, we may need to modify  the original configuration graph given below.\nSince each vertex of a configuration graph is labeled by a ``partial configuration, '' we first define such partial configurations of $M$ on $x$. To simplify the following description, we consider the case where $M$ has only one work tape.\n\nRecall from Section \\ref{sec:basic_model} an auxiliary Turing machine $M = (Q,\\Sigma,\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\Gamma,\\Theta,\\Phi,q_0,q_{acc},q_{rej})$ with its transition function $\\delta$ mapping $(Q-\\{q_{acc},q_{rej}\\}) \\times (\\Sigma\\cup\\{{|}\\!\\!\\mathrm{c},{\\$},\\lambda\\}) \\times \\Gamma \\times (\\Theta\\cup\\{{\\$}\\})$ to $Q\\times \\Gamma\\times (\\Phi\\cup\\{\\lambda\\}) \\times D\\times D_1$.\nThe current tape situation is encoded into $uhw$, which indicates that  the tape content is $uv$ and the tape head is scanning the leftmost symbol of $w$, where $h$ is a special symbol representing the tape heard. A {\\em partial configuration}  of $M$ on input $x$ is a tuple $v = {\\langle {q,w,u,\\tau,\\xi,k} \\rangle}$, which intuitively indicates a snap shot of $M$'s computation at time $k$ ($k\\in{\\mathbb{N}}$) when $q$ is an inner state, $w$ is an encoding of $M$'s input tape, $u$ is an encoding of $M$'s work tape, $\\tau$ is a scanning auxiliary input symbol, and $\\xi$ is an output symbol or $\\lambda$ to write.\n\nWe connect each partial configuration $v=(q,w,u,\\tau,\\xi,k)$ to  others $(p,w',u',\\tau',\\xi',k+1)$ for all $\\tau'\\in\\in\\Gamma\\cup\\{{\\$}\\}$ by applying a transition ``$\\delta(q,\\sigma'_1,\\sigma'_2,\\tau) = (p,\\sigma'_2,\\xi',d_1,d_2)$,'' where $w'$ (resp., $u'$) is an encoding of the input (resp., work) tape obtained from $w$ (resp., $u$) by this transition.\nWhen a machine makes a $\\lambda$-move on the output tape, we use the same symbol ``$\\lambda$'' in place of  $\\xi$ and $\\xi'$. Here, we encode such partial configurations into binary strings of the same length by padding extra garbage bits (if necessary).\n\nThe {\\em weight} of this vertex $v$  is defined as $\\xi$ (expressed in binary).\nWe can view a {\\em computation path} $y$ of $M$ on $x$ together with a series of nondeterministic choices of $M$,  as a sequence of partial configurations.\nFor two vertices $u$ and $v$, $(u,v)$ is a direct edge if, seen as partial configurations, $v$ is obtained from $u$ by a single application of $\\delta$ and a choice of auxiliary input symbol.\nSince  each vertex is represented by $O(n)$ symbols, the total number of  vertices is at most a polynomial in $n$. We denote by $G^{M}_{x}$ the obtained configuration graph of $M$ on $x$ since $M$ halts in polynomial time.\nNote that the size of $G^{M}_{x}$ is bounded from above by a polynomial in the size of input instance $x$ of $M$.\n\nIt is important to note that, from a given encoding of a computation path $y$, we can easily extract an associated auxiliary input, because each partial configuration in $y$ contains a piece of information on the auxiliary input and $M$'s head on the auxiliary tape moves in only one direction.\n\n\\begin{proofof}{Theorem \\ref{Min-Path-complete}}\nFor notational convenience, in the following argument, $\\text{\\sc Min Path-Weight}$ is expressed as $(I_0,SOL_0,m_0,\\text{\\sc min})$. Firstly, we want to claim that $I_0\\in{\\mathrm{L}}$. This follows from the facts that  $\\mathrm{DSTCON}\\in{\\mathrm{NL}}$ and that $(I_0\\circ SOL_0)^{\\exists}$ (more accurately, $(I_0\\circ SOL_0)^{\\exists}_{q}$ for a suitable polynomial $q$) is essentially ``equivalent'' to $\\mathrm{DSTCON}$, except for the presence of a weight function $w$.\nNext, we claim that {\\sc Min Path-Weight} belongs to ${\\mathrm{MinNL}}$.\nThis claim comes from the following facts. On input $x=(G,s,t,w)$, let ${\\cal S} = (v_1,v_2,\\ldots,v_k)$ denote an arbitrary path from $s$ to $t$ in $G$. Since $m_0(x,{\\cal S})$ equals $w({\\cal S})$ by definition, the value $m_0(x,{\\cal S})$ can be computed by an appropriate auxiliary Turing machine that writes down $bin(w(v_i))$ sequentially on a write-only output tape using $O(\\log{n})$ space-bounded work tapes.\nSimilarly, given $x=(G,s,t,w)$ and an arbitrary sequence ${\\cal S}$ of vertices, we can decide whether ${\\cal S}\\in SOL_0(x)$ by checking whether ${\\cal S}$ is a path from $s$ to $t$ using a certain log-space auxiliary Turing machine.\n\nSecondly, we shall claim that {\\sc Min Path-weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hard for ${\\mathrm{MinNL}}$; namely, every minimization problem in ${\\mathrm{NLO}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to {\\sc Min Path-Weight}.\nTo prove this claim, let $P=(I,SOL,m,\\text{\\sc min})$ be any minimization problem in ${\\mathrm{NLO}}$. Note that $I\\in{\\mathrm{L}}$, $I\\circ SOL\\in{\\mathrm{auxL}}$, and $m\\in{\\mathrm{auxFL}}$.\nSince $m\\in{\\mathrm{auxFL}}$, we take an appropriate log-space auxiliary Turing machine $M$\n(with three tapes) computing $m$, where any solution candidate to $P$ is provided on an auxiliary read-once tape.\nNotice that there is a unique initial partial configuration. To ensure that $M$ has a {\\em unique} accepting partial configuration, it suffices to force $M$ to clear out all tapes just before entering a unique accepting state.\n\nLet us define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,1)$ from $P$ to {\\sc Min Path-Weight} as follows. Let $r\\geq1$ and define  $f(x,r)$ to be a configuration graph $G^{M}_{x}$ of $M$ on input $x$.\nIf $x\\in I$, then $f(x,r)\\in I_0$. Let $s$ denote the initial partial configuration of $M$ on $x$ and let $t$ be the unique accepting partial configuration of $M_2$ on $x$. As a solution to {\\sc Min Path-Weight}, let $y$ be any path in the graph $f(x,r)$ starting with $s$.\nEach vertex in $y$ contains the information on content $\\tau_i$ of the tape cell at which the auxiliary-tape head scans at time $i$. Hence, from $y$, we can recover the content of the auxiliary tape as follows. Given $y=(y_0,y_1,\\ldots,y_m)$ with $y_i=(q_i,w_i,u_i,\\tau_i,\\xi_i,k_i)$, we retrieve $\\tau_i$ for all indices $i\\in[0,m]_{{\\mathbb{Z}}}$ and output $\\tau_0\\tau_1\\tau_2\\cdots \\tau_m$. This procedure requires only an AC$^{0}$ circuit.\nLet $g(x,y,r)$ denote the entire content of the auxiliary  tape that is reconstructed from $y$ as described above. Clearly, $g$ is in ${\\mathrm{FAC}^{ {0} }}$ and, for any $y\\in SOL(x)$, we obtain $g(x,y,r)\\in SOL_0(f(x,r))$.  It is not difficult to show that $m(f(x,r),y) = m_0(x,g(x,y,r))$. Hence, $R_2(f(x,r),y)$ equals $R_1(x,g(x,y,r))$.\n\nTo complete the proof, we still need to verify that $f$ belongs to  ${\\mathrm{FAC}^{ {0} }}$.\nFor this, consider the following procedure. Recall that a graph is represented by a list of edges (i.e., vertex pairs). Starting with any input $x=(G,s,t,w)$ and $r\\geq1$, generate all pairs $(u,v)$ of partial configurations and mark $(u,v)$ whenever it is an edge of $G^{M}_{x}$. This procedure needs to wire only a finite number of bits between $u$ and $v$. Hence, $f$ can be computed by an ${\\mathrm{AC}^{ {0} }}$ circuit.\n\nTherefore, {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}$.\n\\end{proofof}\n\n\n\nIn contrast to {\\sc Min Path-Weight}, it is possible to define a maximization problem, {\\sc Max Path-Weight}, simply by taking the maximally-weighted $s$-$t$ path for the minimally-weighted one in the definition of {\\sc Min Path-Weight}. A similar argument in the proof of Theorem \\ref{Min-Path-complete} establishes the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc max Path-Weight} for ${\\mathrm{MaxNL}}$.\n\n\\begin{corollary}\\label{Max-Path-Weight-complete}\n{\\sc Max Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MaxNL}}$.\n\\end{corollary}\n\nIs {\\sc Min Path-weight} also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MaxNL}}$ and thus for ${\\mathrm{NLO}}$ ($={\\mathrm{MaxNL}}\\cup{\\mathrm{MinNL}}$)?\nUnlike NPO problems, the log-space limitation of work tapes of Turing machines complicates the circumstances around NLO problems.\nAt present, we do not know that {\\sc Min Path-Weight} is  ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}$. This issue will be discussed later in Section \\ref{sec:PBP}.\nUnder a certain assumption on ${\\mathrm{auxFL}}$, nevertheless, it is possible to achieve the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc Min Path-Weight} for ${\\mathrm{NLO}}$.\n\nWe say that ${\\mathrm{auxFL}}$ is {\\em closed under division} if, for any two functions $f,g\\in{\\mathrm{auxFL}}$ outputting natural numbers in binary, the function $h$ defined by $h(x,y)={\\lceil {f(x,y)/g(x,y)} \\rceil}$ for all inputs $x$ and all auxiliary inputs $y$ is in ${\\mathrm{auxFL}}$, provided that $g(x,y)>0$ for all inputs $(x,y)$.\n\n\\begin{proposition}\\label{Min-Path-in-NLO}\nAssume that ${\\mathrm{auxFL}}$ is closed under division. {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}$.\n\\end{proposition}\n\nWe have already proven that {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}$ in Theorem \\ref{Min-Path-complete}. In Lemma \\ref{reduction-MaxNL-MinNL}, we shall demonstrate that every problem $P_1$ in ${\\mathrm{MaxNL}}$ is sAP${\\mathrm{AC}^{ {0} }}$-reducible to an appropriately chosen  problem $P_2$ in ${\\mathrm{MinNL}}$ if ${\\mathrm{auxFL}}$ is closed under division. Since ${\\mathrm{NLO}}= {\\mathrm{MaxNL}}\\cup {\\mathrm{MinNL}}$, this implies that {\\sc Min Path-Weight} is also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{MaxNL}}$, completing the proof of Proposition \\ref{Min-Path-in-NLO}.\n\nWe shall prove the remaining lemma, Lemma \\ref{reduction-MaxNL-MinNL}.\n\n\\begin{lemma}\\label{reduction-MaxNL-MinNL}\nAssume that ${\\mathrm{auxFL}}$ is closed under division. Every problem $P_1$ in ${\\mathrm{MaxNL}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to an appropriate  problem $P_2$ in ${\\mathrm{MinNL}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $P_1=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any optimization problem in ${\\mathrm{MaxNL}}$.   Take an appropriate polynomial $p$ satisfying $2^{p(|x|)} \\geq m_1^*(x)$ for every instance $x\\in I_1$. For brevity, we set $b(x) = 2^{p(|x|)}$ for all $x\\in I_1$. We shall construct the desired minimization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc min})$ in ${\\mathrm{MinNL}}$.\nLet $I_2=I_1$ and $SOL_2=SOL_1$. Moreover, for every pair  $(x,y)\\in I_2\\circ SOL_2$,  define $m_2(x,y) = {\\lceil {\\frac{b(x)^2}{m_1(x,y)}} \\rceil}$.\nIt is important to note that, by our definition of measure function, $m_1$ always returns {\\em positive values}. From this definition, it follows that  $\\frac{b(x)^2}{m_2(x,y)}\\leq m_1(x,y) \\leq \\frac{b(x)^2}{m_2(x,y)+1}$ for any $(x,y)\\in I_2\\circ SOL_2$.\n\nClearly, $I_2\\in{\\mathrm{L}}$ and $I_2\\circ SOL_2\\in{\\mathrm{auxFL}}$. From our assumption on the closure property of ${\\mathrm{auxFL}}$ under division, $m_2$ falls into ${\\mathrm{auxFL}}$.\nTherefore, $P_2$ belongs to ${\\mathrm{NLO}}$.\n\nLet us define an sAPAC$^{0}$-reduction $(f,g,c)$ from $P_1$ to $P_2$ as follows.  Let  $f(x,r)=x$ and $g(x,y,r)=y$ for $r\\in{\\mathbb{Q}}^{\\geq1}$, $x\\in I_1$, and $y\\in SOL_2(f(x,r))$. Obviously, $f,g\\in{\\mathrm{FAC}^{ {0} }}$ follows.\nIf $R_2(f(x,r),y)\\leq r$ for $r\\geq1$; namely, $m_2^*(x)/r \\leq m_2(x,y)\\leq m_2^*(x)$, then\nthe performance ratio $R_1(x,g(x,y,r))$ for $P_1$ is upper-bounded as\n", "itemtype": "equation", "pos": 46118, "prevtext": "\n\n\n\n\\begin{center}\n{\\Large {\\bf Uniform-Circuit and Logarithmic-Space Approximations of {\\smallskip}\\\\\nRefined Combinatorial Optimization Problems}}\\footnote{A preliminary report  appeared in the Proceedings of the 7th International Conference on Combinatorial Optimization and Applications (COCOA 2013), Chengdu, China, December 12--14, 2013, Lecture Notes in Computer Science, Springer-Verlag, vol.8287, pp.318--329, 2013.} {\\bigskip}\\\\\n{\\sc Tomoyuki Yamakami}\\footnote{Present Affiliation: Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan} {\\bigskip}\\\\\n\\end{center}\n\n\\pagestyle{plain}\n\n\n\\begin{quote}\n{\\noindent}{\\bf Abstract:}\nA significant progress has been made in the past three decades over the study of combinatorial NP optimization problems and their associated optimization and approximate classes, such as NPO, PO, APX (or APXP), and PTAS. Unfortunately, a collection of problems that are simply placed inside the P-solvable optimization class PO never have been studiously analyzed regarding their exact computational complexity. To improve this situation, the existing framework based on polynomial-time computability  needs to be expanded and further refined for an insightful analysis of various approximation algorithms targeting optimization problems within PO.\nIn particular, we deal with those problems characterized in terms of logarithmic-space computations and uniform-circuit computations.\nWe are focused on nondeterministic logarithmic-space (NL)  optimization problems or NPO problems.\nOur study covers a wide range of optimization and approximation classes, dubbed as, NLO, LO, APXL, and LSAS as well as new classes NC$^{1}$O, APXNC$^{1}$, NC$^{1}$AS, and AC$^{0}$O, which are founded on uniform families of Boolean circuits.\nAlthough many NL decision problems can be naturally converted into NL optimization (NLO) problems, few NLO problems have been studied vigorously.\nWe thus provide a number of new NLO problems falling into\nthose low-complexity classes.\nWith the help of NC$^{1}$ or AC$^{0}$ approximation-preserving reductions, we also identify the most difficult problems (known as complete problems) inside those classes. Finally, we demonstrate a number of collapses and separations among those refined optimization and approximation classes with or without unproven complexity-theoretical assumptions.\n\n{\\smallskip}\n\n{\\noindent}{\\bf Keywords:}\noptimization problem, approximation-preserving reduction, approximation algorithm, NC$^1$ circuit, AC$^{0}$ circuit, logarithmic space, complete problem\n\\end{quote}\n\n\n\n\\sloppy\n\n\\section{Refined Combinatorial Optimization Problems}\\label{sec:intro}\n\n\\subsection{NL Optimization Problems}\n\nMany combinatorial problems can be understood as sets of constraints (or requirements), which specify certain relations between {\\em admissible instances}  and {\\em feasible solutions}. Of such problems, a {\\em combinatorial optimization problem}, in particular, asks to find an ``optimal'' solution that satisfies  certain constraints specified by  each given admissible instance, where the optimality usually takes a form of either ``maximization'' or ``minimization'' according to a predetermined ordering over all feasible solutions.\nWhen finding  such optimal solutions is costly, we often resort to look for  solutions that are close enough to the desired optimal solutions.\nA significant progress had been made in a field of fundamental research on these combinatorial optimization problems during 1990s and its trend has continued promoting our understandings of the approximability of the problems.  In particular, {\\em NP optimization problems} (or {\\em NPO problems}, in short) have been a centerfold of our interests because of their direct connection to NP (nondeterministic polynomial time) decision problems.\n\nNPO problems are naturally derived from NP decision problems. As a typical NP problem, let us consider the {\\em CNF Boolean formula satisfiability problem} (SAT) of determining whether a satisfying assignment exists for a given Boolean formula in conjunctive normal form. It is easy to convert {\\sc SAT} to its corresponding  optimization problem, {\\sc Maximum Weighted Satisfiability}, of finding a satisfying assignment having the maximal weight. This problem is an ${\\mathrm{NPO}}$ problem.\nAs is customary, the notation ${\\mathrm{NPO}}$ also denotes the collection of such optimization problems. Of those ${\\mathrm{NPO}}$ problems, those that can be solved exactly in polynomial time form a ``tractable'' optimization class ${\\mathrm{PO}}$, whereas an approximation class ${\\mathrm{APX}}$ (which is hereafter denoted by ${\\mathrm{APXP}}$ to emphasize its feature of ``polynomial time'' in comparison with ``logarithmic space'' and ``circuits'') consists of ${\\mathrm{NPO}}$ problems whose optimal solutions are {\\em relatively approximated} within constant factors in polynomial time.\nAnother optimization problem, {\\sc Maximum Cut}, of finding a partition of a given graph into two disjoint sets that maximize the number of crossing edges falls into this approximation class ${\\mathrm{APXP}}$.\n\nUp to now, a large number of ${\\mathrm{NPO}}$ problems have been nicely  classified into those classes of optimization problems (see, e.g., \\cite[Compendium]{ACG+03}).\nAmong those optimization and approximation classes, ${\\mathrm{PO}}$ is the smallest class and has been proven to contain a number of intriguing optimization problems, including a minimization problem, {\\sc Min Weight-st-Cut}, of finding a minimal $s$-$t$ cut of a given directed graph. In a study on NPO problems, the use of {\\em approximation-preserving reductions} helps us identify the most difficult optimization problems in a given class of optimization problems and\nmany natural problems have been classified as the computationally hardest problems for ${\\mathrm{NPO}}$, ${\\mathrm{PO}}$, or ${\\mathrm{APXP}}$. Those problems are known as ``complete'' problems. {\\sc Maximum Weighted Satisfiability} and {\\sc Maximum Cut} are respectively proven to be complete for ${\\mathrm{NLO}}$ and ${\\mathrm{APXP}}$.\n\nThe above classification of optimization problems is all described from a single viewpoint of ``polynomial-time''\ncomputability and approximability and, as a result,\na systematic discussion on optimization problems inside ${\\mathrm{PO}}$ has been vastly neglected although ${\\mathrm{PO}}$ contains numerous intriguing problems of various complexities. For instance, the {\\em minimum path weight problem} ({\\sc Min Path-Weight}) is to find in a given directed graph $G$ a path ${\\cal S}=(v_1,v_2,\\ldots,v_k)$ with $k\\geq2$ from given vertex  $v_1$ to another vertex $v_k$ having its (biased) path weight having binary representation of the form  $bin(w(v_1))bin(w(v_2))\\cdots bin(w(v_k))$, where $bin(a)$ denotes the binary representation of a nonnegative integer $a$. This minimization problem {\\sc Min Path-Weight} belongs to ${\\mathrm{PO}}$.\nAnother example is the {\\em maximum Boolean formula value problem} ({\\sc Max BFVP}) of finding a maximal subset of a given set of Boolean formulas that are satisfied by a given truth assignment. This simple problem also resides inside ${\\mathrm{PO}}$; however, it apparently looks much easier to solve than {\\sc Min Path-Weight}.  This circumstantial evidence leads us to ponder that there might exist a finer and richer structure inside ${\\mathrm{PO}}$.\nConsequently, we may raise a natural question of whether it is possible to find  such a finer structure within ${\\mathrm{PO}}$.\n\nTo achieve this goal, we first seek to develop a {\\em new, finer framework}---a low-complexity world of optimization problems---and reexamine the computational complexity of such optimization problems within this new framework.\nFor this purpose, we need to reshape the existing framework of expressing optimization complexity classes by clarifying the scope and complexity of verification processes used for solutions using objective (or measure) functions.  While {\\sc Min Weight-st-Cut} is known to be one of the most difficult problems in ${\\mathrm{PO}}$ under ${\\mathrm{P}}$-reductions (even under ${\\mathrm{NC}^{ {1} }}$-reductions, shown in Proposition \\ref{min-st-cut-is-po}), the computational complexity of {\\sc Min Path-Weight} seems to be significantly lower than {\\sc Min Weight-st-Cut} residing in ${\\mathrm{PO}}$.\nTo study the fine structures inside ${\\mathrm{PO}}$, we wish to shift our interest from a paradigm of polynomial-time optimization to much lower-complexity optimization, notably logarithmic-space or uniform-circuit optimization.\n\nIn the past decades, {\\em logarithmic-space} (or {\\em log-space}) computation has exhibited intriguing features, which are often different from those of polynomial-time computation. A notable result is the closure property of ${\\mathrm{NL}}$ (nondeterministic logarithmic space) under complementation \\cite{Imm88,Sze88}.\n\n{\\`A}lvarez and Jenner \\cite{AJ93,AJ95} first studied optimization problems from a viewpoint of log-space computability and discussed a class $\\mathrm{OptL}$ of functions that compute optimal solutions using only a logarithmic amount of memory storage. In contrast, along the line of a study on NP optimization problems, Tantau \\cite{Tan07} investigated {\\em nondeterministic logarithmic-space (NL) optimization\nproblems} or {\\em NLO problems}.\nIntuitively, an NLO problem $Q$  is asked to to find its optimal solutions among all possible feasible solutions of size polynomial in input size $n$, provided that,\n(i) we can check, using only $O(\\log{n})$ memory space, whether any  given solution candidate $y$ is indeed a solution of the problem $Q$ and, if so, (ii) we can calculate the objective value of $y$ using $O(\\log{n})$ memory space.\nWe simply write ${\\mathrm{NLO}}$ for the collection of all ${\\mathrm{NLO}}$ problems. It turns out that significant differences actually exist between two optimization classes ${\\mathrm{NPO}}$ and ${\\mathrm{NLO}}$. One of the  crucial differences is caused by the way that an underlying Turing machine produces its output strings on its output tape. When a log-space machine writes such a string, the machine must produce it {\\em obliviously} because the output string is usually longer than the machine's memory size. In short, log-space computation cannot remember polynomially-many symbols.\nAs a result, unlike ${\\mathrm{NPO}}$ problems, such machines do not seem to implement a typical approximation-preserving reduction between minimization problems and maximization problems inside ${\\mathrm{NLO}}$ (see Section \\ref{sec:PBP}).\nWhen we discuss ${\\mathrm{NLO}}$ problems, we need to heed the size of objective functions. An optimization problem is {\\em polynomially bounded} if its objective (or measure) function outputs only polynomially-large integers.\n\nThroughout this paper, we shall target those intriguing NLO problems. As unfolded  in later sections, NLO problems occupy a substantial portion of PO and they include numerous important and natural problems.\nThe aforementioned problems {\\sc Min Path-Weight} and {\\sc Max BFVP} are typical examples of the NLO problems. As other examples, the class NLO contains a restricted knapsack problem, called {\\sc Max 2BCU-Knapsack}, and a restricted algebraic problem, called {\\sc Max AGen} (see Section \\ref{sec:approximation-class} for their definitions).\nWhen we refer to PO, APXP, and PTAS in the existing framework based on NPO problems, we need to clarify their underlying framework; therefore, we intend to use new notations ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ (instead of ${\\mathrm{PO}}$), ${\\mathrm{PTAS}}_{{\\mathrm{NPO}}}$ (instead of ${\\mathrm{PTAS}}$), and ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ (instead of ${\\mathrm{APXP}}$), when we discuss exact solvability and approximability of ``${\\mathrm{NPO}}$  problems.''\n\n\n\\subsection{Optimization Problems Inside NLO}\n\nBy shifting the paradigm of optimization problems, we wish to look into a world of NLO problems and to unearth rich and complex structures underlying in this  world.\nOf all ${\\mathrm{NLO}}$ problems, those that cane be {\\em ${\\mathrm{L}}$-solvable} ({\\textrm{i.e.},\\hspace*{2mm}} solvable exactly by multi-tape deterministic Turing machines using logarithmic space)\nform an optimization class ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. If we restrict input graphs of {\\sc Min Path-Weight} onto undirected forests, then the resulted problem, called {\\sc Min Forest-Path-Weight}, belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nUsing uniform families of ${\\mathrm{NC}^{ {1} }}$-circuits and ${\\mathrm{AC}^{ {0} }}$-circuits in place of log-space Turing machines used in the existing notion of AP reduction, respectively, we can introduce two extra optimization classes ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{AC}^{ {0} }}\\mathrm{O}_{{\\mathrm{NLO}}}$,\nwhere\n${\\mathrm{NC}^{ {1} }}$ refers to $O(\\log{n})$-depth polynomial-size circuits of bounded fan-in AND and OR gates and AC$^{0}$ indicates constant-depth polynomial-size circuits of unbounded fan-in AND and OR gates.\n\nIn analogy with ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$, another refined approximation class ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ is introduced using log-space approximation algorithms for NLO problems. Between ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ exists a special class of optimization problems that have log-space approximation schemes. We call this class ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$, similar to ${\\mathrm{PTAS}}_{{\\mathrm{NPO}}}$.\nIn a similar way, we define ${\\mathrm{APXNC}^{ {k} }}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXAC}^{ {k} }}_{{\\mathrm{NLO}}}$, ${\\mathrm{NC}^{ {k} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{AC}^{ {k} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$ for each index $k\\in\\{0,1\\}$.\n\nTo compare the complexity of ${\\mathrm{NLO}}$ problems, we consider {\\em approximation-preserving (AP) reduction}, {\\em exact (EX) reduction}, and {\\em strong AP (sAP) reduction} using logarithmic space or by ${\\mathrm{NC}^{ {1} }}$-circuits (or even ${\\mathrm{AC}^{ {0} }}$-circuits).\nUsing those weak reductions, we shall present  in Section \\ref{sec:completeness}--\\ref{sec:PBP} a number of concrete optimization problems that are complete for the aforementioned refined classes of optimization problems.  As discussed in Section \\ref{sec:why-NC1}, those weak reductions are necessary for low-complexity optimization problems, because strong reductions tend to obscure the essential characteristics of ``complete'' problems.\nBecause of their fundamental nature, approximation classes are quite sensitive to the use of weak reductions. To use such reductions, we need to guarantee the existence of certain approximation bounds that must be easy to estimate.\n\nUnlike NPO problems, a special attention is required for ``complete'' problems among NLO problems. Because of its logarithmic space-constraint, at this moment, it is unknown that complete problems actually exist in ${\\mathrm{NLO}}$. What we do know is the existence of complete problems for the class ${\\mathrm{MaxNL}}$ of all maximization NLO problems (or the class ${\\mathrm{MinNL}}$ of all minimization NLO problems) as shown in  Section \\ref{sec:completeness}. More specifically, we manage to demonstrate that {\\sc Min Path-Weight} is indeed complete for ${\\mathrm{MinNL}}$. A similar situation is observed also for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. In contrast, the class ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ of ${\\mathrm{L}}$-solvable NLO problems possesses complete problems. When we limit our attention to polynomially-bounded NLO problems, each of ${\\mathrm{NLO}}$, ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, and ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ actually owns complete problems (Section \\ref{sec:PBP}).\n\nAmong the aforementioned refined classes, we shall  also prove relationships concerning collapses and separations in Section \\ref{sec:complexity-OP}.\nIf we limit our optimization problems onto ${\\mathrm{NLO}}$, then ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$,   ${\\mathrm{PTAS}}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ all coincide with ${\\mathrm{NLO}}$ (Lemmas \\ref{PO=APXP=NLO} and \\ref{char-with-AC1}(1)).\nFor polynomially-bounded ${\\mathrm{NLO}}$ problems, in contrast, we can characterize them in terms of ${\\mathrm{LO}}$ problems if their underlying log-space Turing machines are further allowed to access ${\\mathrm{NL}}$ oracles.\nFollowing \\cite{Tan07}, ${\\mathrm{L}}\\neq{\\mathrm{NL}}$ if and only if the polynomially-bounded subclasses of ${\\mathrm{NLO}}$, ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ are all distinct (Theorem \\ref{refined-relation}(1)). Similarly, we can show that ${\\mathrm{NC}^{ {1} }}\\neq{\\mathrm{L}}$ if and only if  the polynomially-bounded subclasses of  ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ are all different  (Theorem \\ref{refined-relation}(2)). For much lower-complexity optimization problems, we can separate ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$, ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$,  ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$, and ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ one from another (Theorem \\ref{AC0-separation}). Those separations directly follow from  the well-known separation ${\\mathrm{AC}^{ {0} }}\\neq {\\mathrm{NC}^{ {1} }}$ \\cite{Ajt83,FSS84}.\n\nTo help the reader overview intrinsic relationships among the aforementioned optimization (complexity) classes, we include Figure~\\ref{fig:inclusion-map}, which illustrates class containments and class separations obtained in Section \\ref{sec:complexity-OP}. The last section provides a short list of open problems. \n\n\n\n\n\\begin{figure}[t]\n \\begin{center}\n \\includegraphics[width=9.5cm]{figure-hierarchy.eps}\n \\end{center}\n\\caption{A hierarchy of optimization and approximation classes. Single lines indicate simple class inclusions, whereas double lines indicate proper inclusions.}\\label{fig:inclusion-map}\n\\end{figure}\n\n\n\n\nIn the subsequent section, we shall  provide a set of basic terminology on the approximation complexity of optimization problems.\n\n\n\\section{Optimization and Approximation Preliminaries}\\label{sec:preliminaries}\n\nWe aim at {\\em refining} an existing framework for studying combinatorial optimization problems of, in particular, low computational complexity. Throughout this paper, the notation ${\\mathbb{N}}$ denotes the set of all {\\em natural numbers} (i.e., nonnegative integers) and ${\\mathbb{N}}^{+}$ indicates ${\\mathbb{N}}-\\{0\\}$. Moreover, ${\\mathbb{Q}}$ (resp., ${\\mathbb{R}}$) indicates the set of all {\\em rational numbers} (resp., {\\em real numbers}). Two special notations ${\\mathbb{Q}}^{>1}$ and ${\\mathbb{Q}}^{\\geq1}$ respectively express the sets $\\{q\\in{\\mathbb{Q}}\\mid q>1\\}$ and $\\{q\\in{\\mathbb{Q}}\\mid q\\geq 1\\}$.\nGiven two numbers $m,n\\in{\\mathbb{N}}$ with $m\\leq n$, an {\\em integer interval}  $[m,n]_{{\\mathbb{Z}}}$ is the set $\\{m,m+1,m+2,\\ldots,n\\}$. In the case of $[1,n]_{{\\mathbb{Z}}}$ for $n\\geq1$, we abbreviate it as $[n]$.\nA {\\em(multi-variate) polynomial} is always assumed to have nonnegative integer coefficients. We also  assume that all logarithms are {\\em  to base $2$}.\n\nFor any set $A$, ${\\cal P}(A)$ denotes the {\\em power set} of $A$, i.e., the set of all subsets of $A$. Given two sequences ${\\cal S}=(s_1,s_2,\\ldots,s_k)$ and ${\\cal S}'=(t_1,t_2,\\ldots,t_m)$, the notation ${\\cal S}*{\\cal S}'$ denotes a concatenated sequence $(s_1,s_2,\\ldots,s_k,t_1,t_2,\\ldots,t_m)$.\n\nAn {\\em alphabet} is a finite nonempty set of ``symbols'' and a {\\em string} (or a {\\em word}) over alphabet $\\Sigma$ is a finite series of symbols taken from $\\Sigma$. In particular, the {\\em empty string} is denoted $\\lambda$. Let $|x|$ denote the {\\em length} of string $x$. The set $\\Sigma^*$ is composed of all strings over $\\Sigma$ and $\\Sigma^{+}$ denotes $\\Sigma^*-\\{\\lambda\\}$. A {\\em language} over $\\Sigma$ is a subset of $\\Sigma^*$.  Given two languages $A$ and $B$, their {\\em disjoint union} $A\\oplus B$ is the set $\\{0x\\mid x\\in A\\}\\cup \\{1x\\mid x\\in B\\}$.\nGiven each number $n\\in{\\mathbb{N}}^{+}$ and $\\Sigma=\\{0,1\\}$, $bin(n)$ represents a string $w$ in $1\\Sigma^*$ ($=\\{1x\\mid x\\in\\Sigma^*\\}$) that represents $n$ in binary. Additionally, we set $bin(0)=\\lambda$. For example, we obtain $bin(1)=1$, $bin(2)=10$,  $bin(5)=101$, and $bin(7)=111$. Note that $|bin(n)|={\\lceil {\\log_2(n+1)} \\rceil}$ for every number $n\\in{\\mathbb{N}}^{+}$. By the contrary, for any string $w$ in $\\{\\lambda\\}\\cup 1\\Sigma^*$, $rep(w)$ denotes a positive integer satisfying $bin(n)=w$. For a number $n\\in{\\mathbb{N}}^{+}$, $bin(n)^{(-)}$ denotes the binary string obtained from $bin(n)$ by removing its first bit ``$1$.'' We also set $bin(0)^{(-)}=\\lambda$.\nA function $f:\\Sigma_1^*\\to\\Sigma_2^*$ (resp., $f:\\Sigma_1^*\\to{\\mathbb{R}}^{\\geq0}$) for two alphabets $\\Sigma_1$ and $\\Sigma_2$ is {\\em polynomially bounded} if there exists a polynomial $p$ satisfying $|f(x)|\\leq p(|x|)$ (resp., $f(x)\\leq p(|x|)$) for all inputs $x\\in\\Sigma_1^*$.\n\nA{\\em  directed graph} is a pair $(V,E)$ for which $V$ is a finite set of vertices and $E$ is a binary relation on $V$ and each element $(a,b)$ in $E$ is an edge. An {\\em undirected graph} is similarly defined but $E$ is required to be a symmetric relation, and we treat $(a,b)$ and $(b,a)$ in $E$ equivalently.\nGiven any graph $G=(V,E)$ with vertex set $V$ and edge set $E$, a {\\em path} of $G$ is a sequence $(v_1,v_2,\\ldots,v_k)$ of vertices in $V$ satisfying that $(v_i,v_{i+1})$ is an edge for every index $i\\in[k-1]$. Such a path is called {\\em simple} exactly when there are no repeated vertices in it; that is, a simple path has no loop. The {\\em length} of a path is the number of edges in it. A {\\em tree} is an undirected connected graph with no cycle whereas a {\\em forest} is an acyclic undirected graph. A {\\em weighted graph} is a graph $G=(V,E)$ for which each edge (or vertex) has an associated weight given by a weight function $w:E\\to{\\mathbb{R}}$ (or $w:V\\to{\\mathbb{R}}$).\n\nA {\\em Boolean formula} is made up of (Boolean) variables and three logical connectives: $\\wedge$ (AND), $\\vee$ (OR), and $\\neg$ (NOT) {\\em in infix notation}.\nGiven a (Boolean) truth assignment $\\sigma$, which maps $\\{T,F\\}$ to variables, a Boolean formula $\\phi$ is said to be {\\em satisfied} by $\\sigma$ if $\\phi$ is evaluated to be true after $\\sigma$ assigns truth values to the variables in the formula.\n\n\n\\paragraph{Representation of Graphs, Matrices, Circuits, and Boolean Formulas:}\nWhen we consider weak computations, it is often critical to choose what types of representation of input instances, such as graphs, matrices, circuits, and Boolean formulas.\nFor example, as noted in \\cite{JLM97}, if we describe trees and forests using {\\em bracketed expressions} as part of inputs, then the connectivity problem between two designated nodes in a given forest becomes solvable even on ${\\mathrm{NC}^{ {1} }}$-circuits.\nWith respect to logarithmic-space computation, however, the representation of graphs via incidence matrices, adjacency matrices, or sets of ordered pairs are all ``equivalent'' \\cite{JLL76}.  Unless otherwise specified, we assume that every graph is expressed by a {\\em listing of its edge relation}, such as $\\{(v_1,v_3),(v_2,v_5),(v_5,v_4)\\}$; namely, all ordered pairs of vertices that define edges of a given graph. For isolated vertices, we list them as the names of those vertices, such as $\\{(v_3),(v_6)\\}$, instead of $\\{(v_3,v_3),(v_6,v_6)\\}$, which indicate self-loops in directed graphs. Boolean circuits are viewed as directed acyclic graphs. Boolean formulas are expressed in infix notation.\\footnote{Boolean formulas in infix notation are defined inductively as follows: (i) $0$ and $1$ are Boolean formulas and (ii) if $\\alpha$ and $\\beta$ are Boolean formulas, then $(\\neg\\alpha)$, $(\\alpha\\vee\\beta)$, and $(\\alpha\\wedge\\beta)$ are Boolean formulas.}\n\n\n\\subsection{Basic Models of Computation}\\label{sec:basic_model}\n\nAs a mechanical model of computation, we shall  use the following basic form of {\\em (multi-tape) deterministic Turing machine}. For the formal definition of Turing machine, refer to, e.g., \\cite{DK00,HMU01}. Our machine is equipped with a read-only input tape, multiple work tapes, and possibly an output tape.\nAn input $x$ of length $n$ is given on the input tape, surrounded by two endmarkers: ${|}\\!\\!\\mathrm{c}$ (left endmarker) and ${\\$}$ (right endmarker) and all input tape cells are consecutively indexed by integers between $0$ and $n+1$, where ${|}\\!\\!\\mathrm{c}$ is at cell $0$ and ${\\$}$ at cell $n+1$.\nAt any moment, a tape head working on the input tape either stays still on the same tape cell or moves to the left or the right.\nThe {\\em running time} (or {\\em runtime}) of a Turing machine is the total number of steps (or moves) taken by the machine starting with the input, whereas its {\\em (tape) space} is the maximum number of distinct tape cells visited by a tape head during the\nmachine's  computation.\n\nThe behaviors of tapes and their tape heads are quite important in this paper; thus, we wish to pay our special attention to the following terminology. A tape is said to be {\\em read-once} if it is a read-only tape and its tape head does not scan the same cell more than once; namely, it either stays at the same cell without reading any information (known as a {\\em $\\lambda$-move} or an {\\em $\\varepsilon$-move}) or moves instantly to the right cell.\nIn contrast, a {\\em write-only} tape indicates that, whenever its tape head writes a nonempty symbol in a tape cell, the head should move immediately to its right cell. In this paper, ``output tapes'' are always assumed to be write-only tapes. Turing machines with write-only output tapes are considered to compute {\\em (multi-valued partial) functions}, by viewing strings left on the output tapes (when the machines halt) as ``outputs.''\n\nTo describe low-complexity classes, we also use a notion of ``random access'' input tapes. In this mode, a machine is further equipped with an index tape and tries to write on this index tape a string of the form $bin(k)$ for a certain number $k\\in[0,n+1]_{{\\mathbb{Z}}}$. Whenever the machine enters a specific inner state (an input-query state), the input-tape head jumps in a single step to the cell indexed by $k$ and reads a symbol written in this particular cell. If $k$ is not in the range $[0,n+1]_{{\\mathbb{Z}}}$, then $M$ simply reads a blank symbol as an ``out of range'' symbol. Whenever we need to clarify a use of this special model, we refer to it as {\\em random-access Turing machines}.\n\nTo express ``nondeterminism'' in our framework, we introduce a special tape called a {\\em read-once auxiliary input tape} and equip Turing machines with such auxiliary tapes.\nAn {\\em auxiliary Turing machine} is the above-mentioned deterministic Turing machine equipped with an extra read-once auxiliary input tape on which a sequence of (nonblank) symbols (called an {\\em auxiliary input}) is provided as an extra input (other than an ordinary input given on the input tape).\nIn the rest of this paper, we shall understand that ``auxiliary tapes'' means read-only auxiliary input tapes unless otherwise stated.\nSuch an auxiliary input given on the auxiliary input tape is surrounded by the two endmarkers. This machine can therefore read off two symbols (except for work-tape symbols) at once, one of which is from the input tape and the other from the auxiliary tape at each step in order to make a deterministic move. As our convention, when a tape head on the auxiliary tape reaches ${\\$}$, the head must remain at this endmarker in the rest of a computation.\n\nMore formally, a $(k+2)$-tape auxiliary Turing machine $M$ is a tuple $(Q,\\Sigma,\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\Gamma,\\Theta,\\Phi,q_0,q_{acc},q_{rej})$, where $Q$ is a finite set of inner states, $\\Sigma$ is an input alphabet, $\\Gamma$ is a work alphabet, $\\Theta$ is an auxiliary input alphabet, $\\Phi$ is an output alphabet, $q_0$ is the initial state in $Q$, $q_{acc}$ (resp., $q_{rej}$) is an accepting (resp., a rejecting) state in $Q$, and $\\delta$ is a transition function from $(Q-\\{q_{acc},q_{rej}\\})\\times (\\Sigma\\cup\\{{|}\\!\\!\\mathrm{c},{\\$},\\lambda\\})\\times \\Gamma^k \\times (\\Theta\\cup\\{{\\$}\\})$ to $Q\\times \\Gamma^k \\times (\\Phi\\cup\\{\\lambda\\}) \\times D\\times D_1 \\times \\cdots \\times D_{k}$, where $D$ and each $D_i$ ($i\\in[k]$) are sets of head directions, $\\{-1,0,+1\\}$, of an input tape and the $i$th work tape. Notice that, since tape heads on an auxiliary tape and an output tape move in one direction, we do not need to include their head directions.\n\nWe say that an auxiliary Turing machine {\\em uses log space} if there exist two constants $a,b>0$ for which, on every input $x$ and every auxiliary input $y$, $M$ uses the total of at most $a\\log{|x|}+b$ cells of all work tapes (where an auxiliary input tape is not a work tape). Such a machine is succinctly called a {\\em log-space auxiliary Turing machine}. Similarly, we define the notion of {\\em polynomial-time auxiliary Turing machine}.\n\nWe assume that the reader is familiar with the foundation of computational complexity theory, in particular,  the definitions and properties of those   fundamental classes. See, e.g., \\cite{DK00} for their fundamental properties.\nThe complexity class ${\\mathrm{P}}$ (deterministic polynomial time) is composed of all decision problems (or languages) solved by deterministic Turing machines in polynomial time, whereas ${\\mathrm{L}}$ (deterministic logarithmic space) contains decision problems solved by log-space deterministic Turing machines.\nThe notation ${\\mathrm{FP}}$ (resp., ${\\mathrm{FL}}$) refers to a functional version of ${\\mathrm{P}}$ (resp., ${\\mathrm{L}}$), provided that all functions in ${\\mathrm{FL}}$ output only strings of size polynomial in the lengths of inputs. Thus, ${\\mathrm{FL}}\\subseteq {\\mathrm{FP}}$ holds.\n\nFor later convenience, we denote by ${\\mathrm{auxP}}$ (resp., ${\\mathrm{auxL}}$) the collection of all sets $A \\subseteq \\Sigma^*\\times\\Sigma^*$ over alphabet $\\Sigma$ for which there exist a polynomial $p$ and a polynomial-time (resp., log-space)  auxiliary Turing machine $M$ such that,  for every $x$ and $y$, (i) $(x,y)\\in A$ implies $|y|\\leq p(|x|)$ and (ii) whenever $|y|\\leq p(|x|)$, $M$ accepts $(x,y)$ iff $(x,y)\\in A$, where $y$ is given on $M$'s auxiliary tape. Their  functional versions with polynomially-bounded outputs (i.e., the size of output strings is bounded from above by a suitable polynomial in the input size) are denoted by ${\\mathrm{auxFP}}$ (resp., ${\\mathrm{auxFL}}$). These classes ${\\mathrm{auxP}}$ and ${\\mathrm{auxL}}$ are respectively associated with nondeterministic classes ${\\mathrm{NP}}$ and ${\\mathrm{NL}}$ in the following fashion. Given a set $A\\subseteq\\Sigma^*\\times\\Sigma^*$ and any polynomial $p$, let $A_p = \\{(x,y)\\in A\\mid |y|\\leq p(|x|)\\}$ and $A_p^{\\exists} =\\{x\\in\\Sigma^*\\mid \\exists y\\;[(x,y)\\in A_p]\\}$. When $p$ is clear from the context, we tend to drop subscript ``$p$'' and write $A^{\\exists}$ instead of $A^{\\exists}_{p}$.\nThe nondeterministic class ${\\mathrm{NP}}$ (resp., ${\\mathrm{NL}}$) is composed of all languages of the form $A^{\\exists}_{p}$ for all $A\\subseteq \\Sigma^*\\times\\Sigma^*$ and all polynomials $p$ satisfying $A_p\\in{\\mathrm{auxP}}$ (resp., ${\\mathrm{auxL}}$). In other words, $A_{p}\\in{\\mathrm{auxP}}$ (resp., ${\\mathrm{auxL}}$) if and only if $A_p^{\\exists}\\in {\\mathrm{NP}}$ (resp., ${\\mathrm{NL}}$).\n\nIn addition, the notation ${\\mathrm{DLOGTIME}}$ is used to express the collection of all languages recognized by random-access Turing machines in $O(\\log{n})$ time.\nA function $f:\\Sigma_1^*\\to\\Sigma_2^*$ is {\\em DLOGTIME-computable} if the output size of $f$ is polynomially bounded and the language $A_f=\\{(x,i,b)\\mid \\text{ the $i$th bit of $f(x)$ equals $b$}\\}$ belongs to ${\\mathrm{DLOGTIME}}$.\n\nIn the subsequent sections, we shall concentrate mostly on functions and languages (which can be viewed as Boolean functions) whose domains are limited to certain subsets $I$ of $\\Sigma^*$ (for alphabets $\\Sigma$), and thus any given input to those functions and languages are always assumed, as a ``promise,'' to be taken from  those domains $I$. Our functions and languages are therefore promise problems. To simplify our discussion in the later sections, however, we tend to teat those  promise problems $F$ as if they have no promise and we explicitly write, e.g.,  $F\\in {\\mathrm{FL}}$ and $F\\in{\\mathrm{auxL}}$ unless there is no confusion.\n\nTo describe circuit-based complexity classes, we use a standard notion of {\\em Boolean circuits} (or just {\\em circuits}), which is a labeled acyclic directed graph whose nodes of indegree $0$ are called inputs and the other nodes are called gates. In our setting, a circuit is made up only of two basic gates $AND$ and $OR$ with inputs, which are labeled by {\\em literals} (that is, either Boolean variables or their negations). A {\\em fan-in} of a gate is the number of incoming edges. A fan-in is said to be {\\em bounded} (resp., {\\em unbounded}) if it is smaller than or equal to $2$ (resp., it has no upper bound).\nThe {\\em size} of a circuit is the number of its nodes and the {\\em depth} is the number of the longest path from an input to an output.\nA {\\em family of circuits} is a set $\\{C_n\\mid n\\in{\\mathbb{N}}\\}$, where each $C_i$ is a Boolean circuit with $n$ distinct variables.\n\nThere have been a number of uniformity notions proposed in the past literature,  e.g., \\cite{BIS90,DHR97,Ruz81}. The different choice of uniformity endows circuit families with (possibly) different computational power.\nTo explain such uniformity, we define the {\\em direct connection language} of a circuit family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ as a set of all tuples ${\\langle {t,a,b,y} \\rangle}$, where $a$ and $b$ are numbers of nodes in $C_n$, $b$ is a child of $a$, $t$ is the type (e.g., literals, $AND$,   $OR$, $NOT$, etc.) of $a$, and $y$ is any string of length $n$.\nThe {\\em standard encoding} of $C_n$ is a string, each symbol of which is of the form $(a,t,b_{L},b_{R})$, where $a,B_{L},B_{R}$ are gate numbers, $b_{L}$ (resp., $B_{R}$) is the left (resp., right) child of $a$, and $t$ is the type of $a$.\n\nA family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ of Boolean circuits is {\\em log-space uniform} (or {\\em L-uniform}) if there exists a log-space deterministic Turing machine computing a function that maps $1^n$ to the standard encoding of $C_n$.\nWe say that a family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ of Boolean circuits is {\\em DLOGTIME-uniform}\\footnote{As shown in \\cite[Theorem 9.1]{BIS90}, this definition is equivalent to the one used in \\cite{Bus87,BIS90} using formula languages.}\nif the directed connection language of $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ can be recognized by a log-time random-access Turing machines. Other uniformity notions include {\\em $U_{E^*}$-uniformity} and\n{\\em P-uniformity} \\cite{Ruz81}.\nFor each $k\\in{\\mathbb{N}}$, ${\\mathrm{NC}^{ {k} }}$ (resp., ${\\mathrm{AC}^{ {k} }}$) denotes the class of decision problems (or  languages) solvable by ${\\mathrm{DLOGTIME}}$-uniform families of bounded (resp., unbounded) fan-in Boolean circuits of polynomial size and $O(\\log^{k}n)$ depth. To refer to ${\\mathrm{NC}^{ {1} }}$ of different uniformity, when clarification is necessary, we tend to describe it as ``${\\mathrm{L}}$-uniform ${\\mathrm{NC}^{ {1} }}$''\nor ``${\\mathrm{P}}$-uniform ${\\mathrm{NC}^{ {1} }}$.''\nTo describe their functional versions, we intentionally use the notations ${\\mathrm{FAC}^{ {k} }}$ and ${\\mathrm{FNC}^{ {k} }}$, respectively. It is known that $\\mathrm{ALOGTIME}$ (alternating logarithmic time) coincides with ${\\mathrm{DLOGTIME}}$-uniform ${\\mathrm{NC}^{ {1} }}$ \\cite{Bus87}, which also equals ${\\mathrm{NC}^{ {1} }}$-uniform ${\\mathrm{NC}^{ {1} }}$ \\cite{BIS90}.\n\nAnother characterization of ${\\mathrm{NC}^{ {1} }}$ is given in \\cite{BIS90} as follows. The {\\em formula language} of a Boolean formula family $\\{F_n\\}_{n\\in{\\mathbb{N}}}$ is composed of all tuples ${\\langle {c,i,y} \\rangle}$ such that $|y|=n$ and the $i$th character of the $n$th formula $F_n$ is $c$. A language $A$ is in ${\\mathrm{NC}^{ {1} }}$ iff there exists a family $\\{F_n\\}_{n\\in{\\mathbb{N}}}$ of Boolean formulas with depth $O(\\log{n})$ such that (i) for every $x$, $F_{|x|}(x)$ is true exactly when $x\\in A$ and (ii) there exists a log-time deterministic Turing machine recognizes the formal language of $\\{F_n\\}_{n\\in{\\mathbb{N}}}$.\n\nKnown inclusion relationships among the aforementioned complexity classes are shown as: ${\\mathrm{NC}^{ {0} }}\\subsetneqq {\\mathrm{AC}^{ {0} }}\\subsetneqq {\\mathrm{TC}^{ {0} }} \\subseteq {\\mathrm{NC}^{ {1} }}\\subseteq {\\mathrm{L}} \\subseteq {\\mathrm{NL}} = {\\mathrm{co}\\mbox{-}}{\\mathrm{NL}} \\subseteq {\\mathrm{AC}^{ {1} }} \\subseteq {\\mathrm{NC}^{ {2} }} \\subseteq{\\mathrm{P}}\\subseteq{\\mathrm{NP}}$. For more details, refer to, e.g., \\cite{DK00}.\n\nIt is important to note that, on an output tape of a machine, a natural number is represented in binary, where the least significant bit is always placed at the right end of the output bits.\nIn the rest of paper, a generic but informal term of ``algorithm'' will be often used to refer to either a deterministic Turing machine or a uniform family of circuits.\n\n\\begin{lemma}\\label{operation-complexity}\nFor $n$-bit numbers $x,y,x_i\\in{\\mathbb{N}}$ with $i\\in[n]$, the operations $x+y$ and $\\max\\{0,x-y\\}$ are in ${\\mathrm{AC}^{ {0} }}$, and ${\\lfloor {x/y} \\rfloor}$, $\\sum_{i=1}^{n}x_i$, and $\\prod_{i=1}^{n}x_i$ are in ${\\mathrm{TC}^{ {0} }}$ \\cite{Hes01,HAB02}.\n\\end{lemma}\n\n\n\\subsection{Refined Optimization Problems}\\label{sec:comb-OPs}\n\nAn {\\em optimization  problem} is simply a search problem, in which we are asked to look for a best possible feasible solution of the problem for each given admissible input.\nIn the past literature, NP optimization problems have been a centerfold of the intensive study and low-complexity optimization problems have been mostly neglected except for \\cite{Tan07}. To deal with those problems, we intend to refine the existing framework of NP optimization problems in terms of log-space and uniform-circuit computations.\n\nIn what follows, we shall formally introduce 14 different classes of refined combinatorial optimization problems, including 4 well-known classes ${\\mathrm{NPO}}$, ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$, ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$, and ${\\mathrm{PTAS}}_{{\\mathrm{NPO}}}$, in order to justify the correctness of our definitions.\n\n\\paragraph{NPO and NLO.}\nAs a starting point of our study, we formally introduce  {\\em NP optimization problems} or {\\em NPO problems} in the style of  \\cite{ACG+03}.\nSince our purpose is to investigate low-complexity optimization problems,\nit is better for us to formulate a notion of ${\\mathrm{NPO}}$ problems using auxiliary Turing machines instead of nondeterministic Turing machines. An ${\\mathrm{NPO}}$ problem $P$ is formally a quadruple $(I,SOL,m,goal)$ whose entries satisfy the following properties.\n\n{\\smallskip}\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\\item $I$ is a finite set of {\\em admissible instances}. There must be a deterministic Turing machine that recognizes $I$ in polynomial time; that is, $I$ belongs to ${\\mathrm{P}}$.\n\n\\item $SOL$ is a function mapping $I$ to a collection of certain finite sets, where $SOL(x)$ is a set of {\\em feasible solutions} of input instance $x$. There must be a polynomial $q$ such that (i) for every $x\\in I$ and every $y\\in SOL(x)$, it holds that $|y|\\leq q(|x|)$ and (ii) the set $I\\circ SOL = \\{(x,y) \\mid x\\in I, y\\in SOL(x)\\}$ is in ${\\mathrm{auxP}}$; namely, $I\\circ SOL$ is recognized in time polynomial in $|x|$ by a certain auxiliary Turing machine stating with $x$ on an input tape and $y$ on an auxiliary tape. By the definition of $SOL$, the set $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ matches $(I\\circ SOL)_{q}^{\\exists}$, and thus it belongs to ${\\mathrm{NP}}$.\n\n\\item $goal$ is either {\\sc max} or {\\sc min}. When $goal=\\text{\\sc max}$, $P$ is called a {\\em maximization problem}; when $goal=\\text{\\sc min}$, it is a {\\em minimization problem}.\n\n\\item $m$ is a {\\em measure function} (or an {\\em objective function}) from $I\\circ SOL$ to ${\\mathbb{N}}^{+}$ whose value $m(x,y)$ is computed  in time polynomial in $|x|$ by a certain auxiliary Turing machine starting with $x$ written  on an input tape and $y$ on an auxiliary tape. Technically speaking, $m$ is a promise problem; however, by abusing notations, we often express $m$ as a member of ${\\mathrm{auxFP}}$ (i.e., $m\\in{\\mathrm{auxFP}}$). For any instance $x\\in I$,  $m^*(x)$ denotes the optimal value $goal\\{m(x,y)\\mid y\\in SOL(x)\\}$. Moreover, $SOL^*(x)$ expresses the  set $\\{y\\in SOL(x)\\mid m(x,y) = m^*(x)\\}$ of optimal solutions of $x$.\n\\end{itemize}\n\nNotice that, in polynomial time, an auxiliary\nTuring machine can copy any string $y$ given on an auxiliary tape into its work tape and then manipulate it freely. This makes the read-once requirement of an auxiliary tape redundant. Therefore, the above definition logically matches\nthe existing notion of ${\\mathrm{NPO}}$ problems in, e.g., \\cite{ACG+03}.\nLet the notation ${\\mathrm{NPO}}$ also express the class of all ${\\mathrm{NPO}}$ problems.\n\nA measure function $m$ is called {\\em polynomially bounded} if there exists a polynomial $p$ such that $m(x,y)\\leq p(|x|,|y|)$ holds for all pairs $(x,y)\\in I\\circ SOL$. An optimization problem is also said to be {\\em polynomially bounded} if its measure function is polynomially bounded. For convenience,  a succinct notation ${\\mathrm{PBO}}$ indicates the collection of all optimization problems that are polynomially bounded.\n\nTo analyze the behaviors of low-complexity optimization problems,  Tantau \\cite{Tan07} formulated a notion of {\\em NL optimization problems} (or {\\em NLO problems}, in short), which are obtained simply by replacing the term ``polynomial time'' in the above definition of ${\\mathrm{NPO}}$ problems with ``logarithmic space.''\nFor those ${\\mathrm{NLO}}$ problems, the use of auxiliary Turing machine is essential and it may not be replaced by any Turing machine having no read-once auxiliary input  tapes.\n\nHere, we draw our attention to the read-once requirement posed on an auxiliary input tape. This requirement is quite severe for Turing machines. To see this fact, let us consider the following maximization problem {\\sc Max Weight-2SAT}. In the {\\em maximum weighted 2-satisfiability problem} ({\\sc Max Weight-2SAT}), we seek a truth assignment $\\sigma$ satisfying  a given 2CNF formula on a set $X$ of variables and a variable weight function $w:X\\to{\\mathbb{N}}^{+}$ such that the sum $\\sum_{x\\in X}\\sigma(x)w(x)+1$ must be maximized.\nAlthough its associated decision problem 2SAT, in which we are asked to decide whether a given 2CNF formula is satisfiable, is NL-complete (from a result of \\cite{JLL76}), it is not clear whether {\\sc Max Weight-2SAT} belongs to ${\\mathrm{NLO}}$.\n\nTo express the class of all ${\\mathrm{NLO}}$ problems, we use the notation of ${\\mathrm{NLO}}$. It follows that ${\\mathrm{NLO}}\\subseteq {\\mathrm{NPO}}$. Moreover, ${\\mathrm{MinNL}}$ (resp., ${\\mathrm{MaxNL}}$) denotes the class of all minimization (resp., maximization) problems in ${\\mathrm{NLO}}$; thus, ${\\mathrm{NLO}}$ equals the union ${\\mathrm{MinNL}}\\cup {\\mathrm{MaxNL}}$.\n\n\n\\paragraph{PO, LO, NC$^{i}$O, and AC$^{i}$O.}\nWe say that an ${\\mathrm{NPO}}$ problem $P = (I,SOL,m,goal)$ is {\\em P-solvable} if there exists a polynomial-time deterministic Turing machine $M$ such that, for every instance $x\\in I$, if $SOL(x)\\neq{\\mathrm{\\O}}$, then $M$ returns an optimal solution $y$ in $SOL(x)$  and, otherwise, $M$ returns ``no solution'' (or a designated symbol $\\bot$). Moreover, the values $m(x,M(x))$ ($=m^*(x)$) must be computed in polynomial time from inputs $x$.\nAs a result, the set $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ must be in ${\\mathrm{P}}$.\nGiven a class ${\\cal D}$ of optimization problems, the notation ${\\mathrm{PO}}_{{\\cal D}}$ expresses the class of all optimization problems in ${\\cal D}$ that are ${\\mathrm{P}}$-solvable. Similarly, we can define the notations of ${\\mathrm{LO}}_{{\\cal D}}$, ${\\mathrm{NC}^{ {i} }\\mathrm{O}}_{{\\cal D}}$, and ${\\mathrm{AC}^{ {i} }\\mathrm{O}}_{{\\cal D}}$ by replacing the term ``${\\mathrm{P}}$-solvable'' with ``${\\mathrm{L}}$-solvable,'' ``${\\mathrm{NC}^{ {i} }}$-solvable,'' and ``${\\mathrm{AC}^{ {i} }}$-solvable,'' respectively, for each index $i\\in{\\mathbb{N}}$. Conventionally, ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ is written as ${\\mathrm{PO}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ is noted briefly as ${\\mathrm{LO}}$ in \\cite{Tan07}. Notice that ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\cal D}} \\subseteq {\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\cal D}} \\subseteq {\\mathrm{LO}}_{{\\cal D}} \\subseteq {\\mathrm{PO}}_{{\\cal D}}$ for any reasonable class ${\\cal D}$.\n\nIt is important to note that, as in the case of ${\\mathrm{LO}}_{{\\mathrm{NPO}}}$, for example, when a problem $P$ is ${\\mathrm{L}}$-solvable, its log-space algorithm, say, $M$ that solves $P$ does not need to check whether an input $x$ given to $M$ is actually admissible instance (i.e., $x\\in I$), because such a task may be in general impossible for log-space machines. Hence, $P$ is technically a promise problem and we normally allow $M$ to behave arbitrarily on inputs outside of $I$ or $I\\circ SOL$.\n\n\n\\paragraph{APXP, APXL, APXNC$^{i}$, and APXAC$^{i}$.}\nNext, we shall  define approximation classes using a notion of $\\gamma$-approximation. Given an optimization problem $P=(I,SOL,m,goal)$, the {\\em performance ratio} of solution $y$ with respect to instance $x$ is defined as\n", "index": 1, "text": "\n\\[\nR(x,y) = \\max\\left\\{ \\left|\\frac{m(x,y)}{m^*(x)}\\right|, \\left|\\frac{m^*(x)}{m(x,y)}\\right| \\right\\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"R(x,y)=\\max\\left\\{\\left|\\frac{m(x,y)}{m^{*}(x)}\\right|,\\left|\\frac{m^{*}(x)}{m%&#10;(x,y)}\\right|\\right\\},\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>|</mo><mfrac><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msup><mi>m</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>|</mo></mrow><mo>,</mo><mrow><mo>|</mo><mfrac><mrow><msup><mi>m</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>|</mo></mrow><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\nThe last term is further upper-bounded by $\\frac{r m_2^*(x)}{m_2^*(x)+1}\\leq 1+c(r-1)$, where $c=1$, since $m_2(x,y)\\leq r m_2^*(x)$. Overall, we obtain $R_1(x,g(x,y,r))\\leq 1+c(r-1)$. We then conclude that $(f,g,c)$ is indeed an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction from $P_1$ to $P_2$.\n\\end{proof}\n\nHenceforth, we shall discuss several variants of {\\sc Min Path-Weight}. A simple variant is an {\\em undirected-graph version} of {\\sc Min Path-Weight}, denoted by  {\\sc Min UPath-Weight}. It is possible to demonstrate that {\\sc Min UPath-Weight} is log-space ${n^{O(1)}}$-approximable because, by the result of Reingold  \\cite{Rei08}, using only log space, we not only determine the existence of a certain feasible solution for {\\sc Min UPath-Weight} but also find at least one feasible solution if any. The special case where the weights of all vertices are exactly $1$ is the problem of finding the {\\em shortest $s$-$t$ path}. This problem was discussed in \\cite{Tan07}; nonetheless, it is unknown that {\\sc Min UPath-Weight} belongs to ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\nAs another variant of {\\sc Min Path-Weight}, we consider {\\em forests}.  Cook and McKenzie \\cite{CM87} showed that the $s$-$t$ connectivity problem for  forests is complete for ${\\mathrm{L}}$ under L-uniform NC$^{1}$ many-one reductions. Similarly, when all admissible input graphs of {\\sc Min UPath-Weight} are restricted to be forests, we call the corresponding problem {\\sc Min Forest-Path-Weight}. As shown in the following proposition, {\\sc Min Forest-Path-Weight} turns out to be one of the most difficult problems in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nIt is of importance that, unlike ${\\mathrm{NLO}}$, the class ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ does possess complete problems.\n\n\\begin{proposition}\\label{forest-path-weight}\n$\\text{\\sc Min Forest-Path-Weight}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\\end{proposition}\n\nTo simplify the proof of Proposition \\ref{forest-path-weight}, we first give a useful lemma that helps us pay central attention to optimization problems of particular form.\nHere, we say that an optimization problem $P=(I,SOL,m,goal)$ {\\em admits unique solutions} if $|SOL(x)|\\leq1$ holds for all $x\\in I$.\n\n\\begin{lemma}\\label{LO-simple-form}\nFor any maximization problem $Q\\in {\\mathrm{LO}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$), there are another maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$) and a log-space deterministic Turing machine $M_P$ such that, for any $x\\in I$, (i) $Q{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}P$, (ii) $m(x,z)=m(x,M_P(x))$ for all $z\\in SOL(x)$, and (iii) $P$ admits unique solutions. The same statement holds for minimization problems.\n\\end{lemma}\n\n\\begin{proof}\nLet $Q=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Let $M_Q$ be a log-space deterministic Turing machine producing optimal solutions of $Q$. We then define $P=(I_2,SOL_2,m_2,\\text{\\sc max})$ as follows. First, we set $I_2=I_1$ and $SOL_2(x) =\\{M_P(x)\\}$ if $M_P(x))\\neq\\bot$, and $SOL_2(x)={\\mathrm{\\O}}$ otherwise.\nFrom this definition follows $|SOL_2(x)|\\leq1$ for all $x\\in I_2$. Moreover, we define $m_2$ by setting $m_2(x,z)=m_1(x,M_Q(x))$ for any $(x,z)x\\in I_2\\circ SOL_2$. Here, we set $M_P$ to be the same as $M_Q$. Obviously, $m_2(x,M_P(x))=m_2^*(x)$ holds if $SOL_1(x)\\neq{\\mathrm{\\O}}$ since $M_Q(x)\\in SOL_1^*(x)$.\n\nFor the desired ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$, we define $c=1$, $f(x,r)=x$, and $g(x,y,r)=y$. Clearly, $f,g\\in{\\mathrm{FAC}^{ {0} }}$. Consider the performance ratio $R_1$ and $R_2$ for $Q$ and $P$, respectively, and assume that $R_2(f(x,r),y)\\leq r$ for any $y\\in SOL_2(f(x,r))$ and $r\\in{\\mathbb{Q}}^{\\geq1}$. This assumption yields $y=M_P(x)$. Hence, $R_1(x,g(x,y,r)) = R_1(x,y) = R_1(x,M_Q(x)) =1\\leq r$. Therefore, $(f,g,c)$ reduces $Q$ to $P$.\n\\end{proof}\n\nLet us begin the proof of Proposition \\ref{forest-path-weight}.\n\n\\begin{proofof}{Proposition \\ref{forest-path-weight}}\n{\\sc Min Forest-Path-Weight} is assumed to have the form $(I_0,SOL_0,m_0,\\text{\\sc min})$. The membership relation $\\text{\\sc Min Forest-Path-Weight}\\in {\\mathrm{LO}}_{{\\mathrm{NLO}}}$ essentially comes from a simple fact that, by the forest property of a given graph $G$, two nodes\n$s$ and $t$ are connected in $G$ if and only if a {\\em unique} path exists  between them. We can search such a unique path by starting from $s$ and following recursively adjacent edges to next nodes until either no more edges remain unsearched or $t$ is found. At the same time, we progressively write down the weight, in binary, of each node along this found path.\nThe recursive part of this procedure works as follows. Let $u$ be the currently visiting node. We then pick each neighbor, say, $v$ and check if there is a path between $v$ and $t$ in a graph obtained from $G$ by deleting the edge $(u,v)$. This procedure needs no more than log space.\n\nLet $P=(I,SOL,m,\\text{\\sc min})$ be any minimization problem in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. We assume that $P$ satisfies Conditions (ii)--(iii) of Lemma \\ref{LO-simple-form}.  Our goal is to show that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to {\\sc Min Forest-Path-Weight} via a suitably constructed ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$.\nChoose a log-space deterministic Turing machine $M_1$ that produces optimal solutions of $P$. For convenience, we set $b(x)$ to be $m(x,M_1(x))$ for any instance $x\\in (I\\circ SOL)^{\\exists}$.\nSince $m\\in{\\mathrm{auxFL}}$, there is a log-space auxiliary Turing machine $M_m$ computing $m$. By combining $M_1$ and $M_m$ properly, we can design another log-space deterministic Turing machine, say, $M_2$ that computes $b$ with no auxiliary tape.\n\nTo make all final partial configurations unique, we want to force $M_2$ to erase all symbols on all tapes just before entering a halting state. To avoid the same partial configurations to be reached along a single computation, we additionally equip an {\\em internal clock} to $M_2$.\n\nLet us consider partial configurations of $M_2$. Note that $M_2$ is deterministic and the internal clock marks all partial configurations of $M_2$ on $x$ with different time stamps.\nNote also that each symbol of the string $M_1(x)$ appears as a symbol read from the auxiliary input tape encoded into certain partial configurations of $M_1$. Hence, if we have a valid series $y$ of partial configurations associated with an accepting computation path of $M_2$ on $x$, then we can recover the string $M_1(x)$ correctly. Notationally,  $\\eta(y)$ denotes this unique string obtained from a valid series $y$ of partial configurations.\n\nTake a configuration graph $G^{M_2}_{x} = (V,E)$ from $M_2$. Note that there is at most one correct computation path of $M_2$. We set $s$ to be the initial partial configuration of $M_2$ on $x$ and set $t$ be a unique accepting partial configuration of $M_2$ on $x$.\nThe resulted graph forms an {\\em acyclic undirected graph}, namely a forest,  because, otherwise, there are two accepting computation paths on the same input $x$. Given any partial configuration $v\\in V$, we define $w(v)$ to be one bit written down newly on the output tape in this partial configuration $v$.\nFor the desired reduction, we define $c=1$, $f(x,r)={\\langle {G,s,t,w} \\rangle}$, and $g(x,y,r)=\\eta(y)$ for any $y\\in SOL_0(f(x,r))$.\nWe obtain $f,g\\in{\\mathrm{FNC}^{ {1} }}$. It follows that $m_0(f(x,r),y)=m(x,\\eta(y))$ for any $x\\in I$ and $y\\in SOL_0(f(x,r))$. In particular, $g(x,y,r)$ is a minimal solution of $x$ if and only if $y$ is a minimal solution of $f(x,r)$. Thus, $(f,g,c)$ ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduces $P$ to {\\sc Min Forest-Path-Weight}.\n\nNext, we consider any maximization problem $P$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Since $P$ also satisfies Condition (ii)--(iii) of Lemma \\ref{LO-simple-form}, the above argument also works for this $P$ and thus establishes the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducibility of $P$ to {\\sc Min Forest-Path-Weight}.\n\\end{proofof}\n\nAs other variants of {\\sc Min Path-Weight}, Nickelsen and Tantau \\cite{NT05}  studied {\\em series-parallel graphs} and {\\em tournaments}.\n\n\n\\subsection{Complete Problems Concerning Finite Automata}\\label{sec:approximation-class}\n\nWe shall leave graph problems behind and look into problems associated with   finite automata. \\`{A}lvarez and Jenner \\cite{AJ93} and later Tantau \\cite{Tan07} discussed an intimate relationship between accepting computations of nondeterministic finite automata and log-space search procedures for optimal solutions.\nThose problems are also closely related to {\\em maximal word problems (or functions)} for fixed underlying machines.\nAllender, Bruschi, and Pighizzini \\cite{ABP93}, for instance, discussed the maximal word problems of various types of auxiliary pushdown automata. Within our framework of NLO problems, Tantau \\cite{Tan07} presented a maximization problem finding the maximal input strings accepted by nondeterministic finite automata and demonstrated that this problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{MaxNL}}$.\nHere, we shall show that a restricted version of this problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\nA {\\em one-way one-head nondeterministic finite automaton with $\\lambda$-moves} (or a {\\em $\\lambda$-1nfa}, in short) $M$ is a tuple $(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ working with the input alphabet $\\{0,1\\}$ and a transition function $\\delta: (Q-F)\\times\\{0,1,\\lambda\\}\\to{\\cal P}(Q)$, where $q_0\\in Q$, and $F\\subseteq Q$. Initially, an input $x\\in\\{0,1\\}^*$ is written on an input tape, surrounded by two endmarkers ${|}\\!\\!\\mathrm{c}$ (left) and ${\\$}$ (tight).  If $M$ makes a $\\lambda$-move simply by applying $p\\in\\delta(q,\\lambda)$, then $M$'s read-only tape head stays still; otherwise, the tape head moves to the next right cell.\nA {\\em configuration} of $M$ is a pair $(q,\\sigma)$ of current inner state $q$ and scanning symbol $\\sigma$. An {\\em accepting computation path} $p_{M,x}$ of $M$ on input $x$ is a series of configurations starting with an initial configuration $(q_0,{|}\\!\\!\\mathrm{c})$ and ending with a final configuration $(q_f,{\\$})$ with $q_f\\in F$ and, for any consecutive two elements $(q_i,\\sigma_i)$ and $(q_{i+1},\\sigma_{i+1})$ in $p_{M,x}$, $q_{i+1}$ is obtained in a single step from $(q_i,\\sigma_i)$ by applying a transition of the form $q_{i+1}\\in \\delta(q_i,\\sigma_i)$ with $\\sigma_i\\in\\{0,1,\\lambda\\}$, where $\\sigma_1\\sigma_2\\cdots\\sigma_k$ is a partition of the input string ${|}\\!\\!\\mathrm{c} x{\\$}$.\nIf $M$ enters a certain final state in $F$ along a certain accepting computation path, then $M$ is said to {\\em accept} $x$; otherwise, $M$ {\\em rejects} $x$.\nAssociated with such $\\lambda$-1nfa's, we consider the following optimization problem.\nFor succinctness, we hereafter express a transition ``$p\\in \\delta(q,\\sigma)$'' as a triplet $(q,\\sigma,p)$.\n\n{\\medskip}\n{\\sc Maximum Fixed-Length $\\lambda$-Nondeterministic Finite Automata Problem} ({\\sc Max FL-$\\lambda$-NFA}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a $\\lambda$-1nfa $M=(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ and a string $0^n$ for a length parameter $n$, provided that $0^n\\in L(M)$.\n\n\\item {\\sc Solution:} an accepting computation path of $M$ of length at most $|Q|$ on a certain input $y$ of length exactly $n$.\n\n\\item {\\sc Measure:} an integer $rep(1y)$.\n\\end{itemize}\n\nIn the above definition, if we remove the requirement ``$0^n\\in L(M)$'' and we allow $y$ to have any length up to $n$, then we obtain {\\sc Max $\\lambda$-NFA}, which is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{MaxNL}}$  \\cite{Tan07}.\n\n\\begin{proposition}\\label{Mix-2Path-Weight}\n$\\text{\\sc Max FL-$\\lambda$-NFA}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\\end{proposition}\n\nBefore proving this proposition, we show a\nuseful supporting lemma. The lemma helps us concentrate only on optimization problems in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ that have a certain simple structure.\n\n\\begin{lemma}\\label{NLO-to-APXL}\nFor any maximization problem $Q$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$), there exist another maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$)  and a log-space deterministic Turing machine $M_P$ such that,  for all $x\\in I$, (i) $Q{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}} P$, (ii) $\\max_{y\\in SOL(x)}\\{m(x,y)\\}\\leq 2 \\min_{y\\in SOL(x)}\\{m(x,y)\\}$ holds for all $x\\in I$, (iii) there exists a function $b\\in{\\mathrm{FL}}$ such that  $m(x,z)\\geq 2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$  for all $z\\in SOL(x)$, and (iv) for any $x\\in I$ with $SOL(x)\\neq{\\mathrm{\\O}}$,  $m(x,M_P(x))=2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$.\nA similar statement holds for minimization problems; however, we need to replace (iii) by (iii') $m(x,z)\\leq 2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $Q=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. In what follows, we shall modify $Q$ to obtain the desired problem  $P=(I_2,SOL_2,m_2,\\text{\\sc max})$.\n\nTake a polynomial $p$ such that, for any $(x,y)\\in I_1\\circ SOL_1$, $|y|\\leq p(|x|)$ holds.\nSince $Q\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, take a log-space deterministic Turing machine $M_P$ producing $\\beta$-approximate solutions of $P$ for a certain constant $\\beta>1$. We obtain $m_1(x,M_Q(x))\\leq m_1^*(x)\\leq \\beta m_1(x,M_Q(x))$ for all $x\\in I_1$ with $SOL_1(x)\\neq{\\mathrm{\\O}}$. For such an $x$, we further set $b_0(x) = 2^{{\\lfloor {\\log{m_1(x,M_Q(x))}} \\rfloor}+1}$. Note that $b_0\\in{\\mathrm{FL}}$\nsince $m_1\\in{\\mathrm{auxFL}}$.\nSince $b_0(x)\\leq b(x)\\leq 2b_0(x)$, it follows that $b_0(x)\\leq m_1^*(x)\\leq 2\\beta b_0(x)$.\nLet us consider a configuration graph $G^{M_Q}_{x}$ of $M_Q$ on $x$.  Note that we can compute a string $M_Q(x)$ from $(x,G^{M_Q}_{x})$ using only log space.\nFor later use, we set $\\alpha= 2^{{\\lfloor {\\log{2\\beta}} \\rfloor}+1}+1$, which implies  $b_0(x)\\leq m_1^*(x)\\leq \\alpha b_0(x)$.\n\nHere, we define the desired problem $P = (I_2,SOL_2,m_2,\\text{\\sc max})$. For convenience, set $\\Delta_{x} = (\\alpha-2)b_0(x)$. Let $I_2=\\{x\\natural G^{M_Q}_{x}\\mid x\\in I_1\\}$. Given $\\tilde{x}=x\\natural G^{M_Q}_{x} \\in I_2$, $SOL_2(\\tilde{x})$ contains the following strings: (i) $y\\natural M_Q(x)$ for all $y\\in SOL_1(x)$ satisfying $m_1(x,y)\\geq b_0(x)$ and (ii) $By\\natural M_Q(x)$ for every $y\\in SOL_1(x)$ satisfying $m_1(x,y)<b_0(x)$, where $B$ is a special symbol. Obviously, $I_2\\circ SOL_2$ is a member of ${\\mathrm{auxL}}$.\n\nFor any $\\tilde{y}\\in SOL_2(\\tilde{x})$, if $\\tilde{y}= y\\natural M_Q(x)$, then we set $m_2(\\tilde{x},\\tilde{y})= m_1(x,y) +\\Delta_{x}$; if $\\tilde{y}=By\\natural M_P(x)$, then we set $m_2(\\tilde{x},\\tilde{y}) = m_1(x,y)\\cdot 2^{t} + \\Delta_{x}$, where $t=|bin(b_0(x))|-|bin(m_1(x,y))|$. It follows that $m_2(\\tilde{x},\\tilde{y})\\geq b_0(x)+\\Delta_{x}$ for all $(\\tilde{x},\\tilde{y})\\in I_2\\circ SOL_2$. Note that $m_2(\\tilde{x},M_Q(x)\\natural M_Q(x))$ equals $b_0(x)+\\Delta_{x}$, which is $2^{{\\lfloor {\\log2\\beta} \\rfloor}+{\\lfloor {\\log{b_0(x)}} \\rfloor}+2}$. Choose a function $b$ so that ${\\lfloor {\\log{b(x)}} \\rfloor} = {\\lfloor {\\log{2\\beta}} \\rfloor}+{\\lfloor {\\log{b_0(x)}} \\rfloor}+2$\nfor all $x\\in (I_1\\circ SOL_1)^{\\exists}$. Clearly, $m_2\\in{\\mathrm{auxFL}}$ holds.\n\nThe desired $M_P(\\tilde{x})$ outputs $M_Q(x)\\natural M_Q(x)$ for any $\\tilde{x} \\in I_2$ if $SOL_2(\\tilde{x})\\neq{\\mathrm{\\O}}$, and it outputs $\\bot$ otherwise. Let $\\tilde{x}\\in (I_2\\circ SOL_2)^{\\exists}$.\nIt follows that $m_2^*(\\tilde{x}) = \\max_{\\tilde{y}\\in SOL_2(x)}\\{m_2(\\tilde{x},\\tilde{y})\\} = \\max_{y\\in SOL_1(x)}\\{m_1(x,y)+\\Delta_{x}\\}=m_1^*(x)+\\Delta_{x}$ since $m_1^*(x)\\geq b_0(x)$. Moreover, we obtain  $m_1(x,g(x,\\tilde{y},r)) \\geq b_0(x)$. It follows that\n\n", "itemtype": "equation", "pos": 91761, "prevtext": "\nprovided that neither $m(x,y)$ nor $m^*(x)$ is zero. Notice that  $R(x,y)=1$ iff $y\\in SOL^*(x)$. Let $\\gamma>1$ be a constant indicating an upper bound of the performance ratio.\nWith this constant $\\gamma$, we say that $P$ is\n{\\em polynomial-time $\\gamma$-approximable} if there exists a  polynomial-time deterministic Turing machine $M$  such that, for any instance $x\\in I$, if $SOL(x)\\neq{\\mathrm{\\O}}$, then $M(x)\\in SOL(x)$ and $R(x,M(x))\\leq \\gamma$; otherwise, $M(x)$ outputs ``no solution'' (or a symbol $\\bot$); in addition,\nthe values\\footnote{The polynomial-time computability of the value $m(x,M(x))$ is trivial; however, the computability requirement for this value is quite important for the log-space computability and the NC$^{1}$ computability.} $m(x,M(x))$ must be computed in polynomial time from inputs $x$.\nSuch a machine is referred to as a {\\em $\\gamma$-approximate algorithm}. The $\\gamma$-approximability clearly implies that the set  $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ belongs to ${\\mathrm{P}}$.\nThe notation ${\\mathrm{APXP}}_{{\\cal D}}$ denotes a class consisting of problems $P$ in   class ${\\cal D}$ of optimization problems such that, for a certain fixed constant  $\\gamma>1$, $P$ is polynomial-time $\\gamma$-approximable. Notice that ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ is conventionally expressed as ${\\mathrm{APX}}$ (see, {\\textrm{e.g.},\\hspace*{2mm}} \\cite{ACG+03}).\n\nLikewise, we define three extra notions of ``log-space $\\gamma$-approximation'' \\cite{Tan07}, ``${\\mathrm{NC}^{ {i} }}$ $\\gamma$-approximation,''  and ``${\\mathrm{AC}^{ {i} }}$ $\\gamma$-approximation'' by replacing ``polynomial-time Turing machine'' in the above definition with ``logarithmic-space (auxiliary) Turing machine,'' ``uniform family of ${\\mathrm{NC}^{ {i} }}$-circuits,'' and ``uniform family of ${\\mathrm{AC}^{ {i} }}$-circuits,'' respectively, for every index $i\\in{\\mathbb{N}}$.\nWe then introduce the notations of ${\\mathrm{APXL}}_{{\\cal D}}$, ${\\mathrm{APXNC}^{ {i} }}_{{\\cal D}}$, and ${\\mathrm{APXAC}^{ {i} }}_{{\\cal D}}$ using ``log-space $\\gamma$-approximation,'' ``${\\mathrm{NC}^{ {i} }}$ $\\gamma$-approximation,'' and ``${\\mathrm{AC}^{ {i} }}$ $\\gamma$-approximation,''  respectively. It follows that ${\\mathrm{APXAC}^{ {0} }}_{{\\cal D}} \\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\cal D}} \\subseteq {\\mathrm{APXL}}_{{\\cal D}} \\subseteq {\\mathrm{APXP}}_{{\\cal D}}$ for any reasonable optimization/approximation class ${\\cal D}$.\n\n\n\\paragraph{PTAS, LSAS, NC$^{i}$AS, and AC$^{i}$AS.}\nA deterministic Turing machine $M$ is called a {\\em polynomial-time approximation scheme} (or a PTAS) if, for any ``fixed constant'' $r\\in{\\mathbb{Q}}^{>1}$, there exists a polynomial $p_r(n)$ such that, for every admissible instance $x\\in I$, if $SOL(x)\\neq{\\mathrm{\\O}}$, then $M$ takes $(x,r)$ as its input and outputs an $r$-approximate solution of $x$ in time at most $p_r(|x|)$; otherwise, $M(x)$ outputs ``no solution'' (or a symbol $\\bot$).\nExamples of such polynomial $p_r(n)$ are   ${\\lceil {\\frac{r}{r-1}} \\rceil}n^3$ and $n^{{\\lceil {1/(r-1)} \\rceil}}$.\nAny approximation scheme is also a $\\gamma$-approximate algorithm for any chosen constant $\\gamma>1$. The approximation class $\\mathrm{PTAS}_{{\\mathrm{NPO}}}$ denotes a collection of all NPO problems that admit PTAS's. In a similar manner, we can define a notion of {\\em logarithmic-space approximation scheme} (or LSAS) and the associated approximation class $\\mathrm{LSAS}_{{\\mathrm{NLO}}}$ by replacing ``polynomial time'' and ``polynomial'' with ``logarithmic space'' and ``logarithmic function,'' respectively.\n\nThe definitions of ${\\mathrm{NC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ and ${\\mathrm{AC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ are given essentially in the same way with a slight technical complication on uniformity condition. ${\\mathrm{NC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{AC}^{ {i} }}\\mathrm{AS}_{{\\mathrm{NLO}}}$) can be introduced using circuits of size $p_r(n)$ and depth $\\ell_r(n)$ with bounded (resp., unbounded) fan-in gates, where $p_r(n)$ is a polynomial and $\\ell_r(n)$ is a logarithmic function as long as $r$ is treated as a fixed constant. Here, the uniformity requires $\\mathrm{DTIME}(\\ell'_r(n))$ for another logarithmic function $\\ell'_r$ with $r$ being treated as a constant.  \n{\\medskip}\n\nWe have so far given 14 classes of optimization problems, which we shall discuss in details in the subsequent sections. Given an arbitrary nonempty class ${\\cal D}$ of optimization problems, it holds that  ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\cal D}}\\subseteq {\\mathrm{LO}}_{{\\cal D}} \\subseteq {\\mathrm{PO}}_{{\\cal D}}$ and  ${\\mathrm{APXNC}^{ {1} }}_{{\\cal D}}\\subseteq {\\mathrm{APXL}}_{{\\cal D}}\\subseteq {\\mathrm{APXP}}_{{\\cal D}}$. It also follows that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\cal D}} \\subseteq {\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\cal D}} \\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\cal D}}$, ${\\mathrm{LO}}_{{\\cal D}} \\subseteq {\\mathrm{LSAS}}_{{\\cal D}}  \\subseteq {\\mathrm{APXL}}_{{\\cal D}}$, and ${\\mathrm{PO}}_{{\\cal D}} \\subseteq {\\mathrm{PTAS}}_{{\\cal D}} \\subseteq {\\mathrm{APXP}}_{{\\cal D}}$.\nWhen ${\\cal D}={\\mathrm{NLO}}$, in particular, three classes ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$, ${\\mathrm{PTAS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$ coincide with ${\\mathrm{NLO}}$. Since the proof of this fact is short, we include it here.\n\n\\begin{lemma}\\label{PO=APXP=NLO}\n${\\mathrm{APXP}}_{{\\mathrm{NLO}}} = {\\mathrm{PTAS}}_{{\\mathrm{NLO}}} = {\\mathrm{PO}}_{{\\mathrm{NLO}}} = {\\mathrm{NLO}}$.\n\\end{lemma}\n\n\\begin{proof}\nNote that ${\\mathrm{PO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{PTAS}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{APXP}}_{{\\mathrm{NLO}}}$. First, we claim that ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{NLO}}$. By the definition of ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$, all problems in ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$ must be ${\\mathrm{NLO}}$ problems, and hence they are in ${\\mathrm{NLO}}$.\n\nNext, we show that ${\\mathrm{NLO}}\\subseteq {\\mathrm{PO}}_{{\\mathrm{NLO}}}$.\nLet $P=(I,SOL,m,goal)$ be any problem in ${\\mathrm{NLO}}$.\nHere, we consider only the case of $goal=\\text{\\sc max}$ because the case of {\\sc min} is analogous. We want to show that $P$ belongs to ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$. Let $x$ be any instance in $I$. Consider the following algorithm on input $x$.\nHere, we define $D=\\{(x,y)\\in I\\circ SOL \\mid \\exists z\\in SOL(x)\\,[ z \\geq y \\wedge m(x,z)\\geq m(x,y)]\\}$, where the notation $\\geq$ used for strings $x$ and $y$ is the lexicographic ordering. Note that $D\\in{\\mathrm{NL}}\\subseteq{\\mathrm{P}}$. Now, we can use a binary search technique using $D$ to find a maximal solution $y\\in SOL^*(x)$ in polynomial time.\nTherefore, we conclude that ${\\mathrm{NLO}} \\subseteq {\\mathrm{PO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{PTAS}}_{{\\mathrm{NLO}}} \\subseteq {\\mathrm{APXP}}_{{\\mathrm{NLO}}} \\subseteq {\\mathrm{NLO}}$. This implies the lemma.\n\\end{proof}\n\n\n\nTaking a slightly different approach toward a study on ${\\mathrm{NPO}}$ problems, Krentel \\cite{Kre88} introduced a class ${\\mathrm{OptP}}$ of optimization functions. Let ${\\mathrm{MaxP}}$ (resp., ${\\mathrm{MinP}}$) denote the class of all functions from $\\Sigma_1^*$ to $\\Sigma_2^*$, each of which satisfies the following property: there exists a polynomial-time nondeterministic Turing machine $M$ such that, for every input $x\\in\\Sigma_1^*$, $f(x)$ denotes the maximal (resp., minimal) string (in the lexicographic order) generated by $M$ on $x$ \\cite{KST89}, where $\\Sigma_1$ and $\\Sigma_2^*$ are alphabets.\nThe class ${\\mathrm{OptP}}$ is simply defined as ${\\mathrm{MaxP}}\\cup {\\mathrm{MinP}}$. We further define ${\\mathrm{OptL}}$ in a similar way but using log-space nondeterministic Turing machines.\nNotice that \\`{A}lvarez and Jenner \\cite{AJ93} originally defined ${\\mathrm{OptL}}$ as the set of {\\em only} maximization problems and that we need to pay a special attention to their results whenever we apply them in our setting.\n\n\n\\subsection{Approximation-Preserving Reductions}\\label{sec:AP-reducibility}\n\nTo compare the computational complexity of two optimization problems, we wish to use three types of reductions between those two problems. We follow well-studied reductions, known as {\\em approximation-preserving (AP) reductions} and {\\em exact (EX) reductions}.\nGiven two optimization problems $P=(I_1,SOL_1,m_1,goal)$ and $Q=(I_2,SOL_2,m_2,goal)$, $P$ is {\\em  polynomial-time AP-reducible} (or more conveniently, {\\em APP-reducible}) to $Q$, denoted $P{\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}} Q$, if there are two functions $f$ and $g$ and a constant $c\\geq1$ such that the following {\\em APP-condition} is satisfied:\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\\item for any instance $x\\in I_1$ and any $r\\in{\\mathbb{Q}}^{>1}$, it holds that $f(x,r)\\in I_2$,\n\n\\item for any $x\\in I_1$ and any $r\\in{\\mathbb{Q}}^{>1}$, if $SOL_1(x)\\neq{\\mathrm{\\O}}$ then $SOL_2(f(x,r))\\neq{\\mathrm{\\O}}$,\n\n\\item for any $x\\in I_1$, any $r\\in{\\mathbb{Q}}^{>1}$, and any $y\\in SOL_2(f(x,r))$, it holds that $g(x,y,r)\\in SOL_1(x)$,\n\n\\item $f(x,r)$ is computed by a deterministic Turing machine and $g(x,y,r)$ is computed by an auxiliary Turing machine, both of which run in time polynomial in $(|x|,|y|)$ for any $(x,y)\\in I\\circ SOL$ and any number  $r\\in{\\mathbb{Q}}^{>1}$, and\n\n\\item for any $x\\in I_1$, any $r\\in{\\mathbb{Q}}^{>1}$, and any $y\\in SOL_2(f(x,r))$, $R_2(f(x,r),y) \\leq r$ implies $R_1(x,g(x,y,r))\\leq 1+c(r-1)$, where $R_1$ and $R_2$ respectively express the performance ratios for $P_1$ and $P_2$.\n\\end{itemize}\nNotice that the above APP-condition makes us concentrate only on instances of $\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$ and that, for other instances $x$, we might possibly set the value $f(x,r)$ arbitrarily (as long as $x\\in I_1$ iff $f(x,r)\\in I_2$).\nWhen this APP-condition holds, we also say that $P$ {\\em APP-reduces}\nto $Q$. The triplet $(f,g,c)$ is called a {\\em polynomial-time AP-reduction} (or an {\\em APP-reduction}) from $P$ to $Q$. For more details, refer to, e.g., \\cite{ACG+03}.\n\nTo discuss optimization problems within ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$,\nwe further need to introduce another type of reduction $(f,g)$, in which $g$ ``exactly'' transforms in polynomial time an optimal solution for $Q$ to another optimal solution for $P$ so that ``$Q\\in {\\mathrm{PO}}_{{\\mathrm{NPO}}}$'' directly implies ``$P\\in {\\mathrm{PO}}_{{\\mathrm{NPO}}}$.'' We write $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{P}}} Q$ when the following {\\em EX-condition} holds:\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\\item for any instance $x\\in I_1$, it holds that $f(x)\\in I_2$,\n\n\\item for any $x\\in I_1$, if $SOL_1(x)\\neq{\\mathrm{\\O}}$ then $SOL_2(f(x))\\neq{\\mathrm{\\O}}$,\n\n\\item for any $x\\in I_1$ and any $y\\in SOL_2(f(x))$, it holds that $g(x,y)\\in SOL_1(x)$,\n\n\\item $f(x)$ is computed by deterministic Turing machine and $g(x,y)$ is computed by an auxiliary Turing machine, both of which run in time polynomial in $(|x|,|y|)$, and\n\n\\item for any $x\\in I_1$ and any $y\\in SOL_2(f(x))$, $R_2(f(x),y) =1$ implies $R_1(x,g(x,y)) =1$, where $R_1$ and $R_2$ respectively express the performance ratios for $P_1$ and $P_2$.\n\\end{itemize}\nThe above pair $(f,g)$ is called a {\\em polynomial-time EX-reduction} (or an {\\em EXP-reduction}) from $P$ to $Q$.\n\nIt is quite useful to introduce a notion that combines both  ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}}$ and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{P}}}$. Let us define the notion of {\\em polynomial-time strong AP-reduction} (strong APP-reduction or sAPP-reduction), denoted ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{P}}}$, obtained from ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{P}}}$ by allowing $r$ (used in the above definition of APP-reduction) to be chosen from ${\\mathbb{Q}}^{\\geq1}$ (instead of ${\\mathbb{Q}}^{>1}$).\n\nNext, we weaken the behaviors of polynomial-time (strong) APP-reductions by modifying the ``polynomial-time'' requirement imposed on the aforementioned definition of (strong) APP-condition.  When we replace ``polynomial-time'' by ``logarithmic-space,''  ``uniform family of ${\\mathrm{NC}^{ {1} }}$-circuits,'' and  ``uniform family of ${\\mathrm{AC}^{ {0} }}$-circuits,''  we respectively obtain the corresponding notions of {\\em (strong) APL-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$),  {\\em (strong) APNC$^1$-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$), and {\\em (strong) APAC$^0$-reduction} (${\\leq_{\\mathrm{AP}}}^{{\\mathrm{AC}^{ {0} }}}$, ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$). Notice that the notion of error-preserving reduction (or E-reduction), which was used in \\cite{Tan07}, essentially matches sAPL-reduction. Likewise, we define {\\em EXL-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$), {\\em EXNC$^{1}$-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$), and {\\em EXAC$^{0}$-reduction} (${\\leq_{\\mathrm{EX}}}^{{\\mathrm{AC}^{ {0} }}}$) from EXP-reduction.\n\nThe following two lemmas are immediate from the definition of sAP-reductions and we omit their proofs.\n\n\\begin{lemma}\\label{reduction-transitive}\nFor any two reduction type $e_1,e_2\\in\\{{\\mathrm{P}},{\\mathrm{L}},{\\mathrm{NC}^{ {1} }},{\\mathrm{AC}^{ {0} }}\\}$, if $e_1\\subseteq e_2$ (seen as complexity classes), then $P_1{\\leq_{\\mathrm{sAP}}}^{e_1}P_2$ implies $P_1{\\leq_{\\mathrm{sAP}}}^{e_2}P_2$. The same statement holds for ${\\leq_{\\mathrm{AP}}}^{e}$ and ${\\leq_{\\mathrm{EX}}}^{e}$.\n\\end{lemma}\n\n\\begin{lemma}\nFor any reduction type $e\\in\\{{\\mathrm{P}},{\\mathrm{L}},{\\mathrm{NC}^{ {1} }},{\\mathrm{AC}^{ {0} }}\\}$, $P_1{\\leq_{\\mathrm{sAP}}}^{e} P_2$ implies both $P_1{\\leq_{\\mathrm{AP}}}^{e} P_2$ and $P_1{\\leq_{\\mathrm{EX}}}^{c} P_2$.\n\\end{lemma}\n\nIn the next lemma, we shall present a useful property, called a {\\em downward closure property}, for ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reductions. A similar property holds also for ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions.\n\n\\begin{lemma}\\label{downward-property}\n[downward closure property]\nLet $P$ and $Q$ be any two optimization problems in ${\\mathrm{NLO}}$.\n\\begin{enumerate}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{APXNC}^{ {1} }},{\\mathrm{NC}^{ {1} }\\mathrm{AS}}\\}$. If $P{\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$, where ${\\mathrm{NLO}}_{{\\mathrm{NLO}}}$ is understood as ${\\mathrm{NLO}}$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{LO}},{\\mathrm{NC}^{ {1} }\\mathrm{O}}\\}$. If $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$.\n\\end{enumerate}\n\\end{lemma}\n\nAn immediate consequence of Lemma \\ref{downward-property} is the following corollary. In comparison, by setting  ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{LO}}\\}$, for any $P,Q\\in {\\mathrm{NLO}}$, if $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$ \\cite{Tan07}.\n\n\\begin{corollary}\nLet ${\\cal D}\\in\\{{\\mathrm{NLO}},{\\mathrm{APXL}},{\\mathrm{LSAS}},{\\mathrm{LO}}\\}$. For any $P,Q\\in {\\mathrm{NLO}}$, if $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}Q$ and $Q\\in {\\cal D}_{{\\mathrm{NLO}}}$, then $P\\in{\\cal D}_{{\\mathrm{NLO}}}$, where ${\\mathrm{NLO}}_{{\\mathrm{NLO}}}$ is ${\\mathrm{NLO}}$.\n\\end{corollary}\n\nHere, we shall briefly give the proof of Lemma \\ref{downward-property}.\n\n\\begin{proofof}{Lemma \\ref{downward-property}}\nTake any two optimization problems $P=(I_1,SOL_1,m_1,goal_1)$ and $Q=(I_2,SOL_2,m_2,goal_2)$ in ${\\mathrm{NLO}}$. In what follows, we shall prove only the case of ${\\cal D}={\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, because the other cases can be similarly\ntreated.\n\n(1)  Assume that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}} Q$ via an APNC$^{1}$-reduction $(f,g,c)$ and that $Q$ is in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. Given any constant $r'>1$, let $C_{r'}$ be an  NC$^{1}$ $r'$-approximate algorithm solving $Q$.\nTo show that $P\\in{\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, it suffices to construct, for each constant $r>1$, an appropriate NC$^{1}$ circuit, say, $D_r$ that finds $r$-approximate solutions for $P$.\n\nGiven a constant $r>1$, let us define $r'=1+(r-1)/c >1$ and consider $C_{r'}$.  Since $C_{r'}$ is an NC$^{1}$ $r'$-approximate algorithm, it follows that the performance ratio $R_2$ for $C_{r'}$ satisfies $R_2(z,C_{r'}(z))\\leq r'$ for any $z\\in I_2$.\nNext, we define the desired algorithm $N_{r}$ as follows: on input $x\\in I_1$, compute simultaneously $z= f(x,r)$ and $y = C_{r'}(z)$ and then output $g(x,y,r')$. Since $R_2(z,C_{r'}(z))\\leq r'$, it follows by the definition of ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$ that $R_1(x,N_{r}(x)) = R_1(x,g(x,C_{r'}(z),r'))\\leq 1+c(r'-1)=r$. Hence, $N_r$ is an $r$-approximate algorithm for $P$.\n\nWe still need to show that $N_{r}$ can be realized by an NC$^{1}$-circuit.\nFor this purpose, we prepare an NC$^{1}$ circuit $C_f$ that, on input $(x,e)\\in \\Sigma^*\\times 1\\{0,1\\}^*$,  outputs the $rep(e)$-th bit of $f(x,r)$. Notice that  $|f(x,r)|$ is polynomially bounded.\nMoreover, let $C_g$ denote an NC$^{1}$ circuit computing $g$.\nWe construct an NC$^{1}$-circuit $M'$ that, on input $(x,e)\\in \\Sigma^*\\times 1\\{0,1\\}^*$, computes the $rep(e)$-th bit of $C_{r'}(f(x,r'))$. During this procedure, whenever $C_{r'}$ tries to access the $j$th bit of $f(x,r')$, we run $C_f$ on $(x,bin(j))$.\nThe desired algorithm $N_{r}$ is executed as follows. We first run $C_g$ using the first and third input tapes for $x$ and $r'$ and leaving the second tape blank. Whenever $C_g$ tries to access the $i$th bit $y_i$ of $y=C_{r'}(f(x,r'))$, we run $M'$ on $(x,bin(i))$. It is not difficult to show that this procedure can be implemented on an appropriate NC$^{1}$-circuit.\n\n(2) Assume that $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}} Q$ via an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g)$ with  $Q\\in{\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.  Since $Q\\in{\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$,\nthere exists an NC$^{1}$ circuit $M$ for which  $M(x)\\in SOL_{2}(x)$ and $R_2(x,M(x))=1$ for any $x\\in I_2$. To show that $P$ is in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$, let us consider the following algorithm $N$. On input $x$, compute $y=M(f(x))$ and output $w=g(x,y)$. For a similar reason to (1), $N$ can be implemented by a certain NC$^{1}$ circuit. Since $R_1(x,N(x))=R_1(x,g(x,y))$, by the definition of an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction, we obtain $R_2(f(x),M(f(x))) = 1$. Therefore, $N$ exactly solves $P$.\n\\end{proofof}\n\nOur AP-, EX-, and sAP-reductions can help us identify the most difficult problems in a given optimization/approximation class.\nSuch problems are generally called ``complete problems,'' which have played a crucial role in understanding the structural features of\noptimization and approximation classes.\n\nFormally, let $\\leq$ be any reduction discussed in this section, and let  ${\\cal D}$ be any class of optimization problems. An optimization problem $P$ is called {\\em $\\leq$-hard} for ${\\cal D}$ if, for every problem $Q$ in ${\\cal D}$,  $Q\\leq P$ holds. Moreover, $P$ is said to be {\\em $\\leq$-complete} for ${\\cal D}$ if $P$ is in ${\\cal D}$ and it is $\\leq$-hard for ${\\cal D}$.\nThis completeness will be a central subject in Sections \\ref{sec:completeness}--\\ref{sec:PBP}.\n\n\n\n\n\\section{General Complete Problems}\\label{sec:completeness}\n\nComplete problems represent a certain structure of a given optimization or approximation class and they provide useful insights into specific features of the class. To develop a coherent theory of NLO problems, it is essential to study such complete problems. In the subsequent subsections, we shall present numerous complete problems for various optimization and approximation classes.\n\n\n\\subsection{Why APNC$^{1}$- and EXNC$^{1}$-Reductions?}\\label{sec:why-NC1}\n\nTo discuss complete problems for refined optimization and approximation classes under certain reductions, it is crucial to choose reasonable types of reductions. By Lemma \\ref{reduction-transitive}, for example, any ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problem for an optimization/approximation class ${\\cal D}$ is also ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$-complete, but the converse may not be true in general. In what follows, we briefly argue that the  ${\\leq_{\\mathrm{AP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reductions are so powerful that all problems in  ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ respectively become reducible to similar problems residing even in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ and ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{proposition}\\label{characterization}\n\\begin{enumerate}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item ${\\mathrm{APXL}}_{{\\mathrm{NLO}}} = \\{P\\in{\\mathrm{NLO}}\\mid \\exists Q\\in {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}} \\cap{\\mathrm{PBO}} \\,[ P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}} Q]\\}$.\n\n\\item ${\\mathrm{LO}}_{{\\mathrm{NLO}}} = \\{P\\in{\\mathrm{NLO}}\\mid \\exists Q\\in {\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}} \\cap{\\mathrm{PBO}} \\,[ P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}} Q]\\}$.\n\\end{enumerate}\n\\end{proposition}\n\n\\begin{proof}\n(1) This claim is split into two opposite containments.\n\n($\\supseteq$) Let $P\\in {\\mathrm{NLO}}$ and $Q\\in {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$, and assume that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$. Notice that $Q$ also belongs to ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\nLemma \\ref{downward-property}(1) therefore implies that $P\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\n($\\subseteq$) Since  ${\\mathrm{APXL}}_{{\\mathrm{NLO}}} = {\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}\\cup {\\mathrm{APXL}}_{{\\mathrm{MinNL}}}$, we first consider the case of ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. Take any maximization problem $P=(I_1,SOL_2,m_1,\\text{\\sc max})$ in  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.  We want to define a new maximization problem $Q = (I_2,SOL_2,m_2,\\text{\\sc max})$ and show that $Q\\in{\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$ and $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$.\n\nSince $P\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, there exists a constant $e\\in{\\mathbb{Q}}^{>1}$ and a log-space deterministic Turing machine $M$ that produces $e$-approximate solutions of $P$; namely, the performance ratio $R_1$ of $M$'s outcome for $P$ satisfies $R_1(x,M(x))\\leq e$ for every $x\\in (I_1\\circ SOL_1)^{\\exists}$.\nFirst, we set $I_2$ to be composed of all instances of the form $(x,M(x))$ for  $x\\in I_1$. Notice that, whenever $SOL_1(x)={\\mathrm{\\O}}$, $M$ outputs the designated symbol $\\bot$. Since $M$ uses only log space, $I_2\\in{\\mathrm{L}}$ follows. Next, we define $SOL_2(x,y) = SOL_1(x)$ and $m_2((x,y),z)= m_1(x,z)$ for any $x\\in I_1$ and $y,z\\in SOL_1(x)$. By those definitions, $Q$ is a problem in ${\\mathrm{NLO}}$.\n\nLet us consider an ${\\mathrm{AC}^{ {0} }}$-circuit that outputs $y$ on admissible instance   $(x,y)$ in $(I_2\\circ SOL_2)^{\\exists}$. For any $(x,y)\\in (I_2\\circ SOL_2)^{\\exists}$, it follows that $R_2((x,y),C(x,y)) = \\frac{m_2^*(x,y)}{m_1((x,y),C(x,y))} =  \\frac{m_1^*(x)}{m_1(x,y)} = R_1(x,M(x))\\leq e$, where $R_2$ means the performance ratio for $Q$.\nHence, $C(x,y)$ is an $e$-approximate solution of $Q$. Thus, $Q$ belongs to ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\nNext, we want to show that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}} Q$ via $(f,g,1)$. Take any number $r\\in{\\mathbb{Q}}^{\\geq1}$ and define $f(x,r)=(x,M(x))$ and $g((x,y),z,r)=z$ for $x\\in I_1$ and $y,z\\in SOL_1(x)$. It follows that $R_2(f(x,r),z) = \\frac{m_1^*(x)}{m_1(x,z)} = \\frac{m_1^*(x)}{m_1(x,g((x,y),z,r))} =R_1(x,g((x,y),z,r))$. Since  $f$ is in ${\\mathrm{FL}}$ and $g$ is in ${\\mathrm{FAC}^{ {0} }}$, $P$ indeed  sAPL-reduces to $Q$. Because $P$ is arbitrary, we conclude that every maximization problem in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-reducible to $Q$.\n\nIn a similar fashion, we can show that every minimization problem $P'$ in ${\\mathrm{APXL}}_{{\\mathrm{MinNL}}}$ can be reduced to a certain minimization problem $Q'$ in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\n(2) This claim can be proven in a similar way to (1).\n\n($\\supseteq$) Take two optimization problems  $P\\in{\\mathrm{NLO}}$ and $Q\\in{\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Assume that $P{\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}Q$ via $(f,g)$. Lemma \\ref{downward-property}(2) then ensures that $P$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\n($\\subseteq$) We begin with the case of ${\\mathrm{LO}}_{{\\mathrm{MaxNL}}}$.\nLet $P=(I_1,SOL_1,m_1,\\text{\\sc max})$ be an arbitrary problem in ${\\mathrm{LO}}_{{\\mathrm{MaxNL}}}$ and take a deterministic Turing machine $M$ that solves $P$ using log space.\nWe intend to construct another problem $Q = (I_2,SOL_2,m_2,\\text{\\sc max})$ so that  $P$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reducible to $Q$. For this desired problem $Q$, we set $I_2= I_1\\circ SOL_1$ and $SOL_2(x,y)=\\{y\\}$ for any $(x,y)\\in I_2$. The measure function $m_2$ is defined as $m_2((x,y),z)=2$ if $y=z$, and $1$ otherwise. Obviously, $m_2$ is polynomially bounded and is in ${\\mathrm{FAC}^{ {0} }}$.\nFor a particular input $(x,M(x))$, since $m_2((x,M(x)),M(x)) = m_2^*(x,M(x))$, we obtain $M(x)\\in SOL_2^*(x,M(x))$. Thus, it follows that $Q\\in{\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Finally, we define a reduction $(f,g)$ as\n$f(x)=(x,M(x))$ and $g(x,z)=M(x)$ for any $z\\in SOL_2(f(x))$. Note that $R_2(f(x),z) =1$ implies $z=M(x)$, and thus the performance ratio $R_1(x,g(x,z))$ for $P$ satisfies $R_1(x,g(x,M(x))) = R_1(x,M(x)) =1$. Therefore, $(f,g)$ ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-reduces $P$ to $Q$.\n\\end{proof}\n\nProposition \\ref{characterization} suggests that  the notions of ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{L}}}$-completeness do not capture the essential difficulty of the optimization complexity class ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Therefore, in what follows, we intend to use weaker types of reductions. In particular, we limit our interest within  ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions.\n\n\n\n\nAs a quick example of ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problems, let us consider the {\\em minimum weighted $s$-$t$ cut problem} ({\\sc Min Weight-st-Cut}), which is to find an $s$-$t$ cut of a given weighted directed graph so  that the {\\em (weighted) capacity}  of the cut (i.e., the total weight of edges from $S_0$ to $S_1$)  is minimized, where an {\\em $s$-$t$ cut} for two distinct vertices $s,t\\in V$ is a partition $(S_0,S_1)$ of the vertices for which $s\\in S_0$ and $t\\in S_1$.\nWe represent this cut $(S_0,S_1)$ by an assignment $\\sigma$ from $V$ to $\\{0,1\\}$ satisfying the following condition: for every $v\\in V$ and every $i\\in\\{0,1\\}$, $\\sigma(v)=i$ iff $v\\in S_i$.\n\n{\\medskip}\n{\\sc Minimum Weighted s-t Cut Problem} ({\\sc Min Weight-st-Cut}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\\item {\\sc instance:} a directed graph $G=(V,E)$, two distinguished vertices $s,t\\in V$, where $s$ is a {\\em source} and $t$ is a {\\em sink} (or a {\\em target}), and an edge weight function $c:E\\to{\\mathbb{N}}^{+}$.\n\n\\item {\\sc Solution:} an $s$-$t$ cut $(S_0,S_1)$, specified by an assignment $\\sigma:V\\to\\{0,1\\}$ as described above.\n\n\\item {\\sc Measure:} the {\\em (weighted) capacity} of the $s$-$t$ cut (i.e., $\\sum_{(v,w)\\in E\\wedge v\\in S_0\\wedge w\\in S_1}c(v,w)$).\n\\end{itemize}\n\nNote that the capacity of any $s$-$t$ cut is at most $\\max_{e\\in E}\\{c(e)\\}|E|$. It is possible to prove that {\\sc Min Weight-st-Cut} is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\n\\begin{proposition}\\label{min-st-cut-is-po}\n{\\sc Min Weight-st-Cut} is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\\end{proposition}\n\nThe proof of Proposition \\ref{min-st-cut-is-po} can be obtained by an appropriate modification of the ${\\mathrm{P}}$-completeness proof of Goldschlager {\\textrm{et al.}}~\\cite{GSS82} for the ``decision version'' of the {\\em maximum $s$-$t$ flow problem}.\nThe proof of Proposition \\ref{min-st-cut-is-po} is placed in Appendix for readability. The proposition will be used in Section \\ref{sec:complexity-OP}.\n\n\n\\subsection{Complete Problems Concerning Path Weight}\\label{sec:general-complete}\n\nWe have seen in Section \\ref{sec:why-NC1} the importance of ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$- and ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reductions for discussing the computational complexity of our refined optimization problems.\nIn this and the next subsections under those special reductions, we shall present a few complete problems for various optimization and approximation classes.\n\nThere are two categories of NLO problems to distinguish in our course of studying  the complexity of NLO problems. The first category contains NLO problems $(I,SOL,m,goal)$ for which the set $(I\\circ SOL)^{\\exists}$ ($=\\{x\\in I\\mid SOL(x)\\neq{\\mathrm{\\O}}\\}$) belongs to ${\\mathrm{NL}}$ but may not fall into ${\\mathrm{L}}$ unless ${\\mathrm{L}} = {\\mathrm{NL}}$. The second category, in contrast, requires the set $(I\\circ SOL)^{\\exists}$ to be in ${\\mathrm{L}}$. Many of the optimization problems of the first category are unlikely to fall into ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ or ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\nFirst, we shall look into an optimization analogue of\nthe well-known {\\em directed $s$-$t$ connectivity problem} (also known as the {\\em graph accessibility problem} and the {\\em graph reachability problem} in the past literature), denoted by $\\mathrm{DSTCON}$, in which, for any  directed graph $G=(V,E)$ and two vertices $s,t\\in V$, we are asked to determine whether there is a path from $s$ to $t$ in $G$.\nEarlier, Jones \\cite{Jon75} showed that $\\mathrm{DSTCON}$ is ${\\mathrm{NL}}$-complete under $\\leq_{m}^{{\\mathrm{L}}}$ (log-space many-one) reductions. These reductions can be replaced by appropriate $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-reductions, and thus  $\\mathrm{DSTCON}$ becomes $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NL}}$.\nLet us consider a series of problems associated with minimum path weights of  graphs. First, recall the minimum path weight problem ({\\sc Min Path-Weight}) introduced in Section \\ref{sec:intro}.\n\n{\\medskip}\n{\\sc Minimum Path Weight Problem} ({\\sc Min Path-Weight}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a directed graph $G=(V,E)$, two distinguished vertices $s,t\\in V$, and a (vertex) weight function $w:V\\to{\\mathbb{N}}$.\n\n\\item {\\sc Solution:} a path ${\\cal S}=(v_1,v_2,\\ldots,v_k)$ from $s$ to $t$ (i.e., $s=v_1$ and $t=v_k$).\n\n\\item {\\sc Measure:} ``biased'' path weight $w(S) = \\max\\{1,rep(bin(w(v_1))bin(w(v_2))\\cdots bin(w(v_k)))\\}$.\n\\end{itemize}\n\nIn the above definition, we generally do not demand that $s$ is a {\\em source} (i.e., a node of indegree $0$) and $t$ is a {\\em sink} (i.e., a node of outdegree $0$)  although such a restriction does not change the completeness of the problem.\n\nHere, we need to remark that the choice of our measure function for {\\sc Min Path-Weight} is quite artificial.\nAs a quick example, if ${\\cal S}=(v_1,v_2,v_3,v_4)$ with $w(v_1)=3$, $w(v_2)=0$,  $w(v_3)=2$, and $w(v_4)=4$, then $w({\\cal S}) = rep(1110100)$ since $bin(0)=\\lambda$ (the empty string).\nIt is important to note that we use the {\\em biased path weight} instead of a {\\em standard path weight} defined as $\\sum_{i\\in[k]}w(v_i)$. This comes from the fact that,  because log-space computation cannot store super-logarithmically many bits, it cannot sum up all super-logarithmically large weights of vertices. However, if we set all vertices of a given input graph have weights of exactly  $1$, then {\\sc Min Path-Weight} is essentially identical to a problem of finding the ``shortest'' $s$-$t$ path in the graph.\n\nIn comparison, we also define a polynomially-bounded form of {\\sc Min Path-Weight} simply by demanding that $1\\leq w(v)\\leq|V|$ for all $v\\in V$ and by changing $w({\\cal S})$ to the {\\em total path weight} $w'({\\cal S})=\\sum_{i=1}^{k}w(v_i)$. Notice that $w'({\\cal S})\\leq k|V|$. For our later reference in Section \\ref{sec:PBP}, we call this modified problem the {\\em minimum bounded path weight problem} ({\\sc Min BPath-Weight}) to emphasize the polynomially-boundedness of the problem.\n\nHereafter, we shall prove that {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{MinNL}}$.\n\n\\begin{theorem}\\label{Min-Path-complete}\n$\\text{\\sc Min Path-Weight}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{MinNL}}$.\n\\end{theorem}\n\nFor the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hardness part of Theorem \\ref{Min-Path-complete}, we want to introduce a useful notion of {\\em configuration graph} of a log-space auxiliary Turing machine $M$ on a given input $x$ together with any possible auxiliary input $y$,  which describes an entire computation tree of $M$ working on $x$ and $y$.\nThis is a weighted directed graph, which will be used in later proofs, establishing the hardness of target optimization problems; however, in those proofs, we may need to modify  the original configuration graph given below.\nSince each vertex of a configuration graph is labeled by a ``partial configuration, '' we first define such partial configurations of $M$ on $x$. To simplify the following description, we consider the case where $M$ has only one work tape.\n\nRecall from Section \\ref{sec:basic_model} an auxiliary Turing machine $M = (Q,\\Sigma,\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\Gamma,\\Theta,\\Phi,q_0,q_{acc},q_{rej})$ with its transition function $\\delta$ mapping $(Q-\\{q_{acc},q_{rej}\\}) \\times (\\Sigma\\cup\\{{|}\\!\\!\\mathrm{c},{\\$},\\lambda\\}) \\times \\Gamma \\times (\\Theta\\cup\\{{\\$}\\})$ to $Q\\times \\Gamma\\times (\\Phi\\cup\\{\\lambda\\}) \\times D\\times D_1$.\nThe current tape situation is encoded into $uhw$, which indicates that  the tape content is $uv$ and the tape head is scanning the leftmost symbol of $w$, where $h$ is a special symbol representing the tape heard. A {\\em partial configuration}  of $M$ on input $x$ is a tuple $v = {\\langle {q,w,u,\\tau,\\xi,k} \\rangle}$, which intuitively indicates a snap shot of $M$'s computation at time $k$ ($k\\in{\\mathbb{N}}$) when $q$ is an inner state, $w$ is an encoding of $M$'s input tape, $u$ is an encoding of $M$'s work tape, $\\tau$ is a scanning auxiliary input symbol, and $\\xi$ is an output symbol or $\\lambda$ to write.\n\nWe connect each partial configuration $v=(q,w,u,\\tau,\\xi,k)$ to  others $(p,w',u',\\tau',\\xi',k+1)$ for all $\\tau'\\in\\in\\Gamma\\cup\\{{\\$}\\}$ by applying a transition ``$\\delta(q,\\sigma'_1,\\sigma'_2,\\tau) = (p,\\sigma'_2,\\xi',d_1,d_2)$,'' where $w'$ (resp., $u'$) is an encoding of the input (resp., work) tape obtained from $w$ (resp., $u$) by this transition.\nWhen a machine makes a $\\lambda$-move on the output tape, we use the same symbol ``$\\lambda$'' in place of  $\\xi$ and $\\xi'$. Here, we encode such partial configurations into binary strings of the same length by padding extra garbage bits (if necessary).\n\nThe {\\em weight} of this vertex $v$  is defined as $\\xi$ (expressed in binary).\nWe can view a {\\em computation path} $y$ of $M$ on $x$ together with a series of nondeterministic choices of $M$,  as a sequence of partial configurations.\nFor two vertices $u$ and $v$, $(u,v)$ is a direct edge if, seen as partial configurations, $v$ is obtained from $u$ by a single application of $\\delta$ and a choice of auxiliary input symbol.\nSince  each vertex is represented by $O(n)$ symbols, the total number of  vertices is at most a polynomial in $n$. We denote by $G^{M}_{x}$ the obtained configuration graph of $M$ on $x$ since $M$ halts in polynomial time.\nNote that the size of $G^{M}_{x}$ is bounded from above by a polynomial in the size of input instance $x$ of $M$.\n\nIt is important to note that, from a given encoding of a computation path $y$, we can easily extract an associated auxiliary input, because each partial configuration in $y$ contains a piece of information on the auxiliary input and $M$'s head on the auxiliary tape moves in only one direction.\n\n\\begin{proofof}{Theorem \\ref{Min-Path-complete}}\nFor notational convenience, in the following argument, $\\text{\\sc Min Path-Weight}$ is expressed as $(I_0,SOL_0,m_0,\\text{\\sc min})$. Firstly, we want to claim that $I_0\\in{\\mathrm{L}}$. This follows from the facts that  $\\mathrm{DSTCON}\\in{\\mathrm{NL}}$ and that $(I_0\\circ SOL_0)^{\\exists}$ (more accurately, $(I_0\\circ SOL_0)^{\\exists}_{q}$ for a suitable polynomial $q$) is essentially ``equivalent'' to $\\mathrm{DSTCON}$, except for the presence of a weight function $w$.\nNext, we claim that {\\sc Min Path-Weight} belongs to ${\\mathrm{MinNL}}$.\nThis claim comes from the following facts. On input $x=(G,s,t,w)$, let ${\\cal S} = (v_1,v_2,\\ldots,v_k)$ denote an arbitrary path from $s$ to $t$ in $G$. Since $m_0(x,{\\cal S})$ equals $w({\\cal S})$ by definition, the value $m_0(x,{\\cal S})$ can be computed by an appropriate auxiliary Turing machine that writes down $bin(w(v_i))$ sequentially on a write-only output tape using $O(\\log{n})$ space-bounded work tapes.\nSimilarly, given $x=(G,s,t,w)$ and an arbitrary sequence ${\\cal S}$ of vertices, we can decide whether ${\\cal S}\\in SOL_0(x)$ by checking whether ${\\cal S}$ is a path from $s$ to $t$ using a certain log-space auxiliary Turing machine.\n\nSecondly, we shall claim that {\\sc Min Path-weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hard for ${\\mathrm{MinNL}}$; namely, every minimization problem in ${\\mathrm{NLO}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to {\\sc Min Path-Weight}.\nTo prove this claim, let $P=(I,SOL,m,\\text{\\sc min})$ be any minimization problem in ${\\mathrm{NLO}}$. Note that $I\\in{\\mathrm{L}}$, $I\\circ SOL\\in{\\mathrm{auxL}}$, and $m\\in{\\mathrm{auxFL}}$.\nSince $m\\in{\\mathrm{auxFL}}$, we take an appropriate log-space auxiliary Turing machine $M$\n(with three tapes) computing $m$, where any solution candidate to $P$ is provided on an auxiliary read-once tape.\nNotice that there is a unique initial partial configuration. To ensure that $M$ has a {\\em unique} accepting partial configuration, it suffices to force $M$ to clear out all tapes just before entering a unique accepting state.\n\nLet us define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,1)$ from $P$ to {\\sc Min Path-Weight} as follows. Let $r\\geq1$ and define  $f(x,r)$ to be a configuration graph $G^{M}_{x}$ of $M$ on input $x$.\nIf $x\\in I$, then $f(x,r)\\in I_0$. Let $s$ denote the initial partial configuration of $M$ on $x$ and let $t$ be the unique accepting partial configuration of $M_2$ on $x$. As a solution to {\\sc Min Path-Weight}, let $y$ be any path in the graph $f(x,r)$ starting with $s$.\nEach vertex in $y$ contains the information on content $\\tau_i$ of the tape cell at which the auxiliary-tape head scans at time $i$. Hence, from $y$, we can recover the content of the auxiliary tape as follows. Given $y=(y_0,y_1,\\ldots,y_m)$ with $y_i=(q_i,w_i,u_i,\\tau_i,\\xi_i,k_i)$, we retrieve $\\tau_i$ for all indices $i\\in[0,m]_{{\\mathbb{Z}}}$ and output $\\tau_0\\tau_1\\tau_2\\cdots \\tau_m$. This procedure requires only an AC$^{0}$ circuit.\nLet $g(x,y,r)$ denote the entire content of the auxiliary  tape that is reconstructed from $y$ as described above. Clearly, $g$ is in ${\\mathrm{FAC}^{ {0} }}$ and, for any $y\\in SOL(x)$, we obtain $g(x,y,r)\\in SOL_0(f(x,r))$.  It is not difficult to show that $m(f(x,r),y) = m_0(x,g(x,y,r))$. Hence, $R_2(f(x,r),y)$ equals $R_1(x,g(x,y,r))$.\n\nTo complete the proof, we still need to verify that $f$ belongs to  ${\\mathrm{FAC}^{ {0} }}$.\nFor this, consider the following procedure. Recall that a graph is represented by a list of edges (i.e., vertex pairs). Starting with any input $x=(G,s,t,w)$ and $r\\geq1$, generate all pairs $(u,v)$ of partial configurations and mark $(u,v)$ whenever it is an edge of $G^{M}_{x}$. This procedure needs to wire only a finite number of bits between $u$ and $v$. Hence, $f$ can be computed by an ${\\mathrm{AC}^{ {0} }}$ circuit.\n\nTherefore, {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}$.\n\\end{proofof}\n\n\n\nIn contrast to {\\sc Min Path-Weight}, it is possible to define a maximization problem, {\\sc Max Path-Weight}, simply by taking the maximally-weighted $s$-$t$ path for the minimally-weighted one in the definition of {\\sc Min Path-Weight}. A similar argument in the proof of Theorem \\ref{Min-Path-complete} establishes the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc max Path-Weight} for ${\\mathrm{MaxNL}}$.\n\n\\begin{corollary}\\label{Max-Path-Weight-complete}\n{\\sc Max Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MaxNL}}$.\n\\end{corollary}\n\nIs {\\sc Min Path-weight} also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MaxNL}}$ and thus for ${\\mathrm{NLO}}$ ($={\\mathrm{MaxNL}}\\cup{\\mathrm{MinNL}}$)?\nUnlike NPO problems, the log-space limitation of work tapes of Turing machines complicates the circumstances around NLO problems.\nAt present, we do not know that {\\sc Min Path-Weight} is  ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}$. This issue will be discussed later in Section \\ref{sec:PBP}.\nUnder a certain assumption on ${\\mathrm{auxFL}}$, nevertheless, it is possible to achieve the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc Min Path-Weight} for ${\\mathrm{NLO}}$.\n\nWe say that ${\\mathrm{auxFL}}$ is {\\em closed under division} if, for any two functions $f,g\\in{\\mathrm{auxFL}}$ outputting natural numbers in binary, the function $h$ defined by $h(x,y)={\\lceil {f(x,y)/g(x,y)} \\rceil}$ for all inputs $x$ and all auxiliary inputs $y$ is in ${\\mathrm{auxFL}}$, provided that $g(x,y)>0$ for all inputs $(x,y)$.\n\n\\begin{proposition}\\label{Min-Path-in-NLO}\nAssume that ${\\mathrm{auxFL}}$ is closed under division. {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}$.\n\\end{proposition}\n\nWe have already proven that {\\sc Min Path-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}$ in Theorem \\ref{Min-Path-complete}. In Lemma \\ref{reduction-MaxNL-MinNL}, we shall demonstrate that every problem $P_1$ in ${\\mathrm{MaxNL}}$ is sAP${\\mathrm{AC}^{ {0} }}$-reducible to an appropriately chosen  problem $P_2$ in ${\\mathrm{MinNL}}$ if ${\\mathrm{auxFL}}$ is closed under division. Since ${\\mathrm{NLO}}= {\\mathrm{MaxNL}}\\cup {\\mathrm{MinNL}}$, this implies that {\\sc Min Path-Weight} is also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{MaxNL}}$, completing the proof of Proposition \\ref{Min-Path-in-NLO}.\n\nWe shall prove the remaining lemma, Lemma \\ref{reduction-MaxNL-MinNL}.\n\n\\begin{lemma}\\label{reduction-MaxNL-MinNL}\nAssume that ${\\mathrm{auxFL}}$ is closed under division. Every problem $P_1$ in ${\\mathrm{MaxNL}}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to an appropriate  problem $P_2$ in ${\\mathrm{MinNL}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $P_1=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any optimization problem in ${\\mathrm{MaxNL}}$.   Take an appropriate polynomial $p$ satisfying $2^{p(|x|)} \\geq m_1^*(x)$ for every instance $x\\in I_1$. For brevity, we set $b(x) = 2^{p(|x|)}$ for all $x\\in I_1$. We shall construct the desired minimization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc min})$ in ${\\mathrm{MinNL}}$.\nLet $I_2=I_1$ and $SOL_2=SOL_1$. Moreover, for every pair  $(x,y)\\in I_2\\circ SOL_2$,  define $m_2(x,y) = {\\lceil {\\frac{b(x)^2}{m_1(x,y)}} \\rceil}$.\nIt is important to note that, by our definition of measure function, $m_1$ always returns {\\em positive values}. From this definition, it follows that  $\\frac{b(x)^2}{m_2(x,y)}\\leq m_1(x,y) \\leq \\frac{b(x)^2}{m_2(x,y)+1}$ for any $(x,y)\\in I_2\\circ SOL_2$.\n\nClearly, $I_2\\in{\\mathrm{L}}$ and $I_2\\circ SOL_2\\in{\\mathrm{auxFL}}$. From our assumption on the closure property of ${\\mathrm{auxFL}}$ under division, $m_2$ falls into ${\\mathrm{auxFL}}$.\nTherefore, $P_2$ belongs to ${\\mathrm{NLO}}$.\n\nLet us define an sAPAC$^{0}$-reduction $(f,g,c)$ from $P_1$ to $P_2$ as follows.  Let  $f(x,r)=x$ and $g(x,y,r)=y$ for $r\\in{\\mathbb{Q}}^{\\geq1}$, $x\\in I_1$, and $y\\in SOL_2(f(x,r))$. Obviously, $f,g\\in{\\mathrm{FAC}^{ {0} }}$ follows.\nIf $R_2(f(x,r),y)\\leq r$ for $r\\geq1$; namely, $m_2^*(x)/r \\leq m_2(x,y)\\leq m_2^*(x)$, then\nthe performance ratio $R_1(x,g(x,y,r))$ for $P_1$ is upper-bounded as\n", "index": 3, "text": "\n\\[\nR_1(x,g(x,y,r)) = \\frac{m_1^*(x)}{m_1(x,y)} \\leq \\frac{b(x)^2}{m_2^*(x)+1} \\div  \\frac{b(x)^2}{m_2(x,y)} = \\frac{m_2(x,y)}{m_2^*(x)}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"R_{1}(x,g(x,y,r))=\\frac{m_{1}^{*}(x)}{m_{1}(x,y)}\\leq\\frac{b(x)^{2}}{m_{2}^{*}%&#10;(x)+1}\\div\\frac{b(x)^{2}}{m_{2}(x,y)}=\\frac{m_{2}(x,y)}{m_{2}^{*}(x)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mi>m</mi><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mrow><mfrac><mrow><mi>b</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow></mfrac><mo>\u00f7</mo><mfrac><mrow><mi>b</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mfrac><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\n\nWe remark that, by the construction of $P$ from $Q$, if $Q$ is polynomially bounded, then so is $P$. We wish to define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,c)$ from $Q$ to $P$. First, we set $c= \\alpha-1$. We then define $f(x,r)=x\\natural G^{M_Q}_{x}$ ($=\\tilde{x}$),  and $g(x,\\tilde{y},r) = y$ if $\\tilde{y}$ is of the form $y\\natural z$, $g(x,\\tilde{y},r)= z$ if $\\tilde{y}$ is of the form $By\\natural z$, and $g(x,\\tilde{y},r)=x$ otherwise.\nIf $x\\in I_1$, then $f(x,r)\\in I_2$ since $f(x,r)=x\\natural G^{M_Q}_{x}$. If $y\\in SOL_1(x)$, then $y\\natural M_P(x)\\in SOL_2(f(x,r))$ because $M_Q(x)$ can be constructed from $(x,G^{M_Q}_{x})$ using log space.\nNext, we assume that the performance ratio $R_2$ for $P$ satisfies $R_2(f(x,r),\\tilde{y})\\leq r$ for $x\\in I_1$ and $\\tilde{y}\\in SOL_2(f(x,r))$. Note that $m_1(x,g(x,\\tilde{y},r)) = m_1(x,y)$ if $\\tilde{y}=y\\natural M_Q(x)$. If $\\tilde{y}=By\\natural M_Q(x)$, then $m_1(x,g(x,\\tilde{y},r)) = m_1(x,M_Q(x))$. We obtain\n$m_1(x,g(x,\\tilde{y},r))\\geq m_1(x,M_Q(x))$. Write $u= g(x,\\tilde{y},r)$ for simplicity. As for the performance ratio $R_1$ for $Q$, it follows that\n\n", "itemtype": "equation", "pos": 108373, "prevtext": "\nThe last term is further upper-bounded by $\\frac{r m_2^*(x)}{m_2^*(x)+1}\\leq 1+c(r-1)$, where $c=1$, since $m_2(x,y)\\leq r m_2^*(x)$. Overall, we obtain $R_1(x,g(x,y,r))\\leq 1+c(r-1)$. We then conclude that $(f,g,c)$ is indeed an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction from $P_1$ to $P_2$.\n\\end{proof}\n\nHenceforth, we shall discuss several variants of {\\sc Min Path-Weight}. A simple variant is an {\\em undirected-graph version} of {\\sc Min Path-Weight}, denoted by  {\\sc Min UPath-Weight}. It is possible to demonstrate that {\\sc Min UPath-Weight} is log-space ${n^{O(1)}}$-approximable because, by the result of Reingold  \\cite{Rei08}, using only log space, we not only determine the existence of a certain feasible solution for {\\sc Min UPath-Weight} but also find at least one feasible solution if any. The special case where the weights of all vertices are exactly $1$ is the problem of finding the {\\em shortest $s$-$t$ path}. This problem was discussed in \\cite{Tan07}; nonetheless, it is unknown that {\\sc Min UPath-Weight} belongs to ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\nAs another variant of {\\sc Min Path-Weight}, we consider {\\em forests}.  Cook and McKenzie \\cite{CM87} showed that the $s$-$t$ connectivity problem for  forests is complete for ${\\mathrm{L}}$ under L-uniform NC$^{1}$ many-one reductions. Similarly, when all admissible input graphs of {\\sc Min UPath-Weight} are restricted to be forests, we call the corresponding problem {\\sc Min Forest-Path-Weight}. As shown in the following proposition, {\\sc Min Forest-Path-Weight} turns out to be one of the most difficult problems in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nIt is of importance that, unlike ${\\mathrm{NLO}}$, the class ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ does possess complete problems.\n\n\\begin{proposition}\\label{forest-path-weight}\n$\\text{\\sc Min Forest-Path-Weight}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\\end{proposition}\n\nTo simplify the proof of Proposition \\ref{forest-path-weight}, we first give a useful lemma that helps us pay central attention to optimization problems of particular form.\nHere, we say that an optimization problem $P=(I,SOL,m,goal)$ {\\em admits unique solutions} if $|SOL(x)|\\leq1$ holds for all $x\\in I$.\n\n\\begin{lemma}\\label{LO-simple-form}\nFor any maximization problem $Q\\in {\\mathrm{LO}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$), there are another maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$) and a log-space deterministic Turing machine $M_P$ such that, for any $x\\in I$, (i) $Q{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}P$, (ii) $m(x,z)=m(x,M_P(x))$ for all $z\\in SOL(x)$, and (iii) $P$ admits unique solutions. The same statement holds for minimization problems.\n\\end{lemma}\n\n\\begin{proof}\nLet $Q=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Let $M_Q$ be a log-space deterministic Turing machine producing optimal solutions of $Q$. We then define $P=(I_2,SOL_2,m_2,\\text{\\sc max})$ as follows. First, we set $I_2=I_1$ and $SOL_2(x) =\\{M_P(x)\\}$ if $M_P(x))\\neq\\bot$, and $SOL_2(x)={\\mathrm{\\O}}$ otherwise.\nFrom this definition follows $|SOL_2(x)|\\leq1$ for all $x\\in I_2$. Moreover, we define $m_2$ by setting $m_2(x,z)=m_1(x,M_Q(x))$ for any $(x,z)x\\in I_2\\circ SOL_2$. Here, we set $M_P$ to be the same as $M_Q$. Obviously, $m_2(x,M_P(x))=m_2^*(x)$ holds if $SOL_1(x)\\neq{\\mathrm{\\O}}$ since $M_Q(x)\\in SOL_1^*(x)$.\n\nFor the desired ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$, we define $c=1$, $f(x,r)=x$, and $g(x,y,r)=y$. Clearly, $f,g\\in{\\mathrm{FAC}^{ {0} }}$. Consider the performance ratio $R_1$ and $R_2$ for $Q$ and $P$, respectively, and assume that $R_2(f(x,r),y)\\leq r$ for any $y\\in SOL_2(f(x,r))$ and $r\\in{\\mathbb{Q}}^{\\geq1}$. This assumption yields $y=M_P(x)$. Hence, $R_1(x,g(x,y,r)) = R_1(x,y) = R_1(x,M_Q(x)) =1\\leq r$. Therefore, $(f,g,c)$ reduces $Q$ to $P$.\n\\end{proof}\n\nLet us begin the proof of Proposition \\ref{forest-path-weight}.\n\n\\begin{proofof}{Proposition \\ref{forest-path-weight}}\n{\\sc Min Forest-Path-Weight} is assumed to have the form $(I_0,SOL_0,m_0,\\text{\\sc min})$. The membership relation $\\text{\\sc Min Forest-Path-Weight}\\in {\\mathrm{LO}}_{{\\mathrm{NLO}}}$ essentially comes from a simple fact that, by the forest property of a given graph $G$, two nodes\n$s$ and $t$ are connected in $G$ if and only if a {\\em unique} path exists  between them. We can search such a unique path by starting from $s$ and following recursively adjacent edges to next nodes until either no more edges remain unsearched or $t$ is found. At the same time, we progressively write down the weight, in binary, of each node along this found path.\nThe recursive part of this procedure works as follows. Let $u$ be the currently visiting node. We then pick each neighbor, say, $v$ and check if there is a path between $v$ and $t$ in a graph obtained from $G$ by deleting the edge $(u,v)$. This procedure needs no more than log space.\n\nLet $P=(I,SOL,m,\\text{\\sc min})$ be any minimization problem in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. We assume that $P$ satisfies Conditions (ii)--(iii) of Lemma \\ref{LO-simple-form}.  Our goal is to show that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to {\\sc Min Forest-Path-Weight} via a suitably constructed ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$.\nChoose a log-space deterministic Turing machine $M_1$ that produces optimal solutions of $P$. For convenience, we set $b(x)$ to be $m(x,M_1(x))$ for any instance $x\\in (I\\circ SOL)^{\\exists}$.\nSince $m\\in{\\mathrm{auxFL}}$, there is a log-space auxiliary Turing machine $M_m$ computing $m$. By combining $M_1$ and $M_m$ properly, we can design another log-space deterministic Turing machine, say, $M_2$ that computes $b$ with no auxiliary tape.\n\nTo make all final partial configurations unique, we want to force $M_2$ to erase all symbols on all tapes just before entering a halting state. To avoid the same partial configurations to be reached along a single computation, we additionally equip an {\\em internal clock} to $M_2$.\n\nLet us consider partial configurations of $M_2$. Note that $M_2$ is deterministic and the internal clock marks all partial configurations of $M_2$ on $x$ with different time stamps.\nNote also that each symbol of the string $M_1(x)$ appears as a symbol read from the auxiliary input tape encoded into certain partial configurations of $M_1$. Hence, if we have a valid series $y$ of partial configurations associated with an accepting computation path of $M_2$ on $x$, then we can recover the string $M_1(x)$ correctly. Notationally,  $\\eta(y)$ denotes this unique string obtained from a valid series $y$ of partial configurations.\n\nTake a configuration graph $G^{M_2}_{x} = (V,E)$ from $M_2$. Note that there is at most one correct computation path of $M_2$. We set $s$ to be the initial partial configuration of $M_2$ on $x$ and set $t$ be a unique accepting partial configuration of $M_2$ on $x$.\nThe resulted graph forms an {\\em acyclic undirected graph}, namely a forest,  because, otherwise, there are two accepting computation paths on the same input $x$. Given any partial configuration $v\\in V$, we define $w(v)$ to be one bit written down newly on the output tape in this partial configuration $v$.\nFor the desired reduction, we define $c=1$, $f(x,r)={\\langle {G,s,t,w} \\rangle}$, and $g(x,y,r)=\\eta(y)$ for any $y\\in SOL_0(f(x,r))$.\nWe obtain $f,g\\in{\\mathrm{FNC}^{ {1} }}$. It follows that $m_0(f(x,r),y)=m(x,\\eta(y))$ for any $x\\in I$ and $y\\in SOL_0(f(x,r))$. In particular, $g(x,y,r)$ is a minimal solution of $x$ if and only if $y$ is a minimal solution of $f(x,r)$. Thus, $(f,g,c)$ ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduces $P$ to {\\sc Min Forest-Path-Weight}.\n\nNext, we consider any maximization problem $P$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Since $P$ also satisfies Condition (ii)--(iii) of Lemma \\ref{LO-simple-form}, the above argument also works for this $P$ and thus establishes the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducibility of $P$ to {\\sc Min Forest-Path-Weight}.\n\\end{proofof}\n\nAs other variants of {\\sc Min Path-Weight}, Nickelsen and Tantau \\cite{NT05}  studied {\\em series-parallel graphs} and {\\em tournaments}.\n\n\n\\subsection{Complete Problems Concerning Finite Automata}\\label{sec:approximation-class}\n\nWe shall leave graph problems behind and look into problems associated with   finite automata. \\`{A}lvarez and Jenner \\cite{AJ93} and later Tantau \\cite{Tan07} discussed an intimate relationship between accepting computations of nondeterministic finite automata and log-space search procedures for optimal solutions.\nThose problems are also closely related to {\\em maximal word problems (or functions)} for fixed underlying machines.\nAllender, Bruschi, and Pighizzini \\cite{ABP93}, for instance, discussed the maximal word problems of various types of auxiliary pushdown automata. Within our framework of NLO problems, Tantau \\cite{Tan07} presented a maximization problem finding the maximal input strings accepted by nondeterministic finite automata and demonstrated that this problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{MaxNL}}$.\nHere, we shall show that a restricted version of this problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\nA {\\em one-way one-head nondeterministic finite automaton with $\\lambda$-moves} (or a {\\em $\\lambda$-1nfa}, in short) $M$ is a tuple $(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ working with the input alphabet $\\{0,1\\}$ and a transition function $\\delta: (Q-F)\\times\\{0,1,\\lambda\\}\\to{\\cal P}(Q)$, where $q_0\\in Q$, and $F\\subseteq Q$. Initially, an input $x\\in\\{0,1\\}^*$ is written on an input tape, surrounded by two endmarkers ${|}\\!\\!\\mathrm{c}$ (left) and ${\\$}$ (tight).  If $M$ makes a $\\lambda$-move simply by applying $p\\in\\delta(q,\\lambda)$, then $M$'s read-only tape head stays still; otherwise, the tape head moves to the next right cell.\nA {\\em configuration} of $M$ is a pair $(q,\\sigma)$ of current inner state $q$ and scanning symbol $\\sigma$. An {\\em accepting computation path} $p_{M,x}$ of $M$ on input $x$ is a series of configurations starting with an initial configuration $(q_0,{|}\\!\\!\\mathrm{c})$ and ending with a final configuration $(q_f,{\\$})$ with $q_f\\in F$ and, for any consecutive two elements $(q_i,\\sigma_i)$ and $(q_{i+1},\\sigma_{i+1})$ in $p_{M,x}$, $q_{i+1}$ is obtained in a single step from $(q_i,\\sigma_i)$ by applying a transition of the form $q_{i+1}\\in \\delta(q_i,\\sigma_i)$ with $\\sigma_i\\in\\{0,1,\\lambda\\}$, where $\\sigma_1\\sigma_2\\cdots\\sigma_k$ is a partition of the input string ${|}\\!\\!\\mathrm{c} x{\\$}$.\nIf $M$ enters a certain final state in $F$ along a certain accepting computation path, then $M$ is said to {\\em accept} $x$; otherwise, $M$ {\\em rejects} $x$.\nAssociated with such $\\lambda$-1nfa's, we consider the following optimization problem.\nFor succinctness, we hereafter express a transition ``$p\\in \\delta(q,\\sigma)$'' as a triplet $(q,\\sigma,p)$.\n\n{\\medskip}\n{\\sc Maximum Fixed-Length $\\lambda$-Nondeterministic Finite Automata Problem} ({\\sc Max FL-$\\lambda$-NFA}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a $\\lambda$-1nfa $M=(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ and a string $0^n$ for a length parameter $n$, provided that $0^n\\in L(M)$.\n\n\\item {\\sc Solution:} an accepting computation path of $M$ of length at most $|Q|$ on a certain input $y$ of length exactly $n$.\n\n\\item {\\sc Measure:} an integer $rep(1y)$.\n\\end{itemize}\n\nIn the above definition, if we remove the requirement ``$0^n\\in L(M)$'' and we allow $y$ to have any length up to $n$, then we obtain {\\sc Max $\\lambda$-NFA}, which is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{MaxNL}}$  \\cite{Tan07}.\n\n\\begin{proposition}\\label{Mix-2Path-Weight}\n$\\text{\\sc Max FL-$\\lambda$-NFA}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\\end{proposition}\n\nBefore proving this proposition, we show a\nuseful supporting lemma. The lemma helps us concentrate only on optimization problems in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ that have a certain simple structure.\n\n\\begin{lemma}\\label{NLO-to-APXL}\nFor any maximization problem $Q$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$), there exist another maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ (resp., ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$)  and a log-space deterministic Turing machine $M_P$ such that,  for all $x\\in I$, (i) $Q{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}} P$, (ii) $\\max_{y\\in SOL(x)}\\{m(x,y)\\}\\leq 2 \\min_{y\\in SOL(x)}\\{m(x,y)\\}$ holds for all $x\\in I$, (iii) there exists a function $b\\in{\\mathrm{FL}}$ such that  $m(x,z)\\geq 2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$  for all $z\\in SOL(x)$, and (iv) for any $x\\in I$ with $SOL(x)\\neq{\\mathrm{\\O}}$,  $m(x,M_P(x))=2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$.\nA similar statement holds for minimization problems; however, we need to replace (iii) by (iii') $m(x,z)\\leq 2^{{\\lfloor {\\log{b(x)}} \\rfloor}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $Q=(I_1,SOL_1,m_1,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. In what follows, we shall modify $Q$ to obtain the desired problem  $P=(I_2,SOL_2,m_2,\\text{\\sc max})$.\n\nTake a polynomial $p$ such that, for any $(x,y)\\in I_1\\circ SOL_1$, $|y|\\leq p(|x|)$ holds.\nSince $Q\\in{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, take a log-space deterministic Turing machine $M_P$ producing $\\beta$-approximate solutions of $P$ for a certain constant $\\beta>1$. We obtain $m_1(x,M_Q(x))\\leq m_1^*(x)\\leq \\beta m_1(x,M_Q(x))$ for all $x\\in I_1$ with $SOL_1(x)\\neq{\\mathrm{\\O}}$. For such an $x$, we further set $b_0(x) = 2^{{\\lfloor {\\log{m_1(x,M_Q(x))}} \\rfloor}+1}$. Note that $b_0\\in{\\mathrm{FL}}$\nsince $m_1\\in{\\mathrm{auxFL}}$.\nSince $b_0(x)\\leq b(x)\\leq 2b_0(x)$, it follows that $b_0(x)\\leq m_1^*(x)\\leq 2\\beta b_0(x)$.\nLet us consider a configuration graph $G^{M_Q}_{x}$ of $M_Q$ on $x$.  Note that we can compute a string $M_Q(x)$ from $(x,G^{M_Q}_{x})$ using only log space.\nFor later use, we set $\\alpha= 2^{{\\lfloor {\\log{2\\beta}} \\rfloor}+1}+1$, which implies  $b_0(x)\\leq m_1^*(x)\\leq \\alpha b_0(x)$.\n\nHere, we define the desired problem $P = (I_2,SOL_2,m_2,\\text{\\sc max})$. For convenience, set $\\Delta_{x} = (\\alpha-2)b_0(x)$. Let $I_2=\\{x\\natural G^{M_Q}_{x}\\mid x\\in I_1\\}$. Given $\\tilde{x}=x\\natural G^{M_Q}_{x} \\in I_2$, $SOL_2(\\tilde{x})$ contains the following strings: (i) $y\\natural M_Q(x)$ for all $y\\in SOL_1(x)$ satisfying $m_1(x,y)\\geq b_0(x)$ and (ii) $By\\natural M_Q(x)$ for every $y\\in SOL_1(x)$ satisfying $m_1(x,y)<b_0(x)$, where $B$ is a special symbol. Obviously, $I_2\\circ SOL_2$ is a member of ${\\mathrm{auxL}}$.\n\nFor any $\\tilde{y}\\in SOL_2(\\tilde{x})$, if $\\tilde{y}= y\\natural M_Q(x)$, then we set $m_2(\\tilde{x},\\tilde{y})= m_1(x,y) +\\Delta_{x}$; if $\\tilde{y}=By\\natural M_P(x)$, then we set $m_2(\\tilde{x},\\tilde{y}) = m_1(x,y)\\cdot 2^{t} + \\Delta_{x}$, where $t=|bin(b_0(x))|-|bin(m_1(x,y))|$. It follows that $m_2(\\tilde{x},\\tilde{y})\\geq b_0(x)+\\Delta_{x}$ for all $(\\tilde{x},\\tilde{y})\\in I_2\\circ SOL_2$. Note that $m_2(\\tilde{x},M_Q(x)\\natural M_Q(x))$ equals $b_0(x)+\\Delta_{x}$, which is $2^{{\\lfloor {\\log2\\beta} \\rfloor}+{\\lfloor {\\log{b_0(x)}} \\rfloor}+2}$. Choose a function $b$ so that ${\\lfloor {\\log{b(x)}} \\rfloor} = {\\lfloor {\\log{2\\beta}} \\rfloor}+{\\lfloor {\\log{b_0(x)}} \\rfloor}+2$\nfor all $x\\in (I_1\\circ SOL_1)^{\\exists}$. Clearly, $m_2\\in{\\mathrm{auxFL}}$ holds.\n\nThe desired $M_P(\\tilde{x})$ outputs $M_Q(x)\\natural M_Q(x)$ for any $\\tilde{x} \\in I_2$ if $SOL_2(\\tilde{x})\\neq{\\mathrm{\\O}}$, and it outputs $\\bot$ otherwise. Let $\\tilde{x}\\in (I_2\\circ SOL_2)^{\\exists}$.\nIt follows that $m_2^*(\\tilde{x}) = \\max_{\\tilde{y}\\in SOL_2(x)}\\{m_2(\\tilde{x},\\tilde{y})\\} = \\max_{y\\in SOL_1(x)}\\{m_1(x,y)+\\Delta_{x}\\}=m_1^*(x)+\\Delta_{x}$ since $m_1^*(x)\\geq b_0(x)$. Moreover, we obtain  $m_1(x,g(x,\\tilde{y},r)) \\geq b_0(x)$. It follows that\n\n", "index": 5, "text": "\\begin{equation}\\label{eqn:max-min-ratio}\n\\frac{\\max_{y\\in SOL_2(\\tilde{x})}\\{m_2(\\tilde{x},y)\\}}{\\min_{y\\in SOL_2(\\tilde{x})}\\{m_2(\\tilde{x},y)\\}} =\n\\frac{m_1^*(x) + \\Delta_{x}}{b_0(x) + \\Delta_{x}} \\leq\n\\frac{\\alpha b_0(x) + (\\alpha-2)b_0(x)}{b_0(x) + (\\alpha-2)b_0(x)} = 2.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\max_{y\\in SOL_{2}(\\tilde{x})}\\{m_{2}(\\tilde{x},y)\\}}{\\min_{y\\in SOL_{2}%&#10;(\\tilde{x})}\\{m_{2}(\\tilde{x},y)\\}}=\\frac{m_{1}^{*}(x)+\\Delta_{x}}{b_{0}(x)+%&#10;\\Delta_{x}}\\leq\\frac{\\alpha b_{0}(x)+(\\alpha-2)b_{0}(x)}{b_{0}(x)+(\\alpha-2)b_%&#10;{0}(x)}=2.\" display=\"block\"><mrow><mrow><mfrac><mrow><msub><mi>max</mi><mrow><mi>y</mi><mo>\u2208</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><msub><mi>L</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><msub><mi>min</mi><mrow><mi>y</mi><mo>\u2208</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><msub><mi>L</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>x</mi></msub></mrow><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>x</mi></msub></mrow></mfrac><mo>\u2264</mo><mfrac><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mi>b</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>b</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>b</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><mn>2</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\nThe last term is further calculated as\n\n", "itemtype": "equation", "pos": 109819, "prevtext": "\n\nWe remark that, by the construction of $P$ from $Q$, if $Q$ is polynomially bounded, then so is $P$. We wish to define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,c)$ from $Q$ to $P$. First, we set $c= \\alpha-1$. We then define $f(x,r)=x\\natural G^{M_Q}_{x}$ ($=\\tilde{x}$),  and $g(x,\\tilde{y},r) = y$ if $\\tilde{y}$ is of the form $y\\natural z$, $g(x,\\tilde{y},r)= z$ if $\\tilde{y}$ is of the form $By\\natural z$, and $g(x,\\tilde{y},r)=x$ otherwise.\nIf $x\\in I_1$, then $f(x,r)\\in I_2$ since $f(x,r)=x\\natural G^{M_Q}_{x}$. If $y\\in SOL_1(x)$, then $y\\natural M_P(x)\\in SOL_2(f(x,r))$ because $M_Q(x)$ can be constructed from $(x,G^{M_Q}_{x})$ using log space.\nNext, we assume that the performance ratio $R_2$ for $P$ satisfies $R_2(f(x,r),\\tilde{y})\\leq r$ for $x\\in I_1$ and $\\tilde{y}\\in SOL_2(f(x,r))$. Note that $m_1(x,g(x,\\tilde{y},r)) = m_1(x,y)$ if $\\tilde{y}=y\\natural M_Q(x)$. If $\\tilde{y}=By\\natural M_Q(x)$, then $m_1(x,g(x,\\tilde{y},r)) = m_1(x,M_Q(x))$. We obtain\n$m_1(x,g(x,\\tilde{y},r))\\geq m_1(x,M_Q(x))$. Write $u= g(x,\\tilde{y},r)$ for simplicity. As for the performance ratio $R_1$ for $Q$, it follows that\n\n", "index": 7, "text": "\\begin{equation}\\label{eqn:R_1-alpha}\nR_1(x,g(x,\\tilde{y},r)) -1 = \\frac{m_1^*(x)-m_1(x,u)}{m_1(x,u)} \\leq\n (\\alpha-1)\\cdot \\frac{m_2^*(\\tilde{x})-m_2(\\tilde{x},\\tilde{y})}{(\\alpha-1)m_1(x,u)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"R_{1}(x,g(x,\\tilde{y},r))-1=\\frac{m_{1}^{*}(x)-m_{1}(x,u)}{m_{1}(x,u)}\\leq(%&#10;\\alpha-1)\\cdot\\frac{m_{2}^{*}(\\tilde{x})-m_{2}(\\tilde{x},\\tilde{y})}{(\\alpha-1%&#10;)m_{1}(x,u)}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo>=</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\nThe last term equals $c(R_2(f(x,r),\\tilde{y})-1)$. Since $R_2(f(x,r),\\tilde{y})\\leq r$, it follows that $R_1(x,g(x,\\tilde{y},r)) \\leq 1+ c(r-1)$. Therefore, $(f,g,c)$ reduces $Q$ to $P$.\n\\end{proof}\n\nLet us begin the proof of Proposition \\ref{Mix-2Path-Weight}.\n\n{\\smallskip}\n\\begin{proofof}{Proposition \\ref{Mix-2Path-Weight}}\nFirst, we shall  argue that {\\sc Max FL-$\\lambda$-NFA} belongs to   ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. For simplicity, we set $\\text{\\sc Max FL-$\\lambda$-NFA}$ as  $(I_0,SOL_0,m_0,\\text{\\sc min})$. Let $x=(M,0^n)$ be any instance in $I_0$ with $M=(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$, where $M$ is demanded to accept $0^n$. Since we feed only inputs $y$ of length $n$, the value $rep(1y)$ varies from $2^n$ to $2^{n+1}-1$. It thus follows that, for any $u\\in SOL_0(x)$,\n$m_0^*(x)/2 \\leq m_0(x,u) \\leq m_0^*(x)$. Consider the following algorithm $N$: take $x$ as input and simulate $M$ on input $0^n$ (also by checking the size of $0^n$). This algorithm requires only log space.\nWe then obtain $m_0^*(x)/2\\leq m_0(x,N(x))\\leq m_0^*(x)$ for any $x\\in I_0$. These bounds imply that $\\text{\\sc Max FL-$\\lambda$-NFA}$ is a member of  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\nNext, we shall show the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max FL-$\\lambda$-NFA}. Let $P=(I,SOL,m,\\text{\\sc max})$ be any problem in ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. Without loss of generality, we assume that $P$ satisfies Conditions (ii)--(iv) of Lemma \\ref{NLO-to-APXL}, and thus $P$ admits a $2$-approximate algorithm.\nOur goal is to show that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reducible to {\\sc Max FL-$\\lambda$-NFA} via a suitable reduction $(f,g,c)$. Let $M_m$ be a log-space auxiliary Turing machine computing $m$ and let $M_P$ denote a log-space $2$-approximate algorithm for $P$. There is a function $b\\in{\\mathrm{FL}}$ such that $b(x)\\leq m^*(x)\\leq 2b(x)$  and $m(x,y)\\geq b(x)$  for all $x\\in (I\\circ SOL)^{\\exists}$, where $b(x)$ is of the form $2^{{\\lfloor {\\log{C(x)}} \\rfloor}}$ for appropriate function $C\\in{\\mathrm{FL}}$.\nMoreover, we assume that, for each $x\\in (I\\circ SOL)^{\\exists}$, there is a solution $y\\in SOL(x)$ such that $m(x,y)=b(x)$.\n\nAs in the proof of Proposition \\ref{Min-Path-complete}, we consider partial configurations of $M_m$.\nWe define $Q$ to be the set of all possible partial configurations of $M_m$.\nFix $x\\in (I\\circ SOL)^{\\exists}$ arbitrarily and let $n$ be the size of binary string $bin(b(x))$.\n\nWe want to define a $\\lambda$-1nfa $N$, which ``mimics'' a computation of $M_m$. $N$'s inner states are partial configurations of $M_m$. An input to $N$ is $bin(m(x,y))^{(-)}$ for a certain auxiliary input $y\\in SOL(x)$. A move of $N$ is described as follows. Given a string $u$ and a number $k\\in{\\mathbb{N}}^{+}$, $u_k$ denotes the $k$th symbol of $u$.\nOn such an input, $N$ nondeterministically guesses a string $y\\in\\{0,1\\}^n$.\nBy reading an input symbol $\\tau'\\in\\{0,1\\}$ from $bin(m(x,y))$ one by one from left to right, $N$ changes an inner state $v=(q,x,j,\\xi,u,k,\\tau)$ to another inner state  $v'=(p,x,j+d_1,\\xi',w,k+d_2,\\tau')$ in a single step by applying $M_m$'s transition ``$(p,\\tau',w_{k},d_1,d_2)\\in \\delta(q,x_{j},\\xi,u_{k})$,'' where $\\xi'$ is the next bit of $\\xi$ in $y$. More precisely, we define $N$'s transition as  ${\\langle {p,x,j+d_1,\\xi',w,k+d_2,\\tau'} \\rangle}\\in \\delta_{N}({\\langle {q,x,j,\\xi,u,k,\\tau} \\rangle},\\tau')$ iff $(p,\\tau',w_k,d_1,d_2)\\in\\delta(q,x_j,\\xi,u_k)$, where $w$ is obtained from $u$ by changing $u_k$ to $w_k$.\n\nWe modify $N$ so that it simultaneously checks whether its input is of the form $0^n$. If so, $N$ enters a designated accepting state. Hence, $0^n\\in L(N)$.\n\nHere, let us define the desired reduction $(f,g,c)$. First, we set $f(x,r)={\\langle {N} \\rangle}$. Given an accepting computation path $e$ in $SOL_0(f(x,r))$, we define $g(x,e,r)$ to be an auxiliary input $y_{(e)}$ fed into $M_m$, which can be obtained from $e$. It follows that $y_{(e)}\\in SOL(x)$ and that $m(x,y_{(e)})=m_0(f(x,r),e)$.\nLet $r\\in{\\mathbb{Q}}^{\\geq1}$. The performance ratio $R_P$ for $P$ satisfies that $R_P(x,g(x,e,r)) = \\frac{m^*(x)}{m(x,g(x,e,r))} = \\frac{m_0^*(f(x,r))}{m_0(f(x,r),e)} = R_0(f(x,r),e)$.\n\nTherefore, the reduction $(f,g,c)$ ensures $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Max FL-$\\lambda$-NFA}$.\n\\end{proofof}\n\n\n\nAs a simple variant of {\\sc Max $\\lambda$-NFA}, we shall consider  {\\em one-way one-head deterministic finite automata with $\\lambda$-moves} (or $\\lambda$-1dfa's, in short). A $\\lambda$-1dfa $M$ is a tuple $(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ working with input alphabet $\\{0,1\\}$ and $\\delta:(Q_{\\lambda}\\times\\{\\lambda\\})\\cup(Q_{+}\\times \\{0,1,\\natural\\})\\to Q$, where $Q=Q_{\\lambda}\\cup Q_{+}$, $Q_{\\lambda}\\cap Q_{+}={\\mathrm{\\O}}$, $q_0\\in Q$, and $F\\subseteq Q$. This $M$ must satisfy the following condition: if $M$ is in state $q\\in Q_{\\lambda}$, then $M$'s read-only tape head stays still; otherwise, the tape head moves to the next right cell.\nHere, each transition ``$\\delta(q,\\sigma)=p$'' is succinctly expressed as  $(q,\\sigma,p)$.\n\n{\\medskip}\n{\\sc Maximum Input-Restricted $\\lambda$-Deterministic Finite Automata Problem} ({\\sc Max IR-$\\lambda$-DFA}):\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a $\\lambda$-1dfa $M =(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ and a list $Y=(y_1,y_2,\\ldots,y_k)$ of strings over $\\{0,1\\}$, where $\\delta$ is given as a list of (partial) transitions of the form $(q,\\sigma,p)$.\n\n\\item {\\sc Solution:} an accepting computation path  of $M$ of length at most $|Q|$ on a certain input $y\\in Y$, which is surrounded by ${|}\\!\\!\\mathrm{c}$ and ${\\$}$.\n\n\\item {\\sc Measure:} an integer $rep(1y)$.\n\\end{itemize}\n\n\\begin{proposition}\\label{MAX-IR-lambda-DFA}\n{\\sc Max IR-$\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\\end{proposition}\n\nIf we  take $(M,1^n)$ as an instance and demand $y$ to have length at most $n$, then we obtain another problem, {\\sc Max $\\lambda$-DFA}. It is not clear that {\\sc Max $\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for either ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ or ${\\mathrm{MaxNL}}$.\n\n\\begin{proofof}{Proposition \\ref{MAX-IR-lambda-DFA}}\nFor convenience, let {\\sc Max IR-$\\lambda$-DFA} have the form $(I_0,SOL_0,m_0,\\text{\\sc max})$.\nLet us claim that  {\\sc Max IR-$\\lambda$-DFA} is in ${\\mathrm{NLO}}$.\nNote that it is easy to check using log space whether a given instance $M =(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ is indeed a $\\lambda$-1dfa; thus, $I_0\\in{\\mathrm{L}}$ follows.\nTo see $I_0\\circ SOL_0\\in{\\mathrm{auxL}}$, on input $(M,Y)$ together with a sequence $p = (p_1,p_2,\\ldots,p_m)$ of configurations of $M$ as an auxiliary input, we can check using only log space whether $p$ is indeed an accepting computation path of $M$ (by checking that $p_1=(q_0,{|}\\!\\!\\mathrm{c})$, $p_m=(q_f,{\\$})$, $(p_i,\\sigma,p_{i+1})$ is a transition for each $i\\in[m]$ with $m\\leq |Q|$ for a certain $\\sigma\\in\\{0,1,\\lambda\\}$, and a series of such symbols matches one of $y$'s in $Y$. As for $m_0\\in{\\mathrm{auxFL}}$, it is possible to retrieve an input $y$ from $p$ and output $1y$ using log space if $y$ is in $Y$.\n\nNext, we shall show that {\\sc Max IR-$\\lambda$-DFA} belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nRecursively, we pick each $y$ in $Y$ in the lexicographic order and simulate a given $\\lambda$-1dfa $M$ on this input $y$ to check if $M$ accepts $y$ within $|Q|$ steps. This process determines the maximal accepted input $y$ in $Y$.  Finally, we generate an accepting computation path of $M$ on this $y$. This whole procedure requires log space.\nHence, {\\sc Max IR-$\\lambda$-DFA} can be solved using only log space.\n\nHereafter, we shall  show that {\\sc Max IR-$\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nLet us consider  any maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. We assume that $P$ satisfies Conditions (ii)--(iii) of Lemma \\ref{LO-simple-form}. Since optimal solutions and their objective values are both computed using log space, for the purpose of defining $g$, we can build a log-space deterministic Turing machine $M$ that, on input $x$, records each symbol $y_i$ of a solution $y=y_1y_2\\cdots y_k\\in SOL^*(x)$ in one cell of one work tape (after erasing the previous symbol $y_{i-1}$ if any) and produces $bin(m(x,y))^{(-)}$ on an output tape (by removing the first bit ``$1$'' from $bin(m(x,y))$).\n\nWe shall  construct a pair $(f,g)$ of functions that ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduces $P$ to {\\sc Max IR-$\\lambda$-DFA}.\nLet $x$ be any instance in $I$. We construct a $\\lambda$-1dfa $N_x = (Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ as follows.\nWe view each configuration of $M$ (including the content of its output symbol  and an input symbol) as an inner state of $N_x$. Let $Q$ be a set of all such configurations. Note that $|Q|\\geq |x|$. We roughly treat an output tape of $M$ as an input tape of $N_x$. More precisely, when $M$ writes a symbol $\\sigma\\in\\{0,1\\}$ on its output tape, $N_x$ reads $\\sigma$ on the input tape. When $M$ does not write any non-blank output symbol, $N_x$ makes its associated  $\\lambda$-move.\nFinally, we set $f(x,r) ={\\langle {N_x} \\rangle}$.  For $N_x$, let $p = (p_0,p_1,\\ldots,p_m)$ be an accepting computation path of $N_x$ of length $\\leq|Q|$.\nWe also define $g(x,p,r)$ to be an input $y_p$ to $N_x$ that is recovered from $p$ as stated above.\nNote that $m_0(f(x,r),p) = rep(1bin(m(x,y_p)^{(-)})) = m(x,y_p)$.\nConcerning the performance ratio $R_P$ and $R_0$ for $P$ and {\\sc Max IP-$\\lambda$-DFA}, respectively, it follows that $R_{P}(x,g(x,p,r)) = \\frac{m^*(x)}{m(x,y_p)} = \\frac{m_0^*(f(x,r))}{m_0(f(x,r),p)} = R_0(f(x),p)$. Therefore, $(f,g,1)$ is an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction from $P$ to {\\sc Max IR-$\\lambda$-DFA}, as requested. The case where $P$ is a minimization problem can be similarly treated.\n\\end{proofof}\n\n\n\n\\section{Polynomially-Bounded Complete Problems}\\label{sec:PBP}\n\nLet us recall from Section \\ref{sec:comb-OPs} that an optimization problem is said to be  {\\em polynomially bounded} exactly when its measure function is polynomially bounded. Recall also the notation ${\\mathrm{PBO}}$, which expresses the set of all polynomially-bounded optimization problems.\nFor many low-complexity optimization/approximation classes below ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$, polynomially-bounded optimization problems play a quite special role. With respect to log-space computation, it appears more natural to deal with polynomially-bounded optimization problems than polynomially-unbounded ones because, through Section \\ref{sec:completeness}, we have been unable to present any complete problem in ${\\mathrm{NLO}}$ and ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ but, as we shall see shortly, we can exhibit complete problems in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ and\n${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\nIn the subsequent subsections, we shall present polynomially-bounded optimization problems, which turn out to be complete for various NL optimization and approximation classes.\n\n\n\\subsection{Maximization Versus Minimization}\\label{sec:key-lemma}\n\nAssume that we wish to show the completeness of a certain optimization problem $P$ for a target optimization/approximation class ${\\cal D}$. Since ${\\cal D}$ may be composed of maximization problems as well as minimization problems, it is necessary to construct desirable reductions to $P$ from all maximization problems in ${\\cal D}$ and also from all minimization problems in ${\\cal D}$.\nRegarding ${\\mathrm{NPO}}$ problems, it is well-known that every minimization problem $Q$ in ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ has its maximization counterpart $Q'$ in ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ whose complexity is at least as hard as $Q$ (see, e.g., \\cite[Theorem 8.7]{ACG+03} for the proof).\n\nA similar statement holds for polynomially-bounded problems in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. This is because a log-space auxiliary Turing machine that computes a polynomially-bounded measure function can freely manipulate the outcome of the function using its space-bounded work tapes before writing it down onto an output tape.\n\n\\begin{lemma}\\label{min-reduces-max}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item For any minimization (resp., maximization) problem $P$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$, there  exists a maximization (resp., minimization) problem $Q$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{APXL}},{\\mathrm{APXNC}^{ {1} }}\\}$. For every minimization (resp., maximization) problem $P$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, there exists a maximization (resp., minimization) problem $Q$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{LO}},{\\mathrm{NC}^{ {1} }\\mathrm{O}}\\}$. For any minimization (resp., maximization) problem $P$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, there  exists a maximization (resp., minimization) problem $Q$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\n(1) This proof is similar in essence to that of Lemma \\ref{reduction-MaxNL-MinNL}.\nGiven an arbitrary problem $P_1=(I_1,SOL_1,m_1,\\text{\\sc min})$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$, we aim at constructing a maximization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc max})$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ to which $P_1$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible.\nSince $P_1\\in{\\mathrm{PBO}}$, there is a polynomial $p$ satisfying $m_1(x,y)\\leq p(|x|)$ for every $(x,y)\\in I_1\\circ SOL_1$. Let $I_2=I_1$, $SOL_2=SOL_1$, and $m_2(x,y) = {\\lceil {\\frac{p(|x|)^2}{m_1(x,y)}} \\rceil}$ for every $(x,y)\\in I_2\\circ SOL_2$. Since $m_1$ is polynomially-bounded, we can generate the entire value $m_1(x,y)$ on one of log-space work tapes and manipulate it freely as if a normal input. (If $m_2$ is not polynomially bounded, then there is no guarantee that a log-space auxiliary Turing machines  can compute $m_2$.) Since ``division'' can be implemented on TC$^{0}$ circuit \\cite{Hes01}\n(and thus, by a log-space machine),  $m_2(x,y)$ can be generated on a log-space work tape. Here, we define $f(x,r)=x$ and $g(x,y,r)=y$ so that $f$ and $g$ belong to ${\\mathrm{FAC}^{ {0} }}$. For the performance ratio $R_2$ for $P_2$, assume that $R_2(f(x,r),y)\\leq r$ for any $r\\geq1$; that is, $m_2^*(x)\\leq m_2(x,y)\\leq r m_2^*(x)$. The ratio $R_1(x,g(x,y,r)) = \\frac{m_1(x,y)}{m_1^*(x)}$ is thus at most $1+c(r-1)$, where $c=1$. Hence, $(f,g,c)$ reduces $P_1$ to $P_2$.\n\n(2) We shall show only the case of ${\\cal D}={\\mathrm{APXNC}^{ {1} }}$.\nLet $P_1=(I_1,SOL_1,m_1,\\text{\\sc min})$ be any polynomially-bounded minimization problem in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. We want to construct a polynomially-bounded maximization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc max})$, which is in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$ and is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduced from  $P_1$. Take a constant $\\gamma> 1$ and an ${\\mathrm{NC}^{ {1} }}$ $\\gamma$-approximate algorithm $C$ for $P_1$.\nFor convenience, we set $b(x) = m_1(x,C(x))$ for each instance $x\\in (I_1\\circ SOL_1)^{\\exists}$. By the definition of ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, $b$ must belong to ${\\mathrm{FNC}^{ {1} }}$. Note that $m_1^*(x)\\leq b(x)\\leq \\gamma m_1^*(x)$ for all $x\\in (I_1\\circ SOL_1)^{\\exists}$.\n\nWe choose a constant $\\Delta$ so that $\\Delta >\\gamma$ holds. Note that $\\Delta-1>0$ since $\\gamma\\geq1$. Next, we define $I_2=I_1$, $SOL_2(x) = \\{y\\in SOL_1(x)\\mid m_1(x,y)\\leq b(x)\\}$, and $m_2(x,y) = \\Delta b(x) - \\gamma m_1(x,y)$ for any $(x,y)\\in I_2\\circ SOL_2$.\nNotice that  $m_2$ is computed by an appropriate log-space auxiliary Turing machine, and thus $I_2\\circ SOL_2$ is also computed by a certain log-space auxiliary Turing machine. This implies that $P_2$ is an ${\\mathrm{NLO}}$ problem.\n\nWe shall  claim that $P_2\\in{\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. Note that $m^*_2(x) = \\Delta b(x) -\\gamma m^*_1(x)$ for every $x\\in (I_2\\circ SOL_2)^{\\exists}$. Moreover, $m_2(x,C(x)) = (\\Delta-\\gamma)b(x)$ holds. Since $m^*_1(x)\\leq b(x)$, we obtain $(\\Delta-\\gamma)b(x)\\leq m^*_2(x)$, which implies $m_2(x,C(x))\\leq m^*_2(x)$. Since $b\\in{\\mathrm{FNC}^{ {1} }}$, the value $m_2(x,C(x))$ can be computed from $x$ by a certain ${\\mathrm{NC}^{ {1} }}$-circuit.  Since $b(x)\\leq \\gamma m^*_1(x)$, it follows that $m^*_2(x)\\leq (\\Delta-1)b(x)$; thus, we obtain $m^*_2(x) \\leq \\frac{\\Delta-1}{\\Delta-\\gamma}m_2(x,C(x))$. In summary, it holds that $\\frac{\\Delta-\\gamma}{\\Delta-1}m^*_2(x)\\leq m_2(x,C(x))\\leq m^*_2(x)$. Hence, $P_2$ belongs to ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$.\n\nAs for the desired ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction, we define $f(x,r)=x$ and $g(x,y,r)=y$, where $r$ is any number in ${\\mathbb{Q}}^{\\geq1}$. It is obvious that $f$ and $g$ are in ${\\mathrm{FAC}^{ {0} }}$. For any fixed solution $y\\in SOL_2(x)$, we assume that  $R_2(f(x,r),y)\\leq r$; that is, $m_2(x,y)\\leq m_2^*(x)\\leq r m_2(x,y)$. We want to claim that $m_1(x,y)\\leq [1+(\\Delta-1)(r-1)]m_1^*(x)$.\nFirst, we note that\n\n", "itemtype": "equation", "pos": 110067, "prevtext": "\nThe last term is further calculated as\n\n", "index": 9, "text": "\\begin{equation}\\label{eqn:ration-with-c}\nc\\cdot \\frac{m_2^*(\\tilde{x})-m_2(\\tilde{x},\\tilde{y})}{m_1(x,u)+(\\alpha-2)m_1(x,u)}\n\\leq c\\cdot \\frac{m_2^*(\\tilde{x})-m_2(\\tilde{x},\\tilde{y})}{m_1(x,u)+\\Delta_{x}}\n= c\\cdot \\frac{m_2^*(\\tilde{x})-m_2(\\tilde{x},\\tilde{y})}{m_2(\\tilde{x},\\tilde{y})}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"c\\cdot\\frac{m_{2}^{*}(\\tilde{x})-m_{2}(\\tilde{x},\\tilde{y})}{m_{1}(x,u)+(%&#10;\\alpha-2)m_{1}(x,u)}\\leq c\\cdot\\frac{m_{2}^{*}(\\tilde{x})-m_{2}(\\tilde{x},%&#10;\\tilde{y})}{m_{1}(x,u)+\\Delta_{x}}=c\\cdot\\frac{m_{2}^{*}(\\tilde{x})-m_{2}(%&#10;\\tilde{x},\\tilde{y})}{m_{2}(\\tilde{x},\\tilde{y})}.\" display=\"block\"><mrow><mrow><mrow><mi>c</mi><mo>\u22c5</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>\u2264</mo><mrow><mi>c</mi><mo>\u22c5</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>x</mi></msub></mrow></mfrac></mrow><mo>=</mo><mrow><mi>c</mi><mo>\u22c5</mo><mfrac><mrow><mrow><msubsup><mi>m</mi><mn>2</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\nbecause $m_2^*(x)\\leq r m_2(x,y)$ and $m_2^*(x) = \\Delta b(x) - \\gamma m_1^*(x)$. Since $b(x)\\leq \\gamma m_1^*(x)$, we obtain $m_1(x,y)\\leq (1/r)[(r-1)\\Delta+1]m_1^*(x)$. Hence, it follows that $m_1(x,y)\\leq [1+\\theta(r-1)]m_1^*(x)$, where $\\theta= (\\Delta-1)/r$, which is at most $\\Delta-1$ since $r\\geq1$.  This shows that $P_1$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $P_2$.\n\n(3) Consider the case of ${\\cal D}={\\mathrm{LO}}$. The construction of the desired reduction is similar to (2). Assuming that $P_1$ is in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, we want to show that $P_2$ is also in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. This is obtained simply by mapping minimal solutions for $P_1$ to maximal solutions for $P_2$.\n\\end{proof}\n\n\n\\subsection{Completeness of Graph Problems}\\label{sec:graph-problems}\n\nAs our starting point, we recall from Section \\ref{sec:general-complete} a bounded variant of {\\sc Min Path-Weight}, called {\\sc Min BPath-Weight}, which uses the {\\em total path weight} (i.e., $w({\\cal S})= \\sum_{i=1}^{k}w(v_i)$ for ${\\cal S}=(v_1,v_2,\\ldots.v_k)$ with $k\\leq|V|$) as a measure function with an extra condition that $1\\leq w(v)\\leq |V|$ for all $v\\in V$. Earlier, Tantau \\cite[Theorem 5.1]{Tan07} discussed the case when all vertices of a given graph have weight exactly $1$. To verify that {\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}\\cap{\\mathrm{PBO}}$, we need to modify the proof of Proposition \\ref{Min-Path-complete} in the following way.\nFor this purpose, we first modify a given measure function $m$ so that its log-space auxiliary Turing machine $M_m$ produces $m(x,y)$ on one of its work tapes and then copies each bit (including ``$0$'') from the lower bit to the higher bit at each step. We further modify $M_m$ so that its internal clock helps it halt in exactly $p(n)$ steps, for a suitable polynomial $p$. We then\ndefine $w(v)$ to be $b_1 2^{e-1} +1$ if $v$ contains a string $b_1b_2\\cdots b_e$ written  on an output tape of $M_m$ in a target partial final configuration.\nOtherwise, define $w(v)=1$. It follows that $\\sum_{i=1}^{q(n)}w(v_i) = q(n)+ m(x,y)$.\nBy reducing minimization problems to maximization problems by Lemma \\ref{min-reduces-max}(1), we can prove that every minimization problem in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ is also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reducible to {\\sc Min BPath-Weight}. Therefore, we obtain the following completeness result.\n\n\\begin{lemma}\\label{Bpath-complete}\n{\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\n\nIn Section \\ref{sec:general-complete}, we have mostly dealt with optimization problems that are associated with the path weights of graphs. Another natural type of optimization problems is\na problem of searching a path of a directed graph starting at a given source   toward an appropriately chosen vertex whose weight is well-defined and must be maximal. Tantau  \\cite[Theorem 3.2]{Tan07} earlier demonstrated that this maximization problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\nMoreover, its slightly modified version was shown to be ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ \\cite[Theorem 3.7]{Tan07}.\nIn what follows, we shall discuss a similar optimization problem using ``undirected'' graphs with ``total'' weight functions, particularly, in the case where vertex weights are all bounded.\n\nLet us define formally this problem as follows.\n\n{\\medskip}\n{\\sc Maximum Undirected Bounded Vertex Weight Problem} ({\\sc Max UB-Vertex}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} an undirected graph $G=(V,E)$, a source $s\\in V$, and a (vertex) weight function  $w:V\\rightarrow {\\mathbb{N}}^{+}$ satisfying $w(v)\\leq |V|$ for every $v\\in V$.\n\n\\item {\\sc Solution:} a path of $G$ starting at $s$ and ending at a certain vertex $t$ in $V$.\n\n\\item {\\sc Measure:} the weight $w(t)$ of $t$.\n\\end{itemize}\n\nA {\\em directed-graph version} of {\\sc Max UB-Vertex}, called {\\sc Max B-Vertex},\nhas a log-space $n^{O(1)}$-approximate algorithm \\cite{Tan07} but is not known to fall into ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\n\\begin{proposition}\\label{max-bvertex-complete}\n$\\text{\\sc Max UB-Vertex}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{proposition}\n\nEven if the vertex $t$ in the above definition of {\\sc Max UB-Vertex} is restricted to a vertex of degree exactly $1$ as the following proof shows, the obtained problem is still ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n{\\medskip}\n\n\\begin{proof}\nWe begin with setting {\\sc max UB-Vertex} as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Let $G=(V,E)$ be any graph given as an instance in $I_0$. notice that $SOL_0(x)\\neq{\\mathrm{\\O}}$ for all $x\\in I_1$.\nSince the weight of every vertex in $G$ is at most the input size,  $\\text{\\sc Max UB-Vertex}$ is polynomially bounded. Thus, it is not difficult to show that optimal solutions for $\\text{\\sc Max UB-Vertex}$ can be found using log space by making a series of nonadaptive queries to oracle $A=\\{{\\langle {G,s,w,1^k} \\rangle}\\mid \\exists t\\in V\\,[w(t)\\geq k \\wedge \\,\\text{$s$ and $t$ are connected}\\,]\\}$ by incrementing $k$ from $1$ to $|V|$.\nTo see that $A$ is in ${\\mathrm{L}}$, we sequentially pick a different $t\\in V$, check if $s$ and $t$ are connected using Reingold's log-space algorithm for DSTCON, and finally  check if $w(t)\\geq k$. Therefore, {\\sc Max UB-Vertex} is ${\\mathrm{L}}$-solvable; that is, $\\text{\\sc Max UB-Vertex}$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\nConcerning the ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max UB-Vertex}, let us consider a polynomially-bounded maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. The case of minimization problems follows from Lemma \\ref{min-reduces-max}(3).\nBy Lemma \\ref{LO-simple-form}, it is possible to  assume that $P$ satisfies Conditions (ii)--(iii) of the lemma. Take a log-space auxiliary Turing machine $M_m$ that computes $m$. Moreover, we take a log-space deterministic Turing machine $M_P$ that produces maximal solutions for $P$. Define $b(x) = m(x,M_P(x))$ for each instance $x\\in (I\\circ SOL)^{\\exists}$.\nTake a polynomial $p$ such that $b(x)\\leq p(|x|)$ for all $x\\in (I\\circ SOL)^{\\exists}$. For technicality, we demand that $p(n)>1$ for all $n$.\nNotice that $b$ is computed using log space by the following simple machine $M'$   equipped with\na special work tape, called a {\\em solution tape}, in which we use only one tape cell. Starting with input $x$, $M'$ simulates $M_P$ on $x$. Whenever $M_P$ writes a symbol $\\sigma$ on its output tape, $M'$ writes $\\sigma$ on the solution tape,  simulates $M_m$ on $x$ while an auxiliary-tape head is scanning $\\sigma$, and erases $\\sigma$ from the solution tape. This deletion of the symbol $\\sigma$ is necessary because the output $M_P(x)$ may be super-logarithmically long.\nFinally, $M'$ outputs $b(x)$.\n\nHere, we construct a configuration graph $G$ in a way similar\nto the proof of Proposition \\ref{forest-path-weight} using $M'$; however, we use a quite different weight function.\nSince all weights are polynomially bounded,  we can embed an entire content of an output tape of $M'$ into a partial configuration. To be more precise, we can force $M'$ to use one of its work tapes to compute $b(x)$ and, in the end, copy  the content of this tape into its write-only output tape.\n\nFor a vertex $v$ representing a certain partial halting configuration, its weight $w(v)$ is set to be the value written on the output tape in this partial configuration unless the value is not zero. For any other vertex associated with partial non-halting configurations, we simply set $w(v)=1$.\n\nWe further define $f(x,r)$ to be the above-mentioned configuration graph $G$ together with the weight function $w$. Given a computation path $y$ of $M'$ on $x$, since each partial configuration in $y$ contains the information on an output symbol produced by $M_P$, it is possible to recover from $y$ an entire output string $M_P(x)$. We thus set $g(x,y,r)$ to be $M_P(x)$ reconstructed from $y$. It is easy to check that  $f,g\\in{\\mathrm{FNC}^{ {1} }}$.\nNote that $m_0(f(x,r),y) = b(x)$ if $y$ is in $SOL_0(f(x,r))$. This implies that the performance ratio $R$ of $g(x,y,r)$ always satisfies $R(x,g(x,y,r))=1$ for any $y\\in SOL_0(f(x,r))$. As a result, $(f,g,1)$ reduces $P$ to {\\sc max UB-Vertex}, as requested.\n\\end{proof}\n\n\n\nTo obtain an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problem for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, we place a restriction on the behavior of a (vertex) weight function of $\\text{\\sc Max UB-Vertex}$.\nThe {\\em maximum undirected 2 vertex weight problem} ({\\sc Max U2-Vertex}) is a variant of {\\sc Max UB-Vertex} with an additional requirement: for any instance ${\\langle {G,s,w} \\rangle}$ with $G=(V,E)$, it holds that\n$\\max_{v\\in V}\\{w(v)\\}\\leq 2\\min_{v\\in V}\\{w(v)\\}$.\n\n\\begin{proposition}\n{\\sc Max U$2$-Vertex} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$.\n\\end{proposition}\n\n\\begin{proof}\nTo simplify the following proof, we write $\\text{\\sc Max U2-Vertex}$ as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Similar to {\\sc Max UB-Vertex}, {\\sc Max U2-Vertex} can be shown to be in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. In particular, to verify the extra condition that (*) $\\max_{v\\in V}\\{w(v)\\}\\leq 2\\min_{v\\in V}\\{w(v)\\}$ for an instance $(G,s,w)$, we pick each pair $(v_1,v_2)$ of $G$'s vertices and check that either $w(v_1)\\leq 2w(v_2)$ or  $w(v_2)\\leq 2w(v_1)$ holds. This procedure requires only log space.\n\nThe following algorithm $C$ confirms that {\\sc Max U$2$-Vertex} belongs to  ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. On input $x =(G,s,w) \\in I_0$, output a length-$1$ path of $G$ from vertex $s$ to the vertex that appears first in an instance (where $G$ is given in binary as a list of edge relations).\nThis process can be implemented by ${\\mathrm{NC}^{ {1} }}$-circuits. It thus follows from Condition (*) that $m_0(x,C(x)) \\leq m_0^*(x) \\leq 2 m_0(x,C(x))$. Therefore, $C$ is an 2-approximate algorithm for {\\sc Max U2-Veterx}. This shows that {\\sc Max U2-Vertex} falls into ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$.\n\nNext, we want to show that $\\text{\\sc Max U2-Vertex}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. Let $P=(I,SOL,m,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$.\nThe case of minimization is handled by Lemma \\ref{min-reduces-max}(2).\nLet $M_m$ be an auxiliary Turing machine computing $m$ using log space.\nSince $m$ is polynomially bounded, $M_m$ first produces $m(x,y)$ on one of its work tapes and then copies it onto an output tape just before halting. Let $C_P$ be an ${\\mathrm{NC}^{ {1} }}$-circuit producing $\\alpha$-approximate solutions of $P$ for a certain constant $\\alpha>1$. For convenience, let $b(x) = m(x,C_P(x))$ for each $x\\in (I\\circ SOL)^{\\exists}$ and $b(x)=\\bot$ for all the others $x$. It follows that $b\\in{\\mathrm{FNC}^{ {1} }}$ and that $b(x)\\leq m^*(x)\\leq \\alpha b(x)$ for all $x\\in (I\\circ SOL)^{\\exists}$.\nBecause of the definition of partial configurations, from any accepting computation path of $M_m(x,y)$, we can easily recover an auxiliary input $y$.\n\nLet us prove $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Max U2-Vertex}$ via a certain ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$. Let $r\\geq1$ and $x\\in (I\\circ SOL)^{\\exists}$. Firstly, we define $f(x,r)$ to be $(G,s,w)$, where $G$ is a configuration graph\nof $M_m$ using normal input $x$, $s$ is the initial partial configuration of $M_m$, and $w$ is defined later.  Given a computation path $u=(v_1,v_2,\\ldots,v_k)$ of $M_m$ using $x$, let $y_u$ denote a unique auxiliary input used for $M_m$ constructed from $u$. Note that $y_u$ is in $SOL(x)$.\nHere, we further define $g(x,u,r)=y_u$ and set $c=\\alpha-1$. In addition, we set $\\Delta_x=(\\alpha-2)b(x)$.\nFor an accepting configuration $v$, let $w(v) = w' + \\Delta_x$, where $w'$ is the number written on $M_m$'s output tape.  For other configurations $v$, let $w(v)= b(x)+\\Delta_x$. Note that $b(x)+\\Delta_x \\leq w(v)\\leq m^*(x)+\\Delta_x$.\nSimilarly to Eqn.(\\ref{eqn:max-min-ratio}) in the proof of Lemma \\ref{NLO-to-APXL}, it holds that $\\max_{v\\in V}\\{w(v)\\}/\\min_{v\\in V}\\{w(v)\\}\\leq 2$. Since $b\\in{\\mathrm{FNC}^{ {1} }}$, $f(x,r)$ is computed by an ${\\mathrm{NC}^{ {1} }}$-circuit. For any $u=(v_1,v_2,\\ldots,v_k)\\in SOL_0(f(x,r))$, it follows that $m_0(f(x,r),u) = w(v_k) = m(x,y_u)+\\Delta_x = m(x,g(x,u,r)) +\\Delta_x$. Moreover, we obtain $m_0^*(f(x,r)) = m^*(x)+\\Delta_x$.\nGiven a number $r\\in{\\mathbb{Q}}^{\\geq1}$, consider the performance ratio $R$ of $g(x,u,r)$ with respect to $x$. Since $m(x,g(x,u,r))=b(x)$ for any $u\\in SOL(x)$, calculations similar to  Eqns.(\\ref{eqn:R_1-alpha})--(\\ref{eqn:ration-with-c})  lead to $R(x,g(x,u,r)) -1 \\leq c (R_0(f(x,r),u)-1)$.\nWe therefore conclude that $(f,g,c)$ reduces $P$ to {\\sc Max U2-Vertex}.\n\\end{proof}\n\n\n\\subsection{Completeness of Algebraic and Combinatorial Problems}\\label{sec:algebraic-problem}\n\nApart from graph problems, we shall study algebraic and combinatorial problems. We begin with an algebraic problem, which turns out to be complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. As {\\sc Max UB-Vertex} and {\\sc Max U2-Vertex} have orderly structures induced by ``edge relations,'' the algebraic problem that we shall consider below has a similar structure induced by ``operations.''\n\nGiven a finite set $X$, we consider a binary operation $\\circ:X\\times X\\to X$. A binary operation $\\circ$ is {\\em associative} if $(x\\circ y)\\circ z = x\\circ (y\\circ z)$ holds for all $x,y,z\\in X$. For a subset $S$ of $X$, we say that a set $G(S)$ is {\\em generated  by $\\circ$ from $S$} if $G(S)$ is the smallest set that contains $X$ and is closed under $\\circ$.\nThe decision problem, called {\\sc AGen}, of determining whether $t$ is in $G(S)$  for a given instance $(X,S,\\circ,t)$ with $t\\in X$ is $\\leq_{m}^{{\\mathrm{L}}}$-complete for ${\\mathrm{NL}}$ \\cite{JLL76}.\nLet us consider its optimization counterpart, which we call {\\sc Min AGen}.\n\n{\\medskip}\n{\\sc Minimum Associative Generation Problem} ({\\sc Min AGen}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a finite set $X$, an associative binary operation $\\circ:X\\times X\\to X$, a set $S\\subseteq X$, an element $t\\in X$, and a weight function $w:X\\to{\\mathbb{N}}^{+}$ satisfying $w(x)\\leq|X|$ for all $x\\in X$.\n\n\\item {\\sc Solution:} a sequence $(x_1,x_2,\\ldots,x_m)$ of elements in $X$ with $1\\leq m\\leq|X|$ so that the element $x_1\\circ x_2\\circ \\cdots \\circ x_m$ in $G(S)$ equals $t$.\n\n\\item {\\sc Measure:} the value $\\sum_{i=1}^{m}w(x_i)$.\n\\end{itemize}\n\nNote that $s$ belongs to $G(S)$ iff there is a sequence $(x_1,x_2,\\ldots,x_m)$ of elements in $S$ with $1\\leq m\\leq|X|$ for which $s=x_1\\circ x_2\\circ \\cdots \\circ x_m$ holds \\cite{JLL76}. In what follows, we demonstrate the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc Min AGen} for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{proposition}\n{\\sc Min AGen} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\\end{proposition}\n\n\\begin{proof}\nFor convenience, we set $\\text{\\sc Min AGen} = (I_0,SOL_0,m_0,\\text{\\sc min})$.\nFirst, we show that {\\sc Min AGen} is in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. Let $(X,S,\\circ,w,t)$ be any instance to {\\sc Min AGen}. We can build an auxiliary Turing machine $M$ as follows. Let $u= (x_1,x_2,\\ldots,x_m)$ be a sequence of $X$ and is given to an auxiliary tape of $M$.\nTo see that $I_0\\in{\\mathrm{L}}$, it suffices to check, using log space, whether (i) $S\\subseteq X$, (ii) $\\circ$ is associative, and (iii) $w(x)\\leq|X|$ for all $x\\in X$.\nIt is also easy to see that $I_0\\circ SOL_0\\in{\\mathrm{auxFL}}$. To obtain  $m_0((X,S,\\circ,w,t),u)$, we need to compute the value $x_1\\circ x_2\\circ \\cdots \\circ x_m$ and then output the value $\\sum_{i=1}^{m}w(x_i)$. Since $m_0$ is polynomially bounded, this value can be obtained using log space. This $m_0$ therefore belongs to ${\\mathrm{auxFL}}$.\nThis show that {\\sc Min AGen} is in ${\\mathrm{NLO}}$.\n\nNext, we want to show that {\\sc Min AGen} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for   ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. For this purpose, we shall show that $\\text{\\sc Min BPath-Weight}{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Min AGen}$ since, by Lemma \\ref{Bpath-complete}, {\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. Let $\\text{\\sc Min BPath-Weight} = (I_1,SOL_1,m_1,\\text{\\sc min})$.\nLet $h= (G,s,t,w)$ be any instance in $I_1$ with $G=(V,E)$.\nNote that $w(v)\\leq|V|$ for all $v\\in V$. For a sequence ${\\cal S}=(v_1,v_2,\\ldots,v_k)$ with $k\\leq |V|$, since $m_1(h,{\\cal S}) = \\sum_{i=1}^{k}w(v_i)$, we obtain $m_1(h,{\\cal S})\\leq|V|^2$.\n\nWe want to define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,c)$ from {\\sc Min BPath-Weight} to {\\sc Min AGen} as follows.\nOur construction essentially follows from the proof of \\cite[Theorem 5]{JLL76}. Let $c=1$. Letting $f(h,r) = (X,S,\\circ,\\tilde{w},t)$, we define $X$, $S$, $\\circ$, $\\tilde{w}$ as follows. Let $X=V\\cup(V\\times V)\\cup\\{\\natural\\}$ and $S=E\\cup\\{s\\}$. A binary operation $\\circ$ admits the following rules: for all $u\\in X$ and $x,y,z\\in V$, (i) $u\\circ \\natural = \\natural\\circ u=\\natural$, (ii) $x\\circ y=\\natural$ for all $x,y\\in V$, (iii) $(x,y)\n\\circ z = \\natural$, $x\\circ (x,z) =z$,  and $x\\circ (y,z)=\\natural$ if $y\\neq z$, and (iv) $(x,y)\\circ (y,v) = (x,v)$ and $(x,y)\\circ (u,v)=\\natural$ if $y\\neq u$. The desired weight function $\\tilde{w}$ is defined as  $\\tilde{w}(\\natural)=|X|$, $\\tilde{w}(x,y) = w(x)+w(y)$,  and $\\tilde{w}(x)=w(x)$  for all $x,y\\in V$. Note that $\\tilde{w}(x)\\leq \\max\\{|X|,2|V|\\}\\leq |X|$  for all $x\\in X$ and $\\tilde{w}(x_1)+\\sum_{i=1}^{k}\\tilde{w}(x_i,x_{i+1}) + \\tilde{w}(x_k) = 2\\sum_{i=1}^{k}w(x_i)$ for $x_1,x_2\\,\\ldots,x_k\\in V$.\n\nNote that, given an $s$-$t$ path $(x_1,x_2,\\ldots,x_k)$ with $s=x_1$ and $t=x_k$, since $s\\in S$, it is possible for us to prove recursively the membership $x_i\\in G(S)$ for every  $i\\in[2,k]_{{\\mathbb{Z}}}$; hence, $t\\in G(S)$ follows.\nFor any sequence $u=(x_1,(x_1,x_2),(x_2,x_3),\\ldots,(x_{k-1},x_k))$ in $SOL_{0}(f(h,r))$ with $s=x_1$ and $t=x_k$,  we define $g(h,u,r) = (x_1,x_2,\\ldots,x_k)$.\nIt follows that $m_0(f(h,r),u) = \\tilde{w}(x_1) + \\sum_{i=1}^{k}\\tilde{w}(x_i,x_{i+1}) = 2\\sum_{i=1}^{k}w(x_i) -w(t)$ and $m_1(h,g(h,u,r)) = \\sum_{i=1}^{k}w(x_i)$. From those equalities, we obtain $2m_1(h,g(h,u,r)) = m_0(f(h,r),u) + w(t)$.\n\nHere, we intend to verify that $(f,g,c)$ correctly reduces {\\sc Min BPath-Weight} to {\\sc Min AGen}. Take any $r\\in{\\mathbb{Q}}^{\\geq1}$ and any $u\\in SOL_0(f(h,r))$.  Assume that the performance ratio $R_0$ for {\\sc Min AGen} satisfies $R_0(f(h,r),u)\\leq r$. It then follows that $R_1(h,g(h,u,r)) = \\frac{m_1(h,g(h,u,r))}{m_1^*(h)} = \\frac{m_0(f(h,r),u)+w(t)}{m_0^*(f(h,r))+w(t)} \\leq \\frac{m_0(f(h,r),u)}{m_0^*(f(h,r))} \\leq r$.\nTherefore, $(f,g,c)$ is a correct ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction.\n\\end{proof}\n\n\n\nJenner \\cite{Jen95} studied a few variants of the well-known {\\em knapsack problem}. In particular, she introduced a decision problem, called CUK, of determining whether, given unary string pairs  $(0^w,0^p), (0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$, where  $w,w_i\\in{\\mathbb{N}}^{+}$ and $p,p_i\\in{\\mathbb{N}}$ for $i\\in[n]$, there is a $\\{0,1\\}$-sequence $(z_0,z_1,z_2,\\ldots,z_n)$ satisfying $w2^{p} = \\sum_{i=0}^{n}z_i\\cdot w_i2^{p_i}$.\nShe showed that this problem is (${\\mathrm{L}}$-uniform) $\\leq^{{\\mathrm{NC}^{ {1} }}}_{m}$-complete for ${\\mathrm{NL}}$. Here, we turn this decision problem into an optimization problem, which will be proven to be ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n{\\medskip}\n{\\sc Maximization 2-Bounded Close-to-Unary Knapsack Problem} ({\\sc Max 2BCU-Knapsack}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a pair $(0^w,0^p)$, a sequence $(0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$ of pairs, and a weight sequence $(c_0,c_1,\\ldots,c_n)$ of positive integers, where $w,w_i\\in{\\mathbb{N}}^{+}$, $p,p_i\\in{\\mathbb{N}}$, $1\\leq c_i\\leq nwp$ for all $i\\in[0,n]_{{\\mathbb{Z}}}$, and $\\max_{0\\leq i\\leq n}\\{c_i\\}\\leq 2\\min_{0\\leq i\\leq n}\\{c_i\\}$, provided that $w_0=w$ and $p_0=p$. Here, the notation $0^0$ expresses the empty string $\\lambda$.\n\n\\item {\\sc Solution:} a sequence $z=(z_0,z_1,\\ldots,z_n)$ of Boolean values satisfying $w2^p = \\sum_{i=1}^{n}z_i\\cdot w_i2^{p_i}$.\n\n\\item {\\sc Measure:} $\\max_{0\\leq i\\leq n}\\{ c_i z_i \\}$.\n\\end{itemize}\n\nA trivial solution $z=(1,0,\\ldots,0)$, which indicates the choice of $(0^{w_0},0^{p_0})$, is needed to ensure that $\\text{\\sc Max 2BCU-Knapsack}$ is indeed in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\nRegarding each value $w_i2^{p_i}$, the following simple inequalities hold:  $|bin(w_i2^{p_i})| \\leq \\log|0^{w_i}|+|0^{p_i}|+1 = \\log{w_i}+p_i+1$.\n\nIn what follows, we show the completeness of {\\sc Max 2BCU-Knapsack} for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{lemma}\n{\\sc Max 2BCU-Knapsack} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete\nfor ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $\\text{\\sc Max 2BCU-Knapsack} = (I_0,SOL_0,m_0,\\text{\\sc max})$. First, we argue that {\\sc Max 2BCU-Knapsack} is in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. It is obvious that $m_0$ is polynomially bounded. Earlier, Jenner \\cite{Jen95} demonstrated  that $\\mathrm{CUK}\\in{\\mathrm{NL}}$. A similar argument shows that $I_0\\circ SOL_0\\in{\\mathrm{auxL}}$ and $m_0\\in{\\mathrm{auxFL}}$; therefore, {\\sc Max 2BCU-Knapsack} falls into ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\nLet $x=(K,C)$ be any instance to {\\sc Max 2BCU-Knapsack}, where $C=(c_0,c_1,\\ldots,c_n)$ and $K$ is composed of $(0^{w},0^{p}),(0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$.\nChoose $z_0=(1,0,\\ldots,0)\\in\\{0,1\\}^{n+1}$.  It follows that $z_0\\in SOL_0(x)$ and that\n$R_0(x,z_0) = \\frac{m_0^*(x)}{m_0(x,z_0)} \\leq 2$ since $\\min_{0\\leq i\\leq n}\\{c_i\\}\\leq m_0^*(x)\\leq 2\\min_{0\\leq i\\leq n}\\{c_i\\}$.\nThis implies that {\\sc Max 2BCU-Knapsack} is in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\nFor the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max 2BCU-Knapsack}, let us consider an arbitrary maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ that satisfies Conditions (ii)--(iv) of  Lemma \\ref{NLO-to-APXL}.\nTo construct an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,2)$ from $P$ to {\\sc Max 2BCU-Knapsack}, we first consider a log-space deterministic Turing machine $M_P$ that produces  $2$-approximate solutions of $P$.\nIn addition, we take a function $b\\in{\\mathrm{FL}}$ for which\n$b(x)\\leq m^*(x)\\leq 2b(x)$ and $m(x,y)\\geq b(x)$ for all $(x,y)\\in I\\circ SOL$ by Lemma \\ref{NLO-to-APXL}. Let $M_m$ be a log-space auxiliary Turing machine computing $m$.\n\nFor the purpose of defining an appropriate reduction, we assume that $M_m$ on input $(x,y)$ writes each symbol of $y$ on a designated cell of a particular work tape so that,\n\nThe definition of partial configurations makes it possible to retrieve the entire string $y$ from any halting computation path $e$ of $M_m$ on input $(x,y)$. We write $h(e)$ for the auxiliary input $y$ fed into $M_m$.\nSince $m$ is polynomially bounded, we force $M_m$ to calculate $m(x,y)$ on one of its work tapes and copies it onto an output tape just before halting so that, for any  $y\\in SOL(x)$, $m(x,y)$ equals the sum of the numbers written on the output tape of configuration along a computation path of $M_{m}$ on input $(x,y)$.\n\nLet us recall the notion of configuration graph from the proof of Proposition \\ref{Min-Path-complete} and consider a configuration graph $G_{x}^{M_m}$ of $M_m$ on input $x\\in I$. In what follows, we consider only the case where $SOL(x)\\neq{\\mathrm{\\O}}$.\nFor convenience, we set $G^{M_m}_{x} = (V,E)$ with $V=\\{v_1,v_2,\\ldots,v_m\\}$ for a  certain integer $m\\geq1$, where $v_1$ expresses the initial partial configuration of $M_m$. Note that the first move of $M_m$ is fixed and does not depend on the choice of inputs.\nWe further make $M_m$ terminate in exactly $q(|x|)$ steps for a suitable polynomial $q$, independent of the choice of $y$ satisfying $|y|\\leq p(|x|)$.\n\nFix $x\\in(I\\circ SOL)^{\\exists}$ and write $q$ for $q(|x|)$.\nTake an arbitrary number $r\\in{\\mathbb{Q}}^{\\geq1}$.\nFollowing \\cite[Theorem 1]{Jen95}, let $w=2^{t}$ and $p=2(q+1)t$, where $t = 2{\\lceil {\\log{q}} \\rceil}$. If $(v_i,v_j)\\in E$, then let $w_{ij} = (2^{t}-j)2^{2t}-(2^{t}-i)$ and $p_{ij,k} = 2kt$ for  $0\\leq k\\leq q-1$.\nLet $w_0=w$, $p_0=p$, $w_1=2^{t}$, $p_1=0$, $w_{q}=q2^{2t}$, and $p_{q} = 2(q-1)t$.\nLet $K$ be composed of the following pairs: $(0^w,0^p)$, $(0^{w_0},0^{p_0})$,  $(0^{w_1},0^{p_1})$, $(0^{w_{q}},0^{p_{q}})$, and $(0^{w_{ij}},0^{p_{ij}})$ for all $(v_i,v_j)\\in E$. A series $C=(c_0,c_1,c_{q},c_{ij})_{(v_i,v_j)\\in E}$ is defined as follows.\nFor any partial configuration pair $(v_i,v_j)$, if $v_j$ is an accepting partial configuration, then let $c_{ij}$ be the number written on an output tape of $M_m$; otherwise, let $c_{ij}=b(x)$. Note that $\\min_{(v_i,v_j)\\in e}\\{c_0,c_1,c_q,c_{ij}\\}\\geq b(x)$.\nFor any $x\\in I$ and $r\\in{\\mathbb{Q}}^{\\geq1}$, we define $f(x,r)$ to be the pair $(K,C)$ given above.\n\nGiven any accepting computation path $e=(v_1,v_2,\\ldots,v_q)$ of $M_m$ on input $x$ ($|x|=n$), we define a series $z=(z_0,z_1,z_q,z_{ij})_{(v_i,v_j)\\in e}$ as $z_0=0$, $z_1=z_q=1$, and $z_{ij}=1$ if $(v_i,v_j)\\in e$, and $z_{ij}=0$ otherwise.\nTo emphasize $e$, we write $z_{(e)}$ for this series $z$.\nAs was shown in \\cite{Jen95}, we obtain $z\\in SOL_0(f(x,r))$ with $z\\neq(1,0,\\ldots,0)$ iff there exists an accepting computation path $e$ satisfying $z=z_{(e)}$, namely, $w2^{p} = w_12^{p_1}+w_q2^{p_q}+ \\sum_{(v_i,v_j)\\in e}\\sum_{0\\leq k\\leq q-1}w_{ij}2^{p_{ij,k}}$ and thus $m_0(f(x,r),z_{(e)}) = \\max_{(v_i,v_j)\\in e}\\{c_0,c_1,c_q,c_{ij}\\} = m(x,h(e))$ since $m(x,y)\\geq b(x)$ for all $y\\in SOL(x)$. From this fact, we define $g(x,z_{(e)},r) = h(e)$ for every  solution $z_{(e)}\\in SOL_0(f(x,r))$.\nSince $z\\in SOL_0(f(x,r))$, there exists a suitable accepting computation path $e$ in $G^{M_m}_{x}$ satisfying $z=z_{(e)}$.\nThus,  we obtain\n", "itemtype": "equation", "pos": 128217, "prevtext": "\nThe last term equals $c(R_2(f(x,r),\\tilde{y})-1)$. Since $R_2(f(x,r),\\tilde{y})\\leq r$, it follows that $R_1(x,g(x,\\tilde{y},r)) \\leq 1+ c(r-1)$. Therefore, $(f,g,c)$ reduces $Q$ to $P$.\n\\end{proof}\n\nLet us begin the proof of Proposition \\ref{Mix-2Path-Weight}.\n\n{\\smallskip}\n\\begin{proofof}{Proposition \\ref{Mix-2Path-Weight}}\nFirst, we shall  argue that {\\sc Max FL-$\\lambda$-NFA} belongs to   ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. For simplicity, we set $\\text{\\sc Max FL-$\\lambda$-NFA}$ as  $(I_0,SOL_0,m_0,\\text{\\sc min})$. Let $x=(M,0^n)$ be any instance in $I_0$ with $M=(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$, where $M$ is demanded to accept $0^n$. Since we feed only inputs $y$ of length $n$, the value $rep(1y)$ varies from $2^n$ to $2^{n+1}-1$. It thus follows that, for any $u\\in SOL_0(x)$,\n$m_0^*(x)/2 \\leq m_0(x,u) \\leq m_0^*(x)$. Consider the following algorithm $N$: take $x$ as input and simulate $M$ on input $0^n$ (also by checking the size of $0^n$). This algorithm requires only log space.\nWe then obtain $m_0^*(x)/2\\leq m_0(x,N(x))\\leq m_0^*(x)$ for any $x\\in I_0$. These bounds imply that $\\text{\\sc Max FL-$\\lambda$-NFA}$ is a member of  ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$.\n\nNext, we shall show the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max FL-$\\lambda$-NFA}. Let $P=(I,SOL,m,\\text{\\sc max})$ be any problem in ${\\mathrm{APXL}}_{{\\mathrm{MaxNL}}}$. Without loss of generality, we assume that $P$ satisfies Conditions (ii)--(iv) of Lemma \\ref{NLO-to-APXL}, and thus $P$ admits a $2$-approximate algorithm.\nOur goal is to show that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reducible to {\\sc Max FL-$\\lambda$-NFA} via a suitable reduction $(f,g,c)$. Let $M_m$ be a log-space auxiliary Turing machine computing $m$ and let $M_P$ denote a log-space $2$-approximate algorithm for $P$. There is a function $b\\in{\\mathrm{FL}}$ such that $b(x)\\leq m^*(x)\\leq 2b(x)$  and $m(x,y)\\geq b(x)$  for all $x\\in (I\\circ SOL)^{\\exists}$, where $b(x)$ is of the form $2^{{\\lfloor {\\log{C(x)}} \\rfloor}}$ for appropriate function $C\\in{\\mathrm{FL}}$.\nMoreover, we assume that, for each $x\\in (I\\circ SOL)^{\\exists}$, there is a solution $y\\in SOL(x)$ such that $m(x,y)=b(x)$.\n\nAs in the proof of Proposition \\ref{Min-Path-complete}, we consider partial configurations of $M_m$.\nWe define $Q$ to be the set of all possible partial configurations of $M_m$.\nFix $x\\in (I\\circ SOL)^{\\exists}$ arbitrarily and let $n$ be the size of binary string $bin(b(x))$.\n\nWe want to define a $\\lambda$-1nfa $N$, which ``mimics'' a computation of $M_m$. $N$'s inner states are partial configurations of $M_m$. An input to $N$ is $bin(m(x,y))^{(-)}$ for a certain auxiliary input $y\\in SOL(x)$. A move of $N$ is described as follows. Given a string $u$ and a number $k\\in{\\mathbb{N}}^{+}$, $u_k$ denotes the $k$th symbol of $u$.\nOn such an input, $N$ nondeterministically guesses a string $y\\in\\{0,1\\}^n$.\nBy reading an input symbol $\\tau'\\in\\{0,1\\}$ from $bin(m(x,y))$ one by one from left to right, $N$ changes an inner state $v=(q,x,j,\\xi,u,k,\\tau)$ to another inner state  $v'=(p,x,j+d_1,\\xi',w,k+d_2,\\tau')$ in a single step by applying $M_m$'s transition ``$(p,\\tau',w_{k},d_1,d_2)\\in \\delta(q,x_{j},\\xi,u_{k})$,'' where $\\xi'$ is the next bit of $\\xi$ in $y$. More precisely, we define $N$'s transition as  ${\\langle {p,x,j+d_1,\\xi',w,k+d_2,\\tau'} \\rangle}\\in \\delta_{N}({\\langle {q,x,j,\\xi,u,k,\\tau} \\rangle},\\tau')$ iff $(p,\\tau',w_k,d_1,d_2)\\in\\delta(q,x_j,\\xi,u_k)$, where $w$ is obtained from $u$ by changing $u_k$ to $w_k$.\n\nWe modify $N$ so that it simultaneously checks whether its input is of the form $0^n$. If so, $N$ enters a designated accepting state. Hence, $0^n\\in L(N)$.\n\nHere, let us define the desired reduction $(f,g,c)$. First, we set $f(x,r)={\\langle {N} \\rangle}$. Given an accepting computation path $e$ in $SOL_0(f(x,r))$, we define $g(x,e,r)$ to be an auxiliary input $y_{(e)}$ fed into $M_m$, which can be obtained from $e$. It follows that $y_{(e)}\\in SOL(x)$ and that $m(x,y_{(e)})=m_0(f(x,r),e)$.\nLet $r\\in{\\mathbb{Q}}^{\\geq1}$. The performance ratio $R_P$ for $P$ satisfies that $R_P(x,g(x,e,r)) = \\frac{m^*(x)}{m(x,g(x,e,r))} = \\frac{m_0^*(f(x,r))}{m_0(f(x,r),e)} = R_0(f(x,r),e)$.\n\nTherefore, the reduction $(f,g,c)$ ensures $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Max FL-$\\lambda$-NFA}$.\n\\end{proofof}\n\n\n\nAs a simple variant of {\\sc Max $\\lambda$-NFA}, we shall consider  {\\em one-way one-head deterministic finite automata with $\\lambda$-moves} (or $\\lambda$-1dfa's, in short). A $\\lambda$-1dfa $M$ is a tuple $(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ working with input alphabet $\\{0,1\\}$ and $\\delta:(Q_{\\lambda}\\times\\{\\lambda\\})\\cup(Q_{+}\\times \\{0,1,\\natural\\})\\to Q$, where $Q=Q_{\\lambda}\\cup Q_{+}$, $Q_{\\lambda}\\cap Q_{+}={\\mathrm{\\O}}$, $q_0\\in Q$, and $F\\subseteq Q$. This $M$ must satisfy the following condition: if $M$ is in state $q\\in Q_{\\lambda}$, then $M$'s read-only tape head stays still; otherwise, the tape head moves to the next right cell.\nHere, each transition ``$\\delta(q,\\sigma)=p$'' is succinctly expressed as  $(q,\\sigma,p)$.\n\n{\\medskip}\n{\\sc Maximum Input-Restricted $\\lambda$-Deterministic Finite Automata Problem} ({\\sc Max IR-$\\lambda$-DFA}):\n\\begin{itemize}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a $\\lambda$-1dfa $M =(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ and a list $Y=(y_1,y_2,\\ldots,y_k)$ of strings over $\\{0,1\\}$, where $\\delta$ is given as a list of (partial) transitions of the form $(q,\\sigma,p)$.\n\n\\item {\\sc Solution:} an accepting computation path  of $M$ of length at most $|Q|$ on a certain input $y\\in Y$, which is surrounded by ${|}\\!\\!\\mathrm{c}$ and ${\\$}$.\n\n\\item {\\sc Measure:} an integer $rep(1y)$.\n\\end{itemize}\n\n\\begin{proposition}\\label{MAX-IR-lambda-DFA}\n{\\sc Max IR-$\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\\end{proposition}\n\nIf we  take $(M,1^n)$ as an instance and demand $y$ to have length at most $n$, then we obtain another problem, {\\sc Max $\\lambda$-DFA}. It is not clear that {\\sc Max $\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for either ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$ or ${\\mathrm{MaxNL}}$.\n\n\\begin{proofof}{Proposition \\ref{MAX-IR-lambda-DFA}}\nFor convenience, let {\\sc Max IR-$\\lambda$-DFA} have the form $(I_0,SOL_0,m_0,\\text{\\sc max})$.\nLet us claim that  {\\sc Max IR-$\\lambda$-DFA} is in ${\\mathrm{NLO}}$.\nNote that it is easy to check using log space whether a given instance $M =(Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ is indeed a $\\lambda$-1dfa; thus, $I_0\\in{\\mathrm{L}}$ follows.\nTo see $I_0\\circ SOL_0\\in{\\mathrm{auxL}}$, on input $(M,Y)$ together with a sequence $p = (p_1,p_2,\\ldots,p_m)$ of configurations of $M$ as an auxiliary input, we can check using only log space whether $p$ is indeed an accepting computation path of $M$ (by checking that $p_1=(q_0,{|}\\!\\!\\mathrm{c})$, $p_m=(q_f,{\\$})$, $(p_i,\\sigma,p_{i+1})$ is a transition for each $i\\in[m]$ with $m\\leq |Q|$ for a certain $\\sigma\\in\\{0,1,\\lambda\\}$, and a series of such symbols matches one of $y$'s in $Y$. As for $m_0\\in{\\mathrm{auxFL}}$, it is possible to retrieve an input $y$ from $p$ and output $1y$ using log space if $y$ is in $Y$.\n\nNext, we shall show that {\\sc Max IR-$\\lambda$-DFA} belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nRecursively, we pick each $y$ in $Y$ in the lexicographic order and simulate a given $\\lambda$-1dfa $M$ on this input $y$ to check if $M$ accepts $y$ within $|Q|$ steps. This process determines the maximal accepted input $y$ in $Y$.  Finally, we generate an accepting computation path of $M$ on this $y$. This whole procedure requires log space.\nHence, {\\sc Max IR-$\\lambda$-DFA} can be solved using only log space.\n\nHereafter, we shall  show that {\\sc Max IR-$\\lambda$-DFA} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nLet us consider  any maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. We assume that $P$ satisfies Conditions (ii)--(iii) of Lemma \\ref{LO-simple-form}. Since optimal solutions and their objective values are both computed using log space, for the purpose of defining $g$, we can build a log-space deterministic Turing machine $M$ that, on input $x$, records each symbol $y_i$ of a solution $y=y_1y_2\\cdots y_k\\in SOL^*(x)$ in one cell of one work tape (after erasing the previous symbol $y_{i-1}$ if any) and produces $bin(m(x,y))^{(-)}$ on an output tape (by removing the first bit ``$1$'' from $bin(m(x,y))$).\n\nWe shall  construct a pair $(f,g)$ of functions that ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduces $P$ to {\\sc Max IR-$\\lambda$-DFA}.\nLet $x$ be any instance in $I$. We construct a $\\lambda$-1dfa $N_x = (Q,\\{0,1\\},\\{{|}\\!\\!\\mathrm{c},{\\$}\\},\\delta,q_0,F)$ as follows.\nWe view each configuration of $M$ (including the content of its output symbol  and an input symbol) as an inner state of $N_x$. Let $Q$ be a set of all such configurations. Note that $|Q|\\geq |x|$. We roughly treat an output tape of $M$ as an input tape of $N_x$. More precisely, when $M$ writes a symbol $\\sigma\\in\\{0,1\\}$ on its output tape, $N_x$ reads $\\sigma$ on the input tape. When $M$ does not write any non-blank output symbol, $N_x$ makes its associated  $\\lambda$-move.\nFinally, we set $f(x,r) ={\\langle {N_x} \\rangle}$.  For $N_x$, let $p = (p_0,p_1,\\ldots,p_m)$ be an accepting computation path of $N_x$ of length $\\leq|Q|$.\nWe also define $g(x,p,r)$ to be an input $y_p$ to $N_x$ that is recovered from $p$ as stated above.\nNote that $m_0(f(x,r),p) = rep(1bin(m(x,y_p)^{(-)})) = m(x,y_p)$.\nConcerning the performance ratio $R_P$ and $R_0$ for $P$ and {\\sc Max IP-$\\lambda$-DFA}, respectively, it follows that $R_{P}(x,g(x,p,r)) = \\frac{m^*(x)}{m(x,y_p)} = \\frac{m_0^*(f(x,r))}{m_0(f(x,r),p)} = R_0(f(x),p)$. Therefore, $(f,g,1)$ is an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction from $P$ to {\\sc Max IR-$\\lambda$-DFA}, as requested. The case where $P$ is a minimization problem can be similarly treated.\n\\end{proofof}\n\n\n\n\\section{Polynomially-Bounded Complete Problems}\\label{sec:PBP}\n\nLet us recall from Section \\ref{sec:comb-OPs} that an optimization problem is said to be  {\\em polynomially bounded} exactly when its measure function is polynomially bounded. Recall also the notation ${\\mathrm{PBO}}$, which expresses the set of all polynomially-bounded optimization problems.\nFor many low-complexity optimization/approximation classes below ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$, polynomially-bounded optimization problems play a quite special role. With respect to log-space computation, it appears more natural to deal with polynomially-bounded optimization problems than polynomially-unbounded ones because, through Section \\ref{sec:completeness}, we have been unable to present any complete problem in ${\\mathrm{NLO}}$ and ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ but, as we shall see shortly, we can exhibit complete problems in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ and\n${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\nIn the subsequent subsections, we shall present polynomially-bounded optimization problems, which turn out to be complete for various NL optimization and approximation classes.\n\n\n\\subsection{Maximization Versus Minimization}\\label{sec:key-lemma}\n\nAssume that we wish to show the completeness of a certain optimization problem $P$ for a target optimization/approximation class ${\\cal D}$. Since ${\\cal D}$ may be composed of maximization problems as well as minimization problems, it is necessary to construct desirable reductions to $P$ from all maximization problems in ${\\cal D}$ and also from all minimization problems in ${\\cal D}$.\nRegarding ${\\mathrm{NPO}}$ problems, it is well-known that every minimization problem $Q$ in ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ has its maximization counterpart $Q'$ in ${\\mathrm{APXP}}_{{\\mathrm{NPO}}}$ whose complexity is at least as hard as $Q$ (see, e.g., \\cite[Theorem 8.7]{ACG+03} for the proof).\n\nA similar statement holds for polynomially-bounded problems in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. This is because a log-space auxiliary Turing machine that computes a polynomially-bounded measure function can freely manipulate the outcome of the function using its space-bounded work tapes before writing it down onto an output tape.\n\n\\begin{lemma}\\label{min-reduces-max}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item For any minimization (resp., maximization) problem $P$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$, there  exists a maximization (resp., minimization) problem $Q$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{APXL}},{\\mathrm{APXNC}^{ {1} }}\\}$. For every minimization (resp., maximization) problem $P$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, there exists a maximization (resp., minimization) problem $Q$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\n\\item Let ${\\cal D}\\in\\{{\\mathrm{LO}},{\\mathrm{NC}^{ {1} }\\mathrm{O}}\\}$. For any minimization (resp., maximization) problem $P$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, there  exists a maximization (resp., minimization) problem $Q$ in ${\\cal D}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ such that $P$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $Q$.\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\n(1) This proof is similar in essence to that of Lemma \\ref{reduction-MaxNL-MinNL}.\nGiven an arbitrary problem $P_1=(I_1,SOL_1,m_1,\\text{\\sc min})$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$, we aim at constructing a maximization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc max})$ in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ to which $P_1$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible.\nSince $P_1\\in{\\mathrm{PBO}}$, there is a polynomial $p$ satisfying $m_1(x,y)\\leq p(|x|)$ for every $(x,y)\\in I_1\\circ SOL_1$. Let $I_2=I_1$, $SOL_2=SOL_1$, and $m_2(x,y) = {\\lceil {\\frac{p(|x|)^2}{m_1(x,y)}} \\rceil}$ for every $(x,y)\\in I_2\\circ SOL_2$. Since $m_1$ is polynomially-bounded, we can generate the entire value $m_1(x,y)$ on one of log-space work tapes and manipulate it freely as if a normal input. (If $m_2$ is not polynomially bounded, then there is no guarantee that a log-space auxiliary Turing machines  can compute $m_2$.) Since ``division'' can be implemented on TC$^{0}$ circuit \\cite{Hes01}\n(and thus, by a log-space machine),  $m_2(x,y)$ can be generated on a log-space work tape. Here, we define $f(x,r)=x$ and $g(x,y,r)=y$ so that $f$ and $g$ belong to ${\\mathrm{FAC}^{ {0} }}$. For the performance ratio $R_2$ for $P_2$, assume that $R_2(f(x,r),y)\\leq r$ for any $r\\geq1$; that is, $m_2^*(x)\\leq m_2(x,y)\\leq r m_2^*(x)$. The ratio $R_1(x,g(x,y,r)) = \\frac{m_1(x,y)}{m_1^*(x)}$ is thus at most $1+c(r-1)$, where $c=1$. Hence, $(f,g,c)$ reduces $P_1$ to $P_2$.\n\n(2) We shall show only the case of ${\\cal D}={\\mathrm{APXNC}^{ {1} }}$.\nLet $P_1=(I_1,SOL_1,m_1,\\text{\\sc min})$ be any polynomially-bounded minimization problem in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. We want to construct a polynomially-bounded maximization problem $P_2=(I_2,SOL_2,m_2,\\text{\\sc max})$, which is in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$ and is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduced from  $P_1$. Take a constant $\\gamma> 1$ and an ${\\mathrm{NC}^{ {1} }}$ $\\gamma$-approximate algorithm $C$ for $P_1$.\nFor convenience, we set $b(x) = m_1(x,C(x))$ for each instance $x\\in (I_1\\circ SOL_1)^{\\exists}$. By the definition of ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, $b$ must belong to ${\\mathrm{FNC}^{ {1} }}$. Note that $m_1^*(x)\\leq b(x)\\leq \\gamma m_1^*(x)$ for all $x\\in (I_1\\circ SOL_1)^{\\exists}$.\n\nWe choose a constant $\\Delta$ so that $\\Delta >\\gamma$ holds. Note that $\\Delta-1>0$ since $\\gamma\\geq1$. Next, we define $I_2=I_1$, $SOL_2(x) = \\{y\\in SOL_1(x)\\mid m_1(x,y)\\leq b(x)\\}$, and $m_2(x,y) = \\Delta b(x) - \\gamma m_1(x,y)$ for any $(x,y)\\in I_2\\circ SOL_2$.\nNotice that  $m_2$ is computed by an appropriate log-space auxiliary Turing machine, and thus $I_2\\circ SOL_2$ is also computed by a certain log-space auxiliary Turing machine. This implies that $P_2$ is an ${\\mathrm{NLO}}$ problem.\n\nWe shall  claim that $P_2\\in{\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. Note that $m^*_2(x) = \\Delta b(x) -\\gamma m^*_1(x)$ for every $x\\in (I_2\\circ SOL_2)^{\\exists}$. Moreover, $m_2(x,C(x)) = (\\Delta-\\gamma)b(x)$ holds. Since $m^*_1(x)\\leq b(x)$, we obtain $(\\Delta-\\gamma)b(x)\\leq m^*_2(x)$, which implies $m_2(x,C(x))\\leq m^*_2(x)$. Since $b\\in{\\mathrm{FNC}^{ {1} }}$, the value $m_2(x,C(x))$ can be computed from $x$ by a certain ${\\mathrm{NC}^{ {1} }}$-circuit.  Since $b(x)\\leq \\gamma m^*_1(x)$, it follows that $m^*_2(x)\\leq (\\Delta-1)b(x)$; thus, we obtain $m^*_2(x) \\leq \\frac{\\Delta-1}{\\Delta-\\gamma}m_2(x,C(x))$. In summary, it holds that $\\frac{\\Delta-\\gamma}{\\Delta-1}m^*_2(x)\\leq m_2(x,C(x))\\leq m^*_2(x)$. Hence, $P_2$ belongs to ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$.\n\nAs for the desired ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction, we define $f(x,r)=x$ and $g(x,y,r)=y$, where $r$ is any number in ${\\mathbb{Q}}^{\\geq1}$. It is obvious that $f$ and $g$ are in ${\\mathrm{FAC}^{ {0} }}$. For any fixed solution $y\\in SOL_2(x)$, we assume that  $R_2(f(x,r),y)\\leq r$; that is, $m_2(x,y)\\leq m_2^*(x)\\leq r m_2(x,y)$. We want to claim that $m_1(x,y)\\leq [1+(\\Delta-1)(r-1)]m_1^*(x)$.\nFirst, we note that\n\n", "index": 11, "text": "\\begin{equation*}\nm_1(x,y) =(1/\\gamma)[\\Delta b(x) - m_2(x,y)] \\leq (1/r\\gamma)[(r-1)\\Delta b(x) + \\gamma m_1^*(x)]\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"m_{1}(x,y)=(1/\\gamma)[\\Delta b(x)-m_{2}(x,y)]\\leq(1/r\\gamma)[(r-1)\\Delta b(x)+%&#10;\\gamma m_{1}^{*}(x)]\" display=\"block\"><mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msubsup><mi>m</mi><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01118.tex", "nexttext": "\nTherefore, $(f,g,2)$ reduces $P$ to {\\sc Max 2BCU-Knapsack}.\n\\end{proof}\n\nNext, we shall present a complete problem in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$. Our problem is a simple extension of the Boolean formula value problem.\nThe {\\em Boolean formula value problem} (BFVP) is to determine whether a given Boolean formula $\\phi$ is satisfied by a given truth assignment $\\sigma$. This problem can be compared with CVP (circuit value problem), which is known to be ${\\mathrm{P}}$-complete \\cite{Lad75}.\nBuss \\cite{Bus87} showed the membership of BFVP to $\\mathrm{ALOGTIME}$ and the $\\leq_{m}^{\\mathrm{DLOGTIME}}$-hardness of BFVP for $\\mathrm{ALOGTIME}$ is relatively easy to verify. Therefore, since $\\mathrm{ALOGTIME}$ equals ${\\mathrm{NC}^{ {1} }}$,\n$\\text{\\sc BFVP}$ is $\\leq^{{\\mathrm{DLOGTIME}}}_{m}$-complete for ${\\mathrm{NC}^{ {1} }}$.\n\nLet us consider its optimization counterpart.\n\n{\\medskip}\n{\\sc Maximum Boolean Formula Value Problem} ({\\sc Max BFVP}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:}  a set $\\Phi = \\{\\phi_1,\\phi_2,\\ldots,\\phi_n\\}$ of Boolean formulas and a Boolean assignment $\\sigma$ for all variables in the formulas in $\\Phi$ with $n\\geq1$.\n\n\\item {\\sc Solution:} a {\\em nonempty} subset $S\\subseteq \\Phi$ of formulas satisfied by $\\sigma$.\n\n\\item {\\sc Measure:} the cardinality $|S|$ of $S$.\n\\end{itemize}\n\nRecall that the formula language of a Boolean formula family $\\{\\phi_n\\}_{n\\in{\\mathbb{N}}}$ is composed of all tuples ${\\langle {c,i,y} \\rangle}$ such that $|y|=n$ and the $i$th character of the $n$th formula $\\phi_n$ is $c$. Following \\cite{BIS90}, a language $L$ is in ${\\mathrm{NC}^{ {1} }}$ iff there exists a family of Boolean formulas representing $L$ whose formula language is in ${\\mathrm{DLOGTIME}}$.\n\n\\begin{lemma}\n{\\sc Max BFVP} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\\begin{proof}\nFor our convenience, express $\\text{\\sc Max BFVP}$ as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Given a set $\\Phi= \\{\\phi_1,\\phi_2,\\ldots,\\phi_n\\}$ of Boolean formulas and its truth assignment $\\sigma$, since the number of all Boolean formulas $\\phi_i$ satisfied by $\\sigma$ is upper-bounded by the input size, {\\sc Max BFVP} is polynomially bounded. Next, we argue that {\\sc Max BFVP} is in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. It is obvious that {\\sc Max BFVP} belongs to ${\\mathrm{NLO}}$. On input $(\\Phi,\\sigma)$,\nwe check in parallel whether $\\sigma$ satisfies $\\phi_i$  by making nonadaptive queries to oracle BFVP (i.e.,  $(\\phi_i,\\sigma)\\in\\mathrm{BFVP}$), and finally we count the number of satisfied formulas $\\phi_i$.\nThe last counting process requires another ${\\mathrm{NC}^{ {1} }}$-circuit.  Since $\\mathrm{BFVP}\\in{\\mathrm{NC}^{ {1} }}$ \\cite{Bus87}, we can implement the whole procedure using ${\\mathrm{NC}^{ {1} }}$-circuits. Thus, {\\sc Max BFVP} belongs to ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\nTo see the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hardness of {\\sc Max BFVP} for ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, let $P=(I,SOL,m,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$. The minimization case can be similarly treated.\nLet us take a uniform family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ of ${\\mathrm{NC}^{ {1} }}$-circuits computing maximal solutions of $P$, that is, $C_{|x|}(x)\\in SOL^*(x)$ for any $x\\in (I\\circ SOL)^{\\exists}$. Define $SOL'(x)=\\{C_{|x|}(x)\\mid C_{|x|}(x)\\neq\\bot\\}$ for each $x\\in I$.\nNote that $|SOL'(x)|\\leq 1$ holds for all $x\\in I$.\nFor simplicity, we assume that $I\\subseteq \\{0,1\\}^*$ and take any binary string  $x=x_1x_2\\cdots x_n$ in $I$. Let the output size of $C_n$ be $k$. For each index $i\\in[k]$, we define a new ${\\mathrm{AC}^{ {0} }}$-circuit $D_k^{(i)}(y_1,\\ldots,y_k)=y_i$. It follows that $D_k^{(i)}(C_n(z_1,\\ldots,z_n))$ is an ${\\mathrm{NC}^{ {1} }}$-circuit, where  $z_1,z_2,\\ldots,z_n$ are Boolean variables.\n\nOwing to \\cite[Theorem 9.1]{BIS90}, we can replace a uniform family of ${\\mathrm{NC}^{ {1} }}$-circuits by a family of Boolean formulas whose formula language is in ${\\mathrm{DLOGTIME}}$.\nFor each $i\\in[k]$, let $\\phi_i$ be a Boolean formula expressing the circuit $D_k^{(i)}(C_n(z_1,\\ldots,z_n))$. Note that $D_k^{(i)}(C_n(x_1\\cdots x_n)) = u_i$ if $C_n(x_1x_2\\cdots x_n)=u_1u_2\\cdots u_k$.  Let $\\sigma_x$ be the assignment that assigns value $x_i$ to variable $z_i$.\n\nFinally, we define $f(x,r) = {\\langle {{\\langle {\\phi_1,\\ldots,\\phi_m} \\rangle},\\sigma_x} \\rangle}$ and $g(x,y,r)= u_1u_2\\cdots u_k$ for each $y\\in SOL_0(f(x,r))$, where $y$ is of the form $(\\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_t})$, and $u_i=1$ if $\\phi_i\\in y$ and $u_i=0$ otherwise. It follows that $|SOL_0(f(x,r))|\\leq 1$ for all $x\\in I_0$. If $y=(\\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_t})\\in SOL_0(f(x,r))$ with $f(x,r) = {\\langle {{\\langle {\\phi_1,\\ldots,\\phi_m} \\rangle},\\sigma_x} \\rangle}$, then $\\sigma_x$ satisfies $\\{\\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_t}\\}$. Hence, we obtain $g(x,y,r)\\in SOL^*(x)$. Hence, $\\frac{m^*(x)}{m(x,g(x,y,r))} = \\frac{m^*(x)}{m(x,u_1u_2\\cdots u_k)}=1$. It thus follows that $P$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to {\\sc Max BFVP}.\n\\end{proof}\n\n\n\n\\section{Weak Approximation Schemes}\\label{sec:LSAS}\n\nApproximability of optimization problems is a crucial concept in a course of our study on the computational complexity of those problems. The classes ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$ both admit approximation algorithms; however, the effectiveness of approximation is often far from the desirable one used in practice. Approximation schemes, on the contrary, provide a much finer notion of approximability because the performance ratio of used approximation algorithms can be made arbitrarily smaller; in other words, we can find solutions that are arbitrary close to optimal solutions.\n\nWe shall discuss optimization problems that admit such approximation schemes.  Concerning ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$, Nickelsen and Tantau \\cite{NT05} and also Tantau \\cite{Tan07} proposed ${\\mathrm{NLO}}$ problems that admit an ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$; in particular, a polynomially-bounded maximization problem, called {\\sc Max-HPP},   was shown in \\cite[Theorem 5.7]{Tan07} as a member of ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$. Here, we rephrase this problem in terms of {\\em complete graphs} in comparison with {\\sc Min BPath-Weight}.\n\n\n\n{\\medskip}\n{\\sc Maximum Complete-Graph Path Weight Problem} ({\\sc Max CPath-Weight}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:}   a directed complete graph $G=(V,E)$ with self-loops, a source $s\\in V$, and an edge weight function $w:V\\times V\\to{\\mathbb{N}}^{+}$ with $w(v_1,v_2)\\leq|V|$ for any $v_1,v_2\\in V$.\n\n\\item {\\sc Solution:} a path ${\\cal S}=(v_1,v_2,\\ldots,v_{k})$ of length $k\\leq |V|$  in $G$ starting at $s$ (i.e., $s=v_1$).\n\n\\item {\\sc Measure:} total path  weight  $w({\\cal S}) = \\sum_{i=1}^{k-1}w(v_{i},v_{i+1})$.\n\\end{itemize}\n\n\n\\begin{proposition}\\label{Min-Comp-Wight-in-LSAS}\n\\begin{enumerate}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item \\cite{Tan07} {\\sc Max CPath-Weight} is in ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\item {\\sc Max CPath-Weight} is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{enumerate}\n\\end{proposition}\n\n\\begin{proof}\n(2) Let $\\text{\\sc Max CPath-Weight} = (I_0,SOL_0,m_0,\\text{\\sc max})$.\nGiven any maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, we want to define an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g)$ from $P$ to {\\sc Max CPath-Weight} in the following fashion.\nLet $M_P$ be a log-space deterministic Turing machine computing optimal solutions of $P$; in other words, $m^*(x)= m(x,M_P(x))$ for all $x\\in (I\\circ SOL)^{\\exists}$ with $SOL(x)\\neq{\\mathrm{\\O}}$.\nLet $x$ be any instance in $(I\\circ SOL)^{\\exists}$ and consider a configuration graph  with $G^{M_P}_{x}=(V_x,E_x)$ of $M_P$ on $x$.\nLet $s$ denote an initial partial configuration of $M_P$ on $x$.\n\nDefine $V=V_x$ and $E=V\\times V$ and set $G=(V,E)$. Since $M$ is deterministic,\nthere exists a unique computation path $(p_1,p_2,\\ldots,p_k)$ starting at  $s=p_1$ and ending at an accepting configuration $t=p_k$.\nAs for an edge weight function $w$, we define $w(v,v')=|V|$ if either (i) $v$ is an accepting configuration and $v=v'$ or (ii) $v'$ is obtained from $v$ in a single step by $M_P$; otherwise, let $w(v,v')=1$.\nThe desired $f$ is obtained by setting $f(x) = (G,s,w)$. This $f$ can be computed by an appropriate ${\\mathrm{AC}^{ {0} }}$-circuit.\nIt is not difficult to see that any optimal solution of {\\sc Max CPath-Weight} is $u_x= (p_1,p_2,\\ldots,p_k,p_k,\\ldots,p_k)$ of length exactly $|V|$.\nSince $(p_1,p_2,\\ldots,p_k)$ is a computation path, we can retrieve from it an output string produced by $M_P$ on $x$. More generally, given a path $u$ starting at $s$ and ending at a certain accepting configuration, we reconstruct from $u$ an output  string $y_u$ of $M_P$ on $x$.\nUsing these strings, we set $g(x,u)=y_u$. This function $g$ can be implemented by a certain ${\\mathrm{AC}^{ {0} }}$-circuit. Note that $m_0^*(f(x)) = (|V|-1)|V|$.\n\nIf $u\\in SOL_0^*(f(x))$, then, since $m_0^*(f(x)) = (|V|-1)|V|$, $u$ coincides with $u_x$. Hence, $y_u$ equals $M_P(x)$. Thus, $(f,g)$ reduces $P$ to {\\sc Max CPath-Weight}.\n\\end{proof}\n\n\n\nNext, we shall look into another approximation class ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\nThe next example is slightly artificial but it can be proven to fall into  ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$.\nRecall from Section \\ref{sec:PBP} {\\sc Max U2-Vertex}, which is a member of  ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$. We modify this problem by changing its requirement of  $\\max_{v\\in V}\\{w(v)\\}\\leq 2\\min_{v\\in V}\\{w(v)\\}$ for each instance $(G,s,w)$ to $\\max_{v\\in V}\\{w(v)\\}\\leq (1+1/k_G)\\min_{v\\in V}\\{w(v)\\}$ with  $k_G = \\log\\log{|V|}$. We call the resulted problem {\\sc Max UApp-Vertex} (maximum undirected approximable vertex weight problem).\n\n\\begin{proposition}\\label{ncsas-example}\n\\begin{enumerate}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc Max UApp-Vertex} is in ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\item {\\sc Max UApp-Vertex} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-hard for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{enumerate}\n\\end{proposition}\n\n\\begin{proof}\n(1) We express $\\text{\\sc Max UApp-Vertex}$ as $(I_0,SOL_0,m_0,\\text{\\sc max})$ and we plan to define its approximation scheme, which can be implemented by certain ${\\mathrm{AC}^{ {0} }}$-circuits.\nLet $r\\in{\\mathbb{Q}}^{>1}$ and set $k=\\frac{1}{r-1}$. Take any instance $(x,r)$ in $I_0$ given to {\\sc Max UApp-Vertex} with $G=(V,E)$. We consider\ntwo cases separately, depending on the value of $k$.\n\n(i) Assume that $k\\leq \\log\\log|V|$. It follows from the requirement for $I_0$ that $\\max_{v\\in V}\\{w(v)\\}\\leq (1+1/k)\\min_{v\\in V}\\{w(v)\\}$. In this case, our desired approximation algorithm first searches the nearest vertex, say, $v_0$ connected from $s$ and directly outputs it. This vertex $v_0$ gives $R(x,v_0) = \\frac{m_0^*(x)}{w(v_0)} \\leq \\frac{\\max_{v\\in V}\\{w(v)\\}}{\\min_{v\\in V}\\{w(v)\\}} \\leq 1+1/k = r$. This procedure provides an $r$-approximate solution and can be easily implemented by an appropriate ${\\mathrm{AC}^{ {0} }}$-circuit.\n\n(ii) Assume that $k>\\log\\log|V|$; in other words, $|V|< 2^{2^k}$. Consider the following brute force algorithm: pick a vertex $y$ one by one, check if $s$ and $y$ are connected, and choose the one $y$ that has the largest value $w(y)$. This procedure requires space $O(|V|\\log|V|)$, which equals $O(2^{2^k}\\log{n})$.\n\n(2) Similarly to the proof of Proposition \\ref{max-bvertex-complete}, let\n$P=(I,SOL,m,\\text{\\sc max})$ denote any maximization problem in\n${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$. Write $M_m$ for a log-space auxiliary Turing machine computing $m$ and write $M_P$ for a log-space deterministic Turing machine finding maximal solutions of $P$. Let $b(x)=m(x,M_P(x))$ for all $x\\in (I\\circ SOL)^{\\exists}$.  Since $b$ is in ${\\mathrm{FL}}$, choose a log-space deterministic Turing machine $M'$ that computes $b$ together with generating each symbol of $M_P(x)$ for the purpose of the later reconstruction of $M_P(x)$ from any halting computation patj of $M'$ on $x$.\nFor the problem $P$, we assume Conditions (ii)--(iii) of Lemma \\ref{LO-simple-form}. Note that $|SOL(x)|\\leq 1$ holds for\nall instances $x\\in I$.\n\nLet us define $f(x,r) = (G,s,w)$, where $G=(V,E)$ is a configuration graph of $M'$ on $x$ and $s$ is the initial configuration. Here, we assume that $M'$ is $c\\log{n}$ space-bounded for a certain constant $c>0$; moreover, by modifying $M'$ slightly, we also assume that $M'$ has one work tape with tape alphabet of cardinality at least $2$. Because of this space bound, there are at least $2^{c\\log{n}}=n^c$ vertices in $G$. Thus, $\\log\\log{|V|}\\geq \\log\\log{n}$ follows.\nLet $w(v)$ be ${\\lceil {\\log\\log{n}} \\rceil}+1$ if $v$ is a halting partial  configuration of $M'$ on $x$; otherwise, let $w(v) = {\\lceil {\\log\\log{n}} \\rceil}$.  It follows that ${\\lceil {\\log\\log{n}} \\rceil} \\leq m(x,y) \\leq {\\lceil {\\log\\log{n}} \\rceil}+1$ for all path $y$ in $G$. Hence, $\\max_{y\\in SOL(f(x,r))}\\{m(x,y)\\}\\leq {\\lceil {\\log\\log{n}} \\rceil}+1\\leq (1+1/\\log\\log{n}){\\lceil {\\log\\log{n}} \\rceil} \\leq \\min_{y\\in SOL(f(x,r))}\\{m(x,y)\\}$. Next, we define $g(x,y,r)$ to be the string $M_P(x)$ recovered from $y$. This is possible by the assumption on the behavior of $M'$. Finally, we set $c'=1$. It is not difficult to show that $(f,g,c')$ reduces $P$ to {\\sc Max UApp-Vertex}.\n\\end{proof}\n\n\n\n\n\\section{Relations among Refined Optimization/Approximation  Classes}\\label{sec:complexity-OP}\n\nLet us turn our attention to relationships among classes of refined optimization problems introduced in Section \\ref{sec:preliminaries}.\nOptimization problems are, by definition, associated directly with their {\\em underlying decision problems}. It is therefore natural to ask what relationships exist between classes of optimization problems and classes of decision problems.\nWe intend to investigate relationships between optimization problems and decision problems.\nFirst, let us recall from Lemma \\ref{PO=APXP=NLO} the collapse  of classes ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$, ${\\mathrm{PTAS}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{APXP}}_{{\\mathrm{NLO}}}$ down to ${\\mathrm{NLO}}$. The class ${\\mathrm{NLO}}$ may further collapses to much lower-complexity classes if ${\\mathrm{AC}^{ {1} }}$ collapses to ${\\mathrm{L}}$ or ${\\mathrm{NC}^{ {1} }}$.\nHere, we prove the following assertions concerning the class ${\\mathrm{NLO}}$.\n\n\\begin{proposition}\\label{by-AJ-result}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item \\cite{Tan07} If ${\\mathrm{L}} = {\\mathrm{AC}^{ {1} }}$, then ${\\mathrm{LO}}_{{\\mathrm{NLO}}} ={\\mathrm{NLO}}$.\n\n\\item If ${\\mathrm{NC}^{ {1} }} ={\\mathrm{AC}^{ {1} }}$, then  ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} ={\\mathrm{NLO}}$.\n\\end{enumerate}\n\\end{proposition}\n\nThe opposite direction of this proposition is not known to hold.\nThe proposition comes directly from a more general result stated below. In particular, Lemma \\ref{char-with-AC1}(1) extends Lemma \\ref{PO=APXP=NLO}.\n\n\\begin{lemma}\\label{char-with-AC1}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} = {\\mathrm{NLO}}$.\n\n\\item If ${\\mathrm{L}} = {\\mathrm{AC}^{ {1} }}$, then ${\\mathrm{LO}}_{{\\mathrm{NLO}}} ={\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\n\\item If ${\\mathrm{NC}^{ {1} }} ={\\mathrm{AC}^{ {1} }}$, then ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} ={\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\n(1) Obviously, ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ is included in ${\\mathrm{NLO}}$. For the opposite inclusion, consider {\\sc Max $\\lambda$-NFA}, given in Section \\ref{sec:approximation-class}, which is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for\n${\\mathrm{MaxNL}}$ \\cite[Theorem 3.1]{Tan07}.\nIt is sufficient to show that (i) {\\sc Max $\\lambda$-NFA} is in ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ and (ii) ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ is closed under ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$ (and thus ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$).\n\n(i)  \\`{A}lvarez and Jenner \\cite{AJ95} also Allender {\\textrm{et al.}}~\\cite{ABP93} showed the containment ${\\mathrm{OptL}}\\subseteq {\\mathrm{FAC}^{ {1} }}$.\nTake any $\\lambda$-1nfa $M$ as an instance to {\\sc Max $\\lambda$-NFA}.\nWe want to find a maximal solution using ${\\mathrm{AC}^{ {1} }}$ circuits.\nFor this purpose, using ${\\mathrm{AC}^{ {1} }}$ circuits, we convert $M$ into an equivalent regular grammar $G$ in {\\em Chomsky Normal Form}.\nAs done in the proof of \\cite[Corollary 4.6]{ABP93}, we can find the lexicographically first string over $\\{0,1\\}$ of length at most $n$ produced by $G$. This process can be implemented on ${\\mathrm{AC}^{ {1} }}$-circuits. Therefore, {\\sc Max $\\lambda$-NFA} can be solved by ${\\mathrm{AC}^{ {1} }}$-circuits.\n\n(ii) Assume that $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}Q$ and $Q\\in{\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Take an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-reduction $(f,g,c)$ from $P$ to $Q$. Let $P=(I_1,SOL_1,m_1,\\text{\\sc max})$ and $Q=(I_2,SOL_2,m_2,\\text{\\sc max})$. Let $C_Q$ be an ${\\mathrm{AC}^{ {1} }}$ circuit computing optimal solutions of $Q$. Consider the following algorithm $D$. On input $x\\in I_1$, compute $f(x,1)$ and output $C_Q(f(x,1))$. Clearly, $D(x)\\in SOL_1(x)$. Note that ${\\mathrm{FAC}^{ {1} }}$ is closed under functional composition. Since ${\\mathrm{FL}}\\subseteq {\\mathrm{FAC}^{ {1} }}$, $D$ is also in ${\\mathrm{FAC}^{ {1} }}$. Hence, $P$ is in ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\n(2)  Assume that ${\\mathrm{L}}={\\mathrm{AC}^{ {1} }}$. This implies that ${\\mathrm{FL}}={\\mathrm{FAC}^{ {1} }}$. Take $P=(I,SOL,m,goal)$ in ${\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. There is an ${\\mathrm{AC}^{ {1} }}$ circuit $C_P$ computing optimal solutions of $P$ and another ${\\mathrm{AC}^{ {1} }}$-circuit $C_m$ that computes $m(x,C_P(x))$ for all $x\\in (I\\circ SOL)^{\\exists}$. Treating circuits as functions they compute, we obtain $C_P,C_m\\in{\\mathrm{FAC}^{ {1} }}$. Since ${\\mathrm{FL}}={\\mathrm{FAC}^{ {1} }}$, $C_P$ and $C_m$ fall into ${\\mathrm{FL}}$. This implies that $P$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\nThus, ${\\mathrm{LO}}_{{\\mathrm{NLO}}}={\\mathrm{AC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ follows.\n\n(3) Here, we assume that ${\\mathrm{NC}^{ {1} }}={\\mathrm{AC}^{ {1} }}$. Note that this assumption implies ${\\mathrm{FNC}^{ {1} }}={\\mathrm{FAC}^{ {1} }}$. Let us consider an arbitrary optimization problem $P=(I,SOL,m,goal)$ in ${\\mathrm{AC}^{ {1} }\\mathrm{O}}$. Take ${\\mathrm{AC}^{ {1} }}$-circuits that compute optimal solutions of $P$ as well as their values.\nBy a way similar to (2), since ${\\mathrm{FNC}^{ {1} }}={\\mathrm{FAC}^{ {1} }}$, $P$'s optimal solutions can be computed  using appropriate ${\\mathrm{NC}^{ {1} }}$-circuits. As a result, $P$ belongs to ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\\end{proof}\n\n\n\n\nAs revealed in Section \\ref{sec:PBP}, the polynomial-boundedness property  is  crucial for optimization problems in discussing their computational complexity.  If we fix our focal point on polynomially-bounded optimization problems, we can manage to give another characterization of ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ in terms of adaptive relativization.\n\nTo explain a notion of relativization, we empower each log-space Turing machine $M$ with {\\em query mechanism}, in which $M$ makes a series of ``queries'' to a given ``oracle'' $A$ (which is simply a language) and $A$ returns its answers to $M$ in a single step. The machine $M$ has an extra {\\em query tape} on which $M$ writes a query string $z$ by moving its tape head from the left to the right. After entering a designated inner state, called a {\\em query state}, the string $z$ is transmitted to the oracle $A$. The oracle erases all symbols on the query tape and it writes $1$ if $z\\in A$ and writes $0$ otherwise. The machine's inner state is changed to an answer state, from which $M$ resumes its computation. We make the length of every query string polynomially bounded. This machine $M$ is generally known as an {\\em oracle Turing machine}.\nWe write ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{A}$ to denote the class of all ${\\mathrm{NLO}}$ problems whose optimal solutions and their values are computed by log-space oracle Turing machines using oracles $A$. The union of ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{A}$ for all sets $A\\in{\\mathrm{NL}}$ is denoted by ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}$.\n\n\\begin{lemma}\\label{LO-with-oracle-NL}\n${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}\\cap{\\mathrm{PBO}} = {\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\\begin{proof}\n($\\subseteq$) This inclusion is trivial because\n${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}\\subseteq {\\mathrm{NLO}}$ holds by the definition of\n${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}$.\n\n($\\supseteq$) Let $P=(I,SOL,m,goal)$ be any ${\\mathrm{NLO}}$ problem. It suffices to show the existence of log-space oracle Turing machines that compute optimal solutions and their values relative to oracles in ${\\mathrm{NL}}$.\nHere, we consider only the case of $goal=\\text{\\sc max}$. We define two sets $A$ and $B$. Take a polynomial $p$ for which $y\\in SOL(x)$ implies both $|y|\\leq p(|x|)$ and $m(x,y)\\leq p(|x|)$. Let $A$ be composed of all strings $(x,k)$ such that there exists a string $z\\in SOL(x)$ with $k\\leq p(|x|)$ satisfying  $m(x,z)=k$. By making a series of queries $(x,1),(x,2),\\ldots,(x,p(|x|))$ one by one to oracle $A$, we can find the maximal number $k_0$ satisfying $(x,k_0)\\in B$. Next, we define $B$ as the set of all strings $(x,k,u,b)$ with $b\\in\\{0,1\\}$ and $k,|ub|\\leq p(|x|)$ such that there is a string $z\\in SOL(x)$ for which $m(x,z)=k$, $ub$ is an initial segment of $z$. Given the maximal number $k_0$, by making the $i$th query $(x,k_0,u_i,0)$ and $(x,k_0,u_i,1)$ for all $i\\in[0,p(|x|)]_{{\\mathbb{Z}}}$ sequentially, we reconstruct $z_0$ satisfying  $m(x,z_0)=k_0$. As the desired oracle, we take $A\\oplus B$, which clearly belongs to ${\\mathrm{NL}}$. Therefore, we find a maximal solution $z_0$ using only log space. This yields $P\\in{\\mathrm{LO}}_{{\\mathrm{NLO}}}^{A\\oplus B}\\subseteq {\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}$.\n\\end{proof}\n\nHere, we present three close connections between classes of decision problems and classes of optimization problems.\n\n\\begin{proposition}\\label{equivalent-relation}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item \\cite{Tan07} ${\\mathrm{L}}={\\mathrm{NL}}$ iff ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\n\\item ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$ iff ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ iff ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\n\\item ${\\mathrm{L}}={\\mathrm{P}}$ iff ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{PO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$ iff ${\\mathrm{LO}}_{{\\mathrm{NPO}}} = {\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\\end{enumerate}\n\\end{proposition}\n\nFor Proposition \\ref{equivalent-relation}(1)--(2), we use DSTCON and USTCON on unweighted directed graphs. Earlier, Jones \\cite{Jon75} showed that $\\mathrm{DSTCON}$ is ${\\mathrm{NL}}$-complete under log-space reduction. His reduction can be improved to ${\\mathrm{NC}^{ {1} }}$-reduction, and thus DSTCON is $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NL}}$. Reingold \\cite{Rei08} proved that USTCON belongs to ${\\mathrm{L}}$. From its $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-hardness for ${\\mathrm{L}}$, we conclude that USTCON is $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{L}}$.\nFor Proposition \\ref{equivalent-relation}(3), we use the {\\em circuit value problem} ($CVP$) of determining whether a given Boolean circuit outputs $1$ on a certain input. Ladner \\cite{Lad75} pointed out that this is $\\leq^{{\\mathrm{L}}}_{m}$-complete for ${\\mathrm{P}}$; more strongly, it is $\\leq^{{\\mathrm{NC}^{ {1} }}}_{m}$-complete for ${\\mathrm{P}}$.\n\n\\begin{proofof}{Proposition \\ref{equivalent-relation}}\n(1) Here, we give a proof different from the one given in \\cite{Tan07}. Our proof relies on Lemma \\ref{LO-with-oracle-NL}. If ${\\mathrm{L}}={\\mathrm{NL}}$, then ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}$ equals ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{L}}}$, which coincides with ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Since ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}\\cap{\\mathrm{PBO}} ={\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ by Lemma \\ref{LO-with-oracle-NL}, we obtain ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}={\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. The desired consequence follows because ${\\mathrm{LO}}_{{\\mathrm{NLO}}}={\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{L}}}$.\n\nConversely, assume that ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. Lemma \\ref{LO-with-oracle-NL} implies that ${\\mathrm{LO}}_{{\\mathrm{NLO}}}^{{\\mathrm{NL}}}\\cap{\\mathrm{PBO}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. Consider DSTCON, which is $\\leq_{m}^{{\\mathrm{AC}^{ {0} }}}$-complete for ${\\mathrm{NL}}$. Define $P=(I,SOL,m,\\text{\\sc max})$ as follows. Let $I$ consist of all strings $(G,s,t)$ for which $G=(V,E)$, $s,t\\in V$, and there is an $s$-$t$ path in $G$. Let $x=(G,s,t)$. Define $SOL(x)$ to be a collection of all $s$-$t$ path in $G$. Let $m(x,y)=2$ if $y\\in SOL(x)$ and $m(x,y)=1$ otherwise. Note that $P$ is in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. By our assumption, $P$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Hence, take a log-space Turing machine $M$ that computes optimal solutions of $P$. It follows that $M(x)\\neq\\bot$ iff $x\\in\\mathrm{DSTCON}$. Hence, DSTCON can be recognized using log space, yielding ${\\mathrm{L}}={\\mathrm{NL}}$.\n\n(2) For this claim, we shall show separately that (i) if  ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, then ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$ and (ii) if ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$, then ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Since ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}={\\mathrm{LO}}_{{\\mathrm{NLO}}}$ implies ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, the claim follows immediately.\n\n(i) First, we assume that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.  Since $\\mathrm{USTCON}$ is $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{L}}$, it suffices for us to verify that $\\mathrm{USTCON}$ falls into ${\\mathrm{NC}^{ {1} }}$ using our assumption.\nWe intend to define a polynomially-bounded maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in the following manner.  An instance $w$ to $P$ is a tuple $(G,s,t)$ for an undirected graph $G=(V,E)$, including an edge $(s,t)$, and two vertices $s,t\\in V$. A feasible solution of $w$ is a path $y=(y_1,y_2,\\cdots, y_k)$ in $G$ starting at $s=y_1$. If\n$y$ is a path from $s$ to $t$ with $k\\geq2$, we set its objective value $m(w,y)$ to be $2$; otherwise, we set  $m(w,y)=1$. Obviously, $m$ is polynomially bounded and belongs to ${\\mathrm{auxFL}}$.\nNote that $w\\in \\mathrm{USTCON}$ iff there exists an $s$-$t$  path $y$ for which  $m(w,y)=2$.\nNext, we claim that $P$ is in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Consider a log-space deterministic Turing machine $M$ that computes 2-approximate solutions by simply outputting the special edge $(s,t)$.\nHence, $P$ falls into ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\nSince $P\\in{\\mathrm{PBO}}$ and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$, our assumption implies that $P$ is in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\nThere exists a uniform family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$  of ${\\mathrm{NC}^{ {1} }}$-circuits finding maximal solutions of $P$ (if any). Using those $C_n$'s, we construct another uniform ${\\mathrm{NC}^{ {1} }}$-circuit family $\\{D_n\\}_{n\\in{\\mathbb{N}}}$ such that $D_n(w)=1$ iff $C_n(w)$ is an $s$-$t$ path by checking that every consecutive elements in $y$ are connected by a single edge and the first and the last elements are $s$ and $t$, respectively. Clearly, $\\{D_n\\}_{n\\in{\\mathbb{N}}^{+}}$ recognizes $\\mathrm{USTCON}$. This implies that $\\mathrm{USTCON}\\in{\\mathrm{NC}^{ {1} }}$, as requested.\n\n(ii)  Assuming ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$, let us consider any problem $P=(I,SOL,m,goal)$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. To simplify the following argument, we assume that all feasible solutions of $P$ are expressed {\\em in binary}. Consider the case of $goal=\\text{\\sc min}$. Let $p$ be a polynomial that upper-bounds the size of any feasible solution of $P$.\nLet $M$ be a log-space deterministic Turing machine computing minimal solutions of $P$. To build a solver for $P$, we first define an oracle $A$ as a collection of ${\\langle {x,1^i,b} \\rangle}$ for which there exists a binary string $y$ with $|y|\\leq p(|x|)$ such that $M(x)$ outputs $y$ and the $i$th bit of $y$ is $b\\in\\{0,1\\}$.  Since $A$ belongs to ${\\mathrm{L}}$, our assumption implies that $A$ falls into  ${\\mathrm{NC}^{ {1} }}$.\nNote that, if $M(x)=y$ with $y=y_1y_2y_3\\cdots y_e$, then $y$ coincides with $A({\\langle {x,1,y_1} \\rangle}) A({\\langle {x,1^2,y_2} \\rangle})\nA({\\langle {x,1^3,y_3} \\rangle}) \\cdots A({\\langle {x,1^{e},y_e} \\rangle})$, where $A$ is treated as its characteristic function.\n\nTo recover the $i$th bit $y_i$ of $M(x)$ for all $i\\in[e]$, we compute both $A({\\langle {x,1^i,0} \\rangle})$ and $A({\\langle {x,1^i,1} \\rangle})$ and decide that $y_i$ takes a value $b\\in\\{0,1\\}$ exactly when  $A({\\langle {x,1^i,b} \\rangle})=1$ and $A({\\langle {x,1^i,\\overline{b}} \\rangle})=0$.  This procedure can be done in parallel with accesses to the oracle $A$ suing an ${\\mathrm{NC}^{ {1} }}$ circuit. Since $A\\in{\\mathrm{NC}^{ {1} }}$, the procedure can be implemented using uniform ${\\mathrm{NC}^{ {1} }}$-circuits without\nany oracle Since ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$ implies ${\\mathrm{FNC}^{ {1} }}={\\mathrm{FL}}$, the measure function $m$ is in ${\\mathrm{auxFNC}^{ {1} }}$. From this, we conclude that $m(x,M(x))$ is computed from $x$ by a suitable ${\\mathrm{NC}^{ {1} }}$-circuit.\nThus, $P$ belongs to ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\n(3) Notice that, in this proof, we consider ${\\mathrm{NPO}}$ problems instead of ${\\mathrm{NLO}}$ problems. Similarly to (2), we wish to prove that (i) ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{PO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$ implies ${\\mathrm{L}}={\\mathrm{P}}$ and (ii) ${\\mathrm{L}}={\\mathrm{P}}$ implies ${\\mathrm{LO}}_{{\\mathrm{NPO}}}={\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\n(i) Assume that ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}} = {\\mathrm{PO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$.\nConsider CVP, which is $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{P}}$.\nFounded on CVP, we intend to build a maximization problem $P$ of the form $(I,SOL,m,\\text{\\sc max})$ lying  in  ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$.\nDefine $I$ as the set of all Boolean circuits and, for any circuit $C\\in I$, define  $SOL(x)$ to be a set of all variable assignments $\\sigma$ setting all input variables of $C$ to either $0$ or $1$ for which $\\sigma$ forces $C$ to output $1$. Since $\\mathrm{CVP}\\in{\\mathrm{P}}$, $P$ belongs to ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$.\nBy our assumption, $P$ falls into ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\cap{\\mathrm{PBO}}$. This yields a log-space deterministic Turing machine solving $P$. By running $M$ and checking $M$'s outputs, we can solve $\\mathrm{CVP}$ using only log space. Hence, $\\mathrm{CVP}$ is in ${\\mathrm{L}}$, implying that ${\\mathrm{L}}={\\mathrm{P}}$.\n\n(ii) Assuming ${\\mathrm{L}}={\\mathrm{P}}$, let us consider {\\sc Min Weight-st-Cut}, defined in Section \\ref{sec:why-NC1}, which is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ by Proposition \\ref{min-st-cut-is-po}.\nLet $M$ be any  polynomial-time deterministic Turing machine computing minimal  solutions of {\\sc Min Weight-st-Cut}.\nLet $p$ be a polynomial satisfying that,  for all instances $x$ to {\\sc Min Weight-st-Cut}, $p(|x|)$ bounds $M$'s output size. We define a problem $A$ to be composed of all strings of the form ${\\langle {x,1^i,b} \\rangle}$ for which $M(x)$ outputs a certain string $y$ and $y$'s $i$th bit is $b$.\nIt is not difficult to show that $A$ is in ${\\mathrm{P}}$. Our assumption of ${\\mathrm{L}}={\\mathrm{P}}$ then implies that $A$ indeed belongs to ${\\mathrm{L}}$.\n\nThe following log-space algorithm can compute optimal solutions of {\\sc Min Weight-st-Cut}: on input $x$ to {\\sc Min Weight-st-Cut}, reconstruct $M(x)$ bit by bit by incrementing $i$ by one from $1$ to $p(|x|)$, check whether ${\\langle {x,1^i,0} \\rangle}\\in A$ or ${\\langle {x,1^i,1} \\rangle}\\in A$ or neither, and determine the $i$th bit of $M(x)$ to be $b\\in\\{0,1\\}$ if ${\\langle {x,1^i,b} \\rangle}\\in A$ and ${\\langle {x,1^i,\\overline{b}} \\rangle}\\notin A$.  Since $A\\in {\\mathrm{L}}$, this procedure requires only log space. Hence, {\\sc Min Weight-st-Cut} belongs to ${\\mathrm{LO}}_{{\\mathrm{NPO}}}$.\n\\end{proofof}\n\n\n\nHere, we further refine Proposition \\ref{equivalent-relation}.\n\n\\begin{theorem}\\label{refined-relation}\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{0mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item  ${\\mathrm{L}}\\neq {\\mathrm{NL}}$  iff  ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} \\neq\n{\\mathrm{LSAS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} \\neq\n{\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} \\neq {\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\n\\item ${\\mathrm{NC}^{ {1} }}\\neq{\\mathrm{L}}$ iff the following classes are mutually distinct: ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$, ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, and ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\n\\item  ${\\mathrm{L}}\\neq {\\mathrm{P}}$ iff the following classes are mutually distinct: ${\\mathrm{LO}}_{{\\mathrm{NPO}}}$, ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}$, ${\\mathrm{APXL}}_{{\\mathrm{NPO}}}$, and ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\\end{enumerate}\n\\end{theorem}\n\nGiven any string $y\\in\\{0,1\\}^{+}$, we set $del(y)$ to be the string obtained from $y$ by deleting all $0$s from the leftmost bit of $y$ until the first $1$. For example, $del(0010)=10$, $del(01001)=1001$, and $del(00)=\\lambda$. We then define $rep_{+}(y)$ to be {\\em  one plus} the natural number represented in binary as $del(y)$; that is, $rep_{+}(y)=1+rep(del(y))$.\n\n\\begin{proofof}{Theorem \\ref{refined-relation}}\n(1) Tantau \\cite[Theorem 4.1]{Tan07} showed that, under the assumption of ${\\mathrm{L}}\\neq{\\mathrm{NL}}$, the following four classes are mutually distinct:  ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, and ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\nTo see the converse, assume that those classes are different.  If ${\\mathrm{L}}={\\mathrm{NL}}$, then Proposition \\ref{equivalent-relation}(1) implies ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} ={\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. This obviously contradicts our assumption because ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{LSAS}}_{{\\mathrm{NLO}}} \\subseteq {\\mathrm{APXL}}_{{\\mathrm{NLO}}} \\subseteq{\\mathrm{NLO}}$.\nHence, the desired separation between ${\\mathrm{L}}$ and ${\\mathrm{NL}}$ follows immediately.\n\n(2) We shall show separately two directions of ``iff.''\n\n(If--part) To show that ${\\mathrm{NC}^{ {1} }}\\neq{\\mathrm{L}}$, we assume that the following four classes are mutually different: ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}o$, ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$, ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, and  ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. Toward a contradiction, we assume that ${\\mathrm{NC}^{ {1} }}={\\mathrm{L}}$. Proposition \\ref{equivalent-relation}(2) implies that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}} = {\\mathrm{LO}}_{{\\mathrm{NLO}}}$. This is a clear contradiction; therefore, we obtain ${\\mathrm{NC}^{ {1} }}\\neq{\\mathrm{L}}$, as requested.\n\n(Only If--part)\nAssume that ${\\mathrm{NC}^{ {1} }}\\neq{\\mathrm{L}}$. First, notice that, by Proposition \\ref{equivalent-relation}(2), we obtain ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} \\neq {\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. Since ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$, it suffices to verify four  inequalities: (a) ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}} \\neq {\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$, (b) ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}} \\neq {\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$, (c) ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}\\nsubseteq {\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$, where (c) implies ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\neq{\\mathrm{LO}}_{{\\mathrm{NLO}}}$ and also ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}} \\neq {\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\n(a) To lead to a contradiction, let us assume that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}} = {\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$. We express $\\text{\\sc Max UApp-Vertex}$ given in Section \\ref{sec:LSAS} as $(I,SOL,m,\\text{\\sc max})$. Since {\\sc Max UApp-Vertex} belongs to ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ by Proposition \\ref{ncsas-example}(1), our assumption places {\\sc Max UApp-Vertex} in ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\nTake an ${\\mathrm{NC}^{ {1} }}$ circuit $C$ computing optimal solutions of {\\sc Max UApp-Vertex}. Consider the set $L$ composed of $(G,s,w,\\ell)$ such that $(G,s,w)\\in I$, $\\ell\\in{\\mathbb{N}}^{+}$, and there exists a vertex $t\\in V$ for which $w(t)\\geq \\ell$ and $s$ and $t$ are connected in $G$. This problem $L$ falls into ${\\mathrm{NC}^{ {1} }}$, because it can be computed by the following ${\\mathrm{NC}^{ {1} }}$-circuit: on input $x=(G,s,w,\\ell)$, we first compute $y=C(G,s,w)$, extract a vertex $t\\in V$ from $y$ satisfying $w(t)=m^*(x)$, and check whether $w(t)\\geq \\ell$. It therefore suffices to verify that $\\mathrm{USTCON}\\leq_{m}^{{\\mathrm{NC}^{ {1} }}} L$, because this implies that $\\mathrm{USTCON}\\in{\\mathrm{NC}^{ {1} }}$, a contradiction.\nFrom any instance $(G,s,t)$ to $\\mathrm{USTCON}$, we define $(G,s,w,k)$ as follows. Define $w(t)=1+\\log\\log|V|$ and $w(v)=\\log\\log|V|$ for all $v\\in V-\\{t\\}$. Let $k=w(t)$.\nClearly, $(G,s,w,k)$ can be computed from $(G,s,t)$ using an appropriate ${\\mathrm{NC}^{ {1} }}$-circuit.\nIt is not difficult to show that $(G,s,t)\\in \\mathrm{USTCON}$ iff $(G,s,w,k)\\in L$. Hence, $\\mathrm{USTCON}\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}{\\mathrm{L}}$ follows.\n\n(b) Since $\\mathrm{USTCON}$ in ${\\mathrm{L}}$, it follows from our assumption that  $\\mathrm{USTCON}\\notin{\\mathrm{NC}^{ {1} }}$. From $\\mathrm{USTCON}$, we construct the maximization problem $P=(I,SOL,m,\\text{\\sc max})$ defined as in the proof (i) of Proposition  \\ref{equivalent-relation}(2). This problem $P$ actually belongs to ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, because there is an NC$^{1}$ algorithm computing 2-approximate solutions to $P$. For such a solution $y$ on an instance $x$, we obtain $m(x,y)/m^*(x)\\leq 2$, which means $m(x,y)\\leq 2$. If $P\\in{\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$, then, by setting $r=3/2$, we obtain $r$-approximate solutions $z$ on input $x$ using an ${\\mathrm{NC}^{ {1} }}$-circuit. If $x\\in\\mathrm{USTCON}$, then, since $R(x,z)\\leq 3/2$,  $m(x,z)=2$ follows; thus, $z$ must be a $s$-$t$ path.\nIf $x\\notin\\mathrm{USTCON}$, then $m(x,z)=1$ and $z\\neq t$. This implies that $\\mathrm{USTCON}\\in{\\mathrm{NC}^{ {1} }}$, a contradiction. Therefore, $P$ is not in ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$.\n\n(c) Assuming that ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}} \\subseteq {\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$, we plan to derive that $\\mathrm{DSTCON}\\in{\\mathrm{NC}^{ {1} }}$. Consider {\\sc Max UB-Vertex} and set it as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Proposition \\ref{max-bvertex-complete} shows its ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. From our assumption, we derive that this problem is also in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$.\nWith a suitable constant $r>1$, take an NC$^{1}$ circuit $C$ producing  $r$-approximate solutions to {\\sc Max UB-Vertex}.\nLet us  construct another NC$^{1}$ circuit solving DSTCON. Given any instance $(G,s,t)$ to DSTCON with $G=(V,E)$, we define a weight function $w$ as $w(t)=r+1$ and $w(v)=1$ for all $v\\in V-\\{t\\}$. Here, we consider $x=(G,s,w)$. We run $C$ to find an $r$-approximate solution, say, $u\\in V$.  If $(G,s,t)\\in \\mathrm{DSTCON}$, then, since $\\frac{m_0^*(x)}{m_0(x,u)}\\leq r$, it follows that $m_0(x,u)\\geq \\frac{m_0^*(x)}{r}=1+\\frac{1}{r}>1$. Since $w(u)>1$, $u$ must be $t$. If $(G,s,t)\\notin\\mathrm{DSTCON}$, then we can conclude that $u$ cannot be $t$. Hence, $\\mathrm{DSTCON}$ must be in ${\\mathrm{NC}^{ {1} }}$, as requested.\n\n(3) Note that ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\subseteq {\\mathrm{PO}}_{{\\mathrm{NPO}}}$ and ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\subseteq {\\mathrm{LSAS}}_{{\\mathrm{NPO}}}\\subseteq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$. In Proposition \\ref{equivalent-relation}(3), we have already proven that ${\\mathrm{L}}\\neq{\\mathrm{P}}$ iff ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\neq{\\mathrm{PO}}_{{\\mathrm{NPO}}}$. It still remains to show that ${\\mathrm{L}}\\neq{\\mathrm{P}}$ implies (a) ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\nsubseteq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$, (b)  ${\\mathrm{LO}}_{{\\mathrm{NPO}}}\\neq {\\mathrm{LSAS}}_{{\\mathrm{NPO}}}$, and (c) ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}\\neq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$.\nNote that ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\nsubseteq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$ yields both ${\\mathrm{APXL}}_{{\\mathrm{NPO}}}\\neq{\\mathrm{PO}}_{{\\mathrm{NPO}}}$ and ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}\\neq{\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\n(a) For this claim, it suffices to prove the following statement: (*)  ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\subseteq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$ implies ${\\mathrm{L}}={\\mathrm{P}}$.\nLet us assume that ${\\mathrm{PO}}_{{\\mathrm{NPO}}}\\subseteq {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$. Our goal is to show that $\\mathrm{CVP}$ belongs to ${\\mathrm{L}}$ since ${\\mathrm{L}}={\\mathrm{P}}$ follows from the $\\leq_{m}^{{\\mathrm{NC}^{ {1} }}}$-completeness of $\\mathrm{CVP}$ for ${\\mathrm{P}}$.\nConsider $\\text{\\sc MinCVP}$, which is closely related to the decision problem {\\sc CVP}, defined in Appendix.\nNotice that $\\text{\\sc MinCVP}$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for  ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ by Lemma \\ref{MinCVP-complete}.  Let $\\text{\\sc MinCVP} = (I,SOL,m,\\text{\\sc min})$. Since $\\mathrm{MinCVP}\\in {\\mathrm{APXL}}_{{\\mathrm{NPO}}}$ by our assumption, we can take a function $h\\in{\\mathrm{FL}}$ and a constant $\\gamma\\geq 1$ such that $R(x,h(x))\\leq \\gamma$ for all $x\\in I$, where $R$ indicates the performance ratio for {\\sc MinCVP}. Take an integer $k\\geq1$ for which $\\gamma<2^k$.\n\nLet $z={\\langle {C_x} \\rangle}$ be an instance of $\\mathrm{CVP}$. This implies that  ${\\langle {C_x} \\rangle}\\in\\mathrm{CVP}$ iff $1\\in SOL^*({\\langle {C_x,1^{n},1} \\rangle})$, where $n=|x|$.\nWe define $C'_x$ as follows. For the outcome $z$ of $C_x$, $C'_x$ outputs $0^k$ if $z=0$; $10^{k-1}$ otherwise. The construction of $C'_x$ can be done using log space. Given any outcome $z'$ of $C'_x$, $rep(z')$ is either $0$ or $2^{k}$. Let $w={\\langle {C'_x,1^n,1^k} \\rangle}$, an instance to {\\sc MinCVP}. It holds that, since $R(w,h(w))\\leq \\gamma$, $C_x$ outputs $0$ (resp., $1$) iff $h(w)=0^k$ (resp., $10^{k-1}$). Hence, we can determine whether ${\\langle {C_x} \\rangle}\\in \\mathrm{CVP}$ simply by constructing $w={\\langle {C'_x,1^n,1^k} \\rangle}$ and checking whether $h(w)=10^{k-1}$. This implies that $\\mathrm{CVP}$ belongs to ${\\mathrm{L}}$, as requested.\n\n(b) In this case, we define $\\text{\\sc MinASCVP}=(I,SOL,m,\\text{\\sc min})$ as follows.\nDefine $I=\\{(C,x)\\mid C: \\text{circuit with $|x|$ inputs and $1$ output},\\, x\\in \\{0,1\\}^*\\}$ and $SOL(C,x)=\\{y1^{|x|}\\mid y\\geq C(x)\\}$ for any $(C,x)\\in I$.\nGiven any $(C,x)\\in I$ and any $y\\in SOL(C,x)$, define $m((C,x),y)= 2^{|x|+1}+C(x)$. Note that $\\max_{y\\in SOL(C,x)}\\{m((C,x),y)\\}\\leq (1+\\frac{1}{2^{n+1}})\\min_{y\\in SOL(C,x)}\\{m((C,x),y)\\}$. Consider the following algorithm $M$: on input $((C,x),r)$ with $r\\in{\\mathbb{Q}}^{>1}$, if $n+1>\\log(\\frac{1}{r-1})$, then $M$ outputs $1^{|x|+1}$; otherwise, $M$ computes $C(x)$ by brute force and outputs $C(x)1^{|x|}$. Since $C(x)$ can be computed deterministically within $O(n^2)$ steps, $M$ requires the total tape space of $O(\\log{n})+O(n^2)$, which equals $O(\\log{n})+O(\\log^2(\\frac{1}{r-1}))$.\nI also follows from  $1+2^{-(n+1)}<r$ that $M$ produces an $r$-approximate solution to $x$.\nHence, {\\sc MinASCVP} belongs to ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}$.\n\nNext, we claim that $\\text{\\sc MinASCVP}\\in{\\mathrm{LO}}_{{\\mathrm{NPO}}}$ implies $\\mathrm{CVP}\\in{\\mathrm{L}}$. Assume that $\\text{\\sc MinASCVP}\\in{\\mathrm{LO}}_{{\\mathrm{NPO}}}$ and take a log-space deterministic Turing machine $N$ that solves {\\sc MinASCVP}. The following log-space algorithm then solves CVP. On input $z=(C,x)$, run $N$ on $z$ and output $y$ if $N$'s output is of the form $y1^{|x|}$. It is easy to verify that this algorithm is correct. Thus, CVP is in ${\\mathrm{L}}$. This contradicts our assumption that ${\\mathrm{L}}\\neq{\\mathrm{P}}$ since CVP is ${\\mathrm{P}}$-complete. Therefore, {\\sc MinASCVP} does not belong to ${\\mathrm{LO}}_{{\\mathrm{NPO}}}$.\n\n(c) Toward a contradiction, we assume that ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}={\\mathrm{APXL}}_{{\\mathrm{NPO}}}$.\nHere, we define a new problem $\\text{\\sc Min1CVP}=(I,SOL,m,\\text{\\sc min})$ by setting $I=\\{(C,x)\\mid C:\\,\\text{circuit with $|x|$ inputs and 1 output},\\, x\\in\\{0,1\\}^{+}\\}$, $SOL(C,x)=\\{y1^{|x|}\\mid y\\geq C(x)\\}$,  and $m((C,x),y1^{|x|}) = rep_{(+)}(y1^{|x|})$ for any $(C,x)\\in I$ and any $y1^{|x|}\\in SOL(C,x)$.\n\nTo claim that {\\sc Min1CVP} belongs to ${\\mathrm{APXL}}_{{\\mathrm{NPO}}}$, consider the following  log-space deterministic Turing machine $M$: on each input $(C,x)\\in I$, $M$  produces $1^{|x|+1}$ on its output tape. Since $rep_{(+)}(M(C,x))=2^{|x|+2}$ and $rep_{(+)}(y1^{|x|})\\in\\{2^{|x|+2},2^{|x|+1}\\}$, $M(C,x)$ is a $2$-approximate solution to $(C,x)$. Thus, {\\sc Min1CVP} is in ${\\mathrm{APXL}}_{{\\mathrm{NPO}}}$.\nBy our assumption, {\\sc Min1CVP} is also in ${\\mathrm{LSAS}}_{{\\mathrm{NPO}}}$. There is a log-space deterministic Turing machine $N$ that produces $3/2$-approximate solutions to {\\sc Min1CVP}. From this $N$, we define $N'$ as follows. On input $z=(C,x)$, $N'$ first  runs $N$, and outputs $1$ if $N(C,x)$ equals $1^{|x|+1}$, and outputs $0$ otherwise. Clearly, $N'$ solves $\\mathrm{CVP}$. Hence, $\\mathrm{CVP}$ belongs to ${\\mathrm{L}}$, a contradiction against ${\\mathrm{L}}\\neq{\\mathrm{P}}$ because $\\mathrm{CVP}$ is $\\leq_{m}^{{\\mathrm{L}}}$-complete for ${\\mathrm{P}}$.\n\\end{proofof}\n\n\n\nTo close this section, we wish to demonstrate further separations with {\\em no unproven assumption}. Notice that ${\\mathrm{AC}^{ {0} }}$ is known to be {\\em properly} included inside ${\\mathrm{NC}^{ {1} }}$ because the parity function, which is in ${\\mathrm{NC}^{ {1} }}$,  requires at least non-uniform constant-depth circuits of super-polynomial size \\cite{Ajt83,FSS84}.\nThe following proof relies on this fact.\n\n\\begin{theorem}\\label{AC0-separation}\n${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\neq {\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\neq {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}} \\neq {\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.\n\\end{theorem}\n\nConcerning ${\\mathrm{NLO}}$ and ${\\mathrm{NPO}}$,  most optimization and approximation classes enjoy the following {\\em upward separation property}: if ${\\cal D}^{(1)}_{{\\mathrm{NLO}}}\\neq {\\cal D}^{(2)}_{{\\mathrm{NLO}}}$, then ${\\cal D}^{(1)}_{{\\mathrm{NPO}}}\\neq {\\cal D}^{(2)}_{{\\mathrm{NPO}}}$. Theorem \\ref{AC0-separation} thus yields the separations ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NPO}}}\\neq {\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NPO}}} \\neq{\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NPO}}} \\neq {\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NPO}}}$.\n\nTo derive Theorem \\ref{AC0-separation}, we shall use the {\\em parity function} $\\pi$, which is defined as $\\pi(x_1,x_2,\\ldots,x_n) = x_1\\oplus x_2\\oplus \\cdots \\oplus x_n$, where each variable  $x_i$ takes a binary value.\nThe fact that the parity function is out of ${\\mathrm{AC}^{ {0} }}$ was first claimed by  Furst, Saxe, and Sipser \\cite{FSS84} as well as Ajtai \\cite{Ajt83}.\nUsing this $\\pi$, we further define\n$\\pi^*(x_{11},\\ldots,x_{1n},x_{21},\\ldots,x_{2n},\\ldots,x_{n1},\\ldots,x_{nn})$ to be the $n$-bit string $\\pi(x_{11},\\ldots,x_{1n}) \\pi(x_{21},\\ldots,x_{2n})\\cdots \\pi(x_{n1},\\ldots,x_{nn})$. It is not difficult to show that $\\pi^*$ is in ${\\mathrm{FNC}^{ {1} }}$ but not in ${\\mathrm{FAC}^{ {0} }}$ because $\\pi$ resides in the difference ${\\mathrm{NC}^{ {1} }}-{\\mathrm{AC}^{ {0} }}$.\n\n\n\\begin{proofof}{Theorem \\ref{AC0-separation}}\nThe subsequent proof consists of three parts: (1) ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\nsubseteq {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$,  (2) ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\neq {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$, and (3) ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\neq {\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$.\n\n(1) For this separation,  we consider a minimization problem $\\text{\\sc Min m-Parity}=(I,SOL,m,\\text{\\sc min})$ defined as follows.\nLet $I = \\bigcup_{n\\in{\\mathbb{N}}^{+}}\\{0,1\\}^{n^2}$ and let $SOL(x) = \\{y\\in\\{0,1\\}^{|x|} \\mid  rep_{+}(y)\\geq rep_{+}(\\pi^*(x))\\}$ for each $x\\in I$ with  $|x|=n^2$. Let $m(x,y) = rep_{+}(y)$ for all $y\\in SOL(x)$. Clearly, $I$ is in ${\\mathrm{L}}$, $I\\circ SOL$ is in ${\\mathrm{auxL}}$, and $m$ is in ${\\mathrm{auxFL}}$. This indicates that {\\sc Min m-Parity} is an ${\\mathrm{NLO}}$ problem.\nNext, we shall  argue that $\\text{\\sc Min m-Parity}\\in{\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}$.  Note that, for every $x\\in I$, $SOL^*(x)=\\{\\pi^*(x)\\}$ holds. Since $\\pi^*$ is in ${\\mathrm{FNC}^{ {1} }}$, it follows that $\\text{\\sc Min m-Parity}$ is ${\\mathrm{NC}^{ {1} }}$-solvable.\n\nLet us prove that $\\text{\\sc Min m-Parity}\\not\\in{\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$. To lead to a contradiction, we assume otherwise; that is, $\\text{\\sc Min m-Parity}$ is in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$. There exist a constant $\\gamma>1$ and a uniform family $\\{C_n\\}_{n\\in{\\mathbb{N}}^{+}}$ of ${\\mathrm{AC}^{ {0} }}$-circuits such that,  for every $x\\in I$,  $C_{|x|}(x)$ computes a string $y$ in $SOL(x)$ satisfying that (*)\n$rep_{+}(y)/\\gamma \\leq rep_{+}(\\pi^*(x)) \\leq rep_{+}(y)$. Take any number $n$ satisfying $2^n > \\gamma$ and any string $x\\in\\{0,1\\}^n$. Consider $x^n$ and define  $y = C_{n^2}(x^n)$.\nIf $\\pi(x)=1$, then we obtain $rep_{+}(\\pi^*(x^n))=2^{n+1}$.  Since $|y|=n$ and $y\\in SOL(x)$, it must hold that $rep_{+}(y)=2^{n+1}$, implying $y=1^n$. By contrast, if $\\pi(x)=0$, then we obtain $rep_{+}(\\pi^*(x^n))=1$. By Condition (*), it follows that $1\\leq rep_{+}(y)\\leq \\gamma$. Since $\\gamma<2^n$, $y$ must have the form $0z$ for a certain string $z$ in $\\{0,1\\}^{|x|-1}$. As a consequence, $\\pi(x)$ equals the first bit of $y$. This gives an ${\\mathrm{AC}^{ {0} }}$-circuit that computes $\\pi$.\nThis is a contradiction against the fact that $\\pi$ is not in ${\\mathrm{AC}^{ {0} }}$. Therefore, $\\text{\\sc Min m-Parity}$ is not in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\n(2) Based on the parity function $\\pi$, we define a simple minimization problem, {\\sc Min 1-Parity}, which is somewhat similar to {\\sc Min m-Parity} defined in the proof of (1). {\\sc Min 1-Parity} takes an instance $x\\in\\{0,1\\}^n$ and finds a solution $y\\in\\{0,1\\}$ such that $rep(1y)\\geq rep(1\\pi(x))$ with the measure function $m(x,y) = rep(1y)$.\n\nHere, we claim that {\\sc Min 1-Parity} is in ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$. Consider the following ${\\mathrm{AC}^{ {0} }}$-circuit $C$: on input $x$, output $y_1=1$. Since $rep(1y_1)=3$ and $rep(1\\pi(x))\\in\\{2,3\\}$, it follows that $rep(1y_1)/2 \\leq rep(1\\pi(x))\\leq rep(1y_1)$. Thus, $C$ is a $2$-approximate algorithm for {\\sc Min 1-Parity}. We then conclude that {\\sc Min 1-Parity} belongs to ${\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$.\n\nAssume that ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}= {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}}$. This means that {\\sc Min 1-Parity} is in ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$. Take a uniform family $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ of ${\\mathrm{AC}^{ {0} }}$-circuits such that, for any $x\\in\\{0,1\\}^*$, $C_{|x|}(x)$ outputs a 5/4-approximate solution $y$. For simplicity, write $y$ for $C_{|x|}(x)$.\nConsider the case of $\\pi(x)=0$. If $y=1$, then $rep(1y)=3$. This contradicts the inequalities: $(4/5)rep(1y)\\leq rep(1\\pi(x))\\leq rep(1y)$. This implies that $y$ is $0$. In the case of $\\pi(x)=0$, when $y=0$, it does not hold that $(4/5)rep(1y)\\leq rep(1\\pi(x))\\leq rep(1y)$. Hence, $y$ must be $1$.\nTherefore, $\\{C_n\\}_{n\\in{\\mathbb{N}}}$ computes $\\pi$ correctly. This implies that $\\pi$ is actually in ${\\mathrm{AC}^{ {0} }}$, a contradiction against $\\pi\\notin{\\mathrm{AC}^{ {0} }}$. Therefore, ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\neq {\\mathrm{APXAC}^{ {0} }}_{{\\mathrm{NLO}}|}$ follows.\n\n(3) We define another minimization problem {\\sc Min Bit-Parity}, which belongs to ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$ by setting $I=\\bigcup_{n\\in{\\mathbb{N}}^{+}}\\{0,1\\}^{n}$, $SOL(x) =\\{ y\\in \\{0,1\\}^{|x|}\\mid rep(y)\\geq rep(1^{|x|}\\pi(x))\\}$ for each $x\\in I$, and $m(x,y) = \\max\\{1,rep(y)\\}$ for $y\\in SOL(x)$.\n\nConsider a circuit $C_n$ that takes input $(x,r)$ with $n=|x|$ and, if $r\\geq 1+\\frac{1}{2^{n+1}-2}$, then  produces $1^{n}0$, and  if $1<r< 1+\\frac{1}{2^{n+1}-2}$, then  computes $\\pi(x)$ exactly and outputs $1^n\\pi(x)$. Note that the value $\\pi(x)$ can be exactly computed by an appropriately chosen circuit, say, $D$ of size $n^{O(1)}$ and depth $O(\\log{n})$. When $1<r< 1+\\frac{1}{2^{n+1}-2}$, since this is equivalent to $n<\\log(\\frac{2r-1}{r-1})-1$ and $r$ is treated as a constant, $D$ has depth $O(\\log\\log(\\frac{2r-1}{r-1}))$, which is bounded by a certain constant. Thus, $C_n$ is an ${\\mathrm{AC}^{ {0} }}$-circuit parameterized by $r$. Let $y=C_n(x,r)$. In the case of $r\\geq 1+\\frac{1}{2^{n+1}-2}$, we obtain $rep(y)=2^{n+1}-1$. When $\\pi(x)=0$, since $rep(1^n\\pi(x))=2^{n+1}-2$, it follows that $rep(y)/r = 2^{n+1}-2\\leq rep(1^n\\pi(x))\\leq rep(y)$. In contrast, when $\\pi(x)=1$, clearly we obtain $rep(y)/r<rep(1^n\\pi(x))\\leq rep(y)$. This implies that $C_n$ computes  $r$-approximate solutions.\n\nToward a contradiction, we assume that ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}={\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$. This implies that {\\sc Min Bit-Parity} is in ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$. Let $\\text{\\sc Min Bit-Parity} =(I,SOL,m,\\text{\\sc min})$.\nNext, we argue that $\\pi\\in{\\mathrm{AC}^{ {0} }}$, leading to a clear contradiction. Since $\\text{\\sc Min Bit-Parity}\\in{\\mathrm{AC}^{ {0} }\\mathrm{AS}}$, there is a uniform family $\\{C'_n\\}_{n\\in{\\mathbb{N}}}$ of ${\\mathrm{AC}^{ {0} }}$-circuits solving {\\sc Min Bit-Parity}. Since $SOL(x)=\\{1^{|x|}\\pi(x)\\}$, it follows that $C'_n(x)=1^{|x|}\\pi(x)$. From $C'_n$, we can design another ${\\mathrm{AC}^{ {0} }}$-circuit that computes $\\pi(x)$. Thus, we obtain the desired conclusion of $\\pi\\in{\\mathrm{AC}^{ {0} }}$. This yields the separation between ${\\mathrm{AC}^{ {0} }\\mathrm{O}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{AC}^{ {0} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$.\n\\end{proofof}\n\n\n\\section{Discussions and Future Research Directions}\n\nWe have refined the existing framework of combinatorial optimization problems in hope of providing a useful means of classifying various optimization problems lying inside the ${\\mathrm{P}}$-solvable class ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$; in particular, we have been focused on NL optimization problems or ${\\mathrm{NLO}}$ problems, following early work of \\'{A}lvarez and Jenner \\cite{AJ93,AJ95} and Tantau \\cite{Tan07}. In a course of\nour exploring study on such optimization problems, unfortunately, we have left numerous fundamental issues unsettled. For an advance of our knowledge, they definitely require reasonable answers and explanations. As a quick guide to those pending issues, we want to shed clear light on some of the challenging issues arising naturally in the course of our study.\n\n\\begin{enumerate}{\\vspace*{ {-1} mm}}\n\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {[Class Separations and Inclusions]} We have introduced numerous classes of refined optimization problems in Section \\ref{sec:comb-OPs} but a few classes are successfully proven to be different (cf. Theorem \\ref{AC0-separation}). One of our ultimate goals is to give proofs for separations of other important optimization and approximation classes. As seen in Section \\ref{sec:complexity-OP}, those separations are closely tied up to long-standing open questions regarding the classes of their associated decision problems, and thus the separations of refined classes immediately lead to definitive answers to such open questions. Beside the separation issues, there are important unsettled questions concerning the inclusion relationships among newly refined classes. For example, contrary to the well-known inclusion ${\\mathrm{NC}^{ {1} }}\\subseteq {\\mathrm{L}}$, we suspect that  ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\nsubseteq {\\mathrm{LO}}_{{\\mathrm{NLO}}}$, ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\nsubseteq{\\mathrm{LO}}_{{\\mathrm{NLO}}}$, and ${\\mathrm{APXNC}^{ {1} }}\\nsubseteq{\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$.\n\n\\item {[Completeness Issues]} (1) In this paper, a number of optimization  problems have been demonstrated to be complete for certain optimization and approximation classes within ${\\mathrm{NLO}}$. It is imperative to find  more natural and useful optimization problems and prove them to be complete for target classes, in particular, ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ and ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n    (2) In Theorem \\ref{Min-Path-complete} and Corollary \\ref{Max-Path-Weight-complete}, we have shown that both ${\\mathrm{MaxNL}}$ and ${\\mathrm{MinNL}}$ contain ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problems. Unlike ${\\mathrm{NPO}}$, we do not know whether ${\\mathrm{NLO}}$ ($={\\mathrm{MaxNL}}\\cup {\\mathrm{MinNL}}$) contains  ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problems unless ${\\mathrm{auxFL}}$ is closed under division (cf. Proposition \\ref{Min-Path-in-NLO}. Can we remove this closure property? Similarly, are there any complete problem in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ without any assumption? Moreover, several optimization problems discussed in this paper have not proven to be complete. Those pending problems include {\\sc Max $\\lambda$-DFA}, {\\sc Max B-Vertex}, and {\\sc Min UPath-Weight}.\n\n    (3) There is a large collection of works presenting L-complete and NL-complete decision  problems \\cite{AG00,CM87,JLL76,Jon75}. In most cases, it is relatively straightforward to turn those problems into their associated NLO problems. Which of them are actually complete for refined optimization and approximation classes under suitably chosen reductions?\n\n\\item {[Polynomially-Bounded Problems]} The relationship between NL and NLO appears to be quite different from the relationship between NP and NPO. Such difference comes primarily from an architectural limitation of log-space (auxiliary)  Turing machines. In particular, polynomially-bounded optimization problems are quite special for log-space computation. We have proven that ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}={\\mathrm{LO}}_{{\\mathrm{NLO}}}$ iff ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} ={\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ (Proposition \\ref{equivalent-relation}(2)) but we do not know whether (i) ${\\mathrm{LO}}_{{\\mathrm{NLO}}}={\\mathrm{NLO}}$ iff ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}={\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ and (ii) ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}={\\mathrm{NLO}}$ iff ${\\mathrm{NC}^{ {1} }\\mathrm{O}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}} ={\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$? Prove or disprove these equivalences (i)--(ii).\n\n\\item {[Further Refinement of Approximability]} Beyond ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, Tantau \\cite{Tan07} discussed classes of NLO problems whose $n^{O(1)}$-approximate (as well as $2^{n^{O(1)}}$-approximate) solutions are computed by log-space deterministic Turing machines. We briefly call them $\\mathrm{poly\\mbox{-}}{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and $\\mathrm{exp\\mbox{-}}{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. Those two classes fill a seemingly wide gap between ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{NLO}}$. For example, we have already mentioned in Section \\ref{sec:general-complete} and \\ref{sec:graph-problems} that {\\sc Min UPath-Weight} and {\\sc Max B-Vertex} belong to $\\mathrm{poly\\mbox{-}}{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$. Find more natural complete problems falling into these special approximation classes. These classes can be further expanded into a more general class denoted by $\\ell(n)\\mbox{-}{\\mathrm{APXL}}_{{\\mathrm{NLO}}}$, where $\\ell(n)$ refers to $\\ell(n)$-approximate solutions. Explore its features by finding natural problems inside it.\n\n\\item {[Heuristic and Probabilistic Approaches]} This paper has aimed at cultivating a basic theory of NLO problems with a major focus on complete problems. As a consequence, this paper has neglected any practical heuristic approach toward the NLO problems. The next possible step is to find such heuristic approaches to real-life NLO problems. Study also the power of probabilistic algorithms to solve NLO problems.\n\n\\item {[Help by Advice]} In 1980s, Karp and Lipton \\cite{KL82} studied a notion of advice, which is an external source used to enhance any underlying Turing machines, and they introduced two major complexity classes, ${\\mathrm{P}}/\\log$ and ${\\mathrm{P}}/{\\mathrm{poly}}$, where ``$\\log$'' and ``${\\mathrm{poly}}$'' refer to advice sizes of $O(\\log{n})$ and $n^{O(1)}$, respectively. In analogy with these advised classes, it may be possible to formulate ${\\mathrm{NLO}}/\\log$ and ${\\mathrm{NLO}}/{\\mathrm{poly}}$ containing advised optimization problems. Explore the properties of those advised classes and show the separations among them.\n\n\\item {[Fixed Parameter Complexity]} Recall that optimization problems in ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$ and ${\\mathrm{NC}^{ {1} }\\mathrm{AS}}_{{\\mathrm{NLO}}}$ admit certain approximation schemes, which are algorithms whose  resources (i.e., tape space or circuit size) described in terms of {\\em fixed parameters}. NLO problems whose log-space approximation schemes have the performance ratios of, e.g.,  $k+\\log{n}$ and $2^k\\log{n}$, are treated equally inside ${\\mathrm{LSAS}}_{{\\mathrm{NLO}}}$, but those schemes behave differently in practice. Therefore, it is more practical  to specify those fixed parameters and refine those classes.\n\n\\item {[More Classes Inside ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$]} (1: Classes inside NLO) If the number of solutions of a given problem is relatively small, can we use this fewness information to find an optimal solution much more efficiently? Similarly to $\\mathrm{FewP}$ (few polynomial time), we may define its log-space optimization counterpart, denoted by $\\mathrm{FewLO}$, which is a class of NLO problems whose  sets of feasible solutions have sizes bounded from above by certain polynomials in $|x|$.\n    Study the computational complexity of optimization problems inside  $\\mathrm{FewLO}$.\n\n    (2: Classes between ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ and NLO) A focal point of the current paper is optimization problems inside NLO. It still remains to fill the gap between ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$ and NLO. Uniform circuit families, such as ${\\mathrm{NC}^{ {k} }}$, ${\\mathrm{AC}^{ {k} }}$, ${\\mathrm{SAC}^{ {k} }}$, and $\\mathrm{TC}^{k}$  for $k\\geq1$, may be used to form optimization/approximation classes beyond NLO. For example, SAC$^{1}$-circuits may introduce optimization and approximation classes, say, $\\mathrm{SAC}^{1}\\mathrm{O}_{{\\mathrm{NPO}}}$ and $\\mathrm{APXSAC}^{1}_{{\\mathrm{NPO}}}$. Find complete problems for those new classes lying above NLO. Concerning to the power of ${\\mathrm{SAC}^{ {1} }}$, it is also important to determine whether  ${\\mathrm{NLO}}$ coincides with $\\mathrm{SAC}^{1}\\mathrm{O}_{{\\mathrm{NLO}}}$, expanding Lemma \\ref{char-with-AC1}(1).\n\n\\item {[Other Categories of Optimization Problems]} Lately, Yamakami \\cite{Yam14} studied OptCFL, which is a CFL analogue of Krentel's OptP \\cite{Kre88} and \\`{A}lvarez and Jenner's OptL \\cite{AJ93,AJ95}, where CFL stands for {\\em context-free languages}. This fact suggests a possibility of introducing CFL-based optimization problems under the current framework of refined optimization problems. Cultivate a theory based on those weak optimization problems.\n\\end{enumerate}\n\n\n\n\n\\section*{Appendix: Proof of Proposition \\ref{min-st-cut-is-po}}\n\nProposition \\ref{min-st-cut-is-po} states the completeness of {\\sc Min Weight-st-Cut} for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$. Here, we restate the proposition.\n\n{\\medskip}\n{\\noindent}{\\bf {\\em Proposition \\ref{min-st-cut-is-po} (again).}}\n{\\it $\\text{\\sc Min st{-}Cut}$ (on weighted directed graphs) is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n}\n{\\medskip}\n\nNote that an unweighted version of {\\sc Min Weight-st-Cut} is also known as the {\\em edge connectivity problem}. Gabow \\cite{Gab91} gave a sequential algorithm of finding the edge connectivity $c$  of a (directed or undirected) graph of $n$ vertices and $m$ edges in time $O(cn \\log (n^2/m))$.\n\nFor readability, we have left Proposition \\ref{min-st-cut-is-po} unproven in Section \\ref{sec:why-NC1}. In this appendix, we shall give the proof of Proposition \\ref{min-st-cut-is-po}  for completeness.\nAs noted in Section \\ref{sec:why-NC1}, this result is drawn from the ${\\mathrm{P}}$-completeness proof of Goldschlager {\\textrm{et al.}}~\\cite{GSS82} for a decision version of the maximum $s$-$t$ flow problem.\n\nTo prove the proposition, we first look into the decision problem $\\mathrm{CVP}$ (Circuit Value Problem) into a minimization problem in ${\\mathrm{PO}}_{{\\mathrm{NLO}}}$. Given each   binary string $x$, the notation $rep(x)$ denotes one plus the natural number represented in binary by $x$.\n\n{\\medskip}\n{\\sc Minimum Circuit Value Problem} ({\\sc MinCVP}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\\item {\\sc instance:} ${\\langle {C_x,1^n,1^k} \\rangle}$ with a circuit $C_x$ of $n$ inputs and $k$ outputs with AND, OR, and NOT gates for an input $x\\in\\{0,1\\}^n$ (which must be specified in the description of the circuit).\n\n\\item {\\sc Solution:} a binary string $y$ of length $k$ such that $rep(y) \\geq rep(z)$, where $z$ is the outcome of the circuit $C_x$.\n\n\\item {\\sc Measure:} the number $rep(y)$.\n\\end{itemize}\n\nWe further demand a circuit to satisfy the following conditions: (i) it is monotone (i.e., using only $AND$ and $OR$ gates), (ii) each input has fan-out at most $1$, (iii) each gate has fan-out at most $2$, and (iv) the top (i.e., root) gate is an $OR$ gate. With those conditions, we obtain the {\\sc Minimum Monotone Circuit Value 2 Problem} ({\\sc MinMCV2}) defined similarly to {\\sc MinCVP}.\nThe decision version of {\\sc MinMCV2} was shown to be $\\leq_{m}^{{\\mathrm{L}}}$-complete for ${\\mathrm{P}}$ \\cite{GSS82}.\n\nHere, we establish the ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc MinCVP} and {\\sc MinMCV2}.\n\n\\begin{lemma}\\label{MinCVP-complete}\n$\\text{\\sc MinCVP}$ and $\\text{\\sc MinMCV2}$  are ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\\end{lemma}\n\n\\begin{proof}\nWe first show that {\\sc MinCVP} is in ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.  Given an instance ${\\langle {C_x,1^n,1^k} \\rangle}$ of {\\sc MinCVP}, it is possible to calculate the outcome of $C_x$ by evaluating each gate of $C_x$ one by one within polynomial time. Hence, {\\sc MinCVP} is in ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\n\nNext, we shall  show the ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of $\\mathrm{MinCVP}$ for ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$. Let $P=(I,SOL,m,goal)$ be any optimization problem in ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$. Let $x$ be any instance of $P$. Since $P$ is ${\\mathrm{P}}$-solvable, we take a function $h\\in{\\mathrm{FP}}$ satisfying $h(x)\\in SOL^*(x)$ for all $x\\in (I\\circ SOL)^{\\exists}$. For each $x$, we pad extra $k'_x$ zeros so that $k'_x\\geq0$ and $|h(x)10^{k'_x}|=k(n)$ for a certain absolute polynomial $k$. For convenience, we set $h'(x)=h(x)10^{k'_x}$.  It is known that any function in ${\\mathrm{FP}}$ can be computed by a certain ${\\mathrm{NC}^{ {1} }}$-uniform family of polynomial-size Boolean circuits (see, e.g., \\cite{DK00}).\n\nLet us define an ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g)$ from $P$ to $\\mathrm{MinCVP}$ as follows.\nSince $h'\\in{\\mathrm{FP}}$, take  an ${\\mathrm{NC}^{ {1} }}$-uniform family $\\{C_{n}\\}_{n\\in{\\mathbb{N}}^{+}}$  of polynomial-size Boolean circuits $C_n$ of $n$ inputs and $k(n)$ outputs that computes $h'$. Define $f(x)={\\langle {C_x,1^{|x|},1^{k(|x|)}} \\rangle}$, where $C_x$ is obtained from $C_{|x|}$ by replacing $|x|$ input variables with $|x|$ constant bits $x\\in\\{0,1\\}^{|x|}$.\nSince $\\{C_n\\}_{n\\in{\\mathbb{N}}^{+}}$ is ${\\mathrm{NC}^{ {1} }}$-uniform, there is an ${\\mathrm{NC}^{ {1} }}$-circuit family to construct each $C_x$. Hence, $f$ must be in ${\\mathrm{FNC}^{ {1} }}$. If $y$ is of the form $z10^{k'}$, then we write $\\tilde{y}$ to denote $z$. Define $g(x,y) = \\tilde{y}$. It is not difficult to show that if $y=C_x$ then $g(x,y) = \\tilde{y} = h(x)$. Hence, $P$ is ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reducible to $\\mathrm{MinCVP}$.  The transformation $f$ is computed by a certain family of ${\\mathrm{NC}^{ {1} }}$-circuits.\n\nThe case for {\\sc MinMCV2} is similar, however, by using restricted monotone circuits for $h'$.\n\\end{proof}\n\n\n\nUsing Lemma \\ref{MinCVP-complete}, we want to derive Proposition \\ref{min-st-cut-is-po}.\n\n\\begin{proofof}{Proposition \\ref{min-st-cut-is-po}}\nIt is known that {\\sc Min st-cut} is in ${\\mathrm{PO}}_{{\\mathrm{NPO}}}$.\nThus, it suffices to reduce $\\text{\\sc MinMCV2}$ to $\\text{\\sc Min st{-}Cut}$ by an appropriate ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g)$. For convenience, we set $\\text{\\sc MinMCV2}=(I_1,SOL_1,m_1,\\text{\\sc min})$. \nLet ${\\langle {C_x,1^n,1^k} \\rangle}$ be any instance of MinMCV2, where $C_x$ is expressed as a sequence $(\\alpha_{n},\\alpha_{n-1},\\ldots,\\alpha_0)$.\n\nFirst, we consider the case where $k=1$. The function $f$ is of the form  $f({\\langle {C_x,1^n,1^k} \\rangle})={\\langle {G,s,t,c} \\rangle}$, where $G=(V,E)$ is defined as follows. Let $V=\\{i\\mid 0\\leq i\\leq n\\}\\cup\\{s,t\\}$. Let $c(s,i)=b\\cdot 2^i$ if $\\alpha_i$ is an input $b\\in\\{0,1\\}$. If $\\alpha_i$ is of the form $AND(j,k)$ or $OR(j,k)$, then let $c(j,i)=2^{j}$ and $c(k,i)=2^k$. Moreover, when  $\\alpha_i$ is $AND(j,k)$, let $c(i,t)=2^j+2^k-d\\cdot 2^i$; when $\\alpha_i$ is $OR(j,k)$, let $c(i,s)=2^j+2^k-d\\cdot 2^i$, where $d$ is the fan-out of $\\alpha_i$. Now, let $E=\\{(i,j)\\mid c(i,j) \\text{ is defined }\\}$.  Note that $c(i,j)\\leq 2^{n+1}\\leq 2^{|V|}$ for any $(i,j)\\in E$. For any $s$-$t$ cut $(S'_0,S'_1)$, we set $g({\\langle {G,s,t,c} \\rangle},(S'_0,S'_1)) = 1$ if the cut capacity of $(S'_0,S'_1)$ is odd; $0$ otherwise. Note that $g$ can be computed by certain ${\\mathrm{NC}^{ {1} }}$-circuits. Let $x={\\langle {C_x,1^n,1} \\rangle}$. If $(S'_0,S'_1)$ is an $s$-$t$ cut, then $m_1(x,g(x,(S'_0,S'_1)))=m_1^*(x)=1$.  \n\nHere, we define a special $s$-$t$ cut $(S_0,S_1)$ inductively. The sink $t$ and all vertices corresponding to inputs with $0$ given to $C_x$ are in $S_1$. For a vertex $i$ corresponding to a gate $AND$ or $OR$, if this gate outputs $0$, then the vertex $i$ is in $S_1$. The set $S_0$ is defined as $S_0 = V-S_1$. It is possible to verify that $C_n$ outputs $1$ iff the cut capacity of $(S_0,S_1)$ is odd. Moreover, we can prove that $(S_0,S_1)$ is minimal. \n\nLet $k\\geq2$. Given $C_x$ with $n$ input bits and $k$ output bits, we first make $k$ copies $C_{x,1},C_{x,2},\\ldots,C_{x,k}$ of $C_x$ and, for each $C_{x,i}$ ($i\\in[k]$), we add the following new nodes: an input $\\beta$ having $0$, two gates $AND(\\beta,\\gamma_1,\\gamma_2,\\ldots,\\gamma_{i-1}, \\gamma_{i+1},\\ldots,\\gamma_{k})$ and $OR(\\gamma_i,\\zeta)$, where $\\gamma_j$ corresponds to the $j$th output bit of $C_x$ and $\\zeta$ is the index corresponding to this $AND$ gate and by renaming all \nvertices $v$ of $C_{x,i}$ as $v^{(i)}$.\nLet $\\tilde{C}_{x,i}$ denote the circuit obtained from $C_{x,i}$ by adding these extra gates. Clearly, the output of $C_{x}$ coincides with the $k$-bit string $r_1r_2\\cdots r_n$, where $r_i$ is the output of $C_{x,i}$.\nFor each $\\tilde{C}_{x,i}$, we define a graph $G_i=(V_i,E_i)$ and a weight function $c_i$ as in the case of $k=1$.\n\nFinally, we add new source $\\tilde{s}$ and new sink $\\tilde{t}$, and then \ndefine $\\tilde{c}$ as follows.  We set $\\tilde{c}(\\tilde{s},s_i)=2^{n^2}$ and $\\tilde{c}(t_i,\\tilde{t})=2^{n^2}$ for every $i\\in[k]$.  Moreover, we define $\\tilde{c}(\\ell_1,\\ell_2) = 2^{in}c_{i}(\\ell'_1,\\ell'_2)$ if $\\ell'_1,\\ell'_2$ are in the same $\\tilde{C}_{x,i}$; $0$ otherwise.\nA new graph $\\tilde{G}=(\\tilde{V},\\tilde{E})$ is defined similarly as before: let $\\tilde{V}=\\bigcup_{i\\in[k]}V_i\\cup \\{\\tilde{s},\\tilde{t}\\}$ (provided that $V_i\\cap V_j={\\mathrm{\\O}}$ for any two distinct $i,j\\in[k]$)  and $\\tilde{E}=\\{(i,j)\\mid \\tilde{c}(i,j)\\;\\text{is defined}\\}$. Let $x={\\langle {C_x,1^n,1^k} \\rangle}$. Note that $m_1(x,g(x,(\\tilde{S}_0,\\tilde{S}_1)))=m_1^*(x)=y_1y_2\\cdots y_k$. Hence, $(f,g)$ reduces {\\sc MinMCV2} to {\\sc Min Weight-st-Cut}. \nGiven a minimal $s^{(i)}$-$t^{(i)}$ cut $(S_{0,i},S_{1,i})$ of $G_i$ for each $i\\in[k]$, define $\\tilde{S}_0 = \\{\\tilde{s}\\}\\cup (\\bigcup_{i\\in[k]}S_{0,i})$  and $\\tilde{S}_1 = \\{\\tilde{t}\\}\\cup (\\bigcup_{i\\in[k]}S_{1,i})$.  The cut  $(\\tilde{S}_0,\\tilde{S}_1)$ is a minimal $\\tilde{s}$-$\\tilde{t}$ cut of $G$.\nWe define $g$ as follows: the $i$th bit of $g({\\langle {\\tilde{G},\\tilde{s},\\tilde{t},\\tilde{c}} \\rangle},(\\tilde{S}_0,\\tilde{S}_1))$ is $1$ iff the $s^{(i)}$-$t^{(i)}$ cut $(S_{0,i},S_{1,i})$ has odd capacity. This equivalence gives rise to the desired equivalence: $C_x$ outputs $y_1y_2\\cdots y_k$ iff  $g(x,(\\tilde{S}_0,\\tilde{S}_1)) = y_1y_2\\cdots y_k$.\n\\end{proofof}\n\n\n\n\n\n\\let\\oldbibliography\\thebibliography\n\\bibliographystyle{plain}\n\\begin{thebibliography}{Gur91}\n{\\small\n\n\\bibitem{Ajt83}\nM. Ajtai. $\\Sigma^{1}_{1}$-formula on finite structures. Ann. Pure Appl. Logic 24, 1--48, 1983.\n\n\\bibitem{ABP93}\nE. Allender, D. Bruschi, and G. Pighizzini. The complexity of computing maximal word functions. Comput. Complexity 3 (1993) 368--391.\n\n\\bibitem{AG00}\nC. \\`{A}lvarez and R. Greenlaw. A compendium of problems complete for symmetric logarithmic space. {Comp. Complex.} 9 (2000) 123--145.\n\n\\bibitem{AJ93}\nC. \\`{A}lvarez and B. Jenner. A very hard log-space counting class.\n{Theoret. Comput. Sci.} 107 (1993) 3--30.\n\n\\bibitem{AJ95}\nC. \\`{A}lvarez and B. Jenner. A note on logspace optimization.\n{Comput. Complexity} 5 (1995) 155--166.\n\n\\bibitem{ACG+03}\nG. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, A. Marchetti-Spaccamela, and M. Protasi. {Complexity and Approximation: Combinatorial Optimization Problems and Their Approximability Properties}, \nSpringer-Verlag, 2003.\n\n\\bibitem{BIS90}\nD. A. M. Barrington, N. Immerman, and H. Straubing. On uniformity within NC$^{1}$. {J. Comput. System. Sci.} 41 (1990) 274--306.\n\n\\bibitem{BM95}\nM. Beaudry and P. McKenzie. Circuits, matrices, and nonassociative computation. {J. Comput. System Sci.} 50 (1995) 441--455.\n\n\\bibitem{Bus87}\nS. Buss. The Boolean formula value problem is in ALOGTIME. In the {Proc. of the 19th Symposium on Theory of Computing} (STOC'87), pp.123--131, ACM Press, 1987.\n\n\\bibitem{CSV84}\nA. K. Chandra, L. Stockmeyer, and U. Vishkin. Constant depth reducibility. {SIAM J. Comput.} 13 (1984) 423--439.\n\n\\bibitem{CKST99}\nP. Crescenzi, V. Kann, R. Silverstri, and L. Trevisan. Structure in  approximation classes. {SIAM J. Comput.}, 28 (1999) 1750--1782.\n\n\\bibitem{CM87}\nS. A. Cook and P. McKenzie. Problems complete for determinsitic logarithmic space. {J. Algorithms} 8 (1987) 385--394.\n\n\\bibitem{DHR97}\nC. Damm, M. Holzer, and P. Rossmanith. Expressing uniformity via oracle.\nTheory Comput. Syst. 30 (1997) 355-366.\n\n\\bibitem{DK00}\nD. Du and K. Ko. Theory of Computational Complexity. John Wiley \\& Sons, Inc., 2000.\n\n\\bibitem{FSS84}\nM. Furst, J. Saxe, and M. Sipser. Parity, circuits and the polynomial-time hierarchy. Math. Systems Theory 17, 13--27, 1984.\n\n\\bibitem{Gab91}\nH. N. Gabow. A matroid approach to finding edge connectivity and packing arborescences. In the\n{Proc. of the 23rd Annual Symposium on Theory of Computing} (STOC'91), ACM Press, pp.112--122, 1991.\n\n\\bibitem{GSS82}\nL. M. Goldschlager, R. A. Shaw, and J. Staples. The maximum flow problem is log space complete for P. {Theoret. Comput. Sci.} 21 (1982) 105--111.\n\n\\bibitem{Has86}\nJ. H{\\aa}stad. Computational Limitations for Small-Depth Circuits. Ph.D. dissertation, MIT Press, Cambridge, MA, 1986.\n\n\\bibitem{Hes01}\nW. Hesse. Division is in uniform TC$^{0}$. In the Proc. of the 28th International Colloquium on Automata, Languages and Programming (ICALP 2001), Lecture Notes in Computer Science, Springer, vol.2076, pp.104--114, 2001.\n\n\\bibitem{HAB02}\nW. Hesse, E. Allender, D. A. M. Barrington:\nUniform constant-depth threshold circuits for division and iterated multiplication. J. Comput. Syst. Sci. 65 (2002) 695--716.\nCorrigendum, J. Comput. Syst. Sci. 80 (2014) 496--497.\n\n\\bibitem{HMU01}\nJ. E. Hopcroft, R. Motowani, and J. D. Ullman. Introduction to Automata Theory, Languages, and Computation. Second Edition, Addison Wesley, 2001.\n\n\\bibitem{Imm88}\nN. Immerman. Nondeterministic space is closed under complement. {SIAM J. Comput.} 17 (1988) 935--938.\n\n\\bibitem{Jen95}\nB. Jenner. Knapsack problems for NL. {Inf. Process. Lett.} 54 (1995) 169--174.\n\n\\bibitem{JLM97}\nB. Jenner, K. Lange, and P. McKenzie. Tree isomorphism and some other\ncomplete problems for deterministic logspace. \\#1059, DIRO, University of Montr\\'{e}al, 1997.\n\n\\bibitem{JMT98}\nB. Jenner, P. McKenzie, and J. Tor\\'{a}n.\nA note on the hardness of tree isomorphism. In the\nProc. of the 13th Annual IEEE Conference on Computational Complexity (CCC'98), IEEE Computer Society, pp.101--105 (1998).\n\n\\bibitem{Jon75}\nN. D. Jones. Space-bounded reducibility among combinatorial problems. {J. Comput. System Sci.} 11 (1975) 68--75.\n\n\\bibitem{JLL76}\nN. D. Jones, Y. E. Lien, and W. T. Laaser. New problems complete for nondeterministic log space. {Math. Systems Theory} 10 (1976) 1--17.\n\n\\bibitem{Kar92}\nD. R. Karger. Global min-cuts in RNC, and other ramifications of a simple min-cut algorithm. In the\n{Proc. of the 4th ACM/SIGACT-SIAM Symposium on Discrete Algorithms}\n(SODA'93), pp.21--30, 1993.\n\n\\bibitem{KL82}\nR. Karp and R. Lipton. Turing machines that take advice. L'Ensignement Math\\'{e}matique 28, 191--210, 1982.\n\n\\bibitem{KMSV98}\nS. Khanna, R. Motwani, M. Sudan, and U. V. Vazirani. On syntactic versus computational views of approximability. {SIAM J. Comput.}, 28, 164--191, 1998.\n\n\\bibitem{KST89}\nJ. K\\\"{o}bler, U. Sch\\\"{o}ning, and J. T\\'{o}ran. On counting and approximation. {Acta Informatica} 26 (1989) 363--379.\n\n\\bibitem{Kre88}\nM. Krentel. The complexity of optimization problems. {J. Comput. System Sci.} 36 (1988) 490--509.\n\n\\bibitem{Lad75}\nR. E. Ladner. The circuit value problem is log-space complete for P. SIGACT News 7 (1975) 18--20.\n\n\\bibitem{NT05}\nA. Nickelsen and T. Tantau. The complexity of finding paths in graphs with bounded independence number. {SIAM J. Comput.} (2005) 1176--1195.\n\n\\bibitem{PY91}\nC. H. Papadimitriou and M. Yannakakis. Optimization, approximation and complexity classes. {J. Comput. System Sci.}, 43, 425--440, 1991.\n\n\\bibitem{Rei08}\nO. Reingold. Undirected connectivity in log-space. {J. ACM}   55 (2008) article 17.\n\n\\bibitem{Ruz81}\nW. L. Ruzzo. On uniform circuit complexity. {J. Comput. System Sci.} 21 (1981) 365--383.\n\n\\bibitem{Sze88}\nR. Szelepcs\\'{e}nyi. The method of forced enumeration for nondeterministic automata. {Acta Inform.} 26 (1988) 279--284.\n\n\\bibitem{Tan07}\nT. Tantau. Logspace optimisation problems and their approximation properties. {Theory Comput. Syst.} 41 (2007) 327--350.\n\n\\bibitem{Yam11}\nT. Yamakami. Optimization, randomized approximability, and constraint satisfaction problems. In the {Proc. of the 22nd International Symposium on Algorithms and Computation} (ISAAC 2011), Lecture Notes in Computer Science, Springer, vol.7074, pp.454--463, 2011. See also arXiv:1109.3651.\n\n\\bibitem{Yam14}\nT. Yamakami. Structural complexity of multi-valued partial functions computed by nondeterministic pushdown automata (extended abstract). In the Proc. of the 15th Italian Conference of Theoretical Computer Science (ICTCS 2014), CEUR Workshop Proceedings, vol.1231, pp.225--236, 2014. See also arXiv:1508.05814.\n\n\\bibitem{Yao85}\nA. C. Yao. Separating the polynomial-time hierarchy by oracles. In the Proc. of the 26th IEEE Symposium on Foundations of Computer Science (FOCS'85), pp.1--10, 1985. }\n\\end{thebibliography}\n\n\n\n\n\n", "itemtype": "equation", "pos": 155725, "prevtext": "\nbecause $m_2^*(x)\\leq r m_2(x,y)$ and $m_2^*(x) = \\Delta b(x) - \\gamma m_1^*(x)$. Since $b(x)\\leq \\gamma m_1^*(x)$, we obtain $m_1(x,y)\\leq (1/r)[(r-1)\\Delta+1]m_1^*(x)$. Hence, it follows that $m_1(x,y)\\leq [1+\\theta(r-1)]m_1^*(x)$, where $\\theta= (\\Delta-1)/r$, which is at most $\\Delta-1$ since $r\\geq1$.  This shows that $P_1$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reducible to $P_2$.\n\n(3) Consider the case of ${\\cal D}={\\mathrm{LO}}$. The construction of the desired reduction is similar to (2). Assuming that $P_1$ is in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, we want to show that $P_2$ is also in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. This is obtained simply by mapping minimal solutions for $P_1$ to maximal solutions for $P_2$.\n\\end{proof}\n\n\n\\subsection{Completeness of Graph Problems}\\label{sec:graph-problems}\n\nAs our starting point, we recall from Section \\ref{sec:general-complete} a bounded variant of {\\sc Min Path-Weight}, called {\\sc Min BPath-Weight}, which uses the {\\em total path weight} (i.e., $w({\\cal S})= \\sum_{i=1}^{k}w(v_i)$ for ${\\cal S}=(v_1,v_2,\\ldots.v_k)$ with $k\\leq|V|$) as a measure function with an extra condition that $1\\leq w(v)\\leq |V|$ for all $v\\in V$. Earlier, Tantau \\cite[Theorem 5.1]{Tan07} discussed the case when all vertices of a given graph have weight exactly $1$. To verify that {\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{MinNL}}\\cap{\\mathrm{PBO}}$, we need to modify the proof of Proposition \\ref{Min-Path-complete} in the following way.\nFor this purpose, we first modify a given measure function $m$ so that its log-space auxiliary Turing machine $M_m$ produces $m(x,y)$ on one of its work tapes and then copies each bit (including ``$0$'') from the lower bit to the higher bit at each step. We further modify $M_m$ so that its internal clock helps it halt in exactly $p(n)$ steps, for a suitable polynomial $p$. We then\ndefine $w(v)$ to be $b_1 2^{e-1} +1$ if $v$ contains a string $b_1b_2\\cdots b_e$ written  on an output tape of $M_m$ in a target partial final configuration.\nOtherwise, define $w(v)=1$. It follows that $\\sum_{i=1}^{q(n)}w(v_i) = q(n)+ m(x,y)$.\nBy reducing minimization problems to maximization problems by Lemma \\ref{min-reduces-max}(1), we can prove that every minimization problem in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$ is also ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reducible to {\\sc Min BPath-Weight}. Therefore, we obtain the following completeness result.\n\n\\begin{lemma}\\label{Bpath-complete}\n{\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\n\nIn Section \\ref{sec:general-complete}, we have mostly dealt with optimization problems that are associated with the path weights of graphs. Another natural type of optimization problems is\na problem of searching a path of a directed graph starting at a given source   toward an appropriately chosen vertex whose weight is well-defined and must be maximal. Tantau  \\cite[Theorem 3.2]{Tan07} earlier demonstrated that this maximization problem is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\nMoreover, its slightly modified version was shown to be ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{L}}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ \\cite[Theorem 3.7]{Tan07}.\nIn what follows, we shall discuss a similar optimization problem using ``undirected'' graphs with ``total'' weight functions, particularly, in the case where vertex weights are all bounded.\n\nLet us define formally this problem as follows.\n\n{\\medskip}\n{\\sc Maximum Undirected Bounded Vertex Weight Problem} ({\\sc Max UB-Vertex}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} an undirected graph $G=(V,E)$, a source $s\\in V$, and a (vertex) weight function  $w:V\\rightarrow {\\mathbb{N}}^{+}$ satisfying $w(v)\\leq |V|$ for every $v\\in V$.\n\n\\item {\\sc Solution:} a path of $G$ starting at $s$ and ending at a certain vertex $t$ in $V$.\n\n\\item {\\sc Measure:} the weight $w(t)$ of $t$.\n\\end{itemize}\n\nA {\\em directed-graph version} of {\\sc Max UB-Vertex}, called {\\sc Max B-Vertex},\nhas a log-space $n^{O(1)}$-approximate algorithm \\cite{Tan07} but is not known to fall into ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\n\\begin{proposition}\\label{max-bvertex-complete}\n$\\text{\\sc Max UB-Vertex}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{proposition}\n\nEven if the vertex $t$ in the above definition of {\\sc Max UB-Vertex} is restricted to a vertex of degree exactly $1$ as the following proof shows, the obtained problem is still ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{LO}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n{\\medskip}\n\n\\begin{proof}\nWe begin with setting {\\sc max UB-Vertex} as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Let $G=(V,E)$ be any graph given as an instance in $I_0$. notice that $SOL_0(x)\\neq{\\mathrm{\\O}}$ for all $x\\in I_1$.\nSince the weight of every vertex in $G$ is at most the input size,  $\\text{\\sc Max UB-Vertex}$ is polynomially bounded. Thus, it is not difficult to show that optimal solutions for $\\text{\\sc Max UB-Vertex}$ can be found using log space by making a series of nonadaptive queries to oracle $A=\\{{\\langle {G,s,w,1^k} \\rangle}\\mid \\exists t\\in V\\,[w(t)\\geq k \\wedge \\,\\text{$s$ and $t$ are connected}\\,]\\}$ by incrementing $k$ from $1$ to $|V|$.\nTo see that $A$ is in ${\\mathrm{L}}$, we sequentially pick a different $t\\in V$, check if $s$ and $t$ are connected using Reingold's log-space algorithm for DSTCON, and finally  check if $w(t)\\geq k$. Therefore, {\\sc Max UB-Vertex} is ${\\mathrm{L}}$-solvable; that is, $\\text{\\sc Max UB-Vertex}$ belongs to ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$.\n\nConcerning the ${\\leq_{\\mathrm{EX}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max UB-Vertex}, let us consider a polynomially-bounded maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{LO}}_{{\\mathrm{NLO}}}$. The case of minimization problems follows from Lemma \\ref{min-reduces-max}(3).\nBy Lemma \\ref{LO-simple-form}, it is possible to  assume that $P$ satisfies Conditions (ii)--(iii) of the lemma. Take a log-space auxiliary Turing machine $M_m$ that computes $m$. Moreover, we take a log-space deterministic Turing machine $M_P$ that produces maximal solutions for $P$. Define $b(x) = m(x,M_P(x))$ for each instance $x\\in (I\\circ SOL)^{\\exists}$.\nTake a polynomial $p$ such that $b(x)\\leq p(|x|)$ for all $x\\in (I\\circ SOL)^{\\exists}$. For technicality, we demand that $p(n)>1$ for all $n$.\nNotice that $b$ is computed using log space by the following simple machine $M'$   equipped with\na special work tape, called a {\\em solution tape}, in which we use only one tape cell. Starting with input $x$, $M'$ simulates $M_P$ on $x$. Whenever $M_P$ writes a symbol $\\sigma$ on its output tape, $M'$ writes $\\sigma$ on the solution tape,  simulates $M_m$ on $x$ while an auxiliary-tape head is scanning $\\sigma$, and erases $\\sigma$ from the solution tape. This deletion of the symbol $\\sigma$ is necessary because the output $M_P(x)$ may be super-logarithmically long.\nFinally, $M'$ outputs $b(x)$.\n\nHere, we construct a configuration graph $G$ in a way similar\nto the proof of Proposition \\ref{forest-path-weight} using $M'$; however, we use a quite different weight function.\nSince all weights are polynomially bounded,  we can embed an entire content of an output tape of $M'$ into a partial configuration. To be more precise, we can force $M'$ to use one of its work tapes to compute $b(x)$ and, in the end, copy  the content of this tape into its write-only output tape.\n\nFor a vertex $v$ representing a certain partial halting configuration, its weight $w(v)$ is set to be the value written on the output tape in this partial configuration unless the value is not zero. For any other vertex associated with partial non-halting configurations, we simply set $w(v)=1$.\n\nWe further define $f(x,r)$ to be the above-mentioned configuration graph $G$ together with the weight function $w$. Given a computation path $y$ of $M'$ on $x$, since each partial configuration in $y$ contains the information on an output symbol produced by $M_P$, it is possible to recover from $y$ an entire output string $M_P(x)$. We thus set $g(x,y,r)$ to be $M_P(x)$ reconstructed from $y$. It is easy to check that  $f,g\\in{\\mathrm{FNC}^{ {1} }}$.\nNote that $m_0(f(x,r),y) = b(x)$ if $y$ is in $SOL_0(f(x,r))$. This implies that the performance ratio $R$ of $g(x,y,r)$ always satisfies $R(x,g(x,y,r))=1$ for any $y\\in SOL_0(f(x,r))$. As a result, $(f,g,1)$ reduces $P$ to {\\sc max UB-Vertex}, as requested.\n\\end{proof}\n\n\n\nTo obtain an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete problem for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$, we place a restriction on the behavior of a (vertex) weight function of $\\text{\\sc Max UB-Vertex}$.\nThe {\\em maximum undirected 2 vertex weight problem} ({\\sc Max U2-Vertex}) is a variant of {\\sc Max UB-Vertex} with an additional requirement: for any instance ${\\langle {G,s,w} \\rangle}$ with $G=(V,E)$, it holds that\n$\\max_{v\\in V}\\{w(v)\\}\\leq 2\\min_{v\\in V}\\{w(v)\\}$.\n\n\\begin{proposition}\n{\\sc Max U$2$-Vertex} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$.\n\\end{proposition}\n\n\\begin{proof}\nTo simplify the following proof, we write $\\text{\\sc Max U2-Vertex}$ as $(I_0,SOL_0,m_0,\\text{\\sc max})$. Similar to {\\sc Max UB-Vertex}, {\\sc Max U2-Vertex} can be shown to be in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. In particular, to verify the extra condition that (*) $\\max_{v\\in V}\\{w(v)\\}\\leq 2\\min_{v\\in V}\\{w(v)\\}$ for an instance $(G,s,w)$, we pick each pair $(v_1,v_2)$ of $G$'s vertices and check that either $w(v_1)\\leq 2w(v_2)$ or  $w(v_2)\\leq 2w(v_1)$ holds. This procedure requires only log space.\n\nThe following algorithm $C$ confirms that {\\sc Max U$2$-Vertex} belongs to  ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$. On input $x =(G,s,w) \\in I_0$, output a length-$1$ path of $G$ from vertex $s$ to the vertex that appears first in an instance (where $G$ is given in binary as a list of edge relations).\nThis process can be implemented by ${\\mathrm{NC}^{ {1} }}$-circuits. It thus follows from Condition (*) that $m_0(x,C(x)) \\leq m_0^*(x) \\leq 2 m_0(x,C(x))$. Therefore, $C$ is an 2-approximate algorithm for {\\sc Max U2-Veterx}. This shows that {\\sc Max U2-Vertex} falls into ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}$.\n\nNext, we want to show that $\\text{\\sc Max U2-Vertex}$ is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. Let $P=(I,SOL,m,\\text{\\sc max})$ be any maximization problem in ${\\mathrm{APXNC}^{ {1} }}_{{\\mathrm{NLO}}}\\cap {\\mathrm{PBO}}$.\nThe case of minimization is handled by Lemma \\ref{min-reduces-max}(2).\nLet $M_m$ be an auxiliary Turing machine computing $m$ using log space.\nSince $m$ is polynomially bounded, $M_m$ first produces $m(x,y)$ on one of its work tapes and then copies it onto an output tape just before halting. Let $C_P$ be an ${\\mathrm{NC}^{ {1} }}$-circuit producing $\\alpha$-approximate solutions of $P$ for a certain constant $\\alpha>1$. For convenience, let $b(x) = m(x,C_P(x))$ for each $x\\in (I\\circ SOL)^{\\exists}$ and $b(x)=\\bot$ for all the others $x$. It follows that $b\\in{\\mathrm{FNC}^{ {1} }}$ and that $b(x)\\leq m^*(x)\\leq \\alpha b(x)$ for all $x\\in (I\\circ SOL)^{\\exists}$.\nBecause of the definition of partial configurations, from any accepting computation path of $M_m(x,y)$, we can easily recover an auxiliary input $y$.\n\nLet us prove $P{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Max U2-Vertex}$ via a certain ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{AC}^{ {0} }}}$-reduction $(f,g,c)$. Let $r\\geq1$ and $x\\in (I\\circ SOL)^{\\exists}$. Firstly, we define $f(x,r)$ to be $(G,s,w)$, where $G$ is a configuration graph\nof $M_m$ using normal input $x$, $s$ is the initial partial configuration of $M_m$, and $w$ is defined later.  Given a computation path $u=(v_1,v_2,\\ldots,v_k)$ of $M_m$ using $x$, let $y_u$ denote a unique auxiliary input used for $M_m$ constructed from $u$. Note that $y_u$ is in $SOL(x)$.\nHere, we further define $g(x,u,r)=y_u$ and set $c=\\alpha-1$. In addition, we set $\\Delta_x=(\\alpha-2)b(x)$.\nFor an accepting configuration $v$, let $w(v) = w' + \\Delta_x$, where $w'$ is the number written on $M_m$'s output tape.  For other configurations $v$, let $w(v)= b(x)+\\Delta_x$. Note that $b(x)+\\Delta_x \\leq w(v)\\leq m^*(x)+\\Delta_x$.\nSimilarly to Eqn.(\\ref{eqn:max-min-ratio}) in the proof of Lemma \\ref{NLO-to-APXL}, it holds that $\\max_{v\\in V}\\{w(v)\\}/\\min_{v\\in V}\\{w(v)\\}\\leq 2$. Since $b\\in{\\mathrm{FNC}^{ {1} }}$, $f(x,r)$ is computed by an ${\\mathrm{NC}^{ {1} }}$-circuit. For any $u=(v_1,v_2,\\ldots,v_k)\\in SOL_0(f(x,r))$, it follows that $m_0(f(x,r),u) = w(v_k) = m(x,y_u)+\\Delta_x = m(x,g(x,u,r)) +\\Delta_x$. Moreover, we obtain $m_0^*(f(x,r)) = m^*(x)+\\Delta_x$.\nGiven a number $r\\in{\\mathbb{Q}}^{\\geq1}$, consider the performance ratio $R$ of $g(x,u,r)$ with respect to $x$. Since $m(x,g(x,u,r))=b(x)$ for any $u\\in SOL(x)$, calculations similar to  Eqns.(\\ref{eqn:R_1-alpha})--(\\ref{eqn:ration-with-c})  lead to $R(x,g(x,u,r)) -1 \\leq c (R_0(f(x,r),u)-1)$.\nWe therefore conclude that $(f,g,c)$ reduces $P$ to {\\sc Max U2-Vertex}.\n\\end{proof}\n\n\n\\subsection{Completeness of Algebraic and Combinatorial Problems}\\label{sec:algebraic-problem}\n\nApart from graph problems, we shall study algebraic and combinatorial problems. We begin with an algebraic problem, which turns out to be complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. As {\\sc Max UB-Vertex} and {\\sc Max U2-Vertex} have orderly structures induced by ``edge relations,'' the algebraic problem that we shall consider below has a similar structure induced by ``operations.''\n\nGiven a finite set $X$, we consider a binary operation $\\circ:X\\times X\\to X$. A binary operation $\\circ$ is {\\em associative} if $(x\\circ y)\\circ z = x\\circ (y\\circ z)$ holds for all $x,y,z\\in X$. For a subset $S$ of $X$, we say that a set $G(S)$ is {\\em generated  by $\\circ$ from $S$} if $G(S)$ is the smallest set that contains $X$ and is closed under $\\circ$.\nThe decision problem, called {\\sc AGen}, of determining whether $t$ is in $G(S)$  for a given instance $(X,S,\\circ,t)$ with $t\\in X$ is $\\leq_{m}^{{\\mathrm{L}}}$-complete for ${\\mathrm{NL}}$ \\cite{JLL76}.\nLet us consider its optimization counterpart, which we call {\\sc Min AGen}.\n\n{\\medskip}\n{\\sc Minimum Associative Generation Problem} ({\\sc Min AGen}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a finite set $X$, an associative binary operation $\\circ:X\\times X\\to X$, a set $S\\subseteq X$, an element $t\\in X$, and a weight function $w:X\\to{\\mathbb{N}}^{+}$ satisfying $w(x)\\leq|X|$ for all $x\\in X$.\n\n\\item {\\sc Solution:} a sequence $(x_1,x_2,\\ldots,x_m)$ of elements in $X$ with $1\\leq m\\leq|X|$ so that the element $x_1\\circ x_2\\circ \\cdots \\circ x_m$ in $G(S)$ equals $t$.\n\n\\item {\\sc Measure:} the value $\\sum_{i=1}^{m}w(x_i)$.\n\\end{itemize}\n\nNote that $s$ belongs to $G(S)$ iff there is a sequence $(x_1,x_2,\\ldots,x_m)$ of elements in $S$ with $1\\leq m\\leq|X|$ for which $s=x_1\\circ x_2\\circ \\cdots \\circ x_m$ holds \\cite{JLL76}. In what follows, we demonstrate the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-completeness of {\\sc Min AGen} for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{proposition}\n{\\sc Min AGen} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\n\\end{proposition}\n\n\\begin{proof}\nFor convenience, we set $\\text{\\sc Min AGen} = (I_0,SOL_0,m_0,\\text{\\sc min})$.\nFirst, we show that {\\sc Min AGen} is in ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. Let $(X,S,\\circ,w,t)$ be any instance to {\\sc Min AGen}. We can build an auxiliary Turing machine $M$ as follows. Let $u= (x_1,x_2,\\ldots,x_m)$ be a sequence of $X$ and is given to an auxiliary tape of $M$.\nTo see that $I_0\\in{\\mathrm{L}}$, it suffices to check, using log space, whether (i) $S\\subseteq X$, (ii) $\\circ$ is associative, and (iii) $w(x)\\leq|X|$ for all $x\\in X$.\nIt is also easy to see that $I_0\\circ SOL_0\\in{\\mathrm{auxFL}}$. To obtain  $m_0((X,S,\\circ,w,t),u)$, we need to compute the value $x_1\\circ x_2\\circ \\cdots \\circ x_m$ and then output the value $\\sum_{i=1}^{m}w(x_i)$. Since $m_0$ is polynomially bounded, this value can be obtained using log space. This $m_0$ therefore belongs to ${\\mathrm{auxFL}}$.\nThis show that {\\sc Min AGen} is in ${\\mathrm{NLO}}$.\n\nNext, we want to show that {\\sc Min AGen} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hard for   ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. For this purpose, we shall show that $\\text{\\sc Min BPath-Weight}{\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}\\text{\\sc Min AGen}$ since, by Lemma \\ref{Bpath-complete}, {\\sc Min BPath-Weight} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$. Let $\\text{\\sc Min BPath-Weight} = (I_1,SOL_1,m_1,\\text{\\sc min})$.\nLet $h= (G,s,t,w)$ be any instance in $I_1$ with $G=(V,E)$.\nNote that $w(v)\\leq|V|$ for all $v\\in V$. For a sequence ${\\cal S}=(v_1,v_2,\\ldots,v_k)$ with $k\\leq |V|$, since $m_1(h,{\\cal S}) = \\sum_{i=1}^{k}w(v_i)$, we obtain $m_1(h,{\\cal S})\\leq|V|^2$.\n\nWe want to define an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,c)$ from {\\sc Min BPath-Weight} to {\\sc Min AGen} as follows.\nOur construction essentially follows from the proof of \\cite[Theorem 5]{JLL76}. Let $c=1$. Letting $f(h,r) = (X,S,\\circ,\\tilde{w},t)$, we define $X$, $S$, $\\circ$, $\\tilde{w}$ as follows. Let $X=V\\cup(V\\times V)\\cup\\{\\natural\\}$ and $S=E\\cup\\{s\\}$. A binary operation $\\circ$ admits the following rules: for all $u\\in X$ and $x,y,z\\in V$, (i) $u\\circ \\natural = \\natural\\circ u=\\natural$, (ii) $x\\circ y=\\natural$ for all $x,y\\in V$, (iii) $(x,y)\n\\circ z = \\natural$, $x\\circ (x,z) =z$,  and $x\\circ (y,z)=\\natural$ if $y\\neq z$, and (iv) $(x,y)\\circ (y,v) = (x,v)$ and $(x,y)\\circ (u,v)=\\natural$ if $y\\neq u$. The desired weight function $\\tilde{w}$ is defined as  $\\tilde{w}(\\natural)=|X|$, $\\tilde{w}(x,y) = w(x)+w(y)$,  and $\\tilde{w}(x)=w(x)$  for all $x,y\\in V$. Note that $\\tilde{w}(x)\\leq \\max\\{|X|,2|V|\\}\\leq |X|$  for all $x\\in X$ and $\\tilde{w}(x_1)+\\sum_{i=1}^{k}\\tilde{w}(x_i,x_{i+1}) + \\tilde{w}(x_k) = 2\\sum_{i=1}^{k}w(x_i)$ for $x_1,x_2\\,\\ldots,x_k\\in V$.\n\nNote that, given an $s$-$t$ path $(x_1,x_2,\\ldots,x_k)$ with $s=x_1$ and $t=x_k$, since $s\\in S$, it is possible for us to prove recursively the membership $x_i\\in G(S)$ for every  $i\\in[2,k]_{{\\mathbb{Z}}}$; hence, $t\\in G(S)$ follows.\nFor any sequence $u=(x_1,(x_1,x_2),(x_2,x_3),\\ldots,(x_{k-1},x_k))$ in $SOL_{0}(f(h,r))$ with $s=x_1$ and $t=x_k$,  we define $g(h,u,r) = (x_1,x_2,\\ldots,x_k)$.\nIt follows that $m_0(f(h,r),u) = \\tilde{w}(x_1) + \\sum_{i=1}^{k}\\tilde{w}(x_i,x_{i+1}) = 2\\sum_{i=1}^{k}w(x_i) -w(t)$ and $m_1(h,g(h,u,r)) = \\sum_{i=1}^{k}w(x_i)$. From those equalities, we obtain $2m_1(h,g(h,u,r)) = m_0(f(h,r),u) + w(t)$.\n\nHere, we intend to verify that $(f,g,c)$ correctly reduces {\\sc Min BPath-Weight} to {\\sc Min AGen}. Take any $r\\in{\\mathbb{Q}}^{\\geq1}$ and any $u\\in SOL_0(f(h,r))$.  Assume that the performance ratio $R_0$ for {\\sc Min AGen} satisfies $R_0(f(h,r),u)\\leq r$. It then follows that $R_1(h,g(h,u,r)) = \\frac{m_1(h,g(h,u,r))}{m_1^*(h)} = \\frac{m_0(f(h,r),u)+w(t)}{m_0^*(f(h,r))+w(t)} \\leq \\frac{m_0(f(h,r),u)}{m_0^*(f(h,r))} \\leq r$.\nTherefore, $(f,g,c)$ is a correct ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction.\n\\end{proof}\n\n\n\nJenner \\cite{Jen95} studied a few variants of the well-known {\\em knapsack problem}. In particular, she introduced a decision problem, called CUK, of determining whether, given unary string pairs  $(0^w,0^p), (0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$, where  $w,w_i\\in{\\mathbb{N}}^{+}$ and $p,p_i\\in{\\mathbb{N}}$ for $i\\in[n]$, there is a $\\{0,1\\}$-sequence $(z_0,z_1,z_2,\\ldots,z_n)$ satisfying $w2^{p} = \\sum_{i=0}^{n}z_i\\cdot w_i2^{p_i}$.\nShe showed that this problem is (${\\mathrm{L}}$-uniform) $\\leq^{{\\mathrm{NC}^{ {1} }}}_{m}$-complete for ${\\mathrm{NL}}$. Here, we turn this decision problem into an optimization problem, which will be proven to be ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n{\\medskip}\n{\\sc Maximization 2-Bounded Close-to-Unary Knapsack Problem} ({\\sc Max 2BCU-Knapsack}):\n\\begin{itemize}{\\vspace*{ {-2} mm}}\n  \\setlength{\\topsep}{-2mm}\n  \\setlength{\\itemsep}{1mm}\n  \\setlength{\\parskip}{0cm}\n\n\\item {\\sc instance:} a pair $(0^w,0^p)$, a sequence $(0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$ of pairs, and a weight sequence $(c_0,c_1,\\ldots,c_n)$ of positive integers, where $w,w_i\\in{\\mathbb{N}}^{+}$, $p,p_i\\in{\\mathbb{N}}$, $1\\leq c_i\\leq nwp$ for all $i\\in[0,n]_{{\\mathbb{Z}}}$, and $\\max_{0\\leq i\\leq n}\\{c_i\\}\\leq 2\\min_{0\\leq i\\leq n}\\{c_i\\}$, provided that $w_0=w$ and $p_0=p$. Here, the notation $0^0$ expresses the empty string $\\lambda$.\n\n\\item {\\sc Solution:} a sequence $z=(z_0,z_1,\\ldots,z_n)$ of Boolean values satisfying $w2^p = \\sum_{i=1}^{n}z_i\\cdot w_i2^{p_i}$.\n\n\\item {\\sc Measure:} $\\max_{0\\leq i\\leq n}\\{ c_i z_i \\}$.\n\\end{itemize}\n\nA trivial solution $z=(1,0,\\ldots,0)$, which indicates the choice of $(0^{w_0},0^{p_0})$, is needed to ensure that $\\text{\\sc Max 2BCU-Knapsack}$ is indeed in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\nRegarding each value $w_i2^{p_i}$, the following simple inequalities hold:  $|bin(w_i2^{p_i})| \\leq \\log|0^{w_i}|+|0^{p_i}|+1 = \\log{w_i}+p_i+1$.\n\nIn what follows, we show the completeness of {\\sc Max 2BCU-Knapsack} for ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\n\\begin{lemma}\n{\\sc Max 2BCU-Knapsack} is ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-complete\nfor ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$.\n\\end{lemma}\n\n\\begin{proof}\nLet $\\text{\\sc Max 2BCU-Knapsack} = (I_0,SOL_0,m_0,\\text{\\sc max})$. First, we argue that {\\sc Max 2BCU-Knapsack} is in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$. It is obvious that $m_0$ is polynomially bounded. Earlier, Jenner \\cite{Jen95} demonstrated  that $\\mathrm{CUK}\\in{\\mathrm{NL}}$. A similar argument shows that $I_0\\circ SOL_0\\in{\\mathrm{auxL}}$ and $m_0\\in{\\mathrm{auxFL}}$; therefore, {\\sc Max 2BCU-Knapsack} falls into ${\\mathrm{NLO}}\\cap{\\mathrm{PBO}}$.\nLet $x=(K,C)$ be any instance to {\\sc Max 2BCU-Knapsack}, where $C=(c_0,c_1,\\ldots,c_n)$ and $K$ is composed of $(0^{w},0^{p}),(0^{w_0},0^{p_0}),(0^{w_1},0^{p_1}),\\ldots,(0^{w_n},0^{p_n})$.\nChoose $z_0=(1,0,\\ldots,0)\\in\\{0,1\\}^{n+1}$.  It follows that $z_0\\in SOL_0(x)$ and that\n$R_0(x,z_0) = \\frac{m_0^*(x)}{m_0(x,z_0)} \\leq 2$ since $\\min_{0\\leq i\\leq n}\\{c_i\\}\\leq m_0^*(x)\\leq 2\\min_{0\\leq i\\leq n}\\{c_i\\}$.\nThis implies that {\\sc Max 2BCU-Knapsack} is in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}$.\n\nFor the ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-hardness of {\\sc Max 2BCU-Knapsack}, let us consider an arbitrary maximization problem $P=(I,SOL,m,\\text{\\sc max})$ in ${\\mathrm{APXL}}_{{\\mathrm{NLO}}}\\cap{\\mathrm{PBO}}$ that satisfies Conditions (ii)--(iv) of  Lemma \\ref{NLO-to-APXL}.\nTo construct an ${\\leq_{\\mathrm{sAP}}}^{{\\mathrm{NC}^{ {1} }}}$-reduction $(f,g,2)$ from $P$ to {\\sc Max 2BCU-Knapsack}, we first consider a log-space deterministic Turing machine $M_P$ that produces  $2$-approximate solutions of $P$.\nIn addition, we take a function $b\\in{\\mathrm{FL}}$ for which\n$b(x)\\leq m^*(x)\\leq 2b(x)$ and $m(x,y)\\geq b(x)$ for all $(x,y)\\in I\\circ SOL$ by Lemma \\ref{NLO-to-APXL}. Let $M_m$ be a log-space auxiliary Turing machine computing $m$.\n\nFor the purpose of defining an appropriate reduction, we assume that $M_m$ on input $(x,y)$ writes each symbol of $y$ on a designated cell of a particular work tape so that,\n\nThe definition of partial configurations makes it possible to retrieve the entire string $y$ from any halting computation path $e$ of $M_m$ on input $(x,y)$. We write $h(e)$ for the auxiliary input $y$ fed into $M_m$.\nSince $m$ is polynomially bounded, we force $M_m$ to calculate $m(x,y)$ on one of its work tapes and copies it onto an output tape just before halting so that, for any  $y\\in SOL(x)$, $m(x,y)$ equals the sum of the numbers written on the output tape of configuration along a computation path of $M_{m}$ on input $(x,y)$.\n\nLet us recall the notion of configuration graph from the proof of Proposition \\ref{Min-Path-complete} and consider a configuration graph $G_{x}^{M_m}$ of $M_m$ on input $x\\in I$. In what follows, we consider only the case where $SOL(x)\\neq{\\mathrm{\\O}}$.\nFor convenience, we set $G^{M_m}_{x} = (V,E)$ with $V=\\{v_1,v_2,\\ldots,v_m\\}$ for a  certain integer $m\\geq1$, where $v_1$ expresses the initial partial configuration of $M_m$. Note that the first move of $M_m$ is fixed and does not depend on the choice of inputs.\nWe further make $M_m$ terminate in exactly $q(|x|)$ steps for a suitable polynomial $q$, independent of the choice of $y$ satisfying $|y|\\leq p(|x|)$.\n\nFix $x\\in(I\\circ SOL)^{\\exists}$ and write $q$ for $q(|x|)$.\nTake an arbitrary number $r\\in{\\mathbb{Q}}^{\\geq1}$.\nFollowing \\cite[Theorem 1]{Jen95}, let $w=2^{t}$ and $p=2(q+1)t$, where $t = 2{\\lceil {\\log{q}} \\rceil}$. If $(v_i,v_j)\\in E$, then let $w_{ij} = (2^{t}-j)2^{2t}-(2^{t}-i)$ and $p_{ij,k} = 2kt$ for  $0\\leq k\\leq q-1$.\nLet $w_0=w$, $p_0=p$, $w_1=2^{t}$, $p_1=0$, $w_{q}=q2^{2t}$, and $p_{q} = 2(q-1)t$.\nLet $K$ be composed of the following pairs: $(0^w,0^p)$, $(0^{w_0},0^{p_0})$,  $(0^{w_1},0^{p_1})$, $(0^{w_{q}},0^{p_{q}})$, and $(0^{w_{ij}},0^{p_{ij}})$ for all $(v_i,v_j)\\in E$. A series $C=(c_0,c_1,c_{q},c_{ij})_{(v_i,v_j)\\in E}$ is defined as follows.\nFor any partial configuration pair $(v_i,v_j)$, if $v_j$ is an accepting partial configuration, then let $c_{ij}$ be the number written on an output tape of $M_m$; otherwise, let $c_{ij}=b(x)$. Note that $\\min_{(v_i,v_j)\\in e}\\{c_0,c_1,c_q,c_{ij}\\}\\geq b(x)$.\nFor any $x\\in I$ and $r\\in{\\mathbb{Q}}^{\\geq1}$, we define $f(x,r)$ to be the pair $(K,C)$ given above.\n\nGiven any accepting computation path $e=(v_1,v_2,\\ldots,v_q)$ of $M_m$ on input $x$ ($|x|=n$), we define a series $z=(z_0,z_1,z_q,z_{ij})_{(v_i,v_j)\\in e}$ as $z_0=0$, $z_1=z_q=1$, and $z_{ij}=1$ if $(v_i,v_j)\\in e$, and $z_{ij}=0$ otherwise.\nTo emphasize $e$, we write $z_{(e)}$ for this series $z$.\nAs was shown in \\cite{Jen95}, we obtain $z\\in SOL_0(f(x,r))$ with $z\\neq(1,0,\\ldots,0)$ iff there exists an accepting computation path $e$ satisfying $z=z_{(e)}$, namely, $w2^{p} = w_12^{p_1}+w_q2^{p_q}+ \\sum_{(v_i,v_j)\\in e}\\sum_{0\\leq k\\leq q-1}w_{ij}2^{p_{ij,k}}$ and thus $m_0(f(x,r),z_{(e)}) = \\max_{(v_i,v_j)\\in e}\\{c_0,c_1,c_q,c_{ij}\\} = m(x,h(e))$ since $m(x,y)\\geq b(x)$ for all $y\\in SOL(x)$. From this fact, we define $g(x,z_{(e)},r) = h(e)$ for every  solution $z_{(e)}\\in SOL_0(f(x,r))$.\nSince $z\\in SOL_0(f(x,r))$, there exists a suitable accepting computation path $e$ in $G^{M_m}_{x}$ satisfying $z=z_{(e)}$.\nThus,  we obtain\n", "index": 13, "text": "\n\\[\nR(x,g(x,z_{(e)},r)) = \\frac{m^*(x)}{m(x,g(x,z_{(e)},r))} = \\frac{m_0^*(f(x,r))}{m_0(f(x,r),z_{(e)})} = R_0(f(x,r),z_{(e)}).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"R(x,g(x,z_{(e)},r))=\\frac{m^{*}(x)}{m(x,g(x,z_{(e)},r))}=\\frac{m_{0}^{*}(f(x,r%&#10;))}{m_{0}(f(x,r),z_{(e)})}=R_{0}(f(x,r),z_{(e)}).\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>m</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mi>m</mi><mn>0</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>m</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><msub><mi>R</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]