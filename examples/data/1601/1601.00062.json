[{"file": "1601.00062.tex", "nexttext": "\n\\end{definition}\n\nWe may also refer to $\\delta$ as the \\textit{isometry constant}. Intuitively, this notion of near-isometry requires the distance of every pair of points in $\\mathcal{X}$ to be nearly preserved. Hegde, et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} develop a framework that seeks to find low rank matrices that satisfy the RIP.\n\n\\subsection{NuMax}\nIn this section, we review Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}'s work on NuMax. Given a data set $\\mathcal{X} \\subset \\mathbb{R}^N$, Hegde et. al. formulate the secant set as follows:\n\n", "itemtype": "equation", "pos": 7432, "prevtext": "\n\\maketitle\n\n\n\\footnotetext[2]{Department of Mathematics, University of Arizona, Tucson, AZ 85721. (jerryluo@math.arizona.edu)}\n\\footnotetext[3]{Department of Computing, Imperial College London, London SW7 2AZ, United Kingdom. (k.shapiro@berkeley.edu)}\n\\footnotetext[4]{Department of Mathematics, University of California, Los Angeles, Los Angeles, CA 90024. (hjmshi@ucla.edu)}\n\\footnotetext[5]{Department of Mathematics, University of Southern California, Los Angeles, 90089. (yangq@usc.edu) }\n\\footnotetext[6]{Department of Computer Science, Columbia University, New York, NY 10027. (kzhu9@ucla.edu)}\n\n\\slugger{siopt}{xxxx}{xx}{x}{x--x}\n\n\\begin{abstract} \nWe propose two practical non-convex approaches for learning near-isometric, linear embeddings of finite sets of data points. Following Hegde, et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}, given a set of training points $\\mathcal{X}$, we consider the secant set $S(\\mathcal{X})$ that consists of all pairwise difference vectors of $\\mathcal{X}$, normalized to lie on the unit sphere. The problem can be formulated as finding a symmetric and positive semi-definite matrix $\\psi$ that preserves the norms of all the vectors in $S(\\mathcal{X})$ up to a distortion parameter $\\delta$. Motivated by non-negative matrix factorization, we reformulate our problem into a Frobenius norm minimization problem, which is solved by the Alternating Direction Method of Multipliers (ADMM) and develop an algorithm, \\textit{FroMax}. Another method solves for a projection matrix $\\psi$ by minimizing the restricted isometry property (RIP) directly over the set of symmetric, postive semi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal mapping, we develop another algorithm, \\textit{NILE-Pro}, for dimensionality reduction. Both non-convex approaches are then empirically demonstrated to be more computationally efficient than prior convex approaches for a number of applications in machine learning and signal processing.\n\\end{abstract} \n\n\\begin{keywords}\ndimensionality reduction, linear embeddings, compressive sensing, approximate nearest neighbors, classification\n\\end{keywords}\n\n\\pagestyle{myheadings}\n\\thispagestyle{plain}\n\n\\section{Introduction}\n\n\\subsection{Motivation}\n\nWe are currently in a ``data crisis\" in which the size and complexity of raw data acquired and processed by diverse modalities poses a challenge to current state-of-the-art information processing systems. Since many machine learning algorithms' computational efficiency scale with the complexity of the data, machine learning researchers have introduced a family of algorithms for \\textit{dimensionality reduction} to address this issue. Dimensionality reduction algorithms devise a concise representation of high-dimensional data on a lower-dimensional subspace, with as minimal loss of intrinsic information as possible. This representation is often referred to as a low-dimensional \\textit{embedding}.\n\nThe canonical approach in statistics for constructing a linear embedding is principal components analysis (PCA) \\cite{moore1981principal}. PCA is a linear embedding technique that projects data points onto a lower-dimensional subspace spanned by the principal components that contain the most variability within the data. PCA enjoys the benefits of being computationally efficient and easily generalizable to new data sets; however, it fails to preserve pairwise distances between sample data points, sometimes rendering two distinct points indistinguishable in the low-dimensional embedding space. This can potentially hamper the performance of PCA and other similar algorithms.\n\nOther popular nonlinear, manifold learning methods, such as ISOMAP and locally linear embedding (LLE), preserve geometric structure by approximating geodesics from $k$-nearest neighbors. However, most fail to preserve all pairwise distances between data points and produce embeddings which are easy to explicitly store and generalize. Note that linear embeddings can be explicitly stored using a matrix operator and can therefore be quickly applied to any new data point.\n\nA linear embedding technique that preserves all pairwise distances is the method of \\textit{random projections}. Given $\\mathcal{X}$, a cloud of $Q$ data points in a high-dimensional Euclidean space $\\mathbb{R}^N$, the Johnson-Lindenstrauss Lemma \\cite{johnsonlindenstrauss} states that there exists a linear, near-isometric, or distance preserving, embedding such that $\\mathcal{X}$ can be mapped to a subspace of dimension $M = \\mathcal{O}(\\log Q)$ with high probability. Despite its conceptual simplicity, random projections suffers from probabilistic and asymptotic theoretical guarantees. A random projections mapping is also independent of the data under consideration, failing to utilize the geometric structure of the data.\n\n\\subsection{Related Work}\n\nUsing the geometric structure of the data, Hegde, et. al. developed a new deterministic approach, NuMax, to construct a near-isometric, linear embedding \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}. Given a training set $\\mathcal{X} \\subset \\mathbb{R}^N$, the \\textit{secant set} is constructed by taking all pairwise difference vectors of $\\mathcal{X}$, which are then normalized to lie on the unit sphere. Hegde, et. al. formulated an affine rank minimization problem to construct a \\textit{projection matrix} $\\psi$ that preserves norms of all vectors in $S(\\mathcal{X})$ up to a distortion parameter $\\delta$. They then relax this problem to a convex program that can be solved using a tractable semidefinite program (SDP), with the help of column generation, and develop NuMax based on the Alternating Direction Method of Multipliers (ADMM). This framework deterministically produces a linear embedding that is near-isometric; however, the algorithm is computationally expensive due to the need to compute a singular value decomposition at each iteration to minimize the nuclear norm. Our proposed approaches build on their original framework by proposing non-convex problems which are solved to produce projection matrices in much faster time. Other algorithmic approaches for finding near-isometric linear embeddings are also described in \\cite{grant2013nearly,hegde2012near,sadeghian2013energy}.\n\n\\subsection{Organization}\n\nThe rest of the paper is organized as follows. We review the restricted isometry property and NuMax algorithm in \\S \\ref{background}. The FroMax algorithm is introduced in \\S \\ref{fromax}. NILE-Pro is discussed in \\S \\ref{nile-pro}. Rank adjustment and column generation methods which increase computational efficiency for large data sets is introduced in \\S \\ref{racg}. Numerical simulations and runtime performance results are presented in \\S \\ref{experiments}. Lastly, \\S \\ref{discussion} concludes the paper and gives direction for future work.\n\n\\section{Background} \\label{background}\n\n\\subsection{Restricted Isometry Property (RIP)}\n\nE. Candes, et. al. introduce a formal, relaxed notion of isometry in \\cite{candes2005decoding} as follows:\\\\\n\n\\begin{definition} \nSuppose $M \\leq N$ and consider $\\mathcal{X} \\subset \\mathbb{R}^N$. An embedding operator $\\mathcal{P} : \\mathcal{X} \\rightarrow \\mathcal{R}^M$ satisfies the \\textit{restricted isometry property (RIP)} on $\\mathcal{X}$ if there exists a positive constant $\\delta > 0$ such that, for every $x, x'$ in $\\mathcal{X}$, the following relation holds:\n\n", "index": 1, "text": "\\begin{equation}\n(1 - \\delta)\\| x - x' \\|_2^2 \\leq \\| \\mathcal{P}x - \\mathcal{P}x' \\|_2^2 \\leq (1 + \\delta)\\|x - x'\\|_2^2.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"(1-\\delta)\\|x-x^{\\prime}\\|_{2}^{2}\\leq\\|\\mathcal{P}x-\\mathcal{P}x^{\\prime}\\|_{%&#10;2}^{2}\\leq(1+\\delta)\\|x-x^{\\prime}\\|_{2}^{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>\u2264</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n Hegde, et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} seeks to find a projection matrix $\\psi \\in \\mathbb{R}^{M \\times N}$ with the smallest possible rank that satisfies the RIP on $S(\\mathcal{X})$ for a given $\\delta > 0$. This problem is then cast as an optimization problem over all symmetric matrices which we denote as $\\mathbb{S}^{N \\times N}$. Let $P = \\psi^T\\psi \\in \\mathbb{S}^{N \\times N}$ with $\\operatorname{rank}(P) = M$. Then for all secants $v_i \\in S(\\mathcal{X})$, we may rewrite the RIP constraint as:\n\n", "itemtype": "equation", "pos": 8158, "prevtext": "\n\\end{definition}\n\nWe may also refer to $\\delta$ as the \\textit{isometry constant}. Intuitively, this notion of near-isometry requires the distance of every pair of points in $\\mathcal{X}$ to be nearly preserved. Hegde, et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} develop a framework that seeks to find low rank matrices that satisfy the RIP.\n\n\\subsection{NuMax}\nIn this section, we review Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}'s work on NuMax. Given a data set $\\mathcal{X} \\subset \\mathbb{R}^N$, Hegde et. al. formulate the secant set as follows:\n\n", "index": 3, "text": "\\begin{equation}\nS(\\mathcal{X}) = \\left\\{ \\dfrac{x-x'}{\\|x - x'\\|_2} , x, x' \\in \\mathcal{X}, x \\neq x'\\right \\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"S(\\mathcal{X})=\\left\\{\\dfrac{x-x^{\\prime}}{\\|x-x^{\\prime}\\|_{2}},x,x^{\\prime}%&#10;\\in\\mathcal{X},x\\neq x^{\\prime}\\right\\}\" display=\"block\"><mrow><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow><msub><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mfrac><mo>,</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>,</mo><mi>x</mi><mo>\u2260</mo><msup><mi>x</mi><mo>\u2032</mo></msup><mo>}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nLet $1_S$ denote the $S$-dimensional ones vector and $\\mathcal{A}: X \\rightarrow \\{ v_i^T X v_i\\}_{i = 1}^S$. This admits the rank minimization problem:\n\n", "itemtype": "equation", "pos": 8818, "prevtext": "\n Hegde, et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} seeks to find a projection matrix $\\psi \\in \\mathbb{R}^{M \\times N}$ with the smallest possible rank that satisfies the RIP on $S(\\mathcal{X})$ for a given $\\delta > 0$. This problem is then cast as an optimization problem over all symmetric matrices which we denote as $\\mathbb{S}^{N \\times N}$. Let $P = \\psi^T\\psi \\in \\mathbb{S}^{N \\times N}$ with $\\operatorname{rank}(P) = M$. Then for all secants $v_i \\in S(\\mathcal{X})$, we may rewrite the RIP constraint as:\n\n", "index": 5, "text": "\\begin{align}\n(1 - \\delta)\\| v_i \\|_2^2 \\leq \\| \\psi v_i \\|_2^2 & \\leq (1 + \\delta)\\|v_i\\|_2^2\\\\\n| \\| \\psi v_i \\|_2^2 - \\| v_i\\|_2^2 | & \\leq \\delta\\\\\n| \\| \\psi v_i \\|_2^2 - 1 | & \\leq \\delta\\\\\n|v_i^TPv_i - 1 | & \\leq \\delta\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1-\\delta)\\|v_{i}\\|_{2}^{2}\\leq\\|\\psi v_{i}\\|_{2}^{2}\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>i</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>\u2264</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq(1+\\delta)\\|v_{i}\\|_{2}^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>i</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|\\|\\psi v_{i}\\|_{2}^{2}-\\|v_{i}\\|_{2}^{2}|\" display=\"inline\"><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>-</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>i</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">|</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\delta\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|\\|\\psi v_{i}\\|_{2}^{2}-1|\" display=\"inline\"><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">|</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\delta\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|v_{i}^{T}Pv_{i}-1|\" display=\"inline\"><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>v</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">|</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\delta\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nHowever, since rank minimization is a non-convex, NP-hard problem, a convex relaxation is performed on the objective to obtain the following nuclear-norm minimization:\n\n", "itemtype": "equation", "pos": 9209, "prevtext": "\n\nLet $1_S$ denote the $S$-dimensional ones vector and $\\mathcal{A}: X \\rightarrow \\{ v_i^T X v_i\\}_{i = 1}^S$. This admits the rank minimization problem:\n\n", "index": 7, "text": "\\begin{equation}\n\\begin{aligned}\n& \\underset{P}{\\text{minimize}}\n& & \\operatorname{rank}(P) \\\\\n& \\text{subject to}\n& & \\|\\mathcal{A}(P) - 1_S\\|_\\infty \\leq \\delta\\\\ \n& & & P \\succeq 0\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mo>\ud835\udc43</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\operatorname{rank}(P)\" display=\"inline\"><mrow><mo>rank</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mathcal{A}(P)-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle P\\succeq 0\" display=\"inline\"><mrow><mi>P</mi><mo>\u2ab0</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nwhere $\\| P \\|_*$ is the nuclear norm, which is the sum of the singular values of $P$. Then the desired linear embedding $\\psi \\in \\mathbb{R}^{M \\times N}$ can be found by taking a matrix square root of the minimizer $P^* = U \\Gamma U^T$ by\n\n", "itemtype": "equation", "pos": 9590, "prevtext": "\nHowever, since rank minimization is a non-convex, NP-hard problem, a convex relaxation is performed on the objective to obtain the following nuclear-norm minimization:\n\n", "index": 9, "text": "\\begin{equation}\n\\begin{aligned}\n&\\underset{P}{\\text{minimize}}\n& & \\|P\\|_* \\\\\n& \\text{subject to}\n& & \\|\\mathcal{A}(P) - 1_S\\|_\\infty \\leq \\delta \\\\\n& & & P \\succeq 0\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mo>\ud835\udc43</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|P\\|_{*}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mi>P</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mathcal{A}(P)-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle P\\succeq 0\" display=\"inline\"><mrow><mi>P</mi><mo>\u2ab0</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nwhere $\\Gamma_M = \\operatorname{diag}\\{\\lambda_1, ..., \\lambda_M\\}$ denotes the M leading (non-zero) eigenvalues of $P^*$ and $U_M$ are the corresponding eigenvectors. \n\nApplying the Alternating Direction Method of Multipliers (ADMM), the optimization problem is rewritten by introducing auxilliary variables $L \\in \\mathbb{S}^{N \\times N}$ and $q \\in \\mathbb{R}^S$:\n\n", "itemtype": "equation", "pos": 10028, "prevtext": "\nwhere $\\| P \\|_*$ is the nuclear norm, which is the sum of the singular values of $P$. Then the desired linear embedding $\\psi \\in \\mathbb{R}^{M \\times N}$ can be found by taking a matrix square root of the minimizer $P^* = U \\Gamma U^T$ by\n\n", "index": 11, "text": "\\begin{equation}\n\\psi = \\Gamma_M^{1/2}U_M^T\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\psi=\\Gamma_{M}^{1/2}U_{M}^{T}\" display=\"block\"><mrow><mi>\u03c8</mi><mo>=</mo><mrow><msubsup><mi mathvariant=\"normal\">\u0393</mi><mi>M</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><msubsup><mi>U</mi><mi>M</mi><mi>T</mi></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nThe linear constraints are then relaxed to form an augmented Lagrangian as follows:\n\n", "itemtype": "equation", "pos": 10454, "prevtext": "\nwhere $\\Gamma_M = \\operatorname{diag}\\{\\lambda_1, ..., \\lambda_M\\}$ denotes the M leading (non-zero) eigenvalues of $P^*$ and $U_M$ are the corresponding eigenvectors. \n\nApplying the Alternating Direction Method of Multipliers (ADMM), the optimization problem is rewritten by introducing auxilliary variables $L \\in \\mathbb{S}^{N \\times N}$ and $q \\in \\mathbb{R}^S$:\n\n", "index": 13, "text": "\\begin{equation}\n\\begin{aligned}\n&\\underset{P, L, q}{\\text{minimize}}\n& & \\|P\\|_* \\\\\n& \\text{subject to}\n& & P = L\\\\\n& & & \\mathcal{A}(L) = q, \\\\\n& & &\\|q - 1_S\\|_\\infty \\leq \\delta\\\\ \n& & & P \\succeq 0\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P,L,q}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>P</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>q</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|P\\|_{*}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mi>P</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle P=L\" display=\"inline\"><mrow><mi>P</mi><mo>=</mo><mi>L</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{A}(L)=q,\" display=\"inline\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>q</mi></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xc.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|q-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xd.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle P\\succeq 0\" display=\"inline\"><mrow><mi>P</mi><mo>\u2ab0</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nNuMax then solves the following augmented Lagrangian problem:\n\n", "itemtype": "equation", "pos": 10770, "prevtext": "\nThe linear constraints are then relaxed to form an augmented Lagrangian as follows:\n\n", "index": 15, "text": "\\begin{equation}\nL_A(P, L, q; \\Gamma, \\omega) = \\|P\\|_* + \\frac{\\beta_1}{2} \\|P - L - \\Gamma\\|_F^2 + \\frac{\\beta_2}{2}\\|\\mathcal{A}(L) - q - \\omega\\|_2^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"L_{A}(P,L,q;\\Gamma,\\omega)=\\|P\\|_{*}+\\frac{\\beta_{1}}{2}\\|P-L-\\Gamma\\|_{F}^{2}%&#10;+\\frac{\\beta_{2}}{2}\\|\\mathcal{A}(L)-q-\\omega\\|_{2}^{2}\" display=\"block\"><mrow><mrow><msub><mi>L</mi><mi>A</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>q</mi><mo>;</mo><mi mathvariant=\"normal\">\u0393</mi><mo>,</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mo>\u2225</mo><mi>P</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>1</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mi>L</mi><mo>-</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>2</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi><mo>-</mo><mi>\u03c9</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nwhere $\\Gamma \\in \\mathbb{S}^{N \\times N}$ and $\\omega \\in \\mathbb{R}^S$ represent the scaled Lagrange multipliers. $P,L$ and $q$ are optimized in an alternating fashion, i.e. optimized one at a time with the others held fixed. This optimization can then be solved by three easier sub-problems, admitting a computationally efficient solution. \n\nFor more information regarding theoretical and empirical properties of NuMax, please refer to Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}.\n\nThis framework, though slower than conventional methods such as PCA and random projections, admits a projection matrix satisfying the RIP. However, NuMax computes a singular value decomposition of $P$ each iteration, which is computationally expensive. Furthermore, though minimizing the nuclear-norm tends to give low rank matrices, NuMax does not theoretically guarantee the lowest rank embedding for a given $\\delta$. \n\nThese issues motivate the pursuit of other practical algorithms that optimize similar non-convex problems that may admit low rank, near-isometric projection matrices that give faster, but sufficient (not necessarily optimal) results. Rather than solving both the rank minimization and near-isometry problems simultaneously, we solve a simpler non-convex problem quickly to find a near-isometric projection matrix and apply a rank adjustment heuristic to choose a minimal rank.\n\n\\subsection{Non-Negative Matrix Factorization (NMF)}\n\nOne of our algorithms is motivated by ideas from \\textit{non-negative matrix factorization}. Non-negative matrix factorization (NMF) is a group of algorithms that factorize a non-negative matrix $V$ into two low-rank non-negative matrices $W$ and $H$ \\cite{NIPS2000_1861}. More rigorously, let $V \\in \\mathbb{R}^{N \\times M}$ be given, then we solve for $W \\in \\mathbb{R}^{M \\times Q}$, and $H \\in \\mathbb{R}^{Q \\times N}$ by solving the following optimization problem:\n\n", "itemtype": "equation", "pos": 11002, "prevtext": "\n\nNuMax then solves the following augmented Lagrangian problem:\n\n", "index": 17, "text": "\\begin{equation}\n\\begin{aligned}\n&\\underset{P, L, q, \\Gamma, \\omega}{\\text{minimize}}\n& & L_A(P, L, q, \\Gamma, \\omega)\\\\\n& \\text{subject to}\n& & \\|q - 1_S\\|_\\infty \\leq \\delta\\\\\n& & & P \\succeq 0\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P,L,q,\\Gamma,\\omega}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>P</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi mathvariant=\"normal\">\u0393</mi><mo>,</mo><mi>\u03c9</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle L_{A}(P,L,q,\\Gamma,\\omega)\" display=\"inline\"><mrow><msub><mi>L</mi><mi>A</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi mathvariant=\"normal\">\u0393</mi><mo>,</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|q-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle P\\succeq 0\" display=\"inline\"><mrow><mi>P</mi><mo>\u2ab0</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nNMF motivates the problem formulation for our first algorithm, FroMax.\n\n\\section{FroMax} \\label{fromax}\n\nOur first algorithm, \\textit{Frobenius norm minimization with Max-norm constraints}, or \\textit{FroMax} mixes ideas from NuMax and NMF to formulate a Frobenius norm minimization problem which we then solve based on ADMM, similar to NuMax \\cite{XuYinWenZhang2012_alternating}. Note that this algorithm does not discover the optimal rank for $\\psi$. We combine FroMax with a rank adjustment heuristic to find low rank embeddings.\n\n\\subsection{Optimization Framework}\n\nWe formulate a specialized matrix factorization minimization problem to solve for a near-isometric linear embedding as follows: \nGiven a desired rank $r$, let $\\psi \\in \\mathbb{R}^{r \\times N}$. Here, we seek to solve:\n\n", "itemtype": "equation", "pos": 13157, "prevtext": "\nwhere $\\Gamma \\in \\mathbb{S}^{N \\times N}$ and $\\omega \\in \\mathbb{R}^S$ represent the scaled Lagrange multipliers. $P,L$ and $q$ are optimized in an alternating fashion, i.e. optimized one at a time with the others held fixed. This optimization can then be solved by three easier sub-problems, admitting a computationally efficient solution. \n\nFor more information regarding theoretical and empirical properties of NuMax, please refer to Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax}.\n\nThis framework, though slower than conventional methods such as PCA and random projections, admits a projection matrix satisfying the RIP. However, NuMax computes a singular value decomposition of $P$ each iteration, which is computationally expensive. Furthermore, though minimizing the nuclear-norm tends to give low rank matrices, NuMax does not theoretically guarantee the lowest rank embedding for a given $\\delta$. \n\nThese issues motivate the pursuit of other practical algorithms that optimize similar non-convex problems that may admit low rank, near-isometric projection matrices that give faster, but sufficient (not necessarily optimal) results. Rather than solving both the rank minimization and near-isometry problems simultaneously, we solve a simpler non-convex problem quickly to find a near-isometric projection matrix and apply a rank adjustment heuristic to choose a minimal rank.\n\n\\subsection{Non-Negative Matrix Factorization (NMF)}\n\nOne of our algorithms is motivated by ideas from \\textit{non-negative matrix factorization}. Non-negative matrix factorization (NMF) is a group of algorithms that factorize a non-negative matrix $V$ into two low-rank non-negative matrices $W$ and $H$ \\cite{NIPS2000_1861}. More rigorously, let $V \\in \\mathbb{R}^{N \\times M}$ be given, then we solve for $W \\in \\mathbb{R}^{M \\times Q}$, and $H \\in \\mathbb{R}^{Q \\times N}$ by solving the following optimization problem:\n\n", "index": 19, "text": "\\begin{equation}\n\\begin{aligned}\n&\\underset{W, H}{\\text{minimize}}\n& & \\|WH - V \\|_F^2\\\\\n& \\text{subject to}\n& & W_{ij} \\geq 0, H_{ij} \\geq 0, \\forall ~ i,j \\\\\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{W,H}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>W</mi><mo>,</mo><mi>H</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|WH-V\\|_{F}^{2}\" display=\"inline\"><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>W</mi><mo>\u2062</mo><mi>H</mi></mrow><mo>-</mo><mi>V</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle W_{ij}\\geq 0,H_{ij}\\geq 0,\\forall~{}i,j\" display=\"inline\"><mrow><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>H</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2265</mo><mrow><mn>0</mn><mo>,</mo><mrow><mo rspace=\"5.8pt\">\u2200</mo><mi>i</mi></mrow><mo>,</mo><mi>j</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nWe introduce auxiliary variables to apply ADMM. In particular, let $Y = \\psi \\in \\mathbb{R}^{r \\times N}$, $X = Y^T \\in \\mathbb{R}^{N \\times r}$ and $P \\in \\mathbb{R}^{N \\times N}$. Then\n\n", "itemtype": "equation", "pos": 14136, "prevtext": "\nNMF motivates the problem formulation for our first algorithm, FroMax.\n\n\\section{FroMax} \\label{fromax}\n\nOur first algorithm, \\textit{Frobenius norm minimization with Max-norm constraints}, or \\textit{FroMax} mixes ideas from NuMax and NMF to formulate a Frobenius norm minimization problem which we then solve based on ADMM, similar to NuMax \\cite{XuYinWenZhang2012_alternating}. Note that this algorithm does not discover the optimal rank for $\\psi$. We combine FroMax with a rank adjustment heuristic to find low rank embeddings.\n\n\\subsection{Optimization Framework}\n\nWe formulate a specialized matrix factorization minimization problem to solve for a near-isometric linear embedding as follows: \nGiven a desired rank $r$, let $\\psi \\in \\mathbb{R}^{r \\times N}$. Here, we seek to solve:\n\n", "index": 21, "text": "\\begin{equation}\\label{eq:fromax}\n\\begin{aligned}\n& \\underset{P, \\psi}{\\text{minimize}}\n& & \\frac{1}{2}\\|P - \\psi^T \\psi \\|_F^2 \\\\\n& \\text{subject to}\n& & \\| \\mathcal{A}(P) - 1_S\\|_\\infty \\leq \\delta\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P,\\psi}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>P</mi><mo>,</mo><mi>\u03c8</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2}\\|P-\\psi^{T}\\psi\\|_{F}^{2}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mathcal{A}(P)-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nThis gives $Y = \\psi \\in \\mathbb{R}^{r \\times N}$ such that the RIP holds for all secant vectors in the secant set $S(\\mathcal{X})$ for an isometry constant $\\delta$. The optimization formulation for (\\ref{eq:fromax}) is conceptually simple, only requiring the input data set $\\mathcal{X}$, desired isometry constant $\\delta > 0$ and desired rank $r$.\n\nAn important caveat is that our optimization problem is non-convex. Thus, we cannot guarantee that FroMax will converge to the optimal solution of (\\ref{eq:fromax}). However, various experiments in \\S \\ref{experiments} indicate that FroMax yields excellent, stable results for real-world data sets and finds projection matrices much more quickly than NuMax and other convex approaches. We implement ADMM since Wang et. al. \\cite{wang2015global} indicate that ADMM is more likely to converge than the Augmented Lagrangian Method for nonconvex, nonsmooth problems.\n\n\\subsection{ADMM}\n\nWe develop our algorithm, FroMax, to solve (\\ref{eq:fromax_admm}) based on ADMM. We relax the linear constraints and form an augmented Lagrangian of (\\ref{eq:fromax_admm}) as follows:\n\n", "itemtype": "equation", "pos": 14553, "prevtext": "\n\nWe introduce auxiliary variables to apply ADMM. In particular, let $Y = \\psi \\in \\mathbb{R}^{r \\times N}$, $X = Y^T \\in \\mathbb{R}^{N \\times r}$ and $P \\in \\mathbb{R}^{N \\times N}$. Then\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:fromax_admm}\n\\begin{aligned}\n& \\underset{P,X,Y,q}{\\text{minimize}}\n& & \\frac{1}{2}\\|P - XY\\|_F^2 \\\\\n& \\text{subject to}\n& &  \\mathcal{A}(P) = q\\\\ \n& & & Y = X^T\\\\\n& & & \\| q - 1_S\\|_\\infty \\leq \\delta\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{P,X,Y,q}{\\text{minimize}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>P</mi><mo>,</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>q</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2}\\|P-XY\\|_{F}^{2}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{A}(P)=q\" display=\"inline\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>q</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle Y=X^{T}\" display=\"inline\"><mrow><mi>Y</mi><mo>=</mo><msup><mi>X</mi><mi>T</mi></msup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15Xc.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|q-1_{S}\\|_{\\infty}\\leq\\delta\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nHere, $\\Gamma \\in \\mathbb{R}^{N \\times N}$ and $\\Pi \\in \\mathbb{R}^{r \\times N}$ represent the scaled Lagrange multipliers. The indicator function, $\\iota_{\\{q:\\|q - 1_S\\|_\\infty \\leq \\delta\\}}$, is defined as \n\n", "itemtype": "equation", "pos": 15929, "prevtext": "\nThis gives $Y = \\psi \\in \\mathbb{R}^{r \\times N}$ such that the RIP holds for all secant vectors in the secant set $S(\\mathcal{X})$ for an isometry constant $\\delta$. The optimization formulation for (\\ref{eq:fromax}) is conceptually simple, only requiring the input data set $\\mathcal{X}$, desired isometry constant $\\delta > 0$ and desired rank $r$.\n\nAn important caveat is that our optimization problem is non-convex. Thus, we cannot guarantee that FroMax will converge to the optimal solution of (\\ref{eq:fromax}). However, various experiments in \\S \\ref{experiments} indicate that FroMax yields excellent, stable results for real-world data sets and finds projection matrices much more quickly than NuMax and other convex approaches. We implement ADMM since Wang et. al. \\cite{wang2015global} indicate that ADMM is more likely to converge than the Augmented Lagrangian Method for nonconvex, nonsmooth problems.\n\n\\subsection{ADMM}\n\nWe develop our algorithm, FroMax, to solve (\\ref{eq:fromax_admm}) based on ADMM. We relax the linear constraints and form an augmented Lagrangian of (\\ref{eq:fromax_admm}) as follows:\n\n", "index": 25, "text": "\\begin{multline}\\label{eq:fromax_LA}\nL_A(X,Y,q,P) = \\frac{1}{2}\\|P-XY\\|_F^2 + \\Gamma \\cdot (\\mathcal{A}(P) - q) + \\Pi \\cdot (Y - X^T) \\\\ + \\frac{\\beta_1}{2} \\|\\mathcal{A}(P) - q\\|_2^2 + \\frac{\\beta_2}{2}\\|Y - X^T\\|_F^2+\\iota_{\\{q:\\|q - 1_S\\|_\\infty \\leq \\delta\\}}\n\\end{multline}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle L_{A}(X,Y,q,P)=\\frac{1}{2}\\|P-XY\\|_{F}^{2}+\\Gamma\\cdot(\\mathcal{%&#10;A}(P)-q)+\\Pi\\cdot(Y-X^{T})\\\\&#10;\\displaystyle+\\frac{\\beta_{1}}{2}\\|\\mathcal{A}(P)-q\\|_{2}^{2}+\\frac{\\beta_{2}}%&#10;{2}\\|Y-X^{T}\\|_{F}^{2}+\\iota_{\\{q:\\|q-1_{S}\\|_{\\infty}\\leq\\delta\\}}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>L</mi><mi>A</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>1</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>2</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><msub><mi>\u03b9</mi><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": " The optimization in (\\ref{eq:fromax_LA}) is carried out over $P \\in \\mathbb{R}^{N\\times N}$, $X \\in \\mathbb{R}^{N \\times r}$, $Y \\in \\mathbb{R}^{r \\times N}$, and $q \\in \\mathbb{R}^S$, while $\\Gamma$ and $\\Pi$ are also iteratively updated. We optimize each variable in an alternating fashion like NuMax. The following steps below are performed until convergence.\n\n\\textbf{Update $q$}: Isolating the terms that involve $q$, we obtain a new estimate $q_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nHere, $\\Gamma \\in \\mathbb{R}^{N \\times N}$ and $\\Pi \\in \\mathbb{R}^{r \\times N}$ represent the scaled Lagrange multipliers. The indicator function, $\\iota_{\\{q:\\|q - 1_S\\|_\\infty \\leq \\delta\\}}$, is defined as \n\n", "index": 27, "text": "$$\\iota_{\\{q:\\|q - 1_S\\|_\\infty \\leq \\delta\\}} = \\begin{cases}\n0 & \\text{ if }\\|q - 1_S \\|_\\infty \\leq \\delta\\\\\n\\infty & \\text{ otherwise}\n\\end{cases}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\iota_{\\{q:\\|q-1_{S}\\|_{\\infty}\\leq\\delta\\}}=\\begin{cases}0&amp;\\text{ if }\\|q-1_{%&#10;S}\\|_{\\infty}\\leq\\delta\\\\&#10;\\infty&amp;\\text{ otherwise}\\end{cases}\" display=\"block\"><mrow><msub><mi>\u03b9</mi><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">}</mo></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mo>\u2264</mo><mi>\u03b4</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi mathvariant=\"normal\">\u221e</mi></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nDefine $z = \\mathcal{A}(P) - \\Pi - 1_S$. Using a component-wise truncation procedure for entries in $q$, we easily see that\n\n", "itemtype": "equation", "pos": 17088, "prevtext": " The optimization in (\\ref{eq:fromax_LA}) is carried out over $P \\in \\mathbb{R}^{N\\times N}$, $X \\in \\mathbb{R}^{N \\times r}$, $Y \\in \\mathbb{R}^{r \\times N}$, and $q \\in \\mathbb{R}^S$, while $\\Gamma$ and $\\Pi$ are also iteratively updated. We optimize each variable in an alternating fashion like NuMax. The following steps below are performed until convergence.\n\n\\textbf{Update $q$}: Isolating the terms that involve $q$, we obtain a new estimate $q_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 29, "text": "\\begin{equation}\nq_{k + 1} \\leftarrow \\underset{q}{\\arg \\min } ~ \\Gamma \\cdot (\\mathcal{A}(P) - q) + \\frac{\\beta_1}{2}\\|\\mathcal{A}(P) - q \\|_2^2 + \\iota_{\\{q:\\|q - \\textbf{1}_S\\|_\\infty \\leq \\delta\\}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"q_{k+1}\\leftarrow\\underset{q}{\\arg\\min}~{}\\Gamma\\cdot(\\mathcal{A}(P)-q)+\\frac{%&#10;\\beta_{1}}{2}\\|\\mathcal{A}(P)-q\\|_{2}^{2}+\\iota_{\\{q:\\|q-\\textbf{1}_{S}\\|_{%&#10;\\infty}\\leq\\delta\\}}\" display=\"block\"><mrow><msub><mi>q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc5e</mo></munder></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>1</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msub><mi>\u03b9</mi><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mtext>\ud835\udfcf</mtext><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nwhere the sign and min operators are applied component-wise. \n\n\\textbf{Update $P$}: Isolating the terms that involve $P$, we obtain a new estimate $P_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": 17429, "prevtext": "\nDefine $z = \\mathcal{A}(P) - \\Pi - 1_S$. Using a component-wise truncation procedure for entries in $q$, we easily see that\n\n", "index": 31, "text": "\\begin{equation} \nq_{k + 1} = 1_S + \\operatorname{sign}(z) \\cdot \\min(|z|, \\delta)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"q_{k+1}=1_{S}+\\operatorname{sign}(z)\\cdot\\min(|z|,\\delta)\" display=\"block\"><mrow><msub><mi>q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mn>1</mn><mi>S</mi></msub><mo>+</mo><mrow><mrow><mo>sign</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><mi>z</mi><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nsuch that $P \\succeq 0$. Since this is a least-squares problem, we can solve for the minimum by solving the linear system of equations\n\n", "itemtype": "equation", "pos": 17742, "prevtext": "\nwhere the sign and min operators are applied component-wise. \n\n\\textbf{Update $P$}: Isolating the terms that involve $P$, we obtain a new estimate $P_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 33, "text": "\\begin{equation}\nP_{k + 1} \\leftarrow \\underset{P}{\\arg \\min} ~ \\frac{1}{2}\\|P-XY\\|_F^2 + \\Gamma \\cdot (\\mathcal{A}(P) - q) + \\frac{\\beta_1}{2} \\|\\mathcal{A}(P) - q\\|_2^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"P_{k+1}\\leftarrow\\underset{P}{\\arg\\min}~{}\\frac{1}{2}\\|P-XY\\|_{F}^{2}+\\Gamma%&#10;\\cdot(\\mathcal{A}(P)-q)+\\frac{\\beta_{1}}{2}\\|\\mathcal{A}(P)-q\\|_{2}^{2}\" display=\"block\"><mrow><msub><mi>P</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc43</mo></munder></mpadded><mo>\u2062</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>1</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nwhere $\\mathcal{A^*}$ is the adjoint of $\\mathcal{A}$.\n\n\\textbf{Update $X$}: Isolating the terms that involve $X$, we obtain a new estimate $X_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": 18063, "prevtext": "\nsuch that $P \\succeq 0$. Since this is a least-squares problem, we can solve for the minimum by solving the linear system of equations\n\n", "index": 35, "text": "\\begin{equation}\n(P - XY) + \\sum_{j = 1}^s \\Gamma_j v_j v_j^T + \\beta_1 \\mathcal{A}^*(\\mathcal{A}(P) - q) = 0\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"(P-XY)+\\sum_{j=1}^{s}\\Gamma_{j}v_{j}v_{j}^{T}+\\beta_{1}\\mathcal{A}^{*}(%&#10;\\mathcal{A}(P)-q)=0\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>v</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>v</mi><mi>j</mi><mi>T</mi></msubsup></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b2</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nIt is easily seen that this can be solved similarly to the $P$ update.\n\n\\textbf{Update $Y$}: Isolating the terms that involve $Y$, we obtain a new estimate $Y_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": 18396, "prevtext": "\nwhere $\\mathcal{A^*}$ is the adjoint of $\\mathcal{A}$.\n\n\\textbf{Update $X$}: Isolating the terms that involve $X$, we obtain a new estimate $X_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 37, "text": "\\begin{equation}\nX_{k+1} \\leftarrow \\underset{X}{\\arg \\min} ~ \\frac{1}{2}\\|P - XY \\|_F^2 + \\Pi \\cdot (Y - X^T) + \\frac{\\beta_2}{2}\\|Y - X^T\\|_F^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"X_{k+1}\\leftarrow\\underset{X}{\\arg\\min}~{}\\frac{1}{2}\\|P-XY\\|_{F}^{2}+\\Pi\\cdot%&#10;(Y-X^{T})+\\frac{\\beta_{2}}{2}\\|Y-X^{T}\\|_{F}^{2}\" display=\"block\"><mrow><msub><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc4b</mo></munder></mpadded><mo>\u2062</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>2</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nIt is easily seen that this can be solved similarly to the $X$ update.\n\n\\textbf{Update $\\Gamma, \\Pi$}: Following standard augmented Lagrangian methods, we update $\\Gamma, \\Pi$ according to the following equations\n\n", "itemtype": "equation", "pos": 18782, "prevtext": "\n\nIt is easily seen that this can be solved similarly to the $P$ update.\n\n\\textbf{Update $Y$}: Isolating the terms that involve $Y$, we obtain a new estimate $Y_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 39, "text": "\\begin{equation}\nY_{k+1} \\leftarrow \\underset{Y}{\\arg \\min} ~ \\frac{1}{2}\\|P - XY \\|_F^2 + \\Pi \\cdot (Y - X^T) + \\frac{\\beta_2}{2}\\|Y - X^T\\|_F^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"Y_{k+1}\\leftarrow\\underset{Y}{\\arg\\min}~{}\\frac{1}{2}\\|P-XY\\|_{F}^{2}+\\Pi\\cdot%&#10;(Y-X^{T})+\\frac{\\beta_{2}}{2}\\|Y-X^{T}\\|_{F}^{2}\" display=\"block\"><mrow><msub><mi>Y</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc4c</mo></munder></mpadded><mo>\u2062</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>P</mi><mo>-</mo><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>\u03b2</mi><mn>2</mn></msub><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>Y</mi><mo>-</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nPseudocode for FroMax may be found in Algorithm \\ref{alg:fromax}. Convergence properties of FroMax are highly dependent on chosen parameters $\\eta, \\beta_1,$ and $\\beta_2$. \n\n\\begin{algorithm}[tb]\n   \\caption{FroMax}\\label{alg:fromax}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, desired rank for $P$ $r$, max iterations $m > 0$\n   \\FOR{$k = 1$ {\\bfseries to} $m$}\n   \\STATE $z \\leftarrow \\mathcal{A}(P_k) + \\frac{\\Gamma}{\\beta_1} - 1_S$\n\\STATE $q_{k+1} \\leftarrow {\\bf 1}_S + \\operatorname{sign}(z) \\cdot \\min(|z|,\\delta)$\n\\STATE $P_{k+1} \\leftarrow (I + \\beta_1 \\mathcal{A} ^*\\mathcal{A})^\\dag(\\beta_1 \\mathcal{A}^*q_{k+1}+X_kY_k-\\sum_{j = 1}^s \\Gamma_j v_j v_j^T)$\n\\STATE $X_{k+1} \\leftarrow (\\Pi_k^T + \\beta_2Y_k^T + P_{k+1}Y_k^T)(Y_kY_k^T + \\beta_2I)^{-1}$\n\\STATE $Y_{k+1} \\leftarrow (X_{k+1}^TX_{k+1} + \\beta_2I)^{-1}(X_{k+1}^TP_{k+1} - \\Pi_k + \\beta_2X_{k+1}^T)$\n\\STATE $\\Gamma_{k + 1} \\leftarrow \\Gamma_k + \\eta\\beta_1(\\mathcal{A}(P_{k+1}) - q_{k+1})$\n\\STATE $\\Pi_{k+1} \\leftarrow \\Pi_k + \\eta\\beta_2(Y_{k+1} - X_{k+1}^T)$\n\\IF{$\\frac{1}{2}\\|P_{k+1} - X_{k+1}Y_{k+1}\\|_F^2 < \\epsilon$}\n\\STATE break\n\\ENDIF\n   \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{NILE-Pro} \\label{nile-pro}\n\nOur second algorithm, \\textit{Near-Isometric Linear Embedding via Proximal Mapping}, or \\textit{NILE-Pro} seeks to minimize the RIP constraint directly to solve for $\\psi$. This minimization problem is solved using ADMM and a Moreau decomposition on a proximal mapping.\n\n\\subsection{Optimization Framework}\n\nWe formulate a new framework for NILE-Pro. We solve for our desired linear embedding $\\psi$ directly:\n\n", "itemtype": "equation", "pos": 19157, "prevtext": "\n\nIt is easily seen that this can be solved similarly to the $X$ update.\n\n\\textbf{Update $\\Gamma, \\Pi$}: Following standard augmented Lagrangian methods, we update $\\Gamma, \\Pi$ according to the following equations\n\n", "index": 41, "text": "\\begin{align}\n\\Gamma_{k + 1} \\leftarrow & \\Gamma_k + \\eta\\beta_1(\\mathcal{A}(P_{k+1}) - q_{k+1})\\\\\n\\Pi_{k+1} \\leftarrow  & \\Pi_k + \\eta \\beta_2(Y_{k+1} - X_{k+1}^T)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Gamma_{k+1}\\leftarrow\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Gamma_{k}+\\eta\\beta_{1}(\\mathcal{A}(P_{k+1})-q_{k+1})\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mi>k</mi></msub><mo>+</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>P</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pi_{k+1}\\leftarrow\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pi_{k}+\\eta\\beta_{2}(Y_{k+1}-X_{k+1}^{T})\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mi>k</mi></msub><mo>+</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Y</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mi>T</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nBy introducing another variable $q$, we then have the following minimization problem:\n\n", "itemtype": "equation", "pos": 21032, "prevtext": "\nPseudocode for FroMax may be found in Algorithm \\ref{alg:fromax}. Convergence properties of FroMax are highly dependent on chosen parameters $\\eta, \\beta_1,$ and $\\beta_2$. \n\n\\begin{algorithm}[tb]\n   \\caption{FroMax}\\label{alg:fromax}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, desired rank for $P$ $r$, max iterations $m > 0$\n   \\FOR{$k = 1$ {\\bfseries to} $m$}\n   \\STATE $z \\leftarrow \\mathcal{A}(P_k) + \\frac{\\Gamma}{\\beta_1} - 1_S$\n\\STATE $q_{k+1} \\leftarrow {\\bf 1}_S + \\operatorname{sign}(z) \\cdot \\min(|z|,\\delta)$\n\\STATE $P_{k+1} \\leftarrow (I + \\beta_1 \\mathcal{A} ^*\\mathcal{A})^\\dag(\\beta_1 \\mathcal{A}^*q_{k+1}+X_kY_k-\\sum_{j = 1}^s \\Gamma_j v_j v_j^T)$\n\\STATE $X_{k+1} \\leftarrow (\\Pi_k^T + \\beta_2Y_k^T + P_{k+1}Y_k^T)(Y_kY_k^T + \\beta_2I)^{-1}$\n\\STATE $Y_{k+1} \\leftarrow (X_{k+1}^TX_{k+1} + \\beta_2I)^{-1}(X_{k+1}^TP_{k+1} - \\Pi_k + \\beta_2X_{k+1}^T)$\n\\STATE $\\Gamma_{k + 1} \\leftarrow \\Gamma_k + \\eta\\beta_1(\\mathcal{A}(P_{k+1}) - q_{k+1})$\n\\STATE $\\Pi_{k+1} \\leftarrow \\Pi_k + \\eta\\beta_2(Y_{k+1} - X_{k+1}^T)$\n\\IF{$\\frac{1}{2}\\|P_{k+1} - X_{k+1}Y_{k+1}\\|_F^2 < \\epsilon$}\n\\STATE break\n\\ENDIF\n   \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{NILE-Pro} \\label{nile-pro}\n\nOur second algorithm, \\textit{Near-Isometric Linear Embedding via Proximal Mapping}, or \\textit{NILE-Pro} seeks to minimize the RIP constraint directly to solve for $\\psi$. This minimization problem is solved using ADMM and a Moreau decomposition on a proximal mapping.\n\n\\subsection{Optimization Framework}\n\nWe formulate a new framework for NILE-Pro. We solve for our desired linear embedding $\\psi$ directly:\n\n", "index": 43, "text": "\\begin{equation}\\label{eq:nile-pro}\n\\underset{\\psi}{\\text{minimize }} \\|\\mathcal{A}(\\psi^T\\psi)-1_S\\|_\\infty\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\underset{\\psi}{\\text{minimize }}\\|\\mathcal{A}(\\psi^{T}\\psi)-1_{S}\\|_{\\infty}\" display=\"block\"><mrow><munder accentunder=\"true\"><mtext>minimize\u00a0</mtext><mo>\ud835\udf13</mo></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nWe apply ADMM and use a Moreau decomposition on a proximal mapping to solve for updates. Like FroMax, this optimization problem is non-convex and thus, we cannot guarantee that NILE-Pro will converge to the optimal solution of (\\ref{eq:nile-pro}). However, we demonstrate in \\S \\ref{experiments} that NILE-Pro may produce stable, excellent results for synthetic and real-world data sets at a much faster rate than both FroMax and NuMax due to the simplified problem it solves.\n\n\\subsection{Proximal Mapping and Moreau Decomposition}\n\nWe introduce some machinery to solve this minimization problem \\cite{Rockafellar70convexanalysis}:\n\n\\begin{definition} \nThe \\textit{proximal mapping} of a convex and proper function $f$ is defined to be \n", "itemtype": "equation", "pos": 21243, "prevtext": "\n\nBy introducing another variable $q$, we then have the following minimization problem:\n\n", "index": 45, "text": "\\begin{equation}\n\\begin{aligned}\n\\underset{q,\\psi}{\\text{minimize }} & \\|q- 1_s\\| _\\infty\\\\\n\\text{subject to }&  q =  \\mathcal{A}(\\psi^T\\psi)\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{q,\\psi}{\\text{minimize }}\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mrow><mi>q</mi><mo>,</mo><mi>\u03c8</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|q-1_{s}\\|_{\\infty}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>s</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle q=\\mathcal{A}(\\psi^{T}\\psi)\" display=\"inline\"><mrow><mi>q</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\\end{definition}\\\\\n\n\\begin{theorem} \nIf a function $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ is proper, closed, and convex, then ${\\operatorname{prox}}_f(x)$ exists, well-defined, and unique for all $x$.\n\\end{theorem}\\\\\n\nMoreau's identity allows us to decompose any $x$ into \n", "itemtype": "equation", "pos": 22151, "prevtext": "\nWe apply ADMM and use a Moreau decomposition on a proximal mapping to solve for updates. Like FroMax, this optimization problem is non-convex and thus, we cannot guarantee that NILE-Pro will converge to the optimal solution of (\\ref{eq:nile-pro}). However, we demonstrate in \\S \\ref{experiments} that NILE-Pro may produce stable, excellent results for synthetic and real-world data sets at a much faster rate than both FroMax and NuMax due to the simplified problem it solves.\n\n\\subsection{Proximal Mapping and Moreau Decomposition}\n\nWe introduce some machinery to solve this minimization problem \\cite{Rockafellar70convexanalysis}:\n\n\\begin{definition} \nThe \\textit{proximal mapping} of a convex and proper function $f$ is defined to be \n", "index": 47, "text": "$${\\operatorname{prox}}_f(x) = \\underset{u}{\\arg\\min} ( f(u) + \\frac{1}{2}\\|u - x\\|_2^2)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{prox}}_{f}(x)=\\underset{u}{\\arg\\min}(f(u)+\\frac{1}{2}\\|u-x\\|_{2%&#10;}^{2})\" display=\"block\"><mrow><mrow><msub><mo>prox</mo><mi>f</mi></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc62</mo></munder><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>u</mi><mo>-</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": " where $f^*$ is the convex conjugate of $f$. This decomposition, called the \\textit{Moreau decomposition}, generalizes the orthogonal decomposition on subspaces. We apply this machinery to help us solve for the update for $q$.\n\n\\subsection{ADMM}\n\nFollowing a similar method as FroMax, we relax our linear constraints and find our augmented Lagrangian of (14): \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\end{definition}\\\\\n\n\\begin{theorem} \nIf a function $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ is proper, closed, and convex, then ${\\operatorname{prox}}_f(x)$ exists, well-defined, and unique for all $x$.\n\\end{theorem}\\\\\n\nMoreau's identity allows us to decompose any $x$ into \n", "index": 49, "text": "$$x = {\\operatorname{prox}}_f(x) + {\\operatorname{prox}}_{f^*}(x)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"x={\\operatorname{prox}}_{f}(x)+{\\operatorname{prox}}_{f^{*}}(x)\" display=\"block\"><mrow><mi>x</mi><mo>=</mo><mrow><mrow><msub><mo>prox</mo><mi>f</mi></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mo>prox</mo><msup><mi>f</mi><mo>*</mo></msup></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nHere, $\\omega \\in \\mathbb{R}^{S}$ is the scaled Lagrange multiplier. The optimization in (\\ref{eq:nile-pro_LA}) is carried out over $\\psi \\in \\mathbb{R}^{r \\times N}$ and $q \\in \\mathbb{R}^S$, while $\\omega$ is updated. Each variable is updated in an alternating fashion. The following steps below are performed until convergence.\n\n\\textbf{Update $\\psi$}: Isolating the terms that involve $\\psi$, we obtain a new estimate $\\psi_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": 22943, "prevtext": " where $f^*$ is the convex conjugate of $f$. This decomposition, called the \\textit{Moreau decomposition}, generalizes the orthogonal decomposition on subspaces. We apply this machinery to help us solve for the update for $q$.\n\n\\subsection{ADMM}\n\nFollowing a similar method as FroMax, we relax our linear constraints and find our augmented Lagrangian of (14): \n\n", "index": 51, "text": "\\begin{equation}\\label{eq:nile-pro_LA}\nL_A(\\psi, q; \\omega) = \\|q - 1_S\\|_\\infty + \\frac{\\beta}{2} \\|\\mathcal{A}(\\psi^T\\psi) - q - \\omega \\|_2^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"L_{A}(\\psi,q;\\omega)=\\|q-1_{S}\\|_{\\infty}+\\frac{\\beta}{2}\\|\\mathcal{A}(\\psi^{T%&#10;}\\psi)-q-\\omega\\|_{2}^{2}\" display=\"block\"><mrow><mrow><msub><mi>L</mi><mi>A</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c8</mi><mo>,</mo><mi>q</mi><mo>;</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi><mo>-</mo><mi>\u03c9</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\n\\textbf{Update $q$}: Isolating the terms that involve $\\psi$, we obtain a new estimate $\\psi_{k + 1}$ as the solution of the constrained optimization problem\n\n", "itemtype": "equation", "pos": 23596, "prevtext": "\nHere, $\\omega \\in \\mathbb{R}^{S}$ is the scaled Lagrange multiplier. The optimization in (\\ref{eq:nile-pro_LA}) is carried out over $\\psi \\in \\mathbb{R}^{r \\times N}$ and $q \\in \\mathbb{R}^S$, while $\\omega$ is updated. Each variable is updated in an alternating fashion. The following steps below are performed until convergence.\n\n\\textbf{Update $\\psi$}: Isolating the terms that involve $\\psi$, we obtain a new estimate $\\psi_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 53, "text": "\\begin{equation}\n\\psi_{k + 1} \\leftarrow \\underset{\\psi}{\\arg\\min} ~ \\frac{\\beta}{2} \\| \\mathcal{A}(\\psi^T\\psi) - q - \\omega \\|_2^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\psi_{k+1}\\leftarrow\\underset{\\psi}{\\arg\\min}~{}\\frac{\\beta}{2}\\|\\mathcal{A}(%&#10;\\psi^{T}\\psi)-q-\\omega\\|_{2}^{2}\" display=\"block\"><mrow><msub><mi>\u03c8</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udf13</mo></munder></mpadded><mo>\u2062</mo><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi><mo>-</mo><mi>\u03c9</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\nSetting $X = q - 1_S$ and $\\tau = \\mathcal{A}(\\psi^T\\psi) - \\omega - 1_S$, we apply a Moreau decomposition on a proximal mapping to solve for the $q$ update:\n\n", "itemtype": "equation", "pos": 23902, "prevtext": "\n\n\\textbf{Update $q$}: Isolating the terms that involve $\\psi$, we obtain a new estimate $\\psi_{k + 1}$ as the solution of the constrained optimization problem\n\n", "index": 55, "text": "\\begin{equation}\nq_{k + 1} \\leftarrow \\underset{q}{\\arg\\min} ~ \\|q - 1_S \\|_\\infty +  \\frac{\\beta}{2} \\| \\mathcal{A}(\\psi^T\\psi) - q - \\omega \\|_2^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"q_{k+1}\\leftarrow\\underset{q}{\\arg\\min}~{}\\|q-1_{S}\\|_{\\infty}+\\frac{\\beta}{2}%&#10;\\|\\mathcal{A}(\\psi^{T}\\psi)-q-\\omega\\|_{2}^{2}\" display=\"block\"><mrow><msub><mi>q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>min</mi></mrow><mo>\ud835\udc5e</mo></munder></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>-</mo><msub><mn>1</mn><mi>S</mi></msub></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c8</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>q</mi><mo>-</mo><mi>\u03c9</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\n\\textbf{Update $\\omega$}: Following standard augmented Lagrangian methods, we update $\\omega$ according to the following equation\n\n", "itemtype": "equation", "pos": 24224, "prevtext": "\nSetting $X = q - 1_S$ and $\\tau = \\mathcal{A}(\\psi^T\\psi) - \\omega - 1_S$, we apply a Moreau decomposition on a proximal mapping to solve for the $q$ update:\n\n", "index": 57, "text": "\\begin{align}\nX & = \\frac{1}{\\beta}(\\beta \\tau - {\\operatorname{prox}}_{(\\|X\\|_1 \\leq 1)}(\\beta \\tau))\\\\\nq & = \\frac{1}{\\beta}(\\beta \\tau - {\\operatorname{prox}}_{(\\|X\\|_1 \\leq 1)}(\\beta \\tau)) + 1_S\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle X\" display=\"inline\"><mi>X</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{\\beta}(\\beta\\tau-{\\operatorname{prox}}_{(\\|X\\|_{1}\\leq 1%&#10;)}(\\beta\\tau))\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03b2</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo>-</mo><mrow><msub><mo>prox</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u2225</mo><mi>X</mi><msub><mo>\u2225</mo><mn>1</mn></msub><mo>\u2264</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle q\" display=\"inline\"><mi>q</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{\\beta}(\\beta\\tau-{\\operatorname{prox}}_{(\\|X\\|_{1}\\leq 1%&#10;)}(\\beta\\tau))+1_{S}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03b2</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo>-</mo><mrow><msub><mo>prox</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u2225</mo><mi>X</mi><msub><mo>\u2225</mo><mn>1</mn></msub><mo>\u2264</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mn>1</mn><mi>S</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\nPseudocode for NILE-Pro may be found in Algorithm \\ref{alg:nile-pro}.\n\n\\begin{algorithm}[tb]\n\\caption{NILE-Pro}\\label{alg:nile-pro}\n\\begin{algorithmic}\n\\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, max iterations $m > 0$, initial rank $r$\n\\FOR{$k = 0, ..., m$}\n\\STATE $\\tau \\leftarrow \\mathcal{A}(\\psi_k^T\\psi_k) -\\omega - 1_s $\n\\STATE $q_{k+1} \\leftarrow\\frac{1}{\\beta} \\Big(\\beta\\tau-\\mathcal{P}_{\\{\\|x\\|_1 \\leq 1\\}}(\\beta\\tau) \\Big) +1_S$\n\\STATE $\\psi_{k+1} \\leftarrow \\psi_k - 2\\eta \\psi_k \\mathcal{A}^*(\\mathcal{A}(\\psi_k^T\\psi_k) - q_{k+1}-\\omega)$\n\\STATE $\\omega_{k + 1} \\leftarrow \\omega_k - \\beta(\\mathcal{A}(\\psi_{k+1}^T\\psi_{k+1}) - q_{k+1})$\n\\STATE $\\epsilon_0 \\leftarrow \\|\\mathcal{A}(\\psi_{k+1}^T\\psi_{k+1})- 1_s\\|_\\infty$\\\\\n\\IF{$ \\epsilon_0 < \\epsilon$}\n\\STATE break\n\\ENDIF\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Rank Adjustment and Column Generation} \\label{racg}\n\nIn this section, we discuss rank adjustment and column generation heuristics. We develop rank adjustment methods to discover the lowest optimal rank for both FroMax and NILE-Pro. Abbreviating rank adjustment to \\textit{RA}, we call our rank adjusted algorithms \\textit{FroMax RA} and \\textit{NILE-Pro RA}, respectively. We also use column generation techniques following Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} to work with subsets of $S(\\mathcal{X})$ to lower the memory complexity of these algorithms, which we name \\textit{FroMax CG} and \\textit{NILE-Pro CG}, respectively. We discuss each heuristic algorithm in detail below.\n\n\\subsection{Rank Adjustment}\n\nThough FroMax and NILE-Pro may dramatically decrease the time of solving for projection matrix $\\psi$, both algorithms do not find an optimal rank for dimensionality reduction like NuMax. Hence, we propose a heuristic rank adjustment method that uses the discovered matrix $P = \\psi^T\\psi$ to give a good initialization for $\\psi$ of lower rank.\n\nGiven a sufficiently large rank, $R_0 \\gg r$, the optimal rank, we run our dimensionality reduction algorithm for a maximum number of iterations or until convergence to find $\\psi$. If our algorithm converges, we return $P = \\psi^T\\psi$ and find $\\psi_0 = \\Gamma_M^{1/2}U_M^T$, where $P = U \\Gamma U^T$ from $P$'s eigendecomposition. We then initialize our algorithm again with rank $R_1 = R_0-1$ and $\\psi_0$ which we found in the last iteration and test again for convergence.  We continue this process until we reach the maximum number of iterations within the algorithm and return the $\\psi$ given in the last iteration, considering its rank $r = R_k$ to be optimal. We summarize our rank adjustment heuristic in Algorithm \\ref{alg:RA}.\n\n\\begin{algorithm}[tb]\n\\caption{FroMax/NILE-Pro RA}\\label{alg:RA}\n\\begin{algorithmic}\n\\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, max iterations for algorithm $m > 0$, initial rank $R_0$, max iterations for RA $M$, $\\psi_0$\n\\FOR{k = 1, ..., M}\n\\STATE $\\psi \\leftarrow \\operatorname{FroMax/NILE-Pro}(S, \\delta, m, R_0, \\psi_0)$\n\\STATE $P \\leftarrow \\psi^T\\psi$\n\\STATE $R_{k + 1} \\leftarrow R_k - 1$\n\\STATE $[\\Gamma, U] \\leftarrow \\operatorname{eig}(P)$\n\\STATE $\\psi_0 \\leftarrow \\Gamma^{1/2}U^T$\n\\IF{FroMax/NILE-Pro does not converge}\n\\STATE break\n\\ENDIF\n\\ENDFOR\n\\STATE return $\\psi, r \\leftarrow R_k$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Column Generation}\n\nSince FroMax and NILE-Pro use the secants of a given data set, applications involving millions of secants may be prohibited by the memory complexity of these algorithms. Some methods that are used to address large data sets include stochastic and online methods. Stochastic methods use random subsets of the data to learn an estimate for the entire data set. Online methods uses sequentially available data to update the current iterate then discards the information. Our column generation algorithms, FroMax CG and NILE-Pro CG, combines stochastic and online methods to estimate solutions to large-scale problems.\n\nSimilar to NuMax's column generation, which is based off of the Karush-Kuhn-Tucker (KKT) conditions, we apply a simple, greedy method to rapidly find the active constraints for (\\ref{eq:fromax}) or (\\ref{eq:nile-pro}).\n\\begin{enumerate}\n\\item Solve (\\ref{eq:fromax}) or (\\ref{eq:nile-pro}) with a small subset $S_0 \\subset S(\\mathcal{X})$ using FroMax (Algorithm \\ref{alg:fromax}) or NILE-Pro (Algorithm \\ref{alg:nile-pro}), respectively to obtain an initial estimate $\\widehat{\\psi}$. Identify the set $\\widehat{S}$ of secants that correspond to the active constraints: \n", "itemtype": "equation", "pos": 24567, "prevtext": "\n\n\\textbf{Update $\\omega$}: Following standard augmented Lagrangian methods, we update $\\omega$ according to the following equation\n\n", "index": 59, "text": "\\begin{equation}\n\\omega_{k + 1} \\leftarrow \\omega_k - \\beta(\\mathcal{A}(\\psi^T_{k + 1}\\psi) - q_{k + 1})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\omega_{k+1}\\leftarrow\\omega_{k}-\\beta(\\mathcal{A}(\\psi^{T}_{k+1}\\psi)-q_{k+1})\" display=\"block\"><mrow><msub><mi>\u03c9</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2190</mo><mrow><msub><mi>\u03c9</mi><mi>k</mi></msub><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03c8</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\\item Select additional secants $S_1 \\subset S$ not selected previously and identify all secants among $S_1$ that violate the constraint at the current estimate $\\widehat{\\psi}$. Then, append these secants to the set of active constraints $\\widehat{S}$ to obtain an augmented set $\\widehat{S}$ \n", "itemtype": "equation", "pos": 29348, "prevtext": "\n\nPseudocode for NILE-Pro may be found in Algorithm \\ref{alg:nile-pro}.\n\n\\begin{algorithm}[tb]\n\\caption{NILE-Pro}\\label{alg:nile-pro}\n\\begin{algorithmic}\n\\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, max iterations $m > 0$, initial rank $r$\n\\FOR{$k = 0, ..., m$}\n\\STATE $\\tau \\leftarrow \\mathcal{A}(\\psi_k^T\\psi_k) -\\omega - 1_s $\n\\STATE $q_{k+1} \\leftarrow\\frac{1}{\\beta} \\Big(\\beta\\tau-\\mathcal{P}_{\\{\\|x\\|_1 \\leq 1\\}}(\\beta\\tau) \\Big) +1_S$\n\\STATE $\\psi_{k+1} \\leftarrow \\psi_k - 2\\eta \\psi_k \\mathcal{A}^*(\\mathcal{A}(\\psi_k^T\\psi_k) - q_{k+1}-\\omega)$\n\\STATE $\\omega_{k + 1} \\leftarrow \\omega_k - \\beta(\\mathcal{A}(\\psi_{k+1}^T\\psi_{k+1}) - q_{k+1})$\n\\STATE $\\epsilon_0 \\leftarrow \\|\\mathcal{A}(\\psi_{k+1}^T\\psi_{k+1})- 1_s\\|_\\infty$\\\\\n\\IF{$ \\epsilon_0 < \\epsilon$}\n\\STATE break\n\\ENDIF\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Rank Adjustment and Column Generation} \\label{racg}\n\nIn this section, we discuss rank adjustment and column generation heuristics. We develop rank adjustment methods to discover the lowest optimal rank for both FroMax and NILE-Pro. Abbreviating rank adjustment to \\textit{RA}, we call our rank adjusted algorithms \\textit{FroMax RA} and \\textit{NILE-Pro RA}, respectively. We also use column generation techniques following Hegde et. al. \\cite{HegdeSankaranarayananYinBaraniuk2015_numax} to work with subsets of $S(\\mathcal{X})$ to lower the memory complexity of these algorithms, which we name \\textit{FroMax CG} and \\textit{NILE-Pro CG}, respectively. We discuss each heuristic algorithm in detail below.\n\n\\subsection{Rank Adjustment}\n\nThough FroMax and NILE-Pro may dramatically decrease the time of solving for projection matrix $\\psi$, both algorithms do not find an optimal rank for dimensionality reduction like NuMax. Hence, we propose a heuristic rank adjustment method that uses the discovered matrix $P = \\psi^T\\psi$ to give a good initialization for $\\psi$ of lower rank.\n\nGiven a sufficiently large rank, $R_0 \\gg r$, the optimal rank, we run our dimensionality reduction algorithm for a maximum number of iterations or until convergence to find $\\psi$. If our algorithm converges, we return $P = \\psi^T\\psi$ and find $\\psi_0 = \\Gamma_M^{1/2}U_M^T$, where $P = U \\Gamma U^T$ from $P$'s eigendecomposition. We then initialize our algorithm again with rank $R_1 = R_0-1$ and $\\psi_0$ which we found in the last iteration and test again for convergence.  We continue this process until we reach the maximum number of iterations within the algorithm and return the $\\psi$ given in the last iteration, considering its rank $r = R_k$ to be optimal. We summarize our rank adjustment heuristic in Algorithm \\ref{alg:RA}.\n\n\\begin{algorithm}[tb]\n\\caption{FroMax/NILE-Pro RA}\\label{alg:RA}\n\\begin{algorithmic}\n\\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, max iterations for algorithm $m > 0$, initial rank $R_0$, max iterations for RA $M$, $\\psi_0$\n\\FOR{k = 1, ..., M}\n\\STATE $\\psi \\leftarrow \\operatorname{FroMax/NILE-Pro}(S, \\delta, m, R_0, \\psi_0)$\n\\STATE $P \\leftarrow \\psi^T\\psi$\n\\STATE $R_{k + 1} \\leftarrow R_k - 1$\n\\STATE $[\\Gamma, U] \\leftarrow \\operatorname{eig}(P)$\n\\STATE $\\psi_0 \\leftarrow \\Gamma^{1/2}U^T$\n\\IF{FroMax/NILE-Pro does not converge}\n\\STATE break\n\\ENDIF\n\\ENDFOR\n\\STATE return $\\psi, r \\leftarrow R_k$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Column Generation}\n\nSince FroMax and NILE-Pro use the secants of a given data set, applications involving millions of secants may be prohibited by the memory complexity of these algorithms. Some methods that are used to address large data sets include stochastic and online methods. Stochastic methods use random subsets of the data to learn an estimate for the entire data set. Online methods uses sequentially available data to update the current iterate then discards the information. Our column generation algorithms, FroMax CG and NILE-Pro CG, combines stochastic and online methods to estimate solutions to large-scale problems.\n\nSimilar to NuMax's column generation, which is based off of the Karush-Kuhn-Tucker (KKT) conditions, we apply a simple, greedy method to rapidly find the active constraints for (\\ref{eq:fromax}) or (\\ref{eq:nile-pro}).\n\\begin{enumerate}\n\\item Solve (\\ref{eq:fromax}) or (\\ref{eq:nile-pro}) with a small subset $S_0 \\subset S(\\mathcal{X})$ using FroMax (Algorithm \\ref{alg:fromax}) or NILE-Pro (Algorithm \\ref{alg:nile-pro}), respectively to obtain an initial estimate $\\widehat{\\psi}$. Identify the set $\\widehat{S}$ of secants that correspond to the active constraints: \n", "index": 61, "text": "$$\\widehat{S} \\leftarrow \\{ v_i \\in S_0 : |v_i^T\\widehat{\\psi}^T \\widehat{\\psi} v_i - 1| \\geq \\delta \\}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\widehat{S}\\leftarrow\\{v_{i}\\in S_{0}:|v_{i}^{T}\\widehat{\\psi}^{T}\\widehat{%&#10;\\psi}v_{i}-1|\\geq\\delta\\}\" display=\"block\"><mrow><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mo>\u2190</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi>S</mi><mn>0</mn></msub></mrow><mo>:</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>v</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\u03c8</mi><mo>^</mo></mover><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03c8</mi><mo>^</mo></mover><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2265</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00062.tex", "nexttext": "\n\\item Solve (\\ref{eq:fromax}) or (\\ref{eq:nile-pro}) with the new augmented set $\\widehat{S}$ using FroMax or NILE-Pro to obtain a new estimate $\\widehat{\\psi}$.\n\\item Identify the secants that correspond to active constraints and repeat Steps 2 and 3 until convergence is reached for $\\widehat{\\psi}$.\n\\end{enumerate}\n\nColumn generation allows us to perform a large numerical optimization procedure on smaller subsets of $S(\\mathcal{X})$, resulting in significant computational gains. A key benefit of FroMax CG and NILE-Pro CG is that the subsets of secants used during each iteration never has to be explicitly stored in memory and can be generated on the fly. This leads to significant improvements in memory complexity. \n\nHowever, because FroMax and NILE-Pro are already both non-convex, column generation makes these algorithms even less predictable. Though these algorithms are not guaranteed to converge to an optimal solution, they appear to yield excellent results on large, real-world data sets, as we will show in \\S \\ref{experiments}. \n\nPseudocode for FroMax/Nile-Pro CG is found in Algorithm \\ref{alg:CG}. Our column generation method converges when no additional secants violate our constraint.\n\n\\begin{algorithm}[tb]\n\\caption{FroMax/NILE-Pro CG}\\label{alg:CG}\n\\begin{algorithmic}\n\\STATE {\\bfseries Inputs:} Secant set $\\mathcal{S}(\\mathcal{X}) = \\{v_i\\}_{i = 1}^S$, isometry constant $\\delta$, max iterations for algorithm $m > 0$, rank $r$, the FroMax or NILE-Pro algorithm\n\\WHILE{not converged}\n\\STATE $\\widehat{S} \\leftarrow \\{v_i \\in S_0 : |v_i^T\\psi^T\\psi v_i - 1| \\geq \\delta \\}$\n\\STATE $S_1 \\leftarrow \\{v_i \\in S : v_i \\notin S_0 \\}_{i = 1}^{S''}$\n\\STATE $\\widehat{S} \\leftarrow \\widehat{S} \\bigcup \\{v_i \\in S_1 : |v_i^T \\psi^T \\psi v_i = 1| \\geq \\delta \\}$\\\\\n\\STATE $\\psi \\leftarrow \\operatorname{FroMax/NILE-Pro}(\\widehat{S}, \\delta)$\n\\STATE $S_0 \\leftarrow \\widehat{S}$\n\\ENDWHILE\n\\STATE return $\\psi$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Convergence of Algorithms}\n\nSince FroMax and NILE-Pro are derived from applying the ADMM to non-convex problems, the convergence properties of these algorithms can be understood based on the convergence properties of ADMM. For certain types of convex problems, ADMM has been shown to converge at a rate of $o(1/k)$ \\cite{deng20131}. However, since our problems are non-convex, convergence analyses of ADMM do not apply. \n\n\\section{Numerical Experiments} \\label{experiments}\n\nWe demonstrate the performance of the FroMax and NILE-Pro algorithms in comparison to prior methods including NuMax. All of our experiments are performed on computers with Intel i5-650 processors and 4 GB of RAM unless otherwise specified. We test and compare the speed and accuracy of our algorithms through various tests on real-world and synthetic data sets.\n\n\\subsection{Linear Low-Dimensional Embeddings}\\label{linear1}\n\nWe first consider synthetic data sets $\\mathcal{X}_1$ and $\\mathcal{X}_2$ consisting of $7 \\times 7 = 49$ and $14 \\times 14 = 196$ dimensional images of translations of a white square on a black box respectively. We construct our training sets by randomly generating $60$ $49$-dimensional images for $\\mathcal{X}_1$ and $200$ $196$-dimensional images for $\\mathcal{X}_2$. We then construct secant sets $S(\\mathcal{X}_1)$ and $S(\\mathcal{X}_2)$ by computing the normalized pairwise difference vectors between different images. We compare FroMax and NILE-Pro's performance of producing linear, low-dimensional embeddings on these two data sets in Table 1.\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lc c c c c r}\n\\hline\n& & \\multicolumn{2}{c}{FroMax} & \\multicolumn{2}{c}{NILE-Pro}\\\\\n\\#Data & $\\delta$ & Rank & Time & Rank & Time\\\\\n\\hline  \n\\multirow{3}{*}{60} & 0.4  & 9 & 1.3 & 9 & 0.7\\\\\n& 0.25 & 9 & 1.2 & 9 & 1.1\\\\\n& 0.1 & 13 & 1.4 & 13 & 1.5\\\\\n\\hline  \n\\multirow{3}{*}{200} & 0.4  & 16 & 511.2 & 16 & 109.1\\\\\n& 0.25 & 18 & 269.4 & 18 & 144.4\\\\\n& 0.1 & 27 & 74.5 & 27 & 448.5\\\\\n\\hline\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\caption{Comparison of runtime performance for FroMax and NILE-Pro on $S(\\mathcal{X}_1)$ and $S(\\mathcal{X}_2)$ given $\\delta$ and rank.}\n\\end{table}\n\nSince NILE-Pro minimizes the RIP directly, NILE-Pro intuitively will converge faster for larger $\\delta$. Our experimental results match our theoretical intuition since NILE-Pro converges significantly faster for larger $\\delta$ than lower $\\delta$. \n\nFroMax experimentally converges faster for smaller $\\delta$ than larger $\\delta$. Smaller $\\delta$ restricts $q$ to a smaller feasible set given by the RIP, leading to faster convergence.\n\nAlso note that both algorithms' computational complexity scale significantly with the size of the data set due to the use of the secant set. Our runtime results comparing $S(\\mathcal{X}_1)$ and $S(\\mathcal{X}_2)$ reflect this.\n\n\\subsection{Linear Low-Dimensional Embeddings with Rank Adjustment}\n\nIn \\S \\ref{linear1}, we input a given rank for FroMax and NILE-Pro and compare their run time. However, usually the optimal rank for dimension reduction is not known, motivating the development of rank adjustment heuristics. To analyze the performance of our rank adjustment heuristic, we consider a synthetic data set $\\mathcal{X}$ comprised of $16 \\times 16 = 256$ dimensional images of translations of a white square or a disk on a black box respectively, see figure~\\ref{fig:disk}. We construct a secant set $S(\\mathcal{X})$ and compare PCA, Numax RA, FroMax RA and NILE-Pro RA's performance of producing linear, low-dimensional embeddings on this data set.\n\nFigure~\\ref{fig:bw} plots the variation of the number of measurements $M$ as a function of the isometry constant $\\delta$. We observe that NILE-Pro RA achieves the desired isometry constant on the secants using by far the fewest number of measurements. FroMax RA performs better for small $\\delta$ due to the correlation between $\\delta$ and $q$, as we discussed before. Moreover, both Numax RA and Fromax RA greatly outperform PCA, a popular embedding technique in the literature. \n\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=2in]{disk.jpg}\n\\includegraphics[width=2in]{square.jpg}\n\\caption[Sample Disk Figure]{Our synthetic training set consists of sixty $256$-dimensional random generated translating disks and squares figure.}\n\\label{fig:disk}\n\\end{figure}\n\n\\begin{figure}[ht!]\n\n\\hspace{0.5in}\n\\includegraphics[scale = .36]{bw_square_clear.png}\n\\caption{A comparison of the isometry constant $\\delta$ with the number of measurements for PCA, Numax RA, FroMax RA and NILE-Pro RA's performance of producing linear, low-dimensional embeddings. }\n\\label{fig:bw}\n\\end{figure}\n\n\n\\subsection{Runtime Performance on MNIST with Rank Adjustment}\n\nIn this experiment, we consider a more challenging, real-world data set, the MNIST data set, see figure~\\ref{fig:MNIST}. MNIST contains many digital images of handwritten digits and is a common benchmark data set for machine learning. We examine subsets of the training set for the digit ``5''. We take subsets consisting of 95, 200, and 500 data points with original dimension 49.\n\nWe test runtime performance of FroMax and NILE-Pro RA on these data sets. Our results may be found in Table 2.\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=2in]{MNIST5.png}\n\\caption{Examples of \u00e2\u0080\u009c5\u00e2\u0080\u009d images from the MNIST dataset.}\n\\label{fig:MNIST}\n\\end{figure}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{l c c c c c c c r}\n\\hline  \n& & \\multicolumn{2}{c}{NILE-Pro} & \\multicolumn{2}{c}{FroMax} & \\multicolumn{2}{c}{NuMax}\\\\\n$\\delta$ & \\#Data & Rk & Time & Rk & Time & Rk & Time\\\\\n\\hline  \n\\multirow{3}{*}{0.4} & 95 & 7 & 25 & 9 & 102 & 12 & 71\\\\\n & 200 & 9 & 96 & 15 & 520 & 21 & 311\\\\\n & 500 & 11 & 710 & 27 & 2490 & 25 & 3477\\\\\n\\hline  \n\\multirow{3}{*}{0.2} & 95 & 11 & 28 & 11 & 111 & 14 & 56\\\\\n & 200 & 14 & 130 & 16 & 569 & 18 & 557\\\\\n & 500 & 18 & 751 & 40 & 1498 & 27 & 3517\\\\\n\\hline  \n\\multirow{3}{*}{0.1} & 95 & 15 & 41 & 15 & 91 & 16 & 21\\\\\n & 200 & 20 & 165 & 19 & 823 & 21 & 279\\\\\n & 500 & 25 & 1285 & 44 & 650 & 30 & 3410\\\\\n\\hline\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\caption{Comparison of runtime performance for FroMax RA, NILE-Pro RA, and NuMax on subsets of ``5'' images from MNIST.}\n\\end{table}\n\nOur experimental results show that NILE-Pro RA may perform significantly faster and give a better optimal rank than NuMax while FroMax RA converges slower for larger data sets. This may be due to the nature of the local minima found in FroMax; the estimate for $P = \\psi^T \\psi$ given for a larger rank does not correspond to the local minima for lower ranks so that this initialization is beneficial.\n\nOur results for FroMax RA reveal another issue with our rank adjustment method. FroMax RA appears to struggle with determining the optimal rank, sometimes performing worse than NuMax. We believe that our algorithm may be converging to local minima, which makes our rank adjustment ineffective. This issue motivates us to look into other rank adjustment methods that start at a sufficiently low rank and examine higher ranks to discover the optimal rank. \n\nAlso, since rank adjustment for NILE-Pro is still based on the core NILE-Pro algorithm, we see that NILE-Pro RA converges in much slower time for smaller $\\delta$.\n\nThe former caveat motivates us to continue looking for better rank adjustment methods for FroMax.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Nearest Neighbor Classification on MNIST}\n\nThe MNIST data set consists of 60,000 training data points and 10,000 test data points of handwritten digits \\cite{lecun1998mnist}. The dataset contains 10 classes corresponding to each digit from 0-9. For this experiment, we use the $N = 20 \\times 20 = 400$-dimensional data set that excludes extra space at the boundaries. We use NuMax CG and FroMax CG to embed our MNIST training set into a lower dimensional space and nearest neighbor classification.\n\nThe misclassification rate of nearest neighbor classification on the unchanged data set is 3.47\\%. Table \\ref{table:classification} gives the nearest neighbor classification misclassification rate for NuMax CG and FroMax CG for given $\\delta$ and rank applied on the MNIST data set. In particular, though NuMax CG and FroMax CG give similar misclassification rates, FroMax CG has significantly better runtime performance than NuMax CG. Though a combined rank adjustment and column generation method has not been implemented for FroMax, the results suggest that FroMax may find a sufficiently good projection matrix in much less time.\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{l c c c c c}\n\\hline\n$\\delta$ & Rank & NuMax CG & Time (hrs) & FroMax CG & Time (hrs)\\\\\n\\hline\n0.4 & 72 & 3.09\\% & 0.926 & 3.00\\% & 0.411\\\\\n0.25 & 98 & 3.15\\% & 1.273 & 3.26\\% & 0.811\\\\\n0.1 & 167 & 3.31\\% & 2.664 & 3.42\\% & 1.358\\\\\n\\hline\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\caption{Comparison of misclassification rates and run-time performance of approximate nearest neighbor classifiers using NuMax CG and FroMax CG for given $\\delta$ and rank on the MNIST test set.} \\label{table:classification}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Approximate Nearest Neighbors}\n\nGiven a data set modeled by points in Euclidean space and a query point, \\textit{nearest neighbors} identifies the $k$ closest points in the data set \\cite{cover1967nearest}. These points are usually used for further processing, such as unsupervised or supervised regression and classification.\n\nHowever, as the dimension $N$ of the data set grows, the computational cost of identifying the $k$ nearest neighbors also becomes increasingly expensive. An alternative to computing nearest neighbors directly is to embed the data into a lower-dimensional subspace while preserving near-isometry, then applying nearest neighbor techniques. This method is called \\textit{approximate nearest neighbors}. Since NuMax, FroMax, and NILE-Pro construct low rank, near-isometric linear embeddings for a given distortion $\\delta$, they may potentially enable efficient ANN computations for high-dimensional data sets.\n\nFor this experiment, we use the LabelMe data set consisting of 4000 images of indoor and outdoor scenes \\cite{russell2008labelme}. We then computed GIST descriptors for each image, which are vectors of size $N = 512$ that roughly describe the overall spatial statistics of the image \\cite{oliva2001modeling}. We then used NuMax CG and FroMax CG to estimate low rank, near-isometric linear embeddings for this data set for a given distortion parameter $\\delta$. Then we perform ANN computations on 1000 test data points in the corresponding low dimensional space. We compute embeddings of various ranks for FroMax CG to compare performance between different ranks.\n\n\\begin{figure}\n\\includegraphics[scale=0.45]{delta1.png}\n\\includegraphics[scale=0.45]{delta2.png}\\\\\n\\includegraphics[scale=0.45]{delta3.png}\n\\includegraphics[scale=0.45]{delta5.png}\n\\caption{Comparison of FroMax CG and NuMax CG on preserving nearest neighbors.}\n\\label{fig:ann}\n\\end{figure}\n\nFigure \\ref{fig:ann} demonstrates that FroMax CG generally attains similar if not better performance than NuMax CG for the same rank. In fact, our results suggest that FroMax CG could perform similarly at a lower rank than NuMax CG. We leave further investigation for future research.\n\n\\section{Discussion} \\label{discussion}\n\n\\subsection{Research Overview}\n\nIn this paper, we construct two comprehensive algorithmic frameworks for finding near-isometric linear embeddings of high-dimensional data sets. Based on the convex optimization formulation in NuMax, we proposed two non-convex minimization approaches which approximately preserve the norms of all pairwise secants of the given dataset. In particular, we developed two algorithms, FroMax and NILE-Pro, that may construct the desired embedding with smaller computational complexity than NuMax. \n\nSince NuMax automatically discovers the optimal rank, we created a rank adjustment method for finding the best rank for our algorithms. We also implemented column generation in addition to FroMax and NILE-Pro so our algorithms can be adapted to perform on larger data sets. \n\nConstructing linear, information-preserving embeddings of high-dimensional signals to lower-dimensional signals have become of significant importance for a wide range of machine learning and compressive sensing applications. However, little is known about near-isometric linear embeddings beyond the Johnson-Lindenstrauss Lemma. The frameworks discussed in this paper build on the convex, deterministic approach of NuMax to produce practical, potentially more computationally efficient dimension reduction algorithms that are both information-preserving and feasible for a broad range of applications. Though we do not provide an analytical foundation to our work due to the non-convex nature of our algorithms, we hope to initiate work in developing a theoretical basis for similar work.\n\n\\subsection{Future Work}\n\nThere are still many challenges left to tackle. As discussed in \\S \\ref{experiments}, we still need to further develop rank adjustment methods for FroMax and column generation techniques for NILE-Pro. We would also like to incorporate both rank adjustment and column generation together. One direction is to consider an eigengap heuristic for rank adjustment, in which some heuristic is set based on the difference between singular values of the matrix $P$ to determine the next chosen rank.\n\nIn addition, further testing and parameter-tweaking is necessary to analyze and optimize the stability and performance of our algorithms on various data sets. Other heuristics for non-convex optimization, such as applying perturbations to avoid local minima, may also be applied to give better solutions. We defer the study of these challenges and heuristics for future research.\n\n\\section*{Acknowledgements}\n\nThe work of Jerry Luo, Kayla Shapiro, and Hao-Jun Michael Shi were supported in part by the California Research Training Program for Computational and Applied Mathematics 2014 under NSF Grant DMS-1045536. The work of Qi Yang was supported in part by USC Provost's Undergraduate Research Fellowship and the WiSE Research Experience for Undergradutes. Thanks to Dr. Ming Yan and Dr. Wotao Yin for their consistent advice, support, and mentorship throughout the entirety of this project.\n\n\\bibliographystyle{siam.bst}\n\\bibliography{bibl}\n\n\n", "itemtype": "equation", "pos": 29748, "prevtext": "\n\\item Select additional secants $S_1 \\subset S$ not selected previously and identify all secants among $S_1$ that violate the constraint at the current estimate $\\widehat{\\psi}$. Then, append these secants to the set of active constraints $\\widehat{S}$ to obtain an augmented set $\\widehat{S}$ \n", "index": 63, "text": "$$\\widehat{S} \\leftarrow \\widehat{S} \\bigcup \\{ v_i \\in S_1 : |v_i^T\\widehat{\\psi}^T \\widehat{\\psi}v_i - 1 | \\geq \\delta \\}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\widehat{S}\\leftarrow\\widehat{S}\\bigcup\\{v_{i}\\in S_{1}:|v_{i}^{T}\\widehat{%&#10;\\psi}^{T}\\widehat{\\psi}v_{i}-1|\\geq\\delta\\}\" display=\"block\"><mrow><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mo>\u2190</mo><mrow><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi>S</mi><mn>1</mn></msub></mrow><mo>:</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>v</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\u03c8</mi><mo>^</mo></mover><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03c8</mi><mo>^</mo></mover><mo>\u2062</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2265</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]