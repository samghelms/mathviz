[{"file": "1601.04736.tex", "nexttext": "\nAn alternative is to define $h^{\\star}$ as:\n\n", "itemtype": "equation", "pos": 13712, "prevtext": "\n\n\\baselineskip=0.65cm\n\n\n\n\\title{A Consistent Direct Method for Estimating Parameters \\\\in Ordinary Differential Equations Models}\n\\author{Sarah E. Holte  \\\\\nDivision of Public Health Sciences\\\\\nFred Hutchinson Cancer Research Center \\\\\n1100 Fairview Ave North, M2-B500 \\\\\nSeattle, WA 98109, USA  \\\\\nemail: \\texttt{sholte@fhcrc.org}\n}\n\n\\maketitle\n\n\n\n\n\\begin{abstract}\nOrdinary differential equations provide an attractive framework for modeling temporal\ndynamics in a variety of scientific settings. We show how consistent estimation for\nparameters in ODE models can be obtained by modifying a direct (non-iterative) least\nsquares method similar to the direct methods originally developed by Himmelbau, Jones\nand Bischoff. Our method is called the bias-corrected least squares (BCLS) method since\nit is a modification of least squares methods known to be biased. Consistency of the\nBCLS method is established and simulations are used to compare the BCLS method to other\nmethods for parameter estimation in ODE models.\n\\end{abstract}\n\n\\noindent\\textsc{Keywords}: Linear regression, longitudinal data, nonlinear regression, ordinary differential equations, parameter estimation.\n\n\n\n\\section{Introduction}\nOrdinary differential equations (ODEs) have been used extensively in a wide variety of\nscientific areas to describe time varying phenomena. Physicists have used equations of\nthis type to understand the motion of objects for centuries. ODE's are used in nearly\nevery field of engineering and chemists use ODE models to evaluate chemical reactions.\nEcologists have used ODE's to understand the dynamics of animal and plant populations\n\\cite{Fre:80}, meteorologists have used them to understand and predict patterns in the\nweather and atmosphere \\cite{Lor:63}, and economists have used them to understand and\npredict patterns in financial markets \\cite{Zha:05}.  More recently, biologists and\nhealth scientists have begun to use differential equations to make sense of observed\nnonlinear behaviors in their fields of study. One area where the use of differential\nequations is well integrated with data analysis is the evaluation of drug metabolism, the\nstudy of pharmacokinetics and pharmacodynamics (PK/PD).  Studies of how drugs are\nmetabolized are part of the development of nearly all compounds available for use in\nhumans today. Reviews of the use of differential equations in both individual and\npopulation level PK/PD include \\cite{Csa:06,Dan:08,Dar:07,Mag:03,She:00}. Software\npackages for parameter estimation for population PK/PD models are available in both SAS\n\\cite{Gal:04} and Splus \\cite{Tor:04} . In other areas of biology and clinical research,\nordinary differential equation (ODE) models have been used as a tool in exploring the\netiology of HIV and Hepatitis B and C infections and the effects of therapy on these\ndiseases. In \\cite{Ho:95,Wei:95,Per:96,Per:97} ordinary differential equations were used\nto analyze the temporal dynamics of HIV viral load measurements in AIDS patients. The\nresults had a tremendous scientific impact, revealing that the HIV virus replicates\nrapidly and continuously, in spite of the prolonged interval between infection and\ndevelopment of AIDS.\n\n\nIn some cases, ODE's are used for theoretical purposes only; to provide ``what if\"\nscenarios to be further studied by experimental scientists. However, more and more, ODE's\nare used in combination with observed data for parameter estimation and statistical\ninference of parameters in ODE models. In most cases, the parameters of interest appear\nin a nonlinear form in the likelihood or cost equation.  Estimates are often obtained\nusing standard nonlinear least squares (NLS) estimation techniques.  One drawback of the\nNLS estimation technique using conventional gradient based optimization methods such as\nGauss-Newton or Levenberg-Marquart is the instability of the procedure since estimates\nare obtained by an iterative numerical procedure (\\cite{Pre:86}, Chapter 10). NLS\nprocedures may fail to converge to global minima depending on the choice of starting\nvalues for iteration, particularly if the least squares solution has multiple local\nminima or saddle points. Global optimization methods are often available, however many\nanalysts continue to rely on algorithms such as nonlinear least squares that require a\nchoice of starting values. Certain likelihood surfaces or cost functions may be\nexceptionally complex with ridges which tend to appear near bifurcation values of the\nparameters in the ODE model.  Furthermore, it is well known that nonlinear regression\nestimates may be biased \\cite{Cook:86} when small samples are used for estimation.\n\nWhen parameters in an ODE model appear linearly in the vector field which defines the ODE\nsystem, it is possible to estimate the parameters either iteratively (e.g., nonlinear\nleast squares) or non-iteratively. In this paper we will use the term {\\em direct} to\nrefer to non-iterative methods. Direct methods can be based on differentiation or\nintegration and are generally combined with a least squares estimation step. Direct\nmethods based on integration were originally described in \\cite{HimJon:67} and are\nfurther developed in \\cite{Bard:74,Foss:71,Hosten:79,Jac:74}. In general, the ODEs are\ntransformed into integral equations and the integrals are approximated via methods of\nquadrature which yields algebraic equations that can be solved for the approximate\nparameters using a linear least squares approach. Differentiation methods are described\nin \\cite{Bard:74,Hosten:79,SwaBre:75,Varah:82} and involve constructing a cubic spline\nusing the observed data for each dependent variable. Values for the unknown parameters\nare estimated using linear least squares, i.e., minimizing the differences between the\nderivatives of the spline functions and corresponding model equations. Both the\ndifferentiation and integration approaches result in linear least squares problems which\ncan be solved without iteration. When there is negligible measurement error and equally\nspaced values of the dependent variable, the two approaches provide similar estimates\n\\cite{Wikstrom:97a}. In the case where these two assumptions are not met, the integration\napproach is preferred over the differentiation approach \\cite{Wikstrom:97b}.\nDifferentiation requires differencing of observed data, an approach which is known to\ninflate errors in the data.  In contrast, integration involves sums of observed data; a\nsmoothing approach which can reduce errors. An approach which relies on integration\nallows estimation of the initial size of the states of the system if these are not known.\n\nThere has been very little investigation of the statistical properties of parameter estimates in ODE models\nobtained by direct methods, possibly since it is known that parameter estimates obtained via these direct\nmethods do not have optimal statistical properties \\cite{Hosten:79}.\nSpecifically, the estimates are not unbiased, efficient, or\nconsistent.  While these early direct methods did not have optimal statistical properties it\nis possible that adjustments can be identified so that the resulting direct method has\ndesirable statistical features.\n\nRecently, Liang and Wu \\cite{Lia:08} describe a\ndirect method based on differentiation, local smoothing of the data\nand linear least squares regression of correlated quantities, which they\nrefer to as pseudo-least squares (PsLS) method.  They show that under\nreasonably general conditions their PsLS estimate of parameters\nin an ODE model is consistent with an asymptotically normal distribution.\nHowever, choice of bandwidth for the kernel smoothing required by this\nmethod is critical and may require further\nresearch in order to provide the most accurate and efficient results.\nRamsey and colleagues \\cite{RamSil:05,RHCC:07}\nhave used functional estimation or principle differential analysis (PDA)\napproaches to replace state variables with their (smoothed) estimates, and proceed\nwith estimation of parameters using estimated values of the solutions\nto the system of ODEs. These methods, like those of Liang and Wu, use differentiation rather than integration.\n\n\nIn this paper we introduce a direct algorithm similar to those developed by\nor based on the work of Himmelbau, Jones and Bischoff \\cite{HimJon:67}.\nOur approach incorporates a bias correction factor so that the method is consistent, and\ndoes not require smoothing or functional estimation. As such it retains the simplicity\nof early direct methods \\cite{Bard:74,Jac:74,Foss:71,HimJon:67,Hosten:79,SwaBre:75,Varah:82}\nbut has many of the desirable statistical properties of the more recently developed direct\nmethods \\cite{Lia:08,RHCC:07}. Our method relies\non integration similar to the methods described in \\cite{Bard:74,Foss:71,HimJon:67,Hosten:79,Jac:74}.\nWe refer to the proposed method as the {\\it bias-corrected least squares} method (BCLS)\nand the focus of this paper is\nto evaluate the statistical properties of the estimates produced by the BCLS method.\nSince the BCLS method relies on integration\nrather than differentiation, estimation of the initial states of the system in addition\nto parameters appearing linearly in the system is possible.\nThe method is applied to two examples and its properties contrasted with those of the NLS\nand PsLS methods with simulation studies. A proof of the consistency of the method is\nprovided in the appendix.\n\n\\section{The Bias-Corrected Least Squares Method}\n\n\\subsection{ Mathematical and Statistical  Model Specification }\n\nThe bias-corrected least squares method applies to data, ${\\bf y}({\\bf t})$, on observations with time varying\nexpectation given by the solutions of system of  $q=1,\\cdots, s$ ODEs, observed\nat ${\\bf t}=(t_0,\\cdots,t_n)$, $n$ observation times. It is not necessary that all $s$ compartments be\nsampled at the same times, but for ease of notation it is assumed that this is the case in this work.\nThe statistical and mathematical model assumptions for the the data\n${\\bf y}({\\bf t})$ with distribution ${\\bf Y(t)}$ are summarized as follows:\n\\begin{itemize}\n\n\\item[(A.1)]  : Specification of the mean structure via a mathematical model\n   \\begin{itemize}\n   \\item There exists a vector of functions\n  ${\\bf X}(t)=\\{X_1(t),\\cdots,X_s(t)\\}$ which satisfies the\n  system of differential equations:\n  \\begin{eqnarray}\n  \\frac{dX_q}{dt}=\\sum_{k=1}^{m_q} \\beta_{q,k} h_{q,k}(X_1,\\cdots,X_s), \\ \\\n  X_q(t_0)=X_{q,0}, \\ \\ q=1,\\cdots,s \\ \\\n  \\label{geneq}\n  \\end{eqnarray}\n  such that that the expectation, $E\\{{\\bf Y}(t)\\}$ is equal to $ {\\bf X}(t)$.\n    \\end{itemize}\n\n\\vskip-.1in\n\\item[(A.2)] : Specification of variance/covariance structure\n \\begin{itemize}\n     \\item $Y_q ({\\bf t}) = X_q ({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}}_q,\\ \\ {\\mbox{\\boldmath ${\\epsilon}$}}_q \\sim({\\bf 0},\\Sigma_q), \\ \\ q = 1,\\cdots,s.$\n     \\item $\\Sigma_q$ is a diagonal matrix with constant diagonal entries $\\sigma_q$, \\ \\ $q = 1,\\cdots,s$.\n     \\item $ {\\mbox{\\boldmath ${\\epsilon}$}}_r$ and ${\\mbox{\\boldmath ${\\epsilon}$}}_q$ are independent for all $1 \\leq r < q \\leq s$.\n   \\item  $\\mbox{var}(Y_i)$ and $\\mbox{var}\\{h_{q,k}(Y_i)\\}$ are bounded by a common bound $B$ for\nall $i,q$, and $k$.\n   \\end{itemize}\n\n\\vskip-.1in\n  \\item[(A.3)] : Data sampling requirements\n  \\begin{itemize}\n  \\item The maximum interval length defined by the sampling times\nis $O(n^{-1})$.\n  \\item At least one of the compartments is not in\nequilibrium throughout the entire time course of data collection .\n  \\end{itemize}\n\\end{itemize}\n\n\n\nThe values $\\beta_{q,k}$ and in some cases the initial conditions $X_{q,0}$ are\nparameters that will be estimated from the data. The functions, $h_{q,k}$, are any\n(linear or nonlinear) continuous functions of the states of the system, which together\nwith the parameters, $\\beta_{q,k}$, and initial conditions, $X_{q,0}$, define the vector\nfield of the differential equations relating the states ${\\mbox{\\boldmath ${X}$}}$ to their time derivatives.\nFor the $q^{th}$ state, the value $m_q$ is the number of terms in the expression which\nspecifies $\\frac{d{\\mbox{\\boldmath ${X}$}}_q}{dt}$. Note that the vector field defining the system of\ndifferential equations can be nonlinear in the states, i.e., $h_{q,k}({\\mbox{\\boldmath ${X}$}})$ is not\nnecessarily linear in the states, but must be linear in the parameters in order for the\nbias-corrected least squares method (or any direct least squares based method) to be\napplicable.  Thus the BCLS method applies to a wide variety of nonlinear systems of\nODE's.\n\nCondition (A.1) provides the relationship between the time-varying expectation of the data and a system of ODEs.\nCondition (A.2) requires that conditional on the expected value, observations from different\ncompartments of the system of ODE's are independent.  This requirement is not overly restrictive, since\nthe use of ODE's is intended to capture the relationships between the observed compartments.\nCondition (A.2) also specifies that the variance of observations from different compartments can differ.\nThe second part of condition (A.3) is included to prevent non-identifiability, but\ndoes not insure identifiability of the estimation procedure.\n\nThe key component of the BCLS method is the use of bias correction functions which are needed to account for bias which is introduced when\nmaking nonlinear transformations of random variables. These bias correction functions are used as weights in the least squares estimation.\nThese functions can be defined in a number of ways.\nFor a function $h = h_{q,k}$,\nidentify the difference, $E[h\\{{\\mbox{\\boldmath ${Y}$}}(t)\\}] - h\\{E[{\\mbox{\\boldmath ${Y}$}}(t)]\\}$ and then subtract it from $h$ to define $h^{\\star}$ as follows:\n\n", "index": 1, "text": "\\begin{equation}\\label{b1}\nh^{\\star}\\{ {\\mbox{\\boldmath ${Y}$}}(t)\\} =h \\{ {\\mbox{\\boldmath ${Y}$}}(t) \\} - \\left(  E[h\\{{\\mbox{\\boldmath ${Y}$}}(t)\\}] - h\\{E[{\\mbox{\\boldmath ${Y}$}}(t)]\\}    \\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"h^{\\star}\\{{\\mbox{\\boldmath${Y}$}}(t)\\}=h\\{{\\mbox{\\boldmath${Y}$}}(t)\\}-\\left(%&#10;E[h\\{{\\mbox{\\boldmath${Y}$}}(t)\\}]-h\\{E[{\\mbox{\\boldmath${Y}$}}(t)]\\}\\right).\" display=\"block\"><mrow><mrow><mrow><msup><mi>h</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><mrow><mo>(</mo><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nTaking expectations of both sides of  (\\ref{b1}) or (\\ref{b2}) shows that either definition satisfies\n\\begin{eqnarray}\\label{ehstar}\nE[h_{q,k}^{\\star}\\{{\\bf Y}(t)\\}]=h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(t) \\}, \\ \\ q=1,\\cdots,s,\\  k=1,\\cdots,m_q . \\ \\\n\\end{eqnarray}\nAny function, $h^{\\star}$ which satisfies (\\ref{ehstar}) can be used to weight the linear regression step\nof the estimation procedure to eliminate bias in the resulting point estimates.  The previous two examples\nshow that such a function always exists; the most efficient form of $h^{\\star}$ for a specific model generally depends on\nthe error structure of the data (e.g additive vs multiplicative).\n\nThe following notation will be used:\nLet $y_{q,i}$, $q=1,\\cdots,s$, $i=1,\\cdots,n$, denote an observation with expectation given\nby $X_q$ in Eq. (\\ref{geneq}) at time $t_i$;\n$y_q({\\bf t})$ denote the vector of all observations from the $q^{th}$\ncompartment, $X_q({\\bf t})$; ${\\bf y}(t_i)$ denote the vector of observations on all\ncompartments at a single observation time, $t_i$; and ${\\bf y}({\\bf\nt})$ denote the matrix of observations on all compartments at all\nobservation times. In general, lower case letters represent observed\ndata, and upper case represent the associated random variables or state variables of the system of ODE's.\nIn the following sections, when direct estimation of parameters in an ODE model is performed without the\nbias-correction  we will refer to the resulting point estimates as simply least squares (LS) estimates or\nleast squares without bias correction.\n\n\n\\subsection{ Estimation Algorithm }\nThe BCLS method is designed to simplify a\nnonlinear regression problem by reducing it to a linear regression problem.\nThis is achieved by transforming the system of differential equations (\\ref{geneq})\ninto a system of integral equations and treating the transformed system as a\nstatistical model\nfor a collection of linear regressions with covariates which approximate the integrals.\nApproximations to these covariates are obtained using\nthe observed data.\nSpecifically, Eq. (\\ref{geneq})\nis equivalent to the system of integral equations:\n\\begin{eqnarray*}\n  X_q(t) = \\sum_{k=1}^{m_q} \\beta_{q,k}\n    \\int_{t_0}^{t} h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau)\\} d\\tau+ { \\mbox{\\boldmath ${X}$}}_{q,0}\\ \\\n    q=1,\\cdots, s.\n\\end{eqnarray*}\nso that (A.1) implies\n\\begin{eqnarray}\n  E\\{Y_q(t)\\} & = &  \\sum_{k=1}^{m_q} \\beta_{q,k}\n    \\int_{t_0}^{t} h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\} d\\tau+ { \\mbox{\\boldmath ${X}$}}_{q,0}\\ \\\n    q=1,\\cdots, s.\n\\label{ey}\n\\end{eqnarray}\nThe expectations in Eq. (\\ref{ey}) suggest\n$s$ linear regression models with response variables (outcomes) $y_q({\\bf t})$ and\ncovariates which approximate the integrals $\\int_{t_0}^{t}  h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\} d\\tau $ ,\n$k=1,\\cdots,m_q$, $q=1,\\cdots, s$. If an intercept is included in the regression model it provides an estimate\nfor the initial condition, $X_q(t_0)$, and the estimated coefficients of the covariates\nprovide estimates of the parameters $\\beta_{q,1}, \\cdots, \\beta_{q,m_q}$.\nIf the initial value $ y_q(t_0)$ is known, the response variable\ncan be defined as $y_q({\\bf t}) - y_q(t_0)$ and linear regression without\nintercept can be used.\nFor ease of notation, we now drop the subscript $q$ in the response\nvariable and describe each of the $s$ states separately.\n\nTo obtain covariates for the regression analysis, any method of integral approximation can be used.\nFor the examples and simulations in this manuscript, the trapezoid method is used\nto approximate the integrals\n$ \\int_0^{t_i}  h_{k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\}  d\\tau $ and construct\ncovariates, $z_k(t_i)$, defined by:  $z_k(t_0)=0$ and for $k=1,\\cdots,m$ and $i=0,\\cdots,n$,\n", "itemtype": "equation", "pos": 13973, "prevtext": "\nAn alternative is to define $h^{\\star}$ as:\n\n", "index": 3, "text": "\\begin{equation}\\label{b2}\nh^{\\star}\\{ {\\mbox{\\boldmath ${Y}$}}(t)\\} =  h \\{ {\\mbox{\\boldmath ${Y}$}}(t) \\}  \\frac { h\\{E[{\\mbox{\\boldmath ${Y}$}}(t)]\\}   }  { E[h\\{{\\mbox{\\boldmath ${Y}$}}(t)\\}] }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"h^{\\star}\\{{\\mbox{\\boldmath${Y}$}}(t)\\}=h\\{{\\mbox{\\boldmath${Y}$}}(t)\\}\\frac{h%&#10;\\{E[{\\mbox{\\boldmath${Y}$}}(t)]\\}}{E[h\\{{\\mbox{\\boldmath${Y}$}}(t)\\}]}\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo>\u2062</mo><mfrac><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc80</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nwhere $h_k^{\\star}$ is the bias correction adjustment which satisfies (\\ref{ehstar}), e.g (\\ref{b1}) or (\\ref{b2}).\nLet $z_k({\\bf t})$ denote the vector, $\\{z_k(t_0),\\cdots,z_k(t_n)\\}$, and ${\\bf z(t)}$ denote the matrix\nwith columns $z_k({\\bf t})$, $k=1,\\cdots,m$.\nAlthough the trapezoid rule for integral approximations is used for the examples and simulations presented in this work,\nmany methods are available for integral approximation (\\cite{Pre:86}, Chapter 4).\nThe most simple integral approximation is defined by\nusing only the left endpoints of the intervals and can be described using the\nobserved data as follows:\n", "itemtype": "equation", "pos": 17941, "prevtext": "\nTaking expectations of both sides of  (\\ref{b1}) or (\\ref{b2}) shows that either definition satisfies\n\\begin{eqnarray}\\label{ehstar}\nE[h_{q,k}^{\\star}\\{{\\bf Y}(t)\\}]=h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(t) \\}, \\ \\ q=1,\\cdots,s,\\  k=1,\\cdots,m_q . \\ \\\n\\end{eqnarray}\nAny function, $h^{\\star}$ which satisfies (\\ref{ehstar}) can be used to weight the linear regression step\nof the estimation procedure to eliminate bias in the resulting point estimates.  The previous two examples\nshow that such a function always exists; the most efficient form of $h^{\\star}$ for a specific model generally depends on\nthe error structure of the data (e.g additive vs multiplicative).\n\nThe following notation will be used:\nLet $y_{q,i}$, $q=1,\\cdots,s$, $i=1,\\cdots,n$, denote an observation with expectation given\nby $X_q$ in Eq. (\\ref{geneq}) at time $t_i$;\n$y_q({\\bf t})$ denote the vector of all observations from the $q^{th}$\ncompartment, $X_q({\\bf t})$; ${\\bf y}(t_i)$ denote the vector of observations on all\ncompartments at a single observation time, $t_i$; and ${\\bf y}({\\bf\nt})$ denote the matrix of observations on all compartments at all\nobservation times. In general, lower case letters represent observed\ndata, and upper case represent the associated random variables or state variables of the system of ODE's.\nIn the following sections, when direct estimation of parameters in an ODE model is performed without the\nbias-correction  we will refer to the resulting point estimates as simply least squares (LS) estimates or\nleast squares without bias correction.\n\n\n\\subsection{ Estimation Algorithm }\nThe BCLS method is designed to simplify a\nnonlinear regression problem by reducing it to a linear regression problem.\nThis is achieved by transforming the system of differential equations (\\ref{geneq})\ninto a system of integral equations and treating the transformed system as a\nstatistical model\nfor a collection of linear regressions with covariates which approximate the integrals.\nApproximations to these covariates are obtained using\nthe observed data.\nSpecifically, Eq. (\\ref{geneq})\nis equivalent to the system of integral equations:\n\\begin{eqnarray*}\n  X_q(t) = \\sum_{k=1}^{m_q} \\beta_{q,k}\n    \\int_{t_0}^{t} h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau)\\} d\\tau+ { \\mbox{\\boldmath ${X}$}}_{q,0}\\ \\\n    q=1,\\cdots, s.\n\\end{eqnarray*}\nso that (A.1) implies\n\\begin{eqnarray}\n  E\\{Y_q(t)\\} & = &  \\sum_{k=1}^{m_q} \\beta_{q,k}\n    \\int_{t_0}^{t} h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\} d\\tau+ { \\mbox{\\boldmath ${X}$}}_{q,0}\\ \\\n    q=1,\\cdots, s.\n\\label{ey}\n\\end{eqnarray}\nThe expectations in Eq. (\\ref{ey}) suggest\n$s$ linear regression models with response variables (outcomes) $y_q({\\bf t})$ and\ncovariates which approximate the integrals $\\int_{t_0}^{t}  h_{q,k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\} d\\tau $ ,\n$k=1,\\cdots,m_q$, $q=1,\\cdots, s$. If an intercept is included in the regression model it provides an estimate\nfor the initial condition, $X_q(t_0)$, and the estimated coefficients of the covariates\nprovide estimates of the parameters $\\beta_{q,1}, \\cdots, \\beta_{q,m_q}$.\nIf the initial value $ y_q(t_0)$ is known, the response variable\ncan be defined as $y_q({\\bf t}) - y_q(t_0)$ and linear regression without\nintercept can be used.\nFor ease of notation, we now drop the subscript $q$ in the response\nvariable and describe each of the $s$ states separately.\n\nTo obtain covariates for the regression analysis, any method of integral approximation can be used.\nFor the examples and simulations in this manuscript, the trapezoid method is used\nto approximate the integrals\n$ \\int_0^{t_i}  h_{k}\\{ {\\mbox{\\boldmath ${X}$}}(\\tau) \\}  d\\tau $ and construct\ncovariates, $z_k(t_i)$, defined by:  $z_k(t_0)=0$ and for $k=1,\\cdots,m$ and $i=0,\\cdots,n$,\n", "index": 5, "text": "\n\\[  z_k(t_i) =  {\\displaystyle \\sum_{j=1}^i }\n \\frac{ [  h_k^{\\star} \\{ {\\bf y}(t_{j-1}) \\}  + h_k ^{\\star} \\{ {\\bf  y}(t_j)\n    \\} ] *  (t_{j}-t_{j-1}) }{2}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"z_{k}(t_{i})={\\displaystyle\\sum_{j=1}^{i}}\\frac{[h_{k}^{\\star}\\{{\\bf y}(t_{j-1%&#10;})\\}+h_{k}^{\\star}\\{{\\bf y}(t_{j})\\}]*(t_{j}-t_{j-1})}{2}\" display=\"block\"><mrow><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><mfrac><mrow><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msubsup><mi>h</mi><mi>k</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>h</mi><mi>k</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mo>*</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>-</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nNote that if only the left hand endpoint is used in the definition of $z_k({\\bf t})$,\nthe covariates are independent of the corresponding response, since the\ndefinition of $z_k(t_i)$ depends only on data observed at time points $t_0,\\cdots,t_{i-1}$\nand the $i^{th}$ response is the data observed at time $t_i$.\nAlthough we use this simple approximation to demonstrate consistency (Appendix 1),\nin practice the trapezoid rule produces more\naccurate estimates.\n\nWe can now define the linear regression model as\n", "itemtype": "equation", "pos": 18728, "prevtext": "\nwhere $h_k^{\\star}$ is the bias correction adjustment which satisfies (\\ref{ehstar}), e.g (\\ref{b1}) or (\\ref{b2}).\nLet $z_k({\\bf t})$ denote the vector, $\\{z_k(t_0),\\cdots,z_k(t_n)\\}$, and ${\\bf z(t)}$ denote the matrix\nwith columns $z_k({\\bf t})$, $k=1,\\cdots,m$.\nAlthough the trapezoid rule for integral approximations is used for the examples and simulations presented in this work,\nmany methods are available for integral approximation (\\cite{Pre:86}, Chapter 4).\nThe most simple integral approximation is defined by\nusing only the left endpoints of the intervals and can be described using the\nobserved data as follows:\n", "index": 7, "text": "\n\\[  z_k(t_i)=  {\\displaystyle \\sum_{j<i} }\n  h_k^{\\star}\\{ {\\bf y}(t_{j-1}) \\}  * (t_{j}-t_{j-1})\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"z_{k}(t_{i})={\\displaystyle\\sum_{j&lt;i}}h_{k}^{\\star}\\{{\\bf y}(t_{j-1})\\}*(t_{j}%&#10;-t_{j-1})\" display=\"block\"><mrow><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mrow><mrow><msubsup><mi>h</mi><mi>k</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>*</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>-</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nwhere $\\Sigma^2$ is the variance/covariance matrix for the residual errors.\nIf only the left hand endpoint of each interval is used in the definition of $z_k({\\bf t})$,\nCondition (A.2) guarantees that $\\Sigma$ is a diagonal matrix, although the\nvariances associated with each state are not assumed to be the same for all states.\nIn this work, we assume that $\\Sigma$ is known or will be obtained using an\nindependent method.  These variances are usually needed to construct the\nbias correction functions, $h_k^{\\star}$.\n\nTo estimate ${\\mbox{\\boldmath ${\\beta}$}}$, linear regression with response $y({\\bf t})$\n( or $y({\\bf t}) - y(t_0)$ ) and the covariate matrix ${\\bf z(t)}$ is performed\nso that\n  \\begin{eqnarray}\n  {\\widehat{\\mbox{\\boldmath ${\\beta}$}}} = ({\\bf z(t)}'{\\bf z(t)})^{-1}{\\bf z(t)}'y({\\bf t}) \\ \\\n  \\label{linreg}\n  \\end{eqnarray}\nIn \\cite{Lia:08}, the authors refer to regression analysis of this type as pseudo-least squares,\n(PsLS) since the minimization that occurs with the linear regression algorithm is not minimizing\na true likelihood. The same is true of bias-corrected least squares.\n\\vskip0.2in\n\nTo demonstrate consistency of the BCLS method we rely on the following Theorem.\n{\\small\n\\begin{theorem}\nAssume conditions (A.1)-(A.3) (from Section 2.1 ) are\nsatisfied and that ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_{n}$ is the unique root of the estimating function,\ndefined by \\ \\ $\\displaystyle {\\overline{U}}_{n}({\\mbox{\\boldmath ${\\beta}$}})=\\frac{1}{n}\n\\sum_{i=1}^n {\\mbox{\\boldmath ${z}$}}_i^T \\left\\{ {\\mbox{\\boldmath ${y}$}}_i - {\\mbox{\\boldmath ${z}$}}_i  {\\mbox{\\boldmath ${\\beta}$}} \\right\\}=0$ .\\ \\\nThen:\n\\begin{itemize}\n\\item[I]  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero.\n\\item[II] $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and\nnon-zero.\n\\item[III]  $ \\mbox{var}\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i\\} \\rightarrow 0$  as $n \\rightarrow \\infty$.\n\\end{itemize}\nThus, the linear least squares estimate, ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}} = ({\\bf z(t)}'{\\bf z(t)})^{-1}{\\bf z(t)}'y({\\bf t})$\nis a consistent estimator of ${\\mbox{\\boldmath ${\\beta}$}}$.\n\\end{theorem}\n}\n\nThe final conclusion, that the three conditions listed in Theorem 1\nimply consistency of estimator ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}$ from standard linear regression\nfollows from results in \\cite{Cro:86}.\nThe key technical issue is that the derived covariates, ${\\bf z}({\\bf t})$, are\nfunctions of the observed data rather than of the state means and are therefore\nmeasured with error, and that the covariates themselves are correlated with each other.\nHowever, contrary to the classical measurement error\nsetting the variance of the covariates is $O(n^{-1})$ and therefore\nbias due to error in the covariates is asymptotically eliminated.  It will be shown in the appendix that the BCLS method\nsatisfies condition [I-III].\n\n\n\\section{Example: A single compartment nonlinear ODE }\n\n\\subsection{Data Analysis: Growth colonies of paramecium aurelium}\nTo illustrate the method using a model defined by a nonlinear\ndifferential equation with a single compartment, data on the growth of four colonies of the\nunicellular organism, paramecium aurelium, were analyzed.  These data are described in \\cite{Dig:90,Gau:34}.\nIn this example a differential equation represents the size of the population of\nthe colony of paramecium over time.  We\nassume that the data on colony size, $y({\\bf t})$, follows a log-normal distribution, with\n", "itemtype": "equation", "pos": 19338, "prevtext": "\nNote that if only the left hand endpoint is used in the definition of $z_k({\\bf t})$,\nthe covariates are independent of the corresponding response, since the\ndefinition of $z_k(t_i)$ depends only on data observed at time points $t_0,\\cdots,t_{i-1}$\nand the $i^{th}$ response is the data observed at time $t_i$.\nAlthough we use this simple approximation to demonstrate consistency (Appendix 1),\nin practice the trapezoid rule produces more\naccurate estimates.\n\nWe can now define the linear regression model as\n", "index": 9, "text": "\n\\[ Y({\\bf t}) = {\\bf Z(t)}\\ {\\mbox{\\boldmath ${\\beta}$}} + {\\mbox{\\boldmath ${\\epsilon}$}}, \\ \\ \\  {\\mbox{\\boldmath ${\\epsilon}$}} \\sim \\ (0,\\Sigma^2) \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"Y({\\bf t})={\\bf Z(t)}\\ {\\mbox{\\boldmath${\\beta}$}}+{\\mbox{\\boldmath${\\epsilon}%&#10;$}},\\ \\ \\ {\\mbox{\\boldmath${\\epsilon}$}}\\sim\\ (0,\\Sigma^{2})\" display=\"block\"><mrow><mrow><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\ud835\udc19</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>+</mo><mi mathvariant=\"bold-italic\">\u03f5</mi></mrow></mrow><mo rspace=\"17.5pt\">,</mo><mrow><mi mathvariant=\"bold-italic\">\u03f5</mi><mo rspace=\"7.5pt\">\u223c</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi mathvariant=\"normal\">\u03a3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nwhere $X(t)$ satisfies\nthe standard logistic growth curve described by the nonlinear differential\nequation\n\\begin{eqnarray}\n  \\frac{dX}{dt}=X (a- b X), \\ \\  { \\mbox{\\boldmath ${X}$}}(0)=y_0\n  \\label{pareq}\n\\end{eqnarray}\nThe parameter  $a$ is the growth\nrate per capita, $\\frac{a}{b}$ is the carrying capacity of the\npopulation, and $y_0$ is the initial size of the populations which\nis known by design; $y_0=2$ in all four data sets.\nA log transform is applied to the data\nfor analysis and parameter estimation so that a log-transform of the differential equation\n(\\ref{pareq}) is required.\n\\begin{eqnarray}\n  \\frac{d\\{\\log(X) \\} }{dt}=a - b X, \\ \\  \\log\\{X(0)\\}=\\log(y_0).\n  \\label{logpareq}\n\\end{eqnarray}\n\nTo perform the BCLS method on the growth colony data,\nequation (\\ref{logpareq}) is rewritten as the integral equation\n\\begin{eqnarray*}\n  \\log\\{ X(t) \\} -\\log(y_0) = a \\int_0^{t} d\\tau- b \\int_0^{t} h\\{ X(\\tau)\\} d\\tau.\n\\end{eqnarray*}\nwhere $h\\{X\\}= X$. The first covariate $\\int_0^{t} d\\tau$ is simply\n${\\bf t}$ and no weighting is necessary for this integral approximation.\nTo determine the correct form of $h^{\\star}$ for computing the integral\napproximation for  $\\int_0^{t} h\\{X(\\tau)\\} d\\tau = \\int_0^{t} X(\\tau) d\\tau$ we calculate\n$E[h\\left\\{Y(t_i)\\right\\}] = E[Y(t_i)] = X(t_i) e^{\\frac{\\sigma^2}{2}}$\nsince the data, $y({\\bf t})$ follows a log-normal distribution. If $h^{\\star}(X) = X e^{-\\frac{\\sigma^2}{2}}$ it follows\nthat $E[h^{\\star}\\{Y(t_i)\\}] = E[ Y(t_i)\\  e^{-\\frac{\\sigma^2}{2}}] = X(t_i)\\  e^{\\frac{\\sigma^2}{2}}\\ e^{-\\frac{\\sigma^2}{2}} = X(t_i)$\nas required by (\\ref{ehstar}). Therefore, to approximate the\nintegral, $ \\int_0^{t} X(\\tau) d\\tau $, we\ndefine the covariates $z({\\bf t})=\\{z(t_0),z(t_1),\\cdots,z(t_n)\\}$ using the trapezoid rule for integral approximation:\n$z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = & e^{\\frac{-\\sigma^2}{2}} \\left\\{ \\sum_{j=1}^{i-1} \\frac{(y_j+y_{j-1})*(t_j-t_{j-1})}{2} \\right\\}\n\\end{eqnarray*}\nFinally, since  the initial value, $y_0$, is known, the response\nfor the regression model for the BCLS method is given by $y'({\\bf t})=\\log\\{y({\\bf t})\\}-\\log\\{y_0\\}$.\n\nTo obtain the BCLS estimates of $a$ and $b$ we\nuse standard linear regression and the following model without intercept.\n\\begin{eqnarray}\ny'({\\bf t}) \\sim a\\ {\\bf t}+ b\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\quad  {\\mbox{\\boldmath ${\\epsilon}$}} \\sim i.i.d. \\;  N(0,\\sigma^2 ).\n\\label{regmod}\n\\end{eqnarray}\nThe estimated coefficient of ${\\bf t}$ will provide an unbiased estimate of $a$ and the\nestimated coefficient of $z({\\bf t})$ will provide an unbiased estimate of $b$.  If the\nweights, $e^{\\frac{-\\sigma^2}{2}}$, are not used in the construction of the covariate, $z({\\bf t})$, the\nresulting estimates of one or both of the parameters $a$ and $b$ would likely be biased.\nIf the initial value of the system, $y_0$, is unknown, it can be treated\nas an additional parameter and\ncan be estimated by including an intercept in the regression model (\\ref{regmod}) and\nusing $log\\{y({\\bf t})\\}$ as the response.\n\nAn estimate of the measurement error, $\\sigma$, was obtained by fitting a natural spline\nwith 3 degrees of freedom to the data (the ns() macro in R version 2.9.2). The residuals\nfrom that fit were used to obtain an estimate of $\\sigma$ of approximately $0.23$ which\nwas used to construct the regression covariates $z({\\bf t})$. Next, the parameters $a$\nand $b$ were estimated for the four different data sets on colonies of paramecium\naureilium using the LS without bias correction and the BCLS method. For nonlinear least\nsquares regression, estimates of the parameters from the BCLS method were used as\nstarting values for the Marquart estimation algorithm. Parametric bootstrap techniques\nwere used to obtain 95\\% confidence intervals \\cite{Efr:93}.\n\nThe estimated values of $a$ and $b$ and their 95\\%  parametric bootstrap confidence intervals\nare shown in Table 1.  Non-parametric bootstrap\nconfidence intervals were also obtained for the BCLS method, since non-parametric\nbootstrap does not require an analytic or\nnumerical solution to equation (\\ref{pareq}). The data sets and fitted\ntrajectories from both the NLS and BCLS estimation procedures\nare shown in Figure 1.  The estimates and confidence\nintervals obtained from the two methods are quite similar, and Figure\n1 suggests that both methods produce estimates of the underlying\ntrajectories  which fit the data well.  For these four data sets, the BCLS method\nproduced virtually the same results as the LS method without bias correction (results not shown).\n\n\n\\subsection{Simulations}\nTo compare the BCLS method to NLS and the PsLS method described in \\cite{Lia:08} for\naccuracy and efficiency we simulated data from the statistical model\n\\begin{eqnarray*}\n\\log\\{Y ({\\bf t})\\} & = &  \\log\\{X({\\bf t})\\} + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\quad {\\mbox{\\boldmath ${\\epsilon}$}}  \\sim i.i.d \\;  N(0,\\sigma^2)\n\\end{eqnarray*}\nwhere  $X({\\bf t})$ is the solution of equation (\\ref{pareq}) with\nparameters set as follows: $a=0.8$, $b=0.0015$, and $y_0=2$.  These\nparameters were chosen based on the fits to the four data sets shown\nin Table 1. A range of values for residual error, $\\sigma$, were evaluated in\norder to demonstrate the increase in bias in\nthe estimate of the parameter $b$ using LS without\nadjusting the covariate $z(t)$ as described in the previous section.\nTwo sets of observations times were\nevaluated; Each consisted of observation times between $t=0$ and\n$t=20$.  The first set of observation times consisted of 21 total\nobservations, evenly spaced 1 unit apart between 0 and 20.  The\nsecond set of observation times consisted of 201 total observations,\nevenly spaced 0.1 units apart between 0 and 20.\nIndividual simulations we conducted for\neach of the two sets of observation times and each of the following values\nof $\\sigma$, $(0.2,0.4,0.6,0.8)$ for a total of 8 simulation studies.\n\nFor each combination of observation\ntimes and values of $\\sigma$, the data were\nsimulated 1000 times and the BCLS, LS (without weights to adjust for bias), PsLS and NLS\nestimates of $a$ and $b$ were used to estimate model parameters.  To evaluate the PsLS method the local polynomial smoothing macro locpoly()\nfrom the R KernSmooth package was used to obtain\nthe smoothed state and derivative curves.  A bandwidth of\norder $\\log(nt)^{-1/10}*nt^{-2/7}$ was chosen for both state and\nderviative estimation as suggested in \\cite{Lia:08}, where $nt$ is the sample size in each simulation.  A number of other choices for bandwidth we considered,\nincluding the standard choice of order $nt^{-1/5}$.  The choice of bandwidth used in the simulations was made based on\nselecting the bandwidth that produced the least biased estimates.\n\n\nFigure 2 depicts the estimated values of the parameters $a$ and $b$ using the LS method (dotted lines), the BCLS method (combined dash and dotted lines), the NLS method (solid line) and the PsLS method (dashed line).  No bias correction is necessary for the integral approximation of the covariate used to estimate the parameter $a$ so that bias-corrected and unadjusted least squares estimates of $a$ are identical.  Panels A and B of Figure 2 show how bias in the\nestimated parameters varies with measurement error in the simulated data when 21 equally\nspaced time points between $t=0$ and $t=20$ are used; Panels C and D depict the same when the sample size is increased to 201 time points.\nThe Monte Carlo means and standard errors for the estimated parameters from each of the simulations with 21 equally spaced observations  are shown in Table 2.\n\nAs expected, Figure 2, panels B and D show that bias increases in the LS estimate of the\nparameter $b$ as simulated measurement error, $\\sigma$, increases from 0.2 to 0.8 when\nbias correction is not used in covariate definition. Also as expected, the bias in the LS\nestimate of $b$ without bias-correction is essentially unaffected by sample size (Figure\n2, Panels C and D). With a total of 21 time points, the BCLS method appears to\nunderestimate the parameter $a$ slightly, likely due to errors in integral approximation\nof a convex function with relatively sparse measurements.  The NLS method appears to\noverestimate that same parameter, probably due to the bias associated with nonlinear\nregression \\cite{Cook:86}.  The bias in the BCLS and NLS estimates decreases\nsubstantially when sample size is increased to 201 timepoints(Panes C and D). Figure 2\nalso shows that bias in PsLS estimates of both $a$ and $b$ changes as $\\sigma$ increases,\nindicating the constant in the choice of bandwidth affect bias in parameters estimated\nusing this method. . An increase in sample size from 21 time points to 201 time points\nreduces the bias in the estimate of the parameter $a$ from the strongly consistent PsLS\nmethod \\cite{Lia:08}, however, bias in the estimate of parameter $b$ seems to shift but\nnot decrease, possibly due to an improper choice of bandwidth. Additional simulations\nwith larger samples sizes showed this bias decreases but not until an extremely large\nsample size is used. The numeric results from the simulation studies with 21 observation\npoints are shown in Table 2 and confirm the findings described for Figure 2.\n\nThe efficiency of the four methods can be evaluated using the Monte Carlo standard errors shown in Table 2.\nThese standard errors indicate that the\nvariability of the estimation procedures are quite similar, with the NLS method providing slightly more\nprecise (lower standard error) estimates.  This is expected, since in this setting, NLS is the maximum likelihood\nmethod for parameter estimation.  In general, the Monte Carlo standard errors for estimates obtained with the PsLS method are slightly larger\nthan those obtained with BCLS, possibly due to differencing of the data required by methods which rely on differentiation.\nThe Monte Carlo standard errors from all three methods for the simulations with 201\nequally spaced time points between $t=0$ and $t=20$ are show a similar pattern in standard errors from the three methods.\n(Results not shown).\n\n\n\\section{Example 2: Nonlinear System of ODEs with two compartments}\n\nThe Fitzhugh-Nagumo system of\ndifferential equations \\cite{Fit:61,Nag:62} is a simplified version of\nthe the well known Hodgkin-Hukley model \\cite{Hod:52} for the behavior of spike potentials in\nthe giant axon of squid neurons. The equations\n\\begin{eqnarray}\n \\frac{dV}{dt}= C (V - \\frac{V^3}{3} + R), \\qquad V(0)=v(0) \\label{fhV} \\\\\n \\frac{dR}{dt}= -\\frac{1}{C}(V - a + bR),\\qquad R(0)=r(0) \\label{fhR}\n\\end{eqnarray}\ndescribe the reciprocal\ndependencies of the voltage compartment $V$ across an axon\nmembrane and a recovery compartment $R$ summarizing outward currents. Solutions to this system\nof differential equations quickly converge for a range of starting values to periodic\nbehavior that alternates between smooth evolution and sharp changes (bifurcations) in direction.\nThese solutions exhibit features common to elements of biological neural networks\n\\cite{Wilson:99}.  The system has been used by a number of authors including \\cite{RHCC:07,Lia:08}\nto demonstrate methods of parameter estimation for data with time varying expectation given by this\nsystem of differential equations.\n\nFor this example there are two random variables: the voltage across\nan axon membrane, $Y_1({\\bf t})$, and a recovery measurement of\noutward currents, $Y_2({\\bf t})$. We assume that observed data,\n$y_1({\\bf t})$ and $y_2({\\bf t})$ follow independent normal\ndistributions with\n\\begin{eqnarray}\n Y_1({\\bf t})  \\  = \\ V({\\bf t})\\ + {\\mbox{\\boldmath ${\\epsilon}$}}_1,\\ {\\mbox{\\boldmath ${\\epsilon}$}}_1 \\   \\sim \\  i.i.d.\\;  N(0,\\sigma_1^2) \\label{YV} \\\\\n Y_2({\\bf t}) \\  = \\ R({\\bf t})\\ + {\\mbox{\\boldmath ${\\epsilon}$}}_2,\\ {\\mbox{\\boldmath ${\\epsilon}$}}_2 \\ \\sim\\  i.i.d.\\;  N(0,\\sigma_2^2) \\label{YR}\n\\end{eqnarray}\nwhere $V(t)$ and $R(t)$ satisfy the differential equations (\\ref{fhV}) and (\\ref{fhR}).\n\nThe likelihood surface for data with expectation given by a nonlinear system\nof differential equations may be extremely complex and can contain multiple local maxima\n(equivalently, local minimum solutions to the nonlinear least\nsquares regression model). For the Fitzhugh-Nagumo equations we\nconsidered two sets of parameters; $C=3$, $a = 0.58$, $b=0.58$ and\n$C=3$, $a = 0.34$, $b=0.20$.  The parameters $C=3$, $a = 0.58$,\n$b=0.58$ are very near a supercritical Hopf bifurcation value of the system; from a\nstable steady state to an oscillating limit cycle.  There is a\ndramatic change in the least squares surface as the parameters $a$\nand $b$ vary about 0.58 (with $C = 3$ fixed).  Contours of the\nleast squares surface for a set of zero noise data are shown in\nFigure 3 panels A, B,\nand C. Panel A shows the contours of the least square surface for a\nrange of $a$ = 0.4 to 1.4 and $b$ = 0 to 0.2 to 2.2.  The blue box\ncontains the true solution, $a=0.58$ and $b=0.58$, the red box contains a local minimum.\nPanels B and C show close ups of the red and blue regions, respectively.\nPanel B suggests that there is local minimum in the least squares fit surface\nat approximately $a$ = 1.5, $b$ = 2.0.  Panel C suggests that the least squares\nfit surface is badly behaved near the true solution, with a long ridge containing the true\nsolution, which is near a bifurcation values for the parameters $a$ and $b$.\n\nFigure 3 Panels D,E, and F show similar regions of a least squares surface for zero noise\nobservations from the Fitzhugh-Nagumo system with parameters $C=3$, $a=0.34$, and\n$b=0.2$.  This system has a local minimum shown in the red square and\nthe true solutions shown in the blue square Panel D.  Close-ups\nof these regions are shown in panels E and F respectively.  For this\ncollection of parameters, the least squares surface near the true solution\nis well behaved (Panel F), however, a local minimum in the surface\nexists near $a = 1.7$, $b$ =2.8 (Panel E).\n\n\\subsection{Simulations}\n\nTo compare the BCLS method to NLS and PsLS for accuracy and efficiency in estimating\nparameters in the Fitzhugh-Nagumo model, data was simulated from the\nstatistical model (\\ref{YV}) and (\\ref{YR}), for two sets of parameter values; $C$ =3,\n$a$ = 0.34, $b$ = 0.2 and $C$ =3, $a$ = 0.58, $b$ = 0.58.  For each of the\ntwo sets of parameter values, a range of residual standard errors $\\sigma_1$ and\n$\\sigma_2$ were evaluted ; $\\sigma_1$ = 0.05, 0.1, and 0.15, and\n$\\sigma_2$ = 0.05, 0.1, and 0.15.\nTime points evenly spaced time points at intervals of 0.1 between $0$ and $20$ for a total of 201 time points were considered.\nInitial conditions for the system,\n$v_0$ = -1 and $r_0$ =1 were used for all simulations. The data were\nsimulated 1000 times for each set of parameter values and\nvalues of $\\sigma_1$ and $\\sigma_2$. All three parameters\n$C$, $a$, and $b$ as well as the initial conditions, $v_0$ and $r_0$ were estimated\nusing nonlinear least squares, bias-corrected least squares and least square without bias correction.\nSince PsLS relies on derivatives rather than integrals, estimation of the intial conditions $v_0$ and $r_0$ is not possible,\nso that only estimates of the three model parameters, $a$, $b$ and $C$ were obtained using the PsLS method.\n\nFor each of the 1000 replicates, nonlinear least squares regression\nof all five values is conducted simultaneously\nusing simulated data $y_1({\\bf t})$ and $y_2({\\bf t})$ and solutions to\n(\\ref{fhV}) and (\\ref{fhR}) to form the weighted (when $\\sigma_1^2 \\neq \\sigma_2^2)$ least squares expression\n", "itemtype": "equation", "pos": 23091, "prevtext": "\nwhere $\\Sigma^2$ is the variance/covariance matrix for the residual errors.\nIf only the left hand endpoint of each interval is used in the definition of $z_k({\\bf t})$,\nCondition (A.2) guarantees that $\\Sigma$ is a diagonal matrix, although the\nvariances associated with each state are not assumed to be the same for all states.\nIn this work, we assume that $\\Sigma$ is known or will be obtained using an\nindependent method.  These variances are usually needed to construct the\nbias correction functions, $h_k^{\\star}$.\n\nTo estimate ${\\mbox{\\boldmath ${\\beta}$}}$, linear regression with response $y({\\bf t})$\n( or $y({\\bf t}) - y(t_0)$ ) and the covariate matrix ${\\bf z(t)}$ is performed\nso that\n  \\begin{eqnarray}\n  {\\widehat{\\mbox{\\boldmath ${\\beta}$}}} = ({\\bf z(t)}'{\\bf z(t)})^{-1}{\\bf z(t)}'y({\\bf t}) \\ \\\n  \\label{linreg}\n  \\end{eqnarray}\nIn \\cite{Lia:08}, the authors refer to regression analysis of this type as pseudo-least squares,\n(PsLS) since the minimization that occurs with the linear regression algorithm is not minimizing\na true likelihood. The same is true of bias-corrected least squares.\n\\vskip0.2in\n\nTo demonstrate consistency of the BCLS method we rely on the following Theorem.\n{\\small\n\\begin{theorem}\nAssume conditions (A.1)-(A.3) (from Section 2.1 ) are\nsatisfied and that ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_{n}$ is the unique root of the estimating function,\ndefined by \\ \\ $\\displaystyle {\\overline{U}}_{n}({\\mbox{\\boldmath ${\\beta}$}})=\\frac{1}{n}\n\\sum_{i=1}^n {\\mbox{\\boldmath ${z}$}}_i^T \\left\\{ {\\mbox{\\boldmath ${y}$}}_i - {\\mbox{\\boldmath ${z}$}}_i  {\\mbox{\\boldmath ${\\beta}$}} \\right\\}=0$ .\\ \\\nThen:\n\\begin{itemize}\n\\item[I]  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero.\n\\item[II] $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and\nnon-zero.\n\\item[III]  $ \\mbox{var}\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i\\} \\rightarrow 0$  as $n \\rightarrow \\infty$.\n\\end{itemize}\nThus, the linear least squares estimate, ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}} = ({\\bf z(t)}'{\\bf z(t)})^{-1}{\\bf z(t)}'y({\\bf t})$\nis a consistent estimator of ${\\mbox{\\boldmath ${\\beta}$}}$.\n\\end{theorem}\n}\n\nThe final conclusion, that the three conditions listed in Theorem 1\nimply consistency of estimator ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}$ from standard linear regression\nfollows from results in \\cite{Cro:86}.\nThe key technical issue is that the derived covariates, ${\\bf z}({\\bf t})$, are\nfunctions of the observed data rather than of the state means and are therefore\nmeasured with error, and that the covariates themselves are correlated with each other.\nHowever, contrary to the classical measurement error\nsetting the variance of the covariates is $O(n^{-1})$ and therefore\nbias due to error in the covariates is asymptotically eliminated.  It will be shown in the appendix that the BCLS method\nsatisfies condition [I-III].\n\n\n\\section{Example: A single compartment nonlinear ODE }\n\n\\subsection{Data Analysis: Growth colonies of paramecium aurelium}\nTo illustrate the method using a model defined by a nonlinear\ndifferential equation with a single compartment, data on the growth of four colonies of the\nunicellular organism, paramecium aurelium, were analyzed.  These data are described in \\cite{Dig:90,Gau:34}.\nIn this example a differential equation represents the size of the population of\nthe colony of paramecium over time.  We\nassume that the data on colony size, $y({\\bf t})$, follows a log-normal distribution, with\n", "index": 11, "text": "\n\\[\n\\log\\{ Y({\\bf t}) \\} \\  = \\  \\log\\{ X({\\bf t})\\} + {\\mbox{\\boldmath ${\\epsilon}$}},\\ {\\mbox{\\boldmath ${\\epsilon}$}} \\   \\sim \\  i.i.d.\\;  N(0,\\sigma^2)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\log\\{Y({\\bf t})\\}\\ =\\ \\log\\{X({\\bf t})\\}+{\\mbox{\\boldmath${\\epsilon}$}},\\ {%&#10;\\mbox{\\boldmath${\\epsilon}$}}\\ \\sim\\ i.i.d.\\;N(0,\\sigma^{2})\" display=\"block\"><mrow><mrow><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>+</mo><mi mathvariant=\"bold-italic\">\u03f5</mi></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mpadded width=\"+5pt\"><mi mathvariant=\"bold-italic\">\u03f5</mi></mpadded><mo rspace=\"7.5pt\">\u223c</mo><mi>i</mi></mrow></mrow><mo>.</mo><mi>i</mi><mo>.</mo><mi>d</mi><mo rspace=\"5.3pt\">.</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nThe BCLS estimation is conducted in two steps. First,the parameter $C$  and initial condition $v_0$ are estimated using\ndata $y_1({\\bf t})$ a regression model based on (\\ref{fhV}).  Next the parameters $a$ and $b$ and initial condition $r_0$ are estimated using data $y_2({\\bf t})$, a regression model based on (\\ref{fhR}), and the estimate of $C$ obtained in the first step.\n\nTo estimate $C$ and $v_0$, equation (\\ref{fhV}) is re-written as the integral equation\n\\begin{eqnarray}\nV(t)  & = &  C \\int_0^{t} h_{1,1} (V,R) d\\tau  + v_0 \\label {fhintV}\n\\end{eqnarray}\nwhere $h_{1,1}(V,R) = V - \\frac{V^3}{3} + R$. Since $E[Y_1^3] \\neq E[Y_1]^3$ a bias correction function\nmust be identified.  Calculating $E[Y_1-\\frac{Y_1^3}{3}+Y_2] = V + (\\frac{V^3}{3} + 3 V \\sigma_1^2) + R$,\nit is clear that\n", "itemtype": "equation", "pos": 38793, "prevtext": "\nwhere $X(t)$ satisfies\nthe standard logistic growth curve described by the nonlinear differential\nequation\n\\begin{eqnarray}\n  \\frac{dX}{dt}=X (a- b X), \\ \\  { \\mbox{\\boldmath ${X}$}}(0)=y_0\n  \\label{pareq}\n\\end{eqnarray}\nThe parameter  $a$ is the growth\nrate per capita, $\\frac{a}{b}$ is the carrying capacity of the\npopulation, and $y_0$ is the initial size of the populations which\nis known by design; $y_0=2$ in all four data sets.\nA log transform is applied to the data\nfor analysis and parameter estimation so that a log-transform of the differential equation\n(\\ref{pareq}) is required.\n\\begin{eqnarray}\n  \\frac{d\\{\\log(X) \\} }{dt}=a - b X, \\ \\  \\log\\{X(0)\\}=\\log(y_0).\n  \\label{logpareq}\n\\end{eqnarray}\n\nTo perform the BCLS method on the growth colony data,\nequation (\\ref{logpareq}) is rewritten as the integral equation\n\\begin{eqnarray*}\n  \\log\\{ X(t) \\} -\\log(y_0) = a \\int_0^{t} d\\tau- b \\int_0^{t} h\\{ X(\\tau)\\} d\\tau.\n\\end{eqnarray*}\nwhere $h\\{X\\}= X$. The first covariate $\\int_0^{t} d\\tau$ is simply\n${\\bf t}$ and no weighting is necessary for this integral approximation.\nTo determine the correct form of $h^{\\star}$ for computing the integral\napproximation for  $\\int_0^{t} h\\{X(\\tau)\\} d\\tau = \\int_0^{t} X(\\tau) d\\tau$ we calculate\n$E[h\\left\\{Y(t_i)\\right\\}] = E[Y(t_i)] = X(t_i) e^{\\frac{\\sigma^2}{2}}$\nsince the data, $y({\\bf t})$ follows a log-normal distribution. If $h^{\\star}(X) = X e^{-\\frac{\\sigma^2}{2}}$ it follows\nthat $E[h^{\\star}\\{Y(t_i)\\}] = E[ Y(t_i)\\  e^{-\\frac{\\sigma^2}{2}}] = X(t_i)\\  e^{\\frac{\\sigma^2}{2}}\\ e^{-\\frac{\\sigma^2}{2}} = X(t_i)$\nas required by (\\ref{ehstar}). Therefore, to approximate the\nintegral, $ \\int_0^{t} X(\\tau) d\\tau $, we\ndefine the covariates $z({\\bf t})=\\{z(t_0),z(t_1),\\cdots,z(t_n)\\}$ using the trapezoid rule for integral approximation:\n$z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = & e^{\\frac{-\\sigma^2}{2}} \\left\\{ \\sum_{j=1}^{i-1} \\frac{(y_j+y_{j-1})*(t_j-t_{j-1})}{2} \\right\\}\n\\end{eqnarray*}\nFinally, since  the initial value, $y_0$, is known, the response\nfor the regression model for the BCLS method is given by $y'({\\bf t})=\\log\\{y({\\bf t})\\}-\\log\\{y_0\\}$.\n\nTo obtain the BCLS estimates of $a$ and $b$ we\nuse standard linear regression and the following model without intercept.\n\\begin{eqnarray}\ny'({\\bf t}) \\sim a\\ {\\bf t}+ b\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\quad  {\\mbox{\\boldmath ${\\epsilon}$}} \\sim i.i.d. \\;  N(0,\\sigma^2 ).\n\\label{regmod}\n\\end{eqnarray}\nThe estimated coefficient of ${\\bf t}$ will provide an unbiased estimate of $a$ and the\nestimated coefficient of $z({\\bf t})$ will provide an unbiased estimate of $b$.  If the\nweights, $e^{\\frac{-\\sigma^2}{2}}$, are not used in the construction of the covariate, $z({\\bf t})$, the\nresulting estimates of one or both of the parameters $a$ and $b$ would likely be biased.\nIf the initial value of the system, $y_0$, is unknown, it can be treated\nas an additional parameter and\ncan be estimated by including an intercept in the regression model (\\ref{regmod}) and\nusing $log\\{y({\\bf t})\\}$ as the response.\n\nAn estimate of the measurement error, $\\sigma$, was obtained by fitting a natural spline\nwith 3 degrees of freedom to the data (the ns() macro in R version 2.9.2). The residuals\nfrom that fit were used to obtain an estimate of $\\sigma$ of approximately $0.23$ which\nwas used to construct the regression covariates $z({\\bf t})$. Next, the parameters $a$\nand $b$ were estimated for the four different data sets on colonies of paramecium\naureilium using the LS without bias correction and the BCLS method. For nonlinear least\nsquares regression, estimates of the parameters from the BCLS method were used as\nstarting values for the Marquart estimation algorithm. Parametric bootstrap techniques\nwere used to obtain 95\\% confidence intervals \\cite{Efr:93}.\n\nThe estimated values of $a$ and $b$ and their 95\\%  parametric bootstrap confidence intervals\nare shown in Table 1.  Non-parametric bootstrap\nconfidence intervals were also obtained for the BCLS method, since non-parametric\nbootstrap does not require an analytic or\nnumerical solution to equation (\\ref{pareq}). The data sets and fitted\ntrajectories from both the NLS and BCLS estimation procedures\nare shown in Figure 1.  The estimates and confidence\nintervals obtained from the two methods are quite similar, and Figure\n1 suggests that both methods produce estimates of the underlying\ntrajectories  which fit the data well.  For these four data sets, the BCLS method\nproduced virtually the same results as the LS method without bias correction (results not shown).\n\n\n\\subsection{Simulations}\nTo compare the BCLS method to NLS and the PsLS method described in \\cite{Lia:08} for\naccuracy and efficiency we simulated data from the statistical model\n\\begin{eqnarray*}\n\\log\\{Y ({\\bf t})\\} & = &  \\log\\{X({\\bf t})\\} + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\quad {\\mbox{\\boldmath ${\\epsilon}$}}  \\sim i.i.d \\;  N(0,\\sigma^2)\n\\end{eqnarray*}\nwhere  $X({\\bf t})$ is the solution of equation (\\ref{pareq}) with\nparameters set as follows: $a=0.8$, $b=0.0015$, and $y_0=2$.  These\nparameters were chosen based on the fits to the four data sets shown\nin Table 1. A range of values for residual error, $\\sigma$, were evaluated in\norder to demonstrate the increase in bias in\nthe estimate of the parameter $b$ using LS without\nadjusting the covariate $z(t)$ as described in the previous section.\nTwo sets of observations times were\nevaluated; Each consisted of observation times between $t=0$ and\n$t=20$.  The first set of observation times consisted of 21 total\nobservations, evenly spaced 1 unit apart between 0 and 20.  The\nsecond set of observation times consisted of 201 total observations,\nevenly spaced 0.1 units apart between 0 and 20.\nIndividual simulations we conducted for\neach of the two sets of observation times and each of the following values\nof $\\sigma$, $(0.2,0.4,0.6,0.8)$ for a total of 8 simulation studies.\n\nFor each combination of observation\ntimes and values of $\\sigma$, the data were\nsimulated 1000 times and the BCLS, LS (without weights to adjust for bias), PsLS and NLS\nestimates of $a$ and $b$ were used to estimate model parameters.  To evaluate the PsLS method the local polynomial smoothing macro locpoly()\nfrom the R KernSmooth package was used to obtain\nthe smoothed state and derivative curves.  A bandwidth of\norder $\\log(nt)^{-1/10}*nt^{-2/7}$ was chosen for both state and\nderviative estimation as suggested in \\cite{Lia:08}, where $nt$ is the sample size in each simulation.  A number of other choices for bandwidth we considered,\nincluding the standard choice of order $nt^{-1/5}$.  The choice of bandwidth used in the simulations was made based on\nselecting the bandwidth that produced the least biased estimates.\n\n\nFigure 2 depicts the estimated values of the parameters $a$ and $b$ using the LS method (dotted lines), the BCLS method (combined dash and dotted lines), the NLS method (solid line) and the PsLS method (dashed line).  No bias correction is necessary for the integral approximation of the covariate used to estimate the parameter $a$ so that bias-corrected and unadjusted least squares estimates of $a$ are identical.  Panels A and B of Figure 2 show how bias in the\nestimated parameters varies with measurement error in the simulated data when 21 equally\nspaced time points between $t=0$ and $t=20$ are used; Panels C and D depict the same when the sample size is increased to 201 time points.\nThe Monte Carlo means and standard errors for the estimated parameters from each of the simulations with 21 equally spaced observations  are shown in Table 2.\n\nAs expected, Figure 2, panels B and D show that bias increases in the LS estimate of the\nparameter $b$ as simulated measurement error, $\\sigma$, increases from 0.2 to 0.8 when\nbias correction is not used in covariate definition. Also as expected, the bias in the LS\nestimate of $b$ without bias-correction is essentially unaffected by sample size (Figure\n2, Panels C and D). With a total of 21 time points, the BCLS method appears to\nunderestimate the parameter $a$ slightly, likely due to errors in integral approximation\nof a convex function with relatively sparse measurements.  The NLS method appears to\noverestimate that same parameter, probably due to the bias associated with nonlinear\nregression \\cite{Cook:86}.  The bias in the BCLS and NLS estimates decreases\nsubstantially when sample size is increased to 201 timepoints(Panes C and D). Figure 2\nalso shows that bias in PsLS estimates of both $a$ and $b$ changes as $\\sigma$ increases,\nindicating the constant in the choice of bandwidth affect bias in parameters estimated\nusing this method. . An increase in sample size from 21 time points to 201 time points\nreduces the bias in the estimate of the parameter $a$ from the strongly consistent PsLS\nmethod \\cite{Lia:08}, however, bias in the estimate of parameter $b$ seems to shift but\nnot decrease, possibly due to an improper choice of bandwidth. Additional simulations\nwith larger samples sizes showed this bias decreases but not until an extremely large\nsample size is used. The numeric results from the simulation studies with 21 observation\npoints are shown in Table 2 and confirm the findings described for Figure 2.\n\nThe efficiency of the four methods can be evaluated using the Monte Carlo standard errors shown in Table 2.\nThese standard errors indicate that the\nvariability of the estimation procedures are quite similar, with the NLS method providing slightly more\nprecise (lower standard error) estimates.  This is expected, since in this setting, NLS is the maximum likelihood\nmethod for parameter estimation.  In general, the Monte Carlo standard errors for estimates obtained with the PsLS method are slightly larger\nthan those obtained with BCLS, possibly due to differencing of the data required by methods which rely on differentiation.\nThe Monte Carlo standard errors from all three methods for the simulations with 201\nequally spaced time points between $t=0$ and $t=20$ are show a similar pattern in standard errors from the three methods.\n(Results not shown).\n\n\n\\section{Example 2: Nonlinear System of ODEs with two compartments}\n\nThe Fitzhugh-Nagumo system of\ndifferential equations \\cite{Fit:61,Nag:62} is a simplified version of\nthe the well known Hodgkin-Hukley model \\cite{Hod:52} for the behavior of spike potentials in\nthe giant axon of squid neurons. The equations\n\\begin{eqnarray}\n \\frac{dV}{dt}= C (V - \\frac{V^3}{3} + R), \\qquad V(0)=v(0) \\label{fhV} \\\\\n \\frac{dR}{dt}= -\\frac{1}{C}(V - a + bR),\\qquad R(0)=r(0) \\label{fhR}\n\\end{eqnarray}\ndescribe the reciprocal\ndependencies of the voltage compartment $V$ across an axon\nmembrane and a recovery compartment $R$ summarizing outward currents. Solutions to this system\nof differential equations quickly converge for a range of starting values to periodic\nbehavior that alternates between smooth evolution and sharp changes (bifurcations) in direction.\nThese solutions exhibit features common to elements of biological neural networks\n\\cite{Wilson:99}.  The system has been used by a number of authors including \\cite{RHCC:07,Lia:08}\nto demonstrate methods of parameter estimation for data with time varying expectation given by this\nsystem of differential equations.\n\nFor this example there are two random variables: the voltage across\nan axon membrane, $Y_1({\\bf t})$, and a recovery measurement of\noutward currents, $Y_2({\\bf t})$. We assume that observed data,\n$y_1({\\bf t})$ and $y_2({\\bf t})$ follow independent normal\ndistributions with\n\\begin{eqnarray}\n Y_1({\\bf t})  \\  = \\ V({\\bf t})\\ + {\\mbox{\\boldmath ${\\epsilon}$}}_1,\\ {\\mbox{\\boldmath ${\\epsilon}$}}_1 \\   \\sim \\  i.i.d.\\;  N(0,\\sigma_1^2) \\label{YV} \\\\\n Y_2({\\bf t}) \\  = \\ R({\\bf t})\\ + {\\mbox{\\boldmath ${\\epsilon}$}}_2,\\ {\\mbox{\\boldmath ${\\epsilon}$}}_2 \\ \\sim\\  i.i.d.\\;  N(0,\\sigma_2^2) \\label{YR}\n\\end{eqnarray}\nwhere $V(t)$ and $R(t)$ satisfy the differential equations (\\ref{fhV}) and (\\ref{fhR}).\n\nThe likelihood surface for data with expectation given by a nonlinear system\nof differential equations may be extremely complex and can contain multiple local maxima\n(equivalently, local minimum solutions to the nonlinear least\nsquares regression model). For the Fitzhugh-Nagumo equations we\nconsidered two sets of parameters; $C=3$, $a = 0.58$, $b=0.58$ and\n$C=3$, $a = 0.34$, $b=0.20$.  The parameters $C=3$, $a = 0.58$,\n$b=0.58$ are very near a supercritical Hopf bifurcation value of the system; from a\nstable steady state to an oscillating limit cycle.  There is a\ndramatic change in the least squares surface as the parameters $a$\nand $b$ vary about 0.58 (with $C = 3$ fixed).  Contours of the\nleast squares surface for a set of zero noise data are shown in\nFigure 3 panels A, B,\nand C. Panel A shows the contours of the least square surface for a\nrange of $a$ = 0.4 to 1.4 and $b$ = 0 to 0.2 to 2.2.  The blue box\ncontains the true solution, $a=0.58$ and $b=0.58$, the red box contains a local minimum.\nPanels B and C show close ups of the red and blue regions, respectively.\nPanel B suggests that there is local minimum in the least squares fit surface\nat approximately $a$ = 1.5, $b$ = 2.0.  Panel C suggests that the least squares\nfit surface is badly behaved near the true solution, with a long ridge containing the true\nsolution, which is near a bifurcation values for the parameters $a$ and $b$.\n\nFigure 3 Panels D,E, and F show similar regions of a least squares surface for zero noise\nobservations from the Fitzhugh-Nagumo system with parameters $C=3$, $a=0.34$, and\n$b=0.2$.  This system has a local minimum shown in the red square and\nthe true solutions shown in the blue square Panel D.  Close-ups\nof these regions are shown in panels E and F respectively.  For this\ncollection of parameters, the least squares surface near the true solution\nis well behaved (Panel F), however, a local minimum in the surface\nexists near $a = 1.7$, $b$ =2.8 (Panel E).\n\n\\subsection{Simulations}\n\nTo compare the BCLS method to NLS and PsLS for accuracy and efficiency in estimating\nparameters in the Fitzhugh-Nagumo model, data was simulated from the\nstatistical model (\\ref{YV}) and (\\ref{YR}), for two sets of parameter values; $C$ =3,\n$a$ = 0.34, $b$ = 0.2 and $C$ =3, $a$ = 0.58, $b$ = 0.58.  For each of the\ntwo sets of parameter values, a range of residual standard errors $\\sigma_1$ and\n$\\sigma_2$ were evaluted ; $\\sigma_1$ = 0.05, 0.1, and 0.15, and\n$\\sigma_2$ = 0.05, 0.1, and 0.15.\nTime points evenly spaced time points at intervals of 0.1 between $0$ and $20$ for a total of 201 time points were considered.\nInitial conditions for the system,\n$v_0$ = -1 and $r_0$ =1 were used for all simulations. The data were\nsimulated 1000 times for each set of parameter values and\nvalues of $\\sigma_1$ and $\\sigma_2$. All three parameters\n$C$, $a$, and $b$ as well as the initial conditions, $v_0$ and $r_0$ were estimated\nusing nonlinear least squares, bias-corrected least squares and least square without bias correction.\nSince PsLS relies on derivatives rather than integrals, estimation of the intial conditions $v_0$ and $r_0$ is not possible,\nso that only estimates of the three model parameters, $a$, $b$ and $C$ were obtained using the PsLS method.\n\nFor each of the 1000 replicates, nonlinear least squares regression\nof all five values is conducted simultaneously\nusing simulated data $y_1({\\bf t})$ and $y_2({\\bf t})$ and solutions to\n(\\ref{fhV}) and (\\ref{fhR}) to form the weighted (when $\\sigma_1^2 \\neq \\sigma_2^2)$ least squares expression\n", "index": 13, "text": "\n\\[ \\sum_{i=1}^n \\frac{(y_1(t_i) - V(t_i))^2}{\\sigma_1^2} + \\frac{(y_2(t_i) - R(t_i))^2}{\\sigma_2^2}. \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{n}\\frac{(y_{1}(t_{i})-V(t_{i}))^{2}}{\\sigma_{1}^{2}}+\\frac{(y_{2}(%&#10;t_{i})-R(t_{i}))^{2}}{\\sigma_{2}^{2}}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msubsup><mi>\u03c3</mi><mn>1</mn><mn>2</mn></msubsup></mfrac></mrow><mo>+</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>y</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msubsup><mi>\u03c3</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nsatisfies condition \\ref{ehstar}.\nTherefore, to approximate the\nintegral in (\\ref{fhintV}) we\ndefine the covariates $z({\\bf t})=\\{z(t_0),z(t_1),\\cdots,z(t_n)\\}$ using the trapezoid rule for integral approximation:\n$z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = &  \\sum_{j=1}^{i} \\frac{ \\left(h_{1,1}^{\\star}\\left[y_1(t_j),y_2(t_j)\\right] +  h_{1,1}^{\\star}\\left[y_1(t_{j-1}),y_2(t_{j-1})\\right] \\right) *(t_j-t_{j-1})}{2}\n\\end{eqnarray*}\nFinally, since  the initial value, $v_0$, will be estimated, the response\nfor the regression model for the BCLS method estimates of $v_0$ and $C$ is given by $y'({\\bf t})=y_1({\\bf t})$.\nTo obtain the integrated-data estimates of  $v_0$ and $C$ we\nfit a linear regression model with intercept based on\n\\begin{eqnarray}\ny'({\\bf t}) \\sim v_0\\ {\\bf 1}+ C\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}}_1,\n\\quad  {\\mbox{\\boldmath ${\\epsilon}$}}_1 \\sim i.i.d. \\;  N(0,\\sigma_1^2 ).\n\\label{FN1regmod}\n\\end{eqnarray}\nThe estimated coefficient of $z({\\bf t})$ will provide an unbiased estimate of $C$, denoted by ${\\hat C}$ and the e\nstimated intercept will provide an unbiased estimated of $v_0$. When $h_{1,1}$ is used to create the\ncovariates, rather than $h^{\\star}_{1,1}$, biased estimates of $C$ and $v_0$ are expected.\n\nUsing the estimated value ${\\hat C}$ the BCLS method estimates of $a$, $b$ and $r_0$  are obtained\nby first transforming (\\ref{fhR}) to the integral equation\n\\begin{eqnarray}\n{\\hat C}\\ R(t) + \\int_0^{t} V(\\tau) d\\tau &=& r_0 + a {\\bf t} - b \\int_0^{t} R(\\tau) d\\tau \\label{fhintR}.\n\\end{eqnarray}\nIn this case integral approximations using the observed data are required to define both the response and a covariate\nfor the linear regression model that will be used to estimate $a$, $b$ and $r_0$.\nThe response for the linear regression model approximates the left-hand side of (\\ref{fhintR})\nand is denoted by $y'({\\bf t}) =  \\{ y'(t_0),y'(t_1),\\cdots,y'(t_n)\\} $ where $y'(t_i)$ is defined as\n$y'(t_0) = {\\hat C} y_2(t_0)$ and for $i=1,\\cdots,n$:\n\n", "itemtype": "equation", "pos": 39689, "prevtext": "\nThe BCLS estimation is conducted in two steps. First,the parameter $C$  and initial condition $v_0$ are estimated using\ndata $y_1({\\bf t})$ a regression model based on (\\ref{fhV}).  Next the parameters $a$ and $b$ and initial condition $r_0$ are estimated using data $y_2({\\bf t})$, a regression model based on (\\ref{fhR}), and the estimate of $C$ obtained in the first step.\n\nTo estimate $C$ and $v_0$, equation (\\ref{fhV}) is re-written as the integral equation\n\\begin{eqnarray}\nV(t)  & = &  C \\int_0^{t} h_{1,1} (V,R) d\\tau  + v_0 \\label {fhintV}\n\\end{eqnarray}\nwhere $h_{1,1}(V,R) = V - \\frac{V^3}{3} + R$. Since $E[Y_1^3] \\neq E[Y_1]^3$ a bias correction function\nmust be identified.  Calculating $E[Y_1-\\frac{Y_1^3}{3}+Y_2] = V + (\\frac{V^3}{3} + 3 V \\sigma_1^2) + R$,\nit is clear that\n", "index": 15, "text": "\n\\[h_{1,1}^{\\star}(V,R) = V - \\frac{V^3}{3} + R -  3 V \\sigma_1^2\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"h_{1,1}^{\\star}(V,R)=V-\\frac{V^{3}}{3}+R-3V\\sigma_{1}^{2}\" display=\"block\"><mrow><mrow><msubsup><mi>h</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow><mo>\u22c6</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>V</mi><mo>-</mo><mfrac><msup><mi>V</mi><mn>3</mn></msup><mn>3</mn></mfrac></mrow><mo>+</mo><mi>R</mi></mrow><mo>-</mo><mrow><mn>3</mn><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nThe covariates for\nthe linear regression model are ${\\bf t}$ and $z({\\bf t})$ where $z({\\bf t})$ approximates $\\int_0^{t} R(\\tau) d\\tau$ and\nis calculated using the observed data, $y_2({\\bf t})$, and the trapezoid rule.  There is no need for bias\ncorrection since $E[Y_2] = R$.  The covariates are defined as $z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = &  \\sum_{j=1}^{i} \\frac{ \\left[ y_2(t_j) +  y_2(t_{j-1}) \\right] *(t_j-t_{j-1})}{2}\n\\end{eqnarray*}\n\nTo obtain estimates of $a$, $b$ and $r_0$ the following linear regression model with intercept is used.\n\\begin{eqnarray*}\ny'({\\bf t}) \\sim r_0\\ {\\bf 1} + a\\ {\\bf t}- b\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\qquad  {\\mbox{\\boldmath ${\\epsilon}$}} \\sim i.i.d. \\;  N(0,\\sigma^2 ).\n\\end{eqnarray*}\nThe estimated intercept provides an estimate of $r_0$, the estimated coefficient of ${\\bf t}$ provides an estimate of $a$\nand the estimated coefficient of $z({\\bf t})$ provides an estimate of $b$.\n\nA similar two-stage approach was used to obtain estimates of $a$, $b$, and $C$ using the PsLS method. Since this method\nrelies on derivatives rather than integrals, estimation of the intial conditions $v0$ and $r_0$ is not possible.\nThe PsLS method estimates using local polynomial smoothing with bandwidth of order $\\log(nt)^{-1/2}*nt^{-2/7}$ (0.20).\nA variety of other bandwidths were considered before choosing the one which resulted in the smallest overall bias in parameter estimates.\nSimulations were also conducted using PsLS with a smoothing bandwidth of order $\\log(nt)^{-1/10}*nt^{-2/7}$ (0.37)\nto evaluate sensitivity of this method to choice of bandwidth.\n\nThe bias from each method as a function of measurement error is shown in Figure 4.\nTable 3 provides the Monte Carlo means and standard errors for LS, BCLS, and PsLS estimated parameters based on simulations\nequal values for $\\sigma_1 = \\sigma_2 = 0.05$. The simulation results\nwith all combinations of $\\sigma_1$ and $\\sigma_2$ provided similar results and are not shown. Nonlinear least squares estimates\nwere unbiased with the lowest Monte Carlo standard errors (results not shown). However, NLS failed to converge in 7\\% of the simulations\nwith $a=0.34$ and $b=0.2$ and in 9\\% of the simulation with  $a=0.58$ and $b=0.58$ when the true values were used as initial conditions\nfor the NLS algorithm.\nAs expected, bias was detected in the LS estimated parameters when bias correction was not used.\nMost affected were the estimates of $C$ , $v_0$, and $r_0$.  Using the\nBCLS method resulted in elimination of most of the bias observed in the unadjusted LS estimated parameters.  The PsLS method\nwith a bandwidth of 0.2 produced less biased and more accurate estimates of the parameter $C$, but slightly more biased and less accurate\nestimates of the parameters $a$ and $b$.  When bandwidth of 0.37 was used, the PsLS method produced estimates with more bias than the\nBCLS method.\n\nSince choice of starting values can influence NLS parameter estimation, additional analysis of simulated data\nwere performed with the incorrect value of the parameters used for starting estimates in the NLS algorithm.\nFor these analysis, data\nfrom the single compartment for recovery of outward current, $Y_2({\\bf t})$, were used as outcomes in the nonlinear least square regression analysis\nand only the parameters $a$ and $b$ estimated.\nThe results of these analysis for simulated data with 201 equally spaced time points and residual standard error, $\\sigma_2= 0.05$\nare shown in Table 4 for the model with $a=0.34$ and $b=0.2$ (first 3 columns) and for the model with $a=0.58$ and $b=0.58$ (last 3 columns).\nThe values $a=0.4,0.8$, and $1.2$ and $b=0.4$ and $0.8$\nwere used as starting values for the NLS algorithm. The BCLS estimates of $a$ and $b$ were also used as starting values\nfor the NLS algorithm.\n\nIn most cases, when the incorrect values of the parameters were used as starting values for NLS regression, the algorithm\nfailed to converge, and when convergence occurred, the estimated values of the parameters were incorrect.  For example,\nwith the true value of $a = 0.34$ and $b=0.2$ if starting values of $a=0.8$ or $a=1.2$ is used (with either $b=0.4$ or $b=0.8$)\nthe algorithm converged for approximately 35\\% of the simulation replicates.  When the algorithm did converge, the Monte Carlo\naverage of the estimate of $a$ is approximately $1.72$ and the average estimate of $b$ is approximately $2.77$.  These values\ncorrespond the local minimum of the least squares surface shown in Figure 3 Panel E. Similarly, when the true value of $a$ and $b$\nare both $0.58$, for most of the incorrect starting estimates, the NLS algorithm converged less for less than 35\\% of the simulations\nThe Monte Carlo average of the estimate of $a$ when the NLS algorithm did converge to the wrong value is approximately $1.15$ and the average estimate of\n$b$ is approximately $2.02$ which corresponds to the local minimum in the least squares surface for those parameters\n(Figure 3 Panel B). When BCLS method was used to obtain starting values for the NLS regression algorithm, unbiased estimates of both $a$ and $b$\nare obtained, and the NLS algorithm converged for more than 90\\% of the simulation replicates.\n\n\n\\section{Discussion}\n\nThe bias-corrected least squares method is a computationally simple, non-iterative, and easily implemented\nmethod for estimation of parameters in ODE models.  It's development and the associated proof of\nconsistency of the resulting parameter estimates illustrates that direct methods (non-iterative) can be modified to\nso that the resulting estimator has desirable statistical properties.  The BCLS method retains the simplicity of early direct\nmethods and has most of the desirable statistical properties of more recently\ndeveloped direct methods. A key advantage of the BCLS method (and all direct methods) is that it\ndoes not require starting values for the estimation algorithm; in fact, it can provide starting values for\nnonlinear least squares regression if desired. Furthermore, it does not require choice of smoothing bandwidth of functional data analysis\nmethods.  Since the BCLS method is based on\nintegration rather than differentiation it can be used to estimate initial states of the ODE model\nstates in addition to ODE model parameters. This is not true of direct methods which rely on differentiation such as PsLS.\n\nThe analysis and simulations described in Section 3 based on the population growth of paramecium aurelium\nshow that the BCLS method performs comparably with the NLS method.  In the simulation\nstudies conducted using the logistic growth equation, the BCLS method produces estimates that are often less biased\nthan the recently developed PsLS method, a direct method which uses differentiation and smoothing instead of integration.\nThe bias observed in the PsLS method is likely a result of the choice of bandwidth in the smoothing step;\nthe use of integration in the BCLS method alleviates the need for any smoothing of the data since integration\nitself is a smoothing procedure. However, the bias due to integral approximations required by the BCLS can be expected when the\ndata are not densely sampled. The simulation studies conducted with the Fitzhugh Nagamu equations in Section 4\nprovide further evidence that the BCLS method\nis a viable alternative to NLS or PsLS regression.  Those simulations demonstrate that\nthe use of the BCLS method to obtain starting values for NLS regression can reduce problems\nwith convergence and accuracy associated with the choice of starting values in the NLS method,\nespecially in settings with an exceptionally complex likelihood surface or cost function with\nmultiple minima.  These simulations also show that while the PsLS method has the capacity to produce less biased\nand more accurate estimates for some parameters, these estimates rely heavily on the choice of bandwidth.\nFurthermore, since BCLS is based on integration and PsLS is based on differentiation, the BCLS method is\nable to provide estimates of the initial states of a system of ODE's. This not possible with the PsLS method.\nWhile both the BCLS method and the PsLS method are asymptotically unbiased, the simulations in this paper suggest that\nthe BCLS method has better small sample properties if an optimal bandwidth is not used with PsLS. Furthermore, simulations\nsuggest that a bandwidth which produces the least bias in one estimated parameter may not be the optimal choice for all\nparameters in the system.\n\nThere are a number of drawbacks to the BCLS method.  In order to obtain unbiased\nparameter estimates a separate analysis may be needed to obtain estimates of residual\nerrors to be used in the bias correction functions.  Although the linear regression\nalgorithm is used to obtain point estimates for the parameters, this approach is not\nlikelihood based so regression techniques for conducting inference or obtaining\nconfidence intervals cannot be used directly. However, bootstrapping techniques are\navailable to obtain confidence intervals for the estimated parameters. Another drawback\nto the BCLS method is the assumption that parameters must appear linearly in the ODE\nsystem (A.1).  This excludes estimation of certain parameters in models such as the\nMichaelis-Menten model for enzyme kinetics or saturating growth models for population\ndynamics.  Nonlinear estimation techniques could be used when parameters appear\nnonlinearly in the equations motivated by Eq (\\ref{ey}), and may offer improvements over\ndirectly solving the ODE and applying nonlinear estimation techniques to the resulting\nsolution.  This is an approach that was not considered in this work.  However, when known\nparameter appears nonlinearly in the system of ODEs it can be used to create the\ncovariates (or response) for the bias-corrected least squares method as demonstrated in\nthe estimation of $a$ and $b$ in the Fitzhugh-Nagumo model in Section 4.\n\n\nIn conclusion, the bias-corrected least squares method offers a relatively straightforward\nand easily implemented consistent estimation technique for parameters and initial conditions in\nmodels with means defined by ordinary differential equations. A variety of extensions of the methods\nare being evaluated including relaxing the assumptions that all states be observed at the same time points\nas well as evaluating the statistical properties of other estimation procedures which improve on the original direct methods\nproposed by Himmelbau, Jones and Bischoff (HJB). The identification of the bias corrections function in this work\nsuggests that there may be relatively straightforward modifications that can be made to a variety of direct methods so that\nthe resulting estimates have desirable statistical properties. Simple modifications and the study of associated statistical\nproperties of direct methods similar to the HJB methods is a promising area that is likely to provide new tools to investigators\nworking with data generated by mechanisms that can be described with ordinary differential equations.\n\n\n\n\n\n\n\\section{Appendix}\n\n\n\\subsection*{Proof of Theorem 1}\n\nRecall that upper case notation, $Y$ and $Z$\nindicates that we are working with random variables rather than observed data,\nwhich we represent with lower case variables.\nThe bias-corrected least squares method (BCLS) is appropriate for random variables\n${\\bf Y}({\\bf t})=(Y_1({\\bf t}),\\cdots,Y_m({\\bf t}))$ which are\nobserved at times ${\\bf t}=(t_0,\\cdots,t_n)$ and have time varying expected\nvalues given by the compartments (states) of a system of differential equations:\n  \\begin{eqnarray}\n  \\frac{d\\mu_q}{dt}=\\sum_{k=1}^{m_q} \\beta_{q,k} h_{q,k}({\\mbox{\\boldmath ${\\mu}$}}), \\ \\\n  \\mu_q(t_0)=\\mu_{q,0}, \\ \\ q=1,\\cdots,s \\ \\\n  \\end{eqnarray}\nSpecifically, the expectation, $E\\{{\\bf Y}({\\bf t})\\}$ is equal to $ {\\mbox{\\boldmath ${\\mu}$}}({\\bf t})$\nand the conditions (A.1)-(A.3) listed in Section 2 are satisfied. To simplify notation\nnotation, we assume that the interval lengths defined by the observation\ntimes, $(t_{i+1}-t_i)$ are equal to $\\frac{1}{n}$. The\nresults hold for any choice of sampling times where the maximum\ninterval length defined by the sampling times is $O(n^{-1})$.\n\nAs described in Section 2, we define covariates  $Z_{q,k}({\\bf t})$, $q=1,\\cdots s$, $k=1\\cdots m_q$,\nusing the left endpoint integral approximation as:\n\\begin{eqnarray*}\nZ_{q,k}(t_0) & = & 0 \\\\\nZ_{q,k}(t_i) & = & \\sum_{j < i} h_{q,k}^{\\star}\\{ {\\bf Y}(t_j) \\} \\frac{1}{n}, \\;\\; i=1,\\cdots, n\n\\label{covdefn}\n\\end{eqnarray*}\nwhere the functions $ h_{q,k}^{\\star} $ are described in (\\ref{ehstar}). Note that as functions of the\nrandom variables ${\\bf Y}(t_j)$ these covariates are also random variables.\n\nTo further ease of notation, we now drop the subscript $q$ and describe results for a single regression analysis\nusing data, $\\{y_1,\\cdots,y_n\\}$, with expectation from an ODE with a single compartment.\nThe results are easily extended\nfor data with expectations from a system of ODE's with more than one compartment.\nIf ${\\bf z }({\\bf t})$ is the covariate matrix with columns $z_{k}({\\bf t}), \\ k=1,\\cdots,m$,\nthen the parameters ${\\mbox{\\boldmath ${\\beta}$}} = \\{\\beta_1,\\cdots,\\beta_m\\}$ are estimated using linear regression so that\n", "itemtype": "equation", "pos": 41777, "prevtext": "\nsatisfies condition \\ref{ehstar}.\nTherefore, to approximate the\nintegral in (\\ref{fhintV}) we\ndefine the covariates $z({\\bf t})=\\{z(t_0),z(t_1),\\cdots,z(t_n)\\}$ using the trapezoid rule for integral approximation:\n$z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = &  \\sum_{j=1}^{i} \\frac{ \\left(h_{1,1}^{\\star}\\left[y_1(t_j),y_2(t_j)\\right] +  h_{1,1}^{\\star}\\left[y_1(t_{j-1}),y_2(t_{j-1})\\right] \\right) *(t_j-t_{j-1})}{2}\n\\end{eqnarray*}\nFinally, since  the initial value, $v_0$, will be estimated, the response\nfor the regression model for the BCLS method estimates of $v_0$ and $C$ is given by $y'({\\bf t})=y_1({\\bf t})$.\nTo obtain the integrated-data estimates of  $v_0$ and $C$ we\nfit a linear regression model with intercept based on\n\\begin{eqnarray}\ny'({\\bf t}) \\sim v_0\\ {\\bf 1}+ C\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}}_1,\n\\quad  {\\mbox{\\boldmath ${\\epsilon}$}}_1 \\sim i.i.d. \\;  N(0,\\sigma_1^2 ).\n\\label{FN1regmod}\n\\end{eqnarray}\nThe estimated coefficient of $z({\\bf t})$ will provide an unbiased estimate of $C$, denoted by ${\\hat C}$ and the e\nstimated intercept will provide an unbiased estimated of $v_0$. When $h_{1,1}$ is used to create the\ncovariates, rather than $h^{\\star}_{1,1}$, biased estimates of $C$ and $v_0$ are expected.\n\nUsing the estimated value ${\\hat C}$ the BCLS method estimates of $a$, $b$ and $r_0$  are obtained\nby first transforming (\\ref{fhR}) to the integral equation\n\\begin{eqnarray}\n{\\hat C}\\ R(t) + \\int_0^{t} V(\\tau) d\\tau &=& r_0 + a {\\bf t} - b \\int_0^{t} R(\\tau) d\\tau \\label{fhintR}.\n\\end{eqnarray}\nIn this case integral approximations using the observed data are required to define both the response and a covariate\nfor the linear regression model that will be used to estimate $a$, $b$ and $r_0$.\nThe response for the linear regression model approximates the left-hand side of (\\ref{fhintR})\nand is denoted by $y'({\\bf t}) =  \\{ y'(t_0),y'(t_1),\\cdots,y'(t_n)\\} $ where $y'(t_i)$ is defined as\n$y'(t_0) = {\\hat C} y_2(t_0)$ and for $i=1,\\cdots,n$:\n\n", "index": 17, "text": "$$\ny'(t_i) =  {\\hat C}\\  y_2(t_i)) +  \\sum_{j=1}^{i} \\frac{ \\left[ y_1(t_j) +  y_1(t_{j-1}) \\right] *(t_j-t_{j-1})}{2}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"y^{\\prime}(t_{i})={\\hat{C}}\\ y_{2}(t_{i}))+\\sum_{j=1}^{i}\\frac{\\left[y_{1}(t_{%&#10;j})+y_{1}(t_{j-1})\\right]*(t_{j}-t_{j-1})}{2}.\" display=\"block\"><mrow><msup><mi>y</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mpadded width=\"+5pt\"><mover accent=\"true\"><mi>C</mi><mo stretchy=\"false\">^</mo></mover></mpadded><msub><mi>y</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo><mo>+</mo><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msub><mi/><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></msub><msup><mi/><mi>i</mi></msup><mfrac><mrow><mrow><mo>[</mo><mrow><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo>*</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>-</mo><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mn>2</mn></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nUsing results from \\cite{Cro:86} consistency of ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}$ is achieved by showing\nthat if  ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n$ is the unique root of the estimating function  $\\displaystyle {\\overline{U}}_{n}({\\mbox{\\boldmath ${\\beta}$}})=\\frac{1}{n}\n\\sum_{i=1}^n {\\mbox{\\boldmath ${z}$}}_i^T  \\left\\{ {\\mbox{\\boldmath ${y}$}}_i - {\\mbox{\\boldmath ${z}$}}_i  {\\mbox{\\boldmath ${\\beta}$}} \\right\\}=0$ and Conditions (A1) - (A3) are satisfied then:\n\\begin{itemize}\n\\item[I]  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero.\n\\item[II] $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and\nnon-zero.\n\\item[III]  $ \\mbox{var}\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i\\} \\rightarrow 0$  as $n \\rightarrow \\infty$.\n\\end{itemize}\n\n\n\\subsection*{I\\ \\  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero}\n\nTo establish that  ${\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}})$ is a consistent estimator of zero  we need to show that,\n\\begin{eqnarray*}\n\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ] & = & 0 \\\\\n\\lim_{n\\rightarrow\\infty} \\mbox{var} [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ] & = &  0\n\\end{eqnarray*}\nWe consider the situation where ${ Z }({\\bf t})=Z({\\bf t)}$ is a column vector, i.e., there\nis a single covariate for the regression problem so that ${\\mbox{\\boldmath ${\\beta}$}}$ is a scalar, $\\beta$ Since\n$Y(t_1),\\cdots,Y(t_{i-1}),Y(t_i)$ are independent and the covariates $Z(t_i)$ are based\nonly first $i-1$ of these random variables (using the left endpoint approximation for\nintegrals), it follows that $Z(t_i)$ and $Y(t_i)$ are independent so that $E\\{ Z(t_i)\nY(t_i) \\} = E\\{Z(t_i)\\} ; E\\{Y(t_i)\\}$\n\nTo further simplify the notation define:\n$Z(t_i)  =  Z_i ,\\\nY(t_i)  =  Y_i ,\\\nE\\{ Z(t_i) \\}  =  M_i ,\\\nE\\{ Y(t_i) \\}  =  \\mu_i ,\\\nU_i  =  Z_i ( Y_i - Z_i \\beta ) ,\\\nZ_{i \\backslash j}  =  Z_i-Z_j = \\frac{1}{n} \\displaystyle{\\sum_{j\\leq l < i}} h^{\\star}(Y_l).\n$\nTo study the limit properties of ${\\overline{U}}_n(\\beta)$ the following results will be useful:\n\\begin{eqnarray}\nE(Z_i)  &  = & O(n^{-1}) \\; O(i)  \\label{r0} \\\\\n\\mbox{var}(Z_i) & =  & O(n^{-2}) \\; O(i)  \\label{r1} \\\\\n\\mbox{cov}(Z_i,Z_i^2) & = & O(n^{-3}) \\; O(i^2) \\label{r2}  \\\\\n\\mbox{var}(Z_i^2) & = & O(n^{-4}) \\; O(i^3)  \\label{r3} \\\\\n\\mbox{for } j<i,\\ \\mbox{cov}(Z_j^2,Z_i^2) & = & O(n^{-3})\\ O(j^2) + O(n^{-4})\\ O(j^3) \\label{r4}\n\\end{eqnarray}\n\n\nIt is straightforward to see that $E(Z_i)$ and $\\mbox{var}( Z_i )$ are\nbounded. This follows from (A.3) and\n", "itemtype": "equation", "pos": 55156, "prevtext": "\nThe covariates for\nthe linear regression model are ${\\bf t}$ and $z({\\bf t})$ where $z({\\bf t})$ approximates $\\int_0^{t} R(\\tau) d\\tau$ and\nis calculated using the observed data, $y_2({\\bf t})$, and the trapezoid rule.  There is no need for bias\ncorrection since $E[Y_2] = R$.  The covariates are defined as $z(t_0)=0$ and for $i=1,\\cdots,n$\n\\begin{eqnarray*}\n z(t_i) & = &  \\sum_{j=1}^{i} \\frac{ \\left[ y_2(t_j) +  y_2(t_{j-1}) \\right] *(t_j-t_{j-1})}{2}\n\\end{eqnarray*}\n\nTo obtain estimates of $a$, $b$ and $r_0$ the following linear regression model with intercept is used.\n\\begin{eqnarray*}\ny'({\\bf t}) \\sim r_0\\ {\\bf 1} + a\\ {\\bf t}- b\\ z({\\bf t}) + {\\mbox{\\boldmath ${\\epsilon}$}},\n\\qquad  {\\mbox{\\boldmath ${\\epsilon}$}} \\sim i.i.d. \\;  N(0,\\sigma^2 ).\n\\end{eqnarray*}\nThe estimated intercept provides an estimate of $r_0$, the estimated coefficient of ${\\bf t}$ provides an estimate of $a$\nand the estimated coefficient of $z({\\bf t})$ provides an estimate of $b$.\n\nA similar two-stage approach was used to obtain estimates of $a$, $b$, and $C$ using the PsLS method. Since this method\nrelies on derivatives rather than integrals, estimation of the intial conditions $v0$ and $r_0$ is not possible.\nThe PsLS method estimates using local polynomial smoothing with bandwidth of order $\\log(nt)^{-1/2}*nt^{-2/7}$ (0.20).\nA variety of other bandwidths were considered before choosing the one which resulted in the smallest overall bias in parameter estimates.\nSimulations were also conducted using PsLS with a smoothing bandwidth of order $\\log(nt)^{-1/10}*nt^{-2/7}$ (0.37)\nto evaluate sensitivity of this method to choice of bandwidth.\n\nThe bias from each method as a function of measurement error is shown in Figure 4.\nTable 3 provides the Monte Carlo means and standard errors for LS, BCLS, and PsLS estimated parameters based on simulations\nequal values for $\\sigma_1 = \\sigma_2 = 0.05$. The simulation results\nwith all combinations of $\\sigma_1$ and $\\sigma_2$ provided similar results and are not shown. Nonlinear least squares estimates\nwere unbiased with the lowest Monte Carlo standard errors (results not shown). However, NLS failed to converge in 7\\% of the simulations\nwith $a=0.34$ and $b=0.2$ and in 9\\% of the simulation with  $a=0.58$ and $b=0.58$ when the true values were used as initial conditions\nfor the NLS algorithm.\nAs expected, bias was detected in the LS estimated parameters when bias correction was not used.\nMost affected were the estimates of $C$ , $v_0$, and $r_0$.  Using the\nBCLS method resulted in elimination of most of the bias observed in the unadjusted LS estimated parameters.  The PsLS method\nwith a bandwidth of 0.2 produced less biased and more accurate estimates of the parameter $C$, but slightly more biased and less accurate\nestimates of the parameters $a$ and $b$.  When bandwidth of 0.37 was used, the PsLS method produced estimates with more bias than the\nBCLS method.\n\nSince choice of starting values can influence NLS parameter estimation, additional analysis of simulated data\nwere performed with the incorrect value of the parameters used for starting estimates in the NLS algorithm.\nFor these analysis, data\nfrom the single compartment for recovery of outward current, $Y_2({\\bf t})$, were used as outcomes in the nonlinear least square regression analysis\nand only the parameters $a$ and $b$ estimated.\nThe results of these analysis for simulated data with 201 equally spaced time points and residual standard error, $\\sigma_2= 0.05$\nare shown in Table 4 for the model with $a=0.34$ and $b=0.2$ (first 3 columns) and for the model with $a=0.58$ and $b=0.58$ (last 3 columns).\nThe values $a=0.4,0.8$, and $1.2$ and $b=0.4$ and $0.8$\nwere used as starting values for the NLS algorithm. The BCLS estimates of $a$ and $b$ were also used as starting values\nfor the NLS algorithm.\n\nIn most cases, when the incorrect values of the parameters were used as starting values for NLS regression, the algorithm\nfailed to converge, and when convergence occurred, the estimated values of the parameters were incorrect.  For example,\nwith the true value of $a = 0.34$ and $b=0.2$ if starting values of $a=0.8$ or $a=1.2$ is used (with either $b=0.4$ or $b=0.8$)\nthe algorithm converged for approximately 35\\% of the simulation replicates.  When the algorithm did converge, the Monte Carlo\naverage of the estimate of $a$ is approximately $1.72$ and the average estimate of $b$ is approximately $2.77$.  These values\ncorrespond the local minimum of the least squares surface shown in Figure 3 Panel E. Similarly, when the true value of $a$ and $b$\nare both $0.58$, for most of the incorrect starting estimates, the NLS algorithm converged less for less than 35\\% of the simulations\nThe Monte Carlo average of the estimate of $a$ when the NLS algorithm did converge to the wrong value is approximately $1.15$ and the average estimate of\n$b$ is approximately $2.02$ which corresponds to the local minimum in the least squares surface for those parameters\n(Figure 3 Panel B). When BCLS method was used to obtain starting values for the NLS regression algorithm, unbiased estimates of both $a$ and $b$\nare obtained, and the NLS algorithm converged for more than 90\\% of the simulation replicates.\n\n\n\\section{Discussion}\n\nThe bias-corrected least squares method is a computationally simple, non-iterative, and easily implemented\nmethod for estimation of parameters in ODE models.  It's development and the associated proof of\nconsistency of the resulting parameter estimates illustrates that direct methods (non-iterative) can be modified to\nso that the resulting estimator has desirable statistical properties.  The BCLS method retains the simplicity of early direct\nmethods and has most of the desirable statistical properties of more recently\ndeveloped direct methods. A key advantage of the BCLS method (and all direct methods) is that it\ndoes not require starting values for the estimation algorithm; in fact, it can provide starting values for\nnonlinear least squares regression if desired. Furthermore, it does not require choice of smoothing bandwidth of functional data analysis\nmethods.  Since the BCLS method is based on\nintegration rather than differentiation it can be used to estimate initial states of the ODE model\nstates in addition to ODE model parameters. This is not true of direct methods which rely on differentiation such as PsLS.\n\nThe analysis and simulations described in Section 3 based on the population growth of paramecium aurelium\nshow that the BCLS method performs comparably with the NLS method.  In the simulation\nstudies conducted using the logistic growth equation, the BCLS method produces estimates that are often less biased\nthan the recently developed PsLS method, a direct method which uses differentiation and smoothing instead of integration.\nThe bias observed in the PsLS method is likely a result of the choice of bandwidth in the smoothing step;\nthe use of integration in the BCLS method alleviates the need for any smoothing of the data since integration\nitself is a smoothing procedure. However, the bias due to integral approximations required by the BCLS can be expected when the\ndata are not densely sampled. The simulation studies conducted with the Fitzhugh Nagamu equations in Section 4\nprovide further evidence that the BCLS method\nis a viable alternative to NLS or PsLS regression.  Those simulations demonstrate that\nthe use of the BCLS method to obtain starting values for NLS regression can reduce problems\nwith convergence and accuracy associated with the choice of starting values in the NLS method,\nespecially in settings with an exceptionally complex likelihood surface or cost function with\nmultiple minima.  These simulations also show that while the PsLS method has the capacity to produce less biased\nand more accurate estimates for some parameters, these estimates rely heavily on the choice of bandwidth.\nFurthermore, since BCLS is based on integration and PsLS is based on differentiation, the BCLS method is\nable to provide estimates of the initial states of a system of ODE's. This not possible with the PsLS method.\nWhile both the BCLS method and the PsLS method are asymptotically unbiased, the simulations in this paper suggest that\nthe BCLS method has better small sample properties if an optimal bandwidth is not used with PsLS. Furthermore, simulations\nsuggest that a bandwidth which produces the least bias in one estimated parameter may not be the optimal choice for all\nparameters in the system.\n\nThere are a number of drawbacks to the BCLS method.  In order to obtain unbiased\nparameter estimates a separate analysis may be needed to obtain estimates of residual\nerrors to be used in the bias correction functions.  Although the linear regression\nalgorithm is used to obtain point estimates for the parameters, this approach is not\nlikelihood based so regression techniques for conducting inference or obtaining\nconfidence intervals cannot be used directly. However, bootstrapping techniques are\navailable to obtain confidence intervals for the estimated parameters. Another drawback\nto the BCLS method is the assumption that parameters must appear linearly in the ODE\nsystem (A.1).  This excludes estimation of certain parameters in models such as the\nMichaelis-Menten model for enzyme kinetics or saturating growth models for population\ndynamics.  Nonlinear estimation techniques could be used when parameters appear\nnonlinearly in the equations motivated by Eq (\\ref{ey}), and may offer improvements over\ndirectly solving the ODE and applying nonlinear estimation techniques to the resulting\nsolution.  This is an approach that was not considered in this work.  However, when known\nparameter appears nonlinearly in the system of ODEs it can be used to create the\ncovariates (or response) for the bias-corrected least squares method as demonstrated in\nthe estimation of $a$ and $b$ in the Fitzhugh-Nagumo model in Section 4.\n\n\nIn conclusion, the bias-corrected least squares method offers a relatively straightforward\nand easily implemented consistent estimation technique for parameters and initial conditions in\nmodels with means defined by ordinary differential equations. A variety of extensions of the methods\nare being evaluated including relaxing the assumptions that all states be observed at the same time points\nas well as evaluating the statistical properties of other estimation procedures which improve on the original direct methods\nproposed by Himmelbau, Jones and Bischoff (HJB). The identification of the bias corrections function in this work\nsuggests that there may be relatively straightforward modifications that can be made to a variety of direct methods so that\nthe resulting estimates have desirable statistical properties. Simple modifications and the study of associated statistical\nproperties of direct methods similar to the HJB methods is a promising area that is likely to provide new tools to investigators\nworking with data generated by mechanisms that can be described with ordinary differential equations.\n\n\n\n\n\n\n\\section{Appendix}\n\n\n\\subsection*{Proof of Theorem 1}\n\nRecall that upper case notation, $Y$ and $Z$\nindicates that we are working with random variables rather than observed data,\nwhich we represent with lower case variables.\nThe bias-corrected least squares method (BCLS) is appropriate for random variables\n${\\bf Y}({\\bf t})=(Y_1({\\bf t}),\\cdots,Y_m({\\bf t}))$ which are\nobserved at times ${\\bf t}=(t_0,\\cdots,t_n)$ and have time varying expected\nvalues given by the compartments (states) of a system of differential equations:\n  \\begin{eqnarray}\n  \\frac{d\\mu_q}{dt}=\\sum_{k=1}^{m_q} \\beta_{q,k} h_{q,k}({\\mbox{\\boldmath ${\\mu}$}}), \\ \\\n  \\mu_q(t_0)=\\mu_{q,0}, \\ \\ q=1,\\cdots,s \\ \\\n  \\end{eqnarray}\nSpecifically, the expectation, $E\\{{\\bf Y}({\\bf t})\\}$ is equal to $ {\\mbox{\\boldmath ${\\mu}$}}({\\bf t})$\nand the conditions (A.1)-(A.3) listed in Section 2 are satisfied. To simplify notation\nnotation, we assume that the interval lengths defined by the observation\ntimes, $(t_{i+1}-t_i)$ are equal to $\\frac{1}{n}$. The\nresults hold for any choice of sampling times where the maximum\ninterval length defined by the sampling times is $O(n^{-1})$.\n\nAs described in Section 2, we define covariates  $Z_{q,k}({\\bf t})$, $q=1,\\cdots s$, $k=1\\cdots m_q$,\nusing the left endpoint integral approximation as:\n\\begin{eqnarray*}\nZ_{q,k}(t_0) & = & 0 \\\\\nZ_{q,k}(t_i) & = & \\sum_{j < i} h_{q,k}^{\\star}\\{ {\\bf Y}(t_j) \\} \\frac{1}{n}, \\;\\; i=1,\\cdots, n\n\\label{covdefn}\n\\end{eqnarray*}\nwhere the functions $ h_{q,k}^{\\star} $ are described in (\\ref{ehstar}). Note that as functions of the\nrandom variables ${\\bf Y}(t_j)$ these covariates are also random variables.\n\nTo further ease of notation, we now drop the subscript $q$ and describe results for a single regression analysis\nusing data, $\\{y_1,\\cdots,y_n\\}$, with expectation from an ODE with a single compartment.\nThe results are easily extended\nfor data with expectations from a system of ODE's with more than one compartment.\nIf ${\\bf z }({\\bf t})$ is the covariate matrix with columns $z_{k}({\\bf t}), \\ k=1,\\cdots,m$,\nthen the parameters ${\\mbox{\\boldmath ${\\beta}$}} = \\{\\beta_1,\\cdots,\\beta_m\\}$ are estimated using linear regression so that\n", "index": 19, "text": "\n\\[\n{\\widehat{\\mbox{\\boldmath ${\\beta}$}}} =  ({\\bf z }({\\bf t})'{\\bf z }({\\bf t}))^{-1}{\\bf z}({\\bf t})'{\\bf z}({\\bf t})\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"{\\widehat{\\mbox{\\boldmath${\\beta}$}}}=({\\bf z}({\\bf t})^{\\prime}{\\bf z}({\\bf t%&#10;}))^{-1}{\\bf z}({\\bf t})^{\\prime}{\\bf z}({\\bf t})\" display=\"block\"><mrow><mover accent=\"true\"><mi>\ud835\udf37</mi><mo>^</mo></mover><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc33</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc33</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nand\n", "itemtype": "equation", "pos": 57974, "prevtext": "\nUsing results from \\cite{Cro:86} consistency of ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}$ is achieved by showing\nthat if  ${\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n$ is the unique root of the estimating function  $\\displaystyle {\\overline{U}}_{n}({\\mbox{\\boldmath ${\\beta}$}})=\\frac{1}{n}\n\\sum_{i=1}^n {\\mbox{\\boldmath ${z}$}}_i^T  \\left\\{ {\\mbox{\\boldmath ${y}$}}_i - {\\mbox{\\boldmath ${z}$}}_i  {\\mbox{\\boldmath ${\\beta}$}} \\right\\}=0$ and Conditions (A1) - (A3) are satisfied then:\n\\begin{itemize}\n\\item[I]  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero.\n\\item[II] $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and\nnon-zero.\n\\item[III]  $ \\mbox{var}\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i\\} \\rightarrow 0$  as $n \\rightarrow \\infty$.\n\\end{itemize}\n\n\n\\subsection*{I\\ \\  ${\\overline{U}}_{n}({\\widehat{\\mbox{\\boldmath ${\\beta}$}}}_n)$ is a consistent estimator of zero}\n\nTo establish that  ${\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}})$ is a consistent estimator of zero  we need to show that,\n\\begin{eqnarray*}\n\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ] & = & 0 \\\\\n\\lim_{n\\rightarrow\\infty} \\mbox{var} [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ] & = &  0\n\\end{eqnarray*}\nWe consider the situation where ${ Z }({\\bf t})=Z({\\bf t)}$ is a column vector, i.e., there\nis a single covariate for the regression problem so that ${\\mbox{\\boldmath ${\\beta}$}}$ is a scalar, $\\beta$ Since\n$Y(t_1),\\cdots,Y(t_{i-1}),Y(t_i)$ are independent and the covariates $Z(t_i)$ are based\nonly first $i-1$ of these random variables (using the left endpoint approximation for\nintegrals), it follows that $Z(t_i)$ and $Y(t_i)$ are independent so that $E\\{ Z(t_i)\nY(t_i) \\} = E\\{Z(t_i)\\} ; E\\{Y(t_i)\\}$\n\nTo further simplify the notation define:\n$Z(t_i)  =  Z_i ,\\\nY(t_i)  =  Y_i ,\\\nE\\{ Z(t_i) \\}  =  M_i ,\\\nE\\{ Y(t_i) \\}  =  \\mu_i ,\\\nU_i  =  Z_i ( Y_i - Z_i \\beta ) ,\\\nZ_{i \\backslash j}  =  Z_i-Z_j = \\frac{1}{n} \\displaystyle{\\sum_{j\\leq l < i}} h^{\\star}(Y_l).\n$\nTo study the limit properties of ${\\overline{U}}_n(\\beta)$ the following results will be useful:\n\\begin{eqnarray}\nE(Z_i)  &  = & O(n^{-1}) \\; O(i)  \\label{r0} \\\\\n\\mbox{var}(Z_i) & =  & O(n^{-2}) \\; O(i)  \\label{r1} \\\\\n\\mbox{cov}(Z_i,Z_i^2) & = & O(n^{-3}) \\; O(i^2) \\label{r2}  \\\\\n\\mbox{var}(Z_i^2) & = & O(n^{-4}) \\; O(i^3)  \\label{r3} \\\\\n\\mbox{for } j<i,\\ \\mbox{cov}(Z_j^2,Z_i^2) & = & O(n^{-3})\\ O(j^2) + O(n^{-4})\\ O(j^3) \\label{r4}\n\\end{eqnarray}\n\n\nIt is straightforward to see that $E(Z_i)$ and $\\mbox{var}( Z_i )$ are\nbounded. This follows from (A.3) and\n", "index": 21, "text": "\n\\[ E(Z_i)\\ =\\   \\frac{1}{n} \\displaystyle{\\sum_{j<i}} h^{\\star}(Y_j)\\  =\n\\  \\frac{1}{n} \\displaystyle{\\sum_{j<i}} h\\{\\mu(t_j)\\}\\  \\leq \\ \\frac{i}{n} B \\  \\leq\\   B \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"E(Z_{i})\\ =\\ \\frac{1}{n}\\displaystyle{\\sum_{j&lt;i}}h^{\\star}(Y_{j})\\ =\\ \\frac{1}%&#10;{n}\\displaystyle{\\sum_{j&lt;i}}h\\{\\mu(t_{j})\\}\\ \\leq\\ \\frac{i}{n}B\\ \\leq\\ B\" display=\"block\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mrow><msup><mi>h</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>j</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">\u2264</mo><mrow><mfrac><mi>i</mi><mi>n</mi></mfrac><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>B</mi></mpadded></mrow><mo rspace=\"7.5pt\">\u2264</mo><mi>B</mi></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\n\n\nTo show that $E\\{ {\\overline{U}}_n(\\beta) \\} \\rightarrow 0$ as $n \\rightarrow \\infty$ we calculate\n\\begin{eqnarray*}\nE\\{ {\\overline{U}}_n(\\beta) \\} & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i \\; Y_i) - E(Z_i^2)\\; \\beta \\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i) \\; E(Y_i) \\; - \\; E( Z_i)^2 \\; \\beta - \\;  \\mbox{var}( Z_i ) \\; \\beta\\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i) \\left[ \\beta \\int_{0}^{t_i}\n  h\\{\\mu(s)\\} ds -\\beta \\sum_{j<i} h\\{\\mu(t_j)\\} \\frac{1}{n} \\right] - \\;  \\mbox{var}( Z_i ) \\; \\beta \\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i)\\; \\sum_{j<i} \\beta \\left[  \\int_{t_{j-1}}^{t_{j}}\n  h\\{\\mu(s)\\} ds -  h\\{\\mu(t_j)\\} \\frac{1}{n} \\right] - \\; \\mbox{var}( Z_i ) \\;  \\beta \\\\\n  & = & E_n^{(1)}(\\beta) \\;\\; + \\;\\; E_n^{(2)}(\\beta ) \\\\\n\\mbox{where} & & \\\\\n E_n^{(1)}(\\beta) & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i)\\; \\beta\\;\n    \\sum_{j<i} \\left[ \\int_{t_j}^{t_{j+1}} h\\{ \\mu(s)\\}- h\\{\\mu(t_j)\\} ds \\right] \\\\\n E_n^{(2)}(\\beta) & = & \\frac{1}{n} \\sum_{i=1}^n \\mbox{var}( Z_i )\\; \\beta\n\\end{eqnarray*}\n\nLet $\\epsilon_n =\n \\mbox{max }_{i=1,\\cdots, n} \\left[\\  |\\ h\\{ \\mu(s)\\}-\n h\\{\\mu(t_i)\\}\\ |\\  :\\  s\\in [t_{i-1},i_j]\\  \\right\\}$.\nSince $h$ and $\\mu$ are continuous on $[0,t_n]$ it follows\nthat $\\epsilon_n \\rightarrow 0$ as $n\\rightarrow \\infty$ and\n\n", "itemtype": "equation", "pos": 58145, "prevtext": "\nand\n", "index": 23, "text": "\n\\[ \\mbox{var}( Z_i )\\  =\\  \\mbox{var} \\left\\{\n   \\frac{1}{n} \\sum_{j < i} h^{\\star}( Y_j ) \\right\\} \\\n   = \\  \\frac{1}{n^2} \\sum_{j < i} \\mbox{var}\\{ h^{\\star}( Y_j ) \\} \\\n   \\leq \\  \\frac{i}{n^2} B \\  \\leq \\   B \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\mbox{var}(Z_{i})\\ =\\ \\mbox{var}\\left\\{\\frac{1}{n}\\sum_{j&lt;i}h^{\\star}(Y_{j})%&#10;\\right\\}\\ =\\ \\frac{1}{n^{2}}\\sum_{j&lt;i}\\mbox{var}\\{h^{\\star}(Y_{j})\\}\\ \\leq\\ %&#10;\\frac{i}{n^{2}}B\\ \\leq\\ B\" display=\"block\"><mrow><mrow><mtext>var</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mtext>var</mtext><mo>\u2062</mo><mrow><mo>{</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mrow><msup><mi>h</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">}</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mfrac><mn>1</mn><msup><mi>n</mi><mn>2</mn></msup></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mrow><mtext>var</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>h</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">\u2264</mo><mrow><mfrac><mi>i</mi><msup><mi>n</mi><mn>2</mn></msup></mfrac><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>B</mi></mpadded></mrow><mo rspace=\"7.5pt\">\u2264</mo><mi>B</mi></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": " Since $E(Z_i)$ is bounded for all $i$,\nwe have,\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\nTo show that $E\\{ {\\overline{U}}_n(\\beta) \\} \\rightarrow 0$ as $n \\rightarrow \\infty$ we calculate\n\\begin{eqnarray*}\nE\\{ {\\overline{U}}_n(\\beta) \\} & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i \\; Y_i) - E(Z_i^2)\\; \\beta \\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i) \\; E(Y_i) \\; - \\; E( Z_i)^2 \\; \\beta - \\;  \\mbox{var}( Z_i ) \\; \\beta\\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i) \\left[ \\beta \\int_{0}^{t_i}\n  h\\{\\mu(s)\\} ds -\\beta \\sum_{j<i} h\\{\\mu(t_j)\\} \\frac{1}{n} \\right] - \\;  \\mbox{var}( Z_i ) \\; \\beta \\\\\n  & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i)\\; \\sum_{j<i} \\beta \\left[  \\int_{t_{j-1}}^{t_{j}}\n  h\\{\\mu(s)\\} ds -  h\\{\\mu(t_j)\\} \\frac{1}{n} \\right] - \\; \\mbox{var}( Z_i ) \\;  \\beta \\\\\n  & = & E_n^{(1)}(\\beta) \\;\\; + \\;\\; E_n^{(2)}(\\beta ) \\\\\n\\mbox{where} & & \\\\\n E_n^{(1)}(\\beta) & = & \\frac{1}{n} \\sum_{i=1}^n E(Z_i)\\; \\beta\\;\n    \\sum_{j<i} \\left[ \\int_{t_j}^{t_{j+1}} h\\{ \\mu(s)\\}- h\\{\\mu(t_j)\\} ds \\right] \\\\\n E_n^{(2)}(\\beta) & = & \\frac{1}{n} \\sum_{i=1}^n \\mbox{var}( Z_i )\\; \\beta\n\\end{eqnarray*}\n\nLet $\\epsilon_n =\n \\mbox{max }_{i=1,\\cdots, n} \\left[\\  |\\ h\\{ \\mu(s)\\}-\n h\\{\\mu(t_i)\\}\\ |\\  :\\  s\\in [t_{i-1},i_j]\\  \\right\\}$.\nSince $h$ and $\\mu$ are continuous on $[0,t_n]$ it follows\nthat $\\epsilon_n \\rightarrow 0$ as $n\\rightarrow \\infty$ and\n\n", "index": 25, "text": "$$\\int_{t_j}^{t_{j+1}} |\\ h\\{ \\mu(s)\\}- h\\{\\mu(t_j)\\}|\\  ds \\leq\n\\epsilon_n \\frac{1}{n}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\int_{t_{j}}^{t_{j+1}}|\\ h\\{\\mu(s)\\}-h\\{\\mu(t_{j})\\}|\\ ds\\leq\\epsilon_{n}\\frac%&#10;{1}{n}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mi>j</mi></msub><msub><mi>t</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></msubsup><mrow><mrow><mo rspace=\"7.5pt\" stretchy=\"false\">|</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>s</mi></mrow></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>\u03f5</mi><mi>n</mi></msub><mo>\u2062</mo><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nWe use variance bounds to show that $E_n^{(2)}(\\beta)\\rightarrow 0$ as $n\\rightarrow \\infty$.\n", "itemtype": "equation", "pos": 59752, "prevtext": " Since $E(Z_i)$ is bounded for all $i$,\nwe have,\n", "index": 27, "text": "\n\\[\nE_n^{(1)}(\\beta) \\   \\leq  \\   \\frac{B\\beta}{n} \\sum_{i=1}^n \\sum_{j<i}\\frac{\\epsilon_n}{n}\n \\  = \\   O(n^{-2})\\  \\epsilon_n\\  \\sum_{i=1}^n i\n \\  = \\   O(n^{-2})\\  O(n^2)\\  \\epsilon_n\n \\  \\rightarrow \\  0  \\mbox{ as }  n\\rightarrow \\infty\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"E_{n}^{(1)}(\\beta)\\ \\leq\\ \\frac{B\\beta}{n}\\sum_{i=1}^{n}\\sum_{j&lt;i}\\frac{%&#10;\\epsilon_{n}}{n}\\ =\\ O(n^{-2})\\ \\epsilon_{n}\\ \\sum_{i=1}^{n}i\\ =\\ O(n^{-2})\\ O%&#10;(n^{2})\\ \\epsilon_{n}\\ \\rightarrow\\ 0\\mbox{ as }n\\rightarrow\\infty\" display=\"block\"><mrow><mrow><msubsup><mi>E</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">\u2264</mo><mrow><mfrac><mrow><mi>B</mi><mo>\u2062</mo><mi>\u03b2</mi></mrow><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mpadded width=\"+5pt\"><mfrac><msub><mi>\u03f5</mi><mi>n</mi></msub><mi>n</mi></mfrac></mpadded></mrow></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+5pt\"><msub><mi>\u03f5</mi><mi>n</mi></msub></mpadded><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mpadded width=\"+5pt\"><mi>i</mi></mpadded></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+5pt\"><msub><mi>\u03f5</mi><mi>n</mi></msub></mpadded></mrow><mo>\u2192</mo><mrow><mn>\u20050</mn><mo>\u2062</mo><mtext>\u00a0as\u00a0</mtext><mo>\u2062</mo><mi>n</mi></mrow><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nThis completes the proof that $\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =  0$.\n\\vskip0.1in\n\nNext, we consider $\\mbox{var}\\{{\\overline{U}}_n(\\beta)\\}\\  =\\  \\frac{1}{n^2} \\left\\{ \\sum_{i=1}^n\\mbox{var}(U_i)\n + 2 \\sum_{j<i} \\mbox{cov}(U_j,U_i) \\right\\}$.\nFirst we evaluate $ \\mbox{var}(U_i) =  \\mbox{var}(  Z_i Y_i) + \\mbox{var}(  Z_i^2 \\beta)\n- 2 \\mbox{cov}(Z_i Y_i,Z_i^2 \\beta) $. Using equations (\\ref{r0})-(\\ref{r4})\nand independence\nof $Z_i$ and $Y_i$ we obtain\n\\begin{eqnarray*}\n \\mbox{var}(  Z_i\\ Y_i) & = & E[Z_i^2\\ Y_i^2] - E[Z_i\\ Y_i]^2 \\\\\n   & = &  E[Z_i^2]\\ E[Y_i^2] - E[Z_i]^2\\ E[Y_i]^2 \\\\\n   & = &  (\\mbox{var}(Z_i)-M_i^2)\\ (\\mbox{var}(Y_i) - \\mu_i^2) - M_i^2 \\mu_i^2 \\\\\n   & = &  \\{\\mbox{var}(Z_i) \\; \\mbox{var}(Y_i) \\} - \\mu_i^2 \\mbox{ var}\n   ( Z_i) - M_i^2 \\mbox{ var} (Y_i) \\\\\n   & = &  O(n^{-2})\\  O(i) - O(n^{-2})\\ O(i^2) \\\\\n\\mbox{var}( Z_i^2 \\beta) & = & O(n^{-4})\\  O(i^3) \\\\\n\\mbox{cov}( Z_i\\ Y_i \\; Z_i^2 \\beta) & = & \\beta\\ \\mu_i\n    \\mbox{ cov}(Z_i,Z_i^2) \\\\\n    & = & O(n^{-3}) \\; O(i^2)\n\\end{eqnarray*}\n\nThus, we have a contribution to $\\mbox{var}[{\\overline{U}}_n(\\beta)]$ from the\ndiagonal (variance) terms of:\n\\begin{eqnarray*}\n\\frac{1}{n^2} \\sum_{i=1}^n \\mbox{var}(U_i) & = & O(n^{-2})\\\n \\sum_{i=1}^n\\ \\left\\{  O(n^{-2})\\ O(i) + O(n^{-2})\\ O(i^2) +  O(n^{-4})\\ O(i^3) + O(n^{-3}) \\;\n O(i^2) \\right\\} \\\\\n & = &  O(n^{-2}) \\left\\{  O(n^{-2})\\  O(n^2) + O(n^{-2})\\ O(n^3)\\ +\\ O(n^{-4})\\  O(n^4) + O(n^{-3}) \\ O(n^3) \\right\\}\\\\\n & = &  O(n^{-2}) + O(n^{-1}) \\ \\rightarrow \\ 0 \\mbox{ as }\n n\\rightarrow \\infty\\end{eqnarray*}\n\nTo evaluate $ \\mbox{cov}(U_j,U_i)$ assuming $j<i$ consider:\n\\begin{eqnarray*}\n \\mbox{cov}(U_j,U_i) & = &  \\mbox{cov}( Z_j Y_j-Z_j^2\\beta,Z_iY_i - Z_i^2\\beta ) \\\\\n  & = &  \\mbox{cov}( Z_jY_j,  Z_iY_i  )   -\n   \\mbox{cov}( Z_jY_j,  Z_i^2\\beta  )  -\n   \\mbox{cov}(  Z_j^2\\beta,   Z_iY_i  )   +\n   \\mbox{cov}( Z_j^2\\beta,   Z_i^2\\beta  )\n\\end{eqnarray*}\n\nUsing equations (\\ref{r0})-(\\ref{r4}) it follows that:\n\\begin{eqnarray*}\n  \\mbox{cov}(Z_j Y_j, \\; Z_iY_i ) & = &  \\mbox{cov}(Z_j Y_j,(Z_{i\\backslash (j+1)} + Z_{j+1}) Y_i) \\\\\n                                  & = &  \\mbox{cov}(Z_jY_j,Z_{i\\backslash (j+1)} Y_i) + \\mbox{cov} (Z_j Y_j,Z_{j+1} Y_i) \\\\\n                                  & = &  0\\; + \\mbox{cov} (Z_j Y_j, [\\frac{1}{n}h^{\\star}(Y_j) + Z_j ] Y_i)  \\\\\n                                  & = &  \\frac{1}{n} \\{\\mbox{cov} (Z_j Y_j, h^{\\star}(Y_j) Y_i)\\} +\\mbox{cov}(Z_j Y_j,Z_j Y_i)\\\\\n                                  & = &  \\frac{1}{n} \\{ E[Z_j Y_j h^{\\star}(Y_j) Y_i] - M_j \\mu_j h^{\\star}(\\mu_j) \\mu_i  \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &  \\frac{1}{n} \\{ M_j \\mu_j E[Y_j h^{\\star}(Y_j)] - M_j \\mu_j h^{\\star}(\\mu_j) \\mu_i  \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &  \\frac{1}{n} \\{ M_j \\mu_j \\left(E[Y_j h^{\\star}(Y_j)] - h^{\\star}(\\mu_j) \\mu_i \\right) \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &   O(n^{-2}) \\;\\; O(j) \\\\\n\\end{eqnarray*}\nSimilar calculations show that\n\\begin{eqnarray*}\n\\mbox{cov}( \\; Z_j Y_j, \\; Z_i^2 \\beta \\; ) & = &  O(n^{-2}) \\;\\; O(j) + O(n^{-3}) \\;\\; O(j^2) \\\\\n\\mbox{cov}( \\; Z_j^2\\beta, \\; Z_iY_i \\; ) & = & O(n^{-3}) \\;\\; O(j^2) \\\\\n\\mbox{cov}( \\; Z_j^2 \\beta, \\; Z_i^2 \\beta \\; ) & = &  O(n^{-3}) \\;\\; O(j^2) + O(n^{-4}) \\;\\; O(j^3)\n\\end{eqnarray*}\n\nIt follows that the contribution to $\\mbox{var}\\{{\\overline{U}}_n(\\beta)\\}$ from the\noff-diagonal terms is:\n\\begin{eqnarray*}\n2 \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j<i} \\mbox{cov}(U_i,U_j)  & = &\n    O(n^{-2}) \\sum_{i=1}^n \\sum_{j<i} \\; O(n^{-2}) \\  O(j)  \\\\\n  &  &  \\; - \\  O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-2}) \\  O(j) + O(n^{-3}) \\  O(j^2) \\\\\n  &  & \\; - \\   O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-3}) \\  O(j^2) \\\\\n  &  &  \\; + \\  O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-3}) \\  O(j^2) + O(n^{-4}) \\  O(j^3) \\\\\n  & = & O(n^{-2}) \\; O(n^{-2}) \\  O(n^3) \\\\\n  &  &  - O(n^{-2}) \\ \\left\\{ O(n^{-2}) \\  O(n^3) + O(n^{-3}) \\\n    O(n^4) \\right\\} \\\\\n  &  &  - \\  O(n^{-2}) \\ \\left\\{ O(n^{-3}) \\  O(n^4) \\right\\} \\\\\n  &  &   + \\  O(n^{-2}) \\ \\left\\{ O(n^{-3}) \\  O(n^4) + O(n^{-4})\n    \\  O(n^5) \\right\\} \\\\\n  & = & O(n^{-1}) \\rightarrow 0 \\mbox{ as } n\\rightarrow \\infty\n\\end{eqnarray*}\n\nThis completes the proof that $\\lim_{n\\rightarrow\\infty} \\mbox{var} [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =   0 $.\nCombined with the previous proof that $\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =  0$ we\nhave established that ${\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}})$ is a consistent estimator of $0$.\n\n\\subsection*{II\\ \\  $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and non-zero}\n\nTo see that   $ E \\left(\\frac{1}{n} \\sum_{i=1}^n Z_i^T Z_i \\right)$ is bounded and non-zero\nwe calculate:\n\\begin{eqnarray*}\n E \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i^T Z_i \\right)  & = & \\frac{1}{n} \\sum_{i=1}^n \\{ \\mbox{var}(Z_i) + E(Z_i)^2\\}  \\\\\n   & = & O(n^{-1}) \\left\\{ \\sum_{i=1}^n O(n^{-2}) \\; O(i) + M_i^2 \\right\\} \\\\\n   & \\leq & O(n^{-1}) \\; \\{O(n^{-2}) O(n^2) + O(n) \\} \\\\\n   & = & O(1)\n\\end {eqnarray*}\nso that  $ E\\left\\{\\frac{1}{n} \\sum_{i=1}^n Z_i^2\\right\\}$ is bounded.  As long as\n$E(Z_i^2) > 0$ for at least one $i$ we have\n\n", "itemtype": "equation", "pos": 60091, "prevtext": "\nWe use variance bounds to show that $E_n^{(2)}(\\beta)\\rightarrow 0$ as $n\\rightarrow \\infty$.\n", "index": 29, "text": "\n\\[\nE_n^{(2)}(\\beta) \\  = \\  \\frac{1}{n} \\sum_{i=1}^n \\mbox{var}( Z_i ) \\beta\n  \\  = \\  O(n^{-1})\\  \\sum_{i=1}^n O(n^{-2})\\ O(i)\n  \\  = \\   O(n^{-3}) \\ O(n^2)  \\  \\rightarrow \\  0 \\mbox{ as }  n\\rightarrow \\infty\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"E_{n}^{(2)}(\\beta)\\ =\\ \\frac{1}{n}\\sum_{i=1}^{n}\\mbox{var}(Z_{i})\\beta\\ =\\ O(n%&#10;^{-1})\\ \\sum_{i=1}^{n}O(n^{-2})\\ O(i)\\ =\\ O(n^{-3})\\ O(n^{2})\\ \\rightarrow\\ 0%&#10;\\mbox{ as }n\\rightarrow\\infty\" display=\"block\"><mrow><mrow><msubsup><mi>E</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mtext>var</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>\u03b2</mi></mpadded></mrow></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>3</mn></mrow></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>\u2192</mo><mrow><mn>\u20050</mn><mo>\u2062</mo><mtext>\u00a0as\u00a0</mtext><mo>\u2062</mo><mi>n</mi></mrow><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></math>", "type": "latex"}, {"file": "1601.04736.tex", "nexttext": "\nwhich holds as long as the original system in not at equilibrium throughout the entire\nobservation time, which is assumption (A.3).\n\n\n\\subsection*{III\\ \\  $ \\mbox{var}\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i\\} \\rightarrow 0$  as $n \\rightarrow \\infty$}\n\nFinally we show that\n$\\mbox{var}\\left(\\frac{1}{n} \\sum_{i=1}^n Z_i^2\\right)\n\\rightarrow 0 \\mbox{ as } n\\rightarrow \\infty$\nby calculating\n\\begin{eqnarray*}\n\\mbox{var}\\left(\\frac{1}{n} \\sum_{i=1}^n Z_i^2\\right) & = &\n    O(n^{-2}) \\sum_{i=1}^n \\left\\{\\mbox{var}(Z_i^2) + 2\\sum_{j<i}\n    \\mbox{cov}(Z_j^2,Z_i^2) \\right\\} \\\\\n & = & O(n^{-2})\\sum_{i=1}^n\\left\\{ O(n^{-2})\\ O(i) + \\sum_{j<i}\n    O(n^{-2})\\ O(j) + O(n^{-4})\\ O(j^2) \\right\\} \\\\\n & = & O(n^{-2})\\sum_{i=1}^n \\left\\{ O(n^{-2})\\ O(i) + O(n^{-2})\\\n    O(i^2) + O(n^{-4})\\ O(i^3) \\right\\} \\\\\n & = & O(n^{-2}) \\left\\{ O(n^{-2})\\ O(n^2) + O(n^{-2})\\\n    O(n^3) + O(n^{-4})\\ O(n^4) \\right\\} \\\\\n & = & O(n^{-1}) \\rightarrow 0 \\mbox{ as } n\\rightarrow \\infty\n\\end{eqnarray*}\n\n\n\n\\begin{thebibliography}{50}\n\n\\bibitem{Bard:74} Bard, Y. (1974) \\newblock \\textit{ Nonlinear parameter\n    estimation}.\n\\newblock Academic Press.\n\n\\bibitem{Cook:86} Cook R.D, Tsai C.L., Wei B.C. (1986), ``Bias\n    in Nonlinear Regression,\" \\textit{Biometrika}, 73, 615--623.\n\n\\bibitem{Csa:06} Csajka, C., Verotta, D. (2006)\n    ``Pharmacokinetic-Pharmacodynamic Modelling: History and Perspective,\"\n    \\textit{Journal of Pharmacokinetic and Pharmacodynamics}, 33, 227--279.\n\n\\bibitem{Cro:86} Crowder, M. (1986) ``On Consistency and Inconcistency of\n    Estimating Equations,\" \\textit{Econometric Theory}, 2, 305--330.\n\n\\bibitem{Dan:08}  Danhof, M., de Lange, E.C.M, Della Pasqua, O.E,\n    Ploeger, B.A, and Voskuyl, R.A. (2008) 11Mechanixm-based\n    pharmacokinetic-pharmacodynamic(PK-PD) modeling in translational drug research,\"\n    \\textit{Trends in Pharmacological Sciences}, 29, 186--191.\n\n\\bibitem{Dar:07} Dartois, C., Brendel, K., Comets, E., Laffont, C.\n    M, Laveliil, C., Tranchand, B., Mentre, F., Lemenuel-Diot, A., and Girard P. (2007)\n    ``Overview of model-building strategies in population PK/PD analysis: 2002-2004\n    literature survey\", \\textit{British Journal of Pharmacology},  64, 603--612.\n\n\\bibitem{Dig:90} Diggle, PJ. (1990) \\textit{Time Series: A Biostatisical\n    Introduction}, Oxford University Press:New York.\n\n\n\\bibitem{Efr:93} Efron, B., and Tibshirani, R. J. (1993)\n    \\textit{ An Introduction to the Boostrap}  Chapman Hall:New York.\n\n\\bibitem{Fit:61} FitzHugh, R. (1961) ``Impulses and Physiological States\n    in MOndels of Nerve Membrame\", \\textit{ Biophysical Journal}  1,  445--466.\n\n\\bibitem{Foss:71} Foss, S. D. (1971), ``Estimates of chemical kinetic rate\n    constants by numerical integration,\" \\textit{ Chemical Engineering Science}, 26,\n    485--486.\n\n\\bibitem{Fre:80} Freedman, H.I. (1980) \\textit{ Deterministic\n    Mathematical Models in Population Ecology}  Marcel Dekker:New York.\n\n\\bibitem{Gal:04} Galecki A.T, Wolfinger R.D, Linares O.A, Smith\n    M.J,Halter J.B. (2004), ``Ordinary Differential Equations PK/PD Models Using the SAS\n    Macro NLINMIX,\" \\textit{Journal of Biopharmaceutical Statistics}{ 14}, 483--503.\n\n\\bibitem{Gau:34} Gause, GF. (1934) \\textit{The Struggle for Existence}\n    Willianms and Williams:Baltimore.\n\n\\bibitem{HimJon:67}  Himmelblau, D. M.,  Jones, C.\n    R., and  Bischoff, K. B. ``Determination of rate constants for complex kinetics\n    models,\" \\textit{Industrial and Engineering Chemistry Fundamentals}, 6, 539--543,\n    1967.\n\n\\bibitem{Ho:95} Ho, D. D., Neumann, A. U., Perelson, A. S., Chen, W.,\n    Leonard, J. M., and  Markowitz M. (1995), ``Rapid turnover of plasma virions and CD4\n    lymphocytes in HIV-1 infection,\" \\textit{Nature}, 373, 123--126.\n\n\\bibitem{Hosten:79} Hosten, L. H. (1979)  ``A comparative study of short\n    cut procedures for parameter estimation   in differential equations,\"\n    \\textit{Computers and Chemical Engineering}, 3, 117--126.\n\n\\bibitem{Hod:52} Hodgkin, A. L., and Huxley, A. F. (1952) ``A\n    Quantitative Description of Membrane Current and Its Application to Conduction and\n    Excitation in Nerve,\" \\textit{Jorunal of Physiology} { 133}, 444--479.\n\n\\bibitem{Jac:74} Jaquez JA.  (1974) \\textit{Compartmental Analysis in\n    Biology and Medicine} Elsevier:New York.\n\n\\bibitem{Lia:08} Liang, H. and Wu, H. (2008) ``Parameter Estimation\n    for differential Equation Models Using a Framework of Measurement Error in Regression\n    Models,\" \\textit{Journal of the American Statistical Association}, {103}, 1570--1583.\n\n\\bibitem{Lor:63} Lorenz, E.N. (1963) ``Deterministic Nonperiodic Flow\"\n    \\textit{Journal of Atmospheric Sciences}, 26, 130--141.\n\n\n\\bibitem{Mag:03} Mager, D. E, Wyska, E., and Jusko, W. J.\n    ''Minireview Diversity of Mechanism-Based Pharmacodynamic Models,\" (2003), \\textit{\n    Drug Metabolism and Disposition } { 31}, 510--519.\n\n\\bibitem{Nag:62} Nagumo, J. S., Arimoto, S.,\n    Yoshizawa, S. (1962), ``An Active Pulse Transmission Line Simulating a Nerve Axon,\"\n    \\textit{ Proceedings of the IRE}{ 50}, 2061--2070.\n\n\n\\bibitem{Per:96} Perelson, A. S., Neumann, A. T., Markowitz, M.,\n    Leonard, J. M., Ho, D. D. (1996), ``HIV--1 dynamics in vivo: virion clearance rate,\n    infected cell life--span, and viral generation time,\" \\textit{Science}, {  271},\n    1582--1586.\n\n\\bibitem{Per:97} Perelson, A. S., Essunger, P., Cao, .Y.,\n    Vesanen, M., Hurley, A., Saksela, K., Markowitz, M., Ho, D. D. (1997), ``Decay\n    characteristics of HIV--1 infected compartments during combination therapy,\"\n    \\textit{Nature}  { 387}, 188--191.\n\n\n\n\\bibitem{Pre:86} Press, W. H., Teukolsky, S. A., Vetterline, W. T.,\n    Flannery, B. P. (1986), \\textit{Numerical Recipes in Fortran} Cambridge University\n    Press:New York.\n\n\\bibitem{Ram:96} Ramsay, J. O. (1996) ``Principal Differential Analysis:\n    Data Reduction of Differential Operators,\" \\textit{Journal of the Royal Statistical\n    Society, Ser. B} { 58}, 495--508\n\n\\bibitem{RamSil:05} Ramsay, J. O. and Silverman, B. W.\n    (2005) \\textit{Functional Data Analysis} (2nd ed.), Springer:New York.\n\n\\bibitem{RHCC:07} Ramsay, J. O., Hooker, G., Campbell, D., and Cao,\n    J. (2007), ``Parameter Estimation for Differential Equations: A Generalized Smoothing Approach (with discussion),\" \\textit{Journal of the Royal Statistical Society, Ser. B}, { 69}, 741--96.\n\n\\bibitem{She:00} Sheiner, L.L. and Steimer, J.L. (2000)\n    P``harmacokinetic/Pharmacodynaic Modeling in Drug Development,\" \\textit{Annual Reveiw\n    of Pharmacological Toxicology}, { 40},  67--95\n\n\\bibitem{SwaBre:75} Swartz, J. and Bremermann, H. (1975)\n    ``Discussion of parameter estimation in biological modeling: Algorithms   for\n    estimation and evaluation of the estimates,\" \\textit{ Journal of Mathematical\n    Biology}, 1, 241--257.\n\n\\bibitem{Tor:04} Tornoe, C. W., Agerso, H., Johsson, E. N., Madsen,\n    H., and Nielsen, H. A. (2004) ``Non-linear mixed-effects\n    pharmacokinetic/pharmacodynamic modelling in NLME using differential equations,\"\n    \\textit{Computer Methods and Programs in Biomedicine}, { 76}, 31--40.\n\n\\bibitem{Varah:82} Varah, J. M. (1982),``A spline least squares method for\n    numerical parameter estimation in   differential equations\", \\textit{ SIAM, Journal\n    of Scientific and Statistical Computation}, 3, 28--46.\n\n\n\\bibitem{Wei:95}  Wei, X., Ghosh, S. K., Taylor, M. E.,  Johnson, V.\n    A. Emini, E. A., Deutsch, P., Lifson, J. D., Bonhoeffer, S., Nowak, M. A., Hahn, B.\n    H., Saag, M. S., Shaw, G. M. (1995),``Viral dynamics of HIV-1 infection,\" \\textit{\n    Nature}  {373}, 117--122.\n\n\\bibitem{Wikstrom:97a} Wikstrom, G. (1997) ``Computation of parameters\n    occurring linearly in systems of ordinary   differential equations, part i,\"\n    Technical report, Department of Computing Science, Umea University, 1997.\n\n\\bibitem{Wikstrom:97b} Wikstrom, G. (1997) ``Computation of parameters\n    occurring linearly in systems of ordinary  differential equations, part ii,\"\n    Technical report, Department of Computing Science, Umea University,\n  1997.\n\n\\bibitem{Wilson:99} Wilson, H. (1999) \\textit{ Spikes, Decisions and\n    Actions: the Dynamical Foundations of\n  Neuroscience} Oxford University Press, Oxford, England.\n\n\n\\bibitem{Zha:05} Zhang, W.B. (2005) \\textit{Differential Equations,\n    Bifurcations, and Chaos in Economics } World Scientific Publishing Company.\n\n\\end{thebibliography}\n\n\\pagebreak\n\n\n\\begin{figure} \\label{fig1}\n\n\\caption{BCLS and NLS estimates and associated trajectories based on\ndata from four growth colonies of paramecium aurelium }\n\\centerline{\n  \\epsfig{file=fig1.eps,height=5in,width=7in}\n}\n\\end{figure}\n\n\n\\begin{figure} \\label{fig2}\n\n\\caption{LS, BCLS, NLS and PsLS estimates and associated bias in estimated\nparameters based on 1000 simulations with levels of\nmeasurement error varying from 0.2 to 0.8.\nThe upper panels, A. and B., represent simulations\nwhen 21 times points, evenly spaced between 0 and 20, are\nused in the simulations and the lower panels, C. and D., when 201\ntime points, evenly spaced between 0 and 20, are used in the\nsimulation.}\n\\centerline{\n  \\epsfig{file=ParaBias2.eps,height=6in,width=7in}\n}\n\\end{figure}\n\n\n\n\n\\begin{figure} \\label{fig3}\n\n\n\\caption{Least squares surfaces for parameters $a$ and $b$ in the Fitzhugh-Nagumo system.\nPanels A, B, C: Values $a=0.58$ and $b=0.58$.\nPanels C, D, E: Values $a=0.34$ and $b=0.20$}\n\\centerline{\n\\epsfig{file=FHNsurfaces.eps,height=5in,width=7in}\n}\n\\end{figure}\n\n\\begin{figure} \\label{fig4}\n\n\n\\caption{Bias in estimates of $a$, $b$, and $C$ in the Fitzhugh-Nagumo system\nwhen $a=0.34$ and $b=0.2$ (Panels A,B,C) and when $a=0.58$ and $b=0.58$ (Panels C,D,F)\nas a function of measurement error.}\n\\centerline{\n\\epsfig{file=FHNbias2.eps,height=5in,width=7in}\n}\n\\end{figure}\n\n\n\\pagebreak\n\n\n\\begin{table}\n\\caption{Results of estimation of parameters in the logistic growth\nmodel to four data sets on growth colonies of the bacteria\nParamecium Aurelium using Nonlinear Least-Square (NLS) and Integrated\nData (BCLS) with bias adjustment methods}\n\\vspace*{0.25cm}\n\\begin{center}\n\\begin{tabular}{l|cc|cc}\n\\hline\n & \\multicolumn{2}{c}{ {\\bf NLS} }  &  \\multicolumn{2}{c}{ {\\bf BCLS} }   \\\\\n\\hline\n &  &  & & \\\\\n& {\\bf a} & {\\bf b } &  {\\bf a} &  {\\bf b} \\\\\n &  &  & & \\\\\n\\hline\n &  &  & & \\\\\nData Set  1  &  &  & & \\\\\nEstimate &  0.789  &  0.0014  &  0.776  &  0.0015  \\\\\n & & & & \\\\\nParametric &  ( 0.733 , 0.860 )  &  ( 0.0012 , 0.0017 )  &  ( 0.697 , 0.864 )  & ( 0.0012 , 0.0019 )  \\\\\nBootstrap & & & & \\\\\nNon-Parametric &  &    &  ( 0.738 , 0.839 )  &  ( 0.0014 , 0.0017 )   \\\\\nBootstrap & & & & \\\\\n &  &  & & \\\\\n\\hline\n &  &  & & \\\\\nData Set  2   &  &  & & \\\\\nEstimate &  0.837  &  0.0017  &  0.801  &  0.0017  \\\\\n & & & & \\\\\nParametric &  ( 0.770 , 0.908 )  &  ( 0.0013 , 0.0021 )  &  ( 0.727 ,0.883 )  & ( 0.0014 , 0.0021 )  \\\\\nBootstrap & & & & \\\\\nNon-Parametric &  &    &   ( 0.762 , 0.850 )  &  ( 0.0015 , 0.0019 )   \\\\\nBootstrap & & & & \\\\\n &  &  & & \\\\\n\\hline\n &  &  & & \\\\\nData Set  3   &  &  & & \\\\\nEstimate &  0.892  &  0.0016  &  0.867  &  0.0017  \\\\\n & & & & \\\\\nParametric &  ( 0.810 , 0.989 )  &  ( 0.0012 , 0.0020 )  &  ( 0.746 , 0.971 )  & ( 0.0013 , 0.0024 )  \\\\\nBootstrap & & & & \\\\\nNon-Parametric &   &    &  ( 0.832 , 0.911 )  &  ( 0.0015 , 0.0019 )   \\\\\nBootstrap & & & & \\\\\n &  &  & & \\\\\n\\hline\n &  &  & & \\\\\nData Set  4   &  &  & & \\\\\nEstimate &  0.844  &  0.0016  &  0.817  &  0.0016  \\\\\n & & & & \\\\\nParametric &  ( 0.776 , 0.908 )  &  ( 0.0013 , 0.0019 )  &  ( 0.733 , 0.899 )  &\n ( 0.0014 , 0.0018 )\n\\\\ Bootstrap & & & & \\\\\nNon-Parametric &   &    &  ( 0.787 , 0.849 )  &  ( 0.0015 , 0.0017 )  \\\\\nBootstrap & & & & \\\\\n &  &  & & \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n\n\\begin{table}\n\\caption{Simulation Results for Parameters in Logistic Growth Model\nusing Nonlinear Least-squares (NLS), least squares with (BCLS) and without bias correction and Psuedo-Least square (PsLS) methods with 21 equally spaced observations between $t=0$ and $t=20$. }\n\\vspace*{0.5cm}\n\\begin{center}\n\n\n\\begin{tabular}{c l | c c c | c c c c}\n\\toprule \n & &  & & & & & &  \\\\\n &  & \\multicolumn{3}{c|}{{\\bf a}=0.8} &  \\multicolumn{4}{c|}{{\\bf b}=0.0015} \\\\\n & & & & & & & & \\\\\n\\hline\n& & & & & & & &  \\\\\n  Measurement & & NLS & BCLS & PsLS & NLS &  BCLS  & LS   & PsLS \\\\\n  Error       & &     &    &      &     &     & no bias adj &      \\\\\n& & & & & & & & \\\\\n\\hline\n  & & & & & & & & \\\\\n0.2 & MC Mean   &  0.801 & 0.799  & 0.846 & 0.00151 & 0.00151 & 0.00148 & 0.00163\\\\\n    & MC s.e     & 0.023 & 0.029  & 0.035 & 0.00011 & 0.00012 & 0.00012 & 0.00018 \\\\\n & & & & & & & &   \\\\\n\\hline\n  & & & & & & & &  \\\\\n0.4 & MC Mean   &  0.802 & 0.794  & 0.837 & 0.00151 & 0.00149 & 0.00138 & 0.00153 \\\\\n    & MC s.e.   &  0.046 & 0.057 & 0.072 & 0.00022 & 0.00023 & 0.00022 & 0.00033 \\\\\n & & & & & & & &  \\\\\n\\hline\n  & & & & & & & & \\\\\n0.6 & MC Mean   &  0.810 & 0.792  & 0.820 & 0.00154 & 0.00151 & 0.00126 & 0.00138 \\\\\n      & MC s.e  &  0.074 & 0.085  & 0.105 & 0.00035 & 0.00039 & 0.00032 & 0.00045 \\\\\n  & & & & & & & & \\\\\n\\hline\n & & & & & & & & \\\\\n0.8 & MC Mean   &  0.813 & 0.777 & 0.789 & 0.00159 & 0.00151 & 0.00110 & 0.00121 \\\\\n    & MC s.e.      &  0.105 & 0.110 & 0.141 & 0.00041 & 0.00056 & 0.00041 &  0.00054 \\\\\n & & & & & & & &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n\n\n\\begin{table}\n\\caption{Simulation Results for Parameters in FitzHugh-Nagamu Model.  Least squares\nwithout bias correction (LS). Bias-corrected least squares (BCLS). Psuedo least squares\nwith smoothing bandwidth 0.2 (PsLS bw = 0.2). Psuedo least squares with smoothing bandwidth 0.37 (PsLS bw = 0.37).}\n\\resizebox{\\textwidth}{!}{ \n\n\\begin{tabular}{  l  c c   c c   c c   c c   c c  }\n\\toprule \n  \nActual  & \\multicolumn{2}{c}{{\\bf a = 0.34}} & \\multicolumn{2}{c}{{\\bf  b = 0.2}} & \\multicolumn{2}{c}{{\\bf C = 3}} & \\multicolumn{2}{c}{{\\boldmath $v_0$ =-1}} & \\multicolumn{2}{c}{{\\boldmath $r_0$ =1}}  \\\\\n  &  \\multicolumn{9}{c}{  } & \\\\\n\\cmidrule(l){2-11}\n  & mean &  bias &  mean & bias &  mean  & bias & mean  & bias &  mean  & bias  \\\\\n        & (s.e.) &   &  (s.e.) &   &  (s.e.)  &   & (s.e.)  &   &  (s.e.)  &    \\\\\n\\bottomrule \n LS  &  0.340  &  -0.001  &  0.200  &  0.002  &  2.958  &  -0.014  &  -0.936  &  -0.064  &  1.013  &  0.013  \\\\\n      & ( 0.004 ) &     & ( 0.014 )&         & ( 0.086 ) &          & ( 0.281 ) &       & ( 0.030 ) &  \\\\\n \\cmidrule(l){2-11}\n BCLS &  0.340  &  0.000  &  0.200  &  0.001  &  2.967  &  -0.011  &  -0.984  &  -0.016  &  1.010  &  0.010  \\\\\n      & ( 0.004 ) &      & ( 0.013 ) &       & ( 0.084 ) &           & ( 0.280 ) &      & ( 0.029 ) &  \\\\\n\\cmidrule(l){2-11}\n PsLS bw = 0.2 &  0.340  &  -0.001  &  0.199  &  -0.007  &  2.973  &  -0.009  & \\multicolumn{2}{c}{NA}    &  \\multicolumn{2}{c}{NA}   \\\\\n          & ( 0.024 ) &  & ( 0.042 ) &  & ( 0.045 ) &  &  &  &  &  \\\\\n \\cmidrule(l){2-11}\n PsLS bw = 0.37 &  0.336  &  -0.013  &  0.206  &  0.028  &  3.056  &  0.019  & \\multicolumn{2}{c}{NA}    &  \\multicolumn{2}{c}{NA}   \\\\\n          & ( 0.013 ) &  & ( 0.029 ) &  & ( 0.046 ) &  &  &  &  &  \\\\\n \\cmidrule(l){2-11}\n\\toprule \nActual  & \\multicolumn{2}{c}{{\\bf a = 0.58}} & \\multicolumn{2}{c}{{\\bf  b = 0.58}} & \\multicolumn{2}{c}{{\\bf C = 3}} & \\multicolumn{2}{c}{{\\boldmath $v_0$ =-1}} & \\multicolumn{2}{c}{{\\boldmath $r_0$ =1}}  \\\\\n  &  \\multicolumn{9}{c}{  } & \\\\\n\\cmidrule(l){2-11}\nLS  &  0.581  &  0.002  &  0.578  &  -0.003  &  2.938  &  -0.021  &  -0.892  &  -0.108  &  1.020  &  0.020  \\\\\n  & ( 0.005 ) &          & ( 0.014 ) &      & ( 0.118 ) &         & ( 0.248 ) &         & ( 0.041 ) &  \\\\\n \\cmidrule(l){2-11}\nBCLS  &  0.581  &  0.002  &  0.579  &  -0.002  &  2.948  &  -0.017  &  -0.966  &  -0.034  &  1.017  &  0.017  \\\\\n     & ( 0.005 ) &        & ( 0.014 ) &     & ( 0.122 ) &          & ( 0.242 ) &         & ( 0.042 ) &  \\\\\n \\cmidrule(l){2-11}\nPsLS bw = 0.2 &  0.585  &  0.008  &  0.577  &  -0.005  &  2.959  &  -0.014   & \\multicolumn{2}{c}{NA}    &  \\multicolumn{2}{c}{NA}   \\\\\n     & ( 0.025 ) &          & ( 0.057) &      & ( 0.060 ) &  &  &  & &  \\\\\n\\cmidrule(l){2-11}\nPsLS bw = 0.37 &  0.581  &  0.002  &  0.573  &  -0.012  &  3.025  &  0.008   & \\multicolumn{2}{c}{NA}    &  \\multicolumn{2}{c}{NA}   \\\\\n     & ( 0.016 ) &          & ( 0.036) &      & ( 0.065 ) &  &  &  & &  \\\\\n \\cmidrule(l){2-11}\n\\bottomrule \n\\end{tabular}\n  } \n\\end{table}\n\n\n\n\n\\begin{small}\n\\begin{table}\n\\caption{Simulation Results for Parameters in FitzHugh-Nagamu Model\nusing Nonlinear Least-squares (NLS) with various starting values} \n\\begin{center}\n\\begin{tabular}{c c r r r r r r}\n\\toprule\n  \\multicolumn{2}{c}{} & {\\bf a=0.34} & {\\bf b=0.20} &        &  {\\bf a =0.58}  &  {\\bf b=0.58} & \\\\\n  \\cmidrule(l){3-4} \\cmidrule(l){6-7}\n   \\multicolumn{2}{c}{Starting} & \\multicolumn{5}{c}{} \\\\\n  \\multicolumn{2}{c}{Values} & Mean      & Mean   &  percent    & Mean &  Mean   & percent\\\\\n  a & b &  (s.e.)  &  (s.e.)  & conv.      & (s.e.) &  (s.e.) & conv. \\\\\n\\midrule\n0.40 & 0.40      &  0.340   & 0.200   & 93.9\\%  & 1.154 & 2.018 & 31.7\\% \\\\\n     &         &  (0.002) &  (0.015) &       & (0.011) &  (0.032) & \\\\\n     & 0.80     &  0.340   & 0.200    & 94.4\\%  & 0.580  & 0.581  & 94.7\\% \\\\\n     &        &  (0.002) &  (0.015) &           & (0.007)  &  (0.010) & \\\\\n\\cmidrule(l){3-8}\n0.80 & 0.40   &  1.720   & 2.771    & 32.6 \\%  & 1.155 & 2.018 & 28.5\\% \\\\\n     &        &  (0.012) &  (0.043) &         & (0.010)  & (0.031) & \\\\\n     & 0.80     &  1.720   & 2.770    & 33.7\\% & 1.154 & 2.017 & 32.1\\% \\\\\n     &        &  (0.013) &  (0.043) &     & (0.011) & (0.031)  & \\\\\n\\cmidrule(l){3-8}\n1.2 & 0.40     &  1.720   &  2.770    & 36.2\\% & 1.155 & 2.018 & 31.1\\%\\\\\n     &        &  (0.012) &  (0.044) &          &  (0.011) & (0.031) & \\\\\n     & 0.80     &  1.720   & 2.772    & 36.2\\%  & 1.155 & 2.016 & 30.2\\% \\\\\n     &        &  (0.013) &  (0.043) &           & (0.011)  & (0.030) & \\\\\n\\cmidrule(l){3-8}\n\\multicolumn{2}{c}{From BCLS}    &  0.340  &  0.200  & 94.2\\% & 0.580 & 0.581 & 95.1\\% \\\\\n     &                         &  (0.002) &  (0.015) &          &  (0.007) & (0.010) & \\\\\n\\bottomrule\n\n\\end{tabular}\n\\end{center}\n\\end{table}\n\\end{small}\n\n\n", "itemtype": "equation", "pos": 65761, "prevtext": "\nThis completes the proof that $\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =  0$.\n\\vskip0.1in\n\nNext, we consider $\\mbox{var}\\{{\\overline{U}}_n(\\beta)\\}\\  =\\  \\frac{1}{n^2} \\left\\{ \\sum_{i=1}^n\\mbox{var}(U_i)\n + 2 \\sum_{j<i} \\mbox{cov}(U_j,U_i) \\right\\}$.\nFirst we evaluate $ \\mbox{var}(U_i) =  \\mbox{var}(  Z_i Y_i) + \\mbox{var}(  Z_i^2 \\beta)\n- 2 \\mbox{cov}(Z_i Y_i,Z_i^2 \\beta) $. Using equations (\\ref{r0})-(\\ref{r4})\nand independence\nof $Z_i$ and $Y_i$ we obtain\n\\begin{eqnarray*}\n \\mbox{var}(  Z_i\\ Y_i) & = & E[Z_i^2\\ Y_i^2] - E[Z_i\\ Y_i]^2 \\\\\n   & = &  E[Z_i^2]\\ E[Y_i^2] - E[Z_i]^2\\ E[Y_i]^2 \\\\\n   & = &  (\\mbox{var}(Z_i)-M_i^2)\\ (\\mbox{var}(Y_i) - \\mu_i^2) - M_i^2 \\mu_i^2 \\\\\n   & = &  \\{\\mbox{var}(Z_i) \\; \\mbox{var}(Y_i) \\} - \\mu_i^2 \\mbox{ var}\n   ( Z_i) - M_i^2 \\mbox{ var} (Y_i) \\\\\n   & = &  O(n^{-2})\\  O(i) - O(n^{-2})\\ O(i^2) \\\\\n\\mbox{var}( Z_i^2 \\beta) & = & O(n^{-4})\\  O(i^3) \\\\\n\\mbox{cov}( Z_i\\ Y_i \\; Z_i^2 \\beta) & = & \\beta\\ \\mu_i\n    \\mbox{ cov}(Z_i,Z_i^2) \\\\\n    & = & O(n^{-3}) \\; O(i^2)\n\\end{eqnarray*}\n\nThus, we have a contribution to $\\mbox{var}[{\\overline{U}}_n(\\beta)]$ from the\ndiagonal (variance) terms of:\n\\begin{eqnarray*}\n\\frac{1}{n^2} \\sum_{i=1}^n \\mbox{var}(U_i) & = & O(n^{-2})\\\n \\sum_{i=1}^n\\ \\left\\{  O(n^{-2})\\ O(i) + O(n^{-2})\\ O(i^2) +  O(n^{-4})\\ O(i^3) + O(n^{-3}) \\;\n O(i^2) \\right\\} \\\\\n & = &  O(n^{-2}) \\left\\{  O(n^{-2})\\  O(n^2) + O(n^{-2})\\ O(n^3)\\ +\\ O(n^{-4})\\  O(n^4) + O(n^{-3}) \\ O(n^3) \\right\\}\\\\\n & = &  O(n^{-2}) + O(n^{-1}) \\ \\rightarrow \\ 0 \\mbox{ as }\n n\\rightarrow \\infty\\end{eqnarray*}\n\nTo evaluate $ \\mbox{cov}(U_j,U_i)$ assuming $j<i$ consider:\n\\begin{eqnarray*}\n \\mbox{cov}(U_j,U_i) & = &  \\mbox{cov}( Z_j Y_j-Z_j^2\\beta,Z_iY_i - Z_i^2\\beta ) \\\\\n  & = &  \\mbox{cov}( Z_jY_j,  Z_iY_i  )   -\n   \\mbox{cov}( Z_jY_j,  Z_i^2\\beta  )  -\n   \\mbox{cov}(  Z_j^2\\beta,   Z_iY_i  )   +\n   \\mbox{cov}( Z_j^2\\beta,   Z_i^2\\beta  )\n\\end{eqnarray*}\n\nUsing equations (\\ref{r0})-(\\ref{r4}) it follows that:\n\\begin{eqnarray*}\n  \\mbox{cov}(Z_j Y_j, \\; Z_iY_i ) & = &  \\mbox{cov}(Z_j Y_j,(Z_{i\\backslash (j+1)} + Z_{j+1}) Y_i) \\\\\n                                  & = &  \\mbox{cov}(Z_jY_j,Z_{i\\backslash (j+1)} Y_i) + \\mbox{cov} (Z_j Y_j,Z_{j+1} Y_i) \\\\\n                                  & = &  0\\; + \\mbox{cov} (Z_j Y_j, [\\frac{1}{n}h^{\\star}(Y_j) + Z_j ] Y_i)  \\\\\n                                  & = &  \\frac{1}{n} \\{\\mbox{cov} (Z_j Y_j, h^{\\star}(Y_j) Y_i)\\} +\\mbox{cov}(Z_j Y_j,Z_j Y_i)\\\\\n                                  & = &  \\frac{1}{n} \\{ E[Z_j Y_j h^{\\star}(Y_j) Y_i] - M_j \\mu_j h^{\\star}(\\mu_j) \\mu_i  \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &  \\frac{1}{n} \\{ M_j \\mu_j E[Y_j h^{\\star}(Y_j)] - M_j \\mu_j h^{\\star}(\\mu_j) \\mu_i  \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &  \\frac{1}{n} \\{ M_j \\mu_j \\left(E[Y_j h^{\\star}(Y_j)] - h^{\\star}(\\mu_j) \\mu_i \\right) \\}\n                                                + \\mu_j \\mu_i \\mbox{var}(Z_j) \\\\\n                                  & = &   O(n^{-2}) \\;\\; O(j) \\\\\n\\end{eqnarray*}\nSimilar calculations show that\n\\begin{eqnarray*}\n\\mbox{cov}( \\; Z_j Y_j, \\; Z_i^2 \\beta \\; ) & = &  O(n^{-2}) \\;\\; O(j) + O(n^{-3}) \\;\\; O(j^2) \\\\\n\\mbox{cov}( \\; Z_j^2\\beta, \\; Z_iY_i \\; ) & = & O(n^{-3}) \\;\\; O(j^2) \\\\\n\\mbox{cov}( \\; Z_j^2 \\beta, \\; Z_i^2 \\beta \\; ) & = &  O(n^{-3}) \\;\\; O(j^2) + O(n^{-4}) \\;\\; O(j^3)\n\\end{eqnarray*}\n\nIt follows that the contribution to $\\mbox{var}\\{{\\overline{U}}_n(\\beta)\\}$ from the\noff-diagonal terms is:\n\\begin{eqnarray*}\n2 \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j<i} \\mbox{cov}(U_i,U_j)  & = &\n    O(n^{-2}) \\sum_{i=1}^n \\sum_{j<i} \\; O(n^{-2}) \\  O(j)  \\\\\n  &  &  \\; - \\  O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-2}) \\  O(j) + O(n^{-3}) \\  O(j^2) \\\\\n  &  & \\; - \\   O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-3}) \\  O(j^2) \\\\\n  &  &  \\; + \\  O(n^{-2})  \\sum_{i=1}^n \\sum_{j<i}  O(n^{-3}) \\  O(j^2) + O(n^{-4}) \\  O(j^3) \\\\\n  & = & O(n^{-2}) \\; O(n^{-2}) \\  O(n^3) \\\\\n  &  &  - O(n^{-2}) \\ \\left\\{ O(n^{-2}) \\  O(n^3) + O(n^{-3}) \\\n    O(n^4) \\right\\} \\\\\n  &  &  - \\  O(n^{-2}) \\ \\left\\{ O(n^{-3}) \\  O(n^4) \\right\\} \\\\\n  &  &   + \\  O(n^{-2}) \\ \\left\\{ O(n^{-3}) \\  O(n^4) + O(n^{-4})\n    \\  O(n^5) \\right\\} \\\\\n  & = & O(n^{-1}) \\rightarrow 0 \\mbox{ as } n\\rightarrow \\infty\n\\end{eqnarray*}\n\nThis completes the proof that $\\lim_{n\\rightarrow\\infty} \\mbox{var} [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =   0 $.\nCombined with the previous proof that $\\lim_{n\\rightarrow\\infty} E [ {\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}}) ]  =  0$ we\nhave established that ${\\overline{U}}_n({\\mbox{\\boldmath ${\\beta}$}})$ is a consistent estimator of $0$.\n\n\\subsection*{II\\ \\  $E\\{\\frac{1}{n} \\sum_{i=1}^n {\\mbox{\\boldmath ${Z}$}}_i^T {\\mbox{\\boldmath ${Z}$}}_i \\}$ is bounded and non-zero}\n\nTo see that   $ E \\left(\\frac{1}{n} \\sum_{i=1}^n Z_i^T Z_i \\right)$ is bounded and non-zero\nwe calculate:\n\\begin{eqnarray*}\n E \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i^T Z_i \\right)  & = & \\frac{1}{n} \\sum_{i=1}^n \\{ \\mbox{var}(Z_i) + E(Z_i)^2\\}  \\\\\n   & = & O(n^{-1}) \\left\\{ \\sum_{i=1}^n O(n^{-2}) \\; O(i) + M_i^2 \\right\\} \\\\\n   & \\leq & O(n^{-1}) \\; \\{O(n^{-2}) O(n^2) + O(n) \\} \\\\\n   & = & O(1)\n\\end {eqnarray*}\nso that  $ E\\left\\{\\frac{1}{n} \\sum_{i=1}^n Z_i^2\\right\\}$ is bounded.  As long as\n$E(Z_i^2) > 0$ for at least one $i$ we have\n\n", "index": 31, "text": "$$ E\\left(\\frac{1}{n} \\sum_{i=1}^n Z_i^T Z_i\\right) > 0 $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"E\\left(\\frac{1}{n}\\sum_{i=1}^{n}Z_{i}^{T}Z_{i}\\right)&gt;0\" display=\"block\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>Z</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>Z</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow></math>", "type": "latex"}]