[{"file": "1601.07600.tex", "nexttext": "\nand the Dirac delta function $\\delta(\\cdot)$~(more precisely, a distribution~(see, e.g.,~\\cite{dirac})), a generalized function whose discrete analog is referred to as the Kronecker delta function:\n\n", "itemtype": "equation", "pos": 834, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nWe first give an elementary derivation of the shrinkage function widely used in compressive sensing and statistical estimations. Then we demonstrate how it can be used in solving several well-known problems and proving one new result in matrix approximations.\n\\end{abstract}\n\n\\begin{keywords}\n\tShrinkage function, singular value decomposition, low-rank approximation, sparse approximation. \n\\end{keywords}\n\n\\begin{AMS}\n\t65F15, 65F30, 65F35, 65F50, 65K10\n\\end{AMS}\n\n\n\\pagestyle{myheadings}\n\\thispagestyle{plain}\n\\markboth{BOAS, DUTTA, LI, MERCIER AND NIDERMAN}{Constrained low rank approximation of matrices}\n\n\n\\section{Introduction} There are a few mathematical functions which are introduced for convenience. For example, the Heaviside step function $H(\\cdot)$, a piecewise constant function given by:\n\n", "index": 1, "text": "$$\nH(x)=\\left\\{\\begin{array}{ll}\n1,&x> 0\\\\\n\\frac{1}{2},&x=0\\\\\n0,&x<0\n\\end{array},\n\\right.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"H(x)=\\left\\{\\begin{array}[]{ll}1,&amp;x&gt;0\\\\&#10;\\frac{1}{2},&amp;x=0\\\\&#10;0,&amp;x&lt;0\\end{array},\\right.\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\nThis article is on a newcomer, the shrinkage function $S_{\\lambda}(\\cdot)$, first introduced by Donoho and Johnstone in their landmark paper~(\\cite{soft-threshold:donoho-johnstone}, see also \\cite{soft-threshold2}) on function estimation using wavelets in the early 1990's. Recently, the shrinkage function has been heavily used in the solutions of several optimization and approximation problems of matrices (see, e.g.,~\\cite{svt:cai-candes-shen,lin-chen-ma,tao-yuan,yuan-yang}). We give an elementary treatment that is accessible to a vast group of researchers, as it only requires basic knowledge in calculus and linear algebra and show how naturally the shrinkage function can be used in solving more advanced problems.\n\n\\section{A calculus problem} We start with a regular calculus problem. Let $\\lambda>0$ and $a\\in {\\mathbb R}$ be given. Consider\nthe following problem:\n\n", "itemtype": "equation", "pos": 1125, "prevtext": "\nand the Dirac delta function $\\delta(\\cdot)$~(more precisely, a distribution~(see, e.g.,~\\cite{dirac})), a generalized function whose discrete analog is referred to as the Kronecker delta function:\n\n", "index": 3, "text": "$$\\delta_{ij}=\\left\\{\\begin{array}{ll}\n1,&i=j\\\\\n0,&i\\not =j\n\\end{array}.\n\\right.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\delta_{ij}=\\left\\{\\begin{array}[]{ll}1,&amp;i=j\\\\&#10;0,&amp;i\\not=j\\end{array}.\\right.\" display=\"block\"><mrow><mrow><msub><mi>\u03b4</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nWe adopt the notation $a=\\displaystyle{\\arg\\min_{x\\in A}f(x)}$ to mean that $a\\in A$ is a solution of the minimization problem $\\displaystyle{\\min_{x\\in A}f(x)}$ and define:\n\n", "itemtype": "equation", "pos": 2087, "prevtext": "\n\nThis article is on a newcomer, the shrinkage function $S_{\\lambda}(\\cdot)$, first introduced by Donoho and Johnstone in their landmark paper~(\\cite{soft-threshold:donoho-johnstone}, see also \\cite{soft-threshold2}) on function estimation using wavelets in the early 1990's. Recently, the shrinkage function has been heavily used in the solutions of several optimization and approximation problems of matrices (see, e.g.,~\\cite{svt:cai-candes-shen,lin-chen-ma,tao-yuan,yuan-yang}). We give an elementary treatment that is accessible to a vast group of researchers, as it only requires basic knowledge in calculus and linear algebra and show how naturally the shrinkage function can be used in solving more advanced problems.\n\n\\section{A calculus problem} We start with a regular calculus problem. Let $\\lambda>0$ and $a\\in {\\mathbb R}$ be given. Consider\nthe following problem:\n\n", "index": 5, "text": "\\begin{equation}\\label{calcp}\n\\min_{x\\in {\\mathbb R}}\\left[\\lambda|x|+\\frac{1}{2}(x-a)^2\\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\min_{x\\in{\\mathbb{R}}}\\left[\\lambda|x|+\\frac{1}{2}(x-a)^{2}\\right].\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>x</mi><mo>\u2208</mo><mi>\u211d</mi></mrow></munder><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\n\\begin{theorem}\\label{theorem 1}\n{ Let $\\lambda>0$ be fixed. For\neach $a \\in {\\mathbb R}$, there is one and only one solution $S_{\\lambda}(a)$, to\nthe minimization problem~(\\ref{P}). Furthermore,\n\n", "itemtype": "equation", "pos": 2373, "prevtext": "\nWe adopt the notation $a=\\displaystyle{\\arg\\min_{x\\in A}f(x)}$ to mean that $a\\in A$ is a solution of the minimization problem $\\displaystyle{\\min_{x\\in A}f(x)}$ and define:\n\n", "index": 7, "text": "\\begin{equation}\\label{P}\nS_{\\lambda}(a):=\\displaystyle{\\arg\\min_{x\\in {\\mathbb R}}\\left[\\lambda|x|+\\frac{1}{2}(x-a)^2\\right]}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"S_{\\lambda}(a):=\\displaystyle{\\arg\\min_{x\\in{\\mathbb{R}}}\\left[\\lambda|x|+%&#10;\\frac{1}{2}(x-a)^{2}\\right]}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>x</mi><mo>\u2208</mo><mi>\u211d</mi></mrow></munder><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n}\n\\end{theorem}\n\n\n\\noindent {\\bf Remark.} The function $S_{\\lambda}(\\cdot )$ defined\nabove is called the shrinkage function (also referred to as\nsoft shrinkage or soft threshold, \\cite{soft-threshold:donoho-johnstone,soft-threshold2}). One may imagine that $S_{\\lambda}(a)$ ``shrinks'' $a$ to zero when~$|a|\\leq \\lambda.$\nA plot of $S_{\\lambda}(\\cdot)$  for $\\lambda = 1$ is\ngiven in Figure~\\ref{fig1}.\n\\begin{figure}[H]\n\t\\centering\n\t\t\\includegraphics[width=0.5\\textwidth]{shrinkage.eps}\n\t\t\\caption{A plot of $S_{\\lambda}$ for $\\lambda=1$.}\n\t\t\\label{fig1}\n\t\t\\end{figure}\n\n\n\\begin{proof} Let $f(x)=\\lambda|x|+\\frac{1}{2}(x-a)^2$. Note that $f(x)\\to\\infty$ when $|x|\\to\\infty$ and $f$ is\ncontinuous on ${\\mathbb R}$ and differentiable everywhere except a single point $x=0$.\nSo, $f$ achieves its minimum value on ${\\mathbb R}$ at one of its critical points. Let $x^*=\\displaystyle{\\arg\\min_{x\\in {\\mathbb R}}f(x)}$.\n\\begin{figure}[httb!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{parabola1.eps}\n\t\\caption{Plots of $f(x)$ for different values of $a$ with $\\lambda=1$.}\n\t\\label{fig2}\n\\end{figure}\n\n\nWe consider three cases.\n\nCase 1: $x^*>0$. Since $f$ is differentiable at $x=x^*$ and achieves its minimum,\nwe must have $f'(x^*)=0$. Note that, for $x>0$, we have \n", "itemtype": "equation", "pos": 2713, "prevtext": "\n\n\\begin{theorem}\\label{theorem 1}\n{ Let $\\lambda>0$ be fixed. For\neach $a \\in {\\mathbb R}$, there is one and only one solution $S_{\\lambda}(a)$, to\nthe minimization problem~(\\ref{P}). Furthermore,\n\n", "index": 9, "text": "$$\nS_{\\lambda}(a)=\\left\\{\\begin{array}{ll}\na-\\lambda,&a>\\lambda\\\\\n0,&|a|\\leq \\lambda\\\\\na+\\lambda,&a<-\\lambda\n\\end{array}.\\right.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"S_{\\lambda}(a)=\\left\\{\\begin{array}[]{ll}a-\\lambda,&amp;a&gt;\\lambda\\\\&#10;0,&amp;|a|\\leq\\lambda\\\\&#10;a+\\lambda,&amp;a&lt;-\\lambda\\end{array}.\\right.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>a</mi><mo>-</mo><mi>\u03bb</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>a</mi><mo>&gt;</mo><mi>\u03bb</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>\u03bb</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>a</mi><mo>+</mo><mi>\u03bb</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>a</mi><mo>&lt;</mo><mrow><mo>-</mo><mi>\u03bb</mi></mrow></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nSo, \n", "itemtype": "equation", "pos": 4112, "prevtext": "\n}\n\\end{theorem}\n\n\n\\noindent {\\bf Remark.} The function $S_{\\lambda}(\\cdot )$ defined\nabove is called the shrinkage function (also referred to as\nsoft shrinkage or soft threshold, \\cite{soft-threshold:donoho-johnstone,soft-threshold2}). One may imagine that $S_{\\lambda}(a)$ ``shrinks'' $a$ to zero when~$|a|\\leq \\lambda.$\nA plot of $S_{\\lambda}(\\cdot)$  for $\\lambda = 1$ is\ngiven in Figure~\\ref{fig1}.\n\\begin{figure}[H]\n\t\\centering\n\t\t\\includegraphics[width=0.5\\textwidth]{shrinkage.eps}\n\t\t\\caption{A plot of $S_{\\lambda}$ for $\\lambda=1$.}\n\t\t\\label{fig1}\n\t\t\\end{figure}\n\n\n\\begin{proof} Let $f(x)=\\lambda|x|+\\frac{1}{2}(x-a)^2$. Note that $f(x)\\to\\infty$ when $|x|\\to\\infty$ and $f$ is\ncontinuous on ${\\mathbb R}$ and differentiable everywhere except a single point $x=0$.\nSo, $f$ achieves its minimum value on ${\\mathbb R}$ at one of its critical points. Let $x^*=\\displaystyle{\\arg\\min_{x\\in {\\mathbb R}}f(x)}$.\n\\begin{figure}[httb!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{parabola1.eps}\n\t\\caption{Plots of $f(x)$ for different values of $a$ with $\\lambda=1$.}\n\t\\label{fig2}\n\\end{figure}\n\n\nWe consider three cases.\n\nCase 1: $x^*>0$. Since $f$ is differentiable at $x=x^*$ and achieves its minimum,\nwe must have $f'(x^*)=0$. Note that, for $x>0$, we have \n", "index": 11, "text": "$$f'(x)=\\frac{d}{dx}[\\lambda x+\\frac{1}{2}(x-a)^2]=\\lambda+(x-a).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"f^{\\prime}(x)=\\frac{d}{dx}[\\lambda x+\\frac{1}{2}(x-a)^{2}]=\\lambda+(x-a).\" display=\"block\"><mrow><mrow><mrow><msup><mi>f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03bb</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhich implies \n", "itemtype": "equation", "pos": 4184, "prevtext": "\nSo, \n", "index": 13, "text": "$$\\lambda +(x^*-a)=0,$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\lambda+(x^{*}-a)=0,\" display=\"block\"><mrow><mrow><mrow><mi>\u03bb</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>x</mi><mo>*</mo></msup><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nTo be consistent with $x^*>0$, it is necessary that $a-\\lambda>0$ or, equivalently, $a>\\lambda$.\n\nCase 2: $x^*<0$. By proceeding similarly as in Case 1 above, we can arrive at\n\n", "itemtype": "equation", "pos": 4222, "prevtext": "\nwhich implies \n", "index": 15, "text": "$$x^*=a-\\lambda.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"x^{*}=a-\\lambda.\" display=\"block\"><mrow><mrow><msup><mi>x</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>a</mi><mo>-</mo><mi>\u03bb</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\nCase 3: $x^*=0$. Note that $f(x)$ is no longer differentiable at $x^*=0$~(so we could not use the condition $f'(x^*)=0$ as before). But since $f$ has a minimum at $x^*=0$ and since $f$ is differentiable on each side of $x^*=0$, it is necessary that\n\n", "itemtype": "equation", "pos": 4417, "prevtext": "\nTo be consistent with $x^*>0$, it is necessary that $a-\\lambda>0$ or, equivalently, $a>\\lambda$.\n\nCase 2: $x^*<0$. By proceeding similarly as in Case 1 above, we can arrive at\n\n", "index": 17, "text": "$$\nx^*=a+\\lambda ~~{\\rm with }~~a<-\\lambda.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"x^{*}=a+\\lambda~{}~{}{\\rm with}~{}~{}a&lt;-\\lambda.\" display=\"block\"><mrow><mrow><msup><mi>x</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>a</mi><mo>+</mo><mrow><mpadded width=\"+6.6pt\"><mi>\u03bb</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>with</mi></mpadded><mo>\u2062</mo><mi>a</mi></mrow></mrow><mo>&lt;</mo><mrow><mo>-</mo><mi>\u03bb</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\\\\\nSo,\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nCase 3: $x^*=0$. Note that $f(x)$ is no longer differentiable at $x^*=0$~(so we could not use the condition $f'(x^*)=0$ as before). But since $f$ has a minimum at $x^*=0$ and since $f$ is differentiable on each side of $x^*=0$, it is necessary that\n\n", "index": 19, "text": "$$f'(x)>0\\;\\;\\text{for}\\;\\;{x>0}\\;\\;\\text{and}\\;\\;f'(x)<0\\;\\text{for}\\;\\;x<0.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"f^{\\prime}(x)&gt;0\\;\\;\\text{for}\\;\\;{x&gt;0}\\;\\;\\text{and}\\;\\;f^{\\prime}(x)&lt;0\\;\\text%&#10;{for}\\;\\;x&lt;0.\" display=\"block\"><mrow><mrow><mrow><msup><mi>f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+5.6pt\"><mtext>for</mtext></mpadded><mo>\u2062</mo><mi>x</mi></mrow><mo>&gt;</mo><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+5.6pt\"><mtext>and</mtext></mpadded><mo>\u2062</mo><msup><mi>f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mpadded width=\"+2.8pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+5.6pt\"><mtext>for</mtext></mpadded><mo>\u2062</mo><mi>x</mi></mrow><mo>&lt;</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nThus, \n\n", "itemtype": "equation", "pos": 4799, "prevtext": "\\\\\nSo,\n\n", "index": 21, "text": "$$ \n\\lambda+x-a>0\\;\\;\\text{for}\\;x>0\\;\\;\\text{and}\\;-\\lambda+x-a<0\\;\\;\\text{for}\\;x<0.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\lambda+x-a&gt;0\\;\\;\\text{for}\\;x&gt;0\\;\\;\\text{and}\\;-\\lambda+x-a&lt;0\\;\\;\\text{for}\\;%&#10;x&lt;0.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>\u03bb</mi><mo>+</mo><mi>x</mi></mrow><mo>-</mo><mi>a</mi></mrow><mo>&gt;</mo><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>for</mtext></mpadded><mo>\u2062</mo><mi>x</mi></mrow><mo>&gt;</mo><mrow><mrow><mrow><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>and</mtext></mpadded></mrow><mo>-</mo><mi>\u03bb</mi></mrow><mo>+</mo><mi>x</mi></mrow><mo>-</mo><mi>a</mi></mrow><mo>&lt;</mo><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>for</mtext></mpadded><mo>\u2062</mo><mi>x</mi></mrow><mo>&lt;</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nor, equivalently,\n\n", "itemtype": "equation", "pos": 4896, "prevtext": "\nThus, \n\n", "index": 23, "text": "$$ \n\\lambda-a>0\\;\\;\\text{and}\\;-\\lambda-a<0,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\lambda-a&gt;0\\;\\;\\text{and}\\;-\\lambda-a&lt;0,\" display=\"block\"><mrow><mrow><mrow><mi>\u03bb</mi><mo>-</mo><mi>a</mi></mrow><mo>&gt;</mo><mrow><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>and</mtext></mpadded></mrow><mo>-</mo><mi>\u03bb</mi><mo>-</mo><mi>a</mi></mrow><mo>&lt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nTo summarize, we have\n\n", "itemtype": "equation", "pos": 4962, "prevtext": "\nor, equivalently,\n\n", "index": 25, "text": "$$\n|a|\\le\\lambda.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"|a|\\leq\\lambda.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>\u03bb</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\nSince one and only one of the three cases (1) $a>\\lambda$, (2)\n$a<-\\lambda$, and (3) $|a|\\leq \\lambda$ holds, we obtain the\nuniqueness in general. With the uniqueness, it is straightforward\nto verify that each of\nthe three cases would imply the corresponding formula for $x^*$.\nThis completes the proof.\n\\end{proof}\n\n\\section{A sparse recovery problem} Recently, research in\ncompressive sensing leads to the recognition of the fact that the $\\ell_1$-norm\nof a vector is a good substitute for the count of the number of\nnon-zero entries of the vector in many minimization problems. In this section, we solve some simple minimization problems using the count of non-zero entries or the $\\ell_1$-norm.\nGiven a vector\n${\\mathbf v}\\in {\\mathbb R}^n$, we want to solve\n\n", "itemtype": "equation", "pos": 5005, "prevtext": "\nTo summarize, we have\n\n", "index": 27, "text": "$$\nx^*=\\left\\{\n\\begin{array}{ll}\na-\\lambda &{\\rm with}~a>\\lambda,\\\\\na+\\lambda &{\\rm with}~a<-\\lambda,\\\\\n0 &{\\rm with}~|a|\\leq \\lambda.\n\\end{array}\n\\right.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"x^{*}=\\left\\{\\begin{array}[]{ll}a-\\lambda&amp;{\\rm with}~{}a&gt;\\lambda,\\\\&#10;a+\\lambda&amp;{\\rm with}~{}a&lt;-\\lambda,\\\\&#10;0&amp;{\\rm with}~{}|a|\\leq\\lambda.\\end{array}\\right.\" display=\"block\"><mrow><msup><mi>x</mi><mo>*</mo></msup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>a</mi><mo>-</mo><mi>\u03bb</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><mi>with</mi></mpadded><mo>\u2062</mo><mi>a</mi></mrow><mo>&gt;</mo><mi>\u03bb</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>a</mi><mo>+</mo><mi>\u03bb</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><mi>with</mi></mpadded><mo>\u2062</mo><mi>a</mi></mrow><mo>&lt;</mo><mrow><mo>-</mo><mi>\u03bb</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded width=\"+3.3pt\"><mi>with</mi></mpadded><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2264</mo><mi>\u03bb</mi></mrow><mo>.</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere ${\\rm card}({\\mathbf u})$ denotes the number of non-zero\nentries of ${\\mathbf u}$, $\\|\\cdot\\|_{\\ell_2}$ denotes the\nEuclidean norm in ${\\mathbb R}^n$, and $\\beta>0$ is a given balancing\nparameter. We can solve problem~(\\ref{SP}) component-wise~(in each $u_i$) as follows. Notice that, given $\\mathbf{u} \\in \\mathbb{R}^n$, each entry $u_i$ of $\\mathbf{u}$ contributes 1 to ${\\rm card}(\\mathbf{u})$ if $u_i$ is non-zero, and contributes 0 if $u_i$ is zero.  Since we are minimizing $g({\\mathbf u}):={\\rm card}({\\mathbf u})+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2$, if $u_i$ is zero then the contribution to $g({\\mathbf u})$ depending on this $u_i$ is $\\frac{\\beta}{2}v_i^2$; otherwise, if $u_i$ is non-zero, then we should minimize $\\frac{\\beta}{2}(u_i - v_i)^2$ for $u_i \\in \\mathbb{R}\\setminus \\{0\\}$, which forces that $u_i = v_i$ and contributes 1 to $g({\\mathbf u})$ as the minimum value.  Therefore, the solution $\\mathbf{u}$ to problem (\\ref{SP}) is given component-wise by\n\n", "itemtype": "equation", "pos": 5926, "prevtext": "\n\nSince one and only one of the three cases (1) $a>\\lambda$, (2)\n$a<-\\lambda$, and (3) $|a|\\leq \\lambda$ holds, we obtain the\nuniqueness in general. With the uniqueness, it is straightforward\nto verify that each of\nthe three cases would imply the corresponding formula for $x^*$.\nThis completes the proof.\n\\end{proof}\n\n\\section{A sparse recovery problem} Recently, research in\ncompressive sensing leads to the recognition of the fact that the $\\ell_1$-norm\nof a vector is a good substitute for the count of the number of\nnon-zero entries of the vector in many minimization problems. In this section, we solve some simple minimization problems using the count of non-zero entries or the $\\ell_1$-norm.\nGiven a vector\n${\\mathbf v}\\in {\\mathbb R}^n$, we want to solve\n\n", "index": 29, "text": "\\begin{equation}\\label{SP}\n\\min_{{\\mathbf u} \\in \\mathbb{R}^n}[{\\rm card}({\\mathbf u})+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\mathbf{u}}\\in\\mathbb{R}^{n}}[{\\rm card}({\\mathbf{u}})+\\frac{\\beta}{2}%&#10;\\|{\\mathbf{u}}-{\\mathbf{v}}\\|_{\\ell_{2}}^{2}],\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc2e</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>n</mi></msup></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>card</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc2e</mi><mo>-</mo><mi>\ud835\udc2f</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nNext, we replace $card({\\mathbf u})$ by $\\|{\\mathbf u}\\|_{l_1}$ in (\\ref{SP}) and solve:\n\n", "itemtype": "equation", "pos": 7083, "prevtext": "\nwhere ${\\rm card}({\\mathbf u})$ denotes the number of non-zero\nentries of ${\\mathbf u}$, $\\|\\cdot\\|_{\\ell_2}$ denotes the\nEuclidean norm in ${\\mathbb R}^n$, and $\\beta>0$ is a given balancing\nparameter. We can solve problem~(\\ref{SP}) component-wise~(in each $u_i$) as follows. Notice that, given $\\mathbf{u} \\in \\mathbb{R}^n$, each entry $u_i$ of $\\mathbf{u}$ contributes 1 to ${\\rm card}(\\mathbf{u})$ if $u_i$ is non-zero, and contributes 0 if $u_i$ is zero.  Since we are minimizing $g({\\mathbf u}):={\\rm card}({\\mathbf u})+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2$, if $u_i$ is zero then the contribution to $g({\\mathbf u})$ depending on this $u_i$ is $\\frac{\\beta}{2}v_i^2$; otherwise, if $u_i$ is non-zero, then we should minimize $\\frac{\\beta}{2}(u_i - v_i)^2$ for $u_i \\in \\mathbb{R}\\setminus \\{0\\}$, which forces that $u_i = v_i$ and contributes 1 to $g({\\mathbf u})$ as the minimum value.  Therefore, the solution $\\mathbf{u}$ to problem (\\ref{SP}) is given component-wise by\n\n", "index": 31, "text": "$$\nu_i =\n\\begin{cases}\n0, & \\text{if }\\frac{\\beta}{2}(v_i)^2 \\le 1 \\\\\nv_i, & \\text{ otherwise.}\n\\end{cases}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"u_{i}=\\begin{cases}0,&amp;\\text{if }\\frac{\\beta}{2}(v_{i})^{2}\\leq 1\\\\&#10;v_{i},&amp;\\text{ otherwise.}\\end{cases}\" display=\"block\"><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mstyle displaystyle=\"false\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>\u2264</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise.</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere $\\|\\cdot \\|_{\\ell_1}$ denotes the $\\ell_1$ norm in ${\\mathbb\nR}^n$.\n\nUsing Theorem~\\ref{theorem 1}, we can solve (\\ref{P2}) component-wise as follows.\n\n\\begin{theorem}\\label{theorem 2} \\cite{yin-hale-zhang} { Let $\\beta > 0$ and $\\mathbf{v} \\in \\mathbb{R}^n$ be given and let \n", "itemtype": "equation", "pos": 7283, "prevtext": "\nNext, we replace $card({\\mathbf u})$ by $\\|{\\mathbf u}\\|_{l_1}$ in (\\ref{SP}) and solve:\n\n", "index": 33, "text": "\\begin{equation}\\label{P2}\n\\min_{{\\mathbf u}\\in \\mathbb{R}^n}[\\|{\\mathbf u}\\|_{\\ell_1}+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\mathbf{u}}\\in\\mathbb{R}^{n}}[\\|{\\mathbf{u}}\\|_{\\ell_{1}}+\\frac{\\beta}{%&#10;2}\\|{\\mathbf{u}}-{\\mathbf{v}}\\|_{\\ell_{2}}^{2}],\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc2e</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>n</mi></msup></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc2e</mi><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc2e</mi><mo>-</mo><mi>\ud835\udc2f</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nthen\n\n", "itemtype": "equation", "pos": 7723, "prevtext": "\nwhere $\\|\\cdot \\|_{\\ell_1}$ denotes the $\\ell_1$ norm in ${\\mathbb\nR}^n$.\n\nUsing Theorem~\\ref{theorem 1}, we can solve (\\ref{P2}) component-wise as follows.\n\n\\begin{theorem}\\label{theorem 2} \\cite{yin-hale-zhang} { Let $\\beta > 0$ and $\\mathbf{v} \\in \\mathbb{R}^n$ be given and let \n", "index": 35, "text": "$$ {\\mathbf\nu}^*=\\arg\\min_{{\\mathbf u}\\in \\mathbb{R}^n}[\\|{\\mathbf\nu}\\|_{\\ell_1}+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2],\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{u}}^{*}=\\arg\\min_{{\\mathbf{u}}\\in\\mathbb{R}^{n}}[\\|{\\mathbf{u}}\\|_{%&#10;\\ell_{1}}+\\frac{\\beta}{2}\\|{\\mathbf{u}}-{\\mathbf{v}}\\|_{\\ell_{2}}^{2}],\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udc2e</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc2e</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>n</mi></msup></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc2e</mi><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc2e</mi><mo>-</mo><mi>\ud835\udc2f</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere, $S_{1/\\beta}({\\mathbf v})$ denotes the vector\nwhose entries are obtained by applying the shrinkage function\n$S_{1/\\beta}(\\cdot)$ to the corresponding entries of ${\\mathbf\nv}$.}\n\\end{theorem}\n\n\\begin{proof}  If $u_i$ and $v_i$ denote the $i$th entry of the vectors $\\mathbf{u}$ and $\\mathbf{v}$, respectively, $i = 1,2,\\dots,n$, then we have,\n\\begin{eqnarray*}\n{\\mathbf\nu}^*&=&\\arg\\min_{{\\mathbf u}\\in \\mathbb{R}^n}[\\|{\\mathbf\nu}\\|_{\\ell_1}+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2] \\\\\n\n\n&=& \\arg\\min_{\\mathbf{u} \\in \\mathbb{R}^{n}} \\left[\\sum_{i=1}^n\\left( \\frac{1}{\\beta}|u_i| + \\frac{1}{2}(u_i -v_i)^2\\right)\\right].\n\\end{eqnarray*}\nSince the $i$-th term in the summation depends  only on $u_i$, the vector $\\mathbf{u}^*$ must have components $u_i^*$ satisfying\n\n", "itemtype": "equation", "pos": 7868, "prevtext": "\nthen\n\n", "index": 37, "text": "$${\\mathbf u}^*=S_{1/\\beta}({\\mathbf v}),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{u}}^{*}=S_{1/\\beta}({\\mathbf{v}}),\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udc2e</mi><mo>*</mo></msup><mo>=</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nfor $i = 1,2,\\dots,n$.  But by Theorem~\\ref{theorem 1}, the solution to each of these problems is given precisely by $S_{1/\\beta}(v_i)$.  This yields the result.\n\\end{proof}\n\n\n\\noindent\n{\\bf Remark.} The previous proof still works if we replace the vectors by\nmatrices and use the extension of the $\\ell_1$ and $\\ell_2$-norms to matrices {\\it by treating them as vectors}.  Thus by using the same argument we can easily show the following matrix version of the previous theorem.\n\n\n\\begin{theorem}\\label{theorem 3}~\\cite{yin-hale-zhang} \nLet $\\beta > 0$ and $\\mathbf{V} \\in \\mathbb{R}^{m\\times n}$ be given.  Then\n\n", "itemtype": "equation", "pos": 8696, "prevtext": "\nwhere, $S_{1/\\beta}({\\mathbf v})$ denotes the vector\nwhose entries are obtained by applying the shrinkage function\n$S_{1/\\beta}(\\cdot)$ to the corresponding entries of ${\\mathbf\nv}$.}\n\\end{theorem}\n\n\\begin{proof}  If $u_i$ and $v_i$ denote the $i$th entry of the vectors $\\mathbf{u}$ and $\\mathbf{v}$, respectively, $i = 1,2,\\dots,n$, then we have,\n\\begin{eqnarray*}\n{\\mathbf\nu}^*&=&\\arg\\min_{{\\mathbf u}\\in \\mathbb{R}^n}[\\|{\\mathbf\nu}\\|_{\\ell_1}+\\frac{\\beta}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2] \\\\\n\n\n&=& \\arg\\min_{\\mathbf{u} \\in \\mathbb{R}^{n}} \\left[\\sum_{i=1}^n\\left( \\frac{1}{\\beta}|u_i| + \\frac{1}{2}(u_i -v_i)^2\\right)\\right].\n\\end{eqnarray*}\nSince the $i$-th term in the summation depends  only on $u_i$, the vector $\\mathbf{u}^*$ must have components $u_i^*$ satisfying\n\n", "index": 39, "text": "$$ u_i^* = \\arg \\min_{u_i^* \\in \\mathbb{R}}[\\frac{1}{\\beta}|u_i| + \\frac{1}{2}(u_i - v_i)^2],$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"u_{i}^{*}=\\arg\\min_{u_{i}^{*}\\in\\mathbb{R}}[\\frac{1}{\\beta}|u_{i}|+\\frac{1}{2}%&#10;(u_{i}-v_{i})^{2}],\" display=\"block\"><mrow><mrow><msubsup><mi>u</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><msubsup><mi>u</mi><mi>i</mi><mo>*</mo></msubsup><mo>\u2208</mo><mi>\u211d</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mfrac><mn>1</mn><mi>\u03b2</mi></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>-</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere $S_{1/\\beta}(\\mathbf{V})$ is again defined component-wise.\n\\end{theorem}\\\\\nTheorem~\\ref{theorem 3} solves the problem of approximating a given\nmatrix by a sparse matrix by using the shrinkage function.\n\n\\section{Approximation by low rank matrices}\nThe sparse approximation as given by Theorem~\\ref{theorem 3} has many applications such as data compression and dimension reduction. In these areas, one may also be interested in finding matrices of low rank. For example,\ngiven a matrix ${\\mathbf A}\\in {\\mathbb R}^{m\\times n}$, we want\nto solve the following approximation problem:\n\n", "itemtype": "equation", "pos": 9405, "prevtext": "\nfor $i = 1,2,\\dots,n$.  But by Theorem~\\ref{theorem 1}, the solution to each of these problems is given precisely by $S_{1/\\beta}(v_i)$.  This yields the result.\n\\end{proof}\n\n\n\\noindent\n{\\bf Remark.} The previous proof still works if we replace the vectors by\nmatrices and use the extension of the $\\ell_1$ and $\\ell_2$-norms to matrices {\\it by treating them as vectors}.  Thus by using the same argument we can easily show the following matrix version of the previous theorem.\n\n\n\\begin{theorem}\\label{theorem 3}~\\cite{yin-hale-zhang} \nLet $\\beta > 0$ and $\\mathbf{V} \\in \\mathbb{R}^{m\\times n}$ be given.  Then\n\n", "index": 41, "text": "$$S_{1/\\beta}(\\mathbf{V}) = \\arg \\min_{\\mathbf{U} \\in \\mathbb{R}^{m\\times n}}[\\|\\mathbf{U}\\|_{\\ell_1} + \\frac{\\beta}{2}\\|\\mathbf{U} - \\mathbf{V}\\|^2_{\\ell_2}],$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"S_{1/\\beta}(\\mathbf{V})=\\arg\\min_{\\mathbf{U}\\in\\mathbb{R}^{m\\times n}}[\\|%&#10;\\mathbf{U}\\|_{\\ell_{1}}+\\frac{\\beta}{2}\\|\\mathbf{U}-\\mathbf{V}\\|^{2}_{\\ell_{2}%&#10;}],\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc15</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc14</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc14</mi><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc14</mi><mo>-</mo><mi>\ud835\udc15</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm of matrices (which\nturns out to be equivalent to the vector $l_2$ norm if we treat a matrix as a\nvector - see more discussion on the matrix norms in subsection 4.1).\n\nThis is a harder problem since rank$(X)$ is not a convex function. A convex relaxation (see, e.g.,  \\cite{fazel}) of\nthe problem is provided by replacing the term ${\\rm rank}({\\mathbf\nX})$ by the nuclear norm of ${\\mathbf X}$, $\\|{\\mathbf X}\\|_*$, (again, see subsection 4.1 for a discussion on the nuclear norm and its properties).  The problem then becomes:\n\n", "itemtype": "equation", "pos": 10154, "prevtext": "\nwhere $S_{1/\\beta}(\\mathbf{V})$ is again defined component-wise.\n\\end{theorem}\\\\\nTheorem~\\ref{theorem 3} solves the problem of approximating a given\nmatrix by a sparse matrix by using the shrinkage function.\n\n\\section{Approximation by low rank matrices}\nThe sparse approximation as given by Theorem~\\ref{theorem 3} has many applications such as data compression and dimension reduction. In these areas, one may also be interested in finding matrices of low rank. For example,\ngiven a matrix ${\\mathbf A}\\in {\\mathbb R}^{m\\times n}$, we want\nto solve the following approximation problem:\n\n", "index": 43, "text": "\\begin{equation*}\n\\min_{{\\mathbf X \\in \\mathbb{R}^{m\\times n}}}\\left[{\\rm rank}({\\mathbf X})+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf A}\\|_F^2\\right],\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\mathbf{X}\\in\\mathbb{R}^{m\\times n}}}\\left[{\\rm rank}({\\mathbf{X}})+%&#10;\\frac{\\beta}{2}\\|{\\mathbf{X}}-{\\mathbf{A}}\\|_{F}^{2}\\right],\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mi>rank</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mi>\ud835\udc00</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nThis problem again yields an explicit solution\n(\\cite{svt:cai-candes-shen,svt2}), but in these literatures, the formula is derived by using advanced tools from convex analysis\n(``subdifferentials'' to be more specific). Here,\nwe will show how we can obtain the solution\nby using simple ideas from the previous section.\n\\subsection{Singular value decomposition and matrix norms} It will be beneficial to\nrecall the various matrix norms. Many useful matrix norms can be defined in terms of the singular\nvalues of the matrices. We will deal with two of them: the nuclear norm $\\|\\cdot \\|_{*}$ and the Frobenius norm $\\|\\cdot \\|_F$.\n\nLet ${\\mathbf A}\\in{\\mathbb R}^{m\\times n}$ and ${\\mathbf A}={\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ be a singular value decomposition~(SVD)\nof ${\\mathbf A}$ with ${\\mathbf U}\\in {\\mathbb R}^{m\\times m}$ and ${\\mathbf V}\\in {\\mathbb R}^{n\\times n}$ being two\northogonal matrices (that is, ${\\mathbf U}^{-1}={\\mathbf U}^{T}$ and ${\\mathbf V}^{-1}={\\mathbf V}^{T}$)\nand $\\tilde{\\mathbf A}={\\rm diag}(\\sigma_1({\\mathbf A})\\;\\sigma_2({\\mathbf A})\\cdots \\sigma_{\\min \\{m,n\\}}({\\mathbf A}))$ being a diagonal matrix\nsuch that $\\sigma_1({\\mathbf A})\\geq \\sigma_2({\\mathbf A})\\geq \\cdots \\geq \\sigma_{\\min \\{m,n\\}}({\\mathbf A})\\geq 0$. The $\\sigma_i({\\mathbf A})$'s are\ncalled the singular values of ${\\mathbf A}$.\nIt is known (\\cite{linearAlgebra}) that every matrix in ${\\mathbb R}^{m\\times n}$ has a SVD and that SVD of a matrix is not\nunique. \n\n\nThen the nuclear norm of ${\\mathbf A}$ is given by\n\n", "itemtype": "equation", "pos": 10892, "prevtext": "\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm of matrices (which\nturns out to be equivalent to the vector $l_2$ norm if we treat a matrix as a\nvector - see more discussion on the matrix norms in subsection 4.1).\n\nThis is a harder problem since rank$(X)$ is not a convex function. A convex relaxation (see, e.g.,  \\cite{fazel}) of\nthe problem is provided by replacing the term ${\\rm rank}({\\mathbf\nX})$ by the nuclear norm of ${\\mathbf X}$, $\\|{\\mathbf X}\\|_*$, (again, see subsection 4.1 for a discussion on the nuclear norm and its properties).  The problem then becomes:\n\n", "index": 45, "text": "\\begin{equation}\\label{NN}\n\\min_{{\\mathbf X}\\in \\mathbb{R}^{m\\times n}}\\left[\\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf A}\\|_F^2\\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\mathbf{X}}\\in\\mathbb{R}^{m\\times n}}\\left[\\|{\\mathbf{X}}\\|_{*}+\\frac{%&#10;\\beta}{2}\\|{\\mathbf{X}}-{\\mathbf{A}}\\|_{F}^{2}\\right].\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mo>[</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc17</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mi>\ud835\udc00</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nand we can also define the Frobenius norm  of ${\\mathbf A}$ as\n\n", "itemtype": "equation", "pos": 12585, "prevtext": "\nThis problem again yields an explicit solution\n(\\cite{svt:cai-candes-shen,svt2}), but in these literatures, the formula is derived by using advanced tools from convex analysis\n(``subdifferentials'' to be more specific). Here,\nwe will show how we can obtain the solution\nby using simple ideas from the previous section.\n\\subsection{Singular value decomposition and matrix norms} It will be beneficial to\nrecall the various matrix norms. Many useful matrix norms can be defined in terms of the singular\nvalues of the matrices. We will deal with two of them: the nuclear norm $\\|\\cdot \\|_{*}$ and the Frobenius norm $\\|\\cdot \\|_F$.\n\nLet ${\\mathbf A}\\in{\\mathbb R}^{m\\times n}$ and ${\\mathbf A}={\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ be a singular value decomposition~(SVD)\nof ${\\mathbf A}$ with ${\\mathbf U}\\in {\\mathbb R}^{m\\times m}$ and ${\\mathbf V}\\in {\\mathbb R}^{n\\times n}$ being two\northogonal matrices (that is, ${\\mathbf U}^{-1}={\\mathbf U}^{T}$ and ${\\mathbf V}^{-1}={\\mathbf V}^{T}$)\nand $\\tilde{\\mathbf A}={\\rm diag}(\\sigma_1({\\mathbf A})\\;\\sigma_2({\\mathbf A})\\cdots \\sigma_{\\min \\{m,n\\}}({\\mathbf A}))$ being a diagonal matrix\nsuch that $\\sigma_1({\\mathbf A})\\geq \\sigma_2({\\mathbf A})\\geq \\cdots \\geq \\sigma_{\\min \\{m,n\\}}({\\mathbf A})\\geq 0$. The $\\sigma_i({\\mathbf A})$'s are\ncalled the singular values of ${\\mathbf A}$.\nIt is known (\\cite{linearAlgebra}) that every matrix in ${\\mathbb R}^{m\\times n}$ has a SVD and that SVD of a matrix is not\nunique. \n\n\nThen the nuclear norm of ${\\mathbf A}$ is given by\n\n", "index": 47, "text": "$$\n\\|{\\mathbf A}\\|_*=\\sum_{i=1}^{\\min \\{m,n\\}}\\sigma_i({\\mathbf A}),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\|{\\mathbf{A}}\\|_{*}=\\sum_{i=1}^{\\min\\{m,n\\}}\\sigma_{i}({\\mathbf{A}}),\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nThis norm turns out to be the same as the $\\ell_2$ norm of ${\\mathbf A}$, treated as a vector in $\\mathbb{R}^{mn\\times 1}$.\n\nThis is because the nonzero singular values $\\sigma_i({\\mathbf A})$'s are exactly the square root of the nonzero eigenvalues of ${\\mathbf A}{\\mathbf A}^T$ or ${\\mathbf A}^T{\\mathbf A}$. \nSo, \n", "itemtype": "equation", "pos": 12719, "prevtext": "\nand we can also define the Frobenius norm  of ${\\mathbf A}$ as\n\n", "index": 49, "text": "$$\n\\|{\\mathbf A}\\|_F=\\left(\\sum_{i=1}^{\\min \\{m,n\\}}(\\sigma_i({\\mathbf A}))^2\\right)^{1/2}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\|{\\mathbf{A}}\\|_{F}=\\left(\\sum_{i=1}^{\\min\\{m,n\\}}(\\sigma_{i}({\\mathbf{A}}))^%&#10;{2}\\right)^{1/2}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nHere we have used ${\\rm trace}(\\cdot)$ to denote the trace of a matrix (which is equal to the sum of all\ndiagonal entries of the matrix).\n\n\n\nWe will need the following simple fact about the nuclear norms of a matrix and that of its diagonal: Let $D({\\mathbf A})$ denote the diagonal matrix using the diagonal of ${\\mathbf A}$. We have\n\n", "itemtype": "equation", "pos": 13129, "prevtext": "\nThis norm turns out to be the same as the $\\ell_2$ norm of ${\\mathbf A}$, treated as a vector in $\\mathbb{R}^{mn\\times 1}$.\n\nThis is because the nonzero singular values $\\sigma_i({\\mathbf A})$'s are exactly the square root of the nonzero eigenvalues of ${\\mathbf A}{\\mathbf A}^T$ or ${\\mathbf A}^T{\\mathbf A}$. \nSo, \n", "index": 51, "text": "$$\n\\|{\\mathbf A}\\|_{l_2}^2=\\sum_{i=1}^m\\sum_{j=1}^n(a_{ij})^2={\\rm trace}({\\mathbf A}{\\mathbf A}^{T})\n\n\n\n\n=\\sum_{i=1}^{\\min \\{m,n\\}}(\\sigma_i({\\mathbf A}))^2.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\|{\\mathbf{A}}\\|_{l_{2}}^{2}=\\sum_{i=1}^{m}\\sum_{j=1}^{n}(a_{ij})^{2}={\\rm&#10;trace%&#10;}({\\mathbf{A}}{\\mathbf{A}}^{T})\\par&#10;\\par&#10;\\par&#10;\\par&#10;=\\sum_{i=1}^{\\min\\{m,n\\}}(%&#10;\\sigma_{i}({\\mathbf{A}}))^{2}.\" display=\"block\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><msub><mi>l</mi><mn>2</mn></msub><mn>2</mn></msubsup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>=</mo><mrow><mi>trace</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc00\ud835\udc00</mi><mi>T</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nThis inequality can be verified by using a SVD of ${\\mathbf A}={\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ as follows.\nWrite ${\\mathbf U}=(u_{ij})$, ${\\mathbf V}=(v_{ij})$, and $t=\\min\\{m,n\\}$. Then\n\n", "itemtype": "equation", "pos": 13625, "prevtext": "\nHere we have used ${\\rm trace}(\\cdot)$ to denote the trace of a matrix (which is equal to the sum of all\ndiagonal entries of the matrix).\n\n\n\nWe will need the following simple fact about the nuclear norms of a matrix and that of its diagonal: Let $D({\\mathbf A})$ denote the diagonal matrix using the diagonal of ${\\mathbf A}$. We have\n\n", "index": 53, "text": "\\begin{equation}\\label{diag}\n\\|D({\\mathbf A})\\|_*\\leq \\|{\\mathbf A}\\|_*.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\|D({\\mathbf{A}})\\|_{*}\\leq\\|{\\mathbf{A}}\\|_{*}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>\u2264</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 13913, "prevtext": "\nThis inequality can be verified by using a SVD of ${\\mathbf A}={\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ as follows.\nWrite ${\\mathbf U}=(u_{ij})$, ${\\mathbf V}=(v_{ij})$, and $t=\\min\\{m,n\\}$. Then\n\n", "index": 55, "text": "$$\n\\|D({\\mathbf A})\\|_*=\\|D({\\mathbf U}\\tilde{{\\mathbf A}}{\\mathbf V}^{T})\\|_*\n=\\sum_{i=1}^{m}\\left|\\sum_{j=1}^{t}\\sigma_j({\\mathbf A})u_{ij}v_{ij}\\right|\n\\leq \\sum_{j=1}^{t}\\sigma_j({\\mathbf A})\\sum_{i=1}^m|u_{ij}v_{ij}|\n~~~$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\|D({\\mathbf{A}})\\|_{*}=\\|D({\\mathbf{U}}\\tilde{{\\mathbf{A}}}{\\mathbf{V}}^{T})%&#10;\\|_{*}=\\sum_{i=1}^{m}\\left|\\sum_{j=1}^{t}\\sigma_{j}({\\mathbf{A}})u_{ij}v_{ij}%&#10;\\right|\\leq\\sum_{j=1}^{t}\\sigma_{j}({\\mathbf{A}})\\sum_{i=1}^{m}|u_{ij}v_{ij}|~%&#10;{}~{}~{}\" display=\"block\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc14</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mi>\ud835\udc15</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo>|</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msub><mi>\u03c3</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>|</mo></mrow></mrow><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msub><mi>\u03c3</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo rspace=\"12.4pt\" stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere we have used the Cauchy-Schwarz inequality in obtaining the second inequality, and the orthogonality of ${\\mathbf U}$ and ${\\mathbf V}$ (so that $\\sum_{i=1}^m|u_{ij}|^2\\leq 1$ and $\\sum_{i=1}^m|v_{ij}|^2\\leq 1$) in the last inequality.\n\nWe will also use the fact that for any orthogonal matrices ${\\mathbf O}$ and ${\\mathbf R}$, $\\mathbf{OAR}$ and ${\\mathbf A}$ have the same singular values, and therefore their Frobenius norms and nuclear norms are same:\n\n", "itemtype": "equation", "pos": 14141, "prevtext": "\n\n", "index": 57, "text": "$$\n\\leq \\sum_{j=1}^{t}\\sigma_j({\\mathbf A})\\cdot \\left(\\sum_{i=1}^{m}|u_{ij}|^2\\right)^{1/2}\\left(\\sum_{i=1}^{m}|v_{ij}|^2\\right)^{1/2}\n\\leq\n\\sum_{j=1}^{t}\\sigma_j({\\mathbf A})=\\|{\\mathbf A}\\|_*,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\leq\\sum_{j=1}^{t}\\sigma_{j}({\\mathbf{A}})\\cdot\\left(\\sum_{i=1}^{m}|u_{ij}|^{2%&#10;}\\right)^{1/2}\\left(\\sum_{i=1}^{m}|v_{ij}|^{2}\\right)^{1/2}\\leq\\sum_{j=1}^{t}%&#10;\\sigma_{j}({\\mathbf{A}})=\\|{\\mathbf{A}}\\|_{*},\" display=\"block\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mrow><mrow><msub><mi>\u03c3</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mrow><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mrow><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msub><mi>\u03c3</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nThis is known as the unitary invariance of the Frobenius norm and nuclear norm. \n\n\\subsection{Solution to (\\ref{NN}) via problem (\\ref{P})} We are ready to show how problem (\\ref{NN}) is problem (\\ref{P}) in disguise.  Given $\\beta> 0$, using the unitary invariance of the Frobenius norm and the nuclear norm, we have\n\\begin{eqnarray*}\n\\min_{{\\mathbf X}}[ \\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf A}\\|_F^2]&=&\\min_{{\\mathbf X}}[\\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T} \\|_F^2]\\\\\n\n\n\n\n\n&=&\\min_{{\\mathbf X}}[\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}-\\tilde{\\mathbf A}\\|_F^2].\n\\end{eqnarray*}\nIt can be seen from the last expression that the minimum occurs when\n$\\tilde{\\mathbf X}:={\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}$ is diagonal since both terms in that expression get no larger when\n$\\tilde{\\mathbf X}$ is replaced by its diagonal matrix (with the help of (\\ref{diag})). So, the matrix\n$\n{\\mathbf E}=({e}_{ij}):=\\tilde{\\mathbf X}-\\tilde{\\mathbf A}\n$\nis a diagonal matrix. Thus, \n", "itemtype": "equation", "pos": 14803, "prevtext": "\nwhere we have used the Cauchy-Schwarz inequality in obtaining the second inequality, and the orthogonality of ${\\mathbf U}$ and ${\\mathbf V}$ (so that $\\sum_{i=1}^m|u_{ij}|^2\\leq 1$ and $\\sum_{i=1}^m|v_{ij}|^2\\leq 1$) in the last inequality.\n\nWe will also use the fact that for any orthogonal matrices ${\\mathbf O}$ and ${\\mathbf R}$, $\\mathbf{OAR}$ and ${\\mathbf A}$ have the same singular values, and therefore their Frobenius norms and nuclear norms are same:\n\n", "index": 59, "text": "$$\\|\\mathbf{OAR}\\|_F=\\|\\mathbf{A}\\|_F\\;\\text{and}\\;\\|\\mathbf{OAR}\\|_*=\\|\\mathbf{A}\\|_*.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\|\\mathbf{OAR}\\|_{F}=\\|\\mathbf{A}\\|_{F}\\;\\text{and}\\;\\|\\mathbf{OAR}\\|_{*}=\\|%&#10;\\mathbf{A}\\|_{*}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc0e\ud835\udc00\ud835\udc11</mi><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>=</mo><mrow><mpadded width=\"+2.8pt\"><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mi>F</mi></msub></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>and</mtext></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc0e\ud835\udc00\ud835\udc11</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc00</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhich yields a SVD of ${\\mathbf X}$ (using the same matrices ${\\mathbf U}$ and ${\\mathbf V}$ as in a SVD of ${\\mathbf A}$~!). Then,\n\n", "itemtype": "equation", "pos": 15995, "prevtext": "\nThis is known as the unitary invariance of the Frobenius norm and nuclear norm. \n\n\\subsection{Solution to (\\ref{NN}) via problem (\\ref{P})} We are ready to show how problem (\\ref{NN}) is problem (\\ref{P}) in disguise.  Given $\\beta> 0$, using the unitary invariance of the Frobenius norm and the nuclear norm, we have\n\\begin{eqnarray*}\n\\min_{{\\mathbf X}}[ \\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf A}\\|_F^2]&=&\\min_{{\\mathbf X}}[\\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T} \\|_F^2]\\\\\n\n\n\n\n\n&=&\\min_{{\\mathbf X}}[\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}-\\tilde{\\mathbf A}\\|_F^2].\n\\end{eqnarray*}\nIt can be seen from the last expression that the minimum occurs when\n$\\tilde{\\mathbf X}:={\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}$ is diagonal since both terms in that expression get no larger when\n$\\tilde{\\mathbf X}$ is replaced by its diagonal matrix (with the help of (\\ref{diag})). So, the matrix\n$\n{\\mathbf E}=({e}_{ij}):=\\tilde{\\mathbf X}-\\tilde{\\mathbf A}\n$\nis a diagonal matrix. Thus, \n", "index": 61, "text": "$${\\mathbf X}={\\mathbf U}\\tilde{\\mathbf X}{\\mathbf V}^{T},\n ~{\\rm with}~\\tilde{\\mathbf X}=\\tilde{\\mathbf A}+{\\mathbf E},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}={\\mathbf{U}}\\tilde{\\mathbf{X}}{\\mathbf{V}}^{T},~{}{\\rm with}~{}%&#10;\\tilde{\\mathbf{X}}=\\tilde{\\mathbf{A}}+{\\mathbf{E}},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc17</mi><mo>=</mo><mrow><mi>\ud835\udc14</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mi>\ud835\udc15</mi><mi>T</mi></msup></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>with</mi></mpadded><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>=</mo><mrow><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover><mo>+</mo><mi>\ud835\udc04</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere ${\\rm ``diag\"}$ is the set of diagonal matrices in $\\mathbb{R}^{m\\times n}$. Above is an optimization problem like (\\ref{P}) (for vectors $(\\tilde{x}_{11},\\tilde{x}_{22},\\cdots)^T$\nas $\\tilde{\\mathbf X}$ varies) whose solution is given by\n\n\n", "itemtype": "equation", "pos": 16250, "prevtext": "\nwhich yields a SVD of ${\\mathbf X}$ (using the same matrices ${\\mathbf U}$ and ${\\mathbf V}$ as in a SVD of ${\\mathbf A}$~!). Then,\n\n", "index": 63, "text": "\\begin{align*}\n\\min_{{\\mathbf X}}[\\|{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf X}-{\\mathbf A}\\|_F^2]&=\n\\min_{\\tilde{\\mathbf X}\\in {\\rm diag}}[\\|\\tilde{\\mathbf X}\\|_*+\\frac{\\beta}{2}\\|\\tilde{\\mathbf X}-\\tilde{\\mathbf A}\\|_F^2]\\\\\n&=\\min_{\\tilde{\\mathbf X}\\in {\\rm diag}}[ \\sum_{i}\\tilde{x}_{ii}+\\frac{\\beta}{2}\\sum_{i}(\\tilde{x}_{ii}-\\sigma_i({\\mathbf A}))^2],\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{{\\mathbf{X}}}[\\|{\\mathbf{X}}\\|_{*}+\\frac{\\beta}{2}\\|{%&#10;\\mathbf{X}}-{\\mathbf{A}}\\|_{F}^{2}]\" display=\"inline\"><mrow><munder><mi>min</mi><mi>\ud835\udc17</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\ud835\udc17</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mi>\ud835\udc00</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\min_{\\tilde{\\mathbf{X}}\\in{\\rm diag}}[\\|\\tilde{\\mathbf{X}}\\|_{*%&#10;}+\\frac{\\beta}{2}\\|\\tilde{\\mathbf{X}}-\\tilde{\\mathbf{A}}\\|_{F}^{2}]\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><munder><mi>min</mi><mrow><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2208</mo><mi>diag</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mrow><mo>\u2225</mo><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\min_{\\tilde{\\mathbf{X}}\\in{\\rm diag}}[\\sum_{i}\\tilde{x}_{ii}+%&#10;\\frac{\\beta}{2}\\sum_{i}(\\tilde{x}_{ii}-\\sigma_{i}({\\mathbf{A}}))^{2}],\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><munder><mi>min</mi><mrow><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2208</mo><mi>diag</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>-</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nTo summarize, we have proven the following.\n\n\\begin{theorem} \\label{theorem 4}\\cite{svt:cai-candes-shen} {Suppose that ${\\mathbf A} \\in \\mathbb{R}^{m\\times n}$ and $\\beta > 0$ are given.\nThen the solution to the minimization problem (\\ref{NN})\n\nis given by $\\hat{\\mathbf X}={\\mathbf U}\\tilde{\\mathbf X}{\\mathbf V}^{T}$ where the diagonal matrix $\\tilde{\\mathbf{X}}$ has diagonal entries\n\n", "itemtype": "equation", "pos": 16867, "prevtext": "\nwhere ${\\rm ``diag\"}$ is the set of diagonal matrices in $\\mathbb{R}^{m\\times n}$. Above is an optimization problem like (\\ref{P}) (for vectors $(\\tilde{x}_{11},\\tilde{x}_{22},\\cdots)^T$\nas $\\tilde{\\mathbf X}$ varies) whose solution is given by\n\n\n", "index": 65, "text": "$$\n\\tilde{x}_{ii}=S_{{1}/{\\beta}}(\\sigma_i({\\mathbf A})),~i=1,2,\\cdots.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}_{ii}=S_{{1}/{\\beta}}(\\sigma_{i}({\\mathbf{A}})),~{}i=1,2,\\cdots.\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u22ef</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhere ${\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ is a SVD of ${\\mathbf A}$.\n}\n\\end{theorem}\n\n\\noindent {\\bf Remark.}\n{\\bf 1.} A recent proof of this\ntheorem is given by Cai, Candes, and Shen in\n\\cite{svt:cai-candes-shen} where they give an advanced\nverification of the result.  Our proof given above has the advantage that it is elementary and allows the reader to ``discover'' the result.\\\\\n{\\bf 2.} There are many earlier discoveries of related results\n(\\cite{strang}) where rank$({\\mathbf X})$ is used instead of the\nnuclear norm $\\|{\\mathbf X}\\|_*$. We will examine one\nsuch variant in the next section.\\\\\n{\\bf 3.} One key ingredient in the above discussion is the unitary\ninvariance of the norms $\\|\\cdot\\|_*$ and $\\|\\cdot\\|_F$. It was von Neumann (see, e.g., \\cite{fazel})\nwho was among the first to study the family of all unitarily invariant\nmatrix norms in matrix approximation, $\\|\\cdot\\|_F$ being one of them.\\\\\n{\\bf 4.} A closely related (but harder) problem is {\\it compressive sensing} (\\cite{candes-romberg-tao,candes-tao2010}). Readers are strongly recommended to the recently survey by Bryan and Leise (\\cite{siam}).\n\n\n\\section{A variation} More problems can be solved by applying similar ideas. For example, let us consider a\nvariant of a well-known result of Schmidt~(see, e.g.,\n\\cite[Section 5]{strang}), replacing the rank by the nuclear\nnorm: For a fixed positive number $\\tau$, consider\n\n", "itemtype": "equation", "pos": 17328, "prevtext": "\nTo summarize, we have proven the following.\n\n\\begin{theorem} \\label{theorem 4}\\cite{svt:cai-candes-shen} {Suppose that ${\\mathbf A} \\in \\mathbb{R}^{m\\times n}$ and $\\beta > 0$ are given.\nThen the solution to the minimization problem (\\ref{NN})\n\nis given by $\\hat{\\mathbf X}={\\mathbf U}\\tilde{\\mathbf X}{\\mathbf V}^{T}$ where the diagonal matrix $\\tilde{\\mathbf{X}}$ has diagonal entries\n\n", "index": 67, "text": "$$\\tilde{x}_{ii} = S_{{1}/{\\beta}}(\\sigma_i({\\mathbf{A}})), ~i=1,2,\\dots,\\min\\{m,n\\}, $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\tilde{x}_{ii}=S_{{1}/{\\beta}}(\\sigma_{i}({\\mathbf{A}})),~{}i=1,2,\\dots,\\min\\{%&#10;m,n\\},\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msub><mi>S</mi><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nUsing similar methods as in Section 3, this problem can be transformed into the following:\n\n", "itemtype": "equation", "pos": 18830, "prevtext": "\nwhere ${\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ is a SVD of ${\\mathbf A}$.\n}\n\\end{theorem}\n\n\\noindent {\\bf Remark.}\n{\\bf 1.} A recent proof of this\ntheorem is given by Cai, Candes, and Shen in\n\\cite{svt:cai-candes-shen} where they give an advanced\nverification of the result.  Our proof given above has the advantage that it is elementary and allows the reader to ``discover'' the result.\\\\\n{\\bf 2.} There are many earlier discoveries of related results\n(\\cite{strang}) where rank$({\\mathbf X})$ is used instead of the\nnuclear norm $\\|{\\mathbf X}\\|_*$. We will examine one\nsuch variant in the next section.\\\\\n{\\bf 3.} One key ingredient in the above discussion is the unitary\ninvariance of the norms $\\|\\cdot\\|_*$ and $\\|\\cdot\\|_F$. It was von Neumann (see, e.g., \\cite{fazel})\nwho was among the first to study the family of all unitarily invariant\nmatrix norms in matrix approximation, $\\|\\cdot\\|_F$ being one of them.\\\\\n{\\bf 4.} A closely related (but harder) problem is {\\it compressive sensing} (\\cite{candes-romberg-tao,candes-tao2010}). Readers are strongly recommended to the recently survey by Bryan and Leise (\\cite{siam}).\n\n\n\\section{A variation} More problems can be solved by applying similar ideas. For example, let us consider a\nvariant of a well-known result of Schmidt~(see, e.g.,\n\\cite[Section 5]{strang}), replacing the rank by the nuclear\nnorm: For a fixed positive number $\\tau$, consider\n\n", "index": 69, "text": "\\begin{equation}\\label{BA}\n\\min_{\\mathbf X\\in \\mathbb{R}^{m\\times n}} \\|{\\mathbf X}-{\\mathbf A}\\|_F\n~~{\\rm subject~to}~~\\|{\\mathbf X}\\|_*\\leq \\tau.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{X}\\in\\mathbb{R}^{m\\times n}}\\|{\\mathbf{X}}-{\\mathbf{A}}\\|_{F}~{}%&#10;~{}{\\rm subject~{}to}~{}~{}\\|{\\mathbf{X}}\\|_{*}\\leq\\tau.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2062</mo><mpadded width=\"+6.6pt\"><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mi>\ud835\udc00</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>to</mi></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc17</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>\u2264</mo><mi>\u03c4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nNote that,~(\\ref{BAS}) is related to a Lasso problem~\\cite{soft-threshold2,Gen_lasso,LASSO_dual}. But unlike a LASSO problem, no special assumption~(like 0 mean) is made on $\\mathbf{v}$ in~(\\ref{BAS}). As in~\\cite{soft-threshold2}, one can form a {\\it Lagrangian} of~(\\ref{BAS}) and solve:\n\n", "itemtype": "equation", "pos": 19084, "prevtext": "\nUsing similar methods as in Section 3, this problem can be transformed into the following:\n\n", "index": 71, "text": "\\begin{equation}\\label{BAS}\n\\min_{\\mathbf u\\in \\mathbb{R}^{\\min\\{m,n\\}}} \\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}\n~~{\\rm subject~to}~~\\|{\\mathbf u}\\|_{\\ell_1}\\leq \\tau.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{u}\\in\\mathbb{R}^{\\min\\{m,n\\}}}\\|{\\mathbf{u}}-{\\mathbf{v}}\\|_{%&#10;\\ell_{2}}~{}~{}{\\rm subject~{}to}~{}~{}\\|{\\mathbf{u}}\\|_{\\ell_{1}}\\leq\\tau.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc2e</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></msup></mrow></munder><mo>\u2062</mo><mpadded width=\"+6.6pt\"><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc2e</mi><mo>-</mo><mi>\ud835\udc2f</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub></msub></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>to</mi></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc2e</mi><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub></mrow><mo>\u2264</mo><mi>\u03c4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhich has a solution ${\\mathbf u^*}=S_{\\lambda}({\\mathbf v})$ according to Theorem~\\ref{theorem 2}.~(The reason for us to use $\\lambda$ instead of $\\beta$ in~(\\ref{BAS}) is nonessential: it is only for indicating the similarity with LASSO formulation.)\nWe now sketch the derivation of converting~(\\ref{BA}) to~(\\ref{BAS}):\nAs before, let ${\\mathbf A} = {\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ be a SVD of ${\\mathbf A}$. Then,\n\n", "itemtype": "equation", "pos": 19554, "prevtext": "\nNote that,~(\\ref{BAS}) is related to a Lasso problem~\\cite{soft-threshold2,Gen_lasso,LASSO_dual}. But unlike a LASSO problem, no special assumption~(like 0 mean) is made on $\\mathbf{v}$ in~(\\ref{BAS}). As in~\\cite{soft-threshold2}, one can form a {\\it Lagrangian} of~(\\ref{BAS}) and solve:\n\n", "index": 73, "text": "\\begin{equation}\\label{Lagrange BAS1}\n{\\mathbf u^*}=\\arg\\min_{\\mathbf u\\in \\mathbb{R}^{\\min\\{m,n\\}}} \\{\\frac{1}{2}\\|{\\mathbf u}-{\\mathbf v}\\|_{\\ell_2}^2+\\lambda\\|{\\mathbf u}\\|_{\\ell_1}\\},\\;\\;{\\rm with}\\;\\|S_{\\lambda}({\\mathbf v})\\|_{\\ell_1}=\\tau,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{u}^{*}}=\\arg\\min_{\\mathbf{u}\\in\\mathbb{R}^{\\min\\{m,n\\}}}\\{\\frac{1}{2}%&#10;\\|{\\mathbf{u}}-{\\mathbf{v}}\\|_{\\ell_{2}}^{2}+\\lambda\\|{\\mathbf{u}}\\|_{\\ell_{1}%&#10;}\\},\\;\\;{\\rm with}\\;\\|S_{\\lambda}({\\mathbf{v}})\\|_{\\ell_{1}}=\\tau,\" display=\"block\"><mrow><mrow><mrow><msup><mi>\ud835\udc2e</mi><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc2e</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc2e</mi><mo>-</mo><mi>\ud835\udc2f</mi></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msub><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc2e</mi><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo rspace=\"8.1pt\">,</mo><mrow><mrow><mpadded width=\"+2.8pt\"><mi>with</mi></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msub><mi>S</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mn>1</mn></msub></msub></mrow><mo>=</mo><mi>\u03c4</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": " Note that $\\|{\\mathbf X}\\|_*=\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}\\|_*$, so (\\ref{BA}) can be written as\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhich has a solution ${\\mathbf u^*}=S_{\\lambda}({\\mathbf v})$ according to Theorem~\\ref{theorem 2}.~(The reason for us to use $\\lambda$ instead of $\\beta$ in~(\\ref{BAS}) is nonessential: it is only for indicating the similarity with LASSO formulation.)\nWe now sketch the derivation of converting~(\\ref{BA}) to~(\\ref{BAS}):\nAs before, let ${\\mathbf A} = {\\mathbf U}\\tilde{\\mathbf A}{\\mathbf V}^{T}$ be a SVD of ${\\mathbf A}$. Then,\n\n", "index": 75, "text": "$$\\min_{\\mathbf X\\in \\mathbb{R}^{m\\times n}}\\|\\mathbf{X - A}\\|_F = \\min_{\\mathbf{X}\\in \\mathbb{R}^{m\\times n}}\\|\\mathbf{U^{T}XV} - \\tilde{\\mathbf A} \\|_F.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{X}\\in\\mathbb{R}^{m\\times n}}\\|\\mathbf{X-A}\\|_{F}=\\min_{\\mathbf{X%&#10;}\\in\\mathbb{R}^{m\\times n}}\\|\\mathbf{U^{T}XV}-\\tilde{\\mathbf{A}}\\|_{F}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mi>\ud835\udc00</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>\ud835\udc14</mi><mi>\ud835\udc13</mi></msup><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc15</mi></mrow><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nwhich, by using (\\ref{diag}), can be further transformed to\n\n", "itemtype": "equation", "pos": 20511, "prevtext": " Note that $\\|{\\mathbf X}\\|_*=\\|{\\mathbf U}^{T}{\\mathbf X}{\\mathbf V}\\|_*$, so (\\ref{BA}) can be written as\n\n", "index": 77, "text": "\\begin{equation*}\n\\min_{\\mathbf X\\in \\mathbb{R}^{m\\times n}} \\|{\\mathbf X}-\\tilde{\\mathbf A}\\|_F\n~~{\\rm subject~to}~~\\|{\\mathbf X}\\|_*\\leq \\tau,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{X}\\in\\mathbb{R}^{m\\times n}}\\|{\\mathbf{X}}-\\tilde{\\mathbf{A}}\\|_%&#10;{F}~{}~{}{\\rm subject~{}to}~{}~{}\\|{\\mathbf{X}}\\|_{*}\\leq\\tau,\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2062</mo><mpadded width=\"+6.6pt\"><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>to</mi></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc17</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>\u2264</mo><mi>\u03c4</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\nNext, if we let $\\mathbf{u}$ and $\\mathbf{v}$ be two vectors in $\\mathbb{R}^{\\min\\{m, n\\}}$ consisting of the diagonal elements of $\\mathbf{{X}}$ and ${\\tilde{\\mathbf A}}$, respectively, then\n(\\ref{BAd}) is (\\ref{BAS}). Thus we have established the following result. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{theorem 5} With the notations above,  the solution to problem (\\ref{BA}) is\n\tgiven by\n\t\n", "itemtype": "equation", "pos": 20732, "prevtext": "\nwhich, by using (\\ref{diag}), can be further transformed to\n\n", "index": 79, "text": "\\begin{equation}\\label{BAd}\n\\min_{\\mathbf X\\in \\mathbb{R}^{m\\times n}} \\|{\\mathbf X}- \\tilde{\\mathbf A}\\|_F\n~~{\\rm subject~to}~~{\\mathbf X} ~{\\rm being}~{\\rm diagonal}~ {\\rm and}~\\|{\\mathbf X}\\|_*\\leq \\tau.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{X}\\in\\mathbb{R}^{m\\times n}}\\|{\\mathbf{X}}-\\tilde{\\mathbf{A}}\\|_%&#10;{F}~{}~{}{\\rm subject~{}to}~{}~{}{\\mathbf{X}}~{}{\\rm being}~{}{\\rm diagonal}~{%&#10;}{\\rm and}~{}\\|{\\mathbf{X}}\\|_{*}\\leq\\tau.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc17</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow></munder><mo>\u2062</mo><mpadded width=\"+6.6pt\"><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mpadded width=\"+6.6pt\"><mi>to</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>\ud835\udc17</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>being</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>diagonal</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>and</mi></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc17</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>\u2264</mo><mi>\u03c4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07600.tex", "nexttext": "\n\tfor some $\\lambda$ such that $\\|S_{\\lambda}(\\tilde{\\mathbf A})\\|_{\\ell_1}=\\tau.$\n\\end{theorem}\\\\\n\\\\\n\n\\noindent\n{\\bf Acknowledgment.}\nThis work is partially supported\nby a CSUMS program of the National Science Foundation DMS-0803059. Toby Boas, Katie Mercier, and Eric Niederman contributed to this work as undergraduate students.\n\n\\begin{thebibliography}{100} \n\n\\bibitem{dirac}\n{\\sc R. Bracewell},~The Impulse Symbol, {\\em The Fourier Transform and Its Applications}~(3rd ed., 2000), Ch. 5, McGraw-Hill, 74--104.\n\\bibitem{soft-threshold:donoho-johnstone}\n\n{\\sc D. L. Donoho and I. M. Johnstone}, {\\em Ideal spatial adaptation by wavelet shrinkage},\nBiometrika, 81~(1994) 425--455.\n\n\\bibitem{soft-threshold2}\n{\\sc R. Tibshirani}, {\\em Regression shrinkage and selection via the LASSO}, J. of the Royal statistical society, series B, 58~(1996) 267--288.\n\n\\bibitem{svt:cai-candes-shen}\n{\\sc J. Cai, E. J. Candes, and Z. Shen}, {\\em A singular value thresholding algorithm for matrix completion}, SIAM\nJ. Optim., 20~(2010) 1956--1982.\n\n\\bibitem{lin-chen-ma}\n{\\sc Z. Lin, M. Chen, and Y. Ma}, {\\em The augmented Lagrange multiplier method for exact recovery of corrupted\nlow-rank matrices}, arXiv:1009.5055v2, March 9, 2011.\n\n\\bibitem{tao-yuan}\n{\\sc M. Tao and X. Yuan}, {\\em Recovering low-rank and sparse components of matrices from incomplete and\nnoisy observations}, SIAM J. Optim., 21~(2011) 57--81.\n\n\\bibitem{yuan-yang}\n{\\sc X. Yuan and J. Yang}, {\\em Sparse and low-rank matrix decomposition via alternating direction methods},\nTechnical report (available from http://www.optimization-online.org/DBFILE/2009/11/2447.pdf), Dept. of Mathematics, Hong Kong Baptist University, 2009.\n\n\\bibitem{yin-hale-zhang}\n{\\sc W. Yin, E. Hale, and Y. Zhang}, {\\em  Fixed-point continuation for $l_1$-minimization: methodology and\nconvergence}, SIAM J. Optim., 19~(2008) 1107--1130.\n\n\\bibitem{fazel}\n{\\sc M. Fazel}, {\\em Matrix Rank Minimization with Applications}, Ph.D.\ndissertation, Department of Electrical Engineering, Stanford University, 2002.\n\n\\bibitem{svt2}\n{\\sc S. Q. Ma, D. Goldfarb and L. F. Chen}, {\\em Fixed point and Bregman iterative methods for\nmatrix rank minimization}, Math. Prog. Ser. A, (2009)\nDOI 10.1007/s10107-009-0306-5.\n\n\\bibitem{linearAlgebra}\n{\\sc G. W. Strang}, {\\em Introduction to Linear Algebra}, 3rd ed., Wellesley-Cambridge Press,  1998.\n\n\\bibitem{strang}\n{\\sc G. W. Stewart}, {\\em On the early history of the singular value decomposition}, SIAM Review, 35~(1993)\n551--566.\n\n\\bibitem{candes-romberg-tao}\n{\\sc E. J. Candes, J. Romberg, and T. Tao}, {\\em Robust uncertainty principles: Exact signal reconstruction from\nhighly incomplete frequency information}, IEEE Transactions on Information Theory, 52~(2006) 489--509.\n\n\\bibitem{candes-tao2010}\n{\\sc E. J. Candes and T. Tao}, {\\em The power of convex relaxation: Near-optimal matrix completion}, IEEE\nTransactions on Information Theory, 56~(2010) 2053--2080.\n\n\\bibitem{siam} \n{\\sc K. Bryan and T. Leise},~{\\em Making do with less: an introduction to\ncompressed sensing}, SIAM Review, 55~(2013), 547--566.\n\n\\bibitem{Gen_lasso}\n{\\sc R. J. Tibshirani and J. Taylor},~{\\em The solution path of the generalized LASSO},~The Annals of Statistics, 39-3~(2011), 1335--1371.\n\n\\bibitem{LASSO_dual}\n{\\sc M.~R.~Osborne,~B. Presnell, and B. A. Turlach},~{\\em On the LASSO and its dual},~J. of Computational and Graphical Stat,~9~(1999), 319--337.\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 21348, "prevtext": "\nNext, if we let $\\mathbf{u}$ and $\\mathbf{v}$ be two vectors in $\\mathbb{R}^{\\min\\{m, n\\}}$ consisting of the diagonal elements of $\\mathbf{{X}}$ and ${\\tilde{\\mathbf A}}$, respectively, then\n(\\ref{BAd}) is (\\ref{BAS}). Thus we have established the following result. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{theorem 5} With the notations above,  the solution to problem (\\ref{BA}) is\n\tgiven by\n\t\n", "index": 81, "text": "$$\n\t\\hat{\\mathbf X} ={\\mathbf  U} S_{\\lambda}(\\tilde{\\mathbf A}) {\\mathbf V}^{T},\n\t$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\mathbf{X}}={\\mathbf{U}}S_{\\lambda}(\\tilde{\\mathbf{A}}){\\mathbf{V}}^{T},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mi>\ud835\udc14</mi><mo>\u2062</mo><msub><mi>S</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc15</mi><mi>T</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]