[{"file": "1601.03916.tex", "nexttext": "\n\nIn this paper, we approach this problem by a general cross-lingual reranking framework where for a given pair of source caption and image, monolingual captions in the target language are used to rerank the output of the SMT system. We present two approaches which find target language captions for reranking by pivoting on images that are similar to the input image. One approach calculates similarity in visual space by comparing manually annotated object categories (providing an upper bound). Another approach calculates image similarity based deep convolutional neural network (CNN) representations. Crucially, both approaches rely on monolingual captions and use parallel captions only for in-domain tuning. We compare the multimodal pivot  approaches to reranking approaches that are based on text only, and find that despite improvements can be gained without image information, maximal gains of $1.4$ to $1.6$ BLEU points are gained by multimodal pivots whereby the CNN results are statistically indistinguishable from pivoting on manually labeled object categories.\n\n\n\\section{Related Work}\n\n\\newcite{Waeschle2015} presented a framework for integrating a large, in-domain, target-side monolingual corpus into machine translation by making use of techniques from cross lingual information retrieval. The intuition behind their approach is to generate one or several translation hypotheses using an SMT system, which act as queries to find matching, semantically similar sentences in the target side corpus. These can in turn be used as templates for refinement of the translation hypotheses, with the overall effect of improving translation quality. They compare their work with the more canonical approach for integration of translation memories into MT, which makes use of retrieval techniques on the source side of a parallel corpus in order to find similar matches on the target side. They show improvements for both approaches individually, as well as in combination, on several narrow-domain corpora. Our work can be seen as an extension of this method, with visual similarity feedback as additional constraint on the cross-lingual retrieval model.\n\nCaption generation from images alone has only recently come into the scope of realistically solvable problems in image processing (\\newcite{KulkarniETAL:11}, \\newcite{KarpathyFeiFei:15}, \\newcite{VinyalsETAL:15}, \\emph{inter alia}). Because the visual information in an image $i$ underdetermines the exact wording of target-language caption $e_i$ more strongly than the source-language caption $f_i$ (there are a great number of adequate captions to describe image $i$ in the target language, but not all of them are acceptable translations of $f_i$), caption generation by itself currently does not appear to be a promising alternative for caption translation.\n\n\\newcite{Calixto2012} suggest using images as supplementary context information for statistical machine translation. They cite examples from the news domain where visual context could potentially be helpful in the disambiguation aspect of SMT and discuss possible features and distance metrics for context images, but do not report experiments involving a full SMT pipeline using visual context.\n\nConvolutional neural networks have greatly increased the quality of feature representations of images, enabling robust and semantically salient analysis of image content \\cite{Socher2014,RCNN}. This has raised the prospect of solving semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback. \\newcite{silberer-lapata:2014:P14-1} show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks. The orthogonality of visual feedback has previously been exploited in a multilingual setting by \\newcite{kiela-vulic-clark:2015:EMNLP} (relying on previous work by \\newcite{Bergsma:11}), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image search engine\\footnote{https://images.google.com/}. \\newcite{funaki-nakayama:2015:EMNLP} use visual similarity for cross-lingual document retrieval in a multimodal and bilingual vector space obtained by generalized canonical correlation analysis, greatly reducing the need for parallel training data.  The common element of this body of work on visually grounded multilingual representations is that CNN-based visual similarity information is used as  a ``hub'' \\cite{funaki-nakayama:2015:EMNLP} or \\emph{pivot} connecting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation. \n\nIn parallel to our work, \\newcite{DBLP:journals/corr/ElliottFH15} addressed the problem of caption translation from the perspective of neural machine translation. They report improvements of similar magnitude to ours over a translation baseline on a parallel corpus of image descriptions. Their approach uses a model which is considerably more involved than ours and relies on the availability of parallel captions as training data. Both approaches crucially rely on neural networks, where they use a visually enriched neural encoder-decoder SMT approach, while we follow a retrieval paradigm for caption translation, using CNNs to compute similarity in visual space.\n\n\n\n\\section{Models}\n\n\\subsection{Overview}\n\nFollowing the basic approach set out by \\newcite{Waeschle2015}, we use a cross-lingual retrieval model to find sentences in a target language document collection $C$, and use these to rerank target language translations $e$ of a source caption $f$.\n\nThe systems described in our work differ from that of \\newcite{Waeschle2015} in a number of aspects. Instead of\n\na two-step architecture of coarse-grained and fine-grained retrieval, our system uses relevance scoring functions for both retrieval of matches in the document collection $C$, and for reranking of translation candidates that are based on inverse document frequency of terms \\cite{SpaerckJones1972} and represent variants of the popular TF-IDF relevance measure.\n\n\n\nFigure \\ref{figure:pipeline} gives a schematic overview over the pipeline: We generate hypothesis list, which is used to retrieve a list of target captions (with the option of using multimodal similarity information), which itself is used to refine the ranking of a second hypothesis list. More formally, for a source caption $f_i$ of image $i$, a list of $k_n$-best translation hypotheses $N_{f_i}$ is generated. This list $N_{f_i}$ is used to find the $k_m$-most relevant target-side image captions $M_{f_i}$ in a target language document collection $C$ using a heuristic relevance scoring function $S(m, N_{f_i},i), m \\in C$. This scoring function incorporates both textual and visual information to score each $m \\in C$, with associated image $i_m$, based on the similarity of $m$ to all $ n \\in N_{f_i}$ and of $i_m $ to $i$ . The top $k_m$-scoring image-caption pairs from $C$ are then used as multimodal pivot documents to refine the translation of $f_i$. A second $k_r$-best list of translation hypothesis $R_{f_i}$ is generated \\footnote{In practice, the first hypothesis list may be reused. We distinguish between the two hypothesis lists $N_{f_i}$ and $R_{f_i}$ for notational clarity. Note that the two hypothesis lists need not be of equal length.} and the relevance scoring function $F(r, M_{f_i})$ is applied to each entry $r \\in R_{f_i}$. The best translation hypothesis $\\hat{e_i}$ is determined by interpolating the decoder score $d_r$ for a hypothesis $r$ with its relevance score $F(r, M_{f_i})$ with weight $q$:\n\n\n", "itemtype": "equation", "pos": 3001, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nWe present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. Image similarity is computed by a convolutional neural network and incorporated into a target-side translation memory retrieval model where descriptions of most similar images are used to rerank translation outputs. Our approach does not depend on the availability of in-domain parallel data and achieves improvements of 1.4 BLEU over strong baselines.  \n\\end{abstract}\n\n\\section{Introduction}\n\nMultimodal data consisting of images and natural language descriptions (henceforth called \\emph{captions}) are an abundant source of information that have led to a recent surge in research on integrating language and vision. However, most research datasets\\footnote{See \\newcite{FerraroETAL:15} for an overview over current datasets for vision and language research.} provide captions only in one language while there is clearly a demand for multilingual image captions, e.g., automatic translation of descriptions of art works would allow access to digitized art catalogues across language barriers and is thus of social and cultural interest; multilingual product descriptions are of high commercial interest since they would allow to widen e-commerce transactions automatically to international markets.\n\nIn this paper, we want to address the problem of multilingual captioning from the perspective of statistical machine translation (SMT). In contrast to prior work on generating captions directly from images (\\newcite{KulkarniETAL:11}, \\newcite{KarpathyFeiFei:15}, \\newcite{VinyalsETAL:15}, \\emph{inter alia}), our goal is to integrate visual information into an SMT pipeline. The crucial idea is that visual context information provides orthogonal information that is free of the ambiguities of natural language. It therefore serves to disambiguate and guide the translation process by grounding the translation of a source caption in the accompanying image. However, datasets that consist of source captions, images, and target captions are not available in large quantities. Thus, we would like to utilize large datasets of images and target-side monolingual captions instead, and use parallel captions as little as possible (in our case, only for meta-parameter tuning and testing).\n\nLet the task of \\emph{caption translation} be defined as follows: For production of a target caption $e_i$ of an image $i$, a system may use as input an image caption for image $i$ in the source language $f_i$, as well as the image $i$ itself. The system may safely assume that $f_i$ is relevant to $i$, i.e., the identification of relevant captions for $i$ \nis not itself part of the task of caption translation. In contrast to the inference problem of finding $\\hat{e} = {\\operatornamewithlimits{argmax}}_e p(e|f)$ in text-based SMT, multimodal caption translation allows to take into consideration $i$ as well as $f_i$ in finding $\\hat{e_i}$: \n\n", "index": 1, "text": "$$\\hat{e_i} = {\\operatornamewithlimits{argmax}}_{e_i} p(e_i|f_i,i)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\hat{e_{i}}={\\operatornamewithlimits{argmax}}_{e_{i}}p(e_{i}|f_{i},i)\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>=</mo><munder><mo movablelimits=\"false\">argmax</mo><msub><mi>e</mi><mi>i</mi></msub></munder><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>f</mi><mi>i</mi></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\n\n\\begin{figure}\n    \\centering \n    \\includegraphics[scale=0.23]{pipeline_multimodal.pdf}\n    \\caption{\\label{figure:pipeline} Overview of model architecture.}\n\\end{figure}\n\nWe present three variants of target-side translation-memory retrieval, all of which make use of the procedure outlined above. They differ in the choice of scoring function $S(m, N_{f_i},i)$.\n\nIn the baseline text-based reranking model (TMR-TXT), we use relevance scoring function $S_{TXT}$. This function is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side translation memory retrieval in \\newcite{Waeschle2015}). In the retrieval model enhanced by visual information from a deep convolutional neural network (TMR-CNN), the scoring function $S_{CNN}$ incorporates a textual relevance score with visual similarity information extracted from the neural network. Finally, we evaluate these models against a relevance score based on human object-category annotations (TMR-HCA), using the scoring function $S_{HCA}$. This function makes use of the object annotations available for the MS COCO corpus \\cite{mscoco} to give an indication of the effectiveness of our automatically extracted visual similarity metric. The three models are discussed in detail below.\n\n\n\\subsection{Text-Based Target-side Translation Memory Retrieval (TMR-TXT)}\n\\label{section:text_retrieval}\n\nIn the text-based retrieval scenario, a match candidate $m \\in C$ is scored in the following way:\n\n\n", "itemtype": "equation", "pos": 10779, "prevtext": "\n\nIn this paper, we approach this problem by a general cross-lingual reranking framework where for a given pair of source caption and image, monolingual captions in the target language are used to rerank the output of the SMT system. We present two approaches which find target language captions for reranking by pivoting on images that are similar to the input image. One approach calculates similarity in visual space by comparing manually annotated object categories (providing an upper bound). Another approach calculates image similarity based deep convolutional neural network (CNN) representations. Crucially, both approaches rely on monolingual captions and use parallel captions only for in-domain tuning. We compare the multimodal pivot  approaches to reranking approaches that are based on text only, and find that despite improvements can be gained without image information, maximal gains of $1.4$ to $1.6$ BLEU points are gained by multimodal pivots whereby the CNN results are statistically indistinguishable from pivoting on manually labeled object categories.\n\n\n\\section{Related Work}\n\n\\newcite{Waeschle2015} presented a framework for integrating a large, in-domain, target-side monolingual corpus into machine translation by making use of techniques from cross lingual information retrieval. The intuition behind their approach is to generate one or several translation hypotheses using an SMT system, which act as queries to find matching, semantically similar sentences in the target side corpus. These can in turn be used as templates for refinement of the translation hypotheses, with the overall effect of improving translation quality. They compare their work with the more canonical approach for integration of translation memories into MT, which makes use of retrieval techniques on the source side of a parallel corpus in order to find similar matches on the target side. They show improvements for both approaches individually, as well as in combination, on several narrow-domain corpora. Our work can be seen as an extension of this method, with visual similarity feedback as additional constraint on the cross-lingual retrieval model.\n\nCaption generation from images alone has only recently come into the scope of realistically solvable problems in image processing (\\newcite{KulkarniETAL:11}, \\newcite{KarpathyFeiFei:15}, \\newcite{VinyalsETAL:15}, \\emph{inter alia}). Because the visual information in an image $i$ underdetermines the exact wording of target-language caption $e_i$ more strongly than the source-language caption $f_i$ (there are a great number of adequate captions to describe image $i$ in the target language, but not all of them are acceptable translations of $f_i$), caption generation by itself currently does not appear to be a promising alternative for caption translation.\n\n\\newcite{Calixto2012} suggest using images as supplementary context information for statistical machine translation. They cite examples from the news domain where visual context could potentially be helpful in the disambiguation aspect of SMT and discuss possible features and distance metrics for context images, but do not report experiments involving a full SMT pipeline using visual context.\n\nConvolutional neural networks have greatly increased the quality of feature representations of images, enabling robust and semantically salient analysis of image content \\cite{Socher2014,RCNN}. This has raised the prospect of solving semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback. \\newcite{silberer-lapata:2014:P14-1} show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks. The orthogonality of visual feedback has previously been exploited in a multilingual setting by \\newcite{kiela-vulic-clark:2015:EMNLP} (relying on previous work by \\newcite{Bergsma:11}), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image search engine\\footnote{https://images.google.com/}. \\newcite{funaki-nakayama:2015:EMNLP} use visual similarity for cross-lingual document retrieval in a multimodal and bilingual vector space obtained by generalized canonical correlation analysis, greatly reducing the need for parallel training data.  The common element of this body of work on visually grounded multilingual representations is that CNN-based visual similarity information is used as  a ``hub'' \\cite{funaki-nakayama:2015:EMNLP} or \\emph{pivot} connecting corpora in two natural languages which lack direct parallelism, a strategy which we apply to the problem of caption translation. \n\nIn parallel to our work, \\newcite{DBLP:journals/corr/ElliottFH15} addressed the problem of caption translation from the perspective of neural machine translation. They report improvements of similar magnitude to ours over a translation baseline on a parallel corpus of image descriptions. Their approach uses a model which is considerably more involved than ours and relies on the availability of parallel captions as training data. Both approaches crucially rely on neural networks, where they use a visually enriched neural encoder-decoder SMT approach, while we follow a retrieval paradigm for caption translation, using CNNs to compute similarity in visual space.\n\n\n\n\\section{Models}\n\n\\subsection{Overview}\n\nFollowing the basic approach set out by \\newcite{Waeschle2015}, we use a cross-lingual retrieval model to find sentences in a target language document collection $C$, and use these to rerank target language translations $e$ of a source caption $f$.\n\nThe systems described in our work differ from that of \\newcite{Waeschle2015} in a number of aspects. Instead of\n\na two-step architecture of coarse-grained and fine-grained retrieval, our system uses relevance scoring functions for both retrieval of matches in the document collection $C$, and for reranking of translation candidates that are based on inverse document frequency of terms \\cite{SpaerckJones1972} and represent variants of the popular TF-IDF relevance measure.\n\n\n\nFigure \\ref{figure:pipeline} gives a schematic overview over the pipeline: We generate hypothesis list, which is used to retrieve a list of target captions (with the option of using multimodal similarity information), which itself is used to refine the ranking of a second hypothesis list. More formally, for a source caption $f_i$ of image $i$, a list of $k_n$-best translation hypotheses $N_{f_i}$ is generated. This list $N_{f_i}$ is used to find the $k_m$-most relevant target-side image captions $M_{f_i}$ in a target language document collection $C$ using a heuristic relevance scoring function $S(m, N_{f_i},i), m \\in C$. This scoring function incorporates both textual and visual information to score each $m \\in C$, with associated image $i_m$, based on the similarity of $m$ to all $ n \\in N_{f_i}$ and of $i_m $ to $i$ . The top $k_m$-scoring image-caption pairs from $C$ are then used as multimodal pivot documents to refine the translation of $f_i$. A second $k_r$-best list of translation hypothesis $R_{f_i}$ is generated \\footnote{In practice, the first hypothesis list may be reused. We distinguish between the two hypothesis lists $N_{f_i}$ and $R_{f_i}$ for notational clarity. Note that the two hypothesis lists need not be of equal length.} and the relevance scoring function $F(r, M_{f_i})$ is applied to each entry $r \\in R_{f_i}$. The best translation hypothesis $\\hat{e_i}$ is determined by interpolating the decoder score $d_r$ for a hypothesis $r$ with its relevance score $F(r, M_{f_i})$ with weight $q$:\n\n\n", "index": 3, "text": "$$ \\hat{e_i} = {\\operatornamewithlimits{argmax}}_{r \\in R_{f_i}} d_r + q \\cdot F(r, M_{f_i})$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\hat{e_{i}}={\\operatornamewithlimits{argmax}}_{r\\in R_{f_{i}}}d_{r}+q\\cdot F(r%&#10;,M_{f_{i}})\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmax</mo><mrow><mi>r</mi><mo>\u2208</mo><msub><mi>R</mi><msub><mi>f</mi><mi>i</mi></msub></msub></mrow></munder><mo>\u2061</mo><msub><mi>d</mi><mi>r</mi></msub></mrow><mo>+</mo><mrow><mrow><mi>q</mi><mo>\u22c5</mo><mi>F</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\n\n\n\n\nwhere $\\delta$ is the Kronecker $\\delta$-function, $N_{f_i}$ is the set of the $k_n$-best translation hypotheses for a source caption $f_i$ of image $i$ by decoder score, $typ(a)$ is a function yielding the set of types (unique tokens) contained in a caption $a$\\footnote{The choice for per-type scoring of reference captions was primarily driven by performance considerations. Since captions rarely contain repetitions of low-frequency terms, this has very little effect in practice, other than to mitigate the influence of stopwords.}, $tok(a)$ is a function yielding the tokens of caption $a$, $idf(w)$ is the inverse document frequency \\cite{SpaerckJones1972} of term $w$, and $Z_m = \\frac{1}{|typ(m)|}$ is a normalization term introduced in order to avoid biasing the system toward long match candidates containing many low-frequency terms. Term frequencies were computed on monolingual data from Europarl \\cite{Koehn2005} and the News Commentary and News Discussions English datasets provided for the WMT15 workshop\\footnote{http://www.statmt.org/wmt15/translation-task.html}. Note that in this model, information from the image $i$ is not used. \n\n\\subsection{Image-Enhanced Retrieval using a Deep Convolutional Neural Network (TMR-CNN)}\n\nIn this scenario, we supplement the textual target-side translation-memory retrieval model with visual similarity information from a deep convolutional neural network. We formalize this by introduction of the positive-semidefinite visual distance function $v(i_x, i_y) \\rightarrow [0,\\infty)$ for images $i_x$, $i_y$ (smaller values indicating more similar images). The relevance scoring function $S_{CNN}$ used in this model takes the following form:\n\n", "itemtype": "equation", "pos": 12400, "prevtext": "\n\n\\begin{figure}\n    \\centering \n    \\includegraphics[scale=0.23]{pipeline_multimodal.pdf}\n    \\caption{\\label{figure:pipeline} Overview of model architecture.}\n\\end{figure}\n\nWe present three variants of target-side translation-memory retrieval, all of which make use of the procedure outlined above. They differ in the choice of scoring function $S(m, N_{f_i},i)$.\n\nIn the baseline text-based reranking model (TMR-TXT), we use relevance scoring function $S_{TXT}$. This function is purely text-based and does not make use of multimodal context information (as such, it comes closest to the models used for target-side translation memory retrieval in \\newcite{Waeschle2015}). In the retrieval model enhanced by visual information from a deep convolutional neural network (TMR-CNN), the scoring function $S_{CNN}$ incorporates a textual relevance score with visual similarity information extracted from the neural network. Finally, we evaluate these models against a relevance score based on human object-category annotations (TMR-HCA), using the scoring function $S_{HCA}$. This function makes use of the object annotations available for the MS COCO corpus \\cite{mscoco} to give an indication of the effectiveness of our automatically extracted visual similarity metric. The three models are discussed in detail below.\n\n\n\\subsection{Text-Based Target-side Translation Memory Retrieval (TMR-TXT)}\n\\label{section:text_retrieval}\n\nIn the text-based retrieval scenario, a match candidate $m \\in C$ is scored in the following way:\n\n\n", "index": 5, "text": "\\begin{align*}\n&S_{TXT}(m, N_{f_i}) = \\\\\n& Z_m\\sum_{n \\in N_{f_i}}\\sum_{w_n \\in tok(n)}\\sum_{w_m \\in typ(m)} \\delta(w_m,w_n)  idf(w_m), \\\\\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{TXT}(m,N_{f_{i}})=\" display=\"inline\"><mrow><mrow><msub><mi>S</mi><mrow><mi>T</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>T</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>,</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Z_{m}\\sum_{n\\in N_{f_{i}}}\\sum_{w_{n}\\in tok(n)}\\sum_{w_{m}\\in&#10;typ%&#10;(m)}\\delta(w_{m},w_{n})idf(w_{m}),\" display=\"inline\"><mrow><mrow><msub><mi>Z</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>\u2208</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>\u2208</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>m</mi></msub><mo>\u2208</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>m</mi></msub><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\nwhere $i_m$ is the image to which the caption $m$ refers and $d$ is a cutoff maximum distance, above which match candidates are considered irrelevant.\n\nWe computed visual distance measures $v$ with Fast R-CNN \\cite{girshick15fastrcnn} using the VGG16 deep convolutional model, which was pre-trained on ImageNet \\cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}, with weights optimized for object recognition on the PASCAL VOC 2012 \\cite{Everingham10} training data. As we are interested in a holistic similarity score taking into account the entirety of $i$ and $i_m$ rather than the similarity between individual objects contained in the images, we run the model on a single bounding box comprising the whole of an image. We measure the visual similarity $v$ by computing the Euclidean distance between the vectors of per-class regression offsets for the 20 PASCAL VOC object classes returned by Fast R-CNN (yielding 4 real-numbered offset values for each class, 80 in total, for each image). This relatively simple distance metric proved sufficient for our purposes. If no neighboring images fell within distance $d$, the text-based retrieval procedure $S_{TXT}$ was used as a fallback strategy.\n\n\\subsection{Retrieval using Human Object Category Annotations (TMR-HCA)}\n\\label{section:image_retrieval_human}\n\nFor contrastive purposes, we evaluated a retrieval model which makes use of the human object category annotations for MS COCO. Each image in the MS COCO corpus is annotated with object polygons classified into 91 categories of common objects. In this scenario, a match candidate $m$ is scored in the following way:\n\n", "itemtype": "equation", "pos": 14253, "prevtext": "\n\n\n\n\nwhere $\\delta$ is the Kronecker $\\delta$-function, $N_{f_i}$ is the set of the $k_n$-best translation hypotheses for a source caption $f_i$ of image $i$ by decoder score, $typ(a)$ is a function yielding the set of types (unique tokens) contained in a caption $a$\\footnote{The choice for per-type scoring of reference captions was primarily driven by performance considerations. Since captions rarely contain repetitions of low-frequency terms, this has very little effect in practice, other than to mitigate the influence of stopwords.}, $tok(a)$ is a function yielding the tokens of caption $a$, $idf(w)$ is the inverse document frequency \\cite{SpaerckJones1972} of term $w$, and $Z_m = \\frac{1}{|typ(m)|}$ is a normalization term introduced in order to avoid biasing the system toward long match candidates containing many low-frequency terms. Term frequencies were computed on monolingual data from Europarl \\cite{Koehn2005} and the News Commentary and News Discussions English datasets provided for the WMT15 workshop\\footnote{http://www.statmt.org/wmt15/translation-task.html}. Note that in this model, information from the image $i$ is not used. \n\n\\subsection{Image-Enhanced Retrieval using a Deep Convolutional Neural Network (TMR-CNN)}\n\nIn this scenario, we supplement the textual target-side translation-memory retrieval model with visual similarity information from a deep convolutional neural network. We formalize this by introduction of the positive-semidefinite visual distance function $v(i_x, i_y) \\rightarrow [0,\\infty)$ for images $i_x$, $i_y$ (smaller values indicating more similar images). The relevance scoring function $S_{CNN}$ used in this model takes the following form:\n\n", "index": 7, "text": "\\begin{align*}\n  &S_{CNN}(m, N_{f_i}, i) \\\\\n  &=\n  \\begin{cases}\n    S_{TXT}(m, N_{f_i}) e^{ - v(i_m,i)},  & v(i_m,i) < d \\\\\n    0 & otherwise,\\\\\n  \\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{CNN}(m,N_{f_{i}},i)\" display=\"inline\"><mrow><msub><mi>S</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mi>N</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>,</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\begin{cases}S_{TXT}(m,N_{f_{i}})e^{-v(i_{m},i)},&amp;v(i_{m},i)&lt;d\\\\&#10;0&amp;otherwise,\\\\&#10;\\end{cases}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>S</mi><mrow><mi>T</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>T</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>,</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>i</mi><mi>m</mi></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>i</mi><mi>m</mi></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>d</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>o</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>w</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>e</mi></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\n where $cat(i)$ returns the set of object categories with which image $i$ is annotated. The amounts to enforcing a strict match between the category annotations of $i$ and the reference image $i_m$.\\footnote{Attempts to relax this strict matching criterion led to strong performance degradation on the development test set.} In cases where $i$ was annotated with a unique set of object categories and thus no match candidates with nonzero scores were returned by $S_{HCA}$, $S_{TXT}$ was used as a fallback strategy.\n\n\\subsection{Translation Candidate Re-scoring}\n\\label{section:rescoring}\n\nThe relevance score $F(r, M_{f_i})$ used in the reranking model was computed in the following way for all three modes:\n\n\n\n", "itemtype": "equation", "pos": 16053, "prevtext": "\nwhere $i_m$ is the image to which the caption $m$ refers and $d$ is a cutoff maximum distance, above which match candidates are considered irrelevant.\n\nWe computed visual distance measures $v$ with Fast R-CNN \\cite{girshick15fastrcnn} using the VGG16 deep convolutional model, which was pre-trained on ImageNet \\cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}, with weights optimized for object recognition on the PASCAL VOC 2012 \\cite{Everingham10} training data. As we are interested in a holistic similarity score taking into account the entirety of $i$ and $i_m$ rather than the similarity between individual objects contained in the images, we run the model on a single bounding box comprising the whole of an image. We measure the visual similarity $v$ by computing the Euclidean distance between the vectors of per-class regression offsets for the 20 PASCAL VOC object classes returned by Fast R-CNN (yielding 4 real-numbered offset values for each class, 80 in total, for each image). This relatively simple distance metric proved sufficient for our purposes. If no neighboring images fell within distance $d$, the text-based retrieval procedure $S_{TXT}$ was used as a fallback strategy.\n\n\\subsection{Retrieval using Human Object Category Annotations (TMR-HCA)}\n\\label{section:image_retrieval_human}\n\nFor contrastive purposes, we evaluated a retrieval model which makes use of the human object category annotations for MS COCO. Each image in the MS COCO corpus is annotated with object polygons classified into 91 categories of common objects. In this scenario, a match candidate $m$ is scored in the following way:\n\n", "index": 9, "text": "\\begin{align*}\n  &S_{HCA}(m, N_{f_i}, i) \\\\\n  &= \\delta(cat(i_m),cat(i)) S_{TXT}(m, N_{f_i}),\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{HCA}(m,N_{f_{i}},i)\" display=\"inline\"><mrow><msub><mi>S</mi><mrow><mi>H</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>A</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>,</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\delta(cat(i_{m}),cat(i))S_{TXT}(m,N_{f_{i}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>i</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>S</mi><mrow><mi>T</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>T</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>,</mo><msub><mi>N</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\n\n \n\n\n\nwith normalization term\n", "itemtype": "equation", "pos": 16874, "prevtext": "\n where $cat(i)$ returns the set of object categories with which image $i$ is annotated. The amounts to enforcing a strict match between the category annotations of $i$ and the reference image $i_m$.\\footnote{Attempts to relax this strict matching criterion led to strong performance degradation on the development test set.} In cases where $i$ was annotated with a unique set of object categories and thus no match candidates with nonzero scores were returned by $S_{HCA}$, $S_{TXT}$ was used as a fallback strategy.\n\n\\subsection{Translation Candidate Re-scoring}\n\\label{section:rescoring}\n\nThe relevance score $F(r, M_{f_i})$ used in the reranking model was computed in the following way for all three modes:\n\n\n\n", "index": 11, "text": "\\begin{align*}\n&F(r, M_{f_i}) =\\\\\n& Z_{M_{f_i}} \\sum_{m \\in M_{f_i}}\\sum_{w_m \\in typ(m)}\\sum_{w_r \\in tok(r)}\\delta(w_m, w_r)idf(w_m)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle F(r,M_{f_{i}})=\" display=\"inline\"><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Z_{M_{f_{i}}}\\sum_{m\\in M_{f_{i}}}\\sum_{w_{m}\\in typ(m)}\\sum_{w_%&#10;{r}\\in tok(r)}\\delta(w_{m},w_{r})idf(w_{m})\" display=\"inline\"><mrow><msub><mi>Z</mi><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>\u2208</mo><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>m</mi></msub><mo>\u2208</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>r</mi></msub><mo>\u2208</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>m</mi></msub><mo>,</mo><msub><mi>w</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03916.tex", "nexttext": "\nwhere $r$ is a translation candidate and $M_{f_i}$ is a list of $k_m$-top retrieved translation memory matches. Because the model should return a score that is reflective of the relevance of $r$ with respect to $M_{f_i}$, irrespective of the length of $M_{f_i}$, normalization with respect to the token count of $M_{f_i}$ is necessary. The term $Z_{M_{f_i}}$ serves this purpose.\n\n\n\n\n\\section{Experiments}\n\\label{chapter:experiments}\n\n\\subsection{Dataset}\n\nWe constructed a German-English parallel dataset based on the MS COCO \\cite{mscoco} image corpus. 1,000 images were selected at random from the 2014 training section\\footnote{We elected to construct our parallel dataset using the training rather than the validation section of MS COCO so as to keep the latter pristine for future work based on this research.} and, in a second step, one of their five English captions was chosen randomly. This caption was then translated into German by a native German speaker. Note that our experiments were performed with German as the source and English as the target language, therefore, our reference data was not produced by a single speaker but reflects the heterogeneity of the MS COCO dataset at large. The data was split into a development set of 250 captions, a development test set of 250 captions for testing work in progress, and a test set of 500 captions. For our retrieval experiments, we used only those images and their captions not included in the development, development test or test data, a total of 81,822 images with 5 English captions per image. All data was tokenized and converted to lower case using the {\\tt cdec} utilities {\\tt tokenized-anything.pl} and {\\tt lowercase.pl}. Our parallel development, development test and test datasets will be made publicly available.\\footnote{http://www.cl.uni-heidelberg.de/\\\\statnlpgroup/resources.mhtml}\n\n\\subsection{Translation System}\n\\label{section:translation_system}\n\n\nWe generated $k_n$- and $k_r$-best lists of translation candidates using the implementation of a hierarchical phrase-based translation model using synchronous context free grammars \\cite{Chiang2007} provided as part the {\\tt cdec} decoder \\cite{Dyer2010}\\footnote{https://github.com/redpony/cdec}. Data from the Europarl \\cite{Koehn2005}, News Comment and Common Crawl corpora \\cite{Smith2013} as provided for the WMT15 workshop was used to train the translation model, with German as source and English as target language.\n\nLike the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same {\\tt cdec} tools. Sentences with lengths over 80 words in either the source or the target language were discarded before training. Source text compound splitting was performed using the method described by \\cite{Dyer2009}, (as implemented by the {\\tt cdec} utility {\\tt compound-split.pl}). Alignments were extracted bidirectionally using the {\\tt fast-align} utility of {\\tt cdec} and symmetrized with the {\\tt atools} utility (also part of {\\tt cdec}) using the {\\tt grow-diag-final-and} symmetrization heuristic. The alignments were then used by the {\\tt cdec} grammar extractor to extract a synchronous context free grammar from the parallel data. \n\nThe target language model was trained on monolingual data from Europarl \\cite{Koehn2005}, as well as the News Commentary and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the {\\tt KenLM} toolkit \\cite{Heafield-estimate,Heafield-kenlm}\\footnote{https://kheafield.com/code/kenlm/}.\n\nWe optimized the parameters of the translation system for translation quality as measured by IBM BLEU \\cite{Papineni2002} using the Margin Infused Relaxed Algorithm (MIRA) \\cite{Crammer2003,Hasler2011}. For tuning the translation models used for testing work in progress, MIRA was run for 20 iterations on the development set. For tuning the translation models used for extraction of the hypothesis lists for final evaluation, MIRA was run for 20 iterations on the combined development and development test sets.\n\n\n\n\\subsection{Optimization of Translation Memory Retrieval Hyperparameters}\n\\label{section:retrieval_tuning}\n\nFor each of our retrieval models, we performed a step-wise exhaustive search of the hyperparameter space over the four system hyperparameters for IBM BLEU on the development set: The length of the $k_n$-best list, the entries of which are used as queries for retrieval, the number of $k_m$-best-matching captions retrieved, the length of the final $k_r$-best list used in reranking, as well as the feature weight $q$ of the relevance score $F$ relative to the translation hypothesis log probability returned by the decoder. The parameter ranges to be explored were determined manually, by examining system output for prototypical examples. Table \\ref{table:tuning} gives an overview over the hyperparameter values obtained. \n\nFor TMR-CNN, we initially set the cutoff distance $d$ to 0.3, after manually inspecting sets of nearest neighbors returned for various maximum distance values. After optimization of retrieval parameters, we performed an exhaustive search from $d=0.25$ to $d=0.35$, with step size 0.01 on the development set, while keeping all other hyperparameters fixed, which confirmed out initial choice of $d=0.3$ as the optimal value. \n\nExplored parameter spaces were identical for all models and each model was evaluated on the test set using its own optimal configuration of hyperparameters. \n\n\\begin{table}[H]\n\\begin{center}\n\\begin{tabular}{| l | c | c | c | c | }\n\n\\hline\n  Model & $k_n$ & $k_m$ & $k_r$ &$q$ \\\\\n\\hline\n  TMR-TXT & 300 & 15 & 15 & $610 \\cdot 10^{4}$ \\\\\n  TMR-CNN & 300 & 30 & 15 & $565 \\cdot 10^{4}$ \\\\\n  TMR-HCA & 300 & 300 & 15 & $515 \\cdot 10^{4}$ \\\\\n\\hline \n\\end{tabular}\n\\end{center}\n\\caption{\\label{table:tuning} Optimized hyperparameter values used in final evaluation.}\n\\end{table}\n\n\\subsection{Significance Testing}\n\\label{section:sigtesting}\n\nSignificance tests on the differences in translation quality were performed using the approximate randomization technique for measuring performance differences of machine translation systems described in \\newcite{Riezler2005} and implemented by \\newcite{Clark2011} as part of the {\\tt Multeval} toolkit \\footnote{https://github.com/jhclark/multeval}. \n\n\\section{Results}\n\n\nTable \\ref{tab:scores} summarizes the results for all models on an unseen test set of 500 captions. We found that the target-side translation memory retrieval model enhanced with multimodal pivots from a deep convolutional neural network, TMR-CNN consistently outperformed both the {\\tt cdec} baseline as well as the text-based translation-memory retrieval model TMR-TXT. The gain in performance was significant to within $p<0.05$ for IBM BLEU and METEOR, but not for Translation Edit Rate (TER), where only the retrieval system based on human object category annotations TMR-HCA showed a significant performance gain over all other models. For BLEU and METEOR, the difference between TMR-CNN and TMR-HCA was not significant, demonstrating that retrieval using our CNN-derived distance metric could match retrieval based the human object category annotations for these two metrics. \n\nThe text-based retrieval baseline TMR-TXT never significantly outperformed the {\\tt cdec} baseline, but there were slight nominal improvements in terms of IBM BLEU and METEOR. This finding is actually consistent with those of \\newcite{Waeschle2015}, who report performance gains for text-based, target-side translation memory retrieval models only on highly technical, narrow-domain corpora and even report performance degradation on medium-diversity corpora such as Europarl. Our experiments show that it is the addition of visual similarity information by incorporation of multimodal pivots into the image-enhanced models TMR-CNN and TMR-HCA which makes such techniques effective on MS COCO, thus upholding our hypothesis that visual information can be exploited for improvement of caption translation.\n\n\\begin{table}[H]\n\\begin{center}\n\\begin{footnotesize}\n\\begin{tabular}{|l|l|l|l|l|l|}\n\\hline\n\\bf Metric & \\bf System & Score & \\bf $p_{c}$ & \\bf $p_t$ & \\bf $p_b$ \\\\\n\\hline\n{BLEU $\\uparrow$}\n&{\\tt cdec}  & 25.5 &  &  & \\\\\n& TMR-TXT & 25.7 &  &  & 0.69 \\\\\n& TMR-CNN & \\textbf{26.9} &  & 0.02 & 0.01 \\\\\n& TMR-HCA & \\textbf{27.1} & 0.63 & 0.00 & 0.00 \\\\\n\\hline\n{METEOR $\\uparrow$}\n&{\\tt cdec}  & 31.7 &  &  &  \\\\\n& TMR-TXT & 32.0 &  &  & 0.21 \\\\\n& TMR-CNN & \\textbf{32.4} &  & 0.03 & 0.00 \\\\\n& TMR-HCA & \\textbf{32.5} & 0.58 & 0.00 & 0.00 \\\\\n\\hline\n{TER $\\downarrow$}\n&{\\tt cdec}  & 49.3 &  &  &  \\\\\n& TMR-TXT & 49.3 &  &  & 0.88 \\\\\n& TMR-CNN & 48.8 &  & 0.18 & 0.14 \\\\\n& TMR-HCA & \\textbf{48.0} & 0.02 & 0.00 & 0.00 \\\\\n\\hline\n\\end{tabular}\n\\end{footnotesize}\n\\end{center}\n\\caption{\\label{tab:scores} Metric scores for all systems and their significance levels as reported by {\\tt Multeval} . $p_b$-values are relative to the {\\tt cdec} baseline, $p_t$-values are relative to TMR-TXT and $p_c$-values are relative to TMR-CNN. Best results are reported in \\textbf{bold} face. }\n\\end{table}\n\nTable \\ref{tab:examples} shows example translations produced by the {\\tt cdec} baseline, TMR-TXT, TMR-CNN, and TMR-HCA, together with source caption, image, and reference translation. The domain knowledge induced by target side captions allows a disambiguation of translation alternatives such as ``racquet'' versus ``bat'', ``shield'' versus ``sign'', and is able to repair partial translations of ``Stadtbus'' into ``city'' (missing ``bus'').\n\n\\begin{table}\n\\begin{center}\n\\begin{footnotesize}\n\n\\begin{tabular}{|p{2cm} |p{5cm}|}\n\n\\hline\n\nImage: &\\vspace{1pt} \\includegraphics[scale=0.19]{example1.jpg}\\vspace{3pt}\\\\\n\\hline\n Source:& Ein Junge bereitet sich darauf vor, seinen Schl\\\"ager zu schwingen. \\\\ \n\\hline\n{\\tt cdec}: & a boy is preparing to shake its head . \\\\\n\\hline\n TMR-TXT:&  a boy is preparing to shake up his racquet . \\\\ \n\\hline\nTMR-CNN: & a young is preparing to shake up his racquet .\\\\\n\\hline\n TMR-HCA:&  a young prepares to his bat to swing .   \\\\\n\\hline\n Reference:& the boy prepares to swing the bat at the game  \\\\          \n\\hline\n\\hline\nImage: &\\vspace{1pt} \\includegraphics[scale=1.0]{example4.jpg}\\vspace{3pt}\\\\\n\\hline\n Source:& Ein Hundeauslaufschild vor einem hohen Baum. \\\\ \n\\hline\n{\\tt cdec}: & a dog - shield in a high tree . \\\\\n\\hline\n TMR-TXT:&  a dog - shield in a high tree .\\\\ \n\\hline\n TMR-CNN: & a dog - sign before a high tree . \\\\\n\\hline\n TMR-HCA:& a dog - sign before a high tree .  \\\\\n\\hline\n Reference:& a dog walking sign stands in front of a tall tree . \\\\          \n\\hline\n\\hline\n Image: & \\vspace{1pt} \\includegraphics[scale=0.45]{example3.jpg}\\vspace{3pt} \\\\\n\\hline\n Source:& Ein gro\\ss er Stadtbus in dichtem Verkehr. \\\\ \n\\hline\n{\\tt cdec}:&a great city in heavy traffic . \\\\   \n\\hline\n TMR-TXT:&  a great city in dense traffic .  \\\\ \n\\hline\nTMR-CNN: & a large city bus in heavy traffic . \\\\\n\\hline\n TMR-HCA:&  a large city bus in heavy traffic .   \\\\\n\\hline\n Reference:& the large city bus is pulling into the traffic .\\\\          \n\\hline\n\n\\end{tabular}\n\\end{footnotesize}\n\\end{center}\n\\caption{\\label{tab:examples} Examples for improved caption translation by multimodal feedback from the test set.}\n\\end{table}\n\n\\subsection{Comparison Source-side Translation Memory Re-ranking}\n\nIt should be noted that we also observed a gain of +0.62 BLEU percentage points over the {\\tt cdec} baseline on the test set by deploying a text-based source-side translation memory retrieval model very similar to the source-side retrieval models of \\newcite{Waeschle2015} on our parallel corpus of 1,000 captions. The model works by retrieving similar captions on the source side using a TF-IDF-based relevance score and re-scoring translation candidates based on the target sides of the retrieved parallel captions pairs and exploits the corpus parallelism of the in-domain corpus directly, which is assumed as a given. However, this approach makes unrealistic assumptions about the availability of in-domain parallel caption data at translation time. Our target-side retrieval models only observe the source caption $f_i$ and image $i$ at translation time and do not assume access to in-domain parallel data. The results, while of some inherent interest, are therefore not directly comparable to those of the models for caption translation presented here.   \n\n\\section{Conclusions and Further Work}\n\nWe demonstrated that the incorporation of multimodal pivots into a target-side translation memory retrieval model improved SMT performance in terms of BLEU and METEOR on our parallel dataset derived from MS COCO. The gain in performance was comparable between a distance metric based on object category regression offsets from a deep convolutional network and one based on human object category annotations, demonstrating the effectiveness of the CNN-derived distance measure. Using our approach, SMT can, in certain cases, profit from multimodal context information. Crucially, this is possible without using large amounts of in-domain parallel text data, but instead using large amounts of monolingual image captions that are more readily available.\n\nLearning semantically informative distance metrics using deep learning techniques is an area under active investigation \\cite{Wu:2013:OMD:2502081.2502112,wang2014,Wang:2015:SSL:2764065.2764206}. Despite the fact that our simple, per-class regression-offset-based distance metrics performed comparably to human object annotations, using such high-level semantic distance metrics for caption translation by multimodal pivots is a promising avenue for further research.\n\nBoth work on neural machine translation \\cite{bahdanau2014neural} and caption generation \\cite{xu2015show} has recently led to the development of so-called ``attention mechanisms''. In the former case, they help to guide the translation process by influencing the sequence in which source tokens are translated, while in the latter case, they decide which part of the image should influence which part of the generated caption. Combining these two types of attention mechanism in a neural caption translation model is a natural next step in caption translation. While this is beyond the scope of this work, our models should provide an informative baseline against which to evaluate such methods.\n\nThe results were achieved on one language pair (German-English) and one corpus (MS COCO) only. As with all retrieval-based methods, generalized statements about the relative performance on corpora of various domains, sizes and qualities are difficult to substantiate. This problem is aggravated in the multimodal case, since the relevance of captions with respect to images varies greatly between different corpora \\cite{Hodosh2013}. It remains to be seen whether similar results to ours can be achieved in settings other than corpora such as MS COCO, with highly concrete and relevant captions for each image, and which methodologies for incorporating multimodal context information into SMT in different scenarios (such as, for example, the translation of web content) will emerge as the optimal ones. By formulating the problem of caption translation and by demonstrating performance improvements on MS COCO, we hope to inspire more attempts at incorporating multimodal information into SMT in various settings.\n  \n\\section*{Acknowledgments}\n\nThis research was supported in part by DFG grant RI-2221/2-1 \u00e2\u0080\u009cGrounding Statistical Machine Translation in Perception and Action\u00e2\u0080\u009d.\n\n\n\\bibliography{literatur}\n\\bibliographystyle{naaclhlt2016}\n\n\n\n", "itemtype": "equation", "pos": 17051, "prevtext": "\n\n \n\n\n\nwith normalization term\n", "index": 13, "text": "\n\\[\nZ_{M_{f_i}} = (\\sum_{m \\in M_{f_i}}|tok(m)|)^{-1},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"Z_{M_{f_{i}}}=(\\sum_{m\\in M_{f_{i}}}|tok(m)|)^{-1},\" display=\"block\"><mrow><mrow><msub><mi>Z</mi><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub></msub><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>\u2208</mo><msub><mi>M</mi><msub><mi>f</mi><mi>i</mi></msub></msub></mrow></munder><mrow><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}]