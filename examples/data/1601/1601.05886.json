[{"file": "1601.05886.tex", "nexttext": "\nwhere ${N_{cl}}$ is the total number of sub-likelihood objects considered,\n$w=(w_1, \\dots, w_{{N_{cl}}})^T \\in \\Omega =\\{0,1\\}^{{N_{cl}}}$ is a vector of\nbinary weights referred to as composition rule, and\n$\\ell_{k}(\\theta) \\propto \\log f(S_k;\\theta)$\nis the sub-likelihood defined on the $k$th data subset $S_k$. The composite likelihood\ndesign is typically user-specified \\citep{varin2011overview, Lindsay2011issues}. For example, $\\ell_k$ can be  based\non all marginal events ($S_k=\\{y_k \\}$, $k=1,\\dots,d$), all pair-wise events\n($S_k=\\{y_j, y_l\\}$,  $1 \\leq j<l\\leq d$ ), or conditional events ($S_k=\\{y_k|y_j, j\\neq k\\}$, $k=1,\\dots, d$).\n\nIn our parsimonious composition framework, each sub-likelihood $\\ell_{k}(\\theta)$\nis allowed to be selected or not, depending on whether\n$w_k$ takes value $1$ or $0$, which results in an efficient use of the data. The total number of selected\nsub-likelihoods, $\\Vert w \\Vert ={\\sum_{k=1}^{\\Ncl}} w_k$, can be much smaller than the total ${N_{cl}}$ ones available.\nThis is in contrast with the frequently used composite likelihood setting where all the ${N_{cl}}$\nsub-likelihoods are selected. Particularly, in the latter case $w = w_{all} = (1,\\dots, 1)^T$, and no data\nnoise reduction is attained.\n\nA complication related to notations in composite likelihood is that the parameter $\\theta$ does not always have\nall its elements involved in each sub-likelihood $\\ell_{k}(\\theta)$. To facilitate presentation in the sequel, we rewrite\n$\\ell_{k}(\\theta)$ as $\\ell_k(\\theta_k)$ by using $\\theta_k$ to represent the parameter involved in $\\ell_k(\\cdot)$.\nThus the parameter $\\theta$ is equivalently represented by $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ in composite\nlikelihood. This necessarily means $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ may contain common elements or elements\nof known values. For example, if $Y$ follows a $d$-variate normal distribution $Y \\sim N_d(\\mu, \\sigma^2 I)$\nwith $\\mu = (\\mu_1, \\dots, \\mu_{d-1},0)^T$ and $I$ being the identity matrix, one may define sub-likelihoods\nusing marginal normal distributions\n$N_1(\\mu_k, \\sigma^2_k)$, $k=1,\\dots, d ={N_{cl}}$ and equate $\\mu_d$ with 0 and all $\\sigma^2_k$'s with $\\sigma^2$.\nIn applying parsimonious likelihood composition a subset of $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ indexed by\nthe composition rule $w$ may be adequate for representing $\\theta$; such a subset is denoted as $\\theta(w)$ from\nthe subspace $\\Theta_w$. It is easy to see that $\\Theta_w \\subseteq \\Theta$ and $\\dim(\\Theta_w)\\leq \\dim(\\Theta)$\nalthough the cardinality $q_w = |\\Theta_w|$ may be greater than $q=|\\Theta|=\\dim(\\Theta)$. In the above\nexample of $Y \\sim N_d(\\mu, \\sigma^2 I)$, $q=d$ and $q_{w}=2(d-1)$ if $w=(1,\\dots,1,0)$.\nThere also exist examples where $\\dim(\\Theta_w)<\\dim(\\Theta)$.\nThe parameter design discussed here is often used to\nsimplify formulation and computation in complex models \\citep{varin2011overview}.\nWith this in mind we regard $\\theta(w)$ as representing $\\theta$ or one of its sub-vectors in this paper,\nand denote the effective dimension of $\\theta(w)$ as $d_w$, $d_w\\leq q$.\n\n\n\n\nFor fixed $w$, the MCLE $\\hat{\\theta}(w)$ based on data of sample size $n$ is a $\\sqrt{n}$-consistent and asymptotically\nnormally distributed estimator of $\\theta(w)$ under appropriate regularity conditions \\citep{varin2011overview}. Specifically,\n$\\sqrt{n} (\\hat{\\theta}(w)-\\theta(w))$ follows asymptotically a $d_w$-variate normal distribution\nwith zero mean and $d_w \\times d_w$ covariance matrix $V(\\theta,w)= G^{-1}(\\theta,w)$, where\n$G(\\theta,w)=n^{-1}H(\\theta,w){J(\\theta,w)}^{-1}H(\\theta,w)$, with $H(\\theta,w)= -E[\\nabla^2 \\ell_{cl}(\\theta; w)]$ and\n$ J(\\theta,w)= Var[\\nabla \\ell_{cl}(\\theta; w)]$ being the Godambe information matrix\n\\citep{godambe1960optimum}.  Next, we exploit MCLE's asymptotic normality to derive sensible test\nstatistics for group difference testing.\n\n\\subsection{Wald-type tests for group differences} \\label{sec:tests}\nLet $Y_i^{(g)} \\sim f(y; \\theta_g)$, $i=1,\\dots,n_g$ ,  $\\theta_g \\in \\Theta\\subseteq \\mathbb{R}^q$,\nbe $d$-vector observations in two groups indexed by $g=0,1$ (e.g. case and control groups).\n\nAs just discussed in section~\\ref{sec2.1}, we represent $\\theta_g$ by $\\theta_g(w_{\\text{all}})=(\\theta_{g1},\\dots,\\theta_{g{N_{cl}}})$,\nwith each $\\theta_{gj}$, $j=1,\\dots,{N_{cl}}$, being a $p$-dimensional parameter vector corresponding to the $j$th\nsub-likelihood. Note that the effective dimension of $\\theta_g(w_{\\text{all}})$ here still equals $q$ thus some $\\theta_{gj}$'s\ngiven $g$ must contain common elements or some elements of known values.\nSuppose $\\theta_g(w_{\\text{all}})$'s are to be estimated by MCLE. A Wald-type statistic can be naturally constructed to\ntest $H_{0}: \\delta \\equiv \\theta_1 - \\theta_0 = 0$ vs. $H_1:\\delta\\neq 0$, which is\n\n", "itemtype": "equation", "pos": 7747, "prevtext": "\n\\doublespacing\n\n\\title{Parsimonious and powerful composite likelihood testing for group difference and genotype-phenotype association}\n\n\\author[1]{Zhendong Huang}\n\\author[1]{Davide Ferrari \\thanks{Corresponding author: Davide Ferrari, School of Mathematics and Statistics, The University of\nMelbourne, Parkville, VIC 3010, Australia. E-mail: \\url{dferrari@unimelb.edu.au}.}}\n\\author[1]{Guoqi Qian}\n\\affil[1]{School of Mathematics and Statistics, The University of Melbourne}\n\\date{}\n\\maketitle\n\n\\begin{abstract}\nTesting the association between a phenotype and many genetic variants from case-control data is essential in genome-wide association study (GWAS). This is a\nchallenging task as many such variants are correlated or non-informative. Similarities exist in testing the population difference between two groups of\nhigh dimensional data with intractable full likelihood function. Testing may be tackled by a maximum composite\nlikelihood (MCL) not entailing the full likelihood, but current MCL tests are subject to power loss for involving non-informative or redundant\nsub-likelihoods.  In this paper, we develop a forward search and test method for simultaneous powerful group difference testing and\ninformative sub-likelihoods composition. Our method constructs a sequence of Wald-type test statistics by including\nonly informative sub-likelihoods progressively so as to improve the test power under local sparsity alternatives.\nNumerical studies show that it achieves considerable improvement over\nthe available tests as the modeling complexity grows.\nOur method is further validated  by testing the motivating GWAS data on breast cancer with interesting results obtained.\n\n\\noindent\n\\textbf{Keywords:} Composite likelihood, Wald test, forward search, SNPs association test\n\\end{abstract}\n\n\\section{Introduction}\n\nTesting population difference between two groups of multivariate data is common in many fields of statistical\nresearch. Due to significant development of data acquisition technologies in recent years, more and more complex data\n--- e.g. involving temporal or spatial dependence among the sample units --- can now be readily collected for statistical\nanalysis. However, this entails the use of tractable statistical models which are not easily available. In particular, it may be\ndifficult or even impossible to specify the full likelihood function for testing the group difference.\nThese challenges are common in analyzing case-control data in genome-wide association study (GWAS), where\nfor example we test associations between a binary breast cancer phenotype and\nvarious genotype variants known as the single nucleotide polymorphisms (SNPs).\nNote that testing genotype-phenotype association from case-control data can be formulated as a two-sample\nstatistical test problem.\nBut association testing for many genotype variants altogether entails a high-dimensional statistical model, and makes\nit difficult to formulate a computationally tractable full likelihood  \\citep{han2012composite}.\n\nThese issues naturally suggests approximating the full likelihood function by a computationally tractable one for\nconstructing the test statistics for association testing. A well-developed approximation is based on the maximum\ncomposite likelihood estimator (MCLE),  obtained by maximizing the product of low-dimensional sub-likelihood objects\ninstead of the full likelihood.   \\cite{besag1974spatial} proposed\ncomposite likelihood estimation for spatial data while \\cite{lindsay1988composite} developed composite\nlikelihood estimation in its generality.  Over the years, composite likelihood methods have proved useful in\nmany applied fields, including  geo-statistics, spatial extremes and  statistical genetics.\nSee \\cite{varin2011overview} for a comprehensive survey on methods and applications.\n\nLike the familiar maximum likelihood estimator (MLE), the MCLE is asymptotically unbiased and normally distributed\nunder regularity conditions. This feature, is beneficial for constructing Wald-type statistics for testing group differences\n(see \\cite{geys1999pseudolikelihood} and \\cite{molenberghs2005models} among others), can also be used in\nMCLE based testing. The standard approach here is to form a statistic using all the available\ndata-subsets (so that the MCLE is computed by combining all the feasible sub-likelihood components).\nAlthough the resulting Wald test has known null distribution in the limit due to the\nasymptotic normality of MCLE, it may exhibit unsatisfactory power when the number of parameters in the model is\nmoderate or large relative to the sample size.\n\nIn our view, forming a test statistic by all the available sub-likelihoods is\nnot always well-justified from either statistical or computational perspective. Specifically, when the noise in the data\nis evident and the statistical model considered is very complex,  inclusion of sub-likelihoods that do not explain group\ndifferences will mainly be adding noise to the Wald statistic. Clearly, this unwanted noise has the undesirable effect\nof deteriorating the overall test power. A better strategy would be to choose only informative sub-likelihoods\nrelevant to group differences, while dropping noisy or redundant components as much as possible.\n\nPrompted by the above discussion, we propose a new approach --- referred to as the forward step-up\ncomposite likelihood (FS-CL) testing --- for group difference testing. Given a set of candidate data subsets used for\nconstructing the sub-likelihood objects, our FS-CL method carries out simultaneous testing and data noise reduction by\nselecting a best set of sub-likelihoods so as to improve the resulting test power. Differently from the existing approaches,\nwe impose a sparsity requirement on our alternative hypothesis reflecting the notion that only certain portion of\ndata subsets fundamentally explains the difference between groups. While testing the null hypothesis of no difference\nbetween groups, our method makes efficient use of data by dropping noisy or redundant data subsets to\nthe maximum extent. This procedure is implemented by a forward search algorithm which, similar to the well-established\nmethods in variable selection, progressively includes one more sub-likelihood at each step until no significant\nimprovement in terms of power is observed.\n\nThe new approach proposed can be extended to general linear hypothesis testing (cf. chapter 7 of\n\\cite{Lehmann2005testing}) without fundamental difficulty, but will not be pursued in detail in this paper.\nThe remainder of the paper is organized as follows. In Section~\\ref{sec:framework}, we describe the main\nframework for composite likelihood estimation and overview the existing Wald-type association tests.  In\nSection~\\ref{sec:method}, we describe the new FS-CL methodology and propose the forward search algorithm.\nIn Section~\\ref{sec:numerical}, we study the finite-sample properties of our method in terms of Type I error probability\nand power using simulated data. In Section~\\ref{sec:real}, we apply our test to the case-control GWAS\ndata from Australian Breast Cancer Family Study. In Section~\\ref{sec:conclusion}, we conclude the paper\nby providing some final remarks.\n\n\\section{Composite likelihood inference} \\label{sec:framework}\n\n\\subsection{Sparse composite likelihood estimation}\\label{sec2.1}\n\nConsider a random sample of $n$ observations on a $d$-dimensional random vector $Y=(Y_1, \\dots, Y_d)^T$ following a probability\ndensity function $f(y; \\theta)$, with unknown parameter $\\theta \\in \\Theta \\subseteq \\mathbb{R}^q$ and\n$q=\\dim(\\Theta) \\geq 1$.\nLet ${\\hat{\\theta}}(w)$ be the profiled maximum composite\nlikelihood estimator (MCLE) of $\\theta$, obtained by  maximizing the composite likelihood function\n\n", "index": 1, "text": "\\begin{equation}  \\label{comp_lik}\n\\ell_{cl}(\\theta; w) = \\left({\\sum_{k=1}^{\\Ncl}} w_k\\right)^{-1}{\\sum_{k=1}^{\\Ncl}}  w_{k} \\ell_{k}(\\theta),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\ell_{cl}(\\theta;w)=\\left({\\sum_{k=1}^{\\Ncl}}w_{k}\\right)^{-1}{\\sum_{k=1}^{%&#10;\\Ncl}}w_{k}\\ell_{k}(\\theta),\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Ncl</mtext></merror></munderover><msub><mi>w</mi><mi>k</mi></msub></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Ncl</mtext></merror></munderover><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\hat{\\delta}  =  \\hat{\\theta}_{1}(w_{\\text{all}})- \\hat{\\theta}_{0}(w_{\\text{all}})$ with $w_{\\text{all}} =(1, \\dots, 1)^T$ and\n$\\hat{\\theta}_{g}(w_{\\text{all}})$, $g=0,1$, being the MCLEs for the two groups;\nand $\\widehat{V} = \\widehat{V}(w_{\\text{all}})$  is a consistent\nestimator of the asymptotic covariance matrix of $\\sqrt{n}\\hat{\\delta}$.\nIt is easy to see that $\\hat{\\delta}$ can be regarded as an MCLE for the parameter difference of the two\ngroups when no sub-likelihood selection is taken.\n\nUnder the null hypothesis $H_0: \\delta =   0$, the statistic $T_{{\\text{Wald}}}$\nfollows asymptotically a chi-square distribution with $q$ degrees of freedom \\citep{molenberghs2005models}.\nAlthough $T_{{\\text{Wald}}}$ has a known null distribution, the power  of\nthe test can be unsatisfactory when $q$ is relatively large. This is due to the fact that with no selection of\nsub-likelihood components, pronounced noise in data subsets that does not explain the difference  between groups\nmay deteriorate Wald test's power as a consequence of inflating the covariance matrix $\\widehat{V}$.\n\nTo mitigate the above issues, \\cite{han2012composite} studied modifications of the basic Wald test.\nThey replaced $\\widehat{V}$ by simpler matrices resulting in two test statistics they called\n$T_{{\\text{LSSB}}}\\equiv n \\hat{\\delta}^{T} \\hat{\\delta}$\nand $T_{{\\text{LSSBw}}}\\equiv n\\hat{\\delta}^{T} \\text{diag}(\\hat{V})^{-1} \\hat{\\delta}$,\n where $\\text{diag}(\\hat{V})$ denotes the diagonal matrix of $\\hat{V}$. The\nasymptotic null distributions for both statistics have the form\n$\\sum_{j=1}^{q}\\tau_{j}X_{j}^{2}$, where $X_{1}^2,\\dots,X_q^2$ are independent chi-square random variables\nwith $1$ degree of freedom, and $\\tau_{j}$ denotes the $j$th eigenvalue of\n$\\hat{V}$ and $\\hat{V} \\text{diag}(\\hat{V})^{-1}$, for the LSSB test and LSSBw test,\nrespectively. Following \\cite{zhang2005approximate}, the distribution of\n$\\sum_{j=1}^{q}\\tau_{j}X_{j}^{2}$ can be approximated by a scaled-shifted\nchi-square distribution for $a\\chi_{r}^{2}+b$, where $\\chi_{r}^{2}$ is random variable having a chi-square\ndistribution with $r$ degrees of freedom, with $a$, $b$ and $r$ given by\n\n", "itemtype": "equation", "pos": 12713, "prevtext": "\nwhere ${N_{cl}}$ is the total number of sub-likelihood objects considered,\n$w=(w_1, \\dots, w_{{N_{cl}}})^T \\in \\Omega =\\{0,1\\}^{{N_{cl}}}$ is a vector of\nbinary weights referred to as composition rule, and\n$\\ell_{k}(\\theta) \\propto \\log f(S_k;\\theta)$\nis the sub-likelihood defined on the $k$th data subset $S_k$. The composite likelihood\ndesign is typically user-specified \\citep{varin2011overview, Lindsay2011issues}. For example, $\\ell_k$ can be  based\non all marginal events ($S_k=\\{y_k \\}$, $k=1,\\dots,d$), all pair-wise events\n($S_k=\\{y_j, y_l\\}$,  $1 \\leq j<l\\leq d$ ), or conditional events ($S_k=\\{y_k|y_j, j\\neq k\\}$, $k=1,\\dots, d$).\n\nIn our parsimonious composition framework, each sub-likelihood $\\ell_{k}(\\theta)$\nis allowed to be selected or not, depending on whether\n$w_k$ takes value $1$ or $0$, which results in an efficient use of the data. The total number of selected\nsub-likelihoods, $\\Vert w \\Vert ={\\sum_{k=1}^{\\Ncl}} w_k$, can be much smaller than the total ${N_{cl}}$ ones available.\nThis is in contrast with the frequently used composite likelihood setting where all the ${N_{cl}}$\nsub-likelihoods are selected. Particularly, in the latter case $w = w_{all} = (1,\\dots, 1)^T$, and no data\nnoise reduction is attained.\n\nA complication related to notations in composite likelihood is that the parameter $\\theta$ does not always have\nall its elements involved in each sub-likelihood $\\ell_{k}(\\theta)$. To facilitate presentation in the sequel, we rewrite\n$\\ell_{k}(\\theta)$ as $\\ell_k(\\theta_k)$ by using $\\theta_k$ to represent the parameter involved in $\\ell_k(\\cdot)$.\nThus the parameter $\\theta$ is equivalently represented by $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ in composite\nlikelihood. This necessarily means $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ may contain common elements or elements\nof known values. For example, if $Y$ follows a $d$-variate normal distribution $Y \\sim N_d(\\mu, \\sigma^2 I)$\nwith $\\mu = (\\mu_1, \\dots, \\mu_{d-1},0)^T$ and $I$ being the identity matrix, one may define sub-likelihoods\nusing marginal normal distributions\n$N_1(\\mu_k, \\sigma^2_k)$, $k=1,\\dots, d ={N_{cl}}$ and equate $\\mu_d$ with 0 and all $\\sigma^2_k$'s with $\\sigma^2$.\nIn applying parsimonious likelihood composition a subset of $(\\theta_1,\\dots, \\theta_{{N_{cl}}})$ indexed by\nthe composition rule $w$ may be adequate for representing $\\theta$; such a subset is denoted as $\\theta(w)$ from\nthe subspace $\\Theta_w$. It is easy to see that $\\Theta_w \\subseteq \\Theta$ and $\\dim(\\Theta_w)\\leq \\dim(\\Theta)$\nalthough the cardinality $q_w = |\\Theta_w|$ may be greater than $q=|\\Theta|=\\dim(\\Theta)$. In the above\nexample of $Y \\sim N_d(\\mu, \\sigma^2 I)$, $q=d$ and $q_{w}=2(d-1)$ if $w=(1,\\dots,1,0)$.\nThere also exist examples where $\\dim(\\Theta_w)<\\dim(\\Theta)$.\nThe parameter design discussed here is often used to\nsimplify formulation and computation in complex models \\citep{varin2011overview}.\nWith this in mind we regard $\\theta(w)$ as representing $\\theta$ or one of its sub-vectors in this paper,\nand denote the effective dimension of $\\theta(w)$ as $d_w$, $d_w\\leq q$.\n\n\n\n\nFor fixed $w$, the MCLE $\\hat{\\theta}(w)$ based on data of sample size $n$ is a $\\sqrt{n}$-consistent and asymptotically\nnormally distributed estimator of $\\theta(w)$ under appropriate regularity conditions \\citep{varin2011overview}. Specifically,\n$\\sqrt{n} (\\hat{\\theta}(w)-\\theta(w))$ follows asymptotically a $d_w$-variate normal distribution\nwith zero mean and $d_w \\times d_w$ covariance matrix $V(\\theta,w)= G^{-1}(\\theta,w)$, where\n$G(\\theta,w)=n^{-1}H(\\theta,w){J(\\theta,w)}^{-1}H(\\theta,w)$, with $H(\\theta,w)= -E[\\nabla^2 \\ell_{cl}(\\theta; w)]$ and\n$ J(\\theta,w)= Var[\\nabla \\ell_{cl}(\\theta; w)]$ being the Godambe information matrix\n\\citep{godambe1960optimum}.  Next, we exploit MCLE's asymptotic normality to derive sensible test\nstatistics for group difference testing.\n\n\\subsection{Wald-type tests for group differences} \\label{sec:tests}\nLet $Y_i^{(g)} \\sim f(y; \\theta_g)$, $i=1,\\dots,n_g$ ,  $\\theta_g \\in \\Theta\\subseteq \\mathbb{R}^q$,\nbe $d$-vector observations in two groups indexed by $g=0,1$ (e.g. case and control groups).\n\nAs just discussed in section~\\ref{sec2.1}, we represent $\\theta_g$ by $\\theta_g(w_{\\text{all}})=(\\theta_{g1},\\dots,\\theta_{g{N_{cl}}})$,\nwith each $\\theta_{gj}$, $j=1,\\dots,{N_{cl}}$, being a $p$-dimensional parameter vector corresponding to the $j$th\nsub-likelihood. Note that the effective dimension of $\\theta_g(w_{\\text{all}})$ here still equals $q$ thus some $\\theta_{gj}$'s\ngiven $g$ must contain common elements or some elements of known values.\nSuppose $\\theta_g(w_{\\text{all}})$'s are to be estimated by MCLE. A Wald-type statistic can be naturally constructed to\ntest $H_{0}: \\delta \\equiv \\theta_1 - \\theta_0 = 0$ vs. $H_1:\\delta\\neq 0$, which is\n\n", "index": 3, "text": "\\begin{align} \\label{Wald}\nT_{\\text{Wald}}\\equiv n \\ \\hat{\\delta}^{T} \\widehat{V}^{-1} \\hat{\\delta} ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T_{\\text{Wald}}\\equiv n\\ \\hat{\\delta}^{T}\\widehat{V}^{-1}\\hat{%&#10;\\delta},\" display=\"inline\"><mrow><mrow><msub><mi>T</mi><mtext>Wald</mtext></msub><mo>\u2261</mo><mrow><mpadded width=\"+5pt\"><mi>n</mi></mpadded><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mi>T</mi></msup><mo>\u2062</mo><msup><mover accent=\"true\"><mi>V</mi><mo>^</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nThe LSSB and LSSBw statistics are easier to compute compared to (\\ref{Wald}). But they may still have\nlow power when the sample size is not large enough. Another common test  is the UminP test with test statistic\n$T_{{\\text{UminP}}}= \\max_{\\substack{1\\leq j\\leq q}}\\{\\sqrt{n}|\\hat{\\delta}|_{j}/{\\hat{V}^{1/2}_{jj}} \\}$\nwhere $\\hat{V}_{jj}$ is the $j$th diagonal element of $\\hat{V}$ \\citep{pan2009asymptotic}.  Other test statistics\nhave been derived from the composite likelihood ratio (CLR) test and score test reviewed in \\cite{varin2011overview}.\nWe focus on Wald-type tests in this paper, but our rationale can be easily extended to CLR and score tests.\n\n\\section{Parsimonious composite likelihood testing} \\label{sec:method}\n\\subsection{Optimal Wald composite test under sparse local alternatives} \\label{sec:optimal}\n\nRecall that $\\delta = \\theta_1 -\\theta_0$ as defined in Section~\\ref{sec:tests} is equivalent to\n$\\delta(w_{\\text{all}})=\\theta_1(w_{\\text{all}})-\\theta_0(w_{\\text{all}})$ which is a $p\\times{N_{cl}}$ matrix of effective dimension\n$q$ giving the group difference. Given a composition rule $w$ which is an ${N_{cl}}$-vector of 1s and 0s,\nlet $\\delta(w)$ be the same as $\\delta(w_{\\text{all}})$ except its $j$th column $\\delta_j(w)=0$ whenever $w_j=0$,\n$j=1,\\dots, {N_{cl}}$. Following the discussion in Section~\\ref{sec2.1}, we still use $d_w$ to denote the effective\ndimension of $\\delta(w)$ knowing that $d_w\\leq q$,\nand we want to test $H_0: \\delta=0$ against $H_1: \\delta \\neq 0$.\nSince some sub-likelihoods for the data of the pooled vector variable $Y=(Y^{(0)T},Y^{(1)T})^T$ may not\ncontain any significant information about $\\delta$, testing these hypotheses using all candidate sub-likelihoods\nwithout selection is unlikely to have a good power.\n\nA plausible approach to overcoming this difficulty is to use more specific alternative hypotheses\nby incorporating the composite rule information.\nSince models containing redundant sub-likelihoods are unlikely to efficiently capture the group difference\ninformation, we prefer to exclude them from consideration in our test by further adding a\nsparsity specification on the composition rule to the alternative hypothesis.\nNow we expect a powerful group difference test can be achieved by sequentially testing the\nnull hypothesis against some alternatives containing a priori composition information and sparsity specification:\n\n", "itemtype": "equation", "pos": 15000, "prevtext": "\nwhere $\\hat{\\delta}  =  \\hat{\\theta}_{1}(w_{\\text{all}})- \\hat{\\theta}_{0}(w_{\\text{all}})$ with $w_{\\text{all}} =(1, \\dots, 1)^T$ and\n$\\hat{\\theta}_{g}(w_{\\text{all}})$, $g=0,1$, being the MCLEs for the two groups;\nand $\\widehat{V} = \\widehat{V}(w_{\\text{all}})$  is a consistent\nestimator of the asymptotic covariance matrix of $\\sqrt{n}\\hat{\\delta}$.\nIt is easy to see that $\\hat{\\delta}$ can be regarded as an MCLE for the parameter difference of the two\ngroups when no sub-likelihood selection is taken.\n\nUnder the null hypothesis $H_0: \\delta =   0$, the statistic $T_{{\\text{Wald}}}$\nfollows asymptotically a chi-square distribution with $q$ degrees of freedom \\citep{molenberghs2005models}.\nAlthough $T_{{\\text{Wald}}}$ has a known null distribution, the power  of\nthe test can be unsatisfactory when $q$ is relatively large. This is due to the fact that with no selection of\nsub-likelihood components, pronounced noise in data subsets that does not explain the difference  between groups\nmay deteriorate Wald test's power as a consequence of inflating the covariance matrix $\\widehat{V}$.\n\nTo mitigate the above issues, \\cite{han2012composite} studied modifications of the basic Wald test.\nThey replaced $\\widehat{V}$ by simpler matrices resulting in two test statistics they called\n$T_{{\\text{LSSB}}}\\equiv n \\hat{\\delta}^{T} \\hat{\\delta}$\nand $T_{{\\text{LSSBw}}}\\equiv n\\hat{\\delta}^{T} \\text{diag}(\\hat{V})^{-1} \\hat{\\delta}$,\n where $\\text{diag}(\\hat{V})$ denotes the diagonal matrix of $\\hat{V}$. The\nasymptotic null distributions for both statistics have the form\n$\\sum_{j=1}^{q}\\tau_{j}X_{j}^{2}$, where $X_{1}^2,\\dots,X_q^2$ are independent chi-square random variables\nwith $1$ degree of freedom, and $\\tau_{j}$ denotes the $j$th eigenvalue of\n$\\hat{V}$ and $\\hat{V} \\text{diag}(\\hat{V})^{-1}$, for the LSSB test and LSSBw test,\nrespectively. Following \\cite{zhang2005approximate}, the distribution of\n$\\sum_{j=1}^{q}\\tau_{j}X_{j}^{2}$ can be approximated by a scaled-shifted\nchi-square distribution for $a\\chi_{r}^{2}+b$, where $\\chi_{r}^{2}$ is random variable having a chi-square\ndistribution with $r$ degrees of freedom, with $a$, $b$ and $r$ given by\n\n", "index": 5, "text": "\\begin{align*}\na=\\frac{\\sum_{j=1}^q\\tau_{j}^{3}}{\\sum_{j=1}^q\\tau_{j}^{2}},\\;\\;\\;\\;\\;\nb=\\sum_{j=1}^q\\tau_{j}-\\frac{(\\sum_{j=1}^q\\tau_{j}^{2})^{2}}{\\sum_{j=1}^q\\tau_{j}^{3}},\n\\;\\;\\;\\;\\;r=\\frac{(\\sum_{j=1}^q\\tau_{j}^{2})^{3}}{(\\sum_{j=1}^q\\tau_{j}^{3})^{2}}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle a=\\frac{\\sum_{j=1}^{q}\\tau_{j}^{3}}{\\sum_{j=1}^{q}\\tau_{j}^{2}},%&#10;\\;\\;\\;\\;\\;b=\\sum_{j=1}^{q}\\tau_{j}-\\frac{(\\sum_{j=1}^{q}\\tau_{j}^{2})^{2}}{%&#10;\\sum_{j=1}^{q}\\tau_{j}^{3}},\\;\\;\\;\\;\\;r=\\frac{(\\sum_{j=1}^{q}\\tau_{j}^{2})^{3}%&#10;}{(\\sum_{j=1}^{q}\\tau_{j}^{3})^{2}}.\" display=\"inline\"><mrow><mrow><mrow><mi>a</mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>3</mn></msubsup></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle></mrow><mo rspace=\"16.5pt\">,</mo><mrow><mrow><mi>b</mi><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover></mstyle><msub><mi>\u03c4</mi><mi>j</mi></msub></mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>3</mn></msubsup></mrow></mfrac></mstyle></mrow></mrow><mo rspace=\"16.5pt\">,</mo><mrow><mi>r</mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mn>3</mn></msup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><msubsup><mi>\u03c4</mi><mi>j</mi><mn>3</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nHere $w$ is a composition rule given a priori; and $\\Vert w \\Vert = {N_{cl}}^\\ast$, with ${N_{cl}}^\\ast\\leq {N_{cl}}$ also given a priori,\nis regarded as a constraint on the model composition complexity. We will investigate how to choose $w$ and ${N_{cl}}^\\ast$\nin detail in sections~\\ref{sec:alg} and \\ref{sec:modelcomplexity}.\n\n\n\n\n\nGiven a composition rule $w$ of size ${N_{cl}}^\\ast$, we consider an MCLE of\n$\\delta$ defined as ${\\hat{\\delta}}(w) ={\\hat{\\theta}}_1(w) - {\\hat{\\theta}}_0(w)$, where ${\\hat{\\theta}}_g(w)$, $g=0,1$,\nare group-specific profiled MCLEs, and test (\\ref{eq3}) by the Wald test statistic\n\n", "itemtype": "equation", "pos": 17676, "prevtext": "\nThe LSSB and LSSBw statistics are easier to compute compared to (\\ref{Wald}). But they may still have\nlow power when the sample size is not large enough. Another common test  is the UminP test with test statistic\n$T_{{\\text{UminP}}}= \\max_{\\substack{1\\leq j\\leq q}}\\{\\sqrt{n}|\\hat{\\delta}|_{j}/{\\hat{V}^{1/2}_{jj}} \\}$\nwhere $\\hat{V}_{jj}$ is the $j$th diagonal element of $\\hat{V}$ \\citep{pan2009asymptotic}.  Other test statistics\nhave been derived from the composite likelihood ratio (CLR) test and score test reviewed in \\cite{varin2011overview}.\nWe focus on Wald-type tests in this paper, but our rationale can be easily extended to CLR and score tests.\n\n\\section{Parsimonious composite likelihood testing} \\label{sec:method}\n\\subsection{Optimal Wald composite test under sparse local alternatives} \\label{sec:optimal}\n\nRecall that $\\delta = \\theta_1 -\\theta_0$ as defined in Section~\\ref{sec:tests} is equivalent to\n$\\delta(w_{\\text{all}})=\\theta_1(w_{\\text{all}})-\\theta_0(w_{\\text{all}})$ which is a $p\\times{N_{cl}}$ matrix of effective dimension\n$q$ giving the group difference. Given a composition rule $w$ which is an ${N_{cl}}$-vector of 1s and 0s,\nlet $\\delta(w)$ be the same as $\\delta(w_{\\text{all}})$ except its $j$th column $\\delta_j(w)=0$ whenever $w_j=0$,\n$j=1,\\dots, {N_{cl}}$. Following the discussion in Section~\\ref{sec2.1}, we still use $d_w$ to denote the effective\ndimension of $\\delta(w)$ knowing that $d_w\\leq q$,\nand we want to test $H_0: \\delta=0$ against $H_1: \\delta \\neq 0$.\nSince some sub-likelihoods for the data of the pooled vector variable $Y=(Y^{(0)T},Y^{(1)T})^T$ may not\ncontain any significant information about $\\delta$, testing these hypotheses using all candidate sub-likelihoods\nwithout selection is unlikely to have a good power.\n\nA plausible approach to overcoming this difficulty is to use more specific alternative hypotheses\nby incorporating the composite rule information.\nSince models containing redundant sub-likelihoods are unlikely to efficiently capture the group difference\ninformation, we prefer to exclude them from consideration in our test by further adding a\nsparsity specification on the composition rule to the alternative hypothesis.\nNow we expect a powerful group difference test can be achieved by sequentially testing the\nnull hypothesis against some alternatives containing a priori composition information and sparsity specification:\n\n", "index": 7, "text": "\\begin{align}\nH_0: \\delta=  0 \\ \\ \\text{against } \\ \\ \\ H_1: \\delta(w) &  \\neq 0 \\;\\; \\text{and} \\;\\;\\Vert w \\Vert ={N_{cl}}^\\ast.\n\\label{eq3}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H_{0}:\\delta=0\\ \\ \\text{against }\\ \\ \\ H_{1}:\\delta(w)\" display=\"inline\"><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mrow><mi>\u03b4</mi><mo>=</mo><mrow><mn>0</mn><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mtext>against\u00a0</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><msub><mi>H</mi><mn>1</mn></msub></mrow></mrow><mo>:</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\neq 0\\;\\;\\text{and}\\;\\;\\|w\\|={N_{cl}}^{\\ast}.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2260</mo><mrow><mpadded width=\"+5.6pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mpadded width=\"+5.6pt\"><mtext>and</mtext></mpadded><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow></mrow><mo>=</mo><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $V(w)$ is the asymptotic variance matrix of $\\sqrt{n}{\\hat{\\delta}}(w)$. For given ${N_{cl}}^\\ast$,\nwe assume there is an optimal  composition rule, $w^\\ast \\in \\{0,1\\}^{{N_{cl}}}$,\ntypically with size $\\Vert w^\\ast \\Vert={N_{cl}}^\\ast$ much smaller than ${N_{cl}}$, such that the\ncorresponding test statistic $T(w^\\ast)$ is most powerful among those derived from\nall composition rules of size ${N_{cl}}^\\ast$. Namely,\n\n", "itemtype": "equation", "pos": 18447, "prevtext": "\nHere $w$ is a composition rule given a priori; and $\\Vert w \\Vert = {N_{cl}}^\\ast$, with ${N_{cl}}^\\ast\\leq {N_{cl}}$ also given a priori,\nis regarded as a constraint on the model composition complexity. We will investigate how to choose $w$ and ${N_{cl}}^\\ast$\nin detail in sections~\\ref{sec:alg} and \\ref{sec:modelcomplexity}.\n\n\n\n\n\nGiven a composition rule $w$ of size ${N_{cl}}^\\ast$, we consider an MCLE of\n$\\delta$ defined as ${\\hat{\\delta}}(w) ={\\hat{\\theta}}_1(w) - {\\hat{\\theta}}_0(w)$, where ${\\hat{\\theta}}_g(w)$, $g=0,1$,\nare group-specific profiled MCLEs, and test (\\ref{eq3}) by the Wald test statistic\n\n", "index": 9, "text": "\\begin{align}\\label{ideal}\nT(w)\\equiv  n\\hat{\\delta}(w)^{T} V(w)^{-1} \\hat{\\delta}(w),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T(w)\\equiv n\\hat{\\delta}(w)^{T}V(w)^{-1}\\hat{\\delta}(w),\" display=\"inline\"><mrow><mrow><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mrow><mi>n</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\chi^2(a,b)$ denotes a random variable following a non-central chi-square distribution with degree of\nfreedom $a$ and non-centrality parameter $b$, and $Q_\\alpha(k)$ is the upper $\\alpha$-quantile of\n$\\chi^{2}(k,0)$ with $\\alpha$ being the significance level. The non-centrality quantity in\n(\\ref{Waldo}) is $\\lambda(w) = nE(\\hat{\\delta}(w))^{T}V(w)^{-1}E(\\hat{\\delta}(w))$.\n\nThe optimal test statistic $T(w^\\ast)$ has a straightforward interpretation:\nit is determined by the MCLE $\\hat{\\delta}(w^\\ast)$ and according to (\\ref{Waldo}), gives the\nlargest power among all Wald test statistics of form (\\ref{ideal}) for testing (\\ref{eq3}).\nUnder $H_0: \\delta=0$, $T(w^\\ast)$ follows the chi-square distribution with degrees of freedom\n $d_{w^\\ast}$ as $n \\rightarrow \\infty$ when $w^\\ast$ is given. This null\ndistribution is the same as that for the usual Wald test statistic (\\ref{Wald}), except that the degree of freedom\n$d_{w^\\ast}$ may be smaller than $q$ due to the use of the informative composition rule $w^\\ast$.\n\n\n\n\\subsection{Forward-search algorithm} \\label{sec:alg}\n\nThe ideal test statistic $T(w^\\ast)$ outlined in the previous section is appealing from a theoretical viewpoint, but\nnot very useful in practice, since it is not obvious how to compute the optimal composition rule $w^\\ast$ and the\nasymptotic covariance $V(w^\\ast)$. Such quantities need to be carefully estimated in order to maintain $T(w^\\ast)$'s power.\nWe first proceed to estimate the optimal composition rule $w^\\ast$, which is computationally challenging\neven when the number of feasible sub-likelihoods ${N_{cl}}$ is moderate since the search space contains\n$2^{{N_{cl}}}-1$ possible composition rules. Assuming ${N_{cl}}$ is given and $\\widehat{V}(w)$ is available for\nestimating $V(w)$, we propose the\nfollowing step-up forward search algorithm to efficiently estimate $w^\\ast$.\n\nLet $w_{{\\mathcal{A}}} \\in \\{0,1\\}^{{N_{cl}}}$ be a vector with its elements at index ${\\mathcal{A}} \\subseteq \\{1, \\dots, {N_{cl}}\\}$ equal to 1,\nand zero elsewhere. At each iteration $t=0, 1, 2, \\dots$ of the\nfollowing algorithm, ${\\mathcal{A}}^{(t)}$ denotes the index set of the active sub-likelihoods used in the Wald test statistic (\\ref{ideal}).\n\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\\label{algorithm1}\n\\textbf{Main Algorithm: forward step-up composite likelihood (FS-CL) based test}\\\\\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\\begin{enumerate}\n\\item[0.]\nInitialization. Set $t=0$ (iteration counter) and ${\\mathcal{A}}^{(0)} = \\emptyset$\n(active set of sub-likelihoods).\n\\item\nFind a new sub-likelihood component with its index\n\n", "itemtype": "equation", "pos": 18971, "prevtext": "\nwhere $V(w)$ is the asymptotic variance matrix of $\\sqrt{n}{\\hat{\\delta}}(w)$. For given ${N_{cl}}^\\ast$,\nwe assume there is an optimal  composition rule, $w^\\ast \\in \\{0,1\\}^{{N_{cl}}}$,\ntypically with size $\\Vert w^\\ast \\Vert={N_{cl}}^\\ast$ much smaller than ${N_{cl}}$, such that the\ncorresponding test statistic $T(w^\\ast)$ is most powerful among those derived from\nall composition rules of size ${N_{cl}}^\\ast$. Namely,\n\n", "index": 11, "text": "\\begin{align} \\label{Waldo}\nw^\\ast = {\\operatorname{arg\\,max}}_{w: \\ \\Vert w \\Vert = {N_{cl}}^\\ast} \\ \\ P\\left\\{ \\chi^2(d_w, \\lambda(w)) >\nQ_\\alpha(d_w)  \\right\\},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w^{\\ast}={\\operatorname{arg\\,max}}_{w:\\ \\|w\\|={N_{cl}}^{\\ast}}\\ %&#10;\\ P\\left\\{\\chi^{2}(d_{w},\\lambda(w))&gt;Q_{\\alpha}(d_{w})\\right\\},\" display=\"inline\"><mrow><msup><mi>w</mi><mo>\u2217</mo></msup><mo>=</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mi>w</mi><mo rspace=\"7.5pt\">:</mo><mrow><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mo>=</mo><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts></mrow></mrow></msub><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>P</mi><mrow><mo>{</mo><msup><mi>\u03c7</mi><mn>2</mn></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>w</mi></msub><mo>,</mo><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><msub><mi>Q</mi><mi>\u03b1</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>}</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $w_i^{(t)}= w_{{\\mathcal{A}}^{(t)} \\cup \\{i\\}}$ augmenting $w_{{\\mathcal{A}}^{(t)}}$,\n$\\overline{{\\mathcal{A}}}^{(t)}=\\{1,\\dots, {N_{cl}}\\} \\setminus {\\mathcal{A}}^{(t)}$ complementing ${\\mathcal{A}}^{(t)}$, $\\hat{\\delta}(w)$ is the MCLE of\n$\\delta(w)$ and $\\widehat{V}(w)$ is a consistent estimate of $V(w)$.\n\\item\nUpdate the active set of sub-likelihoods ${\\mathcal{A}}^{(t+1)}=\n{\\mathcal{A}}^{(t)}\\cup{\\{h^{(t+1)}\\}}$.\n\\item\nSet $t=t+1$. Repeat 1 and 2 if $t < {N_{cl}}^\\ast$. Otherwise, stop the\nalgorithm and obtain the composition rule $\\hat{w}\\equiv w_{{\\mathcal{A}}^{({N_{cl}}^\\ast)}}$, regarding it as an optimal estimate of\n$w^\\ast$.\n\\end{enumerate}\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\n\nThe rationale underlying the above algorithm is similar to well-established step-wise\nalgorithms used in the context of regression variable selection.\nStep 1 finds the most promising sub-likelihood component in terms of its added signal relative to noise\nin the current test statistics.\nStep 2 simply augments the current active set of sub-likelihoods, ${\\mathcal{A}}^{(t+1)}$, by\nincluding the newly selected sub-likelihood. Step 3 gives a stopping criterion in\nterms of the allowed maximum number of sub-likelihood components, ${N_{cl}}^\\ast$, which is regarded as a\ncomplexity parameter for the overall composite likelihood function. A separate discussion on the choice of\n${N_{cl}}^\\ast$ is given in Section \\ref{sec:modelcomplexity}.\n\n\nThe algorithm carries out ${N_{cl}}^\\ast({N_{cl}}-0.5({N_{cl}}^\\ast-1))$ evaluations of the MCLE of $\\delta$, which\nis much smaller than the exponential rate in exhaustive evaluation.  The final test statistic is\n\n", "itemtype": "equation", "pos": 21791, "prevtext": "\nwhere $\\chi^2(a,b)$ denotes a random variable following a non-central chi-square distribution with degree of\nfreedom $a$ and non-centrality parameter $b$, and $Q_\\alpha(k)$ is the upper $\\alpha$-quantile of\n$\\chi^{2}(k,0)$ with $\\alpha$ being the significance level. The non-centrality quantity in\n(\\ref{Waldo}) is $\\lambda(w) = nE(\\hat{\\delta}(w))^{T}V(w)^{-1}E(\\hat{\\delta}(w))$.\n\nThe optimal test statistic $T(w^\\ast)$ has a straightforward interpretation:\nit is determined by the MCLE $\\hat{\\delta}(w^\\ast)$ and according to (\\ref{Waldo}), gives the\nlargest power among all Wald test statistics of form (\\ref{ideal}) for testing (\\ref{eq3}).\nUnder $H_0: \\delta=0$, $T(w^\\ast)$ follows the chi-square distribution with degrees of freedom\n $d_{w^\\ast}$ as $n \\rightarrow \\infty$ when $w^\\ast$ is given. This null\ndistribution is the same as that for the usual Wald test statistic (\\ref{Wald}), except that the degree of freedom\n$d_{w^\\ast}$ may be smaller than $q$ due to the use of the informative composition rule $w^\\ast$.\n\n\n\n\\subsection{Forward-search algorithm} \\label{sec:alg}\n\nThe ideal test statistic $T(w^\\ast)$ outlined in the previous section is appealing from a theoretical viewpoint, but\nnot very useful in practice, since it is not obvious how to compute the optimal composition rule $w^\\ast$ and the\nasymptotic covariance $V(w^\\ast)$. Such quantities need to be carefully estimated in order to maintain $T(w^\\ast)$'s power.\nWe first proceed to estimate the optimal composition rule $w^\\ast$, which is computationally challenging\neven when the number of feasible sub-likelihoods ${N_{cl}}$ is moderate since the search space contains\n$2^{{N_{cl}}}-1$ possible composition rules. Assuming ${N_{cl}}$ is given and $\\widehat{V}(w)$ is available for\nestimating $V(w)$, we propose the\nfollowing step-up forward search algorithm to efficiently estimate $w^\\ast$.\n\nLet $w_{{\\mathcal{A}}} \\in \\{0,1\\}^{{N_{cl}}}$ be a vector with its elements at index ${\\mathcal{A}} \\subseteq \\{1, \\dots, {N_{cl}}\\}$ equal to 1,\nand zero elsewhere. At each iteration $t=0, 1, 2, \\dots$ of the\nfollowing algorithm, ${\\mathcal{A}}^{(t)}$ denotes the index set of the active sub-likelihoods used in the Wald test statistic (\\ref{ideal}).\n\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\\label{algorithm1}\n\\textbf{Main Algorithm: forward step-up composite likelihood (FS-CL) based test}\\\\\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\\begin{enumerate}\n\\item[0.]\nInitialization. Set $t=0$ (iteration counter) and ${\\mathcal{A}}^{(0)} = \\emptyset$\n(active set of sub-likelihoods).\n\\item\nFind a new sub-likelihood component with its index\n\n", "index": 13, "text": "$$\nh^{(t+1)} =\\underset{i\\in \\overline{{\\mathcal{A}}}^{(t)}}{\\text{argmax}} \\  \\\nn\\hat{\\delta}(w_i^{(t)})^{T} \\{\\widehat{V}(w_i^{(t)})\\}^{-1}\n\\hat{\\delta}(w_i^{(t)}) \\equiv \\underset{i\\in \\overline{{\\mathcal{A}}}^{(t)}}{\\text{argmax}}  \\ \\lambda^{(t)}_i\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"h^{(t+1)}=\\underset{i\\in\\overline{{\\mathcal{A}}}^{(t)}}{\\text{argmax}}\\ \\ n%&#10;\\hat{\\delta}(w_{i}^{(t)})^{T}\\{\\widehat{V}(w_{i}^{(t)})\\}^{-1}\\hat{\\delta}(w_{%&#10;i}^{(t)})\\equiv\\underset{i\\in\\overline{{\\mathcal{A}}}^{(t)}}{\\text{argmax}}\\ %&#10;\\lambda^{(t)}_{i}\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munder accentunder=\"true\"><mtext>argmax</mtext><mrow><mi>i</mi><mo>\u2208</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></munder></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">{</mo><mrow><mover accent=\"true\"><mi>V</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mrow><mpadded width=\"+5pt\"><munder accentunder=\"true\"><mtext>argmax</mtext><mrow><mi>i</mi><mo>\u2208</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></munder></mpadded><mo>\u2062</mo><msubsup><mi>\u03bb</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nOnce the null distribution of (\\ref{estimated}) is determined, which will be detailed in Section~\\ref{sec:nulldistribution},\n$T_{{\\text{fscl}}}$ will be used to test (\\ref{eq3}).\nNote that if ${N_{cl}}^\\ast={N_{cl}}$, the resulting test is then equivalent to the classic Wald test including all\nthe sub-likelihood components but may incur much unnecessary computing. However,\nthe test can be much more powerful if many sub-likelihoods are\nredundant and be computationally efficient if ${N_{cl}}^\\ast$ is not large.\nWhen ${N_{cl}}^\\ast = 1$ and there is only one parameter to estimate in each sub-likelihood component (i.e. $p =1$),\n$T_{\\text{fscl}}$ will have the same form as the $T^2_{\\text{UminP}}$ test statistic of \\cite{pan2009asymptotic}\ngiven in  Section~\\ref{sec:tests}.\n\nIt is difficult to estimate the asymptotic covariance matrix $V(w)$ based on the analytical formula provided at\nthe end of Section~\\ref{sec2.1}. Instead, the estimated asymptotic covariance matrix $\\widehat{V}(w)$\nin Step $1$ of the algorithm can be obtained by using non-parametric bootstrap.\nSpecifically, we sample with replacement the observations within each group and then compute the bootstrap\nreplicates of the MCLE for $\\delta(w)$, denoted as $\\delta^\\ast_1(w),\\dots,\\delta^\\ast_B(w)$.\nWe use the replicates to compute\n$\\widehat{V}(w)$ as\n\n", "itemtype": "equation", "pos": 23726, "prevtext": "\nwhere $w_i^{(t)}= w_{{\\mathcal{A}}^{(t)} \\cup \\{i\\}}$ augmenting $w_{{\\mathcal{A}}^{(t)}}$,\n$\\overline{{\\mathcal{A}}}^{(t)}=\\{1,\\dots, {N_{cl}}\\} \\setminus {\\mathcal{A}}^{(t)}$ complementing ${\\mathcal{A}}^{(t)}$, $\\hat{\\delta}(w)$ is the MCLE of\n$\\delta(w)$ and $\\widehat{V}(w)$ is a consistent estimate of $V(w)$.\n\\item\nUpdate the active set of sub-likelihoods ${\\mathcal{A}}^{(t+1)}=\n{\\mathcal{A}}^{(t)}\\cup{\\{h^{(t+1)}\\}}$.\n\\item\nSet $t=t+1$. Repeat 1 and 2 if $t < {N_{cl}}^\\ast$. Otherwise, stop the\nalgorithm and obtain the composition rule $\\hat{w}\\equiv w_{{\\mathcal{A}}^{({N_{cl}}^\\ast)}}$, regarding it as an optimal estimate of\n$w^\\ast$.\n\\end{enumerate}\n\\noindent\\makebox[\\linewidth]{\\rule{\\textwidth}{0.4pt}}\n\n\nThe rationale underlying the above algorithm is similar to well-established step-wise\nalgorithms used in the context of regression variable selection.\nStep 1 finds the most promising sub-likelihood component in terms of its added signal relative to noise\nin the current test statistics.\nStep 2 simply augments the current active set of sub-likelihoods, ${\\mathcal{A}}^{(t+1)}$, by\nincluding the newly selected sub-likelihood. Step 3 gives a stopping criterion in\nterms of the allowed maximum number of sub-likelihood components, ${N_{cl}}^\\ast$, which is regarded as a\ncomplexity parameter for the overall composite likelihood function. A separate discussion on the choice of\n${N_{cl}}^\\ast$ is given in Section \\ref{sec:modelcomplexity}.\n\n\nThe algorithm carries out ${N_{cl}}^\\ast({N_{cl}}-0.5({N_{cl}}^\\ast-1))$ evaluations of the MCLE of $\\delta$, which\nis much smaller than the exponential rate in exhaustive evaluation.  The final test statistic is\n\n", "index": 15, "text": "\\begin{align}\\label{estimated}\nT_{{\\text{fscl}}} =  n\\hat{\\delta}(\\hat{w})^{T}\n\\widehat{V}(\\hat{w})^{-1} \\hat{\\delta}(\\hat{w}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T_{{\\text{fscl}}}=n\\hat{\\delta}(\\hat{w})^{T}\\widehat{V}(\\hat{w})%&#10;^{-1}\\hat{\\delta}(\\hat{w}).\" display=\"inline\"><mrow><mrow><msub><mi>T</mi><mtext>fscl</mtext></msub><mo>=</mo><mrow><mi>n</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mover accent=\"true\"><mi>V</mi><mo>^</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\overline{\\delta^\\ast}(w)=(1/B)\\sum_{b=1}^B \\delta^\\ast_b(w)$. Our empirical study shows that\nthe one-dimensional $T_{{\\text{fscl}}}$ statistics is robust under non-parametric bootstrap and it is\nsufficient for setting $B=1000$ for most cases in practice. The Jackknife method for computing $\\widehat{V}(w)$\nmay also be used.\n\n\n\n\n\n\n\nFigure~\\ref{fig:powerplot12} illustrates the power of the final test statistic $T_{{\\text{fscl}}}$ in a simple simulated example\nwhere two samples of 20-dimensional normal vectors, each of size 18, are generated to test the mean difference.\nThe normal distribution for the first sample is $\\mathcal{N}_{20}(\\theta_0\\!=\\!0,\\, 9I)$ and that for the second sample is\n$\\mathcal{N}_{20}(\\theta_1, 9I)$ with $I$ being the $20\\times 20$ identity matrix and $\\theta_1=(\\delta_0, 0,\\dots,0)^T$.\nOnce the data are simulated, we ignore the parameter values underlying the true distributions and proceed to\ntest $H_0: \\delta=\\theta_1-\\theta_0=0$ at significance level $\\alpha=0.05$ using the proposed forward search\nalgorithm and the FS-CL test statistic $T_{{\\text{fscl}}}$ together with its simulated null distribution.\nWe set the ${N_{cl}}=20$ marginal sub-likelihoods as all the ones available and consider four specified\nvalues for ${N_{cl}}^\\ast$ in $H_1$ in (\\ref{eq3}), i.e. ${N_{cl}}^\\ast= 1, 5, 10$, or 20 (which gives the classic Wald test).\nThe power results based on 10,000 simulations of the two-sample data are plotted in Figure~\\ref{fig:powerplot12}.\nWe see that the FS-CL test dominates the Wald\ntest in terms of power for any ${N_{cl}}^\\ast<20$. Clearly the largest power gain is obtained when ${N_{cl}}^\\ast=1$,\nwhich should be the case since only the first marginal sub-likelihood contains the information about the\nnonzero component in $\\delta$ in truth.\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[h]\n\\centering\n\n\\includegraphics[scale=0.7]{n=36.pdf}\n    \\caption{Power of the FS-CL test for hypotheses given in (\\ref{eq3}) where the first sample of size 18 come from\n$\\mathcal{N}_{20}(0,9I)$ and the second sample of size 18 from $\\mathcal{N}_{20}(\\delta, 9I)$ with\n$\\delta=(\\delta_0, 0,\\dots,0)$.\nThe solid black curve represents the Wald test corresponding to ${N_{cl}}^\\ast=20$.\nPower curves are estimated using 10,000 Monte Carlo simulations.}\n\\label{fig:powerplot12}\n\\end{figure}\n\n\n\n\\subsection{Null distribution for the FS-CL test}\n\\label{sec:nulldistribution}\n\nThe null distribution of the FS-CL test statistic $T_{{\\text{fscl}}}$ is needed for drawing a conclusion for the test.\nLet's first consider a trivial case where the MCLE ${\\hat{\\delta}} = ({\\hat{\\delta}}_1, \\dots,\n{\\hat{\\delta}}_{{N_{cl}}})$ has its columns independent of each other; and each of its columns has the same\neffective dimension $p^\\prime$ and the same asymptotic distribution.\nThen one can deduce the asymptotic distribution for the FS-CL\ntest statistic under $H_0: \\delta=0$ with given ${N_{cl}}^\\ast\\leq {N_{cl}}$, which is\n\n", "itemtype": "equation", "pos": 25195, "prevtext": "\nOnce the null distribution of (\\ref{estimated}) is determined, which will be detailed in Section~\\ref{sec:nulldistribution},\n$T_{{\\text{fscl}}}$ will be used to test (\\ref{eq3}).\nNote that if ${N_{cl}}^\\ast={N_{cl}}$, the resulting test is then equivalent to the classic Wald test including all\nthe sub-likelihood components but may incur much unnecessary computing. However,\nthe test can be much more powerful if many sub-likelihoods are\nredundant and be computationally efficient if ${N_{cl}}^\\ast$ is not large.\nWhen ${N_{cl}}^\\ast = 1$ and there is only one parameter to estimate in each sub-likelihood component (i.e. $p =1$),\n$T_{\\text{fscl}}$ will have the same form as the $T^2_{\\text{UminP}}$ test statistic of \\cite{pan2009asymptotic}\ngiven in  Section~\\ref{sec:tests}.\n\nIt is difficult to estimate the asymptotic covariance matrix $V(w)$ based on the analytical formula provided at\nthe end of Section~\\ref{sec2.1}. Instead, the estimated asymptotic covariance matrix $\\widehat{V}(w)$\nin Step $1$ of the algorithm can be obtained by using non-parametric bootstrap.\nSpecifically, we sample with replacement the observations within each group and then compute the bootstrap\nreplicates of the MCLE for $\\delta(w)$, denoted as $\\delta^\\ast_1(w),\\dots,\\delta^\\ast_B(w)$.\nWe use the replicates to compute\n$\\widehat{V}(w)$ as\n\n", "index": 17, "text": "$$\n\\widehat{V}(w)=\\frac{1}{B-1}\\sum^{B}_{b=1}\\left(\\delta^\\ast_b(w)-\n\\overline{\\delta^\\ast}(w)\\right)\\left(\\delta^\\ast_b(w)-\\overline{\\delta^\\ast}(w)\\right)^T,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\widehat{V}(w)=\\frac{1}{B-1}\\sum^{B}_{b=1}\\left(\\delta^{\\ast}_{b}(w)-\\overline%&#10;{\\delta^{\\ast}}(w)\\right)\\left(\\delta^{\\ast}_{b}(w)-\\overline{\\delta^{\\ast}}(w%&#10;)\\right)^{T},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>V</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>B</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\u03b4</mi><mi>b</mi><mo>\u2217</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mover accent=\"true\"><msup><mi>\u03b4</mi><mo>\u2217</mo></msup><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\u03b4</mi><mi>b</mi><mo>\u2217</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mover accent=\"true\"><msup><mi>\u03b4</mi><mo>\u2217</mo></msup><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mi>T</mi></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere ``$\\overset{\\mathcal{D}}{\\rightarrow}$'' stands for convergence in\ndistribution and $\\chi^2_{(1)}(p^\\prime)\\geq  \\cdots \\geq \\chi^2_{({N_{cl}})}(p^\\prime)$ are reverse\norder statistics from ${N_{cl}}$ independent $\\chi^2(p^\\prime)$ random variables.\nA closed-form expression for the probability density function of\n$\\sum_{i=1}^{{N_{cl}}^\\ast}\\chi^2_{(i)}(p^\\prime)$ conditional on ${N_{cl}}$ is reported in the Appendix.\nFrom the Bayesian viewpoint, before observing the data there is a\nquite large number of equally plausible test statistics  $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$, corresponding to a priori\nmodels satisfying the constraint $\\Vert w \\Vert\\leq {N_{cl}}^\\ast$. In the Bayesian framework, the complexity\nparameter ${N_{cl}}^\\ast$ is treated as a random variable with uninformative prior distribution\n$\\pi({N_{cl}}^\\ast)=(1/{N_{cl}}, \\dots, 1/ {N_{cl}})^T$. Its approximate posterior distribution is discussed in\nSection~\\ref{sec:modelcomplexity}.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:nulldensity} shows the asymptotic null density $f_T(t|{N_{cl}}^\\ast)$ of $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$ in\n(\\ref{disktppclfs}) obtained from formula (\\ref{null}) for different values of ${N_{cl}}^\\ast$, together with its\nhistogram estimate at ${N_{cl}}^\\ast=1$ obtained from Monte Carlo simulation.\n\nNote that the right tail of the null density becomes lighter when ${N_{cl}}^\\ast$ is smaller.\nWhen ${N_{cl}}^\\ast$ takes the largest value ${N_{cl}}$ the null density becomes the same as that for the Wald\ntest statistic (\\ref{Wald}).  This property of the null distribution makes it more likely for the FS-CL test than\nthe classic Wald test to reject the null hypothesis $H_0:\\delta=0$ when the alternative hypothesis is true.\n\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.7]{nulldensityk.pdf}\n\\caption{Null distribution for the test statistic $T_{{\\text{fscl}}}$ in (\\ref{disktppclfs}) when\n$p^\\prime=5$, ${N_{cl}}^\\ast=1,2, 5$, and ${N_{cl}}=5$ (so ${N_{cl}}^\\ast=5$ corresponds to the Wald\ntest). The histogram is generated by Monte Carlo simulation using  ${N_{cl}}^\\ast=1$\nwhereas the smooth pdf curves correspond to the analytical density (\\ref{null}) given in the appendix.}\n\\label{fig:nulldensity}\n\\end{figure}\n\nIn general the columns of the MCLE ${\\hat{\\delta}}$ are correlated with each other, thus the null distribution\nof $T_{{\\text{fscl}}}$ is difficult to obtain. We propose to use a random permutation method to acquire the null\ndistribution of the FS-CL test statistic. The main idea is to permute the data many times and use each\npermutation to compute a replicate of the test statistic. The empirical distribution of the permutation\nreplicates is used as an estimated null distribution of the test statistic.\n\nSpecifically, we draw all the observations together and randomly distribute them into different groups\nwith the sample size in each group unchanged. By doing so, each permutation can\nbe treated as generating a new data sample under the null hypothesis that there are no characteristic differences\nbetween groups. Using each newly generated data sample, we compute the MCLE of the difference parameter $\\delta$\nand the corresponding FS-CL test statistic as a permutation replicate.\nRepeating this procedure for $B$ times, we will then acquire $B$ permutation replicates for the\ntest statistic, denoted $(T^\\ast_{(1)}, \\dots, T^\\ast_{(B)})$. We use the empirical distribution of\n$(T^\\ast_{(1)}, \\dots, T^\\ast_{(B)})$ as an estimate of the null distribution, and use the upper\n$\\alpha$-quantile as the rejection threshold of the FS-CL test.\n\n\n\n\n\\subsection{Choice of ${N_{cl}}^\\ast$ and the maximum posterior test statistic}\n\\label{sec:modelcomplexity}\n\nNote that choosing different ${N_{cl}}^\\ast$, the maximum number of\nallowed sub-likelihoods in the FS-CL algorithm in Section~\\ref{sec:alg}, leads to\ndifferent test statistics $T_{{\\text{fscl}}}$ defined in (\\ref{estimated}). It is thus\nimportant to discuss how to choose an appropriate value of ${N_{cl}}^\\ast$. Since ${N_{cl}}^\\ast$ can be regarded as\na model complexity parameter, it seems natural to use a well-established\nmodel-selection criterion for choosing ${N_{cl}}^\\ast$.  We propose to use the composite likelihood\nBayesian information criterion  (CL-BIC) studied by \\cite{gao2010composite}.\nThe CL-BIC is a robust generalization of the classic Bayesian information criterion (BIC) not requiring that the\nestimating equation used corresponds to the true model. For a\nsingle group of data of sample size $n$, the classic CL-BIC for a composition model including all\nsub-likelihoods available is defined by\n\n", "itemtype": "equation", "pos": 28299, "prevtext": "\nwhere $\\overline{\\delta^\\ast}(w)=(1/B)\\sum_{b=1}^B \\delta^\\ast_b(w)$. Our empirical study shows that\nthe one-dimensional $T_{{\\text{fscl}}}$ statistics is robust under non-parametric bootstrap and it is\nsufficient for setting $B=1000$ for most cases in practice. The Jackknife method for computing $\\widehat{V}(w)$\nmay also be used.\n\n\n\n\n\n\n\nFigure~\\ref{fig:powerplot12} illustrates the power of the final test statistic $T_{{\\text{fscl}}}$ in a simple simulated example\nwhere two samples of 20-dimensional normal vectors, each of size 18, are generated to test the mean difference.\nThe normal distribution for the first sample is $\\mathcal{N}_{20}(\\theta_0\\!=\\!0,\\, 9I)$ and that for the second sample is\n$\\mathcal{N}_{20}(\\theta_1, 9I)$ with $I$ being the $20\\times 20$ identity matrix and $\\theta_1=(\\delta_0, 0,\\dots,0)^T$.\nOnce the data are simulated, we ignore the parameter values underlying the true distributions and proceed to\ntest $H_0: \\delta=\\theta_1-\\theta_0=0$ at significance level $\\alpha=0.05$ using the proposed forward search\nalgorithm and the FS-CL test statistic $T_{{\\text{fscl}}}$ together with its simulated null distribution.\nWe set the ${N_{cl}}=20$ marginal sub-likelihoods as all the ones available and consider four specified\nvalues for ${N_{cl}}^\\ast$ in $H_1$ in (\\ref{eq3}), i.e. ${N_{cl}}^\\ast= 1, 5, 10$, or 20 (which gives the classic Wald test).\nThe power results based on 10,000 simulations of the two-sample data are plotted in Figure~\\ref{fig:powerplot12}.\nWe see that the FS-CL test dominates the Wald\ntest in terms of power for any ${N_{cl}}^\\ast<20$. Clearly the largest power gain is obtained when ${N_{cl}}^\\ast=1$,\nwhich should be the case since only the first marginal sub-likelihood contains the information about the\nnonzero component in $\\delta$ in truth.\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[h]\n\\centering\n\n\\includegraphics[scale=0.7]{n=36.pdf}\n    \\caption{Power of the FS-CL test for hypotheses given in (\\ref{eq3}) where the first sample of size 18 come from\n$\\mathcal{N}_{20}(0,9I)$ and the second sample of size 18 from $\\mathcal{N}_{20}(\\delta, 9I)$ with\n$\\delta=(\\delta_0, 0,\\dots,0)$.\nThe solid black curve represents the Wald test corresponding to ${N_{cl}}^\\ast=20$.\nPower curves are estimated using 10,000 Monte Carlo simulations.}\n\\label{fig:powerplot12}\n\\end{figure}\n\n\n\n\\subsection{Null distribution for the FS-CL test}\n\\label{sec:nulldistribution}\n\nThe null distribution of the FS-CL test statistic $T_{{\\text{fscl}}}$ is needed for drawing a conclusion for the test.\nLet's first consider a trivial case where the MCLE ${\\hat{\\delta}} = ({\\hat{\\delta}}_1, \\dots,\n{\\hat{\\delta}}_{{N_{cl}}})$ has its columns independent of each other; and each of its columns has the same\neffective dimension $p^\\prime$ and the same asymptotic distribution.\nThen one can deduce the asymptotic distribution for the FS-CL\ntest statistic under $H_0: \\delta=0$ with given ${N_{cl}}^\\ast\\leq {N_{cl}}$, which is\n\n", "index": 19, "text": "\\begin{equation}\\label{disktppclfs}\nT_{{\\text{fscl}}}|{N_{cl}}^\\ast \\overset{\\mathcal{D}}{\\rightarrow} \\sum_{i=1}^{{N_{cl}}^\\ast}\n\\chi^2_{(i)}(p^\\prime)\\quad\\text{as}\\;\\;n\\rightarrow \\infty,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"T_{{\\text{fscl}}}|{N_{cl}}^{\\ast}\\overset{\\mathcal{D}}{\\rightarrow}\\sum_{i=1}^%&#10;{{N_{cl}}^{\\ast}}\\chi^{2}_{(i)}(p^{\\prime})\\quad\\text{as}\\;\\;n\\rightarrow\\infty,\" display=\"block\"><mrow><msub><mi>T</mi><mtext>fscl</mtext></msub><mo stretchy=\"false\">|</mo><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts><mover accent=\"true\"><mo>\u2192</mo><mo class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mo></mover><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts></munderover><msubsup><mi>\u03c7</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msubsup><mrow><mo stretchy=\"false\">(</mo><msup><mi>p</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mpadded width=\"+5.6pt\"><mtext>as</mtext></mpadded><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\ell_{cl}(\\theta;w)$ is the weighted composite likelihood function\ndefined in (\\ref{comp_lik}) with $w=(1,\\dots,1)^T$, and ${\\hat{\\theta}}_{all}$ is the corresponding\nMCLE. The term\n$\\hat{p}^\\ast=\\text{Tr}(\\widehat{H}^{-1}\\widehat{J})$ represents the\nestimated effective degrees freedom in the parameter with $\\text{Tr}(\\cdot)$  denoting the trace function, and\n$\\widehat{H}$ and $\\widehat{J}$ are estimates of the Hessian and\nscore variance obtained as\n\n", "itemtype": "equation", "pos": 33108, "prevtext": "\nwhere ``$\\overset{\\mathcal{D}}{\\rightarrow}$'' stands for convergence in\ndistribution and $\\chi^2_{(1)}(p^\\prime)\\geq  \\cdots \\geq \\chi^2_{({N_{cl}})}(p^\\prime)$ are reverse\norder statistics from ${N_{cl}}$ independent $\\chi^2(p^\\prime)$ random variables.\nA closed-form expression for the probability density function of\n$\\sum_{i=1}^{{N_{cl}}^\\ast}\\chi^2_{(i)}(p^\\prime)$ conditional on ${N_{cl}}$ is reported in the Appendix.\nFrom the Bayesian viewpoint, before observing the data there is a\nquite large number of equally plausible test statistics  $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$, corresponding to a priori\nmodels satisfying the constraint $\\Vert w \\Vert\\leq {N_{cl}}^\\ast$. In the Bayesian framework, the complexity\nparameter ${N_{cl}}^\\ast$ is treated as a random variable with uninformative prior distribution\n$\\pi({N_{cl}}^\\ast)=(1/{N_{cl}}, \\dots, 1/ {N_{cl}})^T$. Its approximate posterior distribution is discussed in\nSection~\\ref{sec:modelcomplexity}.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:nulldensity} shows the asymptotic null density $f_T(t|{N_{cl}}^\\ast)$ of $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$ in\n(\\ref{disktppclfs}) obtained from formula (\\ref{null}) for different values of ${N_{cl}}^\\ast$, together with its\nhistogram estimate at ${N_{cl}}^\\ast=1$ obtained from Monte Carlo simulation.\n\nNote that the right tail of the null density becomes lighter when ${N_{cl}}^\\ast$ is smaller.\nWhen ${N_{cl}}^\\ast$ takes the largest value ${N_{cl}}$ the null density becomes the same as that for the Wald\ntest statistic (\\ref{Wald}).  This property of the null distribution makes it more likely for the FS-CL test than\nthe classic Wald test to reject the null hypothesis $H_0:\\delta=0$ when the alternative hypothesis is true.\n\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.7]{nulldensityk.pdf}\n\\caption{Null distribution for the test statistic $T_{{\\text{fscl}}}$ in (\\ref{disktppclfs}) when\n$p^\\prime=5$, ${N_{cl}}^\\ast=1,2, 5$, and ${N_{cl}}=5$ (so ${N_{cl}}^\\ast=5$ corresponds to the Wald\ntest). The histogram is generated by Monte Carlo simulation using  ${N_{cl}}^\\ast=1$\nwhereas the smooth pdf curves correspond to the analytical density (\\ref{null}) given in the appendix.}\n\\label{fig:nulldensity}\n\\end{figure}\n\nIn general the columns of the MCLE ${\\hat{\\delta}}$ are correlated with each other, thus the null distribution\nof $T_{{\\text{fscl}}}$ is difficult to obtain. We propose to use a random permutation method to acquire the null\ndistribution of the FS-CL test statistic. The main idea is to permute the data many times and use each\npermutation to compute a replicate of the test statistic. The empirical distribution of the permutation\nreplicates is used as an estimated null distribution of the test statistic.\n\nSpecifically, we draw all the observations together and randomly distribute them into different groups\nwith the sample size in each group unchanged. By doing so, each permutation can\nbe treated as generating a new data sample under the null hypothesis that there are no characteristic differences\nbetween groups. Using each newly generated data sample, we compute the MCLE of the difference parameter $\\delta$\nand the corresponding FS-CL test statistic as a permutation replicate.\nRepeating this procedure for $B$ times, we will then acquire $B$ permutation replicates for the\ntest statistic, denoted $(T^\\ast_{(1)}, \\dots, T^\\ast_{(B)})$. We use the empirical distribution of\n$(T^\\ast_{(1)}, \\dots, T^\\ast_{(B)})$ as an estimate of the null distribution, and use the upper\n$\\alpha$-quantile as the rejection threshold of the FS-CL test.\n\n\n\n\n\\subsection{Choice of ${N_{cl}}^\\ast$ and the maximum posterior test statistic}\n\\label{sec:modelcomplexity}\n\nNote that choosing different ${N_{cl}}^\\ast$, the maximum number of\nallowed sub-likelihoods in the FS-CL algorithm in Section~\\ref{sec:alg}, leads to\ndifferent test statistics $T_{{\\text{fscl}}}$ defined in (\\ref{estimated}). It is thus\nimportant to discuss how to choose an appropriate value of ${N_{cl}}^\\ast$. Since ${N_{cl}}^\\ast$ can be regarded as\na model complexity parameter, it seems natural to use a well-established\nmodel-selection criterion for choosing ${N_{cl}}^\\ast$.  We propose to use the composite likelihood\nBayesian information criterion  (CL-BIC) studied by \\cite{gao2010composite}.\nThe CL-BIC is a robust generalization of the classic Bayesian information criterion (BIC) not requiring that the\nestimating equation used corresponds to the true model. For a\nsingle group of data of sample size $n$, the classic CL-BIC for a composition model including all\nsub-likelihoods available is defined by\n\n", "index": 21, "text": "\\begin{align}\\label{CL-BIC}\n\\text{CL-BIC}^\\ast = -2\\ell_{cl}({\\hat{\\theta}}_{all};w_{all})+ \\log(n)\\hat{p}^\\ast,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{CL-BIC}^{\\ast}=-2\\ell_{cl}({\\hat{\\theta}}_{all};w_{all})+%&#10;\\log(n)\\hat{p}^{\\ast},\" display=\"inline\"><mrow><mrow><msup><mtext>CL-BIC</mtext><mo>\u2217</mo></msup><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>;</mo><msub><mi>w</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msup><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2217</mo></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nCL-BIC can naturally be extended for any composition model specified by the composition rule $w$,\nwhere we just need to replace $\\hat{\\theta}_{{\\text{all}}}$ with $\\hat{\\theta}_w$, $w_{{\\text{all}}}$ with $w$, and\n$\\hat{p}^\\ast$ with $\\hat{p}^\\ast_w=\\text{Tr}(\\widehat{H}^{-1}(w)\\widehat{J}(w))$.\nIn the special case of $H(\\theta, w)$ and $J(\\theta, w)$ defined in Section~\\ref{sec2.1} being equal to\neach other (cf. \\cite{Lindsay2011issues}), $\\hat{p}^\\ast_w$ should equal $d_w$ approximately.\n\nIn our two-group sequential test,\n\n\n\nthe group-specific composite likelihoods are\n\n", "itemtype": "equation", "pos": 33694, "prevtext": "\nwhere $\\ell_{cl}(\\theta;w)$ is the weighted composite likelihood function\ndefined in (\\ref{comp_lik}) with $w=(1,\\dots,1)^T$, and ${\\hat{\\theta}}_{all}$ is the corresponding\nMCLE. The term\n$\\hat{p}^\\ast=\\text{Tr}(\\widehat{H}^{-1}\\widehat{J})$ represents the\nestimated effective degrees freedom in the parameter with $\\text{Tr}(\\cdot)$  denoting the trace function, and\n$\\widehat{H}$ and $\\widehat{J}$ are estimates of the Hessian and\nscore variance obtained as\n\n", "index": 23, "text": "\\begin{align} \\label{matrices}\n   \\widehat{H}\\equiv\\widehat{H}(w_{{\\text{all}}})= -  \\nabla^2\n\\ell_{cl}({\\hat{\\theta}}_{{\\text{all}}}; w_{{\\text{all}}}) ,  \\ \\\n   \\widehat{J} \\equiv \\widehat{J}(w_{{\\text{all}}})= \\widehat{\\text{Var}}\\left\\{ \\nabla\n\\ell_{cl}({\\hat{\\theta}}_{{\\text{all}}}; w_{{\\text{all}}})\\right\\}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widehat{H}\\equiv\\widehat{H}(w_{{\\text{all}}})=-\\nabla^{2}\\ell_{%&#10;cl}({\\hat{\\theta}}_{{\\text{all}}};w_{{\\text{all}}}),\\ \\ \\widehat{J}\\equiv%&#10;\\widehat{J}(w_{{\\text{all}}})=\\widehat{\\text{Var}}\\left\\{\\nabla\\ell_{cl}({\\hat%&#10;{\\theta}}_{{\\text{all}}};w_{{\\text{all}}})\\right\\}.\" display=\"inline\"><mrow><mrow><mrow><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mo>\u2261</mo><mrow><mover accent=\"true\"><mi>H</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mtext>all</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mtext>all</mtext></msub><mo>;</mo><msub><mi>w</mi><mtext>all</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mover accent=\"true\"><mi>J</mi><mo>^</mo></mover><mo>\u2261</mo><mrow><mover accent=\"true\"><mi>J</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mtext>all</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mover accent=\"true\"><mtext>Var</mtext><mo>^</mo></mover><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><msub><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mtext>all</mtext></msub><mo>;</mo><msub><mi>w</mi><mtext>all</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\ell^{(j)}_k$ denotes the $k$th sub-likelihood in group $j$. Thus, we\npropose to construct the combined two-sample CL-BIC for a composition rule $w$ as\n\n", "itemtype": "equation", "pos": 34600, "prevtext": "\nCL-BIC can naturally be extended for any composition model specified by the composition rule $w$,\nwhere we just need to replace $\\hat{\\theta}_{{\\text{all}}}$ with $\\hat{\\theta}_w$, $w_{{\\text{all}}}$ with $w$, and\n$\\hat{p}^\\ast$ with $\\hat{p}^\\ast_w=\\text{Tr}(\\widehat{H}^{-1}(w)\\widehat{J}(w))$.\nIn the special case of $H(\\theta, w)$ and $J(\\theta, w)$ defined in Section~\\ref{sec2.1} being equal to\neach other (cf. \\cite{Lindsay2011issues}), $\\hat{p}^\\ast_w$ should equal $d_w$ approximately.\n\nIn our two-group sequential test,\n\n\n\nthe group-specific composite likelihoods are\n\n", "index": 25, "text": "\\begin{align} \\label{combined}\n\\ell^{(j)}_{cl}(\\theta_j; w) =   {\\sum_{k=1}^{\\Ncl}} \\dfrac{w_k\n\\ell^{(j)}_k(\\theta_j)}{{\\sum_{k=1}^{\\Ncl}} w_k}, \\  \\ j =0,1,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ell^{(j)}_{cl}(\\theta_{j};w)={\\sum_{k=1}^{\\Ncl}}\\dfrac{w_{k}\\ell%&#10;^{(j)}_{k}(\\theta_{j})}{{\\sum_{k=1}^{\\Ncl}}w_{k}},\\ \\ j=0,1,\" display=\"inline\"><mrow><mrow><mrow><mrow><msubsup><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>j</mi></msub><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Ncl</mtext></merror></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u2113</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Ncl</mtext></merror></msubsup><msub><mi>w</mi><mi>k</mi></msub></mrow></mfrac></mstyle></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere ${\\hat{\\theta}}_0$ and ${\\hat{\\theta}}_1$ are group-specific MCLEs, and\n$\\hat{p}^{\\ast (j)}_w=\\text{Tr}(\\{{\\widehat{H}^{(j)}}(w)\\}^{-1}\\widehat{J}^{(j)} (w))$ with\n$\\widehat{H}^{(j)}(w)$, $\\widehat{J}^{(j)}(w)$ computed similarly as in (\\ref{matrices}).\nIn the special case of $H^{(j)}(\\theta_j, w)$ and $J^{(j)}(\\theta_j, w)$ being equal to\neach other, $j=0,1$, we should have  $\\hat{p}^{\\ast(0)}_w + \\hat{p}^{\\ast(1)}_w= 2d_w$ approximately.\n\nWith the proposed $\\text{CL-BIC}(w)$ we choose the best value of ${N_{cl}}^\\ast$ as\n\n", "itemtype": "equation", "pos": 34929, "prevtext": "\nwhere $\\ell^{(j)}_k$ denotes the $k$th sub-likelihood in group $j$. Thus, we\npropose to construct the combined two-sample CL-BIC for a composition rule $w$ as\n\n", "index": 27, "text": "\\begin{align}\n\\label{TIC}\n\\text{CL-BIC}(w)=- 2\\left\\{ \\ell^{(0)}_{cl}(\\hat{\\theta}_0; w) +\n\\ell^{(1)}_{cl}(\\hat{\\theta}_1; w) \\right\\}+ \\log(n) \\left\\{ \\hat{p}^{\\ast (0)}_w +\n\\hat{p}^{\\ast (1)}_w \\right\\}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{CL-BIC}(w)=-2\\left\\{\\ell^{(0)}_{cl}(\\hat{\\theta}_{0};w)+%&#10;\\ell^{(1)}_{cl}(\\hat{\\theta}_{1};w)\\right\\}+\\log(n)\\left\\{\\hat{p}^{\\ast(0)}_{w%&#10;}+\\hat{p}^{\\ast(1)}_{w}\\right\\}\" display=\"inline\"><mrow><mrow><mtext>CL-BIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><msubsup><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mn>0</mn></msub><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>w</mi><mrow><mi/><mo>\u2217</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo>+</mo><msubsup><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>w</mi><mrow><mi/><mo>\u2217</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $\\hat{w}^{(t)}$ is the\nfinal composition rule obtained from the FS-CL Algorithm in Section~\\ref{algorithm1} after $t$ steps.\n\nIn our setting, the CL-BIC model-selection framework offers a natural interpretation as being induced from\na posterior distribution for the composition complexity parameter ${N_{cl}}^\\ast$.\nSpecifically, under the discrete uniform prior for ${N_{cl}}^\\ast$,\n$\\pi_{{N_{cl}}^\\ast}(k) = 1/{N_{cl}}$, $k=1,\\cdots, {N_{cl}}$, the posterior of ${N_{cl}}^\\ast$ is\n\n", "itemtype": "equation", "pos": 35681, "prevtext": "\nwhere ${\\hat{\\theta}}_0$ and ${\\hat{\\theta}}_1$ are group-specific MCLEs, and\n$\\hat{p}^{\\ast (j)}_w=\\text{Tr}(\\{{\\widehat{H}^{(j)}}(w)\\}^{-1}\\widehat{J}^{(j)} (w))$ with\n$\\widehat{H}^{(j)}(w)$, $\\widehat{J}^{(j)}(w)$ computed similarly as in (\\ref{matrices}).\nIn the special case of $H^{(j)}(\\theta_j, w)$ and $J^{(j)}(\\theta_j, w)$ being equal to\neach other, $j=0,1$, we should have  $\\hat{p}^{\\ast(0)}_w + \\hat{p}^{\\ast(1)}_w= 2d_w$ approximately.\n\nWith the proposed $\\text{CL-BIC}(w)$ we choose the best value of ${N_{cl}}^\\ast$ as\n\n", "index": 29, "text": "$${N_{cl}}^\\ast = \\text{argmin}_{1\\leq t \\leq {N_{cl}}} \\text{CL-BIC}(\\hat{w}^{(t)}),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{N_{cl}}^{\\ast}=\\text{argmin}_{1\\leq t\\leq{N_{cl}}}\\text{CL-BIC}(\\hat{w}^{(t)}),\" display=\"block\"><mrow><mrow><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts><mo>=</mo><mrow><msub><mtext>argmin</mtext><mrow><mn>1</mn><mo>\u2264</mo><mi>t</mi><mo>\u2264</mo><msub><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></msub><mo>\u2062</mo><mtext>CL-BIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nThen, the best ${N_{cl}}^\\ast$ value corresponds to the maximum a posteriori (MAP) estimate of ${N_{cl}}^\\ast$\nand the resulting test statistic $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$ is referred to\nas maximum a posteriori test statistic.\n\n\\section{Numerical examples and simulation study} \\label{MonteCarlo}\n\\label{sec:numerical}\n\n\\textbf{Example 1: Normally distributed MCLEs.} Consider a simulated example of MCLE-based testing for\nthe difference of group means from two samples of 40-dimensional normal data with known covariance matrix.\nThis is equivalent to testing $H_{0}: \\sqrt{n}{\\hat{\\delta}} \\sim \\mathcal{N}_{40}(0 ,V)$ against\n$H_{1}: \\sqrt{n}{\\hat{\\delta}} \\sim\\mathcal{N}_{40}(\\delta,V)$,\nwhere ${\\hat{\\delta}}$ is the MCLE of $\\delta$ having $40$ elements.\nWe set the considered composite likelihood function to comprise up to ${N_{cl}}=20$ independent pairwise sub-likelihoods,\nwhere each candidate sub-likelihood is for a 2-dimensional subset of the data and contains just two elements of $\\delta$.\nIn generating the data satisfying $H_1$, we consider four models ($m=1,\\dots,4$) and set the elements of $\\delta$ as\n$\\delta_{j}=(-1)^j\\cdot0.5(m+1)\\cdot I(j\\leq 6-m)$, $j=1,\\dots,40$, where $I(\\cdot)$ is the indicator function.\n The covariance matrix of $\\sqrt{n}{\\hat{\\delta}}$ is set as\n\n", "itemtype": "equation", "pos": 36258, "prevtext": "\nwhere $\\hat{w}^{(t)}$ is the\nfinal composition rule obtained from the FS-CL Algorithm in Section~\\ref{algorithm1} after $t$ steps.\n\nIn our setting, the CL-BIC model-selection framework offers a natural interpretation as being induced from\na posterior distribution for the composition complexity parameter ${N_{cl}}^\\ast$.\nSpecifically, under the discrete uniform prior for ${N_{cl}}^\\ast$,\n$\\pi_{{N_{cl}}^\\ast}(k) = 1/{N_{cl}}$, $k=1,\\cdots, {N_{cl}}$, the posterior of ${N_{cl}}^\\ast$ is\n\n", "index": 31, "text": "$$\n\\pi_{{N_{cl}}^\\ast}(k|Y_1, \\dots, Y_n) = \\dfrac{\\exp \\left\\{ -\\text{CL-BIC}(\\hat{w}^{(k)})\\right\\} }{\\sum_{s=1}^{{N_{cl}}}\n\\exp\\left\\{-\\text{CL-BIC}(\\hat{w}^{(s)})\\right\\}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\pi_{{N_{cl}}^{\\ast}}(k|Y_{1},\\dots,Y_{n})=\\dfrac{\\exp\\left\\{-\\text{CL-BIC}(%&#10;\\hat{w}^{(k)})\\right\\}}{\\sum_{s=1}^{{N_{cl}}}\\exp\\left\\{-\\text{CL-BIC}(\\hat{w}%&#10;^{(s)})\\right\\}}.\" display=\"block\"><mrow><msub><mi>\u03c0</mi><mmultiscripts><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow><none/><none/><mo>\u2217</mo></mmultiscripts></msub><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">|</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>Y</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mtext>CL-BIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></msubsup><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mtext>CL-BIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $I_{{N_{cl}}}$ is an ${N_{cl}}$-dimensional identity matrix and ``$\\otimes$\" denotes the Kronecker product.\nWith this setting the number of the pairwise sub-likelihoods containing nonzero elements of $\\delta$ under $H_1$\ndecreases from 3 to 1 as $m$ increases from 1 to 4, while the magnitude of each nonzero $\\delta_j$ increases.\nUsing the above setting we generate 10,000 replicates of $\\sqrt{n}{\\hat{\\delta}}$ under $H_0$ and $H_1$,\nrespectively. After that we set the significance level $\\alpha=0.05$, and compute the Monte Carlo estimates\nof the Type I error probability and the power for the FS-CL test, the Wald test and two of its variants LSSB and LSSBw.\nThe results are summarized in Table~\\ref{Table1}.\n\n\\begin{table}[htp]\n\\centering\n\\begin{tabular}{ cccccccccc }\n\\hline\nModel ($m$) & \\multicolumn{4}{c}{Type I error}&& \\multicolumn{4}{c}{Power} \\\\\n             & FS-CL & Wald & LSSB & LSSBw& & FS-CL & Wald & LSSB & LSSBw   \\\\ \\hline\n1           & 0.0461 & 0.0447 & 0.0462 & 0.0467 && 0.2736& 0.2785 & 0.2040 & 0.2159   \\\\\n2           & 0.0481 & 0.0466 & 0.0467 & 0.0458 && 0.5793 & 0.5436 & 0.4073 & 0.4426  \\\\\n3           & 0.0491 & 0.0499 & 0.0501 & 0.0495 && 0.7951 & 0.7153 & 0.5615 & 0.6015  \\\\\n4           & 0.0509 & 0.0506 & 0.0511 & 0.0527 && 0.8568 & 0.7423 & 0.5977 & 0.6355  \\\\ \\hline\n\\end{tabular}\n\\caption{Estimated Type I error and power for the FS-CL test, the Wald test, and the LSSB and LSSBw tests\ndescribed in Section \\ref{sec:tests}, for testing $H_0$ vs. $H_1$. Results are based on $10,000$ samples\ngenerated according to the setting described in Example 1.}\n\\label{Table1}\n\\end{table}\n\nFrom Table \\ref{Table1}, we see all the tests have similar Type I error probabilities around $\\alpha$.\nHowever, although the LSSB and LSSBw\ntests are computationally easier than our FS-CL test, they are inferior in terms of power. This is expected since the LSSB\ntest sets $V=I$ and the LSSBw test uses only the diagonal elements of $V$, leading to loss of information on the\ncorrelation between components of ${\\hat{\\delta}}$ belonging to the same sub-likelihood.\nIn summary, the FS-CL test performs uniformly better than all the other tests in terms of power,\nin all situations where the number of informative sub-likelihoods involved is sparse.\n\nFinally, we assessed the quality of the model-selection procedure annexed to our FS-CL algorithm by\ncomputing the Monte Carlo estimates of the Hamming distance between the optimally estimated composition rule\n$\\hat{w}$ given by the FS-CL test and the true composition rule $w^\\ast$ informed by each of the prescribed\nmodels. In all our simulations we have found that the Hamming distance is essentially equal to 0 up to a negligible\nsimulation error, which confirms the FS-CL test systematically selects\nonly informative sub-likelihoods to construct the test statistic $T_{\\text{fscl}}$.\n\n\n\n\\paragraph{Example 2: Latent multivariate Gaussian model.} Now we investigate the performance of our new test\nfor categorical data analysis based on a latent multivariate Gaussian model. This model has been previously\nstudied for analyzing single nucleotide polymorphisms (SNPs) data (cf. \\cite{pan2009asymptotic, han2012composite}).\nThe advantage of using the latent multivariate Gaussian model is its ability to model correlated categorical\nvariables through the latent quantiles and covariance matrix.\n\nConsider independent $d$-vector observations $Y_i =(Y_{i1}, \\dots, Y_{id})^T$, $i=1,\\dots,n$, with each\nelement $Y_{ij}$ being categorical taking one of $C$ labels $1, \\dots, C$.\nFor the $i$th $d$-vector observation, we assume there is a latent vector variable\n$Z_{i}=(Z_{i1}, \\dots,Z_{id})^T$ that follows a multivariate Gaussian distribution,\n$Z_{i}\\sim \\mathcal{N}_d(0,\\Sigma)$,  where $\\Sigma$ is a $d\\times d$ correlation matrix.\nWe also assume the existence of $C-1$ quantile constants $\\gamma_{j1}, \\gamma_{j2}, \\dots, \\gamma_{j(C-1)}$\nfor each $j=1,\\dots, d$, such that for all $i=1,\\dots, n$, $Y_{ij}$ takes label $k$ if $Z_{ij}\\in \\Gamma_{jk}\\equiv\n(\\gamma_{j(k-1)}, \\, \\gamma_{jk}]$, where $k=1,\\dots, C$ with $\\gamma_{j0}=-\\infty$ and $\\gamma_{jC}=\\infty$.\nIt is easy to see that the marginal and joint probability distributions of $(Y_{i1}, \\dots, Y_{id})$ are\ndetermined by the quantile parameters $\\gamma=\\{\\gamma_{j1}, \\cdots, \\gamma_{j(C-1)};\\, j=1,\\cdots,d\\}$\nand correlation matrix parameter $\\Sigma$. For example, $\\Pr(Y_{ij}=2) =\\Pr(\\gamma_{j1}<Z_{ij}\\leq \\gamma_{j2})$\nand $\\Pr(Y_{ij}=2, Y_{ij^\\prime}=3) =\\Pr(\\gamma_{j1}<Z_{ij}\\leq \\gamma_{j2}, \\,\n\\gamma_{j^\\prime 2}<Z_{ij^\\prime}\\leq \\gamma_{j^\\prime 3})$, etc.\n\n\n\n\n\n\n\n\nWe use $\\theta$ to denote the vector collecting the elements in $\\gamma$ and $\\Sigma$.\n\nLet  $f(z_1, \\dots, z_d;\\Sigma)$ denote the $d$-variate normal density $\\mathcal{N}_d(0,\\Sigma)$.\nNow estimating the quantile parameters $\\gamma$ from the data $(Y_1,\\cdots, Y_n)$ can be done by\nmaximizing the weighted one-wise marginal composite likelihood function\n\n", "itemtype": "equation", "pos": 37739, "prevtext": "\nThen, the best ${N_{cl}}^\\ast$ value corresponds to the maximum a posteriori (MAP) estimate of ${N_{cl}}^\\ast$\nand the resulting test statistic $T_{{\\text{fscl}}}|{N_{cl}}^\\ast$ is referred to\nas maximum a posteriori test statistic.\n\n\\section{Numerical examples and simulation study} \\label{MonteCarlo}\n\\label{sec:numerical}\n\n\\textbf{Example 1: Normally distributed MCLEs.} Consider a simulated example of MCLE-based testing for\nthe difference of group means from two samples of 40-dimensional normal data with known covariance matrix.\nThis is equivalent to testing $H_{0}: \\sqrt{n}{\\hat{\\delta}} \\sim \\mathcal{N}_{40}(0 ,V)$ against\n$H_{1}: \\sqrt{n}{\\hat{\\delta}} \\sim\\mathcal{N}_{40}(\\delta,V)$,\nwhere ${\\hat{\\delta}}$ is the MCLE of $\\delta$ having $40$ elements.\nWe set the considered composite likelihood function to comprise up to ${N_{cl}}=20$ independent pairwise sub-likelihoods,\nwhere each candidate sub-likelihood is for a 2-dimensional subset of the data and contains just two elements of $\\delta$.\nIn generating the data satisfying $H_1$, we consider four models ($m=1,\\dots,4$) and set the elements of $\\delta$ as\n$\\delta_{j}=(-1)^j\\cdot0.5(m+1)\\cdot I(j\\leq 6-m)$, $j=1,\\dots,40$, where $I(\\cdot)$ is the indicator function.\n The covariance matrix of $\\sqrt{n}{\\hat{\\delta}}$ is set as\n\n", "index": 33, "text": "\\begin{align*}\nV= I_{{N_{cl}}} \\otimes\n   \\begin{pmatrix}\n   1.5  &  0.2 \\\\   0.2 & 1\n   \\end{pmatrix},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V=I_{{N_{cl}}}\\otimes\\begin{pmatrix}1.5&amp;0.2\\\\&#10;0.2&amp;1\\end{pmatrix},\" display=\"inline\"><mrow><mrow><mi>V</mi><mo>=</mo><mrow><msub><mi>I</mi><msub><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></msub><mo>\u2297</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mn>1.5</mn></mtd><mtd columnalign=\"center\"><mn>0.2</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0.2</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere $I(Y_{ij}=k)$ is an indicator function and $(w_1,\\cdots, w_d)$ is the binary weight vector (hence\n${N_{cl}}=d$ here). One can extend (\\ref{CL1}) by including pairwise likelihood components as\ndescribed in \\cite{han2012composite} so that $\\gamma$\nand $\\Sigma$ can be estimated simultaneously.\n\nWe consider having two groups (case and control) of $d=6$ dimensional observations each taking\none of $C=3$ labels (categories). The correlation structure of the latent vector variable is set to be\n$\\Sigma=I$ where $I$ is a $d$-dimensional  identity matrix. The group-specific latent parameters\nare then $\\theta_0=\\gamma_0$ and\n$\\theta_1=\\gamma_1$.  For the control group we set\n$\\gamma_{0}=(-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3)$ as the true values, while for the\ncase group we set $\\gamma_1 = \\gamma_0 + \\delta$ where\n$\\delta=(-\\epsilon,\\epsilon;0,0;0,0;0,0;0,0;0,0)$ and $\\epsilon  = 0.3, 0.4, 0.5$. We\ngenerate 1000 Monte Carlo samples of size $n= 200$  ($100$ controls and $100$ cases) using the above\nsetting, and denote the group-specific MCLEs by\n$\\hat{\\gamma}_{0}$ and $\\hat{\\gamma}_{1}$, respectively, where $\\hat{\\gamma}_{0}$ and $\\hat{\\gamma}_{1}$\nare $2\\times6$ dimensional vectors. For each sample, we compute the MCLE difference\n$\\hat{\\delta}  = \\hat{\\gamma}_{1}-\\hat{\\gamma}_{0}$ and estimate the asymptotic covariance matrix\n$V$ of $\\sqrt{n}\\hat{\\delta}$ by nonparametric bootstrap as described in Section \\ref{sec:alg}.\nWe then perform the FS-CL test and the Wald, LSSB and LSSBw tests discussed in the paper for testing $H_0: \\delta=0$\nagainst a sparse local alternative. We use the permutation method to simulate the null distributions and the\nassociated 0.05 level critical values in these tests, by which we compute the Monte Carlo estimates\nof the Type I error and the power of these tests based on the 1000 generated samples.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccccc } \\hline\n            &  \\multicolumn{5}{c}{FS-CL}             & Wald & LSSB & LSSBw  \\\\\n$\\epsilon$ $\\setminus$ ${N_{cl}}^\\ast$  & 1& 2 & 3 & 4&  5 &6 &  &  \\\\\n\\hline\n$0$         &  0.045 & 0.052 & 0.048 & 0.047 & 0.046 & 0.045&  0.056 & 0.053         \\\\\n$0.3$       &  0.614 & 0.585 & 0.543 & 0.518 & 0.501 & 0.496&  0.203 & 0.181        \\\\\n$0.4$       &  0.873 & 0.840 & 0.812 & 0.776 & 0.755 & 0.752& 0.380 & 0.328        \\\\\n$0.5$       &  0.979 & 0.972 & 0.950 & 0.938  & 0.927 & 0.924& 0.653 & 0.584 \\\\ \\hline\n\\end{tabular}}\n\\caption{Monte Carlo estimates of Type I error probability ($\\epsilon=0$) and power ($\\epsilon>0$) of the\nvarious tests when the data have the latent multivariate Gaussian model described in\nExample 2. The tests considered are FS-CL with ${N_{cl}}^\\ast$ ranging from 1 to 6 and the Wald test, LSSB and LSSBw\ntests described in Section \\ref{sec:framework}. Note that the Wald test corresponds to ${N_{cl}}^\\ast=6$ (no selection).\nResults are based on $1000$ Monte Carlo samples.}\n\\label{Table2}\n\\end{table}\n\nTable~\\ref{Table2} gives the Monte Carlo estimates of the Type I error probability (in row $\\epsilon = 0$) and the\npower (in rows $\\epsilon > 0$). The table reveals the power for the FS-CL test is considerably larger than that\nfor all the other tests in all simulated situations of group difference of size $\\epsilon$.\nSpecifically, the power improvement is dramatic when comparing the FS-CL test with the LSSB and LSSBw tests\nfor values of $\\epsilon$ closer to zero.\n\nAlso Table~\\ref{Table3} shows the values of CL-BIC described in Section~\\ref{sec:modelcomplexity}\nfor each ${N_{cl}}^\\ast$. We see CL-BIC is minimized at ${N_{cl}}^\\ast=1$ in all the scenarios of $\\epsilon$.\nThis result conforms to the true setting used in generating the Monte Carlo samples. Namely, only the first\none-wise marginal sub-likelihood contains the information about $\\epsilon$.\nThus the power of the FS-CL test should be the largest at selecting ${N_{cl}}^\\ast=1$, which is clearly confirmed by\nthe results in Table~\\ref{Table2}.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccc }\n\\hline\n   $\\epsilon$  $\\setminus$ ${N_{cl}}^\\ast$        & 1   & 2  & 3 & 4 &  5     &        6            \\\\\n\\hline\n$0.3$     &  431.76 & 440.49 & 449.11 & 457.46& 465.90& 474.23       \\\\\n$0.4$     & 424.61 & 436.60 & 446.48& 455.46& 464.29& 472.86  \\\\\n$0.5$     &  414.31 & 431.38 & 442.96& 452.83& 462.19& 471.13 \\\\ \\hline\n\\end{tabular}}\n\\caption{Composite likelihood Bayesian information criterion (CL-BIC) values at different maximum numbers of steps\n(${N_{cl}}^\\ast$) in the forward step-up algorithm described in Section~\\ref{sec:alg}.\nThe values are the averages computed based on 1000 Monte Carlo samples generated from the multivariate latent\nGaussian model in Example~2 with various magnitudes of the parameter difference under the alternative\nhypothesis (i.e. $\\epsilon=0.3,0.4,0.5$). }\n\\label{Table3}\n\\end{table}\n\n\\noindent\\textbf{Example 3: Effects of increasing the number of candidate sub-likelihoods.}\nWe continue by considering the latent Gaussian model underlying the case-control data  described\nin Example 2  to see how the FS-CL test procedure and the other discussed tests perform as  the number\nof candidate one-wise sub-likelihoods, ${N_{cl}}$, grows. In the set-up we let ${N_{cl}}=d$ change from 6 to 20 but\nlet only the first one-wise sub-likelihood contain the information of case-control difference.\nWe continue to assume $C=3$ categories for each variable in the data. But differently from Example 2, we consider\nparameter vectors $\\gamma_0^{(m)}$ and $\\gamma_1^{(m)}$  of\nlength $2(m+5)$, for $m=1, \\dots 15$ (note ${N_{cl}}=m+1$). Specifically, we set\n$\\gamma^{(m)}_{0,i}=0.3\\times(-1)^i$, $i=1,\\cdots, 2(m+5)$,\nand $\\gamma^{(m)}_{1} =\\gamma^{(m)}_{0}+\\delta^{(m)}$\nwhere $\\delta^{(m)}$ is a vector of length $2(m+5)$ with $i$th element $\\delta_i=0.8\\times(-1)^i$,\nif $i\\leq2$, and $\\delta_i=0$ otherwise.\nThe covariance matrix $\\Sigma$ of the latent vector variable is set as the identity matrix.\nFor each model $m=1,\\dots, 15$, we generate 200 Monte Carlo samples of size $n=120$ (60 cases and 60 controls)\nand estimate the test power where the significance level is set as $\\alpha=0.05$.\n\n\n\\begin{figure}[t]\n     \\centering\n     \\includegraphics[scale=0.7]{snpchangepic1.pdf}\n          \\caption{Monte Carlo estimates of the power for the FS-CL test, and the Wald, LSSB and LSSBw tests\ndescribed in Section \\ref{sec:tests} for increasing numbers of sub-likelihoods ranging from 6 to 20.\nVertical bars denote the simulated 95\\% probability intervals of the power. The results are\nobtained based on 200 Monte Carlo samples from the multivariate latent Gaussian model as described in Example 3.}\n     \\label{fig:snpchangepic}\n\\end{figure}\n\nFigure~\\ref{fig:snpchangepic} displays the Monte Carlo estimates of power (represented by the dots),\ntogether with their 95\\% probability intervals, for the FS-CL, Wald, LSSB and LSSBw tests as the number of\ncandidate sub-likelihoods grows. As the number of uninformative sub-likelihoods increases,\nit shows the LSSB  and LSSBw tests are increasingly weak compared to the FS-CL and\nWald tests. The FS-CL and Wald tests have similar power when the number of candidate sub-likelihoods is small.\nRemarkably, the Wald test's performance decreases dramatically when ${N_{cl}}$ increases, while the power of the\nFS-CL test remains stable regardless of the number of irrelevant sub-likelihoods considered.\nThis behavior can be explained by noting that, as the data dimension $d$ (consequently the number of candidate\nsub-likelihoods ${N_{cl}}$) increases, more noise is added to the unweighted composite likelihood.\nTherefore those sub-likelihoods informative for distinguishing the alternative hypothesis from the null\nwill become less significant in the Wald, LSSB  and LSSBw tests.\nIn contrast, the FS-CL test tends to keep such informative sub-likelihoods and to remove the noisy ones,\ntherefore having achieved a stable high power (always near $0.9$ in Figure~\\ref{fig:snpchangepic}).\n\n\\subsection{Analysis of the Australian Breast Cancer Family genomic data}\n\\label{sec:real}\nIn this section, we apply the FS-CL procedure and the Wald, LSSB and LSSBw tests to data from\na case-control study on breast cancer. Cases are obtained from the Australian Breast Cancer Family\n(ABCF) study \\citep{mccredie2003risk} while controls are from the Australian Mammographic Density Twins and\nSisters Study \\citep{odefrey2010common}. The data set consists of 356 observations (284 controls and\n72 cases) on 100 SNPs. SNPs are the mutated pairs of single nucleotide\n(A,T,C,G) in a DNA sequence. These mutated pairs can be categorized into three groups denoted as 0, 1 and 2\n(0 and 2 are homozygous and 1 denotes heterozygous).\n\nAfter recommended data cleaning and quality control, the final dataset comprises\n356 vector observations on 61 SNP variables and contains no missing data.\nOur objective is to test the significance of association between the SNPs and breast cancer.\n\n\\textit{Case 1: Weakly dependent SNPs.}  In order to illustrate our testing procedure in the context of weakly\ndependent SNPs, we select 10 SNPs (\\texttt{rs10082248\\_A}, \\texttt{rs806645\\_T}, \\texttt{rs3765945\\_G},\n\\texttt{rs1056836\\_C}, \\texttt{rs4148326\\_C}, \\texttt{rs6717546\\_A}, \\texttt{rs1845557\\_C},\n\\texttt{rs3775774\\_C}, \\texttt{rs1651074\\_A}, and \\texttt{rs528723\\_C}) as reported in Figure~\\ref{fig:snpplot1}.\nThese SNPs are selected by investigating sample correlations of all the 61 SNPs and picking those SNPs\nwith the pairwise sample correlations, among the selected, being smaller than 0.1. To test the significance\nof association between the selected SNPs and breast cancer, we fit a latent Gaussian model described in\nExample~2 for these SNPs using the maximum composite likelihood method, and then implement the FS-CL\nprocedure, with various choices of ${N_{cl}}^\\ast$, for testing the case-control difference between the\nquantile parameters involved in the latent model.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{0.97}{\n\\begin{tabular}{ cccccccccccccc}\\hline\n& \\multicolumn{9}{c}{FS-CL}  & Wald  & LSSB & LSSBw \\\\\n${N_{cl}}^\\ast$    & 1    & 2  & 3 & 4 &  5     &        6 & 7& 8 &   9  &  10   &     &        \\\\\n\\hline\nCL-BIC         &  676 & 465 &  573 &  651  & 653  & 703 & 750 &782 & 821 &   \\\\\n$p$-value    &0.12  &0.04&  0.08&  0.08 & 0.09 & 0.10 & 0.10  & 0.10 & 0.09  & 0.09    &  0.50  & 0.27 \\\\ \\hline\n\\end{tabular}}\n\\caption{Composite likelihood Bayesian information criterion (CL-BIC) with respect to ${N_{cl}}^\\ast$ ranking\nfrom 1 to 9, as well as the $p$-values of the FS-CL test and the Wald type tests described in Section~\\ref{sec:tests},\nfor the 10 weakly dependent SNPs. The CL-BIC values are computed using (\\ref{TIC}), and the $p$-values of\nthe FS-CL test are acquired from the permutation null distribution of the test statistics as described in\nSection~\\ref{sec:nulldistribution}. }\n\\label{Table4}\n\\end{table}\n\nTable~\\ref{Table4} shows the $p$-values of the FS-CL test, as well as the CL-BIC values for ${N_{cl}}^\\ast$\nranking from 1 to 9. The CL-BIC values suggest that ${N_{cl}}^\\ast=2$ yields the best fitted model.\nWhen ${N_{cl}}^\\ast=2$, the $p$-value of the FS-CL test is 0.04, while the $p$-value for the Wald, LSSB\nand LSSBw tests are 0.09, 0.50, and 0.27 respectively. At the 0.05 significance level the FS-CL test rejects the\nnull hypothesis of no SNPs association with the disease, while the other tests cannot reach the same conclusion.\n\n\nWhen ${N_{cl}}^\\ast=2$, the FS-CL procedure selects SNPs \\texttt{rs806645\\_T} and \\texttt{rs10082248\\_A} as having\nsignificant association with the disease. To investigate the validity of this selection, we conduct marginal\nchi-square association tests for between individual SNPs and the disease. Figure~\\ref{fig:snpplot1} shows the\ncorrelations between the SNPs under consideration and the $p$-values from the marginal association tests.\nFrom Figure~\\ref{fig:snpplot1} we see the 10 weakly dependent SNPs considered in this case include the first\nSNP from the first 5 correlated ones and the last 9 ones. The two SNPs selected by the FS-CL\nhave very small $p$-values (marked with thick lines), compared to the other SNPs.\n\nFor illustration, Figure~\\ref{fig:tourplot} (left) displays bootstrap distributions of the MCLEs of the latent quantile\nparameters for a selected SNP (\\texttt{rs806645\\_T}) in the respective case and control groups, while\nFigure~\\ref{fig:tourplot} (right) displays the counterparts for an unselected SNP (\\texttt{rs3765945\\_G}). The\ntriangles and circles represent the bootstrap replicates from the case and control groups respectively.\nWhen comparing case and control groups for a selected SNP, the figure implies the quantile parameters values\nfor the two groups are well separated, concentrating into different clusters.\nOn the other hand, the bootstrap distributions for an unselected SNP are overlapping and not clearly\ndistinguishable. Both Figures~\\ref{fig:snpplot1} and \\ref{fig:tourplot} suggest that the SNPs selected by the FS-CL\nprocedure are more likely to change their values from control to case,\nand they appear to have significant effects on breast cancer.\n\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.111]{snpplot14.pdf}\n    \\caption{SNP plot of the 14 SNPs used in assessing the group difference in regard to their associations with\nbreast cancer. The first SNP and the last 9 SNPs are used in Case 1 in Section~\\ref{sec:real}; and the first 5 SNPs\nare used in Case 2 in Section~\\ref{sec:real}. The right hand side of the figure shows the $p$-values (under a negative log scale)\nof the association tests between individual SNPs and breast cancer. The thick lines refer to the two selected SNPs by\nthe FS-CL procedure. LHS of the figure shows the correlation heat map among SNPs (light color for small correlation). }\n    \\label{fig:snpplot1}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.495]{tourplot21.pdf}\n    \\caption{Bootstrap distributions for the quantile parameters estimates $\\hat{\\gamma}_1$ and $\\hat{\\gamma}_2$\nfor the selected SNP \\texttt{rs806645\\_T} (left) and the unselected SNPs \\texttt{rs3765945\\_G} (right),\nobtained from 100 bootstrap replicates. Triangles represent estimates from the case group and the solid contour lines\nspecify the estimated confidence regions in the case group. Small circles represent estimates from the control group and the\ndashed contour lines specify the estimated confidence regions in the control group.}\n    \\label{fig:tourplot}\n\\end{figure}\n\n\\textit{Case 2: Dependent SNPs.} Next, we focus on clusters of dependent SNPs having high correlations.\nFor illustration purpose, we choose the cluster of SNPs \\texttt{rs806645\\_T}, \\texttt{rs2754530\\_T}, \\texttt{rs2268796\\_A},\n\\texttt{rs4952220\\_C}, and \\texttt{rs2300697\\_C}. They are the first five SNPs in Figure~\\ref{fig:snpplot1}\nwhich are highly correlated. Other clusters can also be analyzed which will not be detailed here.\n\nTable~\\ref{Table5} shows the $p$-values of the FS-CL test at specified ${N_{cl}}^\\ast=1,\\dots,4$. It also gives the\ncorresponding CL-BIC values, which suggest that the composite likelihood containing a single sub-likelihood with\n${N_{cl}}^\\ast=1$ gives the best modelling. The $p$-value of the FS-CL test at ${N_{cl}}^\\ast=1$ equals 0.04, while the\n$p$-values for the Wald, LSSB and LSSBw tests are 0.08, 0.09, and 0.03 respectively. At significance level 0.05,\nthe FS-CL and LSSBw tests suggest the null hypothesis be correctly rejected, while the other tests cannot reach\nthe same conclusion.\n\n\\begin{table}[htb]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccccccccc}\\hline\n& \\multicolumn{4}{c}{FS-CL}  & Wald  & LSSB & LSSBw \\\\\n${N_{cl}}^\\ast$    & 1    & 2  & 3 & 4 &   5    &      &     &             \\\\\n\\hline\nCL-BIC          &  676 & 739 &  772 & 801 &   &  & \\\\\n$p$-value    &0.04  &0.02&  0.06&  0.08 & 0.08 & 0.09 & 0.03 \\\\ \\hline\n\\end{tabular}}\n\\caption{CL-BIC values with respect to ${N_{cl}}^\\ast$ ranking from 1 to 4,\nas well as the $p$-values of the FS-CL test and the Wald type tests describe in Section~\\ref{sec:tests}, for the 5\ndependent SNPs. The CL-BIC values are computed using (\\ref{TIC}), and the $p$-values of the FS-CL test are\nacquired from the permutation null distribution of the test statistics as described in Section~\\ref{sec:nulldistribution}. }\n\\label{Table5}\n\\end{table}\n\n\\section{Conclusion and discussion}\n\\label{sec:conclusion}\n\nBuilding on the well-established composite likelihood estimation framework, we have developed a method of\nsimultaneous composition rule selection and group difference testing in multivariate parametric models for\nhigh-dimensional data. The method is particularly useful for multiple genotype-phenotype association testing in\ngenome-wide association study.\nIt constructs sparse composite likelihood by including a small number of informatively selected\nsub-likelihoods, while dropping redundant or noisy sub-likelihoods that do not contribute to explaining\nthe group difference or genomic association. The procedure is implemented by our forward search and test algorithm\nwhich progressively includes useful sub-likelihoods by step-up maximizations of the bootstrap estimated\npower. In all our numerical experiments, the resultant FS-CL test has higher power than\nthe composite likelihood based Wald, LSSB and LSSBw tests, with remarkable power gains when the model\ncomplexity increases.\n\nThe FS-CL method has been applied to analyze a case-control dataset for GWAS, obtained from Australian Breast Cancer\nFamily Study, under the multivariate latent Gaussian framework studied by \\cite{han2012composite}.\n\n\nThe FS-CL test enables us to conclude about the significant overall association between particular SNPs\nand breast cancer, while the other Wald-type tests often cannot identify any such association.\nBased on the performance of the FS-CL test in our numerical experiments, we believe the FS-CL procedure can be\na valuable tool for simultaneous model selection and group difference (or genomic association) testing.\n\nGeneralizing the FS-CL procedure is possible, which may lead to further improvements in terms of estimation accuracy\nand test power. First, recall that the composite likelihood function (\\ref{comp_lik}) admits only binary weights with\n$w \\in \\{0,1\\}^{{N_{cl}}}$. A natural implication of this framework is the sparsity of the resulting\nlikelihood composition (and the induced parameter space). Developing a continuous weighting scheme\nfor strengthening informativeness of the selected sub-likelihoods may further decrease the MCLE variance and\nincrease the test power.\nSo far the overall model complexity in our framework is kept under control by running a forward step-up\nprocedure for including informative sub-likelihoods progressively, and by limiting the maximum\nnumber of sub-likelihoods ${N_{cl}}^\\ast$ (cf. Section~\\ref{sec:modelcomplexity}).\nIn using continuous and sparse weights, however, the model complexity control may be better achieved by a\nsparsity-inducing smoothness penalization scheme for the weights, in the same spirit of the well established\nhigh-dimensional variable selection procedures in the regression literature (see e.g. \\cite{buhlmann2011statistics}).\n\n\n\\subsection*{Appendix: Density for the sum of ordered gamma variables}\nLet $Y_1,\\cdots, Y_K$ be $K$ i.i.d. $\\Gamma(2^{-1}p^\\prime,1)$ random variables. Define\n$S_k=\\sum_{j=1}^k Y_{(j)}$, $k=1,\\dots, K$, with $Y_{(1)}\\geq \\cdots\\geq Y_{(K)}$ being the reverse\norder statistics of $Y_1, \\cdots, Y_K$. Let $f_S(t|k)$ be the density of $S_k$. It is easy to show that the asymptotic\ndensity of the FS-CL statistic $T_{{\\text{fscl}}}$ following (\\ref{disktppclfs}) is $f_T(t|{N_{cl}}^\\ast)=\n2^{-1}f_S(2^{-1}t|k={N_{cl}}^\\ast)$.\nLet $g_{\\nu}(x)$, $G_{\\nu}(x)$ denote the density function and distribution function\nof a $\\Gamma(\\nu,1)$ distribution, respectively.\n\\cite{alam1979distribution} derived an analytic form for the density of $S_k$ which is given as\n\n", "itemtype": "equation", "pos": 42830, "prevtext": "\nwhere $I_{{N_{cl}}}$ is an ${N_{cl}}$-dimensional identity matrix and ``$\\otimes$\" denotes the Kronecker product.\nWith this setting the number of the pairwise sub-likelihoods containing nonzero elements of $\\delta$ under $H_1$\ndecreases from 3 to 1 as $m$ increases from 1 to 4, while the magnitude of each nonzero $\\delta_j$ increases.\nUsing the above setting we generate 10,000 replicates of $\\sqrt{n}{\\hat{\\delta}}$ under $H_0$ and $H_1$,\nrespectively. After that we set the significance level $\\alpha=0.05$, and compute the Monte Carlo estimates\nof the Type I error probability and the power for the FS-CL test, the Wald test and two of its variants LSSB and LSSBw.\nThe results are summarized in Table~\\ref{Table1}.\n\n\\begin{table}[htp]\n\\centering\n\\begin{tabular}{ cccccccccc }\n\\hline\nModel ($m$) & \\multicolumn{4}{c}{Type I error}&& \\multicolumn{4}{c}{Power} \\\\\n             & FS-CL & Wald & LSSB & LSSBw& & FS-CL & Wald & LSSB & LSSBw   \\\\ \\hline\n1           & 0.0461 & 0.0447 & 0.0462 & 0.0467 && 0.2736& 0.2785 & 0.2040 & 0.2159   \\\\\n2           & 0.0481 & 0.0466 & 0.0467 & 0.0458 && 0.5793 & 0.5436 & 0.4073 & 0.4426  \\\\\n3           & 0.0491 & 0.0499 & 0.0501 & 0.0495 && 0.7951 & 0.7153 & 0.5615 & 0.6015  \\\\\n4           & 0.0509 & 0.0506 & 0.0511 & 0.0527 && 0.8568 & 0.7423 & 0.5977 & 0.6355  \\\\ \\hline\n\\end{tabular}\n\\caption{Estimated Type I error and power for the FS-CL test, the Wald test, and the LSSB and LSSBw tests\ndescribed in Section \\ref{sec:tests}, for testing $H_0$ vs. $H_1$. Results are based on $10,000$ samples\ngenerated according to the setting described in Example 1.}\n\\label{Table1}\n\\end{table}\n\nFrom Table \\ref{Table1}, we see all the tests have similar Type I error probabilities around $\\alpha$.\nHowever, although the LSSB and LSSBw\ntests are computationally easier than our FS-CL test, they are inferior in terms of power. This is expected since the LSSB\ntest sets $V=I$ and the LSSBw test uses only the diagonal elements of $V$, leading to loss of information on the\ncorrelation between components of ${\\hat{\\delta}}$ belonging to the same sub-likelihood.\nIn summary, the FS-CL test performs uniformly better than all the other tests in terms of power,\nin all situations where the number of informative sub-likelihoods involved is sparse.\n\nFinally, we assessed the quality of the model-selection procedure annexed to our FS-CL algorithm by\ncomputing the Monte Carlo estimates of the Hamming distance between the optimally estimated composition rule\n$\\hat{w}$ given by the FS-CL test and the true composition rule $w^\\ast$ informed by each of the prescribed\nmodels. In all our simulations we have found that the Hamming distance is essentially equal to 0 up to a negligible\nsimulation error, which confirms the FS-CL test systematically selects\nonly informative sub-likelihoods to construct the test statistic $T_{\\text{fscl}}$.\n\n\n\n\\paragraph{Example 2: Latent multivariate Gaussian model.} Now we investigate the performance of our new test\nfor categorical data analysis based on a latent multivariate Gaussian model. This model has been previously\nstudied for analyzing single nucleotide polymorphisms (SNPs) data (cf. \\cite{pan2009asymptotic, han2012composite}).\nThe advantage of using the latent multivariate Gaussian model is its ability to model correlated categorical\nvariables through the latent quantiles and covariance matrix.\n\nConsider independent $d$-vector observations $Y_i =(Y_{i1}, \\dots, Y_{id})^T$, $i=1,\\dots,n$, with each\nelement $Y_{ij}$ being categorical taking one of $C$ labels $1, \\dots, C$.\nFor the $i$th $d$-vector observation, we assume there is a latent vector variable\n$Z_{i}=(Z_{i1}, \\dots,Z_{id})^T$ that follows a multivariate Gaussian distribution,\n$Z_{i}\\sim \\mathcal{N}_d(0,\\Sigma)$,  where $\\Sigma$ is a $d\\times d$ correlation matrix.\nWe also assume the existence of $C-1$ quantile constants $\\gamma_{j1}, \\gamma_{j2}, \\dots, \\gamma_{j(C-1)}$\nfor each $j=1,\\dots, d$, such that for all $i=1,\\dots, n$, $Y_{ij}$ takes label $k$ if $Z_{ij}\\in \\Gamma_{jk}\\equiv\n(\\gamma_{j(k-1)}, \\, \\gamma_{jk}]$, where $k=1,\\dots, C$ with $\\gamma_{j0}=-\\infty$ and $\\gamma_{jC}=\\infty$.\nIt is easy to see that the marginal and joint probability distributions of $(Y_{i1}, \\dots, Y_{id})$ are\ndetermined by the quantile parameters $\\gamma=\\{\\gamma_{j1}, \\cdots, \\gamma_{j(C-1)};\\, j=1,\\cdots,d\\}$\nand correlation matrix parameter $\\Sigma$. For example, $\\Pr(Y_{ij}=2) =\\Pr(\\gamma_{j1}<Z_{ij}\\leq \\gamma_{j2})$\nand $\\Pr(Y_{ij}=2, Y_{ij^\\prime}=3) =\\Pr(\\gamma_{j1}<Z_{ij}\\leq \\gamma_{j2}, \\,\n\\gamma_{j^\\prime 2}<Z_{ij^\\prime}\\leq \\gamma_{j^\\prime 3})$, etc.\n\n\n\n\n\n\n\n\nWe use $\\theta$ to denote the vector collecting the elements in $\\gamma$ and $\\Sigma$.\n\nLet  $f(z_1, \\dots, z_d;\\Sigma)$ denote the $d$-variate normal density $\\mathcal{N}_d(0,\\Sigma)$.\nNow estimating the quantile parameters $\\gamma$ from the data $(Y_1,\\cdots, Y_n)$ can be done by\nmaximizing the weighted one-wise marginal composite likelihood function\n\n", "index": 35, "text": "\\begin{align} \\label{CL1}\n&CL_{1}(\\gamma)= \\prod_{i=1}^{n}\\prod_{j=1}^{d}\\left(\\prod_{k=1}^C\\left[\\int_{\\Gamma_{jk}}\nf(z_{j};1)d z_{j}\\right]^{I(Y_{ij}=k)}\\right)^{w_j},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle CL_{1}(\\gamma)=\\prod_{i=1}^{n}\\prod_{j=1}^{d}\\left(\\prod_{k=1}^{%&#10;C}\\left[\\int_{\\Gamma_{jk}}f(z_{j};1)dz_{j}\\right]^{I(Y_{ij}=k)}\\right)^{w_{j}},\" display=\"inline\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><msub><mi>L</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover></mstyle><msup><mrow><mo>[</mo><mrow><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></msub></mstyle><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo>;</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>z</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>]</mo></mrow><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow><mo>)</mo></mrow><msub><mi>w</mi><mi>j</mi></msub></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhere\n$l_{r,k}=\\int_0^\\infty\nx^{2^{-1}p^\\prime (K-k)+r}(1-G_{2^{-1}p^\\prime}(x))^{k-1}g_{2^{-1}p^\\prime}(x)\\text{d}x$;\n$d_{r,s}$ is computed recursively as\n\n", "itemtype": "equation", "pos": 63044, "prevtext": "\nwhere $I(Y_{ij}=k)$ is an indicator function and $(w_1,\\cdots, w_d)$ is the binary weight vector (hence\n${N_{cl}}=d$ here). One can extend (\\ref{CL1}) by including pairwise likelihood components as\ndescribed in \\cite{han2012composite} so that $\\gamma$\nand $\\Sigma$ can be estimated simultaneously.\n\nWe consider having two groups (case and control) of $d=6$ dimensional observations each taking\none of $C=3$ labels (categories). The correlation structure of the latent vector variable is set to be\n$\\Sigma=I$ where $I$ is a $d$-dimensional  identity matrix. The group-specific latent parameters\nare then $\\theta_0=\\gamma_0$ and\n$\\theta_1=\\gamma_1$.  For the control group we set\n$\\gamma_{0}=(-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3;-0.3,0.3)$ as the true values, while for the\ncase group we set $\\gamma_1 = \\gamma_0 + \\delta$ where\n$\\delta=(-\\epsilon,\\epsilon;0,0;0,0;0,0;0,0;0,0)$ and $\\epsilon  = 0.3, 0.4, 0.5$. We\ngenerate 1000 Monte Carlo samples of size $n= 200$  ($100$ controls and $100$ cases) using the above\nsetting, and denote the group-specific MCLEs by\n$\\hat{\\gamma}_{0}$ and $\\hat{\\gamma}_{1}$, respectively, where $\\hat{\\gamma}_{0}$ and $\\hat{\\gamma}_{1}$\nare $2\\times6$ dimensional vectors. For each sample, we compute the MCLE difference\n$\\hat{\\delta}  = \\hat{\\gamma}_{1}-\\hat{\\gamma}_{0}$ and estimate the asymptotic covariance matrix\n$V$ of $\\sqrt{n}\\hat{\\delta}$ by nonparametric bootstrap as described in Section \\ref{sec:alg}.\nWe then perform the FS-CL test and the Wald, LSSB and LSSBw tests discussed in the paper for testing $H_0: \\delta=0$\nagainst a sparse local alternative. We use the permutation method to simulate the null distributions and the\nassociated 0.05 level critical values in these tests, by which we compute the Monte Carlo estimates\nof the Type I error and the power of these tests based on the 1000 generated samples.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccccc } \\hline\n            &  \\multicolumn{5}{c}{FS-CL}             & Wald & LSSB & LSSBw  \\\\\n$\\epsilon$ $\\setminus$ ${N_{cl}}^\\ast$  & 1& 2 & 3 & 4&  5 &6 &  &  \\\\\n\\hline\n$0$         &  0.045 & 0.052 & 0.048 & 0.047 & 0.046 & 0.045&  0.056 & 0.053         \\\\\n$0.3$       &  0.614 & 0.585 & 0.543 & 0.518 & 0.501 & 0.496&  0.203 & 0.181        \\\\\n$0.4$       &  0.873 & 0.840 & 0.812 & 0.776 & 0.755 & 0.752& 0.380 & 0.328        \\\\\n$0.5$       &  0.979 & 0.972 & 0.950 & 0.938  & 0.927 & 0.924& 0.653 & 0.584 \\\\ \\hline\n\\end{tabular}}\n\\caption{Monte Carlo estimates of Type I error probability ($\\epsilon=0$) and power ($\\epsilon>0$) of the\nvarious tests when the data have the latent multivariate Gaussian model described in\nExample 2. The tests considered are FS-CL with ${N_{cl}}^\\ast$ ranging from 1 to 6 and the Wald test, LSSB and LSSBw\ntests described in Section \\ref{sec:framework}. Note that the Wald test corresponds to ${N_{cl}}^\\ast=6$ (no selection).\nResults are based on $1000$ Monte Carlo samples.}\n\\label{Table2}\n\\end{table}\n\nTable~\\ref{Table2} gives the Monte Carlo estimates of the Type I error probability (in row $\\epsilon = 0$) and the\npower (in rows $\\epsilon > 0$). The table reveals the power for the FS-CL test is considerably larger than that\nfor all the other tests in all simulated situations of group difference of size $\\epsilon$.\nSpecifically, the power improvement is dramatic when comparing the FS-CL test with the LSSB and LSSBw tests\nfor values of $\\epsilon$ closer to zero.\n\nAlso Table~\\ref{Table3} shows the values of CL-BIC described in Section~\\ref{sec:modelcomplexity}\nfor each ${N_{cl}}^\\ast$. We see CL-BIC is minimized at ${N_{cl}}^\\ast=1$ in all the scenarios of $\\epsilon$.\nThis result conforms to the true setting used in generating the Monte Carlo samples. Namely, only the first\none-wise marginal sub-likelihood contains the information about $\\epsilon$.\nThus the power of the FS-CL test should be the largest at selecting ${N_{cl}}^\\ast=1$, which is clearly confirmed by\nthe results in Table~\\ref{Table2}.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccc }\n\\hline\n   $\\epsilon$  $\\setminus$ ${N_{cl}}^\\ast$        & 1   & 2  & 3 & 4 &  5     &        6            \\\\\n\\hline\n$0.3$     &  431.76 & 440.49 & 449.11 & 457.46& 465.90& 474.23       \\\\\n$0.4$     & 424.61 & 436.60 & 446.48& 455.46& 464.29& 472.86  \\\\\n$0.5$     &  414.31 & 431.38 & 442.96& 452.83& 462.19& 471.13 \\\\ \\hline\n\\end{tabular}}\n\\caption{Composite likelihood Bayesian information criterion (CL-BIC) values at different maximum numbers of steps\n(${N_{cl}}^\\ast$) in the forward step-up algorithm described in Section~\\ref{sec:alg}.\nThe values are the averages computed based on 1000 Monte Carlo samples generated from the multivariate latent\nGaussian model in Example~2 with various magnitudes of the parameter difference under the alternative\nhypothesis (i.e. $\\epsilon=0.3,0.4,0.5$). }\n\\label{Table3}\n\\end{table}\n\n\\noindent\\textbf{Example 3: Effects of increasing the number of candidate sub-likelihoods.}\nWe continue by considering the latent Gaussian model underlying the case-control data  described\nin Example 2  to see how the FS-CL test procedure and the other discussed tests perform as  the number\nof candidate one-wise sub-likelihoods, ${N_{cl}}$, grows. In the set-up we let ${N_{cl}}=d$ change from 6 to 20 but\nlet only the first one-wise sub-likelihood contain the information of case-control difference.\nWe continue to assume $C=3$ categories for each variable in the data. But differently from Example 2, we consider\nparameter vectors $\\gamma_0^{(m)}$ and $\\gamma_1^{(m)}$  of\nlength $2(m+5)$, for $m=1, \\dots 15$ (note ${N_{cl}}=m+1$). Specifically, we set\n$\\gamma^{(m)}_{0,i}=0.3\\times(-1)^i$, $i=1,\\cdots, 2(m+5)$,\nand $\\gamma^{(m)}_{1} =\\gamma^{(m)}_{0}+\\delta^{(m)}$\nwhere $\\delta^{(m)}$ is a vector of length $2(m+5)$ with $i$th element $\\delta_i=0.8\\times(-1)^i$,\nif $i\\leq2$, and $\\delta_i=0$ otherwise.\nThe covariance matrix $\\Sigma$ of the latent vector variable is set as the identity matrix.\nFor each model $m=1,\\dots, 15$, we generate 200 Monte Carlo samples of size $n=120$ (60 cases and 60 controls)\nand estimate the test power where the significance level is set as $\\alpha=0.05$.\n\n\n\\begin{figure}[t]\n     \\centering\n     \\includegraphics[scale=0.7]{snpchangepic1.pdf}\n          \\caption{Monte Carlo estimates of the power for the FS-CL test, and the Wald, LSSB and LSSBw tests\ndescribed in Section \\ref{sec:tests} for increasing numbers of sub-likelihoods ranging from 6 to 20.\nVertical bars denote the simulated 95\\% probability intervals of the power. The results are\nobtained based on 200 Monte Carlo samples from the multivariate latent Gaussian model as described in Example 3.}\n     \\label{fig:snpchangepic}\n\\end{figure}\n\nFigure~\\ref{fig:snpchangepic} displays the Monte Carlo estimates of power (represented by the dots),\ntogether with their 95\\% probability intervals, for the FS-CL, Wald, LSSB and LSSBw tests as the number of\ncandidate sub-likelihoods grows. As the number of uninformative sub-likelihoods increases,\nit shows the LSSB  and LSSBw tests are increasingly weak compared to the FS-CL and\nWald tests. The FS-CL and Wald tests have similar power when the number of candidate sub-likelihoods is small.\nRemarkably, the Wald test's performance decreases dramatically when ${N_{cl}}$ increases, while the power of the\nFS-CL test remains stable regardless of the number of irrelevant sub-likelihoods considered.\nThis behavior can be explained by noting that, as the data dimension $d$ (consequently the number of candidate\nsub-likelihoods ${N_{cl}}$) increases, more noise is added to the unweighted composite likelihood.\nTherefore those sub-likelihoods informative for distinguishing the alternative hypothesis from the null\nwill become less significant in the Wald, LSSB  and LSSBw tests.\nIn contrast, the FS-CL test tends to keep such informative sub-likelihoods and to remove the noisy ones,\ntherefore having achieved a stable high power (always near $0.9$ in Figure~\\ref{fig:snpchangepic}).\n\n\\subsection{Analysis of the Australian Breast Cancer Family genomic data}\n\\label{sec:real}\nIn this section, we apply the FS-CL procedure and the Wald, LSSB and LSSBw tests to data from\na case-control study on breast cancer. Cases are obtained from the Australian Breast Cancer Family\n(ABCF) study \\citep{mccredie2003risk} while controls are from the Australian Mammographic Density Twins and\nSisters Study \\citep{odefrey2010common}. The data set consists of 356 observations (284 controls and\n72 cases) on 100 SNPs. SNPs are the mutated pairs of single nucleotide\n(A,T,C,G) in a DNA sequence. These mutated pairs can be categorized into three groups denoted as 0, 1 and 2\n(0 and 2 are homozygous and 1 denotes heterozygous).\n\nAfter recommended data cleaning and quality control, the final dataset comprises\n356 vector observations on 61 SNP variables and contains no missing data.\nOur objective is to test the significance of association between the SNPs and breast cancer.\n\n\\textit{Case 1: Weakly dependent SNPs.}  In order to illustrate our testing procedure in the context of weakly\ndependent SNPs, we select 10 SNPs (\\texttt{rs10082248\\_A}, \\texttt{rs806645\\_T}, \\texttt{rs3765945\\_G},\n\\texttt{rs1056836\\_C}, \\texttt{rs4148326\\_C}, \\texttt{rs6717546\\_A}, \\texttt{rs1845557\\_C},\n\\texttt{rs3775774\\_C}, \\texttt{rs1651074\\_A}, and \\texttt{rs528723\\_C}) as reported in Figure~\\ref{fig:snpplot1}.\nThese SNPs are selected by investigating sample correlations of all the 61 SNPs and picking those SNPs\nwith the pairwise sample correlations, among the selected, being smaller than 0.1. To test the significance\nof association between the selected SNPs and breast cancer, we fit a latent Gaussian model described in\nExample~2 for these SNPs using the maximum composite likelihood method, and then implement the FS-CL\nprocedure, with various choices of ${N_{cl}}^\\ast$, for testing the case-control difference between the\nquantile parameters involved in the latent model.\n\n\\begin{table}[h]\n\\centering\n\\scalebox{0.97}{\n\\begin{tabular}{ cccccccccccccc}\\hline\n& \\multicolumn{9}{c}{FS-CL}  & Wald  & LSSB & LSSBw \\\\\n${N_{cl}}^\\ast$    & 1    & 2  & 3 & 4 &  5     &        6 & 7& 8 &   9  &  10   &     &        \\\\\n\\hline\nCL-BIC         &  676 & 465 &  573 &  651  & 653  & 703 & 750 &782 & 821 &   \\\\\n$p$-value    &0.12  &0.04&  0.08&  0.08 & 0.09 & 0.10 & 0.10  & 0.10 & 0.09  & 0.09    &  0.50  & 0.27 \\\\ \\hline\n\\end{tabular}}\n\\caption{Composite likelihood Bayesian information criterion (CL-BIC) with respect to ${N_{cl}}^\\ast$ ranking\nfrom 1 to 9, as well as the $p$-values of the FS-CL test and the Wald type tests described in Section~\\ref{sec:tests},\nfor the 10 weakly dependent SNPs. The CL-BIC values are computed using (\\ref{TIC}), and the $p$-values of\nthe FS-CL test are acquired from the permutation null distribution of the test statistics as described in\nSection~\\ref{sec:nulldistribution}. }\n\\label{Table4}\n\\end{table}\n\nTable~\\ref{Table4} shows the $p$-values of the FS-CL test, as well as the CL-BIC values for ${N_{cl}}^\\ast$\nranking from 1 to 9. The CL-BIC values suggest that ${N_{cl}}^\\ast=2$ yields the best fitted model.\nWhen ${N_{cl}}^\\ast=2$, the $p$-value of the FS-CL test is 0.04, while the $p$-value for the Wald, LSSB\nand LSSBw tests are 0.09, 0.50, and 0.27 respectively. At the 0.05 significance level the FS-CL test rejects the\nnull hypothesis of no SNPs association with the disease, while the other tests cannot reach the same conclusion.\n\n\nWhen ${N_{cl}}^\\ast=2$, the FS-CL procedure selects SNPs \\texttt{rs806645\\_T} and \\texttt{rs10082248\\_A} as having\nsignificant association with the disease. To investigate the validity of this selection, we conduct marginal\nchi-square association tests for between individual SNPs and the disease. Figure~\\ref{fig:snpplot1} shows the\ncorrelations between the SNPs under consideration and the $p$-values from the marginal association tests.\nFrom Figure~\\ref{fig:snpplot1} we see the 10 weakly dependent SNPs considered in this case include the first\nSNP from the first 5 correlated ones and the last 9 ones. The two SNPs selected by the FS-CL\nhave very small $p$-values (marked with thick lines), compared to the other SNPs.\n\nFor illustration, Figure~\\ref{fig:tourplot} (left) displays bootstrap distributions of the MCLEs of the latent quantile\nparameters for a selected SNP (\\texttt{rs806645\\_T}) in the respective case and control groups, while\nFigure~\\ref{fig:tourplot} (right) displays the counterparts for an unselected SNP (\\texttt{rs3765945\\_G}). The\ntriangles and circles represent the bootstrap replicates from the case and control groups respectively.\nWhen comparing case and control groups for a selected SNP, the figure implies the quantile parameters values\nfor the two groups are well separated, concentrating into different clusters.\nOn the other hand, the bootstrap distributions for an unselected SNP are overlapping and not clearly\ndistinguishable. Both Figures~\\ref{fig:snpplot1} and \\ref{fig:tourplot} suggest that the SNPs selected by the FS-CL\nprocedure are more likely to change their values from control to case,\nand they appear to have significant effects on breast cancer.\n\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.111]{snpplot14.pdf}\n    \\caption{SNP plot of the 14 SNPs used in assessing the group difference in regard to their associations with\nbreast cancer. The first SNP and the last 9 SNPs are used in Case 1 in Section~\\ref{sec:real}; and the first 5 SNPs\nare used in Case 2 in Section~\\ref{sec:real}. The right hand side of the figure shows the $p$-values (under a negative log scale)\nof the association tests between individual SNPs and breast cancer. The thick lines refer to the two selected SNPs by\nthe FS-CL procedure. LHS of the figure shows the correlation heat map among SNPs (light color for small correlation). }\n    \\label{fig:snpplot1}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[scale=0.495]{tourplot21.pdf}\n    \\caption{Bootstrap distributions for the quantile parameters estimates $\\hat{\\gamma}_1$ and $\\hat{\\gamma}_2$\nfor the selected SNP \\texttt{rs806645\\_T} (left) and the unselected SNPs \\texttt{rs3765945\\_G} (right),\nobtained from 100 bootstrap replicates. Triangles represent estimates from the case group and the solid contour lines\nspecify the estimated confidence regions in the case group. Small circles represent estimates from the control group and the\ndashed contour lines specify the estimated confidence regions in the control group.}\n    \\label{fig:tourplot}\n\\end{figure}\n\n\\textit{Case 2: Dependent SNPs.} Next, we focus on clusters of dependent SNPs having high correlations.\nFor illustration purpose, we choose the cluster of SNPs \\texttt{rs806645\\_T}, \\texttt{rs2754530\\_T}, \\texttt{rs2268796\\_A},\n\\texttt{rs4952220\\_C}, and \\texttt{rs2300697\\_C}. They are the first five SNPs in Figure~\\ref{fig:snpplot1}\nwhich are highly correlated. Other clusters can also be analyzed which will not be detailed here.\n\nTable~\\ref{Table5} shows the $p$-values of the FS-CL test at specified ${N_{cl}}^\\ast=1,\\dots,4$. It also gives the\ncorresponding CL-BIC values, which suggest that the composite likelihood containing a single sub-likelihood with\n${N_{cl}}^\\ast=1$ gives the best modelling. The $p$-value of the FS-CL test at ${N_{cl}}^\\ast=1$ equals 0.04, while the\n$p$-values for the Wald, LSSB and LSSBw tests are 0.08, 0.09, and 0.03 respectively. At significance level 0.05,\nthe FS-CL and LSSBw tests suggest the null hypothesis be correctly rejected, while the other tests cannot reach\nthe same conclusion.\n\n\\begin{table}[htb]\n\\centering\n\\scalebox{1}{\n\\begin{tabular}{ cccccccccccccc}\\hline\n& \\multicolumn{4}{c}{FS-CL}  & Wald  & LSSB & LSSBw \\\\\n${N_{cl}}^\\ast$    & 1    & 2  & 3 & 4 &   5    &      &     &             \\\\\n\\hline\nCL-BIC          &  676 & 739 &  772 & 801 &   &  & \\\\\n$p$-value    &0.04  &0.02&  0.06&  0.08 & 0.08 & 0.09 & 0.03 \\\\ \\hline\n\\end{tabular}}\n\\caption{CL-BIC values with respect to ${N_{cl}}^\\ast$ ranking from 1 to 4,\nas well as the $p$-values of the FS-CL test and the Wald type tests describe in Section~\\ref{sec:tests}, for the 5\ndependent SNPs. The CL-BIC values are computed using (\\ref{TIC}), and the $p$-values of the FS-CL test are\nacquired from the permutation null distribution of the test statistics as described in Section~\\ref{sec:nulldistribution}. }\n\\label{Table5}\n\\end{table}\n\n\\section{Conclusion and discussion}\n\\label{sec:conclusion}\n\nBuilding on the well-established composite likelihood estimation framework, we have developed a method of\nsimultaneous composition rule selection and group difference testing in multivariate parametric models for\nhigh-dimensional data. The method is particularly useful for multiple genotype-phenotype association testing in\ngenome-wide association study.\nIt constructs sparse composite likelihood by including a small number of informatively selected\nsub-likelihoods, while dropping redundant or noisy sub-likelihoods that do not contribute to explaining\nthe group difference or genomic association. The procedure is implemented by our forward search and test algorithm\nwhich progressively includes useful sub-likelihoods by step-up maximizations of the bootstrap estimated\npower. In all our numerical experiments, the resultant FS-CL test has higher power than\nthe composite likelihood based Wald, LSSB and LSSBw tests, with remarkable power gains when the model\ncomplexity increases.\n\nThe FS-CL method has been applied to analyze a case-control dataset for GWAS, obtained from Australian Breast Cancer\nFamily Study, under the multivariate latent Gaussian framework studied by \\cite{han2012composite}.\n\n\nThe FS-CL test enables us to conclude about the significant overall association between particular SNPs\nand breast cancer, while the other Wald-type tests often cannot identify any such association.\nBased on the performance of the FS-CL test in our numerical experiments, we believe the FS-CL procedure can be\na valuable tool for simultaneous model selection and group difference (or genomic association) testing.\n\nGeneralizing the FS-CL procedure is possible, which may lead to further improvements in terms of estimation accuracy\nand test power. First, recall that the composite likelihood function (\\ref{comp_lik}) admits only binary weights with\n$w \\in \\{0,1\\}^{{N_{cl}}}$. A natural implication of this framework is the sparsity of the resulting\nlikelihood composition (and the induced parameter space). Developing a continuous weighting scheme\nfor strengthening informativeness of the selected sub-likelihoods may further decrease the MCLE variance and\nincrease the test power.\nSo far the overall model complexity in our framework is kept under control by running a forward step-up\nprocedure for including informative sub-likelihoods progressively, and by limiting the maximum\nnumber of sub-likelihoods ${N_{cl}}^\\ast$ (cf. Section~\\ref{sec:modelcomplexity}).\nIn using continuous and sparse weights, however, the model complexity control may be better achieved by a\nsparsity-inducing smoothness penalization scheme for the weights, in the same spirit of the well established\nhigh-dimensional variable selection procedures in the regression literature (see e.g. \\cite{buhlmann2011statistics}).\n\n\n\\subsection*{Appendix: Density for the sum of ordered gamma variables}\nLet $Y_1,\\cdots, Y_K$ be $K$ i.i.d. $\\Gamma(2^{-1}p^\\prime,1)$ random variables. Define\n$S_k=\\sum_{j=1}^k Y_{(j)}$, $k=1,\\dots, K$, with $Y_{(1)}\\geq \\cdots\\geq Y_{(K)}$ being the reverse\norder statistics of $Y_1, \\cdots, Y_K$. Let $f_S(t|k)$ be the density of $S_k$. It is easy to show that the asymptotic\ndensity of the FS-CL statistic $T_{{\\text{fscl}}}$ following (\\ref{disktppclfs}) is $f_T(t|{N_{cl}}^\\ast)=\n2^{-1}f_S(2^{-1}t|k={N_{cl}}^\\ast)$.\nLet $g_{\\nu}(x)$, $G_{\\nu}(x)$ denote the density function and distribution function\nof a $\\Gamma(\\nu,1)$ distribution, respectively.\n\\cite{alam1979distribution} derived an analytic form for the density of $S_k$ which is given as\n\n", "index": 37, "text": "\\begin{align}\nf_S(t|k)= \\frac{k {K\\choose k}}{\\Gamma(2^{-1}p^\\prime+1)^{K-k}}\\sum_{r=0}^\\infty\n(-1)^r d_{r,K-k} l_{r,k} \\frac{1}{r!} g_{2^{-1}p^\\prime K+r}(t),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{S}(t|k)=\\frac{k{K\\choose k}}{\\Gamma(2^{-1}p^{\\prime}+1)^{K-k}%&#10;}\\sum_{r=0}^{\\infty}(-1)^{r}d_{r,K-k}l_{r,k}\\frac{1}{r!}g_{2^{-1}p^{\\prime}K+r%&#10;}(t),\" display=\"inline\"><mrow><msub><mi>f</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>K</mi><mi>k</mi></mfrac><mo>)</mo></mrow></mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>p</mi><mo>\u2032</mo></msup></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>K</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mi>r</mi></msup><msub><mi>d</mi><mrow><mi>r</mi><mo>,</mo><mrow><mi>K</mi><mo>-</mo><mi>k</mi></mrow></mrow></msub><msub><mi>l</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>r</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mstyle><msub><mi>g</mi><mrow><mrow><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>p</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>K</mi></mrow><mo>+</mo><mi>r</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nThus, the asymptotic density of the test statistic $T_{fscl}$  following (\\ref{disktppclfs}) is given by\n\n", "itemtype": "equation", "pos": 63372, "prevtext": "\nwhere\n$l_{r,k}=\\int_0^\\infty\nx^{2^{-1}p^\\prime (K-k)+r}(1-G_{2^{-1}p^\\prime}(x))^{k-1}g_{2^{-1}p^\\prime}(x)\\text{d}x$;\n$d_{r,s}$ is computed recursively as\n\n", "index": 39, "text": "\\begin{align*}\nd_{0,s}=1, \\quad d_{r,1} =\\frac{p^\\prime}{p^\\prime+2r}, \\quad\nd_{r+1,s}=\\frac{p^\\prime s}{2}\\sum_{j=0}^r {r \\choose j}\n\\left(2^{-1}p^\\prime+1+r-j \\right)^{-1} d_{j,s-1}, \\ \\  s>1.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle d_{0,s}=1,\\quad d_{r,1}=\\frac{p^{\\prime}}{p^{\\prime}+2r},\\quad d%&#10;_{r+1,s}=\\frac{p^{\\prime}s}{2}\\sum_{j=0}^{r}{r\\choose j}\\left(2^{-1}p^{\\prime}%&#10;+1+r-j\\right)^{-1}d_{j,s-1},\\ \\ s&gt;1.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>d</mi><mrow><mn>0</mn><mo>,</mo><mi>s</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><msub><mi>d</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>p</mi><mo>\u2032</mo></msup><mrow><msup><mi>p</mi><mo>\u2032</mo></msup><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></mrow></mfrac></mstyle></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><msub><mi>d</mi><mrow><mrow><mi>r</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>p</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>s</mi></mrow><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>r</mi></munderover></mstyle><mrow><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>r</mi><mi>j</mi></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><mrow><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>p</mi><mo>\u2032</mo></msup></mrow><mo>+</mo><mn>1</mn><mo>+</mo><mi>r</mi></mrow><mo>-</mo><mi>j</mi></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>d</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>s</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>s</mi><mo>&gt;</mo><mn>1</mn></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05886.tex", "nexttext": "\nwhich is a mixture of $\\chi^2$ distributions with varying degrees of freedom.\n\n\\bibliographystyle{abbrvnat} \n\n\\bibliography{Bibliography} \n\n\n\n\n\n", "itemtype": "equation", "pos": 63685, "prevtext": "\nThus, the asymptotic density of the test statistic $T_{fscl}$  following (\\ref{disktppclfs}) is given by\n\n", "index": 41, "text": "\\begin{align} \\label{null}\nf_T(t|k)= \\frac{k {K\\choose k}}{2\\Gamma(2^{-1}p^\\prime+1)^{K-k}}\\sum_{r=0}^\\infty\n(-1)^r d_{r,K-k} l_{r,k} \\frac{1}{r!} g_{2^{-1}p^\\prime K+r}(2^{-1}t),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{T}(t|k)=\\frac{k{K\\choose k}}{2\\Gamma(2^{-1}p^{\\prime}+1)^{K-k%&#10;}}\\sum_{r=0}^{\\infty}(-1)^{r}d_{r,K-k}l_{r,k}\\frac{1}{r!}g_{2^{-1}p^{\\prime}K+%&#10;r}(2^{-1}t),\" display=\"inline\"><mrow><msub><mi>f</mi><mi>T</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>K</mi><mi>k</mi></mfrac><mo>)</mo></mrow></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>p</mi><mo>\u2032</mo></msup></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>K</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mi>r</mi></msup><msub><mi>d</mi><mrow><mi>r</mi><mo>,</mo><mrow><mi>K</mi><mo>-</mo><mi>k</mi></mrow></mrow></msub><msub><mi>l</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>r</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mstyle><msub><mi>g</mi><mrow><mrow><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>p</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>K</mi></mrow><mo>+</mo><mi>r</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msup><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}]