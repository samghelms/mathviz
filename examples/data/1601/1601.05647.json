[{"file": "1601.05647.tex", "nexttext": "\nDifferent metrics are motivated due to different treatment of positive/negative match and mismatches in indicators of phonological classes. The most effective similarity measure for linguistic parsing can imply different cognitive mechanism governing human perception of linguistic attributes. \n \nIn the top-down approach to linguistic parsing, syllable boundaries are first estimated from the speech signal. Then, the similarity between the class-specific codebook members and a phonological posterior is measured. The class label is determined based on the maximum similarity. We provide empirical results on linguistic parsing in the following Section~\\ref{sec:exp}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Experiments}\n\\label{sec:exp}\n\n\n\n\n\n\\subsection{Experimental setup}\n\nWe use an open-source phonological vocoding\nplatform\\footnote{\\url{https://github.com/idiap/phonvoc}} to obtain\nphonological posteriors. Briefly, the platform is based on cascaded\nspeech analysis and synthesis that works internally with the\nphonological speech representation. In the phonological analysis part,\nphonological posteriors are detected directly from the speech signal\nby a bank of parallel Deep Neural Networks (DNNs). Each DNN determines\nthe probability of a particular phonological class. In the following,\nwe describe the databases and DNN training procedure to estimate the\nphonological posterior features. \n\n\n\\subsubsection{Speech Databases}\n\nTo confirm that uniqueness of class-specific sparsity structures is a\nlanguage-independent property, we conducted our evaluations on English\nand French speech corpora. Accordingly, to confirm independence of the\nproposed methodology on a phonological system, two different\nphonological speech representations are considered: the SPE feature\nset~\\citep{chomsky68sound}, and the extended SPE feature set~\\citep{Cernak15icassp} are used in\ntraining of the DNNs for phonological posterior estimation on English\nand French data respectively. Table~\\ref{tab:data} lists data used\nin the experimental setup.\n\n\\begin{table}[th]\n  \\caption{\\label{tab:data} {\\it Data used for DNN training to obtain\n      phonological posteriors, and evaluation data.}}\n\\vspace{2mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|c|}\n\\hline \nPurpose & Database & Size (hours) \\\\\n\\hline  \\hline\nTraining English data & WSJ & 66 \\\\\nTraining French data & Ester & 58\\\\\n\\hline\nEvaluation English data & Nancy &  1.5 \\\\\nEvaluation French data & SIWIS &  1 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}\n\nTo train the DNNs for phonological posterior estimation on English\ndata, we use the Wall Street Journal (WSJ0 and WSJ1) continuous speech\nrecognition corpora \\citep{WSJDB}. To train the DNNs for phonological\nposterior estimation on French data, we use the Ester\ndatabase~\\citep{ester06} containing standard French radio broadcast\nnews in various recording conditions.\n\nOnce DNNs are trained, the phonological posterior features are estimated for\nthe Nancy and SIWIS recordings which is used for the subsequent\ncross-database linguistic parsing experiments.\n\nThe Nancy database is provided in Blizzard\nChallenge\\footnote{\\url{http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011}}. The\nspeaker is known as ``Nancy'', and she is a US English native female\nspeaker. The database consists of 16.6 hours of high quality\nrecordings of natural expressive human speech made in an anechoic\nchamber at a 96K sampling rate during 2007 and 2008. The audio of the\nlast 1.5 hours of the recordings was selected and re-sampled to\nsampling frequency of 16kHz for our experiments. The transcription of\nthe audio data comprised of around 12k utterances. The text was\nprocessed by a conventional and freely available TTS\nfront-end~\\citep{festival}, resulting the segmental (quinphone\nphonetic context) and supra-segmental (full-context) labels. The\nfull-context labels included binary lexical stress and prosodic\naccents. The labels were forced aligned with the audio recordings.\n \n\nThe SIWIS\ndatabase\\footnote{\\url{https://www.idiap.ch/project/siwis/downloads/siwis-database}}\nconsists of 26 native French speakers. The labels were obtained using\nforced alignment. We generated full-context labels using the French\ntext analyzer eLite~\\citep{Roekhaut2014}. Unlike Nancy speech\nrecordings, SIWIS data is noisy and recorded in less restricted\nacoustic conditions. Evaluations on both English and French\ncorpora enables us to confirm and compare the applicability of our\nlinguistic parsing method across languages with different phonological\nclasses as well as uncontrolled recording scenarios. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{DNN Training for Phonological Posterior Estimation}\n\nFirst, we trained a phoneme-based automatic speech recognition system\nusing mel frequency cepstral coefficients (MFCC) as acoustic features. The phoneme set comprising of 40 phonemes\n(including ``sil'', representing silence) was defined by the CMU\npronunciation dictionary. The three-state, cross-word triphone models\nwere trained with the HTS variant~\\citep{Zen:HTS} of the HTK toolkit\non the 90\\% subset of the \\textit{si\\_tr\\_s\\_284} set. The remaining\n10\\% subset was used for cross-validation. The acoustic models were\nused to get boundaries of the phoneme labels.\n\nThen, the labels of phonemes were mapped to the SPE phonological\nclasses. In total, $K$ DNNs were trained as the phonological analyzers\nusing the short segment (frame) alignment with two output labels indicating\nwhether the $k$-th phonological class exists for the aligned phoneme\nor not. The number of $K$ is determined from the set of phonological\nclasses and it is equal to 15 for the English data, and 24 for the\nFrench data.  \nThe DNNs have the architecture of 351x1024x1024x1024x2 neurons,\ndetermined  empirically. The input vectors are 39 order MFCC features\nwith the temporal context of 9 successive frames.\n\n\n\nThe parameters were initialized using\ndeep belief network pre-training done by single-step contrastive \ndivergence (CD-1) procedure of \\cite{Hinton06}. The DNNs with the\nsoftmax output function were then trained using a mini-batch based\nstochastic gradient descent algorithm with the cross-entropy cost\nfunction of the KALDI\ntoolkit~\\citep{PoveyASRU2011}. Table~\\ref{tab:SPEaccuracies} lists the\ndetection accuracy for different phonological classes. \nThe DNNs\noutputs for individual phonological classes determine the phonological\nposterior probabilities. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we trained French phonological posterior estimators. The\nphoneme set comprising 38 phonemes (including ``sil'') was defined by\nthe BDLex~\\citep{bdlex} lexicon. The aligned phoneme labels were\nmapped to the French extended SPE (eSPE) phonological classes.\n\nThe DNN architecture is similar to the English data, and it is\ninitialized by deep belief network\npre-training. Table~\\ref{tab:eSPEaccuracies} lists the detection\naccuracy for various eSPE classes.\n\n\n{\\small\n\\begin{table}[h]\n  \\caption{\\label{tab:SPEaccuracies} {\\it Classification accuracies (\\%)\n      of English phonological class detectors on train and cross-validation (CV) data.}}\n\\vspace{-4mm} \n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nPhonological & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Phonological &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nClasses & Train & CV & Classes & Train & CV \\\\\n\\hline \\hline\nvocalic & 97.3 & 96.5 & round & 98.7 & 98.1 \\\\\nconsonantal & 96.3 & 95.0 & tense & 96.6 & 95.3 \\\\\nhigh & 97.0 & 95.7 & voice & 96.5 & 95.6 \\\\\nback & 96.2 & 94.8 & continuant & 97.3 & 96.3 \\\\\nlow & 98.4 & 97.6 & nasal & 98.9 & 98.4 \\\\\nanterior & 96.8 & 95.6 & strident & 98.7 & 98.2  \\\\\ncoronal & 96.1 & 94.6 & rising & 98.6 & 97.8 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}}\n\n{\\small\n\\begin{table}[h]\n  \\caption{\\label{tab:eSPEaccuracies} {\\it Classification accuracies (\\%)\n      of the French phonological class detectors on train and cross-validation (CV) data.}}\n\\vspace{-4mm}\n\\centerline{\n\\begin{tabu}{|l|c|c|[0.8pt]l|c|c|}\n\\hline\nPhonological & \\multicolumn{2}{c|[0.8pt]}{Accuracy (\\%)} & Phonological &\n\\multicolumn{2}{c|}{Accuracy (\\%)} \\\\\n\\cline{2-3} \\cline{5-6}\nClasses & Train & CV & Classes & Train & CV \\\\\n\\hline \\hline\nLabial & 98.2 & 97.4 & Nasal & 99.0 & 98.8 \\\\\nDorsal & 97.3 & 96.3 & Stop & 97.6 & 97.0 \\\\\nCoronal & 95.9 & 94.7 & Approximant & 98.2 & 97.6 \\\\\nAlveolar & 98.9 & 98.4 & Anterior & 95.4 & 94.2 \\\\\nPostalveolar & 99.7 & 99.5 & Back & 98.0 & 97.1 \\\\\nHigh & 97.0 & 95.9 & Lennis & 98.0 & 97.4 \\\\\nLow & 97.4 & 96.5 & Fortis & 97.5 & 96.8 \\\\\nMid & 96.9 & 96.2 & Round & 97.3 & 96.6 \\\\\nUvular & 98.7 & 98.1 & Unround & 95.9 & 95.1 \\\\\nVelar & 99.2 & 98.8 & Voiced & 95.4 & 94.3 \\\\\nVowel & 94.3 & 93.1 & Central & 98.5 & 98.1 \\\\\nFricative & 97.1 & 96.1 & Silence & 97.8 & 97.4 \\\\\n\\hline\n\\end{tabu}}\n\\end{table}}\n\n\n\n\n\\subsection{Linguistic Parsing}\n\nIn this section, we present the evaluation results of our proposed\nmethod of top-down linguistic parsing. We provide empirical results on\nsparsity of phonological posteriors and confirm validity of\nclass-specific codebooks to classify supra-segmental linguistic events\nbased on binary pattern matching.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Binary Sparsity of Phonological Posteriors}\n\n\\label{sec:binary}\nFigure \\ref{fig:binary} illustrates a histogram of phonological\nposteriors distribution. We can see that the distribution exhibits the\nbinary nature of phonological posterior being valued in the range of\n$[0-1]$, and mostly concentrated very close to either 1 or 0. This\nbinary pattern is visible for both stressed and unstressed syllables\nas demonstrated in the right and left plots, respectively.\n\n\\begin{figure}[h]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{fig/binary0-crop}\n  \\caption{Unstressed Syllables}\n  \\label{fig:unstressed}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.97\\linewidth]{fig/binary2-crop}\n  \\caption{Stressed Syllables}\n  \\label{fig:stressed}\n\\end{subfigure}\n\\caption{{\\it Binary sparsity of continuous phonological posteriors $Z$. We can observe that at least the [low] (6th), [round] (9th) and [rising] (10th) classes are significantly more present in stressed binary-ones than in unstressed syllables.}}\n\\label{fig:binary}\n\\end{figure}\n\nThe 1-bit discretization, achieved by rounding of posteriors results\ninto a very small number of unique phonological binary structures,\ncounting merely 0.1\\% of all possible structures. This imply that the\nbinary patterns may encode particular shapes of the vocal tract. Since\na limited number of these shapes can be created for human speech, the\nnumber of unique patterns is very small. \n\nThis property encouraged us also to use this binary approximation in\nlow bit-rate speech coding~\\citep{Cernak15icassp,Asaei15compr}; these studies confirmed that binary approximation has only a negligible\nimpact on perceptual speech quality.\n\n\n\n\n\nFurthermore, comparing Figures~\\ref{fig:unstressed} and\n\\ref{fig:stressed}, we can observe that at least the [low] (6th), [round] (9th) and [rising] (10th) classes are significantly more present\nin stressed binary-ones than in unstressed syllables. This observation indicates\nthat stressed syllables are more prominent in prosodic typology~(e.g.,\n\\citep{Jun05}) -- mouth are more open. We use the [rising] feature to\ndifferentiate diphthongs from monophthongs, that is also more\nprominent in stressed syllables.\n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Class-specific Linguistic Structures}\\label{sec:class-specific}\n\nThe objective of this section is to confirm the hypothesis that phonological posteriors admit class-specific structures which can be used for identification of supra-segmental linguistic events. \n\nFollowing the procedure of codebook construction elaborated in\nSection~\\ref{sec:codebook}, we obtain six different codebooks to\naddress the following parsing scenarios:\n\\begin{itemize}\\itemsep0em\n \\item Consonant vs. vowel (C-V) detection.\n \\item Stress vs. unstressed detection.\n \\item Accented vs. unaccented detection. \n\\end{itemize}\n\nThe size of codebooks equals to the number of unique binary\nclass-specific structures. The number of unique structures is indeed a\nsmall fraction of the whole speech data. For example, the\nratio of unique binary structures for the whole Nancy database (16.6\nhours of speech) is about 0.08\\% of the total number of phonological\nposteriors.\n\nThe detection method relies on binary pattern matching and the codebook with a member which possesses maximum similarity to the phonological posterior determines its supra-segmental linguistic property, i.e. being a consonant or vowel, stressed or unstressed and accented or unaccented. The three parsing scenarios are tested separately so the linguistic parsing amounts to a binary classification problem. \n\nWe process each speech segments independently. To obtain a decision for the supra-segmental events from the segmental labels, the labels of all the segments comprising a supra-segmental event are pulled to form a decision based on majority counting. In other words, the number of segments being recognized as a particular event is counted, and the final supra-segmental label is decided according to the maximum count. If the similarities of a binary phonological posterior to both codebooks are equal, the segment is not labeled, thus excluded from counting. Since we devise a top-down parsing mechanism, we use the knowledge of supra-segmental boundaries to determine the underlying linguistic event. \n\nTo perform pattern matching, the similarity measure of binary structures must be quantified. There are many metrics formulated for this purpose~\\citep{Choi10asurvey} which differ mainly in the way that positive/negative match or different mismatches are addressed. We conducted thorough tests on the metrics defined in~\\citep{Choi10asurvey}; Figure~\\ref{fig:bin-sim} compares and contrasts a few representative results. \n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=1\\linewidth]{fig/Compare_Similarity-Bar}\n  \\caption{{\\it Comparison of the performance of accent detection using various binary similarity measures. The measures are selected from~\\citep{Choi10asurvey}. The results of Jaccard~\\eqref{eq:jac} is the same as innerproduct.}}\n  \\label{fig:bin-sim}\n\\end{figure}\n\nWe can see that the fast and simple \\emph{innerproduct} is the most effective similarity metric; it quantifies the positive and negative matches between the two binary structures. On the other hand, Hamming similarity measure that quantifies the mismatches does not perform well for linguistic parsing. The Jaccard~\\eqref{eq:jac} formula yields similar results to innerproduct. Hence, we choose the innerproduct for its efficiency in our linguistic parsing evaluation. Table~\\ref{tab-nancy} lists the accuracy of different parsing scenarios for English data provided in recordings from Nancy and French data available in SIWIS database. \n\n\n{\\small\n\\begin{table}[h]\n\\centering\n\\caption{\\it Accuracy (\\%) of linguistic parsing using structured sparsity pattern matching with different context sizes. The results are evaluated on Nancy and SIWIS speech recordings. The binary similarity measure is innerproduct.}\n\\label{tab-nancy}\n\\begin{tabular}{|c|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{2}{|c|}{Task \\hspace{+0.5mm} / \\hspace{+0.5mm} Context Size}        & 0    & 1    & 2    & 4    & 6    \\\\ \\hline \\hline\n\\multirow{2}{*}{C-V Detection}    & Nancy & 53.5 & 83.3 & 88.2 & 93.9 & 96.7 \\\\ \\cline{2-7} \n                                  & SIWIS & 64.5 & 82.9 & 85.5 & 87.9 & 90.3 \\\\ \\hline \\hline\n\\multirow{2}{*}{Stress Detection} & Nancy & 75.4 & 95.4 & 96.9 & 99.5 & 99.5 \\\\ \\cline{2-7} \n                                  & SIWIS & 96.9 & 98.5 & 98.5 & 98.5 & 98.5 \\\\ \\hline \\hline\n\\multirow{2}{*}{Accent Detection} & Nancy & 78.4 & 96.8 & 97.3 & 98.4 & 99.5 \\\\ \\cline{2-7} \n                                  & SIWIS & 91.6 & 93.7 & 93.7 & 94.8 & 94.8 \\\\ \\hline\n\\end{tabular}\n\\end{table}\n}\n\n\nThe results are averaged over 5-fold random selection of length 1000 consecutive segments. The high-order structured sparsity patterns are obtained by concatenating each segment with its adjacent segments on the right, and the context size denotes the number of extra segments concatenated. We can see that the higher order structured sparsity patterns enables more accurate linguistic parsing. It also confirms that the proposed structured sparsity principle is independent of language as well as phonological class definitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Dependency of Linguistic Events}\\label{sec:dependency}\n\nFinally, we test  the dependency between different supra-segmental attributes captured in codebook structures. Both stressed and accented syllables convey similar information on linguistic emphasis, the former denotes it at a lexical level while the latter designates it at a prosodic level. Hence, we hypothesize that the codebook constructed from stressed structures can be used for accent detection, and vice versa. Table~\\ref{tab:cross} lists the accuracies using these linguistically relevant codebooks. \n\n{\\small\n\\begin{table}[h]\n\\centering\n\\caption{\\it Accuracy (\\%) of parsing using linguistically relevant codebooks. Namely, we perform stress / accent detection using accent/ stress codebooks to study dependency of stressed and accented structures. } \n\\label{tab:cross}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\nTask \\hspace{+0.5mm} / \\hspace{+0.5mm} Context Size & 0    & 1    & 2    & 4    & 6    \\\\ \\hline\\hline\nStress detection using accent codebooks       & 63.6 & 82.0 & 79.5 & 82.2 & 85.4 \\\\ \\hline\nAccent detection using stress codebooks      & 65.4 & 80.0 & 79.5 & 82.2 & 85.4 \\\\ \\hline\n\n\n\n\n\\end{tabular}\n\\end{table}\n}\n\n{\\color{ib}We can see that a codebook constructed from either of stress/accent structures can be used for detection of the other with high accuracy. This study confirms the hypothesis that codebooks encapsulates linguistically relevant structures and demonstrates that accented structures are indeed highly correlated with the stressed structures.}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Concluding Remarks}\n\\label{sec:concl}\nThe theories of linguistics and cognitive neuroscience suggest that\nthe phonological representation of speech places at the heart of speech\ntemporal organization. We devised a methodology to\nquantify the phonological based supra-segmental primitives as essential building blocks\nfor detection of various linguistic events. Our proposed approach\nrelies on identification of structured sparsity patterns to learn\nclass-specific codebooks characterizing different supra-segmental\nattributes. The experiments confirmed that indeed phonological\nposteriors convey supra-segmental information which is encoded in\ntheir support of active components, and these structures can be\nused as indicators of their higher level linguistic attributes.\n\nIn this context, we also verified that the class-specific structures\nof phonological posteriors is a property independent of language as well as\ndefinition of different phonological classes. In addition, it is\nrobust to unconstrained and noisy recording conditions. Furthermore,\nthe dependency of different linguistic properties such as stress and\naccent is captured in their codebooks which confirm the\nhigh correlation between their underlying structures. \n\nThis work quantified the supra-segmental events through the binary\nrepresentation of posteriors. This quantification can be more accurate\nif multi-level discretization is considered to find a compromise between\nspeaker and environmental variability encoded in the probabilities and\nthe actual contribution of phonological classes.\n\nIn our future work, we plan to investigate more closely the relationship\nof the trajectories of the articulatory-bound phonological posterior features\nto the task dynamic model of inter-articulator coordination in\nspeech~\\citep{Saltzman89}. This study will strengthen our knowledge about\ninterpretation of phonological posteriors, when applied to different\nspeech processing tasks. Applications include detection of syllable boundaries\nand subsequent bottom-up linguistic parsing (i.e., parsing without\nproviding the segment boundaries as discussed by\n\\cite{Ghitza11,Giraud12}), as well as phonetic posterior estimation for automatic\nspeech recognition and synthesis systems, parametric speech coding,\nand automatic assessment of speech production.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Acknowledgment}\nAfsaneh Asaei is supported by funding from SNSF project on ``Parsimonious Hierarchical Automatic Speech Recognition (PHASER)'' grant agreement number 200021-153507.\n\n\n\n\\section{References}\n\\label{sec:ref}\n\n\n\n\n\\bibliographystyle{elsarticle/elsarticle-harv}\n\\bibliography{refs}\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 24381, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\\title{On Structured Sparsity of Phonological Posteriors\\\\ for Linguistic Parsing} \n\n\n\\author[label1]{Milos Cernak\\corref{cor1}}\n\\ead{milos.cernak@idiap.ch}\n\\author[label1]{Afsaneh Asaei\\corref{cor1}}\n\\ead{afsaneh.asaei@idiap.ch}\n\\author[label1,label2]{Herv\\'e Bourlard}\n\\ead{herve.bourlard@idiap.ch}\n\n\\address[label1]{Idiap Research Institute, Martigny, Switzerland}\n\\address[label2]{\\'Ecole Polytechnique F\\'ed\\'erale de Lausanne (EPFL), Switzerland}\n\\cortext[cor1]{Corresponding authors; both authors contributed equally to this manuscript.}\n\n\\begin{abstract}\n\nThe speech signal conveys information on different time scales from\nshort (20--40 ms) time scale or segmental, associated to\nphonological and phonetic information to long (150--250 ms) time scale\nor supra segmental, associated to syllabic and prosodic\ninformation. Linguistic and neurocognitive studies recognize the\n\\emph{phonological} classes at segmental level as the essential and\ninvariant representations used in speech temporal organization. \n\nIn the context of speech processing, a deep neural network (DNN) is an\neffective computational method to infer the probability of\nindividual phonological classes from a short segment of speech signal. A\nvector of all phonological class probabilities is referred to as\n\\emph{phonological posterior}. There are only very few classes\ncomprising a short term speech signal; hence, the phonological posterior is a\nsparse vector. Although the phonological posteriors are estimated at\nsegmental level, we claim that they convey supra-segmental\ninformation. Namely, we demonstrate that phonological posteriors are\nindicative of syllabic and prosodic events.\n\nBuilding on findings from converging linguistic evidence on the\ngestural model of Articulatory Phonology as well as neural\nbasis of speech perception, we hypothesize that phonological\nposteriors convey properties of linguistic classes at multiple time\nscales, and this information is embedded in their support (index) of\nactive coefficients. To verify this hypothesis, we obtain a binary\nrepresentation of phonological posteriors at segmental level which is\nreferred to as first-order sparsity structure; the high-order structures\nare obtained by concatenation of first-order binary vectors. It is\nthen confirmed that classification of supra-segmental linguistic\nevents, the problem known as \\emph{linguistic parsing}, can be\nachieved with high accuracy using a simple binary pattern matching of first-order or high-order structures. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{abstract}\n\n\\begin{keyword}\nPhonological posteriors \\sep Structured sparse representation \\sep Deep neural network (DNN) \\sep Binary pattern matching \\sep Linguistic parsing.\n\n\\end{keyword}\n\\end{frontmatter}\n\n\\section{Introduction}\n\n\n\n\n\n\nA theory of Articulatory Phonology \\citep{Browman86} suggests that an\nutterance is described by temporally overlapped (co-articulated)\ndistinctive constriction actions of the vocal tract organs, actions\nknown as gestures. Gestures are changes in the vocal tract, such as\nopening and closing, widening and narrowing, and they are phonetic in\nnature \\citep{Fowler15}. Gestures compose units of information\n\nand can be used to distinguish words in all languages. Recent work\non Articulatory Phonology \\citep{Goldstein03} further suggests an\nexistence of coupling/synchronisation of gestures that influence the\nsyllable structure of an utterance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhonological classes (e.g., \\citep{Jakobson56,chomsky68sound}) emerge\nduring the phonological encoding process \n-- the processes of speech planning for articulation, namely the\npreparation of an abstract phonological code and its transformation\ninto speech motor plans that guide articulation~\\citep{Levelt93}. \\cite{Stevens08} reviews evidence about a universal\nset of phonological classes that consists of articulator-bound\nclasses and articulator-free classes ([continuant], [sonorant],\n[strident]). We follow the Stevens's view and consider phonological \nclasses in our work as essential and invariant acoustic-phonetic\nelements used in both linguistics and cognitive neuroscience studies\nfor speech temporal organization. \n\n\n\n\n\n\n\n\n\n\n\n\nIn the present paper, we study inferred phonological posterior features that consist of phonological class probabilities given a segment of input speech signal. The class-conditional posterior probabilities are estimated using a Deep Neural Network (DNN). \\cite{Cernak15icassp} introduce the phonological posterior features for phonological analysis and synthesis, and we hypothesise their relation to the\nlinguistic gestural model. \\cite{Saltzman89} describe the constriction\ndynamics model as computational system that incorporates Articulatory\nPhonology approach. This gestural model defines gestural scores as the\ntemporal activation of each gesture in an utterance. Thus, we\nhypothesise relation of the gestural scores to phonological\nposteriors, and that the trajectories of phonological posteriors\ncorrespond to the distal representation of articulatory gestures. \n{\\color{ir}In a broader view, we consider the trajectories of phonological\nposteriors as articulatory-bound and articulatory-free gestures. Since gestures are linguistically relevant} \\citep{Liberman00}, we hypothesize that phonological posteriors should convey supra-segmental information through their inter-dependency low-dimensional structures. Hence, by characterizing the structure of phonological posteriors, it should be possible to perform a top-down linguistic parsing, i.e., by knowing \\textit{a priori} where linguistic boundaries lie.\n\nPreviously in~\\citep{Asaei15compr}, we have shown that phonological posteriors admit sparsity structures underlying short-term segmental representations where the structures are quantified as sparse binary vectors. In this work, we explore this idea further and consider trajectories of phonological posteriors for supra-segmental structures. We show that unique structures (codes) exists for distinct linguistic classes and identification of these structures enables us to perform linguistic parsing. The linguistic parsing is thus achieved through identification of low dimensional sparsity structures of phonological posteriors followed by binary pattern matching. This idea is in line with an assumption that physical and cognitive speech structures are, in fact, the low and high dimensional descriptions of a single (complex) system\\footnote{http://www.haskins.yale.edu/research/gestural.html}.\n\n\nOur contribution to advance the study of phonological posteriors is\ntwo-fold: First, we review converging evidence from linguistic and neural basis of speech {\\color{ib}perception}, that support the\nhypothesis about phonological posteriors conveying properties of linguistic classes at multiple time scales. Second, we propose \nlinguistic parsing based on structured sparsity as low dimensional \ncharacterization of phonological posteriors.\n\n\n\n\n\nThe rest of the paper is organized as\nfollows. Section~\\ref{sec:posteriors} provides review about definition\nand relation of phonological posteriors to the linguistic gestural\nmodel and subsequently to cognitive neuroscience,\nSection~\\ref{sec:linguistic} introduces linguistic parsing, and\nSection~\\ref{sec:exp} presents the details of experimental analysis. Finally,\nSection~\\ref{sec:concl} concludes the paper and discusses the results in a\nbroader cross-field context.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Phonological Class-conditional Posteriors}\n\\label{sec:posteriors}\nFigure~\\ref{fig:phonovocTRN} shows a process of the phonological analysis \\citep{Yu2012,Cernak15icassp}. The phonological posterior features\nare extracted by phonological analysis that starts by converting a segment of speech samples into a sequence of acoustic features $X=\\{\\vec{x}_1,\\ldots,\\vec{x}_n,\\ldots,\\vec{x}_N\\}$ where $N$ denotes the number of segments in the utterance. Conventional cepstral coefficients can be used as acoustic features. Then, a bank of phonological class analysers realised via neural network classifiers converts the acoustic feature observation  sequence $X$ into a sequence of phonological posterior probabilities $Z=\\{\\vec{z}_1,\\ldots,\\vec{z}_n,\\ldots,\\vec{z}_N\\}$; a posterior\nprobability $\\vec{z}_n=[p(c_1|x_n),\\hdots,p(c_k|x_n),\\hdots,p(c_K|x_n)]^\\top$ consists of $K$ phonological class-conditional posterior probabilities where $c_k$ denotes the phonological class and $.^\\top$ stands for the transpose operator.\n\n\\begin{figure}[!h]\n\\centering\n\\resizebox{3.5in}{!}{\n\\begin{tikzpicture}[font=\\scriptsize]\n  \\draw[rounded corners, thick] (0,1.28) rectangle (1,2.23);\n  \\node[align=center] at (0.5,1.75) {Speech\\\\Signal};\n\n  \\draw[rounded corners, thick] (1.60,1.15) rectangle (3,2.35);\n  \\node[align=center] at (2.3,1.75) {Acoustic\\\\Feature\\\\Extraction};\n  \\draw[thick, ->] (1,1.75) -- (1.60,1.75);\n \n \n  \\draw[thick, ->] (3,1.75) -- (4,1.75);\n  \n  \\draw[rounded corners, thick, fill=lightgray] (4,2.5) rectangle (6,3);\n  \\node[align=center] at (5,2.75) {$c_1:\\,$ Anterior};\n\n  \\draw[thick, <->] (4,2.75) -- (3.5,2.75) -- (3.5,0.25) -- (4,0.25);\n  \\draw[rounded corners, thick, fill=lightgray] (4,1.5) rectangle (6,2);\n  \\node[align=center] at (5,1.75) {$c_k:\\,$Coronal};\n  \\draw[thick] (6,2.75) -- (6.5,2.75) -- (6.5,0.25) -- (6,0.25);\n\n  \\draw[rounded corners, thick, fill=lightgray] (4,0) rectangle (6,0.5);\n  \\node[align=center] at (5,0.25) {$c_K:\\,$Strident};\n  \\draw[thick, fill=black] (4.75,1) circle[radius=0.04];\n  \\draw[thick, fill=black] (5,1) circle[radius=0.04];\n  \\draw[thick, fill=black] (5.25,1) circle[radius=0.04];\n  \n  \n\n  \\draw[thick, ->] (6,1.75) -- (7.5,1.75);\n  \n  \\node[above] at (7,1.75)  {$\\vec{z}_n$};\n\\end{tikzpicture}\n}\n\\caption{\\emph{The process of phonological analysis. Each segment of speech signal is represented by phonological posterior probabilities\n  $\\vec{z}_n$ that consist of $K$ class-conditional posterior probabilities. For each phonological class, a DNN is trained to estimate its posterior probability given the input acoustic features.}}\n\\label{fig:phonovocTRN}\n\\end{figure}\n\nThe phonological posteriors $Z$ yield a parametric speech representation, and \nwe hypothesise that the trajectories of the articulatory-bound\nphonological posteriors correspond to the distal representation of the\ngestures in the gestural model of speech production (and\nperception). For example, Figure~\\ref{fig:tt} shows a comparison of\narticulatory tongue tip gestures (vertical direction with respect to\nthe occlusal plane) and the phonological anterior posterior features,\non an EMA recording~\\citep{lee05}. The articulatory gesture and\nphonological posteriors trajectory have the same number of maximums,\nand their relation is evident. \n\nThe hypothesis of correspondence of the phonological posterior features to the gestural trajectories is also motivated by analogy to the constriction dynamics model \\citep{Saltzman89} that takes gestural scores at input and generates articulator trajectories and acoustic output. Alternatively to this constriction dynamics model, we generate acoustic output by a phonological synthesis DNN~\\citep{Cernak15icassp}.\n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=.97\\linewidth]{fig/tt-crop.pdf} \n  \\caption{{\\it Anterior phonological posteriors vs. the\n      electromagnetic articulography tongue tip measurement. {\\color{ib}The correlation between the articulatory gesture score and the trajectory of its corresponding phonological class posterior probability is evident.}}}\n\\label{fig:tt}\n\\end{figure}\n\n\n\n\n\nIn the following sections, we outline converging evidence from linguistics as\nwell as neural basis of speech {\\color{ib}perception}, that support the\nhypothesis  about phonological posteriors conveying properties of\nlinguistic classes at multiple time scales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Linguistic Evidence}\n\\label{sec:lingevidence}\n\n\n\n\n\n\n\n\n\n\n\n\nLinguistics defines two traditional components of speech structures:\n\\begin{enumerate}\n\\item Cognitive structure consisting of system primitives, that is,\n  the units of representation for cognitively relevant objects such\n  as phonemes or syllables. The system primitives are represented by \\emph{canonical} phonological features (classes) that emerge during the phonological encoding process~\\citep{Levelt93}. \n\\item Physical structure generated by a set of permissible operations over cognitive system primitives that yield the observed (surface) patterns. The physical structure is represented by \\emph{surface} phonological features, continuous variables that may be partially estimated from the speech signal by inverse filtering. Phonological posteriors can be also classified as surface phonological features.\n  \n  \n  \n\\end{enumerate}\n\n\n\n\n\nThe canonical (discrete) phonological features have been used over the\nlast 60 years to describe cognitive structures of speech \nsounds. \\cite{Miller55} have experimentally shown that consonant\nconfusions were perceived similarly from the observed and binary\nconfusion values (a consonant present or not in a group of\nconsonants). Canonical features are extensively studied in\nphonology. In the tradition of~\\cite{Jakobson56}\nand~\\cite{chomsky68sound}, phonemes are assumed to consist of feature\nbundles -- the Sound Pattern of English (SPE). Later advanced\nphonological systems were proposed,  such as multi-valued phonological\nfeatures of \\cite{Ladefoged14}, and monovalent Government Phonology\nfeatures of \\cite{Harris95} that describe sounds by fusing and\nsplitting of primes.\n\n\n\n\n\nThe surface code includes co-articulated canonical code, with further\nintrinsic (speaker-based) and extrinsic (channel-based) speech\nvariabilities that contribute to the opacity of the function operating\nbetween the two codes. The surface features may contain additional\ngestures dependent on the prosodic context, such as position within a\nsyllable, word, and sentence. Other changes in surface phonological\nfeatures at different time granularities are due to phonotactic\nconstraints. For example, glides are always syllable-initial, and\nconsonants that follow a non-tense vowel are always in the coda of the\nsyllable~\\citep{Stevens08}.\n\n\\cite{Browman86,Browman89,Browman92} introduced articulatory gestures\nas basis for human speech production. The trajectories of gestures\ncontain overlapping, interleaving, and merging segments, as a result\nof co-articulation. It is said that gestures are phonetic in\nnature \\citep{Fowler15}. \n\n\n\n\n\n\n\\cite{Browman88,Nam09} provide direct evidence on existence of\nsupra-segmental (syllable) structures in gestures. For example,\nthe task dynamic model of inter-articulator coordination in speech\n\\citep{Saltzman89} implements a syllable structure-based gesture\ncoupling model.\n\\newline\n\n\n\n\n\n\\subsection{Cognitive Neuroscience Evidence}\n\n\n\n\n\n\n\n\n\n\nModern cognitive neuroscience studies use phonological classes as\nessential and invariant acoustic-phonetic primitives for speech\ntemporal organization \\citep{Poeppel14}. Neurological data from the\nbrain activity during speech planning, production or perception are\nincreasingly used to inform such cognitive models of speech and\nlanguage. \n\nThe auditory pre-processing is done in the cochlea,\nand then split into two parallel pathways\nleading from the auditory system \\citep{Wernicke1874}. For example,\nthe dual-stream model of the functional anatomy of language \n\\citep{Hickok07} consists of a ventral stream: sound to meaning\nfunction using phonological classes, phonological-level processing\nat superior temporal sulcus bilaterally, and a dorsal stream: sound\nto action, a direct link between sensory and motor representations of\nspeech based again on the phonological classes. The\nformer stream supports the speech perception, and the latter stream\nreflects the observed disruptive effects of altered auditory feedback\non speech production. \\cite{Phillips00,Mesgarani2014} present evidence\nof discrete phonological classes available in the human auditory\ncortex.\n\n\n\n\n\nRecent evidence from psychoacoustics and neuroimaging studies indicate\nthat auditory cortex segregates information emerging from the cochlea\non at least three discrete time-scales processed in the auditory\ncortical hierarchy: (1) ``stress'' $\\delta$ frequency (1--3 Hz), (2)\n``syllabic'' $\\theta$ frequency (4--8 Hz) and (3) ``phonetic'' low\n$\\gamma$ frequency (25--35 Hz) \\citep{Giraud12}. \\cite{Leong14jasa}\nshow that phase relations between the phonetic and syllabic amplitude\nmodulations, known as hierarchical phase locking and nesting or\nsynchronization across different temporal\ngranularity~\\citep{Lakatos05}, is a good indication of the syllable\nstress. Intelligible speech representation with stress and accent\ninformation can be constructed by asynchronous fusion of phonetic and\nsyllabic information~\\citep{Cernak15ieee}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition, not only phase locking across different temporal\ngranularity has linguistic interpretation. \\cite{Bouchard13} claim\nthat functional organisation of ventral sensorimotor cortex supports\ngestural model developed in Articulatory Phonology. Analysis of\nspatial patterns of activity showed a hierarchy of network states that\norganizes phonemes by articulatory-bound phonological\nfeatures. \\cite{Leonard2015} further show how listeners use\nphonotactic knowledge (phoneme sequence statistics) to process spoken\ninput and to link low-level acoustic representations (the\ncoarticulatory dynamics of the sounds through the encoding of\ncombination of phonological features) with linguistic information\nabout word identity and meaning.\nThis is converging evidence on the relation of linguistic\ngestural model and speech and language cognitive neuroscience models\non phonological class-conditional posteriors used in our work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Sparse Phonological Structures for Linguistic Parsing}\\label{sec:linguistic}\n\n{\\color{ib}Building on linguistic and cognitive findings}, the phonological representation of speech lies at the center of human speech processing. Speech analysis is performed at different time granularity broadly categorized as segmental and supra-segmental levels. The phonological classes define the sub-phonetic and phonetic attributes recognized at segmental level whereas the syllables, lexical stress and prosodic accent are the basic supra-segmental events - c.f. Figure~\\ref{fig:parsing}. The phonological representations are often studied at segmental level and their supra-segmental properties are not investigated. It is this supra-segmental characterization of phonological posteriors that this manuscript will explore. \n\n\n\n\n\n\\subsection{Structured Sparsity of Phonological Posteriors}\n\nPhonological posteriors are indicators of the physiological posture of human articulation machinery. Due to the physical constraints, only few\ncombinations can be realized in our vocalization. This physical limitation leads to a small number of unique patterns exhibited over the entire speech corpora~\\citep{Asaei15compr}. We refer to this structure as \\emph{first-order structure} which is exhibited at segmental level.\n \nMoreover, the dynamics of the structured sparsity patterns is slower than the short segments and it is indicative of supra-segmental information, leading to a higher order structure underlying a sequence (trajectory) of phonological posteriors. This structure is exhibited at supra-segmental level by analyzing a long duration of phonological posteriors, and it is associated to the syllabic information or more abstract linguistic attributes. We refer to this structure as \\emph{high-order structure}. \n\nWe hypothesize that the first-order and high-order structures underlying phonological posteriors can be exploited as indicators of supra-segmental linguistic events. To test this hypothesis, we identify all structures exhibited in different linguistic classes. The set of class-specific structures is referred to as the codebook.  \n\n\\begin{figure}[t]\n\\centering\n  \\includegraphics[width=.7\\linewidth]{fig/parsing-crop.pdf} \n  \\caption{{\\it Different time granularity of speech processing. The phonological and phonetic classes are segmental attributes whereas the syllable type, stress and accent are linguistic events recognized at supra-segmental level. Inferring the supra-segmental attributes from sub-phonetic features is the task of linguistic parsing~\\citep{Poeppel03}.}}\n  \\label{fig:parsing}\n\\end{figure}\n\n\n\\subsection{Codebook of Linguistic Structures}\\label{sec:codebook}\n\nThe goal of codebook construction is to collect all the structures associated to a particular linguistic event. To that end, we consider \\emph{binary} phonological posteriors where the probabilities above 0.5 are normalized to 1 and the probabilities less than 0.5 are forced to zero. This rounding procedure enables us to identify the active phonological components as indicators of linguistic events. It also alleviates the speaker and environmental  variability encoded in the continuous probabilities. An immediate extension to this approach is multi-valued quantization of phonological posteriors as opposed to 1-bit quantization. We consider this extension for our future studies and focus on binary phonological indicators to obtain linguistic structures. \n\nDifferent codebooks are constructed for different classes. Namely, one codebook encapsulates all the binary structures of the consonants whereas another codebook has all the binary structures of the vowels. These two codebooks will be used for binary pattern matching to classify consonants versus vowels as will be explained in the next Section~\\ref{sec:match}. Likewise, one codebook encapsulates all the binary structures of stressed syllables whereas another codebook has all the binary structures of unstressed syllables, and these two codebooks are used for stress detection; the similar procedure holds for accent detection. \n\nThe codebook can be constructed from the first-order structures as well as the high-order structures. For example, a second-order codebook is formed from all the binary structures of second-order phonological posteriors obtained by concatenation of two adjacent phonological posteriors to form a super vector from the segmental representations. \n\nThe procedure of codebook construction for classification of linguistic events rely on the assumption that there are unique structures per class (consonant, stressed or syllable) and the number of permissible patterns is small. Hence, classification of any phonological posterior can be performed by finding the closest match to its binary structure from the codebooks characterizing different linguistic classes. \n\n\n\\subsection{Pattern Matching for Linguistic Parsing}\\label{sec:match}\n\nFigure~\\ref{fig:parsing} illustrates different time granularity identified for processing of speech. Inferring the supra-segmental properties such as syllable type or accented / stressed pronunciation is known as linguistic parsing~\\citep{Poeppel03}. Parsing can be performed in a top-down procedure, driven by a-priori known segment boundaries. \n\n\n\n\n\n\n\n\nHaving the codebooks of structures underlying phonological posteriors, linguistic parsing amounts to binary pattern matching. The similarity metric plays a critical role in classification accuracy. Hence, we investigate several metrics found effective in different binary classification settings. The definition of binary similarity measures are expressed by \\emph{operational taxonomic units}~\\citep{Taxonomy}. Consider two binary vectors $i,\\, j$: $a$ denotes the number of elements where the values of both $i,\\,j$ are 1, meaning \\emph{``positive match''}; $b$ denotes the number of elements where the values of $i,\\;j$ is $(0,1)$, meaning \\emph{``$i$ absence mismatch''}; $c$ denotes the number of elements where the values of $i,\\;j$ is $(1,0)$, meaning \\emph{``$j$ absence mismatch''}; $d$ denotes the number of elements where the values of both $i,\\,j$ are 0, meaning \\emph{``negative match''}. \nThe definition of binary similarity measures used for our evaluation of linguistic parsing is as follows~\\citep{Choi10asurvey}:\n\n\n", "index": 1, "text": "\\begin{align}\n &S_{\\text{JACCARD}}=\\frac{a}{a+b+c}\\\\ \\label{eq:jac}\n &S_{\\text{INNERPRODUCT}}=a+d\\\\\n &S_{\\text{HAMMING}}=b+c\\\\\n &S_{\\text{AMPLE}}=\\frac{a(c+d)}{c(a+b)}\\\\\n &S_{\\text{SIMPSON}}=\\frac{a}{\\text{min}(a+b,a+c)}\\\\\n &S_{\\text{HELLINGER}}=2\\sqrt{1-\\frac{a}{\\sqrt{(a+b)(a+c)}}}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{JACCARD}}=\\frac{a}{a+b+c}\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>JACCARD</mtext></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mi>a</mi><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{INNERPRODUCT}}=a+d\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>INNERPRODUCT</mtext></msub><mo>=</mo><mrow><mi>a</mi><mo>+</mo><mi>d</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{HAMMING}}=b+c\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>HAMMING</mtext></msub><mo>=</mo><mrow><mi>b</mi><mo>+</mo><mi>c</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{AMPLE}}=\\frac{a(c+d)}{c(a+b)}\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>AMPLE</mtext></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{SIMPSON}}=\\frac{a}{\\text{min}(a+b,a+c)}\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>SIMPSON</mtext></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mi>a</mi><mrow><mtext>min</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mo>,</mo><mrow><mi>a</mi><mo>+</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\text{HELLINGER}}=2\\sqrt{1-\\frac{a}{\\sqrt{(a+b)(a+c)}}}\" display=\"inline\"><mrow><msub><mi>S</mi><mtext>HELLINGER</mtext></msub><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><msqrt><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi>a</mi><msqrt><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>+</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mstyle></mrow></msqrt></mrow></mrow></math>", "type": "latex"}]