[{"file": "1601.06403.tex", "nexttext": "\n\\noindent where $\\mathbf{A_B}$ is the channel gain matrix that also carries the sign information vector $\\mathbf{B}$, and $\\mathbf{Z}\\sim N(0,\\Sigma_{\\mathbf{z}})$ is the additive noise vector, with a diagonal covariance matrix $\\Sigma_{\\mathbf{z}}$, where the diagonal entries $\\sigma^2_{z_i}$ are the variances of $Z_i$.\n\n\n\\subsection{Studying the properties of sign information vector $\\mathbf{B}$}\n\n\n\nConsider the channel shown in Figure \\ref{fig:broadcast}.\nGiven enough samples from each of the outputs $X_1$, $X_2$, and $X_3$, one can estimate the pairwise correlations $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$ and use existing learning algorithms such as RG \\cite{mit} to solve the corresponding signal model in \\eqref{eq:linear_regression} and to recover the values corresponding to correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$. \nHence, we can completely characterize the entries in the matrix $\\mathbf{A_B}$ (up to sign), and the variances regarding additive Gaussian noise variables in $\\mathbf{Z}=\\{Z_1,Z_2,Z_3\\}$.\nHowever, one can only partially infer the sign, only by observing the sign values corresponding to each $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$. \nIn particular, such approach leads us into two equivalent sign variables $\\mathbf{b}$ and $\\mathbf{-b}$, with the latter obtained by flipping the signs of all correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$.\nFrom \\cite{correlation}, we know for the channel shown in Figure \\ref{fig:broadcast}, we should have $\\rho_{x_1x_2}\\rho_{x_1x_3}\\rho_{x_2x_3}>0$. Hence, there are totally two cases for $\\rho_{x_ix_j},~i\\neq j,~i,j\\in\\{1,2,3\\}$ based on such constraint; either all of them are positive, or two of them are negative and the third one is positive.\nAs a result, one can classify the sign singularity for the broadcast channel shown in Figure \\ref{fig:broadcast} into four groups, each consisting of two instances corresponding to $\\mathbf{b}$ or $\\mathbf{-b}$. \nFor example, suppose we are given enough samples to infer the latent structure shown in Figure \\ref{fig:broadcast}, in which all the pairwise correlations $\\rho_{x_1x_2}$, $\\rho_{x_1x_3}$, and $\\rho_{x_2x_3}$ are derived as positive values. However, we cannot further decide on whether all pairwise correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$ are positive, or all of them are negative, i.e., the ambiguity to choose between $b^{(1)}$ or $-b^{(1)}$.\nFigure \\ref{fig:sign} shows each group consisting of two Gaussian trees, in which the inferred correlation signs for $\\rho_{yx_i},~i\\in\\{1,2,3\\}$ are based on the signs of $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$.\n\n\\begin{figure} [h!]\n\n\\centering \n\\includegraphics[width=0.8\\columnwidth]{sign}\n\\caption{Sign singularity in the star Gaussian tree at each group\\label{fig:sign}} \n\n\\end{figure}\nThus by relying on previous learning approaches \\cite{correlation,mit} there is not enough information to distinguish $\\mathbf{b}$ from $\\mathbf{-b}$, hence such information is lost. \n\nIn Theorem \\ref{thm:B}, whose proof can be found in Appendix \\ref{app:B}, we characterize the size and dependency relations of sign vectors for any minimal latent Gaussian tree.\n\n\\begin{thm} \\label{thm:B}\n{\\it\n$(1)$ The correlation values $\\rho_{yx_i}$ in regard to the outputs $X_i$ that are connected to a single input, say $Y$, share an equivalent sign class, i.e., they either all belong to $B=b$ or $B=-b$.\n\n$(2)$ Given the cardinality of input vector $\\mathbf{Y}=\\{Y_1,Y_2,...,Y_k\\}$ is $k$, then there are totally $2^k$ minimal Gaussian trees with isomorphic structures, but with different correlation signs that induce the same joint density of the outputs, i.e., equal $p_{\\mathbf{X}}(\\mathbf{x})$.\n}\n\\end{thm}\n\nFor example, in a Gaussian tree shown in Figure \\ref{fig:broadcast} there is only one hidden node $Y^{(1)}$, and we already know by previous discussions that there are two latent Gaussian trees with different sign values for $B^{(1)}$, which induce the same output joint density $p_{\\mathbf{X}}(\\mathbf{x})$.\n\n\n\n\nOne may anticipate that there are exactly $k$ Bernoulli variables $B_1,B_2,...,B_k$ needed to represent all of the $2^k$ Gaussian trees. However, we show by the following two examples that this is not the case, since these Bernoulli variables are correlated, hence we need more binary variables to represent all such Gaussian trees. Figure \\ref{fig:B} shows two cases.\n\n\\begin{figure}[h!]\n   \n    \\begin{subfigure}[h]{\\columnwidth}\n    \\centering\n        \\includegraphics[width=0.35\\columnwidth]{B1}\n        \\caption{}\n        \\label{fig:B1}\n    \\end{subfigure}\n    \n    ~ \n      \n    \\begin{subfigure}[h]{\\columnwidth}\n    \\centering\n        \\includegraphics[width=0.6\\columnwidth]{B2}\n        \\caption{}\n        \\label{fig:B2}\n    \\end{subfigure}\n    \\caption{Two possible cases to demonstrate the dependency relations of sign variables: (a) with two hidden inputs, and (b) with $4$ hidden inputs at two layers}\\label{fig:B}\n\\end{figure}\nIn a Gaussian tree shown in Figure \\ref{fig:B1} there are two hidden nodes $Y_1$ and $Y_2$. By Theorem \\ref{thm:B}, we know that there are $4$ Gaussian trees with sign ambiguity. \nAlso, from the first part in Theorem \\ref{thm:B} we may introduce $B^{(1)}_1$ to capture the correlation signs $\\rho_{x_1y_1}$ and $\\rho_{x_2y_1}$, and $B^{(1)}_2$ for the correlation signs $\\rho_{x_3y_2}$ and $\\rho_{x_4y_2}$.\nWe introduce $B_{12}$ as the sign of $\\rho_{y_1y_2}$.\nNote that the link between the variables $Y_1$ and $Y_2$ are in both groups with common correlation sign, so we anticipate that $B_{12}$ should be dependent on both $B^{(1)}_1$ and $B^{(1)}_2$.\n\nSince we need to maintain the correlation signs regarding to $\\rho_{x_ix_j},~i\\in\\{1,2\\},~j\\in\\{3,4\\}$, hence the product $B^{(1)}_1B^{(1)}_2B_{12}$ should maintain its sign. \nThus, we have $B_{12}=B^{(1)}_1B^{(1)}_2$, so $B_{12}$ is completely determined given $B^{(1)}_1$ and $B^{(1)}_2$. \nNext, consider the Gaussian tree shown in Figure \\ref{fig:B2}, in which there are six hidden inputs. \nSimilar to the previous case, we can show that $B^{(1)}_1B^{(1)}_2=B^{(2)}_1B^{(2)}_2$, $B^{(1)}_3B^{(1)}_4=B^{(2)}_3B^{(2)}_4$, and $B_{12}=B^{(1)}_1B^{(2)}_1B^{(2)}_4B^{(1)}_4$. Since, there are nine sign values and three equality constraints, hence we have six free binary variables to represent all equivalent Gaussian trees in this case.\n\n\n\n\\section{Main Results} \\label{sec:main}\n\\subsection{Maximum achievable rate region to generate the output $\\mathbf{X}$}\nIn \\cite{wyner} a \\textit{common information} of variables in $\\mathbf{X}$ is defined to be the minimum rate among all possible sources, through which we can generate the outputs $\\mathbf{X}$ with joint density $\\hat{p}_{\\mathbf{X}}(\\mathbf{x})$ that is asymptotically close (measured by KL-distance) to the true joint density $p_{\\mathbf{X}}(\\mathbf{x})$.\n\n\nLet us define $\\mathbf{\\tilde{Y}}=\\{\\mathbf{Y},\\mathbf{B}\\}$, then the formalized problem has the following form:\n\n\n", "itemtype": "equation", "pos": 11525, "prevtext": "\n\n\n\\newtheorem{thm}{Theorem}\n\\newtheorem{prop}{Proposition}\n\\newtheorem{lem}{Lemma}\n\\newtheorem{defn}{Definition}\n\\newtheorem{ex}{Example}\n\\newtheorem{cor}{Corollary}\n\\newtheorem{prn}{Principle}\n\\newtheorem{case}{Case}\n\\newtheorem{rmk}{Remark}\n\n\n\n\\title{Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach}\n\n\\author{Ali Moharrer,\nShuangqing Wei,\nGeorge T. Amariucai, \nand Jing Deng}\n\\maketitle\n\\footnotetext[1]{A. Moharrer, and S. Wei are with the school of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA 70803, USA (Email: amohar2@lsu.edu, swei@lsu.edu). \n\nG. T. Amariucai is with the department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA (Email: gamari@iastate.edu). \n\nJ. Deng is with the department of Computer Science, University of North Carolina at Greensboro, Greensboro, NC, USA (Email: jing.deng@uncg.edu).\n\nThis material is based upon work supported in part by the National Science\nFoundation under Grant No. 1320351.}\n\n\n\n\n\n\n\n\\begin{abstract}\nLatent Gaussian tree model learning algorithms lack in fully recovering the sign information regarding pairwise correlation values between variables.\nSuch information is vital since it completely determines the direction in which two variables are associated.\nIn this work, we resort to information theoretical approaches to quantify information in regard to lost correlation signs in the recovered model. \n\nWe model the graphical model as a communication channel and propose a new layered encoding framework to synthesize observed data using top layer Gaussian inputs and independent Bernoulli correlation sign inputs from each layer. \nWe show that by maximizing the inferred information about the correlation sign information, one may also find the largest achievable rate region for the rate tuples of multi-layer latent Gaussian messages and missing correlation sign messages to synthesize the desired observables.\n\n\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\n\n\n\n\nIn a latent graphical model inference problem, we are only observing a subset of random variables while our ultimate goal is to recover all \\textit{hidden} (latent) variables with their associated parameters, namely the graph's topology and all the pairwise covariance values.\nLet $\\mathbf{X}=\\{X_1,X_2,...,X_n\\}$ be the $n$ observed variables, while the set of variables $\\mathbf{Y}=\\{Y_1,Y_2,...,Y_k\\}$ are hidden to us. Our goal is to recover the hidden parameters related to those $k$ hidden nodes ($k$ may be unknown).\nHere, we study a special class of graphical models, known as latent \\textit{Gaussian trees}, in which the underlying structure is a tree and the joint density of the variables is captured by a Gaussian density.\nThe Gaussian graphical models are widely studied in the literature because of a direct correspondence between conditional independence relations occurring in the model with zeros happening in the inverse of covariance matrix, known as the \\textit{concentration matrix}.\n\n\nHere, we assume any variable in a given Gaussian tree has zero mean with normalized variance.\n\nThere are several works such as \\cite{mit,nj} that have proposed efficient algorithms to infer the latent Gaussian tree parameters. In fact, Choi et al., proposed a new \\textit{recursive grouping} (RG) algorithm along with its improved version, i.e., \\textit{Chow-Liu} RG (CLRG) algorithm to recover a latent Gaussian tree that is both \\textit{structural} and \\textit{risk} consistent \\cite{mit}, hence it recovers the \\textit{correct} value for the latent parameters.\nThey introduced a \\textit{tree metric} as the negative \\textit{log} of the absolute value of pairwise correlations to perform the algorithm.\nAlso, Shiers et al., in \\cite{correlation} characterized the correlation space of latent Gaussian trees and showed the necessary and sufficient conditions under which the correlation space represents a particular latent Gaussian tree.\nWe noticed that the RG algorithm can be directly related to correlation space of latent Gaussian trees, in a sense that it recursively checks certain constraints on correlations to converge to a latent tree with true parameters.\n\nThese methods have been shown successful in estimating latent parameters in terms of both computational complexity and consistency. However, they all lack in recovering the correct correlation signs (sign of edge-weights) from the observed data. \nFor some of them such as RG algorithm, this is due to the fact that it considers the absolute values of correlation as the input to the algorithm, which automatically results in losing the sign information.\n\nIn general, the sign ambiguity is not restricted only to those models that take absolute values of correlations and it has a much deeper correspondence with all types of latent Gaussian trees.\nA regular (non-singular) model is defined to be both \\textit{identifiable} and have a \\textit{positive definite metric} \\cite[p. 10]{watanabe}. \n\n\nIt is shown that the identifiable and positive definite metric models are not always equivalent \\cite{watanabe}.\nIt turns out that the lack of recovered sign information using aforementioned algorithms in the recovered structure, is due to the fact that Gaussian trees in general might be non-identifiable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure} [h!]\n\n\\centering \n\\includegraphics[width=0.25\\columnwidth]{broadcast}\n\\caption{A simple Gaussian tree with a hidden node $Y^{(1)}$\\label{fig:broadcast}} \n\n\\end{figure}\n\n\n\nTo clarify our approach, let's consider the Gaussian tree shown in Figure \\ref{fig:broadcast}.\nWe may think of this Gaussian tree as a communication channel, where information flows from a Gaussian source $Y^{(1)}\\sim N(0,1)$ through three communication channels $p_{X_i|Y^{(1)}}(x_i|y^{(1)})$ with independent additive Gaussian noises $Z_i\\sim N(0,\\sigma^2_{z_i}),~i\\in\\{1,2,3\\}$ to generate (dependent) outputs with $\\mathbf{X}\\sim N(0,\\Sigma_{\\mathbf{x}})$.\nWe introduce $B^{(1)}\\in\\{-1,1\\}$ as a binary Bernoulli random variable as another input to the channel, which reflects the sign information of pairwise correlations. \nDefine $\\rho_{x_iy}=E[X_iY]$ as true correlation values between the input and each of the three output.\nFor the channel in Figure \\ref{fig:broadcast}, one may assume that $B^{(1)}=1$ to show the case with $\\rho'_{x_iy}=\\rho_{x_iy}$, while $B^{(1)}=-1$ captures $\\rho''_{x_iy}=-\\rho_{x_iy}$, where $\\rho'_{x_iy}$ and $\\rho''_{x_iy},~i\\in\\{1,2,3\\}$, are the recovered correlation values using certain inference algorithm such as RG \\cite{mit}.\nIt is easy to see that both recovered correlation values induce the same covariance matrix $\\Sigma_{\\mathbf{x}}$, showing the sign singularity in such latent Gaussian tree.\n\n\nCuff, in \\cite{cuff} introduced a memoryless channel synthesis model through which the output is generated by certain encoding/decoding on channel inputs.\nSimilarly, in this paper we are concerned about obtaining achievable rates through which the Gaussian output can be synthesized.\nMore specifically, our goal is to characterize the achievable rate region to propose an encoding scheme to synthesize Gaussian output with density $q_\\mathbf{X}(\\mathbf{x})$ using only Gaussian inputs and through a channel with additive Gaussian noises, where the synthesized joint density $q_\\mathbf{X}(\\mathbf{x})$ is indistinguishable from the true output density $p_\\mathbf{X}(\\mathbf{x})$ as measured by \\textit{total variation} metric \\cite{cuff}.\n\n\n\nIn particular, we find a solution for  $\\inf_{\\tilde{\\mathbf{Y}}} I(\\mathbf{X};\\tilde{\\mathbf{Y}})$, where $I(\\mathbf{X};\\tilde{\\mathbf{Y}})$ is the mutual information between the output $\\mathbf{X}$ and the input vector $\\tilde{\\mathbf{Y}}=\\{\\mathbf{Y},\\mathbf{B}\\}$.\nThis corresponds to finding the minimum achievable rate to synthesize the Gaussian output.\nWe show that such quantity is only a function of output joint density, hence, given output it cannot be further optimized.\nHowever, we show that to obtain the maximum rate region to synthesize the output, one may minimize $I(\\mathbf{X};\\mathbf{Y})$, which in turn will be equivalent to maximizing the conditional mutual information $I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$, hence, maximizing our knowledge about the sign information.\nIn such settings, we show that the input $\\mathbf{B}$ and the output $\\mathbf{X}$ are independent, by which we provide a reason on why previous learning approaches \\cite{mit,correlation} are incapable of inferring the sign information.\nOn the other hand, we show the correlation between two sources $\\mathbf{Y}$ and $\\mathbf{B}$, providing a framework to capture the lost sign information.\n\n\n\n\n\n\n\n\n\nThe rest of the paper is organized as follows. Section \\ref{sec:formulation}\ngives the problem formulation and models the sign singularity problem.\nMain results of the paper regarding to achievable rate region are discussed in Section \\ref{sec:main}.\nWe conclude the paper in Section \\ref{sec:conclusion}.\n\n\\section{Problem Formulation} \\label{sec:formulation}\n\n\\subsection{The signal model of a latent Gaussian tree}\nHere, we suppose a latent graphical model, with $\\mathbf{Y}=\\{Y_1,Y_2,...,Y_k\\}$ as the set of hidden variables, and $\\mathbf{X}=\\{X_1,X_2,...,X_n\\}$ as the set of observed variables.\nWe also assume that the underlying network structure is a \\textit{minimal} latent Gaussian tree \\cite{mit}, therefore, making the joint probability $P(X,Y)$ to follow a Gaussian joint density $N(\\mathbf{\\mu},\\Sigma_{\\mathbf{xy}})$, where the covariance matrix $\\Sigma_{\\mathbf{xy}}$ induces tree structure $G_T(V,E,W)$, where $V$ is the set of nodes consisting of both vectors $\\mathbf{X}$ and $\\mathbf{Y}$; $E$ is the set of edges; and $W$ is the set of edge-weights determining the pairwise covariances between any adjacent nodes.\nAlso, in a minimal Gaussian tree we assume all the hidden variables to have at least three neighbors \\cite{mit}, which results in ignoring all those singular cases where there can be arbitrarily redundant hidden variables added to the model without changing the observed joint density $p_{\\mathbf{X}}(\\mathbf{x})$.\nWe consider normalized variances for all variables $X_i\\in \\mathbf{X},~i\\in\\{1,2,...,n\\}$ and $Y_j\\in\\mathbf{Y},~j\\in\\{1,2,...,k\\}$. Such constraints do not affect the channel structure, and hence the independence relations captured by $\\Sigma_{\\mathbf{xy}}$.\nWithout loss of generality, we also assume $\\mathbf{\\mu}=\\mathbf{0}$, since we can always shift the non-zero mean outputs to make their mean zero. This constraint does not change the amount of information carried by the observed vector. \n\n\n\\begin{figure} [h!]\n\n\\centering \n\\includegraphics[scale=0.5]{channel2}\n\\caption{A communication system with both sign and latent variables vectors as an input\\label{fig:channel2}} \n\n\\end{figure}\n\nTo model the lost sign information we model our problem as shown in Figure \\ref{fig:channel2}. \nIn fact, we introduce a vector $\\mathbf{B}=\\{B_1,...,B_m\\}$, with each $B_i\\in\\{-1,1\\}$ being a binary Bernoulli random variable with parameter $\\pi_i=p(B_i=1)$ as another input to the channel, which captures the sign information of pairwise correlations.\nAssume $\\mathbf{Y}$ and $\\mathbf{B}$ as the input vectors, $\\mathbf{X}$ as the output vector, and the noisy channel to be characterized by the conditional probability distribution $P_{\\mathbf{X}|\\mathbf{Y},\\mathbf{B}}(\\mathbf{x}|\\mathbf{y},\\mathbf{b})$, the signal model for such a channel can be written as follows,\n\n", "index": 1, "text": "\\begin{align} \\label{eq:linear_regression}\n\\mathbf{X}=\\mathbf{A_BY}+\\mathbf{Z}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{X}=\\mathbf{A_{B}Y}+\\mathbf{Z}\" display=\"inline\"><mrow><mi>\ud835\udc17</mi><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc00</mi><mi>\ud835\udc01</mi></msub><mo>\u2062</mo><mi>\ud835\udc18</mi></mrow><mo>+</mo><mi>\ud835\udc19</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\nNote that all of the mutual information values should be evaluated under a given Gaussian tree $G_T(V,E,W)$. \nHowever, for simplicity we drop this notation in their expressions.\n\n\nIn this setting, we show in Theorem \\ref{thm:fixed}, whose proof can be found in Appendix \\ref{app:fixed} we show that there is no room to minimize $I(\\mathbf{X};\\tilde{\\mathbf{Y}})$.\n\n\\begin{thm} \\label{thm:fixed}\n{\\it\nGiven $p_{\\mathbf{X}}(x)\\sim~N(0,\\Sigma_\\mathbf{x})$ and the settings in \\eqref{eq:common_info}, the mutual information $I(\\mathbf{X};\\mathbf{\\tilde{Y}})$ is only a function of $\\Sigma_\\mathbf{x}$ and is given by,\n\n", "itemtype": "equation", "pos": 18511, "prevtext": "\n\\noindent where $\\mathbf{A_B}$ is the channel gain matrix that also carries the sign information vector $\\mathbf{B}$, and $\\mathbf{Z}\\sim N(0,\\Sigma_{\\mathbf{z}})$ is the additive noise vector, with a diagonal covariance matrix $\\Sigma_{\\mathbf{z}}$, where the diagonal entries $\\sigma^2_{z_i}$ are the variances of $Z_i$.\n\n\n\\subsection{Studying the properties of sign information vector $\\mathbf{B}$}\n\n\n\nConsider the channel shown in Figure \\ref{fig:broadcast}.\nGiven enough samples from each of the outputs $X_1$, $X_2$, and $X_3$, one can estimate the pairwise correlations $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$ and use existing learning algorithms such as RG \\cite{mit} to solve the corresponding signal model in \\eqref{eq:linear_regression} and to recover the values corresponding to correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$. \nHence, we can completely characterize the entries in the matrix $\\mathbf{A_B}$ (up to sign), and the variances regarding additive Gaussian noise variables in $\\mathbf{Z}=\\{Z_1,Z_2,Z_3\\}$.\nHowever, one can only partially infer the sign, only by observing the sign values corresponding to each $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$. \nIn particular, such approach leads us into two equivalent sign variables $\\mathbf{b}$ and $\\mathbf{-b}$, with the latter obtained by flipping the signs of all correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$.\nFrom \\cite{correlation}, we know for the channel shown in Figure \\ref{fig:broadcast}, we should have $\\rho_{x_1x_2}\\rho_{x_1x_3}\\rho_{x_2x_3}>0$. Hence, there are totally two cases for $\\rho_{x_ix_j},~i\\neq j,~i,j\\in\\{1,2,3\\}$ based on such constraint; either all of them are positive, or two of them are negative and the third one is positive.\nAs a result, one can classify the sign singularity for the broadcast channel shown in Figure \\ref{fig:broadcast} into four groups, each consisting of two instances corresponding to $\\mathbf{b}$ or $\\mathbf{-b}$. \nFor example, suppose we are given enough samples to infer the latent structure shown in Figure \\ref{fig:broadcast}, in which all the pairwise correlations $\\rho_{x_1x_2}$, $\\rho_{x_1x_3}$, and $\\rho_{x_2x_3}$ are derived as positive values. However, we cannot further decide on whether all pairwise correlations $\\rho_{yx_i},~i\\in\\{1,2,3\\}$ are positive, or all of them are negative, i.e., the ambiguity to choose between $b^{(1)}$ or $-b^{(1)}$.\nFigure \\ref{fig:sign} shows each group consisting of two Gaussian trees, in which the inferred correlation signs for $\\rho_{yx_i},~i\\in\\{1,2,3\\}$ are based on the signs of $\\rho_{x_ix_j},~i,j\\in\\{1,2,3\\}$.\n\n\\begin{figure} [h!]\n\n\\centering \n\\includegraphics[width=0.8\\columnwidth]{sign}\n\\caption{Sign singularity in the star Gaussian tree at each group\\label{fig:sign}} \n\n\\end{figure}\nThus by relying on previous learning approaches \\cite{correlation,mit} there is not enough information to distinguish $\\mathbf{b}$ from $\\mathbf{-b}$, hence such information is lost. \n\nIn Theorem \\ref{thm:B}, whose proof can be found in Appendix \\ref{app:B}, we characterize the size and dependency relations of sign vectors for any minimal latent Gaussian tree.\n\n\\begin{thm} \\label{thm:B}\n{\\it\n$(1)$ The correlation values $\\rho_{yx_i}$ in regard to the outputs $X_i$ that are connected to a single input, say $Y$, share an equivalent sign class, i.e., they either all belong to $B=b$ or $B=-b$.\n\n$(2)$ Given the cardinality of input vector $\\mathbf{Y}=\\{Y_1,Y_2,...,Y_k\\}$ is $k$, then there are totally $2^k$ minimal Gaussian trees with isomorphic structures, but with different correlation signs that induce the same joint density of the outputs, i.e., equal $p_{\\mathbf{X}}(\\mathbf{x})$.\n}\n\\end{thm}\n\nFor example, in a Gaussian tree shown in Figure \\ref{fig:broadcast} there is only one hidden node $Y^{(1)}$, and we already know by previous discussions that there are two latent Gaussian trees with different sign values for $B^{(1)}$, which induce the same output joint density $p_{\\mathbf{X}}(\\mathbf{x})$.\n\n\n\n\nOne may anticipate that there are exactly $k$ Bernoulli variables $B_1,B_2,...,B_k$ needed to represent all of the $2^k$ Gaussian trees. However, we show by the following two examples that this is not the case, since these Bernoulli variables are correlated, hence we need more binary variables to represent all such Gaussian trees. Figure \\ref{fig:B} shows two cases.\n\n\\begin{figure}[h!]\n   \n    \\begin{subfigure}[h]{\\columnwidth}\n    \\centering\n        \\includegraphics[width=0.35\\columnwidth]{B1}\n        \\caption{}\n        \\label{fig:B1}\n    \\end{subfigure}\n    \n    ~ \n      \n    \\begin{subfigure}[h]{\\columnwidth}\n    \\centering\n        \\includegraphics[width=0.6\\columnwidth]{B2}\n        \\caption{}\n        \\label{fig:B2}\n    \\end{subfigure}\n    \\caption{Two possible cases to demonstrate the dependency relations of sign variables: (a) with two hidden inputs, and (b) with $4$ hidden inputs at two layers}\\label{fig:B}\n\\end{figure}\nIn a Gaussian tree shown in Figure \\ref{fig:B1} there are two hidden nodes $Y_1$ and $Y_2$. By Theorem \\ref{thm:B}, we know that there are $4$ Gaussian trees with sign ambiguity. \nAlso, from the first part in Theorem \\ref{thm:B} we may introduce $B^{(1)}_1$ to capture the correlation signs $\\rho_{x_1y_1}$ and $\\rho_{x_2y_1}$, and $B^{(1)}_2$ for the correlation signs $\\rho_{x_3y_2}$ and $\\rho_{x_4y_2}$.\nWe introduce $B_{12}$ as the sign of $\\rho_{y_1y_2}$.\nNote that the link between the variables $Y_1$ and $Y_2$ are in both groups with common correlation sign, so we anticipate that $B_{12}$ should be dependent on both $B^{(1)}_1$ and $B^{(1)}_2$.\n\nSince we need to maintain the correlation signs regarding to $\\rho_{x_ix_j},~i\\in\\{1,2\\},~j\\in\\{3,4\\}$, hence the product $B^{(1)}_1B^{(1)}_2B_{12}$ should maintain its sign. \nThus, we have $B_{12}=B^{(1)}_1B^{(1)}_2$, so $B_{12}$ is completely determined given $B^{(1)}_1$ and $B^{(1)}_2$. \nNext, consider the Gaussian tree shown in Figure \\ref{fig:B2}, in which there are six hidden inputs. \nSimilar to the previous case, we can show that $B^{(1)}_1B^{(1)}_2=B^{(2)}_1B^{(2)}_2$, $B^{(1)}_3B^{(1)}_4=B^{(2)}_3B^{(2)}_4$, and $B_{12}=B^{(1)}_1B^{(2)}_1B^{(2)}_4B^{(1)}_4$. Since, there are nine sign values and three equality constraints, hence we have six free binary variables to represent all equivalent Gaussian trees in this case.\n\n\n\n\\section{Main Results} \\label{sec:main}\n\\subsection{Maximum achievable rate region to generate the output $\\mathbf{X}$}\nIn \\cite{wyner} a \\textit{common information} of variables in $\\mathbf{X}$ is defined to be the minimum rate among all possible sources, through which we can generate the outputs $\\mathbf{X}$ with joint density $\\hat{p}_{\\mathbf{X}}(\\mathbf{x})$ that is asymptotically close (measured by KL-distance) to the true joint density $p_{\\mathbf{X}}(\\mathbf{x})$.\n\n\nLet us define $\\mathbf{\\tilde{Y}}=\\{\\mathbf{Y},\\mathbf{B}\\}$, then the formalized problem has the following form:\n\n\n", "index": 3, "text": "\\begin{align} \\label{eq:common_info}\nC(\\mathbf{X})&=\\inf_{\\mathbf{\\tilde{Y}}} I(\\mathbf{X};\\mathbf{\\tilde{Y}}),~s.t.,\\notag\\\\\n&p_{\\mathbf{X},\\mathbf{\\tilde{Y}}}(\\mathbf{x},\\mathbf{\\tilde{y}})~induces~a~minimal~Gaussian~tree\\notag\\\\\n&X_i\\perp X_j|\\mathbf{\\tilde{Y}}\\notag\\\\\n&\\Sigma_{\\tilde{y}\\in\\mathbf{\\tilde{Y}}} p(\\mathbf{x},\\mathbf{\\tilde{y}})=p_{\\mathbf{X}}(\\mathbf{x})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C(\\mathbf{X})\" display=\"inline\"><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\inf_{\\mathbf{\\tilde{Y}}}I(\\mathbf{X};\\mathbf{\\tilde{Y}}),~{}s.t.,\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">inf</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover></munder><mo>\u2061</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mi>s</mi></mrow></mrow><mo>.</mo><mi>t</mi></mrow><mo>.</mo><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle p_{\\mathbf{X},\\mathbf{\\tilde{Y}}}(\\mathbf{x},\\mathbf{\\tilde{y}})%&#10;~{}induces~{}a~{}minimal~{}Gaussian~{}tree\" display=\"inline\"><mrow><msub><mi>p</mi><mrow><mi>\ud835\udc17</mi><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">~</mo></mover><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>s</mi></mpadded><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>a</mi></mpadded><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>l</mi></mpadded><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mpadded width=\"+3.3pt\"><mi>n</mi></mpadded><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>e</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle X_{i}\\perp X_{j}|\\mathbf{\\tilde{Y}}\" display=\"inline\"><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u27c2</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Sigma_{\\tilde{y}\\in\\mathbf{\\tilde{Y}}}p(\\mathbf{x},\\mathbf{%&#10;\\tilde{y}})=p_{\\mathbf{X}}(\\mathbf{x})\" display=\"inline\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2208</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>p</mi><mi>\ud835\udc17</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\\noindent where $X_i$, $X_{j_i}$, $X_{k_i}$ are three distinct variables, which are connected to each other through $Y_{X_i}$, i.e., one of the hidden variables adjacent to $X_i$. \n\n}\n\\end{thm}\n\\begin{rmk}\nIntuitively, given $\\Sigma_x$ and any three outputs that have a common variable as their input, the correlation values between each output and the input is fixed, since varying one correlation results in varying the other correlations in the same direction, hence making the pairwise correlation between the other outputs change, which is impossible. \n\nAlso, as we may observe from Theorem \\ref{thm:fixed}, given $X_i$ we may end up with several options for $X_{j_i}$ and $X_{k_i}$.\nHowever, it can be shown that in a subspace of correlations corresponding to latent Gaussian trees \\cite{correlation}, all those distinct options result in a same value for the term $\\rho_{x_ix_{j_i}}\\rho_{x_ix_{k_i}}/\\rho_{x_{j_i}x_{k_i}}$.\n\n\n\\end{rmk}\n\\begin{rmk}\n\nWe can show that $I(\\mathbf{X};\\mathbf{B})=0$. \nThis is due to the fact that the input elements $Y_i\\in\\mathbf{Y}$ have zero means, which subsequently results in zero mean vector for the conditional density $p_{\\mathbf{X}|\\mathbf{B}}(\\mathbf{x}|\\mathbf{b})\\sim N(\\mathbf{0},\\Sigma_{\\mathbf{x}|\\mathbf{b}})$.\nBy varying $B_i\\in\\mathbf{B}$ we are just flipping the conditional density around the origin, which does not change corresponding conditional entropy $h(\\mathbf{X}|\\mathbf{B})$.\nFrom the equality $I(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{B})+I(\\mathbf{X};\\mathbf{Y}|\\mathbf{B})$ and above arguments, we know $I(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{Y}|\\mathbf{B})$. This, better explains the drawback of previous learning algorithms in neglecting the sign information carried by the sign vector $\\mathbf{B}$. In fact, by ignoring the sign information or taking the absolute value of pairwise correlations \\cite{mit}, they automatically assumed the sign information is given and aimed to infer the model.\n\n\\end{rmk}\n\\begin{rmk}\nOne may easily deduce the following,\n\n", "itemtype": "equation", "pos": 19512, "prevtext": "\n\nNote that all of the mutual information values should be evaluated under a given Gaussian tree $G_T(V,E,W)$. \nHowever, for simplicity we drop this notation in their expressions.\n\n\nIn this setting, we show in Theorem \\ref{thm:fixed}, whose proof can be found in Appendix \\ref{app:fixed} we show that there is no room to minimize $I(\\mathbf{X};\\tilde{\\mathbf{Y}})$.\n\n\\begin{thm} \\label{thm:fixed}\n{\\it\nGiven $p_{\\mathbf{X}}(x)\\sim~N(0,\\Sigma_\\mathbf{x})$ and the settings in \\eqref{eq:common_info}, the mutual information $I(\\mathbf{X};\\mathbf{\\tilde{Y}})$ is only a function of $\\Sigma_\\mathbf{x}$ and is given by,\n\n", "index": 5, "text": "\\begin{align}\nI(\\mathbf{X};\\mathbf{\\tilde{Y}})=\\dfrac{1}{2}\\log\\dfrac{|\\Sigma_\\mathbf{x}|}{\\prod_{i=1}^n \\dfrac{\\rho_{x_ix_{j_i}}\\rho_{x_ix_{k_i}}}{\\rho_{x_{j_i}x_{k_i}}}}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I(\\mathbf{X};\\mathbf{\\tilde{Y}})=\\dfrac{1}{2}\\log\\dfrac{|\\Sigma_%&#10;{\\mathbf{x}}|}{\\prod_{i=1}^{n}\\dfrac{\\rho_{x_{i}x_{j_{i}}}\\rho_{x_{i}x_{k_{i}}%&#10;}}{\\rho_{x_{j_{i}}x_{k_{i}}}}}\" display=\"inline\"><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>\ud835\udc31</mi></msub><mo stretchy=\"false\">|</mo></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c1</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>x</mi><msub><mi>j</mi><mi>i</mi></msub></msub></mrow></msub><mo>\u2062</mo><msub><mi>\u03c1</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>x</mi><msub><mi>k</mi><mi>i</mi></msub></msub></mrow></msub></mrow><msub><mi>\u03c1</mi><mrow><msub><mi>x</mi><msub><mi>j</mi><mi>i</mi></msub></msub><mo>\u2062</mo><msub><mi>x</mi><msub><mi>k</mi><mi>i</mi></msub></msub></mrow></msub></mfrac></mstyle></mrow></mfrac></mstyle></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\nThe result in Theorem \\ref{thm:fixed} combined with \\eqref{eq:equality}, suggests that by minimizing $I(\\mathbf{X};\\mathbf{Y})$, one may eventually maximize $I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$. \n\n, i.e., maximize the inferred knowledge about the sign information.\n\n\\end{rmk}\n\n\n\n\n\\subsection{Synthesis of the Gaussian output vector $\\mathbf{X}$ given $p_{\\mathbf{X}}(\\mathbf{x})$}\nIn this section we provide mathematical formulations to address the following fundamental problem: using channel inputs $\\mathbf{Y}$ and $\\mathbf{B}$, what are the rate conditions under which we can synthesize the Gaussian channel output $\\mathbf{X}$, with a given $p_{\\mathbf{X}}(\\mathbf{x})$.\nWe provide encoding scheme, as well as the corresponding bounds on achievable rate tuples.\n\nSuppose we transmit input messages through $N$ channel uses, in which $t\\in\\{1,2,...,N\\}$ shows the time index.\nWe define $\\vec{Y}^{(l)}_{t}[i]$ to be the $t$-th symbol of the $i$-th codeword, with $i\\in\\{1,2,...,M_{Y^{(l)}}\\}$ where $M_{Y^{(l)}}=2^{NR_{Y^{(l)}}}$ is the codebook cardinality, transmitted from the existing $k_l$ sources at layer $l$.\nHere, we define the source to be at layer $l$, if the shortest path from source to output passes through $l$ links.\nAlso, we assume there are $k_l$ sources $Y^{(l)}_j$ are present at the $l$-th layer, and the channel has $L$ layers.\nWe can similarly define $\\vec{B}^{(l)}_{t}[k]$ to be the $t$-th symbol of the $k$-th codeword, with $k\\in\\{1,2,...,M_{B^{(l)}}\\}$ where $M_{B^{(l)}}=2^{NR_{B^{(l)}}}$ is the codebook cardinality, transmitted from the existing $k_l$ sources at layer $l$.\n\nBy \\textit{soft covering} lemma \\cite[Lemma IV.1]{cuff}, we know that for \\textit{sufficiently} large rates $R_{\\mathbf{Y}}=[R_{Y^{(1)}},R_{Y^{(2)}},...,R_{Y^{(L)}}]$ and $R_{\\mathbf{B}}=[R_{B^{(1)}},R_{B^{(2)}},...,R_{B^{(L)}}]$ and as $N$ grows the output density of synthesized channel converges to $p_{\\mathbf{X}^N(\\mathbf{x}^N)}$, i.e., $N$ i.i.d realization of the given output density $p_{\\mathbf{X}}(\\mathbf{x})$, i.e., the average total variation between the two joint densities vanishes as $N$ grows,\n\n", "itemtype": "equation", "pos": 21751, "prevtext": "\n\\noindent where $X_i$, $X_{j_i}$, $X_{k_i}$ are three distinct variables, which are connected to each other through $Y_{X_i}$, i.e., one of the hidden variables adjacent to $X_i$. \n\n}\n\\end{thm}\n\\begin{rmk}\nIntuitively, given $\\Sigma_x$ and any three outputs that have a common variable as their input, the correlation values between each output and the input is fixed, since varying one correlation results in varying the other correlations in the same direction, hence making the pairwise correlation between the other outputs change, which is impossible. \n\nAlso, as we may observe from Theorem \\ref{thm:fixed}, given $X_i$ we may end up with several options for $X_{j_i}$ and $X_{k_i}$.\nHowever, it can be shown that in a subspace of correlations corresponding to latent Gaussian trees \\cite{correlation}, all those distinct options result in a same value for the term $\\rho_{x_ix_{j_i}}\\rho_{x_ix_{k_i}}/\\rho_{x_{j_i}x_{k_i}}$.\n\n\n\\end{rmk}\n\\begin{rmk}\n\nWe can show that $I(\\mathbf{X};\\mathbf{B})=0$. \nThis is due to the fact that the input elements $Y_i\\in\\mathbf{Y}$ have zero means, which subsequently results in zero mean vector for the conditional density $p_{\\mathbf{X}|\\mathbf{B}}(\\mathbf{x}|\\mathbf{b})\\sim N(\\mathbf{0},\\Sigma_{\\mathbf{x}|\\mathbf{b}})$.\nBy varying $B_i\\in\\mathbf{B}$ we are just flipping the conditional density around the origin, which does not change corresponding conditional entropy $h(\\mathbf{X}|\\mathbf{B})$.\nFrom the equality $I(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{B})+I(\\mathbf{X};\\mathbf{Y}|\\mathbf{B})$ and above arguments, we know $I(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{Y}|\\mathbf{B})$. This, better explains the drawback of previous learning algorithms in neglecting the sign information carried by the sign vector $\\mathbf{B}$. In fact, by ignoring the sign information or taking the absolute value of pairwise correlations \\cite{mit}, they automatically assumed the sign information is given and aimed to infer the model.\n\n\\end{rmk}\n\\begin{rmk}\nOne may easily deduce the following,\n\n", "index": 7, "text": "\\begin{align} \\label{eq:equality}\nI(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{Y},\\mathbf{B})=I(\\mathbf{X};\\mathbf{Y}) + I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I(\\mathbf{X};\\mathbf{\\tilde{Y}})=I(\\mathbf{X};\\mathbf{Y},\\mathbf%&#10;{B})=I(\\mathbf{X};\\mathbf{Y})+I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})\" display=\"inline\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mover accent=\"true\"><mi>\ud835\udc18</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mi>\ud835\udc18</mi><mo>,</mo><mi>\ud835\udc01</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mi>\ud835\udc18</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><mi>\ud835\udc01</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\\noindent where $q(\\mathbf{x}_1,...,\\mathbf{x}_N)$ is the synthesized channel output, and $E||.||_{TV}$, represents the average total variation. For example, for the channel shown in Figure \\ref{fig:broadcast} we may compute the synthesized output as,\n\n", "itemtype": "equation", "pos": 24049, "prevtext": "\nThe result in Theorem \\ref{thm:fixed} combined with \\eqref{eq:equality}, suggests that by minimizing $I(\\mathbf{X};\\mathbf{Y})$, one may eventually maximize $I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$. \n\n, i.e., maximize the inferred knowledge about the sign information.\n\n\\end{rmk}\n\n\n\n\n\\subsection{Synthesis of the Gaussian output vector $\\mathbf{X}$ given $p_{\\mathbf{X}}(\\mathbf{x})$}\nIn this section we provide mathematical formulations to address the following fundamental problem: using channel inputs $\\mathbf{Y}$ and $\\mathbf{B}$, what are the rate conditions under which we can synthesize the Gaussian channel output $\\mathbf{X}$, with a given $p_{\\mathbf{X}}(\\mathbf{x})$.\nWe provide encoding scheme, as well as the corresponding bounds on achievable rate tuples.\n\nSuppose we transmit input messages through $N$ channel uses, in which $t\\in\\{1,2,...,N\\}$ shows the time index.\nWe define $\\vec{Y}^{(l)}_{t}[i]$ to be the $t$-th symbol of the $i$-th codeword, with $i\\in\\{1,2,...,M_{Y^{(l)}}\\}$ where $M_{Y^{(l)}}=2^{NR_{Y^{(l)}}}$ is the codebook cardinality, transmitted from the existing $k_l$ sources at layer $l$.\nHere, we define the source to be at layer $l$, if the shortest path from source to output passes through $l$ links.\nAlso, we assume there are $k_l$ sources $Y^{(l)}_j$ are present at the $l$-th layer, and the channel has $L$ layers.\nWe can similarly define $\\vec{B}^{(l)}_{t}[k]$ to be the $t$-th symbol of the $k$-th codeword, with $k\\in\\{1,2,...,M_{B^{(l)}}\\}$ where $M_{B^{(l)}}=2^{NR_{B^{(l)}}}$ is the codebook cardinality, transmitted from the existing $k_l$ sources at layer $l$.\n\nBy \\textit{soft covering} lemma \\cite[Lemma IV.1]{cuff}, we know that for \\textit{sufficiently} large rates $R_{\\mathbf{Y}}=[R_{Y^{(1)}},R_{Y^{(2)}},...,R_{Y^{(L)}}]$ and $R_{\\mathbf{B}}=[R_{B^{(1)}},R_{B^{(2)}},...,R_{B^{(L)}}]$ and as $N$ grows the output density of synthesized channel converges to $p_{\\mathbf{X}^N(\\mathbf{x}^N)}$, i.e., $N$ i.i.d realization of the given output density $p_{\\mathbf{X}}(\\mathbf{x})$, i.e., the average total variation between the two joint densities vanishes as $N$ grows,\n\n", "index": 9, "text": "\\begin{align} \\label{eq:TV}\n\\lim_{N\\rightarrow\\infty} E||q(\\mathbf{x}_1,...,\\mathbf{x}_N)-\\prod_{t=1}^N p_{\\mathbf{X}_t}(\\mathbf{x}_t)||_{TV}\\rightarrow 0\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lim_{N\\rightarrow\\infty}E||q(\\mathbf{x}_{1},...,\\mathbf{x}_{N})-%&#10;\\prod_{t=1}^{N}p_{\\mathbf{X}_{t}}(\\mathbf{x}_{t})||_{TV}\\rightarrow 0\" display=\"inline\"><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>N</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><mi>E</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>p</mi><msub><mi>\ud835\udc17</mi><mi>t</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mrow><mi>T</mi><mo>\u2062</mo><mi>V</mi></mrow></msub></mrow></mrow><mo>\u2192</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\\noindent where $M_{\\mathbf{B}}=2^{NR_{\\mathbf{B}}}$ and $M_{\\mathbf{Y}}=2^{NR_{\\mathbf{Y}}}$ are the total number of input messages for sources $\\mathbf{B}$ and $\\mathbf{Y}$, respectively. Also, the distribution $p_{\\mathbf{X}|\\mathbf{Y},\\mathbf{B}}(\\mathbf{x}_t|\\mathbf{y}_t[i]\\mathbf{b}_t[k])$ represents each channel use $t$ for corresponding input messages, and can be computed via signal model in \\eqref{eq:linear_regression}. \n\n\nIn the following sections, we provide three case studies through which we obtain the achievable rate region to synthesize the output with joint density $p_{\\mathbf{X}}(\\mathbf{x})$. \nHere, for simplicity of notation, we drop the symbol index and use $Y^{(l)}_{t}$ and $B^{(l)}_{t}$ instead of $\\vec{Y}^{(l)}_{t}[i]$ and $\\vec{B}^{(l)}_{t}[k]$, respectively, since they can be understood from the context.\n\n\n\n\n\n\n\n\n\n\\subsubsection{Channel Synthesis for the Star Model}\n\n\nConsider the channel shown in Figure \\ref{fig:broadcast}. This can be modeled as \n\n", "itemtype": "equation", "pos": 24468, "prevtext": "\n\\noindent where $q(\\mathbf{x}_1,...,\\mathbf{x}_N)$ is the synthesized channel output, and $E||.||_{TV}$, represents the average total variation. For example, for the channel shown in Figure \\ref{fig:broadcast} we may compute the synthesized output as,\n\n", "index": 11, "text": "\\begin{align}\n&q(\\mathbf{x}_1,...,\\mathbf{x}_N)=\\notag\\\\\n&\\dfrac{1}{M_{\\mathbf{B}}}\\dfrac{1}{M_\\mathbf{Y}}\\sum_{i=1}^{M_\\mathbf{Y}}\\sum_{k=1}^{M_\\mathbf{B}}\\prod_{t=1}^N p_{\\mathbf{X}|\\mathbf{Y},\\mathbf{B}}(\\mathbf{x}_t|\\mathbf{y}_t[i]\\mathbf{b}_t[k])\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle q(\\mathbf{x}_{1},...,\\mathbf{x}_{N})=\" display=\"inline\"><mrow><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\dfrac{1}{M_{\\mathbf{B}}}\\dfrac{1}{M_{\\mathbf{Y}}}\\sum_{i=1}^{M_{%&#10;\\mathbf{Y}}}\\sum_{k=1}^{M_{\\mathbf{B}}}\\prod_{t=1}^{N}p_{\\mathbf{X}|\\mathbf{Y}%&#10;,\\mathbf{B}}(\\mathbf{x}_{t}|\\mathbf{y}_{t}[i]\\mathbf{b}_{t}[k])\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>M</mi><mi>\ud835\udc01</mi></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>M</mi><mi>\ud835\udc18</mi></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>\ud835\udc18</mi></msub></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>\ud835\udc01</mi></msub></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>p</mi><mrow><mi>\ud835\udc17</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo>,</mo><mi>\ud835\udc01</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc32</mi><mi>t</mi></msub><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo></mrow><msub><mi>\ud835\udc1b</mi><mi>t</mi></msub><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\n\n\n\nDue to soft covering principle, the following conditions should hold,\n\n", "itemtype": "equation", "pos": 25719, "prevtext": "\n\\noindent where $M_{\\mathbf{B}}=2^{NR_{\\mathbf{B}}}$ and $M_{\\mathbf{Y}}=2^{NR_{\\mathbf{Y}}}$ are the total number of input messages for sources $\\mathbf{B}$ and $\\mathbf{Y}$, respectively. Also, the distribution $p_{\\mathbf{X}|\\mathbf{Y},\\mathbf{B}}(\\mathbf{x}_t|\\mathbf{y}_t[i]\\mathbf{b}_t[k])$ represents each channel use $t$ for corresponding input messages, and can be computed via signal model in \\eqref{eq:linear_regression}. \n\n\nIn the following sections, we provide three case studies through which we obtain the achievable rate region to synthesize the output with joint density $p_{\\mathbf{X}}(\\mathbf{x})$. \nHere, for simplicity of notation, we drop the symbol index and use $Y^{(l)}_{t}$ and $B^{(l)}_{t}$ instead of $\\vec{Y}^{(l)}_{t}[i]$ and $\\vec{B}^{(l)}_{t}[k]$, respectively, since they can be understood from the context.\n\n\n\n\n\n\n\n\n\n\\subsubsection{Channel Synthesis for the Star Model}\n\n\nConsider the channel shown in Figure \\ref{fig:broadcast}. This can be modeled as \n\n", "index": 13, "text": "\\begin{align} \\label{eq:regression_star}\n\\begin{bmatrix}\nX_{1,t}\\\\\nX_{2,t}\\\\\nX_{3,t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha_1\\\\\n\\alpha_2\\\\\n\\alpha_3\n\\end{bmatrix}\nB^{(1)}_t\nY^{(1)}_t\n+\n\\begin{bmatrix}\nZ_{1,t}\\\\\nZ_{2,t}\\\\\nZ_{3,t}\n\\end{bmatrix}\n,~t\\in\\{1,2,...,N\\}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{bmatrix}X_{1,t}\\\\&#10;X_{2,t}\\\\&#10;X_{3,t}\\end{bmatrix}=\\begin{bmatrix}\\alpha_{1}\\\\&#10;\\alpha_{2}\\\\&#10;\\alpha_{3}\\end{bmatrix}B^{(1)}_{t}Y^{(1)}_{t}+\\begin{bmatrix}Z_{1,t}\\\\&#10;Z_{2,t}\\\\&#10;Z_{3,t}\\end{bmatrix},~{}t\\in\\{1,2,...,N\\}\" display=\"inline\"><mrow><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mrow><mn>2</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mrow><mn>3</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mn>3</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>B</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>Y</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>Z</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>Z</mi><mrow><mn>2</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>Z</mi><mrow><mn>3</mn><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\nNote that the sum of the rates $R_{Y^{(1)}}+R_{B^{(1)}}$ is lower bounded by $I(\\mathbf{X};Y^{(1)},B^{(1)})$, which by Theorem \\ref{thm:fixed} is fixed.\nHowever, the minimum rate for $R_{Y^{(1)}}$ is achieved by $min_{Y^{(1)}} I(\\mathbf{X};Y^{(1)})$.\n\n\nIn the following Theorem we prove that the optimal solution occurs when $B^{(1)}$ is uniformly distributed.\n\\begin{thm} \\label{thm:broadcast}\n{\\it\nThe optimal solution to the optimization problem $\\pi^*=arg\\max_{\\pi_1\\in [0,1]} I(\\mathbf{X};B^{(1)}|Y^{(1)})$ is $\\pi^*=1/2$.\n}\n\\end{thm}\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{proof}\nThe proof relies on the results shown in \\cite{capacity}. One can show that given $Y^{(1)}=y$, the broadcast model in \\eqref{eq:regression_star} becomes a bipolar signaling scheme with $\\mathbf{S}_1=[\\alpha_1y,\\alpha_2y,\\alpha_3y]'$ and $\\mathbf{S_2}=-\\mathbf{S_1}$.\nNow, simply by putting $\\mathbf{T}=-\\mathbf{I}_3$, where $\\mathbf{T}$ is an orthonormal matrix and using $\\mathbf{S_2}=\\mathbf{TS_1}$, one can map the signals to each other.\nAlso, we may normalize the noise variances to satisfy all the constraints shown in \\cite{capacity}. This can be simply done by introducing a $3\\times 3$ diagonal matrix $\\mathbf{M}$ with $m_{ii}=1/\\sigma_{z_i}$, through which the new signal model becomes $\\mathbf{X}'=\\mathbf{MX}=\\mathbf{MS_i}+\\mathbf{MZ},~i\\in\\{1,2\\}$, where $Z'_i\\in\\mathbf{Z}'=\\mathbf{MZ}$ are noises with unit variance.\nSince such model is \\textit{circular symmetric} \\cite{capacity}, hence, $\\pi^*=1/2$.\n\\end{proof}\n\nThe results in Theorem \\ref{thm:broadcast} can be further generalized to star topologies with arbitrary number of outputs.\n\n\\begin{cor} \\label{cor:general}\n{\\it\nAssuming a broadcast channel with single input $Y$ and output vector $\\mathbf{X}=\\{X_1,X_2,...,X_n\\},~n\\geq 3$, the optimal solution is at $\\pi^*=1/2$.\n}\n\\end{cor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Channel Synthesis with $4$ outputs and $2$ inputs}\n\n\nConsider the channel shown in Figure \\ref{fig:B1}. In this case, we are given two hidden inputs $Y_1^{(1)}$ and $Y_2^{(1)}$, and by previous arguments we know $\\mathbf{B}^{(1)}=\\{B_1^{(1)},B_2^{(1)},B_{12}\\}$ with $B_{12}=B_1^{(1)}B_2^{(1)}$, completely determined by $B_1^{(1)}$ and $B_2^{(1)}$, which may act independently. \n\nWe may write,\n\n", "itemtype": "equation", "pos": 26067, "prevtext": "\n\n\n\n\nDue to soft covering principle, the following conditions should hold,\n\n", "index": 15, "text": "\\begin{align} \\label{eq:conditions}\nR_{Y^{(1)}}+R_{B^{(1)}}\\geq I(\\mathbf{X};Y^{(1)},B^{(1)})\\notag\\\\\nR_{Y^{(1)}}\\geq I(\\mathbf{X};Y^{(1)})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R_{Y^{(1)}}+R_{B^{(1)}}\\geq I(\\mathbf{X};Y^{(1)},B^{(1)})\" display=\"inline\"><mrow><mrow><msub><mi>R</mi><msup><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo>+</mo><msub><mi>R</mi><msup><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></msub></mrow><mo>\u2265</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><msup><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R_{Y^{(1)}}\\geq I(\\mathbf{X};Y^{(1)})\" display=\"inline\"><mrow><msub><mi>R</mi><msup><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo>\u2265</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>;</mo><msup><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\\noindent where for $j\\in\\{1,2\\}$, $(k,l)\\in\\{(1,2),(3,4)\\}$, which shows each group of outputs corresponding to each of the inputs.\n\nHere, two inputs $Y_1^{(1)}$ and $Y_2^{(1)}$ are dependent and their pairwise correlation can be computed via $E[Y_1^{(1)}Y_2^{(1)}|\\mathbf{B}^{(1)}]=\\gamma_{12}B_{12}=\\gamma_{12}B_1^{(1)}B_2^{(1)}$, in which $\\gamma_{12}$ determines the degree of correlation and is learned by certain inference algorithms, e.g., RG or CLRG \\cite{mit}.\nNote that the dependency relation of symbols $Y_{1,t}^{(1)}$ and $Y_{2,t}^{(1)}$ follows a Gaussian mixture model, since their covariance is a function of binary inputs $B_{1,t}^{(1)}$ and $B_{2,t}^{(1)}$.\nBut, note that in a given codebook consisting of $M_{\\mathbf{Y}^{(1)}}\\times M_{\\mathbf{B}^{(1)}}$ codewords, for each realization of $\\mathbf{b}_{3,t}^{(1)}=\\mathbf{b}_{1,t}^{(1)}.\\mathbf{b}_{2,t}^{(1)}$ the joint density of $\\mathbf{Y}_{t}^{(1)}$ is Gaussian. \nHence, one may divide the codebook $\\mathbb{C}$ into two parts $\\mathbb{S}_i,~i\\in\\{1,2\\}$, in which each part follows a specific Gaussian density with covariance values $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{b}^{(1)}]=\\gamma_{12}b_{1,t}^{(1)}b_{2,t}^{(1)}$.\nNote that such sub-block encoding guarantees the independence between the synthesized output vector and the sign input vector $\\mathbf{B}$.\nThe achievable region can be obtained from \\eqref{eq:conditions}, and by replacing $Y^{(1)}$ with $\\{Y^{(1)}_1,Y^{(1)}_2\\}$ and $B^{(1)}$ with $\\{B^{(1)}_1,B^{(1)}_2\\}$.\n \nIn the following Lemma, whose proof can be found in Appendix \\ref{app:optimal_dumbbell} we showed that the optimal solution $(\\pi^*_1,\\pi^*_2)$ to $arg\\max_{\\pi_1,\\pi_2} I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$ is at $(1/2,1/2)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lem} \\label{lem:optimal_dumbbell}\n{\\it\nFor the channel shown in Figure \\ref{fig:B1}, we have,\n\n$(1)$ $I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})=I(X_1,X_2;B_1^{(1)}|Y_1^{(1)}Y_2^{(1)})+I(X_3,X_4;B_2^{(1)}|Y_1^{(1)}Y_2^{(1)})$.\n\n$(2)$ The optimal solution for the maximization problem $arg\\max_{\\pi_1,\\pi_2} I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$ happens at $\\pi^*_1=\\pi^*_2=1/2$.\n}\n\\end{lem}\n\nIntuitively, for the first part in Lemma \\ref{lem:optimal_dumbbell}, we may divide the structure shown in Figure \\ref{fig:B1} into two substructures each similar to the star topology shown in Figure \\ref{fig:broadcast} (but with only two outputs). Hence, we may use the results in Theorem \\ref{thm:broadcast} to prove the second part of the lemma.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Multi-Layered Channel Synthesis}\n\nHere, we address those channels with multi-layer inputs. Figure \\ref{fig:B2} shows one case, in which the Gaussian tree has two layers of inputs. Similar to previous cases we may write the encoding scheme, through which we can write the pairwise covariance between inputs at the first layer as $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{B}^{(1)}]=\\gamma_{kl}B_{k,t}^{(1)}B_{l,t}^{(1)}$, in which $k\\neq l\\in\\{1,2,3,4\\}$. \nBy the previous example, we know that the input vector $\\mathbf{Y}_{t}^{(1)}$ becomes Gaussian for each realization of $\\mathbf{B}_{t}^{(1)}=\\{\\mathbf{b}_{1,t}^{(1)},\\mathbf{b}_{2,t}^{(1)},\\mathbf{b}_{3,t}^{(1)},\\mathbf{b}_{4,t}^{(1)}\\}$. \nHence, one may divide the codebook $\\mathbb{C}$ into $2^4=16$ parts $\\mathbb{S}_i,~i\\in\\{1,2,...,16\\}$, in which each part follows a specific Gaussian density with covariance values $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{b}^{(1)}]=\\gamma_{kl}b_{k,t}^{(1)}b_{l,t}^{(1)},~k\\neq l\\in\\{1,2,3,4\\}$.\nNow, for each subset, at the second layer we are dealing with the case shown in Figure \\ref{fig:B1}, which has been resolved.\nThus, the lower bound on the possible rates in the second layer are, \n\n", "itemtype": "equation", "pos": 28476, "prevtext": "\n\nNote that the sum of the rates $R_{Y^{(1)}}+R_{B^{(1)}}$ is lower bounded by $I(\\mathbf{X};Y^{(1)},B^{(1)})$, which by Theorem \\ref{thm:fixed} is fixed.\nHowever, the minimum rate for $R_{Y^{(1)}}$ is achieved by $min_{Y^{(1)}} I(\\mathbf{X};Y^{(1)})$.\n\n\nIn the following Theorem we prove that the optimal solution occurs when $B^{(1)}$ is uniformly distributed.\n\\begin{thm} \\label{thm:broadcast}\n{\\it\nThe optimal solution to the optimization problem $\\pi^*=arg\\max_{\\pi_1\\in [0,1]} I(\\mathbf{X};B^{(1)}|Y^{(1)})$ is $\\pi^*=1/2$.\n}\n\\end{thm}\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{proof}\nThe proof relies on the results shown in \\cite{capacity}. One can show that given $Y^{(1)}=y$, the broadcast model in \\eqref{eq:regression_star} becomes a bipolar signaling scheme with $\\mathbf{S}_1=[\\alpha_1y,\\alpha_2y,\\alpha_3y]'$ and $\\mathbf{S_2}=-\\mathbf{S_1}$.\nNow, simply by putting $\\mathbf{T}=-\\mathbf{I}_3$, where $\\mathbf{T}$ is an orthonormal matrix and using $\\mathbf{S_2}=\\mathbf{TS_1}$, one can map the signals to each other.\nAlso, we may normalize the noise variances to satisfy all the constraints shown in \\cite{capacity}. This can be simply done by introducing a $3\\times 3$ diagonal matrix $\\mathbf{M}$ with $m_{ii}=1/\\sigma_{z_i}$, through which the new signal model becomes $\\mathbf{X}'=\\mathbf{MX}=\\mathbf{MS_i}+\\mathbf{MZ},~i\\in\\{1,2\\}$, where $Z'_i\\in\\mathbf{Z}'=\\mathbf{MZ}$ are noises with unit variance.\nSince such model is \\textit{circular symmetric} \\cite{capacity}, hence, $\\pi^*=1/2$.\n\\end{proof}\n\nThe results in Theorem \\ref{thm:broadcast} can be further generalized to star topologies with arbitrary number of outputs.\n\n\\begin{cor} \\label{cor:general}\n{\\it\nAssuming a broadcast channel with single input $Y$ and output vector $\\mathbf{X}=\\{X_1,X_2,...,X_n\\},~n\\geq 3$, the optimal solution is at $\\pi^*=1/2$.\n}\n\\end{cor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Channel Synthesis with $4$ outputs and $2$ inputs}\n\n\nConsider the channel shown in Figure \\ref{fig:B1}. In this case, we are given two hidden inputs $Y_1^{(1)}$ and $Y_2^{(1)}$, and by previous arguments we know $\\mathbf{B}^{(1)}=\\{B_1^{(1)},B_2^{(1)},B_{12}\\}$ with $B_{12}=B_1^{(1)}B_2^{(1)}$, completely determined by $B_1^{(1)}$ and $B_2^{(1)}$, which may act independently. \n\nWe may write,\n\n", "index": 17, "text": "\\begin{align}\n\\begin{bmatrix}\nX_{k,t}\\\\\nX_{l,t}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha_{j,k}\\\\\n\\alpha_{j,l}\n\\end{bmatrix}\nB^{(1)}_{j,t}\nY^{(1)}_{j,t}\n+\n\\begin{bmatrix}\nZ_{k,t}\\\\\nZ_{l,t}\\\\\n\\end{bmatrix}\n,~t\\in\\{1,2,...,N\\}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{bmatrix}X_{k,t}\\\\&#10;X_{l,t}\\end{bmatrix}=\\begin{bmatrix}\\alpha_{j,k}\\\\&#10;\\alpha_{j,l}\\end{bmatrix}B^{(1)}_{j,t}Y^{(1)}_{j,t}+\\begin{bmatrix}Z_{k,t}\\\\&#10;Z_{l,t}\\\\&#10;\\end{bmatrix},~{}t\\in\\{1,2,...,N\\}\" display=\"inline\"><mrow><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mrow><mi>l</mi><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msubsup><mi>B</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>Y</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>Z</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>Z</mi><mrow><mi>l</mi><mo>,</mo><mi>t</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\nThis is due to the fact that we compute subsets of codebook for each realization of $\\mathbf{B}^{(1)}$.\nHence, in general the output at the $l$-th layer $\\mathbf{Y}^{(l)}$ is synthesized by $\\mathbf{Y}^{(l+1)}$ and $\\mathbf{B}^{(l+1)}$, which are at layer $l+1$.\nTherefore, we only need Gaussian sources at the top layer $L$ and Bernoulli sources $B^{(l)}$ for each layer $l$ to gradually synthesize the output that is close enough to the true observable output, measured by total variation.\nSince at each layer $l$ due to Lemma \\ref{lem:optimal_dumbbell}, the maximum achievable region is achieved for uniform $\\mathbf{B}^{(l)}$, we have the following corollary\n\n\\begin{cor}\n{\\it\nThe maximum achievable rate region for any channel whose underlying structure is induced by a Gaussian tree is achievable for uniform sign information inputs $\\mathbf{B}$.\n}\n\\end{cor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion} \\label{sec:conclusion}\n\nWe explained an important drawback of current graphical model learning algorithms, which results in losing the correlation signs information in the recovered model.\nWe then formulated a Gaussian synthesis problem through layered forwarding channels to synthesize the observed data.\nThen we deduced an interesting conclusion under which maximizing the achievable rate region resulted in maximizing the captured information on sign singularity.\nWe showed that the optimal rates are achievable for any Gaussian tree with leaf outputs if the sign input is uniformly distributed.\nIn future, we aim to solve those channels with some outputs acting as internal variables of the underlying latent Gaussian tree structure.\n\n\n\n\\bibliography{reference}\n\\bibliographystyle{IEEEtran}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendices\n\n\\section{Proof of Theorem \\ref{thm:B}} \\label{app:B}\n\nFirst, let's prove the first part.\nConsider the case in Figure \\ref{fig:neighbor}. The hidden node $y$, has $k$ observable neighbors $\\{x_1,...,x_k\\}$, while it is connected through two or more edges to other observable nodes $\\{x_{k+1,...,x_n}\\}$. \nGiven only observable covariance matrix $\\Sigma_x$, we can compute the empirical pairwise covariance values, hence all $\\rho_{x_ixj}$ are fixed.\nNow, for example suppose we \\textbf{flipped} the sign of the edge between $y-x_1$. Now, in order to maintain the same covariance matrix $\\Sigma_x$, we should do the following:\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{neighbor} \n\\caption{Neighborhood of hidden variable $y$}\\label{fig:neighbor} \n\\end{figure}\n\n$(1)$ The sign of all edge weights between $y-x_j$ for all $j\\in\\{2,...,k\\}$ should be flipped. This is easy to see because for all $j\\in\\{2,...,k\\}$, we know $\\rho_{x_1x_j}=\\rho_{x_1y}\\rho_{x_jy}$, and since $\\rho_{x_1x_j}$ is fixed, and we flipped the sign of $\\rho_{x_1y}$, so the sign of $\\rho_{x_jy}$ should certainly be flipped.\n\n$(2)$ The sign of all pairwise covariance values between $y$ and $x_i$, for all $i\\in\\{k+1,...,n\\}$ should be flipped. The same argument as the previous case can be used. However, in this case, all we know is that \\textbf{odd} number of sign-flips for the edge-weights should happen between each $y$ and $x_i$.\nUsing the above arguments, we can see that all $\\rho_{x_jy}$ for $j\\in\\{1,...,k\\}$ maintain their signs, or otherwise all of their signs should be flipped.\n\nFor the second part, \nWe inductively show that given a minimal latent tree, with $n$ observable $x_1,...,x_n$ and with $k$ hidden nodes $y_1,...,y_k$, given any $\\Sigma_x$, we can find $2^k$ latent trees with different edge-signs, which induce the same $\\Sigma_x$:\n\n$(1)$ Basis: The Star topology, shown in Figure \\ref{fig:broadcast}.\n\n$(2)$ We assume that such claim holds for all trees with number of hidden nodes less than $k$. Now, assume we are given an arbitrary latent tree with $k$ hidden nodes and $n$ observable.\n\nSome of these hidden nodes certainly have \\textbf{leaf observable} neighbors. So, let's pack each of of these leaf observable into a single pack. \\textbf{Note that these pack are different than those previously used, in a sense that they only consist of those observable neighbors to $y_i$, which are leaves!}.\n\nNow, note that the problem of finding equivalent sign permutations in this tree can be translated into a problem with smaller tree: \\textbf{delete all of those special packs, and treat their hidden parent $y_i$ as their representative}. Suppose there are $m$ hidden nodes $\\{y_1,...,y_m\\}$, which can represent each of these packs. This case is illustrated in Figure \\ref{fig:induction}. As Figure \\ref{fig:induction} shows, there are some other observable nodes remaining that can be connected to each other and also some other hidden nodes (but if $x_i$ is adjacent to a hidden node, then it should not be leaf, otherwise it is already packed).\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{induction} \n\\caption{Figure illustrating the inductive proof}\\label{fig:induction} \n\\end{figure}\n\nNow, if we replace all of these packs with a single node $y_i$, for all $i\\in\\{1,2,...,m\\}$, we obtain a smaller tree. Now, we can simply assume that all $y_1,...,y_m$ are observable and their pairwise covariance values are determined. Hence, this tree only has $k-m$ remaining hidden nodes, so due to inductive step it has $2^{k-m}$ possible equivalent trees with different edge-signs.\n\nAll its remaining is to show that by adding back those $m$ packs of observable, we obtain the claimed result. However, this is straightforward: Add back two packs corresponding to $y_1$ and $y_2$. Now, $y_1$ and $y_2$ can be regarded as hidden nodes, so now there are $k-m+2$ hidden nodes, which due to inductive step has $2^{k-m+2}$ equivalent representations of edge-weights. This can be shown up to $m-1$-th step by adding back $y_1,...,y_{m-1}$ nodes, and having a size of $k-1$ nodes, and again due to induction having $2^{k-1}$ equivalent sign combinations. Now, let's add the last pack, i.e., pack $m$. Then, we can obtain two equivalent classes: $(a)$ The positive pack, with positive edge-sign of $y_m-z_m$, where $z_m$ can be observable/hidden. $(b)$ The negation of pack, with negation of the edge-sign $y_m-z_m$. This is shown in Figure \\ref{fig:mth}\nHence, we obtain $2\\times 2^{k-1}=2^k$ edge-signs.\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{mth} \n\\caption{Obtaining $m$-th step from $m-1$-th step}\\label{fig:mth} \n\\end{figure}\n\nThis completes the proof.\n\n\\section{Proof of Theorem \\ref{thm:fixed}} \\label{app:fixed}\n\nBy the following remark after Theorem \\ref{thm:fixed} we may argue that the edges between outputs and inputs should be fixed, since altering one correlation value results in contradictory values for other correlation values.\n\nNow, to find each correlation value $\\rho_{x_iy}$, for some $X_i$ and $Y$,\nconsider the star model, with one hidden node, and three leaves, e.g., Figure \\ref{fig:broadcast}. We can write: $\\rho^2_{x_1y}=\\dfrac{{\\sigma_{x_{{1}}x_{{2}}}}{\\sigma_{x_{{1}}x_{{3}}}}}{{\\sigma_{x_{{2}}x_{{3}}}}}$, $\\rho^2_{x_2y}=\\dfrac{{\\sigma_{x_{{1}}x_{{2}}}}{\\sigma_{x_{{2}}x_{{3}}}}}{{\\sigma_{x_{{1}}x_{{3}}}}}$, and $\\rho^2_{x_3y}=\\dfrac{{\\sigma_{x_{{1}}x_{{3}}}}{\\sigma_{x_{{2}}x_{{3}}}}}{{\\sigma_{x_{{1}}x_{{2}}}}}$. \n\nNow, for a general structure, if we replace $1\\leftarrow i$, $2\\leftarrow j$, and $3\\leftarrow k$, we conclude that $\\rho^2_{x_iy}=\\dfrac{{\\sigma_{x_{{i}}x_{{j}}}}{\\sigma_{x_{{i}}x_{{k}}}}}{{\\sigma_{x_{{j}}x_{{k}}}}}$, for any three distinct $i,~j$ and $k$. As it may seem, there are many equations for computing $\\rho^2_{x_iy}$, which all of these expressions should be equal, i.e., the covariance matrix $\\Sigma_x$ should be representable by a given latent tree model.\n\nAt the end, note that we may write down the expression for mutual information and observe that all correlation values $\\rho_{y_iy_j}$ may be factored out, since they are implicitly affecting the $\\rho_{x_ix_j}$ values.\n\n\\section{Proof of Lemma \\ref{lem:optimal_dumbbell}} \\label{app:optimal_dumbbell}\nFor the channel shown in Figure \\ref{fig:B1}, we may write,\n\n\n", "itemtype": "equation", "pos": 32415, "prevtext": "\n\\noindent where for $j\\in\\{1,2\\}$, $(k,l)\\in\\{(1,2),(3,4)\\}$, which shows each group of outputs corresponding to each of the inputs.\n\nHere, two inputs $Y_1^{(1)}$ and $Y_2^{(1)}$ are dependent and their pairwise correlation can be computed via $E[Y_1^{(1)}Y_2^{(1)}|\\mathbf{B}^{(1)}]=\\gamma_{12}B_{12}=\\gamma_{12}B_1^{(1)}B_2^{(1)}$, in which $\\gamma_{12}$ determines the degree of correlation and is learned by certain inference algorithms, e.g., RG or CLRG \\cite{mit}.\nNote that the dependency relation of symbols $Y_{1,t}^{(1)}$ and $Y_{2,t}^{(1)}$ follows a Gaussian mixture model, since their covariance is a function of binary inputs $B_{1,t}^{(1)}$ and $B_{2,t}^{(1)}$.\nBut, note that in a given codebook consisting of $M_{\\mathbf{Y}^{(1)}}\\times M_{\\mathbf{B}^{(1)}}$ codewords, for each realization of $\\mathbf{b}_{3,t}^{(1)}=\\mathbf{b}_{1,t}^{(1)}.\\mathbf{b}_{2,t}^{(1)}$ the joint density of $\\mathbf{Y}_{t}^{(1)}$ is Gaussian. \nHence, one may divide the codebook $\\mathbb{C}$ into two parts $\\mathbb{S}_i,~i\\in\\{1,2\\}$, in which each part follows a specific Gaussian density with covariance values $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{b}^{(1)}]=\\gamma_{12}b_{1,t}^{(1)}b_{2,t}^{(1)}$.\nNote that such sub-block encoding guarantees the independence between the synthesized output vector and the sign input vector $\\mathbf{B}$.\nThe achievable region can be obtained from \\eqref{eq:conditions}, and by replacing $Y^{(1)}$ with $\\{Y^{(1)}_1,Y^{(1)}_2\\}$ and $B^{(1)}$ with $\\{B^{(1)}_1,B^{(1)}_2\\}$.\n \nIn the following Lemma, whose proof can be found in Appendix \\ref{app:optimal_dumbbell} we showed that the optimal solution $(\\pi^*_1,\\pi^*_2)$ to $arg\\max_{\\pi_1,\\pi_2} I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$ is at $(1/2,1/2)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lem} \\label{lem:optimal_dumbbell}\n{\\it\nFor the channel shown in Figure \\ref{fig:B1}, we have,\n\n$(1)$ $I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})=I(X_1,X_2;B_1^{(1)}|Y_1^{(1)}Y_2^{(1)})+I(X_3,X_4;B_2^{(1)}|Y_1^{(1)}Y_2^{(1)})$.\n\n$(2)$ The optimal solution for the maximization problem $arg\\max_{\\pi_1,\\pi_2} I(\\mathbf{X};\\mathbf{B}|\\mathbf{Y})$ happens at $\\pi^*_1=\\pi^*_2=1/2$.\n}\n\\end{lem}\n\nIntuitively, for the first part in Lemma \\ref{lem:optimal_dumbbell}, we may divide the structure shown in Figure \\ref{fig:B1} into two substructures each similar to the star topology shown in Figure \\ref{fig:broadcast} (but with only two outputs). Hence, we may use the results in Theorem \\ref{thm:broadcast} to prove the second part of the lemma.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Multi-Layered Channel Synthesis}\n\nHere, we address those channels with multi-layer inputs. Figure \\ref{fig:B2} shows one case, in which the Gaussian tree has two layers of inputs. Similar to previous cases we may write the encoding scheme, through which we can write the pairwise covariance between inputs at the first layer as $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{B}^{(1)}]=\\gamma_{kl}B_{k,t}^{(1)}B_{l,t}^{(1)}$, in which $k\\neq l\\in\\{1,2,3,4\\}$. \nBy the previous example, we know that the input vector $\\mathbf{Y}_{t}^{(1)}$ becomes Gaussian for each realization of $\\mathbf{B}_{t}^{(1)}=\\{\\mathbf{b}_{1,t}^{(1)},\\mathbf{b}_{2,t}^{(1)},\\mathbf{b}_{3,t}^{(1)},\\mathbf{b}_{4,t}^{(1)}\\}$. \nHence, one may divide the codebook $\\mathbb{C}$ into $2^4=16$ parts $\\mathbb{S}_i,~i\\in\\{1,2,...,16\\}$, in which each part follows a specific Gaussian density with covariance values $E[Y_{k,t}^{(1)}Y_{l,t}^{(1)}|\\mathbf{b}^{(1)}]=\\gamma_{kl}b_{k,t}^{(1)}b_{l,t}^{(1)},~k\\neq l\\in\\{1,2,3,4\\}$.\nNow, for each subset, at the second layer we are dealing with the case shown in Figure \\ref{fig:B1}, which has been resolved.\nThus, the lower bound on the possible rates in the second layer are, \n\n", "index": 19, "text": "\\begin{align}\n&R_{\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)}}\\geq I(\\mathbf{Y}^{(1)};\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)})\\notag\\\\\n&R_{\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)}}+R_{\\mathbf{B}^{(2)}|\\mathbf{B}^{(1)}}\\geq I(\\mathbf{Y}^{(1)};\\mathbf{Y}^{(2)},\\mathbf{B}^{(2)}|\\mathbf{B}^{(1)})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R_{\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)}}\\geq I(\\mathbf{Y}^{(1)};%&#10;\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)})\" display=\"inline\"><mrow><msub><mi>R</mi><mrow><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo>\u2265</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R_{\\mathbf{Y}^{(2)}|\\mathbf{B}^{(1)}}+R_{\\mathbf{B}^{(2)}|%&#10;\\mathbf{B}^{(1)}}\\geq I(\\mathbf{Y}^{(1)};\\mathbf{Y}^{(2)},\\mathbf{B}^{(2)}|%&#10;\\mathbf{B}^{(1)})\" display=\"inline\"><mrow><msub><mi>R</mi><mrow><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo>\u2265</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><msup><mi>\ud835\udc18</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06403.tex", "nexttext": "\n\\noindent where \\eqref{eq:cond_indep} is due to the stated constraints in \\eqref{eq:common_info}. Equation \\eqref{eq:chain_rule} results using the chain rule and the fact that $(X_1,X_2)-Y_1^{(1)}-Y_2^{(1)}-(X_3,X_4)$ forms a Markov Chain, hence $(X_1,X_2)$ is conditionally independent of $(X_3,X_4)$ given $(Y_1^{(1)},Y_2^{(1)})$.\nNote that due to dependency of $B_1^{(1)}$ and $B_2^{(1)}$, we know $h(X_1,X_2|Y_1^{(1)}Y_2^{(1)})\\neq h(X_1,X_2|Y_1^{(1)})$, since the latter ignores such dependency.\n\nHence, to find the optimal solution, one may maximize both terms in \\eqref{eq:split} simultaneously, which by definition is equivalent to maximizing the sum $h(X_1,X_2|Y_1^{(1)}Y_2^{(1)})+h(X_3,X_4|Y_1^{(1)}Y_2^{(1)})$. Considering the first term, we know $h(X_1,X_2|Y_1^{(1)}Y_2^{(1)})=\\sum_{y_1^{(1)}y_2^{(1)}} p_{Y_1^{(1)}Y_2^{(1)}}(y_1^{(1)}y_2^{(1)}) h(X_1,X_2|y_1^{(1)}y_2^{(1)})$, and due to \\cite{capacity} we know the maximum of $h(X_1,X_2|y_1^{(1)}y_2^{(1)})$ happens for uniform conditional density $p(x_1,x_2|y_1^{(1)}y_2^{(1)})$. However, such conditional PDF can be written as conditional PMF $p_{B_1^{(1)}}(b_1^{(1)}|b_1^{(1)}.b_2^{(1)})$, which by previous arguments we may conclude that the optimal solution happens for uniform PMF, which one may easily check that such uniform PMF will be deduced for $(\\pi_1,\\pi_2)=(1/2,1/2)$. This completes the proof.\n\n\n", "itemtype": "equation", "pos": 40864, "prevtext": "\nThis is due to the fact that we compute subsets of codebook for each realization of $\\mathbf{B}^{(1)}$.\nHence, in general the output at the $l$-th layer $\\mathbf{Y}^{(l)}$ is synthesized by $\\mathbf{Y}^{(l+1)}$ and $\\mathbf{B}^{(l+1)}$, which are at layer $l+1$.\nTherefore, we only need Gaussian sources at the top layer $L$ and Bernoulli sources $B^{(l)}$ for each layer $l$ to gradually synthesize the output that is close enough to the true observable output, measured by total variation.\nSince at each layer $l$ due to Lemma \\ref{lem:optimal_dumbbell}, the maximum achievable region is achieved for uniform $\\mathbf{B}^{(l)}$, we have the following corollary\n\n\\begin{cor}\n{\\it\nThe maximum achievable rate region for any channel whose underlying structure is induced by a Gaussian tree is achievable for uniform sign information inputs $\\mathbf{B}$.\n}\n\\end{cor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion} \\label{sec:conclusion}\n\nWe explained an important drawback of current graphical model learning algorithms, which results in losing the correlation signs information in the recovered model.\nWe then formulated a Gaussian synthesis problem through layered forwarding channels to synthesize the observed data.\nThen we deduced an interesting conclusion under which maximizing the achievable rate region resulted in maximizing the captured information on sign singularity.\nWe showed that the optimal rates are achievable for any Gaussian tree with leaf outputs if the sign input is uniformly distributed.\nIn future, we aim to solve those channels with some outputs acting as internal variables of the underlying latent Gaussian tree structure.\n\n\n\n\\bibliography{reference}\n\\bibliographystyle{IEEEtran}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendices\n\n\\section{Proof of Theorem \\ref{thm:B}} \\label{app:B}\n\nFirst, let's prove the first part.\nConsider the case in Figure \\ref{fig:neighbor}. The hidden node $y$, has $k$ observable neighbors $\\{x_1,...,x_k\\}$, while it is connected through two or more edges to other observable nodes $\\{x_{k+1,...,x_n}\\}$. \nGiven only observable covariance matrix $\\Sigma_x$, we can compute the empirical pairwise covariance values, hence all $\\rho_{x_ixj}$ are fixed.\nNow, for example suppose we \\textbf{flipped} the sign of the edge between $y-x_1$. Now, in order to maintain the same covariance matrix $\\Sigma_x$, we should do the following:\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{neighbor} \n\\caption{Neighborhood of hidden variable $y$}\\label{fig:neighbor} \n\\end{figure}\n\n$(1)$ The sign of all edge weights between $y-x_j$ for all $j\\in\\{2,...,k\\}$ should be flipped. This is easy to see because for all $j\\in\\{2,...,k\\}$, we know $\\rho_{x_1x_j}=\\rho_{x_1y}\\rho_{x_jy}$, and since $\\rho_{x_1x_j}$ is fixed, and we flipped the sign of $\\rho_{x_1y}$, so the sign of $\\rho_{x_jy}$ should certainly be flipped.\n\n$(2)$ The sign of all pairwise covariance values between $y$ and $x_i$, for all $i\\in\\{k+1,...,n\\}$ should be flipped. The same argument as the previous case can be used. However, in this case, all we know is that \\textbf{odd} number of sign-flips for the edge-weights should happen between each $y$ and $x_i$.\nUsing the above arguments, we can see that all $\\rho_{x_jy}$ for $j\\in\\{1,...,k\\}$ maintain their signs, or otherwise all of their signs should be flipped.\n\nFor the second part, \nWe inductively show that given a minimal latent tree, with $n$ observable $x_1,...,x_n$ and with $k$ hidden nodes $y_1,...,y_k$, given any $\\Sigma_x$, we can find $2^k$ latent trees with different edge-signs, which induce the same $\\Sigma_x$:\n\n$(1)$ Basis: The Star topology, shown in Figure \\ref{fig:broadcast}.\n\n$(2)$ We assume that such claim holds for all trees with number of hidden nodes less than $k$. Now, assume we are given an arbitrary latent tree with $k$ hidden nodes and $n$ observable.\n\nSome of these hidden nodes certainly have \\textbf{leaf observable} neighbors. So, let's pack each of of these leaf observable into a single pack. \\textbf{Note that these pack are different than those previously used, in a sense that they only consist of those observable neighbors to $y_i$, which are leaves!}.\n\nNow, note that the problem of finding equivalent sign permutations in this tree can be translated into a problem with smaller tree: \\textbf{delete all of those special packs, and treat their hidden parent $y_i$ as their representative}. Suppose there are $m$ hidden nodes $\\{y_1,...,y_m\\}$, which can represent each of these packs. This case is illustrated in Figure \\ref{fig:induction}. As Figure \\ref{fig:induction} shows, there are some other observable nodes remaining that can be connected to each other and also some other hidden nodes (but if $x_i$ is adjacent to a hidden node, then it should not be leaf, otherwise it is already packed).\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{induction} \n\\caption{Figure illustrating the inductive proof}\\label{fig:induction} \n\\end{figure}\n\nNow, if we replace all of these packs with a single node $y_i$, for all $i\\in\\{1,2,...,m\\}$, we obtain a smaller tree. Now, we can simply assume that all $y_1,...,y_m$ are observable and their pairwise covariance values are determined. Hence, this tree only has $k-m$ remaining hidden nodes, so due to inductive step it has $2^{k-m}$ possible equivalent trees with different edge-signs.\n\nAll its remaining is to show that by adding back those $m$ packs of observable, we obtain the claimed result. However, this is straightforward: Add back two packs corresponding to $y_1$ and $y_2$. Now, $y_1$ and $y_2$ can be regarded as hidden nodes, so now there are $k-m+2$ hidden nodes, which due to inductive step has $2^{k-m+2}$ equivalent representations of edge-weights. This can be shown up to $m-1$-th step by adding back $y_1,...,y_{m-1}$ nodes, and having a size of $k-1$ nodes, and again due to induction having $2^{k-1}$ equivalent sign combinations. Now, let's add the last pack, i.e., pack $m$. Then, we can obtain two equivalent classes: $(a)$ The positive pack, with positive edge-sign of $y_m-z_m$, where $z_m$ can be observable/hidden. $(b)$ The negation of pack, with negation of the edge-sign $y_m-z_m$. This is shown in Figure \\ref{fig:mth}\nHence, we obtain $2\\times 2^{k-1}=2^k$ edge-signs.\n\n\\begin{figure} [h!]\n\\centering\n\\includegraphics[scale=0.6]{mth} \n\\caption{Obtaining $m$-th step from $m-1$-th step}\\label{fig:mth} \n\\end{figure}\n\nThis completes the proof.\n\n\\section{Proof of Theorem \\ref{thm:fixed}} \\label{app:fixed}\n\nBy the following remark after Theorem \\ref{thm:fixed} we may argue that the edges between outputs and inputs should be fixed, since altering one correlation value results in contradictory values for other correlation values.\n\nNow, to find each correlation value $\\rho_{x_iy}$, for some $X_i$ and $Y$,\nconsider the star model, with one hidden node, and three leaves, e.g., Figure \\ref{fig:broadcast}. We can write: $\\rho^2_{x_1y}=\\dfrac{{\\sigma_{x_{{1}}x_{{2}}}}{\\sigma_{x_{{1}}x_{{3}}}}}{{\\sigma_{x_{{2}}x_{{3}}}}}$, $\\rho^2_{x_2y}=\\dfrac{{\\sigma_{x_{{1}}x_{{2}}}}{\\sigma_{x_{{2}}x_{{3}}}}}{{\\sigma_{x_{{1}}x_{{3}}}}}$, and $\\rho^2_{x_3y}=\\dfrac{{\\sigma_{x_{{1}}x_{{3}}}}{\\sigma_{x_{{2}}x_{{3}}}}}{{\\sigma_{x_{{1}}x_{{2}}}}}$. \n\nNow, for a general structure, if we replace $1\\leftarrow i$, $2\\leftarrow j$, and $3\\leftarrow k$, we conclude that $\\rho^2_{x_iy}=\\dfrac{{\\sigma_{x_{{i}}x_{{j}}}}{\\sigma_{x_{{i}}x_{{k}}}}}{{\\sigma_{x_{{j}}x_{{k}}}}}$, for any three distinct $i,~j$ and $k$. As it may seem, there are many equations for computing $\\rho^2_{x_iy}$, which all of these expressions should be equal, i.e., the covariance matrix $\\Sigma_x$ should be representable by a given latent tree model.\n\nAt the end, note that we may write down the expression for mutual information and observe that all correlation values $\\rho_{y_iy_j}$ may be factored out, since they are implicitly affecting the $\\rho_{x_ix_j}$ values.\n\n\\section{Proof of Lemma \\ref{lem:optimal_dumbbell}} \\label{app:optimal_dumbbell}\nFor the channel shown in Figure \\ref{fig:B1}, we may write,\n\n\n", "index": 21, "text": "\\begin{align}\nI(&\\mathbf{X};\\mathbf{B}|\\mathbf{Y})=h(\\mathbf{X}|\\mathbf{Y})-h(\\mathbf{X}|\\mathbf{Y,B})\\notag\\\\\n&=h(\\mathbf{X}|\\mathbf{Y}) - h(X_1,X_2|Y_1^{(1)},B_1^{(1)})\\notag\\\\\n&-h(X_3,X_4|Y_2^{(1)},B_2^{(1)})\\label{eq:cond_indep}\\\\ \n&=h(X_1,X_2|Y_1^{(1)},Y_2^{(1)})+h(X_3,X_4|Y_1^{(1)},Y_2^{(1)})\\notag\\\\\n&- h(X_1,X_2|Y_1^{(1)},B_1^{(1)})-h(X_3,X_4|Y_2^{(1)},B_2^{(1)})\\label{eq:chain_rule}\\\\ \n&=h(X_1,X_2|Y_1^{(1)}Y_2^{(1)})+h(X_3,X_4|Y_1^{(1)}Y_2^{(1)})\\notag\\\\ \\label{eq:markov_layer}\n&- h(X_1,X_2|Y_1^{(1)},B_1^{(1)})-h(X_3,X_4|Y_2^{(1)},B_2^{(1)})\\\\ \n&=I(X_1,X_2;B_1^{(1)}|Y_1^{(1)}Y_2^{(1)})\\notag\\\\\n&+I(X_3,X_4;B_2^{(1)}|Y_1^{(1)}Y_2^{(1)}) \\label{eq:split}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I(\" display=\"inline\"><mrow><mi>I</mi><mo stretchy=\"false\">(</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{X};\\mathbf{B}|\\mathbf{Y})=h(\\mathbf{X}|\\mathbf{Y})-h(%&#10;\\mathbf{X}|\\mathbf{Y,B})\" display=\"inline\"><mrow><mi>\ud835\udc17</mi><mo>;</mo><mi>\ud835\udc01</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo stretchy=\"false\">)</mo><mo>-</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo>,</mo><mi>\ud835\udc01</mi><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h(\\mathbf{X}|\\mathbf{Y})-h(X_{1},X_{2}|Y_{1}^{(1)},B_{1}^{(1)})\" display=\"inline\"><mrow><mo>=</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc18</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-h(X_{3},X_{4}|Y_{2}^{(1)},B_{2}^{(1)})\" display=\"inline\"><mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h(X_{1},X_{2}|Y_{1}^{(1)},Y_{2}^{(1)})+h(X_{3},X_{4}|Y_{1}^{(1)}%&#10;,Y_{2}^{(1)})\" display=\"inline\"><mrow><mo>=</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-h(X_{1},X_{2}|Y_{1}^{(1)},B_{1}^{(1)})-h(X_{3},X_{4}|Y_{2}^{(1)}%&#10;,B_{2}^{(1)})\" display=\"inline\"><mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h(X_{1},X_{2}|Y_{1}^{(1)}Y_{2}^{(1)})+h(X_{3},X_{4}|Y_{1}^{(1)}Y%&#10;_{2}^{(1)})\" display=\"inline\"><mrow><mo>=</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-h(X_{1},X_{2}|Y_{1}^{(1)},B_{1}^{(1)})-h(X_{3},X_{4}|Y_{2}^{(1)}%&#10;,B_{2}^{(1)})\" display=\"inline\"><mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>B</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=I(X_{1},X_{2};B_{1}^{(1)}|Y_{1}^{(1)}Y_{2}^{(1)})\" display=\"inline\"><mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>;</mo><msubsup><mi>B</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+I(X_{3},X_{4};B_{2}^{(1)}|Y_{1}^{(1)}Y_{2}^{(1)})\" display=\"inline\"><mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo>;</mo><msubsup><mi>B</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>Y</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><msubsup><mi>Y</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}]