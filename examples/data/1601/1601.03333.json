[{"file": "1601.03333.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 12588, "prevtext": "\n\n\\title{A Score-level Fusion Method for Eye Movement Biometrics}\n\n\n\\author{Anjith~George,~\\IEEEmembership{Member,~IEEE,}\n        and~Aurobinda~Routray,~\\IEEEmembership{Member,~IEEE}\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem A. George and A. Routray are with the Department\nof Electrical Engineering, IIT Kharagpur, India,\n721302.\\protect\\\\\n\nE-mail: anjith2006@gmail.com\n\\IEEEcompsocthanksitem This is the author version of the accepted manuscript. The full version of the method described in this paper is available in : Anjith George and Aurobinda Routray. \"A Score-level Fusion Method for Eye Movement Biometrics, Pattern Recognition Letters. http://www.sciencedirect.com/science/article/pii/S0167865515004067}\n\\thanks{DOI: http://dx.doi.org/10.1016/j.patrec.2015.11.020}}\n\n\n\n\n\n\\markboth{}\n{}\n\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\nThis paper proposes a novel framework for the use of eye movement patterns for biometric applications. Eye movements contain abundant information about cognitive brain functions, neural pathways, etc. In the proposed method, eye movement data is classified into fixations and saccades. Features extracted from fixations and saccades are used by a Gaussian Radial Basis Function Network (GRBFN) based method for biometric authentication. A score fusion approach is adopted to classify the data in the output layer. In the evaluation stage, the algorithm has been tested using two\ntypes of stimuli: random dot following on a screen and text reading. The results indicate the strength of eye movement pattern as a biometric modality.  The algorithm has been evaluated on BioEye 2015 database and found to outperform all the other methods. Eye movements are generated by a complex oculomotor plant which is very hard to spoof by mechanical replicas. Use of eye movement dynamics along with iris recognition technology may lead to a robust counterfeit-resistant person identification system.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nEye tracking, Biometrics, Eye movement biometrics, Gaze tracking.\n\\end{IEEEkeywords}}\n\n\n\\maketitle\n\n\n\\IEEEdisplaynontitleabstractindextext\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\IEEEraisesectionheading{\\section{Introduction}\\label{sec:introduction}}\n\n\\IEEEPARstart{B}{iometrics} is an active area of research in pattern recognition and machine learning community. Potential applications of biometrics include forensics, law enforcement, surveillance, personalized interaction, access control ~\\cite{jain2007handbook}, etc. Physiological features like fingerprint, DNA, earlobe geometry, iris pattern, facial recognition, ~\\cite{jain2004introduction} are widely used in biometrics. Recently, several behavioral biometric modalities have been proposed including gait, eye movement patterns, keystroke dynamics ~\\cite{wang2009behavioral} signature, etc. Even though many such parameters like brain signals ~\\cite{marcel2007person} (using electroencephalogram) and heart beats ~\\cite{plataniotis2006ecg} have been proposed as biometric modalities, their invasive nature limits their practical applications.\n\nAn effective biometric should have the following characteristics ~\\cite{jain2007handbook}: 1) the features should be unique for each individual, 2) they should not change with time (template aging effects), 3) acquisition of parameters should be easy (low computational complexity and noninvasive), 4) accurate and automated algorithms should be available for classification, 5) counterfeit resistance, 6) low cost, and 7) ease of implementation. Other characteristics that might make the system more robust are portability and the ability to extract features from non-co-operative subjects.\n\nOut of many biometric modalities, iris recognition has shown the most promising results ~\\cite{ib2005independent} obtaining Equal Error Rates (EER) close to 0.0011\\%. However, it can only be used when the user is co-operative. Such systems can be spoofed by contact lenses with printed patterns. Even though most of the biometric modalities perform well on evaluation databases, one may be able to spoof such systems with mechanical replicas or artificially fabricated models ~\\cite{roberts2007biometric}. In this regard, several approaches have been presented ~\\cite{schuckers2002issues} to detect the liveliness of tissues or body parts presented to the biometric system. However, such methods are also vulnerable to spoofing.\n\nBiometrics using patterns obtained from eye movements is a relatively new field of research. Most of the conventional biometrics use physiological characteristics of the human body. Eye movement-based biometrics tries to identify the behavioral patterns as well as information regarding physiological properties of tissues and muscles generating eye movements ~\\cite{leigh1999neurology}. They provide abundant information about cognitive brain functions and neural signals controlling eye movements. Saccadic eye movement is   the fastest movement (peak angular velocities up to 900 degrees per second) in the human body. Mechanically replicating such a complex oculomotor plant model is extremely difficult. These properties make eye movement patterns a suitable candidate for biometric applications. The dynamics of eye movement along with these properties can give inbuilt liveliness detection capability.\n\nInitially, eye movement biometrics has been proposed as a soft biometric. However, with the high level of accuracy achieved, it seems there are more opportunities regarding its application as an independent biometric modality. Eye movement detection can be integrated easily into already existing iris recognition systems. A combination of iris recognition and eye movement pattern recognition may lead to a robust counterfeit-resistant biometric modality with embedded liveliness detection and continuous authentication properties. Eye movement biometrics can also be made task-independent ~\\cite{kinnunen2010towards} so that the movements can be captured even for non-co-operative subjects.\n\nThe rest of the paper is organized as follows. Section 2 describes previous works related to the use of eye movement as a biometric. Section 3 presents the proposed algorithm. Evaluation of the algorithm along with the results are outlined in section 4. Conclusions regarding eye movement biometrics and possible extensions are detailed in section 5.\n\n\n\n\\section{Related works}\n\nInitial attempts to use eye movements as a biometric modality were carried out by Kasprowski and Ober ~\\cite{kasprowski2004eye}. They recorded the eye movements of subjects following a jumping dot on a screen. Several frequency domain and Cepstral features were extracted from this data. They applied different classification methods like naive Bayes, C45 decision trees, SVM and KNN methods. The results obtained further motivated research in eye movement-based biometrics. Bednarik et al. ~\\cite{bednarik2005eye} conducted experiments on several  tasks including text reading, moving cross stimulus tracking and free viewing of images. They used FFT and PCA on the eye movement data. Several combinations of such features were tried. However, the best results were obtained using the distance between eyes, which is not related to eye dynamics. Komogortsev et al. ~\\cite{komogortsev2010biometric} used an Oculomotor Plant Mathematical Model (OPMM) to model the complex dynamics of the oculomotor plant. The plant parameters were identified from the eye movement data. This approach was further extended in ~\\cite{komogortsev2012biometric}. Holland and Komogortsev ~\\cite{holland2013complexb} evaluated the applicability of eye movement biometrics with different spatial and temporal accuracies and various types of stimuli. Several parameters of eye movements were extracted from fixations and saccades. Weighted components were used to compare different samples for biometric identification. A temporal resolution of 250 Hz and spatial accuracy of 0.5 degrees were identified as the minimum requirements for accurate gaze-based biometric systems. Kinnunen et al. ~\\cite{kinnunen2010towards} presented a task-independent user authentication system based on eye movements. Gaussian mixture modeling of short-term gaze data was used in their approach. Even though the accuracy rates were fairly low, the study opened up possibilities for the development of task-independent eye movement-based verification systems. Rigas et al. ~\\cite{rigas2012biometric} explored variations in individual gaze patterns while observing human face images. Eye movements resulted were analyzed using a graph-based approach. The Multivariate Wald-Wolfowitz runs test was used to classify the eye movement data. This method achieved 70\\% rank-1 IR and 30\\% EER on a database of 15 subjects. Rigas et al. ~\\cite{rigas2012human} extended this method using features of velocity and acceleration calculated from fixations.  The feature distributions were compared using Wald-Wolfowitz test.\n\nZhang et al. ~\\cite{zhang2012biometric} used saccadic eye movements with machine learning algorithms for biometric verification. They used multilayer perceptron networks, support vector machines, radial basis function networks and logistic discriminant for the classification of eye movement data. Recently Cantoni et al. ~\\cite{cantoni2015gant} proposed a gaze analysis technique called GANT in which fixation patterns were denoted by a graph-based representation. For each user, a fixation model was constructed using the duration and number of visits at various points. Frobenius norm of the density maps was used to find the similarity between two recordings. Holland and Komogortsev presented an approach (CEM) ~\\cite{holland2011biometric} using several scan path features including saccade amplitudes, average saccade velocities, average saccade peak velocities, velocity waveform, fixation counts, average duration of fixation, length of scan path, area of scan path, regions of interest, number of inflections, main sequence relationship, pairwise distances between fixations, amplitude duration relationship, etc. A comparison metric of the features was computed using Gaussian cumulative density function. Another similarity metric was obtained by comparing the scan paths. A weighted fusion of these parameters obtained the best case EER of 27\\%. Holland and Komogortsev proposed a method (CEM-B)~\\cite{holland2013complexa}, in which the fixation and saccade features were compared using statistical methods like Ansari-Bradley test, two-sample t-test, two-sample Kolmogorov-Smirnov test, and the two-sample Cramer-von Mises test. Their approach achieved 83\\% rank-1 IR and 16.5\\% EER on a dataset of 32 subjects.\n\nTo the best knowledge of the authors, the best case EER obtained is 16.5\\% ~\\cite{holland2013complexa}. Most of the works presented in the literature were evaluated on smaller databases. The effect of template aging was not considered in these works. For the application of eye movement as a reliable biometric, the patterns should remain consistent with time. In this paper, we try to improve upon the existing methods. The proposed algorithm can reach an EER up to 2.59\\% and rank-1 accuracy of 89.54\\% in RAN\\_30min dataset of BioEye 2015 database ~\\cite{bioeye} containing 153 subjects. Template aging effect has also been studied using data taken after an interval of 1 year. The average EER obtained is 10.96\\% with a rank-1 accuracy of 81.08\\% with 37 subjects.\n\n\n\n\\section{Proposed method}\nIn the proposed approach, eye movement data from the experiment are classified into fixations and saccades, and their statistical features are used to characterize each individual. For each individual, the properties of saccades of same durations have been reported to be similar \\cite{collewijn1988binocular}. We use this knowledge and extract the statistical properties of the eye movements for biometric identification. Different stages of the algorithm are described below.\n\n\\subsection{Data pre-processing and noise removal}\nThe data contains visual angles in both $x$ and $y$ directions along with stimulus angles. Information about the validity of samples is also available. Eye movement data has been captured at a sampling frequency of 1000Hz. The data obtained is decimated to 250Hz using an anti-aliasing filter. In the proposed feature extraction method, most of the parameters are computed with reference to the screen coordinate system. Hence, in the pre-processing stage, the data obtained is converted to screen\ncoordinates based on head distance and geometry of the acquisition system as:\n\n", "index": 1, "text": "\\begin{equation}\n{x_{screen}} = \\left( {{{d * {w_{pix}}} \\over w}} \\right)\\tan ({\\theta _x}) + {{{w_{pix}}} \\over 2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{x_{screen}}=\\left({{{d*{w_{pix}}}\\over w}}\\right)\\tan({\\theta_{x}})+{{{w_{pix%&#10;}}}\\over 2}\" display=\"block\"><mrow><msub><mi>x</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mfrac><mrow><mi>d</mi><mo>*</mo><msub><mi>w</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow><mi>w</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mrow><mi>tan</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mfrac><msub><mi>w</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mn>2</mn></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\nwhere, $d,{\\theta _x}$ and ${\\theta _y}$ denote distance from the screen and visual angles in $x$ and $y$ direction (in radian) respectively. ${x_{screen}}$ and ${y_{screen}}$ denote the position of gaze on the screen. ${w_{pix}},{h_{pix}}$,$w,h$ denote resolution and physical size of the screen in horizontal and vertical directions respectively.\n\n\nRaw eye gaze positions may contain noise. Most of the features used in this work are extracted from velocity and acceleration profiles. The presence of noise makes it difficult to estimate the velocity and acceleration parameters using differentiation operation. Eye movement signals contain high-frequency components, especially during saccades. High-frequency components would be more prominent in velocity and acceleration profiles ~\\cite{harris1984instrument}. Savitzky-Golay filters are useful for\nfiltering out the noise when the frequency span of the signal is large ~\\cite{krishnan2013selection}. They are reported to be optimal ~\\cite{savitzky1964smoothing} for minimizing the least-square error in fitting a polynomial to frames of the noisy data. We use this filter with polynomial order of 6 and frame size of 15 in our approach.\n\n\\subsection{Eye movement classification and feature extraction }\n\\subsubsection{Eye movement classification}\nThe I-VT (velocity threshold) algorithm ~\\cite{holland2012biometric}, ~\\cite{salvucci2000identifying} is used to classify the filtered eye movement data into a sequence of fixations and saccades (Algorithm 1). Most of the earlier works specify the velocity threshold for angular velocity. The angular velocity computed from the filtered data is used to classify the eye movements. A velocity of 50 degrees/second is used as the threshold in I-VT algorithm.\n\n\n\\begin{algorithm}[h]\n\\label{alg:eyeclassification}\n\\KwData{[Time Gazex Gazey]}\n\\KwResult{Res}\n\\textbf{Constants} : VT=Velocity threshold,MDF=Minimum duration for fixation\\;\n\\textbf{States} $=$[FIXATION,SACCADE]\\;\nfixationStart=1\\;\nVelocity=\\textit{smoothDiff}(data)\\;\n$ N\\leftarrow$ Number of samples of data\\;\n\n\\For {$index \\leftarrow$ 1 \\textbf{to} N} {\n\n\\eIf{Velocity[index] $<$ VT}{\ncurrentState=FIXATION\\;\n\\If{lastState $\\neq$ currentState}{\nfixationStart = index\\;\n}\n}{\n\\If{lastState$ =$ FIXATION}{\nduration = data(index,1) - data(fixationStart,1)\\;\n\\If {duration $<$ MDF}{\n\n\\For {$i \\leftarrow$ fixationStart \\textbf{to} index}{\nres[i]= SACCADE;\n}\n}\n}\ncurrentState=SACCADE\\;\n}\nlastState=currentState\\;\nres[index]=currentState\\;\n}\nRes$\\leftarrow$res\\;\n\\caption{Fixation and Saccade classification algorithm}\n\\end{algorithm}\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{pics/gazedata.JPG}\n\\end{center}\n\\caption{Gaze data and stimulus for RAN\\_30min sequence}\n\\label{fig:gazedata}\n\\label{fig:onecol}\n\\end{figure}\nA minimum duration threshold of 100 milliseconds has been chosen to reduce the false positives in\nfixation identification. Algorithm 1 returns the classification results for each data point as either fixation or saccade. Points that are not a part of fixations are considered as saccades in this stage. In the proposed approach, we consider saccades with their\ndurations more than a specified threshold to minimize the effect of spurious saccade segments. From the results of Algorithm 1, a list containing starting index and duration of all fixations and saccades is created. A post-processing stage is carried out to remove small-duration saccades. Saccades with duration less than 12 milliseconds are removed in this stage.\n\n\n\\subsubsection{Feature extraction}\nAfter the removal of small-duration saccades, each eye movement data\nis arranged into a sequence of fixations and saccades. The sequence of gaze locations and corresponding visual angles are also available for each fixation and saccade. Several statistical features are extracted from the position, velocity and acceleration profiles of\nthe gaze sequence. Other features like duration, dispersion, path length and co-occurrence features\nare also extracted for both fixations and saccades. Earlier works ~\\cite{komogortsev2010biometric} suggested that saccades provide a rich amount of information about the dynamics of oculomotor plant. Hence, we extract several other parameters including the saccadic ratio, main sequence, angle, etc. Saccades in horizontal and vertical directions are generated by different areas of the brain \\cite{harwood2008optimally}. We use the statistical properties of the gaze data in $x$ and $y$ directions to incorporate this information. The distance and angle with the previous fixation/saccade are also used as features to leverage the temporal properties. The method used for computation of features is described below.\n\n\nLet $X{\\rm{ }} = {\\rm{ }}\\left\\{ {{x_1},{x_2},{x_3},...,{x_N}} \\right\\}$\nand $Y{\\rm{ }} = {\\rm{ }}\\left\\{ {{y_1},{y_2},{y_3},...,{y_N}} \\right\\}$ denote the set of coordinate positions of gaze in\neach fixation/saccade and let $N$ denotes the number of data points in any fixation or saccade.\n$\\left( {{x_i},{y_i}} \\right)$ denotes gaze location on the screen coordinate system and $\\left( {\\theta _i^x,\\theta _i^y} \\right)$ denotes the corresponding horizontal and vertical visual angles. \n\nA large number of features are extracted from the gaze sequence in each fixation and saccade. Some features are derived from the angular velocity. The differentiation operation for finding velocity and acceleration is carried out using forward difference method on the smoothed data. List of features extracted from fixations and saccades along with the methods of computation are shown in Table \\ref{Tab:fixation_featuresused} and Table \\ref{Tab:saccade_featuresused}. The features are extracted independently for each fixation and saccade.\n\n\n\n\\begin{table*}[htb]\n\n\\caption{\\label{Tab:fixation_featuresused}List of features extracted from fixations}\n\\centering\n\\begin{tabular}{llll}\n\\hline\n\\multicolumn{2}{l}{Used in} & \\multicolumn{1}{c}{Fixation Features} & Description \\\\ \\cline{1-2}\nTEX & RAN & & \\\\ \\hline\nN & Y & Fixation duration & Obtained from I-VT result \\\\\nN & N & Standard Deviation (X) & \\begin{tabular}[c]{@{}l@{}}From the screen coordinates\\\\ during fixation\\end{tabular} \\\\\nY & N & Standard Deviation (Y) & '' \\\\\nY & Y & Path length & \\begin{tabular}[c]{@{}l@{}}Length of path traveled in screen\\\\ $Path\\,Length = \\sum\\limits_{i = 1}^{N - 1} {\\sqrt {{{\\left( {{x_{i + 1}} - {x_i}} \\right)}^2} + {{\\left( {{y_{i + 1}} - {y_i}} \\right)}^2}} } $\\end{tabular} \\\\\nY & Y & Angle with previous fixation & \\begin{tabular}[c]{@{}l@{}}Angle with centroid of \\\\ previous fixation\\end{tabular} \\\\\nY & Y & Distance from the last fixation & Euclidean distance from the previous fixation \\\\\nY & Y & Skewness(X) & From Screen coordinates \\\\\nY & Y & Skewness(Y) & '' \\\\\nN & N & Kurtosis(X) & '' \\\\\nY & Y & Kurtosis(Y) & '' \\\\\nY & Y & Dispersion & \\begin{tabular}[c]{@{}l@{}}Spatial spread during a fixation, Computed as\\\\$D= \\left( {\\max (X) - min(X)} \\right) + \\left( {\\max (Y) - min(Y)} \\right)$\\end{tabular} \\\\\nY & Y & Average Velocity & $AV = Path\\,Length/Duration$ \\\\ \\hline\n\\multicolumn{4}{l}{$Y$ and $N$ denote inclusion or exclusion of the feature in the particular stimulus after feature selection}\n\\\\ \\hline\n\\end{tabular}\n\n\\end{table*}\n\n\n\n\n\n\\begin{table*}[!htb]\n\\caption{\\label{Tab:saccade_featuresused}List of features extracted from saccades}\n\\centering\n\\begin{tabular}{llll}\n\\hline\n\\multicolumn{2}{l}{Used in} & \\multicolumn{1}{c}{Saccade Features} & Description \\\\\nTEX & RAN & & \\\\ \\hline\nN & N & Saccadic duration & Obtained from I-VT result \\\\\nY & Y & Dispersion & $D= \\left( {\\max (X) - min(X)} \\right) + \\left( {\\max (Y) - min(Y)} \\right)$, during saccade \\\\\nNYYYYY & NNNYYY & M3S2K(Angular Velocity) & Features from angular velocity \\\\\nYYYYYN & YYYYYY & M3S2K(Angular Acceleration) & Features from angular acceleration \\\\\nY & Y & Standard Deviation(X) & Obtained from screen positions \\\\\nY & Y & Standard Deviation(Y) & '' \\\\\nY & Y & Path length & \\begin{tabular}[c]{@{}l@{}}Distance traveled in screen, \\\\$\\sum\\limits_{i = 1}^{N - 1} {\\sqrt {{{\\left( {{x_{i + 1}} - {x_i}} \\right)}^2} + {{\\left( {{y_{i + 1}} - {y_i}} \\right)}^2}} }$\\end{tabular} \\\\\nY & Y & Angle with previous saccade & \\begin{tabular}[c]{@{}l@{}}Difference in saccadic angle with \\\\ previous saccade\\end{tabular} \\\\\nY & Y & Distance from the previous saccade & \\begin{tabular}[c]{@{}l@{}}Euclidean distance between\\\\ the centroid of the previous\\\\ saccade\\end{tabular} \\\\\n\nY & Y & Saccadic ratio & $SR = \\max (Angular\\,Velocity)/Saccade\\,Duration$ \\\\\nY & Y & Saccade angle & \\begin{tabular}[c]{@{}l@{}}Obtained from fisrt and last points\\\\ as, $saccade\\,angle = {\\tan ^{ - 1}}\\left( {\\frac{{{y_N} - {y_1}}}{{{x_N} - {x_1}}}} \\right)$\\end{tabular} \\\\\nY & Y & Saccade amplitude &Obtained as: $\\sqrt {{{\\left( {{x_N} - {x_1}} \\right)}^2} + {{\\left( {{y_N} - {y_1}} \\right)}^2}} $\\\\\nYYYYYY & YYYYYY & M3S2K(Velocity\\_X\\_direction) & Features from screen positions \\\\\nYYYYYY & YYYYNY & M3S2K(Velocity\\_Y\\_direction) & '' \\\\\nYYYYYY & YYYYYY & M3S2K(Acceleration\\_X\\_direction) & '' \\\\\nYYYYYY & YYNYYY & M3S2K(Acceleration\\_Y\\_direction) & '' \\\\\n& & & \\\\ \\hline\n\\multicolumn{4}{l}{*M3S2K - Statistical features:} \\\\\n\\multicolumn{4}{l}{Mean,Median,Max,Std, Skewness, Kurtosis}\n\\\\ \\hline\n\\multicolumn{4}{l}{$Y$ and $N$ denote inclusion or exclusion of the feature in the particular stimulus after feature selection} \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\n\n\n\n\nThe control mechanisms generating fixations and saccades are different. The number of fixations and saccades is also different in each recording. There is a total of 12 and 46 features extracted from fixations and saccades respectively. A feature normalization scheme is used to scale each feature into a common range to ensure equal contribution in the final classification stage.\n\n\\subsubsection{Feature selection}\n\nThe large number of features extracted may contain redundancy and correlation. A backward feature selection algorithm, as shown in Algorithm 2 is used to retain a minimal set of discriminant features. We use the wrapper-based approach ~\\cite{kohavi1997wrappers} for selecting the features. An RBFN classifier is used for finding the Equal Error Rate (EER) in each iteration. Cross-validation has been carried out in the training set to avoid overfitting. We used a random 50\\% subset of the development dataset for the feature selection algorithm.\nFeature selection algorithm starts with a set of all the features. Now in each iteration, the EER with inclusion and exclusion of a particular feature is found. The feature is retained if the EER with the use of the feature is better than EER with exclusion. The procedure is repeated for all the features in a sequential manner. The feature selection algorithm is\niterated ten times each time on a random 50\\% subset for cross-validation. After these iterations, a set of important features is retained. To evaluate the generalization ability of the selected features, we have tested the algorithm (with the selected features) on an entirely disjoint set\nthat was not used in the feature selection process. The results with the evaluation set ~\\cite{bioeye}(as shown in\nthe public results of BioEye 2015 competition) show the stability and generalization capability of the selected features. The subset of\nfeatures selected were different for different stimuli (TEX and RAN sets). The list of features selected for TEX and RAN stimuli is shown in Table \\ref{Tab:fixation_featuresused} (Fixation features) and Table \\ref{Tab:saccade_featuresused} (Saccade features). The features thus selected are used as inputs to the classification algorithm.\n\n\n\n\\begin{algorithm}[h]\n\\label{alg:backward}\n\\KwData{Feature matrix}\n\\KwResult{featureList[1: Included,0:Excluded]}\n$ N\\leftarrow$ Number of features\\;\n$ featureList\\leftarrow ones(N)$\\;\n\\For {$i \\leftarrow$ 1 \\textbf{to} N} {\n$W \\leftarrow featureList$\\;\n$E \\leftarrow +Inf$\\;\n\\For {$j \\leftarrow$ 0 \\textbf{to} 1} {\n$ W[i] \\leftarrow j$\\;\nT $\\leftarrow$ EER with included features using RBFN\\;\n\\If{$T < E$}{\n$featureList[i]\\leftarrow j$\\;\n$ E \\leftarrow T$\\;\n}\n\n}\n\n}\n\\caption{Backward feature selection}\n\\end{algorithm}\n\nAfter obtaining the set of features from fixations and saccades, we develop a model to represent the data. It has been empirically observed that the performance of classification approaches with Kernel-based methods are better than linear classifiers. It has also been reported that the parameters like amplitude-duration and amplitude-peak velocity may vary with the angle of saccade \\cite{goossens1997human}. The nature of saccade dynamics may be different in different directions as the stimulus is changing randomly at various points on the screen. For each person, saccades of different amplitudes and directions form clusters in the feature space. In order to use the multi-mode nature of the data, we represent them by clustering them in the feature space. Representative vectors from each cluster are used to characterize each person. We use Gaussian Radial Basis Function Network (GRBFN) to model these data. The multiple cluster centers in the feature space are used as representative vectors in this approach. This vectors are selected using the K-means algorithm. Two different RBFNs are trained separately for fixation and saccade. Details about the structure of network and score fusion stage are described in the following section.\n\\subsection{RBF network} \n\\label{sec:rbf}\nRadial Basis Function Network (RBFN) is a class of neural networks initially proposed by Broomhead and Lowe \\cite{broomhead1988radial}. Classification in RBFN is done by calculating the similarity between training and test vectors. Multiple prototype vectors corresponding to each class are stored in each neuron. The Euclidean distance between the input vector and the prototype vector is used to calculate neuron activations.\n\nIn the RBF network, input layer is made of feature vectors. $\\varphi (x)$ is a radial basis function that finds the Euclidean distance between the input vector and the prototype vector. A weighted combination of scores from the RBF layer is used to classify the input into different categories.\n\nThe number of prototypes per class can be defined by the user, and these vectors can be found from the data using different algorithms like K-means, Linde-Buzo-Gray (LBG) algorithm, etc.\n\nThe Gaussian activation function of each neuron is chosen as:\n\n", "itemtype": "equation", "pos": 12720, "prevtext": "\n\n", "index": 3, "text": "\\begin{equation}\n{y_{screen}} = \\left( {{{d * {h_{pix}}} \\over h}} \\right)\\tan ({\\theta _y}) + {{{h_{pix}}} \\over 2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{y_{screen}}=\\left({{{d*{h_{pix}}}\\over h}}\\right)\\tan({\\theta_{y}})+{{{h_{pix%&#10;}}}\\over 2}\" display=\"block\"><mrow><msub><mi>y</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mfrac><mrow><mi>d</mi><mo>*</mo><msub><mi>h</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mrow><mi>tan</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>y</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mfrac><msub><mi>h</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mn>2</mn></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\nwhere,\n$\\mu $ is the mean of the distribution. The parameter $\\beta $ can be found from the data.\n\n\nIn this work, we have used K-means algorithm for selecting the representative vectors. For each\nperson, 32 clusters for fixations and 32 cluster centers for saccades are kept, resulting in $32N$\nclusters for each RBFN (where $N$ is the number of persons in the dataset). The number of clusters\nto keep is obtained empirically. We have clustered the fixations/saccades of each\nindividual separately to obtain a fixed number of representative vectors for each person.\nA maximum of 100 iterations is used to form the clusters. A standard K-means algorithm is used with squared Euclidean distance, and the centers are updated in each\niteration. Each data point is assigned to the closest cluster center obtained from the K-means algorithm. For a particular neuron, the value of $\\beta$ is computed from\nthe distance of all points belonging to that particular cluster as:\n\n", "itemtype": "equation", "pos": 27306, "prevtext": "\nwhere, $d,{\\theta _x}$ and ${\\theta _y}$ denote distance from the screen and visual angles in $x$ and $y$ direction (in radian) respectively. ${x_{screen}}$ and ${y_{screen}}$ denote the position of gaze on the screen. ${w_{pix}},{h_{pix}}$,$w,h$ denote resolution and physical size of the screen in horizontal and vertical directions respectively.\n\n\nRaw eye gaze positions may contain noise. Most of the features used in this work are extracted from velocity and acceleration profiles. The presence of noise makes it difficult to estimate the velocity and acceleration parameters using differentiation operation. Eye movement signals contain high-frequency components, especially during saccades. High-frequency components would be more prominent in velocity and acceleration profiles ~\\cite{harris1984instrument}. Savitzky-Golay filters are useful for\nfiltering out the noise when the frequency span of the signal is large ~\\cite{krishnan2013selection}. They are reported to be optimal ~\\cite{savitzky1964smoothing} for minimizing the least-square error in fitting a polynomial to frames of the noisy data. We use this filter with polynomial order of 6 and frame size of 15 in our approach.\n\n\\subsection{Eye movement classification and feature extraction }\n\\subsubsection{Eye movement classification}\nThe I-VT (velocity threshold) algorithm ~\\cite{holland2012biometric}, ~\\cite{salvucci2000identifying} is used to classify the filtered eye movement data into a sequence of fixations and saccades (Algorithm 1). Most of the earlier works specify the velocity threshold for angular velocity. The angular velocity computed from the filtered data is used to classify the eye movements. A velocity of 50 degrees/second is used as the threshold in I-VT algorithm.\n\n\n\\begin{algorithm}[h]\n\\label{alg:eyeclassification}\n\\KwData{[Time Gazex Gazey]}\n\\KwResult{Res}\n\\textbf{Constants} : VT=Velocity threshold,MDF=Minimum duration for fixation\\;\n\\textbf{States} $=$[FIXATION,SACCADE]\\;\nfixationStart=1\\;\nVelocity=\\textit{smoothDiff}(data)\\;\n$ N\\leftarrow$ Number of samples of data\\;\n\n\\For {$index \\leftarrow$ 1 \\textbf{to} N} {\n\n\\eIf{Velocity[index] $<$ VT}{\ncurrentState=FIXATION\\;\n\\If{lastState $\\neq$ currentState}{\nfixationStart = index\\;\n}\n}{\n\\If{lastState$ =$ FIXATION}{\nduration = data(index,1) - data(fixationStart,1)\\;\n\\If {duration $<$ MDF}{\n\n\\For {$i \\leftarrow$ fixationStart \\textbf{to} index}{\nres[i]= SACCADE;\n}\n}\n}\ncurrentState=SACCADE\\;\n}\nlastState=currentState\\;\nres[index]=currentState\\;\n}\nRes$\\leftarrow$res\\;\n\\caption{Fixation and Saccade classification algorithm}\n\\end{algorithm}\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{pics/gazedata.JPG}\n\\end{center}\n\\caption{Gaze data and stimulus for RAN\\_30min sequence}\n\\label{fig:gazedata}\n\\label{fig:onecol}\n\\end{figure}\nA minimum duration threshold of 100 milliseconds has been chosen to reduce the false positives in\nfixation identification. Algorithm 1 returns the classification results for each data point as either fixation or saccade. Points that are not a part of fixations are considered as saccades in this stage. In the proposed approach, we consider saccades with their\ndurations more than a specified threshold to minimize the effect of spurious saccade segments. From the results of Algorithm 1, a list containing starting index and duration of all fixations and saccades is created. A post-processing stage is carried out to remove small-duration saccades. Saccades with duration less than 12 milliseconds are removed in this stage.\n\n\n\\subsubsection{Feature extraction}\nAfter the removal of small-duration saccades, each eye movement data\nis arranged into a sequence of fixations and saccades. The sequence of gaze locations and corresponding visual angles are also available for each fixation and saccade. Several statistical features are extracted from the position, velocity and acceleration profiles of\nthe gaze sequence. Other features like duration, dispersion, path length and co-occurrence features\nare also extracted for both fixations and saccades. Earlier works ~\\cite{komogortsev2010biometric} suggested that saccades provide a rich amount of information about the dynamics of oculomotor plant. Hence, we extract several other parameters including the saccadic ratio, main sequence, angle, etc. Saccades in horizontal and vertical directions are generated by different areas of the brain \\cite{harwood2008optimally}. We use the statistical properties of the gaze data in $x$ and $y$ directions to incorporate this information. The distance and angle with the previous fixation/saccade are also used as features to leverage the temporal properties. The method used for computation of features is described below.\n\n\nLet $X{\\rm{ }} = {\\rm{ }}\\left\\{ {{x_1},{x_2},{x_3},...,{x_N}} \\right\\}$\nand $Y{\\rm{ }} = {\\rm{ }}\\left\\{ {{y_1},{y_2},{y_3},...,{y_N}} \\right\\}$ denote the set of coordinate positions of gaze in\neach fixation/saccade and let $N$ denotes the number of data points in any fixation or saccade.\n$\\left( {{x_i},{y_i}} \\right)$ denotes gaze location on the screen coordinate system and $\\left( {\\theta _i^x,\\theta _i^y} \\right)$ denotes the corresponding horizontal and vertical visual angles. \n\nA large number of features are extracted from the gaze sequence in each fixation and saccade. Some features are derived from the angular velocity. The differentiation operation for finding velocity and acceleration is carried out using forward difference method on the smoothed data. List of features extracted from fixations and saccades along with the methods of computation are shown in Table \\ref{Tab:fixation_featuresused} and Table \\ref{Tab:saccade_featuresused}. The features are extracted independently for each fixation and saccade.\n\n\n\n\\begin{table*}[htb]\n\n\\caption{\\label{Tab:fixation_featuresused}List of features extracted from fixations}\n\\centering\n\\begin{tabular}{llll}\n\\hline\n\\multicolumn{2}{l}{Used in} & \\multicolumn{1}{c}{Fixation Features} & Description \\\\ \\cline{1-2}\nTEX & RAN & & \\\\ \\hline\nN & Y & Fixation duration & Obtained from I-VT result \\\\\nN & N & Standard Deviation (X) & \\begin{tabular}[c]{@{}l@{}}From the screen coordinates\\\\ during fixation\\end{tabular} \\\\\nY & N & Standard Deviation (Y) & '' \\\\\nY & Y & Path length & \\begin{tabular}[c]{@{}l@{}}Length of path traveled in screen\\\\ $Path\\,Length = \\sum\\limits_{i = 1}^{N - 1} {\\sqrt {{{\\left( {{x_{i + 1}} - {x_i}} \\right)}^2} + {{\\left( {{y_{i + 1}} - {y_i}} \\right)}^2}} } $\\end{tabular} \\\\\nY & Y & Angle with previous fixation & \\begin{tabular}[c]{@{}l@{}}Angle with centroid of \\\\ previous fixation\\end{tabular} \\\\\nY & Y & Distance from the last fixation & Euclidean distance from the previous fixation \\\\\nY & Y & Skewness(X) & From Screen coordinates \\\\\nY & Y & Skewness(Y) & '' \\\\\nN & N & Kurtosis(X) & '' \\\\\nY & Y & Kurtosis(Y) & '' \\\\\nY & Y & Dispersion & \\begin{tabular}[c]{@{}l@{}}Spatial spread during a fixation, Computed as\\\\$D= \\left( {\\max (X) - min(X)} \\right) + \\left( {\\max (Y) - min(Y)} \\right)$\\end{tabular} \\\\\nY & Y & Average Velocity & $AV = Path\\,Length/Duration$ \\\\ \\hline\n\\multicolumn{4}{l}{$Y$ and $N$ denote inclusion or exclusion of the feature in the particular stimulus after feature selection}\n\\\\ \\hline\n\\end{tabular}\n\n\\end{table*}\n\n\n\n\n\n\\begin{table*}[!htb]\n\\caption{\\label{Tab:saccade_featuresused}List of features extracted from saccades}\n\\centering\n\\begin{tabular}{llll}\n\\hline\n\\multicolumn{2}{l}{Used in} & \\multicolumn{1}{c}{Saccade Features} & Description \\\\\nTEX & RAN & & \\\\ \\hline\nN & N & Saccadic duration & Obtained from I-VT result \\\\\nY & Y & Dispersion & $D= \\left( {\\max (X) - min(X)} \\right) + \\left( {\\max (Y) - min(Y)} \\right)$, during saccade \\\\\nNYYYYY & NNNYYY & M3S2K(Angular Velocity) & Features from angular velocity \\\\\nYYYYYN & YYYYYY & M3S2K(Angular Acceleration) & Features from angular acceleration \\\\\nY & Y & Standard Deviation(X) & Obtained from screen positions \\\\\nY & Y & Standard Deviation(Y) & '' \\\\\nY & Y & Path length & \\begin{tabular}[c]{@{}l@{}}Distance traveled in screen, \\\\$\\sum\\limits_{i = 1}^{N - 1} {\\sqrt {{{\\left( {{x_{i + 1}} - {x_i}} \\right)}^2} + {{\\left( {{y_{i + 1}} - {y_i}} \\right)}^2}} }$\\end{tabular} \\\\\nY & Y & Angle with previous saccade & \\begin{tabular}[c]{@{}l@{}}Difference in saccadic angle with \\\\ previous saccade\\end{tabular} \\\\\nY & Y & Distance from the previous saccade & \\begin{tabular}[c]{@{}l@{}}Euclidean distance between\\\\ the centroid of the previous\\\\ saccade\\end{tabular} \\\\\n\nY & Y & Saccadic ratio & $SR = \\max (Angular\\,Velocity)/Saccade\\,Duration$ \\\\\nY & Y & Saccade angle & \\begin{tabular}[c]{@{}l@{}}Obtained from fisrt and last points\\\\ as, $saccade\\,angle = {\\tan ^{ - 1}}\\left( {\\frac{{{y_N} - {y_1}}}{{{x_N} - {x_1}}}} \\right)$\\end{tabular} \\\\\nY & Y & Saccade amplitude &Obtained as: $\\sqrt {{{\\left( {{x_N} - {x_1}} \\right)}^2} + {{\\left( {{y_N} - {y_1}} \\right)}^2}} $\\\\\nYYYYYY & YYYYYY & M3S2K(Velocity\\_X\\_direction) & Features from screen positions \\\\\nYYYYYY & YYYYNY & M3S2K(Velocity\\_Y\\_direction) & '' \\\\\nYYYYYY & YYYYYY & M3S2K(Acceleration\\_X\\_direction) & '' \\\\\nYYYYYY & YYNYYY & M3S2K(Acceleration\\_Y\\_direction) & '' \\\\\n& & & \\\\ \\hline\n\\multicolumn{4}{l}{*M3S2K - Statistical features:} \\\\\n\\multicolumn{4}{l}{Mean,Median,Max,Std, Skewness, Kurtosis}\n\\\\ \\hline\n\\multicolumn{4}{l}{$Y$ and $N$ denote inclusion or exclusion of the feature in the particular stimulus after feature selection} \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\n\n\n\n\nThe control mechanisms generating fixations and saccades are different. The number of fixations and saccades is also different in each recording. There is a total of 12 and 46 features extracted from fixations and saccades respectively. A feature normalization scheme is used to scale each feature into a common range to ensure equal contribution in the final classification stage.\n\n\\subsubsection{Feature selection}\n\nThe large number of features extracted may contain redundancy and correlation. A backward feature selection algorithm, as shown in Algorithm 2 is used to retain a minimal set of discriminant features. We use the wrapper-based approach ~\\cite{kohavi1997wrappers} for selecting the features. An RBFN classifier is used for finding the Equal Error Rate (EER) in each iteration. Cross-validation has been carried out in the training set to avoid overfitting. We used a random 50\\% subset of the development dataset for the feature selection algorithm.\nFeature selection algorithm starts with a set of all the features. Now in each iteration, the EER with inclusion and exclusion of a particular feature is found. The feature is retained if the EER with the use of the feature is better than EER with exclusion. The procedure is repeated for all the features in a sequential manner. The feature selection algorithm is\niterated ten times each time on a random 50\\% subset for cross-validation. After these iterations, a set of important features is retained. To evaluate the generalization ability of the selected features, we have tested the algorithm (with the selected features) on an entirely disjoint set\nthat was not used in the feature selection process. The results with the evaluation set ~\\cite{bioeye}(as shown in\nthe public results of BioEye 2015 competition) show the stability and generalization capability of the selected features. The subset of\nfeatures selected were different for different stimuli (TEX and RAN sets). The list of features selected for TEX and RAN stimuli is shown in Table \\ref{Tab:fixation_featuresused} (Fixation features) and Table \\ref{Tab:saccade_featuresused} (Saccade features). The features thus selected are used as inputs to the classification algorithm.\n\n\n\n\\begin{algorithm}[h]\n\\label{alg:backward}\n\\KwData{Feature matrix}\n\\KwResult{featureList[1: Included,0:Excluded]}\n$ N\\leftarrow$ Number of features\\;\n$ featureList\\leftarrow ones(N)$\\;\n\\For {$i \\leftarrow$ 1 \\textbf{to} N} {\n$W \\leftarrow featureList$\\;\n$E \\leftarrow +Inf$\\;\n\\For {$j \\leftarrow$ 0 \\textbf{to} 1} {\n$ W[i] \\leftarrow j$\\;\nT $\\leftarrow$ EER with included features using RBFN\\;\n\\If{$T < E$}{\n$featureList[i]\\leftarrow j$\\;\n$ E \\leftarrow T$\\;\n}\n\n}\n\n}\n\\caption{Backward feature selection}\n\\end{algorithm}\n\nAfter obtaining the set of features from fixations and saccades, we develop a model to represent the data. It has been empirically observed that the performance of classification approaches with Kernel-based methods are better than linear classifiers. It has also been reported that the parameters like amplitude-duration and amplitude-peak velocity may vary with the angle of saccade \\cite{goossens1997human}. The nature of saccade dynamics may be different in different directions as the stimulus is changing randomly at various points on the screen. For each person, saccades of different amplitudes and directions form clusters in the feature space. In order to use the multi-mode nature of the data, we represent them by clustering them in the feature space. Representative vectors from each cluster are used to characterize each person. We use Gaussian Radial Basis Function Network (GRBFN) to model these data. The multiple cluster centers in the feature space are used as representative vectors in this approach. This vectors are selected using the K-means algorithm. Two different RBFNs are trained separately for fixation and saccade. Details about the structure of network and score fusion stage are described in the following section.\n\\subsection{RBF network} \n\\label{sec:rbf}\nRadial Basis Function Network (RBFN) is a class of neural networks initially proposed by Broomhead and Lowe \\cite{broomhead1988radial}. Classification in RBFN is done by calculating the similarity between training and test vectors. Multiple prototype vectors corresponding to each class are stored in each neuron. The Euclidean distance between the input vector and the prototype vector is used to calculate neuron activations.\n\nIn the RBF network, input layer is made of feature vectors. $\\varphi (x)$ is a radial basis function that finds the Euclidean distance between the input vector and the prototype vector. A weighted combination of scores from the RBF layer is used to classify the input into different categories.\n\nThe number of prototypes per class can be defined by the user, and these vectors can be found from the data using different algorithms like K-means, Linde-Buzo-Gray (LBG) algorithm, etc.\n\nThe Gaussian activation function of each neuron is chosen as:\n\n", "index": 5, "text": "\\begin{equation}\n\\varphi (x) = {e^{ - \\beta {{\\left\\| {x - \\mu } \\right\\|}^2}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\varphi(x)={e^{-\\beta{{\\left\\|{x-\\mu}\\right\\|}^{2}}}}\" display=\"block\"><mrow><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><mi>\u03bc</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\nWhere $\\sigma$ is the mean Euclidean distance of the points (assigned to the specific neuron) from the centroid of the corresponding cluster.\n\n\n\\subsubsection{Notations}\nThe biometric identification problem is similar to a multiclass classification problem. Let there be $n$ samples of a $p$ dimensional data. Assume there are $m$ classes (corresponding to $m$ different individuals) with $c$ samples per class ($n = mc$). Let\n${y_i}$ be the label corresponding to\n${i^{th}}$ sample. Let $K$ be the number of representative vectors from each class. The value of $K$ is chosen empirically ($K=32$).\n\n\\subsubsection{Network learning}\nThe activations can be obtained as:\n$A = {\\varphi _{i,j}}({x_k}),\\,\\,i = 1,...,K,\\,\\,j = 1,...,m,\\,\\,k = 1,...,n$\n\nThe output of the network can be represented as a linear combination of the RBF activations as:\n\n\n", "itemtype": "equation", "pos": 28368, "prevtext": "\nwhere,\n$\\mu $ is the mean of the distribution. The parameter $\\beta $ can be found from the data.\n\n\nIn this work, we have used K-means algorithm for selecting the representative vectors. For each\nperson, 32 clusters for fixations and 32 cluster centers for saccades are kept, resulting in $32N$\nclusters for each RBFN (where $N$ is the number of persons in the dataset). The number of clusters\nto keep is obtained empirically. We have clustered the fixations/saccades of each\nindividual separately to obtain a fixed number of representative vectors for each person.\nA maximum of 100 iterations is used to form the clusters. A standard K-means algorithm is used with squared Euclidean distance, and the centers are updated in each\niteration. Each data point is assigned to the closest cluster center obtained from the K-means algorithm. For a particular neuron, the value of $\\beta$ is computed from\nthe distance of all points belonging to that particular cluster as:\n\n", "index": 7, "text": "\\begin{equation}\n\\beta = {1 \\over {2{\\sigma ^2}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\beta={1\\over{2{\\sigma^{2}}}}\" display=\"block\"><mrow><mi>\u03b2</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\n\nwhere,\n$f(x)$ contains the class membership in vector form. Given the activations and output labels, the objective of the training stage is to find the weight parameters of the output layer. The weights are obtained by minimizing the sum of squared errors.\n\n\n\n\nThe output layer is represented by a linear system as:\n\n", "itemtype": "equation", "pos": 29277, "prevtext": "\nWhere $\\sigma$ is the mean Euclidean distance of the points (assigned to the specific neuron) from the centroid of the corresponding cluster.\n\n\n\\subsubsection{Notations}\nThe biometric identification problem is similar to a multiclass classification problem. Let there be $n$ samples of a $p$ dimensional data. Assume there are $m$ classes (corresponding to $m$ different individuals) with $c$ samples per class ($n = mc$). Let\n${y_i}$ be the label corresponding to\n${i^{th}}$ sample. Let $K$ be the number of representative vectors from each class. The value of $K$ is chosen empirically ($K=32$).\n\n\\subsubsection{Network learning}\nThe activations can be obtained as:\n$A = {\\varphi _{i,j}}({x_k}),\\,\\,i = 1,...,K,\\,\\,j = 1,...,m,\\,\\,k = 1,...,n$\n\nThe output of the network can be represented as a linear combination of the RBF activations as:\n\n\n", "index": 9, "text": "\\begin{equation}\nf(x) = \\sum\\limits_{j = 1}^m {{w_j}{\\varphi _j}(x)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"f(x)=\\sum\\limits_{j=1}^{m}{{w_{j}}{\\varphi_{j}}(x)}\" display=\"block\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c6</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\nThe optimal set of weights can be found using the Moore-Penrose pseudoinverse. \n\n\n\n\nAlternatively, these weights can be learned through gradient descent method.\nIn the learning phase, features extracted from each fixation and saccade are used to train the model. Each fixation/saccade\nis treated as a sample in the training process.\n\nThe method described here uses two-phase learning. RBF layer and weight layer trainings are carried out separately. However a joint training similar to back-propagation is also possible ~\\cite{schwenker2001three}.\n\n\\subsubsection{Training stage}\nOnly the session 1 data from the datasets are used in the training stage. Cluster centers and corresponding $\\beta$ values are computed separately for each person (resulting in $32N$ neurons for both fixation and saccade RBFNs). The output weights (${\\hat w_{fix}}$ and ${\\hat w_{sacc}}$) are found using all fixations and saccades from all the subjects in the dataset.\n\\begin{figure*}[htb]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{pics/rbfncombined3.jpg}\n\\caption{Schematic of the proposed framework.}\n\\label{fig:rbfncombined}\n\\end{figure*}\n\\subsubsection{Testing stage}\nSession 2 data is used in the testing stage. Parameters of RBFN are computed separately for fixations and saccades in the training session. The scores from both RBFNs are combined to obtain the final result. The overall configuration of the scheme is shown in Fig. \\ref{fig:rbfncombined}.\n\n For an unlabeled probe, the activations for each fixation and saccade (${A_{fix}}$ and ${A_{sacc}}$) are found separately using the cluster centers obtained in the training stage.\nThe final classification is carried out using the combined score obtained from all saccades and\nfixations. \n Let $n{}_{fix}$ and $n{}_{sacc}$ be the number of fixations and saccades in an unlabeled gaze sequence.\nThe combined score can be obtained as:\n\n", "itemtype": "equation", "pos": 29678, "prevtext": "\n\nwhere,\n$f(x)$ contains the class membership in vector form. Given the activations and output labels, the objective of the training stage is to find the weight parameters of the output layer. The weights are obtained by minimizing the sum of squared errors.\n\n\n\n\nThe output layer is represented by a linear system as:\n\n", "index": 11, "text": "\\begin{equation}\nA\\hat w = \\hat y\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"A\\hat{w}=\\hat{y}\" display=\"block\"><mrow><mrow><mi>A</mi><mo>\u2062</mo><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\n\nwhere, $\\lambda \\in \\left[ {0\\,1} \\right]$ is the weight used in the score fusion.\nThe parameter $\\lambda$ decides the contribution of fixations and saccades in the final decision stage.\nThis value can be obtained empirically. In the present work, $\\lambda$ value of 0.5 is\nused.\n\nThe label of the unknown sample can be obtained as:\n\n", "itemtype": "equation", "pos": 31610, "prevtext": "\nThe optimal set of weights can be found using the Moore-Penrose pseudoinverse. \n\n\n\n\nAlternatively, these weights can be learned through gradient descent method.\nIn the learning phase, features extracted from each fixation and saccade are used to train the model. Each fixation/saccade\nis treated as a sample in the training process.\n\nThe method described here uses two-phase learning. RBF layer and weight layer trainings are carried out separately. However a joint training similar to back-propagation is also possible ~\\cite{schwenker2001three}.\n\n\\subsubsection{Training stage}\nOnly the session 1 data from the datasets are used in the training stage. Cluster centers and corresponding $\\beta$ values are computed separately for each person (resulting in $32N$ neurons for both fixation and saccade RBFNs). The output weights (${\\hat w_{fix}}$ and ${\\hat w_{sacc}}$) are found using all fixations and saccades from all the subjects in the dataset.\n\\begin{figure*}[htb]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{pics/rbfncombined3.jpg}\n\\caption{Schematic of the proposed framework.}\n\\label{fig:rbfncombined}\n\\end{figure*}\n\\subsubsection{Testing stage}\nSession 2 data is used in the testing stage. Parameters of RBFN are computed separately for fixations and saccades in the training session. The scores from both RBFNs are combined to obtain the final result. The overall configuration of the scheme is shown in Fig. \\ref{fig:rbfncombined}.\n\n For an unlabeled probe, the activations for each fixation and saccade (${A_{fix}}$ and ${A_{sacc}}$) are found separately using the cluster centers obtained in the training stage.\nThe final classification is carried out using the combined score obtained from all saccades and\nfixations. \n Let $n{}_{fix}$ and $n{}_{sacc}$ be the number of fixations and saccades in an unlabeled gaze sequence.\nThe combined score can be obtained as:\n\n", "index": 13, "text": "\\begin{equation}\nscore = \\lambda {1 \\over {{n_{fix}}}}\\sum\\limits_{i = 1}^{{n_{fix}}} {A_{fix}^i{{\\hat w}_{fix}} + \\left( {1 - \\lambda } \\right){1 \\over {{n_{sacc}}}}} \\sum\\limits_{i = 1}^{{n_{sacc}}} {A_{sacc}^i{{\\hat w}_{sacc}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"score=\\lambda{1\\over{{n_{fix}}}}\\sum\\limits_{i=1}^{{n_{fix}}}{A_{fix}^{i}{{%&#10;\\hat{w}}_{fix}}+\\left({1-\\lambda}\\right){1\\over{{n_{sacc}}}}}\\sum\\limits_{i=1}%&#10;^{{n_{sacc}}}{A_{sacc}^{i}{{\\hat{w}}_{sacc}}}\" display=\"block\"><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi></mrow><mo>=</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></munderover><mrow><msubsup><mi>A</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03bb</mi></mrow><mo>)</mo></mrow><mo>\u2062</mo><mfrac><mn>1</mn><msub><mi>n</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>c</mi></mrow></msub></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>c</mi></mrow></msub></munderover><mrow><msubsup><mi>A</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>c</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>c</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03333.tex", "nexttext": "\n\n\n\n\n\\section{Experiments and results}\n\\begin{table*}[!htb]\n\\caption{\\label{datasetdetails} Details about the database}\n\\centering\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\n\\textbf{Dataset Name} & \\textbf{RAN\\_30min} & \\textbf{RAN\\_1year} & \\textbf{TEX\\_30min} & \\textbf{TEX\\_1year} \\\\ \\midrule\nNumber of subjects & 153 & 37 & 153 & 37 \\\\ \\midrule\nStimulus & \\begin{tabular}[c]{@{}l@{}}White dot moving\\\\ in a dark\\\\  background\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}White dot moving \\\\ in a dark\\\\ background\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Text \\\\ excerpt\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Text \\\\ excerpt\\end{tabular} \\\\ \\midrule\nDuration of experiment & 100 seconds & 100 seconds & 60 seconds & 60 seconds \\\\ \\midrule\n\\begin{tabular}[c]{@{}l@{}}Interval between\\\\ training\\\\ and testing data\\end{tabular} & 30 minutes & 1 year & 30 minutes & 1 year \\\\ \\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\n\\subsection{Datasets}\nThe data used in this paper are part of the development phase of BioEye 2015 ~\\cite{bioeye} competition. Data recorded in three different sessions are available. First two sessions are separated by a time interval of 30 minutes containing recordings of 153 subjects (ages 18-43). A third session, conducted after 1 year, (37 subjects) is also available to evaluate the robustness against template aging. The database contains gaze sequences obtained using two distinct types of visual stimuli. In one set (RAN), a white dot moving in a dark background was used as the stimulus. The subjects were asked to follow the dot. Text excerpt shown on the screen was used as the stimulus in the other set (TEX). The samples were recorded with an EyeLink eye-tracker (with a reported spatial accuracy of 0.5 degrees) at 1000 Hz and down-sampled to 250 Hz with anti-aliasing filtering. The development dataset contains the ground truth about the identity of the persons. An additional evaluation set is also available without ground truth. \n\nIn each recording, visual angles in $x$ and $y$ direction, stimulus angle in $x$ and $y$ direction and information regarding the validity of the samples are available. Details about the stimulus types in BioEye2015 database are given below.\n\\subsubsection{Random dot stimulus (RAN\\_30min \\& RAN\\_1year)}\nThe stimulus used was a white dot appearing at random locations on a black computer screen. The position of the stimulus would change every second. The subjects were asked to follow the dot on the screen and recording was carried out for 100 seconds.\n\\subsubsection{Text stimulus (TEX\\_30min \\& TEX\\_1year)}\nThe task, in this case, was reading text excerpts from the poem of Lewis Carroll ``The Hunting of the Snark''. The duration of this experiment was 60 seconds.\n\n A comprehensive list of the datasets and parameters are shown in Table \\ref{datasetdetails}.\n\n\\subsection{Evaluation metrics}\nThe proposed algorithm has been evaluated in the labeled development set. Rank-1 accuracy and EER are used for evaluating the algorithm. Rank-1 (R1) accuracy is defined as the ratio of the total number of correct detections to the number of samples used. EER is the percentage at which False Acceptance Rate (FAR) and False Rejection Rate (FRR) are equal. Detection Error Trade-off (DET) curves are shown for all the datasets. Rank(\\textit{n}) accuracy is the number of correct detections in the top $n$ candidates. Cumulative match characteristics (CMC) is the cumulative plot of rank(n) accuracy. CMC curves are also plotted for all the four datasets. The evaluation set in the BioEye2015 dataset is unlabeled. However, we report the R1 accuracy as obtained from the public results ~\\cite{bioeye} of the competition.\n\\subsection{Results}\n\\subsubsection{Performance in the development datasets}\nThe model was trained using 50\\% of data in the development datasets.\nWe have trained and tested the algorithm on completely disjoint sessions to test its generalization ability. For example, in RAN\\_30min\nsequence there are 153 samples available for two different sessions. We have trained the\nAlgorithm only on the first session (using a random 50\\% subset of the data). The evaluation was\ncarried out on the session 2 data. We have not used the data from the same session for training\nand testing since it won't account for intersession variability. \n\nThe average R1 accuracy and EER were calculated from random 50\\% subsets of development datasets. This procedure was repeated 100 times and the average R1 accuracy and EER were\nobtained. The results obtained along with the standard deviations are given in Table \\ref{resultsdev}.\n\n\\begin{figure*}[!htb]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{pics/ran30_march26_dev.jpg}\n\\caption{DET curve for (a) RAN\\_30min and (b) TEX\\_30min}\n\\label{fig:eer_30_dev}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{pics/ran1yr_march26_dev.jpg}\n\\caption{DET curve for (a) RAN\\_1year and (b) TEX\\_1year}\n\\label{fig:eer_1yr_dev}\n\\end{figure*}\n\n\nThe R1 accuracy in RAN\\_30min and TEX\\_30min databases are above 90\\% indicating the robustness of the proposed framework. The EER on RAN\\_30min database is found out to be 2.59\\%, comparable to the accuracy levels of fingerprint (2.07\\% EER) \\cite{maio2004fvc2004}, voice recognition systems, and facial geometry (15\\% EER) ~\\cite{phillips2010frvt} biometrics.\n\n\\begin{table}[h]\n\\centering\n\\caption{\\label{resultsdev} Results in the development datasets}\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\n& RAN\\_30 & RAN\\_1yr & TEX\\_30 & TEX\\_1yr \\\\ \\midrule\nR1 & 90.10$\\pm$2.76 & 79.31$\\pm$6.86 & 92.38$\\pm$2.56 & 83.41$\\pm$6.98 \\\\\nEER & 2.59$\\pm$0.71 & 10.96$\\pm$4.59 & 3.78$\\pm$0.77 & 9.36$\\pm$3.49 \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\nR1 accuracy (Table \\ref{comparedev}) of the proposed algorithm obtained from the development set was compared with the baseline algorithm (CEM-B) \\cite{holland2013complexa}.\nThe average cumulative matching characteristics curves for the four datasets are shown in Fig. \\ref{fig:cmc_30_dev} and Fig. \\ref{fig:cmc_1yr_dev}.\n\n\\begin{figure*}[!htb]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{pics/CMC_rantext30.JPG}\n\\caption{CMC curve for (a) RAN\\_30min and (b) TEX\\_30min}\n\\label{fig:cmc_30_dev}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{pics/CMC_1year.JPG}\n\\caption{CMC curve for (a) RAN\\_1year and (b) TEX\\_1year}\n\\label{fig:cmc_1yr_dev}\n\\end{figure*}\n\n\n\n The Detection Error Trade-off (DET) curves for the development datasets are shown in Fig. \\ref{fig:eer_30_dev} and Fig. \\ref{fig:eer_1yr_dev}. In Fig. \\ref{fig:eer_30_dev} (a) and (b), FNR becomes very small as FPR increases indicating a good separation from\nimpostors. The reduction in FNR may be because of the addition of scores of all the fixations and saccades\nin the score fusion stage. Impostor scores are considerably smaller than genuine scores in the proposed approach. The performance in\n1-year sessions is poor compared to 30-minutes sessions indicating template aging effects.\n\n\n\\begin{table}[h]\n\\centering\n\\caption{\\label{comparedev} Comparison of R1 accuracy in the entire development dataset}\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\n& RAN\\_30 & RAN\\_1yr & TEX\\_30 & TEX\\_1yr \\\\ \\midrule\nOur Method & 89.54\\% & 81.08\\% & 85.62\\% & 78.38\\% \\\\\n\\begin{tabular}[c]{@{}l@{}} Baseline \\end{tabular} & 40.52\\% & 16.22\\% & 52.94\\% & 40.54\\% \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\\subsubsection{Performance in the evaluation sets}\nThe evaluation part of the database is unlabeled. However, the results of the competition are available on the website ~\\cite{bioeye}.  The evaluation set of the dataset had only one unlabeled data for every labeled sample. We have used this one to one correspondence assumption in the final stage of the algorithm.\n\nLet there be $n$ labeled and $n$ unlabeled recordings. The task is to assign each unlabeled file to a labeled file.  The scores obtained from RBF output stage were stored in a matrix $D$ (with dimension $n$x$n$).  $D(i,j)$ denotes the normalized similarity score  between ${i^{th}}$ labeled and ${j^{th}}$ unlabeled samples. We have selected the best match for each unlabeled recording using Algorithm 3.  The use of this one to one assumption improved the results. However, this assumption may not be suitable for practical biometric identification/verification scenarios. The proposed method has been found to outperform all the other methods even without the one to one assumption indicating the robustness for biometric applications. The results with and without this assumption are shown in Table \\ref{compareeval}. \n\\begin{algorithm}[h]\n\\label{alg:onetoone}\n\\KwData{$D$ (Score matrix)}\n\\KwResult{Matches}\n$[n,n]=size(D)$\\;\n\n\n\n\n\\For {$i \\leftarrow$ 1 \\textbf{to} n} {\n$[row,col]=find(D==max(D(:)))$\\;\n$D(row,:)=-\\infty $\\;\n$D(,:col)=-\\infty $\\;\npair=$[row,col]$\\;\nMatches.\\textit{append}(pair)\\;\n\n\n\n\n}\n\\caption{One to one matching}\n\\end{algorithm}\n\n\n\n\\begin{table}[h]\n\\centering\n\n\\caption{\\label{compareeval} Comparison of R1 accuracy with baseline method in evaluation dataset}\n\\label{my-label}\n\\begin{tabular}{@{}lllll@{}}\n\n\\toprule\n& RAN\\_30 & RAN\\_1yr & TEX\\_30 & TEX\\_1yr \\\\ \\midrule\n\\begin{tabular}[c]{@{}l@{}}Our Method \\end{tabular} & 93.46\\% & 83.78\\% & 89.54\\% & 83.78\\% \\\\\n\\begin{tabular}[c]{@{}l@{}}Our Method* \\end{tabular} & 98.69\\% & 89.19\\% & 98.04\\% & 94.59\\% \\\\\nBaseline & 33.99\\% & 40.54\\% & 58.17\\% & 48.65\\% \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n\n\\subsection{Computational complexity}\nThe algorithm has been implemented in an Intel Core i5 CPU, 3.33 GHz PC with 4 GB RAM. The average training time for the network without code optimization (single-threaded) in MATLAB is about 400 seconds (with 153 samples). In the testing phase, for predicting one unlabeled recording, it takes on an average 0.21 seconds (in TEX\\_30min). The time taken for training and testing phase can be improved considerably by implementation in C, C++, using parallel processing platforms like Graphical Processing Units (GPU).\n\\subsection{ Discussions}\n\\subsubsection{Performance of the algorithm}\nThe R1 accuracy of the proposed method is high in both TEX and RAN datasets, which indicates the possibility of developing a task-independent biometric system. The EER and R1 accuracy achieved show the robustness of the proposed score fusion approach. The selected features show good discrimination ability in both stimuli. The accuracy with 1-year datasets is comparatively lesser than that with the 30-minute datasets.\n This lower accuracy may be attributed to template aging effects.  \n Some of the selected features may show variability over time ~\\cite{komogortsev2014template} ~\\cite{kasprowski2013impact}. \n\nThe feature selection was carried out in 30-minute datasets due to the availability of a large number of subjects.  Feature selection with 1-year datasets may lead to overfitting because of fewer subjects. This issue can be solved by using the feature selection in 1-year datasets with a larger number of subjects, which may identify features that are robust against template aging.  However, the results show significant improvement compared to the state of the art methods. The proposed algorithm was ranked first in the BioEye 2015 ~\\cite{bioeye} competition.\n\\subsubsection{Limitations}\nControlled experimental setup was used to collect the data used in this work. The sampling rate and quality of data used in the present work were very high since it was collected in lab conditions using chinrest. Accurate estimation of the features in noisy, low sampling rates is necessary for the use in a practical biometric scenario. The nature of eye movements may be affected by the level of alertness, fatigue, emotions, cognitive loading, etc. Consumption of caffeine and alcohol by the subjects may affect the performance of the proposed algorithm. The features selected for biometrics should be invariant to such variations. Only two sessions of data were available for each subject. Intersession variability and template aging effects need to be studied further. Lack of publicly available databases containing a large number of samples (to account for template aging, uncontrolled environment, affective states, intersession variability) is another problem. Creation of a large database with such variability could provide more robust solutions. \n \n\n \n\\section{Conclusions}\nThis work proposes a novel framework for biometric identification based on dynamic characteristics of eye movements. The raw eye movement data is classified into a sequence of fixations and saccades. We extract a large set of features from fixations and saccades to characterize each individual. The important features extracted from fixations and saccades are identified based on a backward selection framework. Two different Gaussian RBF networks are trained using features from fixations and saccades separately. In the detection phase, scores obtained from both RBF networks are used to get the subject's identity. The high accuracy obtained shows the robustness of the proposed algorithm. The proposed framework can be easily integrated into the existing iris recognition systems. A combination of the proposed approach with conventional iris recognition systems may give rise to a new counterfeit-resistant biometric system. The comparable accuracy in distinct types of stimuli indicates the possibility of developing a task-independent system for eye movement biometrics. The proposed method can also be used for continuous authentication in desktop environments. Robustness of the algorithm against lower sampling rates, calibration error and noise can be explored in future. The effect of duration of data on the level of accuracy is another topic to be investigated. \n\n\n\\ifCLASSOPTIONcompsoc\n\n  \\section*{Acknowledgments}\n\\else\n\n  \\section*{Acknowledgment}\n\\fi\n\n\nThe authors would like to thank the organizers of BioEye 2015 competition for providing the data.\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\\bibliographystyle{myIEEEtran}\n\\bibliography{refsnew}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{pics/anjith.jpg}}]{Anjith George}\nhas received his B.Tech. (Hons.)\ndegree from Calicut University, India in 2010. and\nM-Tech degree from Indian Institute of Technology\n(IIT) Kharagpur, India in 2012. Presently he is\npursuing Ph.D. from IIT Kharagpur. Kharagpur. His\ncurrent research interests include real time computer\nvision and its applications.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{pics/aroutray.jpg}}]{Aurobinda Routray}\nis a professor in the\nDepartment of Electrical Engineering, Indian\nInstitute of Technology, Kharagpur. His research\ninterest includes non-linear and statistical signal\nprocessing, signal based fault detection and\ndiagnosis, real time and embedded signal processing,\nnumerical linear algebra, and data driven\ndiagnostics.\n\\end{IEEEbiography}\n\n\\vfill\n\n\n\n\n\n", "itemtype": "equation", "pos": 32190, "prevtext": "\n\nwhere, $\\lambda \\in \\left[ {0\\,1} \\right]$ is the weight used in the score fusion.\nThe parameter $\\lambda$ decides the contribution of fixations and saccades in the final decision stage.\nThis value can be obtained empirically. In the present work, $\\lambda$ value of 0.5 is\nused.\n\nThe label of the unknown sample can be obtained as:\n\n", "index": 15, "text": "\\begin{equation}\nlabel = \\mathop {\\arg \\max }\\limits_m (score)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"label=\\mathop{\\arg\\max}\\limits_{m}(score)\" display=\"block\"><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi></mrow><mo>=</mo><mrow><munder><mrow><mi>arg</mi><mo movablelimits=\"false\">\u2061</mo><mi>max</mi></mrow><mi>m</mi></munder><mrow><mo stretchy=\"false\">(</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]