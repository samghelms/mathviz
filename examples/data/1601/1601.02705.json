[{"file": "1601.02705.tex", "nexttext": "\n{\\textcolor{Black}{{For instance, given the handle of the espresso machine in Figure~\\ref{fig:espresso_transfer} and an natural language instruction `Push down on the handle to add hot water', the algorithm should output a manipulation trajectory that will correctly accomplish the task on the object part according to the instruction. }}}\n\n\n\\noindent\n\\textbf{Point-cloud Representation.}\nEach instance of a point-cloud $p \\in \\mathcal{P}$ is represented as a set of $n$ points in three-dimensional\nEuclidean space where each point $(x,y,z)$ is represented with its RGB color $(r,g,b)$:\n\n", "itemtype": "equation", "pos": 26073, "prevtext": "\n\n\n\n\\iffalse\n\\noindent\nStuff that's new in IJRR version:\n\\begin{itemize}\n\\item actual details of all baselines (SSVM, LSSVM, tasksimilarity) of ISRR \n\\item details of robotic experiment and video\n\\item noise handling related baselines for Embedding model\n\\item (minor) details of hyper-parameters for both metric and deep learning\n\\item {\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {different similarity?}}]}}{}}\n\\item {\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {different pretraing? violating pc?}}]}}{}}\n\\item {\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {how about adding oracle?}}]}}{}}\n\\item {\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {maybe 3d conv and word2vec}}]}}{}}\n\\item {\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {shoot video of robobarista}}]}}{}}\n\\end{itemize}\n\n\\bigskip\n\\noindent \\textbf{USE FOLLOWING FOR ANY (NON-MERGE) CHANGES!!!!!!!!!!!}\\\\\n\\textcolor{red}{\\textbf{\n\\textbackslash jae{} \\textbackslash jaenote{} \n\\textbackslash jin{}  \\textbackslash jinnote{} \n\\textbackslash ian{} \\textbackslash iannote{}\n}}\n\n\\bigskip\n\\bigskip\n\\noindent \\textbf{--------  UPDATES --------------------------------------}\n\\bigskip\n\n\\textbf{NOV 18th (Wed):}\nI have pretty much finished merging except experiment/result section.\nJin and Ian, please try to make a pass from beginning making sure merged story/contents \n(as well as the order of contents) all make sense. -Jae\n\n\\textbf{NOV 24th (Tues):}\nMerging is done. I will try to fill in missing numbers in tables.\nFill free to edit/comment any section.\n\n\\newpage\n\\fi\n\n\n\n\\maketitle\n\\thispagestyle{empty}\n\\pagestyle{empty}\n\n\n\n\n\n\n\n\n\\begin{abstract}\nThere is a large variety of objects and appliances in human environments, such as stoves, \ncoffee dispensers, juice extractors, and so on.  It is challenging for a roboticist to program a \nrobot for each of these object types and for each of their instantiations.\nIn this work, we present a novel approach to manipulation planning\nbased on the idea that many household objects share similarly-operated object parts.\n\nWe formulate the manipulation planning as a structured prediction problem\n\n\nand learn to transfer manipulation strategy across different objects\nby embedding point-cloud, natural language, and manipulation trajectory data \ninto a shared embedding space using a deep neural network. \nIn order to learn semantically meaningful spaces throughout our network, \nwe introduce a method for pre-training its lower layers for multimodal feature embedding \nand a method for fine-tuning this embedding space using a loss-based margin.\nIn order to collect a large number of manipulation demonstrations for different objects,\nwe develop a new crowd-sourcing platform called Robobarista.\nWe test our model on our dataset consisting of 116 objects and appliances with 249 parts\nalong with 250 language instructions, for which there are 1225 crowd-sourced \nmanipulation demonstrations.\nWe further show that our robot with our model can even prepare a cup of a latte\nwith appliances it has never seen before.\\footnote{Parts of this work were presented at ISRR 2015 (\\citet{sung_robobarista_2015})}\n\n\n\n\n\n\n\n\n\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\n\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=\\columnwidth,trim={2.8cm 0cm 2.8cm 1.7cm},clip]{fig1_v5.pdf}\n  \\end{center}\n  \\vskip -.1in\n  \\caption{ \n  \\textbf{First encounter of an espresso machine} by our PR2 robot. Without ever having seen the machine before,  given language instructions and a point-cloud from Kinect sensor, our robot is \n  capable of finding appropriate  manipulation trajectories from prior experience using our deep multimodal embedding model.}\n  \\label{fig:robobarista_main}\n\\end{figure}\n\n\nConsider the espresso machine in Figure~\\ref{fig:robobarista_main} ---\neven without having seen this machine before, a person can prepare a cup of latte\nby visually observing the machine and\nreading an instruction manual. \nThis is possible because humans have vast prior experience with manipulating\ndifferently-shaped objects that share common parts such as `handles' and `knobs.'\nIn this work, our goal is to give robots the same capabilites -- specifically,\nto enable robots to generalize their manipulation ability to novel objects and tasks (e.g. toaster, sink, water fountain, toilet, soda dispenser, etc.).\n\n\n\n\n\n\n{\\textcolor{Black}{{We build an algorithm that uses a large knowledge base of manipulation demonstrations to infer an appropriate manipulation trajectory for a given pair of point-cloud and natural language instructions.}}}\n\n\\iffalse\n\nConsider a robot manipulating a new appliance in a home kitchen in Figure~\\ref{fig:espresso_transfer}. \nEven though the robot may never have seen \nthis object before, it should be able to use an instruction\nmanual in natural language \nalong with its observations of the object \nto infer how the object should be manipulated.\nThis ability to fuse information from different input modalities\nand map them to actions is extremely useful to a household robot.\n\nIf the robot has prior experience with similar object parts and instructions, \nit should understand how these \ncorrespond to actions \\citep{sung_robobarista_2015}, \nand be able to extend this to new environments. For example, it\nshould understand that all ``rotate clockwise'' operations on vertical knobs\nattached to a vertical surface map to similar manipulation motions. \n\\fi\n \n\nIf the robot's sole task is to manipulate one specific espresso machine or \njust a few types of `handles', \na roboticist could manually program the exact sequence to be executed.\nHowever, human environments contain a huge variety of objects, which makes this approach\nun-scalable and infeasible.\nClassification of objects or object parts (e.g. `handle') alone does not provide enough information for robots to\nactually manipulate them, {\\textcolor{Black}{{since semantically-similar objects or parts might be operated completely differently -- consider, for example, manipulating the `handle' of a urinal, as opposed to a door `handle'.}}}\nThus, rather than relying on scene understanding techniques \\citep{blaschko2008learning,li2009towards,girshick2011object},\nwe directly use 3D point-clouds for manipulation planning\nusing machine learning algorithms.\n\n\nThe key idea of our work is that objects designed for use by humans\nshare many similarly-operated \\emph{object parts} such as `handles', `levers', `triggers', and `buttons';\nthus, manipulation motions can be transferred even between completely different objects\nif we represent these motions  with respect to these parts.\nFor example, even if the robot has never seen an espresso machine before, it should be able to\nmanipulate it if it has previously seen similarly-operated parts \nof other objects such as a urinal, soda dispenser, or restroom sink,\nas illustrated in Figure~\\ref{fig:espresso_transfer}.\nObject parts that are operated in similar fashion may not {\\textcolor{Black}{{necessarily}}} carry the same part name (e.g. `handle')\nbut should have some similarity in their shapes that allows motions to be transferred\nbetween completely different objects. \n\n\n\n\n{\\textcolor{Black}{{Going beyond manipulation based on simple semantic classes is a significant challenge which we address in this work. Instead, we must also make use of visual information (point-clouds), and a  natural language instruction telling the robot what to do since many possible affordances can exist for the same part. Designing useful features for either of these modalities alone is already  difficult, and designing features which combine the two for manipulation purposes is extremely challenging.}}}\n\n\n\n\n\n\n\n\n\n\n\nObtaining a good common representation between different modalities is\nchallenging for two main reasons. First, each modality might intrinsically have\nvery different statistical properties -- for example, most trajectory\nrepresentations are inherently dense, while a bag-of-words representation of\nlanguage is by nature sparse. This makes it challenging to apply algorithms\ndesigned for unimodal data, as one modality might overpower the others.\nSecond, even with expert knowledge, it is extremely challenging to design\njoint features between such disparate modalities. \n\n\nHumans are able to map similar concepts from different sensory system to the same concept using \n\\emph{common representation} between different modalities \\citep{erdogan2014transfer}.\n\n\n\nFor example, we are able to correlate the appearance with feel of a banana, \nor a language instruction with a real-world action.\nThis ability to fuse information from different input modalities\nand map them to actions is extremely useful to a household robot.\n\n\\iffalse\n\nThis ability to fuse information from different input modalities\nand map them to actions is extremely useful to a household robot.\nEven though similar concepts might appear very differently in\ndifferent sensor modalities, \nhumans are able to understand that they map to the same concept.\nFor example, we are able to\ncorrelate the appearance with feel of a banana, \nor a natural language instruction with a real-world action.\nThere is a strong evidence that we do so through a\n\\emph{common representation} between different\nmodalities \\citep{erdogan2014transfer}.\n\\fi\n\n\n\n\nIn this work, we use deep neural networks to learn a shared embedding \nbetween the combination of object parts in the environment (point-cloud) and natural language instructions, \nand manipulation trajectories (Fig.~\\ref{fig:main_fig}). \nThis means that all three modalities are projected to the\n\\emph{same} feature space.\nWe introduce an algorithm that learns to pull semantically\nsimilar environment/language pairs and their corresponding trajectories \nto the same\nregions, and push environment/language pairs away from irrelevant trajectories\nbased on how irrelevant these trajectories are.\nOur algorithm allows for efficient inference because,\ngiven a new instruction and point-cloud,\nwe only need to \nfind the nearest trajectory to the projection of this pair in the learned embedding space, which\ncan be done using fast \nnearest-neighbor algorithms \\citep{flann_pami_2014}.\n\n\n\nIn the past, deep learning methods have shown impressive results\nfor learning features in a wide variety of domains \n\\citep{krizhevsky2012imagenet,socher2011semi,hadsell2008deep}\nand even learning cross-domain embeddings for, for example, language\nand image features \\citep{srivastava2012multimodal}. \nIn contrast to these existing methods, here we present a new\npre-training algorithm for initializing networks to be used for\njoint embedding of different modalities. \n\n\n\n\n\n\nSuch deep learning algorithms require a large dataset for training. \nHowever, collecting a large enough dataset \nof expert demonstrations on a large number of objects\nis very expensive as \nit requires joint physical presence of the robot, an expert, and the object to be manipulated.\nIn this work, we show that we can crowd-source the collection of manipulation demonstrations to the\npublic over the web through our Robobarista platform. \nSince crowd-sourced demonstrations might be noisy and\nsub-optimal, we present a new learning algorithm\nwhich handles this noise.\n{\\textcolor{Black}{{With our noise handling algorithm, our model trained with crowd-sourced demonstrations outperforms the model trained with expert demonstrations, even with the significant amount of noise in crowd-sourced manipulation demonstrations.}}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ Furthermore, in contrast to previous approaches based on learning from demonstration (LfD) that learn a mapping from a state to  an action \\citep{argall2009survey}, our work complements LfD as we focus on the entire manipulation motion, as opposed to a sequential state-action mapping. }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur Robobarista web-based crowdsourcing platform ({\\small \\url{http://robobarista.cs.cornell.edu}}) allowed us to\ncollect a large dataset of \\emph{116 objects} \nwith \\emph{250 natural language instructions}\nfor which there are \\emph{1225 crowd-sourced manipulation trajectories} from 71 non-expert users,\nwhich we use to validate our methods.\nWe also present experiments on our robot using our approach.\nIn summary, the key contributions of this work are:\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ \\begin{itemize} \\item We present a novel approach to manipulation planning via \\textit{part-based transfer} between different objects that allows manipulation of novel objects. \\item We introduce a new \\textit{deep learning model} that handles three modalities with noisy labels from crowd-sourcing. \\item We introduce a new algorithm, which learns an \\textit{embedding space} while enforcing a varying and \\textit{loss-based margin}, along with a new unsupervised pre-training method which outperforms standard pre-training algorithms \\citep{SAE}. \\item We develop an online platform which allows the   incorporation of \\textit{crowd-sourcing} to manipulation planning and introduces a large-scale manipulation dataset. \\item We evaluate our algorithms on this dataset, showing significant improvement over other state-of-the-art methods. \\end{itemize} }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Related Work}\n\nImproving robotic perception and teaching\nmanipulation strategies to robots has been\na major research area in recent years.\nIn this section, we describe related work in\nvarious aspects of learning to manipulate novel\nobjects.\n\n\\iffalse\n{\\textcolor{Black}{{There is a lot of recent work in improving robotic perception and teaching manipulation strategy to robots. We describe related work in various aspects of learning to manipulate novel objects.}}}\n\\fi\n\n{\\smallskip\\noindent\\textbf{{Scene Understanding.}}}\nIn recent years, there has been significant research focus on semantic scene understanding\n\\cite{li2009towards,koppula2011semantic,krizhevsky2012imagenet,wu2014_hierarchicalrgbd},\nhuman activity detection \\cite{sung_rgbdactivity_2012,Hu2014humanact},\nand features for RGB-D images and point-clouds \\cite{socher2012convolutional,lai_icra14}.\nSimilar to our idea of using part-based transfers, \nthe deformable part model \\cite{girshick2011object} was effective\nin object detection.\nHowever, classification of objects, object parts, or human activities alone\ndoes not provide enough information for a robot to reliably plan manipulation.\nEven a simple category such as `kitchen sinks' has a huge amount of variation in how \ndifferent instances are manipulated -- for example, handles and knobs must be manipulated differently,\nand different orientations and positions of these parts require very different strategies such as\npulling the handle upwards, pushing upwards, pushing sideways, and so on.\nOn the other hand, direct perception approaches \\cite{gibson1986ecological,kroemer2012kernel}\nskip the intermediate object labels\nand directly perceive affordances based on the shape of the object.\nThese works focus on detecting the part known to afford certain actions,\nsuch as `pour,' given the object,\nwhile we focus on predicting the correct motion given the object part.\n\n\n\n{\\smallskip\\noindent\\textbf{{Manipulation Strategy.}}}\nMost works in robotic manipulation focus on task-specific\nmanipulation of \\emph{known} objects---for example, \nbaking cookies with known tools \\cite{bollini2011bakebot}\nand folding the laundry \\cite{miller2012geometric} --\nor focus on learning specific motions such as grasping \\cite{kehoe2013cloud}\nand opening doors \\cite{endres2013learning}.\nOthers \\cite{sung_synthesizingsequences_2014,misra2014tellme} focus on sequencing manipulation tasks assuming \nperfect manipulation primitives such as \\textit{grasp} and \\textit{pour} are available.\nInstead, here, we use learning to generalize to manipulating\n\\emph{novel} objects never seen before by the robot, without relying on preprogrammed \nmotion primitives.\n\n\n\n\n\n\n\n\n\n\n\n\nFor the more general task of manipulating new instances of objects, previous approaches rely \non finding articulation \\cite{sturm2011probabilistic,pillai2014articulated} \nor using interaction \\cite{katz2013interactive}, but they are limited by tracking performance of\na vision algorithm.\nMany objects that humans operate daily have small parts such as `knobs', which leads to\nsignificant occlusion as manipulation is demonstrated.\nAnother approach using part-based transfer between objects has been shown to be successful\nfor grasping \\cite{dang2012semantic,detry2013learning}.\nWe extend this approach and introduce a deep learning model that\nenables part-based transfer of \\textit{trajectories}\nby automatically learning relevant features.\nOur focus is on the generalization of manipulation trajectory via part-based transfer\nusing point-clouds without knowing objects a priori\nand without assuming any of the sub-steps (`approach', `grasping', and `manipulation').\n\n\n\n\n{\\textcolor{Black}{{ A few recent works use deep learning approaches for  robotic manipulation. \\citet{levine2015manipulation} use a Gaussian mixture model to learn system dynamics, then use these to learn  a manipulation policy using a deep network.  \\citet{lenz2015deepmpc} use a deep network to learn system dynamics for real-time model-predictive control. Both these works focus on learning low-level controllers, whereas here we learn high-level manipulation trajectories. }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\smallskip\\noindent\\textbf{{Learning from Demonstration.}}}\nSeveral successful approaches for teaching robots tasks,\nsuch as helicopter maneuvers \\cite{abbeel2010autonomous}\nor table tennis \\cite{mulling2013learning},\nhave been based on Learning from Demonstration (LfD) \\cite{argall2009survey}.\nAlthough LfD allows end users to demonstrate a manipulation task by simply taking \ncontrol of the robot's arms,\nit focuses on learning individual actions and separately relies on high level task composition \n\\cite{mangin2011unsupervised, daniel2012learning}\nor is often limited to previously seen objects \\cite{phillipslearning, pastor2009learning}.\nWe believe that learning a single model for an action like `turning on'\nis impossible because human environments have so many variations.\n\n\nUnlike learning a model from demonstration,\ninstance-based learning \\cite{aha1991instance,forbes2014robot} replicates\none of the demonstrations.\nSimilarly, we directly transfer one of the demonstrations, but\nfocus on generalizing manipulation planning to completely new objects,\nenabling robots to manipulate objects they have never seen before.\n\n\n{\\smallskip\\noindent\\textbf{{Metric Embedding.}}}\nSeveral works in machine learning make use of the power of shared embedding spaces.\nLMNN \\cite{weinberger2005distance} learns a max-margin Mahalanobis distance \n\nfor a unimodal input feature space.\n\\citet{weston2011wsabie} learn linear mappings from image and language features to a common embedding \nspace for automatic image annotation. \\citet{moore2012playlist} learn to map songs and natural language\ntags to a shared embedding space.\nHowever, these approaches learn only a shallow, linear mapping from input features, whereas here we learn\na deep non-linear mapping which is less sensitive to input representations.\n\n\n\n{\\smallskip\\noindent\\textbf{{Deep Learning.}}}\nIn recent years, deep learning algorithms have enjoyed huge successes, particularly in the \ndomains of\nof vision and natural language processing (e.g. \\cite{krizhevsky2012imagenet,socher2011semi}).\nIn robotics, deep learning has previously  been  successfully used\nfor detecting grasps for novel objects in multi-channel RGB-D images \\cite{lenz2013deep}\nand for classifying terrain from long-range vision \\cite{hadsell2008deep}.\n\n\n\n\n\n\n\n\n\n\\citet{ngiam2011multimodal} use deep learning to learn features incorporating both video and audio modalities.\n\\citet{sohn2014improved} propose a new generative learning\nalgorithm for multimodal data which improves robustness to missing modalities at inference time.\nIn these works,\n\na single network takes all modalities as\ninputs,\nwhereas here we perform joint embedding of multiple modalities using multiple networks.\n\n\n\nSeveral previous works use deep networks for joint embedding between different feature spaces.\n\\citet{mikolov2013translation} map different languages to a joint feature space for translation. \n\\citet{srivastava2012multimodal} map images and natural language ``tags'' to the same space for\nautomatic annotation and retrieval. \nWhile these works use conventional pre-training algorithms, here we present a new pre-training\napproach for learning embedding spaces and show that it outperforms these existing methods (Sec.~\\ref{sec:results}.)\nOur algorithm trains each layer to map similar\ncases to similar areas of its feature space, as opposed to other\nmethods which either perform variational learning \\citep{hinton2006reducing}\nor train for reconstruction \\citep{SAE}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\citet{hu2014discriminative} also use a deep network for\nmetric learning for the task of face verification.\nSimilar to LMNN \\cite{weinberger2005distance}, \\citet{hu2014discriminative} \nenforces \na constant margin between\ndistances among inter-class objects and among intra-class objects.\nIn Sec.~\\ref{sec:results}, we show that our approach, which uses a\nloss-dependent variable margin, produces better results for our problem.\n{\\textcolor{Black}{{ Our work builds on deep neural network to embed three different modalities of  point-cloud, language, and trajectory into shared embedding space while handling lots of label-noise originating from crowd-sourcing. }}}\n\n\n\n\n\n\n\n\n\n{\\smallskip\\noindent\\textbf{{Crowd-sourcing.}}}\nMany approaches to teaching robots manipulation and other skills have relied on demonstrations\nby skilled experts\n\n\\cite{argall2009survey,abbeel2010autonomous}.\nAmong previous efforts to scale teaching to the crowd \\cite{crick2011human,tellex2014asking,jainsaxena2015_learningpreferencesmanipulation},\n\\citet{forbes2014robot} employs a similar approach towards crowd-sourcing\nbut collects\nmultiple instances of similar table-top manipulation with same object.\nOthers also build web-based platform for crowd-sourcing manipulation\n\\cite{toris2013robotsfor,toris2014robot}.\nHowever, these approaches either depend on the presence of an expert (due to required \nspecial software),\nor require a real robot at a remote location.\nOur Robobarista platform borrows some components of work from \\citet{alexander2012robot},\nbut works on any standard web browser with OpenGL support and incorporates real point-clouds of various scenes.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[tb]\n\\begin{center}\n\\includegraphics[width=\\textwidth,trim={3mm 0 1mm 0}]{figure2_isrr_v2.png}\n\\end{center}\n\n\\caption{\\textbf{Mapping object part and natural language instruction input to manipulation trajectory output.} Objects such as the espresso machine\nconsist of distinct object parts, each of which requires a distinct manipulation trajectory for manipulation. \nFor each part of the machine, we can re-use a manipulation trajectory that was used for some other object with \nsimilar parts. So, for an object part in a point-cloud (each object part colored on left), we \ncan find a trajectory used to manipulate some other object (labeled on the right) that can be \n\\emph{transferred} (labeled in the center). With this approach, a robot can operate a new and previously \nunobserved object such as the `espresso machine', \nby successfully transferring trajectories from other completely different but previously observed objects.\nNote that the input point-cloud is very noisy and incomplete (black represents missing points).}\n\\label{fig:espresso_transfer}\n\n\\end{figure*}\n\n\n\n\n\n\n\\section{Our Approach}\n\n{\\textcolor{Black}{{ Our goal is to build an algorithm that allows a robot to infer a manipulation trajectory when it is introduced to a new object or appliance and its natural language instruction manual. The intuition for our approach is that many differently-shaped objects share similarly-operated object parts;  thus, the manipulation trajectory of an object can be transferred to a completely different object if they share similarly-operated parts. }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ For example, the motion required to operate the handle of the espresso machine  in Figure~\\ref{fig:espresso_transfer} is almost identical to the motion required to flush a urinal with a handle. By identifying and transferring trajectories from prior experience with parts of other objects, robots can manipulate even objects they have never seen before. }}}\n\n{\\textcolor{Black}{{ We first formulate this problem as a structured prediction problem as shown in Figure~\\ref{fig:espresso_transfer}. Given a point-cloud for each part of an espresso machine and a natural language instruction such as `Push down on the handle to add hot water', our algorithm outputs a trajectory which executes the task,  using a pool of prior motion experience. }}}\n\n{\\textcolor{Black}{{ This is a challenging problem because the object is entirely new to the robot, and because it must jointly consider the point-cloud, natural language instruction, and each potential trajectory. Moreover, manually designing useful features from these three modalities  is extremely challenging. }}}\n\n\n\n{\\textcolor{Black}{{ We introduce a \\textit{deep multimodal embedding} approach  that learns a shared, semantically meaningful embedding space between these modalities, while dealing with a noise in crowd-sourced demonstration data. Then, we introduce our Robobarista crowd-sourcing platform, which allows us to easily scale the collection of manipulation demonstrations to  non-experts on the web. }}}\n\n\n\\subsection{Problem Formulation}\n\\label{sec:prob_form}\n\nOur goal is to learn a function $f$ that maps a given pair of point-cloud \n$p \\in \\mathcal{P}$ of an object part\nand a natural language instruction $l \\in \\mathcal{L}$ to a trajectory $\\tau \\in \n\\mathcal{T}$ that can manipulate the object part as described by free-form natural language $l$:\n\n", "index": 1, "text": "\\begin{equation}\nf : \\mathcal{P} \\times \\mathcal{L} \\rightarrow \\mathcal{T}  \\label{eqn:f}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"f:\\mathcal{P}\\times\\mathcal{L}\\rightarrow\\mathcal{T}\" display=\"block\"><mrow><mi>f</mi><mo>:</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u00d7</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mo>\u2192</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nThe size of this set varies for each instance.\nThese points are often obtained by stitching together a sequence of sensor data \nfrom an RGBD sensor \\citep{izadi2011kinectfusion}.\n\n\n\\noindent\n\\textbf{Trajectory Representation.}\n\\label{sec:trajrep}\nEach trajectory $\\tau \\in \\mathcal{T}$ is represented as a sequence of $m$ \\textit{waypoints},\nwhere each waypoint consists of gripper status $g$, translation $(t_x, t_y, t_z)$,\nand rotation $(r_x, r_y, r_z, r_w)$ with respect to the origin:\n\n", "itemtype": "equation", "pos": 26766, "prevtext": "\n{\\textcolor{Black}{{For instance, given the handle of the espresso machine in Figure~\\ref{fig:espresso_transfer} and an natural language instruction `Push down on the handle to add hot water', the algorithm should output a manipulation trajectory that will correctly accomplish the task on the object part according to the instruction. }}}\n\n\n\\noindent\n\\textbf{Point-cloud Representation.}\nEach instance of a point-cloud $p \\in \\mathcal{P}$ is represented as a set of $n$ points in three-dimensional\nEuclidean space where each point $(x,y,z)$ is represented with its RGB color $(r,g,b)$:\n\n", "index": 3, "text": "$$p = \\{p^{(i)} \\}^n_{i=1} = {\\{(x,y,z,r,g,b)^{(i)}\\}}^n_{i=1}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"p=\\{p^{(i)}\\}^{n}_{i=1}={\\{(x,y,z,r,g,b)^{(i)}\\}}^{n}_{i=1}\" display=\"block\"><mrow><mi>p</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>g</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nwhere  $g \\in \\{\\text{``open''},\\text{``closed''},\\text{``holding''}\\}$.\n$g$ depends on the type of the end-effector,\nwhich we have assumed to be a two-fingered parallel-plate gripper like that of PR2 or Baxter.\nThe rotation is represented as quaternions $(r_x, r_y, r_z, r_w)$\ninstead of the more compact Euler angles to prevent problems such as\ngimbal lock. \n\n\n\n\\noindent\n\\textbf{Smooth Trajectory.} To acquire a smooth trajectory from\na waypoint-based trajectory $\\tau$, \nwe interpolate intermediate waypoints.\nTranslation is linearly interpolated and the quaternion is interpolated\nusing spherical linear interpolation (Slerp) \\citep{shoemake1985animating}.\n\n\n\n\n\n\n\\subsection{Data Pre-processing}\n\\label{sec:preprocessing}\n\n\n\n\nEach of the point-cloud, language, and trajectory $(p, l, \\tau)$ can have any length. \n\nThus, we fit raw data from each modality into a fixed-length vector.\n\n\nWe represent point-cloud $p$ of any arbitrary length\nas an occupancy grid where each cell \nindicates whether any point lives in the space it represents.\nBecause point-cloud $p$ consists of only the part of an object which is limited in size,\nwe can represent $p$ using two occupancy grid-like structures of size $10 \\times 10 \\times 10$ voxels with different scales: \none with each voxel spanning a cube of $1 \\times 1 \\times 1 (cm)$ and the other with \neach cell representing $2.5\\times 2.5 \\times 2.5 (cm)$.\n\nEach language instruction is represented as a fixed-size bag-of-words representation with\nstop words removed.\nFinally, for each trajectory $\\tau \\in \\mathcal{T}$, \nwe first compute its smooth interpolated trajectory $\\tau_s \\in \\mathcal{T}_s$\n(Sec.~\\ref{sec:trajrep}),\nand then normalize all trajectories $\\mathcal{T}_s$ to the same length\nwhile preserving the sequence of gripper states such as `opening', `closing', and `holding'.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Direct manipulation trajectory transfer}\n\\label{sec:transfer_adapt}\n\n\n\nEven if we have a trajectory to transfer,\na conceptually transferable trajectory is not necessarily directly compatible\nif it is represented with respect to an inconsistent reference point.\n\n\n\nTo make a trajectory compatible with a new situation without modifying the \ntrajectory,\nwe need a representation method for trajectories, based on point-cloud information,\nthat allows a \\textit{direct\ntransfer of a trajectory without any modification}.\n\n\n\\noindent\n\\textbf{Challenges.}\nMaking a trajectory compatible when transferred to a different object or to a\ndifferent instance of the same object without modification\n can be challenging depending on the representation\nof trajectories and the variations in the location of the object, \ngiven in point-clouds. \n\n\n\n\n\n{\\textcolor{Black}{{ Many approaches which control high degree of freedom arms such as those of PR2 or Baxter use configuration-space trajectories, which store a time-parameterized series of joint angles \\citep{thrun2005probabilistic}. While such approaches allow for direct control of joint angles during control, they require costly recomputation for even a small change in an object's position or orientation. }}}\n\n\n\\iffalse\nFor robots with high degrees of freedom arms such as PR2 or Baxter,\ntrajectories are often represented as a sequence of joint angles (in configuration space)\n\\citep{thrun2005probabilistic}.\nWhile such configuration-space trajectories allow for direct control of joint angles\nduring control, they require costly recomputation for even a small change in an object's position\nand orientation.\n\n\n\n\n\n\n\n\n\n\n\n\nThey can be executed without modification\n only when the robot is in the exact same position and orientation\nwith respect to the object.\n\n\\fi\n\nOne approach that allows execution without modification \nis representing trajectories with respect to the object\nby aligning via point-cloud registration (e.g. \\citep{forbes2014robot}). \nHowever, a large object such as a stove might have many parts (e.g. knobs and handles) whose\npositions might vary between different stoves. Thus, object-aligned manipulation of these\nparts would not be robust to different stoves, and in general would impede transfer between different\ninstances of the same object.\n\n\\iffalse\nHowever, if the object is large (e.g. a stove) and has many parts (e.g. knobs \nand handles), then object-based\nrepresentation is prone to errors when individual parts have different\ntranslation and rotation.\nThis limits the transfers to be between different instances of the\nsame object that is small or has a simple structure.\n\\fi\n\n\n\nLastly, it is even more challenging if two objects require similar trajectories, but have slightly different shapes.\nAnd this is made more difficult by limitations of the point-cloud data. \nAs shown in left of Fig.~\\ref{fig:espresso_transfer},\nthe point-cloud data, even when stitched from multiple angles, are very noisy \ncompared to the RGB images. \n\n\n\\noindent\n\\textbf{Our Solution.}\nTransferred trajectories become compatible across different objects when trajectories are \nrepresented\n1) in the task space rather than the configuration space, and \n2) relative to the object \\emph{part} in question (aligned based on its principal axis), rather than the object as a whole.\n\n\nTrajectories can be represented in the task space by recording only the position and orientation\nof the end-effector.\nBy doing so, we can focus on the actual interaction between the robot and the \nenvironment\nrather than the movement of the arm.\nIt is very rare that the arm configuration affects the completion of the task as long as there is no collision.\nWith the trajectory represented as a sequence of gripper position and orientation, \nthe robot can find its arm configuration that \nis collision free with the environment using inverse kinematics.\n\nHowever, representing the trajectory in task space is not enough to make transfers\ncompatible.\nThe trajectory must also be represented in a common coordinate frame regardless of the object's orientation and shape. \n\nThus, we align the negative $z$-axis along gravity and align the $x$-axis along the\nprincipal axis of the object \\textit{part} using PCA \\citep{hsiao2010contact}.\nWith this representation, even when the object part's position and orientation changes,\nthe trajectory does not need to change.\nThe underlying assumption is that similarly operated object parts share \nsimilar shapes leading to a similar direction in their principal axes.\n\n\n\n\n\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=0.99\\columnwidth]{main_fig_v7.pdf}\n  \\end{center}\n  \n  \\caption{\n    \\textbf{Deep Multimodal Embedding:} Our deep neural network learns \n    to embed both point-cloud/natural language instruction combinations and\n    manipulation trajectories in the same space. \n    This allows for fast selection of a new trajectory by projecting\n    a new environment/instruction pair and choosing its nearest-neighbor trajectory in this space.\n    }\n  \\label{fig:main_fig}\n\\end{figure}\n\n\n\n\\section{Deep Multimodal Embedding}\n\\label{sec:deep_embedding}\n\n\n\n\n\n\n{\\textcolor{Black}{{ In this work, we use deep learning to find the most appropriate trajectory for a given point-cloud and natural language instruction. This is much more challenging than the uni-modal binary or multi-class classification/regression problems  (e.g. image recognition \\cite{krizhevsky2012imagenet}) to which deep learning has mostly been applied \\citep{bengio2013representation}. We could simply convert our problem  into a binary classification problem, i.e. ``does this trajectory match this point-cloud/language pair?'' Then, we can use a multimodal feed-forward deep network \\cite{ngiam2011multimodal} to solve this problem \\cite{sung_robobarista_2015}. }}}\n\n{\\textcolor{Black}{{ However, this approach has several drawbacks. First, it requires evaluating the network over \\emph{every} combination of potential candidate manipulation trajectory and the given point-cloud/language pair, which is computationally expensive. Second, this  method does nothing to handle noisy labels, a significant problem when dealing with crowd-sourced data as we do here. Finally, while this method is capable of producing reasonable results for our problem, we show in Section~\\ref{sec:results} that a more principled approach to our problem is able to improve over it. }}}\n\n\\iffalse\n{\\textcolor{Black}{{ We use deep learning  to find the most appropriate trajectory for the given point-cloud and natural language. Deep learning is mostly used for binary or multi-class classification or regression problems \\citep{bengio2013representation} with a uni-modal input. We introduce a deep learning model  that can handle three completely different modalities of point-cloud, language, and trajectory  and solve a structural problem with lots of label noise. A simple application of a deep neural network to this problem (equation~\\ref{eqn:f})  could be learning to classify whether a potential manipulation trajectory is a good or bad match for the given novel object part \\cite{sung_robobarista_2015}. While, as shown in Section~\\ref{sec:results}, such approach produces reasonable results, it has the drawback of requiring the network to be evaluated over \\emph{every} combination of  potential candidate manipulation trajectory. }}}\n\\fi\n\n{\\textcolor{Black}{{ Instead, in this work, we present a new deep architecture and algorithm which is a better fit for this problem, directly addressing the challenges inherent in multimodal data and the noisy labels obtained from crowdsourcing. }}}\nWe learn a joint embedding of point-cloud, language, and trajectory data\ninto the same low dimensional space.   \nWe learn non-linear embeddings using a deep learning approach\nwhich maps raw data from these three different modalities to a joint embedding space.\n\nWe then use this space to find known\ntrajectories which are good matches for new\ncombinations of object parts and instructions.\nCompared to previous work  \nthat exhaustively runs a full network over all these combinations \\citep{sung_robobarista_2015}, \nour approach allows us to pre-embed all candidate trajectories into this common feature space.\nThus, the most appropriate trajectory can be identified \nby embedding only a new point-cloud/language pair and then finding its nearest neighbor.\n\n\n\n\n\n\nIn our joint feature space, proximity between two mapped points \nshould reflect how relevant two data-points are to each other,\neven if they are from completely different modalities.\nWe train our network to bring demonstrations that would manipulate a given object \naccording to some language instruction closer to the mapped point for that object/instruction pair,\nand to push away demonstrations that would not correctly manipulate the object.\nTrajectories which have no semantic relevance to the object are pushed much further \naway than trajectories that have some relevance, \neven if the latter would not manipulate the object according to the instruction.\n\n\nPrior to learning a full joint embedding of all three modalities,\nwe pre-train embeddings of subsets of the modalities \nto learn semantically meaningful embeddings for these modalities,\nleading to improved performance as shown in Section ~\\ref{sec:results}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ To solve this problem of learning to manipulate novel objects and appliance as defined in equation~(\\ref{eqn:f}), }}}\nwe learn two different mapping functions\nthat map to a common space---one from a point-cloud/language pair \nand the other from a trajectory.\nMore formally, we want to learn $\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)$ and\n$\\Phi_{\\mathcal{T}}(\\tau)$ which map to a joint feature space $\\mathbb{R}^M$:\n\n", "itemtype": "equation", "pos": 27320, "prevtext": "\nThe size of this set varies for each instance.\nThese points are often obtained by stitching together a sequence of sensor data \nfrom an RGBD sensor \\citep{izadi2011kinectfusion}.\n\n\n\\noindent\n\\textbf{Trajectory Representation.}\n\\label{sec:trajrep}\nEach trajectory $\\tau \\in \\mathcal{T}$ is represented as a sequence of $m$ \\textit{waypoints},\nwhere each waypoint consists of gripper status $g$, translation $(t_x, t_y, t_z)$,\nand rotation $(r_x, r_y, r_z, r_w)$ with respect to the origin:\n\n", "index": 5, "text": "$$\\tau= \\{\\tau^{(i)}\\}^m_{i=1} = {\\{(g, t_x, t_y, t_z, r_x, r_y, r_z, r_w)^{(i)}\\}}^m_{i=1}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\tau=\\{\\tau^{(i)}\\}^{m}_{i=1}={\\{(g,t_{x},t_{y},t_{z},r_{x},r_{y},r_{z},r_{w})%&#10;^{(i)}\\}}^{m}_{i=1}\" display=\"block\"><mrow><mi>\u03c4</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>,</mo><msub><mi>t</mi><mi>x</mi></msub><mo>,</mo><msub><mi>t</mi><mi>y</mi></msub><mo>,</mo><msub><mi>t</mi><mi>z</mi></msub><mo>,</mo><msub><mi>r</mi><mi>x</mi></msub><mo>,</mo><msub><mi>r</mi><mi>y</mi></msub><mo>,</mo><msub><mi>r</mi><mi>z</mi></msub><mo>,</mo><msub><mi>r</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nHere, we represent these mappings with a deep neural network, \nas shown in Figure~\\ref{fig:main_fig}.\n\nThe first, $\\Phi_{\\mathcal{P},\\mathcal{L}}$, which maps point-clouds and trajectories, \nis defined as a combination of two mappings. The first of these maps to a joint \npoint-cloud/language space $\\mathbb{R}^{N_{2,pl}}$ ---\n$\\Phi_{\\mathcal{P}}(p):\\mathcal{P} \\rightarrow \\mathbb{R}^{N_{2,pl}}$ and\n$\\Phi_{\\mathcal{L}}(l):\\mathcal{L} \\rightarrow \\mathbb{R}^{N_{2,pl}}$.\nOnce each is mapped to $\\mathbb{R}^{N_{2,pl}}$, \n\nthis space is then mapped to the joint space shared with trajectory information: \n$\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l): ((\\mathcal{P}, \\mathcal{L}) \\rightarrow \\mathbb{R}^{N_{2,pl}}) \\rightarrow \\mathbb{R}^M$.\n\n\n\n\n\n\n\\subsection{Model}\n\n\n\n\n\nWe use two separate multi-layer deep neural networks,\none for \n$\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)$\nand one for $\\Phi_{\\mathcal{T}}(\\tau)$.\nTake $N_p$ as the size of point-cloud input $p$,\n$N_l$ as similar for natural language input $l$, $N_{1,p}$ and\n$N_{1,l}$ as the number of hidden units in the first hidden\nlayers projected from point-cloud and natural language features,\nrespectively, and $N_{2,pl}$ as the number of hidden units\nin the combined point-cloud/language layer. With $W$'s as network\nweights, which are the learned parameters of our system, and\n$a(\\cdot)$ as a rectified linear unit (ReLU)\nactivation function \\citep{zeiler2013rectified},\nour model for projecting from point-cloud and language features\nto the shared embedding $h^3$ is as follows:\n\n", "itemtype": "equation", "pos": 39076, "prevtext": "\nwhere  $g \\in \\{\\text{``open''},\\text{``closed''},\\text{``holding''}\\}$.\n$g$ depends on the type of the end-effector,\nwhich we have assumed to be a two-fingered parallel-plate gripper like that of PR2 or Baxter.\nThe rotation is represented as quaternions $(r_x, r_y, r_z, r_w)$\ninstead of the more compact Euler angles to prevent problems such as\ngimbal lock. \n\n\n\n\\noindent\n\\textbf{Smooth Trajectory.} To acquire a smooth trajectory from\na waypoint-based trajectory $\\tau$, \nwe interpolate intermediate waypoints.\nTranslation is linearly interpolated and the quaternion is interpolated\nusing spherical linear interpolation (Slerp) \\citep{shoemake1985animating}.\n\n\n\n\n\n\n\\subsection{Data Pre-processing}\n\\label{sec:preprocessing}\n\n\n\n\nEach of the point-cloud, language, and trajectory $(p, l, \\tau)$ can have any length. \n\nThus, we fit raw data from each modality into a fixed-length vector.\n\n\nWe represent point-cloud $p$ of any arbitrary length\nas an occupancy grid where each cell \nindicates whether any point lives in the space it represents.\nBecause point-cloud $p$ consists of only the part of an object which is limited in size,\nwe can represent $p$ using two occupancy grid-like structures of size $10 \\times 10 \\times 10$ voxels with different scales: \none with each voxel spanning a cube of $1 \\times 1 \\times 1 (cm)$ and the other with \neach cell representing $2.5\\times 2.5 \\times 2.5 (cm)$.\n\nEach language instruction is represented as a fixed-size bag-of-words representation with\nstop words removed.\nFinally, for each trajectory $\\tau \\in \\mathcal{T}$, \nwe first compute its smooth interpolated trajectory $\\tau_s \\in \\mathcal{T}_s$\n(Sec.~\\ref{sec:trajrep}),\nand then normalize all trajectories $\\mathcal{T}_s$ to the same length\nwhile preserving the sequence of gripper states such as `opening', `closing', and `holding'.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Direct manipulation trajectory transfer}\n\\label{sec:transfer_adapt}\n\n\n\nEven if we have a trajectory to transfer,\na conceptually transferable trajectory is not necessarily directly compatible\nif it is represented with respect to an inconsistent reference point.\n\n\n\nTo make a trajectory compatible with a new situation without modifying the \ntrajectory,\nwe need a representation method for trajectories, based on point-cloud information,\nthat allows a \\textit{direct\ntransfer of a trajectory without any modification}.\n\n\n\\noindent\n\\textbf{Challenges.}\nMaking a trajectory compatible when transferred to a different object or to a\ndifferent instance of the same object without modification\n can be challenging depending on the representation\nof trajectories and the variations in the location of the object, \ngiven in point-clouds. \n\n\n\n\n\n{\\textcolor{Black}{{ Many approaches which control high degree of freedom arms such as those of PR2 or Baxter use configuration-space trajectories, which store a time-parameterized series of joint angles \\citep{thrun2005probabilistic}. While such approaches allow for direct control of joint angles during control, they require costly recomputation for even a small change in an object's position or orientation. }}}\n\n\n\\iffalse\nFor robots with high degrees of freedom arms such as PR2 or Baxter,\ntrajectories are often represented as a sequence of joint angles (in configuration space)\n\\citep{thrun2005probabilistic}.\nWhile such configuration-space trajectories allow for direct control of joint angles\nduring control, they require costly recomputation for even a small change in an object's position\nand orientation.\n\n\n\n\n\n\n\n\n\n\n\n\nThey can be executed without modification\n only when the robot is in the exact same position and orientation\nwith respect to the object.\n\n\\fi\n\nOne approach that allows execution without modification \nis representing trajectories with respect to the object\nby aligning via point-cloud registration (e.g. \\citep{forbes2014robot}). \nHowever, a large object such as a stove might have many parts (e.g. knobs and handles) whose\npositions might vary between different stoves. Thus, object-aligned manipulation of these\nparts would not be robust to different stoves, and in general would impede transfer between different\ninstances of the same object.\n\n\\iffalse\nHowever, if the object is large (e.g. a stove) and has many parts (e.g. knobs \nand handles), then object-based\nrepresentation is prone to errors when individual parts have different\ntranslation and rotation.\nThis limits the transfers to be between different instances of the\nsame object that is small or has a simple structure.\n\\fi\n\n\n\nLastly, it is even more challenging if two objects require similar trajectories, but have slightly different shapes.\nAnd this is made more difficult by limitations of the point-cloud data. \nAs shown in left of Fig.~\\ref{fig:espresso_transfer},\nthe point-cloud data, even when stitched from multiple angles, are very noisy \ncompared to the RGB images. \n\n\n\\noindent\n\\textbf{Our Solution.}\nTransferred trajectories become compatible across different objects when trajectories are \nrepresented\n1) in the task space rather than the configuration space, and \n2) relative to the object \\emph{part} in question (aligned based on its principal axis), rather than the object as a whole.\n\n\nTrajectories can be represented in the task space by recording only the position and orientation\nof the end-effector.\nBy doing so, we can focus on the actual interaction between the robot and the \nenvironment\nrather than the movement of the arm.\nIt is very rare that the arm configuration affects the completion of the task as long as there is no collision.\nWith the trajectory represented as a sequence of gripper position and orientation, \nthe robot can find its arm configuration that \nis collision free with the environment using inverse kinematics.\n\nHowever, representing the trajectory in task space is not enough to make transfers\ncompatible.\nThe trajectory must also be represented in a common coordinate frame regardless of the object's orientation and shape. \n\nThus, we align the negative $z$-axis along gravity and align the $x$-axis along the\nprincipal axis of the object \\textit{part} using PCA \\citep{hsiao2010contact}.\nWith this representation, even when the object part's position and orientation changes,\nthe trajectory does not need to change.\nThe underlying assumption is that similarly operated object parts share \nsimilar shapes leading to a similar direction in their principal axes.\n\n\n\n\n\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=0.99\\columnwidth]{main_fig_v7.pdf}\n  \\end{center}\n  \n  \\caption{\n    \\textbf{Deep Multimodal Embedding:} Our deep neural network learns \n    to embed both point-cloud/natural language instruction combinations and\n    manipulation trajectories in the same space. \n    This allows for fast selection of a new trajectory by projecting\n    a new environment/instruction pair and choosing its nearest-neighbor trajectory in this space.\n    }\n  \\label{fig:main_fig}\n\\end{figure}\n\n\n\n\\section{Deep Multimodal Embedding}\n\\label{sec:deep_embedding}\n\n\n\n\n\n\n{\\textcolor{Black}{{ In this work, we use deep learning to find the most appropriate trajectory for a given point-cloud and natural language instruction. This is much more challenging than the uni-modal binary or multi-class classification/regression problems  (e.g. image recognition \\cite{krizhevsky2012imagenet}) to which deep learning has mostly been applied \\citep{bengio2013representation}. We could simply convert our problem  into a binary classification problem, i.e. ``does this trajectory match this point-cloud/language pair?'' Then, we can use a multimodal feed-forward deep network \\cite{ngiam2011multimodal} to solve this problem \\cite{sung_robobarista_2015}. }}}\n\n{\\textcolor{Black}{{ However, this approach has several drawbacks. First, it requires evaluating the network over \\emph{every} combination of potential candidate manipulation trajectory and the given point-cloud/language pair, which is computationally expensive. Second, this  method does nothing to handle noisy labels, a significant problem when dealing with crowd-sourced data as we do here. Finally, while this method is capable of producing reasonable results for our problem, we show in Section~\\ref{sec:results} that a more principled approach to our problem is able to improve over it. }}}\n\n\\iffalse\n{\\textcolor{Black}{{ We use deep learning  to find the most appropriate trajectory for the given point-cloud and natural language. Deep learning is mostly used for binary or multi-class classification or regression problems \\citep{bengio2013representation} with a uni-modal input. We introduce a deep learning model  that can handle three completely different modalities of point-cloud, language, and trajectory  and solve a structural problem with lots of label noise. A simple application of a deep neural network to this problem (equation~\\ref{eqn:f})  could be learning to classify whether a potential manipulation trajectory is a good or bad match for the given novel object part \\cite{sung_robobarista_2015}. While, as shown in Section~\\ref{sec:results}, such approach produces reasonable results, it has the drawback of requiring the network to be evaluated over \\emph{every} combination of  potential candidate manipulation trajectory. }}}\n\\fi\n\n{\\textcolor{Black}{{ Instead, in this work, we present a new deep architecture and algorithm which is a better fit for this problem, directly addressing the challenges inherent in multimodal data and the noisy labels obtained from crowdsourcing. }}}\nWe learn a joint embedding of point-cloud, language, and trajectory data\ninto the same low dimensional space.   \nWe learn non-linear embeddings using a deep learning approach\nwhich maps raw data from these three different modalities to a joint embedding space.\n\nWe then use this space to find known\ntrajectories which are good matches for new\ncombinations of object parts and instructions.\nCompared to previous work  \nthat exhaustively runs a full network over all these combinations \\citep{sung_robobarista_2015}, \nour approach allows us to pre-embed all candidate trajectories into this common feature space.\nThus, the most appropriate trajectory can be identified \nby embedding only a new point-cloud/language pair and then finding its nearest neighbor.\n\n\n\n\n\n\nIn our joint feature space, proximity between two mapped points \nshould reflect how relevant two data-points are to each other,\neven if they are from completely different modalities.\nWe train our network to bring demonstrations that would manipulate a given object \naccording to some language instruction closer to the mapped point for that object/instruction pair,\nand to push away demonstrations that would not correctly manipulate the object.\nTrajectories which have no semantic relevance to the object are pushed much further \naway than trajectories that have some relevance, \neven if the latter would not manipulate the object according to the instruction.\n\n\nPrior to learning a full joint embedding of all three modalities,\nwe pre-train embeddings of subsets of the modalities \nto learn semantically meaningful embeddings for these modalities,\nleading to improved performance as shown in Section ~\\ref{sec:results}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ To solve this problem of learning to manipulate novel objects and appliance as defined in equation~(\\ref{eqn:f}), }}}\nwe learn two different mapping functions\nthat map to a common space---one from a point-cloud/language pair \nand the other from a trajectory.\nMore formally, we want to learn $\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)$ and\n$\\Phi_{\\mathcal{T}}(\\tau)$ which map to a joint feature space $\\mathbb{R}^M$:\n\n", "index": 7, "text": "\\begin{align*}\n\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)&: (\\mathcal{P},\\mathcal{L}) \\rightarrow \\mathbb{R}^M  \\\\\n\\Phi_{\\mathcal{T}}(\\tau)&: \\mathcal{T} \\rightarrow \\mathbb{R}^{M}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:(\\mathcal{P},\\mathcal{L})\\rightarrow\\mathbb{R}^{M}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2192</mo><msup><mi>\u211d</mi><mi>M</mi></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Phi_{\\mathcal{T}}(\\tau)\" display=\"inline\"><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\mathcal{T}\\rightarrow\\mathbb{R}^{M}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mo>\u2192</mo><msup><mi>\u211d</mi><mi>M</mi></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nThe model for projecting from trajectory input $\\tau$ is similar,\nexcept it takes input only from a single modality.\n\n\n\n\\subsection{Inference.}\nOnce all mappings are learned, we solve the original problem from equation~(\\ref{eqn:f})\nby choosing, from a library of prior trajectories, the trajectory \nthat gives the highest similarity (closest in distance)\nto the given point-cloud $p$ and language $l$ in our joint embedding space $\\mathbb{R}^M$.\nAs in previous work \\citep{weston2011wsabie}, similarity is defined as $sim(a,b) = a \\cdot b$, and\nthe trajectory that maximizes the magnitude of similarity is selected:\n\n\n", "itemtype": "equation", "pos": 40792, "prevtext": "\nHere, we represent these mappings with a deep neural network, \nas shown in Figure~\\ref{fig:main_fig}.\n\nThe first, $\\Phi_{\\mathcal{P},\\mathcal{L}}$, which maps point-clouds and trajectories, \nis defined as a combination of two mappings. The first of these maps to a joint \npoint-cloud/language space $\\mathbb{R}^{N_{2,pl}}$ ---\n$\\Phi_{\\mathcal{P}}(p):\\mathcal{P} \\rightarrow \\mathbb{R}^{N_{2,pl}}$ and\n$\\Phi_{\\mathcal{L}}(l):\\mathcal{L} \\rightarrow \\mathbb{R}^{N_{2,pl}}$.\nOnce each is mapped to $\\mathbb{R}^{N_{2,pl}}$, \n\nthis space is then mapped to the joint space shared with trajectory information: \n$\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l): ((\\mathcal{P}, \\mathcal{L}) \\rightarrow \\mathbb{R}^{N_{2,pl}}) \\rightarrow \\mathbb{R}^M$.\n\n\n\n\n\n\n\\subsection{Model}\n\n\n\n\n\nWe use two separate multi-layer deep neural networks,\none for \n$\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l)$\nand one for $\\Phi_{\\mathcal{T}}(\\tau)$.\nTake $N_p$ as the size of point-cloud input $p$,\n$N_l$ as similar for natural language input $l$, $N_{1,p}$ and\n$N_{1,l}$ as the number of hidden units in the first hidden\nlayers projected from point-cloud and natural language features,\nrespectively, and $N_{2,pl}$ as the number of hidden units\nin the combined point-cloud/language layer. With $W$'s as network\nweights, which are the learned parameters of our system, and\n$a(\\cdot)$ as a rectified linear unit (ReLU)\nactivation function \\citep{zeiler2013rectified},\nour model for projecting from point-cloud and language features\nto the shared embedding $h^3$ is as follows:\n\n", "index": 9, "text": "\\begin{align*}\nh^{1,p}_i & = a\\left(\\textstyle \\sum_{j=0}^{N_p} W^{1,p}_{i,j}  p_j \\right)\\\\\nh^{1,l}_i & = a\\left(\\textstyle \\sum_{j=0}^{N_l} W^{1,l}_{i,j}  l_j \\right)\\\\\nh^{2,pl}_i & = a\\left(\\textstyle \\sum_{j=0}^{N_{1,p}} W^{2,p}_{i,j} h^{1,p}_j + \\sum_{j=0}^{N_{1,l}} W^{2,l}_{i,j} h^{1,l}_j\\right) \\\\\nh^{3}_i & = a\\left(\\textstyle \\sum_{j=0}^{N_{2,pl}} W^{3,pl}_{i,j} h^{2,pl}_j \\right)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h^{1,p}_{i}\" display=\"inline\"><msubsup><mi>h</mi><mi>i</mi><mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=a\\left(\\textstyle\\sum_{j=0}^{N_{p}}W^{1,p}_{i,j}p_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mi>p</mi></msub></msubsup><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msubsup><mo>\u2062</mo><msub><mi>p</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h^{1,l}_{i}\" display=\"inline\"><msubsup><mi>h</mi><mi>i</mi><mrow><mn>1</mn><mo>,</mo><mi>l</mi></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=a\\left(\\textstyle\\sum_{j=0}^{N_{l}}W^{1,l}_{i,j}l_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mi>l</mi></msub></msubsup><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mn>1</mn><mo>,</mo><mi>l</mi></mrow></msubsup><mo>\u2062</mo><msub><mi>l</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h^{2,pl}_{i}\" display=\"inline\"><msubsup><mi>h</mi><mi>i</mi><mrow><mn>2</mn><mo>,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=a\\left(\\textstyle\\sum_{j=0}^{N_{1,p}}W^{2,p}_{i,j}h^{1,p}_{j}+%&#10;\\sum_{j=0}^{N_{1,l}}W^{2,l}_{i,j}h^{1,l}_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msub></msubsup><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mn>2</mn><mo>,</mo><mi>p</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msubsup></mrow></mrow><mo>+</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>,</mo><mi>l</mi></mrow></msub></msubsup><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mn>2</mn><mo>,</mo><mi>l</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mn>1</mn><mo>,</mo><mi>l</mi></mrow></msubsup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h^{3}_{i}\" display=\"inline\"><msubsup><mi>h</mi><mi>i</mi><mn>3</mn></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=a\\left(\\textstyle\\sum_{j=0}^{N_{2,pl}}W^{3,pl}_{i,j}h^{2,pl}_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mrow><mn>2</mn><mo>,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow></msub></msubsup><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mn>3</mn><mo>,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow></msubsup><mo>\u2062</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mn>2</mn><mo>,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\nThe previous approach to this problem \\citep{sung_robobarista_2015} \nrequired projecting the combination of the current point-cloud and \nnatural language instruction with \\emph{every} trajectory in the training set through the network during inference.\n\nHere, we pre-compute the representations of all training trajectories\nin $h^3$, and need only project the new point-cloud/language pair to $h^3$ and find its nearest-neighbor\ntrajectory in this embedding space. As shown in \nSection~\\ref{sec:results}, this significantly improves \nboth the runtime and accuracy of\nour approach and makes it much more scalable to larger training datasets like those collected with \ncrowdsourcing platforms. \n\n\n\n\n\n\n\n\n\n\\section{Learning Joint Point-cloud/Language/Trajectory Model}\n\nThe main challenge of our work is to learn a model which maps three disparate\nmodalities -- point-clouds, natural language, and trajectories -- to a single\nsemantically meaningful space.\nWe introduce a method that learns a common point-cloud/language/trajectory space such that \nall trajectories relevant to a given task (point-cloud/language combination) \nshould have higher similarity to the projection of that task than \ntask-irrelevant trajectories.\nAmong these irrelevant trajectories, some might be less relevant than others,\nand thus should be pushed further away.\n\nFor example, given a door knob that needs to be grasped normal to the door surface and \nan instruction to rotate it clockwise, \na trajectory that correctly approaches the door knob but rotates counter-clockwise\nshould have higher similarity to the task than one\nwhich approaches the knob from a completely incorrect angle and does not execute any rotation.\n\n\n{\\textcolor{Black}{{ For every training point-cloud/language pair $(p_i, l_i)$,  we have two sets of demonstrations: a set of trajectories $\\mathcal{T}_{i,S}$ that are relevant (similar)  to this task and a set of trajectories $\\mathcal{T}_{i,D}$ that are irrelevant (dissimilar) as described in Sec.~\\ref{sec:noise}. }}}\nFor each pair of $(p_i, l_i)$,\nwe want all projections of $\\tau_j \\in \\mathcal{T}_{i,S}$ to have higher similarity\nto the projection of $(p_i, l_i)$ than $\\tau_k \\in \\mathcal{T}_{i,D}$.\n\nA simple approach would be to train the network\nto distinguish these two sets by enforcing\n\n\na finite distance (safety margin) between the similarities of\nthese two sets \\citep{weinberger2005distance},\nwhich can be written in the form of a constraint:\n\n", "itemtype": "equation", "pos": 41815, "prevtext": "\nThe model for projecting from trajectory input $\\tau$ is similar,\nexcept it takes input only from a single modality.\n\n\n\n\\subsection{Inference.}\nOnce all mappings are learned, we solve the original problem from equation~(\\ref{eqn:f})\nby choosing, from a library of prior trajectories, the trajectory \nthat gives the highest similarity (closest in distance)\nto the given point-cloud $p$ and language $l$ in our joint embedding space $\\mathbb{R}^M$.\nAs in previous work \\citep{weston2011wsabie}, similarity is defined as $sim(a,b) = a \\cdot b$, and\nthe trajectory that maximizes the magnitude of similarity is selected:\n\n\n", "index": 11, "text": "$${\\arg\\!\\max}_{\\tau \\in \\mathcal{T}} sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l), \\Phi_{\\mathcal{T}}(\\tau))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{\\arg\\!\\max}_{\\tau\\in\\mathcal{T}}sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p,l),\\Phi_%&#10;{\\mathcal{T}}(\\tau))\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><munder><mi>max</mi><mrow><mi>\u03c4</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder><mo>\u2061</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\nRather than simply being able to distinguish two sets, we want to learn\nsemantically meaningful embedding spaces from different modalities.\nRecalling our earlier example \nwhere one incorrect trajectory for manipulating\na door knob was much closer to correct than another, it is clear that our\nlearning algorithm should drive some of incorrect trajectories to be more\ndissimilar than others.\nThe difference between the similarities of $\\tau_j$ and $\\tau_k$ to the projected\npoint-cloud/language pair $(p_i, l_i)$ should be at least the loss $\\Delta(\\tau_j,\\tau_k)$.\n\nThis can be written as a form of a constraint:\n\n", "itemtype": "equation", "pos": 44382, "prevtext": "\n\nThe previous approach to this problem \\citep{sung_robobarista_2015} \nrequired projecting the combination of the current point-cloud and \nnatural language instruction with \\emph{every} trajectory in the training set through the network during inference.\n\nHere, we pre-compute the representations of all training trajectories\nin $h^3$, and need only project the new point-cloud/language pair to $h^3$ and find its nearest-neighbor\ntrajectory in this embedding space. As shown in \nSection~\\ref{sec:results}, this significantly improves \nboth the runtime and accuracy of\nour approach and makes it much more scalable to larger training datasets like those collected with \ncrowdsourcing platforms. \n\n\n\n\n\n\n\n\n\n\\section{Learning Joint Point-cloud/Language/Trajectory Model}\n\nThe main challenge of our work is to learn a model which maps three disparate\nmodalities -- point-clouds, natural language, and trajectories -- to a single\nsemantically meaningful space.\nWe introduce a method that learns a common point-cloud/language/trajectory space such that \nall trajectories relevant to a given task (point-cloud/language combination) \nshould have higher similarity to the projection of that task than \ntask-irrelevant trajectories.\nAmong these irrelevant trajectories, some might be less relevant than others,\nand thus should be pushed further away.\n\nFor example, given a door knob that needs to be grasped normal to the door surface and \nan instruction to rotate it clockwise, \na trajectory that correctly approaches the door knob but rotates counter-clockwise\nshould have higher similarity to the task than one\nwhich approaches the knob from a completely incorrect angle and does not execute any rotation.\n\n\n{\\textcolor{Black}{{ For every training point-cloud/language pair $(p_i, l_i)$,  we have two sets of demonstrations: a set of trajectories $\\mathcal{T}_{i,S}$ that are relevant (similar)  to this task and a set of trajectories $\\mathcal{T}_{i,D}$ that are irrelevant (dissimilar) as described in Sec.~\\ref{sec:noise}. }}}\nFor each pair of $(p_i, l_i)$,\nwe want all projections of $\\tau_j \\in \\mathcal{T}_{i,S}$ to have higher similarity\nto the projection of $(p_i, l_i)$ than $\\tau_k \\in \\mathcal{T}_{i,D}$.\n\nA simple approach would be to train the network\nto distinguish these two sets by enforcing\n\n\na finite distance (safety margin) between the similarities of\nthese two sets \\citep{weinberger2005distance},\nwhich can be written in the form of a constraint:\n\n", "index": 13, "text": "$$sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau_j)) \n\\geq 1 + sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau_k))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_{i},l_{i}),\\Phi_{\\mathcal{T}}(\\tau_{j}))%&#10;\\geq 1+sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_{i},l_{i}),\\Phi_{\\mathcal{T}}(\\tau%&#10;_{k}))\" display=\"block\"><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nIntuitively, this forces trajectories with higher DTW-MT distance from the ground truth to embed further \nthan those with lower distance. Enforcing all combinations of these constraints could grow exponentially large.\nInstead, similar to the cutting plane method for structural support vector machines \\citep{tsochantaridis2005large},\nwe find the most violating trajectory $\\tau' \\in \\mathcal{T}_{i,D}$\nfor each training pair of $(p_i, l_i, \\tau_i \\in \\mathcal{T}_{i,S})$\nat each iteration.\nThe most violating trajectory \nhas the highest similarity augmented with the loss scaled by a constant $\\alpha$:\n\n", "itemtype": "equation", "pos": 45156, "prevtext": "\n\nRather than simply being able to distinguish two sets, we want to learn\nsemantically meaningful embedding spaces from different modalities.\nRecalling our earlier example \nwhere one incorrect trajectory for manipulating\na door knob was much closer to correct than another, it is clear that our\nlearning algorithm should drive some of incorrect trajectories to be more\ndissimilar than others.\nThe difference between the similarities of $\\tau_j$ and $\\tau_k$ to the projected\npoint-cloud/language pair $(p_i, l_i)$ should be at least the loss $\\Delta(\\tau_j,\\tau_k)$.\n\nThis can be written as a form of a constraint:\n\n", "index": 15, "text": "\\begin{align*}\n\\forall \\tau_j \\in \\mathcal{T}_{i,S},  \\forall \\tau_k  \\in & \\; \\mathcal{T}_{i,D} \\\\\nsim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i)&, \\Phi_{\\mathcal{T}}(\\tau_j)) \\\\\n&\\geq \\Delta(\\tau_j, \\tau_k) + sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau_k)) \\\\\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall\\tau_{j}\\in\\mathcal{T}_{i,S},\\forall\\tau_{k}\\in\" display=\"inline\"><mrow><mo>\u2200</mo><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mrow><mi>i</mi><mo>,</mo><mi>S</mi></mrow></msub><mo>,</mo><mo>\u2200</mo><msub><mi>\u03c4</mi><mi>k</mi></msub><mo>\u2208</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\;\\mathcal{T}_{i,D}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2004</mi><mo>\u2062</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mrow><mi>i</mi><mo>,</mo><mi>D</mi></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_{i},l_{i})\" display=\"inline\"><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle,\\Phi_{\\mathcal{T}}(\\tau_{j}))\" display=\"inline\"><mrow><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\Delta(\\tau_{j},\\tau_{k})+sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(%&#10;p_{i},l_{i}),\\Phi_{\\mathcal{T}}(\\tau_{k}))\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>,</mo><msub><mi>\u03c4</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\n\n\nThe cost of our deep embedding space $h^3$ is computed \nas the hinge loss of the most violating trajectory.\n\n", "itemtype": "equation", "pos": 46057, "prevtext": "\nIntuitively, this forces trajectories with higher DTW-MT distance from the ground truth to embed further \nthan those with lower distance. Enforcing all combinations of these constraints could grow exponentially large.\nInstead, similar to the cutting plane method for structural support vector machines \\citep{tsochantaridis2005large},\nwe find the most violating trajectory $\\tau' \\in \\mathcal{T}_{i,D}$\nfor each training pair of $(p_i, l_i, \\tau_i \\in \\mathcal{T}_{i,S})$\nat each iteration.\nThe most violating trajectory \nhas the highest similarity augmented with the loss scaled by a constant $\\alpha$:\n\n", "index": 17, "text": "$$\\tau'_i = {\\arg\\!\\max}_{\\tau \\in \\mathcal{T}_{i,D}} (sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau)) \n+ \\alpha \\Delta(\\tau_i, \\tau))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\tau^{\\prime}_{i}={\\arg\\!\\max}_{\\tau\\in\\mathcal{T}_{i,D}}(sim(\\Phi_{\\mathcal{P%&#10;},\\mathcal{L}}(p_{i},l_{i}),\\Phi_{\\mathcal{T}}(\\tau))+\\alpha\\Delta(\\tau_{i},%&#10;\\tau))\" display=\"block\"><mrow><msubsup><mi>\u03c4</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>=</mo><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><munder><mi>max</mi><mrow><mi>\u03c4</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mrow><mi>i</mi><mo>,</mo><mi>D</mi></mrow></msub></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\nThe average cost of each minibatch is back-propagated \nthrough all the layers of the deep neural network\nusing the AdaDelta \\citep{zeiler2012adadelta} algorithm.\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=\\columnwidth]{pretraining_v5.pdf}\n  \\end{center}\n  \\vskip -.07 in\n  \\caption{\\textbf{Pre-training lower layers:} Visualization of our pre-training approaches for\n  $h^{2,pl}$ and $h^{2,\\tau}$. For $h^{2,pl}$, our \n  algorithm pushes matching point-clouds and instructions\n  to be more similar. For $h^{2,\\tau}$, our algorithm\n  pushes trajectories with higher DTW-MT similarity to\n  be more similar.}\n  \\label{fig:joint_embedding_pretraining}\n\\end{figure}\n\n\n\n\n\n\\subsection{Pre-training Joint Point-cloud/Language Model}\n\\label{sec:pre_task}\n\n\nOne major advantage of modern deep learning methods is the use of unsupervised pre-training\nto initialize neural network parameters to a good starting point before the final supervised\nfine-tuning stage. \nPre-training helps these high-dimensional networks\nto avoid overfitting to the training data.\n\nOur lower layers $h^{2,pl}$ and $h^{2,\\tau}$ represent features extracted\nexclusively from the combination of point-clouds and language,  \nand from trajectories, respectively.\nOur pre-training method initializes $h^{2,pl}$ and $h^{2,\\tau}$\nas semantically meaningful embedding spaces similar to $h^3$,\n\nas shown later in Section~\\ref{sec:results}.\n\n\\iffalse\nIn our case, the top layer of our network $h^3$ represents \na joint point-cloud/language/trajectory feature space. \nThe immediate preceding layers, $h^{2,pl}$ and $h^{2,\\tau}$ represent features extracted\nexclusively from the combination of point cloud and language information \nand from trajectory information, respectively. \nIt is important to carefully pre-train these layers \nso they can be effectively projected to a common space.\n\\fi\n\n\\iffalse\nIf the top layer represents the joint Pointcloud-Language-Trajectory space $h^3$, \nthe layer directly below $h^{2,lp}$ and $h^{2,\\tau}$ represents a joint Pointcloud-Language space and a Trajectory space respectively.\nPrior to learning the top layer as shown above, we can pre-train the lower layers (Figure~\\ref{fig:joint_embedding_pretraining}).\n\\fi\n\n\nFirst, we pre-train the layers leading up to these layers using \nspare de-noising autoencoders \\citep{vincent2008extracting,zeiler2013rectified}.\nThen, our process for pre-training $h^{2,pl}$ is similar to \nour approach to fine-tuning a semantically meaningful embedding space for $h^3$\npresented above,\nexcept now we find the most violating language $l'$\nwhile still relying on a loss over the associated optimal trajectory:\n\n", "itemtype": "equation", "pos": 46328, "prevtext": "\n\n\n\nThe cost of our deep embedding space $h^3$ is computed \nas the hinge loss of the most violating trajectory.\n\n", "index": 19, "text": "\\begin{align*}\nL_{h^3}(p_i, l_i, \\tau_i) = |\\Delta(\\tau'_i, \\tau_i) + & sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau'_i))  \\\\\n- & sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_i,l_i), \\Phi_{\\mathcal{T}}(\\tau_i))|_+\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle L_{h^{3}}(p_{i},l_{i},\\tau_{i})=|\\Delta(\\tau^{\\prime}_{i},\\tau_{%&#10;i})+\" display=\"inline\"><mrow><msub><mi>L</mi><msup><mi>h</mi><mn>3</mn></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u0394</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>,</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_{i},l_{i}),\\Phi_{\\mathcal{T%&#10;}}(\\tau^{\\prime}_{i}))\" display=\"inline\"><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-\" display=\"inline\"><mo>-</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{\\mathcal{P},\\mathcal{L}}(p_{i},l_{i}),\\Phi_{\\mathcal{T%&#10;}}(\\tau_{i}))|_{+}\" display=\"inline\"><msub><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mo>+</mo></msub></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 49227, "prevtext": "\n\nThe average cost of each minibatch is back-propagated \nthrough all the layers of the deep neural network\nusing the AdaDelta \\citep{zeiler2012adadelta} algorithm.\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=\\columnwidth]{pretraining_v5.pdf}\n  \\end{center}\n  \\vskip -.07 in\n  \\caption{\\textbf{Pre-training lower layers:} Visualization of our pre-training approaches for\n  $h^{2,pl}$ and $h^{2,\\tau}$. For $h^{2,pl}$, our \n  algorithm pushes matching point-clouds and instructions\n  to be more similar. For $h^{2,\\tau}$, our algorithm\n  pushes trajectories with higher DTW-MT similarity to\n  be more similar.}\n  \\label{fig:joint_embedding_pretraining}\n\\end{figure}\n\n\n\n\n\n\\subsection{Pre-training Joint Point-cloud/Language Model}\n\\label{sec:pre_task}\n\n\nOne major advantage of modern deep learning methods is the use of unsupervised pre-training\nto initialize neural network parameters to a good starting point before the final supervised\nfine-tuning stage. \nPre-training helps these high-dimensional networks\nto avoid overfitting to the training data.\n\nOur lower layers $h^{2,pl}$ and $h^{2,\\tau}$ represent features extracted\nexclusively from the combination of point-clouds and language,  \nand from trajectories, respectively.\nOur pre-training method initializes $h^{2,pl}$ and $h^{2,\\tau}$\nas semantically meaningful embedding spaces similar to $h^3$,\n\nas shown later in Section~\\ref{sec:results}.\n\n\\iffalse\nIn our case, the top layer of our network $h^3$ represents \na joint point-cloud/language/trajectory feature space. \nThe immediate preceding layers, $h^{2,pl}$ and $h^{2,\\tau}$ represent features extracted\nexclusively from the combination of point cloud and language information \nand from trajectory information, respectively. \nIt is important to carefully pre-train these layers \nso they can be effectively projected to a common space.\n\\fi\n\n\\iffalse\nIf the top layer represents the joint Pointcloud-Language-Trajectory space $h^3$, \nthe layer directly below $h^{2,lp}$ and $h^{2,\\tau}$ represents a joint Pointcloud-Language space and a Trajectory space respectively.\nPrior to learning the top layer as shown above, we can pre-train the lower layers (Figure~\\ref{fig:joint_embedding_pretraining}).\n\\fi\n\n\nFirst, we pre-train the layers leading up to these layers using \nspare de-noising autoencoders \\citep{vincent2008extracting,zeiler2013rectified}.\nThen, our process for pre-training $h^{2,pl}$ is similar to \nour approach to fine-tuning a semantically meaningful embedding space for $h^3$\npresented above,\nexcept now we find the most violating language $l'$\nwhile still relying on a loss over the associated optimal trajectory:\n\n", "index": 21, "text": "$$l' = {\\arg\\!\\max}_{l \\in \\mathcal{L}} (sim(\\Phi_{\\mathcal{P}}(p_i), \\Phi_{\\mathcal{L}}(l)) + \\alpha \\Delta(\\tau, \\tau_i^*))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"l^{\\prime}={\\arg\\!\\max}_{l\\in\\mathcal{L}}(sim(\\Phi_{\\mathcal{P}}(p_{i}),\\Phi_{%&#10;\\mathcal{L}}(l))+\\alpha\\Delta(\\tau,\\tau_{i}^{*}))\" display=\"block\"><mrow><msup><mi>l</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><munder><mi>max</mi><mrow><mi>l</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo>,</mo><msubsup><mi>\u03c4</mi><mi>i</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nNotice that although we are training this embedding space to project from point-cloud/language data, \nwe guide learning using trajectory information.\n\n\n\n\nAfter the projections $\\Phi_{\\mathcal{P}}$ and $\\Phi_{\\mathcal{L}}$ are tuned,\nthe output of these two projections are added to form the output of layer $h^{2,pl}$ in the final feed-forward network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Pre-training Trajectory Model}\n\\label{sec:pre_traj}\n\nFor our task of inferring manipulation trajectories for novel objects, it is especially important\nthat similar trajectories $\\tau$ map to similar regions in the feature space defined by $h^{2,\\tau}$, \nso that trajectory embedding $h^{2,\\tau}$ itself is semantically meaningful\nand they can in turn be mapped to similar regions in $h^3$. \nStandard pretraining methods, such as sparse de-noising autoencoder \\citep{vincent2008extracting,zeiler2013rectified}\nwould only pre-train $h^{2,\\tau}$ to reconstruct individual trajectories. \n\n\nInstead, we employ pre-training similar to Sec.~\\ref{sec:pre_task}, \nexcept now we pre-train for only a single modality -- trajectory data.\n\nAs shown on right hand side of Fig.~\\ref{fig:joint_embedding_pretraining},\nthe layer that embeds to $h^{2,\\tau}$ is duplicated.\nThese duplicated embedding layers are treated as if they were two different\nmodalities,\nbut all their weights are shared and updated simultaneously. \nFor every trajectory $\\tau \\in \\mathcal{T}_{i,S}$, \nwe can again find the most violating $\\tau' \\in \\mathcal{T}_{i,D}$ \nand the minimize a similar cost function as we do for $h^{2,pl}$.\n\n\n\\iffalse\n\n", "itemtype": "equation", "pos": 49355, "prevtext": "\n\n", "index": 23, "text": "\\begin{align*}\nL_{h^{2,pl}}(p_i, l_i, \\tau_i) = |\\Delta(\\tau_i, \\tau') + & sim(\\Phi_{\\mathcal{P}}(p_i), \\Phi_{\\mathcal{L}}(l'))  \\\\\n- & sim(\\Phi_{\\mathcal{P}}(p_i), \\Phi_{\\mathcal{L}}(l_i)) |_+\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle L_{h^{2,pl}}(p_{i},l_{i},\\tau_{i})=|\\Delta(\\tau_{i},\\tau^{\\prime%&#10;})+\" display=\"inline\"><mrow><msub><mi>L</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>l</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u0394</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo>,</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{\\mathcal{P}}(p_{i}),\\Phi_{\\mathcal{L}}(l^{\\prime}))\" display=\"inline\"><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>l</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-\" display=\"inline\"><mo>-</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{\\mathcal{P}}(p_{i}),\\Phi_{\\mathcal{L}}(l_{i}))|_{+}\" display=\"inline\"><msub><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mo>+</mo></msub></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 51142, "prevtext": "\nNotice that although we are training this embedding space to project from point-cloud/language data, \nwe guide learning using trajectory information.\n\n\n\n\nAfter the projections $\\Phi_{\\mathcal{P}}$ and $\\Phi_{\\mathcal{L}}$ are tuned,\nthe output of these two projections are added to form the output of layer $h^{2,pl}$ in the final feed-forward network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Pre-training Trajectory Model}\n\\label{sec:pre_traj}\n\nFor our task of inferring manipulation trajectories for novel objects, it is especially important\nthat similar trajectories $\\tau$ map to similar regions in the feature space defined by $h^{2,\\tau}$, \nso that trajectory embedding $h^{2,\\tau}$ itself is semantically meaningful\nand they can in turn be mapped to similar regions in $h^3$. \nStandard pretraining methods, such as sparse de-noising autoencoder \\citep{vincent2008extracting,zeiler2013rectified}\nwould only pre-train $h^{2,\\tau}$ to reconstruct individual trajectories. \n\n\nInstead, we employ pre-training similar to Sec.~\\ref{sec:pre_task}, \nexcept now we pre-train for only a single modality -- trajectory data.\n\nAs shown on right hand side of Fig.~\\ref{fig:joint_embedding_pretraining},\nthe layer that embeds to $h^{2,\\tau}$ is duplicated.\nThese duplicated embedding layers are treated as if they were two different\nmodalities,\nbut all their weights are shared and updated simultaneously. \nFor every trajectory $\\tau \\in \\mathcal{T}_{i,S}$, \nwe can again find the most violating $\\tau' \\in \\mathcal{T}_{i,D}$ \nand the minimize a similar cost function as we do for $h^{2,pl}$.\n\n\n\\iffalse\n\n", "index": 25, "text": "$$\\tau' = {\\arg\\!\\max}_{t \\in \\mathcal{T}} (sim(\\Phi_{h^{2,\\tau}}(\\tau_i), \\Phi_{h^{2,\\tau}}(\\tau)) + \\Delta(\\tau, \\tau_i))$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\tau^{\\prime}={\\arg\\!\\max}_{t\\in\\mathcal{T}}(sim(\\Phi_{h^{2,\\tau}}(\\tau_{i}),%&#10;\\Phi_{h^{2,\\tau}}(\\tau))+\\Delta(\\tau,\\tau_{i}))\" display=\"block\"><mrow><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><munder><mi>max</mi><mrow><mi>t</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo>,</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n{\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {this function is too repetitive. I will probably remove these above}}]}}{}}\n\\fi\n\n\n\n\\subsection{Label Noise}\n\\label{sec:noise}\n\n\n\nWhen our data contains a significant number of noisy trajectories $\\tau$, e.g. due to crowd-sourcing (Sec.~\\ref{sec:crowdsourcing}), not all\ntrajectories should be trusted as equally appropriate, as will be shown\nin Sec.~\\ref{sec:experiments}.\n\n\nFor every pair of inputs $(p_i,l_i)$, we have \n$\\mathcal{T}_i=\\{\\tau_{i,1}, \\tau_{i,2}, ..., \\tau_{i,n_i} \\}$, \na set of trajectories submitted by the crowd for $(p_i,l_i)$. \nFirst, the best candidate label $\\tau^*_i \\in \\mathcal{T}_i$ for $(p_i,l_i)$ is selected as the one with the\nsmallest average trajectory distance to the others:\n\n", "itemtype": "equation", "pos": 51268, "prevtext": "\n\n", "index": 27, "text": "\\begin{align*}\nL_{h^{2,\\tau}} = |\\Delta(\\tau', \\tau_i) + & sim(\\Phi_{h^{2,\\tau}}(\\tau_i), \\Phi_{h^{2,\\tau}}(\\tau'))  \\\\\n- & sim(\\Phi_{h^{2,\\tau}}(\\tau_i), \\Phi_{h^{2,\\tau}}(\\tau))|_+\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle L_{h^{2,\\tau}}=|\\Delta(\\tau^{\\prime},\\tau_{i})+\" display=\"inline\"><mrow><msub><mi>L</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>=</mo><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u0394</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo>,</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{h^{2,\\tau}}(\\tau_{i}),\\Phi_{h^{2,\\tau}}(\\tau^{\\prime}))\" display=\"inline\"><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-\" display=\"inline\"><mo>-</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle sim(\\Phi_{h^{2,\\tau}}(\\tau_{i}),\\Phi_{h^{2,\\tau}}(\\tau))|_{+}\" display=\"inline\"><msub><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><msup><mi>h</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03c4</mi></mrow></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mo>+</mo></msub></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nWe assume that at least half of the crowd tried to give a reasonable demonstration.\nThus a demonstration with the smallest average distance to all other demonstrations\nmust be a good demonstration.\n{\\textcolor{Black}{{ We use the DTW-MT distance function (described later in Sec.~\\ref{sec:metric}) for our loss function $\\Delta(\\tau, \\bar{\\tau})$, but it could be replaced by any function that computes the loss of predicting $\\bar{\\tau}$ when $\\tau$ is the correct demonstration. }}}\n\n{\\textcolor{Black}{{ Using the optimal demonstration and a loss function $\\Delta(\\tau, \\bar{\\tau})$ for comparing demonstrations,  we find a set of trajectories $\\mathcal{T}_{i,S}$ that are relevant (similar) to this task and a set of trajectories $\\mathcal{T}_{i,D}$ that are irrelevant (dissimilar.) We can use thresholds $(t_S, t_D)$ determined by the expert to generate two sets from the pool of trajectories: \n", "itemtype": "equation", "pos": 52252, "prevtext": "\n{\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{PineGreen}{[\\textbf{JAE: {this function is too repetitive. I will probably remove these above}}]}}{}}\n\\fi\n\n\n\n\\subsection{Label Noise}\n\\label{sec:noise}\n\n\n\nWhen our data contains a significant number of noisy trajectories $\\tau$, e.g. due to crowd-sourcing (Sec.~\\ref{sec:crowdsourcing}), not all\ntrajectories should be trusted as equally appropriate, as will be shown\nin Sec.~\\ref{sec:experiments}.\n\n\nFor every pair of inputs $(p_i,l_i)$, we have \n$\\mathcal{T}_i=\\{\\tau_{i,1}, \\tau_{i,2}, ..., \\tau_{i,n_i} \\}$, \na set of trajectories submitted by the crowd for $(p_i,l_i)$. \nFirst, the best candidate label $\\tau^*_i \\in \\mathcal{T}_i$ for $(p_i,l_i)$ is selected as the one with the\nsmallest average trajectory distance to the others:\n\n", "index": 29, "text": "$$\\tau^*_i = {\\arg\\!\\min}_{\\tau \\in \\mathcal{T}_i} \\frac{1}{n_i} \\sum_{j=1}^{n_i} \\Delta(\\tau, \\tau_{i,j}) $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\tau^{*}_{i}={\\arg\\!\\min}_{\\tau\\in\\mathcal{T}_{i}}\\frac{1}{n_{i}}\\sum_{j=1}^{n%&#10;_{i}}\\Delta(\\tau,\\tau_{i,j})\" display=\"block\"><mrow><msubsup><mi>\u03c4</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03c4</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mi>i</mi></msub></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><msub><mi>n</mi><mi>i</mi></msub></mfrac></mrow></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo>,</mo><msub><mi>\u03c4</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": " \n", "itemtype": "equation", "pos": -1, "prevtext": "\nWe assume that at least half of the crowd tried to give a reasonable demonstration.\nThus a demonstration with the smallest average distance to all other demonstrations\nmust be a good demonstration.\n{\\textcolor{Black}{{ We use the DTW-MT distance function (described later in Sec.~\\ref{sec:metric}) for our loss function $\\Delta(\\tau, \\bar{\\tau})$, but it could be replaced by any function that computes the loss of predicting $\\bar{\\tau}$ when $\\tau$ is the correct demonstration. }}}\n\n{\\textcolor{Black}{{ Using the optimal demonstration and a loss function $\\Delta(\\tau, \\bar{\\tau})$ for comparing demonstrations,  we find a set of trajectories $\\mathcal{T}_{i,S}$ that are relevant (similar) to this task and a set of trajectories $\\mathcal{T}_{i,D}$ that are irrelevant (dissimilar.) We can use thresholds $(t_S, t_D)$ determined by the expert to generate two sets from the pool of trajectories: \n", "index": 31, "text": "$$\\mathcal{T}_{i,S} = \\{\\tau \\in \\mathcal{T} | \\Delta(\\tau_i^*, \\tau) < t_S \\}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{T}_{i,S}=\\{\\tau\\in\\mathcal{T}|\\Delta(\\tau_{i}^{*},\\tau)&lt;t_{S}\\}\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mrow><mi>i</mi><mo>,</mo><mi>S</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03c4</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>i</mi><mo>*</mo></msubsup><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><msub><mi>t</mi><mi>S</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": " }}}\n\n{\\textcolor{Black}{{ This method allows our model to be robust against noisy labels and also serves as a method of data augmentation  by also considering demonstrations given for other tasks  in both sets of $\\mathcal{T}_{i,S}$ and  $\\mathcal{T}_{i,D}$. }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[tb]\n\n\\begin{center}\n\\includegraphics[width=\\textwidth]{robobarista_system_isrr.png}\n\\end{center}\n\\vskip -0.2in\n\\caption{\n\\textbf{Screen-shot of Robobarista,} the crowd-sourcing platform running on Chrome browser.\nWe have built Robobarista platform for collecting a large number of crowd demonstrations for teaching the robot.\n}\n\\label{fig:robobarista}\n\n\\end{figure*}\n\n\n\n\n\\section{Loss Function for Manipulation Trajectory}\n\\label{sec:metric}\n\n\n\nFor both learning and evaluation, we need a function\nwhich accurately represents distance between two\ntrajectories.\nPrior metrics for trajectories consider only their translations\n(e.g. \\citep{koppula2013_anticipatingactivities})\nand not their rotations \\emph{and} gripper status.\n\n\n\nWe propose a new measure, which uses dynamic time warping, for evaluating manipulation trajectories.\nThis measure non-linearly warps two trajectories of \narbitrary lengths to produce a matching, then computes\ncumulative distance as the sum of cost of all matched waypoints.\n\n\nThe strength of this measure\nis that weak ordering is maintained among matched waypoints and that every waypoint \ncontributes to the cumulative distance.\n\nFor two trajectories of arbitrary lengths,\n$\\tau_A = \\{\\tau_A^{(i)}\\}_{i=1}^{m_A}$ and\n$\\tau_B = \\{\\tau_B^{(i)}\\}_{i=1}^{m_B}$\n\n\n\n, we define a matrix $D \\in \\mathbb{R}^{m_A \\times m_B}$, \nwhere $D(i, j)$ is the cumulative distance of an optimally-warped matching\nbetween trajectories up to index $i$ and $j$, respectively, of each trajectory.\nThe first column and the first row of\n$D$ is initialized as:\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \n", "index": 33, "text": "$$\\mathcal{T}_{i,D} = \\{\\tau \\in \\mathcal{T} | \\Delta(\\tau_i^*, \\tau) > t_D \\}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{T}_{i,D}=\\{\\tau\\in\\mathcal{T}|\\Delta(\\tau_{i}^{*},\\tau)&gt;t_{D}\\}\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mrow><mi>i</mi><mo>,</mo><mi>D</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03c4</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>i</mi><mo>*</mo></msubsup><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><msub><mi>t</mi><mi>D</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nwhere $c$ is a local cost function between two waypoints (discussed later). The rest of $D$\nis completed using dynamic programming:\n\n", "itemtype": "equation", "pos": 55310, "prevtext": " }}}\n\n{\\textcolor{Black}{{ This method allows our model to be robust against noisy labels and also serves as a method of data augmentation  by also considering demonstrations given for other tasks  in both sets of $\\mathcal{T}_{i,S}$ and  $\\mathcal{T}_{i,D}$. }}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[tb]\n\n\\begin{center}\n\\includegraphics[width=\\textwidth]{robobarista_system_isrr.png}\n\\end{center}\n\\vskip -0.2in\n\\caption{\n\\textbf{Screen-shot of Robobarista,} the crowd-sourcing platform running on Chrome browser.\nWe have built Robobarista platform for collecting a large number of crowd demonstrations for teaching the robot.\n}\n\\label{fig:robobarista}\n\n\\end{figure*}\n\n\n\n\n\\section{Loss Function for Manipulation Trajectory}\n\\label{sec:metric}\n\n\n\nFor both learning and evaluation, we need a function\nwhich accurately represents distance between two\ntrajectories.\nPrior metrics for trajectories consider only their translations\n(e.g. \\citep{koppula2013_anticipatingactivities})\nand not their rotations \\emph{and} gripper status.\n\n\n\nWe propose a new measure, which uses dynamic time warping, for evaluating manipulation trajectories.\nThis measure non-linearly warps two trajectories of \narbitrary lengths to produce a matching, then computes\ncumulative distance as the sum of cost of all matched waypoints.\n\n\nThe strength of this measure\nis that weak ordering is maintained among matched waypoints and that every waypoint \ncontributes to the cumulative distance.\n\nFor two trajectories of arbitrary lengths,\n$\\tau_A = \\{\\tau_A^{(i)}\\}_{i=1}^{m_A}$ and\n$\\tau_B = \\{\\tau_B^{(i)}\\}_{i=1}^{m_B}$\n\n\n\n, we define a matrix $D \\in \\mathbb{R}^{m_A \\times m_B}$, \nwhere $D(i, j)$ is the cumulative distance of an optimally-warped matching\nbetween trajectories up to index $i$ and $j$, respectively, of each trajectory.\nThe first column and the first row of\n$D$ is initialized as:\n\n", "index": 35, "text": "\\begin{align*}\nD(i,1) &= \\sum_{k=1}^{i}c(\\tau_A^{(k)}, \\tau_B^{(1)})\\;\\;\\; \\forall i \\in [1, m_A] \\\\\nD(1,j) &= \\sum_{k=1}^{j}c(\\tau_A^{(1)}, \\tau_B^{(k)})\\;\\;\\; \\forall j \\in [1, m_B] \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D(i,1)\" display=\"inline\"><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{k=1}^{i}c(\\tau_{A}^{(k)},\\tau_{B}^{(1)})\\;\\;\\;\\forall i\\in%&#10;[1,m_{A}]\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover></mstyle><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo rspace=\"10.9pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\u2200</mo><mi>i</mi></mrow></mrow></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msub><mi>m</mi><mi>A</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D(1,j)\" display=\"inline\"><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{k=1}^{j}c(\\tau_{A}^{(1)},\\tau_{B}^{(k)})\\;\\;\\;\\forall j\\in%&#10;[1,m_{B}]\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>j</mi></munderover></mstyle><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo rspace=\"10.9pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\u2200</mo><mi>j</mi></mrow></mrow></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msub><mi>m</mi><mi>B</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\n\n\n\n\n\n\n\nGiven the constraint that $\\tau_A^{(1)}$ is matched to $\\tau_B^{(1)}$,\nthe formulation ensures that every waypoint contributes to the\nfinal cumulative distance $D(m_A, m_B)$. Also,\ngiven a matched pair $(\\tau_A^{(i)}, \\tau_B^{(j)})$, no waypoint\npreceding $\\tau_A^{(i)}$ is matched to a waypoint succeeding $\\tau_B^{(j)}$,\nencoding weak ordering. \n\nThe pairwise cost function $c$ between matched waypoints $\\tau_A^{(i)}$ and $\\tau_B^{(j)}$\nis defined:\n\n{\n\\vskip -.1in\n\\small\n\n", "itemtype": "equation", "pos": 55640, "prevtext": "\nwhere $c$ is a local cost function between two waypoints (discussed later). The rest of $D$\nis completed using dynamic programming:\n\n", "index": 37, "text": "\\begin{align*}\nD(i,j) = & c(\\tau_A^{(i)}, \\tau_B^{(j)}) \\\\\n         & +  \\min\\{D(i-1, j-1), D(i-1,j), D(i, j-1)\\}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D(i,j)=\" display=\"inline\"><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle c(\\tau_{A}^{(i)},\\tau_{B}^{(j)})\" display=\"inline\"><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\min\\{D(i-1,j-1),D(i-1,j),D(i,j-1)\\}\" display=\"inline\"><mrow><mo>+</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\n\\vskip -.05in\n}\n\\noindent\nThe parameters $\\alpha, \\beta$ are for scaling translation and rotation errors, and\ngripper status errors, respectively. $\\gamma$ weighs the importance of a waypoint based on\nits distance to the object part.\n\\footnote{{\\textcolor{Black}{{In this work, we assign $\\alpha_T, \\alpha_R, \\beta, \\gamma$ values of $0.0075$ meters, $3.75^{\\circ}$, $1$ and $4$ respectively.}}}}\n\n\n\n\n\nFinally, as trajectories vary in length,\nwe normalize $D(m_A, m_B)$ by the number of waypoint pairs that contribute\nto the cumulative sum, $|D(m_A, m_B)|_{path^*}$ \n(i.e. the length of the optimal warping path), giving the final form:\n\n", "itemtype": "equation", "pos": 56250, "prevtext": "\n\n\n\n\n\n\n\n\nGiven the constraint that $\\tau_A^{(1)}$ is matched to $\\tau_B^{(1)}$,\nthe formulation ensures that every waypoint contributes to the\nfinal cumulative distance $D(m_A, m_B)$. Also,\ngiven a matched pair $(\\tau_A^{(i)}, \\tau_B^{(j)})$, no waypoint\npreceding $\\tau_A^{(i)}$ is matched to a waypoint succeeding $\\tau_B^{(j)}$,\nencoding weak ordering. \n\nThe pairwise cost function $c$ between matched waypoints $\\tau_A^{(i)}$ and $\\tau_B^{(j)}$\nis defined:\n\n{\n\\vskip -.1in\n\\small\n\n", "index": 39, "text": "\\begin{align*}\nc(\\tau_A^{(i)}, \\tau_B^{(j)}; \\alpha_T, \\alpha_R, & \\beta, \\gamma) =  w(\\tau_A^{(i)};\\gamma) w(\\tau_B^{(j)};\\gamma)  \\\\\n\\bigg(\\frac{d_T(\\tau_A^{(i)}, \\tau_B^{(j)})}{\\alpha_T}& + \\frac{d_R(\\tau_A^{(i)}, \\tau_B^{(j)})}{\\alpha_R}\\bigg) \\bigg(1 + \\beta d_G(\\tau_A^{(i)}, \\tau_B^{(j)}) \\bigg) \\\\\n\\text{where} \\hspace{5 mm} d_T(\\tau_A^{(i)},\\tau_B^{(j)}) &= ||(t_x, t_y, t_z)_{A}^{(i)} - (t_x, t_y, t_z)_{B}^{(j)}||_2  \\\\  \nd_R(\\tau_A^{(i)},\\tau_B^{(j)})  &= \\text{angle difference between $\\tau_{A}^{(i)}$ and $\\tau_B^{(j)}$} \\\\\n\\qquad d_G(\\tau_A^{(i)},\\tau_B^{(j)}) &= \\mathds{1}(g_A^{(i)}=g_B^{(j)}) \\\\\nw(\\tau^{(i)}; \\gamma) &= exp(-\\gamma \\cdot ||\\tau^{(i)}||_2) \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle c(\\tau_{A}^{(i)},\\tau_{B}^{(j)};\\alpha_{T},\\alpha_{R},\" display=\"inline\"><mrow><mi>c</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><msub><mi>\u03b1</mi><mi>T</mi></msub><mo>,</mo><msub><mi>\u03b1</mi><mi>R</mi></msub><mo>,</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\beta,\\gamma)=w(\\tau_{A}^{(i)};\\gamma)w(\\tau_{B}^{(j)};\\gamma)\" display=\"inline\"><mrow><mi>\u03b2</mi><mo>,</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>w</mi><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\bigg{(}\\frac{d_{T}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})}{\\alpha_{T}}\" display=\"inline\"><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>d</mi><mi>T</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mi>\u03b1</mi><mi>T</mi></msub></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{d_{R}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})}{\\alpha_{R}}\\bigg{)}%&#10;\\bigg{(}1+\\beta d_{G}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})\\bigg{)}\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>d</mi><mi>R</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mi>\u03b1</mi><mi>R</mi></msub></mfrac></mstyle><mo maxsize=\"210%\" minsize=\"210%\">)</mo><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mn>1</mn><mo>+</mo><mi>\u03b2</mi><msub><mi>d</mi><mi>G</mi></msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{where}\\hskip 14.226378ptd_{T}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})\" display=\"inline\"><mrow><mtext>where</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><msub><mi>d</mi><mi>T</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=||(t_{x},t_{y},t_{z})_{A}^{(i)}-(t_{x},t_{y},t_{z})_{B}^{(j)}||_%&#10;{2}\" display=\"inline\"><mrow><mi/><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>x</mi></msub><mo>,</mo><msub><mi>t</mi><mi>y</mi></msub><mo>,</mo><msub><mi>t</mi><mi>z</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>-</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>x</mi></msub><mo>,</mo><msub><mi>t</mi><mi>y</mi></msub><mo>,</mo><msub><mi>t</mi><mi>z</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle d_{R}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})\" display=\"inline\"><mrow><msub><mi>d</mi><mi>R</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{angle difference between $\\tau_{A}^{(i)}$ and $\\tau_{B}^{(%&#10;j)}$}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>angle difference between\u00a0</mtext><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mtext>\u00a0and\u00a0</mtext><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad d_{G}(\\tau_{A}^{(i)},\\tau_{B}^{(j)})\" display=\"inline\"><mrow><mi>\u2003\u2003</mi><mo>\u2062</mo><msub><mi>d</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c4</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>\u03c4</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathds{1}(g_{A}^{(i)}=g_{B}^{(j)})\" display=\"inline\"><mrow><mo>=</mo><mn>\ud835\udfd9</mn><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>g</mi><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>g</mi><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w(\\tau^{(i)};\\gamma)\" display=\"inline\"><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=exp(-\\gamma\\cdot||\\tau^{(i)}||_{2})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u22c5</mo><msub><mrow><mo fence=\"true\">||</mo><msup><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02705.tex", "nexttext": "\nThis distance function is used for noise-handling in our model\nand as the final evaluation metric.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Robobarista: crowd-sourcing platform}\n\\label{sec:crowdsourcing}\n\n\n\n\n\n\n\n\n\nIn order to collect a large number of manipulation demonstrations from the crowd, we built a crowd-sourcing web platform that we call Robobarista (see Fig.~\\ref{fig:robobarista}).\n\n\nIt provides a virtual environment where non-expert users can teach robots\nvia a web browser, without expert guidance or  physical presence with a robot and a target object.\n\nThe system simulates a situation where the user encounters \na previously unseen target object and a natural language instruction manual\nfor its manipulation.\n\n\nWithin the web browser, users are shown a point-cloud in the 3-D viewer on the left\nand a \\textit{manual} on the right.\nA manual may involve several instructions,\nsuch as ``Push down and pull the handle to open the door''.\nThe user's goal is to demonstrate how to manipulate\nthe object in the scene for each instruction.\n\nThe user starts by selecting one of the instructions on the right to demonstrate (Fig.~\\ref{fig:robobarista}).\nOnce selected, the target object part\nis highlighted and the trajectory \\textit{edit bar} appears below the 3-D viewer.\nUsing the \\textit{edit bar}, which works like a video editor, the user can \nplayback and edit the demonstration.\n\n\nThe trajectory representation,\nas a set of waypoints (Sec.~\\ref{sec:trajrep}), is directly shown on the \\textit{edit bar}.\nThe bar shows not only the set of waypoints (red/green) but also the interpolated waypoints (gray).\n\nThe user can click the `play' button or hover the cursor over the edit bar \nto examine the current demonstration.\nThe blurred trail of the current trajectory (\\textit{ghosted}) demonstration is\nalso shown in the 3-D viewer to show its full expected path.\n\n\n\n\n\n\n\n\n\nGenerating a full trajectory from scratch can be difficult for non-experts. \nThus, similar to  \\citet{forbes2014robot}, we provide a trajectory that\nthe system has already seen for another object as the initial starting trajectory to \nedit.\\footnote{We have made sure that it does not initialize with trajectories from other\nfolds to keep \\textit{5-fold cross-validation} in experiment section valid.}\n\n\nIn order to simulate a realistic experience of manipulation,\ninstead of simply showing a static point-cloud,\nwe have overlaid CAD models for parts such as `handle'\nso that functional parts actually move as the user tries to manipulate the object.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA demonstration can be edited by: 1) modifying the position/orientation\nof a waypoint, 2) adding/removing a waypoint,\nand 3) opening/closing the gripper.\nOnce a waypoint is selected, the PR2 gripper is shown with six directional arrows \nand three rings, used to modify the gripper's position and orientation, respectively.\nTo add extra waypoints,\nthe user can hover the cursor over an  interpolated  (gray) \nwaypoint on the \\textit{edit bar} and click the plus(+) button.\nTo remove an existing waypoint, the user can hover over it on the \\textit{edit bar}\nand click minus(-) to remove.\nAs modification occurs, the edit bar and ghosted demonstration are updated with a new interpolation.\nFinally, for editing the status (open/close) of the gripper, the user can simply click on the gripper.\n\nFor broader accessibility, all functionality of Robobarista, \nincluding 3-D viewer, is built using Javascript and WebGL.\n{\\textcolor{Black}{{We have made the platform available online ({\\small \\url{http://robobarista.cs.cornell.edu}}) }}}\n\n\n\n\\begin{figure*}[tb]\n\n\\begin{center}\n\\includegraphics[width=\\textwidth]{data_figure_isrr.png}\n\\end{center}\n\n\\caption{\n\\textbf{Examples from our dataset,} each of which  \nconsists of a natural language instruction\n(top), an object part in point-cloud representation (highlighted), and a manipulation\ntrajectory (below) collected via Robobarista. \nObjects range from kitchen appliances such as stove and rice cooker to\nurinals and sinks in restrooms. As our trajectories are collected from non-experts,\nthey vary in quality from being likely to complete the manipulation task\nsuccessfully (left of dashed line) to being unlikely to do so successfully\n(right of dashed line).\n}\n\\label{fig:data_ex}\n\n\\end{figure*}\n\n\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Robobarista Dataset}\n\\label{sec:data}\n\nIn order to test our model,\nwe have collected a dataset of 116 point-clouds of objects\nwith 249 object parts \n(examples shown in Figure~\\ref{fig:data_ex}).\n{\\textcolor{Black}{{Objects range from kitchen appliances such as stoves and rice cookers to bathroom hardware such as sinks and toilets. Figure~\\ref{fig:all_objects} shows a sample of 70 such objects.}}}\nThere are also a total of 250 natural language\ninstructions (in 155 manuals).\\footnote{Although not necessary for training our model,\n we also collected trajectories from the expert for evaluation purposes.}\nUsing the crowd-sourcing platform Robobarista,\nwe collected 1225 trajectories for these objects\nfrom 71 non-expert users on the Amazon Mechanical Turk.\nAfter a user is shown a 20-second instructional video, the user first completes \na 2-minute tutorial task.\nAt each session, the user was asked to complete 10 assignments \nwhere each consists of an object and a manual to be followed.\n\nFor each object, we took raw RGB-D images with the Microsoft Kinect sensor and\nstitched them using Kinect Fusion \\citep{izadi2011kinectfusion} to form a denser point-cloud\nin order to incorporate different viewpoints of objects.\nObjects range from kitchen appliances such as `stove', `toaster', and `rice cooker' to\n`urinal', `soap dispenser', and `sink' in restrooms. \nThe dataset is made available at {\\small \\url{http://robobarista.cs.cornell.edu}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\input{result_table} \n\n\\input{baselines} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Results and Discussions}\n\\label{sec:results}\n\n\nWe evaluated all models on our dataset using \\emph{5-fold cross-validation}\nand the results are in Table~\\ref{tab:results}. \nAll models which required hyper-parameter tuning used $10\\%$ of the training data as the validation set.\n\n\nRows list the models we tested including our model and baselines. Each column\nshows one of three evaluation metrics. The first two use dynamic time warping for\nmanipulation trajectory (DTW-MT) from Sec.~\\ref{sec:metric}.\nThe first column shows averaged DTW-MT for each instruction manual \nconsisting of one\nor more language instructions. The second column shows averaged DTW-MT for every\ntest pair $(p,l)$.\n\nAs DTW-MT values are not intuitive, we also include a measure of ``accuracy,''\nwhich shows the percentage of transferred trajectories with DTW-MT value less than $10$.\nThrough expert surveys, we found that when DTW-MT of manipulation trajectory\nis less than $10$, \nthe robot came up with a reasonable trajectory and will very likely be able to accomplish\nthe given task.\nAdditionally, Fig.~\\ref{fig:accuracy_plot} shows accuracies obtained \nby varying the threshold on the DTW-MT measure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\n\\textbf{Can manipulation trajectories be transferred from completely different objects?}\nOur full model gave $65.1\\%$ accuracy (Table~\\ref{tab:results}),\noutperforming every other baseline approach tested.\n\nFig.~\\ref{fig:success_trans} shows two examples of successful transfers and one \nunsuccessful transfer by our model.\nIn the first example, the trajectory for pulling down on a cereal dispenser\nis transferred to a coffee dispenser.\nBecause our approach to trajectory representation is based on the principal axis\n (Sec.~\\ref{sec:transfer_adapt}),\neven though the cereal and coffee dispenser handles are located and oriented differently, the transfer is a success.\nThe second example shows a successful transfer from a DC power supply to a slow\ncooker, which have ``knobs'' of similar shape. The transfer was successful despite\nthe difference in instructions (``Turn the switch..'' and ``Rotate the knob..'')\nand object type. This highlights the advantages of our end-to-end approach over relying\non semantic classes for parts and actions.\n\n\n\n\\iffalse\n\nThe last example in Fig.~\\ref{fig:success_trans} shows an unsuccessful\ntransfer. Despite the similarity in two instructions, transfer was unsuccessful because\nthe grinder's knob was facing towards the front and the speaker's knob was facing upwards.\nWe fixed the $z$-axis along gravity because point-clouds are noisy and gravity \ncan affect some manipulation tasks,\nbut a more reliable method for finding the object coordinate frame and a \nbetter 3-D sensor should allow for more accurate transfers. \n{\\ifthenelse{\\boolean{include-notes}}  {\\textcolor{Bittersweet}{[\\textbf{IAN: {I feel like this is highlighting a weakness of the method that nobody would notice if we didn't say it. It might be better to find a more ``reasonable'' failure where people can understand why the robot made a mistake. Also, it might be good to show some cases where we  succeed and other approaches fail (maybe another fig?)}}]}}{}}\n\\fi\n\n\n{\\textcolor{Black}{{ The last example in Fig.~\\ref{fig:success_trans} shows a potentially unsuccessful transfer. Despite the similarity in two instructions and similarity in required counterclockwise motions,  the transferred motion might not be successful. While the knob on radiator must be grasped in the middle, the rice cooker has a handle that extends sideways, requiring it to be grasped off-center. For clarity of visualization in figures,  we have overlaid CAD models over some noisy point-clouds. Many of the object parts were too small and/or too glossy for the Kinect sensor. We believe that a better 3-D sensor would allow for more accurate transfers.  On the other hand, it is interesting to note that the transfer in opposite direction from the radiator knob to the rice cooker handle  may have yielded a correct manipulation. }}}\n\n\n\n\\begin{figure}[tb]\n\\begin{center}\n\\includegraphics[width=\\columnwidth,trim={0cm 5cm 0cm 0cm}, clip]{compare_transfers_v2.pdf}\n\\end{center}\n\\vskip -.1in\n\\caption{\n\\textbf{Comparisons of transfers} between our model and the baseline \n(deep multimodal network without embedding \\cite{sung_robobarista_2015}).\nIn these three examples, our model successfully finds correct manipulation trajectory \nfrom these objects while the other one does not.\nGiven the lever of the toaster, our algorithm finds similarly slanted part from the rice cooker\nwhile the other model finds completely irrelevant trajectory.\nFor the opening action of waffle maker, trajectory for paper cutter is correctly identified\nwhile the other model transfers from a handle that has incompatible motion.\n}\n\\label{fig:compare_transfer}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\begin{center}\n    \n    \\includegraphics[width=\\textwidth,trim={0cm 0cm 0cm .5cm},clip]{pr2_demo_v2.pdf}\n\\end{center}\n\n\\caption{\n\\textbf{Examples of transferred trajectories} being executed on PR2.\nOn the left, PR2 is able to rotate the `knob' to turn the lamp on.\nIn the third snapshot, using two transferred trajectories, PR2 is able to hold the cup below the `nozzle'\nand press the `lever' of `coffee dispenser'.  \nIn the last example, PR2 is frothing milk by pulling down on the lever, and is able to prepare a cup of latte with many transferred trajectories.\n}\n\\label{fig:robotic_exp}\n\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\noindent\n\\textbf{Can we crowd-source the teaching of manipulation trajectories?}\nWhen we trained our full model with expert demonstrations, which were collected for evaluation purposes, \nit performed at $56.5\\%$ compared to $65.1\\%$ by our model trained with crowd-sourced data.\nEven though non-expert demonstrations can carry significant noise, as shown in last two examples of Fig.~\\ref{fig:data_ex},\nour noise-handling approach allowed our model to take advantage of the larger, less\naccurate crowd-sourced dataset.\nNote that all of our crowd users are true non-expert users from Amazon Mechanical Turk.\n\n\\noindent\n\\textbf{Is segmentation required for the system?}\nEven with the state-of-the-art techniques \\citep{felzenszwalb2010object,krizhevsky2012imagenet}, \ndetection of `manipulable' object parts such as `handles' and \n`levers' in a point-cloud is by itself \na challenging problem \\citep{lai_icra14}.\nThus, we rely on human experts to pre-label parts of the object to be manipulated.\nThe point-cloud of the scene is over-segmented into thousands of supervoxels,\nfrom which the expert chooses the part of the object to be manipulated.\nEven with expert input, such segmented point-clouds are still extremely noisy\nbecause of sensor failures, e.g. on glossy surfaces. \n\n\n\\noindent\n\\textbf{Is intermediate object part labeling necessary?}\nA multiclass SVM trained on object part labels was able to obtain \nover $70\\%$ recognition accuracy in \nclassifying five major classes of object parts\n(`button', `knob', `handle', `nozzle', `lever'.)\nHowever, the \\emph{Object Part Classifier} baseline, based on this classification,\nperformed at only $23.3\\%$ accuracy for actual trajectory transfer, outperforming chance by merely 12.1\\%, and\nsignificantly underperforming our model's result of 65.1\\%. This shows\nthat object part labels alone are not sufficient to enable manipulation motion\ntransfer, while our model, which makes use of richer information, does a much better\njob.\n\n\n\n\n\n\\noindent\n\\textbf{Can features be hand-coded? What does learned deep embedding space represent?}\n\n\nEven though we carefully designed state-of-the-art task-specific features for the\nSSVM and LSSVM models, \n\nthese models only gave at most 40.8\\% accuracy.\nThe \\textit{task similarity} method gave a better result of $53.7\\%$, but \nit requires access to \\emph{all} of the raw training data (point-clouds, language, and trajectories) at test time,\nwhich leads to heavy computation at test time and requires a large amount of storage \nas the size of training data increases. Our approach, by contrast, requires only\nthe trajectory data, and a low-dimensional representation of the point-cloud\nand language data, which is much less expensive to store than the raw data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows that it is extremely difficult to find a good set of features which \nproperly combines these three modalities.\nOur multimodal embedding model does not require hand-designing such features,\ninstead learning a joint embedding space as shown by our visualization of the top layer $h^3$ in Figure~\\ref{fig:embed_vis}.\n\nThis visualization is created by projecting all training data (point-cloud/language pairs and trajectories) \nof one of the cross-validation folds to $h^3$, \nthen embedding them to 2-dimensional space \nusing t-SNE \\citep{van2008visualizing}.\nAlthough previous work \\citep{sung_robobarista_2015} was able to visualize several nodes in the top layer,\nmost were difficult to interpret.\nWith our model, we can embed all our data and \nvisualize all the layers (see Figs.~\\ref{fig:embed_vis} and \\ref{fig:task_embed_vis}).\n\n\n\nOne interesting result is that our system was able to naturally learn that ``nozzle'' and ``spout''\nare effectively synonyms for purposes of manipulation. It clustered these \ntogether in the lower-right of Fig.~\\ref{fig:embed_vis} based solely on the\nfact that both are associated with similar point-cloud shapes and manipulation\ntrajectories. \nAt the same time, it also identified one exception, a small cluster of ``nozzles'' in the center\nof Fig.~\\ref{fig:embed_vis} which require\ndifferent manipulation motions.\n\n\n\nIn addition to the aforementioned cluster in the\nbottom-right of Fig.~\\ref{fig:embed_vis}, we see several\nother logical clusters. \nImportantly, we can see that our embedding maps vertical\nand horizontal rotation operations to very different \nregions of the space -- roughly 12 o'clock and 8 o'clock\nin Fig.~\\ref{fig:embed_vis}, respectively. Even though\nthese have nearly identical language instructions, our\nalgorithm learns to map them differently based on their\npoint-clouds, mapping nearby the appropriate manipulation\ntrajectories.\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=0.95\\columnwidth,trim={1.9cm 8cm 2cm 9.5cm},clip]{accuracy_plot_v8.pdf}\n  \\end{center}\n  \\vskip -.15in\n  \\caption{\n    \\textbf{Thresholding Accuracy:} \n     Accuracy-threshold graph showing results of varying thresholds on DTW-MT scores. Our algorithm consistently\n     outperforms the previous approach \\citep{sung_robobarista_2015} and an\nLMNN-like cost function \\citep{weinberger2005distance}. \n     }\n  \\label{fig:accuracy_plot}\n\\end{figure}\n\n\n\\begin{figure*}[tb]\n  \\begin{center}\n    \n    \\includegraphics[width=\\textwidth]{projection_v10.pdf}\n  \\end{center}\n  \n  \\caption{\n    \\textbf{Learned Deep Point-cloud/Language/Trajectory Embedding Space: }\n     Joint embedding space $h^3$ after the network is fully fine-tuned, \n     visualized in 2d using t-SNE \\citep{van2008visualizing} .\n     Inverted triangles represent projected point-cloud/language pairs, circles represent projected trajectories. \n     \n     The occupancy grid representation of object part point-clouds is shown \n     in \\textcolor{darkgreen}{green} in \\textcolor{blue}{blue} grids.\n     Among the two occupancy grids (Sec.~\\ref{sec:preprocessing}), \n     we selected the one that is more visually parsable for each object.\n     The legend at the bottom right shows classifications of object parts by an expert, \n     collected for the purpose of building a baseline.\n     As shown by result of this baseline (object part classifier in Table~\\ref{tab:results}), \n     these labels do not necessarily correlate well with the actual manipulation motion.\n     Thus, full separation according to the labels defined in the legend is not optimal and\n     will not occur in this figure or Fig.~\\ref{fig:task_embed_vis}.\n     These figures are best viewed in color. \n     }\n  \\label{fig:embed_vis}\n   \n\n\\end{figure*}\n\n\n\n\n\n\n\\noindent\n\\textbf{Should cost function be loss-augmented?}\nWhen we changed the cost function for pre-training $h^2$ and fine-tuning $h^3$\nto use a constant margin of $1$ between relevant $\\mathcal{T}_{i,S}$ and \nirrelevant $\\mathcal{T}_{i,D}$ demonstrations \\citep{weinberger2005distance},\nperformance drops to $55.5\\%$.\nThis loss-augmentation is also visible in our embedding space.\nNotice the purple cluster around the 6 o'clock region of Fig.~\\ref{fig:embed_vis},\nand the lower part of the cluster in the 5 o'clock region.\nThe purple cluster represents tasks and demonstrations related to pushing a bar (often found on soda fountains),\nand the lower part of the red cluster represents the task of holding a cup below the nozzle.\nAlthough the motion required for one task would not be replaceable by the other,\nthe motions and shapes are very similar, especially compared to \nmost other motions\ne.g. turning a horizontal knob.\n\n\\noindent\n\\textbf{Is pre-embedding important?}\nAs seen in Table~\\ref{tab:results}, without any pre-training\nour model gives an accuracy of only $54.2\\%$. Pre-training\nthe lower layers with the conventional stacked \nde-noising auto-encoder (SDA) algorithm \\citep{vincent2008extracting,zeiler2013rectified}\nincreases performance to $62.6\\%$, still significantly\nunderperforming our pre-training algorithm, \nwhich gives $65.1\\%$. \nThis shows that our metric embedding pre-training approach provides a better\ninitialization for an embedding space than SDA.\n\n\n\nFig.~\\ref{fig:task_embed_vis} shows the \njoint point-cloud/language embedding $h^{2,pl}$\nafter the network is initialized using our pre-training\nalgorithm and then fine-tuned using our cost function\nfor $h^3$. \nWhile this space is not as\nclearly clustered as $h^3$ shown in Fig.~\\ref{fig:embed_vis}, \n\n\nwe note that point-clouds tend to\nappear in the more general center of the space, while natural language \ninstructions appear around the more-specific edges. This makes sense because\none point-cloud might afford many possible actions, while language instructions\nare much more specific.\n\n\n\n\n\n\\noindent\n\\textbf{Does embedding improve efficiency?}\n\nThe previous model \\citep{sung_robobarista_2015} had $749,638$ parameters to be\nlearned, while our model has only \n$418,975$ (and still gives better performance.)\n\n\n\nThe previous model had to compute joint point-cloud/language/trajectory features \nfor all combinations of the current point-cloud/language pair with \\emph{each} candidate trajectory\n(i.e. all trajectories in the training set) to infer an optimal trajectory. \nThis is inefficient and does not scale well with the number of training\ndatapoints.\n\n\n\n\nHowever, our model pre-computes the projection of all trajectories into $h^3$.\nInference in our model then requires only projecting\nthe new point-cloud/language combination to $h^3$ once and \nfinding the trajectory with maximal similarity in this embedding. \n\n\nIn practice, this results in a significant improvement in efficiency, \ndecreasing the average time to infer a trajectory\nfrom $2.3206$ms to $0.0135$ms, a speed-up of about $171$x.\nTime was measured on the same hardware, with a GPU (GeForce GTX Titan X), using the Theano library \\citep{Bastien-Theano-2012}.\nWe measured inference times $10000$ times for first test fold, which has a pool of $962$ trajectories.\nTime to preprocess the data and time to load into GPU memory was not included in this measurement.\nWe note that the only part of our algorithm's runtime\nwhich scales up with the amount of training data is the nearest-neighbor\ncomputation, for which there exist many efficient algorithms \\citep{flann_pami_2014}.\nThus, our algorithm could be scaled to much larger datasets, allowing it to handle a wider\nvariety of tasks, environments, and objects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Robotic Validation} \n\\label{sec:robotic_exp}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\textcolor{Black}{{ To validate the concept of part-based manipulation trajectory transfer in a real-world setting, we tested our algorithm on our PR2 robot.  To ensure real transfers, we tested with four objects the algorithm had never seen before -- a coffee dispenser, coffee grinder, lamp, and espresso  machine. }}}\n\n\\iffalse\n{\\textcolor{Black}{{ In order to test that our idea of part-based transfer of manipulation trajectories are actually feasible on real robots, we have tested our algorithm on our PR2 robot with four objects our robot has never seen  before -- coffee dispenser, lamp, coffee grinder, espresso machine.  }}}\n\\fi\n\n{\\textcolor{Black}{{ The PR2 robot has two 7DoF arms, an omni-directional base, and   many sensors including a Microsoft Kinect, stereo cameras, and a tilting laser scanner. For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller \\cite{bollini2011bakebot} in ROS \\cite{quigley2009ros}. }}}\n\n{\\textcolor{Black}{{ For each object,  the robot is presented with a segmented point-cloud along with a natural language text manual, with each step in the manual associated with a segmented part in the point-cloud. Once our algorithm outputs a trajectory (transferred from a completely different object), we find the manipulation frame for the part's point-cloud by using its principal axis (Sec.~\\ref{sec:preprocessing}). Then, the transferred trajectory can be executed relative to the part using this coordinate frame, without any modification to the trajectory. }}}\n\n\n\n\n\n\n{\\textcolor{Black}{{ The chosen manipulation trajectory, defined as a set of waypoints, is converted to a smooth and densely interpolated trajectory (Sec.~\\ref{sec:prob_form}.) The robot first computes and execute a collision-free motion  to the starting point of the manipulation trajectory. Then, starting from this first waypoint, the interpolated trajectory is executed.  For these experiments, we placed the robot in reach of the object, but one could also find a location using a motion planner that would make all  waypoints of the manipulation trajectory reachable. }}}\n\n{\\textcolor{Black}{{ Some of the examples of successful execution on a PR2 robot are shown in Fig.~\\ref{fig:robotic_exp} and in video at the project website:  {\\small \\url{http://robobarista.cs.cornell.edu/}}. For example, a manipulation trajectory from the task of ``turning on a light switch'' is transferred to  the task of ``flipping on a switch to start extracting espresso'',  and a trajectory for turning on DC power supply (by rotating clockwise) is transferred to turning on the floor lamp. These demonstrations shows that part-based transfer of manipulation trajectories is feasible without any modification to the source trajectories by carefully choosing their representation and coordinate frames (Sec.~\\ref{sec:transfer_adapt}). }}}\n\n\n\\begin{figure*}[tb]\n  \\begin{center}\n    \n    \\includegraphics[width=\\textwidth]{task_projection_v11.pdf}\n  \\end{center}\n  \n  \\caption{ \\textbf{Learned Point-cloud/Language Space: }\n\t  Visualization of the point-cloud/language layer $h^{2,lp}$ in 2d using t-SNE \\citep{van2008visualizing} \n     after the network is fully fine-tuned.\n     Inverted triangles represent projected point-clouds and circles represent projected instructions. \n     A subset of the embedded points are randomly selected for visualization.\n     Since 3D point-clouds of object parts are hard to visualize, \n     we also include a snapshot of a point-cloud showing the whole object.\n     Notice correlations in the motion required to manipulate the object or\n     follow the instruction among nearby point-clouds and natural language.\n     \n     \n    }\n  \\label{fig:task_embed_vis}\n  \n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion and Future Work}\n\n\n{\\textcolor{Black}{{ In this work, we introduce a novel approach to predicting manipulation trajectories via part based transfer, which allows robots to successfully manipulate even objects they have never seen before. We formulate this as a structured-output problem and  approach the problem of inferring manipulation trajectories for novel objects by jointly embedding point-cloud, natural language, and trajectory data into a common space using a deep neural network. We introduce a method for learning a common representation of multimodal data using a new loss-augmented cost function, which learns a semantically meaningful embedding from data. We also introduce a method for pre-training the network's lower layers, learning embeddings for subsets of modalities,  and show that it outperforms standard pre-training algorithms. Learning such an embedding space allows efficient inference by comparing the embedding of a new point-cloud/language pair  against pre-embedded demonstrations. We introduce our crowd-sourcing platform, Robobarista,  which allows non-expert users to easily give manipulation demonstrations over the web. This enables us to collect a large-scale dataset of 249 object parts with 1225 crowd-sourced demonstrations, on which our algorithm outperforms all other methods tried. We also verify on our robot that even manipulation trajectories transferred from completely different objects can be used to successfully manipulate novel objects the robot has never seen before. }}}\n\n\n\n\n\\iffalse\n{\\textcolor{Black}{{ For a large fraction of objects which the robot has never seen before, our model outperforms other models in finding correct manipulation trajectories. However, we do not claim that our model can find and execute manipulation trajectories for all objects.  The contribution of this work is in the novel approach to manipulation planning which enables robots to manipulate objects they have never seen before.}}}\n\\fi\n\n{\\textcolor{Black}{{ While our model is able to give correct manipulation trajectories for most of the objects we tested it on, outperforming all other approaches tried,  open-loop execution of a pose trajectory may not be enough to correctly manipulate some objects. }}}\n{\\textcolor{Black}{{ For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback \\citep{wieland2009combining,vina2013predicting}  in order for the execution to adapt exactly to the object. For example, some dials or buttons have to be rotated or pushed until they click,  and each might require a different amount of displacement to accomplish this task.}}}\n{\\textcolor{Black}{{For such tasks, the robot would have to use this feedback to adapt its  trajectory in an online fashion.}}}\n\n{\\textcolor{Black}{{Our current model also only takes into account the object part and desired action in question. For some objects, a correct trajectory according to these might still collide with other parts of the environment. Once again, solving this problem would require adapting the manipulation trajectory after it's selected, and is an interesting direction for future work.}}}\n\n\\iffalse\n{\\textcolor{Black}{{ Even if the our model has identified correct manipulation trajectory,  which would have correctly manipulated the object when there is no obstacle, there could be situations where there are some obstacles between the robot and the object being manipulated. While still being able to complete the task,  manipulation trajectory has to be adopted \\cite{SOMETHINGXXXXI}}}}\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\\iffalse\n\nIn this work, we introduced a novel approach to predicting manipulation trajectories via part based transfer,\nwhich allowed robots to successfully manipulate objects it has never seen before.\nWe formulated it as a structured-output problem and \npresented a deep learning model capable of handling three completely different modalities\nof point-cloud, language, and trajectory while dealing with large noise in the manipulation demonstrations.\nWe also designed a crowd-sourcing platform Robobarista \nthat allowed non-expert users to easily give manipulation demonstration over the web.\nOur deep learning model was evaluated against many baselines\non a large dataset of 249 object parts with 1225 crowd-sourced demonstrations.\n\n\nIn this work, we approach the problem of inferring manipulation trajectories\nfor novel objects by jointly embedding point-cloud, natural language, and trajectory data into a common space\nusing a deep neural network.\nWe introduce a method for learning a common representation of multimodal data\nwith loss-augmented cost function, which produces semantically meaningful embedding.\nWe also introduce a method for pre-training the network's lower layers, learning embeddings for subsets of modalities, \nand show that it outperforms standard pre-training algorithms.\nLearning such an embedding space allows efficient inference\nby comparing the embedding of a new point-cloud/language pair \nagainst pre-embedded demonstrations.\nWe test our algorithm on the Robobarista Dataset \\citep{sung_robobarista_2015}\nand show that our approach improves accuracy, despite having only\nhalf of the number of learned parameters and being \nmuch more computationally efficient (about $171$x faster) than the \nstate-of-the-art result.\n\\fi\n\n\n\n\\begin{figure*}[tb]\n\\begin{center}\n  \\includegraphics[width=\\textwidth]{all_objects.jpg}\n\\end{center}\n\\caption{\n  \\textbf{Examples of objects and object parts.} Each image shows the point cloud\n  representation of an object. We overlaid some of its parts by CAD models for online Robobarista crowd-sourcing platform.\n  Note that the actual underlying point-cloud of object parts contains much more noise and is not clearly segmented,\n  and none of the models have access to overlaid model for inferring manipulation trajectory.\n  \n}\n\\label{fig:all_objects}\n\\end{figure*}\n\n\n\n\n\n\n\\section*{ACKNOWLEDGMENT}\n\nWe thank Joshua Reichler for building the initial prototype of the crowd-sourcing\nplatform. \nWe thank Ross Knepper and Emin G\\\"un Sirer for useful discussions. We thank\nNVIDIA Corporation for the donation of the Tesla K40\nGPU used for this research. This work was supported by\nNRI award 1426452, ONR award N00014-14-1-0156, and\nby Microsoft Faculty Fellowship and NSF Career Award to\none of us (Saxena).\n\n\n{\n\n\n\n\\bibliography{writeup}\n\\bibliographystyle{abbrvnat}\n}\n\n\n", "itemtype": "equation", "pos": 57577, "prevtext": "\n\\vskip -.05in\n}\n\\noindent\nThe parameters $\\alpha, \\beta$ are for scaling translation and rotation errors, and\ngripper status errors, respectively. $\\gamma$ weighs the importance of a waypoint based on\nits distance to the object part.\n\\footnote{{\\textcolor{Black}{{In this work, we assign $\\alpha_T, \\alpha_R, \\beta, \\gamma$ values of $0.0075$ meters, $3.75^{\\circ}$, $1$ and $4$ respectively.}}}}\n\n\n\n\n\nFinally, as trajectories vary in length,\nwe normalize $D(m_A, m_B)$ by the number of waypoint pairs that contribute\nto the cumulative sum, $|D(m_A, m_B)|_{path^*}$ \n(i.e. the length of the optimal warping path), giving the final form:\n\n", "index": 41, "text": "\\begin{align*}\ndistance(\\tau_A, \\tau_B) = \\frac{D(m_A, m_B)}{|D(m_A, m_B)|_{path^*}}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex37.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle distance(\\tau_{A},\\tau_{B})=\\frac{D(m_{A},m_{B})}{|D(m_{A},m_{B}%&#10;)|_{path^{*}}}\" display=\"inline\"><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>A</mi></msub><mo>,</mo><msub><mi>\u03c4</mi><mi>B</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>A</mi></msub><mo>,</mo><msub><mi>m</mi><mi>B</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mrow><mo stretchy=\"false\">|</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>A</mi></msub><mo>,</mo><msub><mi>m</mi><mi>B</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msup><mi>h</mi><mo>*</mo></msup></mrow></msub></mfrac></mstyle></mrow></math>", "type": "latex"}]