[{"file": "1601.01413.tex", "nexttext": "\n   where, as in Assumption 1, $c = \\min\\left(\\sup \\left( {{\\rm Supp}\\,}(\\tau) \\right) - {{\\rm E}\\,}_G[\\tau], {{\\rm E}\\,}_G[\\tau] - \\inf \\left({{\\rm Supp}\\,}(\\tau)\\right)\\right)$.\n\\end{definition}\n\n\n\nThe target parameter adapts naturally to the closest value in an interval surrounding ${{\\rm E}\\,}_G[\\tau]$, where the width of the interval is defined by the support of $\\tau$. We formalize how each $\\theta_{F_n}$ is a local average treatment effect.\n\n\\begin{prop}\nThere exists a nonnegative weighting associated with each empirical distribution $F_n$, $w_{F_n}$, such that across all $F_n$, $\\theta_{F_n} = \\frac{{{\\rm E}\\,}_G[w_{F_n} \\tau]}{{{\\rm E}\\,}_G[w_{F_n}]}$.\n\\end{prop}\nA proof of Proposition 1 follows directly from the fact that a weighted mean can obtain any value in the interval defined by the infimum and supremum of its distribution's support. We now prove the the superefficiency of $\\hat\\theta$.\n\n\n\n\n\n\n\n\\begin{prop}\nSuppose that Assumption 1 holds. Then for any root-$n$ consistent and asymptotically normal estimator of ${{\\rm E}\\,}_G[\\tau]$, $\\hat\\theta$, $\\sqrt n(\\hat\\theta - \\theta_{F_n}) = o_p(1).$ \n\\end{prop}\n\\begin{proof}\nThe author thanks Jas Sekhon for suggesting the following proof strategy. Decompose $\\hat\\theta$ into $\\tilde\\theta=\\mathcal{N}({{\\rm E}\\,}_G[\\tau],\\sigma^2/n)$ and $u = o_p(n^{-1/2})$, so that $\\hat\\theta = \\tilde\\theta + u$. Since $(\\tilde\\theta -  \\theta_{F_n})$ is $o_p(a_n)$ for any positive sequence $(a_n)$, the rate of convergence of $\\hat \\theta$ is at worst governed by the bound ensured by $u$'s $o_p(n^{-1/2})$ convergence. To prove the claim, note that for any positive $\\epsilon$, $\n\\Pr\\left( \\frac{| \\tilde\\theta -  \\theta_{F_n} |}{a_n} \\geq \\epsilon \\right) \\leq \\Pr\\left( \\tilde\\theta - \\theta_{F_n} \\neq 0 \\right) = 2\\Phi(-c \\sqrt{n}/\\sigma).\n$\nSince $\\lim_{n\\rightarrow \\infty}  2\\Phi(-c \\sqrt{n}/\\sigma) = 0$, $(\\tilde\\theta -  \\theta_{F_n})$ is $o_p(a_n)$. Thus $\\hat\\theta - \\theta_{F_n} =  o_p(a_n) +  o_p(n^{-1/2}) = o_p(n^{-1/2})$, yielding the result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}\n\nWhen an additional regularity condition is imposed on the convergence of $\\hat\\theta$ to normality, a stronger result can be obtained about the rate of mean square convergence.\n\\begin{prop}\nSuppose that $\\hat\\theta$ obeys $\\sqrt n (\\hat\\theta - {{\\rm E}\\,}_G[\\tau]) = \\mathcal{N}(0,\\sigma^2) + \\epsilon$, where ${{\\rm E}\\,}_G[\\epsilon^2] = o(n^{-1/2}).$  Then ${{\\rm E}\\,}_G[(\\hat\\theta - \\theta_{F_n})^2]  = o(n^{-1}).$ \n\\end{prop}\n\\begin{proof}\n\n\n\n\n\n\nWe will show that the mean square error of $(\\tilde\\theta -  \\theta_{F_n})$ converges to zero sufficiently quickly, implying that the rate of convergence of $\\hat \\theta$ is at worst governed by the mean square error bound ensured by $\\epsilon$'s convergence rate. To obtain the rate of convergence of the mean square error of $\\tilde\\theta$, we integrate over its squared deviation from the target parameter.  Within $c$ of ${{\\rm E}\\,}_G[\\tau]$, the squared deviation is zero, thus we need only integrate over the squared deviation over the tails of the normal distribution. To ease calculations, we obtain an upper bound by integrating over the squared deviation from ${{\\rm E}\\,}_G[\\tau]$, rather than from $\\theta_{F_n}$:\n\n", "itemtype": "equation", "pos": 4440, "prevtext": "\n\n\n\n\n\\sloppy\n\n\n\\title{Local average causal effects and superefficiency}\n\\author{Peter M. Aronow\\thanks{Peter M. Aronow is Assistant Professor, Departments of Political Science and Biostatistics, Yale University, 77 Prospect St., New Haven, CT 06520 (Email: peter.aronow@yale.edu). The author thanks Don Green, Cyrus Samii, Jas Sekhon  and Mark van der Laan for helpful comments. All remaining errors are the author's responsibility.}} \n\n\\maketitle\n\n\\begin{abstract}\nRecent approaches in causal inference have proposed estimating average causal effects that are local to some subpopulation, often for reasons of efficiency. These inferential targets are sometimes data-adaptive, in that they are dependent on the empirical distribution of the data. In this short note, we show that if researchers are willing to adapt the inferential target on the basis of efficiency, then extraordinary gains in precision can be obtained. Specifically, when causal effects are heterogeneous, any asymptotically normal and root-$n$ consistent estimator of the population average causal effect is superefficient for a data-adaptive local average causal effect. Our result illustrates the fundamental gain in statistical certainty afforded by indifference about the inferential target.\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\nWhen causal effects are heterogeneous, then inferences depend on the population for which causal effects are estimated. Although population average causal effects have traditionally been the inferential targets, recent results have focused on estimating average causal effects that are {\\it local} to some subpopulation for reasons of efficiency. These approaches include trimming observations based on the distribution of the propensity score \\citep{crumpetal}, using regression adjustment to estimate reweighted causal effects \\citep{angristpischke, humphreys2009}, or implementing calipers for propensity-score matching \\citep{rosenbaumrubin1985,austin2011}. In some cases, the target parameter is dependent on the empirical distribution of the data, including cases where the researcher is explicitly conducting inference on, e.g., the average treatment effect among the treated conditional on the observed covariate distribution \\citep{abadieimbens2002}, or other causal sample functionals \\citep{aronowetal,balzeretal}, without revision to the estimator being used.\n\n\n\nThis approach, taken at full generality, implies a form of indifference to which population causal effects are measured for. This indifference can be codified in the form of a {\\it data-adaptive} target parameter \\citep{vanderlaanetal} that is allowed to vary with the data depending on which subpopulation's local average causal effect is best estimated. When treatment effects are heterogeneous, adaptively changing the target parameter on the basis of efficiency yields an unusual result: if the population average causal effect can be consistently estimated with a root-$n$ consistent and asymptotically normal estimator $\\hat\\theta$, then the same estimator $\\hat\\theta$ is always superefficient (i.e., faster than root-$n$ consistent) for a data-adaptive local average causal effect. Furthermore, with an additional regularity condition on mean square convergence, we show that the mean square error of $\\hat \\theta$ for a data-adaptive local average causal effect is of $o(n^{-{1}})$.\n\n\n\n\n\n\\section{Results}\n\n\nConsider a full data probability distribution $G$ with an associated causal effect distribution $\\tau$ with finite expectation ${{\\rm E}\\,}_G[\\tau]$, where ${{\\rm E}\\,}_G[.]$ denotes the expectation over the distribution $G$. We impose a regularity condition on $\\tau$ establishing non-degeneracy of $\\tau$.\n\n\n\n\n\\begin{assumption}[Effect heterogeneity]\n$\\min\\left(\\sup \\left( {{\\rm Supp}\\,}(\\tau) \\right) - {{\\rm E}\\,}_G[\\tau], {{\\rm E}\\,}_G[\\tau] - \\inf \\left({{\\rm Supp}\\,}(\\tau)\\right)\\right) = c > 0.$\n\\end{assumption}\n\nWe observe an empirical distribution $F_n$. Suppose we have an root-$n$ consistent and asymptotically normal estimator of the average causal effect ${{\\rm E}\\,}_G[\\tau]$, $\\hat \\theta$.\n\\begin{definition}\nAn estimator $\\hat\\theta$ is root-$n$ consistent and asymptotically normal for $\\theta_0$ if $\\sqrt n (\\hat\\theta - \\theta_0) = \\mathcal{N}(0,\\sigma^2) + o_p(1)$, for some $0 < \\sigma^2 < \\infty$.\n\\end{definition}\nWe now define the target parameter, $\\theta_{F_n}$.\n\\begin{definition}\nLet the target parameter \n", "index": 1, "text": "$$\\theta_{F_n} =  \\left\\{\n     \\begin{array}{lr}\n      \\hat \\theta & : | \\hat \\theta - {{\\rm E}\\,}_G[\\tau] | \\leq c  \\\\\n     {{\\rm E}\\,}_G[\\tau] + c & : \\hat\\theta - {{\\rm E}\\,}_G[\\tau] > c  \\\\\n     {{\\rm E}\\,}_G[\\tau] - c  & : \\hat\\theta - {{\\rm E}\\,}_G[\\tau] < -c \n     \\end{array}\n   \\right.,$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\theta_{F_{n}}=\\left\\{\\begin{array}[]{lr}\\hat{\\theta}&amp;:|\\hat{\\theta}-{{\\rm E}%&#10;\\,}_{G}[\\tau]|\\leq c\\\\&#10;{{\\rm E}\\,}_{G}[\\tau]+c&amp;:\\hat{\\theta}-{{\\rm E}\\,}_{G}[\\tau]&gt;c\\\\&#10;{{\\rm E}\\,}_{G}[\\tau]-c&amp;:\\hat{\\theta}-{{\\rm E}\\,}_{G}[\\tau]&lt;-c\\end{array}%&#10;\\right.,\" display=\"block\"><mrow><mrow><msub><mi>\u03b8</mi><msub><mi>F</mi><mi>n</mi></msub></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover></mtd><mtd columnalign=\"right\"><mrow><mi/><mo>:</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo>-</mo><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c4</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>c</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c4</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><mi>c</mi></mrow></mtd><mtd columnalign=\"right\"><mrow><mi/><mo>:</mo><mrow><mrow><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo>-</mo><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c4</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>&gt;</mo><mi>c</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c4</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mi>c</mi></mrow></mtd><mtd columnalign=\"right\"><mrow><mi/><mo>:</mo><mrow><mrow><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo>-</mo><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\u03c4</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mo>-</mo><mi>c</mi></mrow></mrow></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01413.tex", "nexttext": "\nSince ${{\\rm E}\\,}_G[(\\tilde\\theta - \\theta_{F_n})^2] = o(n^{-1})$ and $n^{-1/2} {{\\rm E}\\,}_G[\\epsilon^2] = o(n^{-1})$, the Cauchy-Schwarz inequality ensures that ${{\\rm E}\\,}_G[(\\hat\\theta - \\theta_{F_n})^2]  = o(n^{-1}) + o(n^{-1}) = o(n^{-1})$.\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Discussion}\n\nOur results highlight the additional certainty obtained by indifference about the population for which average causal effects are measured. It is well known that efficiency gains may be obtained through data-adaptive inference. But the extent to which the researcher benefits from indifference about the target parameter has been understated. Under treatment effect heterogeneity -- a precondition for locality to be a concern -- all root-$n$ consistent and asymptotically normal estimators of the average treatment effect are superefficient for a local average treatment effect. And while we do not speak to the substantive implications of indifference in scientific inquiry, we show how such indifference yields greatly increased statistical certainty.\n\n\n\n\n\\begingroup\n\\begin{thebibliography}{}\n\\bibitem[\\protect\\citeauthoryear{Abadie and Imbens}{2002}]{abadieimbens2002}  Abadie, A. and Imbens, G. 2002. Simple and bias-corrected matching estimators for average treatment effects. NBER technical working paper no. 283.\n\n\\bibitem[\\protect\\citeauthoryear{Angrist and Pischke}{2009}]{angristpischke} Angrist, J.D. and Pischke, J.S. 2009. {\\it Mostly harmless econometrics: An empiricist's companion}. Princeton, NJ: Princeton University Press. \n\n\\bibitem[\\protect\\citeauthoryear{Aronow, Green, and Lee}{2014}]{aronowetal}  Aronow, P.M., Green, D.P. and Lee, D.K.K. 2014. Sharp bounds on the variance in randomized experiments. {\\it Annals of Statistics}. 42(3) 850--871.\n \n\n\\bibitem[\\protect\\citeauthoryear{Austin}{2011}]{austin2011}  Austin, P.C., 2011. Optimal caliper widths for propensity-score matching when estimating differences in means and differences in proportions in observational studies. {\\it Pharmaceutical Statistics}, 10(2), pp.150--161.\n\n\\bibitem[\\protect\\citeauthoryear{Balzer, Petersen, and van der Laan}{2015}]{balzeretal}  Balzer, L.B., Petersen, M.L. and van der Laan, M.J. 2015. Targeted estimation and inference for the sample average treatment effect. bepress.\n\n\\bibitem[\\protect\\citeauthoryear{Crump et al.}{2009}]{crumpetal}   Crump, R.K., Hotz, V.J., Imbens, G.W. and Mitnik, O.A. 2009. Dealing with limited overlap in estimation of average treatment effects. {\\it Biometrika}.\n\n\\bibitem[\\protect\\citeauthoryear{Humphreys}{2009}]{humphreys2009}  Humphreys, M., 2009. Bounds on least squares estimates of causal effects in the presence of heterogeneous assignment probabilities. Manuscript, Columbia University.\n\n\\bibitem[\\protect\\citeauthoryear{Rosenbaum and Rubin}{1985}]{rosenbaumrubin1985}  Rosenbaum P.R. and Rubin D.B. 1985. Constructing a control group using multivariate matched sampling methods that incorporate the propensity score. {\\it American Statistician} 39(1):33--38\n\n\\bibitem[\\protect\\citeauthoryear{van der Laan, Hubbard, and Pajouh}{2015}]{vanderlaanetal}  van der Laan, M.J., Hubbard, A.E. and Pajouh, S.K. (2013).  Statistical inference for data adaptive target parameters. bepress.\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 7975, "prevtext": "\n   where, as in Assumption 1, $c = \\min\\left(\\sup \\left( {{\\rm Supp}\\,}(\\tau) \\right) - {{\\rm E}\\,}_G[\\tau], {{\\rm E}\\,}_G[\\tau] - \\inf \\left({{\\rm Supp}\\,}(\\tau)\\right)\\right)$.\n\\end{definition}\n\n\n\nThe target parameter adapts naturally to the closest value in an interval surrounding ${{\\rm E}\\,}_G[\\tau]$, where the width of the interval is defined by the support of $\\tau$. We formalize how each $\\theta_{F_n}$ is a local average treatment effect.\n\n\\begin{prop}\nThere exists a nonnegative weighting associated with each empirical distribution $F_n$, $w_{F_n}$, such that across all $F_n$, $\\theta_{F_n} = \\frac{{{\\rm E}\\,}_G[w_{F_n} \\tau]}{{{\\rm E}\\,}_G[w_{F_n}]}$.\n\\end{prop}\nA proof of Proposition 1 follows directly from the fact that a weighted mean can obtain any value in the interval defined by the infimum and supremum of its distribution's support. We now prove the the superefficiency of $\\hat\\theta$.\n\n\n\n\n\n\n\n\\begin{prop}\nSuppose that Assumption 1 holds. Then for any root-$n$ consistent and asymptotically normal estimator of ${{\\rm E}\\,}_G[\\tau]$, $\\hat\\theta$, $\\sqrt n(\\hat\\theta - \\theta_{F_n}) = o_p(1).$ \n\\end{prop}\n\\begin{proof}\nThe author thanks Jas Sekhon for suggesting the following proof strategy. Decompose $\\hat\\theta$ into $\\tilde\\theta=\\mathcal{N}({{\\rm E}\\,}_G[\\tau],\\sigma^2/n)$ and $u = o_p(n^{-1/2})$, so that $\\hat\\theta = \\tilde\\theta + u$. Since $(\\tilde\\theta -  \\theta_{F_n})$ is $o_p(a_n)$ for any positive sequence $(a_n)$, the rate of convergence of $\\hat \\theta$ is at worst governed by the bound ensured by $u$'s $o_p(n^{-1/2})$ convergence. To prove the claim, note that for any positive $\\epsilon$, $\n\\Pr\\left( \\frac{| \\tilde\\theta -  \\theta_{F_n} |}{a_n} \\geq \\epsilon \\right) \\leq \\Pr\\left( \\tilde\\theta - \\theta_{F_n} \\neq 0 \\right) = 2\\Phi(-c \\sqrt{n}/\\sigma).\n$\nSince $\\lim_{n\\rightarrow \\infty}  2\\Phi(-c \\sqrt{n}/\\sigma) = 0$, $(\\tilde\\theta -  \\theta_{F_n})$ is $o_p(a_n)$. Thus $\\hat\\theta - \\theta_{F_n} =  o_p(a_n) +  o_p(n^{-1/2}) = o_p(n^{-1/2})$, yielding the result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}\n\nWhen an additional regularity condition is imposed on the convergence of $\\hat\\theta$ to normality, a stronger result can be obtained about the rate of mean square convergence.\n\\begin{prop}\nSuppose that $\\hat\\theta$ obeys $\\sqrt n (\\hat\\theta - {{\\rm E}\\,}_G[\\tau]) = \\mathcal{N}(0,\\sigma^2) + \\epsilon$, where ${{\\rm E}\\,}_G[\\epsilon^2] = o(n^{-1/2}).$  Then ${{\\rm E}\\,}_G[(\\hat\\theta - \\theta_{F_n})^2]  = o(n^{-1}).$ \n\\end{prop}\n\\begin{proof}\n\n\n\n\n\n\nWe will show that the mean square error of $(\\tilde\\theta -  \\theta_{F_n})$ converges to zero sufficiently quickly, implying that the rate of convergence of $\\hat \\theta$ is at worst governed by the mean square error bound ensured by $\\epsilon$'s convergence rate. To obtain the rate of convergence of the mean square error of $\\tilde\\theta$, we integrate over its squared deviation from the target parameter.  Within $c$ of ${{\\rm E}\\,}_G[\\tau]$, the squared deviation is zero, thus we need only integrate over the squared deviation over the tails of the normal distribution. To ease calculations, we obtain an upper bound by integrating over the squared deviation from ${{\\rm E}\\,}_G[\\tau]$, rather than from $\\theta_{F_n}$:\n\n", "index": 3, "text": "\\begin{align*}\n{{\\rm E}\\,}_G[(\\tilde\\theta - \\theta_{F_n})^2] & \\leq 2 \\int_{c}^{\\infty} x^2 \\frac{\\sqrt{n}}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{x^2 n}{2 \\sigma^2}} \\\\\n& = c \\sigma \\sqrt{\\frac{2}{\\pi} } \\frac{ e^{-\\frac{c^2 n}{2 \\sigma^2}}}{ \\sqrt{n}}+2 \\sigma^2\\frac{\\Phi \\left(\\frac{-c \\sqrt{n}}{\\sigma}\\right)}{n} \\\\\n& = o(n^{-1}).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{{\\rm E}\\,}_{G}[(\\tilde{\\theta}-\\theta_{F_{n}})^{2}]\" display=\"inline\"><mrow><msub><mpadded width=\"+1.7pt\"><mi mathvariant=\"normal\">E</mi></mpadded><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">~</mo></mover><mo>-</mo><msub><mi>\u03b8</mi><msub><mi>F</mi><mi>n</mi></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq 2\\int_{c}^{\\infty}x^{2}\\frac{\\sqrt{n}}{\\sigma\\sqrt{2\\pi}}e^{%&#10;-\\frac{x^{2}n}{2\\sigma^{2}}}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>c</mi><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><msqrt><mi>n</mi></msqrt><mrow><mi>\u03c3</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mrow></mfrac></mstyle><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>\u2062</mo><mi>n</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=c\\sigma\\sqrt{\\frac{2}{\\pi}}\\frac{e^{-\\frac{c^{2}n}{2\\sigma^{2}}}%&#10;}{\\sqrt{n}}+2\\sigma^{2}\\frac{\\Phi\\left(\\frac{-c\\sqrt{n}}{\\sigma}\\right)}{n}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><mn>2</mn><mi>\u03c0</mi></mfrac></mstyle></msqrt><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><msup><mi>c</mi><mn>2</mn></msup><mo>\u2062</mo><mi>n</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup><msqrt><mi>n</mi></msqrt></mfrac></mstyle></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow><mi>\u03c3</mi></mfrac><mo>)</mo></mrow></mrow><mi>n</mi></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=o(n^{-1}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]