[{"file": "1601.05936.tex", "nexttext": "\nwhere ${\\mathbf A}=[\\alpha_1 \\hdots \\alpha_T]$ and $d_j$ denotes each atom of the dictionary. \n\nClass-specific data of senone posterior features is obtained through GMM-HMM based forced alignment on training data, which is then used to learn individual over-complete basis set ${\\mathbf D}_{\\ell}$ for each senone subspace $S_{\\ell}$ using dictionary learning algorithm. These class-specific dictionaries are concatenated into a larger dictionary ${\\mathbf D}=[\\mathbf{D}_{1} \\cdots \\mathbf{D}_{\\ell} \\cdots \\mathbf{D}_{L}]$ for subspace-sparse acoustic modeling. Since any posterior feature obtained from DNN lies in a union of subspaces $\\cup_{\\ell=1}^{L}\\mathcal{S}_\\ell$, a test posterior feature $z$ can be reconstructed using the atoms of dictionary ${\\mathbf D}$. According to SSR property, only the atoms associated to the correct class (underlying subspace) of $z$ will be used for sparse representation. \n\nIt may be noted that dictionary learning approach is fundamentally different from dictionary construction using a random subset \\cite{sainath2011exemplar, gemmeke2011exemplar} of training features since we use all of the training data to compute an over-complete basis set for sparse representation which is far smaller (less than 3\\% in case of Numbers'95 database) than the actual collection size yet more effective in sparse representation~\\cite{dighe2015sparse}.  \n\n\n\\vspace{-3mm}\n\n\\subsection{Enhanced Acoustic Modeling}\\label{sec:sparse_representations}\n\n\\vspace{-2mm}\nWe use group sparsity based hierarchical Lasso algorithm~\\cite{sprechmann2011c} for sparse coding to enforce group sparsity in $\\alpha$ based on the internal partitioning of dictionary ${\\mathbf D}$ into senone-specific sub-dictionaries ${\\mathbf D}_{\\ell}$. The high dimensional group sparse representation $\\alpha$ is computed for each DNN output posterior feature $z$ by sparse recovery over ${\\mathbf D}$. Projection of a test posterior feature $z$ on training data space is given by computing ${\\mathbf D}\\alpha$.\n\nNote that ${\\mathbf D}\\alpha$ is an approximation of posterior feature $z$ based on $\\ell_{1}$-norm sparse reconstruction using atoms of ${\\mathbf D}$.  Consequently, it has the same dimension as $z$ and it is forced to lie in a probability simplex by normalization. Figure \\ref{fig:dict_sparse} summarizes this procedure. \n\\begin{figure}[t]\n  \\centering\n\n  \\includegraphics[width=0.97\\columnwidth]{dalpha.png}\n\n  \\caption{\\small DNN output senone posteriors $z$ are projected to the space of training posteriors using ${\\mathbf D}\\alpha$. Resulting projected posteriors are used for typical decoding in DNN-HMM framework.}  \n  \\label{fig:dict_sparse}\n\\end{figure}\n\\vspace{-3mm}\n\n\\section{Experimental Analysis}\\label{sec:analysis}\n\n\\vspace{-2mm}\nIn this section, we provide empirical analysis of the theoretical results established in Section~\\ref{sec:union_of_subspace}. These experiments confirm that the information bearing components of DNN class-conditional probabilities indeed live in a very low-dimensional space. Exploiting this structure enables enhancement of DNN based acoustic models and removes the effect of high-dimensional noise leading to improvement in DNN-HMM speech recognition performance. \n\\vspace{-2mm}\n\n\\subsection{Database and Speech Features}\\label{sec:regular}\n\n\nWe use Numbers'95 database for this study where only the utterances consisting of digits are considered (more details in~\\cite{dighe2015sparse}). The phoneset includes 27 phones and accordingly 557 context dependent tied states referred to as senones are learned by forced alignment of the training data using Kaldi speech recognition toolkit~\\cite{povey2011kaldi}.  \nA DNN is trained using sequence discriminative training~\\cite{vesely2013sequence} with 3 hidden layers each having 1024 nodes. For every 10 ms speech frame, the DNN input is a vector of MFCC+$\\Delta$+$\\Delta\\Delta$ features with a context of 9 frames (39$\\times$9=351 dimension). The DNN output is a vector of posterior probabilities corresponding to 557 senone classes. We use DNN posteriors as features $z$ for dictionary learning and sparse coding~\\eqref{eq:optimize_func}.\n\\vspace{-2mm}\n\n\\subsection{Low-rank Posterior Reconstruction}\\label{sec:uos-reconstruct}\n\n\nAs explained in Sections \\ref{sec:sparse}--\\ref{sec:sparse_representations}, DNN posteriors are used to learn senone-specific dictionaries ${\\mathbf D}_{\\ell}$ from the training data. Number of atoms $n_\\ell$ in each senone dictionary ${\\mathbf D}_\\ell$ is approximately 100. A value of $\\lambda=0.2$, optimized on development data, was used for sparse coding to get sparse representations $\\alpha$. Subsequently ${\\mathbf D}\\alpha$ projected posterior probabilities are computed for the test data. Sparsity leads to selection of a few subspaces of the training data resulting in new test posteriors which (1) live in low-dimensions, (2) are projected onto the subspace of the training posteriors, and (3) separated from the subspaces of other senone classes. We investigate these properties below through further analysis. \n\nTo provide an insight into the dimension of the senone subspaces, we construct matrices of 1000 class-specific senone posteriors and \ncompute the number of singular values required to preserve 95\\% variability of the data. Due to skewed distribution of the posteriors, we take their log prior to singular value decomposition. We refer to the number of required singular values as roughly the ``Rank'' of senone matrices.\nAn ideal posterior feature should have its maximum component at the support indicating its associated class. Hence, we group the posteriors as ``correct'' if the maximum component corresponds to the correct class and ``incorrect'' if the maximum component corresponds to the incorrect class. Table~\\ref{table:analysis} shows the average number of required singular values over all senones for DNN and projected posteriors. Another approach referred to as robust PCA based posteriors will be discussed in the subsequent section.  \n{\\footnotesize\n\\begin{table}[b]\\footnotesize\n\\centering\n\\begin{tabular}{lccc}\n\\hline\n& DNN & Projected & Robust PCA \\\\\n\\hhline{====}\nRank-Correct   & 36.6 & 11.9 &  7.6 \\\\\nRank-Incorrect & 45.5 & 21.7 &  11.7 \\\\\n\\hline\n\\end{tabular}\n\\caption{\\footnotesize Comparison of ``Rank'' of DNN posterior matrix, projected posterior matrix and RPCA senone posterior matrix.}\n\\label{table:analysis}\n\\end{table}}\n\nWe can see that the ``correct'' posteriors live in a space which has far lower dimension than the space of ``incorrect'' posteriors. In other words, the information bearing components in ``correct'' senone posteriors are fewer resulting in matrices which have lower rank compared to ``incorrect'' posteriors. Given that the ranks are nevertheless very low (compared to the dimension of the senone posteriors which is 557), the ``incorrect'' posterior are exposed to a high-dimensional spurious noise. Therefore, to enhance the posterior probabilities, \n\\begin{itemize}[label={},leftmargin=*]\n  \\item \\emph{the low-dimensional subspace has to be modeled/identified and the posterior has to be projected onto that space.}\n\\end{itemize}\nTo further investigate the subspaces selected for sparse recovery, the values in sparse representation $\\alpha$ for each class are summed to form $\\alpha$-sum vectors and the ``Rank'' of senone-specific $\\alpha$-sum matrices are computed. According to SSR property, it is expected that sparse recovery should select the subspaces from the underlying classes so the ``Rank'' of $\\alpha$-sum matrices has to be 1. In fact, we found that the empirical results averaged over the whole test set conformed to this theoretical insight indicating that \n\\begin{itemize}[label={},leftmargin=*]\n  \\item \\emph{subspace sparse recovery leads to selection of the subspaces belonging to the underlying senone classes.}\n\\end{itemize}\nThe class-specific dictionary learning for sparse coding enables us to model the non-linear manifold of the training data as a union of low-dimensional subspaces. A DNN posterior $z$ from the test data may not lie on this manifold due to presence of high-dimensional noise embedded in its components. It is important to extract the low-dimensional structure in $z$ while separating the effect of noise. Sparse coding does exactly this by finding the true underlying subspaces in sparse representation $\\alpha$ and enables projecting $z$ on the class-specific subspace of the training data manifold via ${\\mathbf D}\\alpha$ reconstruction. \n\\vspace{-2mm}\n\n\\subsection{Low-rank and Sparse Decomposition}\\label{sec:rpca}\n\n\n\nTo further study the \\emph{true} underlying dimension of the senone-specific subspaces, we consider robust principle component analysis (RPCA) based decomposition of the senone posteriors~\\cite{candes2011robust}. The idea of RPCA is to decompose a data matrix $\\mathbf{M}$ as\n\n", "itemtype": "equation", "pos": 12274, "prevtext": "\n\n\\ninept\n\n\\maketitle\n\n\\begin{abstract} \nWe propose to model the acoustic space of deep neural network (DNN) class-conditional posterior probabilities as a union of low-dimensional subspaces. To that end, the training posteriors are used for dictionary learning and sparse coding. Sparse representation of the test posteriors using this dictionary enables projection to the space of training data.   \nRelying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank, we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions. \nThe enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in both clean and noisy conditions, where upto $15.4\\%$ relative reduction in word error rate (WER) is achieved.\n\n\n\n\\end{abstract}\n\\vspace{-1mm}\n\\begin{keywords}\nSparse coding, Dictionary learning, Deep neural network, Union of Low Dimensional Subspaces, Acoustic modeling.\n\\end{keywords}\n\n\\vspace{-2mm}\n\n\\section{Introduction}\\label{sec:intro}\n\n\\vspace{-1mm}\nA need for sparse representations for better acoustic modeling of speech has been advocated consistently for better characterization of the underlying low-dimensional and parsimonious structure of speech~\\cite{bilmes2006hmms,bengio2009learning,sainath2011exemplar,saon2012bayesian}. \nTwo major emerging trends, namely deep neural networks (DNN) and exemplar-based sparse modeling, are different approaches of exploiting sparsity in speech representations to achieve invariance, discrimination and noise separation~\\cite{deng2013machine,saon2012bayesian,gemmeke2008noise}. \n\nOn the other hand, speech utterances are formed as a union of words which in turn consist of phonetic components and sub-phonetic attributes. \nEach linguistic component is produced through activation of a few highly constrained articulatory mechanisms leading to generation of speech data in union of low-dimensional subspaces~\\cite{deng2004switching,king2007speech,lee2001functional}.   \nHowever, most existing speech classification and acoustic modeling methods do not explicitly take into account the multi-subspace structure of the data.\n\nThe present study focuses on exploiting the multi-subspace low-dimensional structure of speech learned from the training data to enhance DNN based acoustic modeling of unseen test data. Hence, this also has the potential to enable domain adaptation and handling mismatch in the framework of DNN based acoustic modeling.\n\n\n\n\n\\vspace{-2mm}\n\n\\subsection{Prior Works}\\label{sec:prior}\n\n\\vspace{-1mm}\nSparse representation has been proven powerful as features used for acoustic modeling.   \n\nAs argued in \\cite{bengio2009learning}, if data is projected into high-dimensional space, the underlying structures are dis-entangled. These structures form a union of low-dimensional subspaces which models the non-linear manifold where speech data resides. \nPrior work on sparse representation includes exemplar-based methods~\\cite{sainath2011exemplar, gemmeke2011exemplar} where sparse representation, learned using spectral features achieve promising performance in automatic speech recognition (ASR) specially due to their robustness in handling noise and corruption. \n\nRecent advancement in DNN based acoustic modeling relies on estimation of highly sparse sub-word class-conditional posterior probabilities. While the conventional Gaussian mixture models (GMM) are statistically inefficient in modeling data lying on or near non-linear manifolds~\\cite{hinton2012deep,deng2004switching,king2007speech}, DNNs achieve accurate sparse acoustic modeling through multiple layers of non-linear transformations \\cite{yu2013feature}. The hidden layers of DNN successively learn underlying structures at different levels and express them as highly invariant and discriminative representations towards deeper layers. While enforcing sparsity constraints during DNN training is mostly employed for the purpose of regularization to prevent overfitting, various studies have shown that sparsity in DNN architectures directly contributes towards simpler networks and superior performance in ASR. Successful application of sparse activity \\cite{liu2015neuron} (very few neurons being active), sparse connectivity \\cite{yu2012exploiting} (very few non-zero weights) as well as better performance of sparsity inducing techniques like dropout neural network training \\cite{srivastava2014dropout} confirm the belief that \\textit{`sparser'} is better for acoustic modeling in ASR.\n\\vspace{-2mm}\n\n\\subsection{Motivation and Contributions}\\label{sec:motivation}\n\n\\vspace{-1mm}\nWe point out two issues with respect to the state-of-the-art DNN based acoustic models which motivate further consideration of sparse modeling: \n\\begin{description}\n\\item{Q1.} Previous studies~\\cite{tasha2015exploring,mohamed2012understanding} have found sparse activations in DNNs by showing how individual neurons in hidden layers learn being selectively active in different ways towards distinct phone patterns. Since this sparsification learned by hidden layers is not explicitly hand-crafted, we ask upto what extent the union of low-dimensional subspaces structure for speech is actually being exploited by DNNs ?\n\\item{Q2. } Despite of being effective in seen conditions, DNNs are found highly sensitive to unseen variations in data~\\cite{yu2013feature}. The mismatch condition causes erroneous estimates of posterior probabilities which is exhibited as spurious noises in the output posterior probabilities. Can we correct these errors through a low-dimensional model to improve acoustic modeling in noisy conditions ?\n\\end{description}\nIn this paper, we address those issues by explicit modeling of the underlying structures in speech using prior knowledge that speech data lives in the union of low-dimensional subspaces. We implement this idea using the principled dictionary learning and sparse coding algorithms over DNN posterior probabilities to recover sparse representations where non-zero values correspond to class-specific subspaces. \nThese subspace sparse representations are then used to \\textit{enhance} the original DNN posterior probabilities through dictionary based reconstruction. We build upon compressive sensing and subspace sparse recovery theory to provide theoretical support for validity of our approach. We also elaborate on our choice of features (DNN based posterior probabilities) and algorithms for dictionary learning~\\cite{mairal2010online} and structured sparse coding~\\cite{sprechmann2011c} which essentially distinguish our approach from previous exemplar based sparse representation methods~\\cite{sainath2010sparse, sainath2011exemplar, gemmeke2011exemplar}. We demonstrate improvements in performance achieved by the proposed enhanced acoustic modeling in hybrid DNN-HMM continuous ASR system using Numbers'95 database \\cite{Cole95newtelephone} and show increased robustness in noisy conditions. \n\nIn the rest of the paper, the proposed subspace sparse acoustic modeling method is elaborated in Section~\\ref{sec:union_of_subspace}. The experimental analysis are carried out in Section~\\ref{sec:analysis}. Section \\ref{sec:conclusions} provides the concluding remarks and directions for future work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-1mm}\n\n\\section{Subspace Sparse Acoustic Modeling}\\label{sec:union_of_subspace}\n\n\\vspace{-1mm}\nIn this section, we model the space of DNN class-conditional posterior probabilities as a union of low-dimensional subspaces. Relying on the subspace sparse representation, we show how the posteriors can be enhanced for more accurate class-specific representations.\n\\vspace{-3mm}\n\n\\subsection{Subspace Sparse Representation}\n\n\\vspace{-2mm}\nSpeech features reside on or near non-linear manifolds which can be best characterized by union of low-dimensional subspaces. The proposed approach relies on the fact that a data point in a union of subspaces can be more efficiently reconstructed using a sparse combination of data points from its own subspace than data points from other subspaces, thus  resulting in a \\textit{subspace-sparse representation}~\\cite{elhamifar2013sparse}.\n\nTo state it more precisely, let $\\mathbf{S}=\\{\\mathcal{S}_\\ell\\}^L_{\\ell=1}$ be a set of linear disjoint subspaces associated to $L$ classes in $\\mathbb{R}^m$ such that the dimensions of individual subspaces $\\{r_\\ell\\}_{\\ell=1}^L$ are smaller than the dimension of the actual space, i.e. $\\forall\\ell\\textrm{, }r_\\ell < m$. Speech features $z$ lie in the union $\\cup_{\\ell=1}^{L} \\mathcal{S}_\\ell$ of these low-dimensional subspaces. Let $\\mathbf{D}_\\ell \\in \\mathbb{R}^{m\\times n_\\ell}$ be the class-specific over-complete dictionary  for subspace $\\mathcal{S}_\\ell$ where $n_\\ell$ is the number of atoms in ${\\mathbf D}_\\ell$ and $n_\\ell>r_\\ell$. Each data point in $\\mathcal{S}_\\ell$ can then be represented as a sparse linear combination of the atoms from ${\\mathbf D}_{\\ell}$. \n\nDefining $\\ell_1$-norm of a vector (denoted by $\\|.\\|_1$) as the sum of the absolute values of its components, the \\textit{subspace sparse recovery} (SSR) property~\\cite{elhamifar2013sparse} for union of disjoint subspaces asserts that $\\ell_1$-norm sparse representation of a data point over collection of all class-specific dictionaries $\\{{\\mathbf D}_{\\ell}\\}_{\\ell=1}^L$ can lead to separation of the class-specific subspaces by selecting atoms only from the underlying class of the data point for its reconstruction. Thus, the obtained sparse representations have activations only for the atoms corresponding to the actual subspace $\\mathcal{S}_\\ell$ where $z$ lives.\n\n\n\n\n\n\n\n\nConsidering a speech utterance as the union of words, phones or sub-phonetic components, the subspaces $\\mathcal{S}_\\ell$ can be modeled at different levels (time granularity) corresponding to any of these speech units. Consequently a dictionary ${\\mathbf D}$ can be constructed by learning basis sets ${\\mathbf D}_\\ell$ for individual classes. In the present study, we focus on context-dependent senones (c.f. Section~\\ref{sec:features}) for their superior quality in DNN-HMM framework. Nevertheless there is no theoretical/algorithmic impediment in applying it for larger units such as words.\n\nThe rigorous proof of SSR property (see Theorem 2 in~\\cite{elhamifar2013sparse}) requires certain conditions and assumptions on disjoint subspaces. Since we train DNN with binary senone target outputs, the intersection of senone subspaces is expected to be a rare event and suggests disjointedness of subspaces. Although further theoretical  analysis is beyond the scope of the present work, experiments conducted in Section~\\ref{sec:analysis} empirically confirm that SSR property indeed holds for subspace-sparse modeling of senones.\n\n\n\n\\vspace{-2mm}\n\n\\subsection{Class-Specific Dictionary Learning}\\label{sec:sparse}\n\nThere are two key considerations for dictionary learning in sparse subspace acoustic modeling. Namely, the choice of features and algorithmic developments. \n\\vspace{-3mm}\n\n\\subsubsection{Senone Posterior Probabilities as Speech Features}\\label{sec:features}\n\n\\vspace{-1.5mm}\nA posterior feature $z$ is a vector consisting of class-conditional probabilities at the output layer of DNN. In contrast to spectral features, posterior features are proven highly effective for sparse modeling~\\cite{dighe2015sparse,asaei2010analysis}. They are inherently sparse and invariant to speaker/environmental conditions presented in the DNN training data. Although we choose to work with posterior probabilities at context-dependent senone levels (tied triphone states)~\\cite{yu2015automatic}, the theoretical underpinning of the proposed approach is applicable to any type of speech units. \n\\vspace{-3mm}\n\n\\subsubsection{Dictionary Learning and Sparse Coding Algorithms}\\label{sec:algorithms}\n\n\\vspace{-1.5mm}\nBuilding on our previous work on dictionary learning for sparse modeling of posterior features~\\cite{dighe2015sparse}, we use the online dictionary learning~\\cite{mairal2010online} algorithm for solving $l_1$ sparse coding problem expressed as\n\n", "index": 1, "text": "\\begin{equation}\\small \\label{eq:optimize_func}\n\\arg \\min_{\\substack {\\mathbf D},{\\mathbf A}} \\sum_{t=1}^{T} \\| z_t - {\\mathbf D}\\,\\alpha_{t} \\|_2^2 + \\lambda{\\|\\alpha_{t}\\|}_1, \\textrm{  s.t.  } \\| d_j \\|_2^2\\leq1\\, \\forall{j}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\small\\arg\\min_{\\begin{subarray}{c}\\mathbf{D}\\end{subarray},{\\mathbf{A}}}\\sum_%&#10;{t=1}^{T}\\|z_{t}-{\\mathbf{D}}\\,\\alpha_{t}\\|_{2}^{2}+\\lambda{\\|\\alpha_{t}\\|}_{1%&#10;},\\textrm{ s.t. }\\|d_{j}\\|_{2}^{2}\\leq 1\\,\\forall{j}\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi mathsize=\"90%\">arg</mi><mo>\u2061</mo><munder><mi mathsize=\"90%\">min</mi><mrow><mtable class=\"ltx_align_c\"><mtr><mtd><mi mathsize=\"90%\">\ud835\udc03</mi></mtd></mtr></mtable><mo mathsize=\"90%\" stretchy=\"false\">,</mo><mi mathsize=\"90%\">\ud835\udc00</mi></mrow></munder></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"90%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"90%\">t</mi><mo mathsize=\"90%\" stretchy=\"false\">=</mo><mn mathsize=\"90%\">1</mn></mrow><mi mathsize=\"90%\">T</mi></munderover><msubsup><mrow><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo><mrow><msub><mi mathsize=\"90%\">z</mi><mi mathsize=\"90%\">t</mi></msub><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mpadded width=\"+1.7pt\"><mi mathsize=\"90%\">\ud835\udc03</mi></mpadded><mo>\u2062</mo><msub><mi mathsize=\"90%\">\u03b1</mi><mi mathsize=\"90%\">t</mi></msub></mrow></mrow><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo></mrow><mn mathsize=\"90%\">2</mn><mn mathsize=\"90%\">2</mn></msubsup></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"90%\">\u03bb</mi><mo>\u2062</mo><msub><mrow><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo><msub><mi mathsize=\"90%\">\u03b1</mi><mi mathsize=\"90%\">t</mi></msub><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo></mrow><mn mathsize=\"90%\">1</mn></msub></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">,</mo><mrow><mtext mathsize=\"90%\">\u00a0s.t.\u00a0</mtext><mo>\u2062</mo><msubsup><mrow><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo><msub><mi mathsize=\"90%\">d</mi><mi mathsize=\"90%\">j</mi></msub><mo maxsize=\"90%\" minsize=\"90%\">\u2225</mo></mrow><mn mathsize=\"90%\">2</mn><mn mathsize=\"90%\">2</mn></msubsup></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">\u2264</mo><mrow><mpadded width=\"+1.7pt\"><mn mathsize=\"90%\">1</mn></mpadded><mo>\u2062</mo><mrow><mo mathsize=\"90%\" stretchy=\"false\">\u2200</mo><mi mathsize=\"90%\">j</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05936.tex", "nexttext": "\nwhere matrix $\\mathbf{L}$ has low-rank and matrix $\\mathbf{N}$ is sparse (see Figure \\ref{fig:rpca}). Building upon the observations in Section~\\ref{sec:uos-reconstruct}, the low-rank component $\\mathbf{L}$ corresponds to the enhanced posteriors while the high dimensional erroneous estimates are separated out in the sparse matrix $\\mathbf{N}$. \n\nWe collect posterior features for each senone from training data using \\emph{ground truth} based GMM-HMM forced alignment. RPCA decomposition is applied to data of each senone-class to reveal the \\emph{true} underlying dimension of the class-specific senone subspaces. The rank of senone posteriors (i.e. rank of $\\mathbf{L}$) obtained after RPCA decomposition for both ``Correct'' and ``Incorrect'' classes are listed in Table~\\ref{table:analysis}. We can see that the \\emph{true} dimension (7.6) of the class-specific subspaces of senone posteriors is indeed far lower than the DNN posteriors (36.6) and yet lower than the projected posteriors (11.9). Exploiting this multi low-rank structure of speech can lead to posterior enhancement via low-rank representation at utterance level~\\cite{liu2013robust}.  \n\nThe low-rank bottleneck layer based DNN is studied in~\\cite{sainath2013low} which shows that low-dimensional structuring of DNN architecture yields smaller footprint and faster training. In contrast, our proposed method suggests an added layer of sparse coding for structuring DNN outputs relying on the generic sparse and low-rank structures. Since, these generic structures are characterized from the training data, this approach enables us to handle mismatches in DNN train and test conditions.\n\n\\begin{figure}[t]\n  \\centering\n\n  \\includegraphics[width=0.98\\columnwidth]{rpca.png}\n  \\vspace{-2mm}\n  \\caption{\\footnotesize Decomposing a DNN estimated senone posterior matrix $\\mathbf{M}_{\\text{speech}}$ into a low-rank matrix $\\mathbf{L}_{\\text{speech}}$ of enhanced posteriors and a sparse matrix $\\mathbf{N}_{\\text{speech}}$ of spurious noise.}\n  \\label{fig:rpca}\n\\end{figure}\n\\vspace{-3mm}\n\n\\subsection{Enhanced DNN-HMM Speech Recognition}\\label{sec:asr_results}\n\n\\vspace{-2mm}\nContinuous speech recognition is performed using DNN posteriors as well as projected posteriors in the framework of conventional hybrid DNN-HMM. HMM topology learned during training of the hybrid DNN-HMM is used for decoding the word transcription in all cases. Hence, all parameters of different ASR systems shown here are the same and the only difference is in terms of senone posterior probabilities at each frame which results in different best paths being decoded by the Viterbi algorithm. \n\nTo demonstrate the increased robustness in projected posteriors as compared to the DNN posteriors, we also compared their performance in noisy conditions where artificial white Gaussian noise was added at signal level to the test utterances at signal-to-noise (SNR) ratios of 10\\,dB, 15\\,dB and 20\\,dB. DNN trained on clean speech is used for computing posteriors from noisy test spectral features so that the artificially added noise acts as an unseen variation in the data for DNN. Comparison of ASR performance is shown in Table~\\ref{table:wer_asr} in terms of Word Error Rate (WER) percentage. \n\nWe can see that the projected posteriors outperform DNN posteriors in all cases suggesting that projection based on ${\\mathbf D}\\alpha$ provides enhanced acoustic models for DNN-HMM decoding. We note that \n\n\nin all experiments, a consistent decrease in insertion and substitution errors is observed when using projected posteriors in place of DNN posteriors. This implies fewer wrong hypotheses being made in case of projected posteriors at word level as compared to DNN posteriors. A similar insight comes by comparing the GMM-HMM based forced senone alignment (ground truth) with senone alignments achieved by best Viterbi paths in projected posterior and DNN posterior systems. Senone classification error of 24.1\\% in case of DNN posteriors is reduced to 19.8\\% in case of projected posteriors. Improvement in senone alignments and subsequent reduction in WER proves superior quality of projected posteriors over DNN posteriors and supports the hypothesis that projection moves the test features closer to the subspace of the correct classes. \n\n\nFinally, RPCA posteriors (matrix $\\mathbf{L}$ obtained from low-rank and sparse decomposition as explained in Section \\ref{sec:rpca}) which have ranks close to the true underlying dimensions of senone subspaces perform extremely well in ASR (c.f. Table~\\ref{table:wer_asr}). WER of 2.6\\% using DNN posteriors (``Rank'' 36.6) reduces to a WER of 2.2\\% using projected posteriors (``Rank'' 11.9) i.e. a relative improvement of 15.4\\%, and when RPCA posteriors (``Rank'' 7.6) are used, it is reduced to a mere 0.4\\%. Since RPCA based low-rank reconstruction of posteriors has been done using ground truth senone alignment, ASR performance in this case is the best case scenario and demonstrates the scope of improvement possible even after DNN based acoustic modeling.\n{\\footnotesize\n\\begin{table}[t]\\footnotesize\n\\centering\n\\begin{tabular}{llcccc}\n\\hline\n\\textbf{SNR} & \\textbf{Posteriors} & \\textbf{WER (\\%)} & \\textbf{Ins} & \\textbf{Del} & \\textbf{Subs}\\\\\n\\hhline{======}\nClean  & RPCA  & 0.4 &  36 & 18 & 4 \\\\\n\\hline\n\\hline\n\\multirow{2}{*}{Clean} & DNN & 2.6 & 111 & 96 & 152\\\\\n& Projected & 2.2 & 72 & 100 & 137\\\\\n\\hline\t\t\t\t\t\t\n\\multirow{2}{*}{20db} & DNN\t& 4.0\t&160&\t121\t& 293\\\\\n\t& Projected & 3.5\t&\t90\t&162&\t233\\\\\n\\hline\t\t\t\t\t\t\n\\multirow{2}{*}{15db} & DNN\t&6.8\t&\t205&\t249&\t498\\\\\n\t& Projected &\t6.2\t&\t130\t&298\t&442\\\\\n\\hline\t\t\t\t\t\t\n\\multirow{2}{*}{10db}& DNN &14.0\t&199\t&950&\t801\\\\\n\t& Projected &\t13.9\t&\t117\t&1064\t&763\\\\\n\\hline\n\\end{tabular}\n\\vspace{-2mm}\n\\caption{\\footnotesize Comparison of ASR performance using DNN posteriors and projected posteriors in clean and noisy conditions on Numbers'95 database. RPCA posteriors indicate an ideal enhancement through low-dimensional posterior reconstruction. Breakdown of WER in terms of insertions (Ins), deletions (Del), and substitutions (Subs) has also been shown out of a total of 13967 words in all test utterances.}\n\\label{table:wer_asr}\n\\end{table}}\n\n\n\n \\vspace{-3mm}\n\n\\section{Conclusions and Future Directions}\\label{sec:conclusions}\n\n \\vspace{-2mm}\nIn this paper, we demonstrated explicit modeling of low-dimensional structures in speech using dictionary learning and sparse coding over the DNN class conditional probabilities. We showed that albeit their power in representation learning, DNN based acoustic modeling still has room for improvement in 1) exploiting the union of low-dimensional subspaces structure underlying speech data and 2) acoustic modeling in noisy conditions. Using dictionary learning and sparse coding, DNN posteriors were transformed to projected posteriors which were shown to be more suitable acoustic models. Sparse reconstruction moves the test posteriors closer to the correct underlying class of the data by exploiting the fact that the true information is embedded in a low-dimensional subspace thus separating out the high dimensional erroneous estimates. Improvements in ASR performance were shown for both clean and noisy conditions paving the way towards an effective robust ASR framework using DNN in unseen conditions. The importance of low-dimension structures was further confirmed through RPCA analysis. \n\nThe proposed method can be improved through discriminative dictionary learning for better class-specific subspace modeling. Furthermore, we will study the low-rank clustering techniques to enhance posterior probabilities exploiting their low-dimensional multi-subspace structure. Moreover, we will consider further analysis on challenging databases and in particular the case of accented non-native speech recognition. Projection of accented speech posteriors on dictionaries trained with native language speech can result in transformation of accented phonetic space to native phonetic space and lead to improvements in accented speech recognition task. \n\n{\n \\vspace{-2mm}\n\\section{Acknowledgments}\n \\vspace{-2mm}\nThe research leading to these results has received funding from by SNSF project on ``Parsimonious Hierarchical Automatic Speech Recognition (PHASER)'' grant agreement number 200021-153507.\n}\n\n\n\n\n\\balance\n\\bibliographystyle{IEEEbib}\n\n\n\\balance\n\\bibliography{strings,refs}\n\n", "itemtype": "equation", "pos": 21384, "prevtext": "\nwhere ${\\mathbf A}=[\\alpha_1 \\hdots \\alpha_T]$ and $d_j$ denotes each atom of the dictionary. \n\nClass-specific data of senone posterior features is obtained through GMM-HMM based forced alignment on training data, which is then used to learn individual over-complete basis set ${\\mathbf D}_{\\ell}$ for each senone subspace $S_{\\ell}$ using dictionary learning algorithm. These class-specific dictionaries are concatenated into a larger dictionary ${\\mathbf D}=[\\mathbf{D}_{1} \\cdots \\mathbf{D}_{\\ell} \\cdots \\mathbf{D}_{L}]$ for subspace-sparse acoustic modeling. Since any posterior feature obtained from DNN lies in a union of subspaces $\\cup_{\\ell=1}^{L}\\mathcal{S}_\\ell$, a test posterior feature $z$ can be reconstructed using the atoms of dictionary ${\\mathbf D}$. According to SSR property, only the atoms associated to the correct class (underlying subspace) of $z$ will be used for sparse representation. \n\nIt may be noted that dictionary learning approach is fundamentally different from dictionary construction using a random subset \\cite{sainath2011exemplar, gemmeke2011exemplar} of training features since we use all of the training data to compute an over-complete basis set for sparse representation which is far smaller (less than 3\\% in case of Numbers'95 database) than the actual collection size yet more effective in sparse representation~\\cite{dighe2015sparse}.  \n\n\n\\vspace{-3mm}\n\n\\subsection{Enhanced Acoustic Modeling}\\label{sec:sparse_representations}\n\n\\vspace{-2mm}\nWe use group sparsity based hierarchical Lasso algorithm~\\cite{sprechmann2011c} for sparse coding to enforce group sparsity in $\\alpha$ based on the internal partitioning of dictionary ${\\mathbf D}$ into senone-specific sub-dictionaries ${\\mathbf D}_{\\ell}$. The high dimensional group sparse representation $\\alpha$ is computed for each DNN output posterior feature $z$ by sparse recovery over ${\\mathbf D}$. Projection of a test posterior feature $z$ on training data space is given by computing ${\\mathbf D}\\alpha$.\n\nNote that ${\\mathbf D}\\alpha$ is an approximation of posterior feature $z$ based on $\\ell_{1}$-norm sparse reconstruction using atoms of ${\\mathbf D}$.  Consequently, it has the same dimension as $z$ and it is forced to lie in a probability simplex by normalization. Figure \\ref{fig:dict_sparse} summarizes this procedure. \n\\begin{figure}[t]\n  \\centering\n\n  \\includegraphics[width=0.97\\columnwidth]{dalpha.png}\n\n  \\caption{\\small DNN output senone posteriors $z$ are projected to the space of training posteriors using ${\\mathbf D}\\alpha$. Resulting projected posteriors are used for typical decoding in DNN-HMM framework.}  \n  \\label{fig:dict_sparse}\n\\end{figure}\n\\vspace{-3mm}\n\n\\section{Experimental Analysis}\\label{sec:analysis}\n\n\\vspace{-2mm}\nIn this section, we provide empirical analysis of the theoretical results established in Section~\\ref{sec:union_of_subspace}. These experiments confirm that the information bearing components of DNN class-conditional probabilities indeed live in a very low-dimensional space. Exploiting this structure enables enhancement of DNN based acoustic models and removes the effect of high-dimensional noise leading to improvement in DNN-HMM speech recognition performance. \n\\vspace{-2mm}\n\n\\subsection{Database and Speech Features}\\label{sec:regular}\n\n\nWe use Numbers'95 database for this study where only the utterances consisting of digits are considered (more details in~\\cite{dighe2015sparse}). The phoneset includes 27 phones and accordingly 557 context dependent tied states referred to as senones are learned by forced alignment of the training data using Kaldi speech recognition toolkit~\\cite{povey2011kaldi}.  \nA DNN is trained using sequence discriminative training~\\cite{vesely2013sequence} with 3 hidden layers each having 1024 nodes. For every 10 ms speech frame, the DNN input is a vector of MFCC+$\\Delta$+$\\Delta\\Delta$ features with a context of 9 frames (39$\\times$9=351 dimension). The DNN output is a vector of posterior probabilities corresponding to 557 senone classes. We use DNN posteriors as features $z$ for dictionary learning and sparse coding~\\eqref{eq:optimize_func}.\n\\vspace{-2mm}\n\n\\subsection{Low-rank Posterior Reconstruction}\\label{sec:uos-reconstruct}\n\n\nAs explained in Sections \\ref{sec:sparse}--\\ref{sec:sparse_representations}, DNN posteriors are used to learn senone-specific dictionaries ${\\mathbf D}_{\\ell}$ from the training data. Number of atoms $n_\\ell$ in each senone dictionary ${\\mathbf D}_\\ell$ is approximately 100. A value of $\\lambda=0.2$, optimized on development data, was used for sparse coding to get sparse representations $\\alpha$. Subsequently ${\\mathbf D}\\alpha$ projected posterior probabilities are computed for the test data. Sparsity leads to selection of a few subspaces of the training data resulting in new test posteriors which (1) live in low-dimensions, (2) are projected onto the subspace of the training posteriors, and (3) separated from the subspaces of other senone classes. We investigate these properties below through further analysis. \n\nTo provide an insight into the dimension of the senone subspaces, we construct matrices of 1000 class-specific senone posteriors and \ncompute the number of singular values required to preserve 95\\% variability of the data. Due to skewed distribution of the posteriors, we take their log prior to singular value decomposition. We refer to the number of required singular values as roughly the ``Rank'' of senone matrices.\nAn ideal posterior feature should have its maximum component at the support indicating its associated class. Hence, we group the posteriors as ``correct'' if the maximum component corresponds to the correct class and ``incorrect'' if the maximum component corresponds to the incorrect class. Table~\\ref{table:analysis} shows the average number of required singular values over all senones for DNN and projected posteriors. Another approach referred to as robust PCA based posteriors will be discussed in the subsequent section.  \n{\\footnotesize\n\\begin{table}[b]\\footnotesize\n\\centering\n\\begin{tabular}{lccc}\n\\hline\n& DNN & Projected & Robust PCA \\\\\n\\hhline{====}\nRank-Correct   & 36.6 & 11.9 &  7.6 \\\\\nRank-Incorrect & 45.5 & 21.7 &  11.7 \\\\\n\\hline\n\\end{tabular}\n\\caption{\\footnotesize Comparison of ``Rank'' of DNN posterior matrix, projected posterior matrix and RPCA senone posterior matrix.}\n\\label{table:analysis}\n\\end{table}}\n\nWe can see that the ``correct'' posteriors live in a space which has far lower dimension than the space of ``incorrect'' posteriors. In other words, the information bearing components in ``correct'' senone posteriors are fewer resulting in matrices which have lower rank compared to ``incorrect'' posteriors. Given that the ranks are nevertheless very low (compared to the dimension of the senone posteriors which is 557), the ``incorrect'' posterior are exposed to a high-dimensional spurious noise. Therefore, to enhance the posterior probabilities, \n\\begin{itemize}[label={},leftmargin=*]\n  \\item \\emph{the low-dimensional subspace has to be modeled/identified and the posterior has to be projected onto that space.}\n\\end{itemize}\nTo further investigate the subspaces selected for sparse recovery, the values in sparse representation $\\alpha$ for each class are summed to form $\\alpha$-sum vectors and the ``Rank'' of senone-specific $\\alpha$-sum matrices are computed. According to SSR property, it is expected that sparse recovery should select the subspaces from the underlying classes so the ``Rank'' of $\\alpha$-sum matrices has to be 1. In fact, we found that the empirical results averaged over the whole test set conformed to this theoretical insight indicating that \n\\begin{itemize}[label={},leftmargin=*]\n  \\item \\emph{subspace sparse recovery leads to selection of the subspaces belonging to the underlying senone classes.}\n\\end{itemize}\nThe class-specific dictionary learning for sparse coding enables us to model the non-linear manifold of the training data as a union of low-dimensional subspaces. A DNN posterior $z$ from the test data may not lie on this manifold due to presence of high-dimensional noise embedded in its components. It is important to extract the low-dimensional structure in $z$ while separating the effect of noise. Sparse coding does exactly this by finding the true underlying subspaces in sparse representation $\\alpha$ and enables projecting $z$ on the class-specific subspace of the training data manifold via ${\\mathbf D}\\alpha$ reconstruction. \n\\vspace{-2mm}\n\n\\subsection{Low-rank and Sparse Decomposition}\\label{sec:rpca}\n\n\n\nTo further study the \\emph{true} underlying dimension of the senone-specific subspaces, we consider robust principle component analysis (RPCA) based decomposition of the senone posteriors~\\cite{candes2011robust}. The idea of RPCA is to decompose a data matrix $\\mathbf{M}$ as\n\n", "index": 3, "text": "\\begin{equation}\\small \\label{eq:rpca}\n\\mathbf{M}=\\mathbf{L}+\\mathbf{N}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\small\\mathbf{M}=\\mathbf{L}+\\mathbf{N}\" display=\"block\"><mrow><mi mathsize=\"90%\">\ud835\udc0c</mi><mo mathsize=\"90%\" stretchy=\"false\">=</mo><mrow><mi mathsize=\"90%\">\ud835\udc0b</mi><mo mathsize=\"90%\" stretchy=\"false\">+</mo><mi mathsize=\"90%\">\ud835\udc0d</mi></mrow></mrow></math>", "type": "latex"}]