[{"file": "1601.08088.tex", "nexttext": "\nwhere $b(\\cdot)$ and $c(\\cdot)$ are known functions of the linear predictor and response, respectively. For the binomial distribution and the logistic link, $b(\\eta_j) = -n_j\\log(1+e^{\\eta_j})$ and $c(y_j) = \\log(n_j!/[y_j!(n_j-y_j)!])$, with $n_j$ the number of Bernoulli trials made at the $j$th run. For the Poisson distribution and the log link, $b(\\eta_j) = e^{\\eta_j}$ and $c(y_j) = -\\log(y_j!)$.\n\nMaximum likelihood estimators (MLEs) $\\hat{{\\boldsymbol{\\beta}}}$ can be found via (numerical) maximization of~\\eqref{eq:loglik}. For small data sets, however, the MLEs may have considerable bias. For sparse data, such as binomial data with small numbers, $n_j$, of trials for each run, one or more maximum likelihood estimates may be infinite, for example, as the result of separation of the responses into zeros and ones via a hyperplane in the linear predictor \\citep{Silvapulle1981}. To remove this bias and guarantee the existence of estimates for GLMs with a canonical link function, \\citet{Firth1993} defined penalised maximum likelihood estimators $\\tilde{{\\boldsymbol{\\beta}}}$ as maximisers of\n\n", "itemtype": "equation", "pos": 4504, "prevtext": "\n\n\\begin{frontmatter}\n\n\\title{Model selection via Bayesian information capacity designs for generalised linear models}\n\n\n\n\n\n\n\n\n\n\n\\author[UoS]{David C. Woods\\corref{correspondingauthor}}\n\\cortext[correspondingauthor]{Correspondence to: Mathematical Sciences, University of Southampton, Southampton, SO17 1BJ, UK.}\n\\ead{D.Woods@southampton.ac.uk}\n\\author[QUT]{James M. McGree}\n\\author[UoS]{Susan M. Lewis}\n\\address[UoS]{University of Southampton, UK}\n\\address[QUT]{Queensland University of Technology, Australia}\n\n\\begin{abstract}\nWe provide the first investigation of designs for screening experiments where the response variable is approximated by a generalised linear model. We define the Bayesian information capacity criterion for the selection of designs that are robust to the form of the linear predictor. For binomial data and logistic regression, the effectiveness of these designs for screening is assessed through simulation studies using all-subsets regression and model selection via maximum penalised likelihood and a generalised information criterion. For Poisson data and log-linear regression, similar assessments are made using maximum likelihood and AIC for minimally-supported designs that are constructed analytically. The results show that effective screening, that is, high power with moderate type I error rate and false discovery rate, can be achieved through suitable choices for the number of design support points and experiment size. Logistic regression is shown to present a more challenging problem than log-linear regression, and some areas are indicated for future work.  \n\\end{abstract}\n\n\\begin{keyword}\nBayesian $D$-optimality \\sep factorial experiments \\sep generalised information criterion \\sep screening. \n\n\\MSC[2010] 62K05 \\sep 62J12 \n\\end{keyword}\n\n\\end{frontmatter}\n\n\n\n\\section{Introduction}\\label{sec:intro}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn important problem in scientific discovery is to find those variables (or factors) that have a substantive influence on an observed response through experiments on a possibly large set of potentially important variables. There has been much research into such variable screening, or model selection, focussed on the design and analysis of experiments in which the response variable is adequately approximated by a linear model \\citep{DWDLV2014,WL2016}. Such experiments are used increasingly in scientific research and product development, for example, in the pharmaceutical and chemical industries.\n\nIn many practical applications, for example when binary or count data are observed, a generalised linear model (GLM; \\citealp{MN1989}) may be needed to describe a response. In common with other non-linear models, the performance of a design depends on the unknown values of the parameters in the model. One approach to overcoming this problem is to assume a particular value for each parameter and hence obtain a ``locally optimal'' design; that is, a design that is optimal under a given criterion provided the assigned parameter values are correct.  We adopt the alternative approach of making the less stringent assumption of a prior distribution for each model parameter, from which we obtain a ``pseudo -Bayesian'' design \\citep{AW2015}.\n\nIn this paper, we investigate variable screening for GLMs with $q$ independent variables, labelled $x_1, \\ldots, x_q$. In the $j$th run $(j = 1,\\ldots,N)$ of the experiment, a treatment or combination of variable values ${\\boldsymbol{x}}_j = (x_{1j}, \\ldots, x_{qj})^\\mathrm{T}$ is applied to an experimental unit and a univariate response, $y_j$, is observed. We assume that $|x_{ij}|\\le 1$ for $i=1,\\ldots,q;\\,j=1,\\ldots,N$.\n\nThe aim of the experiment is to identify those \\textit{active} variables having a substantial effect on the response variable and to estimate efficiently a GLM involving those variables alone. For $j=1,\\ldots,N$, the $y_j$ have independent exponential family distributions with expectation $\\mu_j$ linked to a linear predictor $\\eta_j = f({\\boldsymbol{x}}_j)^{\\mathrm{T}}{\\boldsymbol{\\beta}}$ via a link function, $g(\\mu_j) = \\eta_j$. The vectors $f({\\boldsymbol{x}})$ and ${\\boldsymbol{\\beta}}$ are $p\\times 1$ vectors of known functions of ${\\boldsymbol{x}}$ and unknown model parameters, respectively.  We also assume that the experimental units are exchangeable, in the sense that the distribution of the response to a treatment does not depend on the unit to which the treatment is applied.\n\nFor canonical link functions, the log-likelihood may be written as\n\n", "index": 1, "text": "\\begin{equation}\\label{eq:loglik}\nl({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}}) = \\sum_{j=1}^N \\left[y_j\\eta_j - b(\\eta_j) + c(y_j)\\right]\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"l({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})=\\sum_{j=1}^{N}\\left[y_{j}\\eta_{j}-b%&#10;(\\eta_{j})+c(y_{j})\\right]\\,,\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo>[</mo><mrow><mrow><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03b7</mi><mi>j</mi></msub></mrow><mo>-</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b7</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"4.2pt\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $X$ is the $N\\times p$ model matrix with $j$th row $f({\\boldsymbol{x}}_j)^{\\mathrm{T}}$ and $W = \\mathrm{diag}\\{\\mathrm{var}(y_j)\\}$ (see also \\citealp{KF2009}). This estimation procedure is equivalent to finding the posterior mode of ${\\boldsymbol{\\beta}}$ assuming the Jeffreys prior distribution.\n\nThe information matrix $X^\\mathrm{T}WX$, which is the asymptotic inverse variance-covariance matrix for both $\\hat{{\\boldsymbol{\\beta}}}$ and $\\tilde{{\\boldsymbol{\\beta}}}$, is used to define the $D$-optimality criterion. This criterion specifies selection of a design that maximises the objective function\n\n", "itemtype": "equation", "pos": 5766, "prevtext": "\nwhere $b(\\cdot)$ and $c(\\cdot)$ are known functions of the linear predictor and response, respectively. For the binomial distribution and the logistic link, $b(\\eta_j) = -n_j\\log(1+e^{\\eta_j})$ and $c(y_j) = \\log(n_j!/[y_j!(n_j-y_j)!])$, with $n_j$ the number of Bernoulli trials made at the $j$th run. For the Poisson distribution and the log link, $b(\\eta_j) = e^{\\eta_j}$ and $c(y_j) = -\\log(y_j!)$.\n\nMaximum likelihood estimators (MLEs) $\\hat{{\\boldsymbol{\\beta}}}$ can be found via (numerical) maximization of~\\eqref{eq:loglik}. For small data sets, however, the MLEs may have considerable bias. For sparse data, such as binomial data with small numbers, $n_j$, of trials for each run, one or more maximum likelihood estimates may be infinite, for example, as the result of separation of the responses into zeros and ones via a hyperplane in the linear predictor \\citep{Silvapulle1981}. To remove this bias and guarantee the existence of estimates for GLMs with a canonical link function, \\citet{Firth1993} defined penalised maximum likelihood estimators $\\tilde{{\\boldsymbol{\\beta}}}$ as maximisers of\n\n", "index": 3, "text": "\\begin{equation}\\label{eq:pl}\nl^\\star({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}}) = l({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}}) + \\frac{1}{2}\\log \\mbox{det}\\left\\{X^\\mathrm{T}WX\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"l^{\\star}({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})=l({\\boldsymbol{\\beta}};\\,{%&#10;\\boldsymbol{y}})+\\frac{1}{2}\\log\\mbox{det}\\left\\{X^{\\mathrm{T}}WX\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><msup><mi>l</mi><mo>\u22c6</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mi>X</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mi>X</mi></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 6580, "prevtext": "\nwhere $X$ is the $N\\times p$ model matrix with $j$th row $f({\\boldsymbol{x}}_j)^{\\mathrm{T}}$ and $W = \\mathrm{diag}\\{\\mathrm{var}(y_j)\\}$ (see also \\citealp{KF2009}). This estimation procedure is equivalent to finding the posterior mode of ${\\boldsymbol{\\beta}}$ assuming the Jeffreys prior distribution.\n\nThe information matrix $X^\\mathrm{T}WX$, which is the asymptotic inverse variance-covariance matrix for both $\\hat{{\\boldsymbol{\\beta}}}$ and $\\tilde{{\\boldsymbol{\\beta}}}$, is used to define the $D$-optimality criterion. This criterion specifies selection of a design that maximises the objective function\n\n", "index": 5, "text": "\\begin{equation}\\label{eq:dopt}\n\\phi_D(\\xi) = \\frac{1}{p}\\log \\mbox{det}\\left\\{X^\\mathrm{T}WX\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\phi_{D}(\\xi)=\\frac{1}{p}\\log\\mbox{det}\\left\\{X^{\\mathrm{T}}WX\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03d5</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03be</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>p</mi></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mi>X</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mi>X</mi></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\n${\\boldsymbol{x}}_1,\\ldots,{\\boldsymbol{x}}_n$ are the distinct treatments in the design (assumed, without loss of generality, to be applied to the first $n$ runs of the experiment), $\\omega_k>0\\in\\mathbb{N}$, and $\\sum_{k=1}^n\\omega_k = N$, the total number of runs. Clearly,~\\eqref{eq:dopt} depends on ${\\boldsymbol{\\beta}}$ through the matrix $W$ and hence selection of a $D$-optimal design requires knowledge of the values of these parameters. Thus a locally optimal design is obtained.\n\nThe relative performance of two designs, $\\xi_1$ and $\\xi_2$, under $D$-optimality may be assessed using relative $D$-efficiency, defined as\n\n", "itemtype": "equation", "pos": 6706, "prevtext": "\nwhere\n\n", "index": 7, "text": "\\begin{equation}\\label{eq:design} \n\\xi = \\left\\{\n\\begin{array}{ccc}\n{\\boldsymbol{x}}_1 & \\ldots & {\\boldsymbol{x}}_n \\\\\n\\omega_1 & \\ldots & \\omega_n \\\\\n\\end{array}\n\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\xi=\\left\\{\\begin{array}[]{ccc}{\\boldsymbol{x}}_{1}&amp;\\ldots&amp;{\\boldsymbol{x}}_{n%&#10;}\\\\&#10;\\omega_{1}&amp;\\ldots&amp;\\omega_{n}\\\\&#10;\\end{array}\\right\\}\\,,\" display=\"block\"><mrow><mrow><mi>\u03be</mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udc99</mi><mn>1</mn></msub></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><msub><mi>\ud835\udc99</mi><mi>n</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\u03c9</mi><mn>1</mn></msub></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><msub><mi>\u03c9</mi><mi>n</mi></msub></mtd></mtr></mtable><mo rspace=\"4.2pt\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $0\\le\\mbox{DEff}(\\xi_1,\\xi_2)$. If $\\xi_2$ is a $D$-optimal design that maximises~\\eqref{eq:dopt}, then~\\eqref{eq:Deff} provides an absolute measure of the performance of design $\\xi_1$.\n\nIn this paper, we address the screening problem of model selection and estimation of parameters in the selected model. We define, in Section~\\ref{sec:methods}, a Bayesian information capacity criterion that generalises $D$-optimality to provide model-robust designs for GLMs. We also present and discuss a model selection strategy that uses all-subsets regression and suitable penalties for model complexity. Sections~\\ref{sec:binomial} and~\\ref{sec:poisson} describe simulation studies of logistic and log-linear regression modelling, respectively, which demonstrate and assess the effectiveness of the methods. In Section~\\ref{sec:disc}, we present some avenues for future work to further develop methodology for screening experiments with non-normal data.\n\n\\section{Information capacity designs and model selection}\\label{sec:methods}\n\nConsider a set $\\mathcal{M}$ of $M = |\\mathcal{M}|$ distinct candidate models, each of which have the same link function. The linear predictor for the $m$th model and $j$th run is given by \n\n", "itemtype": "equation", "pos": 7530, "prevtext": "\n${\\boldsymbol{x}}_1,\\ldots,{\\boldsymbol{x}}_n$ are the distinct treatments in the design (assumed, without loss of generality, to be applied to the first $n$ runs of the experiment), $\\omega_k>0\\in\\mathbb{N}$, and $\\sum_{k=1}^n\\omega_k = N$, the total number of runs. Clearly,~\\eqref{eq:dopt} depends on ${\\boldsymbol{\\beta}}$ through the matrix $W$ and hence selection of a $D$-optimal design requires knowledge of the values of these parameters. Thus a locally optimal design is obtained.\n\nThe relative performance of two designs, $\\xi_1$ and $\\xi_2$, under $D$-optimality may be assessed using relative $D$-efficiency, defined as\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:Deff}\n\\mbox{DEff}(\\xi_1,\\xi_2) = \\exp\\left\\{\\phi_D(\\xi_1) - \\phi_D(\\xi_2)\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mbox{DEff}(\\xi_{1},\\xi_{2})=\\exp\\left\\{\\phi_{D}(\\xi_{1})-\\phi_{D}(\\xi_{2})%&#10;\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><mtext>DEff</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\u03be</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><msub><mi>\u03d5</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03d5</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere the $\\beta_{im}$ are the values of the parameters in model $m$, $I(i,m)=1$ if variable $i$ is in model $m$. If variable $i$ is not included in model $m$, then $I(i,m)=\\beta_{im}=0$. Hence, the number of parameters in model $m$ is $p_m = 1 + \\sum_{i=1}^q I(i,m)$.\n\n\\subsection{Bayesian information capacity}\\label{sec:ic}\n\nInformation capacity (IC) was introduced as a linear-model design selection criterion by \\citet{sun1993}. It has been further developed and applied by, for example, \\citet{wu1993} (supersaturated designs), and \\citet{LN2000} (model-robust factorial designs). In essence, this criterion seeks a design whose projections onto subsets of the variables produce sub-designs having good estimation properties for the corresponding submodels. This is achieved by selecting a design that maximises a weighted average of the $D$-criterion objective function for each submodel.\n\nFor GLMs, \\citet{woods2010} employed the criterion of \\citet{WLER2006} to find locally optimal information capacity designs for an example having five variables. Designs were found that maximised\n\n", "itemtype": "equation", "pos": 8879, "prevtext": "\nwhere $0\\le\\mbox{DEff}(\\xi_1,\\xi_2)$. If $\\xi_2$ is a $D$-optimal design that maximises~\\eqref{eq:dopt}, then~\\eqref{eq:Deff} provides an absolute measure of the performance of design $\\xi_1$.\n\nIn this paper, we address the screening problem of model selection and estimation of parameters in the selected model. We define, in Section~\\ref{sec:methods}, a Bayesian information capacity criterion that generalises $D$-optimality to provide model-robust designs for GLMs. We also present and discuss a model selection strategy that uses all-subsets regression and suitable penalties for model complexity. Sections~\\ref{sec:binomial} and~\\ref{sec:poisson} describe simulation studies of logistic and log-linear regression modelling, respectively, which demonstrate and assess the effectiveness of the methods. In Section~\\ref{sec:disc}, we present some avenues for future work to further develop methodology for screening experiments with non-normal data.\n\n\\section{Information capacity designs and model selection}\\label{sec:methods}\n\nConsider a set $\\mathcal{M}$ of $M = |\\mathcal{M}|$ distinct candidate models, each of which have the same link function. The linear predictor for the $m$th model and $j$th run is given by \n\n", "index": 11, "text": "\\begin{equation}\\label{eq:IClinpred}\n\\eta_j^{m} = \\beta_{0m} + \\sum_{i = 1}^q \\beta_{im}x_{ij}I(i, m)\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\eta_{j}^{m}=\\beta_{0m}+\\sum_{i=1}^{q}\\beta_{im}x_{ij}I(i,m)\\,,\" display=\"block\"><mrow><mrow><msubsup><mi>\u03b7</mi><mi>j</mi><mi>m</mi></msubsup><mo>=</mo><mrow><msub><mi>\u03b2</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><mrow><msub><mi>\u03b2</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $X_m$ and $W_m$ are the respective model and weight matrices for the $m$th model in $\\mathcal{M}$.\n\nWe define the Bayesian IC criterion which incorporates into~\\eqref{eq:locoptIC} uncertainty in the parameter values assumed for each model. This criterion selects a design that maximises the objective function\n\n", "itemtype": "equation", "pos": 10092, "prevtext": "\nwhere the $\\beta_{im}$ are the values of the parameters in model $m$, $I(i,m)=1$ if variable $i$ is in model $m$. If variable $i$ is not included in model $m$, then $I(i,m)=\\beta_{im}=0$. Hence, the number of parameters in model $m$ is $p_m = 1 + \\sum_{i=1}^q I(i,m)$.\n\n\\subsection{Bayesian information capacity}\\label{sec:ic}\n\nInformation capacity (IC) was introduced as a linear-model design selection criterion by \\citet{sun1993}. It has been further developed and applied by, for example, \\citet{wu1993} (supersaturated designs), and \\citet{LN2000} (model-robust factorial designs). In essence, this criterion seeks a design whose projections onto subsets of the variables produce sub-designs having good estimation properties for the corresponding submodels. This is achieved by selecting a design that maximises a weighted average of the $D$-criterion objective function for each submodel.\n\nFor GLMs, \\citet{woods2010} employed the criterion of \\citet{WLER2006} to find locally optimal information capacity designs for an example having five variables. Designs were found that maximised\n\n", "index": 13, "text": "\\begin{equation}\\label{eq:locoptIC}\n\\Psi(\\xi) = \\sum_{m = 1}^M \\frac{1}{p_m}\\log\\mbox{det}\\left\\{ X_m^\\mathrm{T} W_m X_m\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\xi)=\\sum_{m=1}^{M}\\frac{1}{p_{m}}\\log\\mbox{det}\\left\\{X_{m}^{\\mathrm{T}}%&#10;W_{m}X_{m}\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03be</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mfrac><mn>1</mn><msub><mi>p</mi><mi>m</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mi>X</mi><mi>m</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>m</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>m</mi></msub></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $\\mathcal{B}_m\\subset\\mathbb{R}$ is the parameter space for model $m$, ${\\boldsymbol{\\beta}}_m = (\\beta_{0m}, \\ldots, \\beta_{qm})^\\mathrm{T}$, and $\\pi_m({\\boldsymbol{\\beta}}_m)$ is the prior distribution for ${\\boldsymbol{\\beta}}_m$. The choice of $\\pi({\\boldsymbol{\\beta}}_m)$ and $\\mathcal{B}_m$, and the evaluation of~\\eqref{eq:BayesIC}, are discussed in Section~\\ref{sec:binic} for logistic regression. For log-linear regression, we make use of results in the literature that enable analytical construction of minimally-supported $D$- and Bayesian $D$-optimal designs, see Section~\\ref{sec:poisic}.\n\n\\subsection{Model selection}\\label{sec:ms}\n\nA variety of model selection procedures exist for determining the most appropriate GLM from a set of models, including Bayesian \\citep{CHIK2008} and shrinkage methods \\citep{PH2007}. To focus our investigations on the impact of design selection, we restrict attention to all-subset regression and use an information criterion to adjust for the bias inherent from in-sample estimation of the prediction error (see \\citealp{BA2002}, ch. 2). When maximum likelihood estimation is employed, we use AIC as the model selection criterion, and choose a model that minimises\n\n", "itemtype": "equation", "pos": 10555, "prevtext": "\nwhere $X_m$ and $W_m$ are the respective model and weight matrices for the $m$th model in $\\mathcal{M}$.\n\nWe define the Bayesian IC criterion which incorporates into~\\eqref{eq:locoptIC} uncertainty in the parameter values assumed for each model. This criterion selects a design that maximises the objective function\n\n", "index": 15, "text": "\\begin{equation}\\label{eq:BayesIC}\n\\Phi(\\xi) = \\sum_{m = 1}^M \\frac{1}{p_m}\\int _{\\mathcal{B}_m} \\log\\mbox{det}\\left\\{ X_m^\\mathrm{T} W_m X_m\\right\\}\\pi_m({\\boldsymbol{\\beta}}_m)\\,\\mathrm{d}{\\boldsymbol{\\beta}}_m\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\Phi(\\xi)=\\sum_{m=1}^{M}\\frac{1}{p_{m}}\\int_{\\mathcal{B}_{m}}\\log\\mbox{det}%&#10;\\left\\{X_{m}^{\\mathrm{T}}W_{m}X_{m}\\right\\}\\pi_{m}({\\boldsymbol{\\beta}}_{m})\\,%&#10;\\mathrm{d}{\\boldsymbol{\\beta}}_{m}\\,,\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03be</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mfrac><mn>1</mn><msub><mi>p</mi><mi>m</mi></msub></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>m</mi></msub></msub><mrow><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mi>X</mi><mi>m</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>m</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>m</mi></msub></mrow><mo>}</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf37</mi><mi>m</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mpadded width=\"+1.7pt\"><msub><mi>\ud835\udf37</mi><mi>m</mi></msub></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $l_m(\\cdot;\\cdot)$ is the log-likelihood function~\\eqref{eq:loglik} for model $m$. \n\nWhen ${\\boldsymbol{\\beta}}$ is estimated via penalised maximum likelihood, see~\\eqref{eq:pl}, AIC is no longer an appropriate criterion. This is because the effective number of parameters is reduced, equivalent to the inclusion of prior information (\\citealp{GHV2014}). The reduction depends on the number, $N$, of runs, with a smaller number of effective parameters for smaller $N$. Hence when $N$ is small, use of AIC will over-penalise larger models. To avoid this problem, we use a generalised information criterion (GIC; \\citealp{KK1996}) that relaxes the assumptions of (i) estimation via maximum likelihood, and (ii) inclusion of the true model in $\\mathcal{M}$. Hence, we select the model that minimises\n\n", "itemtype": "equation", "pos": 12007, "prevtext": "\nwhere $\\mathcal{B}_m\\subset\\mathbb{R}$ is the parameter space for model $m$, ${\\boldsymbol{\\beta}}_m = (\\beta_{0m}, \\ldots, \\beta_{qm})^\\mathrm{T}$, and $\\pi_m({\\boldsymbol{\\beta}}_m)$ is the prior distribution for ${\\boldsymbol{\\beta}}_m$. The choice of $\\pi({\\boldsymbol{\\beta}}_m)$ and $\\mathcal{B}_m$, and the evaluation of~\\eqref{eq:BayesIC}, are discussed in Section~\\ref{sec:binic} for logistic regression. For log-linear regression, we make use of results in the literature that enable analytical construction of minimally-supported $D$- and Bayesian $D$-optimal designs, see Section~\\ref{sec:poisic}.\n\n\\subsection{Model selection}\\label{sec:ms}\n\nA variety of model selection procedures exist for determining the most appropriate GLM from a set of models, including Bayesian \\citep{CHIK2008} and shrinkage methods \\citep{PH2007}. To focus our investigations on the impact of design selection, we restrict attention to all-subset regression and use an information criterion to adjust for the bias inherent from in-sample estimation of the prediction error (see \\citealp{BA2002}, ch. 2). When maximum likelihood estimation is employed, we use AIC as the model selection criterion, and choose a model that minimises\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:aic}\nAIC(m;\\, \\hat{{\\boldsymbol{\\beta}}}) = -2l_m(\\hat{{\\boldsymbol{\\beta}}};\\,{\\boldsymbol{y}}) + 2p_m\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"AIC(m;\\,\\hat{{\\boldsymbol{\\beta}}})=-2l_{m}(\\hat{{\\boldsymbol{\\beta}}};\\,{%&#10;\\boldsymbol{y}})+2p_{m}\\,,\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo rspace=\"4.2pt\">;</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>l</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">^</mo></mover><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>p</mi><mi>m</mi></msub></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 12958, "prevtext": "\nwhere $l_m(\\cdot;\\cdot)$ is the log-likelihood function~\\eqref{eq:loglik} for model $m$. \n\nWhen ${\\boldsymbol{\\beta}}$ is estimated via penalised maximum likelihood, see~\\eqref{eq:pl}, AIC is no longer an appropriate criterion. This is because the effective number of parameters is reduced, equivalent to the inclusion of prior information (\\citealp{GHV2014}). The reduction depends on the number, $N$, of runs, with a smaller number of effective parameters for smaller $N$. Hence when $N$ is small, use of AIC will over-penalise larger models. To avoid this problem, we use a generalised information criterion (GIC; \\citealp{KK1996}) that relaxes the assumptions of (i) estimation via maximum likelihood, and (ii) inclusion of the true model in $\\mathcal{M}$. Hence, we select the model that minimises\n\n", "index": 19, "text": "\\begin{equation}\\label{eq:gic}\nGIC(m;\\,\\tilde{{\\boldsymbol{\\beta}}}) = -2l_m(\\tilde{{\\boldsymbol{\\beta}}};\\, {\\boldsymbol{y}}) + 2{\\mathrm{tr}}\\left\\{J^{-1}(\\tilde{{\\boldsymbol{\\beta}}})I(\\tilde{{\\boldsymbol{\\beta}}})\\right\\}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"GIC(m;\\,\\tilde{{\\boldsymbol{\\beta}}})=-2l_{m}(\\tilde{{\\boldsymbol{\\beta}}};\\,{%&#10;\\boldsymbol{y}})+2{\\mathrm{tr}}\\left\\{J^{-1}(\\tilde{{\\boldsymbol{\\beta}}})I(%&#10;\\tilde{{\\boldsymbol{\\beta}}})\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo rspace=\"4.2pt\">;</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>l</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">t</mi><mo>\u2062</mo><mi mathvariant=\"normal\">r</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mi>J</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwith $l_m^\\star(\\cdot;\\cdot)$ the penalised log-likelihood function~\\eqref{eq:pl} for model $m$; see also \\citet{MYA1994} and \\citet{ZLT2010}. The evaluation of $J(\\tilde{{\\boldsymbol{\\beta}}})$ and $I(\\tilde{{\\boldsymbol{\\beta}}})$ is straightforward for the GLMs and penalised likelihood estimation method used in this paper. The performance of the GIC is investigated in Section~\\ref{sec:binresults}.\n\nFollowing analysis of the data from an experiment, those variables found to be involved in the selected model are deemed to be active. In simulation studies to assess the performance of the model selection strategies, we use three summary measures: (i) \\textit{power}: the proportion of truly active variables that are correctly identified as active; (ii) \\textit{type I error rate}: the proportion of inactive variables that are incorrectly identified as active; and (iii) \\textit{false discovery rate} (FDR): the proportion of variables identified as active that are truly inactive. \n\n\\section{Designs and model selection for binomial response and logistic regression}\\label{sec:binomial}\n\nTo investigate the performance of the methodology for logistic regression we study a five variable example, with linear predictors of the form~\\eqref{eq:IClinpred}. We assume that any subset of these variables may be the set of active variables. Therefore there are 31 possible models. The models are ordered lexicographically within each model size and assigned labels $1,\\ldots,31$. Models $1,\\ldots,5$ have linear predictors that contain a single variable, $1,\\ldots,5$, respectively; models $6,\\ldots,15$ have two variables, $1,2;\\,1,3;\\,\\ldots,4,5$. Similarly, models $16,\\ldots,25$ are three variable models, 26 - 30 are four-variable models and model 31 contains all five variables. \n\nTo find optimal designs and perform subsequent simulation studies, the model parameters $\\beta_{im}$ are assumed to have independent prior distributions of the form\n\n", "itemtype": "equation", "pos": 13208, "prevtext": "\nwhere\n\n", "index": 21, "text": "\\begin{equation*}\nJ(\\tilde{{\\boldsymbol{\\beta}}}) = -\\frac{1}{N}\\,\\left.\\frac{\\partial^2 l_m^\\star({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{\\partial{\\boldsymbol{\\beta}}\\partial{\\boldsymbol{\\beta}}^\\mathrm{T}}\\right|_{\\tilde{{\\boldsymbol{\\beta}}}}\\,,\n\\quad I(\\tilde{{\\boldsymbol{\\beta}}}) = \\frac{1}{N}\\, \\left.\\frac{\\partial l_m^\\star({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{\\partial{\\boldsymbol{\\beta}}}\\frac{\\partial l_m({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{\\partial{\\boldsymbol{\\beta}}^\\mathrm{T}}\\right|_{\\tilde{{\\boldsymbol{\\beta}}}}\\,,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"J(\\tilde{{\\boldsymbol{\\beta}}})=-\\frac{1}{N}\\,\\left.\\frac{\\partial^{2}l_{m}^{%&#10;\\star}({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{\\partial{\\boldsymbol{\\beta}}%&#10;\\partial{\\boldsymbol{\\beta}}^{\\mathrm{T}}}\\right|_{\\tilde{{\\boldsymbol{\\beta}}%&#10;}}\\,,\\quad I(\\tilde{{\\boldsymbol{\\beta}}})=\\frac{1}{N}\\,\\left.\\frac{\\partial l%&#10;_{m}^{\\star}({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{\\partial{\\boldsymbol{%&#10;\\beta}}}\\frac{\\partial l_{m}({\\boldsymbol{\\beta}};\\,{\\boldsymbol{y}})}{%&#10;\\partial{\\boldsymbol{\\beta}}^{\\mathrm{T}}}\\right|_{\\tilde{{\\boldsymbol{\\beta}}%&#10;}}\\,,\" display=\"block\"><mrow><mrow><mrow><mrow><mi>J</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mpadded width=\"+1.7pt\"><msub><mrow><mrow><mpadded width=\"+1.7pt\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mpadded><mo>\u2062</mo><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msubsup><mi>l</mi><mi>m</mi><mo>\u22c6</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\udf37</mi></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi>\ud835\udf37</mi><mi mathvariant=\"normal\">T</mi></msup></mrow></mrow></mfrac></mrow><mo fence=\"true\">|</mo></mrow><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover></msub></mpadded></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mrow><mrow><mpadded width=\"+1.7pt\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mpadded><mo>\u2062</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>l</mi><mi>m</mi><mo>\u22c6</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\udf37</mi></mrow></mfrac><mo>\u2062</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>l</mi><mi>m</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo rspace=\"4.2pt\">;</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi>\ud835\udf37</mi><mi mathvariant=\"normal\">T</mi></msup></mrow></mfrac></mrow><mo fence=\"true\">|</mo></mrow><mover accent=\"true\"><mi>\ud835\udf37</mi><mo stretchy=\"false\">~</mo></mover></msub></mpadded></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\n\nwhere $\\kappa = 1,2,3$ and we assume $\\beta_{0m} = 0$ for $m = 1, \\ldots, M$.\n\n\n\\subsection{Information capacity designs}\\label{sec:binic}\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{>{\\centering\\arraybackslash}m{.9cm}>{\\centering\\arraybackslash}m{5cm}>{\\centering\\arraybackslash}m{6cm}}\n & $n = 6$ & $n=30$ \\[-2ex] \n$\\kappa = 1$ & \\includegraphics[scale={.3}]{figs/binomial15n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\  \n$\\kappa = 2$ & \\includegraphics[scale={.3}]{figs/binomial25n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\\n$\\kappa = 3$ & \\includegraphics[scale={.3}]{figs/binomial35n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:bindeff}Maximum, mean and minimum $D$-efficiencies for Bayesian information capacity designs for logistic regression with $n=6$ and 30 support points and three prior distributions ($\\kappa=1,2,3$) for the model parameters.}\n\\end{figure}\n\nWe relax the assumption in~\\eqref{eq:design} that $\\omega_k$ is integer ($k = 1,\\ldots,n$) and consider approximate designs (e.g. \\citealp{ADT}, ch. 9). An approximate Bayesian IC design for logistic regression maximises\n\n", "itemtype": "equation", "pos": 15728, "prevtext": "\nwith $l_m^\\star(\\cdot;\\cdot)$ the penalised log-likelihood function~\\eqref{eq:pl} for model $m$; see also \\citet{MYA1994} and \\citet{ZLT2010}. The evaluation of $J(\\tilde{{\\boldsymbol{\\beta}}})$ and $I(\\tilde{{\\boldsymbol{\\beta}}})$ is straightforward for the GLMs and penalised likelihood estimation method used in this paper. The performance of the GIC is investigated in Section~\\ref{sec:binresults}.\n\nFollowing analysis of the data from an experiment, those variables found to be involved in the selected model are deemed to be active. In simulation studies to assess the performance of the model selection strategies, we use three summary measures: (i) \\textit{power}: the proportion of truly active variables that are correctly identified as active; (ii) \\textit{type I error rate}: the proportion of inactive variables that are incorrectly identified as active; and (iii) \\textit{false discovery rate} (FDR): the proportion of variables identified as active that are truly inactive. \n\n\\section{Designs and model selection for binomial response and logistic regression}\\label{sec:binomial}\n\nTo investigate the performance of the methodology for logistic regression we study a five variable example, with linear predictors of the form~\\eqref{eq:IClinpred}. We assume that any subset of these variables may be the set of active variables. Therefore there are 31 possible models. The models are ordered lexicographically within each model size and assigned labels $1,\\ldots,31$. Models $1,\\ldots,5$ have linear predictors that contain a single variable, $1,\\ldots,5$, respectively; models $6,\\ldots,15$ have two variables, $1,2;\\,1,3;\\,\\ldots,4,5$. Similarly, models $16,\\ldots,25$ are three variable models, 26 - 30 are four-variable models and model 31 contains all five variables. \n\nTo find optimal designs and perform subsequent simulation studies, the model parameters $\\beta_{im}$ are assumed to have independent prior distributions of the form\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:prior}\n\\beta_{im} \\sim \\left\\{\n\\begin{array}{ll}\n\\mbox{Uniform}(\\kappa, 5) & \\mbox{for } i = 1,3,4 \\mbox{ and } I(i, m) = 1\\,, \\\\\n\\mbox{Uniform}(-5, -\\kappa) & \\mbox{for } i = 2,5 \\mbox{ and } I(i, m) = 1\\,, \\\\\n\n\\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\beta_{im}\\sim\\left\\{\\begin{array}[]{ll}\\mbox{Uniform}(\\kappa,5)&amp;\\mbox{for }i=%&#10;1,3,4\\mbox{ and }I(i,m)=1\\,,\\\\&#10;\\mbox{Uniform}(-5,-\\kappa)&amp;\\mbox{for }i=2,5\\mbox{ and }I(i,m)=1\\,,\\\\&#10;\\par&#10;\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>\u03b2</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>\u223c</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03ba</mi><mo>,</mo><mn>5</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>3</mn></mrow></mrow><mo>,</mo><mrow><mrow><mn>4</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mn>1</mn></mpadded></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mn>5</mn></mrow><mo>,</mo><mrow><mo>-</mo><mi>\u03ba</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mn>2</mn></mrow><mo>,</mo><mrow><mrow><mn>5</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mn>1</mn></mpadded></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $0<\\tilde{\\omega}_k = \\omega_k/N\\le 1$ and $f_m({\\boldsymbol{x}}_k)^{\\mathrm{T}}{\\boldsymbol{\\beta}}_m$ is the linear predictor for the $m$th model. Clearly, an optimal choice of approximate Bayesian IC design can be made independently of the total experiment size $N$. We found designs using simulated annealing \\citep{haines1987, woods2010} with the integral in~\\eqref{eq:icapprox} evaluated numerically as a summation across a quasi-Monte Carlo sample \\citep[][ch. 5]{Lemieux2009}.\n\nFigure~\\ref{fig:bindeff} summarises the $D$-efficiencies of the Bayesian IC designs for $n=6$ and $n=30$ support points and each choice of prior distribution for $\\kappa=1,2,3$ from~\\eqref{eq:prior}, obtained from (i) 500 random draws of ${\\boldsymbol{\\beta}}_m$ values from distribution~\\eqref{eq:prior}; (ii) the locally $D$-optimal design for each value of ${\\boldsymbol{\\beta}}_m$, again found using simulated annealing; and (iii) calculation of efficiency~\\eqref{eq:Deff} to compare the Bayesian IC design with the locally $D$-optimal design. In general, the efficiencies decrease with model size. For $n=6$, the efficiencies are highly variable, even for models of the same size. It is not uncommon for a Bayesian design for logistic regression to require a large number of support points (see \\citealp{cl1989}, and \\citealp{wl2011}). This variability is also evident in the simulation results for model selection. Hence, in the next section, we present only assessments of the designs with $n=30$ support points.\n\n\\subsection{Model selection results}\\label{sec:binresults}\n\nTo assess the performance of the designs for model selection, a simulation study was performed for each design in which (i) each of the models, in turn, was used as the true model for the data generating process; (ii) 1000 data sets were generated independently by simulating values of ${\\boldsymbol{\\beta}}_m$ from the prior distribution, followed by simulation of responses ${\\boldsymbol{y}}$ from a Binomial distribution; (iii) for each data set, each of the models in $\\mathcal{M}$ was fitted using maximum penalised likelihood, and the model selected that minimises GIC~\\eqref{eq:gic}; and (iv) power, type I error rate and FDR were calculated for each simulated data set. The optimal approximate designs were converted into exact designs via rounding $\\omega_k$ to the nearest integer. Results are provided for $N = 30, 50, 80, 100$ in Figures~\\ref{fig:binsim1} and~\\ref{fig:binsim3}, for $\\kappa = 1,3$ respectively. The results for $\\kappa = 2$ (not shown) are similar to those for $\\kappa=3$.   \n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$ & $N = 50$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic30_1} & \\includegraphics[scale={.33}]{figs/logistic50_1} \\\\\n$N = 80$ & $N = 100$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic80_1} & \\includegraphics[scale={.33}]{figs/logistic100_1}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:binsim1}Average power, type I error rate and FDR for logistic regression with $\\kappa = 1$ and $N=30,50,80,100$ runs.}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$ & $N = 50$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic30_3} & \\includegraphics[scale={.33}]{figs/logistic50_3} \\\\\n$N = 80$ & $N = 100$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic80_3} & \\includegraphics[scale={.33}]{figs/logistic100_3}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:binsim3}Average power, type I error rate and FDR for logistic regression with $\\kappa = 3$ and $N=30,50,80,100$ runs.}\n\\end{figure}\n\nFor all three prior distributions, high power is achieved for all experiment sizes: greater than 80\\% for $N=30, 50$, and greater than 90\\% for $N=80, 100$. The type I error rate is, unsurprisingly, an increasing function of the number of variables in the data-generating (true) model, as there are fewer inactive variables (smaller denominator) for larger true models. The maximum type I error rate of about 0.5 occurs for those true models involving four variables and corresponds to identifying, on average, less than one additional active variable. In contrast, FDR is a decreasing function of true model size, as again there are fewer inactive variables for larger true models. The maximum FDR of approximately 0.5 occurs when $N=30$ for true models that contain a single variable, and corresponds to identifying one fewer additional active variable on average. \n\nThe results are fairly similar for the different prior distributions. The major difference is lower type I error rates when $\\kappa = 3$, where there is a greater distinction between the sizes of the model coefficients for active and inactive variables. \n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$, Model 1 & $N = 100$, Model 1 \\[-2ex]\n\\includegraphics[scale={.33}]{figs/penlogistic30_11} & \\includegraphics[scale={.33}]{figs/penlogistic100_11} \\\\\n$N = 30$, Model 16 & $N = 100$, model 16 \\[-2ex]\n\\includegraphics[scale={.33}]{figs/penlogistic30_116} & \\includegraphics[scale={.33}]{figs/penlogistic100_116}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:pen}Boxplots of the GIC penalty from~\\eqref{eq:gic} with $\\kappa = 1$ and data generated from models 1 (variable 1 only) and 16 (variables 1 and 2 only).}\n\\end{figure}\n\nA key determinant of the model selection findings is the size of the GIC penalty term in~\\eqref{eq:gic}. Our numerical studies have shown that this depends not only on the size, $p_m$, of the model but also on the estimated model parameters and the goodness of fit, with better-fitting models having a smaller penalty. Figure~\\ref{fig:pen} shows the distributions of the penalties obtained when model 1 (variable 1) and model 16 (variables 1 and 2) are true for $N=30, 100$ and $\\kappa=1$. In general, the penalty is somewhat less than $2p_m$, although it increases with $N$, and hence does not penalise larger models to the same degree as AIC. Models that include the correct variables have smaller penalty than other models. Further research on the use of this penalty is needed.\n\n\\section{Design and model selection for Poisson response and log-linear regression}\\label{sec:poisson}\n\nTo investigate the performance of the methodology for log-linear regression, simulation studies were performed for two examples. Both assume the log link, $g(\\mu) = \\eta$, and linear predictors of the form~\\eqref{eq:IClinpred}. In the first example, there are again $q = 5$ variables that may affect the response (31 possible models) and prior distribution~\\eqref{eq:prior} is assumed. In the second example, there are $q=10$ variables but, in line with factor sparsity \\citep{BM1986}, we consider only linear predictors including at most three active variables (175 possible models). Prior distributions for $\\beta_{im}$ are given by~\\eqref{eq:prior} for $i=1,\\ldots,5$ and for the remaining parameters by\n\n\n", "itemtype": "equation", "pos": 17206, "prevtext": "\n\nwhere $\\kappa = 1,2,3$ and we assume $\\beta_{0m} = 0$ for $m = 1, \\ldots, M$.\n\n\n\\subsection{Information capacity designs}\\label{sec:binic}\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{>{\\centering\\arraybackslash}m{.9cm}>{\\centering\\arraybackslash}m{5cm}>{\\centering\\arraybackslash}m{6cm}}\n & $n = 6$ & $n=30$ \\[-2ex] \n$\\kappa = 1$ & \\includegraphics[scale={.3}]{figs/binomial15n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\  \n$\\kappa = 2$ & \\includegraphics[scale={.3}]{figs/binomial25n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\\n$\\kappa = 3$ & \\includegraphics[scale={.3}]{figs/binomial35n6.pdf} & \\includegraphics[scale={.3}]{figs/binomial15n30.pdf} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:bindeff}Maximum, mean and minimum $D$-efficiencies for Bayesian information capacity designs for logistic regression with $n=6$ and 30 support points and three prior distributions ($\\kappa=1,2,3$) for the model parameters.}\n\\end{figure}\n\nWe relax the assumption in~\\eqref{eq:design} that $\\omega_k$ is integer ($k = 1,\\ldots,n$) and consider approximate designs (e.g. \\citealp{ADT}, ch. 9). An approximate Bayesian IC design for logistic regression maximises\n\n", "index": 25, "text": "\\begin{equation}\\label{eq:icapprox}\n\\Phi^\\dagger(\\xi) = \\sum_{m = 1}^M \\frac{1}{p_m}\\int _{\\mathcal{B}_m} \\log\\mbox{det}\\left\\{ \\sum_{k=1}^n \\tilde{\\omega}_k\\mathrm{var}(y_k)f_m({\\boldsymbol{x}}_k)f_m({\\boldsymbol{x}}_k)^\\mathrm{T}\\right\\}\\pi_m({\\boldsymbol{\\beta}}_m)\\,\\mathrm{d}{\\boldsymbol{\\beta}}_m\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\Phi^{\\dagger}(\\xi)=\\sum_{m=1}^{M}\\frac{1}{p_{m}}\\int_{\\mathcal{B}_{m}}\\log%&#10;\\mbox{det}\\left\\{\\sum_{k=1}^{n}\\tilde{\\omega}_{k}\\mathrm{var}(y_{k})f_{m}({%&#10;\\boldsymbol{x}}_{k})f_{m}({\\boldsymbol{x}}_{k})^{\\mathrm{T}}\\right\\}\\pi_{m}({%&#10;\\boldsymbol{\\beta}}_{m})\\,\\mathrm{d}{\\boldsymbol{\\beta}}_{m}\\,,\" display=\"block\"><mrow><mrow><mrow><msup><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03be</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mfrac><mn>1</mn><msub><mi>p</mi><mi>m</mi></msub></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>m</mi></msub></msub><mrow><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mover accent=\"true\"><mi>\u03c9</mi><mo stretchy=\"false\">~</mo></mover><mi>k</mi></msub><mo>\u2062</mo><mi>var</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>f</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc99</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>f</mi><mi>m</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc99</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">T</mi></msup></mrow></mrow><mo>}</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf37</mi><mi>m</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mpadded width=\"+1.7pt\"><msub><mi>\ud835\udf37</mi><mi>m</mi></msub></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nAgain, $\\beta_{0m}=0$ and $\\kappa=1,2,3$.\n\n\\subsection{Minimally-supported designs}\\label{sec:poisic}\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n(a) & (d) \\[-2ex] \n\\includegraphics[scale={.3}]{figs/poisson5_1.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_1.pdf} \\\\  \n(b) & (e) \\[-2ex]\n\\includegraphics[scale={.3}]{figs/poisson5_2.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_2.pdf} \\\\\n(c) & (f) \\[-2ex]\n\\includegraphics[scale={.3}]{figs/poisson5_3.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_3.pdf} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:poissdeff}Maximum, mean and minimum $D$-efficiencies for robust designs for log-linear regression: five variables (a) $\\kappa = 1$, (b) $\\kappa = 2$ and (c) $\\kappa = 3$; 10 variables (d) $\\kappa = 1$, (e) $\\kappa = 2$ and (f) $\\kappa = 3$.}\n\\end{figure}\n\nWe restrict attention to designs that are minimally supported with respect to the maximal model, that is, where the number, $n$, of distinct support points is $q+1$. For this class of designs, \\citet{RWLE2009} and \\citet{ME2012} presented analytical design construction methods. \\citet{AW2015} showed that for these designs with $-1\\le x_{ij} \\le 1$ and $E(\\beta_{im})\\ge 1$, for any $m=1,\\ldots,M$,\n\n", "itemtype": "equation", "pos": 24411, "prevtext": "\nwhere $0<\\tilde{\\omega}_k = \\omega_k/N\\le 1$ and $f_m({\\boldsymbol{x}}_k)^{\\mathrm{T}}{\\boldsymbol{\\beta}}_m$ is the linear predictor for the $m$th model. Clearly, an optimal choice of approximate Bayesian IC design can be made independently of the total experiment size $N$. We found designs using simulated annealing \\citep{haines1987, woods2010} with the integral in~\\eqref{eq:icapprox} evaluated numerically as a summation across a quasi-Monte Carlo sample \\citep[][ch. 5]{Lemieux2009}.\n\nFigure~\\ref{fig:bindeff} summarises the $D$-efficiencies of the Bayesian IC designs for $n=6$ and $n=30$ support points and each choice of prior distribution for $\\kappa=1,2,3$ from~\\eqref{eq:prior}, obtained from (i) 500 random draws of ${\\boldsymbol{\\beta}}_m$ values from distribution~\\eqref{eq:prior}; (ii) the locally $D$-optimal design for each value of ${\\boldsymbol{\\beta}}_m$, again found using simulated annealing; and (iii) calculation of efficiency~\\eqref{eq:Deff} to compare the Bayesian IC design with the locally $D$-optimal design. In general, the efficiencies decrease with model size. For $n=6$, the efficiencies are highly variable, even for models of the same size. It is not uncommon for a Bayesian design for logistic regression to require a large number of support points (see \\citealp{cl1989}, and \\citealp{wl2011}). This variability is also evident in the simulation results for model selection. Hence, in the next section, we present only assessments of the designs with $n=30$ support points.\n\n\\subsection{Model selection results}\\label{sec:binresults}\n\nTo assess the performance of the designs for model selection, a simulation study was performed for each design in which (i) each of the models, in turn, was used as the true model for the data generating process; (ii) 1000 data sets were generated independently by simulating values of ${\\boldsymbol{\\beta}}_m$ from the prior distribution, followed by simulation of responses ${\\boldsymbol{y}}$ from a Binomial distribution; (iii) for each data set, each of the models in $\\mathcal{M}$ was fitted using maximum penalised likelihood, and the model selected that minimises GIC~\\eqref{eq:gic}; and (iv) power, type I error rate and FDR were calculated for each simulated data set. The optimal approximate designs were converted into exact designs via rounding $\\omega_k$ to the nearest integer. Results are provided for $N = 30, 50, 80, 100$ in Figures~\\ref{fig:binsim1} and~\\ref{fig:binsim3}, for $\\kappa = 1,3$ respectively. The results for $\\kappa = 2$ (not shown) are similar to those for $\\kappa=3$.   \n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$ & $N = 50$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic30_1} & \\includegraphics[scale={.33}]{figs/logistic50_1} \\\\\n$N = 80$ & $N = 100$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic80_1} & \\includegraphics[scale={.33}]{figs/logistic100_1}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:binsim1}Average power, type I error rate and FDR for logistic regression with $\\kappa = 1$ and $N=30,50,80,100$ runs.}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$ & $N = 50$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic30_3} & \\includegraphics[scale={.33}]{figs/logistic50_3} \\\\\n$N = 80$ & $N = 100$ \\[-2ex]\n\\includegraphics[scale={.33}]{figs/logistic80_3} & \\includegraphics[scale={.33}]{figs/logistic100_3}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:binsim3}Average power, type I error rate and FDR for logistic regression with $\\kappa = 3$ and $N=30,50,80,100$ runs.}\n\\end{figure}\n\nFor all three prior distributions, high power is achieved for all experiment sizes: greater than 80\\% for $N=30, 50$, and greater than 90\\% for $N=80, 100$. The type I error rate is, unsurprisingly, an increasing function of the number of variables in the data-generating (true) model, as there are fewer inactive variables (smaller denominator) for larger true models. The maximum type I error rate of about 0.5 occurs for those true models involving four variables and corresponds to identifying, on average, less than one additional active variable. In contrast, FDR is a decreasing function of true model size, as again there are fewer inactive variables for larger true models. The maximum FDR of approximately 0.5 occurs when $N=30$ for true models that contain a single variable, and corresponds to identifying one fewer additional active variable on average. \n\nThe results are fairly similar for the different prior distributions. The major difference is lower type I error rates when $\\kappa = 3$, where there is a greater distinction between the sizes of the model coefficients for active and inactive variables. \n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n$N = 30$, Model 1 & $N = 100$, Model 1 \\[-2ex]\n\\includegraphics[scale={.33}]{figs/penlogistic30_11} & \\includegraphics[scale={.33}]{figs/penlogistic100_11} \\\\\n$N = 30$, Model 16 & $N = 100$, model 16 \\[-2ex]\n\\includegraphics[scale={.33}]{figs/penlogistic30_116} & \\includegraphics[scale={.33}]{figs/penlogistic100_116}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:pen}Boxplots of the GIC penalty from~\\eqref{eq:gic} with $\\kappa = 1$ and data generated from models 1 (variable 1 only) and 16 (variables 1 and 2 only).}\n\\end{figure}\n\nA key determinant of the model selection findings is the size of the GIC penalty term in~\\eqref{eq:gic}. Our numerical studies have shown that this depends not only on the size, $p_m$, of the model but also on the estimated model parameters and the goodness of fit, with better-fitting models having a smaller penalty. Figure~\\ref{fig:pen} shows the distributions of the penalties obtained when model 1 (variable 1) and model 16 (variables 1 and 2) are true for $N=30, 100$ and $\\kappa=1$. In general, the penalty is somewhat less than $2p_m$, although it increases with $N$, and hence does not penalise larger models to the same degree as AIC. Models that include the correct variables have smaller penalty than other models. Further research on the use of this penalty is needed.\n\n\\section{Design and model selection for Poisson response and log-linear regression}\\label{sec:poisson}\n\nTo investigate the performance of the methodology for log-linear regression, simulation studies were performed for two examples. Both assume the log link, $g(\\mu) = \\eta$, and linear predictors of the form~\\eqref{eq:IClinpred}. In the first example, there are again $q = 5$ variables that may affect the response (31 possible models) and prior distribution~\\eqref{eq:prior} is assumed. In the second example, there are $q=10$ variables but, in line with factor sparsity \\citep{BM1986}, we consider only linear predictors including at most three active variables (175 possible models). Prior distributions for $\\beta_{im}$ are given by~\\eqref{eq:prior} for $i=1,\\ldots,5$ and for the remaining parameters by\n\n\n", "index": 27, "text": "\\begin{equation*}\\label{eq:prior2}\n\\beta_{im} \\sim \\left\\{\n\\begin{array}{ll}\n\\mbox{Uniform}(\\kappa, 5) & \\mbox{for } i = 6,8,9 \\mbox{ and } I(i, m) = 1\\,, \\\\\n\\mbox{Uniform}(-5, -\\kappa) & \\mbox{for } i = 7,10 \\mbox{ and } I(i, m) = 1\\,. \\\\\n\n\\end{array}\n\\right.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\beta_{im}\\sim\\left\\{\\begin{array}[]{ll}\\mbox{Uniform}(\\kappa,5)&amp;\\mbox{for }i=%&#10;6,8,9\\mbox{ and }I(i,m)=1\\,,\\\\&#10;\\mbox{Uniform}(-5,-\\kappa)&amp;\\mbox{for }i=7,10\\mbox{ and }I(i,m)=1\\,.\\\\&#10;\\par&#10;\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>\u03b2</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>\u223c</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03ba</mi><mo>,</mo><mn>5</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mrow><mn>6</mn><mo>,</mo><mn>8</mn></mrow></mrow><mo>,</mo><mrow><mrow><mn>9</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mn>1</mn></mpadded></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mn>5</mn></mrow><mo>,</mo><mrow><mo>-</mo><mi>\u03ba</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mn>7</mn></mrow><mo>,</mo><mrow><mrow><mn>10</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mn>1</mn></mpadded></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\nwhere $W_m^\\star = \\mathrm{diag}\\{e^{\\eta_{jm}^\\star}\\}$ with $\\eta_{jm}^\\star = f({\\boldsymbol{x}}_j)^{\\mathrm{T}}{\\boldsymbol{\\beta}}_m^\\star$ and ${\\boldsymbol{\\beta}}_m^\\star = E({\\boldsymbol{\\beta}}_m)$, the prior expectation of ${\\boldsymbol{\\beta}}_m$. Hence, numerical integration is no longer required for design evaluation. To exploit the available theory, we find designs that maximise\n\n", "itemtype": "equation", "pos": 25922, "prevtext": "\nAgain, $\\beta_{0m}=0$ and $\\kappa=1,2,3$.\n\n\\subsection{Minimally-supported designs}\\label{sec:poisic}\n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n(a) & (d) \\[-2ex] \n\\includegraphics[scale={.3}]{figs/poisson5_1.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_1.pdf} \\\\  \n(b) & (e) \\[-2ex]\n\\includegraphics[scale={.3}]{figs/poisson5_2.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_2.pdf} \\\\\n(c) & (f) \\[-2ex]\n\\includegraphics[scale={.3}]{figs/poisson5_3.pdf} & \\includegraphics[scale={.3}]{figs/poisson10_3.pdf} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:poissdeff}Maximum, mean and minimum $D$-efficiencies for robust designs for log-linear regression: five variables (a) $\\kappa = 1$, (b) $\\kappa = 2$ and (c) $\\kappa = 3$; 10 variables (d) $\\kappa = 1$, (e) $\\kappa = 2$ and (f) $\\kappa = 3$.}\n\\end{figure}\n\nWe restrict attention to designs that are minimally supported with respect to the maximal model, that is, where the number, $n$, of distinct support points is $q+1$. For this class of designs, \\citet{RWLE2009} and \\citet{ME2012} presented analytical design construction methods. \\citet{AW2015} showed that for these designs with $-1\\le x_{ij} \\le 1$ and $E(\\beta_{im})\\ge 1$, for any $m=1,\\ldots,M$,\n\n", "index": 29, "text": "$$\n\\int _{\\mathcal{B}_M} \\log\\mbox{det}\\left\\{ X_m^\\mathrm{T} W_mX_m\\right\\}\\pi_m({\\boldsymbol{\\beta}}_m)\\,\\mathrm{d}{\\boldsymbol{\\beta}}_m = \\log\\mbox{det}\\left\\{X_m^\\mathrm{T} W_m^\\star X_m\\right\\}\\,,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\int_{\\mathcal{B}_{M}}\\log\\mbox{det}\\left\\{X_{m}^{\\mathrm{T}}W_{m}X_{m}\\right%&#10;\\}\\pi_{m}({\\boldsymbol{\\beta}}_{m})\\,\\mathrm{d}{\\boldsymbol{\\beta}}_{m}=\\log%&#10;\\mbox{det}\\left\\{X_{m}^{\\mathrm{T}}W_{m}^{\\star}X_{m}\\right\\}\\,,\" display=\"block\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>M</mi></msub></msub><mrow><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mi>X</mi><mi>m</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>m</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>m</mi></msub></mrow><mo>}</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf37</mi><mi>m</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msub><mi>\ud835\udf37</mi><mi>m</mi></msub></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mi>X</mi><mi>m</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><msubsup><mi>W</mi><mi>m</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><msub><mi>X</mi><mi>m</mi></msub></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.08088.tex", "nexttext": "\n\nMaximisation of~\\eqref{eq:PoissIC} defines a (pseudo-) Bayesian $D$-optimality criterion for the maximal model. A heuristic justification for using this criterion to find model-robust designs was given by \\citet{ME2012} who pointed out that, assuming common prior distributions, the levels included for each variable in the minimally-supported Bayesian $D$-optimal design for each individual model $m$ are the same. Only the numbers of replications of each variable value differ between the designs. Hence the sub-designs defined as projections of the minimally-supported design for the maximal model into a subset of the variables will contain the same values of the variables as a minimally-supported optimal design for that subset of variables but with different replication. Typically, designs defined in this way display less balance in the variable levels than the $D$-optimal designs for the sub-models. The advantage of maximising~\\eqref{eq:PoissIC} is that no numerical optimisation is required for design selection, and hence large examples (e.g. 10 variables) can be investigated.  \n\nWe replicate minimally-supported Bayesian $D$-optimal designs that maximise~\\eqref{eq:PoissIC} to obtain designs with $N=16$ runs for five variables and $N=32$ runs for 10 variables. Figure~\\ref{fig:poissdeff} summarises, for each model, the $D$-efficiencies~\\eqref{eq:Deff} for the five variable and 10 variable designs, calculated as described in Section~\\ref{sec:binic} except that in step (ii), the locally $D$-optimal designs obtained from the Theorem of \\citet{RWLE2009} are used. In general, the efficiencies are somewhat higher than those achieved by the equivalent five variable designs for logistic regression. \n\nThere are three main points of interest: (1) the $D$-efficiencies are higher for the five variable design, due to the smaller number of variables in the maximal model leading to less imbalance in the variable values in the sub-designs; (2) for both the five-variable and 10-variable designs, the $D$-efficiency increases with the size of the model, reflecting the construction method of maximizing~\\eqref{eq:PoissIC} for the maximal model; and (3) the spread of the $D$-efficiencies decreases as $\\kappa$ increases, making the prior distribution more concentrated. In both examples and for all $\\kappa$ values, the minimum efficiency is greater than $\\sim$0.4, and the mean efficiency is greater than $\\sim$0.55. For smaller $\\kappa$ and models with larger numbers of variables, the designs often have much higher $D$-efficiencies.\n\n\\subsection{Model selection results}\\label{sec:poisresults}\n\nSimulations to assess the performance of the designs for model selection were conducted as described in Section~\\ref{sec:binresults} except that, in step (iii), the model parameters were estimated using maximum likelihood and a model was chosen using AIC. Figure~\\ref{fig:poisssim} shows the results for five variable and 10 variable studies with $\\kappa = 1$. Results in both cases are very encouraging, with almost uniformly high power and low type I error rates ($<0.2$). For data-generating models with only one active variable (models 1-5 for the five variable experiment and models 1-10 for the 10 variable experiment), the truly active variable is occasionally missed, and another variable is identified as active. These errors lead to slightly lower power for these models, and non-zero FDR. For models with larger numbers of variables, all active variables are successfully identified (power equal to 1). For the five variable study, the FDR is consistently just below 0.3, corresponding to a maximum of about one non-active variable being incorrectly identified as active. For the 10 variable study, no screening errors are made for models containing three active variables (model 56 onwards). For both studies, the somewhat counter-intuitive result that performance improves for true models containing more active variables is explained by the construction method of the design (see Section~\\ref{sec:poisic}), which focusses on the model containing the maximum number of variables.  \n\n\\begin{figure}\n\\begin{center}\n\\begin{tabular}{cc}\n(a) & (b) \\[-2ex]\n\\includegraphics[scale={.33}]{figs/log5_1} & \\includegraphics[scale={.33}]{figs/log10_1}\n\\end{tabular}\n\\end{center}\n\\caption{\\label{fig:poisssim}Power, type I error rate and FDR for log-linear regression for $\\kappa=1$: (a) five variables and $N=16$; (b) 10 variables and $N=32$.}\n\\end{figure}\n\n\\section{Discussion and further research}\\label{sec:disc}\n\nThis paper provides a first investigation of designs for screening variables under a generalised linear model. The results demonstrate that effective screening (high power with only moderate type I error rate and FDR) is achievable. For a binomial response and logistic regression, both design and model selection are more challenging than for a Poisson response, and larger designs are required to achieve good model selection results. For a binomial response, the results presented here can easily be extended to linear predictors that include products of variables representing interactions.  \n\nFuture work is needed to investigate in more detail the use of the GIC penalty with maximum penalised likelihood. In some experiments, it may also be necessary to choose the link function in addition to the linear predictor. Compromise designs for this situation were found by \\citet{WLER2006}. In this paper, we have restricted the size of the model space under consideration by applying the principle of factor sparsity or by restricting the number of variables. For larger model spaces, the curse of dimensionality may prevent an all-subsets approach to model selection and alternative methods, such as sampling the model space \\citep{SD2015} or shrinkage regression \\citep{FHT2010}, would need to be employed.\n\nWe chose designs for the binomial and Poisson examples that ensured all models were estimable. This strategy is in contrast to use of a design criterion tailored to model discrimination alone such as $T$-optimality \\citep{AF1975a}, where the requirement of model estimability is often not met for nested models. An alternative approach is to generalise to multiple models those design selection methods that focus on both estimation and discrimination, such as the use of compound criteria \\citep{Atkinson2008} or hybrid designs \\citep{WWEL2008}. \n\n\\section*{Acknowledgements}\nD. C. Woods was supported by Fellowship EP/J018317/1 from the UK Engineering and Physical Sciences Research Council.\n\n\\section*{References}\n\n\\bibliography{biblio}\n\n\n", "itemtype": "equation", "pos": 26525, "prevtext": "\nwhere $W_m^\\star = \\mathrm{diag}\\{e^{\\eta_{jm}^\\star}\\}$ with $\\eta_{jm}^\\star = f({\\boldsymbol{x}}_j)^{\\mathrm{T}}{\\boldsymbol{\\beta}}_m^\\star$ and ${\\boldsymbol{\\beta}}_m^\\star = E({\\boldsymbol{\\beta}}_m)$, the prior expectation of ${\\boldsymbol{\\beta}}_m$. Hence, numerical integration is no longer required for design evaluation. To exploit the available theory, we find designs that maximise\n\n", "index": 31, "text": "\\begin{equation}\\label{eq:PoissIC}\n\\Phi_D(\\xi) = \\log\\mbox{det}\\left\\{X_M^\\mathrm{T} W_M^\\star X_M\\right\\}\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{D}(\\xi)=\\log\\mbox{det}\\left\\{X_{M}^{\\mathrm{T}}W_{M}^{\\star}X_{M}\\right%&#10;\\}\\,.\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03be</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mtext>det</mtext></mrow><mo>\u2062</mo><mrow><mo>{</mo><mrow><msubsup><mi>X</mi><mi>M</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><msubsup><mi>W</mi><mi>M</mi><mo>\u22c6</mo></msubsup><mo>\u2062</mo><msub><mi>X</mi><mi>M</mi></msub></mrow><mo rspace=\"4.2pt\">}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]