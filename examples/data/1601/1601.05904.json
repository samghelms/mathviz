[{"file": "1601.05904.tex", "nexttext": "\n\nThe above equation is a simple IDW weighting function, as defined by \\cite{01Shepard:1968:TIF:800186.810616}, where $x$ denotes a prediction location, $x_{i }$ is a data \npoint, $d$ is the distance from the known data point $x_{i}$ to the unknown \ninterpolated point $x$, $n$ is the total number of data points used in \ninterpolating, and $p$ is an arbitrary positive real number called the power \nparameter or the distance-decay parameter (typically, $\\alpha $ = 2 in the \nstandard IDW). Note that in the standard IDW, the power/distance-decay \nparameter $\\alpha $ is a user-specified constant value for all unknown \ninterpolated points. \n\n\\subsection{The AIDW Interpolation}\nThe AIDW is an improved version of the standard IDW \\citep{01Shepard:1968:TIF:800186.810616}, which is \noriginated by \\cite{28DBLP:journals/gandc/LuW08}. The basic and most interesting \nidea behind the AIDW is that: it adaptively determines the distance-decay \nparameter $\\alpha $ according to the spatial pattern of data points in the \nneighborhood of the interpolated points. In other words, the distance-decay \nparameter $\\alpha $ is no longer a pre-specified constant value but \nadaptively adjusted for a specific unknown interpolated point according to \nthe distribution of the nearest neighboring data points.\n\nWhen predicting the desired values for the interpolated points using AIDW, \nthere are typically two phases: the first one is to determine adaptively the \npower parameter $\\alpha $ according to the spatial pattern of data points; \nand the second is to perform the weighting average of the values of data \npoints. The second phase is the same as that in the standard IDW; see \nEquation (\\ref{eq1}).\n\nIn AIDW, for each interpolated point, the\nparameter $\\alpha $ can be adaptively determined according to the following steps.\n\n\\textbf{Step 1}: Determine the spatial pattern by comparing the observed \naverage nearest neighbor distance with the expected nearest neighbor \ndistance.\n\n\\begin{enumerate}[1)]\n\t\\item Calculate the expected nearest neighbor distance $r_{\\exp } $ for a random pattern using:\n\n", "itemtype": "equation", "pos": 12632, "prevtext": "\n\n \n\\begin{frontmatter}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Improving GPU-accelerated Adaptive IDW Interpolation Algorithm \\\\Using \n\t   Fast \\textit{k}NN Search}\n\n\n\n\n\n\n\\author[myaddress]{Gang Mei}\n\\author[myaddress]{Nengxiong Xu\\corref{mycorrespondingauthor}}\n\\cortext[mycorrespondingauthor]{Corresponding author}\\ead{xunengxiong@cugb.edu.cn}\n\\author[myaddress]{Liangliang Xu}\n\\address[myaddress]{School of Engineering and Technology, China University of Geosciences, Beijing 100083, China}\n\n\n\\begin{abstract}\n\nThis paper presents an efficient parallel Adaptive Inverse Distance \nWeighting (AIDW) interpolation algorithm on modern Graphics Processing Unit \n(GPU). The presented algorithm is an improvement of our previous \nGPU-accelerated AIDW algorithm by adopting fast $k$-\\textbf{N}earest \n\\textbf{N}eighbors ($k$NN) search. In AIDW, it needs to find several nearest \nneighboring data points for each interpolated point to adaptively determine \nthe power parameter; and then the desired prediction value of the interpolated point \nis obtained by weighted interpolating using the power parameter. In this \nwork, we develop a fast $k$NN search approach based on the space-partitioning \ndata structure, even grid, to improve the previous GPU-accelerated AIDW algorithm. \nThe improved algorithm is composed of the stages of $k$NN search and weighted \ninterpolating. To evaluate the performance of the improved algorithm, we \nperform five groups of experimental tests. Experimental results show that: \n(1) the improved algorithm can achieve a speedup of up to 1017 over the \ncorresponding serial algorithm; (2) the improved algorithm is at least two \ntimes faster than our previous GPU-accelerated AIDW algorithm; and (3) the utilization \nof fast $k$NN search can significantly improve the computational efficiency of \nthe entire GPU-accelerated AIDW algorithm.\n\\end{abstract}\n\n\\begin{keyword}\nGraphics Processing Unit (GPU) \\sep $k$-Nearest Neighbors \n($k$NN) \\sep Inverse Distance Weighting (IDW) \\sep Spatial Interpolation\n\n\n\n\n\n\n\n\\end{keyword}\n\n\n\\end{frontmatter}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec1:introduction}\n\nSpatial interpolation is a fundamental tool in Geographic Information System \n(GIS). The most frequently used spatial interpolation algorithms include the \nInverse Distance Weighting (IDW) \\citep{01Shepard:1968:TIF:800186.810616}, Kriging \\citep{02Krige1951JCMMS}, Discrete \nSmoothing Interpolation (DSI) \\citep{03DBLP:journals/tog/Mallet89,04DBLP:journals/cad/Mallet92}, nearest neighbors, \netc; see a comparative survey investigated by \\cite{05DBLP:journals/gandc/FaliveneCTS10}. When applying those \ninterpolation algorithms for large-scale datasets, the computational cost is \nin general too high \\citep{06DBLP:journals/gandc/HuangY11}. A common and effective solution to \nthe above problem is to perform the interpolation procedures in parallel. \nCurrently, many efforts have been carried out to parallelize those \ninterpolation algorithms in various environments on multi-core CPU and/or \nmany-core GPU platforms \\citep{07ISI:000320704100005}. \n\nFor example, to accelerate the Kriging method, \\cite{08DBLP:journals/gandc/PesquerCP11}\nproposed a solution to parallelizing the ordinary Kriging using MPI (Message \nPassing Interface) libraries in an HPC (High Performance Computing) \nenvironment, and significantly reduced the final execution time of the \nentire process. Similarly, \\cite{09DBLP:conf/para/StrzelczykP10} presented a new \nparallel Kriging algorithm to deal with unevenly spaced data. \\cite{10DBLP:journals/gandc/Cheng13} proposed an efficient parallel scheme to accelerate \nthe universal Kriging algorithm on the NVIDIA CUDA platform by optimizing \nthe compute-intensive steps in the Kriging algorithm, such as matrix--vector \nmultiplication and matrix--matrix multiplication and achieved a nearly 18 \nspeedup over the serial program.\n\n \\cite{11DBLP:conf/iccS/AllombertMDBBAJ14} introduced an efficient out-of-core algorithm \nthat fully benefited from graphics cards acceleration on a desktop computer, \nand found that it was able to speed up Kriging on the GPU with data 4 times \nlarger than a classical in-core GPU algorithm, with a limited loss of \nperformances.\n\nTo improve the computational efficiency of the most time-consuming steps in \nordinary Kriging, i.e., the weights calculation and then the estimate for \neach unknown point, \\cite{12DBLP:journals/gandc/RaveJAG14} investigated the \npotential reduction in execution time by selecting the suitable operations \ninvolved in those steps to be parallelized by using general-purpose \ncomputing on GPUs and CUDA. \n\n\\cite{13DBLP:journals/gandc/HuS15} proposed an improved coarse-grained parallel \nalgorithm to accelerate ordinary Kriging interpolation in a homogeneously \ndistributed memory system using the MPI (Message Passing Interface) model \nand achieved the speedups of up to 20.8. \\cite{14doi:10.1080/15481603.2014.1002379} \nproposed an algorithm based on the $k$-d tree method to partition a big dataset \ninto workload-balanced child data groups, and achieved high efficiency when \nthe datasets were divided into an optimal number of child data groups.\n\nThe IDW interpolation algorithm has been also parallelized on various \nplatforms. For example, exemplified by a hybrid IDW algorithm to generate \nDEM from LiDAR point clouds, \\cite{15DBLP:journals/gandc/GuanW10} designed and \nimplemented a parallel pipeline algorithm for multi-core platforms, and \nprocessed nearly one billion LiDAR points in about 12 min and produced a \n27,500 $\\times $ 30,500 raster DEM using less than 800 MB of main memory on a \n2.0 GHz Quad-Core Intel Xeon platform. \\cite{16,17} accelerated the \nIDW method on the GPU for predicting the snow cover depth at the desired \npoint.\n\n\\cite{18,19DBLP:journals/jzusc/XiaKL11} proposed a generic methodological framework for geospatial \nanalysis based on GPU and explored how to map the inherent parallelism \ndegrees of IDW interpolation, which gave rise to a high computational \nthroughput. \\cite{20DBLP:journals/gandc/HuangLTWCH11} explored of the implementation of a \nparallel IDW interpolation algorithm in a Linux cluster-based parallel GIS. \n\\cite{21} developed their IDW interpolation application uses the \nJava Virtual Machine (JVM) for the multi-threading functionality.\n\n\\cite{22ISI:000332539900001} developed two GPU implementations of the standard IDW \ninterpolation algorithm, the tiled version and the CDP version, by \nexploiting the shared memory and CUDA Dynamic Parallelism, and observed that \nthe tiled version can achieve the speedups of 120 and 670 over the CPU \nversion when the power parameter was set to 2 and 3.0, respectively. \\cite{23} \nalso evaluated the impact of several data layouts on the efficiency of \nGPU-accelerated IDW interpolation.\n\nSome of the other efforts have been also carried out to parallelize other \ninterpolation algorithms. For example, \\cite{24DBLP:journals/gandc/WangGY10} presented a \ncomputing scheme to speed up the Projection-Onto-Convex-Sets (POCS) \ninterpolation for 3D irregular seismic data with GPUs. \\cite{25DBLP:journals/gis/GuanKG11}\ndeveloped a parallel the fast Fourier transform (FFT) based \ngeostatistical areal interpolation algorithm in a homogeneously distributed \nmemory system using the MPI programming model. \\cite{26DBLP:journals/esi/HuangCCLW12}\nemployed the $k$-d tree in nearest neighbors search to accelerate the grid \ninterpolation on the GPU. \\cite{27DBLP:conf/fedcsis/CuomoGGS13} proposed a parallel method \nbased on radial basis functions for surface reconstruction on GPU. \n\nThe Adaptive IDW (AIDW) is an improved version of the standard IDW \\citep{01Shepard:1968:TIF:800186.810616}, which was originally proposed by \\cite{28DBLP:journals/gandc/LuW08}. \nThe basic and most interesting idea behind the AIDW is that: it attempts to \ncalculate the power parameter adaptively according to the spatial \ndistribution pattern of the data points, while in the standard IDW the power \nparameter is a user-specified constant value. Due to the adaptive \ndetermination of the power parameter, the AIDW method can achieve much more \naccurate prediction results than those by the standard IDW. \n\nIn our previous work \\citep{29DBLP:journals/corr/MeiXX15}, we have designed and implemented a parallel AIDW \nalgorithm on a GPU. And we have also evaluated the performance of the \nparallel AIDW method by comparing its efficiency with that of the \ncorresponding serial one. We have observed that our GPU-accelerated AIDW \nalgorithm can achieve the speedups of up to 400 for one million data points \nand interpolated points on single precision.\n\nIn our previous GPU implementations of the parallel AIDW method, we have \nfound that the most computationally intensive step is the $k$ Nearest Neighbors \n($k$NN) search for each interpolated points. We have designed a straightforward \nmethod to find the $k$ nearest neighboring data points for each interpolated \npoint within a single thread. Although the GPU implementing using our \nstraightforward $k$NN search approach can achieve satisfied computational \nefficiency, for example, the obtained speedups are about 100 $\\sim $ 400 on \nsingle precision, further performance improvement probably can be achieved by \noptimizing the $k$NN search. \n\nThe task of the $k$NN search is to find the nearest neighbors to an input \nquery. Previous research works on the $k$NN search are mainly implemented and \noptimized in CPU \\citep{n01DBLP:journals/cg/SankaranarayananSV07}. Recently, GPU-accelerated implementations \nhave improved performance by utilizing the massively parallel architecture \nof a single GPU \\citep{n02DBLP:conf/cvpr/GarciaDB08,n03DBLP:journals/ijpp/LeiteTFRTK12,n04DBLP:conf/icde/PanM12,n05Liang5382329,06DBLP:journals/gandc/HuangY11, n07DBLP:journals/prl/BeliakovL12, n08DBLP:journals/corr/KomarovDD13, n09DBLP:journals/prl/LiuW15}, multi-GPUs \\citep{n10DBLP:journals/concurrency/KatoH12,n11arefin2012gpu}, and GPU clusters \\citep{n12}. \nAmong those GPU-accelerated $k$NN search algorithms, most of them focusing on \nspeeding up the brute-force $k$NN search algorithm; and several of them are \ndesigned and optimized using space partitioning data structures such as grid \n\\citep{n03DBLP:journals/ijpp/LeiteTFRTK12}, RP-tree \\citep{n04DBLP:conf/icde/PanM12}, VP-tree \n\\citep{n09DBLP:journals/prl/LiuW15}, and $k$-d tree \\citep{n07DBLP:journals/prl/BeliakovL12}. \n\nIn this paper, we attempt to improve the efficiency of our previous \nGPU-accelerated AIDW algorithm by adopting a more efficient $k$NN search \napproach. The efficient $k$NN search is expected to be performed in a separate \nstage with the use of the data structure, grid. The resulting values of the \n$k$NN search are the distances between the $k$ nearest neighboring data points to \neach interpolated point. Those distances are then transferred into another \nstage of the AIDW to adaptively calculate the power parameter and the \nexpected prediction value (i.e., the weighted average). To evaluate the \nimproved parallel AIDW algorithm, we also compare its efficiency with that \nof our previous one introduced in \\cite{29DBLP:journals/corr/MeiXX15}.\n\nThe rest of this paper is organized as follows. Section 2 introduces the \nbackground principles of the IDW algorithm, the AIDW algorithm, and the \n$k$NN search. Section 3 describes the strategies and considerations for \nimproving our previous GPU-accelerated AIDW algorithm. Section 4 presents \nsome implementation details of the improved algorithm. Some comparative \nexperimental tests and analysis are provided in Section 5. Finally, Section \n6 draws several conclusions.\n\n\\section{Background}\nThis section will briefly introduce the principles of the standard IDW \ninterpolation method \\citep{01Shepard:1968:TIF:800186.810616}, the AIDW interpolation method \n\\citep{28DBLP:journals/gandc/LuW08}, and the $k$NN search.\n\n\\subsection{The Standard IDW Interpolation}\nThe IDW algorithm is one of the most popular spatial interpolation methods \nin Geosciences, which calculates the prediction values of \nunknown/interpolated points by weighting average of the values of known/data \npoints. The name given to this type of methods was motivated by the weighted \naverage applied since it resorts to the inverse of the distance to each \nknown point when calculating the weights. The difference between different \nforms of IDW interpolation is that they calculate the weights variously. \n\nA general form of predicting an interpolated value $Z$ at a given point $x$ based \non samples $Z_{i}=Z(x_{i})$ for $i$ = 1, 2, {\\ldots}, $n$ using IDW is an \ninterpolating function: \n\n\n", "index": 1, "text": "\\begin{equation}\n\t\\label{eq1}\n\tZ(x)=\\sum\\limits_{i=1}^n {\\frac{\\omega _i (x)z_i }{\\sum\\limits_{j=1}^n \n\t\t\t{\\omega _j (x)} }} ,\n\t\\quad\n\t\\omega _i (x)=\\frac{1}{d(x,x_i )^\\alpha }.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Z(x)=\\sum\\limits_{i=1}^{n}{\\frac{\\omega_{i}(x)z_{i}}{\\sum\\limits_{j=1}^{n}{%&#10;\\omega_{j}(x)}}},\\quad\\omega_{i}(x)=\\frac{1}{d(x,x_{i})^{\\alpha}}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><mrow><msub><mi>\u03c9</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c9</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><msub><mi>\u03c9</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>d</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05904.tex", "nexttext": "\nwhere $n$ is the number of points in the study area, and $A$ is the area of the \nstudy region.\n\n\t\\item Calculate the observed average nearest neighbor distance $r_{obs} $ by taking the average of the nearest neighbor distances for all points:\n\n\n", "itemtype": "equation", "pos": 14918, "prevtext": "\n\nThe above equation is a simple IDW weighting function, as defined by \\cite{01Shepard:1968:TIF:800186.810616}, where $x$ denotes a prediction location, $x_{i }$ is a data \npoint, $d$ is the distance from the known data point $x_{i}$ to the unknown \ninterpolated point $x$, $n$ is the total number of data points used in \ninterpolating, and $p$ is an arbitrary positive real number called the power \nparameter or the distance-decay parameter (typically, $\\alpha $ = 2 in the \nstandard IDW). Note that in the standard IDW, the power/distance-decay \nparameter $\\alpha $ is a user-specified constant value for all unknown \ninterpolated points. \n\n\\subsection{The AIDW Interpolation}\nThe AIDW is an improved version of the standard IDW \\citep{01Shepard:1968:TIF:800186.810616}, which is \noriginated by \\cite{28DBLP:journals/gandc/LuW08}. The basic and most interesting \nidea behind the AIDW is that: it adaptively determines the distance-decay \nparameter $\\alpha $ according to the spatial pattern of data points in the \nneighborhood of the interpolated points. In other words, the distance-decay \nparameter $\\alpha $ is no longer a pre-specified constant value but \nadaptively adjusted for a specific unknown interpolated point according to \nthe distribution of the nearest neighboring data points.\n\nWhen predicting the desired values for the interpolated points using AIDW, \nthere are typically two phases: the first one is to determine adaptively the \npower parameter $\\alpha $ according to the spatial pattern of data points; \nand the second is to perform the weighting average of the values of data \npoints. The second phase is the same as that in the standard IDW; see \nEquation (\\ref{eq1}).\n\nIn AIDW, for each interpolated point, the\nparameter $\\alpha $ can be adaptively determined according to the following steps.\n\n\\textbf{Step 1}: Determine the spatial pattern by comparing the observed \naverage nearest neighbor distance with the expected nearest neighbor \ndistance.\n\n\\begin{enumerate}[1)]\n\t\\item Calculate the expected nearest neighbor distance $r_{\\exp } $ for a random pattern using:\n\n", "index": 3, "text": "\\begin{equation}\n\t\\label{eq2}\n\tr_{\\exp } =\\frac{1}{2\\sqrt {n \\mathord{\\left/ {\\vphantom {n A}} \\right. \n\t\t\t\t\\kern-\\nulldelimiterspace} A} },\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"r_{\\exp}=\\frac{1}{2\\sqrt{n\\mathord{\\left/{\\vphantom{nA}}\\right.\\kern-1.2pt}A}},\" display=\"block\"><mrow><mrow><msub><mi>r</mi><mi>exp</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><msqrt><mrow><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>A</mi></mrow></msqrt></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05904.tex", "nexttext": "\nwhere $k$ is the number of nearest neighbor points, and $d_i $ is the \nnearest neighbor distances. The $k$ can be specified before interpolating.\n\n\n\t\\item Obtain the nearest neighbor statistic $R\\left( {S_0 } \\right)$ by:\n\n\n", "itemtype": "equation", "pos": 15318, "prevtext": "\nwhere $n$ is the number of points in the study area, and $A$ is the area of the \nstudy region.\n\n\t\\item Calculate the observed average nearest neighbor distance $r_{obs} $ by taking the average of the nearest neighbor distances for all points:\n\n\n", "index": 5, "text": "\\begin{equation}\n\t\\label{eq3}\n\tr_{obs} =\\frac{1}{k}\\sum\\limits_{i=1}^k {d_i } ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"r_{obs}=\\frac{1}{k}\\sum\\limits_{i=1}^{k}{d_{i}},\" display=\"block\"><mrow><mrow><msub><mi>r</mi><mrow><mi>o</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>d</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05904.tex", "nexttext": "\nwhere $S_{0 }$ is the location of an interpolated point.\n\\end{enumerate}\n\n\\textbf{Step 2}: Normalize the $R\\left( {S_0 } \\right)$ measure to $\\mu _R $ \nsuch that $\\mu _R $ is bounded by 0 and 1 by a fuzzy membership function: \n\n", "itemtype": "equation", "pos": 15636, "prevtext": "\nwhere $k$ is the number of nearest neighbor points, and $d_i $ is the \nnearest neighbor distances. The $k$ can be specified before interpolating.\n\n\n\t\\item Obtain the nearest neighbor statistic $R\\left( {S_0 } \\right)$ by:\n\n\n", "index": 7, "text": "\\begin{equation}\n\t\\label{eq4}\n\tR\\left( {S_0 } \\right)=\\frac{r_{obs} }{r_{\\exp } },\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"R\\left({S_{0}}\\right)=\\frac{r_{obs}}{r_{\\exp}},\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><msub><mi>r</mi><mrow><mi>o</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><msub><mi>r</mi><mi>exp</mi></msub></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05904.tex", "nexttext": "\n\twhere $R_{\\min } \\mbox{ }$ or $R_{\\max } $ refers to a local nearest neighbor \n\tstatistic value (in general, the $R_{\\min } \\mbox{ }$ and $R_{\\max } $ can \n\tbe set to 0.0 and 2.0, respectively).\n\t\n\t\\textbf{Step 3}: Determine the distance-decay parameter $\\alpha $ by mapping \n\tthe $\\mu _{R}$ value to a range of $\\alpha _{ }$ by a triangular \n\tmembership function that belongs to certain levels or categories of \n\tdistance-decay value; see Equation (\\ref{eq6}).\n\n", "itemtype": "equation", "pos": 15961, "prevtext": "\nwhere $S_{0 }$ is the location of an interpolated point.\n\\end{enumerate}\n\n\\textbf{Step 2}: Normalize the $R\\left( {S_0 } \\right)$ measure to $\\mu _R $ \nsuch that $\\mu _R $ is bounded by 0 and 1 by a fuzzy membership function: \n\n", "index": 9, "text": "\\begin{equation}\n\\label{eq5}\n\\mu _R =\\left\\{ {\\begin{array}{ll}\n\t0&R\\left( {S_0 } \\right)\\le \\mbox{ }R_{\\min } \\mbox{ } \\\\ \n\t0.5-0.5\\cos \\left[ {\\frac{\\pi }{R_{\\max } }\\left( {R\\left( {S_0 } \n\t\t\t\\right)-R_{\\min } } \\right)} \\right]&R_{\\min } \\le R\\left( {S_0 } \n\t\\right)\\le \\mbox{ }R_{\\max } \\mbox{ } \\\\ \n\t1&R\\left( {S_0 } \\right)\\ge \\mbox{ }R_{\\max } \\\\ \n\t\\end{array}} \\right.,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mu_{R}=\\left\\{{\\begin{array}[]{ll}0&amp;R\\left({S_{0}}\\right)\\leq\\mbox{ }R_{\\min}%&#10;\\mbox{ }\\\\&#10;0.5-0.5\\cos\\left[{\\frac{\\pi}{R_{\\max}}\\left({R\\left({S_{0}}\\right)-R_{\\min}}%&#10;\\right)}\\right]&amp;R_{\\min}\\leq R\\left({S_{0}}\\right)\\leq\\mbox{ }R_{\\max}\\mbox{ }%&#10;\\\\&#10;1&amp;R\\left({S_{0}}\\right)\\geq\\mbox{ }R_{\\max}\\\\&#10;\\end{array}}\\right.,\" display=\"block\"><mrow><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mtext>\u00a0</mtext><mo>\u2062</mo><msub><mi>R</mi><mi>min</mi></msub><mo>\u2062</mo><mtext>\u00a0</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0.5</mn><mo>-</mo><mrow><mn>0.5</mn><mo>\u2062</mo><mrow><mi>cos</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mfrac><mi>\u03c0</mi><msub><mi>R</mi><mi>max</mi></msub></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow><mo>-</mo><msub><mi>R</mi><mi>min</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><msub><mi>R</mi><mi>min</mi></msub><mo>\u2264</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mtext>\u00a0</mtext><mo>\u2062</mo><msub><mi>R</mi><mi>max</mi></msub><mo>\u2062</mo><mtext>\u00a0</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><mtext>\u00a0</mtext><mo>\u2062</mo><msub><mi>R</mi><mi>max</mi></msub></mrow></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05904.tex", "nexttext": "\n\t\t\twhere the $\\alpha _{1}$, $\\alpha _{2}$, $\\alpha _{3}$, $\\alpha \n\t\t\t_{4}$, $\\alpha _{5}$ are the assigned to be five levels or categories of \n\t\t\tdistance-decay value.\n\t\t\t\n\t\t\tAfter determining the parameter $\\alpha $, the desired prediction value of \n\t\t\teach interpolated point can be obtained via the weighting average. This \n\t\t\tstage is the same as that in the standard IDW; see Equation (\\ref{eq1}).\n\t\t\t\n\t\t\t\\subsection{$k$NN Search}\n\t\t\tThe principle and major steps of the brute-force $k$NN search are as follows \\citep{n02DBLP:conf/cvpr/GarciaDB08}:\n\t\t\t\n\t\t\tConsidering a set $R$ of $m$ reference points in a \\textit{d}-dimensional \n\t\t\tspace $R=\\{r_1 ,r_2 ,.....,r_m \\}$, and a set $Q$ of $n$ query points in the \n\t\t\tsame space $Q = \\mbox{\\{}q_1 ,q_2 , ... , q_n \\mbox{\\}}$, for a query point \n\t\t\t$q \\in Q$, the brute-force algorithm is composed of the following steps:\n\t\t\t\n\t\t\t1) Compute the distance between $q$ and the $m$ reference points of $R$:\n\t\t\t\n\t\t\t2) Sort the $m$ distances;\n\t\t\t\n\t\t\t3) Output the distances in the ordered of increasing distance.\n\t\t\t\n\t\t\tWhen applying this algorithm for the $n$ query points with considering the typical \n\t\t\tcase of large sets, the complexity of this algorithm is overwhelming:\n\t\t\t\n\t\t\t\\begin{itemize}\n\t\t\t\t\\item $O(nmd)$ multiplications for the $n\\times m$ distances computed;\n\t\t\t\t\\item $O(nm\\log m)$ is for the $n$ sorting processes.\n\t\t\t\\end{itemize}\n\t\t\t\n\t\t\tThe brute-force $k$NN search method is by nature highly parallelizable and \n\t\t\tperfectly suitable for a GPU implementation.\n\t\t\t\n\t\t\t\\section{The Improved GPU-accelerated AIDW Method}\n\t\t\tThis section will briefly introduce the considerations and strategies in the \n\t\t\tdevelopment of the improved GPU-accelerated AIDW interpolation algorithm.\n\t\t\t\n\t\t\t\\subsection{Overview and Basic Ideas}\n\t\t\tThe basic and most interesting concept behind the AIDW method is that: it \n\t\t\tattempts to determine adaptively the power parameter according to the \n\t\t\tspatial distribution pattern of each interpolated point. In AIDW algorithm, \n\t\t\tthe spatial distribution pattern is considered as the distribution density \n\t\t\tof several nearest neighboring data points locating around an interpolated \n\t\t\tpoint, which can be roughly measured by using the average distance from \n\t\t\tthose neighboring data points to the interpolated point.\n\t\t\t\n\t\t\tIn our previous work, we present a straightforward, easy-to-implement, and \n\t\t\tsuitable for GPU-parallelization algorithm to find the $k$ nearest neighboring \n\t\t\tdata points of each interpolated point. Assuming there are $n$ interpolated \n\t\t\tpoints and $m$ data points, for each interpolated point we carry out the \n\t\t\tfollowing steps \\citep{29DBLP:journals/corr/MeiXX15}:\n\t\t\t\n\t\t\tStep 1: Calculate the first $k$ distances between the first $k$ data points and \n\t\t\tthe interpolated points; \n\t\t\t\n\t\t\tStep 2: Sort the first $k$ distances in ascending order; \n\t\t\t\n\t\t\tStep 3: For each of the rest ($m-k)$ data points, \n\t\t\t\n\t\t\t1) Calculate the distance \\textit{dist};\n\t\t\t\n\t\t\t2) Compare the \\textit{dist} with the $k$th distance:\n\t\t\t\n\t\t\tif \\textit{dist} $<$ the $k$th distance, then replace the $k$th distance with the \\textit{dist}\n\t\t\t\n\t\t\t3) Iteratively compare and swap the neighboring two distances from the $k$th \n\t\t\tdistance to the first distance until all the $k$ distances are newly sorted in \n\t\t\tascending order.\n\t\t\t\n\t\t\tThe major advantage of the above algorithm is that: it is simple and easy to \n\t\t\timplement. Obviously, there is no need to utilize any complex space \n\t\t\tpartitioning data structures such as various types of \\textit{trees}. In contrast, only \n\t\t\tarrays for storing distances and coordinates are needed. Also, we find the \n\t\t\tdesired nearest neighbors without the use of explicit sorting algorithms \n\t\t\tsuch as binary search. In general, most sorting algorithms are \n\t\t\tcomputationally complex and not suitable for entirely being invoked within a \n\t\t\tsingle GPU thread. \n\t\t\t\n\t\t\tThe most obvious shortcoming of the above algorithm for finding nearest \n\t\t\tneighboring data points is that: it is computationally inefficient due to \n\t\t\tthe global search for nearest neighbors. In that algorithm, the first $k$ \n\t\t\tdistances are calculated and recorded; and then the distances to the rest \n\t\t\tpoints are calculated and then compared with those first $k$ distances. The \n\t\t\tabove procedure obviously needs a global search, which is not \n\t\t\tcomputationally optimal. One of the frequently used optimization strategies \n\t\t\tis to perform a local search by filtering those data points and distances \n\t\t\tthat are not needed to be considered. \n\t\t\t\n\t\t\tIn this work, we focus on improving our previous GPU-accelerated AIDW \n\t\t\talgorithm by using a fast $k$NN search algorithm. Our considerations and basic \n\t\t\tideas behind developing the efficient $k$NN search algorithm are as follows: \n\t\t\t\n\t\t\t(1) Create an even grid to partition the planar region that encloses the \n\t\t\tprojected positions of all data points and interpolated points;\n\t\t\t\n\t\t\t(2) Distribute all the data points and interpolated points into the grid and \n\t\t\trecord the locations;\n\t\t\t\n\t\t\t(3) Perform a \\textit{local} and fast search within the grid to find the nearest \n\t\t\tneighboring data points for each interpolated point.\n\t\t\t\n\t\t\tAfter obtaining the average distance of those neighboring data points, the \n\t\t\tadaptive power parameter $\\alpha $ will be determined according to the \n\t\t\taverage distance. Finally, the desired prediction value for each \n\t\t\tinterpolated point can be obtained via weighting average using the parameter \n\t\t\t$\\alpha $; see more descriptions in Subsection 2.2.\n\t\t\t\n\t\t\tIn summary, the improved GPU-accelerated AIDW algorithm is mainly composed of\n\t\t\ttwo stages: (1) the $k$NN search and average distances calculation, and (2) the \n\t\t\tdetermination of adaptive power parameter and prediction value by weighted \n\t\t\tinterpolating; see Figure \\ref{fig1}.\n\t\t\t\n\t\t\t\\begin{figure}[ht]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{Figure1.eps}\n\t\t\t\t\\caption{Process of the improved GPU-accelerated AIDW interpolation algorithm}\n\t\t\t\t\\label{fig1}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\t\\subsection{Stage 1: $k$NN Search}\n\t\t\tThe workflow of the stage of $k$NN search is listed in Figure \\ref{fig1}. In this \n\t\t\tsection, more descriptions on this stage will be presented.\n\t\t\t\n\t\t\t\\subsubsection{Creating an Even Grid}\n\t\t\t\n\t\t\tThe even grid is a simple type of data structure for space partitioning, \n\t\t\twhich is composed of regular cells such as squares or cubes; see an example of \n\t\t\tplanar grid illustrated in Figure \\ref{fig2}. Compared to other efficient but complex \n\t\t\tspace partitioning data structures such as the $k$-d tree, the even grid is \n\t\t\tmuch easier to create and search objects. In this work, we use a planar even \n\t\t\tgrid to partition all data points to speed up the $k$NN search via local \n\t\t\tsearch.\n\t\t\t\n\t\t\tThe building of an even planar grid is straightforward. We first calculate \n\t\t\tor specify the width of the square cell, then determine the planar \n\t\t\trectangular region for partitioning according to the minimum and maximum $x$ \n\t\t\tand $y$ coordinates of all points, i.e., obtain the length and width of the \n\t\t\trectangle. After that, the numbers of rows and columns of the grid can be \n\t\t\tquite easily determined by dividing the rectangle.\n\t\t\t\n\t\t\t\\begin{figure}[ht]\n\t\t\t\t\t\t\t\\centering\n\t\t\t\t\t\t\t\\includegraphics[width=\\linewidth]{Figure2.eps}\n\t\t\t\t\t\t\t\\caption{The creation of an even grid according to the minimum and maximum coordinates of all the data points and interpolated points}\n\t\t\t\t\t\t\t\\label{fig2}\n\t\t\t\t\t\t\\end{figure}\n\t\t\t\n\t\t\t\\subsubsection{Distributing Data Points into Cells}\n\t\t\t\n\t\t\tThe distribution of each data point is to find out that in which grid cell \n\t\t\tthe data point locates. Since each grid cell can be located and recorded \n\t\t\tusing its row and column indices, the distribution of each data point is in \n\t\t\tfact to obtained the row and column indices of the cell in which it locates. \n\t\t\t\n\t\t\tThis procedure can also be quite easily performed. First, the differences \n\t\t\tbetween the coordinates of the data points and the minimum coordinates of \n\t\t\tall cells are calculated; then the indices of row and column can be obtained \n\t\t\tby dividing the above differences with the cell width.\n\t\t\t\n\t\t\t\\subsubsection{Determining Data Points in Each Cell}\n\t\t\t\n\t\t\tThe most important and basic idea behind utilizing a space partitioning is \n\t\t\tto perform a local search within local regions rather than a global search. \n\t\t\tWhen searching nearest neighbors, it is computationally optimal to first \n\t\t\tsearch approximate nearest neighbors within several local cells and then to \n\t\t\tfind the exact nearest neighbors by filtering undesired points. \n\t\t\t\n\t\t\tSince the local search is operated within cells, it is thus needed to \n\t\t\tdetermine that which data points locate inside a specific cell. In other \n\t\t\twords, it is needed to know the number and the indices of those data points \n\t\t\tlocating in the same cell. Moreover, the layout for storing the number and \n\t\t\tindices should be carefully handled.\n\t\t\t\n\t\t\tFor each grid cell, to store the above-mentioned number and indices of those \n\t\t\tdata points locating in the same cell, in general, a dynamic array of \n\t\t\tintegers needs to be allocated. In the traditional CPU computing, the \n\t\t\tallocation and operations of dynamic arrays are easy-to-implement and \n\t\t\tcomputationally inexpensive. However, in GPU computing, it is no longer easy \n\t\t\tto implement or computationally cheap. This is because that: (1) in GPU \n\t\t\tcomputing the programming model such as CUDA cannot support the allocation \n\t\t\tand operations of dynamic arrays/containers like \\texttt{vector} and \\texttt{list} in C++ STL \n\t\t\t(Standard Template Library); and (2) the allocation of a large-enough static \n\t\t\tarray of integers, e.g., \\texttt{int index[1000]}, for storing the indices of data \n\t\t\tpoints within each GPU thread is not memory efficient. \n\t\t\t\n\t\t\tDue to the above reasons, we design an optimal layout for storing the number \n\t\t\tand indices of data points. The basic idea is that: if the indices of those \n\t\t\tpoints locating inside the same cell are stored in a continuous \n\t\t\tsegment/piece of integer values, then we only need to know the address of \n\t\t\tthe first point in the segment and the number of points in the same segment \n\t\t\t(i.e., the size of the segment).\n\t\t\t\n\t\t\tIn this case, for each cell, we can only use two integer values to record \n\t\t\tthe number and the indices of those data points that locate in the same \n\t\t\tcell. One integer is used to hold the number, and the other is used to \n\t\t\trecord the address of the head/first point in each segment. The above two \n\t\t\tvalues can be very efficiently determined in a parallel fashion.\n\t\t\t\n\t\t\t\\begin{figure}[ht]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{Figure3.eps}\n\t\t\t\t\\caption{Demonstration of determining the number of data points \n\t\t\t\t\t\t\tdistributed in each cell and the index of the head point. (a) the number of \n\t\t\t\t\t\t\tpoints; (b) the index of the head point}\n\t\t\t\t\\label{fig3}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\t\n\t\t\tBefore determining the number and indices of data points locating in the \n\t\t\tsame cell, those data points should be recorded continuously. Since we have \n\t\t\tobtained the index of the cell in which each data point locates, if we sort \n\t\t\tall data points according to their corresponding cell indices in ascending \n\t\t\torder, then those data points locating in the same cell can be gathered in a \n\t\t\tcontinuous segment. This sorting procedure is suited to be parallelized on \n\t\t\tthe GPU.\n\t\t\t\n\t\t\tThe number of data points locating in the same cell is determined using \n\t\t\t\\textit{segmented} parallel reduction. As described above, after sorting all data points \n\t\t\taccording to cell indices, all data points are stored in a group of \n\t\t\tsegments; each segment is flagged with the cell index, and contains the \n\t\t\tindices of data points locating in the same cell. The number of data points \n\t\t\tlocating in the same cell can be achieved by performing a reduction for each \n\t\t\tsegment; see Figure \\ref{fig3}(a). Similarly, the head index of the first point of \n\t\t\teach segment can be obtained using segmented parallel scan; see Figure \\ref{fig3}(b).\n\t\t\t\n\t\t\t\\subsubsection{Searching Nearest Neighbors}\n\t\t\t\n\t\t\tIn this work, a space-partitioning data structure, the even grid, is \n\t\t\temployed to enhance the $k$NN search algorithm. The most important and basic \n\t\t\tidea behind utilizing the space partitioning is to perform a local search \n\t\t\twithin local regions rather than a global search. This idea is quite \n\t\t\teffective in practice for that the number of points that are needed to find \n\t\t\tand compare can be significantly reduced, and therefore, the computational \n\t\t\tefficiency can be improved.\n\t\t\t\n\t\t\tThe process of $k$NN search for each interpolated point can be summarized as \n\t\t\tfollows.\n\t\t\t\\begin{itemize}\n\t\t\t\t\\item Step 1: Locate the interpolate point into the even grid\n\t\t\t\t\\item Step 2: Determine the level of cell expanding\n\t\t\t\t\\item Step 3: Find the nearest neighbors within the local region\n\t\t\t\t\\item Step 4: Calculate the average distance\n\t\t\t\\end{itemize}\n\t\t\t\n\t\t\tThe locating of each interpolated point into the previously created planar \n\t\t\tgrid is quite straightforward. Since each grid cell can be located and \n\t\t\trecorded using its row and column indices, the distribution of each \n\t\t\tinterpolated point is in fact to obtained the row and column indices of the \n\t\t\tcell in which it locates. First, the differences between the coordinates of \n\t\t\tthe interpolated point and the minimum coordinates of all cells are \n\t\t\tcalculated; then the indices of row and column can be obtained by dividing \n\t\t\tthe above differences with the cell width.\n\t\t\t\n\t\t\tThe determining of the level of cell expanding is in fact to determine the \n\t\t\tregion of cells in which the local nearest neighbors search should be \n\t\t\tcarried out; see three levels of cell expanding in Figure \\ref{fig2}. In $k$NN search, \n\t\t\tthe number of nearest neighbors, $k$, is typically pre-specified; and \n\t\t\tobviously, the number of data points locating in the local cells must be \n\t\t\tlarger than the number $k$. Thus, the level of cell expanding can be \n\t\t\titeratively determined by comparing the number of currently found data \n\t\t\tpoints with the number $k$. For example, when the $k$ is specified as 15, and \n\t\t\twithin the first level of local cells there are only 10 data points, and \n\t\t\tthus the level 1 needs to expand to level 2. Similarly, if only 14 data \n\t\t\tpoints can be found within the second level of local cells, the level needs \n\t\t\tto be further expanded to 3. This procedure is iteratively repeated until \n\t\t\tenough data points have been found.\n\t\t\t\n\t\t\t\\textbf{Remark:} Note that after iteratively determining the level of cell \n\t\t\texpanding, for example, level 3, the final level of cell expanding needs to \n\t\t\tincrease with 1, i.e., level 4. This is because that: without expanding \n\t\t\tadditional one level, the nearest neighbors found in the initial level of \n\t\t\tlocal cells may not the desired exact $k$ nearest neighbors; see the marked \n\t\t\tdata point in Figure \\ref{fig4}. When $k$ = 10, the determined level of cell expanding \n\t\t\tis 0 (i.e., the yellow region). However, the marked data point is obvious \n\t\t\tone of the nearest neighbors of the only interpolated point because it is \n\t\t\tmuch nearer to the interpolated point than several data points locating in \n\t\t\tthe yellow region. This demonstrates that: without expanding additional one \n\t\t\tlevel, incorrect/undesired nearest neighboring data points are probably \n\t\t\tfound; and several of the expected nearest neighboring data points may not able to be found.\n\t\t\t\n\t\t\t\\begin{figure}[ht]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=0.6\\linewidth]{Figure4.eps}\n\t\t\t\t\\caption{An example for demonstrating the failure of finding exact \n\t\t\t\t\t\t\t\tnearest data points for an interpolated point}\n\t\t\t\t\\label{fig4}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\tThe $k$NN search in the local cells is, in fact, to further find exact nearest \n\t\t\tneighbors by filtering some undesired points. We first allocate an array \n\t\t\twith the size of $k$ for storing distances, and initiate all distances to 0. \n\t\t\tThen for each of those data points locating in the local cells, we calculate \n\t\t\tthe distance \\textit{dist}, and compare the \\textit{dist} with the $k$th distance; and if \\textit{dist} is smaller \n\t\t\tthan the $k$th distance, then replace the $k$th distance with the \\textit{dist}; after that, we \n\t\t\titeratively compare and swap the neighboring two distances from the $k$th \n\t\t\tdistance to the first distance until all the $k$ distances are newly sorted in \n\t\t\tascending order; see \\cite{29DBLP:journals/corr/MeiXX15} for more details.\n\t\t\t\n\t\t\tAfter finding the nearest neighbors of each interpolated point, the \n\t\t\tdistances between each nearest neighbor and the interpolated point can be \n\t\t\tcalculated; and finally, the desired average distance can be obtained. \n\t\t\t\n\t\t\t\\subsection{Stage 2: Weighted Interpolating}\n\t\t\tDue to the inherent feature of the AIDW interpolation algorithm, it is \n\t\t\tperfect that a single GPU thread can take the responsibility to calculate \n\t\t\tthe prediction value of an interpolated point. For example, assuming there \n\t\t\tare $n$ interpolation points that are needed to be predicted their values such \n\t\t\tas elevations, and then it is required to allocate $n$ threads to calculate the \n\t\t\tdesired prediction values for all those $n$ interpolated points concurrently. \n\t\t\t\n\t\t\tIn GPU computing, shared memory is inherently much faster than global \n\t\t\tmemory; thus, any opportunity to replace global memory access by shared \n\t\t\tmemory access should therefore be utilized. Since the shared memory \n\t\t\tresiding in the GPU is limited per SM (Stream Multiprocessor), a common \n\t\t\toptimization strategy called ``tiling'' is frequetnly used to handle the \n\t\t\tabove problem, which partitions the data stored in global memory into \n\t\t\tsubsets called tiles so that each tile fits into the shared memory.\n\t\t\t\n\t\t\tThis optimization strategy ``tiling'' is also adopted to accelerate the AIDW \n\t\t\tinterpolation algorithm: the coordinates of all data points are first \n\t\t\ttransferred from global memory to shared memory; then each thread within a \n\t\t\tthread block can access the coordinates stored in shared memory \n\t\t\tconcurrently. By exploiting the ``tiling'' strategy, the global memory \n\t\t\taccess can be significantly reduced; and thus, the overall computational \n\t\t\tefficiency is expected to be improved.\n\t\t\t\n\t\t\t\\section{Implementation Details}\n\t\t\tAs introduced in the above section, the improved GPU-accelerated AIDW \n\t\t\tinterpolation algorithm is mainly composed of two stages, i.e., the $k$NN \n\t\t\tsearch stage and the weighted interpolating stage. In this section, we will \n\t\t\tdescribe some implementation details on the above two stages. \n\t\t\t\n\t\t\t\\subsection{Stage 1: $k$NN Search}\n\t\t\t\\subsubsection{Creating an Even Grid}\n\t\t\t\n\t\t\tAn even grid is composed of a group of grid cells, and in this work, each \n\t\t\tgrid cell is a square. The creation of an even grid is in fact to determine \n\t\t\tthe position of the grid, the size of the cell, and the distribution layout \n\t\t\tof the cells. In our algorithm, an even planar grid is created to cover the \n\t\t\tplanar region in which the projected positions of all data points and \n\t\t\tinterpolated points locate. \n\t\t\t\n\t\t\tWe first obtain the minimum and maximum coordinates of all the data points \n\t\t\tand interpolated points using the parallel reduction \n\t\t\t\\texttt{thrust::minmax{\\_}element()} provided by the library \\textit{Thrust} \\citep{30Bell2012359}, and \n\t\t\tcalculate the differences between those minimum and maximum coordinates in \n\t\t\t$x$- and $y$- direction. After approximately determining the planar region, we \n\t\t\tthen calculate the length of interval \\texttt{cellWidth}, i.e., the width of a square \n\t\t\tcell, according to Equation (\\ref{eq2}). After that, the number of rows and columns \n\t\t\tof grid cells can be easily calculated as follows:\n\t\t\t\n\t\t\t\\texttt{int nCol = (maxX - minX + cellWidth) / cellWidth;}\n\t\t\t\n\t\t\t\\texttt{int nRow = (maxY - minY + cellWidth) / cellWidth;}\n\t\t\t\n\t\t\t\\subsubsection{Distributing Data Points into Cells}\n\t\t\t\n\t\t\tAfter creating the even grid, the subsequent step is to distribute all the \n\t\t\tdata points into the grid. This procedure can be naturally parallelized \n\t\t\tsince the distributing of each data point can be performed independently. \n\t\t\tAssuming there are $m$ data points, we allocate $m$ GPU threads to distribute all \n\t\t\tthe data points. Each thread is responsible for calculating the position of \n\t\t\tone data point locating in the grid, i.e., to determine the index of the \n\t\t\tcell where the data point locates. This can be very easily achieved using \n\t\t\tthe following formulations.\n\t\t\t\n\t\t\t\\texttt{int col{\\_}idx = (int) (dx[i] - minX) / cellWidth;}\n\t\t\t\n\t\t\t\\texttt{int row{\\_}idx = (int) (dy[i] - minY) / cellWidth;}\n\t\t\t\n\t\t\tA cell in a grid can be exactly positioned according to the indices of row \n\t\t\tand column, i.e., \\texttt{int col{\\_}idx}, \\texttt{row{\\_}idx}. Also, the position of each \n\t\t\tgrid can be found according to its global index that can be calculated using \n\t\t\tthe simple transformation, \\texttt{global{\\_}idx = row{\\_}idx * nCol + col{\\_}idx}. \n\t\t\t\n\t\t\tThe above transformation formulation can be used to transform a \n\t\t\ttwo-dimensional index of each grid cell to a unique one-dimensional index. \n\t\t\tObviously, this transformation can be easily transformed back. The reason \n\t\t\twhy we carry out the transformation is that: first the memory requirement is \n\t\t\treduced since only one array of integers is needed to be stored, and the \n\t\t\tsecond is that sorting with using one value as the key is much faster than \n\t\t\tthat with two values as keys. \n\t\t\t\n\t\t\tTo obtain the indices and numbers of those data points locating in each \n\t\t\tcell, an effective solution is to store those data points that locate in the \n\t\t\tsame cell continuously. Then, operations on the continuous pieces of data \n\t\t\t(i.e., segments) can be very efficient; see more descriptions in the closely \n\t\t\tsubsequent section.\n\t\t\t\n\t\t\t\\subsubsection{Determining Data Points in Each Cell}\n\t\t\t\n\t\t\tIn the stage of the $k$NN search, our objective is to find $k$ nearest neighboring \n\t\t\tdata points for each interpolated point. The $k$NN search for each interpolated \n\t\t\tpoint is locally performed within several grid cells. The first requirement \n\t\t\tis to determine how many and which data points locate in each grid cell. \n\t\t\tMore specifically, we need to know the indices and the number of those data \n\t\t\tpoints locating in each grid cell. We obtain this simply by using parallel \n\t\t\treduction and scan; see our ideas illustrated in Figure \\ref{fig3}.\n\t\t\t\n\t\t\tBefore carrying out the parallel reduction and scan, those data points that \n\t\t\tlocate inside the same cell should be stored continuously. This requirement \n\t\t\tcan be fulfilled by utilizing a parallel sort with the use of the global index \n\t\t\tof cells as keys. The parallel sort is realized by using the corresponding \n\t\t\tparallel primitive provided by the powerful library \\textit{Thrust}, \n\t\t\t\\texttt{thrust::sort{\\_}by{\\_}key(keys, values)}.\n\t\t\t\n\t\t\tNote that those data points locating in the same cell are stored \n\t\t\tcontinuously, and if we know the number of data points locating in the same \n\t\t\tcell, then we only to know the first address of the first data point; and \n\t\t\teach of the rest data points can be referenced according to the address of \n\t\t\tthe first point and its local position. This idea is quite similar to the \n\t\t\treference of any value/element in an array. \n\t\t\t\n\t\t\tThen, the parallel reduction and scan are also performed by using the \n\t\t\tprimitives provided by \\textit{Thrust}. We also use the global index of cells as the keys \n\t\t\tfor \\textbf{\\textit{Segmented}} reduction and scan. The motivation why we \n\t\t\tuse the segmented reduction and scan rather than the global reduction and \n\t\t\tscan is that: in the current step we only need to operate on the data points \n\t\t\tlocating in the same cell; and those data points locating in the same cell \n\t\t\thave been stored continuously and marked using the global index of cell as \n\t\t\tflags; see Figure \\ref{fig3}.\n\t\t\t\n\t\t\tThe number of those data points locating in the same cell is obtained by \n\t\t\tusing the primitive \\texttt{thrust::reduce{\\_}by{\\_}keys()}; and the index of the \n\t\t\tfirst/head point of each segment of data points are found using \n\t\t\t\\texttt{thrust::unique{\\_}by{\\_}keys()}. As illustrated in Figure \\ref{fig3}, a helper array \n\t\t\tof constant integers is additionally used to count the number of data points \n\t\t\tstored in the same piece/segment. \n\t\t\t\n\t\t\t\\subsubsection{Searching Nearest Neighbors}\n\t\t\t\n\t\t\tThe finding of $k$ nearest neighboring data points for each interpolated points \n\t\t\tcan be inherently parallelized. Assuming there are $n$ interpolated points, and \n\t\t\twe allocate $n$ threads to search the nearest neighbors for all the \n\t\t\tinterpolated points. Each thread is invoked to find the nearest neighbors \n\t\t\tfor only one interpolated point. \n\t\t\t\n\t\t\tWithin each thread, we first distribute the interpolated point into the \n\t\t\tcreated grid by calculating its row index and column index; see lines 13 \n\t\t\t$\\sim $ 14 in Figure \\ref{fig5}. Then we determine the region of the local cells by \n\t\t\tapproximately calculating the level of expanding according to the number of \n\t\t\tdata points; see lines 16 $\\sim $ 29 in Figure \\ref{fig5}. Note that currently those \n\t\t\tdata points locating in the determined local cells are the \n\t\t\t\\textbf{\\textit{Approximate}} nearest neighbors of the interpolated points. \n\t\t\tAfter that, we further find the \\textbf{\\textit{Exact}} nearest neighbors by \n\t\t\tfiltering those approximate nearest neighbors by inserting and swapping; see \n\t\t\tlines 31 $\\sim $ 58 in Figure \\ref{fig5}. Finally, the desired average distance \n\t\t\tbetween the exact nearest neighboring data points and the target \n\t\t\tinterpolated point is calculated. \n\t\t\t\n\t\t\tA remarkable implementation detail is that: when finding the nearest \n\t\t\tneighbors according to the \\textit{Euclidean} distances between points, we do not use the real \n\t\t\tdistance value but the square value of the distance. This is because that: \n\t\t\tin GPU computing the calculation of square root is quite computationally \n\t\t\texpensive; and any choice to avoid the use of calculating square root should \n\t\t\tbe exploited. Thus, we calculate the square root in the last step of \n\t\t\tcomputing the average distance, rather in the step of searching nearest \n\t\t\tneighbors.\n\n\t\t\t\\begin{figure}[!h]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=0.91\\linewidth]{Figure5.eps}\n\t\t\t\t\\caption{A CUDA kernel of the $k$NN search}\n\t\t\t\t\\label{fig5}\n\t\t\t\\end{figure}\t\n\n\t\t\t\n\t\t\t\\subsection{Stage 2: Weighted Interpolating}\n\t\t\tThis subsection will present the details on implementing the interpolating \n\t\t\tstage in the GPU-accelerated AIDW algorithm. We implement two versions: the \n\t\t\t\\textit{naive} version and the \\textit{tiled} version, by employing the data layout Structure-of-Arrays \n\t\t\t(SoA) only. Both the naive and the tiled implementations developed in this \n\t\t\twork are the same as those corresponding implementations presented in our \n\t\t\tprevious work \\citep{29DBLP:journals/corr/MeiXX15}.\n\t\t\t\n\t\t\t\\subsubsection{Naive Version}\n\t\t\t\n\t\t\tIn this version, the global memory and registers on GPU architecture are \n\t\t\temployed without exploiting the shared memory. The input data and the output \n\t\t\tdata are stored in the global memory. Assuming that there are $m$ data points \n\t\t\tused to evaluate the prediction values for $n$ interpolation points, we \n\t\t\tallocate $n$ threads to parallelize the interpolating. \n\t\t\t\n\t\t\tThe data layout SoA is employed in this version. The coordinates of all data \n\t\t\tpoints and interpolated points are stored in the arrays \\texttt{float dx[dnum]}, \\texttt{dy[dnum]}, \\texttt{dz[dnum]}, \\texttt{ix[inum]}, \\texttt{iy[inum]}, and \\texttt{iz[inum]}. \n\t\t\t\n\t\t\tSince that after invoking the $k$NN kernel, we have obtained the average \n\t\t\tdistance, i.e., the $r_{obs} $ defined in Equation (\\ref{eq3}), thus in this stage \n\t\t\teach thread is only responsible for computing the $r_{\\exp } $ and $R\\left( \n\t\t\t{S_0 } \\right)$ according to the Equations (\\ref{eq2}) and (\\ref{eq4}). After that, the \n\t\t\t$R\\left( {S_0 } \\right)$ measure is normalized to $\\mu _R $ such that $\\mu \n\t\t\t_R $ is bounded by 0 and 1 by a fuzzy membership function; see Equation (\\ref{eq5}). \n\t\t\tFinally, the power parameter $\\alpha $ is determined by mapping the $\\mu \n\t\t\t_{R}$ values to a range of $\\alpha _{ }$ by a triangular membership \n\t\t\tfunction; see Equation (\\ref{eq6}).\n\t\t\t\n\t\t\tAfter adaptively determining the power parameter, the desired prediction \n\t\t\tvalue of each interpolated point can be achieved by weighting average. This \n\t\t\tstep of calculating the weighting average is the same as that in the \n\t\t\tstandard IDW method.\n\t\t\t\n\t\t\t\\subsubsection{Tiled Version}\n\t\t\t\n\t\t\tThe workflow of the tiled version is the same as that of the naive version. \n\t\t\tThe major difference between the two versions is that: in this version, the \n\t\t\tshared memory is exploited to improve the computational efficiency. \n\t\t\t\n\t\t\tIn the tiled version, the tile size is directly set to be identical to the \n\t\t\tblock size. Each thread within a thread block is invoked to load the \n\t\t\tcoordinates of one data point from global memory to shared memory and then \n\t\t\tcompute the distances and corresponding inverse weights to those data points \n\t\t\tstored in current shared memory. After all threads within a block finished \n\t\t\tcomputing these partial distances and weights, the next piece of data in \n\t\t\tglobal memory is loaded into shared memory and used to calculate current \n\t\t\twave of partial distances and weights. After calculating each wave of \n\t\t\tpartial distances and weights, each thread accumulates the results of all \n\t\t\tpartial weights and all weighted values into two registers. Finally, the \n\t\t\tprediction value of each interpolated point can be obtained according to the \n\t\t\tsums of all partial weights and weighted values and then written into global \n\t\t\tmemory.\n\t\t\t\n\t\t\tBy employing the strategy ``tiling'', the global memory access can be \n\t\t\tsignificantly reduced for that the coordinates of all data points are only \n\t\t\tread ($n $/ threadsPerBlock) times rather than $n$ times from global memory, where \n\t\t\t$n$ is the number of interpolated points and threadsPerBlock denotes the number \n\t\t\tof threads per block. \n\n\t\t\n\t\t\t\n\t\t\t\\section{Results and Discussion}\n\t\t\t\n\t\t\t\\subsection{Experimental Environment and Testing Data }\n\t\t\tIn this work, we focus on improving our previous GPU-accelerated AIDW \n\t\t\talgorithm by utilizing a fast $k$NN search method. We refer our previously \n\t\t\tdeveloped GPU-accelerated AIDW algorithm as the \\textit{original} algorithm, and the \n\t\t\tpresented algorithm in this work as the \\textit{improved} algorithm. \n\t\t\t\n\t\t\tTo evaluate the computational efficiency of the improved algorithm, we have \n\t\t\tcarried out five groups of experimental tests on a laptop computer. The \n\t\t\tcomputer is featured with an Intel Core i7 CPU (2.40GHz), 4.0 GB RAM memory, \n\t\t\tand a GeForce GT730M card. All the experimental tests are executed on OS \n\t\t\tWindows 7 Professional (64-bit), Visual Studio 2010, and CUDA v7.0.\n\t\t\t\n\t\t\tTwo versions of the improved GPU-accelerated AIDW, i.e., the naive version \n\t\t\tand the tiled version, are implemented using the SoA layout and evaluated on \n\t\t\tsingle precision. In contrast, the CPU version of the AIDW implementation is \n\t\t\ttested on double precision; and all results of this CPU version \n\t\t\tpresented in our previous work \\citep{29DBLP:journals/corr/MeiXX15} are directly accepted to be used as the \n\t\t\tbaseline. The efficiency of all GPU implementations is benchmarked by \n\t\t\tcomparing to the baseline results.\n\t\t\t\n\t\t\tWhen evaluating the execution time of GPU implementations, the overhead \n\t\t\tspent on transferring the input data (i.e., the coordinates of data points \n\t\t\tand interpolated points) from the host side to the device side and \n\t\t\ttransferring the results from the device side to the host side is \n\t\t\tconsidered. However, the time spent on generating the test data is not \n\t\t\tincluded. \n\t\t\t\n\t\t\tThe input of the AIDW interpolation is the coordinates of data points and \n\t\t\tinterpolated points. The efficiency of the CPU and GPU implementations may \n\t\t\tdiffer due to different sizes of input data. However, the research objective \n\t\t\tin this work is to improve our previous GPU-accelerated AIDW algorithm using \n\t\t\tfast $k$NN search; thus, we only consider a particular situation where the \n\t\t\tnumbers of interpolated points and data points are identical.\n\t\t\t\n\t\t\tAll the testing data including the data points and interpolated points are \n\t\t\trandomly generated within a square. We design five groups of sizes, i.e., \n\t\t\t10K, 50K, 100K, 500K, and 1000K, where one K represents the number of 1024 \n\t\t\t(1K = 1024). Five tests are performed by setting the numbers of both the \n\t\t\tdata points and interpolated points as the above five groups of sizes.\n\t\t\t\n\t\t\t\\subsection{Performance of the Improved GPU-accelerated AIDW Algorithm}\n\t\t\t\\subsubsection{Executing Time and Speedups}\n\t\t\t\n\t\t\tWe evaluate the computational efficiency of the improved GPU-accelerated \n\t\t\tAIDW algorithm with the use of five groups of testing data. The running time \n\t\t\tis listed in Table \\ref{tab1}. Note that, to compare with the original \n\t\t\tGPU-accelerated algorithm, we have also listed the execution time of the \n\t\t\toriginal algorithm in Table \\ref{tab1}; and these experimental results of the original algorithm are directly \n\t\t\tderived from our previous work \\citep{29DBLP:journals/corr/MeiXX15}.\n\t\t\t\n\t\t\t\\begin{table}[htbp]\n\t\t\t\t\\caption{Execution time (/ms) of CPU and GPU versions of the AIDW \n\t\t\t\t\talgorithm on single precision}\n\t\t\t\t\\begin{center}\n\t\t\t\t\t\\small\n\t\t\t\t\t\\begin{tabular}{p{50pt}p{20pt}p{20pt}p{30pt}p{30pt}p{30pt}}\n        \\toprule\n\t\t\t\t\t\t\\raisebox{-1.50ex}[0cm][0cm]{Version}& \n\t\t\t\t\t\t\\multicolumn{5}{c}{Data Size (1K = 1024)}  \\\\\n\t\t\t\t\t\t\\cline{2-6} \n\t\t\t\t\t\t& \n\t\t\t\t\t\t10K& \n\t\t\t\t\t\t50K& \n\t\t\t\t\t\t100K& \n\t\t\t\t\t\t500K& \n\t\t\t\t\t\t1000K \\\\\n\t\t\t\\midrule\n\t\t\t\t\t\tCPU/Serial& \n\t\t\t\t\t\t6791& \n\t\t\t\t\t\t168234& \n\t\t\t\t\t\t673806& \n\t\t\t\t\t\t16852984& \n\t\t\t\t\t\t67471402 \\\\\n\t\t\t\t\t\tOriginal naive version& \n\t\t\t\t\t\t65.3& \n\t\t\t\t\t\t863& \n\t\t\t\t\t\t2884& \n\t\t\t\t\t\t63599& \n\t\t\t\t\t\t250574 \\\\\n\t\t\t\t\t\tOriginal tiled version& \n\t\t\t\t\t\t61.3& \n\t\t\t\t\t\t714& \n\t\t\t\t\t\t2242& \n\t\t\t\t\t\t43843& \n\t\t\t\t\t\t168189 \\\\\n\t\t\t\t\t\tImproved naive version& \n\t\t\t\t\t\t27.9& \n\t\t\t\t\t\t400& \n\t\t\t\t\t\t1366& \n\t\t\t\t\t\t31306& \n\t\t\t\t\t\t124353 \\\\\n\t\t\t\t\t\tImproved tiled version& \n\t\t\t\t\t\t21.0& \n\t\t\t\t\t\t233& \n\t\t\t\t\t\t771& \n\t\t\t\t\t\t16797& \n\t\t\t\t\t\t66338 \\\\\n\t\t\t\\bottomrule\n\t\t\t\t\t\\end{tabular}\n\t\t\t\t\t\\label{tab1}\n\t\t\t\t\\end{center}\n\t\t\t\\end{table}\n\t\t\t\n\t\t\t\\begin{figure}[!h]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=0.9\\linewidth]{Figure6.eps}\n\t\t\t\t\\caption{Speedups of the improved and the original GPU-accelerated \n\t\t\t\t\t\t\t\tAIDW algorithms over the serial AIDW algorithm}\n\t\t\t\t\\label{fig6}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\tWe have also calculated the speedups of our improved GPU-accelerated AIDW \n\t\t\talgorithm against the corresponding serial algorithm (i.e., the CPU version \n\t\t\tlisted in Table \\ref{tab1}); see Figure \\ref{fig6}. The results indicate that: (1) the highest \n\t\t\tspeedups achieved by the naive version and the tiled version can be up to \n\t\t\t543 and 1017, respectively; and (2) the tiled version is always faster than \n\t\t\tthe naive version.\n\t\t\t\n\t\t\t\\subsubsection{Comparison of the Improved Naive Version and Tiled Version}\n\t\t\t\n\t\t\tAs observed from the experimental tests, the tiled version of the improved \n\t\t\talgorithm is about 1.33 $\\sim $ 1.87 times faster than the naive version. \n\t\t\tThis behavior is due to the reason that: the stage of interpolating in the \n\t\t\ttiled version is much more computationally efficient than that in the naive \n\t\t\tversion; see the execution time of the interpolating stage in Table \\ref{tab2}.\n\t\t\t\n\t\t\tAs described in Section 3, the improved algorithm includes both the naive \n\t\t\tversion and tiled version, which can be divided into two major stages: i.e., \n\t\t\tthe stage of $k$NN search and the stage of weighted interpolating. The first \n\t\t\tstage in the above two versions are the same, while the second stage \n\t\t\tdiffers. \n\t\t\t\n\t\t\tIn the stage of interpolating of the tiled version, the benefit of the use \n\t\t\tof shared memory is exploited, while in the naive version it is not. For \n\t\t\tthis reason, the interpolating stage in the tiled version executes about \n\t\t\t1.79 $\\sim $ 1.89 times faster than that in the naive version. Thus, the \n\t\t\tentire tiled version is more efficient than the naive version. \n\t\t\t\n\t\t\t\\begin{table}[htbp]\n\t\t\t\\caption{Execution time (/ms) of the stage of $k$NN search and the \n\t\t\t\t\t\t\tstage of weighted interpolating in the improved GPU-accelerated AIDW \n\t\t\t\t\t\t\talgorithm}\n\t\t\t\t\\begin{center}\n\t\t\t\t\t\t\t\t\t\t\\small\n\t\t\t\t\t\t\\begin{tabular}{p{90pt}p{15pt}p{15pt}p{20pt}p{20pt}p{20pt}}\n\t\t\t\t\t\t\t\\toprule\n\t\t\t\t\t\t\t\\raisebox{-1.50ex}[0cm][0cm]{Stage}& \n\t\t\t\t\t\t\t\\multicolumn{5}{c}{Data Size (1K = 1024)}  \\\\\n\t\t\t\t\t\t\t\\cline{2-6} \n\t\t\t\t\t\t\t& \n\t\t\t\t\t\t10K& \n\t\t\t\t\t\t50K& \n\t\t\t\t\t\t100K& \n\t\t\t\t\t\t500K& \n\t\t\t\t\t\t1000K \\\\\n\t\t\t\t\t\t\\midrule\n\t\t\t\t\t\t$k$NN Search \\par (Both versions)& \n\t\t\t\t\t\t12.3& \n\t\t\t\t\t\t36& \n\t\t\t\t\t\t81& \n\t\t\t\t\t\t440& \n\t\t\t\t\t\t917 \\\\\n\t\t\t\t\t\tWeighted Interpolating \\par (Improved naive version)& \n\t\t\t\t\t\t15.6& \n\t\t\t\t\t\t364& \n\t\t\t\t\t\t1286& \n\t\t\t\t\t\t30866& \n\t\t\t\t\t\t123437 \\\\\n\t\t\t\t\t\tWeighted Interpolating \\par (Improved tiled version)& \n\t\t\t\t\t\t8.7& \n\t\t\t\t\t\t197& \n\t\t\t\t\t\t691& \n\t\t\t\t\t\t16357& \n\t\t\t\t\t\t65421 \\\\\n\t\t\t\t\t\t\\bottomrule\n\t\t\t\t\t\\end{tabular}\n\t\t\t\t\t\\label{tab2}\n\t\t\t\t\\end{center}\n\t\t\t\\end{table}\n\t\t\t\n\t\t\t\\subsubsection{Workload between the Stages of \\textit{k}NN Search and Weighted Interpolating}\n\t\t\t\n\t\t\tThere are two major stages in the improved GPU-accelerated AIDW algorithm. \n\t\t\tTo understand the efficiency bottleneck for further optimizations in the \n\t\t\tfuture, we in particular record the execution time for the stages of $k$NN \n\t\t\tsearch and weighted interpolating separately; see Table \\ref{tab2}. In addition, we \n\t\t\thave also evaluated the workload percentage between the above two stages in \n\t\t\tboth the naive version and tiled version; see Figure \\ref{fig7}.\n\t\t\t\n\t\t\t\n\t\t\t\\begin{figure}[h!]\n\t\t\t\t\\centering\n\t\t\t\t\\subfigure[Naive version]{\n\t\t\t\t\t\\label{fig7a}       \n\t\t\t\t\t\t\t\t\t\\includegraphics[width=0.85\\linewidth]{Figure7a.eps}\n\t\t\t\t}\n\t\t\t\t\\hspace{1em}\n\t\t\t\t\\subfigure[Tiled version]{\n\t\t\t\t\t\\label{fig7b}       \n\t\t\t\t\t\t\t\t\t\\includegraphics[width=0.85\\linewidth]{Figure7b.eps}\n\t\t\t\t}\n\t\t\t\t\\caption{ Workload of the two stages in the improved GPU-accelerated \n\t\t\t\t\tAIDW algorithm}\n\t\t\t\t\\label{fig7}       \n\t\t\t\\end{figure}\n\t\t\t\n\n\t\t\tWe have found that: the computational cost spent in the stage of $k$NN search \n\t\t\tis much less than that in the stage of the weighted interpolating. Moreover, \n\t\t\twith the increase of the size of testing data, the weight of the running \n\t\t\ttime cost in the stage of $k$NN significantly decreases; and it even reduces to \n\t\t\tabout one percentage. This observation indicates that most overhead in both \n\t\t\tthe naive version and the tiled version is spent in the stage of weighted \n\t\t\tinterpolating rather than the $k$NN search. Therefore, further optimizations \n\t\t\tmay need to be employed to improve the efficiency of the weighted \n\t\t\tinterpolating. \n\t\t\t\n\t\t\t\\subsection{Comparison with the Original GPU-accelerated AIDW Algorithm}\n\t\t\tIn Section 5.1, we have evaluated the efficiency of the improved algorithm \n\t\t\tby comparing it with the serial AIDW algorithm, and found that our improved \n\t\t\talgorithm can achieve quite satisfied speedups. In this section, we will \n\t\t\tcompare our improved GPU-accelerated algorithm presented in this work with \n\t\t\tthe original GPU-accelerated algorithm introduced in \\citep{29DBLP:journals/corr/MeiXX15}.\n\t\t\t\n\t\t\t\n\t\t\t\\begin{figure}[!h]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width = 0.9\\linewidth]{Figure8.eps}\n\t\t\t\t\\caption{Speedups of the improved GPU-accelerated AIDW algorithm \n\t\t\t\t\tover the original algorithm for both the naive version and tiled version}\n\t\t\t\t\\label{fig8}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\tThe speedups of the improved algorithm over the original algorithm are \n\t\t\tillustrated in Figure \\ref{fig8}. The results show that the improved naive version \n\t\t\tand tiled version are at least 2.02 and 2.54 times faster than the original \n\t\t\tnaive version and tiled version, respectively. This also indicates that \n\t\t\tsignificant performance gains have been achieved by improving the original \n\t\t\talgorithm using fast $k$NN search.\n\t\t\t\n\t\t\tThe major difference between the original algorithm and the improved \n\t\t\talgorithm is the use of different $k$NN search approaches. We attempt to \n\t\t\texplain the reason why significant performance gains have been achieved by analyzing \n\t\t\tthe impact of different $k$NN search algorithm on the computational efficiency. \n\t\t\t\n\t\t\tFirst, we obtain the computational time of the $k$NN search in the original \n\t\t\talgorithm by subtracting the time spent in the stage of weighted \n\t\t\tinterpolating from the total execution time; see Table \\ref{tab3}. Note that, the \n\t\t\texecution time cost in the stage of weighted interpolating is directly \n\t\t\tderived from the improved algorithm. This is because that: (1) the weighted \n\t\t\tinterpolating in both the original algorithm and the improved algorithm is \n\t\t\tthe same; and (2) the running time of the weighted interpolating can be \n\t\t\tseparately measured in the improved algorithm, while in contrast it is \n\t\t\tunable to accurately evaluate the execution time specifically for the \n\t\t\tweighted interpolating in the original algorithm. \n\t\t\t\n\t\t\t\\begin{table}[htbp]\n\t\t\t\\caption{Execution time (/ms) of the stage of $k$NN search in the \n\t\t\t\t\t\t\toriginal and the improved GPU-accelerated AIDW algorithm}\n\t\t\t\t\\begin{center}\n\t\t\t\t\t\\small\n\t\t\t\t\t\\begin{tabular}{p{90pt}p{15pt}p{15pt}p{20pt}p{20pt}p{20pt}}\n\t\t\t\t\t\t\\toprule\n\t\t\t\t\t\t\\raisebox{-1.50ex}[0cm][0cm]{Version}& \n\t\t\t\t\t\t\\multicolumn{5}{c}{Data Size (1K = 1024)}  \\\\\n\t\t\t\t\t\t\\cline{2-6} \n\t\t\t\t\t\t& \n\t\t\t\t\t\t10K& \n\t\t\t\t\t\t50K& \n\t\t\t\t\t\t100K& \n\t\t\t\t\t\t500K& \n\t\t\t\t\t\t1000K \\\\\n\t\t\t\t\t\t\\midrule\n\t\t\t\t\t\tOriginal naive version& \n\t\t\t\t\t\t49.7& \n\t\t\t\t\t\t499& \n\t\t\t\t\t\t1598& \n\t\t\t\t\t\t32733& \n\t\t\t\t\t\t127137 \\\\\n\t\t\t\t\t\tOriginal tiled version& \n\t\t\t\t\t\t52.6& \n\t\t\t\t\t\t517& \n\t\t\t\t\t\t1551& \n\t\t\t\t\t\t27486& \n\t\t\t\t\t\t102768 \\\\\n\t\t\t\t\t\tTwo improved versions& \n\t\t\t\t\t\t12.3& \n\t\t\t\t\t\t36& \n\t\t\t\t\t\t81& \n\t\t\t\t\t\t440& \n\t\t\t\t\t\t917 \\\\\n\t\t\t\t\t\t\\bottomrule\n\t\t\t\t\t\\end{tabular}\n\t\t\t\t\t\\label{tab3}\n\t\t\t\t\\end{center}\n\t\t\t\\end{table}\n\t\t\t\n\t\t\tSecond, we calculate the percentages of the running time of the $k$NN search in \n\t\t\tthe improved algorithm over that in the original algorithm; see Figure \\ref{fig9}. We \n\t\t\thave found that: in both the naive version and the tiled version, the \n\t\t\texecution time of the $k$NN search in the improved algorithm is much less than \n\t\t\tthat in the original algorithm, for example, less than one percentage for \n\t\t\tabout one million points. This suggests that: the use of fast $k$NN search \n\t\t\tapproach can significantly improve the efficiency of the entire \n\t\t\tGPU-accelerated AIDW interpolation algorithm. \n\n\t\t\t\n\t\t\t\\begin{figure}[!h]\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width = 0.9\\linewidth]{Figure9.eps}\n\t\t\t\t\\caption{Percentages of the running time of $k$NN search in the \n\t\t\t\t\timproved algorithm over the original algorithm}\n\t\t\t\t\\label{fig9}\n\t\t\t\\end{figure}\n\t\t\t\t\t\t\n\t\t\t\\section{Conclusion}\n\t\t\tIn this work, we have presented an efficient AIDW interpolation algorithm on \n\t\t\tthe GPU by utilizing a fast $k$NN search method. The presented algorithm is \n\t\t\tcomposed of two major stages, i.e., the $k$NN search and weighted \n\t\t\tinterpolating, and is developed by improving a previous GPU-accelerated AIDW \n\t\t\talgorithm with the use of fast $k$NN search. The $k$NN search is carried out based \n\t\t\tupon an even grid, and is capable of finding exact nearest neighbors very \n\t\t\tfast for each interpolated point. We have performed five groups of \n\t\t\texperimental tests to evaluate the performance of the improved \n\t\t\tGPU-accelerated AIDW algorithm. We have found that: (1) the improved \n\t\t\talgorithm can achieve a speedup of up to 1017 over the corresponding serial \n\t\t\talgorithm for one million points; (2) the improved algorithm is at least two times faster than our \n\t\t\tpreviously developed GPU-accelerated AIDW algorithm; and (3) the utilization \n\t\t\tof fast $k$NN search can significantly improve the computational efficiency of \n\t\t\tthe entire GPU-accelerated AIDW algorithm. To benefit the community, all \n\t\t\tsource code and testing data related to the presented AIDW algorithm is \n\t\t\tpublicly available.\n\n\t\t\t\\section*{Acknowledgments}\n\t\t\tThis research was supported by the Natural Science Foundation of China \n\t\t\t(Grant No. 40602037 and 40872183), China Postdoctoral Science Foundation \n\t\t\t(2015M571081), and the Fundamental Research Funds for the Central \n\t\t\tUniversities (2652015065). The authors would like to thank the editor and \n\t\t\tthe reviewers for their contributions on the paper. \n\n\\section*{References}\n\t\t\t\n  \\bibliographystyle{elsarticle-harv} \n  \\bibliography{MG}\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 16818, "prevtext": "\n\twhere $R_{\\min } \\mbox{ }$ or $R_{\\max } $ refers to a local nearest neighbor \n\tstatistic value (in general, the $R_{\\min } \\mbox{ }$ and $R_{\\max } $ can \n\tbe set to 0.0 and 2.0, respectively).\n\t\n\t\\textbf{Step 3}: Determine the distance-decay parameter $\\alpha $ by mapping \n\tthe $\\mu _{R}$ value to a range of $\\alpha _{ }$ by a triangular \n\tmembership function that belongs to certain levels or categories of \n\tdistance-decay value; see Equation (\\ref{eq6}).\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq6}\n\\alpha \\left( {\\mu _R } \\right)=\\left\\{ {{\\begin{array}{ll}\n\t\t{\\alpha _1 } & {\\mbox{0.0}\\le \\mu _R \\le \\mbox{0.1}} \\\\\n\t\t{\\alpha _1 \\left[ {1-5\\left( {\\mu _R -0\\mbox{.}1} \\right)} \\right]+5\\alpha \n\t\t\t_2 \\left( {\\mu _R -0\\mbox{.}1} \\right)} & {\\mbox{0.1}\\le \\mu _R \\le \n\t\t\t\\mbox{0.3}} \\\\\n\t\t{5\\alpha _3 \\left( {\\mu _R -0\\mbox{.}3} \\right)+\\alpha _2 \\left[ {1-5\\left( \n\t\t\t\t{\\mu _R -0\\mbox{.}3} \\right)} \\right]} & {\\mbox{0.3}\\le \\mu _R \\le \n\t\t\t0\\mbox{.}5} \\\\\n\t\t{\\alpha _3 \\left[ {1-5\\left( {\\mu _R -0\\mbox{.5}} \\right)} \\right]+5\\alpha \n\t\t\t_4 \\left( {\\mu _R -0\\mbox{.}5} \\right)} & {\\mbox{0.5}\\le \\mu _R \\le \n\t\t\t\\mbox{0.7}} \\\\\n\t\t{5\\alpha _5 \\left( {\\mu _R -0\\mbox{.7}} \\right)+\\alpha _4 \\left[ {1-5\\left( \n\t\t\t\t{\\mu _R -0\\mbox{.7}} \\right)} \\right]} & {\\mbox{0.7}\\le \\mu _R \\le \n\t\t\t\\mbox{0.9}} \\\\\n\t\t{\\alpha _5 } & {\\mbox{0.9}\\le \\mu _R \\le \\mbox{1.0}} \\\\\n\t\t\\end{array} }} \\right.,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\alpha\\left({\\mu_{R}}\\right)=\\left\\{{{\\begin{array}[]{ll}{\\alpha_{1}}&amp;{\\mbox{0%&#10;.0}\\leq\\mu_{R}\\leq\\mbox{0.1}}\\\\&#10;{\\alpha_{1}\\left[{1-5\\left({\\mu_{R}-0\\mbox{.}1}\\right)}\\right]+5\\alpha_{2}%&#10;\\left({\\mu_{R}-0\\mbox{.}1}\\right)}&amp;{\\mbox{0.1}\\leq\\mu_{R}\\leq\\mbox{0.3}}\\\\&#10;{5\\alpha_{3}\\left({\\mu_{R}-0\\mbox{.}3}\\right)+\\alpha_{2}\\left[{1-5\\left({\\mu_{%&#10;R}-0\\mbox{.}3}\\right)}\\right]}&amp;{\\mbox{0.3}\\leq\\mu_{R}\\leq 0\\mbox{.}5}\\\\&#10;{\\alpha_{3}\\left[{1-5\\left({\\mu_{R}-0\\mbox{.5}}\\right)}\\right]+5\\alpha_{4}%&#10;\\left({\\mu_{R}-0\\mbox{.}5}\\right)}&amp;{\\mbox{0.5}\\leq\\mu_{R}\\leq\\mbox{0.7}}\\\\&#10;{5\\alpha_{5}\\left({\\mu_{R}-0\\mbox{.7}}\\right)+\\alpha_{4}\\left[{1-5\\left({\\mu_{%&#10;R}-0\\mbox{.7}}\\right)}\\right]}&amp;{\\mbox{0.7}\\leq\\mu_{R}\\leq\\mbox{0.9}}\\\\&#10;{\\alpha_{5}}&amp;{\\mbox{0.9}\\leq\\mu_{R}\\leq\\mbox{1.0}}\\\\&#10;\\end{array}}}\\right.,\" display=\"block\"><mrow><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>\u03b1</mi><mn>1</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mtext>0.0</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mtext>0.1</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>\u03b1</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>5</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>1</mn></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mn>5</mn><mo>\u2062</mo><msub><mi>\u03b1</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>1</mn></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>0.1</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mtext>0.3</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>5</mn><mo>\u2062</mo><msub><mi>\u03b1</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>3</mn></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b1</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>5</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>3</mn></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>0.3</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>5</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>\u03b1</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>5</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.5</mtext></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mn>5</mn><mo>\u2062</mo><msub><mi>\u03b1</mi><mn>4</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.</mtext><mo>\u2062</mo><mn>5</mn></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>0.5</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mtext>0.7</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>5</mn><mo>\u2062</mo><msub><mi>\u03b1</mi><mn>5</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.7</mtext></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b1</mi><mn>4</mn></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>5</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>-</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>.7</mtext></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>0.7</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mtext>0.9</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>\u03b1</mi><mn>5</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mtext>0.9</mtext><mo>\u2264</mo><msub><mi>\u03bc</mi><mi>R</mi></msub><mo>\u2264</mo><mtext>1.0</mtext></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]