[{"file": "1601.02709.tex", "nexttext": "\nThe units are in mV and ms, and the constants are as usual: $C$ is membrane\ncapacitance, $g_L$ is a leak conductance, $E_L$ is the leak reversal potential, \n$V_T$ is a spike generating threshold and ${\\Delta} T$ is the spike slope factor. Once the voltage escapes and starts to blow up, we manually stop it at $V=-30$ mV, record a spike time and reset $V^i$ to $V_r< V_T$,\nwhere the voltage is held for a refractory period $T_{ref}$. The values \nof of the constants used in Eq. (\\ref{sys}) are taken from  \\cite{FourcaudTrocme:2003p552}.\n\nThe term $I^i_{in}$ describes the total input to cell $i$. It includes synaptic inputs\n$s_i$ from other cells within the network, a background drive term $I^i_{bkgd}$ representing\ninputs from within the nervous system, \nand an external input in the form of a signal:\n\n", "itemtype": "equation", "pos": 7853, "prevtext": "\n\\title{Dynamic signal tracking in a simple V1 spiking model}\n\\author{Guillaume Lajoie\\footnote{University of Washington's Institute for Neuroengineering, Seattle, WA, glajoie@uw.edu. This research was supported in part by the Washington Research Foundation's Innovation Fund.} \\quad and \\quad \nLai-Sang Young\\footnote{Courant Institute of Mathematical Sciences, New York\nUniversity, NY 10012, lsy@cims.nyu.edu. This research was supported in part by \nthe National Science Foundation, DMS-1363161}}\n\\date{\\today}\n\n\\maketitle\n\n\n\\noindent {\\bf \\large Abstract.} This work is part of an effort to understand \nthe neural basis for our visual system's ability, or failure, to accurately track\nmoving visual signals. We consider here a ring model of spiking neurons,\nintended as a simplified computational model of a single hypercolumn of the primary visual cortex.\nSignals that consist of edges with time-varying orientations localized\nin space are considered. Our model is calibrated to produce spontaneous\nand driven firing rates roughly consistent with experiments, and our\ntwo main findings, for which we offer dynamical explanation on the level\nof neuronal interactions, are the following: (1) We have documented\nconsistent transient overshoots in signal perception following signal\nswitches due to emergent interactions of the E- and I-populations, and (2) for continuously moving signals, we have found that\naccuracy is considerably lower at reversals of orientation than when\ncontinuing in the same direction (as when the signal is a rotating\nbar). To measure\nperformance, we use two metrics, called {\\it fidelity} and {\\it reliability},\nto compare signals reconstructed by the system to the ones presented,\nand to assess trial-to-trial variability. \nWe propose that the same population mechanisms responsible for orientation selectivity also impose constraints on dynamic signal tracking that manifest in perception failures consistent with psychophysical observations.\n \n\n\\newpage\n\n\n\\section*{Introduction}\nThe human visual system is remarkable, but there are limits to its ability\nto accurately track visual stimuli, as confirmed in psychophysical experiments~\\cite{Boff:1986p2122,Pack2008189,Burr:2011p2180}.\nThis is not necessarily a liability: the degradation of our ability to perceive \ndistinct visual frames beyond a certain ``refresh rate\" is what produces \nthe illusion of continuous changes when presented with fast changing static  \nscenes, as is done in cinema, television and computer monitors~\\cite{Efron:1973p1883,Thorpe:1996p1852}. Our perception of visual signals captured by the retina is the result\nof very complex processing by the brain that begins in the primary visual cortex (V1) and \ninvolves a number of higher visual cortical areas~\\cite{Hubel1995Eye}. \nThe signal, which can be \nthought of as encoded in spike trains of neurons, is passed from region to\nregion, via pathways that both feed forward and feed back, transformed at \neach stage of processing by interactions among local neuronal populations. \nConvergence and divergence of projections between regions and \nthe dynamics of local interactions may offer important clues to why \nwe see what we see. \n\nThis paper contains a numerical study of local population activity in a group \nof V1 neurons\nin response to time-varying dynamic signals. In an attempt to strike a balance \nbetween biological realism and simplicity, we use, following~\\cite{BenYishai:1995p1669,BenYishai:1997p119}, a network\nwith a ring structure to model one hypercolumn of V1. That is to say, to focus\non the orientation selectivity of the neurons~\\cite{Hubel:1959p1933,Hubel:1962p1940}, we arrange them  around a circle, \nwith the angular position of the neuron corresponding to its most preferred \norientation. Both excitatory and inhibitory neurons are represented; this is \nimportant, for the dynamics are driven largely by the competition \nbetween these two subpopulations. Realistic \nbiophysical information such as network connectivity and timescales of interaction \nare incorporated whenever possible. \nAs all of the neurons in this network have essentially a common visual field, \nour signals are necessarily presented at that same spatial\nlocation. Thus unlike most other studies with moving stimuli (see e.g.~\\cite{Shadlen:1998p481,Movshon:1985p2070,Pack2008189}), our ``moving stimuli\"\ndo not move in space; they consist of single gratings, the orientation of which changes in time.\n\nOur goal is to study the model's response to such stimuli, in terms of how well\nit tracks the movements, or changes in orientation, of the signal. To quantify\nsystem performance, we will introduce metrics to describe the {\\it fidelity} of the system,  \ni.e., the extent to which its response reflects the true signal, and {\\it reliability},\nreferring to its trial-to-trial variability. We will study system performance as\na function of signal attributes, including its strength, the frequency of orientation\nswitches (or frame rates), the sizes of the jumps in angles\nand the regularity of the signal. \nImportantly, we will attempt to provide mechanistic explanation for system\nperformance, that is, to connect a system's response directly to the dynamical interactions \nbetween its excitatory and inhibitory subpopulations. We report wave-like activity patterns in response to changing stimuli, as was originally found in a comparable (rate) model~\\cite{BenYishai:1997p119} studying purely rotational stimuli.\n\nTwo novel findings are that\n(1) lateral excitation acting in concert with indirect suppression contributes to constrain\nthe speed at which a network can track a moving signal, and (2) in addition to time\nlags, overshoots occur almost invariably at signal switches (i.e. when there is a sudden change in signal orientation), and they are exacerbated\nunder certain conditions predictable from the underlying dynamics.\n\n\\medskip\nThe paper is organized as follows: In Section~\\ref{main_model}, we describe our model and present details of its calibration.\nIn Section~\\ref{signal_response}, we discuss signal-reconstruction procedures and metrics to assess their precision. Section~\\ref{overshoot} is dedicated to the study of signals that contain single orientation switches, where we first describe key network mechanisms in response the changing signals. Finally, in Section~\\ref{cts_signals}, we demonstrate how our network responds to continuously changing stimuli, both regularly rotating and randomly switching. We close with a discussion about the implications\nof this work and future research directions that it suggests.\n\n\n\\section{The model}\n\\label{main_model}\n\n\nOur model is designed to reproduce key dynamical features of neurons in a single\nhypercolumn of the primary visual cortex (V1) of primates, in response to visual stimuli. \nIt is not meant to be a biophysically realistic model, but is rather an attempt to capture \nonly orientation-specific responses of cortical neurons. Each neuron in our model is parametrized \nby an angle ${\\theta}$ between $0$ and $\\pi$ representing its most preferred orientation, \nas in the {\\it ring model} introduced by~\\cite{BenYishai:1995p1669} and~\\cite{BenYishai:1997p119} and studied in many\nother papers e.g.\\cite{Bressloff:2000p1671} and more recently~\\cite{Rubin:2015p1673}.\n\n\n\n\n\\subsection{Model description}\n\\label{model}\nWe use spiking point neurons with exponential integrate-and-fire dynamics (EIF)~\\cite{FourcaudTrocme:2003p552}. There are $N$ cells, $75\\%$ of which are excitatory (E) and the rest are inhibitory (I). We will discuss below separately the\ndynamics of individual neurons and network architecture. \n\n\n\\bigskip \\noindent\n{\\bf Dynamics of individual neurons}\n\\smallskip\n\nThe state of neuron $i$ is described\nby its membrane potential or voltage $V^i$, the dynamics of which are\ngoverned by the following equation:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{sys}\nC\\frac{dV^i}{dt}=-g_L(V^i-E_L)+g_L{\\Delta} T \\exp(\\frac{V^i-V_T}{{\\Delta} T})+I^i_{in}\\ .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"C\\frac{dV^{i}}{dt}=-g_{L}(V^{i}-E_{L})+g_{L}{\\Delta}T\\exp(\\frac{V^{i}-V_{T}}{{%&#10;\\Delta}T})+I^{i}_{in}\\ .\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msup><mi>V</mi><mi>i</mi></msup></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>g</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>V</mi><mi>i</mi></msup><mo>-</mo><msub><mi>E</mi><mi>L</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>g</mi><mi>L</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><mrow><msup><mi>V</mi><mi>i</mi></msup><mo>-</mo><msub><mi>V</mi><mi>T</mi></msub></mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>T</mi></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mpadded width=\"+5pt\"><msubsup><mi>I</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mi>i</mi></msubsup></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": " \nThe synaptic input current is given by\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThe units are in mV and ms, and the constants are as usual: $C$ is membrane\ncapacitance, $g_L$ is a leak conductance, $E_L$ is the leak reversal potential, \n$V_T$ is a spike generating threshold and ${\\Delta} T$ is the spike slope factor. Once the voltage escapes and starts to blow up, we manually stop it at $V=-30$ mV, record a spike time and reset $V^i$ to $V_r< V_T$,\nwhere the voltage is held for a refractory period $T_{ref}$. The values \nof of the constants used in Eq. (\\ref{sys}) are taken from  \\cite{FourcaudTrocme:2003p552}.\n\nThe term $I^i_{in}$ describes the total input to cell $i$. It includes synaptic inputs\n$s_i$ from other cells within the network, a background drive term $I^i_{bkgd}$ representing\ninputs from within the nervous system, \nand an external input in the form of a signal:\n\n", "index": 3, "text": "\\begin{equation} \\label{I}\nI^i_{in}=s_i+w^i_{ext}(a_{sig}I^i_{sig}+I^i_{bkgd})\\ .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"I^{i}_{in}=s_{i}+w^{i}_{ext}(a_{sig}I^{i}_{sig}+I^{i}_{bkgd})\\ .\" display=\"block\"><mrow><mrow><msubsup><mi>I</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>=</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mrow><msubsup><mi>w</mi><mrow><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>t</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>a</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>I</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi></mrow><mi>i</mi></msubsup></mrow><mo>+</mo><msubsup><mi>I</mi><mrow><mi>b</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>d</mi></mrow><mi>i</mi></msubsup></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nwith\n\n", "itemtype": "equation", "pos": 8930, "prevtext": " \nThe synaptic input current is given by\n\n", "index": 5, "text": "\\begin{equation}\ns_i(t)=\\sum_{j, \\, t^{spike}_j} w_{ij}S_{XY}(t-t^{spike}_j-T^{XY}_d).\n\\label{synap}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"s_{i}(t)=\\sum_{j,\\,t^{spike}_{j}}w_{ij}S_{XY}(t-t^{spike}_{j}-T^{XY}_{d}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo rspace=\"4.2pt\">,</mo><msubsup><mi>t</mi><mi>j</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></munder><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>S</mi><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>j</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup><mo>-</mo><msubsup><mi>T</mi><mi>d</mi><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nwhere $w_{ij}$ is the coupling weight from cell $j$ to cell $i$, and $\\{t^{spike}_j\\}$ are\nall the spike times of cell $j$. \nWe assume $w_{ij}$ depends only on the neuron types of $i$ and $j$.\nMore precisely, there are four values, $w_{XY}, \\ X,Y \\in \\{E, I\\}$ (Excitatory, Inhibitory).\nThese numbers are signed: $w_{EE}, w_{IE}>0$, and $w_{EI}w_{II}<0$. \nWe set $w_{ij}=0$ if neuron $j$ does not synapse on neuron $i$, \nand if it does, then $w_{ij}=w_{XY}$ if neuron $i$ is of type $X$ and neuron $j$ is\nof type $Y$. Similarly, we use $\\tau_{XY}$ to denote the synaptic time constant from neuron class $Y$ to \nneuron class $X$, and $T^{XY}_d$ the (short) delay in transmission or rise time. \nThree types of neurotransmitters are considered: fast excitatory synapses \n(corresponding to AMPA) and fast inhibitory synapses (corresponding to GABA-A)\ncharacterized by short time constants $\\tau_{XY}$ of a few ms, and \nNMDA-based excitatory synapses characterized by much longer time constants \n$\\tau_{nmda} \\sim$100 ms. Each excitatory synaptic weight $w_{XE}$ acts on both fast (AMPA) and slow (NMDA) synaptic currents which are scaled according to the fractions $\\rho^{nmda}_X\\in[0,1]$. For example, for an E-to-I synapse, the coupling weight of the slow synaptic current is $\\rho^{nmda}_I w_{IE}$ whereas for the fast current, the weight is $(1-\\rho^{nmda}_I) w_{IE}$.\n\nReturning to Eq (\\ref{I}), the term $I^i_{bkgd}$ contains both a constant mean \n$\\mu$ and a fluctuating term  taken to be white noise with variance ${\\varepsilon}^2$,\nthe latter being independent across cells. This represents input from within \nthe nervous system, both synaptic and modulatory, that we have not modeled.\nA discussion of the signal term is postponed to the next subsection.\nFinally, the pre-factor $w^i_{ext}$, drawn randomly from $[0.9,1]$ for each neuron, is \nintended to introduce heterogeneity among neurons. \n\n\n\\bigskip \\noindent\n{\\bf Network architecture} \n\n\\smallskip\nWe consider a network of $N$ neurons, $N$ being on the order of 1000.\nTo focus on their orientation preferences, we have elected to use a network\nwith a {\\it ring structure}, that is to say, all excitatory and inhibitory neurons \nare placed uniformly in a circle, each neuron being identified with an angle \n$\\theta$ to be thought of as its most preferred orientation \n(see Figure~\\ref{fig:intro} (A)). The probability that a neuron at angle $\\theta$ is\nconnected to one at $\\theta_0$ is $p_{XY}G(\\theta-\\theta_0)$ where\n$G$ is a Gaussian with SD $\\sigma_{XY}$, $p_{XY}$ representing an orientation-dependent connection probability,\nand the pre- and postsynaptic neurons are of types $Y$ and $X$ respectively. \n\n \\begin{figure*}\n\n\\includegraphics{Fig1_current}\n\\caption{ {\\bf (A)} Network connectivity. Left: example network of $N=40$ neurons arranged in a circle with position denoting orientation selectivity (${\\theta}\\in[0,\\pi]$). Right: connectivity probability distributions with respect to pre and post-synaptic type and difference of orientation preference ${\\Delta} {\\theta}$ (e.g. EI means from I to E).  \n{\\bf (B)} Example of network signal: no preferred orientation until 0.5 seconds followed by a strong signal ($a_{sig}=1$) centred at ${\\theta}=\\pi/2$ for the following half second. Colours show signal strength. \n{\\bf (C)} Raster-plots showing network spiking activity separated in E/I populations in response to signal from (B). Dots indicate spike times. Top: Excitatory spikes (red). Bottom: Inhibitory spikes (blue).\n{\\bf(D)} Mean firing rates of each neuron in the network computed over 10 second simulations. Side histograms show firing rate distributions of E (left) and I (right) populations respectively. Arrows indicate mean. Top: in absence of stimulus (spontaneous). Bottom: in the presence of stimulus as in second part of (B). Grey box shows neurons with preferred orientation ${\\theta}$ within ${\\theta}_{sig} \\pm {\\sigma}_{sig}$ used to compute side histograms. Solid curves show firing rates averaged over a 20 neuron sliding window for E (red), I (blue) and the relative difference between the two (i.e. (I-E)/E, black).\n{\\bf(E)} Mean synaptic currents $s_i$ of each neuron in the presence of a strong stimulus as in (B), averaged over 10 seconds. In black, E (positive) and I (negative) currents received by E and I neurons, respectively solid and dashed lines. Thin red and blue lines show the difference between E and I synaptic currents, for E and I neurons respectively. Thick red and blue lines show differences averaged over sliding 10 neuron window.}\n\\label{fig:intro}\n\n\\end{figure*}\n\n\nIn agreement with known experimental measurements (\\cite{Beaulieu:1992p1763,Fitzpatrick:1985p1776}), the numbers $\\sigma_{EE}=\\sigma_{IE}$ representing the extent of axonal trees\nof excitatory neurons, are larger than  $\\sigma_{EI}=\\sigma_{II}$, the\ncorresponding reach for inhibitory neurons (see Figure~\\ref{fig:intro} (A)). Also following experiments, $p_{EE}$ is taken to be $0.15$, significantly smaller than all other $p_{XY}$, which we have taken to be $0.5$~\\cite{Oswald:2009p2183,Holmgren:2003p2182}.\n\nWe stress that our networks are randomly drawn according to the connection\nprobabilities above. Once a graph corresponding to a realization of the network is\nchosen, we fix it for the duration of\nthe study. We then verify that the results we obtain are not dependent of connectivity realization. \nThe simulations we show in this paper are for $N=1000$, and we have checked\nthat qualitative features of our results persist for networks of size $N$ ranging\nfrom $\\sim 500$ to a few thousands.\n\n\n\\subsection{Model calibration} \n\\label{calibration}\nValues of all of the parameters used are given in the Appendix. They are determined as follows: We use  biophysical guidance when we can. As for the rest, which includes in particular\nthe coupling weights $w_{XY}$ and values associated with $I^i_{bkgd}$, \nthey are determined by tuning parameters to produce firing rates that are \nconsistent with experimental data (e.g.~\\cite{Ringach:2002p1807,Ringach:2002p1805,Ringach:2002p1803}). \n\n\nFigure~\\ref{fig:intro} shows various basic features of our model. The top panel\nin (D) shows\nspontaneous firing rates at 3-7 Hz for E and 10-20 Hz for I in a realization \nof our model network, consistent with experiments. \nPanel (B) shows the onset of a strong signal favoring a specific orientation \n(e.g. a grating), and Panel (C) contains rasters showing the elicited response \nin our model neurons. Observe the gamma rhythms, which are especially \napparent in the I-population. These rhythms are an entirely emergent phenomenon, and the 40-50 Hz frequency mimics observations in cortex~\\cite{Henrie:2005p1830,Gray:1989p1811}.\nThe bottom panel in (D) shows firing rates under the strong center drive in (B), \nconfirming the significantly higher firing rates\nfor neurons whose preferred orientations agree with that of the signal.\nFinally, the top and bottom black curves in Panel (E) show the total synaptic\nE- and I-currents \nreceived by individual neurons as functions of $\\theta$. One can see, for example,\nthat this is by and large a balanced network, a hallmark of cortical activity (see e.g. discussion in~\\cite{vanVreeswijk:1996p476}). \n\n\n\n\n\\section{Responses to signals}\n\\label{signal_response}\nThis section discusses some steps used to filter the response to produce a reconstructed signal, and introduces some metrics aimed at evaluating the model's ability to track a signal properly.\nThroughout, we use numerical simulations of model~\\eqref{sys} to sample statistics about its response properties to orientation signals. More precisely, we draw\nrandomly a network realization, and study its response properties to signals of\nvarious types. We then verify that our conclusions are not dependent on specific realizations. \nMore details about numerical methods can be found in the Appendix.\n\n\\subsection{Signals and their reconstruction}\n\\label{reconstruct} \nAs all of our neurons are assumed to share a localized receptive field, the stimuli\nwe use mimics one at a fixed location in visual space. We will refer to a signal \n$\\theta_{sig}(t)  \\equiv \\theta_0 \\in [0, \\pi)$ as a {\\it constant signal}; think of it\nas a drifting grating with a fixed orientation. We are primarily interested, however,\nin dynamic signals, as in sequences of edges with orientations which vary with time. The functions $\\theta_{sig}(t)$ we consider are\nmostly piecewise constant in time.\n\nAt any one instant, the impact of the signal on the network is \nrepresented by a Gaussian-shaped function $\\Phi({\\theta})$\ncentered at $\\theta_{sig}$ with $\\sigma_{sig} = \\pi/10$, adjusted by a multiplicative constant so $\\Phi(\\theta_{sig})=1$. \nThe value of $I^i_{sig}$ in Eq. (\\ref{I}) is the value of $\\Phi$ at  the angle \noccupied by neuron $i$, scaled by the signal strength pre-factor $a_{sig}$,\nwhich can be thought of roughly as {\\it contrast} though the correspondence\nis nonlinear. The Gaussian shape of $\\Phi$ models the fact \nthat neurons in an afferent layer are sensitive to a range of orientations.\nFrom the parameter tuning discussed at the end of Section~\\ref{calibration}, we have \nfound that $a_{sig} \\in [0,1]$ captures an adequate range, leading to realistic firing rates, with $a_{sig}=1$\ncorresponding to the strongest signal. Both E and I neurons are affected \nin the same way by the signal. \n\n\n\n\\bigskip \\noindent\n{\\bf Filters and ``read-out\" functions}\n\n\\smallskip\nDownstream areas in the visual system integrate information from the \nspiking activity of neurons in each layer. The precise mechanisms enabling this readout falls far outside of the scope of this model. Here, we adopt a heuristic approach, and \nconstruct what we believe to be a reasonable filter to summarize the spiking activity of the E-population in response to \na signal. Summarizing statistics are necessary to enable simple, informative \nmetrics aimed at exploring the system's ability to track time-dependent signals \nunder a range of conditions. \n\nLet $\\mathcal E \\subset \\{1,2,\\dots, N\\}$ denote the set of all indices $i$\ncorresponding to E-neurons, and let ${\\theta}_i=i\\pi/N$. \nSuppose upon the presentation of a stimulus (or simply in background) that \nexcitatory firing is comprised of the following collection of spike trains \n\n", "itemtype": "equation", "pos": 9051, "prevtext": "\nwith\n\n", "index": 7, "text": "\\begin{equation*}\nS_{XY}(t)=\n\\begin{cases}\n\\frac{1}{\\tau_{XY}}\\exp(-t/\\tau_{XY}) & ;  t \\ge 0\\\\\n0 &; t<0\\\\\n\\end{cases}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"S_{XY}(t)=\\begin{cases}\\frac{1}{\\tau_{XY}}\\exp(-t/\\tau_{XY})&amp;;t\\geq 0\\\\&#10;0&amp;;t&lt;0\\\\&#10;\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><msub><mi>\u03c4</mi><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></msub></mfrac></mstyle><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><mi>t</mi><mo>/</mo><msub><mi>\u03c4</mi><mrow><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>;</mo><mi>t</mi><mo>\u2265</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mo>;</mo><mi>t</mi><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nConsider the filter or read-out function  $R({\\theta},t)$ defined by\n\n", "itemtype": "equation", "pos": 19482, "prevtext": "\nwhere $w_{ij}$ is the coupling weight from cell $j$ to cell $i$, and $\\{t^{spike}_j\\}$ are\nall the spike times of cell $j$. \nWe assume $w_{ij}$ depends only on the neuron types of $i$ and $j$.\nMore precisely, there are four values, $w_{XY}, \\ X,Y \\in \\{E, I\\}$ (Excitatory, Inhibitory).\nThese numbers are signed: $w_{EE}, w_{IE}>0$, and $w_{EI}w_{II}<0$. \nWe set $w_{ij}=0$ if neuron $j$ does not synapse on neuron $i$, \nand if it does, then $w_{ij}=w_{XY}$ if neuron $i$ is of type $X$ and neuron $j$ is\nof type $Y$. Similarly, we use $\\tau_{XY}$ to denote the synaptic time constant from neuron class $Y$ to \nneuron class $X$, and $T^{XY}_d$ the (short) delay in transmission or rise time. \nThree types of neurotransmitters are considered: fast excitatory synapses \n(corresponding to AMPA) and fast inhibitory synapses (corresponding to GABA-A)\ncharacterized by short time constants $\\tau_{XY}$ of a few ms, and \nNMDA-based excitatory synapses characterized by much longer time constants \n$\\tau_{nmda} \\sim$100 ms. Each excitatory synaptic weight $w_{XE}$ acts on both fast (AMPA) and slow (NMDA) synaptic currents which are scaled according to the fractions $\\rho^{nmda}_X\\in[0,1]$. For example, for an E-to-I synapse, the coupling weight of the slow synaptic current is $\\rho^{nmda}_I w_{IE}$ whereas for the fast current, the weight is $(1-\\rho^{nmda}_I) w_{IE}$.\n\nReturning to Eq (\\ref{I}), the term $I^i_{bkgd}$ contains both a constant mean \n$\\mu$ and a fluctuating term  taken to be white noise with variance ${\\varepsilon}^2$,\nthe latter being independent across cells. This represents input from within \nthe nervous system, both synaptic and modulatory, that we have not modeled.\nA discussion of the signal term is postponed to the next subsection.\nFinally, the pre-factor $w^i_{ext}$, drawn randomly from $[0.9,1]$ for each neuron, is \nintended to introduce heterogeneity among neurons. \n\n\n\\bigskip \\noindent\n{\\bf Network architecture} \n\n\\smallskip\nWe consider a network of $N$ neurons, $N$ being on the order of 1000.\nTo focus on their orientation preferences, we have elected to use a network\nwith a {\\it ring structure}, that is to say, all excitatory and inhibitory neurons \nare placed uniformly in a circle, each neuron being identified with an angle \n$\\theta$ to be thought of as its most preferred orientation \n(see Figure~\\ref{fig:intro} (A)). The probability that a neuron at angle $\\theta$ is\nconnected to one at $\\theta_0$ is $p_{XY}G(\\theta-\\theta_0)$ where\n$G$ is a Gaussian with SD $\\sigma_{XY}$, $p_{XY}$ representing an orientation-dependent connection probability,\nand the pre- and postsynaptic neurons are of types $Y$ and $X$ respectively. \n\n \\begin{figure*}\n\n\\includegraphics{Fig1_current}\n\\caption{ {\\bf (A)} Network connectivity. Left: example network of $N=40$ neurons arranged in a circle with position denoting orientation selectivity (${\\theta}\\in[0,\\pi]$). Right: connectivity probability distributions with respect to pre and post-synaptic type and difference of orientation preference ${\\Delta} {\\theta}$ (e.g. EI means from I to E).  \n{\\bf (B)} Example of network signal: no preferred orientation until 0.5 seconds followed by a strong signal ($a_{sig}=1$) centred at ${\\theta}=\\pi/2$ for the following half second. Colours show signal strength. \n{\\bf (C)} Raster-plots showing network spiking activity separated in E/I populations in response to signal from (B). Dots indicate spike times. Top: Excitatory spikes (red). Bottom: Inhibitory spikes (blue).\n{\\bf(D)} Mean firing rates of each neuron in the network computed over 10 second simulations. Side histograms show firing rate distributions of E (left) and I (right) populations respectively. Arrows indicate mean. Top: in absence of stimulus (spontaneous). Bottom: in the presence of stimulus as in second part of (B). Grey box shows neurons with preferred orientation ${\\theta}$ within ${\\theta}_{sig} \\pm {\\sigma}_{sig}$ used to compute side histograms. Solid curves show firing rates averaged over a 20 neuron sliding window for E (red), I (blue) and the relative difference between the two (i.e. (I-E)/E, black).\n{\\bf(E)} Mean synaptic currents $s_i$ of each neuron in the presence of a strong stimulus as in (B), averaged over 10 seconds. In black, E (positive) and I (negative) currents received by E and I neurons, respectively solid and dashed lines. Thin red and blue lines show the difference between E and I synaptic currents, for E and I neurons respectively. Thick red and blue lines show differences averaged over sliding 10 neuron window.}\n\\label{fig:intro}\n\n\\end{figure*}\n\n\nIn agreement with known experimental measurements (\\cite{Beaulieu:1992p1763,Fitzpatrick:1985p1776}), the numbers $\\sigma_{EE}=\\sigma_{IE}$ representing the extent of axonal trees\nof excitatory neurons, are larger than  $\\sigma_{EI}=\\sigma_{II}$, the\ncorresponding reach for inhibitory neurons (see Figure~\\ref{fig:intro} (A)). Also following experiments, $p_{EE}$ is taken to be $0.15$, significantly smaller than all other $p_{XY}$, which we have taken to be $0.5$~\\cite{Oswald:2009p2183,Holmgren:2003p2182}.\n\nWe stress that our networks are randomly drawn according to the connection\nprobabilities above. Once a graph corresponding to a realization of the network is\nchosen, we fix it for the duration of\nthe study. We then verify that the results we obtain are not dependent of connectivity realization. \nThe simulations we show in this paper are for $N=1000$, and we have checked\nthat qualitative features of our results persist for networks of size $N$ ranging\nfrom $\\sim 500$ to a few thousands.\n\n\n\\subsection{Model calibration} \n\\label{calibration}\nValues of all of the parameters used are given in the Appendix. They are determined as follows: We use  biophysical guidance when we can. As for the rest, which includes in particular\nthe coupling weights $w_{XY}$ and values associated with $I^i_{bkgd}$, \nthey are determined by tuning parameters to produce firing rates that are \nconsistent with experimental data (e.g.~\\cite{Ringach:2002p1807,Ringach:2002p1805,Ringach:2002p1803}). \n\n\nFigure~\\ref{fig:intro} shows various basic features of our model. The top panel\nin (D) shows\nspontaneous firing rates at 3-7 Hz for E and 10-20 Hz for I in a realization \nof our model network, consistent with experiments. \nPanel (B) shows the onset of a strong signal favoring a specific orientation \n(e.g. a grating), and Panel (C) contains rasters showing the elicited response \nin our model neurons. Observe the gamma rhythms, which are especially \napparent in the I-population. These rhythms are an entirely emergent phenomenon, and the 40-50 Hz frequency mimics observations in cortex~\\cite{Henrie:2005p1830,Gray:1989p1811}.\nThe bottom panel in (D) shows firing rates under the strong center drive in (B), \nconfirming the significantly higher firing rates\nfor neurons whose preferred orientations agree with that of the signal.\nFinally, the top and bottom black curves in Panel (E) show the total synaptic\nE- and I-currents \nreceived by individual neurons as functions of $\\theta$. One can see, for example,\nthat this is by and large a balanced network, a hallmark of cortical activity (see e.g. discussion in~\\cite{vanVreeswijk:1996p476}). \n\n\n\n\n\\section{Responses to signals}\n\\label{signal_response}\nThis section discusses some steps used to filter the response to produce a reconstructed signal, and introduces some metrics aimed at evaluating the model's ability to track a signal properly.\nThroughout, we use numerical simulations of model~\\eqref{sys} to sample statistics about its response properties to orientation signals. More precisely, we draw\nrandomly a network realization, and study its response properties to signals of\nvarious types. We then verify that our conclusions are not dependent on specific realizations. \nMore details about numerical methods can be found in the Appendix.\n\n\\subsection{Signals and their reconstruction}\n\\label{reconstruct} \nAs all of our neurons are assumed to share a localized receptive field, the stimuli\nwe use mimics one at a fixed location in visual space. We will refer to a signal \n$\\theta_{sig}(t)  \\equiv \\theta_0 \\in [0, \\pi)$ as a {\\it constant signal}; think of it\nas a drifting grating with a fixed orientation. We are primarily interested, however,\nin dynamic signals, as in sequences of edges with orientations which vary with time. The functions $\\theta_{sig}(t)$ we consider are\nmostly piecewise constant in time.\n\nAt any one instant, the impact of the signal on the network is \nrepresented by a Gaussian-shaped function $\\Phi({\\theta})$\ncentered at $\\theta_{sig}$ with $\\sigma_{sig} = \\pi/10$, adjusted by a multiplicative constant so $\\Phi(\\theta_{sig})=1$. \nThe value of $I^i_{sig}$ in Eq. (\\ref{I}) is the value of $\\Phi$ at  the angle \noccupied by neuron $i$, scaled by the signal strength pre-factor $a_{sig}$,\nwhich can be thought of roughly as {\\it contrast} though the correspondence\nis nonlinear. The Gaussian shape of $\\Phi$ models the fact \nthat neurons in an afferent layer are sensitive to a range of orientations.\nFrom the parameter tuning discussed at the end of Section~\\ref{calibration}, we have \nfound that $a_{sig} \\in [0,1]$ captures an adequate range, leading to realistic firing rates, with $a_{sig}=1$\ncorresponding to the strongest signal. Both E and I neurons are affected \nin the same way by the signal. \n\n\n\n\\bigskip \\noindent\n{\\bf Filters and ``read-out\" functions}\n\n\\smallskip\nDownstream areas in the visual system integrate information from the \nspiking activity of neurons in each layer. The precise mechanisms enabling this readout falls far outside of the scope of this model. Here, we adopt a heuristic approach, and \nconstruct what we believe to be a reasonable filter to summarize the spiking activity of the E-population in response to \na signal. Summarizing statistics are necessary to enable simple, informative \nmetrics aimed at exploring the system's ability to track time-dependent signals \nunder a range of conditions. \n\nLet $\\mathcal E \\subset \\{1,2,\\dots, N\\}$ denote the set of all indices $i$\ncorresponding to E-neurons, and let ${\\theta}_i=i\\pi/N$. \nSuppose upon the presentation of a stimulus (or simply in background) that \nexcitatory firing is comprised of the following collection of spike trains \n\n", "index": 9, "text": "$$\nr_{{\\theta}_i}(t)=\\sum_{t^{spike}_i} {\\delta}(t-t^{spike}_i), \\qquad i \\in \\mathcal E, \\ \\ t \\ge 0\\ .\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"r_{{\\theta}_{i}}(t)=\\sum_{t^{spike}_{i}}{\\delta}(t-t^{spike}_{i}),\\qquad i\\in%&#10;\\mathcal{E},\\ \\ t\\geq 0\\ .\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>r</mi><msub><mi>\u03b8</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msubsup><mi>t</mi><mi>i</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></munder><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><msubsup><mi>t</mi><mi>i</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>t</mi><mo>\u2265</mo><mpadded width=\"+5pt\"><mn>0</mn></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nwhere $G({\\theta})$ is a (periodic) wrapped-Gaussian filter in orientation space $[0,\\pi]$ centered at ${\\theta}=0$ with variance ${\\sigma}_G^2$, $H(t)$ is a half-gaussian causal filter in time centered at $t=0$ with variance ${\\sigma}_H^2$, with gradual decay for $t'<t$ and zero for $t'>t$. Readout filtering parameters are set to ${\\sigma}_H=40$ ms and ${\\sigma}_G=\\frac{\\pi}{10}$ radians. Temporal delays in signal integration by V1 cells have been \nreported to be in a comparable range~\\cite{Hawken:1996p2188} and\nwe verified that varying the filters' widths within reasonable ranges did not qualitatively affect our results.\n\nWhile there are many techniques developed to analyze the spiking activity of neural populations (see e.g.~\\cite{abbott1999neural,Gabbiani:2009p47}), our goal is not to derive an optimal estimate of encoded signals, but rather to mimic the information received by a down-stream population. The temporal filter $H$ represents the population's integration time-constant (see e.g.~\\cite{Ringach:1997p1916}) while the spatial filter $G$ represents the breath of orientation selective projection.\nThe function $R({\\theta},t)$ can be seen as a summary of the state of the system \nas indicated by its very recent spiking activity. More than for rate models, \na summary statistic of this kind is necessary for spiking neuron models, as \nactual spike trains are unwieldy to work with.\nWe expect that read-out functions of this type have been used in other modeling work, \nbut were unable to locate suitable references.\n\n\nExample snapshots of $R({\\theta},t)$ in response to a constant signal of three distinct \nstrengths is shown in the first row of Figure~\\ref{fig:readout}, the many\ncurves representing $R({\\theta},t)$-functions computed from different trials. \n\n \\begin{figure*}\n\\includegraphics{Fig2_current}\n\\caption{{\\bf(A, B, C)} Top row: snapshots of activity profiles on 20 distinct trials. Lines indicate trials' profiles $R({\\theta},t^*)$ at a time $t=t^*$, dots show corresponding reconstructed orientations ${\\Theta}(t^*)$. Red line shows orientation signal when present, arrows show breath of cross-trial variability. Bottom row: temporal evolution of reconstructed signals ${\\Theta}(t)$ (dots in top row) for 20 trials over a half second. (A) No signal: $a_{sig}=0$. (B) Weak signal: $a_{sig}=0.25$. (C) Strong signal: $a_{sig}=1$. \n{\\bf(D, E)} Log-scale plot of the Reliability and Fidelity of the network response to \na constant signal as a function of signal strength $a_{sig}$. Values are estimated for a one minute presentation over 50 trials for a range of signal strengths $a_{sig}$.}\n\\label{fig:readout}\n\\end{figure*}\n\n\n\n\\bigskip \\noindent\n{\\bf Mean vector strengths}\n\n\\smallskip\nFor an even more compact statistic,\nthe estimator \n\n", "itemtype": "equation", "pos": 19659, "prevtext": "\nConsider the filter or read-out function  $R({\\theta},t)$ defined by\n\n", "index": 11, "text": "\\begin{equation}\n\\label{readout}\nR({\\theta},t)=\\sum_{{\\theta}_i=0}^\\pi G({\\theta}-{\\theta}_i)\\int_{-\\infty}^tH(t')r_{{\\theta}_i}(t')dt'\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"R({\\theta},t)=\\sum_{{\\theta}_{i}=0}^{\\pi}G({\\theta}-{\\theta}_{i})\\int_{-\\infty%&#10;}^{t}H(t^{\\prime})r_{{\\theta}_{i}}(t^{\\prime})dt^{\\prime}\" display=\"block\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\u03b8</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><mi>\u03c0</mi></munderover><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b8</mi><mo>-</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mi>t</mi></msubsup><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>r</mi><msub><mi>\u03b8</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msup><mi>t</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\ncalled the {\\it mean vector strength}, represents the weighted mean orientation \nwith respect to population activity~\\cite{Georgopoulos:1986p820,Salinas:1994p1840}. We treat this quantity as the reconstructed signal by the network. Its simplicity as a single scalar makes it a natural tool for exploring our network's response property.\n\nDots on top of the first three panels of Figure~\\ref{fig:readout} show the reconstructed values ${\\Theta}(t)$ on several trials, for three different signal strengths. In (A), not surprisingly, the reconstructed orientation ${\\Theta}(t)$ is random and appears roughly uniformly distributed along $[0,\\pi]$ since there is no signal present. In (B) and (C) the reconstructed signals correctly reflect the\ntrue signal, but still fluctuate due to ongoing network activity and to noise, \nespecially in (B), where the signal is weak. \n\n\n\\subsection{ Fidelity and reliability}\n\nMeasures of the quality of a system's performance in signal reconstructions are needed. We work with a fixed network once it is drawn, and simulate dynamics in response to a given signal on many {\\it trials}. Distinct trials in the discussion to follow correspond to presentation \nof the same signal to the same network, while internal conditions of \nthe network at the time of signal presentation may differ. As an abstraction, \nwe assume that initial network conditions are randomly selected, and \nfluctuating background drive components during presentation are \nindependently drawn from trial to trial.\nHere, we introduce two metrics aimed at quantifying network performance across many trials. \n\nThe first is intended to measure {\\it fidelity},\nby which we mean the ability of the system (model or real brain) to track\nsignals accurately.  The definition below depends not only on the signal but also on the read-out\nfunction from which the estimator ${\\Theta}(t)$ is computed, though we will suppress\nthis and express fidelity only as a function of the signal $\\{\\theta_{sig}(t)\\}_{t\\in [0,T]}$: \n\n", "itemtype": "equation", "pos": 22585, "prevtext": "\nwhere $G({\\theta})$ is a (periodic) wrapped-Gaussian filter in orientation space $[0,\\pi]$ centered at ${\\theta}=0$ with variance ${\\sigma}_G^2$, $H(t)$ is a half-gaussian causal filter in time centered at $t=0$ with variance ${\\sigma}_H^2$, with gradual decay for $t'<t$ and zero for $t'>t$. Readout filtering parameters are set to ${\\sigma}_H=40$ ms and ${\\sigma}_G=\\frac{\\pi}{10}$ radians. Temporal delays in signal integration by V1 cells have been \nreported to be in a comparable range~\\cite{Hawken:1996p2188} and\nwe verified that varying the filters' widths within reasonable ranges did not qualitatively affect our results.\n\nWhile there are many techniques developed to analyze the spiking activity of neural populations (see e.g.~\\cite{abbott1999neural,Gabbiani:2009p47}), our goal is not to derive an optimal estimate of encoded signals, but rather to mimic the information received by a down-stream population. The temporal filter $H$ represents the population's integration time-constant (see e.g.~\\cite{Ringach:1997p1916}) while the spatial filter $G$ represents the breath of orientation selective projection.\nThe function $R({\\theta},t)$ can be seen as a summary of the state of the system \nas indicated by its very recent spiking activity. More than for rate models, \na summary statistic of this kind is necessary for spiking neuron models, as \nactual spike trains are unwieldy to work with.\nWe expect that read-out functions of this type have been used in other modeling work, \nbut were unable to locate suitable references.\n\n\nExample snapshots of $R({\\theta},t)$ in response to a constant signal of three distinct \nstrengths is shown in the first row of Figure~\\ref{fig:readout}, the many\ncurves representing $R({\\theta},t)$-functions computed from different trials. \n\n \\begin{figure*}\n\\includegraphics{Fig2_current}\n\\caption{{\\bf(A, B, C)} Top row: snapshots of activity profiles on 20 distinct trials. Lines indicate trials' profiles $R({\\theta},t^*)$ at a time $t=t^*$, dots show corresponding reconstructed orientations ${\\Theta}(t^*)$. Red line shows orientation signal when present, arrows show breath of cross-trial variability. Bottom row: temporal evolution of reconstructed signals ${\\Theta}(t)$ (dots in top row) for 20 trials over a half second. (A) No signal: $a_{sig}=0$. (B) Weak signal: $a_{sig}=0.25$. (C) Strong signal: $a_{sig}=1$. \n{\\bf(D, E)} Log-scale plot of the Reliability and Fidelity of the network response to \na constant signal as a function of signal strength $a_{sig}$. Values are estimated for a one minute presentation over 50 trials for a range of signal strengths $a_{sig}$.}\n\\label{fig:readout}\n\\end{figure*}\n\n\n\n\\bigskip \\noindent\n{\\bf Mean vector strengths}\n\n\\smallskip\nFor an even more compact statistic,\nthe estimator \n\n", "index": 13, "text": "\\begin{equation}\n\\label{reconstruct}\n\\begin{split}\n{\\Theta}(t)&=\\text{arg} \\left[\\sum_{j \\in \\mathcal E}R({\\theta}_j,t)e^{i2{\\theta}_j} \\right]\\ ,\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\Theta}(t)&amp;\\displaystyle=\\text{arg}\\left[\\sum_{j\\in%&#10;\\mathcal{E}}R({\\theta}_{j},t)e^{i2{\\theta}_{j}}\\right]\\ ,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mtext>arg</mtext><mo>\u2062</mo><mrow><mo>[</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></munder><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>j</mi></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><msub><mi>\u03b8</mi><mi>j</mi></msub></mrow></msup></mrow></mrow><mo rspace=\"7.5pt\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nwhere the average $\\langle \\cdot \\rangle$ is taken over distinct trials \nfor the same signal. $Fid = 0$ or very close to $0$ for a signal means, \naccording to this definition, that in almost every trial the mean vector strength never\ndeviates much from the true value of $\\theta_{sig}(t)$. \nIn the case of nonconstant signals a correction can be used to offset the\ndelay incorporated into the read-out function; this is discussed in Section~\\ref{cts_signals}.\n \nThe second metric we consider is {\\it reliability}, meaning trial-to-trial\nvariability in the system's response to repeated presentations of the same stimulus. \nWe define it to be\n\n", "itemtype": "equation", "pos": 24769, "prevtext": "\ncalled the {\\it mean vector strength}, represents the weighted mean orientation \nwith respect to population activity~\\cite{Georgopoulos:1986p820,Salinas:1994p1840}. We treat this quantity as the reconstructed signal by the network. Its simplicity as a single scalar makes it a natural tool for exploring our network's response property.\n\nDots on top of the first three panels of Figure~\\ref{fig:readout} show the reconstructed values ${\\Theta}(t)$ on several trials, for three different signal strengths. In (A), not surprisingly, the reconstructed orientation ${\\Theta}(t)$ is random and appears roughly uniformly distributed along $[0,\\pi]$ since there is no signal present. In (B) and (C) the reconstructed signals correctly reflect the\ntrue signal, but still fluctuate due to ongoing network activity and to noise, \nespecially in (B), where the signal is weak. \n\n\n\\subsection{ Fidelity and reliability}\n\nMeasures of the quality of a system's performance in signal reconstructions are needed. We work with a fixed network once it is drawn, and simulate dynamics in response to a given signal on many {\\it trials}. Distinct trials in the discussion to follow correspond to presentation \nof the same signal to the same network, while internal conditions of \nthe network at the time of signal presentation may differ. As an abstraction, \nwe assume that initial network conditions are randomly selected, and \nfluctuating background drive components during presentation are \nindependently drawn from trial to trial.\nHere, we introduce two metrics aimed at quantifying network performance across many trials. \n\nThe first is intended to measure {\\it fidelity},\nby which we mean the ability of the system (model or real brain) to track\nsignals accurately.  The definition below depends not only on the signal but also on the read-out\nfunction from which the estimator ${\\Theta}(t)$ is computed, though we will suppress\nthis and express fidelity only as a function of the signal $\\{\\theta_{sig}(t)\\}_{t\\in [0,T]}$: \n\n", "index": 15, "text": "$$\nFid[\\{\\theta_{sig}(t)\\}_{t\\in [0,T]}]= \\frac{1}{T} \\int_0^T \\langle |{\\Theta}(t)-{\\theta}_{sig}(t)|\\rangle\n\\ dt $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"Fid[\\{\\theta_{sig}(t)\\}_{t\\in[0,T]}]=\\frac{1}{T}\\int_{0}^{T}\\langle|{\\Theta}(t%&#10;)-{\\theta}_{sig}(t)|\\rangle\\ dt\" display=\"block\"><mrow><mrow><mi>F</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>\u03b8</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03b8</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02709.tex", "nexttext": "\nwhere the variance is taken over distinct trials with the same signal. \n \n\nThe concepts of fidelity and reliability are related in the following way: Fidelity\nmeasures how faithfully ${\\Theta}(t)$ reproduces the signal $\\theta_{sig}(t)$, \nwhereas reliability measures the variability in the ensemble of\ntrials about its own mean. A system can be reliable but have poor fidelity (meaning\nit consistently\ngives the same wrong results), whereas unreliability in general will \nresult also in poor fidelity. \n\nBoth fidelity and reliability are first and foremost properties of the network, \nthough the same system can perform better for some types of signals than \nfor others as we will discuss in Section~\\ref{cts_signals}.\nPanels (D) and (E) of Figure~\\ref{fig:readout} show the reliability and fidelity \nof our model \nin response to a constant signal as a function of signal strength $a_{sig}$. \nFor stronger signals, these numbers are quite small, confirming that our model \nperforms well for signals that are constant in time. Their performance for time-varying signals\nis the subject of Sections~\\ref{overshoot} and~\\ref{cts_signals}.\n\n\n\n\\medskip\nFidelity and reliability are well studied concepts in neuroscience, though\nwe know of no standardized definitions; see e.g.~\\cite{Tiesinga:2008p462}\n for an overview in the context of spike time, or e.g.\n ~\\cite{Faisal:2008p1563,Amarasingham:2006p1914} at the spike count level. There are also different levels of precision one can consider. \nThe version we use here is defined in terms of ${\\Theta}(t)$, a statistic that summarizes\npopulation activity; it is less refined than, e.g., spike-time reliability for individual neurons,\nwhich has been related to Lyapunov exponents of dynamical systems~\\cite{Ritt:2003p251,Lin:2009p350,Lajoie:2013p785}. In the present context, we believe that filtered activity at the level of populations is a more relevant observable.\n\n\n\n\\section{Overshooting and dynamical explanation}\n\\label{overshoot}\n\nWe are interested in testing the temporal signal-response properties of our network in an effort to better understand the limitations of orientation perception in neural networks with an architecture inspired by visual cortex. In this section, we focus on signal switches,\ni.e., at jump times for piecewise constant $\\theta_{sig}(t)$.  In Section~\\ref{switch_obs}, we report\non some findings surrounding an overshooting phenomenon that to our knowledge is new, and \nin Section~\\ref{switch_dyn}, we propose an explanation in terms of underlying dynamical interactions between the E- and I-populations.\n\n\n\\subsection{Overshooting at signal switches}\n\\label{switch_obs}\n\nOf interest here are single signal switches, by which we mean the following:\nSuppose a signal switch occurs at time $t^*$. That is to say,  for $t<t^*$, \n$\\theta_{sig}(t) \\equiv \\theta_1$ and \n$a_{sig}(t) \\equiv a_1$, whereas for $t>t^*$,  $\\theta_{sig}(t) \\equiv \\theta_2$ and \n$a_{sig}(t) \\equiv a_2$. We assume $\\theta_1 \\ne \\theta_2$, while $a_1$ may or\nmay not be equal to $a_2$.\nOur findings, which are illustrated in Figure~\\ref{fig:gap},\ncan be summarized as follows:\n\n\\begin{itemize}\n\\item Overshoots occur: If, for example, $\\theta_2>\\theta_1$, then our reconstructed signal ${\\Theta}(t)$ is $ > \\theta_2$ for a transient time period\n immediately following the switch.\n\\item  The magnitude of this overshoot depends on the relative strengths of\nthe signal before and after the switch: It is more prominent if $a_1 \\gg a_2$, i.e. \nwhen one switches from a strong to a weak signal, \nless prominent but clearly present when $a_1 \\approx 1 \\approx a_2$,\ni.e. when both signals are strong, and \nis less noticeable when $a_1 \\ll a_2$. \n\n\\item The time it takes for  ${\\Theta}(t)$ to return to the value of $\\theta_{sig}(t)$\nis on the order of 50-150 ms. It is\nroughly proportional to the magnitude of the overshoot and depends on the switch gap ${\\Delta} {\\theta}=|{\\theta}_1-{\\theta}_2|$.\n \\end{itemize}\nThese points are illustrated in the three panels of Figure~\\ref{fig:gap}(A), where\nwe see clearly that fidelity suffers the most in the $\\sim 100$ ms or so following\nsignal switches. \n\n \\begin{figure*}\n\n\\includegraphics{Fig3_current}\n\\caption{{\\bf (A)} Top: Switching signal protocol where signal keeps switching from strong side orientations ($\\pi/2 \\pm 0.8$) to center ($\\pi/2$) with progressively increasing strength. Middle: signal reconstruction ${\\Theta}(t)$ averaged over 50 trials in black with Reliability shown as a shaded area surrounding the curve. Red lines show underlying signal as in top panel. Bottom: Fidelity curve for ${\\Theta}(t)$ in middle panel computed over 50 trials. {\\bf (B)} Sequences of filtered activity profiles $R({\\theta},t)$ of 10 trials at four moments surrounding signal jumps indicated with boxes in (A). Solid coloured lines correspond to $R({\\theta},t)$ and dots show orientation estimates ${\\Theta}(t)$. {\\bf (C)} Color plot of reconstructed signal convergence time following a switch, from a strong to a target signal, as a function of gap size $\\Delta {\\theta}$ and target signal strength. Values estimated using 50 trials undergoing 30 signals jumps for each parameter pair $(a_{sig},\\Delta {\\theta})$.}\n\\label{fig:gap}\n\n\\end{figure*}\n\n\nFigure~\\ref{fig:gap} (B) offers more detail on what happens during these switches.\nIt shows four snapshots of the read-out function $R({\\theta},t)$, which describes \nnetwork-wide activity, for multiple trials. While there is variability, the\noverall trend is robust: each of the $R({\\theta},t)$ profiles evolves in a wave-like fashion\nwith time, from having a peak at $\\theta_1$ to having a peak at $\\theta_2$ following\nvery similar routes. The solid dots at the top of each box show the values of \n${\\Theta}(t)$ for the different trials; they confirm that overshoots occur\nfor all the trials in the examples shown. The right column, for which signal\nstrengths before and after the switch are comparable, show a smaller overshoot\nthan in the left column, where $a_1$ is considerably larger than $a_2$.\n\nFinally, Panel (C) in Figure~\\ref{fig:gap} shows overshoot duration as functions \nof the final signal strength $a_{sig} = a_2$ (with $a_1\\equiv 1)$ \nand switch gap $\\Delta \\theta = |\\theta_1-\\theta_2|$. \nWith regard to signal strength, there are no surprises: \nthe weaker the target signal, the longer the convergence time.\nOf note here is that at every signal strength, the system performs better \nin terms of overshoot duration for some $\\Delta \\theta$ than for others, \nwith the ``worst $\\Delta \\theta$'' occurring at $\\Delta \\theta \\approx 0.9 \\pm 0.1$ \nradian as $a_{sig} \\to 1$.\n\nTo make the overshoot easily visible, $\\Delta \\theta=0.8$  radian is used\nin Panels (A) and (B) of Figure~\\ref{fig:gap}, though the phenomenon is\nrobust for a wide range of $\\Delta \\theta$. \n\n\n\n\\subsection{Underlying dynamical mechanisms}\n\\label{switch_dyn}\n\nTo understand what goes on at signal switches, let us first examine the model's\nresponse to {\\it constant} signals, for this holds\nthe key to understanding the phenomena observed.\nWe begin by identifying a few relevant features, which we do not claim \nare novel or are exclusive to our model. We need to discuss first these mechanisms \nbecause they will be\nused to explain the overshooting phenomenon described in Section~\\ref{switch_obs}, as well as other\nphenomena to be discussed in the next section.\n\nRevisiting Figure~\\ref{fig:intro}, we observe that\nfiring rates aside, the most striking difference between the responses of \nthe E- and I-populations in Panel (C) to the signal in Panel (B) is the following:\n\n\\medskip \\noindent\n(1) {\\it The interval of I-cells with elevated spiking is considerably\nwider than the interval of E-cells.}\n\n\n\\medskip\nThis fact is corroborated in the second plot in Figure~\\ref{fig:intro} (D), which shows a considerably\nwider profile of elevated spiking for I-cells. While these are histograms of population\nspiking activity (and not tuning curves of individual neurons), the two are connected\nin a straightforward way: For a cell at distance $d$ from $\\theta_{sig}$ to have\nelevated spiking when $\\theta_{sig}$ is presented means this neuron responds\nto a signal at distance $d$ from its most preferred orientation (as a result of\nboth network and signal afferents). That is to say, its tuning curve is wide enough\nto include angles at distance $d$ from where it is peaked.\n Thus the simulation results in Panel (D) of Figure~\\ref{fig:intro}\nare in agreement with what is generally believed to be the case for tuning curves, \nnamely that E-cells are more sharply tuned, and I-cells more broadly tuned. \n\nWe remark that in our model,\nObservation (1) is an emergent phenomenon, in that our signal\naffects E- and I-cells at the same $\\theta$ in identical ways. \nAs to why the interval of elevated spiking for I-cells is broader, we conjecture \nthat this is mostly a consequence of the fact that\nthe differential between driven and spontaneous firing rates is larger for I-cells\nthan for E-cells, and the taller peak for I-cells takes a larger $\\theta$-interval to \nreturn to baseline values; a very steep drop in I-firing rates, i.e., nearby I-neurons receiving\nvery similar inputs but having vastly different firing rates, seems counter-intuitive.\nAnother important point is that once the I-cells have an advantage,\nE-cells tend to be further suppressed.\nThese factors are consistent with existing explanations for the sharpening of\norientation-selectivity of E-cells in cortex. See e.g. \\cite{Priebe:2008p1675} for a review on the subject.\n\n\\medskip \\noindent\n(2)  {\\it Population activity in response to a constant signal has \na ``Mexican-hat\" profile, with the largest dips in activity occurring, for the connectivity\nprofiles used in our model, at $\\sim 0.9 \\pm0.1$ radians from $\\theta_{sig}$.}\n\n\\medskip\nThis is evident in a few of the figures shown. In Figure~\\ref{fig:intro} (D), the black curve, \nwhich represents (I-fr -- E-fr)/E-fr ({\\it fr} meaning firing rate), has two bumps on the flanks\nof $\\theta_{sig}$, suggesting that at these locations, the suppressive effect of the I-population\nis likely to have the most significant effect on the E-population. Panel (E) in\nthe same figure\nshows the same for synaptic currents: The red, resp. blue, curves in the \nmiddle represent (E-synaptic current - I-synaptic current) into E, resp. I, \ncells. Notice that the red curve again has two\nvalleys on the flanks of $\\theta_{sig}$, where it in fact dips below zero\n(this does not mean E-cells here cannot fire; \nexternal and background drives, which are identical for E and for I, are not included in these graphs). Similar profiles can be seen in the filtered network responses in Figure~\\ref{fig:readout} (B,C).\n\n``Mexican-hat\" profiles have been observed experimentally (see e.g. \\cite{Priebe:2008p1675}). In theoretical studies they are sometimes assumed for tuning curves \nof individual neurons \\cite{Series:2004p575,Spiridon:2001p715,Laing:2001p1731} \nand sometimes appear as an emergent phenomenon (see e.g. \\cite{Somers:1995p1672,\nKang:2003p1667}). In our model, it is entirely emergent, occurring as a result of the dynamical interaction between the E- and I-populations for reasons similar \nto those given for Observation (1).\n\nFinally, we wish to mention one other model feature not discussed thus far: When an E-cell\nspikes, the neurotransmitters are of two different kinds: AMPA (fast) and NMDA (slow);\nsee Section~\\ref{model}. We have assumed in the model, as is generally believed to be the case in the real brain~\\cite{Lisman:1998p2184,Koulakov:2002p2187,Grunze:1996p2185}, that\n\n\\medskip \\noindent\n(3)  {\\it the NMDA-component in E-synapses is larger for postsynaptic\nI-cells (0.5) than for postsynaptic E-cells (0.25).}\n\n\\bigskip \\noindent\n{\\bf Proposed explanation for overshooting following signal switches}\n\n\\smallskip\nWe claim that (1)--(3) above offer at least partial explanation for the phenomena reported in\nFigure~\\ref{fig:gap}. Suppose the signal switches from $\\theta_1$ to $\\theta_2$ and,\nas in the left column in Panel (B), that $\\theta_2$ lies to the left of $\\theta_1$. \n(All analogous statements apply to the right column in this panel, which depicts\nresponses to a signal that moves in the opposite direction.) \nThe second box represents a snapshot after the signal has switched.\nHere one can see the bump in $R({\\theta},t)$ attempting to follow the signal. The overshoot\nin the third box is accompanied by lowered E-firing rates for $\\theta \\in (\\theta_2,\\theta_1)$ than on the far side of\n$\\theta_2$. We propose that the depressed firing on the interval\nbetween $\\theta_1$ and $\\theta_2$ can be explained by items (1) and (2) above:\nNotice that $\\theta_1$ and $\\theta_2$ are separated by $0.8$ radian, far enough\napart to be clearly distinguishable but not far enough so that the entire\ninterval between them lies in the region of elevated I-firing caused by  \n$\\theta_1$. After time $t^*$, this elevated I-spiking persists for some time, \ntemporarily holding back E-activity even as E-cells near $\\theta_2$ are now\nbeing strong driven by the new signal. Elevated I-spiking in fact extends to the\nfar side of $\\theta_2$, but because it decreases with distance from $\\theta_1$, \nits effect on the far side is less prominent, leading to the asymmetry in $R({\\theta},t)$\nfor a certain duration after time $t^*$.\nFinally, the longer-lasting NMDA-effects on I-cells, i.e. item (3), prolong \nthe excitation of I-neurons in this region for another $50-100$ ms.\n\nThe explanation above is also consistent with the observation that the overshoot is\nmore prominent when one switches from a strong to a weak signal (as discussed\nin the second bullet\nat the beginning of Section~\\ref{switch_obs}): The stronger I-suppression on the interval \nbetween $\\theta_1$ and $\\theta_2$ in relation to the new signal's ability to \nelevate E-spiking nearby leads naturally to a more exaggerated asymmetry \nin the $R({\\theta},t)$ profile. Conversely, when switching from a weak to a strong signal,\nthe lingering weak suppression has little effect on the new and stronger\nsignal's ability to arouse the E-population nearby. \n\nFinally, we note that the ``worst\" value of $\\Delta \\theta$ in the sense\nof overshoot duration in Panel (C) is at $\\sim$ 0.9 radians, and that coincides\nwith the locations\nof the deepest valleys observed in item (2) above. \n\n\\medskip\nWe finish by noting that the dynamical analysis above goes beyond the \novershooting phenomenon described in Section~\\ref{switch_obs}. The profiles $R({\\theta},t)$,\na few snapshots of which are shown in Figure~\\ref{fig:gap} (B), vary in a tractable\nand predictable way as a function of time. They offer much insight\ninto how the system responds to single signal orientation changes.  The dynamics of $R({\\theta},t)$ are predictable because a theoretical understanding can be deduced from\n the interaction between the E- and I-populations in the model network as we have\n discussed, together with smoothing properties of the filter.\n\n\n\n\n\n\n\\section{Continuously varying signals}\n\\label{cts_signals}\n\nHaving analyzed systematically what happens at single signal switches, we \nperform in this section some stress tests on both our model's ability to track more complex signals, and our understanding\nof the dynamics of $R({\\theta},t)$. We will\ncontinue to use piecewise constant $\\theta_{sig}(t)$, with jumps occurring at\ntime intervals $dt$, and study the fidelity of signals of different kinds \nas a function of $dt$ and signal strength $a_{sig}$. \nWe will also investigate bounds for $a_{sig}$ and $dt$ below which the system effectively fails, and to compare these bounds to those for real visual systems.\nIn the electronic media, one generally speaks about {\\it frame rates}, or {\\it refresh rates}.\nAs $t$ in this paper is in ms, the number of frames per sec is given by $1000/dt$,\ne.g. $25$ frames per sec corresponds to $dt = 40$ ms. \n\nBefore going further, there is a technical issue we wish to take care of, namely \nthere is some inherent amount of delay preventing the system from tracking\nany signal perfectly if we compare ${\\Theta}(t)$ to $\\theta_{sig}(t)$ for the same \nvalue of $t$. This delay is due in part to the rise times of E-neurons but \nthe bulk of it comes from our filter $R({\\theta},t)$. As we are not especially interested\nin this time delay, we introduce a notion of fidelity that corrects for it,\nby looking for the optimal $s$ \nwith the property that the readout ${\\Theta}(t)$ best fits the time-shifted signal \n$\\theta_{sig}(t-s)$. \nFor most signals, there appears to be an optimal time shift, usually between\n20-30 ms, as illustrated in\nthe two examples in the top panels of Figure~\\ref{fig:dyn}. This corrected version of fidelity,\ni.e., one that has incorporated into it a time delay, will\nbe used from here on. \n\n\\bigskip \\noindent\n{\\bf Regular vs random signals}\n\n\\smallskip\nWe consider the following 2 types of signals: {\\it regularly rotating}\nand {\\it random switching}, the latter intended to some degree as a model of\n``natural scenes\". For regularly rotating signals, $\\theta_{sig}$ jumps\nby a fixed amount, $\\Delta \\theta$, in a fixed direction at time intervals $dt$. \nFor all simulations presented in Figure~\\ref{fig:dyn}, $\\Delta \\theta= \\pi/10$. For randomly switching signals,\neach time the signal is refreshed, the jump can be in either direction with\nequal probability, and the magnitude of the jump $\\Delta \\theta$ is a random variable which, for definiteness\nwe fix as follows: with probability $\\frac12$, $\\Delta \\theta \\in (0, \\pi/10)$ with \nthe uniform distribution, and with probability $\\frac12$, it is drawn from a distribution\nwhose density is supported on $(\\pi/10, \\pi/2)$ and decreases linearly from\n$\\pi/10$ to $0$ at $\\pi/2$.  \nHeuristically we think of the part of the density in $(0, \\pi/10)$ as due \nto small eye or head movements of\nthe subject, and the part on $(\\pi/10, \\pi/2)$ as due to genuine changes in ``scenery\"\nin the receptive field of the neurons in question. For simplicity,\nwe will refer to these two types of signals as ``regular\" and ``random\". \n\nA motivation for these choices of signals is that our analysis\nin Section~\\ref{switch_dyn} predicts that our model will react quite differently\nto them. \n\n\n\n \\begin{figure*}\n\n\\includegraphics{Fig4_current}\n\\caption{{\\bf(A, B)} Left: Mean reconstructed angles ${\\Theta}(t)$ (computed over 30 trials of 10 seconds) in response to a strong signal ($a_{sig}=1$), composed of piecewise constant values with jumps of $\\pi/10$ every 50 ms. Signal shown in red, mean response in black, shifted mean response in dotted blue minimizing the $Fid$ (error between response and signal). Right: mean fidelity $Fid$ as a function of response temporal shift. Optimal shift is at minimum. (A) shows regular rotating signal and (B), random orientation changes as described in text.\n{\\bf (C,B)} Colour plot of optimal mean fidelity $Fid$ (after temporal shift as in (A,B)). (C) is for regular signals and (D) for random signals (see text for details).\n{\\bf (E,F)} One second examples of mean response (black dots, sampled at every 2 ms) to signals (red) for different parameters indicated by symbols in (C,D). Y-axis is $[0,\\pi]$ circle, meaning the top and bottom are identified. (E) is for regular signals and (D) for random signals.}\n\\label{fig:dyn}\n\n\\end{figure*}\n\n\n\n\\bigskip \\noindent\n{\\bf Our findings} \n\n\\smallskip\nNumerical simulations of our model's responses to these two types of signals\nfor $dt \\in [5,125]$, i.e. from 200 frames per sec to 8 frames per sec, \nand $a_{sig} \\in [0.1,1]$ are performed and the fidelity\nof the signals computed for optimal time-shifts. The results are shown\nin the two color plots in Figure~\\ref{fig:dyn} (C,D),\nand some examples are shown in Panels (E) and (F).\n\nObserve first from Panels (C,D) that\nfidelity in both cases improves with signal strength and \nlonger times between switches, with very good mean fidelity \n(of 5 degrees of less) for large $a_{sig}$ and $dt$. At the other end\nof the spectrum,  there are hard lower bounds: fidelity is poor, to the point\nthat one could say the system fails, \nfor signal strengths \nthat are below a certain threshold or for refresh rates that are too fast.\n\nSecond, for the same values of $dt$ and $a_{sig}$, regular signals perform \ndefinitively better than random signals. Our model\nis able to track with good accuracy rotating signals that turn very fast: $dt=10$\ntranslates into 10 complete rotations per sec. At comparable frame rates, the\nerror is very large for random signals.\n\nTurning to Panels (E,F), which show three examples from each of the two types\nof signals, we see that when $dt$ is too small (e.g. 10 ms), the system is not only perpetually\n``off\" by some amount, it can miss certain features of the signal entirely. As $dt$ is increased, \nmost of the errors captured by our fidelity metric \nare incurred through the rounding of corners; we think this is realistic,\nand not necessarily undesirable. \nFor large $dt$, as in the bottom panels in both (E) and (F),  \nhigh fidelity means also that the read-out correctly reflects the piecewise constant \nnature of the signal, with visible overshoots depending on circumstance. \nIn the case of the rotating signal, for example, the read-out shows\nsteps rather than a continuous motion when frame rate is too low. \nThis seems realistic as well.\n\n\n\n\n\\bigskip\n\\noindent\n{\\bf Dynamical analysis}\n\n\\smallskip\nConsider first the regularly rotating signal, as the situation there is more controlled:\n At $dt \\in (10, 25)$, the signal is\nessentially always in front of the peak of $R({\\theta},t)$ given that the time delay\ndiscussed above is $20-30$ ms. Overshoots for this range of\n$dt$ are irrelevant, and $R({\\theta},t)$ evolves in a fairly regular wave-like fashion, its front\nchasing after and following closely the signal. The phenomenon\nis consistent with that discovered in~\\cite{BenYishai:1997p119} using\na rate model with a similar ring topology. \n\nFor $dt$ too small, the wave can fall behind the signal,\nas rise times of E-neurons do not permit infinitely fast movements of the front.\nWhen the signal gets too far in front, the system starts to ``perceive\" it as coming \nfrom behind. This explains the ``short-cuts\" made by ${\\Theta}(t)$ from one cycle \nto the  next by going backwards in the $dt=5$ plot in Panel (E) of Figure~\\ref{fig:dyn} (see also~\\cite{BenYishai:1997p119}).\n\nFor larger $dt$, say $dt > 60$ ms, the dynamical picture described \nin Section~\\ref{overshoot} applies. One sees first an overshoot before the system corrects itself,\nand pauses at the correct signal orientation for a noticeable duration before\ncontinuing, as signal switches now occur significantly \nmore slowly than the time it takes the wave\nto go from one orientation to the next. Had we used larger jump sizes than\n the $\\pi/10$ radian used here, the overshoots \nwould have been even more pronounced. In any case, a step-like motion of  ${\\Theta}(t)$\nis expected.\n\n\\medskip \nTurning now to the random case, which models to some degree ``natural scenes\",\na comparison of Panels (C) and (D) shows that in terms of fidelity it scores \nless well than the rotational case for nearly all values  of $a_{sig}$ and $dt$ \nand especially for small $dt$. We identify the following as contributing factors.\n\nAt least for larger $dt$, as in the bottom two plots in Panel (F), observe that a good fraction of the error is incurred at the larger jumps, which are absent in the rotational\ncase. At these jumps, averaging effects of our read-out function contributes to \na lion's share of the rounding of corners, though network dynamics contribute\nto that also (as can be deduced from the time constant in our read-out function).\nThe rounding of corners helps smooth out abrupt changes in scenery\n(so it need not be an undesirable feature and is in fact likely to be realistic) but it does count against fidelity.\n\nWe have observed additional loss of fidelity when the signal turns around. Consider, for definiteness, the following scenario: Suppose we start with $\\theta_{sig} = \\theta_1$, and\nthe signal has stayed there long enough that spiking around $\\theta_1$ is\nsolidly elevated. Then the signal switches to $\\theta_2 > \\theta_1$ and shortly thereafter\n to $\\theta_3 < \\theta_2$. \nAs discussed in Section~\\ref{switch_dyn}, the elevated I-spiking on\na wide interval around $\\theta_1$  takes quite some time \nto resolve. If $\\theta_3$ falls on this interval, its effect on the system can be nontrivially compromised. \n\nIn more detail, if $dt$ is not too small, or if the signal remains near $\\theta_3$ for \nlong enough, the system will eventually overcome\nthe lingering suppression around $\\theta_3$ and respond to the new signal. \nThat is to say, it will track the signal, but with some additional delay (in addition\nto that caused by the filter). A few examples of this phenomenon\n can be seen in the bottom two\nplots of Panel (F), e.g. the faster tracking following\nthe signal switch at 0.3 sec and the slower one following the switch at 0.4 sec\nin the bottom plot.\n\nIf $dt$ is too small, and the signal quickly moves away from $\\theta_3$, then \nit can be missed altogether. Often, it is an untidy mix of delays and partially missed signals, many examples of which can be seen in the $dt=10$ ms \nplot in Panel (F). \n\n\\medskip\nTo summarize, $R({\\theta},t)$ moves around in a wave-like fashion following the signal.\nFor large enough $dt$, the loss of fidelity is due in large measure to the filter, which\nmodels heuristically what happens downstream. Focusing on our model network \nof V1, we observe that this early stage of processing already places limitations\non one's ability to track dynamic signals. Specifically, rise times for \nE-neurons impose an upper limit on the speed of the ``wave\", equivalently \na lower limit on $dt$, and this speed can be further\nimpeded by lingering suppression in its intended path when \nthe signal makes a rapid turn-around. \n\n\\bigskip \\noindent\n{\\bf Comparison with frame rates in movies.}\nThough we have tried to use biophysical constants in our modeling of V1,\nwe do not pretend that our filter is an accurate reflection of what goes on downstream,\nand the precise values of fidelity depend on the filter. Thus our results should be\nviewed as qualitative only. Nevertheless, the following comparison is interesting:\nTelevision or movie projectors use frame rates of about 25 frames per sec~\\cite{Efron:1973p1883,Thorpe:1996p1852}.\nIn our model, for strong random signals with $a_{sig}=1$ and $dt =40$, \nequivalently 25 frames per sec, fidelity is about 0.125, which means that \nthe reconstructed signal has an error of about 0.125 radian (or 7-8 degrees) \non average, these errors occurring mostly at the larger jumps in $\\theta_{sig}$.\nAt 70-80 frames per sec, on the other hand, fidelity is poor, and\nbelow about 10 frames per sec, one starts to perceive a visual signal as a series of still images~\\cite{Boff:1986p2122,Thorpe:1996p1852}.\nThis is not inconsistent with our everyday experience, and suggests that \nperhaps our filter is not so far off. \n\n\n\n\\section*{Discussion}\nThe content of this paper can be summarized as follows: We constructed\na simple network of spiking neurons intended to model a single hypercolumn\nof the visual cortex, and used it to study response properties to signals\nin the form of time-varying orientations. To evaluate the network's\nperformance, we introduced two metrics: fidelity, which measures the accuracy\nof the network's reconstructed signal, and reliability, which measures\ntrial-to-trial variability. While it is no surprise that there is a time lag\nin the network's response following a signal switch, we found that this is\nfollowed almost invariably by an overshoot, which can last over 100 ms\nbefore the system corrects itself. Our analysis showed that this is a simple\nconsequence of phenomena that emerge as a result of \nthe interaction between local E- and I-populations. \nWe also compared regularly switching to randomly switching signals, and\nfound that the model can handle substantially faster switches in the first,\nas can be predicted from the same dynamical analysis. As a final curiosity,\nfor random signals, frame rates that our model is able to track effectively\nare comparable to those used in movies. These and other model predictions\ncan, in principle, be tested by measuring evoked responses in the real\nvisual cortex. \n\n{\\it A conclusion of this work is that the same architecture of V1 that enables the\nsharpening of orientation response through lateral interactions of E- and I-neurons \nis also responsible for limiting the speed at which orientation signals can be tracked \nand for producing certain artifacts in our perception.} This suggests that temporal limitations on visual perception occur very early on in the visual pathway.\n\nA natural follow-up to this work is to study more general time-dependent\nsignals, such as moving stimuli. Indeed how the visual cortex computes\nobject velocities such as moving bars and moving patterns has been much\nstudied though far from understood (see e.g.~\\cite{Lorenceau:1993p2113,Born:2005p2112,Smith:2005p2073,Movshon:1985p2070}).\nHuman perception (or misperception, as in illusions) of moving stimuli have\nalso been studied in multitudes of psychophysics experiments. The literature\noffers few mechanistic explanation, however, for how the brain processes\nvisual information. For studies that involve higher cortical areas such as\nMT and beyond, this is likely out of the reach at the present time. But much\nof visual processing (of the local kind) is in fact carried out in V1 though not\ncompleted there. We propose that this initial stage of processing can be\nanalyzed on the level of neuronal interactions. A model similar to ours,\nmade more realistic and enlarged to model a wider region of visual field,\ncan be used to study early-stage processing of moving signals that have\nboth orientation and spatial components, and metrics for quantifying\nsystem performance, such as those introduced here, are clearly generalizable.\nIt is our hope that the present paper will inspire studies along these lines.\n\n\\section*{Appendix}\n\n\n\\subsection*{Numerical methods}\nSimulations were implemented using Python and\nCython programming languages.\nA standard Euler-Marayuma~\\cite{Asmussen:2007p222} numerical integration scheme\nwas used to estimate solutions of\nEquation~\\eqref{sys}, treated as a stochastic\ndifferential equation because of the term $I_{bkgd}^i$. Pseudo-random numbers used as noise increments were generated using the Mersenne Twister algorithm. A time-step of $\\Delta t=0.05$ ms was used for all simulations. We verified that finer temporal resolution did not affect our results. \n\nComputation of statistical quantities such as filtered network responses, $Fid$ and $Rel$ were implemented in the MATLAB framework.\n\n\n\\subsection*{Model parameters}\n\\label{Params}\nParameters used to simulate System~\\eqref{sys} are listed in the table below.\n\n\\small\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\n\\hline\n{\\bf Param} & {\\bf Value} & {\\bf Description} \\\\\n\\hline\\hline\n$N$ & 1000 & total number of neurons \\\\\n\\hline\n$C$ & 1 ($\\mu$ F/cm$^2$) & membrane capacitance \\\\\n\\hline\n$g_L$ & 0.1 (mS/cm$^2$)& leak conductance \\\\\n\\hline\n$E_L$ & -65.0 (mV) & leak reversal potential \\\\\n\\hline\n${\\Delta} T$ & 3.48 (mV) & spike slope factor \\\\\n\\hline\n$V_T$ & -59.9 (mV) & threshold voltage \\\\\n\\hline\n$T_{ref}$ & 1.7 (msec) & refractory period \\\\\n\\hline\n$V_r$ & -68.0 (mV) & reset voltage \\\\\n\\hline\n$\\mu$ & 0.2 & background noise mean \\\\\n\\hline\n${\\varepsilon}$ & 1.0 & background noise standard dev \\\\\n\\hline\n\\hline\n$T^{EE}_{d}$ & 3.5 (msec) & spike delay for E $\\to$ E synapses \\\\\n\\hline\n$T^{IE}_{d}$ & 2.0 (msec) & spike delay for E $\\to$ I synapses \\\\\n\\hline\n$T^{EI}_{d}$ & 2.0 (msec) & spike delay for I $\\to$ E synapses \\\\\n\\hline\n$T^{II}_{d}$ & 2.0 (msec) & spike delay for I $\\to$ I synapses \\\\\n\\hline\n\\hline\n$a_{sig}$ & from 0 to 1.0 & external orientation signal center strength \\\\\n\\hline\n${\\sigma}_{sig}$ & $\\pi$/10 & external orientation signal width \\\\\n\\hline\n\\hline\n${\\sigma}_{EE}$ & $\\pi$/6 & E to E connectivity range standard dev \\\\\n\\hline\n${\\sigma}_{IE}$ & $\\pi$/6 & E to I connectivity range standard dev \\\\\n\\hline\n${\\sigma}_{EI}$ & $\\pi$/10 & I to E connectivity range standard dev \\\\\n\\hline\n${\\sigma}_{II}$ & $\\pi$/10 & I to I connectivity range standard dev \\\\\n\\hline\n$p_{EE}$ & 0.15 & E to E connectivity probability (if in range) \\\\\n\\hline\n$p_{IE}$ & 0.5 & E to I connectivity probability (if in range) \\\\\n\\hline\n$p_{EI}$ & 0.5 & I to E connectivity probability (if in range) \\\\\n\\hline\n$p_{II}$ & 0.5 & I to I connectivity probability (if in range) \\\\\n\\hline\n\\hline\n$w_{EE}$ & 1.0  & E to E coupling \\\\\n\\hline\n$w_{IE}$ & 0.85 & E to I coupling \\\\\n\\hline\n$w_{EI}$ & -0.75 & I to E coupling \\\\\n\\hline\n$w_{II}$ & -0.85 & I to I coupling \\\\\n\\hline\n$\\rho^{nmda}_E$ & 0.25 & NMDA current fraction of EE synapses  \\\\\n\\hline\n$\\rho^{nmda}_I$ & 0.5 & NMDA current fraction of IE synapses \\\\\n\\hline\n$\\tau_{EE}$ & 2.0 (msec) & E to E synaptic time-constant \\\\\n\\hline\n$\\tau_{IE}$ & 2.0 (msec) & E to I synaptic time-constant \\\\\n\\hline\n$\\tau_{EI}$ & 7.0 (msec) & I to E synaptic time-constant \\\\\n\\hline\n$\\tau_{II}$ & 7.0 (msec) & I to I synaptic time-constant \\\\\n\\hline\n$\\tau_{E-nmda}$ & 100.0 (msec) & NMDA to E synaptic time-constant \\\\\n\\hline\n$\\tau_{I-nmda}$ & 100.0 (msec) & NMDA to I synaptic time-constant \\\\\n\\hline\n\n\\end{tabular}\n\n\\end{center}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem{abbott1999neural}\nL~Abbott and TJ~Sejnowski.\n\\newblock {\\em Neural Codes and Distributed Representations: Foundations of\n  Neural Computation}.\n\\newblock 1999.\n\n\\bibitem{Amarasingham:2006p1914}\nA~Amarasingham.\n\\newblock Spike count reliability and the poisson hypothesis.\n\\newblock {\\em Journal of Neuroscience}, 26(3):801--809, Jan 2006.\n\n\\bibitem{Asmussen:2007p222}\nS~Asmussen and P~Glynn.\n\\newblock Stochastic simulation: Algorithms and analysis.\n\\newblock {\\em Springer}, Jan 2007.\n\n\\bibitem{Beaulieu:1992p1763}\nC~Beaulieu, Z~Kisvarday, P~Somogyi, M~Cynader, and A~Cowey.\n\\newblock Quantitative distribution of gaba-immunopositive and-immunonegative\n  neurons and synapses in the monkey striate cortex (area 17).\n\\newblock {\\em Cerebral Cortex}, 2:295--309, Dec 1992.\n\n\\bibitem{BenYishai:1995p1669}\nR~Ben-Yishai, R~L Bar-Or, and H~Sompolinsky.\n\\newblock Theory of orientation tuning in visual cortex.\n\\newblock {\\em Proc Natl Acad Sci USA}, 92(9):3844--8, Apr 1995.\n\n\\bibitem{BenYishai:1997p119}\nRani Ben-Yishai, David Hansel, and Haim Sompolinsky.\n\\newblock Traveling waves and the processing of weakly tuned inputs in a\n  cortical network module.\n\\newblock {\\em Journal of computational neuroscience}, 4(1):57--77, 1997.\n\n\\bibitem{Boff:1986p2122}\nK~Boff, L~Kaufman, and J~Thomas.\n\\newblock Handbook of perception and human performance.\n\\newblock {\\em Wiley-Interscience}, (ISBN 10: 0471885444 ISBN 13:\n  9780471885443), Dec 1986.\n\n\\bibitem{Born:2005p2112}\nR.~T Born, CC~Pack, CR~Ponce, and S~Yi.\n\\newblock Temporal evolution of 2-dimensional direction signals used to guide\n  eye movements.\n\\newblock {\\em Journal of Neurophysiology}, 95(1):284--300, Sep 2005.\n\n\\bibitem{Bressloff:2000p1671}\nP~C Bressloff, N~W Bressloff, and J~D Cowan.\n\\newblock Dynamical mechanism for sharp orientation tuning in an\n  integrate-and-fire model of a cortical hypercolumn.\n\\newblock {\\em Neural Computation}, 12(11):2473--511, Oct 2000.\n\n\\bibitem{Burr:2011p2180}\nDavid Burr and Peter Thompson.\n\\newblock Motion psychophysics: 1985-2010.\n\\newblock {\\em Vision Res}, 51(13):1431--1456, Jun 2011.\n\n\\bibitem{Efron:1973p1883}\nRobert Efron.\n\\newblock Conservation of temporal information by perceptual systems.\n\\newblock {\\em Perception {\\&} Psychophysics}, 14(3):518--530, Dec 1973.\n\n\\bibitem{Faisal:2008p1563}\nA~Faisal, L~Selen, and D~Wolpert.\n\\newblock Noise in the nervous system.\n\\newblock {\\em Nature Reviews Neuroscience}, Jan 2008.\n\n\\bibitem{Fitzpatrick:1985p1776}\nD~Fitzpatrick, J~Lund, and G~Blasdel.\n\\newblock Intrinsic connections of macaque striate cortex: afferent and\n  efferent connections of lamina 4c.\n\\newblock {\\em J Neurosci}, Dec 1985.\n\n\\bibitem{FourcaudTrocme:2003p552}\nNicolas Fourcaud-Trocm{\\'e}, David Hansel, Carl van Vreeswijk, and Nicolas\n  Brunel.\n\\newblock How spike generation mechanisms determine the neuronal response to\n  fluctuating inputs.\n\\newblock {\\em Journal of Neuroscience}, 23(37):11628--40, Dec 2003.\n\n\\bibitem{Gabbiani:2009p47}\nF~Gabbiani and C~Koch.\n\\newblock Principles of spike train analysis.\n\\newblock {\\em Methods in Neural Modeling}, pages 313--360, Dec 2009.\n\n\\bibitem{Georgopoulos:1986p820}\nA~Georgopoulos, A~Schwartz, and R~Kettner.\n\\newblock Neuronal population coding of movement direction.\n\\newblock {\\em Science}, 233, Dec 1986.\n\n\\bibitem{Gray:1989p1811}\nC~Gray, P~K{\\\"o}nig, A~Engel, and W~Singer.\n\\newblock Oscillatory responses in cat visual cortex exhibit inter-columnar\n  synchronization which reflects global stimulus properties.\n\\newblock {\\em Nature}, 338:334--337, Dec 1989.\n\n\\bibitem{Grunze:1996p2185}\nH~C Grunze, D~G Rainnie, M~E Hasselmo, E~Barkai, E~F Hearn, R~W McCarley, and\n  R~W Greene.\n\\newblock Nmda-dependent modulation of ca1 local circuit inhibition.\n\\newblock {\\em J Neurosci}, 16(6):2034--43, Mar 1996.\n\n\\bibitem{Hawken:1996p2188}\nM~J Hawken, R~M Shapley, and D~H Grosof.\n\\newblock Temporal-frequency selectivity in monkey visual cortex.\n\\newblock {\\em Vis Neurosci}, 13(3):477--92, Jan 1996.\n\n\\bibitem{Henrie:2005p1830}\nJ~Henrie and R~Shapley.\n\\newblock Lfp power spectra in v1 cortex: the graded effect of stimulus\n  contrast.\n\\newblock {\\em Journal of Neurophysiology}, Jan 2005.\n\n\\bibitem{Holmgren:2003p2182}\nC~Holmgren, T~Harkany, B~Svennenfors, and Y~Zilberter.\n\\newblock Pyramidal cell communication within local networks in layer 2/3 of\n  rat neocortex.\n\\newblock {\\em The Journal of Physiology}, 551(1):139--153, Aug 2003.\n\n\\bibitem{Hubel:1959p1933}\nD~Hubel and T~Wiesel.\n\\newblock Receptive fields of single neurones in the cat's striate cortex.\n\\newblock {\\em The Journal of physiology}, 148:574--591, 1959.\n\n\\bibitem{Hubel:1962p1940}\nD~Hubel and T~Wiesel.\n\\newblock Receptive fields, binocular interaction and functional architecture\n  in the cat's visual cortex.\n\\newblock {\\em The Journal of physiology}, 160:106--154, 1962.\n\n\\bibitem{Hubel1995Eye}\nDavid~H Hubel.\n\\newblock {\\em Eye, Brain, and Vision (Scientific American Library, No 22)}.\n\\newblock 1995.\n\n\\bibitem{Kang:2003p1667}\nKukjin Kang, Michael Shelley, and Haim Sompolinsky.\n\\newblock Mexican hats and pinwheels in visual cortex.\n\\newblock {\\em Proc Natl Acad Sci USA}, 100(5):2848--53, Mar 2003.\n\n\\bibitem{Koulakov:2002p2187}\nAlexei~A Koulakov, Sridhar Raghavachari, Adam Kepecs, and John~E Lisman.\n\\newblock Model for a robust neural integrator.\n\\newblock {\\em Nature Neuroscience}, 5(8):775--782, Aug 2002.\n\n\\bibitem{Laing:2001p1731}\nC~R Laing and C~C Chow.\n\\newblock Stationary bumps in networks of spiking neurons.\n\\newblock {\\em Neural Computation}, 13(7):1473--94, Jun 2001.\n\n\\bibitem{Lajoie:2013p785}\nGuillaume Lajoie, Kevin~K Lin, and Eric Shea-Brown.\n\\newblock Chaos and reliability in balanced spiking networks with temporal\n  drive.\n\\newblock {\\em Phys. Rev. E}, 87(5):052901, May 2013.\n\n\\bibitem{Lin:2009p350}\nKevin~K Lin, Eric Shea-Brown, and Lai-Sang Young.\n\\newblock Spike-time reliability of layered neural oscillator networks.\n\\newblock {\\em J Comput Neurosci}, 27(1):135--160, Aug 2009.\n\n\\bibitem{Lisman:1998p2184}\nJ~E Lisman, J~M Fellous, and X~J Wang.\n\\newblock A role for nmda-receptor channels in working memory.\n\\newblock {\\em Nature Neuroscience}, 1(4):273--5, Jul 1998.\n\n\\bibitem{Lorenceau:1993p2113}\nJ~Lorenceau, M~Shiffrar, N~Wells, and E~Castet.\n\\newblock Different motion sensitive units are involved in recovering the\n  direction of moving lines.\n\\newblock {\\em Vision Res}, Dec 1993.\n\n\\bibitem{Movshon:1985p2070}\nJA~Movshon, EH~Adelson, MS~Gizzi, and WT~Newsome.\n\\newblock The analysis of moving visual patterns.\n\\newblock {\\em Pontificiae Academiae Scientiarum Scripta Varia}, 54:117--151,\n  Apr 1985.\n\n\\bibitem{Oswald:2009p2183}\nA.-M.~M Oswald, B~Doiron, J~Rinzel, and A.~D Reyes.\n\\newblock Spatial profile and differential recruitment of gabab modulate\n  oscillatory activity in auditory cortex.\n\\newblock {\\em Journal of Neuroscience}, 29(33):10321--10334, Aug 2009.\n\n\\bibitem{Pack2008189}\nCC~Pack and RT~Born.\n\\newblock Cortical mechanisms for the integration of visual motion - (chapter\n  2.11 in) the senses: A comprehensive reference.\n\\newblock pages 189 -- 218. 2008.\n\n\\bibitem{Priebe:2008p1675}\nN~Priebe and D~Ferster.\n\\newblock Inhibition, spike threshold, and stimulus selectivity in primary\n  visual cortex.\n\\newblock {\\em Neuron}, Dec 2008.\n\n\\bibitem{Ringach:2002p1803}\nD~Ringach.\n\\newblock Spatial structure and symmetry of simple-cell receptive fields in\n  macaque primary visual cortex.\n\\newblock {\\em Journal of neurophysiology}, Dec 2002.\n\n\\bibitem{Ringach:1997p1916}\nD~Ringach, M~Hawken, and R~Shapley.\n\\newblock Dynamics of orientation tuning in macaque primary visual cortex.\n\\newblock {\\em nature}, 387, 1997.\n\n\\bibitem{Ringach:2002p1805}\nD~Ringach, M~Hawken, and R~Shapley.\n\\newblock Receptive field structure of neurons in monkey primary visual cortex\n  revealed by stimulation with natural image sequences.\n\\newblock {\\em Journal of vision}, Dec 2002.\n\n\\bibitem{Ringach:2002p1807}\nD~Ringach, R~Shapley, and MJ~Hawken.\n\\newblock Orientation selectivity in macaque v1: diversity and laminar\n  dependence.\n\\newblock {\\em The Journal of Neuroscience}, 22(13):5639--5651, Dec 2002.\n\n\\bibitem{Ritt:2003p251}\nJason Ritt.\n\\newblock Evaluation of entrainment of a nonlinear neural oscillator to white\n  noise.\n\\newblock {\\em Phys. Rev. E}, 68(4):1--7, Oct 2003.\n\n\\bibitem{Rubin:2015p1673}\nDaniel~B Rubin, Stephen D~Van Hooser, and Kenneth~D Miller.\n\\newblock The stabilized supralinear network: A unifying circuit motif\n  underlying multi-input integration in sensory cortex.\n\\newblock {\\em Neuron}, 85(2):402--417, 2015.\n\n\\bibitem{Salinas:1994p1840}\nE~Salinas and L~Abbott.\n\\newblock Vector reconstruction from firing rates.\n\\newblock {\\em J Comput Neurosci}, 1:89--107, Dec 1994.\n\n\\bibitem{Series:2004p575}\nP~Seri{\\`e}s, PE~Latham, and A~Pouget.\n\\newblock Tuning curve sharpening for orientation selectivity: coding\n  efficiency and the impact of correlations.\n\\newblock {\\em Nature neuroscience}, 7(10):1129--1135, 2004.\n\n\\bibitem{Shadlen:1998p481}\nM~N Shadlen and W~T Newsome.\n\\newblock The variable discharge of cortical neurons: implications for\n  connectivity, computation, and information coding.\n\\newblock {\\em J. Neurosci.}, 18:3870--3896, 1998.\n\n\\bibitem{Smith:2005p2073}\nMatthew~A Smith, Najib~J Majaj, and J~Anthony Movshon.\n\\newblock Dynamics of motion signaling by neurons in macaque area mt.\n\\newblock {\\em Nature Neuroscience}, 8(2):220--228, Feb 2005.\n\n\\bibitem{Somers:1995p1672}\nD~C Somers, S~B Nelson, and M~Sur.\n\\newblock An emergent model of orientation selectivity in cat visual cortical\n  simple cells.\n\\newblock {\\em J Neurosci}, 15(8):5448--65, Jul 1995.\n\n\\bibitem{Spiridon:2001p715}\nMona Spiridon and Wulfram Gerstner.\n\\newblock Effect of lateral connections on the accuracy of the population code\n  for a network of spiking neurons.\n\\newblock {\\em Network: Computation in Neural Systems}, 12(4):409--421, 2001.\n\n\\bibitem{Thorpe:1996p1852}\nS~Thorpe, D~Fize, and C~Marlot.\n\\newblock Speed of processing in the human visual system.\n\\newblock {\\em nature}, 381, Jun 1996.\n\n\\bibitem{Tiesinga:2008p462}\nP~Tiesinga, JM~Fellous, and T~J Sejnowski.\n\\newblock Regulation of spike timing in visual cortical circuits.\n\\newblock {\\em Nature Reviews Neuroscience}, 9(2):97--107, 2008.\n\n\\bibitem{vanVreeswijk:1996p476}\nC~van Vreeswijk and H~Sompolinsky.\n\\newblock Chaos in neuronal networks with balanced excitatory and inhibitory\n  activity.\n\\newblock {\\em Science}, 274:1724--1726, 1996.\n\n\\end{thebibliography}\n\n\n\n\n\n", "itemtype": "equation", "pos": 25529, "prevtext": "\nwhere the average $\\langle \\cdot \\rangle$ is taken over distinct trials \nfor the same signal. $Fid = 0$ or very close to $0$ for a signal means, \naccording to this definition, that in almost every trial the mean vector strength never\ndeviates much from the true value of $\\theta_{sig}(t)$. \nIn the case of nonconstant signals a correction can be used to offset the\ndelay incorporated into the read-out function; this is discussed in Section~\\ref{cts_signals}.\n \nThe second metric we consider is {\\it reliability}, meaning trial-to-trial\nvariability in the system's response to repeated presentations of the same stimulus. \nWe define it to be\n\n", "index": 17, "text": "$$\nRel[\\{\\theta_{sig}(t)\\}_{t\\in [0,T]}]= \\frac{1}{T} \\int_0^T \\sqrt{Var({\\Theta}(t))} \\ dt\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"Rel[\\{\\theta_{sig}(t)\\}_{t\\in[0,T]}]=\\frac{1}{T}\\int_{0}^{T}\\sqrt{Var({\\Theta}%&#10;(t))}\\ dt\" display=\"block\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>\u03b8</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mpadded width=\"+5pt\"><msqrt><mrow><mi>V</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}]