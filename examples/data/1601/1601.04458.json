[{"file": "1601.04458.tex", "nexttext": "\nwith a right hand side function $f$ depending on the systems state and some parameter $\\theta$. In Systems Biology examples $f$ is often determined by the stoichiometric matrix $S$ and the rate law vector $v$: $f \\left( x( t ; \\theta, \\nu_{i-1}  )  , \\theta \\right)=\\ S\\  v \\left( x( t ; \\theta, \\nu_{i-1}  )  , \\theta \\right)$.\\[12pt]\n\nWe recently published an objective function for parameter estimation \\cite{Zimmer12} that decomposes the whole time series into intervals and fits to the intervals individually. The initial value for each interval is obtained by a state updating. We suggest a slightly more flexible state updating here, namely the state updating is performed as follows: Choose a subset of the measurement time points: $T\\subseteq \\{t_0,t_1,\\ldots,t_n\\}$ and define the state update $\\hat{\\nu}_i$ at $t_i$ as:\n\n", "itemtype": "equation", "pos": 6305, "prevtext": "\n\n\n\\maketitle\n\n\\noindent\\small\n$ ^{1}$BIOMS, Im Neuenheimer Feld 267, 69120 Heidelberg, Germany\\\\\nEmail: christoph.zimmer@bioquant.uni-heidelberg.de;\\\\\n$ ^{2}$BioQuant, Im Neuenheimer Feld 267, 69120 Heidelberg, Germany\\\\\nEmail: frank.bergmann@bioquant.uni-heidelberg.de;\\\\\n$ ^{3}$BioQuant, Im Neuenheimer Feld 267, 69120 Heidelberg, Germany\\\\\nEmail: sven.sahle@bioquant.uni-heidelberg.de;\\ \\  \\\\\n$ ^*$Corresponding author\n\n\n\\section*{Abstract}\n\n\\noindent\nOrdinary differential equations (ODE) are widely used for modeling in Systems Biology. As most commonly only some of the kinetic parameters are measurable or precisely known, parameter estimation techniques are applied to parametrize the model to experimental data. A main challenge for the parameter estimation is the complexity of the parameter space, especially its high dimensionality and local minima. \\\\\n\nParameter estimation techniques consist of an objective function, measuring how well a certain parameter set describes the experimental data, and an optimization algorithm that optimizes this objective function. A lot of effort has been spent on developing highly sophisticated optimization algorithms to cope with the complexity in the parameter space, but surprisingly few articles address the influence of the objective function on the computational complexity in finding global optima. We extend a recently developed multiple shooting for stochastic systems (MSS) objective function for parameter estimation of stochastic models and apply it to parameter estimation of ODE models. This MSS objective function treats the intervals between measurement points separately. This separate treatment allows the ODE trajectory to stay closer to the data and we show that it reduces the complexity of the parameter space.\\\\\n\nWe use examples from Systems Biology, namely a Lotka-Volterra model, a FitzHugh-Nagumo oscillator and a Calcium oscillation model, to demonstrate the power of the MSS approach for reducing the complexity and the number of local minima in the parameter space. The approach is fully implemented in the COPASI software package and, therefore, easily accessible for a wide community of researchers.\n\n\n\n\n\\section*{Introduction}\n\n\\noindent\nMathematical modeling is a vital technique for analyzing complex systems in the sciences. We focus on models of ordinary differential equations (ODE) as these are widely used to model time course experiments in Systems Biology. These models are often parametrized using parameter estimation methods that find sets of parameters that let the model optimally fit to the data.\\[12pt]\n\nChallenges arise from the complexity of the parameter space that may contain (many) local minima. Parameter estimation techniques consist of two components: an objective function quantifying how well the parametrized model fits the data and an optimization algorithm optimizing this objective function. A huge amount of research focuses on optimization algorithms: there are global optimization techniques \\cite{Moles03,Rodriguez06,PS}, gradient based approaches \\cite{Bock07, LM64} and Bayesian techniques \\cite{Ghasemi11,Girolami08}. However, there have been few investigation on how the choice of the objective function leads to different landscapes influencing the difficulty of the optimization problem \\cite{Bock07,Leander14}.\\[12pt]\n\nWe slightly extend a recently developed method \\cite{Zimmer12} and show how it can be applied to ODE models to drastically reduce the complexity in the parameter search space. This ``multiple shooting for stochastic systems'' (MSS) method was recently developed for parameter estimation in stochastic models. The MSS approach splits the time course data into intervals that are treated separately. The initial values for each interval are composed from actual measurements and a state updating for the unobserved components. This article makes the approach more flexible by allowing to vary the number of intervals that are introduced. Using only one interval corresponds to the common least squares functional for ODE models. Using as many intervals as there is measurements corresponds to the MSS objective function in \\cite{Zimmer12}.\\[12pt]\n\nThe importance of state estimation has been recognized in \\cite{Bock07} that considers the state variables as optimization variables and uses continuity constraints on them to obtain a continuous trajectory in the end. Highly sophisticated structure exploitation reduces the complexity of the optimization problem again. \\cite{Leander14} uses techniques from stochastic differential equations combined with a Kalman filtering. As both approaches and ours add flexibility to the states, this seems to be a crucial point in influencing the objective function landscape. The appeal of our method is the technical simplicity that gives any user the chance to easily implement it on their own. Even more, as the approach is implemented in COPASI, the full model import and analysis functionality of COPASI can be used.\\[12pt]\n\nThis article will demonstrate the benefits of using the extended approach to time course data from ODE models. The parameter estimation landscapes are a lot smoother with this MSS objective function which greatly simplifies optimization. It is fully implemented in the software COPASI \\cite{Copasi} a popular modeling and simulation environment. Apart from its own file format, COPASI supports the import of models encoded in SBML \\cite{hucka_2003full}, \\cite{hucka_2010}. Therefore, a user can apply the method quickly to models available in model databases, or models created by hundreds of other SBML compliant software programs. \\[12pt]\n\nThe article will use three different example models to show the power of the MSS approach on ODE models: a FitzHugh-Nagumo oscillator, a Lotka-Volterra model and a Calcium oscillation model.\n\n\n\\section*{Method}\n\n\n\n\\subsection*{MSS objective function}\n\nAssume that data $\\nu^{\\text{obs}} =(\\nu_0^{\\text{obs}},\\ldots,\\nu_n^{\\text{obs}})$ is observed at time points $t_0,t_1,\\ldots,t_n$. The state of the system at time $t_i$ is composed of the observed and unobserved species: $\\nu_i=( \\nu_i^{\\text{obs}},\\nu_i^{\\text{hid}} )$. A model that describes the dynamical behavior of the system is given with a systems of ordinary differential equations:\n\n", "index": 1, "text": "\\begin{gather}\n \\label{eq:ode}\n \\frac{d}{d t} x( t ; \\theta ,x_0   )    =   f \\left(t, x( t ; \\theta, x_0  )  , \\theta \\right)\\\\\n x(0;\\theta,x_0) = x_0,\n\\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{d}{dt}x(t;\\theta,x_{0})=f\\left(t,x(t;\\theta,x_{0}),\\theta\\right)\" display=\"block\"><mrow><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>;</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>;</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>\u03b8</mi><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x(0;\\theta,x_{0})=x_{0},\" display=\"block\"><mrow><mrow><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>;</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04458.tex", "nexttext": "\n\nwith $\\Delta_i= t_i-t_{i-1}$, ``obs'' denoting the observable components and ``hid'' the unobservable (hidden) components; hence the state estimate $\\hat{\\nu}_i$ is composed by $\\hat{\\nu}_i=( \\hat{\\nu}_i^{\\text{obs}},\\hat{\\nu}_i^{\\text{hid}} )$. The same holds for the ODE solution $x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  ) =  \\left(  x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{obs}},     x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{hid}}    \\right) $. \\[12pt]\nThe objective function for parameter estimation is then defined as\n\n\n", "itemtype": "equation", "pos": 7303, "prevtext": "\nwith a right hand side function $f$ depending on the systems state and some parameter $\\theta$. In Systems Biology examples $f$ is often determined by the stoichiometric matrix $S$ and the rate law vector $v$: $f \\left( x( t ; \\theta, \\nu_{i-1}  )  , \\theta \\right)=\\ S\\  v \\left( x( t ; \\theta, \\nu_{i-1}  )  , \\theta \\right)$.\\[12pt]\n\nWe recently published an objective function for parameter estimation \\cite{Zimmer12} that decomposes the whole time series into intervals and fits to the intervals individually. The initial value for each interval is obtained by a state updating. We suggest a slightly more flexible state updating here, namely the state updating is performed as follows: Choose a subset of the measurement time points: $T\\subseteq \\{t_0,t_1,\\ldots,t_n\\}$ and define the state update $\\hat{\\nu}_i$ at $t_i$ as:\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{aligned}\n \\label{eq:state-update}\n \\hat{\\nu}_i^{\\text{obs}} &= \\nu_i^{\\text{obs}},\\ & t_i\\in T  \\\\\n \\hat{\\nu}_i^{\\text{obs}} &= x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{obs}},\\ & t_i\\notin T   \\\\\n &\\text{ and } &  \\\\\n \\hat{\\nu}_i^{\\text{hid}} &= x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{hid}}, & \\text{ for all } t_i\n \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{\\nu}_{i}^{\\text{obs}}\" display=\"inline\"><msubsup><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mtext>obs</mtext></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\nu_{i}^{\\text{obs}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msubsup><mi>\u03bd</mi><mi>i</mi><mtext>obs</mtext></msubsup></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hskip 10.0ptt_{i}\\in T\" display=\"inline\"><mrow><msub><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>t</mi></mpadded><mi>i</mi></msub><mo>\u2208</mo><mi>T</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{\\nu}_{i}^{\\text{obs}}\" display=\"inline\"><msubsup><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mtext>obs</mtext></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=x(\\Delta_{i},\\theta,\\hat{\\nu}_{i-1})^{\\text{obs}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>x</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>i</mi></msub><mo>,</mo><mi>\u03b8</mi><mo>,</mo><msub><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mtext>obs</mtext></msup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hskip 10.0ptt_{i}\\notin T\" display=\"inline\"><mrow><msub><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>t</mi></mpadded><mi>i</mi></msub><mo>\u2209</mo><mi>T</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{\\nu}_{i}^{\\text{hid}}\" display=\"inline\"><msubsup><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mtext>hid</mtext></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xc.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=x(\\Delta_{i},\\theta,\\hat{\\nu}_{i-1})^{\\text{hid}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>x</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>i</mi></msub><mo>,</mo><mi>\u03b8</mi><mo>,</mo><msub><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mtext>hid</mtext></msup></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xc.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hskip 10.0pt\\text{ for all }t_{i}\" display=\"inline\"><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mtext>\u00a0for all\u00a0</mtext></mpadded><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.04458.tex", "nexttext": "\nwith $x$ as in equation (\\ref{eq:ode}), $\\hat{\\nu}_{i-1}$ as in equation (\\ref{eq:state-update}). \\[12pt]\n\nIf $T=\\emptyset$, the objective function equals the standard least squares functional (LSQ). If $T=\\{t_0,t_1,\\ldots,t_n\\}$, then the objective function equals the MSS objective function in \\cite{Zimmer12,Zimmer14}. The number of points in $T$ determines how many data points are used for updating the observable components of the system.\\[12pt]\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection*{Optimization and Software}\nThe objective function can be optimized with gradient based methods, global \noptimization techniques or Bayesian approaches. The objective function is \nimplemented in the software package COPASI. COPASI is a platform independent, user friendly software tool for \nsetting up, simulating and analysing kinetic models of biochemical reaction networks.\nIt is freely available, open source software and is developed in a well established international\ncooperation.\nFor a user of COPASI the workflow of running a parameter \nestimation with this objective function looks as follows: first the user would import\na dataset containing the observed/measured time points and map the columns of \nthe dataset to the corresponding elements in his model. Once that is done, a feature \nin the \\texttt{Tools} menu called \\texttt{Create Events for Timeseries Experiment}\nturns the measured time points into discrete events. These events force the value of \na model variable at the measured time points to the observed value. Users are free\nto modify the list of events, removing or adding further time points as desired. \nRemoving all automatically created events restores the original LSQ functional. \nExample files have been placed on our website at \\url{http://copasi.org/Projects/piecewise_parameter_fitting/}.\n\n\\begin{figure}[H]\n  \\label{fig:copasi}\n   \\includegraphics[width=\\textwidth]{figure1}\n   \\caption{COPASI running a parameter scan plotting the objective function value for each parameter.}\n\\end{figure}\n\n\n\n\n\n\\newpage\n\\section*{Results}\n\nThe results section demonstrates the power of the method on three examples from Systems Biology: a FitzHugh-Nagumo oscillator, a Lotka-Volterra model and a Calcium oscillation model.\n\n\n\n\\subsection*{FitzHugh-Nagumo oscillator}\n\nThe first example is a FitzHugh-Nagumo oscillator \\cite{Nagumo62,FitzHugh61} with\n\\begin{eqnarray*}\n\\frac{dV}{dt} &=& \\gamma \\left( V-\\frac{V^3}{3} + R \\right)\\\\\n\\frac{dR}{dt} &=&  - \\frac{1}{\\gamma}\\left( V-\\alpha+\\beta R \\right)\n\\end{eqnarray*}\n\nwith initial values: $V(0)=-1$ and $R(0)=1$ and parameters $\\theta=(\\alpha,\\beta,\\gamma)$ with $\\alpha=0.2$, $\\beta=0.2$, $\\gamma=3$. Measurements of $V$ are recorded at $t_k=0,1,2,\\ldots,20$ with normally distributed additive noise with variance $0.1$. The parametrization has been chosen according to \\cite{Leander14}.\\[12pt]\n\nFigure \\ref{fig:fhn} (left panel) shows the fitness landscape with the LSQ objective function ($K=\\emptyset$) and figure \\ref{fig:fhn} (right panel) shows the fitness landscape with MSS objective function ($T=\\{0,1,2,\\ldots,20\\}$. One can see that the objective function landscape is a lot smoother using the MSS objective function rather than the conventional LSQ objective function. Therefore, optimization algorithms have less difficulties using the MSS objective function.\\[12pt]\n\nAs the MSS objective function differs from the LSQ objective function, their global minima are not necessarily identical. The LSQ minimum for this scenario is $(\\alpha,\\beta)=(0.25,0.19)$. Note, that the minimum is not the true parameter as measurement noise has been added to the pseudo data. The MSS minimum is $(\\alpha,\\beta)=(0.22,0.01)$. As most users will be interested in the LSQ minimum, one can use the MSS minimum as start value for a second optimization with the LSQ function. This second optimization will be very fast as the MSS minimum is within the valley of attraction of the global LSQ minimum. As described above both objective functions are implemented in the software COPASI \\cite{Copasi}, making them readily available for users. They can enter their model using a graphical user interface, or start with \nmodels from model databases. Measured data is quickly applied to the model, and simulations can be easily carried out, plotted and analyzed. \n\n\n\\begin{figure}[H]\n    \\begin{minipage}{1\\linewidth}\n   \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{fhn1LAND.png}\n  \\end{minipage}\n  \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{fhn2LAND.png}\n  \\end{minipage}\n \\end{minipage}\n \\caption{Fitness landscape of the FitzHugh Nagumo oscillator with LSQ objective function (left) and MSS objective function (right). Bright color stands for high values and dark color for small values. One can see that the LSQ objective function shows local minima whereas the MSS objective function is free of local minima.}\n \\label{fig:fhn}\n\\end{figure}\n\n\n\n\\newpage\n\\subsection*{Lotka-Volterra}\n\nThe second example is a Lotka-Volterra system which describes the interaction of a prey population $X$ with a predator population $Y$:\n\n", "itemtype": "equation", "pos": 8228, "prevtext": "\n\nwith $\\Delta_i= t_i-t_{i-1}$, ``obs'' denoting the observable components and ``hid'' the unobservable (hidden) components; hence the state estimate $\\hat{\\nu}_i$ is composed by $\\hat{\\nu}_i=( \\hat{\\nu}_i^{\\text{obs}},\\hat{\\nu}_i^{\\text{hid}} )$. The same holds for the ODE solution $x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  ) =  \\left(  x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{obs}},     x( \\Delta_i , \\theta ,\\hat{\\nu}_{i-1}  )^{\\text{hid}}    \\right) $. \\[12pt]\nThe objective function for parameter estimation is then defined as\n\n\n", "index": 5, "text": "\\begin{equation}\n\\begin{aligned}\n \\label{eq:objfunc}\n F(\\nu,\\theta,\\nu_0 ) &=   \\sum_{i=1}^n  \\left(  x(\\Delta_i; \\theta, \\hat{\\nu}_{i-1} ) - \\nu_i \\right) ^2 \n \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle F(\\nu,\\theta,\\nu_{0})\" display=\"inline\"><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bd</mi><mo>,</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>\u03bd</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i=1}^{n}\\left(x(\\Delta_{i};\\theta,\\hat{\\nu}_{i-1})-\\nu_{i}%&#10;\\right)^{2}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><msup><mrow><mo>(</mo><mrow><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>i</mi></msub><mo>;</mo><mi>\u03b8</mi><mo>,</mo><msub><mover accent=\"true\"><mi>\u03bd</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>\u03bd</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04458.tex", "nexttext": "\nwith initial values $X(0)=Y(0)=10$ and parameters $\\theta=(\\alpha,\\beta,\\gamma,\\delta)$ with $\\alpha=1$, $\\beta=0.2$, $\\gamma=1$, $\\delta=0.15$. Measurements of $X$ and $Y$ are recorded at $t_k=0,0.5,1,\\ldots,24$  with additive normally distributed measurement error with variance $1$.\\[12pt]\n\nFigure \\ref{fig:lv} (left) shows the objective function landscape with the LSQ objective function and figure \\ref{fig:lv} (right) shows the objective function landscape using the MSS objective function. The first observation is that the MSS objective function landscape is free of local minima whereas the LSQ objective function landscape shows local minima. Therefore, optimization is much easier with the MSS objective function. \\[12pt]\n\nAgain, as the objective function are not identical, they lead to two slightly different minima: $(\\alpha,\\delta)=(1.01,0.15)$ for LSQ and $(\\alpha,\\delta)=(0.91,0.16)$ for MSS. However, the important point is that the MSS minimum is in the valley of attraction of the global LSQ minimum. This means that one can use the MSS minimum for a second optimization run with the LSQ objective function and this second optimization run is very fast.\\[12pt]\n\n\n\\begin{figure}[H]\n  \\begin{minipage}{1\\linewidth}\n   \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{lvlqLAND.png}\n  \\end{minipage}\n  \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{lvmssLAND.png}\n  \\end{minipage}\n \\end{minipage}\n \\caption{Fitness landscape for the Lotka-Volterra model with LSQ objective function (left) and MSS objective function (right). Bright color stands for high values and dark color for small values. One can see that the LSQ objective function shows local minima whereas the MSS objective function is smooth.}\n   \\label{fig:lv}\n\\end{figure}\n\n\nFurthermore, the Lotka-Volterra example has been used to investigate bias and variance of the LSQ estimates and MSS estimates. To this end, $100$ noisy time series have been created by adding gaussian noise with variance $1$ to a time course of $40$ observations with inter-sample distance $1$. Each of the $100$ time series is used for parameter estimation with the LSQ and MSS objective function, resulting in $100$ estimates $\\hat{\\theta}_k$, $k\\in\\{LSQ,MSS\\}$ for each objective function. The average deviation of the estimates to the true value $\\theta^{(0)}=(1,0.15)$ is calculated: This deviation $\\frac{1}{100} \\sum_{i=1}^{100} \\left(  \\hat{\\theta}_k - \\theta^{(0)} \\right)$ is $(-0.11, 0.0010)$ for the MSS method and $(0.0034, 0.00065)$ for the LSQ approach. This means that the LSQ method has a smaller bias for this example. \nThe standard deviation of the MSS estimates is with $(0.082, 0.011)$ larger than the standard deviation of the LSQ estimates with $(0.028, 0.0039)$.  \nHowever, the authors suggest to use the MSS method for a first estimation and the MSS estimate as start value for a second LSQ estimation.\\[12pt]\n\n\n\\begin{figure}[H]\n    \\center{   \\includegraphics[width=.67\\textwidth]{localMin}   }\n \\caption{Showing fits with the LSQ function with (A1,A2) the global optimal parameter, (B1,B2) a local optimal parameter and (C1,C2) a parameter that is in the middle of global and local optimum.}\n   \\label{fig:lvlocmin}\n\\end{figure}\n\nFigure \\ref{fig:lvlocmin} illustrates why the LSQ objective function landscape has local minima: The global optimal fit in (A1,A2) shows good agreement of data and model. The fit in the local minimum (B1,B2) shows an oscillation with a different frequency, but still some of the peaks are fitted well. The parameter in the middle of global and local optimum, however, shows an oscillation that does not fit any of the peaks and results, therefore, in a worse objective function value. Our MSS method fits the data peace wise. This means that every interval is initialized with a state estimate that incorporates information from the observation. Therefore, effect as out-of-phase oscillation cannot occur any more. This seems to be a major effect in reducing the number of local minima in the objective function landscape.\n\n\n\n\n\\newpage\n\\subsection*{Calcium Oscillation model}\n\nThe third model is a Calcium oscillation core model from \\cite{Kummer05}:\n\n\n", "itemtype": "equation", "pos": 13536, "prevtext": "\nwith $x$ as in equation (\\ref{eq:ode}), $\\hat{\\nu}_{i-1}$ as in equation (\\ref{eq:state-update}). \\[12pt]\n\nIf $T=\\emptyset$, the objective function equals the standard least squares functional (LSQ). If $T=\\{t_0,t_1,\\ldots,t_n\\}$, then the objective function equals the MSS objective function in \\cite{Zimmer12,Zimmer14}. The number of points in $T$ determines how many data points are used for updating the observable components of the system.\\[12pt]\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection*{Optimization and Software}\nThe objective function can be optimized with gradient based methods, global \noptimization techniques or Bayesian approaches. The objective function is \nimplemented in the software package COPASI. COPASI is a platform independent, user friendly software tool for \nsetting up, simulating and analysing kinetic models of biochemical reaction networks.\nIt is freely available, open source software and is developed in a well established international\ncooperation.\nFor a user of COPASI the workflow of running a parameter \nestimation with this objective function looks as follows: first the user would import\na dataset containing the observed/measured time points and map the columns of \nthe dataset to the corresponding elements in his model. Once that is done, a feature \nin the \\texttt{Tools} menu called \\texttt{Create Events for Timeseries Experiment}\nturns the measured time points into discrete events. These events force the value of \na model variable at the measured time points to the observed value. Users are free\nto modify the list of events, removing or adding further time points as desired. \nRemoving all automatically created events restores the original LSQ functional. \nExample files have been placed on our website at \\url{http://copasi.org/Projects/piecewise_parameter_fitting/}.\n\n\\begin{figure}[H]\n  \\label{fig:copasi}\n   \\includegraphics[width=\\textwidth]{figure1}\n   \\caption{COPASI running a parameter scan plotting the objective function value for each parameter.}\n\\end{figure}\n\n\n\n\n\n\\newpage\n\\section*{Results}\n\nThe results section demonstrates the power of the method on three examples from Systems Biology: a FitzHugh-Nagumo oscillator, a Lotka-Volterra model and a Calcium oscillation model.\n\n\n\n\\subsection*{FitzHugh-Nagumo oscillator}\n\nThe first example is a FitzHugh-Nagumo oscillator \\cite{Nagumo62,FitzHugh61} with\n\\begin{eqnarray*}\n\\frac{dV}{dt} &=& \\gamma \\left( V-\\frac{V^3}{3} + R \\right)\\\\\n\\frac{dR}{dt} &=&  - \\frac{1}{\\gamma}\\left( V-\\alpha+\\beta R \\right)\n\\end{eqnarray*}\n\nwith initial values: $V(0)=-1$ and $R(0)=1$ and parameters $\\theta=(\\alpha,\\beta,\\gamma)$ with $\\alpha=0.2$, $\\beta=0.2$, $\\gamma=3$. Measurements of $V$ are recorded at $t_k=0,1,2,\\ldots,20$ with normally distributed additive noise with variance $0.1$. The parametrization has been chosen according to \\cite{Leander14}.\\[12pt]\n\nFigure \\ref{fig:fhn} (left panel) shows the fitness landscape with the LSQ objective function ($K=\\emptyset$) and figure \\ref{fig:fhn} (right panel) shows the fitness landscape with MSS objective function ($T=\\{0,1,2,\\ldots,20\\}$. One can see that the objective function landscape is a lot smoother using the MSS objective function rather than the conventional LSQ objective function. Therefore, optimization algorithms have less difficulties using the MSS objective function.\\[12pt]\n\nAs the MSS objective function differs from the LSQ objective function, their global minima are not necessarily identical. The LSQ minimum for this scenario is $(\\alpha,\\beta)=(0.25,0.19)$. Note, that the minimum is not the true parameter as measurement noise has been added to the pseudo data. The MSS minimum is $(\\alpha,\\beta)=(0.22,0.01)$. As most users will be interested in the LSQ minimum, one can use the MSS minimum as start value for a second optimization with the LSQ function. This second optimization will be very fast as the MSS minimum is within the valley of attraction of the global LSQ minimum. As described above both objective functions are implemented in the software COPASI \\cite{Copasi}, making them readily available for users. They can enter their model using a graphical user interface, or start with \nmodels from model databases. Measured data is quickly applied to the model, and simulations can be easily carried out, plotted and analyzed. \n\n\n\\begin{figure}[H]\n    \\begin{minipage}{1\\linewidth}\n   \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{fhn1LAND.png}\n  \\end{minipage}\n  \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{fhn2LAND.png}\n  \\end{minipage}\n \\end{minipage}\n \\caption{Fitness landscape of the FitzHugh Nagumo oscillator with LSQ objective function (left) and MSS objective function (right). Bright color stands for high values and dark color for small values. One can see that the LSQ objective function shows local minima whereas the MSS objective function is free of local minima.}\n \\label{fig:fhn}\n\\end{figure}\n\n\n\n\\newpage\n\\subsection*{Lotka-Volterra}\n\nThe second example is a Lotka-Volterra system which describes the interaction of a prey population $X$ with a predator population $Y$:\n\n", "index": 7, "text": "\\begin{gather*}\n \\begin{aligned}\n   \\frac{d X}{dt}\\ &=\\ \\alpha X - \\beta X Y\\\\\n   \\frac{d Y}{dt}\\ &=\\ \\delta X Y - \\gamma Y.\n \\end{aligned}\n\\end{gather*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{aligned} \\displaystyle\\frac{dX}{dt}&amp;\\displaystyle=\\ \\alpha&#10;X%&#10;-\\beta XY\\\\&#10;\\displaystyle\\frac{dY}{dt}&amp;\\displaystyle=\\ \\delta XY-\\gamma Y.\\end{aligned}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>X</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>Y</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>Y</mi></mrow><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi>Y</mi></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04458.tex", "nexttext": "\nwith initial values $g(0)=ca(0)=plc(0)=10$ and parameters \\linebreak $\\theta=(212, 2.85, 1.52, 190, 4.88, 1180, 1.24, 32240, 29090, 13.58,153000, 160)$. Measurements are assumed to be recorded at $t_i=0,0.5,1,1.5,\\ldots,50$.\\[12pt]\nThe model with this parametrization exhibits a complex oscillatory behavior with bursting (figure \\ref{fig:Ca} (A)), making parameter estimation especially challenging. \n\nFigure \\ref{fig:Ca} (B) shows the LSQ objective function landscape when varying the parameter $\\theta_2$. The landscape is full of local minima and optimization will require many function evaluations even on the relatively small interval $[2,4]$. Figure \\ref{fig:Ca} (H) plots the MSS objective function landscape for the same setting. It is local minima free and convex which means that an optimization is simple. This shows again the beneficial influence of the MSS method on the objective function landscape. \\[12pt]\n\nTo address the question, how many intermediate points $T$ are necessary to reduce the influence of local minima, the objective function is also plotted with different values of $T$. This shows that already the introduction of only one intermediate point \\ref{fig:Ca} (C) improves the landscape. However, local minima are still present and one needs, indeed, almost all observation points included in $T$ to obtain a local minima free landscape.\\[12pt]\n\nIf the MSS objective function is used with a nonempty $T$, it differs from the LSQ objective function which means that the minimum will not be necessarily identical. Estimating the parameter $\\theta_2$ for all the scenarios one can see that the estimates are all in an interval from $[2.85,2.89]$. This means that the MSS objective function introduces \nonly a small bias compared to the LSQ functional for this model. \nMore importantly, it means that all the pre-estimates are in the valley of attraction of the global minimum of the LSQ functional. The software COPASI enables the user to use this pre-estimate as start value for a second run with the LSQ objective function. As this second run starts very close to the global minimum, an optimization is very fast.\n\n\n\\begin{figure}[H]\n    \\includegraphics[width=\\textwidth]{figureCA}\n   \\caption{Calcium trajectory with 25\\% relative measurement noise (A). Objective function landscape with LSQ (B) and the MSS (C-H) objective function. Panels C-H investigate the influence of the number of intermediate time points.}\n   \\label{fig:Ca}\n\\end{figure}\n\n\n\n\\section*{Discussion}\n\n\\noindent\nWe extended a recently developed objective function for parameter estimation in stochastic models. This extension can be used for parameter estimation in models of ODEs and greatly simplifies the complexity of the parameter search space. We demonstrate this feature on three models where the LSQ objective function exhibits multiple local minina and show the power of our MSS objective function. \\[12pt]\n\nThe MSS method treats intervals between succeeding data points separately and uses a state updating to gain initial values for each interval. The extension increases the flexibility of this approach by allowing some of the intervals separately and some together. This extension allows to investigate the influence of state estimation on the objective function landscape.\\[12pt]\n\nAs \\cite{Bock07} and \\cite{Leander14} also add flexibility to the states, this seems to be a crucial point in influencing the objective function landscape. Figure \\ref{fig:lvlocmin} illustrates how local minima can occur in the LSQ objective function landscape: As the LSQ approach uses only the very first initial value for integration, phase shifts in the oscillations might occur. This leads to an increase in the objective function (C1,C2). As soon as e.g. a double frequency is reached, the objective function value lowers again (B1,B2) leading to a local minimum. More generally, in complex systems an increase in deviation of the parameter from the true parameter does not necessarily lead to a stronger deviation of the model response from the data. However, approaches using state estimation reduce this problem as they are able to incorporate information from all observations (and not only the first) in the initial values for the intervals and, therefore, only require an integration over a shorter time interval. The special appeal of our method is the technical simplicity that gives any user the chance to easily implement it on their own. Even more, as the approach is implemented in COPASI, the full model import and analysis functionality of COPASI can be used.\\[12pt]\n\n\nAs the MSS objective function is not identical to the conventionally used least squares objective function, an MSS estimate will slightly differ from a LSQ estimate. This has been demonstrated for the Lotka-Volterra model for which the MSS method shows in a simulation study of $100$ pseudo data sets a slightly higher bias and variance. We do not have a general result but we think that the flatter curve of the MSS objective function leads to an easier optimization but as well an higher variance as experimental errors might shift the minimum of the curve stronger. Anyways, it is possible to use the MSS parameter estimate as already close start value for a (e.g. gradient based) LSQ based parameter estimation. As the computational cost of an optimization depends heavily on the landscape, these two optimizations have the advantage of both having local minima free landscapes. An additional benefit of the COPASI implementation of our approach is easy switching between the MSS and the LSQ optimization.\\\\\n\n\n\\section*{Acknowledgement}\n\\noindent\nThe software implementation of the presented method benefited from the efforts of the whole COPASI development team, currently Stefan Hoops, Brian Klahn, Ursula Kummer, Pedro Mendes, Ettore Murabito, and J\\\"urgen Pahle.\nChristoph Zimmer is supported by BIOMS, Frank Bergmann by Virtual Liver \\& NIH R01 GM070923 and Sven Sahle by the Klaus Tschira Stiftung. \nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. \n\n\n\n\\section*{References}\n\\bibliographystyle{unsrt}\n\\bibliography{120309}\n\n\n\n\n\n", "itemtype": "equation", "pos": 17910, "prevtext": "\nwith initial values $X(0)=Y(0)=10$ and parameters $\\theta=(\\alpha,\\beta,\\gamma,\\delta)$ with $\\alpha=1$, $\\beta=0.2$, $\\gamma=1$, $\\delta=0.15$. Measurements of $X$ and $Y$ are recorded at $t_k=0,0.5,1,\\ldots,24$  with additive normally distributed measurement error with variance $1$.\\[12pt]\n\nFigure \\ref{fig:lv} (left) shows the objective function landscape with the LSQ objective function and figure \\ref{fig:lv} (right) shows the objective function landscape using the MSS objective function. The first observation is that the MSS objective function landscape is free of local minima whereas the LSQ objective function landscape shows local minima. Therefore, optimization is much easier with the MSS objective function. \\[12pt]\n\nAgain, as the objective function are not identical, they lead to two slightly different minima: $(\\alpha,\\delta)=(1.01,0.15)$ for LSQ and $(\\alpha,\\delta)=(0.91,0.16)$ for MSS. However, the important point is that the MSS minimum is in the valley of attraction of the global LSQ minimum. This means that one can use the MSS minimum for a second optimization run with the LSQ objective function and this second optimization run is very fast.\\[12pt]\n\n\n\\begin{figure}[H]\n  \\begin{minipage}{1\\linewidth}\n   \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{lvlqLAND.png}\n  \\end{minipage}\n  \\begin{minipage}{.5\\linewidth}\n    \\includegraphics[width=7cm,height=7cm]{lvmssLAND.png}\n  \\end{minipage}\n \\end{minipage}\n \\caption{Fitness landscape for the Lotka-Volterra model with LSQ objective function (left) and MSS objective function (right). Bright color stands for high values and dark color for small values. One can see that the LSQ objective function shows local minima whereas the MSS objective function is smooth.}\n   \\label{fig:lv}\n\\end{figure}\n\n\nFurthermore, the Lotka-Volterra example has been used to investigate bias and variance of the LSQ estimates and MSS estimates. To this end, $100$ noisy time series have been created by adding gaussian noise with variance $1$ to a time course of $40$ observations with inter-sample distance $1$. Each of the $100$ time series is used for parameter estimation with the LSQ and MSS objective function, resulting in $100$ estimates $\\hat{\\theta}_k$, $k\\in\\{LSQ,MSS\\}$ for each objective function. The average deviation of the estimates to the true value $\\theta^{(0)}=(1,0.15)$ is calculated: This deviation $\\frac{1}{100} \\sum_{i=1}^{100} \\left(  \\hat{\\theta}_k - \\theta^{(0)} \\right)$ is $(-0.11, 0.0010)$ for the MSS method and $(0.0034, 0.00065)$ for the LSQ approach. This means that the LSQ method has a smaller bias for this example. \nThe standard deviation of the MSS estimates is with $(0.082, 0.011)$ larger than the standard deviation of the LSQ estimates with $(0.028, 0.0039)$.  \nHowever, the authors suggest to use the MSS method for a first estimation and the MSS estimate as start value for a second LSQ estimation.\\[12pt]\n\n\n\\begin{figure}[H]\n    \\center{   \\includegraphics[width=.67\\textwidth]{localMin}   }\n \\caption{Showing fits with the LSQ function with (A1,A2) the global optimal parameter, (B1,B2) a local optimal parameter and (C1,C2) a parameter that is in the middle of global and local optimum.}\n   \\label{fig:lvlocmin}\n\\end{figure}\n\nFigure \\ref{fig:lvlocmin} illustrates why the LSQ objective function landscape has local minima: The global optimal fit in (A1,A2) shows good agreement of data and model. The fit in the local minimum (B1,B2) shows an oscillation with a different frequency, but still some of the peaks are fitted well. The parameter in the middle of global and local optimum, however, shows an oscillation that does not fit any of the peaks and results, therefore, in a worse objective function value. Our MSS method fits the data peace wise. This means that every interval is initialized with a state estimate that incorporates information from the observation. Therefore, effect as out-of-phase oscillation cannot occur any more. This seems to be a major effect in reducing the number of local minima in the objective function landscape.\n\n\n\n\n\\newpage\n\\subsection*{Calcium Oscillation model}\n\nThe third model is a Calcium oscillation core model from \\cite{Kummer05}:\n\n\n", "index": 9, "text": "\\begin{gather}\n\\begin{aligned}\n\\label{eq:ca}\n  \\frac{d g}{d t}\\   &=\\ \\theta_1 + \\theta_2 g - \\frac{\\theta_3\\ g\\ plc}{g+\\theta_4} \n             - \\frac{\\theta_5\\ g\\ plc}{g+\\theta_6}\\\\\n  \\frac{d plc}{d t}\\ &=\\ \\theta_7 g - \\frac{\\theta_8\\ plc}{plc+\\theta_{9}} \\\\\n  \\frac{d ca}{d t}\\  &=\\ \\theta_{10} g - \\frac{\\theta_{11}\\ ca}{ca+\\theta_{12}}. \n\\end{aligned}\n \\end{gather}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{aligned} \\displaystyle\\frac{dg}{dt}&amp;\\displaystyle=\\ \\theta%&#10;_{1}+\\theta_{2}g-\\frac{\\theta_{3}\\ g\\ plc}{g+\\theta_{4}}-\\frac{\\theta_{5}\\ g\\ %&#10;plc}{g+\\theta_{6}}\\\\&#10;\\displaystyle\\frac{dplc}{dt}&amp;\\displaystyle=\\ \\theta_{7}g-\\frac{\\theta_{8}\\ plc%&#10;}{plc+\\theta_{9}}\\\\&#10;\\displaystyle\\frac{dca}{dt}&amp;\\displaystyle=\\ \\theta_{10}g-\\frac{\\theta_{11}\\ ca%&#10;}{ca+\\theta_{12}}.\\end{aligned}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>g</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mrow><msub><mi>\u03b8</mi><mn>1</mn></msub><mo>+</mo><mrow><msub><mi>\u03b8</mi><mn>2</mn></msub><mo>\u2062</mo><mi>g</mi></mrow></mrow><mo>-</mo><mfrac><mrow><mpadded width=\"+5pt\"><msub><mi>\u03b8</mi><mn>3</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>g</mi></mpadded><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>c</mi></mrow><mrow><mi>g</mi><mo>+</mo><msub><mi>\u03b8</mi><mn>4</mn></msub></mrow></mfrac><mo>-</mo><mfrac><mrow><mpadded width=\"+5pt\"><msub><mi>\u03b8</mi><mn>5</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>g</mi></mpadded><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>c</mi></mrow><mrow><mi>g</mi><mo>+</mo><msub><mi>\u03b8</mi><mn>6</mn></msub></mrow></mfrac></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>c</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mrow><msub><mi>\u03b8</mi><mn>7</mn></msub><mo>\u2062</mo><mi>g</mi></mrow><mo>-</mo><mfrac><mrow><mpadded width=\"+5pt\"><msub><mi>\u03b8</mi><mn>8</mn></msub></mpadded><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>c</mi></mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>c</mi></mrow><mo>+</mo><msub><mi>\u03b8</mi><mn>9</mn></msub></mrow></mfrac></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mrow><msub><mi>\u03b8</mi><mn>10</mn></msub><mo>\u2062</mo><mi>g</mi></mrow><mo>-</mo><mfrac><mrow><mpadded width=\"+5pt\"><msub><mi>\u03b8</mi><mn>11</mn></msub></mpadded><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi></mrow><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>a</mi></mrow><mo>+</mo><msub><mi>\u03b8</mi><mn>12</mn></msub></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}]