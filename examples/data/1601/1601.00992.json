[{"file": "1601.00992.tex", "nexttext": ", where $k$ is number of (directly adjacent) neighbors, $m$ is the number of previously exposed neighbors ($0 \\le m_i \\ge M$), $F$ is a ``temperature'' parameter that governs the extent to which the propensity to be infected depends on the infection rate among $i$'s neighbors.  We run the model for just two two time periods ($t \\in \\{0,1\\}$). The Ising\n  model controls actual infection after an experimenter assigns $Z_i$ at\n  $t=0$.\n\n  We specify the potential outcomes of the vertices in our simulation according to the following scheme depending upon the infection status at a given time point. We denote this by $Y(Z_{i0},Z_{i1})$, in which $Z_{i0}$ indicates $i$'s initial treatment status and $Z_{i1}$ indicates whether treatment propagates to $i$ at time 1. We generate a  baseline (pre-treatment) response $ Y(0,0) \\sim U(0,1)$.  Our simple treatment effect model changes the baseline in the same, multiplicative, way regardless of the time or manner of ``infection'' (directly assigned by researcher or propagated from a neighbor), and $Y(1,0) = Y(0,1) =  \\lambda Y(0,0)$. We consider values of $\\lambda \\in \\{0.26, 0.63\\}$, which correspond to approximately one and two standard deviation shifts in the mean of a standard uniform random variable, and simulate 1,000 treatment propagation and outcome sets at each combination of $F \\in \\{0,10,\\hdots,100\\}$ and $\\alpha \\in \\{0.05, 0.10,\\hdots,0.50\\}$.\n\n\n  \\subsection{Application of Inference Methods}\n\n\\citet{Aronow2013} method requires that we  define exposure conditions (which\ninclude assignment to treatment and also the probability of exposure to a\ntreatment via the network). We define the exposure conditions with respect to\nwhat a researcher would be able to observe from the experimental design,\nassuming the network were observed, and the response. Importantly, in defining\nthe exposure conditions of interest, we assume that the researcher does not\nexactly know the set of vertices to which the treatment has propagated. We see\nthis as more realistic than a situation in which the researcher knows exactly\nwhere the treatment has propagated.\n\nWe define the following three distinct exposure conditions, differentiating between those vertices treated initially ($d_1$), those vertices that are untreated initially and are adjacent to at least one treated vertex ($d_{(0,1)}$), and those vertices that are untreated initially and are not adjacent to any treated vertices ($d_{(0,0)}$).\n \\begin{itemize}\n  \t    \\item $d_1 \\equiv D_i(Z_i=1,0 \\le m_{it} \\ge M)$\n\t    \\item $d_{(0,0)} \\equiv D_i(Z_i=0,m_{it}=0)$\n\t    \\item $d_{(0,1)} \\equiv D_i(Z_i=0,m_{it} \\ge 1)$\n\t    \\end{itemize}\nFigure \\ref{fig:exposure} gives a visual example of a subgraph drawn from the Ghana road network. In the study of the \\citet{Aronow2013} methods, we focus on identifying the difference between the $d_{(0,1)}$ and $d_{(0,0)}$ conditions.\n\n\\begin{figure}[htp]\n\\begin{center}\n\\includegraphics[width=.7\\textwidth]{exposure-example.pdf}\n\\end{center}\n\\caption{Illustration of exposure conditions.  One draw from the Ising propagations with  $\\text{pr}(Z_i=1) \\sim \\text{Bernoulli}(.15)$\n  for $t \\in \\{0,1\\}$ and Temperature$=10$.}\n\\label{fig:exposure}\n\\end{figure}\n\nThe \\citet{Aronow2013} estimand is the difference between vertices in two exposure conditions. First, let\n  \n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\n\n\n\n\n\\begin{abstract}\n\n \\noindent How should a network experiment be designed to achieve high statistical power? Experimental\n treatments on networks may spread.  Randomizing assignment of treatment to\n nodes enhances learning about the counterfactual causal effects of a social\n network experiment and also requires new methodology\n \\citep[ex.]{Aronow2013,Bowers:2013,Toulis:2013}. In this paper we show that the way in which a treatment propagates across a social network affects the statistical power of an experimental design. As such, prior information regarding treatment propagation should be incorporated into the experimental design.  Our\n findings run against standard advice in circumstances where units are\n presumed to be independent: information about treatment effects is \\emph{not}\n maximized when we assign half the units to treatment and half to control. We\n also show that statistical power depends on the extent to which the network\n degree of nodes is correlated with treatment assignment probability.   We\n recommend that researchers think carefully about the underlying treatment\n propagation model motivating their study in designing an experiment on a\n network. \\\\~\\\\\n\n\\end{abstract}\n\\thispagestyle{empty}\n\\newpage\n\\doublespacing\n\n\\section{Introduction}\n\nWe consider the problem of designing experiments to causally identify propagation on networks. In fields across the social and physical sciences, there is considerable and growing interest in understanding how features propagate over the vertices (i.e., nodes) in a graph (i.e., network) via the graph topology. Furthermore, precise questions about causal peer, spillover and propagation effects are becoming more common. Recent theoretical developments highlight the barriers to the identification of causal peer/contagion effects in networks with non-randomized, observational, data \\citep{Lyons2011,Shalizi:2011}. Several recent papers have employed randomized experimental designs to facilitate the identification of causal peer effects \\citep{Aral:2011,Ostrovsky:2011,Bapna:2015,Bond:2012, Ichino:2012,nickerson2008voting}. \\citet{Aral:2011} study peer effects in Facebook application adoption through the randomization of `viral' features in a Facebook application. \\citet{Bond:2012} conduct an experiment to study social influence in voting behavior and other forms of political activity on Facebook during the 2010 congressional election. \\citet{Ichino:2012} conduct a field experiment during a national election in Ghana to gauge how voter registration responds to the placement of election monitors at registration workstations -- an effect that is hypothesized to spread geographically through the road network. \\citet{Bapna:2015} run a field experiment to determine whether peers influence each others' adoption of premium online social network services for which they have to pay.\n\nRecent methodological work enables scholars make statistical inferences about peer effects when the topology of a network is known \\citep{Bowers:2013,Aronow2013, Toulis:2013}.  However, there is little in the way of informative methodological research regarding the \\textbf{design} of randomization schemes that account for the form(s) of peer effect(s) in which the researcher is interested.\\footnote{For now, we set to the side the work on identifying how much of a total average effect can be attributed to mechanisms other than direct treatment assignment --- for example, the work on spillovers and indirect effects \\citet{mcconnell2010detecting,sinclair33, nickerson2008voting, nickerson2011social, hudgens2008toward, sobel2006randomized, tchetgen2010causal, vanderweele2008ignorability, vanderweele2010direct,  vanderweele2011components, vanderweele2012mapping, vanderweele2011bounding, vanderweele2011effect, miguel2004worms, chen2010technology, ichino2012deterring}} In this project we consider the performance of different randomization designs using the methods of \\citet{Bowers:2013}, \\citet{Aronow2013} and \\citet{Toulis:2013}  under different models of propagation. Each of the methods we consider depends upon a typology of exposure conditions based on the treatment status of each node and the topology of the graph. For example, a node could be treated directly by an experimenter, isolated from treatment (i.e., several hops away from any treated nodes) or exposed to the treatment at one degree of separation without control by the experimenter. The performance of randomized experimental designs on networks depends on (1) the exposure conditions of theoretical interest (say, direct treatment versus indirect treatment; or more generally some propagation flow parameter), (2) the topology of the network, (3) the ways in which the propagation model affects nodes in each exposure condition, and (4) the exposure condition distribution as determined by the randomization design.\\footnote{We direct readers to \\citet{Basse2015} for a methodological investigation similar to ours. They consider the problem of designing a randomized experiment to minimize estimation error when outcomes are correlated on a network. Their focus is, however, on estimating the direct effects of treatment, not on identifying indirect or propagation effects.}\n\nIn what follows, we study the problem of causal inference given treatment propagation in the context of a fixed graph topology and a single round of randomized treatment followed by a single round of response measurement. We review methods that have been proposed in the literature for designing and analyzing single-round (pre versus post), fixed graph experimental data; and also review the substantive experimental applications that have used such designs. We then conduct a simulation study motivated by the registration monitor randomization in \\citet{Ichino:2012}, using the Ghanaian network of roads between voter registration stations as  a realistic moderate sized graph. In the simulation study, we consider the performance of alternative experimental designs that varying the treatment probability: the number of nodes assigned to initial treatment, who is treated: the association between treatment probability and node degree (i.e., a node's number of ties), and how they are treated: different parameterizations of the propagation model.\n\n\\subsection{Recent Methodological Innovations in \\\\ Statistical Inference for Propagated Causal Effects}\n\nWe consider two general approaches to statistical inference about causal effects when those effects may propagate through a network. The flexible approach developed by \\citet{Bowers:2013} is a hypothesis testing framework designed to evaluate whether differences between the treatment and control groups are more effectively characterized by one model of treatment effects, which can include propagation effects, than another model. \\citet{Bowers:2013} focus on a natural sharp null model of no treatment effects (i.e., stochastic equivalence across all experimental conditions). The null distribution is derived exactly or generated approximately through repeated computations of the test statistic using permutations in which the treatment vector is re-randomized according to the experimental design and the hypothesized effects of the propagation model are removed. There are two highly appealing properties of this approach. First, any test statistic, including general distributional comparisons such as the Kolmogorov-Smirnov (KS) test statistic, can be used to evaluate the differences between treatment and control. Second, the approach can accommodate any model of treatment effects on a network, as the methodology does not require any form of independence assumption or the derivation of an estimator for the model parameters.\n\nThe methods developed by \\citet{Aronow2013} and \\citet{Toulis:2013} compliment those proposed by \\citet{Bowers:2013} in that they propose methods for {\\em estimating} average causal peer effects. \\citet{Aronow2013} develops randomization-based methods and  \\citet{Toulis:2013} develops both randomization and model-based approaches to estimating causal peer effects.  In both \\citet{Aronow2013} and \\citet{Toulis:2013}, the target estimate is the average difference between nodes in different network/treatment exposure conditions. \\citet{Aronow2013} do not stipulate a constrained set of conditions, but present methods that can be applied to any partition of nodes into network/treatment exposure conditions. They present an example in which nodes in a graph are directly treated and assume that treatment can only propagate one degree, which results in four conditions: {\\em control}, which are nodes that are not directly assigned treatment and are not tied to any treated nodes; {\\em direct}, which are nodes that are treated and not tied to any treated nodes, {\\em direct $+$ indirect}, which are nodes that are directly treated and are tied to treated nodes; and {\\em isolated direct}, which are nodes that are untreated and are tied to treated nodes.  \\citet{Toulis:2013}  define $k$-level treatment of a unit as (1) a unit not receiving direct treatment, and (2) having exactly $k$ directly treated neighbors.  A $k$-level control is any vertex with at least $k$ neighbors who did not (1) receive direct treatment and (2) is not connected to any vertices who were directly treated. These approaches assume that the researcher is interested in specific comparisons of averages and require that the researcher articulate mechanisms by which the probability of exposure to treatment may differ across units.\n\nBoth \\citet{Aronow2013} and \\citet{Toulis:2013} recognize the unique challenges that arise in the context of inference regarding response to network/treatment exposure conditions. The limitations are based in the topology of the graph. Since most exposure conditions of interest in the context of interference involve the position of a node in a network, under most randomization designs (e.g., uniform random assignment to treatment), each node is not equally likely to be assigned to each exposure condition. Take the example of $2$-level exposure in the framework of \\citet{Toulis:2013}. A node with only one partner would have zero probability of being assigned to the $2$-level treatment group.   \\citet{Aronow2013} do not discuss this issue at length, but imply a limitation in the derivation of their Horvitz-Thompson type estimators \\citep{Horvitz1952}. The estimators they define require that the analyst be able to calculate the probability $\\pi_i(d_k)$, the probability that node $i$ is in exposure condition $d_k$ and that $0 < \\pi_i(d_k) < 1$ for each node $i$. This means that the framework proposed by \\citet{Aronow2013} cannot be applied to the comparison of exposure conditions to which all nodes cannot be assigned.   \\citet{Toulis:2013}  are more explicit in their discussion of this limitation. They define a causally valid randomization design to be one in which at least one node is $k$-level treated and one is $k$-level controlled.\n\nIn the analysis that follows, we consider the \\citet{Aronow2013} and \\citet{Bowers:2013} approaches to inference with experiments on networks. The methods proposed by \\citet{Toulis:2013} are very similar to those of \\citet{Aronow2013}, and their concepts of $k$-level treated and $k$-level controlled can be seen as special cases of the exposure conditions defined in \\citet{Aronow2013}. Furthermore, the methods of \\citet{Aronow2013} have the added advantage of adjusting the treatment effect estimates (and variance estimates) for the unequal exposure condition probabilities. These Horvitz-Thompson type adjustments will correct for any associations between exposure condition probabilities and potential outcomes (e.g., higher degree nodes may exhibit higher baseline response values {\\em and} be more likely to be indirectly exposed to treatment through propagation).\n\n\\section{Design as a Function of Graph Topology}\n\n\n\\citet{Walker:2014} reviews several applications of experiments in networked environments -- including studies that are not focused on propagation -- and outline several of the fundamental challenges to designing experiments on networks. They summarize the problem of design in experiments on networks succinctly (p. 1949):\n\n\\begin{quotation}\\noindent``The natural connectivity of our world does not only\npresent a challenge to the conventional paradigm of\nexperimental design, but also reveals opportunities to\nleverage connectivity through the creation of novel\ntreatment mechanisms that incorporate both experimental\nsubjects and the connections between them.''\n\\end{quotation}\n\n\nThe practical implication of the dependence between subjects via the network is that efficient experimental designs will account for graph topology. The treatment assignment algorithms presented in \\citet{Toulis:2013} render a clear picture of how the importance of network structure can complicate design. Considering the problem of assuring that sufficient numbers of vertices end up in the $k$-treated and $k$-controlled designs, \\citet{Toulis:2013} present sequential randomization designs that assure that fixed numbers of vertices are assigned to the groups under comparison: for example, if a researcher desires to know the effect of the treatment on nodes having 2 directly connected neighbors in the network a design should ensure enough nodes with degree 2 assigned treatment versus not assigned treatment. Though powerful in their ability to control the distribution of vertices across exposure conditions, the complex sequential randomization algorithms proposed by \\citet{Toulis:2013} make closed-form calculation of the probability of exposure condition assignment intractable in most cases, which may be why they do not derive their estimators using Horvitz-Thompson adjustments such as those in \\citet{Aronow2013}. An example of a non-sequential randomization design for which it is straightforward to derive the Horvitz-Thompson weights is one in which the probability of treatment is biased with respect to vertex degree (e.g., disproportionately treating higher degree vertices). To provide an intuitive example regarding why it might be advantageous to treat high degree vertices at a greater rate than low degree vertices; suppose the researcher is interested in comparing nodes isolated from treatment (e.g., more than two degrees from any directly treated node) to nodes that are adjacent to a treated node, but are not directly treated. Each node that is directly treated is removed from both exposure conditions of interest, so there is an incentive to treat a small proportion of nodes. However, if too few nodes are treated, there will be too few nodes in the adjacent-to-treated condition. By focusing the treatment on higher degree nodes, it takes fewer directly treated nodes to accomplish a sizable sample of adjacent-to-treated nodes. Depending upon the structure of the network and the mechanism by which treatment can propagate, there may be a considerable gain in statistical power from biasing treatment towards high degree nodes, as compared to uniform assignment to treatment.\n\n\\subsection{Design is Dependent on a Model of Propagation}\n\nIn the classical experimental framework, a counterfactual causal effect for a person $i$ is defined as the situation where that person's outcome, $Y_i(Z_i=1)$ under one treatment would differ from that same person's outcome under a different treatment $Y_i(Z_i=0)$ (given at the same moment in time): $Y_i(Z_i) \\ne Y_i(Z'_i) , Z_i \\ne Z'_i$. That is, a causal effect is understood in the subjunctive tense. And we can write this comparison for a single person because we presume that treatment assigned to person $i$ will have no effect on the outcomes presented by any other person in the experiment --- that there is no interference between the treatment or outcomes of person $i$ and those of any other person.  The validity of the classical experimental framework, in which a lack of interference is assumed or assured through design, and the researcher is interested in differences between control and directly treated units, addresses mechanisms for effects only indirectly. A given mechanism or theory tends to motivate a study with implications for a given comparison and clever design can rule out competing theoretical explanations for the same comparison (positive, negative, large, small, absent, etc.).  The links between theoretical models and the analysis and design of experiments on social networks cannot be so loose especially if a scholar desires to learn about propagation processes. In the classical experimental setup, the groups to be compared are obvious and determined by the direct experimental conditions. However, once the assumption of no interference is relaxed, and especially if there is interest in evaluating the effects of interference, an implicit or explicit theoretical model of interference must be drawn upon to guide the search for propagation's footprint.\n\nBoth the methods of \\citet{Aronow2013} and \\citet{Bowers:2013} require that the researcher draw upon a theoretical model of interference. In \\citet{Aronow2013}, a model of interference must be used to identify the exposure conditions (e.g., adjacent to a treated unit, 2 degrees of separation from a treated unit, etc.) to be compared in the study. The model used with the methods of \\citet{Aronow2013} need not provide precise predictions about to which nodes the treatment will propagate and how, but the model must be complete enough to identify the groups of nodes for which different potential outcomes will be observed under a given direct treatment assignment regime. For the methods proposed by \\citet{Bowers:2013}, there is great value in a precise analytic model of interference. Their approach compares the observed data in which the effect of a treatment, through a parameterized interference model, has been removed, to data generated by permuting the initial treatment and removing the effects as parameterized through the interference model. As such, it is not possible to use the methods of \\citet{Bowers:2013} without specifying a model of interference.\n\nThe need to specify a model of propagation in order to identify an efficient experimental design leads us to question where researchers might start in developing such models. There is a vibrant literature, primarily in the fields of physics and applied mathematics, on graph dynamics, that provides several excellent starting points for analytical models of propagation. These models include the susceptible-infected-recovered disease epidemic models \\citep{Kermack_Mathematical_1927, Anderson_Population_1982, Hethcote_The_2000, Daley_Epidemic_2005},  the Bass Diffusion Model \\citep{Bass_Why_1994, Lenk_New_1990} and the Hopfield network \\citet{Hopfield_Neural_1982} and the voter model \\citet{Clifford_A_1973, Liggett_Stochastic_1997, Durrett_Random_1991}. In the simulation study that follows, we present and use a variant of the Ising model, a model that contains several of those mentioned above as special cases \\citep{Gallavotti:1999}.  The Ising model is a general formulation for stochastic binary-state dynamics. Many models of opinion dynamics are based on the classical Ising model of magnetic spins. Here, each node is in either the spin-up or spin-down state, and transitions occur according to dynamical rules that minimize the Hamiltonian of ferromagnetic interactions.\n\n\\section{Simulation Study}\n In what follows we conduct a simulation study in which we evaluate the statistical power of the methods proposed by \\citet{Aronow2013} and \\citet{Bowers:2013}. The objective of this simulation study is to demonstrate that the statistical power of these procedures depends upon design parameters that are intuitively meaningful in the context of propagation and that power is maximized at design parameters that differ considerably from those commonly used in experiments without interference -- random uniform division of the sample into half control and half treatment. It is outside the scope of the current study to compare the merits of the methods proposed by \\citet{Aronow2013} with those proposed by \\citet{Bowers:2013}: further, we see the two approaches as complementary in the same way that hypothesis testing and estimation complement one another.  We hope that our simulation will illustrate the importance and feasibility of simulation analysis for parameterizing designs for studies of propagation in networks. Throughout this simulation we use the Ghana voter registration station network from \\citet{Ichino:2012} as our example network. This network is derived from the roads connecting the registration stations. Two registration stations are considered to be tied if they are within 20km of each other on the road network. This results in a network with 868 vertices, and a density of 2.2\\%. The network is depicted in Figure \\ref{fig:networks}.\n\n\\subsection{Simulation Parameters and Definitions}\n\nWe denote the treatment assigned to vertex $i$ as $Z_i \\in \\{0,1\\}$ and the vector of treatment assignments to all vertices as ${\\bm{Z}}=\\{Z_1,\\ldots,Z_n \\}$. The fixed potential outcome of vertex $i$ that would be shown under treatment vector ${\\bm{Z}}$ is denoted $Y_i({\\bm{Z}})$. Under the sharp null of no treatment effect, all potential outcomes are equal; $Y_i({\\bm{Z}})=Y_i({\\bm{Z}}')=Y_i(Z_i=1,{\\bm{Z}}_{-i})=Y_i(Z_i=0,{\\bm{Z}}'_{-i})~~\\forall {\\bm{Z}}\n  \\ne {\\bm{Z}}'$. $Y_{it}(D_{it})$ is the potential outcome of vertex $i$ at time $t$ under exposure condition $D_{it}$. The exposure condition indicates a mapping of the graph topology and ${\\bm{Z}}$ into the categories of exposure defined by the researcher (e.g., directly treated and not tied to any directly treated vertices, not directly treated and adjacent to at least one directly treated node).\n\nThe model we use for treatment propagation in our simulations is a variant of the Ising Model. The initial treatment assignment of each vertex is drawn independently from $Z_i \\sim \\text{Bernoulli}(\\alpha)$. The infection probability at each iteration of propagation is\n  \n", "index": 1, "text": "$$\\frac{1}{1+\\exp(\\frac{2}{F}(k_i-2m_i))}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{1+\\exp(\\frac{2}{F}(k_{i}-2m_{i}))}\" display=\"block\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mfrac><mn>2</mn><mi>F</mi></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></math>", "type": "latex"}, {"file": "1601.00992.tex", "nexttext": "\n be the estimator of the mean potential outcome among vertices in exposure condition $d_k$, where $\\pi_i(d_k)$ is the probability that vertex $i$ ends up in condition $d_k$. Then the estimator of the difference between potential outcomes in the two exposure conditions is\n\n  \n", "itemtype": "equation", "pos": 25511, "prevtext": ", where $k$ is number of (directly adjacent) neighbors, $m$ is the number of previously exposed neighbors ($0 \\le m_i \\ge M$), $F$ is a ``temperature'' parameter that governs the extent to which the propensity to be infected depends on the infection rate among $i$'s neighbors.  We run the model for just two two time periods ($t \\in \\{0,1\\}$). The Ising\n  model controls actual infection after an experimenter assigns $Z_i$ at\n  $t=0$.\n\n  We specify the potential outcomes of the vertices in our simulation according to the following scheme depending upon the infection status at a given time point. We denote this by $Y(Z_{i0},Z_{i1})$, in which $Z_{i0}$ indicates $i$'s initial treatment status and $Z_{i1}$ indicates whether treatment propagates to $i$ at time 1. We generate a  baseline (pre-treatment) response $ Y(0,0) \\sim U(0,1)$.  Our simple treatment effect model changes the baseline in the same, multiplicative, way regardless of the time or manner of ``infection'' (directly assigned by researcher or propagated from a neighbor), and $Y(1,0) = Y(0,1) =  \\lambda Y(0,0)$. We consider values of $\\lambda \\in \\{0.26, 0.63\\}$, which correspond to approximately one and two standard deviation shifts in the mean of a standard uniform random variable, and simulate 1,000 treatment propagation and outcome sets at each combination of $F \\in \\{0,10,\\hdots,100\\}$ and $\\alpha \\in \\{0.05, 0.10,\\hdots,0.50\\}$.\n\n\n  \\subsection{Application of Inference Methods}\n\n\\citet{Aronow2013} method requires that we  define exposure conditions (which\ninclude assignment to treatment and also the probability of exposure to a\ntreatment via the network). We define the exposure conditions with respect to\nwhat a researcher would be able to observe from the experimental design,\nassuming the network were observed, and the response. Importantly, in defining\nthe exposure conditions of interest, we assume that the researcher does not\nexactly know the set of vertices to which the treatment has propagated. We see\nthis as more realistic than a situation in which the researcher knows exactly\nwhere the treatment has propagated.\n\nWe define the following three distinct exposure conditions, differentiating between those vertices treated initially ($d_1$), those vertices that are untreated initially and are adjacent to at least one treated vertex ($d_{(0,1)}$), and those vertices that are untreated initially and are not adjacent to any treated vertices ($d_{(0,0)}$).\n \\begin{itemize}\n  \t    \\item $d_1 \\equiv D_i(Z_i=1,0 \\le m_{it} \\ge M)$\n\t    \\item $d_{(0,0)} \\equiv D_i(Z_i=0,m_{it}=0)$\n\t    \\item $d_{(0,1)} \\equiv D_i(Z_i=0,m_{it} \\ge 1)$\n\t    \\end{itemize}\nFigure \\ref{fig:exposure} gives a visual example of a subgraph drawn from the Ghana road network. In the study of the \\citet{Aronow2013} methods, we focus on identifying the difference between the $d_{(0,1)}$ and $d_{(0,0)}$ conditions.\n\n\\begin{figure}[htp]\n\\begin{center}\n\\includegraphics[width=.7\\textwidth]{exposure-example.pdf}\n\\end{center}\n\\caption{Illustration of exposure conditions.  One draw from the Ising propagations with  $\\text{pr}(Z_i=1) \\sim \\text{Bernoulli}(.15)$\n  for $t \\in \\{0,1\\}$ and Temperature$=10$.}\n\\label{fig:exposure}\n\\end{figure}\n\nThe \\citet{Aronow2013} estimand is the difference between vertices in two exposure conditions. First, let\n  \n", "index": 3, "text": "$$ \\hat{\\mu}(d_k) = \\frac{1}{n} \\sum_{i=1}^n \\text{\\bf I}(D_i = d_k) \\frac{Y_i(d_k)}{\\pi_i(d_k)},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\mu}(d_{k})=\\frac{1}{n}\\sum_{i=1}^{n}\\text{\\bf I}(D_{i}=d_{k})\\frac{Y_{i}%&#10;(d_{k})}{\\pi_{i}(d_{k})},\" display=\"block\"><mrow><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>\ud835\udc08</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mfrac><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c0</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00992.tex", "nexttext": "\n\nFollowing Horvitz and Thompson (1952), \\citet{Aronow2013} show that \\\\\n  $\\text{Var}( \\hat{\\tau}(d_{(0,1)}, d_{(0,0)}) ) \\le 1/N^2 \\left( \\text{Var}(\\hat{\\mu}(d_{(0,1)})) + \\text{Var}( \\hat{\\mu}(d_{(0,0)})) \\right) $. In the interest of brevity, we do not re-produce the full variance estimators, but refer readers to \\citet{Aronow2013}. A power analysis gauges the probability that a true hypothesis would be rejected against different alternatives. In the results that follow, we set the truth as $\\tau=0$ and thus assess power to reject $H_0: \\tau=0$ at the 0.05 significance level.\n\n\n We test hypotheses about effects following  \\citet{Bowers:2013} using the Anderson-Darling $k$-sample test statistic \\citet{Scholz1987} to compare the outcome distributions of vertices in the three different exposure conditions.\\footnote{Randomization distributions are simulated using a development version of the \\texttt{RItools} package for {\\textnormal{\\sffamily\\bfseries R}} \\citep{Bowers2014}.} Following the norms of assessing power against truth, we test hypotheses generated by the correct Ising model record the amount of false rejections as the parameter values of $\\lambda$ and $F$ move away from their true values. The values of the parameters $\\lambda$ and $F$ are manipulated to test the null of no effects (i.e., $\\lambda =1, F = \\infty$) in order to assess power. In the case of the methods proposed by \\citet{Bowers:2013}, we assess both the power against the null of no effects across all three exposure conditions and the null of no difference between $d_{(0,1)}$ and $d_{(0,0)}$.\n\n\\subsection{Results}\n\nThe results from the \\citet{Aronow2013} tests are reported in Figure \\ref{fig:aranowsamiipower}. Three distinct sets of results are presented: (a) those with a multiplicative effect and stochastic propagation (i.e., $Y(1,0) = Y(0,1) = \\lambda Y(0,0)$), (b) those with an additive effect (i.e., $Y(1,0) = Y(0,1) = \\lambda + Y(0,0)$), and (c) those with an additive effect and certain propagation (i.e., $Y(0,1)= Y(d_{(0,1)})$). We note three characteristics of our results. First, the tests exhibit fairly low power overall, hitting a maximum of approximately 0.8, but sitting below and often far below 0.5. Second, the most powerful design is that in which $\\alpha=0.05$, the smallest proportion assigned to initial treatment.  Third, as indicated by the relationship of power with the $x$-axis of the plots in panels (a) and (b), the larger the sample of vertices in the $d_{(0,1)}$ condition that are actually exposed to treatment in period one, as governed by the temperature parameter, the more powerful the tests.\n\n\n  \\begin{figure}[H]\n\\begin{center} \\vspace{-.7cm}\n    \\begin{tabular}{cc}\n      {$\\lambda$ = 0.26} & {$\\lambda$ = 0.63} \\\\\n            \\multicolumn{2}{c}{(a) Multiplicative Effects  ($Y(1,0) = Y(0,1) = \\lambda Y(0,0)$)}\\\\\n      \\includegraphics[scale=.575]{power05tau26} & \\includegraphics[scale=.575]{power05tau63}\\\\\n      \\multicolumn{2}{c}{(b) Additive Effects  ($Y(1,0) = Y(0,1) = \\lambda + Y(0,0)$)}\\\\\n      \\includegraphics[scale=.575]{power05tau26Add} & \\includegraphics[scale=.575]{power05tau63Add}\\\\\n        \\multicolumn{2}{c}{(c) Perfect Propagation ( $Y(0,1) = Y(d_{0,0})$)}\\\\\n      \\includegraphics[scale=.575]{power05tau26AddC} & \\includegraphics[scale=.575]{power05tau63AddC}\\\\\n    \\end{tabular}\n    \\end{center} \\vspace{-1cm}\n    \\caption{Power Results: \\citet{Aronow2013} test. Lines are labeled by the proportion assigned to treatment. High power depends on low proportions assigned to treatment. The network contains a total of 868 nodes.}\n    \\label{fig:aranowsamiipower}\n\\end{figure}\n\nIf a researcher wants to estimate the network exposure weighted average causal effect developed by  \\citet{Aronow2013} these kinds of results raise the question about whether the best approach to randomization of treatment assignment is simple uniform assignment without any blocking or use of information about the fixed network. To investigate this we plot the correlation between $m$ and $Z_{i0}$ (i.e., the correlation between vertex degree and the initial treatment assignment). The networks depicted in Figure \\ref{fig:networks} represent examples of treatment assignments biased in favor of high degree vertices (a) and low degree vertices (b). The degree-treatment correlation results are given in Figure \\ref{fig:degcor}. We can see that at each $\\alpha$ value there is a strong positive relationship between the degree-treatment correlation and statistical power. This indicates that for the Ghana network structure and the propagation/effects models we have specified, designs that bias treatment assignment towards higher degree nodes would exhibit greater statistical power. This finding generalizes what we know about the use of prognostic background covariates to increase power in randomized experiments to the situation where network degree can be thought of as a moderator of treatment effects.\n\n\\begin{figure}[H]\n\\begin{center}\n(a) Treating High Centrality Nodes \\\\ \\vspace{-.3cm}\n \\includegraphics[scale=.9,clip=true,trim=3cm 2cm 2cm 2cm]{networkHiEC} \\\\  \\vspace{-1cm}\n (b) Treating Low Centrality Nodes \\\\ \\vspace{-.3cm}\n  \\includegraphics[scale=.9,clip=true,trim=3cm 2cm 2cm 2cm]{networkLoEC} \\\\\n \\vspace{-1.5cm}\n  \\end{center}\n \\caption{Examples of treatment assignment to either high or low degree nodes in the Ghana 2008 Voter Registration Fraud Experiment network. Vertices are registration stations and yellow nodes are assigned to initial treatment.}\n \\label{fig:networks}\n\\end{figure}\n\n\n\\begin{figure}[htp]\n\\begin{center}\n\\includegraphics[scale=.95]{dcorPower}\n\\end{center}\n\\caption{When node degree is correlated with probability of treatment assignment, the power to detect indirect effects increases.}\n \\label{fig:degcor}\n\\end{figure}\n\nThe results from the model-testing methods of \\citet{Bowers:2013} in Figures \\ref{fig:bowersmult}--\\ref{fig:bowerscert} differ in their implications from the results based on estimation. First, the power in these tests is much higher overall. This makes some sense: we are using the correct propagation model and only varying parameters, a good test should eventually reject values of parameters that are distant from the truth. The methods focused on differences of averages do not include much information about the propagation model except as a weight arising indirectly from what we can observe, so those procedures have less information to use in statistical inference in this simulation study.  Second, when we focus on the test of the sharp null of no effects when all three exposure conditions are included, we see that the low values of $\\alpha$ exhibit low power. However, when the test  compares only $d_{(0,1)}$ against $d_{(0,0)}$, which can be considered a test for propagation effects (i.e., excluding directly treated vertices), the lower $\\alpha$ designs perform better. This tradeoff between the power to detect any effect versus the power to detect propagated effects replicate the findings from  \\citet{Bowers:2013}: in order to learn about propagation, one should assign relatively few nodes to treatment, in order to test an overall null of no effects, then more power arises from more directly assigned to treatment nodes.  There is also a positive association between the number of vertices in the $d_{(0,1)}$ condition that are exposed to treatment by period one and the power of the tests. This last result arises from the fact that, when few in the $d_{(0,1)}$ condition are exposed to treatment, the $d_{(0,1)}$ outcomes are, in large part, equivalent to the $d_{(0,0)}$ outcomes.\n\n    \\begin{figure}[h!]\n    \\begin{center}\n     \\includegraphics[width=.9\\textwidth]{power05sharp.pdf}\n     \\end{center}\n     \\caption{Power with multiplicative effects model and true Ising propagation model following  \\citet{Bowers:2013} .}\n     \\label{fig:bowersmult}\n    \\end{figure}\n\n      \\begin{figure}[h!]\n    \\begin{center}\n     \\includegraphics[width=.9\\textwidth]{power05sharp-cadd.pdf}\n     \\end{center}\n     \\caption{Power with additive effects model and true Ising propagation model following  \\citet{Bowers:2013} .}\n     \\label{fig:bowersadd}\n    \\end{figure}\n\n          \\begin{figure}[!h]\n    \\begin{center}\n     \\includegraphics[width=.9\\textwidth]{power05sharp-caddC.pdf}\n     \\end{center}\n     \\caption{Power with additive effects and certain propagation and true Ising propagation model following  \\citet{Bowers:2013}.}\n     \\label{fig:bowerscert}\n    \\end{figure}\n\n    \\subsection{Summary}\n\n    We have shown that learning about a simple model of propagation of\n    treatment through a network (via comparisons of nodes possibly indirectly\n    exposed to the treatment to nodes not exposed to treatment) can be enhanced\n    when relatively few of the nodes in a network are assigned treatment. And\n    our analysis acts an example for those desiring to evaluate their own\n    models of propagation and networks and experimental designs before going\n    into the field. We have not shown how the \\citet{Bowers:2013} approach\n    performs when an incorrect model is assessed. We did not do this because we\n    are focusing on design --- and power analysis requires that we create a\n    truth against which to compare alternatives. \\citet{Bowers:2013} show that,\n    in the analysis stage, hypothesis tests may have very low or no power if\n    the model being assessed has no bearing on the underlying mechanism, but we\n    are not certain what kind of design advice would follow from such findings\n    --- merely increasing the size of a fixed network may be impossible. We have also not considered how design affects statistical power when there is uncertainty regarding the network structure. To incorporate uncertainty regarding the network a stochastic model for the network would need to be integrated into the analytical procedure(s) \\citep[e.g., ][]{desmarais2012}.\n\n  \n\\section{Conclusion}\n\n\nWe describe the challenges in experimental design that arise when the researcher is interested in studying the process of propagation on a network with the objective of drawing causal inference. The experimental designs that work most effectively in experiments in which there is no interference are unlikely to be directly transferable to experimental research on propagation. We review two recently developed frameworks for statistical inference regarding interference in networks. One commonality we draw from these two frameworks is that theoretical analytic models of propagation play a key role in their application, which means that theory features more prominently in the statistical analysis of experimental data generated for the study of propagation than in the classical, non-interference, experimental framework.\n\nWe present a simulation study to (1) illustrate how simulation can be a useful guide in identifying design parameters for experimental studies of interference and (2) study the properties of the two frameworks for statistical inference presented in the front end of the paper. Three broad findings from the simulation study are notable. First, statistical power depends upon design parameters and, for example, the optimal proportion assigned to initial treatment may be much lower than the conventional 0.5 that is found in the non-interference experimental literature. Second, the relationship between design parameters and power depends upon the framework for statistical inference. Third, our results regarding the positive relationship between the degree-treatment correlation and power indicates that randomization designs that take into account graph topology are likely to exhibit substantial power gains over uniform randomization designs. Taken together, the results from the simulation study underscore the importance of considering design parameters for experimental studies of propagation, as the standards of the classical experimental framework are unlikely to apply.\n\n\n\n\n\n\\bibliographystyle{apa}\n\\bibliography{NetExperimentsSAMSI}\n\n\n\n", "itemtype": "equation", "pos": 25886, "prevtext": "\n be the estimator of the mean potential outcome among vertices in exposure condition $d_k$, where $\\pi_i(d_k)$ is the probability that vertex $i$ ends up in condition $d_k$. Then the estimator of the difference between potential outcomes in the two exposure conditions is\n\n  \n", "index": 5, "text": "$$ \\hat{\\tau}(d_{(0,1)}, d_{(0,0)}) = \\hat{\\mu}(d_{(0,1)})- \\hat{\\mu}(d_{(0,0)}).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\tau}(d_{(0,1)},d_{(0,0)})=\\hat{\\mu}(d_{(0,1)})-\\hat{\\mu}(d_{(0,0)}).\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>\u03c4</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>,</mo><msub><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]