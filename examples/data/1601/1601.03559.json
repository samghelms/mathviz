[{"file": "1601.03559.tex", "nexttext": "\nHere ${\\bf x}(t)$ is a vector with components $x_i(t)$, which denote\nthe concentration $x_i$ of protein ${\\rm X}_i$, $s(t)$ is the\nconcentration of the clock protein, ${\\bf f}$ is a vector with\ncomponents $f_i$, which describe how the downstream protein ${\\rm\n  X}_i$ is driven by $s(t)$, ${\\bf B}$ is the matrix that describes\nthe regulatory interactions between the downstream proteins, and\n$\\xi(t)$ is a vector with components $\\xi_i(t)$ that describe the\nnoise in the expression of ${\\rm X}_i$. In what follows, we imagine\nthat the clock protein osscilates according to $s(t) = A_s \\sin(\\omega\nt) + r_s + \\xi_s$, where $A_s$ sets the amplitude of the oscillations,\n$r_s$ its mean, and $\\xi_s$ describes the noise in the input signal.\n\nThis linear system can be solved analytically. For example, if the downstream\ngenes do not interact with each other and protein ${\\rm X}_i$ decays\nwith rate $\\mu_i$, then each protein oscillates as\n\n", "itemtype": "equation", "pos": 7316, "prevtext": "\n\n\n\\title{The accuracy of telling time via oscillatory signals} \n\n\\author{Michele Monti}\n\\email{monti@amolf.nl}\n\\affiliation{FOM Institute AMOLF, Science Park 104, 1098 XG Amsterdam,\n  The Netherlands}\n\\author{Pieter Rein ten Wolde}\n\\email{tenwolde@amolf.nl}\n\\affiliation{FOM Institute AMOLF, Science Park 104, 1098 XG Amsterdam, The Netherlands}\n\n\\date{\\today}                                           \n\n\\begin{abstract}\n  Circadian clocks are the central timekeepers of life, allowing cells\n  to anticipate changes between day and night. Experiments in recent\n  years have revealed that circadian clocks can be highly stable,\n  raising the question how reliably they can be read out. Here, we\n  combine mathematical modeling with information theory to address the\n  question how accurately a cell can infer the time from an ensemble\n  of protein oscillations, which are driven by a circadian clock. We\n  show that the precision increases with the number of oscillations\n  and their amplitude relative to their noise. Our analysis also\n  reveals that their exists an optimal phase relation that minimizes\n  the error in the estimate of time, which depends on the relative\n  noise levels of the protein oscillations. Lastly, our work shows\n  that cross-correlations in the noise of the protein oscillations can\n  enhance the mutual information, which suggests that cross-regulatory\n  interactions between the proteins that read out the clock can be\n  beneficial for temporal information transmission.\n\\end{abstract}\n\n\\pacs{\n87.10.Vg,     \n87.16.Xa,     \n87.18.Tt                        \n}\n\n\\maketitle\n\n\n\\section*{Introduction}\nAmong the most fascinating timing devices in biology are circadian\nclocks, which are found in organisms ranging from cyanobacteria and\nfungi, to plants, insects and animals. Circadian clocks are\nbiochemical oscillators that allow organisms to coordinate their\nbehavior with the 24-hour cycle of day and night. Remarkably, these\nclocks can maintain stable rhythms for months or even years in the\nabsence of any daily cue from the environment, such as light/dark or\ntemperature cycles \\cite{Johnson2008}. In multicellular organisms, the\nrobustness can be explained by intercellular interactions\n\\cite{Liu:1997uv,Yamaguchi:2003jj}, but it is now known that even\nunicellular organisms can have very stable rhythms. An excellent\nexample is provided by the clock of the bacterium {\\it Synechococcus\n  elongatus}, which is one of the most studied and best characterized\nmodel systems \\cite{Johnson2008}. This clock has a correlation time of\nseveral months \\cite{Mihalcescu:2004ch}, even though the clocks of the\ndifferent cells in the population do not seem to interact with one\nanother \\cite{Mihalcescu:2004ch}. Clearly, the clock is designed in\nsuch a way that it has become resilient against the intrinsic\nstochasticity of the chemical reactions that constitute the clock\n\\cite{Zwicker2010,Paijmans2015}. The observation that clocks can be very\nstable, suggests that they are also read out reliably. Yet, how\ncells could do so is a wide open question \\cite{Mugler:2010cq}.\n\nIn this manuscript we combine information theory with mathematical\nmodeling to study how accurately cells can infer time from cellular\noscillators. While our analysis is general,  it is inspired by the\ncircadian clock of\n{\\it S. elongatus}. The central clock component of {\\it S. elongatus} is KaiC,\nwhich forms a hexamer \\cite{Kageyama2003}. KaiC has two\nphosphorylation sites per monomer, which are phosphorylated and\ndephosphorylated in a well-defined temporal order, yielding a\nprotein-phosphorylation cycle (PPC) with a 24 hour period\n\\cite{Rust2007,Nishiwaki2007}.  This PPC is coupled to a\ntranscription-translation cycle (TTC) of KaiC \\cite{Kitayama2008},\nwhich is a protein synthesis cycle with a 24 hr rhythm, via the\nresponse regulator RpaA.  KaiC in the phosphorylation phase of the PPC\nactivates the histidine kinase SasA, which in turn activates RpaA via\nphosphorylation\n\\cite{Takai2006,Taniguchi:2007jx,Taniguchi2010,Gutu2013}. In contrast,\nKaiC that is in the dephosphorylation phase of the PPC and bound to\nKaiB, activates the phosphatase CikA, which dephosphorylates and\ndeactivates RpaA \\cite{Taniguchi2010,Gutu2013}.  Active,\nphosphorylated RpaA drives genome-wide transcriptional rhythms, which\ninclude the expression of the clock components\n\\cite{Markson2013}. \n\nIntriguingly, while time could be uniquely encoded in the modification\nstate of the two phosphorylation sites of KaiC, cells do not seem to\nemploy this mechanism \\cite{Gutu2013,Markson2013}. RpaA, the central\nnode between the clock and the downstream genes, has only one\nphosphorylation site \\cite{Gutu2013,Markson2013}. This makes the\nquestion how accurately the cell can infer time a very pertinent one,\nbecause a single readout---the phoshorylation level of RpaA---leads to\nan inherent ambiguity in the mapping between time and clock output: a\ngiven level of active RpaA corresponds to two possible times (see\nFig. \\ref{fig:fig1}). On the other hand, it is known that RpaA controls the\nexpression of many downstream genes \\cite{Markson2013}. While\ntheir expression levels cannot contain more information about time\nthan that which is available in the {\\it time trace} of RpaA, it is\npossible that, collectively, their expression levels do contain more\ninformation about time than that present in the {\\it instantaneous} level of\nRpaA.\n\nIn this manuscript, we study  how the accuracy of telling time depends on the\nnumber of genes that read out a clock, their phase difference, the\nlevel of biochemical noise, and the cross-correlations between the\ngene expression levels. In the next section, we first describe the set\nup of our analysis, and then the measures that we employ to quantify\ninformation transmision. We then show that there exists an optimal\nphase difference that maximizes information\ntransmission. Interestingly, the optimal phase difference depends on\nthe amplitude of the noise in the expression of the readout genes, and\non the cross-correlations between them, akin to what has been observed\nin neuronal coding \\cite{Tkacik2010} and in the gap-gene expression\nsystem of {\\it Drosophila} \\cite{Walczak:2010cv,Dubuis2013}. \n\n\n\\section{Methods}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.5]{draw1.pdf}\n    \\caption{Cells can infer time from an ensemble of protein\n      oscillations. The day-night rhythm entrains a circadian clock, a\n      biochemical oscillator with a 24 hr period, which in turn drives\n      the oscillatory expression of a number of readout genes. The\n      precision of the estimate of time depends on the number of\n      readout proteins, the amplitude of the\n      oscillations, their noise level, their phase difference, and the\n      cross-correlations in the fluctuations.}\n    \\label{fig:fig1}\n\\end{figure}\n\n\\subsection{Model}\n\\label{sec:Model}\nThe analysis we present below applies to any readout system that obeys\nGaussian statistics. Yet, to set the stage, and to introduce the key\nquantities that we will study, it is instructive to consider a\nconcrete system. To this end, imagine an oscillatory clock protein,\nlike RpaA, that drives the expression of a set of downstream genes. Assuming that the system can be linearized, the dynamics of the\nsystem is given by\n\n", "index": 1, "text": "\\begin{equation}\n\\label{Eq1}\n\\frac{d{\\bf x}(t)}{dt} = {\\bf f}s(t) + {\\bf B}{\\bf x}(t) + \\xi (t).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\frac{d{\\bf x}(t)}{dt}={\\bf f}s(t)+{\\bf B}{\\bf x}(t)+\\xi(t).\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mrow><mi>\ud835\udc1f</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udc01\ud835\udc31</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03be</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nwhere\n\\begin{eqnarray}\n\\eta_i(t) &=& \\int_{-\\infty}^t dt' e^{-\\mu_i(t-t')}\n\\left[\\xi_i(t')+f_i \\xi_s(t')\\right],\\label{eq:eta}\\\\\nA_i &=& \\frac{f_i A_s}{\\sqrt{\\mu_i^2 + \\omega^2}}, \\label{eq:Amp}\\\\ \n\\phi_i &=&\\arcsin(\\frac{-\\omega}{\\sqrt{\\mu_i^2 + \\omega^2}}),\\label{eq:phase}\\\\\nr_i &=& \\frac{f_ir_s}{\\mu_i}.\n\\end{eqnarray}\n  Importantly, even in this\nsimple system, the difference in the phase $\\phi$ between the\nexpression of the downstream genes can be modulated, namely by\nchanging the protein degradation rate $\\mu_i$. Also the amplitude\n$A_i$ can be adjusted; it can be set independently from the phase via\nthe synthesis rate $f_i$. Both quantities affect the precision by\nwhich the system can estimate the time.\n\nAnother key quantity is the noise in the expression of the downstream genes.\nFollowing the linear-noise approximation, we assume that the noise in\nthe concentration $x_i$ is Gaussian, such that \n\n", "itemtype": "equation", "pos": 8365, "prevtext": "\nHere ${\\bf x}(t)$ is a vector with components $x_i(t)$, which denote\nthe concentration $x_i$ of protein ${\\rm X}_i$, $s(t)$ is the\nconcentration of the clock protein, ${\\bf f}$ is a vector with\ncomponents $f_i$, which describe how the downstream protein ${\\rm\n  X}_i$ is driven by $s(t)$, ${\\bf B}$ is the matrix that describes\nthe regulatory interactions between the downstream proteins, and\n$\\xi(t)$ is a vector with components $\\xi_i(t)$ that describe the\nnoise in the expression of ${\\rm X}_i$. In what follows, we imagine\nthat the clock protein osscilates according to $s(t) = A_s \\sin(\\omega\nt) + r_s + \\xi_s$, where $A_s$ sets the amplitude of the oscillations,\n$r_s$ its mean, and $\\xi_s$ describes the noise in the input signal.\n\nThis linear system can be solved analytically. For example, if the downstream\ngenes do not interact with each other and protein ${\\rm X}_i$ decays\nwith rate $\\mu_i$, then each protein oscillates as\n\n", "index": 3, "text": "\\begin{equation} \\label{xybeh}\n\tx_i(t) = A_i \\sin (\\omega t +  \\phi_i) + r_i + \\eta_i(t),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"x_{i}(t)=A_{i}\\sin(\\omega t+\\phi_{i})+r_{i}+\\eta_{i}(t),\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c9</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>+</mo><msub><mi>\u03d5</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mi>r</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>\u03b7</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nwhere $\\bar{x}_i(t^*)$ is the mean concentration of protein ${\\rm\n  X}_i$ at time $t^*$, $\\sigma^2_{i}=\\sigma^2_i(t^*)$ is the variance\nof $x_i$ around its mean $\\bar{x}_i$, and $t^*$ is the given time. The\nnoise $\\sigma^2_i(t)$ has a extrinsic contribution coming from the\nnoise in the input signal, an intrinsic contribution from the noise in\nthe expression of $X_i$, and a contribution from the regulatory\ninteractions. Our analysis does not depend on the origins of these\nnoise contributions: in the analysis below, we specify the variance\n$\\sigma^2_i(t)$ and the co-variance of the fluctuations in $x_i$ and\n$x_j$, and then study how this affects the precision of telling time.\nIn general, we expect, however, that $\\sigma^2_i(t)$ depends on the\nmean $\\bar{x}_i(t)$, and a reasonable assumption is that the variance\nequals the mean, $\\sigma^2_i(t) = \\bar{x}_i (t)$, as in gene\nexpression via simple Poissonian birth-death statistics \\cite{paulsson2003}. However, if the mean $r_i$ of the\nprotein oscillations is large compared to their amplitude $A_i$, then\nwe may assume that $\\sigma^2_i(t)$ is constant in time, $\\sigma^2_i(t)\n= \\sigma^2_i=r_i$. As will become clear in the next section, the\nimportance of noise depends on the amplitude of the oscillations: the\nkey control parameter is the relative noise strength\n$\\widetilde{\\sigma}_i \\equiv A_i / \\sigma_i$.  This ratio can be\nvaried independently from gene to gene, $A_i/\\sigma_i \\neq A_j /\n\\sigma_j$ in general, and below we will study how this affects the\nprecision.  If there is no noise in the input $s(t)$ and if the\ndownstream proteins do not interact with each other (as in the example\nconsidered here), then the cross-correlation between the fluctuations\nof the concentrations of the downstream proteins is zero: $\\langle\n\\eta_i \\eta_j\\rangle = \\langle \\eta^2_i \\rangle\n\\delta_{ij}=\\sigma^2_{i}$, where $\\delta_{ij}$ is the Kronecker\ndelta. However, in general, the noise in the expression of the\ndownstream genes will be correlated, which, as we will show, can\neither enhance or reduce the accuracy by which the downstream proteins\ncan infer time.\n\nBelow, we will consider how the accuracy of telling time depends on\nthe cross-correlations between the expression of the downstream genes,\ntheir phase difference, and on $\\widetilde{\\sigma}_i$, and how this varies\nfrom gene to gene. \n\n\\subsection{Reliability measures} \nThe central idea of our analysis is that the system infers the time\nfrom the collective expression of the $N$ downstream proteins,\n$\\{x_i\\}\\equiv \\{x_1(t), x_2(t), \\dots, x_{N-1}(t),\nx_N(t)\\}$. Following work on positional information in {\\it\n  Drosophila} \\cite{Dubuis2013}, we use two approaches to quantify the\naccuracy on telling time. The first is based on the error in the\nestimate of a given time $t$, $\\sigma_t(t)$; a related approach has been widely\nused to derive the fundamental limits on the accuracy of sensing\n\\cite{berg1977,Ueda:2007uq,bialek2005,levinepre2007,levineprl2008,wingreen2009,levineprl2010,mora2010,Govern2012,Mehta2012,Skoge:2011gi,Skoge:2013fq,Kaizu:2014eb,Govern:2014ef,Govern:2014ez,Lang:2014ir}. The\nsecond approach is based on the mutual information, which in recent years has\nbeen used extensively to quantify cellular information transmission\n\\cite{Ziv2007,Tostevin2009,Mehta2009,Tkacik:2009ta,tostevin10,DeRonde2010,Tkacik2010,Walczak:2010cv,DeRonde2011,Cheong:2011jp,deRonde:2012fs,Dubuis2013,Bowsher:2013jh,Selimkhanov:2014gd,DeRonde:2014fq,Govern:2014ez,Sokolowski:2015km,Becker2015}.\n\n\n\\subsubsection{The error in estimating time}\nTo determine the error in estimating the time, we start from the\ngeneralization of Eq. \\ref{noiseterm} to multiple downstream genes: \n\n", "itemtype": "equation", "pos": 9384, "prevtext": "\nwhere\n\\begin{eqnarray}\n\\eta_i(t) &=& \\int_{-\\infty}^t dt' e^{-\\mu_i(t-t')}\n\\left[\\xi_i(t')+f_i \\xi_s(t')\\right],\\label{eq:eta}\\\\\nA_i &=& \\frac{f_i A_s}{\\sqrt{\\mu_i^2 + \\omega^2}}, \\label{eq:Amp}\\\\ \n\\phi_i &=&\\arcsin(\\frac{-\\omega}{\\sqrt{\\mu_i^2 + \\omega^2}}),\\label{eq:phase}\\\\\nr_i &=& \\frac{f_ir_s}{\\mu_i}.\n\\end{eqnarray}\n  Importantly, even in this\nsimple system, the difference in the phase $\\phi$ between the\nexpression of the downstream genes can be modulated, namely by\nchanging the protein degradation rate $\\mu_i$. Also the amplitude\n$A_i$ can be adjusted; it can be set independently from the phase via\nthe synthesis rate $f_i$. Both quantities affect the precision by\nwhich the system can estimate the time.\n\nAnother key quantity is the noise in the expression of the downstream genes.\nFollowing the linear-noise approximation, we assume that the noise in\nthe concentration $x_i$ is Gaussian, such that \n\n", "index": 5, "text": "\\begin{equation}\\label{noiseterm}\nP(\\eta_i) = P(x_i|t^*) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{i}^2}}  e^ \n{-\\frac{({x_i}-\\bar{x}_i(t^*))^2}{2 \\sigma_{i}^2}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"P(\\eta_{i})=P(x_{i}|t^{*})=\\frac{1}{\\sqrt{2\\pi\\sigma_{i}^{2}}}e^{-\\frac{({x_{i%&#10;}}-\\bar{x}_{i}(t^{*}))^{2}}{2\\sigma_{i}^{2}}}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b7</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msup><mi>t</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mfrac><msup><mi>e</mi><mrow><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nHere $ \\delta x_i (t)= x_i(t) -\\bar{x_i} $, ${\\bf C}$ is the covariance\nmatrix with elements $C_{ij}$, $|{\\bf C}|$ is its determinant and\n${\\bf C}^{-1}$ is its inverse.\n\nThe idea is now to invert the problem, and ask what is the\ndistribution of possible times $t$, given that the expression levels\nare $\\{x_i\\}$. This can be obtained from Bayes' rule:\n\n", "itemtype": "equation", "pos": 13244, "prevtext": "\nwhere $\\bar{x}_i(t^*)$ is the mean concentration of protein ${\\rm\n  X}_i$ at time $t^*$, $\\sigma^2_{i}=\\sigma^2_i(t^*)$ is the variance\nof $x_i$ around its mean $\\bar{x}_i$, and $t^*$ is the given time. The\nnoise $\\sigma^2_i(t)$ has a extrinsic contribution coming from the\nnoise in the input signal, an intrinsic contribution from the noise in\nthe expression of $X_i$, and a contribution from the regulatory\ninteractions. Our analysis does not depend on the origins of these\nnoise contributions: in the analysis below, we specify the variance\n$\\sigma^2_i(t)$ and the co-variance of the fluctuations in $x_i$ and\n$x_j$, and then study how this affects the precision of telling time.\nIn general, we expect, however, that $\\sigma^2_i(t)$ depends on the\nmean $\\bar{x}_i(t)$, and a reasonable assumption is that the variance\nequals the mean, $\\sigma^2_i(t) = \\bar{x}_i (t)$, as in gene\nexpression via simple Poissonian birth-death statistics \\cite{paulsson2003}. However, if the mean $r_i$ of the\nprotein oscillations is large compared to their amplitude $A_i$, then\nwe may assume that $\\sigma^2_i(t)$ is constant in time, $\\sigma^2_i(t)\n= \\sigma^2_i=r_i$. As will become clear in the next section, the\nimportance of noise depends on the amplitude of the oscillations: the\nkey control parameter is the relative noise strength\n$\\widetilde{\\sigma}_i \\equiv A_i / \\sigma_i$.  This ratio can be\nvaried independently from gene to gene, $A_i/\\sigma_i \\neq A_j /\n\\sigma_j$ in general, and below we will study how this affects the\nprecision.  If there is no noise in the input $s(t)$ and if the\ndownstream proteins do not interact with each other (as in the example\nconsidered here), then the cross-correlation between the fluctuations\nof the concentrations of the downstream proteins is zero: $\\langle\n\\eta_i \\eta_j\\rangle = \\langle \\eta^2_i \\rangle\n\\delta_{ij}=\\sigma^2_{i}$, where $\\delta_{ij}$ is the Kronecker\ndelta. However, in general, the noise in the expression of the\ndownstream genes will be correlated, which, as we will show, can\neither enhance or reduce the accuracy by which the downstream proteins\ncan infer time.\n\nBelow, we will consider how the accuracy of telling time depends on\nthe cross-correlations between the expression of the downstream genes,\ntheir phase difference, and on $\\widetilde{\\sigma}_i$, and how this varies\nfrom gene to gene. \n\n\\subsection{Reliability measures} \nThe central idea of our analysis is that the system infers the time\nfrom the collective expression of the $N$ downstream proteins,\n$\\{x_i\\}\\equiv \\{x_1(t), x_2(t), \\dots, x_{N-1}(t),\nx_N(t)\\}$. Following work on positional information in {\\it\n  Drosophila} \\cite{Dubuis2013}, we use two approaches to quantify the\naccuracy on telling time. The first is based on the error in the\nestimate of a given time $t$, $\\sigma_t(t)$; a related approach has been widely\nused to derive the fundamental limits on the accuracy of sensing\n\\cite{berg1977,Ueda:2007uq,bialek2005,levinepre2007,levineprl2008,wingreen2009,levineprl2010,mora2010,Govern2012,Mehta2012,Skoge:2011gi,Skoge:2013fq,Kaizu:2014eb,Govern:2014ef,Govern:2014ez,Lang:2014ir}. The\nsecond approach is based on the mutual information, which in recent years has\nbeen used extensively to quantify cellular information transmission\n\\cite{Ziv2007,Tostevin2009,Mehta2009,Tkacik:2009ta,tostevin10,DeRonde2010,Tkacik2010,Walczak:2010cv,DeRonde2011,Cheong:2011jp,deRonde:2012fs,Dubuis2013,Bowsher:2013jh,Selimkhanov:2014gd,DeRonde:2014fq,Govern:2014ez,Sokolowski:2015km,Becker2015}.\n\n\n\\subsubsection{The error in estimating time}\nTo determine the error in estimating the time, we start from the\ngeneralization of Eq. \\ref{noiseterm} to multiple downstream genes: \n\n", "index": 7, "text": "\\begin{equation}\nP(\\{x_i\\}|t) = \\frac{1}{\\sqrt{2 \\pi |{\\bf  C}|}} \\exp\\left[-\\frac{1}{2}\\sum_{i,j}^N \\delta x_i  C^{-1}_{ij} \\delta x_j\\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"P(\\{x_{i}\\}|t)=\\frac{1}{\\sqrt{2\\pi|{\\bf C}|}}\\exp\\left[-\\frac{1}{2}\\sum_{i,j}^%&#10;{N}\\delta x_{i}C^{-1}_{ij}\\delta x_{j}\\right].\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mi>\ud835\udc02</mi><mo stretchy=\"false\">|</mo></mrow></mrow></msqrt></mfrac><mi>exp</mi><mrow><mo>[</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mi>\u03b4</mi><msub><mi>x</mi><mi>i</mi></msub><msubsup><mi>C</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi>\u03b4</mi><msub><mi>x</mi><mi>j</mi></msub><mo>]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nwhere $P(t)=\\frac{1}{T}$ is the uniform prior probability of having a\ncertain time and $P(\\{x_i\\})$ is the joint distribution of the\nexpression levels of the downstream genes.  If the noise $\\eta$ is\nsmall compared to the mean, then $P(t|\\{x_i\\})$ will be a Gaussian distribution that is\npeaked around $t^*(\\{x_i\\})$, which is the best estimate of the\ntime given the expression levels\n \\cite{Tkacik2011,Dubuis2013}:\n\n", "itemtype": "equation", "pos": 13755, "prevtext": "\nHere $ \\delta x_i (t)= x_i(t) -\\bar{x_i} $, ${\\bf C}$ is the covariance\nmatrix with elements $C_{ij}$, $|{\\bf C}|$ is its determinant and\n${\\bf C}^{-1}$ is its inverse.\n\nThe idea is now to invert the problem, and ask what is the\ndistribution of possible times $t$, given that the expression levels\nare $\\{x_i\\}$. This can be obtained from Bayes' rule:\n\n", "index": 9, "text": "\\begin{equation}\\label{Ba_ru}\nP( t |\\{x_i\\}) = P(t) \\frac{P(\\{x_i\\}|t)}{P(\\{x_i\\})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"P(t|\\{x_{i}\\})=P(t)\\frac{P(\\{x_{i}\\}|t)}{P(\\{x_{i}\\})}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mfrac><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nHere $\\sigma^2_t=\\sigma^2_t(t*)$ is the variance in the estimate of the\ntime, and it is given by \\cite{Dubuis2013}\n\n", "itemtype": "equation", "pos": 14270, "prevtext": "\nwhere $P(t)=\\frac{1}{T}$ is the uniform prior probability of having a\ncertain time and $P(\\{x_i\\})$ is the joint distribution of the\nexpression levels of the downstream genes.  If the noise $\\eta$ is\nsmall compared to the mean, then $P(t|\\{x_i\\})$ will be a Gaussian distribution that is\npeaked around $t^*(\\{x_i\\})$, which is the best estimate of the\ntime given the expression levels\n \\cite{Tkacik2011,Dubuis2013}:\n\n", "index": 11, "text": "\\begin{equation}\nP(t|\\{x_i\\})\\simeq  \\frac{1}{\\sqrt{2 \\pi \\sigma_t^2}} \\exp\\left[\n-\\frac{(t-t^*(\\{x_i\\}))^2}{2 \\sigma_t^2}\\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"P(t|\\{x_{i}\\})\\simeq\\frac{1}{\\sqrt{2\\pi\\sigma_{t}^{2}}}\\exp\\left[-\\frac{(t-t^{%&#10;*}(\\{x_{i}\\}))^{2}}{2\\sigma_{t}^{2}}\\right].\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2243</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>t</mi><mn>2</mn></msubsup></mrow></msqrt></mfrac><mi>exp</mi><mrow><mo>[</mo><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><msup><mi>t</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>t</mi><mn>2</mn></msubsup></mrow></mfrac><mo>]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\n\n\nWe first consider the scenario in which the noise in the expression of\nthe downstream genes, $\\eta_i$, is uncorrelated from one gene to the\nnext.  In this case ${\\bf C}$ is a diagonal matrix where the diagonal\nelements are the variances of the respective protein concentrations:\n$C_{ii} = \\sigma_{i}^2$.  Substituting $C_{ii}$ and Eq \\ref{xybeh} in\nEq \\ref{eq:sigt} we find that\n\n", "itemtype": "equation", "pos": 14531, "prevtext": "\nHere $\\sigma^2_t=\\sigma^2_t(t*)$ is the variance in the estimate of the\ntime, and it is given by \\cite{Dubuis2013}\n\n", "index": 13, "text": "\\begin{equation}\\label{eq:sigt}\n\\sigma_t^{-2} \\simeq \\sum_{i,j}^N \\left.\\left[\\frac{d\\bar{x}_i(t)}{dt} C^{-1}_{ij} \\frac{d\\bar{x}_j(t)}{dt}\\right]\\right|_{t=t^*(\\{x_k\\})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{t}^{-2}\\simeq\\sum_{i,j}^{N}\\left.\\left[\\frac{d\\bar{x}_{i}(t)}{dt}C^{-1%&#10;}_{ij}\\frac{d\\bar{x}_{j}(t)}{dt}\\right]\\right|_{t=t^{*}(\\{x_{k}\\})}\" display=\"block\"><mrow><msubsup><mi>\u03c3</mi><mi>t</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2243</mo><msub><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mrow><mo>[</mo><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><msubsup><mi>C</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><mo fence=\"true\">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mrow><msup><mi>t</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nClearly, the accuracy of telling time depends on the relative noise strength,\n{\\it i.e.} the standard deviation $\\sigma_i$ divided by the amplitude\n$A_i$, of the respective genes, the frequency $\\omega$ of the\noscillations, and the phase difference between the different\noscillations. It also depends on time, which means that the precision\nwith which the time can be determined, depends on the moment of\nthe day. The average error in the estimate, $\\sigma_t(t)$ averaged over the oscillation period $T$, is\n\\begin{eqnarray}\n\\langle \\sigma_t \\rangle &=& \\int_0^T P(t) \\sigma_t(t) dt\\label{meanstd}\n\\\\\n\\label{sigmedio}\n &=& \\frac{1}{T}\\int dt \\left( \\sqrt{\\omega^2 \\sum_i^N (A_i/ \\sigma_i)^{2} \\cos^2(\\omega t + \\phi_i)}\\right)^{-1}\n\\end{eqnarray}\nIt is not possible to solve this analytically, and below we have\noptimized $\\langle \\sigma_t \\rangle$ numerically. It is also of\ninterest to know how much the error is constant as a function of\ntime. To this end, we compute\n\n", "itemtype": "equation", "pos": 15098, "prevtext": "\n\n\nWe first consider the scenario in which the noise in the expression of\nthe downstream genes, $\\eta_i$, is uncorrelated from one gene to the\nnext.  In this case ${\\bf C}$ is a diagonal matrix where the diagonal\nelements are the variances of the respective protein concentrations:\n$C_{ii} = \\sigma_{i}^2$.  Substituting $C_{ii}$ and Eq \\ref{xybeh} in\nEq \\ref{eq:sigt} we find that\n\n", "index": 15, "text": "\\begin{equation} \\label{eq:sigT}\n\\sigma^{-2}_t (t) = \\omega^2 \\sum_{i=1}^N (A_i/\\sigma_i)^{2} \\cos^2(\\omega t + \\phi_i).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\sigma^{-2}_{t}(t)=\\omega^{2}\\sum_{i=1}^{N}(A_{i}/\\sigma_{i})^{2}\\cos^{2}(%&#10;\\omega t+\\phi_{i}).\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>\u03c3</mi><mi>t</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>\u03c9</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>/</mo><msub><mi>\u03c3</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><msup><mi>cos</mi><mn>2</mn></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03c9</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>+</mo><msub><mi>\u03d5</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\n\nWith cross-correlations in the expressions of the downstream genes,\nthe off-diagonal terms of ${\\bf C}$ will be non zero, which leads to\nadditional terms in the expression for $\\sigma^2_t$. Rather than\ngiving the generic expression, we show the more informative expression\nfor $N=2$, with $x_1(t) = x(t) = A_x \\sin (\\omega t)$ and $x_2(t) =\ny(t) = A_y\n\\sin(\\omega t+\\phi)$. The covariance matrix, which is symmetric and\nsemi-definite positive, is defined as\n\\begin{eqnarray}\n\\label{eq:C}\n {\\bf C}& = & \\left( \\begin{array}{cc}\n\\sigma^2_x & \\mbox{cov}_{xy}  \\\\\n\\mbox{cov}_{xy} & \\sigma^2_y  \\\\\n \\end{array} \\right)\n\\end{eqnarray}\nwhich yields for its inverse\n\\begin{eqnarray}\n{\\bf C}^{-1}& = & \\frac{1}{|{\\bf C}|}\\left( \\begin{array}{cc}\n\\sigma^2_y & -\\mbox{cov}_{xy}  \\\\\n-\\mbox{cov}_{xy} & \\sigma^2_x  \\\\\n \\end{array} \\right),\n\\end{eqnarray}\nwhere the determinant is $|{\\bf\n  C}|=(\\sigma^2_x\\sigma^2_y-\\mbox{cov}_{xy}^2)$. Combining this with Eq. \\ref{eq:sigt} yields:\n\\begin{eqnarray}\n \\sigma_t^{-2}(t)&=&\\frac{1}{|{\\bf C}|}\\left[ \\sigma^2_y A_x^2 \\cos^2(\\omega t)\\right. \\nonumber\\\\\n&& - 2\\mbox{cov}_{xy} A_x A_y \\cos(\\omega t+\\phi) \\cos(\\omega t) \\nonumber\\\\\n&&\\left.+ \\sigma^2_x A_y^2 \\cos^2(\\omega t + \\phi)\\right].\n\\end{eqnarray}\nThis expression reduces to that of  Eq. \\ref{eq:sigT} when the\nco-variance is zero. However, in general, the error on telling time depends on the co-variance of the\nfluctuations in the expression of gene $x$ and gene $y$.\n\nThe quantity $\\sigma_t(t)$ is a local quantity in that it provides the\nerror in estimating the time as a function of the time of the\nday. This quantity can be useful when certain moments of the day have\nto be determined with higher precision than others. In the next\nsection, we discuss another quantity, the mutual information, which\nmakes it possible to determine how many distinct moments in time can\nbe specified. \n\\subsubsection{Mutual Information}\nThe mutual information quantifies how many different input states can\nbe propagated uniquely \\cite{Shannon1948}.\nIn this context, it\nis defined as\n\n", "itemtype": "equation", "pos": 16205, "prevtext": "\nClearly, the accuracy of telling time depends on the relative noise strength,\n{\\it i.e.} the standard deviation $\\sigma_i$ divided by the amplitude\n$A_i$, of the respective genes, the frequency $\\omega$ of the\noscillations, and the phase difference between the different\noscillations. It also depends on time, which means that the precision\nwith which the time can be determined, depends on the moment of\nthe day. The average error in the estimate, $\\sigma_t(t)$ averaged over the oscillation period $T$, is\n\\begin{eqnarray}\n\\langle \\sigma_t \\rangle &=& \\int_0^T P(t) \\sigma_t(t) dt\\label{meanstd}\n\\\\\n\\label{sigmedio}\n &=& \\frac{1}{T}\\int dt \\left( \\sqrt{\\omega^2 \\sum_i^N (A_i/ \\sigma_i)^{2} \\cos^2(\\omega t + \\phi_i)}\\right)^{-1}\n\\end{eqnarray}\nIt is not possible to solve this analytically, and below we have\noptimized $\\langle \\sigma_t \\rangle$ numerically. It is also of\ninterest to know how much the error is constant as a function of\ntime. To this end, we compute\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:variance_error}\n(\\delta \\sigma_t)^2 = \\int_0^T P(t) (\\sigma_t(t) - \\langle \\sigma_t \\rangle)^2  dt\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"(\\delta\\sigma_{t})^{2}=\\int_{0}^{T}P(t)(\\sigma_{t}(t)-\\langle\\sigma_{t}\\rangle%&#10;)^{2}dt\" display=\"block\"><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>\u03c3</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\u03c3</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>\u03c3</mi><mi>t</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nThe mutual information measures the reduction in uncertainty about $t$ upon measuring\n$\\{x_i\\}$, or vice versa. The quantity is indeed symmetric in $\\{x_i\\}$ and $t$:\n\\begin{eqnarray}\nI(x,y;t) &=& H(x,y) - \\langle H(x,y|t) \\rangle_t \\label{eq:info_H_0}\\\\\n\t\t &=& H(t) - \\langle H(t|x,y) \\rangle_{x,y} \\label{eq:info_H}\n\\end{eqnarray}\nwhere $H(a)= - \\int da P(a) \\ln P(a)$, with $P(a)$ the probability\ndistribution of $a$, is the entropy of variable $a$; $H(a,b|c) = -\n\\int da \\int db P(a,b|c) \\ln P(a,b|c)$ is the information entropy of\n$a,b$ given $c$, with $P(a,b|c)$ the conditional probability\ndistribution of $a$ and $b$ given $c$, and $\\langle f(c) \\rangle_c$\ndenotes an average of $f(c)$ over the distribution $P(c)$. In our\ncontext, Eq. \\ref{eq:info_H} is perhaps the most natural expression,\nsince it quantifies how accurately the cell can infer the time of the\nday $t$ from the expression of $x$ and $y$. \n\n\nThe mutual information is a global quantity, which in contrast to\n$\\sigma_t (t)$, does not make it possible to quantify how accurately a\ngiven moment in time can be specified. The latter could be useful when\nthe system needs to change, e.g., its metabolic program at a\nwell-defined moment in time. On the other hand, the mutual information\ndoes allow us to quantify how many different moments in time can be\nspecified, and thus how many temporal decisions the organism could\nmake. As Eq. \\ref{eq:info_H_0} shows, the magnitude of the mutual information\ndepends on both $H(x,y)$ and $\\langle H(x,y|t\\rangle_t$. As we will\nshow below, cross correlations between the expression of the downstream\ngenes $x$ and $y$ will modify $P(x,y)$, reducing its entropy; this\ntends to reduce information transmission. Yet, cross-correlations can\nalso decrease $\\langle H(x,y|t)\\rangle_t$, meaning that, on average,\nthe distribution of expression levels $x$ and $y$ for a given time $t$\nis more narrow---a given time $t$ then maps more uniquely onto an\nexpression pattern $x,y$; this tends to increase the mutual\ninformaiton.  The balance between these two opposing factors\ndetermines the cross correlations that maximize information\ntransmission.\n\n\n\n\\section{Results}\n\n\\begin{figure*}\n\t\\centering\n  \\includegraphics[scale=0.7]{Fig2.pdf}\n  \\caption{Estimating time via $N=2$ protein oscillations, which have\n    the same relative noise strength $\\widetilde{\\sigma}_x =\n    \\sigma_x / A$. Here $A$ is the amplitude of the oscillations and\n    $\\sigma_x$ is the noise in the oscillations, which is here assumed\n    to be constant in time, and given by the mean of the\n    oscillations, $r$, taken to be the same for both oscillations;\n    there are also no cross correlations. (A) The error in the estimate of time $\\sigma_t (t)$\n    as a function of time $t$, for different phase differences $\\Delta\n    \\phi$ between the two oscillations. Note that for $\\Delta \\phi =\n    \\pi / 2$, the error $\\sigma_t(t)$ is constant in time. (B) The\n    variance $(\\delta \\sigma_t)^2$ in the estimate of time as a\n    function of $\\Delta \\phi$, for different relative noise strengths\n    $\\widetilde{\\sigma}_x$. As expected from panel A, $(\\delta\n    \\sigma_t)^2=0$ for $\\Delta \\phi = \\pi / 2$. (C) The mean error\n    $\\langle \\sigma_t\\rangle$ as a function of $\\Delta \\phi$, for\n    different relative noise strengths $\\widetilde{\\sigma}_x$. The\n    error is proportional to $\\widetilde{\\sigma}_x$, in accordance with\n    Eq. \\ref{eq:mean_error_noise_constant}. Note also that the mean\n    error is minimized at $\\Delta \\phi=\\pi/2$, although the dependence\n    on $\\Delta \\phi$ near the optimum is weak. (D) The mutual\n    information $I(x,y;t)$ between the two protein oscillations $x(t),y(t)$ and\n    time $t$, for different relative noise strengths\n    $\\widetilde{\\sigma}_x$. The mutual information increases with decreasing\n    $\\widetilde{\\sigma}_x$, and is optimized at $\\Delta \\phi = \\pi /\n    2$. Note also that the dependence of $I(x,y;t)$ on $\\Delta \\phi$ is\n  stronger than that of $\\langle \\sigma_t \\rangle$ (panel C). }\n  \\label{fig:fig2}\n\\end{figure*}\n\n\\subsection{No Cross-correlations}\nIn this section, we consider the scenario in which there are no cross\ncorrelations between the noise in the expression of the downstream\ngenes. We first study the case in which the relative noise strength,\n$\\widetilde{\\sigma}_i\\equiv \\sigma_i /A_i=\\widetilde{\\sigma}_x$, is\nthe same for all genes $i$; in this scenario, we use the subscript $x$\nto remind ourselves that we are considering the standard deviation in\n$x$ and not in the estimate of time. We will also first assume that\n$\\sigma_x(t)=\\sigma_x$ is constant in time, depending only on the mean\nof $x$, i.e $r_x$, but not its mean instantaneous level $\\bar{x}(t)$. The\nlatter is reasonable when the amplitude of the oscillations is small\ncompared to the mean.\n\nTo determine the optimal phase relation that miminizes the average\nerror in telling time, given by Eq. \\ref{sigmedio}, we \nsolve \n\n", "itemtype": "equation", "pos": 18406, "prevtext": "\n\nWith cross-correlations in the expressions of the downstream genes,\nthe off-diagonal terms of ${\\bf C}$ will be non zero, which leads to\nadditional terms in the expression for $\\sigma^2_t$. Rather than\ngiving the generic expression, we show the more informative expression\nfor $N=2$, with $x_1(t) = x(t) = A_x \\sin (\\omega t)$ and $x_2(t) =\ny(t) = A_y\n\\sin(\\omega t+\\phi)$. The covariance matrix, which is symmetric and\nsemi-definite positive, is defined as\n\\begin{eqnarray}\n\\label{eq:C}\n {\\bf C}& = & \\left( \\begin{array}{cc}\n\\sigma^2_x & \\mbox{cov}_{xy}  \\\\\n\\mbox{cov}_{xy} & \\sigma^2_y  \\\\\n \\end{array} \\right)\n\\end{eqnarray}\nwhich yields for its inverse\n\\begin{eqnarray}\n{\\bf C}^{-1}& = & \\frac{1}{|{\\bf C}|}\\left( \\begin{array}{cc}\n\\sigma^2_y & -\\mbox{cov}_{xy}  \\\\\n-\\mbox{cov}_{xy} & \\sigma^2_x  \\\\\n \\end{array} \\right),\n\\end{eqnarray}\nwhere the determinant is $|{\\bf\n  C}|=(\\sigma^2_x\\sigma^2_y-\\mbox{cov}_{xy}^2)$. Combining this with Eq. \\ref{eq:sigt} yields:\n\\begin{eqnarray}\n \\sigma_t^{-2}(t)&=&\\frac{1}{|{\\bf C}|}\\left[ \\sigma^2_y A_x^2 \\cos^2(\\omega t)\\right. \\nonumber\\\\\n&& - 2\\mbox{cov}_{xy} A_x A_y \\cos(\\omega t+\\phi) \\cos(\\omega t) \\nonumber\\\\\n&&\\left.+ \\sigma^2_x A_y^2 \\cos^2(\\omega t + \\phi)\\right].\n\\end{eqnarray}\nThis expression reduces to that of  Eq. \\ref{eq:sigT} when the\nco-variance is zero. However, in general, the error on telling time depends on the co-variance of the\nfluctuations in the expression of gene $x$ and gene $y$.\n\nThe quantity $\\sigma_t(t)$ is a local quantity in that it provides the\nerror in estimating the time as a function of the time of the\nday. This quantity can be useful when certain moments of the day have\nto be determined with higher precision than others. In the next\nsection, we discuss another quantity, the mutual information, which\nmakes it possible to determine how many distinct moments in time can\nbe specified. \n\\subsubsection{Mutual Information}\nThe mutual information quantifies how many different input states can\nbe propagated uniquely \\cite{Shannon1948}.\nIn this context, it\nis defined as\n\n", "index": 19, "text": "\\begin{equation}\\label{eq:info}\nI(\\{x_i\\};t) =\\int d{\\bf x} dt P(\\{x_i\\},t) \\log \\frac{P(\\{x_i\\},t)}{P(\\{x_i\\})P(t)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"I(\\{x_{i}\\};t)=\\int d{\\bf x}dtP(\\{x_{i}\\},t)\\log\\frac{P(\\{x_{i}\\},t)}{P(\\{x_{i%&#10;}\\})P(t)}.\" display=\"block\"><mrow><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo>;</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mrow><mo>\ud835\udc51</mo><mi>\ud835\udc31</mi></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mfrac><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": " \nwhere $\\Delta \\phi_i = \\phi_i - \\phi_1$.\nSetting the phase of the first oscillation to zero, i.e. $\\phi_1=0$, we find that\nthe optimal phase relation that minimizes the average error is given\nby\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThe mutual information measures the reduction in uncertainty about $t$ upon measuring\n$\\{x_i\\}$, or vice versa. The quantity is indeed symmetric in $\\{x_i\\}$ and $t$:\n\\begin{eqnarray}\nI(x,y;t) &=& H(x,y) - \\langle H(x,y|t) \\rangle_t \\label{eq:info_H_0}\\\\\n\t\t &=& H(t) - \\langle H(t|x,y) \\rangle_{x,y} \\label{eq:info_H}\n\\end{eqnarray}\nwhere $H(a)= - \\int da P(a) \\ln P(a)$, with $P(a)$ the probability\ndistribution of $a$, is the entropy of variable $a$; $H(a,b|c) = -\n\\int da \\int db P(a,b|c) \\ln P(a,b|c)$ is the information entropy of\n$a,b$ given $c$, with $P(a,b|c)$ the conditional probability\ndistribution of $a$ and $b$ given $c$, and $\\langle f(c) \\rangle_c$\ndenotes an average of $f(c)$ over the distribution $P(c)$. In our\ncontext, Eq. \\ref{eq:info_H} is perhaps the most natural expression,\nsince it quantifies how accurately the cell can infer the time of the\nday $t$ from the expression of $x$ and $y$. \n\n\nThe mutual information is a global quantity, which in contrast to\n$\\sigma_t (t)$, does not make it possible to quantify how accurately a\ngiven moment in time can be specified. The latter could be useful when\nthe system needs to change, e.g., its metabolic program at a\nwell-defined moment in time. On the other hand, the mutual information\ndoes allow us to quantify how many different moments in time can be\nspecified, and thus how many temporal decisions the organism could\nmake. As Eq. \\ref{eq:info_H_0} shows, the magnitude of the mutual information\ndepends on both $H(x,y)$ and $\\langle H(x,y|t\\rangle_t$. As we will\nshow below, cross correlations between the expression of the downstream\ngenes $x$ and $y$ will modify $P(x,y)$, reducing its entropy; this\ntends to reduce information transmission. Yet, cross-correlations can\nalso decrease $\\langle H(x,y|t)\\rangle_t$, meaning that, on average,\nthe distribution of expression levels $x$ and $y$ for a given time $t$\nis more narrow---a given time $t$ then maps more uniquely onto an\nexpression pattern $x,y$; this tends to increase the mutual\ninformaiton.  The balance between these two opposing factors\ndetermines the cross correlations that maximize information\ntransmission.\n\n\n\n\\section{Results}\n\n\\begin{figure*}\n\t\\centering\n  \\includegraphics[scale=0.7]{Fig2.pdf}\n  \\caption{Estimating time via $N=2$ protein oscillations, which have\n    the same relative noise strength $\\widetilde{\\sigma}_x =\n    \\sigma_x / A$. Here $A$ is the amplitude of the oscillations and\n    $\\sigma_x$ is the noise in the oscillations, which is here assumed\n    to be constant in time, and given by the mean of the\n    oscillations, $r$, taken to be the same for both oscillations;\n    there are also no cross correlations. (A) The error in the estimate of time $\\sigma_t (t)$\n    as a function of time $t$, for different phase differences $\\Delta\n    \\phi$ between the two oscillations. Note that for $\\Delta \\phi =\n    \\pi / 2$, the error $\\sigma_t(t)$ is constant in time. (B) The\n    variance $(\\delta \\sigma_t)^2$ in the estimate of time as a\n    function of $\\Delta \\phi$, for different relative noise strengths\n    $\\widetilde{\\sigma}_x$. As expected from panel A, $(\\delta\n    \\sigma_t)^2=0$ for $\\Delta \\phi = \\pi / 2$. (C) The mean error\n    $\\langle \\sigma_t\\rangle$ as a function of $\\Delta \\phi$, for\n    different relative noise strengths $\\widetilde{\\sigma}_x$. The\n    error is proportional to $\\widetilde{\\sigma}_x$, in accordance with\n    Eq. \\ref{eq:mean_error_noise_constant}. Note also that the mean\n    error is minimized at $\\Delta \\phi=\\pi/2$, although the dependence\n    on $\\Delta \\phi$ near the optimum is weak. (D) The mutual\n    information $I(x,y;t)$ between the two protein oscillations $x(t),y(t)$ and\n    time $t$, for different relative noise strengths\n    $\\widetilde{\\sigma}_x$. The mutual information increases with decreasing\n    $\\widetilde{\\sigma}_x$, and is optimized at $\\Delta \\phi = \\pi /\n    2$. Note also that the dependence of $I(x,y;t)$ on $\\Delta \\phi$ is\n  stronger than that of $\\langle \\sigma_t \\rangle$ (panel C). }\n  \\label{fig:fig2}\n\\end{figure*}\n\n\\subsection{No Cross-correlations}\nIn this section, we consider the scenario in which there are no cross\ncorrelations between the noise in the expression of the downstream\ngenes. We first study the case in which the relative noise strength,\n$\\widetilde{\\sigma}_i\\equiv \\sigma_i /A_i=\\widetilde{\\sigma}_x$, is\nthe same for all genes $i$; in this scenario, we use the subscript $x$\nto remind ourselves that we are considering the standard deviation in\n$x$ and not in the estimate of time. We will also first assume that\n$\\sigma_x(t)=\\sigma_x$ is constant in time, depending only on the mean\nof $x$, i.e $r_x$, but not its mean instantaneous level $\\bar{x}(t)$. The\nlatter is reasonable when the amplitude of the oscillations is small\ncompared to the mean.\n\nTo determine the optimal phase relation that miminizes the average\nerror in telling time, given by Eq. \\ref{sigmedio}, we \nsolve \n\n", "index": 21, "text": "\\begin{equation} \n\\frac{d\\langle \\sigma_t \\rangle}{d \\Delta \\phi_i} =0 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; i=1...N,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\frac{d\\langle\\sigma_{t}\\rangle}{d\\Delta\\phi_{i}}=0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1...N,\" display=\"block\"><mrow><mrow><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>\u03c3</mi><mi>t</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>\u03d5</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mn>0</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2002\u2005</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>N</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nClearly, in the optimal system the phases of the downstream\noscillations are evenly spaced when $\\widetilde{\\sigma}_x$ is the same\nfor all genes, and $\\sigma_x$ is constant in time.\n\nThe next question is what is the phase relation that minimizes the\nvariance of $\\sigma_t (t)$ over the oscillation period $T$,\ni.e. minimizes Eq. \\ref{eq:variance_error}. In the appendix we show that the\nsolution is also given by Eq. \\ref{optphase}. Hence, the phase\nrelation that minimizes the average error on telling time, $\\langle\n\\sigma_t\\rangle$, is also the phase relation that minimizes the\nvariance of $\\sigma_t (t)$. Thus, in the optimal system, the phases\nare evenly spaced; this not only minimizes the average error in\ntelling time, but it also yields the same accuracy for all times\n$t$. Moreover, for this optimal system, the average error, obtained\nfrom Eq. \\ref{eq:sigT},  is given by \n\n", "itemtype": "equation", "pos": 23790, "prevtext": " \nwhere $\\Delta \\phi_i = \\phi_i - \\phi_1$.\nSetting the phase of the first oscillation to zero, i.e. $\\phi_1=0$, we find that\nthe optimal phase relation that minimizes the average error is given\nby\n\n", "index": 23, "text": "\\begin{equation}\\label{optphase}\n\\Delta \\phi_i = (i-1) \\frac{\\pi}{N}  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; i=1...N.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\Delta\\phi_{i}=(i-1)\\frac{\\pi}{N}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1...N.\" display=\"block\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>\u03d5</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mi>\u03c0</mi><mi>N</mi></mfrac></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2002\u2005</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>N</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nThis shows that the average error is\nproportional to the relative noise strength $\\widetilde{\\sigma}_x=\\sigma_x/A$\nand inversely proportional to the square root of the number of\nreadout genes, $N$.\n\nThese results are illustrated in Figs. \\ref{fig:fig2}A-C, for $N=2$. Panel\nA shows $\\sigma_t (t)$ as a function of $t$, for different phase\nrelations $\\Delta \\phi \\equiv \\phi_2 - \\phi_1$. It is seen that, in\ngeneral, $\\sigma_t (t)$, depends on $t$. However, when $\\Delta \\phi =\n\\pi / 2$, then $\\sigma_t (t)$ is independent of $t$. Panel B shows\nthat for this phase relation, the variance $(\\delta \\sigma_t)^2$ is\nindeed zero, while panel C shows that in this case also the average error is\nminimal, in accordance with the theoretical analysis. \n\nLastly, Fig. \\ref{fig:fig2}D shows the mutual information $I(x,y;t)$,\nobtained numerically, as a function of the phase shift, for different\nnoise levels. As expected, the mutual information increases as the\nrelative noise strength $\\widetilde{\\sigma}_x$\ndecreases. Moreover, the phase relation that minimizes the average\nerror, $\\langle \\sigma_t\\rangle$, is also the phase relation that\nmaximizes the mutual information.\n\nWhen the noise amplitude $\\sigma_x$ depends on the mean instantaneous copy\nnumber $\\bar{x}(t)$ (rather than its mean averaged over the oscillation period), the noise in the output\n$\\sigma_x(t)$ varies in time. We will assume that $\\sigma_x(t) \\simeq\n\\sqrt{\\bar{x}(t)}$, and consider as above the case that the amplitude\nand the mean of the oscillations are the same for all genes,\nrespectively: $A_i=A_j = \\dots = A$ and $r_i=r_j = \\dots = r_x$. Our\nanalysis described in the appendix reveals that the optimal phase\nrelation that maximizes the mutual information and minimizes both\nthe variance $(\\delta \\sigma_t)^2$ and the mean $\\langle \\sigma_t\\rangle$ of the error, is again given by\nEq. \\ref{optphase}. However, the minimal variance, obtained for the\noptimal phase relation, only reduces to zero in the limit that $r\\to\n\\infty$; in this limit, the noise $\\sigma_x(t)$ becomes constant in\ntime and we recover the case discussed above.  Interestingly, the average\nerror $\\langle \\sigma_t\\rangle$ is larger than that in the case of\nconstant relative noise strength, even when the average relative noise\nstrength is the same.\n\nWhen $N=2$ yet the relative noise strength is not the same for both\ngenes, $\\widetilde{\\sigma}_x\\neq \\widetilde{\\sigma}_y$, the optimal\nphase shift that minimizes the error and maximizes the mutual\ninformation is again $\\Delta \\phi_{xy}=\\pi/2$; indeed, this result,\nfor $N=2$, does not depend on whether $\\widetilde{\\sigma}$ is the same\nfor both genes. Also the variance $(\\delta \\sigma_t)^2$ is zero for\nthis optimal phase shift, as before.\n\nThese results change markedly when the relative noise strength is not\nthe same for all genes and $N>2$. Then the optimal phase shift depends\nin a non-trivial manner on $\\{\\widetilde{\\sigma}_i\\}$. The principle\nis that the oscillations that contain more information about time\nbecause they are less noisy, should be spaced further apart. More\nspecifically, the spacing between them should be closer to that which\nmaximizes the mutual information between them and time.  This\nprinciple is illustrated in Fig. \\ref{fig:fig3}A-B for three genes,\nwhere $\\widetilde{\\sigma}_x=\\widetilde{\\sigma}_y \\equiv \\widetilde{\\sigma}_{x, y}<\n\\widetilde{\\sigma}_z$. Clearly, the oscillations of proteins X and Y\ncontain more information about time than the oscillation of protein\nZ. As a consequence, the phase difference between $x(t)$ and $y(t)$,\n$\\Delta \\phi_{xy} = \\phi_y - \\phi_x$, is more important in accurately\ntelling time than that between the two other pairs of\noscillations. The phase difference $\\Delta \\phi_{xy}$ is therefore\ncloser to $\\pi / 2$, the phase difference that maximizes $I(x,y;t)$,\nthan those of the other pairs of genes. Indeed, the extent\nto which $\\Delta \\phi_{xy}$ approaches $\\pi/2$ depends on\n$\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$, as Fig. \\ref{fig:fig3}B shows: when\n$\\widetilde{\\sigma}_{x,y}=\\widetilde{\\sigma}_z$, all oscillations are equally informative and\nhence the oscillations are evenly spaced, yielding $\\Delta\n\\phi_{xy}=\\Delta \\phi_{yz}=\\Delta \\phi_{zx}=\\pi/3$. In contrast, when\n$\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z = 0$, $\\Delta \\phi_{xy}=\\pi/2$, the same result\nthat would have been obtained if these two genes were the only ones\npresent. In this limit, $\\widetilde{\\sigma}_z$ is infinite, and $z$\ncarries no information on time, making its phase irrelevant.\n\nFig. \\ref{fig:fig3}C gives the mean error $\\langle \\sigma_t\\rangle$\nand Fig. \\ref{fig:fig3}D the mutual information $I(x,y,z;t)$ for the\noptimal phase relation shown in panel B, as a function of\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z$. Here, in varying\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z$,\n$\\widetilde{\\sigma}_{xy}$ is kept constant while\n$\\widetilde{\\sigma}_z$ is varied between $\\widetilde{\\sigma}_{xy}$ and\ninfinity. These panels thus show the gain in employing an additional\nreadout protein in accurately telling time, as a function of its noise\nlevel. The results interpolate between those for $N=2$\nequally informative genes when\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z=0$, and those for $N=3$\nequally informative genes when\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z=1$.\n\n\n\\begin{figure*}\n\t\\centering\n  \\includegraphics[scale=0.6]{Fig3.pdf}\n  \\caption{Estimating time via $N=3$ protein oscillations, where the\n    relative noise strength $\\widetilde{\\sigma}_i \\equiv \\sigma_i / A_i$ of two oscillations is the same,\n    $\\widetilde{\\sigma}_x = \\widetilde{\\sigma}_y \\equiv\n    \\widetilde{\\sigma}_{x,y}$, and different from that of the third\n    oscillation, $\\widetilde{\\sigma}_z$. The noise $\\sigma_i$ is\n    assumed to be constant in time, and there are no cross\n    correlations in the noise. (A) Sketch of the set up, with two\n    reliable oscillations $x(t)$ and $y(t)$ and a third, more noisy\n    oscillation $z(t)$. (B) The optimal phase relation that maximizes\n    the mutual information $I(x,y,z;t)$ and minimizes the mean error\n    $\\langle \\sigma_t \\rangle$, as a function\n    of $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$; here\n    $\\widetilde{\\sigma}_{x,y}$ is kept constant while\n    $\\widetilde{\\sigma}_z$ is varied. When\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z=0$, the third gene\n    $z(t)$ carries no information, and the optimal phase difference\n    $\\Delta \\phi_{xy} = \\phi_y - \\phi_x$ between the oscillations of\n    $x$ and $y$ is $\\Delta \\phi_{xy} = \\pi/2$, the result for $N=2$\n    oscillations; in this limit, the phase of $z$ is irrelevant. As\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$ increases, the\n    third oscillation $z(t)$ becomes more important. The phase\n    difference $\\Delta \\phi_{xy}$ between $x(t)$ and $y(t)$ decreases,\n    while the phase difference $\\Delta \\phi_{yz}$ between\n    $y(t)$ and $z(t)$ increases. When\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z=1$, all genes are\n    equally informative and $\\Delta \\phi_{xy} = \\Delta \\phi_{yz} =\n    \\Delta \\phi_{zx} = \\pi / 3$. (C) The mean error $\\langle \\sigma_t\n    \\rangle$ as a function\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$. It decreases as\n    the third gene becomes more informative. (D) The mutual\n    $I(x,y,z;t)$ increases with $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$. }\n  \\label{fig:fig3}\n\\end{figure*}\n\n\n \\begin{figure*}[t]\n \\centering\n    \\includegraphics[scale=0.6]{Fig4.pdf}\n    \\caption{The importance of cross correlations between the\n      fluctuations in the oscillations of the readout proteins,\n      illustrated here for $N=2$ readout proteins. The top row shows\n      results for the scenario in which the relative noise strength\n      $\\widetilde{\\sigma}_i \\equiv \\sigma_i / A_i$ is low, while the\n      bottom panel displays the results for when it is large.  In all\n      cases, the relative noise strength of the two oscillations is\n      taken to be the same, $\\widetilde{\\sigma}_x =\n      \\widetilde{\\sigma}_y = \\widetilde{\\sigma}_{x,y}$. The panels in\n      the left column show a heat map of the mutual information\n      $I(x,y;t)$ as a function of the phase difference $\\Delta \\phi =\n      \\phi_y - \\phi_x$ between the two oscillations, and the\n      correlation coefficient $b$. Due to the symmetry of the problem\n      the mutual information is symmetric: $I(x,y;t)_{\\Delta \\phi,b} =\n      I(x,y;t)_{\\pi - \\Delta \\phi,-b}$. The top-left panel shows that\n      when the relative noise strength is low, the mutual information\n      is maximized for $|b|\\to 1$ and $\\Delta \\phi \\neq \\pi /\n      2$. Cross correlations thus change the optimal phase difference,\n      and more, importantly, they can enhance the mutual\n      information. However, when the relative noise is large, the\n      cross correlations become less important and the optimal phase\n      difference approaches $\\Delta \\phi = \\pi / 2$ (bottom left\n      panel). The middle panels elucidate how cross correlations can\n      affect the mutual information. Shown are, for different points\n      in the heat map on the left, the average trajectory that\n      ${x}(t)$ and $y(t)$ trace out during a 24 hr period (green\n      solid line), with superimposed, for different times of the day,\n      scatter points of $x(t)$ and $y(t)$, originating from gene\n      expression noise. The main axis of the contour\n      $\\bar{x}(t),\\bar{y}(t)$ is determined by the phase difference\n      $\\Delta \\phi$, while the main axis of the noise (scatter points)\n      is determined by the correlation coefficient $b$. There are\n      moments of the day where cross correlations cause the\n      distributions $P(x,y|t)$ of neighboring times $t$ to overlap\n      less, thus increasing mutual information, but also moments where\n      they increase the overlap, decreasing the mutual\n      information. The net benefit depends on how these contributions\n      are weighted. The system spends more time near the extrema of\n      $\\bar{x}(t),\\bar{y}(t)$, as illustrated in the right panels. Consequently,\n      when $\\Delta \\phi < \\pi / 2$, positive correlations $b>0$ enhance the\n      mutual information, especially when the relative noise strength\n      $\\widetilde{\\sigma}_x$ is low (point B top row). At higher\n      noise (bottom row), cross correlations are less effective in\n      reducing the overlap in $P(x,y|t)$ and the phase difference\n      $\\Delta \\phi$ becomes the dominant control parameter. }\n    \\label{fig:fig4}\n\\end{figure*}\n\n\n\n\n\\subsection{The importance of cross-correlations}\nSo far we have assumed that the noise in the expression of the\ndownstream genes is uncorrelated. However, in general, we expect their\nnoise to be correlated.  Direct or indirect regulatory interactions\nbetween the genes can lead to correlations or anti-correlations in the\nfluctuations of the protein concentrations \\cite{Walczak:2010cv}. And\nalso noise in the input signal can lead to correlated gene\nexpression. In fact, the extrinsic contribution to the noise in gene\nexpression is often larger than the intrinsic one\n\\cite{Taniguchi:2010cb}, which can induce pronounced correlations\nbetween the expression of the downstream genes. Intuitively, we may\nthink that if we need to infer an input variable $t$ from two output\nvariables $x$ and $y$, then cross-correlations between $x$ and $y$\nreduce the accuracy of the estimate---asking two persons $x$ and $y$\na question about $t$ seems to give more information when $x$ and $y$\ngive independent answers. However, this intuition is not always\ncorrect, as will become clear. Indeed, in this section\nwe study how correlations between the expression of downstream genes\naffect the precision by which cells can tell time.\n\n\n \\begin{figure*}[t]\n \\centering\n    \\includegraphics[scale=0.9]{Fig5.pdf}\n    \\caption{Importance of cross correlations in reducing the error in\n      estimating the time, as estimated from $N=2$ protein\n      oscillations. Heat maps of the the variance in the error\n      $(\\delta \\sigma_t)^2$ (A) and the mean error $\\langle \\sigma_t\n      \\rangle$ \n      (B), as a function of the phase difference $\\Delta \\phi$ between\n      the two oscillations and the correlation coefficient $b$ of the\n      fluctuations in the oscillations. The relative noise strength\n      $\\widetilde{\\sigma}_x$ is the same for both oscillations, and\n      equal to that of the low-noise scenario in Fig. \\ref{fig:fig4},\n      $\\widetilde{\\sigma}_x = 0.03$. It is seen that cross\n      correlations can reduce the mean error. Comparing against the\n      top-left panel of Fig. \\ref{fig:fig4} shows, however, that the\n      positions of the optima are different for the two quantities,\n      the mean error $\\langle \\sigma_t \\rangle$ and the mutual\n      information $I(x,y;t)$ , respectively. This is because the\n      quantities $\\sigma_t(t)$ (Eq. \\ref{sigmedio}) and $H(t|x,y)$\n      (Eq. \\ref{eq:info_H_2}) are averaged over different\n      distributions, the uniform distribution $P(t)$ and the\n      non-uniform distribution $P(x,y)$, respectively. }\n    \\label{fig:fig5}\n\\end{figure*}\n\nIn order to dissect the effect of cross-correlations, we study two\ndownstream genes, $N=2$, and take both the amplitudes of their\noscillations and their expression noise to be equal: $A_x = A_y = A$,\n$\\sigma_x=\\sigma_y=\\sigma_{x,y}$, respectively. Using the latter, we can renormalize the\ncovariance matrix Eq. \\ref{eq:C}:\n\n", "itemtype": "equation", "pos": 24790, "prevtext": "\nClearly, in the optimal system the phases of the downstream\noscillations are evenly spaced when $\\widetilde{\\sigma}_x$ is the same\nfor all genes, and $\\sigma_x$ is constant in time.\n\nThe next question is what is the phase relation that minimizes the\nvariance of $\\sigma_t (t)$ over the oscillation period $T$,\ni.e. minimizes Eq. \\ref{eq:variance_error}. In the appendix we show that the\nsolution is also given by Eq. \\ref{optphase}. Hence, the phase\nrelation that minimizes the average error on telling time, $\\langle\n\\sigma_t\\rangle$, is also the phase relation that minimizes the\nvariance of $\\sigma_t (t)$. Thus, in the optimal system, the phases\nare evenly spaced; this not only minimizes the average error in\ntelling time, but it also yields the same accuracy for all times\n$t$. Moreover, for this optimal system, the average error, obtained\nfrom Eq. \\ref{eq:sigT},  is given by \n\n", "index": 25, "text": "\\begin{equation}\n\\langle \\sigma_t\\rangle = \\frac{\\widetilde{\\sigma}_x T}{2\\pi} \\sqrt{\\frac{2}{N}}\n\\label{eq:mean_error_noise_constant}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\langle\\sigma_{t}\\rangle=\\frac{\\widetilde{\\sigma}_{x}T}{2\\pi}\\sqrt{\\frac{2}{N}}\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>\u03c3</mi><mi>t</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mfrac><mrow><msub><mover accent=\"true\"><mi>\u03c3</mi><mo>~</mo></mover><mi>x</mi></msub><mo>\u2062</mo><mi>T</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo>\u2062</mo><msqrt><mfrac><mn>2</mn><mi>N</mi></mfrac></msqrt></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": " \nwhere $b$ is the correlation coefficient, denoting the\ncross-correlation strength: $b=1$ implies that the noise in the\nexpression of X and Y is fully correlated, while $b=-1$ implies full\nanti-correlation. We computed numerically how $I(x,y;t)$, $\\langle\n\\sigma_t\\rangle_t$ and $(\\delta t)^2$ depend on the phase shift\n$\\Delta \\phi = \\phi_y - \\phi_x$, the relative noise strength\n$\\widetilde{\\sigma}_{x,y} = \\sigma_{x,y} / A$, and the correlation\ncoeffient $b$.\n\nFig. \\ref{fig:fig4} shows the mutual information $I(x,y;t)$ as a\nfunction of $\\Delta \\phi$ and $b$, both for low noise, with\n$\\widetilde{\\sigma}_{x,y} = 0.03$ (panels top row), and high noise, with\n$\\widetilde{\\sigma}_{x,y} = 0.4$ (panels bottom row).  The following points\nare worthy of note. First, as expected, $I(x,y;t)$ is symmetric with\nrespect to $\\Delta \\phi$ and $b$: $I(x,y;t)_{\\Delta \\phi,\n  b}=I(x,y;t)_{2\\pi - \\Delta \\phi,-b}$. Secondly, depending on the\nphase shift $\\Delta \\phi$, correlations ($b>0$) or\nanti-correlations ($b<0$) can enhance the mutual information,\nespecially when  the relative\nnoise strength $\\widetilde{\\sigma}_{x,y}$ is low (top panel).\nConcomittantly, the optimal phase shift $\\Delta \\phi$ that maximizes the mutual\ninformation depends on the cross correlation $b$. At low noise, the\nmutual information is maximized either at $0<\\Delta \\phi^*<\\pi / 2$\nand $b\\approx 1$ or at $\\pi - \\Delta \\phi^*$ and $b\\approx -1$. At high\nnoise, cross correlations no longers help to improve the mutual\ninformation  (bottom panel). Moreover, the optimal phase shift is at $\\Delta \\phi^* \\approx\n\\pi / 2$. We now discuss the origin of these observations.\n\nTo elucidate these observations, we start from the definition of the\nmutual information (see Eq. \\ref{eq:info_H}):\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThis shows that the average error is\nproportional to the relative noise strength $\\widetilde{\\sigma}_x=\\sigma_x/A$\nand inversely proportional to the square root of the number of\nreadout genes, $N$.\n\nThese results are illustrated in Figs. \\ref{fig:fig2}A-C, for $N=2$. Panel\nA shows $\\sigma_t (t)$ as a function of $t$, for different phase\nrelations $\\Delta \\phi \\equiv \\phi_2 - \\phi_1$. It is seen that, in\ngeneral, $\\sigma_t (t)$, depends on $t$. However, when $\\Delta \\phi =\n\\pi / 2$, then $\\sigma_t (t)$ is independent of $t$. Panel B shows\nthat for this phase relation, the variance $(\\delta \\sigma_t)^2$ is\nindeed zero, while panel C shows that in this case also the average error is\nminimal, in accordance with the theoretical analysis. \n\nLastly, Fig. \\ref{fig:fig2}D shows the mutual information $I(x,y;t)$,\nobtained numerically, as a function of the phase shift, for different\nnoise levels. As expected, the mutual information increases as the\nrelative noise strength $\\widetilde{\\sigma}_x$\ndecreases. Moreover, the phase relation that minimizes the average\nerror, $\\langle \\sigma_t\\rangle$, is also the phase relation that\nmaximizes the mutual information.\n\nWhen the noise amplitude $\\sigma_x$ depends on the mean instantaneous copy\nnumber $\\bar{x}(t)$ (rather than its mean averaged over the oscillation period), the noise in the output\n$\\sigma_x(t)$ varies in time. We will assume that $\\sigma_x(t) \\simeq\n\\sqrt{\\bar{x}(t)}$, and consider as above the case that the amplitude\nand the mean of the oscillations are the same for all genes,\nrespectively: $A_i=A_j = \\dots = A$ and $r_i=r_j = \\dots = r_x$. Our\nanalysis described in the appendix reveals that the optimal phase\nrelation that maximizes the mutual information and minimizes both\nthe variance $(\\delta \\sigma_t)^2$ and the mean $\\langle \\sigma_t\\rangle$ of the error, is again given by\nEq. \\ref{optphase}. However, the minimal variance, obtained for the\noptimal phase relation, only reduces to zero in the limit that $r\\to\n\\infty$; in this limit, the noise $\\sigma_x(t)$ becomes constant in\ntime and we recover the case discussed above.  Interestingly, the average\nerror $\\langle \\sigma_t\\rangle$ is larger than that in the case of\nconstant relative noise strength, even when the average relative noise\nstrength is the same.\n\nWhen $N=2$ yet the relative noise strength is not the same for both\ngenes, $\\widetilde{\\sigma}_x\\neq \\widetilde{\\sigma}_y$, the optimal\nphase shift that minimizes the error and maximizes the mutual\ninformation is again $\\Delta \\phi_{xy}=\\pi/2$; indeed, this result,\nfor $N=2$, does not depend on whether $\\widetilde{\\sigma}$ is the same\nfor both genes. Also the variance $(\\delta \\sigma_t)^2$ is zero for\nthis optimal phase shift, as before.\n\nThese results change markedly when the relative noise strength is not\nthe same for all genes and $N>2$. Then the optimal phase shift depends\nin a non-trivial manner on $\\{\\widetilde{\\sigma}_i\\}$. The principle\nis that the oscillations that contain more information about time\nbecause they are less noisy, should be spaced further apart. More\nspecifically, the spacing between them should be closer to that which\nmaximizes the mutual information between them and time.  This\nprinciple is illustrated in Fig. \\ref{fig:fig3}A-B for three genes,\nwhere $\\widetilde{\\sigma}_x=\\widetilde{\\sigma}_y \\equiv \\widetilde{\\sigma}_{x, y}<\n\\widetilde{\\sigma}_z$. Clearly, the oscillations of proteins X and Y\ncontain more information about time than the oscillation of protein\nZ. As a consequence, the phase difference between $x(t)$ and $y(t)$,\n$\\Delta \\phi_{xy} = \\phi_y - \\phi_x$, is more important in accurately\ntelling time than that between the two other pairs of\noscillations. The phase difference $\\Delta \\phi_{xy}$ is therefore\ncloser to $\\pi / 2$, the phase difference that maximizes $I(x,y;t)$,\nthan those of the other pairs of genes. Indeed, the extent\nto which $\\Delta \\phi_{xy}$ approaches $\\pi/2$ depends on\n$\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$, as Fig. \\ref{fig:fig3}B shows: when\n$\\widetilde{\\sigma}_{x,y}=\\widetilde{\\sigma}_z$, all oscillations are equally informative and\nhence the oscillations are evenly spaced, yielding $\\Delta\n\\phi_{xy}=\\Delta \\phi_{yz}=\\Delta \\phi_{zx}=\\pi/3$. In contrast, when\n$\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z = 0$, $\\Delta \\phi_{xy}=\\pi/2$, the same result\nthat would have been obtained if these two genes were the only ones\npresent. In this limit, $\\widetilde{\\sigma}_z$ is infinite, and $z$\ncarries no information on time, making its phase irrelevant.\n\nFig. \\ref{fig:fig3}C gives the mean error $\\langle \\sigma_t\\rangle$\nand Fig. \\ref{fig:fig3}D the mutual information $I(x,y,z;t)$ for the\noptimal phase relation shown in panel B, as a function of\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z$. Here, in varying\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z$,\n$\\widetilde{\\sigma}_{xy}$ is kept constant while\n$\\widetilde{\\sigma}_z$ is varied between $\\widetilde{\\sigma}_{xy}$ and\ninfinity. These panels thus show the gain in employing an additional\nreadout protein in accurately telling time, as a function of its noise\nlevel. The results interpolate between those for $N=2$\nequally informative genes when\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z=0$, and those for $N=3$\nequally informative genes when\n$\\widetilde{\\sigma}_{xy}/\\widetilde{\\sigma}_z=1$.\n\n\n\\begin{figure*}\n\t\\centering\n  \\includegraphics[scale=0.6]{Fig3.pdf}\n  \\caption{Estimating time via $N=3$ protein oscillations, where the\n    relative noise strength $\\widetilde{\\sigma}_i \\equiv \\sigma_i / A_i$ of two oscillations is the same,\n    $\\widetilde{\\sigma}_x = \\widetilde{\\sigma}_y \\equiv\n    \\widetilde{\\sigma}_{x,y}$, and different from that of the third\n    oscillation, $\\widetilde{\\sigma}_z$. The noise $\\sigma_i$ is\n    assumed to be constant in time, and there are no cross\n    correlations in the noise. (A) Sketch of the set up, with two\n    reliable oscillations $x(t)$ and $y(t)$ and a third, more noisy\n    oscillation $z(t)$. (B) The optimal phase relation that maximizes\n    the mutual information $I(x,y,z;t)$ and minimizes the mean error\n    $\\langle \\sigma_t \\rangle$, as a function\n    of $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$; here\n    $\\widetilde{\\sigma}_{x,y}$ is kept constant while\n    $\\widetilde{\\sigma}_z$ is varied. When\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z=0$, the third gene\n    $z(t)$ carries no information, and the optimal phase difference\n    $\\Delta \\phi_{xy} = \\phi_y - \\phi_x$ between the oscillations of\n    $x$ and $y$ is $\\Delta \\phi_{xy} = \\pi/2$, the result for $N=2$\n    oscillations; in this limit, the phase of $z$ is irrelevant. As\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$ increases, the\n    third oscillation $z(t)$ becomes more important. The phase\n    difference $\\Delta \\phi_{xy}$ between $x(t)$ and $y(t)$ decreases,\n    while the phase difference $\\Delta \\phi_{yz}$ between\n    $y(t)$ and $z(t)$ increases. When\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z=1$, all genes are\n    equally informative and $\\Delta \\phi_{xy} = \\Delta \\phi_{yz} =\n    \\Delta \\phi_{zx} = \\pi / 3$. (C) The mean error $\\langle \\sigma_t\n    \\rangle$ as a function\n    $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$. It decreases as\n    the third gene becomes more informative. (D) The mutual\n    $I(x,y,z;t)$ increases with $\\widetilde{\\sigma}_{x,y}/\\widetilde{\\sigma}_z$. }\n  \\label{fig:fig3}\n\\end{figure*}\n\n\n \\begin{figure*}[t]\n \\centering\n    \\includegraphics[scale=0.6]{Fig4.pdf}\n    \\caption{The importance of cross correlations between the\n      fluctuations in the oscillations of the readout proteins,\n      illustrated here for $N=2$ readout proteins. The top row shows\n      results for the scenario in which the relative noise strength\n      $\\widetilde{\\sigma}_i \\equiv \\sigma_i / A_i$ is low, while the\n      bottom panel displays the results for when it is large.  In all\n      cases, the relative noise strength of the two oscillations is\n      taken to be the same, $\\widetilde{\\sigma}_x =\n      \\widetilde{\\sigma}_y = \\widetilde{\\sigma}_{x,y}$. The panels in\n      the left column show a heat map of the mutual information\n      $I(x,y;t)$ as a function of the phase difference $\\Delta \\phi =\n      \\phi_y - \\phi_x$ between the two oscillations, and the\n      correlation coefficient $b$. Due to the symmetry of the problem\n      the mutual information is symmetric: $I(x,y;t)_{\\Delta \\phi,b} =\n      I(x,y;t)_{\\pi - \\Delta \\phi,-b}$. The top-left panel shows that\n      when the relative noise strength is low, the mutual information\n      is maximized for $|b|\\to 1$ and $\\Delta \\phi \\neq \\pi /\n      2$. Cross correlations thus change the optimal phase difference,\n      and more, importantly, they can enhance the mutual\n      information. However, when the relative noise is large, the\n      cross correlations become less important and the optimal phase\n      difference approaches $\\Delta \\phi = \\pi / 2$ (bottom left\n      panel). The middle panels elucidate how cross correlations can\n      affect the mutual information. Shown are, for different points\n      in the heat map on the left, the average trajectory that\n      ${x}(t)$ and $y(t)$ trace out during a 24 hr period (green\n      solid line), with superimposed, for different times of the day,\n      scatter points of $x(t)$ and $y(t)$, originating from gene\n      expression noise. The main axis of the contour\n      $\\bar{x}(t),\\bar{y}(t)$ is determined by the phase difference\n      $\\Delta \\phi$, while the main axis of the noise (scatter points)\n      is determined by the correlation coefficient $b$. There are\n      moments of the day where cross correlations cause the\n      distributions $P(x,y|t)$ of neighboring times $t$ to overlap\n      less, thus increasing mutual information, but also moments where\n      they increase the overlap, decreasing the mutual\n      information. The net benefit depends on how these contributions\n      are weighted. The system spends more time near the extrema of\n      $\\bar{x}(t),\\bar{y}(t)$, as illustrated in the right panels. Consequently,\n      when $\\Delta \\phi < \\pi / 2$, positive correlations $b>0$ enhance the\n      mutual information, especially when the relative noise strength\n      $\\widetilde{\\sigma}_x$ is low (point B top row). At higher\n      noise (bottom row), cross correlations are less effective in\n      reducing the overlap in $P(x,y|t)$ and the phase difference\n      $\\Delta \\phi$ becomes the dominant control parameter. }\n    \\label{fig:fig4}\n\\end{figure*}\n\n\n\n\n\\subsection{The importance of cross-correlations}\nSo far we have assumed that the noise in the expression of the\ndownstream genes is uncorrelated. However, in general, we expect their\nnoise to be correlated.  Direct or indirect regulatory interactions\nbetween the genes can lead to correlations or anti-correlations in the\nfluctuations of the protein concentrations \\cite{Walczak:2010cv}. And\nalso noise in the input signal can lead to correlated gene\nexpression. In fact, the extrinsic contribution to the noise in gene\nexpression is often larger than the intrinsic one\n\\cite{Taniguchi:2010cb}, which can induce pronounced correlations\nbetween the expression of the downstream genes. Intuitively, we may\nthink that if we need to infer an input variable $t$ from two output\nvariables $x$ and $y$, then cross-correlations between $x$ and $y$\nreduce the accuracy of the estimate---asking two persons $x$ and $y$\na question about $t$ seems to give more information when $x$ and $y$\ngive independent answers. However, this intuition is not always\ncorrect, as will become clear. Indeed, in this section\nwe study how correlations between the expression of downstream genes\naffect the precision by which cells can tell time.\n\n\n \\begin{figure*}[t]\n \\centering\n    \\includegraphics[scale=0.9]{Fig5.pdf}\n    \\caption{Importance of cross correlations in reducing the error in\n      estimating the time, as estimated from $N=2$ protein\n      oscillations. Heat maps of the the variance in the error\n      $(\\delta \\sigma_t)^2$ (A) and the mean error $\\langle \\sigma_t\n      \\rangle$ \n      (B), as a function of the phase difference $\\Delta \\phi$ between\n      the two oscillations and the correlation coefficient $b$ of the\n      fluctuations in the oscillations. The relative noise strength\n      $\\widetilde{\\sigma}_x$ is the same for both oscillations, and\n      equal to that of the low-noise scenario in Fig. \\ref{fig:fig4},\n      $\\widetilde{\\sigma}_x = 0.03$. It is seen that cross\n      correlations can reduce the mean error. Comparing against the\n      top-left panel of Fig. \\ref{fig:fig4} shows, however, that the\n      positions of the optima are different for the two quantities,\n      the mean error $\\langle \\sigma_t \\rangle$ and the mutual\n      information $I(x,y;t)$ , respectively. This is because the\n      quantities $\\sigma_t(t)$ (Eq. \\ref{sigmedio}) and $H(t|x,y)$\n      (Eq. \\ref{eq:info_H_2}) are averaged over different\n      distributions, the uniform distribution $P(t)$ and the\n      non-uniform distribution $P(x,y)$, respectively. }\n    \\label{fig:fig5}\n\\end{figure*}\n\nIn order to dissect the effect of cross-correlations, we study two\ndownstream genes, $N=2$, and take both the amplitudes of their\noscillations and their expression noise to be equal: $A_x = A_y = A$,\n$\\sigma_x=\\sigma_y=\\sigma_{x,y}$, respectively. Using the latter, we can renormalize the\ncovariance matrix Eq. \\ref{eq:C}:\n\n", "index": 27, "text": "\\begin{equation}\n{\\bf C}=  \\left( \\begin{array}{cc}\n\\sigma_x & \\mbox{cov}_{xy}  \\\\\n\\mbox{cov}_{xy} & \\sigma_y  \\\\\n \\end{array} \\right) = \\sigma_{x,y} \\left( \\begin{array}{cc}\n1 & b  \\\\\nb & 1  \\\\\n \\end{array} \\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"{\\bf C}=\\left(\\begin{array}[]{cc}\\sigma_{x}&amp;\\mbox{cov}_{xy}\\\\&#10;\\mbox{cov}_{xy}&amp;\\sigma_{y}\\\\&#10;\\end{array}\\right)=\\sigma_{x,y}\\left(\\begin{array}[]{cc}1&amp;b\\\\&#10;b&amp;1\\\\&#10;\\end{array}\\right),\" display=\"block\"><mrow><mrow><mi>\ud835\udc02</mi><mo>=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\u03c3</mi><mi>x</mi></msub></mtd><mtd columnalign=\"center\"><msub><mtext>cov</mtext><mrow><mi>x</mi><mo>\u2062</mo><mi>y</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mtext>cov</mtext><mrow><mi>x</mi><mo>\u2062</mo><mi>y</mi></mrow></msub></mtd><mtd columnalign=\"center\"><msub><mi>\u03c3</mi><mi>y</mi></msub></mtd></mtr></mtable><mo>)</mo></mrow><mo>=</mo><mrow><msub><mi>\u03c3</mi><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mi>b</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>b</mi></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nHere, $H(t)$ is the entropy of the input signal, with $P(t) = 1\n/T$. It does not depend on the design of the downstream readout\nsystem. In contrast, the second term, $\\langle H(t|x,y)\n\\rangle_{x,y}$, does depend on it. We now describe how changing\n$\\Delta \\phi$ and $b$ affects this term, using the scatter plots and\ndistributions in the middle and right column of Fig. \\ref{fig:fig4}.\n\nThe middle panel shows for different combinations of $b$ and $\\Delta\n\\phi$, corresponding to the points A,B,C,D in the heat map of\n$I(x,y;t)$ (left panel), scatter plots of $x(t)$ and $y(t)$.  The\noverall shape of each scatter plot is determined by the phase\ndifference $\\Delta \\phi$. When $\\Delta \\phi = \\pi / 2$ (points C and\nD), the average expression levels $\\bar{x}(t)$ and $\\bar{y}(t)$ trace\nout a circle in state space during a 24 hr period, while when $\\Delta\n\\phi = \\pi / 4$ (points A and B), they carve out an ellipsoidal path;\nthese mean paths are indicated by thin solid green lines in the\nscatter plots. For each moment of the day, however, $x$ and $y$ will\nexhibit a distribution of expression levels, due to gene expression\nnoise. This distribution $P(x,y|t)$ is shown as scatter points $(x,y)$\nfor different yet evenly spaced times $t$ in the respective subpanels.  When\nthe main axis of $P(x,y|t)$ is perpendicular to the local tangent of\nthe mean path of $\\bar{x}(t),\\bar{y}(t)$, then cross correlations\nreduce $H(t|x,y)$ for that period of the day: the cross correlations\ncause the distributions $P(x,y|t)$ for neighboring times $t$ to\noverlap less, meaning that a given point $(x,y)$ maps more uniquely\nonto a given time $t$. This tends to increase the mutual\ninformation. However, as the middle panel illustrates, there are not\nonly moments of the day when the main axis of the scatter points is\nperpendicular to the local tangent of the mean path, but also times\nwhen they are parallel, in which case cross correlations are\ndetrimental. Whether the net result of cross correlations is\nbeneficial, depends on how these different contributions are weighted:\n$H(t|x,y)$ has to be averaged over $P(x,y)$, see\nEq. \\ref{eq:info_H_2}. When $\\Delta \\phi = \\pi / 2$, the mean path\n$\\bar{x}(t),\\bar{y}(t)$ is circular, yet the net effect of\ncorrelations on the mutual information is already positive (left\npanel), and independent of the sign of $b$. For $\\Delta \\phi \\neq \\pi\n/ 2$, the effect depends on the sign of $b$. Moreover, the effect is\nalso stronger, because the\nsystem spends more\ntime near the extrema of $\\bar{x}(t),\\bar{y}(t)$, as the right panel\nillustrates.  When $\\Delta \\phi < \\pi / 2$, positive correlations in\nthe expression of $x$ and $y$ ($b>0$) cause the main axis of\n$P(x,y|t)$ to be perpendicular to the local tangent of\n$\\bar{x}(t),\\bar{y}(t)$ near the extrema (point B), thus increasing\nthe mutual information, while anti-correlations ($b<0$) cause\n$P(x,y|t)$ to be parallel to it (point A), decreasing the mutual\ninformation. For $\\Delta \\phi \\rightarrow \\Delta \\phi - \\pi/2$\nprecisely the oppositive behavior is observed, because the mean path\nof $\\bar{x}(t), \\bar{y}(t)$ (the ellipse) is flipped vertically.  The\nprincipal observation is thus that cross-correlations can enhance the\nmutual information by allowing for a less overlapping tiling of state\nspace, and hence a less redundant mapping between the input $t$ and\noutput $(x,y)$.\n \nFor higher noise (panels in lower row of Fig. \\ref{fig:fig4}), each $P(x,y|t)$\nbecomes wider, which means that the benefit of introducing cross\ncorrelations in reducing the overlap between different $P(x,y|t)$\n(corresponding to different times $t$), decreases. Indeed, at higher\nnoise, the mutual information depends much more weakly on the\nmagnitude of the cross correlations (left panel bottom row). The key\ncontrol parameter is now the phase shift $\\Delta \\phi$. For $\\Delta\n\\phi = \\pi / 2$, the distributions $P(x,y|t)$ are most evenly spaced. This\nminimizes the overlap between them and maximizes the mutual\ninformation.\n\nFig. \\ref{fig:fig5} shows the the variance in the error, $(\\delta\nt)^2$, and the average error in telling time, $\\langle\n\\sigma_t \\rangle$, as a\nfunction of $\\Delta \\phi$ and $b$, for $\\widetilde{\\sigma}=0.03$ (as\nin the top row of Fig. \\ref{fig:fig4}). It is seen that increasing\ncorrelations $|b|$ can reduce the average error. Surprisingly,\nhowever, for $|b|\\approx 1$, the average error $\\langle \\sigma_t\n\\rangle$ is minimized at a phase shift that does not maximize the\nmutual information, as a comparison with Fig. \\ref{fig:fig4} shows. \n\n\n\n\nThis is because of how the respective quantities are\naveraged. The quantity $\\sigma_t (t)$ is averaged over $P(t)$, which\nis uniform in time, while $H(t|x,y)$ is averaged over $P(x,y)$, which gives\nmore weight to those points $(x,y)$ that are more probable.\n\n\\section{Discussion}\nCells can increase the transmission of temporal information by\nincreasing the number of oscillatory signals $N$ used to infer the\ntime. In the analysis presented here, it is assumed that the system is\nlinear and obeys Gaussian statistics, yet, especially at high noise,\nit might be beneficial to use non-linear input-output relations to\nenhance information transmission \\cite{DeRonde:2014fq}. Nonetheless,\nour linear model highlights that this is a rich problem. The precision\nof telling time depends on the relative noise $\\widetilde{\\sigma}_i =\n\\sigma_i / A_i$ of the oscillatory signals, their phase shift, and the\ncross-correlations between them. When the relative noise\n$\\widetilde{\\sigma}_i$ is the same for all genes, the optimal phase\nrelation that maximizes the mutual information and minimizes the error\nis one in which the phases are spaced evenly. Under this condition,\nthe error in telling time is also uniform in time, provided that the\nnoise $\\sigma_i(t)$ is constant in time, which, to a good\napproximation, is the case when the amplitude of the oscillations is\nlarge compared to the mean. This is akin to what has been observed for\nthe fruitfly {\\it Drosophila}, where the expression pattern of the gap\ngenes allows the nuclei to specificy their position with nearly\nuniform precision along the anterior-posterior axis \\cite{Dubuis2013}.\nWhen the relative noise amplitudes $\\widetilde{\\sigma}_i$ are not the\nsame for all signals, then the design principle for maximizing\ninformation transmission is that the oscillatory signals which are\nmore reliable, should be spaced more evenly.  Lastly, we have\naddressed the role of cross correlations between the fluctuations in\nthe oscillatory signals. When the relative noise is large,\ncross-correlations do not significantly affect information\ntransmission. However, the situation changes markedly in the low-noise\nregime. In this regime, cross-correlations change the optimal phase\nshift that maximizes information transmission. More strikingly, they\ncan increase the mutual information. At low noise, cross correlations\ncan thus reduce the error in telling time and enhance the transmission\nof temporal information. This phenomenon is similar to what has been\nobserved for neural networks \\cite{Tkacik2010} and spatial gene\nexpression patterns during embryonic development, where\ncross-regulatory interactions between genes can enhance the precision\nby which cells or nuclei determine their spatial position within\nthe developing embryo \\cite{Tkacik:2009ta,Walczak:2010cv,Dubuis2013}. In\nall these cases the principle is that cross-correlations make it\npossible to tile the output space more efficiently, thus allowing for\na less redundant input-output mapping. This is particularly important\nwhen the noise is low, and noise averaging is not important, but\nefficient tiling of state space is\n\\cite{Tkacik:2009ta,Walczak:2010cv}.\n\nThe question that remains is how cells can optimize the relative noise\nof the oscillatory signals, their phase difference and their\ncross-correlations. Fluctuations in the input will lead to correlated\nfluctuations in the oscillations of the output components. Our\nanalysis shows that these correlations can be beneficial. Moreover,\nthey can be tailored via cross-regulatory interactions between the\ntarget genes downstream, as in the gap-gene system of {\\it Drosophila}\n\\cite{Tkacik:2009ta,Walczak:2010cv,Dubuis2013}. Here, it should be\nrealized that in our analysis we assume that the noise is uncorrelated\nfrom the signal; indeed, the mean trajectory $(\\bar{x}(t),\\bar{y}(t))$\ndoes not depend on the noise. Cross-regulatory interactions will,\nhowever, not only affect the noise and hence $P(x,y|t)$, but also the\nmean trajectory $(\\bar{x}(t),\\bar{y}(t))$. This will not change the principle\nthat noise correlations can enhance the input-ouput mapping, but it\nwill influence the magnitude of the effect. On the other hand,\nextrinsic noise sources such as the availability of ribosomes, may\nlead to correlated fluctuations in the expression of $x(t)$ and\n$y(t)$, while leaving their mean unchanged, as assumed\nhere. Experiments will have to tell whether cells use noise\ncorrelations to enhance the precision of telling time. The\ncyanobacterium {\\it S. elongatus} is arguably the best model system to\ntest these ideas. It will certainly be of interest to investigate\nwhether {\\it S. elongatus} exploits cross-regulatory interactions\nbetween the genes downstream from RpaA to enhance its information on\ntime.\n\n\nThe relative noise of the oscillations depends on the noise $\\sigma_i$\nand the amplitude $A_i$ of the oscillations.  The contribution from\nthe intrinsic noise is expected to scale with the copy number $X$ as\n$\\sigma_{{\\rm in},i} \\sim \\sqrt{X_i}$, which, if the amplitude is\nsmall compared to the mean $r_i$, means that $\\sigma_{{\\rm in},i} \\sim\n\\sqrt{r_i}$. The relative intrinsic noise thus goes as\n$\\widetilde{\\sigma}_{{\\rm in},i} \\sim \\sqrt{r_i} / A_i$. For the model\npresented in section \\ref{sec:Model}, it is given by\n\\begin{eqnarray}\n\\widetilde{\\sigma}_{{\\rm in},i} &\\simeq& \\sqrt{r_i} / A_i\\\\\n&=&  \\sqrt{r_s/\n  f_i}\\sqrt{(\\mu_i^2 + \\omega^2)/\\mu_i} / A_s.\n\\end{eqnarray} \nClearly, the relative noise strength $\\widetilde{\\sigma}_{{\\rm in},i}$\ndecreases with $A_s$: the amplitude of the oscillations of the readout\nis proportional to that of the input. The relative noise strength\ndecreases with the square root of $f_i$, because the gain $f_i$\nincreases not only the amplitude of the output oscillations, $A_i\n\\propto f_i$, but also their mean $r_i$ and thereby the noise,\n$\\sigma_{{\\rm in},i} \\propto \\sqrt{r_i} \\propto \\sqrt{f_i}$. It\nincreases with the mean $r_s$ of the input oscillations, because that\nincreases the mean $r_i$ of the output oscillations and thereby the\nnoise $\\sigma_{{\\rm in},i}$, but not their amplitude, thus decreasing\nthe relative noise strength $\\sigma_{{\\rm in},i} / A_i$. Finally,\nthere exists an optimal protein decay rate $\\mu_{\\rm opt}=\\omega$ that\nminimizes the relative noise strength and hence maximizes information\ntransmission. This optimum arises from a trade-off between the\namplitude of the signal and the intrinsic noise: for $\\mu \\gg \\omega$,\nincreasing $\\mu$ reduces the gain and hence the amplitude $A_i$ as\n$A_i \\propto 1/\\mu$ (Eq. \\ref{eq:Amp}) while the noise decreases\nmore slowly as $\\sqrt{r_i}\\propto 1/\\sqrt{\\mu}$, thus increasing the relative noise\nstrength $\\widetilde{\\sigma}_{{\\rm in},i}$; in contrast, for $\\mu \\ll\n\\omega$, the amplitude $A_i$ becomes independent of $\\mu_i$\n(Eq. \\ref{eq:Amp}) while the\nnoise continues to rise as $\\mu_i$ decreases, thus again increasing\nthe relative noise strength.\n\nFor the transmission of a fluctuating input signal, a similar\ntrade-off between the gain and the intrinsic noise has been observed\nin \\cite{tostevin10} and a related trade-off between mechanistic error\narising from the intrinsic noise and dynamical error due to the\ndistortion of the input signal has been described in\n\\cite{Bowsher:2013jh}. A seemingly similar but distinct trade-off,\nalso leading to an optimal decay rate of the output component, has\nbeen reported in \\cite{Becker2015}: in that study the optimal decay\nrate arises from the trade-off between tracking the input signal and\nintegrating out the noise in the input signal. Indeed, in our\ndiscussion here, we have so far ignored the extrinsic noise in the\ninput signal, and only focused on the intrinsic noise. However, the\ndecay rate $\\mu_i$ does\nnot only affect the output copy number and thereby the intrinsic\nnoise, it also determines how effectively fluctuations in the input\nsignal can be integrated out. More\nspecifically, if the noise in the input $\\xi_s$ (Eq. \\ref{eq:eta}) is\nindependent from the input signal, has amplitude $\\sigma_s$ and decays\nexponentially with correlation time $\\lambda$, then we expect that the\nextrinsic contribution to the output noise is $\\sigma^2_{{\\rm ex},i} =\ng_i^2 \\mu_i / (\\mu_i+\\lambda) \\sigma^2_s$\n\\cite{paulsson2003,tanase-nicola06}, where the gain is\n$g_i=f_i/\\mu_i$. Hence, the relative extrinsic noise is\n\n", "itemtype": "equation", "pos": 40391, "prevtext": " \nwhere $b$ is the correlation coefficient, denoting the\ncross-correlation strength: $b=1$ implies that the noise in the\nexpression of X and Y is fully correlated, while $b=-1$ implies full\nanti-correlation. We computed numerically how $I(x,y;t)$, $\\langle\n\\sigma_t\\rangle_t$ and $(\\delta t)^2$ depend on the phase shift\n$\\Delta \\phi = \\phi_y - \\phi_x$, the relative noise strength\n$\\widetilde{\\sigma}_{x,y} = \\sigma_{x,y} / A$, and the correlation\ncoeffient $b$.\n\nFig. \\ref{fig:fig4} shows the mutual information $I(x,y;t)$ as a\nfunction of $\\Delta \\phi$ and $b$, both for low noise, with\n$\\widetilde{\\sigma}_{x,y} = 0.03$ (panels top row), and high noise, with\n$\\widetilde{\\sigma}_{x,y} = 0.4$ (panels bottom row).  The following points\nare worthy of note. First, as expected, $I(x,y;t)$ is symmetric with\nrespect to $\\Delta \\phi$ and $b$: $I(x,y;t)_{\\Delta \\phi,\n  b}=I(x,y;t)_{2\\pi - \\Delta \\phi,-b}$. Secondly, depending on the\nphase shift $\\Delta \\phi$, correlations ($b>0$) or\nanti-correlations ($b<0$) can enhance the mutual information,\nespecially when  the relative\nnoise strength $\\widetilde{\\sigma}_{x,y}$ is low (top panel).\nConcomittantly, the optimal phase shift $\\Delta \\phi$ that maximizes the mutual\ninformation depends on the cross correlation $b$. At low noise, the\nmutual information is maximized either at $0<\\Delta \\phi^*<\\pi / 2$\nand $b\\approx 1$ or at $\\pi - \\Delta \\phi^*$ and $b\\approx -1$. At high\nnoise, cross correlations no longers help to improve the mutual\ninformation  (bottom panel). Moreover, the optimal phase shift is at $\\Delta \\phi^* \\approx\n\\pi / 2$. We now discuss the origin of these observations.\n\nTo elucidate these observations, we start from the definition of the\nmutual information (see Eq. \\ref{eq:info_H}):\n\n", "index": 29, "text": "\\begin{equation}\nI(x,y;t) = H(t) - \\langle H(t|x,y) \\rangle_{x,y}\\label{eq:info_H_2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"I(x,y;t)=H(t)-\\langle H(t|x,y)\\rangle_{x,y}\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>;</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\n We\nfirst note that, in contrast to the relative contribution of the\nintrinsic noise, $\\sigma_{{\\rm in,i}}/A_i$, the relative extrinsic noise\ndoes not depend on $f_i$: increasing $f_i$ raises not only the\namplitude of the signal, but also that of the noise; increasing $f_i$ is\nthus only useful in raising the signal above the {\\em intrinsic} noise. Secondly,\nfor $\\mu_i \\gg \\omega,\\lambda$, $\\sigma_{{\\rm ex},i}/A_i \\simeq\n\\sigma_s$, because the time integration factor $\\mu_i /\n(\\mu_i+\\lambda)$ becomes constant (independent of $\\mu_i$), and both\nthe amplitude of the signal, $A_i$, and the amplification of the input\nnoise, $g_i$ decrease as $\\mu_i^{-1}$. For $\\mu_i \\ll \\omega,\n\\lambda$, $\\sigma_{{\\rm ex},i}/A_i \\simeq \\omega \\sigma_s\n/\\sqrt{\\mu_i\\lambda}$, because the amplitude $A_i$ becomes independent\nof $\\mu_i$, while the extrinsic contribution $\\sigma_{{\\rm ex},i}$\nrises with decreasing $\\mu_i$ as $1/\\sqrt{\\mu_i}$. In fact, the\nrelative strength of the extrinsic noise $\\sigma^2_{{\\rm ex},}/A_i$\nhas a minimum at $\\mu_{\\rm ex}^{\\rm opt} = (\\omega^2 / \\lambda)\n(1+\\sqrt{1+(\\lambda/\\omega)^2})$. We thus conclude that both the\nrelative strength of the intrinsic and extrinsic noise exhibit a\nminimum as function of $\\mu_i$, meaning that there is an optimal\nprotein lifetime that maximizes information transmission. \n\nLastly, how could cells optimize the phase relation between the oscillations of\nthe readout proteins? In the simple model of \\ref{sec:Model} there is\nonly one control variable, namely the protein degradation rate\n(Eq. \\ref{eq:phase}). Clearly, it is not possible, in general, to\nsimultaneously set the decay rate such that the relative noise\nstrength is minimized, as described above, and the phase difference is\noptimized. However, the simple model of \\ref{sec:Model} ignores that\ngene expression is, in fact, a multi-step process leading to a delay,\nand it is possible that nature has tuned this delay so as to optimize\nthe phase relation between the output oscillations. In addition, cells\ncould use gene expression cascades to adjust the delay. Whether cells\nemploy these mechanisms to optimize the phase relation is an interesting\nquestion for future work.\n\n\\appendix\n\\section{The optimal phase relation in the absence of cross\n  correlations}\nWe would like to compute the phase relation that minimizes the\nvariance of the error, $(\\delta \\sigma_t)^2$, as given by\nEq. \\ref{eq:variance_error}, in the absence of cross correlations. However, the problem is that\nEq. \\ref{eq:sigt} is an expression for $\\sigma_t^{-2}(t)$, not\n$\\sigma_t(t)$. Hence, while it is fairly straightforward to\nderive the variance of $\\sigma_t^{-2}$, i.e. $\\langle\n(\\sigma_t^{-2})^2\\rangle - \\langle \\sigma_t^{-2}\\rangle^2$, it is\nimpossible, in general, to derive analytically the variance of the\nquantity we are interested in, $(\\delta \\sigma_t)^2=\\langle\n\\sigma_t^2\\rangle - \\langle \\sigma_t\\rangle^2$. However, we know that\nif the variance of a function $g(t)$ is zero, $\\sigma^2_g = \\int_0^T\ndt P(t) (g(t) - \\langle g(t) \\rangle)^2=0$, and $g(t)$ is thus a\nconstant (independent of time), that then a) $\\langle f(g(t))\\rangle =\nf(\\langle g(t) \\rangle)$ and b) the variance of $f(t)=f(g(t))$ is zero,\n$\\sigma^2_f = \\langle f^2 \\rangle - \\langle f\\rangle^2 = 0$. We now apply this logic with the\nidentification $g(t) = \\sigma_t (t)$ and $f(t)=g^{-2}(t)$. The trick\nthat we thus employ is to establish that the variance which we can\ncompute, $\\sigma^2_f = \\langle (\\sigma_t^{-2})^2\\rangle - \\langle\n\\sigma_t^{-2}\\rangle^2$, is zero. If this is true, then we know that a)\nthe variance of the quantity that we are interested in, $\\sigma^2_g = (\\delta\n\\sigma_t)^2$, must be zero as well. Moreover, we then also know that b)\n$\\langle \\sigma_t \\rangle = \\sigma_t = 1/\\sqrt{\\langle \\sigma^{-2}_t (t)\\rangle}$.\n\nThere are two points worthy of note. First, as mentioned, above, when\n$\\sigma^2_f = \\langle (\\sigma^{-2}_t (t)) ^2\\rangle - \\langle\n\\sigma^{-2}_t (t) \\rangle^2 = 0$, then $(\\delta \\sigma_t)^2=0$. In\nthis case, the phase relation that minimizes $\\sigma^2_f$ is the phase\nrelation that minizes $\\delta \\sigma^2_t$ (making it\nzero indeed). However, when $\\sigma^2_f \\neq 0$, then the phase\nrelation that minimizes $\\sigma^2_f$ is {\\em not} necessarily the phase\nrelation that minimzes $\\delta \\sigma^2_t$. Secondly, the phase\nrelation that minimizes $(\\delta\n\\sigma_t)^2$, is not necessarily the phase relation that minimizes\n$\\sigma_t$, {\\em even when $(\\delta \\sigma_t)^2=0$}. We need to check\neither numerically or, if possible, by analytically minizing $\\langle\n\\sigma_t \\rangle$ whether this is true or not. The same holds for the\nmutual information: the phase relation that minimizes  $(\\delta\n\\sigma_t)^2$, is not necessarily the phase relation that maximizes the\nmutual information.\n\n\\subsection{The phase relation that minimizes $(\\delta \\sigma_t)^2$\n  when the relative noise strengths are the same}\n\nAs explained above, to obtain the optimal phase relation that makes\n$(\\delta \\sigma_t)^2=0$, we aim to find the phase distribution for which:\n\n", "itemtype": "equation", "pos": 53398, "prevtext": "\nHere, $H(t)$ is the entropy of the input signal, with $P(t) = 1\n/T$. It does not depend on the design of the downstream readout\nsystem. In contrast, the second term, $\\langle H(t|x,y)\n\\rangle_{x,y}$, does depend on it. We now describe how changing\n$\\Delta \\phi$ and $b$ affects this term, using the scatter plots and\ndistributions in the middle and right column of Fig. \\ref{fig:fig4}.\n\nThe middle panel shows for different combinations of $b$ and $\\Delta\n\\phi$, corresponding to the points A,B,C,D in the heat map of\n$I(x,y;t)$ (left panel), scatter plots of $x(t)$ and $y(t)$.  The\noverall shape of each scatter plot is determined by the phase\ndifference $\\Delta \\phi$. When $\\Delta \\phi = \\pi / 2$ (points C and\nD), the average expression levels $\\bar{x}(t)$ and $\\bar{y}(t)$ trace\nout a circle in state space during a 24 hr period, while when $\\Delta\n\\phi = \\pi / 4$ (points A and B), they carve out an ellipsoidal path;\nthese mean paths are indicated by thin solid green lines in the\nscatter plots. For each moment of the day, however, $x$ and $y$ will\nexhibit a distribution of expression levels, due to gene expression\nnoise. This distribution $P(x,y|t)$ is shown as scatter points $(x,y)$\nfor different yet evenly spaced times $t$ in the respective subpanels.  When\nthe main axis of $P(x,y|t)$ is perpendicular to the local tangent of\nthe mean path of $\\bar{x}(t),\\bar{y}(t)$, then cross correlations\nreduce $H(t|x,y)$ for that period of the day: the cross correlations\ncause the distributions $P(x,y|t)$ for neighboring times $t$ to\noverlap less, meaning that a given point $(x,y)$ maps more uniquely\nonto a given time $t$. This tends to increase the mutual\ninformation. However, as the middle panel illustrates, there are not\nonly moments of the day when the main axis of the scatter points is\nperpendicular to the local tangent of the mean path, but also times\nwhen they are parallel, in which case cross correlations are\ndetrimental. Whether the net result of cross correlations is\nbeneficial, depends on how these different contributions are weighted:\n$H(t|x,y)$ has to be averaged over $P(x,y)$, see\nEq. \\ref{eq:info_H_2}. When $\\Delta \\phi = \\pi / 2$, the mean path\n$\\bar{x}(t),\\bar{y}(t)$ is circular, yet the net effect of\ncorrelations on the mutual information is already positive (left\npanel), and independent of the sign of $b$. For $\\Delta \\phi \\neq \\pi\n/ 2$, the effect depends on the sign of $b$. Moreover, the effect is\nalso stronger, because the\nsystem spends more\ntime near the extrema of $\\bar{x}(t),\\bar{y}(t)$, as the right panel\nillustrates.  When $\\Delta \\phi < \\pi / 2$, positive correlations in\nthe expression of $x$ and $y$ ($b>0$) cause the main axis of\n$P(x,y|t)$ to be perpendicular to the local tangent of\n$\\bar{x}(t),\\bar{y}(t)$ near the extrema (point B), thus increasing\nthe mutual information, while anti-correlations ($b<0$) cause\n$P(x,y|t)$ to be parallel to it (point A), decreasing the mutual\ninformation. For $\\Delta \\phi \\rightarrow \\Delta \\phi - \\pi/2$\nprecisely the oppositive behavior is observed, because the mean path\nof $\\bar{x}(t), \\bar{y}(t)$ (the ellipse) is flipped vertically.  The\nprincipal observation is thus that cross-correlations can enhance the\nmutual information by allowing for a less overlapping tiling of state\nspace, and hence a less redundant mapping between the input $t$ and\noutput $(x,y)$.\n \nFor higher noise (panels in lower row of Fig. \\ref{fig:fig4}), each $P(x,y|t)$\nbecomes wider, which means that the benefit of introducing cross\ncorrelations in reducing the overlap between different $P(x,y|t)$\n(corresponding to different times $t$), decreases. Indeed, at higher\nnoise, the mutual information depends much more weakly on the\nmagnitude of the cross correlations (left panel bottom row). The key\ncontrol parameter is now the phase shift $\\Delta \\phi$. For $\\Delta\n\\phi = \\pi / 2$, the distributions $P(x,y|t)$ are most evenly spaced. This\nminimizes the overlap between them and maximizes the mutual\ninformation.\n\nFig. \\ref{fig:fig5} shows the the variance in the error, $(\\delta\nt)^2$, and the average error in telling time, $\\langle\n\\sigma_t \\rangle$, as a\nfunction of $\\Delta \\phi$ and $b$, for $\\widetilde{\\sigma}=0.03$ (as\nin the top row of Fig. \\ref{fig:fig4}). It is seen that increasing\ncorrelations $|b|$ can reduce the average error. Surprisingly,\nhowever, for $|b|\\approx 1$, the average error $\\langle \\sigma_t\n\\rangle$ is minimized at a phase shift that does not maximize the\nmutual information, as a comparison with Fig. \\ref{fig:fig4} shows. \n\n\n\n\nThis is because of how the respective quantities are\naveraged. The quantity $\\sigma_t (t)$ is averaged over $P(t)$, which\nis uniform in time, while $H(t|x,y)$ is averaged over $P(x,y)$, which gives\nmore weight to those points $(x,y)$ that are more probable.\n\n\\section{Discussion}\nCells can increase the transmission of temporal information by\nincreasing the number of oscillatory signals $N$ used to infer the\ntime. In the analysis presented here, it is assumed that the system is\nlinear and obeys Gaussian statistics, yet, especially at high noise,\nit might be beneficial to use non-linear input-output relations to\nenhance information transmission \\cite{DeRonde:2014fq}. Nonetheless,\nour linear model highlights that this is a rich problem. The precision\nof telling time depends on the relative noise $\\widetilde{\\sigma}_i =\n\\sigma_i / A_i$ of the oscillatory signals, their phase shift, and the\ncross-correlations between them. When the relative noise\n$\\widetilde{\\sigma}_i$ is the same for all genes, the optimal phase\nrelation that maximizes the mutual information and minimizes the error\nis one in which the phases are spaced evenly. Under this condition,\nthe error in telling time is also uniform in time, provided that the\nnoise $\\sigma_i(t)$ is constant in time, which, to a good\napproximation, is the case when the amplitude of the oscillations is\nlarge compared to the mean. This is akin to what has been observed for\nthe fruitfly {\\it Drosophila}, where the expression pattern of the gap\ngenes allows the nuclei to specificy their position with nearly\nuniform precision along the anterior-posterior axis \\cite{Dubuis2013}.\nWhen the relative noise amplitudes $\\widetilde{\\sigma}_i$ are not the\nsame for all signals, then the design principle for maximizing\ninformation transmission is that the oscillatory signals which are\nmore reliable, should be spaced more evenly.  Lastly, we have\naddressed the role of cross correlations between the fluctuations in\nthe oscillatory signals. When the relative noise is large,\ncross-correlations do not significantly affect information\ntransmission. However, the situation changes markedly in the low-noise\nregime. In this regime, cross-correlations change the optimal phase\nshift that maximizes information transmission. More strikingly, they\ncan increase the mutual information. At low noise, cross correlations\ncan thus reduce the error in telling time and enhance the transmission\nof temporal information. This phenomenon is similar to what has been\nobserved for neural networks \\cite{Tkacik2010} and spatial gene\nexpression patterns during embryonic development, where\ncross-regulatory interactions between genes can enhance the precision\nby which cells or nuclei determine their spatial position within\nthe developing embryo \\cite{Tkacik:2009ta,Walczak:2010cv,Dubuis2013}. In\nall these cases the principle is that cross-correlations make it\npossible to tile the output space more efficiently, thus allowing for\na less redundant input-output mapping. This is particularly important\nwhen the noise is low, and noise averaging is not important, but\nefficient tiling of state space is\n\\cite{Tkacik:2009ta,Walczak:2010cv}.\n\nThe question that remains is how cells can optimize the relative noise\nof the oscillatory signals, their phase difference and their\ncross-correlations. Fluctuations in the input will lead to correlated\nfluctuations in the oscillations of the output components. Our\nanalysis shows that these correlations can be beneficial. Moreover,\nthey can be tailored via cross-regulatory interactions between the\ntarget genes downstream, as in the gap-gene system of {\\it Drosophila}\n\\cite{Tkacik:2009ta,Walczak:2010cv,Dubuis2013}. Here, it should be\nrealized that in our analysis we assume that the noise is uncorrelated\nfrom the signal; indeed, the mean trajectory $(\\bar{x}(t),\\bar{y}(t))$\ndoes not depend on the noise. Cross-regulatory interactions will,\nhowever, not only affect the noise and hence $P(x,y|t)$, but also the\nmean trajectory $(\\bar{x}(t),\\bar{y}(t))$. This will not change the principle\nthat noise correlations can enhance the input-ouput mapping, but it\nwill influence the magnitude of the effect. On the other hand,\nextrinsic noise sources such as the availability of ribosomes, may\nlead to correlated fluctuations in the expression of $x(t)$ and\n$y(t)$, while leaving their mean unchanged, as assumed\nhere. Experiments will have to tell whether cells use noise\ncorrelations to enhance the precision of telling time. The\ncyanobacterium {\\it S. elongatus} is arguably the best model system to\ntest these ideas. It will certainly be of interest to investigate\nwhether {\\it S. elongatus} exploits cross-regulatory interactions\nbetween the genes downstream from RpaA to enhance its information on\ntime.\n\n\nThe relative noise of the oscillations depends on the noise $\\sigma_i$\nand the amplitude $A_i$ of the oscillations.  The contribution from\nthe intrinsic noise is expected to scale with the copy number $X$ as\n$\\sigma_{{\\rm in},i} \\sim \\sqrt{X_i}$, which, if the amplitude is\nsmall compared to the mean $r_i$, means that $\\sigma_{{\\rm in},i} \\sim\n\\sqrt{r_i}$. The relative intrinsic noise thus goes as\n$\\widetilde{\\sigma}_{{\\rm in},i} \\sim \\sqrt{r_i} / A_i$. For the model\npresented in section \\ref{sec:Model}, it is given by\n\\begin{eqnarray}\n\\widetilde{\\sigma}_{{\\rm in},i} &\\simeq& \\sqrt{r_i} / A_i\\\\\n&=&  \\sqrt{r_s/\n  f_i}\\sqrt{(\\mu_i^2 + \\omega^2)/\\mu_i} / A_s.\n\\end{eqnarray} \nClearly, the relative noise strength $\\widetilde{\\sigma}_{{\\rm in},i}$\ndecreases with $A_s$: the amplitude of the oscillations of the readout\nis proportional to that of the input. The relative noise strength\ndecreases with the square root of $f_i$, because the gain $f_i$\nincreases not only the amplitude of the output oscillations, $A_i\n\\propto f_i$, but also their mean $r_i$ and thereby the noise,\n$\\sigma_{{\\rm in},i} \\propto \\sqrt{r_i} \\propto \\sqrt{f_i}$. It\nincreases with the mean $r_s$ of the input oscillations, because that\nincreases the mean $r_i$ of the output oscillations and thereby the\nnoise $\\sigma_{{\\rm in},i}$, but not their amplitude, thus decreasing\nthe relative noise strength $\\sigma_{{\\rm in},i} / A_i$. Finally,\nthere exists an optimal protein decay rate $\\mu_{\\rm opt}=\\omega$ that\nminimizes the relative noise strength and hence maximizes information\ntransmission. This optimum arises from a trade-off between the\namplitude of the signal and the intrinsic noise: for $\\mu \\gg \\omega$,\nincreasing $\\mu$ reduces the gain and hence the amplitude $A_i$ as\n$A_i \\propto 1/\\mu$ (Eq. \\ref{eq:Amp}) while the noise decreases\nmore slowly as $\\sqrt{r_i}\\propto 1/\\sqrt{\\mu}$, thus increasing the relative noise\nstrength $\\widetilde{\\sigma}_{{\\rm in},i}$; in contrast, for $\\mu \\ll\n\\omega$, the amplitude $A_i$ becomes independent of $\\mu_i$\n(Eq. \\ref{eq:Amp}) while the\nnoise continues to rise as $\\mu_i$ decreases, thus again increasing\nthe relative noise strength.\n\nFor the transmission of a fluctuating input signal, a similar\ntrade-off between the gain and the intrinsic noise has been observed\nin \\cite{tostevin10} and a related trade-off between mechanistic error\narising from the intrinsic noise and dynamical error due to the\ndistortion of the input signal has been described in\n\\cite{Bowsher:2013jh}. A seemingly similar but distinct trade-off,\nalso leading to an optimal decay rate of the output component, has\nbeen reported in \\cite{Becker2015}: in that study the optimal decay\nrate arises from the trade-off between tracking the input signal and\nintegrating out the noise in the input signal. Indeed, in our\ndiscussion here, we have so far ignored the extrinsic noise in the\ninput signal, and only focused on the intrinsic noise. However, the\ndecay rate $\\mu_i$ does\nnot only affect the output copy number and thereby the intrinsic\nnoise, it also determines how effectively fluctuations in the input\nsignal can be integrated out. More\nspecifically, if the noise in the input $\\xi_s$ (Eq. \\ref{eq:eta}) is\nindependent from the input signal, has amplitude $\\sigma_s$ and decays\nexponentially with correlation time $\\lambda$, then we expect that the\nextrinsic contribution to the output noise is $\\sigma^2_{{\\rm ex},i} =\ng_i^2 \\mu_i / (\\mu_i+\\lambda) \\sigma^2_s$\n\\cite{paulsson2003,tanase-nicola06}, where the gain is\n$g_i=f_i/\\mu_i$. Hence, the relative extrinsic noise is\n\n", "index": 31, "text": "\\begin{equation}\n\\sigma_{{\\rm\n    ex},i}/A_i=1/\\mu_i\\sqrt{(\\mu_i^2+\\omega^2)\\mu_i/(\\mu_i+\\lambda)}\\sigma_s.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{{\\rm ex},i}/A_{i}=1/\\mu_{i}\\sqrt{(\\mu_{i}^{2}+\\omega^{2})\\mu_{i}/(\\mu_%&#10;{i}+\\lambda)}\\sigma_{s}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c3</mi><mrow><mi>ex</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>/</mo><msub><mi>A</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>\u03bc</mi><mi>i</mi></msub></mrow><mo>\u2062</mo><msqrt><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bc</mi><mi>i</mi><mn>2</mn></msubsup><mo>+</mo><msup><mi>\u03c9</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>i</mi></msub></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mi>i</mi></msub><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt><mo>\u2062</mo><msub><mi>\u03c3</mi><mi>s</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nWhen the cross correlations are zero, $\\sigma^{-2}_t (t)$ is given by\nEq. \\ref{eq:sigT}. The second term in the expression above, $\\langle\n\\sigma^{-2}_t (t) \\rangle^2$, is then, for the case that the noise and\nthe ampltidues are the same for all genes, given by\n\n", "itemtype": "equation", "pos": 58585, "prevtext": "\n We\nfirst note that, in contrast to the relative contribution of the\nintrinsic noise, $\\sigma_{{\\rm in,i}}/A_i$, the relative extrinsic noise\ndoes not depend on $f_i$: increasing $f_i$ raises not only the\namplitude of the signal, but also that of the noise; increasing $f_i$ is\nthus only useful in raising the signal above the {\\em intrinsic} noise. Secondly,\nfor $\\mu_i \\gg \\omega,\\lambda$, $\\sigma_{{\\rm ex},i}/A_i \\simeq\n\\sigma_s$, because the time integration factor $\\mu_i /\n(\\mu_i+\\lambda)$ becomes constant (independent of $\\mu_i$), and both\nthe amplitude of the signal, $A_i$, and the amplification of the input\nnoise, $g_i$ decrease as $\\mu_i^{-1}$. For $\\mu_i \\ll \\omega,\n\\lambda$, $\\sigma_{{\\rm ex},i}/A_i \\simeq \\omega \\sigma_s\n/\\sqrt{\\mu_i\\lambda}$, because the amplitude $A_i$ becomes independent\nof $\\mu_i$, while the extrinsic contribution $\\sigma_{{\\rm ex},i}$\nrises with decreasing $\\mu_i$ as $1/\\sqrt{\\mu_i}$. In fact, the\nrelative strength of the extrinsic noise $\\sigma^2_{{\\rm ex},}/A_i$\nhas a minimum at $\\mu_{\\rm ex}^{\\rm opt} = (\\omega^2 / \\lambda)\n(1+\\sqrt{1+(\\lambda/\\omega)^2})$. We thus conclude that both the\nrelative strength of the intrinsic and extrinsic noise exhibit a\nminimum as function of $\\mu_i$, meaning that there is an optimal\nprotein lifetime that maximizes information transmission. \n\nLastly, how could cells optimize the phase relation between the oscillations of\nthe readout proteins? In the simple model of \\ref{sec:Model} there is\nonly one control variable, namely the protein degradation rate\n(Eq. \\ref{eq:phase}). Clearly, it is not possible, in general, to\nsimultaneously set the decay rate such that the relative noise\nstrength is minimized, as described above, and the phase difference is\noptimized. However, the simple model of \\ref{sec:Model} ignores that\ngene expression is, in fact, a multi-step process leading to a delay,\nand it is possible that nature has tuned this delay so as to optimize\nthe phase relation between the output oscillations. In addition, cells\ncould use gene expression cascades to adjust the delay. Whether cells\nemploy these mechanisms to optimize the phase relation is an interesting\nquestion for future work.\n\n\\appendix\n\\section{The optimal phase relation in the absence of cross\n  correlations}\nWe would like to compute the phase relation that minimizes the\nvariance of the error, $(\\delta \\sigma_t)^2$, as given by\nEq. \\ref{eq:variance_error}, in the absence of cross correlations. However, the problem is that\nEq. \\ref{eq:sigt} is an expression for $\\sigma_t^{-2}(t)$, not\n$\\sigma_t(t)$. Hence, while it is fairly straightforward to\nderive the variance of $\\sigma_t^{-2}$, i.e. $\\langle\n(\\sigma_t^{-2})^2\\rangle - \\langle \\sigma_t^{-2}\\rangle^2$, it is\nimpossible, in general, to derive analytically the variance of the\nquantity we are interested in, $(\\delta \\sigma_t)^2=\\langle\n\\sigma_t^2\\rangle - \\langle \\sigma_t\\rangle^2$. However, we know that\nif the variance of a function $g(t)$ is zero, $\\sigma^2_g = \\int_0^T\ndt P(t) (g(t) - \\langle g(t) \\rangle)^2=0$, and $g(t)$ is thus a\nconstant (independent of time), that then a) $\\langle f(g(t))\\rangle =\nf(\\langle g(t) \\rangle)$ and b) the variance of $f(t)=f(g(t))$ is zero,\n$\\sigma^2_f = \\langle f^2 \\rangle - \\langle f\\rangle^2 = 0$. We now apply this logic with the\nidentification $g(t) = \\sigma_t (t)$ and $f(t)=g^{-2}(t)$. The trick\nthat we thus employ is to establish that the variance which we can\ncompute, $\\sigma^2_f = \\langle (\\sigma_t^{-2})^2\\rangle - \\langle\n\\sigma_t^{-2}\\rangle^2$, is zero. If this is true, then we know that a)\nthe variance of the quantity that we are interested in, $\\sigma^2_g = (\\delta\n\\sigma_t)^2$, must be zero as well. Moreover, we then also know that b)\n$\\langle \\sigma_t \\rangle = \\sigma_t = 1/\\sqrt{\\langle \\sigma^{-2}_t (t)\\rangle}$.\n\nThere are two points worthy of note. First, as mentioned, above, when\n$\\sigma^2_f = \\langle (\\sigma^{-2}_t (t)) ^2\\rangle - \\langle\n\\sigma^{-2}_t (t) \\rangle^2 = 0$, then $(\\delta \\sigma_t)^2=0$. In\nthis case, the phase relation that minimizes $\\sigma^2_f$ is the phase\nrelation that minizes $\\delta \\sigma^2_t$ (making it\nzero indeed). However, when $\\sigma^2_f \\neq 0$, then the phase\nrelation that minimizes $\\sigma^2_f$ is {\\em not} necessarily the phase\nrelation that minimzes $\\delta \\sigma^2_t$. Secondly, the phase\nrelation that minimizes $(\\delta\n\\sigma_t)^2$, is not necessarily the phase relation that minimizes\n$\\sigma_t$, {\\em even when $(\\delta \\sigma_t)^2=0$}. We need to check\neither numerically or, if possible, by analytically minizing $\\langle\n\\sigma_t \\rangle$ whether this is true or not. The same holds for the\nmutual information: the phase relation that minimizes  $(\\delta\n\\sigma_t)^2$, is not necessarily the phase relation that maximizes the\nmutual information.\n\n\\subsection{The phase relation that minimizes $(\\delta \\sigma_t)^2$\n  when the relative noise strengths are the same}\n\nAs explained above, to obtain the optimal phase relation that makes\n$(\\delta \\sigma_t)^2=0$, we aim to find the phase distribution for which:\n\n", "index": 33, "text": "\\begin{equation}\n\\sigma^2_f = \\langle (\\sigma^{-2}_t (t)) ^2\\rangle - \\langle \\sigma^{-2}_t (t) \\rangle^2 = 0.\\label{eq:var_f}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\sigma^{2}_{f}=\\langle(\\sigma^{-2}_{t}(t))^{2}\\rangle-\\langle\\sigma^{-2}_{t}(t%&#10;)\\rangle^{2}=0.\" display=\"block\"><mrow><mrow><msubsup><mi>\u03c3</mi><mi>f</mi><mn>2</mn></msubsup><mo>=</mo><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03c3</mi><mi>t</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">\u27e9</mo></mrow><mo>-</mo><msup><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\u03c3</mi><mi>t</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nThe first term in Eq. \\ref{eq:var_f} can be obtained recursively, and\nis given by \n\\begin{eqnarray}\n&&\\langle (\\sigma^{-2}_t (t)) ^2\\rangle = \\nonumber\\\\ \n && K \\left[ \\frac{N(2N+1)}{8}+ \\frac{1}{4}\\sum_{i<j}^N \\cos(2(\\phi_i\n   -\\phi_j))\\right]\\label{eq:var_f_ft_const_noise}\n\\end{eqnarray}\nwhere $K$ is a constant, $K =\\left(\\frac{ 2\\pi A}{\\sigma_x T}\\right)^4$.\nAs expected this quantity depends on the phase relation.\n\nInstead of finding the phase relation that makes the difference\nbetween the two terms of\n$\\sigma^2_f$ in Eq. \\ref{eq:var_f} zero, we now want to find the\nrelation that makes the ratio of the two terms unity, which is\nequivalent, but mathematically more convenient. This yields \n\n", "itemtype": "equation", "pos": 58989, "prevtext": "\nWhen the cross correlations are zero, $\\sigma^{-2}_t (t)$ is given by\nEq. \\ref{eq:sigT}. The second term in the expression above, $\\langle\n\\sigma^{-2}_t (t) \\rangle^2$, is then, for the case that the noise and\nthe ampltidues are the same for all genes, given by\n\n", "index": 35, "text": "\\begin{equation}\n\\langle \\sigma^{-2}_t (t) \\rangle = \\left(\\frac{ 2\\pi A}{\\sigma_x T}\\right)^2 \\frac{N}{2}.\\label{eq:mean_sigmainvsq}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\langle\\sigma^{-2}_{t}(t)\\rangle=\\left(\\frac{2\\pi A}{\\sigma_{x}T}\\right)^{2}%&#10;\\frac{N}{2}.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mi>\u03c3</mi><mi>t</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><msup><mrow><mo>(</mo><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mi>A</mi></mrow><mrow><msub><mi>\u03c3</mi><mi>x</mi></msub><mo>\u2062</mo><mi>T</mi></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mfrac><mi>N</mi><mn>2</mn></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\n\nBy solving this as a function of $N$, we can recognize a pattern,\nwhich reveals that the optimal phase relation that minimizes $(\\delta\n\\sigma_t)^2$ is given by \n\n", "itemtype": "equation", "pos": 59838, "prevtext": "\nThe first term in Eq. \\ref{eq:var_f} can be obtained recursively, and\nis given by \n\\begin{eqnarray}\n&&\\langle (\\sigma^{-2}_t (t)) ^2\\rangle = \\nonumber\\\\ \n && K \\left[ \\frac{N(2N+1)}{8}+ \\frac{1}{4}\\sum_{i<j}^N \\cos(2(\\phi_i\n   -\\phi_j))\\right]\\label{eq:var_f_ft_const_noise}\n\\end{eqnarray}\nwhere $K$ is a constant, $K =\\left(\\frac{ 2\\pi A}{\\sigma_x T}\\right)^4$.\nAs expected this quantity depends on the phase relation.\n\nInstead of finding the phase relation that makes the difference\nbetween the two terms of\n$\\sigma^2_f$ in Eq. \\ref{eq:var_f} zero, we now want to find the\nrelation that makes the ratio of the two terms unity, which is\nequivalent, but mathematically more convenient. This yields \n\n", "index": 37, "text": "\\begin{equation}\\label{cond1}\n\\frac{2}{N} \\sum_{i \\le j}^N \\cos(2(\\phi_i-\\phi_j)) = -1.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\frac{2}{N}\\sum_{i\\leq j}^{N}\\cos(2(\\phi_{i}-\\phi_{j}))=-1.\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>2</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2264</mo><mi>j</mi></mrow><mi>N</mi></munderover><mrow><mi>cos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03d5</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03d5</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03559.tex", "nexttext": "\nThis means that the $i$-th signal has  a phase $\\Delta \\phi_i =\n(i-1) \\frac{\\pi}{N}$, as found for the phase relation that minimizes\n$\\langle \\sigma_t \\rangle$, given by Eq \\ref{optphase}. So in the case\nwhere the correlations are zero, the optimal\nphase shift minimizes both $\\langle \\sigma_t \\rangle$ and its\nvariance. Moreover, the mean error $\\langle \\sigma_t \\rangle$ can then\ndirectly be obtained from Eq. \\ref{eq:mean_sigmainvsq}.\n\n\\subsection{The phase relation that minimizes $(\\delta \\sigma_t)^2$\n  when the noise $\\sigma_x$ is not constant in time}\nWe now consider the case that $\\sigma_i \\simeq \\sqrt{\\bar{x}_i(t)}$,\nwhich means that $d\\sigma_i / dt \\neq 0$. In order to highlight the\nrole of the time-varying noise, we keep $A_i = A_j = \\dots = A$, $r_i\n= r_j = \\dots = r$. The variance of $\\sigma^{-2}(t)$ is given by:\n\\begin{eqnarray}\n\\sigma^2_f &=& \\langle (\\sigma^{-2}_t (t)) ^2\\rangle - \\langle\n\\sigma^{-2}_t (t) \\rangle^2 \\nonumber\\\\\n& = &\\left(\\frac{A^3 (2\\pi)^2}{16 T^2}\\right)^2\\{ N\n+2N(N+1)r^2 + \\nonumber\\\\\n& &\\sum_{i \\le j}^N  \\left[\\cos(\\phi_i - \\phi_j) +4r^2 \\cos[2(\\phi_i -\\phi_j)\\right] +\\nonumber \\\\\n& &\\left. 4 r^2 \\cos(\\phi_i -\\phi_j)\\right]\\} - \\left (\\frac{NA^3r (2\\pi)^2}{2T^2}\\right)^2\\label{eq:var_f_sigma_not_constant}\n\\end{eqnarray}\n\nWe note that this expression, in contrast to that for the case in which\n$\\sigma_i$ is constant in time, depends on the mean expression level\nof $x$, $r$. We find numerically that the phase relation that\nminimizes $\\sigma^2_f$ is the same as that for the scenario in which\n$\\sigma_i$ is constant in time, Eq. \\ref{phasedif}. However,\n$\\sigma^2_f$ and hence $(\\delta \\sigma_t)^2$ are only zero, when $r\n\\to \\infty$. We also find numerically that the phase relation that\nminimizes $\\sigma^2_f$ equals the phase relation that minimizes the mean\nerror $\\langle \\sigma_t \\rangle$ and maximizes the mutual information.\n\n\\subsection{The phase relation that minimizes $(\\delta \\sigma_t)^2$\n  when the relative noise strengths are {\\em not} the same}\nTo assess the importance of differences in the relative noise\nstrength, we will assume again that $\\sigma_i(t) = \\sigma_i$ is\nconstant in time. Defining the relative noise {\\em amplitude}\n$\\widetilde{A}_i \\equiv \\widetilde{\\sigma}_i^{-1} \\equiv A_i / \\sigma_i$,\nthe variance of $\\sigma^{-2}(t)$ is given by:\n\\begin{eqnarray}\n&&\\sigma^2_f = \\langle (\\sigma^{-2}_t (t)) ^2\\rangle - \\langle\n\\sigma^{-2}_t (t) \\rangle^2 \\nonumber\\\\\n&&= \\frac{1}{8}\\left(\\frac{2\\pi}{T}\\right)^4 \\left[\\sum_{i=1}^N 3\\widetilde{A}_i^2 +\\sum_{i \\le j}^N \\left(\\left[4+2\\cos[2(\\phi_i-\\phi_j)] \n\\widetilde{A}_i \\widetilde{A}_j\\right]\\right)\\right] \\nonumber\\\\\n && -\\left( \\frac{1}{2}\\left(\\frac{2\\pi}{T}\\right)^2 \\sum_{i=1}^N \\widetilde{A}_i\\right)^2\n\\label{eq:var_f_sigma_diff_noise}\n\\end{eqnarray}\nIt can\nbe verified that this reduces to Eq. \\ref{eq:var_f} when $\\sigma_i /\nA_i$ is the same for all genes. Following the\nlogic applied for that scenario, we find that the optimal phase\nrelation that makes $\\sigma^2_f=0$ is given by\n\n\\begin{eqnarray}\\label{ph_sigdif}\n\\sum_{i \\le j}^N \\cos\\left[ 2(\t\\phi_i -\\phi_j)  \\widetilde{A}_i \\widetilde{A}_j \\right] \\widetilde{A}_i^2 \\widetilde{A}_j = \\nonumber\\\\  \n\\sum_{i,j =1}^N  \\widetilde{A}_i \\widetilde{A}_j -\\frac{1}{2}\\sum_{i =1}^N 3 \\widetilde{A}_i^2  - 2\\sum_{i \\le j}^N \\widetilde{A}_i \\widetilde{A}_j \n\\end{eqnarray}\nThis expression reduces to Eq. \\ref{cond1} when $\\sigma_i /\nA_i$ is the same for all genes. It can be verified numerically that\nthe phase relation that makes $\\sigma^2_f$ and hence $(\\delta\n\\sigma_t)^2$ zero, is also the phase relation that minimizes the\nmean error $\\langle \\sigma_t\\rangle$ and maximizes the mutual information.\n\n\n\\begin{acknowledgments}\n  We thank Giulia Malaguti for a critical reading of the\n  manuscript. This work is part of the research programme of the\n  Foundation for Fundamental Research on Matter (FOM), which is part\n  of the Netherlands Organisation for Scientific Research (NWO).\n\\end{acknowledgments}\n\n\n \\bibliographystyle{unsrt}\n\n     \\bibliography{paper_info.bib}\n\n\n\n", "itemtype": "equation", "pos": 60104, "prevtext": "\n\nBy solving this as a function of $N$, we can recognize a pattern,\nwhich reveals that the optimal phase relation that minimizes $(\\delta\n\\sigma_t)^2$ is given by \n\n", "index": 39, "text": "\\begin{equation}\\label{phasedif}\n\\phi_i - \\phi_j = \\frac{\\pi}{N}(i-j). \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\phi_{i}-\\phi_{j}=\\frac{\\pi}{N}(i-j).\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03d5</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03d5</mi><mi>j</mi></msub></mrow><mo>=</mo><mrow><mfrac><mi>\u03c0</mi><mi>N</mi></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>-</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]