[{"file": "1601.07239.tex", "nexttext": "\nwhere $d_H$ represents the Hamming distance. The symbol $m$ and $m'$ denote \nbit sequences of length 2 stored in the flip-flops.\nThe message decoder tries to estimate a message sent from the variable node $v$\nfrom the read-out symbols from the flip-flops $y\\in\\{0,1\\}^2$.\nThe decoding function $\\psi: \\{0,1\\}^2 \\rightarrow \\{0,1,e\\}$  is given by \n\\begin{eqnarray}\\label{eqn:message_decoding_function}\n\\psi(y) {\\stackrel{\\triangle}{=}} \\left\\{\\begin{array}{ll}\n0, & y = 00, \\\\\n1, & y = 11, \\\\\ne, & y \\in \\{01,10 \\}. \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nFinally, the check node $c$ obtains the estimate of a message $\\hat x = \\psi(y)$.\nIn the following analysis, it is convenient to derive the conditional probability \nof $\\hat x$ given $x$, which is denoted by $Q(\\hat x| x)$.\nFrom the definitions of the message encoding and the probabilistic model \nfor the transient faults,  the conditional probability can be immediately derived as \n\\begin{eqnarray}\\nonumber\n\\left(\n\\small\n\\begin{array}{ccc}\nQ(0|0)&Q(1|0)&Q(e|0)\\\\\nQ(0|1)&Q(1|1)&Q(e|1)\\\\\nQ(0|e)&Q(1|e)&Q(e|e)\n\\end{array}\n\\right)\n=\\\\\\label{eqn:message_change_matrix}\n\\left(\n\\small\n\\begin{array}{ccc}\n(1-\\alpha)^2&\\alpha^2&2\\alpha(1-\\alpha)\\\\\n\\alpha^2&(1-\\alpha)^2&2\\alpha(1-\\alpha)\\\\\n\\alpha(1-\\alpha)&\\alpha(1-\\alpha)&\\alpha^2+(1-\\alpha)^2\n\\end{array}\n\\right).\n\\end{eqnarray}\n\nFigure \\ref{fig:faulty_node}(b) indicates a message flow in the reverse direction.\nIt includes a node $z$ representing a received symbol.  In this case, the same \nencoding function,  the decoding function, and the probabilistic fault model are assumed.\nThe dashed box in Fig. \\ref{fig:faulty_node} (a)(b) corresponding to \nthis conditional probability $Q(\\hat x| x)$ is also called an {\\em intermediate node} in a block diagram.\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[scale=0.30]{faulty_node.eps}\n  \\end{center}\n  \\caption\n{\nA message flow in erasure BP process: (a) variable to check message flow, (b) check to variable message flow.\n}\n\\label{fig:faulty_node}\n\\end{figure}\n\n\\subsection{Modification of variable node operation}\n\nIn a conventional erasure BP process, \nthere is no possibility for a variable node to receive \ncontradicting input messages from adjacent check nodes simultaneously.\nHowever, in a fault erasure BP process defined above, \na variable node may have messages containing both 0 and 1 simultaneously.\nWe thus need to modify the variable node process for accepting such contradicting messages.\nIn this paper, we adopt the following simple modification on the variable node process. \nIf a variable node receives \na set of contracting messages that include both 0 and 1,  then the variable node \nsends the erasure symbol to the neighboring check nodes.\nThe same rule is applied to the process for determination of the estimate symbol.\n\n\\section{Density evolution equations}\\label{sec:DE}\n\nThe density evolution (DE) is an important method to unveil the asymptotic behavior of a BP decoding algorithm.\nIn a DE process, we can track the time evolution of\nthe probability distribution of messages  (or the probability density function \nin a case where the messages are continuous).\nThe asymptotic probability distributions obtained by iterative computation\ntell us the asymptotic quantitative features of the decoding algorithm.\nIn this section, we will derive the DE equations for the fault erasure BP decoder.\n\n\\subsection{Derivation of DE equations}\n\nIn the following analysis, we will make several assumptions that have been commonly used \nin related works.\nThe channel is assumed to be a memoryless binary erasure channel (BEC) with \nthe erasure probability $\\epsilon (0 \\le \\epsilon \\le 1)$.\nIn this paper,  we consider a regular LDPC code ensemble with \nthe variable node degree $d_v$ and the check node degree $d_c$.\nThe transmitted word is assumed to be the zero codeword of infinite length.\n\nSuppose that $x$ represents an input to an intermediate node \n(corresponding to the conditional probability $Q(\\cdot|\\cdot)$)\nand that $\\hat x$ represents the corresponding output from the intermediate node.\nIf $x$ is distributed according to the probability distribution $t(\\cdot)$\nover the message alphabet $\\{0,1,e \\}$, then the probability distribution of the output $\\hat x$ \nobeys $t'(\\cdot)$ given by\n\n", "itemtype": "equation", "pos": 6988, "prevtext": "\n\\title{Performance Analysis based on Density Evolution \\\\ on Fault Erasure Belief Propagation Decoder} \n\\author{\n  \\IEEEauthorblockN{Hiroki Mori and Tadashi Wadayama} \\\\\n  \\IEEEauthorblockA{Nagoya Institute of Technology,   Japan\\\\\n      Email: mori@it.cs.nitech.ac.jp, wadayama@nitech.ac.jp} \n}\n\n\\maketitle\n\\begin{abstract}\nIn this paper, we will present an analysis on the fault erasure BP decoders\nbased on the density evolution.\nIn the fault BP decoder, messages exchanged in a  BP process \nare stochastically corrupted due to unreliable logic gates and flip-flops;\ni.e., we assume circuit components with transient faults.\nWe derived a set of the density evolution equations for the fault erasure BP processes.\nOur density evolution analysis reveals the asymptotic behaviors of the estimation error probability \nof the fault erasure BP decoders. In contrast to the fault free cases, it is observed that \nthe error probabilities of the fault erasure BP decoder converge to positive values, and \nthat there exists a discontinuity in an error curve corresponding to the fault BP threshold.\nIt is also shown that an message encoding technique provides \nhigher fault BP thresholds than those of the original decoders at the cost of increased circuit size.\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\n\nRecent advance of CMOS technology leads to denser VLSI implementation and \nthis trend is continuing \\cite{ITRS}\u00ef\u00bc\u008e\nIn near future, faulty behaviors of logic gates and flip-flops due to cosmic rays \nor thermal noises would become more problematic \\cite{fault1}\u00ef\u00bc\u008e\nWe should take care of fault tolerant VLSI design to attain \nhighly reliable circuits based on unreliable components \\cite{fault-tolerant1}\\cite{fault-tolerant2}\u00ef\u00bc\u008e\n\nIn this paper, we call a decoder for an error/erasure correcting code (ECC) composed by unreliable components \na {\\em fault decoder}.  Fault tolerance of the decoder is of critical importance \nbecause ECC is often exploited for ensuring high reliability of data memories in a circuit.\nTherefore, in a digital system based on unreliable components, \nECC behaves as a key component to compose reliable circuits.\nAnother reason for studies on fault decoders comes from the {\\em packet-based communication \nin a VLSI chip.}\nA new paradigm of data exchange in CPU,  {\\em Network on Chip} (NoC), \nis actively studied for replacing conventional on-chip buses for data/address \nexchange in a chip \\cite{network-on-chip}.\nAn NoC system is based on a packet-based network connecting many CPU cores and routers \nfor packet switching. If the network is congested, packet erasures due to collisions at a router \nmay occur and compensation for erased packets is needed\u00ef\u00bc\u008eErasure correction would be\na one of solutions for such packet erasures in a chip \\cite{LT}.\n\n\nSeveral works discussing fault decoders for {\\em Low-Density Parity-Check} (LDPC) codes have been published.\nIn 2011, Varshey presented an analysis for the fault Gallager-A decoder \\cite{fault-decoder1}\u00ef\u00bc\u008e\nHe assumed a probabilistic  model such that independent \ntransient faults may occur in a circuit of the Gallager-A decoder.\nA fault causes deterioration of the quality of the messages exchanged in a decoder and \nit results in degradation of the decoding performance.\nBased on these assumptions, analysis based on the density evolution  was presented in \\cite{fault-decoder1}.\nSadegh et. al showed a similar analysis on the fault Gallaber-B decoder \\cite{fault-decoder2}\u00ef\u00bc\u008e\nThey also derived the density evolution equations for the fault Gallaber-B decoder\nand calculated the thresholds for $q$-ary symmetric channel.\nOther related works on the fault decoders can be found in \n\\cite{fault-decoder3}\\cite{fault-decoder4}\\cite{fault-decoder5}\\cite{fault-decoder6}.\n\n\nA goal of this work is to analyze the asymptotic behavior of the {\\em fault erasure belief propagation (BP) decoder}\nbased on the density evolution. It is expected that the results obtained for fault erasure BP decoder \ngive us a useful insight for appropriate design of BP decoders made from unreliable components.\n\n\\section{Fault erasure BP decoder}\\label{sec:modeling}\nA fault erasure BP decoder is a BP decoder for memoryless erasure channels\nbased on unreliable components such as logic gates and flip-flops.\nIn this section, we are going to define a fault erasure BP decoder.\n\n\\subsection{Fault model for erasure BP decoder}\n\nIn this paper, we assume independent transient faults of logic gates and flip-flops and \ndo not assume occurrences of the permanent faults.\nThe occurrence of transient faults are modeled by a probabilistic model.\nNamely, transient faults are assumed to be independent events and \nthe probability of occurrences of the fault does not depend on the places. \nThis model is based on the Neumann model \\cite{Neumman-model} and \nit was used in the related literatures \\cite{fault-decoder1} \\cite{fault-decoder2}.\n\nIn order to clarify the definition of the fault model used in the paper,  \nwe focus on an erasure correction BP process. \nFigure \\ref{fig:faulty_node} presents \na message flow from a variable node $v$ to a check node $c$ in a Tanner graph.\nThree nodes, called {\\em message encoder}, {\\em fault node}, \nand {\\em message decoder}, are inserted in between the variable and check nodes.\nThe message encoder encodes a BP message in the message alphabet $\\{0,1, e\\}$ into \na binary (i.e., $\\{0,1\\}$) sequence \nthat are stored in flip-flops. The message decoder \nestimates a BP message in $\\{0,1, e\\}$ from a given binary sequence that is the read-out \nsymbols from the flip-flops.  The precise definition of \nthe pair of an encoder and a decoder will be given later. \nWe assume that a binary symbol stored in a flip-flop can be flipped with \nprobability $\\alpha (0 \\le \\alpha < 1)$ due to independent transient faults.\nThe fault node in  Fig. \\ref{fig:faulty_node} corresponds to \nthe memoryless binary symmetric channel with the bit-flip probability $\\alpha$. \n\nAccording to Fig.\\ref{fig:faulty_node} (a), we will explain the details of the message encoding and the probabilistic \nmodel for transient faults.\nThe message of a BP process  is expressed with the message alphabet $\\{0,1,e\\}$ where $e$ represents an erasure.\nThe variable node $v$ encodes a message  into a binary sequence of length 2 that \nis suitable for storing in a 2 flip-flops.\nThe {\\em message encoding function} $\\phi: \\{0,1,e\\} \\rightarrow \\{0,1\\}^2$ is defined by\n\\begin{eqnarray}\\label{eqn:message_coding}\n\\phi(x) {\\stackrel{\\triangle}{=}} \\left\\{\\begin{array}{ll}\n00, & x=0, \\\\\n11, & x=1, \\\\\n01, & x=e.\\\\\n\\end{array} \\right.\n\\end{eqnarray} \nThe output of the message encoder (two binary symbols) are stored in a pair of flip-flops.\n\nThe transient faults are modeled by probabilistic bit flips.\nA binary information in a flip-flop may alter its value with probability $\\alpha$ and \nthis bit flip events are independent.\nThus, the conditional probability $P(m' | m) (m' \\in \\{0,1\\}^2, m  \\in \\{0,1\\}^2)$ is given by\n", "index": 1, "text": "\n\\[\nP(m'|m) = (1 - \\alpha)^{2 - d_H(m',m)} \\alpha^{d_H(m',m)}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"P(m^{\\prime}|m)=(1-\\alpha)^{2-d_{H}(m^{\\prime},m)}\\alpha^{d_{H}(m^{\\prime},m)}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>m</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mn>2</mn><mo>-</mo><mrow><msub><mi>d</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>m</mi><mo>\u2032</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup><msup><mi>\u03b1</mi><mrow><msub><mi>d</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>m</mi><mo>\u2032</mo></msup><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.07239.tex", "nexttext": "\n\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[scale=0.3]{DE.eps}\n  \\end{center}\n  \\caption\n{\nRelation of message probability distributions\n}\n  \\label{fig:DE}\n\\end{figure}\n\nIn the following, the details of the probability distributions are introduced according to Fig.\\ref{fig:DE}.\nThe probability distribution corresponding to the message \nemitted from a check node $c$ is denote by $q_i$. \nThe index $i$ represents the discrete time index in an iterative process.\nA message from a check node enters an intermediate node, which represents \nthe effect of the probabilistic faults.\nThe distribution corresponds to the output of the intermediate node \nis represented by $q'_i$ that is given by\n\\begin{eqnarray}\\label{eqn:q'}\nq'_i (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) q_i(x).\n\\end{eqnarray}\nIn the following, we will use a convention such that the symbol for the output distribution of \nthe intermediate node is expressed with the symbol of the input distribution with the prime symbol\nsuch as $t$ and $t'$.\n\nA variable node $v$ computes a message from the set of messages it receives.\nThe message distribution corresponding to the message from a variable node to \nan intermediate node follows the distribution $p_{i+1}$.\nThe corresponding output distribution from the intermediate node is given by\n\\begin{eqnarray}\\label{eqn:p'}\np'_{i+1} (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) p_{i+1}(x).\n\\end{eqnarray}\n\nWe also assume that probabilistic faults may occur \nin a message flow from a received symbol node to a variable node.\nA received symbol node send a message in $\\{0,1,e\\}$ to an intermediate node\naccording to its received value.\nThe probability distribution $r(\\cdot)$ of the message is given by\n\\begin{eqnarray}\\label{eqn:r}\nr(x) {\\stackrel{\\triangle}{=}} \\left\\{\\begin{array}{ll}\n1-\\epsilon, & x = 0, \\\\\n0, & x =1, \\\\\n\\epsilon, & x = e. \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nThe message distribution of the corresponding message from the intermediate node follows\nthe distribution \n\\begin{eqnarray}\\label{eqn:r'}\nr' (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) r(x).\n\\end{eqnarray}\n\nWe first derive the DE equations on the check node output.\nThere are two cases depending on the output of the check node.\nFirstly, consider the case where the output of the check node is 0.\nThe check node $c$ calculates a message to an adjacent variable node $v$.\nIf and only if the set of $d_c-1$ incoming messages received by $c$ except for the one from $v$ \ncontain even number of 1's and contains no erasure symbols, \nthen the message from $c$ becomes 0.\nTherefore, the distribution of the check node output $q_i(0)$ is given by\n\\begin{eqnarray}\\nonumber\nq_i(0)&\\!=\\!&\\sum_{j: even }^{d_c-1}\\binom{d_c-1}{j}p'_i(1)^{j}p'_i(0)^{d_c-1-j}\\\\\n&\\!=\\!&\\frac{(p'_i(0)\\!+\\!p'_i(1))^{d_c-1}}{2}\\!+\\!\\frac{{(p'_i(0)\\!-\\!p'_i(1))}^{d_c-1}}{2}. \\label{eqn:q0}\n\\end{eqnarray}\nThe binomial theorem is used in the derivation above.\nIn a similar manner,  we can derive $q_i(1)$.\nNote that, in this case, the set of the $d_c-1$ messages consisting of odd number of 1's and no erasure symbols\nleads to the output message 1 from the check node. We thus have \n\\begin{eqnarray}\\nonumber\nq_i(1)&\\!=\\!&\\sum_{j: odd}^{d_c-1}\\binom{d_c-1}{j}p'_i(1)^{j}p'_i(0)^{d_c-1-j}\\\\\n&\\!=\\!&\\frac{(p'_i(0)\\!+\\!p'_i(1))^{d_c-1}}{2}\\!-\\!\\frac{{(p'_i(0)\\!-\\!p'_i(1))}^{d_c-1}}{2}. \\label{eqn:q1}\n\\end{eqnarray}\n\nWe will then consider the DE equations on the variable node output.\nLet us assume that an output of a variable node is 0, and\nthat the variable node $v$ calculates a message to an adjacent check node $c$.\nLet $M$ be  the set of the $d_v-1$ incoming messages to $v$ from adjacent check nodes \nexcept for the one from $c$.\nThe variable node message becomes 0 if and only if \nthe event (A) $y=0$ and $1 \\notin M$ holds,  or  the event (B) $y=e$, $1 \\notin M$, and $0 \\in M$ \nholds where $y$ received symbols corresponding to the variable node $v$.\n\n\nThe probability corresponding to the event (A) becomes\n$\nProb[A] = r'(0) (1 - q_i'(1))^{d_v-1}\n$\nbecause all the incoming messages are independent.\nThe probability of the event (B) is given by \n$\nProb[B] = r'(e) \\left(  (1-q'_i(1))^{d_v-1}-q'_i(e)^{d_v-1} \\right).\n$\nSince these two events are independent, the probability $p_{i+1}(0)$\nis the sum of these two probabilities:\n\\begin{eqnarray} \\nonumber\np_{i+1}(0)&=& Prob[A] + Prob[B] \\\\ \\nonumber\n&=& r'(0)(1-q'_i(1))^{d_v-1} \\\\\n&+& r'(e) \\left(  (1-q'_i(1))^{d_v-1}-q'_i(e)^{d_v-1} \\right). \\label{eqn:p0}\n\\end{eqnarray}\nIn a similar manner, we can derive the probability corresponding to the variable node message to be $1$:\n\\begin{eqnarray} \\nonumber\np_{i+1}(1)\n&=&r'(1)(1-q'_i(0))^{d_v-1} \\\\\n&+&r'(e) \\left( (1-q'_i(0))^{d_v-1}-q'_i(e)^{d_v-1} \\right). \\label{eqn:p1}\n\\end{eqnarray}\n\nIt should be remarked that, for any discrete time index $i$, \nthe equalities\n$\np_i(0) + p_i(1) + p_i(e) = 1\n$\nand \n$\nq_i(0) + q_i(1) + q_i(e) = 1\n$\nhold.\n\nFrom the arguments above, we have all the DE equations required for the DE analysis of\nthe fault erasure BP decoding. Namely, Based on \nEqs. (\\ref{inter})(\\ref{eqn:r})(\\ref{eqn:q0})(\\ref{eqn:q1})(\\ref{eqn:p0})(\\ref{eqn:p1})\nwith the initial condition $q_0(0)=0, q_0(1)=0$,\nan iterative calculation on the message probability functions leads\nto the asymptotic message distributions.\n\n\\subsection{Asymptotic error probability}\n\nAccording to the conventional erasure BP rule, \nif incoming messages to a variable node contain no 1's and contain a 0,\nthen the tentative estimate of the variable node becomes 0.\nLet us denote the probability for such an event by $s_{i}(0)$.\nThe probability $s_{i}(0)$ is given by\n\n", "itemtype": "equation", "pos": 11331, "prevtext": "\nwhere $d_H$ represents the Hamming distance. The symbol $m$ and $m'$ denote \nbit sequences of length 2 stored in the flip-flops.\nThe message decoder tries to estimate a message sent from the variable node $v$\nfrom the read-out symbols from the flip-flops $y\\in\\{0,1\\}^2$.\nThe decoding function $\\psi: \\{0,1\\}^2 \\rightarrow \\{0,1,e\\}$  is given by \n\\begin{eqnarray}\\label{eqn:message_decoding_function}\n\\psi(y) {\\stackrel{\\triangle}{=}} \\left\\{\\begin{array}{ll}\n0, & y = 00, \\\\\n1, & y = 11, \\\\\ne, & y \\in \\{01,10 \\}. \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nFinally, the check node $c$ obtains the estimate of a message $\\hat x = \\psi(y)$.\nIn the following analysis, it is convenient to derive the conditional probability \nof $\\hat x$ given $x$, which is denoted by $Q(\\hat x| x)$.\nFrom the definitions of the message encoding and the probabilistic model \nfor the transient faults,  the conditional probability can be immediately derived as \n\\begin{eqnarray}\\nonumber\n\\left(\n\\small\n\\begin{array}{ccc}\nQ(0|0)&Q(1|0)&Q(e|0)\\\\\nQ(0|1)&Q(1|1)&Q(e|1)\\\\\nQ(0|e)&Q(1|e)&Q(e|e)\n\\end{array}\n\\right)\n=\\\\\\label{eqn:message_change_matrix}\n\\left(\n\\small\n\\begin{array}{ccc}\n(1-\\alpha)^2&\\alpha^2&2\\alpha(1-\\alpha)\\\\\n\\alpha^2&(1-\\alpha)^2&2\\alpha(1-\\alpha)\\\\\n\\alpha(1-\\alpha)&\\alpha(1-\\alpha)&\\alpha^2+(1-\\alpha)^2\n\\end{array}\n\\right).\n\\end{eqnarray}\n\nFigure \\ref{fig:faulty_node}(b) indicates a message flow in the reverse direction.\nIt includes a node $z$ representing a received symbol.  In this case, the same \nencoding function,  the decoding function, and the probabilistic fault model are assumed.\nThe dashed box in Fig. \\ref{fig:faulty_node} (a)(b) corresponding to \nthis conditional probability $Q(\\hat x| x)$ is also called an {\\em intermediate node} in a block diagram.\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[scale=0.30]{faulty_node.eps}\n  \\end{center}\n  \\caption\n{\nA message flow in erasure BP process: (a) variable to check message flow, (b) check to variable message flow.\n}\n\\label{fig:faulty_node}\n\\end{figure}\n\n\\subsection{Modification of variable node operation}\n\nIn a conventional erasure BP process, \nthere is no possibility for a variable node to receive \ncontradicting input messages from adjacent check nodes simultaneously.\nHowever, in a fault erasure BP process defined above, \na variable node may have messages containing both 0 and 1 simultaneously.\nWe thus need to modify the variable node process for accepting such contradicting messages.\nIn this paper, we adopt the following simple modification on the variable node process. \nIf a variable node receives \na set of contracting messages that include both 0 and 1,  then the variable node \nsends the erasure symbol to the neighboring check nodes.\nThe same rule is applied to the process for determination of the estimate symbol.\n\n\\section{Density evolution equations}\\label{sec:DE}\n\nThe density evolution (DE) is an important method to unveil the asymptotic behavior of a BP decoding algorithm.\nIn a DE process, we can track the time evolution of\nthe probability distribution of messages  (or the probability density function \nin a case where the messages are continuous).\nThe asymptotic probability distributions obtained by iterative computation\ntell us the asymptotic quantitative features of the decoding algorithm.\nIn this section, we will derive the DE equations for the fault erasure BP decoder.\n\n\\subsection{Derivation of DE equations}\n\nIn the following analysis, we will make several assumptions that have been commonly used \nin related works.\nThe channel is assumed to be a memoryless binary erasure channel (BEC) with \nthe erasure probability $\\epsilon (0 \\le \\epsilon \\le 1)$.\nIn this paper,  we consider a regular LDPC code ensemble with \nthe variable node degree $d_v$ and the check node degree $d_c$.\nThe transmitted word is assumed to be the zero codeword of infinite length.\n\nSuppose that $x$ represents an input to an intermediate node \n(corresponding to the conditional probability $Q(\\cdot|\\cdot)$)\nand that $\\hat x$ represents the corresponding output from the intermediate node.\nIf $x$ is distributed according to the probability distribution $t(\\cdot)$\nover the message alphabet $\\{0,1,e \\}$, then the probability distribution of the output $\\hat x$ \nobeys $t'(\\cdot)$ given by\n\n", "index": 3, "text": "\\begin{equation}\\label{inter}\nt'(\\hat x) =  \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) t(x).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"t^{\\prime}(\\hat{x})=\\sum_{x\\in\\{0,1,e\\}}Q(\\hat{x}|x)t(x).\" display=\"block\"><mrow><msup><mi>t</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi>e</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munder><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07239.tex", "nexttext": " \nA DE process can evaluate \nthe asymptotic error probability \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[scale=0.3]{DE.eps}\n  \\end{center}\n  \\caption\n{\nRelation of message probability distributions\n}\n  \\label{fig:DE}\n\\end{figure}\n\nIn the following, the details of the probability distributions are introduced according to Fig.\\ref{fig:DE}.\nThe probability distribution corresponding to the message \nemitted from a check node $c$ is denote by $q_i$. \nThe index $i$ represents the discrete time index in an iterative process.\nA message from a check node enters an intermediate node, which represents \nthe effect of the probabilistic faults.\nThe distribution corresponds to the output of the intermediate node \nis represented by $q'_i$ that is given by\n\\begin{eqnarray}\\label{eqn:q'}\nq'_i (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) q_i(x).\n\\end{eqnarray}\nIn the following, we will use a convention such that the symbol for the output distribution of \nthe intermediate node is expressed with the symbol of the input distribution with the prime symbol\nsuch as $t$ and $t'$.\n\nA variable node $v$ computes a message from the set of messages it receives.\nThe message distribution corresponding to the message from a variable node to \nan intermediate node follows the distribution $p_{i+1}$.\nThe corresponding output distribution from the intermediate node is given by\n\\begin{eqnarray}\\label{eqn:p'}\np'_{i+1} (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) p_{i+1}(x).\n\\end{eqnarray}\n\nWe also assume that probabilistic faults may occur \nin a message flow from a received symbol node to a variable node.\nA received symbol node send a message in $\\{0,1,e\\}$ to an intermediate node\naccording to its received value.\nThe probability distribution $r(\\cdot)$ of the message is given by\n\\begin{eqnarray}\\label{eqn:r}\nr(x) {\\stackrel{\\triangle}{=}} \\left\\{\\begin{array}{ll}\n1-\\epsilon, & x = 0, \\\\\n0, & x =1, \\\\\n\\epsilon, & x = e. \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nThe message distribution of the corresponding message from the intermediate node follows\nthe distribution \n\\begin{eqnarray}\\label{eqn:r'}\nr' (\\hat x) = \\sum_{x \\in \\{0,1,e\\}} Q(\\hat x| x) r(x).\n\\end{eqnarray}\n\nWe first derive the DE equations on the check node output.\nThere are two cases depending on the output of the check node.\nFirstly, consider the case where the output of the check node is 0.\nThe check node $c$ calculates a message to an adjacent variable node $v$.\nIf and only if the set of $d_c-1$ incoming messages received by $c$ except for the one from $v$ \ncontain even number of 1's and contains no erasure symbols, \nthen the message from $c$ becomes 0.\nTherefore, the distribution of the check node output $q_i(0)$ is given by\n\\begin{eqnarray}\\nonumber\nq_i(0)&\\!=\\!&\\sum_{j: even }^{d_c-1}\\binom{d_c-1}{j}p'_i(1)^{j}p'_i(0)^{d_c-1-j}\\\\\n&\\!=\\!&\\frac{(p'_i(0)\\!+\\!p'_i(1))^{d_c-1}}{2}\\!+\\!\\frac{{(p'_i(0)\\!-\\!p'_i(1))}^{d_c-1}}{2}. \\label{eqn:q0}\n\\end{eqnarray}\nThe binomial theorem is used in the derivation above.\nIn a similar manner,  we can derive $q_i(1)$.\nNote that, in this case, the set of the $d_c-1$ messages consisting of odd number of 1's and no erasure symbols\nleads to the output message 1 from the check node. We thus have \n\\begin{eqnarray}\\nonumber\nq_i(1)&\\!=\\!&\\sum_{j: odd}^{d_c-1}\\binom{d_c-1}{j}p'_i(1)^{j}p'_i(0)^{d_c-1-j}\\\\\n&\\!=\\!&\\frac{(p'_i(0)\\!+\\!p'_i(1))^{d_c-1}}{2}\\!-\\!\\frac{{(p'_i(0)\\!-\\!p'_i(1))}^{d_c-1}}{2}. \\label{eqn:q1}\n\\end{eqnarray}\n\nWe will then consider the DE equations on the variable node output.\nLet us assume that an output of a variable node is 0, and\nthat the variable node $v$ calculates a message to an adjacent check node $c$.\nLet $M$ be  the set of the $d_v-1$ incoming messages to $v$ from adjacent check nodes \nexcept for the one from $c$.\nThe variable node message becomes 0 if and only if \nthe event (A) $y=0$ and $1 \\notin M$ holds,  or  the event (B) $y=e$, $1 \\notin M$, and $0 \\in M$ \nholds where $y$ received symbols corresponding to the variable node $v$.\n\n\nThe probability corresponding to the event (A) becomes\n$\nProb[A] = r'(0) (1 - q_i'(1))^{d_v-1}\n$\nbecause all the incoming messages are independent.\nThe probability of the event (B) is given by \n$\nProb[B] = r'(e) \\left(  (1-q'_i(1))^{d_v-1}-q'_i(e)^{d_v-1} \\right).\n$\nSince these two events are independent, the probability $p_{i+1}(0)$\nis the sum of these two probabilities:\n\\begin{eqnarray} \\nonumber\np_{i+1}(0)&=& Prob[A] + Prob[B] \\\\ \\nonumber\n&=& r'(0)(1-q'_i(1))^{d_v-1} \\\\\n&+& r'(e) \\left(  (1-q'_i(1))^{d_v-1}-q'_i(e)^{d_v-1} \\right). \\label{eqn:p0}\n\\end{eqnarray}\nIn a similar manner, we can derive the probability corresponding to the variable node message to be $1$:\n\\begin{eqnarray} \\nonumber\np_{i+1}(1)\n&=&r'(1)(1-q'_i(0))^{d_v-1} \\\\\n&+&r'(e) \\left( (1-q'_i(0))^{d_v-1}-q'_i(e)^{d_v-1} \\right). \\label{eqn:p1}\n\\end{eqnarray}\n\nIt should be remarked that, for any discrete time index $i$, \nthe equalities\n$\np_i(0) + p_i(1) + p_i(e) = 1\n$\nand \n$\nq_i(0) + q_i(1) + q_i(e) = 1\n$\nhold.\n\nFrom the arguments above, we have all the DE equations required for the DE analysis of\nthe fault erasure BP decoding. Namely, Based on \nEqs. (\\ref{inter})(\\ref{eqn:r})(\\ref{eqn:q0})(\\ref{eqn:q1})(\\ref{eqn:p0})(\\ref{eqn:p1})\nwith the initial condition $q_0(0)=0, q_0(1)=0$,\nan iterative calculation on the message probability functions leads\nto the asymptotic message distributions.\n\n\\subsection{Asymptotic error probability}\n\nAccording to the conventional erasure BP rule, \nif incoming messages to a variable node contain no 1's and contain a 0,\nthen the tentative estimate of the variable node becomes 0.\nLet us denote the probability for such an event by $s_{i}(0)$.\nThe probability $s_{i}(0)$ is given by\n\n", "index": 5, "text": "\\begin{equation}\ns_{i}(0)=r'(0)(1-q'_i(1))^{d_v}+r'(e) \\left((1-q'_i(1))^{d_v}-q'_i(e)^{d_v}  \\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"s_{i}(0)=r^{\\prime}(0)(1-q^{\\prime}_{i}(1))^{d_{v}}+r^{\\prime}(e)\\left((1-q^{%&#10;\\prime}_{i}(1))^{d_{v}}-q^{\\prime}_{i}(e)^{d_{v}}\\right).\" display=\"block\"><mrow><mrow><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>r</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msubsup><mi>q</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mi>d</mi><mi>v</mi></msub></msup></mrow><mo>+</mo><mrow><msup><mi>r</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msubsup><mi>q</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mi>d</mi><mi>v</mi></msub></msup><mo>-</mo><mrow><msubsup><mi>q</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>d</mi><mi>v</mi></msub></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07239.tex", "nexttext": "\nIn the following parts of this paper, we will focus on the behavior \nof the asymptotic error probability $\\gamma(\\epsilon, \\alpha)$.\n\n\n\\section{Numerical results}\\label{sec:result1}\n\nIn the previous section, we derived the DE equations for the fault erasure BP decoder.\nIn this section, numerical results indicating the asymptotic behavior of the decoder \nwill presented. \n\n\\subsection{Effect of transient faults}\nFigure \\ref{fig:result1} presents the asymptotic error probabilities $\\gamma(\\epsilon,\\alpha)$\nfor $(d_v,d_c)=(3,6)$-regular LDPC code ensemble.\nThe four curves depicted in Fig.\\ref{fig:result1} correspond  to \nthe fault probabilities $\\alpha=10^{-2},10^{-3},10^{-4},10^{-5}$  from left to right.\nWhen the fault probability $\\alpha$ is equal to 0, the system model exactly coincides with the \ncommon erasure BP decoder model without transient faults. In such a case, the asymptotic error probability \nconverges to 0 if $\\epsilon$ is greater than the BP threshold $\\epsilon_{BP}=0.42944$ \n(this value is also included in Fig. \\ref{fig:result1}).\nIn the case of positive $\\alpha$, the situations are totally different.\nWhen $\\alpha>0$, we can observe that\n$\\gamma(\\epsilon,\\alpha)$ converges to positive values due to the faults occurred in the BP decoder.\nFrom this figure, it is also seen that smaller $\\alpha$ gives smaller $\\gamma(\\epsilon,\\alpha)$.\nFurthermore, each curve has a sudden (vertical) jump at a certain erasure probability.\nFor example, the curve of $\\alpha=10^{-3}$ shows $\\gamma(\\epsilon,10^{-3}) > 10^{-1}$ \nin the regime $\\epsilon > 0.36207$.  On the other hand, in the regime $\\epsilon < 0.36207$,\n$\\gamma(\\epsilon,10^{-3})$ takes the values smaller than $10^{-3}$. The behaviors of the decoder \nare sharply separated at the erasure probability $\\epsilon = 0.36207$ that is considered to be \na threshold value for the fault erasure BP decoder.\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[width=8cm,height=6cm]{l3_r6_2bit_de_graph.eps}\n  \\end{center}\n  \\caption\n{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$ \n((3,6)-regular LDPC code ensemble)\n}\n  \\label{fig:result1}\n\\end{figure}\n\n\\subsection{BP dynamics of fault erasure BP decoder}\nFigure \\ref{fig:result2} indicates the dynamics of the BP processes via\nthe evolutions of the pair of the message probabilities  $p_i(0)$ and $p_i(1)$.\nThe ensemble is $(3,6)$-regular LDPC code ensemble and the fault probability is assumed \nto be $\\alpha=10^{-3}$. Each arrow in the figure shows a change of the message probabilities from \n$(p_i(0),p_i(1))$ to  $(p_{i+1}(0),p_{i+1}(1))$ and \neach trajectory corresponds to an erasure probability in the range $\\epsilon = 0.01 j (10 \\le j \\le 50)$.\nSince the zero codeword  is assumed to be transmitted,  the probability $p_i(0)$ \nrepresents the probability for the correct decoding.  \nIt is immediately recognized that there are two groups of the trajectories: one group \ncorresponds to the range $0.37 \\le \\epsilon \\le 0.5$ and the other group corresponds to the range\n$0.1 \\le \\epsilon \\le 0.36$. The trajectories in the first group show the upward movements. \nThis means that the error probability tends to converge to a higher value.\nOn the other hand, the trajectories in the second group indicate that $p_i(0)$ approaches \nto 1 as the number of iterations increases. This numerical results strongly suggest \nthe existence of  a {\\em bifurcation} of this DE evolution processes \nthat can be considered as a non-linear dynamical system. \nAt the erasure probability that corresponds to this bifurcation, we  can observe sudden drop\nof the asymptotic error probability in Fig.\\ref{fig:result1}.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=8cm,height=6cm]{alpha0.001_p0p1.eps}\n  \\end{center}\n  \\caption\n{\nDynamics of BP process: Evolution of message probabilities for variable nodes $p_i(0)$ and $p_i(1)$\n($(3,6)$-regular LDPC code ensemble\u00ef\u00bc\u008cfault probability $\\alpha=10^{-3}$)\n}\n  \\label{fig:result2}\n\\end{figure}\n\n\n\\subsection{Degree and asymptotic error probability}\n\nFigure \\ref{fig:change_degree} presents the asymptotic error probabilities $\\gamma(\\epsilon,\\alpha)$\nfor regular-LDPC code ensembles with degrees $(d_v,d_c)=(2,4),(3,6),(4,8)$.\nAll the ensembles correspond to the design code rate $1/2$. The fault probability is set to $\\alpha=10^{-4}$.\n \\begin{figure}[t]\n  \\centering\n  \\includegraphics[scale=1.1]{reg_rate0.5_alpha0.000100_graph.eps}\n  \\caption{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$\n($(d_v,d_c)$-regular LDPC code ensemble\u00ef\u00bc\u008cdesign code rate 0.5\u00ef\u00bc\u008cfault probability $\\alpha=10^{-4}$)\n}\n\\label{fig:change_degree}\n \\end{figure}\nFrom Fig.\\ref{fig:change_degree}, we can observe that  the $(d_v,d_c)=(3,6)$ ensemble \nprovides the highest fault BP threshold.\nIt is well known that $(3,6)$ ensemble gives the highest threshold in the fault free cases.\nSimilar tendency can be seen in the cases where transient faults exist.\n\n\\section{Message encoding}\\label{sec:message_encoding}\n\nIn the previous section, we observed numerical results of the DE analysis on the \nfault erasure BP decoder. According to the numerical results, it was shown that\nwe must admit non-zero error probability even in the asymptotic regime if the fault probability is positive.\nIn this section, we discuss and compare two methods for improving the decoding performance \nof the fault erasure BP decoding at the cost of increased hardware complexity.\n\nThe simplest way to improve the decoding performance is to exploit several \nidentical erasure BP decoders in parallel. \nBy using the majority votes from the outputs obtained from these component decoders,\nwe can obtain more reliable estimates of transmitted symbols.\nIn this paper, the scheme is called {\\em Majority voting scheme}.\nAnother way to enhance the reliability is to use a longer code to protect BP messages.\nIn Section 2, we introduced the message function $\\phi$ that encodes a BP message \ninto 2-binary symbols.  By replacing the encoding function to an encoding function for a longer code,\nwe can expect that the immunity against possible faults becomes stronger.\nWe call this scheme {\\em message encoding}.\nOf course, both schemes (i.e., majority voting and message encoding) require\nincrease of the circuit size  that can be considered as the cost should be paid for the improvement of the immunity.\n\n\\subsection{Majority voting scheme}\n\nIn this subsection, we introduce a simple majority voting scheme that \nemploies $N$-fault erasure BP decoders for  improving the fault immunity.\nThe majority voting scheme determines its output by majority voting \nbased on $N$-outputs from the component BP decoders.\nAlthough this scheme requires $N$-fold circuit size compared with\nthe single fault erasure BP decoder, it is expected that the majority voting process \nimproves the asymptotic error probability.\n\nIn the following, we will discuss the case where $N=2$. \nThe argument below can be easily extended to general cases where $N>2$.\nLet $\\hat{x}^{(1)},\\hat{x}^{(2)}\\in\\{0,1,e\\}$ be the decoder outputs from \nthe two component decoders $D^{(1)}$ and $D^{(2)}$. \nThe majority voting process is defined the function\n", "itemtype": "equation", "pos": 17255, "prevtext": " \nA DE process can evaluate \nthe asymptotic error probability \n\n", "index": 7, "text": "\\begin{equation}\n\\gamma(\\epsilon, \\alpha) {\\stackrel{\\triangle}{=}} \\lim_{i \\to \\infty}(1-s_i(0)).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\gamma(\\epsilon,\\alpha){\\stackrel{\\triangle}{=}}\\lim_{i\\to\\infty}(1-s_{i}(0)).\" display=\"block\"><mrow><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo>,</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mover><mo movablelimits=\"false\">=</mo><mi mathsize=\"142%\" mathvariant=\"normal\">\u25b3</mi></mover><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>i</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07239.tex", "nexttext": "\nwhere the function $\\tau$ represents the output from the majority voting decoder.\nWe denote the asymptotic error probability for the majority voting scheme \nby  $\\gamma_{maj}(\\epsilon,\\alpha)$.\n\nIn the following, a lower bound on $\\gamma_{maj}(\\epsilon,\\alpha)$ will be discussed.\nThroughout the following argument, we assume that \n\\begin{eqnarray}\\label{eqn:maj_theorem_assumption}\ns(0,0)&\\geq& (s(0))^2 \n\\end{eqnarray}\nholds where the quantity $s(0)$  in the righthand side  is the asymptotic value of $s_0(x)$; i.e., \n$\ns(0) = \\lim_{i \\rightarrow \\infty} s_i(0),\n$\nwhich can be evaluated by the density evolution. The quantity $s(0,0)$ is the asymptotic joint probability \ncorresponds to the event that two decoder outputs take the value $(0,0)$.\nThis is a natural assumption because the outputs from the the component decoders $D^{(1)},D^{(2)}$ \nare expected to be highly correlated. \nUnder the assumption of (\\ref{eqn:maj_theorem_assumption}),\nwe can easily derive a lower bound  of $\\gamma_{maj}(\\epsilon,\\alpha)$\n\\begin{eqnarray}\\label{eqn:parallel_LB}\n    \\gamma_{maj}(\\epsilon,\\alpha)\\geq(1-s(0))^2.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Details of message encoding}\n\nIn this subsection, we will introduce a simple message encoding scheme \nbased on a binary code of length $n$. The parameter $n$ is referred to as\n{\\em message code length}.\nIn the following, we redefine the encoding and decoding functions.\nThe encoding function $\\phi: \\{0,1,e\\} \\rightarrow \\{0,1\\}^n$ is an encoding function \nnow defined by\n\\begin{eqnarray}\n\\phi(x)=\\left\\{\\begin{array}{ll}\n\\overbrace{00\\cdots0}^{n}, & x=0, \\\\\n\\overbrace{11\\cdots1}^{n}, & x=1, \\\\\n\\overbrace{00\\cdots0}^{n/2}\\overbrace{11\\cdots1}^{n/2}, & x=e.\\\\\n\\end{array} \\right.\n\\end{eqnarray} \nThere are several possibilities for choosing decoding functions corresponding to \nthe encoding function defined above.\nOne simple choice is to define a decoding function $\\phi$ as\n\\begin{eqnarray}\n\\psi(y)=\\left\\{\\begin{array}{ll}\n0, & w_H(y) = 0 \\\\\n1, & w_H(y) = n\\\\\ne, & \\mbox{otherwise}, \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nwhere $w_H$ represents the Hamming weight function.\nIn this case, the conditional probability $Q(\\hat{x}|x)$ corresponding to \nthe intermediate node is given by \n", "itemtype": "equation", "pos": 24631, "prevtext": "\nIn the following parts of this paper, we will focus on the behavior \nof the asymptotic error probability $\\gamma(\\epsilon, \\alpha)$.\n\n\n\\section{Numerical results}\\label{sec:result1}\n\nIn the previous section, we derived the DE equations for the fault erasure BP decoder.\nIn this section, numerical results indicating the asymptotic behavior of the decoder \nwill presented. \n\n\\subsection{Effect of transient faults}\nFigure \\ref{fig:result1} presents the asymptotic error probabilities $\\gamma(\\epsilon,\\alpha)$\nfor $(d_v,d_c)=(3,6)$-regular LDPC code ensemble.\nThe four curves depicted in Fig.\\ref{fig:result1} correspond  to \nthe fault probabilities $\\alpha=10^{-2},10^{-3},10^{-4},10^{-5}$  from left to right.\nWhen the fault probability $\\alpha$ is equal to 0, the system model exactly coincides with the \ncommon erasure BP decoder model without transient faults. In such a case, the asymptotic error probability \nconverges to 0 if $\\epsilon$ is greater than the BP threshold $\\epsilon_{BP}=0.42944$ \n(this value is also included in Fig. \\ref{fig:result1}).\nIn the case of positive $\\alpha$, the situations are totally different.\nWhen $\\alpha>0$, we can observe that\n$\\gamma(\\epsilon,\\alpha)$ converges to positive values due to the faults occurred in the BP decoder.\nFrom this figure, it is also seen that smaller $\\alpha$ gives smaller $\\gamma(\\epsilon,\\alpha)$.\nFurthermore, each curve has a sudden (vertical) jump at a certain erasure probability.\nFor example, the curve of $\\alpha=10^{-3}$ shows $\\gamma(\\epsilon,10^{-3}) > 10^{-1}$ \nin the regime $\\epsilon > 0.36207$.  On the other hand, in the regime $\\epsilon < 0.36207$,\n$\\gamma(\\epsilon,10^{-3})$ takes the values smaller than $10^{-3}$. The behaviors of the decoder \nare sharply separated at the erasure probability $\\epsilon = 0.36207$ that is considered to be \na threshold value for the fault erasure BP decoder.\n\n\\begin{figure}[tbp]\n  \\begin{center}\n    \\includegraphics[width=8cm,height=6cm]{l3_r6_2bit_de_graph.eps}\n  \\end{center}\n  \\caption\n{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$ \n((3,6)-regular LDPC code ensemble)\n}\n  \\label{fig:result1}\n\\end{figure}\n\n\\subsection{BP dynamics of fault erasure BP decoder}\nFigure \\ref{fig:result2} indicates the dynamics of the BP processes via\nthe evolutions of the pair of the message probabilities  $p_i(0)$ and $p_i(1)$.\nThe ensemble is $(3,6)$-regular LDPC code ensemble and the fault probability is assumed \nto be $\\alpha=10^{-3}$. Each arrow in the figure shows a change of the message probabilities from \n$(p_i(0),p_i(1))$ to  $(p_{i+1}(0),p_{i+1}(1))$ and \neach trajectory corresponds to an erasure probability in the range $\\epsilon = 0.01 j (10 \\le j \\le 50)$.\nSince the zero codeword  is assumed to be transmitted,  the probability $p_i(0)$ \nrepresents the probability for the correct decoding.  \nIt is immediately recognized that there are two groups of the trajectories: one group \ncorresponds to the range $0.37 \\le \\epsilon \\le 0.5$ and the other group corresponds to the range\n$0.1 \\le \\epsilon \\le 0.36$. The trajectories in the first group show the upward movements. \nThis means that the error probability tends to converge to a higher value.\nOn the other hand, the trajectories in the second group indicate that $p_i(0)$ approaches \nto 1 as the number of iterations increases. This numerical results strongly suggest \nthe existence of  a {\\em bifurcation} of this DE evolution processes \nthat can be considered as a non-linear dynamical system. \nAt the erasure probability that corresponds to this bifurcation, we  can observe sudden drop\nof the asymptotic error probability in Fig.\\ref{fig:result1}.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=8cm,height=6cm]{alpha0.001_p0p1.eps}\n  \\end{center}\n  \\caption\n{\nDynamics of BP process: Evolution of message probabilities for variable nodes $p_i(0)$ and $p_i(1)$\n($(3,6)$-regular LDPC code ensemble\u00ef\u00bc\u008cfault probability $\\alpha=10^{-3}$)\n}\n  \\label{fig:result2}\n\\end{figure}\n\n\n\\subsection{Degree and asymptotic error probability}\n\nFigure \\ref{fig:change_degree} presents the asymptotic error probabilities $\\gamma(\\epsilon,\\alpha)$\nfor regular-LDPC code ensembles with degrees $(d_v,d_c)=(2,4),(3,6),(4,8)$.\nAll the ensembles correspond to the design code rate $1/2$. The fault probability is set to $\\alpha=10^{-4}$.\n \\begin{figure}[t]\n  \\centering\n  \\includegraphics[scale=1.1]{reg_rate0.5_alpha0.000100_graph.eps}\n  \\caption{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$\n($(d_v,d_c)$-regular LDPC code ensemble\u00ef\u00bc\u008cdesign code rate 0.5\u00ef\u00bc\u008cfault probability $\\alpha=10^{-4}$)\n}\n\\label{fig:change_degree}\n \\end{figure}\nFrom Fig.\\ref{fig:change_degree}, we can observe that  the $(d_v,d_c)=(3,6)$ ensemble \nprovides the highest fault BP threshold.\nIt is well known that $(3,6)$ ensemble gives the highest threshold in the fault free cases.\nSimilar tendency can be seen in the cases where transient faults exist.\n\n\\section{Message encoding}\\label{sec:message_encoding}\n\nIn the previous section, we observed numerical results of the DE analysis on the \nfault erasure BP decoder. According to the numerical results, it was shown that\nwe must admit non-zero error probability even in the asymptotic regime if the fault probability is positive.\nIn this section, we discuss and compare two methods for improving the decoding performance \nof the fault erasure BP decoding at the cost of increased hardware complexity.\n\nThe simplest way to improve the decoding performance is to exploit several \nidentical erasure BP decoders in parallel. \nBy using the majority votes from the outputs obtained from these component decoders,\nwe can obtain more reliable estimates of transmitted symbols.\nIn this paper, the scheme is called {\\em Majority voting scheme}.\nAnother way to enhance the reliability is to use a longer code to protect BP messages.\nIn Section 2, we introduced the message function $\\phi$ that encodes a BP message \ninto 2-binary symbols.  By replacing the encoding function to an encoding function for a longer code,\nwe can expect that the immunity against possible faults becomes stronger.\nWe call this scheme {\\em message encoding}.\nOf course, both schemes (i.e., majority voting and message encoding) require\nincrease of the circuit size  that can be considered as the cost should be paid for the improvement of the immunity.\n\n\\subsection{Majority voting scheme}\n\nIn this subsection, we introduce a simple majority voting scheme that \nemploies $N$-fault erasure BP decoders for  improving the fault immunity.\nThe majority voting scheme determines its output by majority voting \nbased on $N$-outputs from the component BP decoders.\nAlthough this scheme requires $N$-fold circuit size compared with\nthe single fault erasure BP decoder, it is expected that the majority voting process \nimproves the asymptotic error probability.\n\nIn the following, we will discuss the case where $N=2$. \nThe argument below can be easily extended to general cases where $N>2$.\nLet $\\hat{x}^{(1)},\\hat{x}^{(2)}\\in\\{0,1,e\\}$ be the decoder outputs from \nthe two component decoders $D^{(1)}$ and $D^{(2)}$. \nThe majority voting process is defined the function\n", "index": 9, "text": "\n\\[\n\\tau(\\hat{x}^{(1)},\\hat{x}^{(2)})=\\left\\{ \\begin{array}{ll}\n0,&(\\hat{x}^{(1)},\\hat{x}^{(2)})=(0,0),(0,e),(e,0)\\\\\ne,&(\\hat{x}^{(1)},\\hat{x}^{(2)})=(0,1),(1,0),(e,e)\\\\\n1,&(\\hat{x}^{(1)},\\hat{x}^{(2)})=(1,1),(1,e),(e,1),\\\\\n\\end{array} \\right.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\tau(\\hat{x}^{(1)},\\hat{x}^{(2)})=\\left\\{\\begin{array}[]{ll}0,&amp;(\\hat{x}^{(1)},%&#10;\\hat{x}^{(2)})=(0,0),(0,e),(e,0)\\\\&#10;e,&amp;(\\hat{x}^{(1)},\\hat{x}^{(2)})=(0,1),(1,0),(e,e)\\\\&#10;1,&amp;(\\hat{x}^{(1)},\\hat{x}^{(2)})=(1,1),(1,e),(e,1),\\\\&#10;\\end{array}\\right.\" display=\"block\"><mrow><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>e</mi><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07239.tex", "nexttext": "\nPlugging this condition probability into the DE equations, \nwe can evaluate the asymptotic error probabilities.\n\n\n\\subsection{Asymptotic error probabilities for message encoding}\\label{sec:result2}\n\nFigure \\ref{fig:result4} presents the asymptotic error probabilities  of\nthe majority voting decoder (two decoders in parallel, $N=2$) and a fault erasure BP decoder with \nmessage encoding $(n=4)$. The $(3,6)$-regular LDPC code ensemble is assumed and\nthe fault probability is set to $\\alpha=10^{-4}$.\nBoth schemes can be considered to have comparable circuit sizes.\nIn Fig. \\ref{fig:result4}, the curve of the majority voting decoder \ncorresponds to the lower bound (\\ref{eqn:parallel_LB}).\nFrom Fig.\\ref{fig:result4}, we can observe that the BP decoder with message encoding archives \na higher threshold that those of the single BP decoder and the majority logic decoder.\nThis observation implies that the message encoding has a potential advantage over\nthe majority logic decoder in terms of the decoding performance close to the threshold.\n\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=8cm,height=6cm]{l3_r6_alpha0.000100_parallel_graph.eps}\n  \\end{center}\n  \\caption\n{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability \n$\\gamma(\\epsilon,\\alpha)$($(3,6)$-regular LDPC code ensemble,  fault probability $\\alpha=10^{-4}$\u00ef\u00bc\u008c\nmessage length in message encoding $n=4$\u00ef\u00bc\u008cthe number of component decoders in majority logic decoder $N=2$)\n}\n  \\label{fig:result4}\n\\end{figure}\n\n\n\\subsection{Choice of message decoding function $\\psi$}\n\nIn a design of an appropriate message encoding scheme,  a choice of message decoding function is critical.\nWhen $n$ becomes large, we have freedom to choose a message decoding function.\nIn this subsection, we will discuss choices for  a message decoding function.\n\nWe redefine the message decoding function as \n\\begin{eqnarray}\n\\psi(y)\\triangleq\\left\\{\\begin{array}{ll}\n0, & 0\\leq w_H(y)\\leq k-1,\\\\\n1, & n-k+1\\leq w_H(y)\\leq n,\\\\\ne, & \\mbox{otherwise}. \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nThe parameter $k(1\\leq k\\leq n/2,k\\in\\mathbb{N})$ controls  the decision region for the messages $\\{0,1,e \\}$.\nFor example, As $k$ gets large, the decision region of $e$ become narrower.\nFigure \\ref{fig:n=8_change_decoding_function} presents \nrelationships between the parameter $k$ and \nthe asymptotic error probability $\\gamma(\\epsilon,\\alpha)$.\nThe message code length is assumed to be $n=8$ and the fault probability is set to $\\alpha=10^{-4}$.\n \\begin{figure}[t]\n  \\centering\n  \\includegraphics[scale=1.1]{l3_r6_alpha0.000100_n=8_decoding_function_graph.eps}\n  \\caption{\nRelationship between  erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$\n($(3,6)$-regular LDPC code ensemble\u00ef\u00bc\u008cfault probability $\\alpha=10^{-4}$\u00ef\u00bc\u008c\ncode length $n=8$)\n}\n  \\label{fig:n=8_change_decoding_function}\n \\end{figure}\nFrom Fig.\\ref{fig:n=8_change_decoding_function},\nwe can see that the asymptotic erasure probabilities depends on the parameter $k$.\nIn this setting, the worst case is $k=4$  and the best case is $k=2$.\nThis result implies that appropriate choice of the message decoding function \nis important to attain better asymptotic BP decoding performance.\n\n\n\\subsection{Relationship between code length and asymptotic error probability}\n\nFigure \\ref{fig:change_message_length} presents the asymptotic error probabilities\nfor message code length $n=2,4,8$.\n \\begin{figure}[t]\n  \\centering\n  \\includegraphics[scale=1.1]{l3_r6_alpha0.000100_message_length_graph.eps}\n  \\caption\n{\nRelationship between erasure probability $\\epsilon$ and asymptotic error probability $\\gamma(\\epsilon,\\alpha)$.\n($(3,6)$-regular LDPC code ensemble, fault probability $\\alpha=10^{-4}$\u00ef\u00bc\u008c\ncode length $n=2,4,8$)\n}\n  \\label{fig:change_message_length}\n \\end{figure}\nNote that we used the optimum parameter for each case such as \n$k=1 (n=2)$, $k=1 (n=4)$, and $k=2(n=8)$.\nThe regular LDPC code ensemble with $(d_v,d_c)=(3,6)$ is assumed and the fault probability is set to $\\alpha=10^{-4}$.\nFrom Fig.\\ref{fig:change_message_length}, it is observed that\nthe asymptotic error probabilities decreases as code length $n$ increases.\nHowever, comparing two cases $n=4$  and $n=8$, we can obtain only small improvement \nin terms of the fault BP threshold.  This means that the major benefit of longer message codes\nis lowering the error floor of the asymptotic error probability when $n$ is sufficiently large.\n\n \\section{Conclusion}\n \nIn this paper, we proposed a model for the fault erasure BP decoders with  transient faults.\nBased on the model, the DE equations were derived and used for \nnumerical evaluation. The DE analysis shows the asymptotic behaviors of \nthe fault erasure BP decoder.\nThe most notable result revealed via the DE analysis is that \nthe asymptotic error probability converges to a positive value in contrast to the the fault free case.\nThe sudden drop of error probability at a certain erasure probability is considered to be a consequence of\na bifurcation of the DE dynamical system.\nIn order to improve the decoding performance, we presented two schemes: the message encoding scheme and\nthe majority voting scheme. The result of the DE analysis indicates that \nthe message encoding scheme has clear advantage over the majority voting scheme in terms of \nthe fault erasure BP threshold.\n\n\\section*{Acknowledgment}\n\n This work was supported by JSPS Grant-in-Aid for Scientific Research\n (B) Grant Number 25289114.\n\n\\begin{thebibliography}{99}\n\\bibitem{ITRS} Semiconductor Industry Association, ``International Technology Roadmap for Semi-conductors (ITRS),'' 2011.\n\\bibitem{fault1}J. Han and P. Jonker, ``A defect-and fault-tolerant architecture for nanocomputers,'' Nanotechnology, vol. 14, no. 2, pp. 224-230, Feb. 2003.\n\\bibitem{fault-tolerant1}D. K. Pradhan, ``Fault-tolerant computer system design,'' Upper Saddle River, NJ: Prentice Hall, 1996.\n\\bibitem{fault-tolerant2}B. W. Jonson, ``Design and analysis of fault-tolerant digital systems,'' Reading, MA: Addison-Wesley Publishing Company 1989.\n\\bibitem{network-on-chip}K. Santanu, C. Santanu, ``The next generation of system-on-chip integration,'' CRC Press, Oct. 2014.\n\\bibitem{LT}H. Wang, ``Hardware designs for LT coding,'' MSc Thesis, Department of Electrical Engineering, Delft University of Technology, 2004.\n\\bibitem{fault-decoder1}L. R. Varshey, ``Performance of LDPC codes under faulty iterative decoding,'' IEEE Trans. Inf. Theory, vol. 57, no. 7, pp. 4427-4444, July 2011.\n\\bibitem{fault-decoder2}S. M. S. Tabatabaei, H. Cho, and L. Dolecek, ``Gallager B decoder on noisy hardware,'' IEEE Trans. Commun. vol. 61, no. 5, pp. 1660-1675, May 2013.\n\\bibitem{fault-decoder3}C. K. Ngassa, V. Savin, and D. Declercq, ``Min-sum-based decoders running on noisy hardware,'' in Proc. IEEE Global Commun. Conference, 2013.\n\\bibitem{fault-decoder4}A. Balatsoukas-Stimming and A. Burg, ``Density evolution for min-sum decoding of LDPC codes under unreliable message storage,'' IEEE Commun. Lett, vol. 18, no. 5, pp. 849-852, May 2014.\n\\bibitem{fault-decoder5}C. H. Huang, Y. Li, and L. Dolecek, ``Gallager B LDPC decoder with transient and permanent errors,'' IEEE Transactions on Communications, vol. 62, no. 1, pp. 15-28, Jan. 2014.\n\\bibitem{fault-decoder6}B. Vasic, P. Ivanis, S. Brkic, and V. Ravanmehr, ``Fault-resilient decoders and memories made of unreliable components,'' in Proc. Inf. theory and Applications Workshop (ITA 2015), San Diego, CA, paper 273, Feb. 2015.\n\\bibitem{Neumman-model}J. von Neumann, ``Probabilistic logic and the synthesis of reliable organisms from unreliable components,'' Automata Studies, vol. 34, pp. 43-98, 1956. \n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 27124, "prevtext": "\nwhere the function $\\tau$ represents the output from the majority voting decoder.\nWe denote the asymptotic error probability for the majority voting scheme \nby  $\\gamma_{maj}(\\epsilon,\\alpha)$.\n\nIn the following, a lower bound on $\\gamma_{maj}(\\epsilon,\\alpha)$ will be discussed.\nThroughout the following argument, we assume that \n\\begin{eqnarray}\\label{eqn:maj_theorem_assumption}\ns(0,0)&\\geq& (s(0))^2 \n\\end{eqnarray}\nholds where the quantity $s(0)$  in the righthand side  is the asymptotic value of $s_0(x)$; i.e., \n$\ns(0) = \\lim_{i \\rightarrow \\infty} s_i(0),\n$\nwhich can be evaluated by the density evolution. The quantity $s(0,0)$ is the asymptotic joint probability \ncorresponds to the event that two decoder outputs take the value $(0,0)$.\nThis is a natural assumption because the outputs from the the component decoders $D^{(1)},D^{(2)}$ \nare expected to be highly correlated. \nUnder the assumption of (\\ref{eqn:maj_theorem_assumption}),\nwe can easily derive a lower bound  of $\\gamma_{maj}(\\epsilon,\\alpha)$\n\\begin{eqnarray}\\label{eqn:parallel_LB}\n    \\gamma_{maj}(\\epsilon,\\alpha)\\geq(1-s(0))^2.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Details of message encoding}\n\nIn this subsection, we will introduce a simple message encoding scheme \nbased on a binary code of length $n$. The parameter $n$ is referred to as\n{\\em message code length}.\nIn the following, we redefine the encoding and decoding functions.\nThe encoding function $\\phi: \\{0,1,e\\} \\rightarrow \\{0,1\\}^n$ is an encoding function \nnow defined by\n\\begin{eqnarray}\n\\phi(x)=\\left\\{\\begin{array}{ll}\n\\overbrace{00\\cdots0}^{n}, & x=0, \\\\\n\\overbrace{11\\cdots1}^{n}, & x=1, \\\\\n\\overbrace{00\\cdots0}^{n/2}\\overbrace{11\\cdots1}^{n/2}, & x=e.\\\\\n\\end{array} \\right.\n\\end{eqnarray} \nThere are several possibilities for choosing decoding functions corresponding to \nthe encoding function defined above.\nOne simple choice is to define a decoding function $\\phi$ as\n\\begin{eqnarray}\n\\psi(y)=\\left\\{\\begin{array}{ll}\n0, & w_H(y) = 0 \\\\\n1, & w_H(y) = n\\\\\ne, & \\mbox{otherwise}, \\\\\n\\end{array} \\right.\n\\end{eqnarray}\nwhere $w_H$ represents the Hamming weight function.\nIn this case, the conditional probability $Q(\\hat{x}|x)$ corresponding to \nthe intermediate node is given by \n", "index": 11, "text": "\n\\[\n\\left(\n\\begin{array}{ccc}\n(1-\\alpha)^n&\\alpha^n&1-(1-\\alpha)^n-\\alpha^n\\\\\n\\alpha^n&(1-\\alpha)^n&1-(1-\\alpha)^n-\\alpha^n\\\\\n\\alpha^\\frac{n}{2}(1-\\alpha)^\\frac{n}{2}&\\alpha^\\frac{n}{2}(1-\\alpha)^\\frac{n}{2}&1-2\\alpha^\\frac{n}{2}(1-\\alpha)^\\frac{n}{2}\\\\\n\\end{array}\n\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\left(\\begin{array}[]{ccc}(1-\\alpha)^{n}&amp;\\alpha^{n}&amp;1-(1-\\alpha)^{n}-\\alpha^{n%&#10;}\\\\&#10;\\alpha^{n}&amp;(1-\\alpha)^{n}&amp;1-(1-\\alpha)^{n}-\\alpha^{n}\\\\&#10;\\alpha^{\\frac{n}{2}}(1-\\alpha)^{\\frac{n}{2}}&amp;\\alpha^{\\frac{n}{2}}(1-\\alpha)^{%&#10;\\frac{n}{2}}&amp;1-2\\alpha^{\\frac{n}{2}}(1-\\alpha)^{\\frac{n}{2}}\\\\&#10;\\end{array}\\right).\" display=\"block\"><mrow><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup></mtd><mtd columnalign=\"center\"><msup><mi>\u03b1</mi><mi>n</mi></msup></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mo>-</mo><msup><mi>\u03b1</mi><mi>n</mi></msup></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msup><mi>\u03b1</mi><mi>n</mi></msup></mtd><mtd columnalign=\"center\"><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mo>-</mo><msup><mi>\u03b1</mi><mi>n</mi></msup></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\u03b1</mi><mfrac><mi>n</mi><mn>2</mn></mfrac></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mi>n</mi><mn>2</mn></mfrac></msup></mrow></mtd><mtd columnalign=\"center\"><mrow><msup><mi>\u03b1</mi><mfrac><mi>n</mi><mn>2</mn></mfrac></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mi>n</mi><mn>2</mn></mfrac></msup></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03b1</mi><mfrac><mi>n</mi><mn>2</mn></mfrac></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mi>n</mi><mn>2</mn></mfrac></msup></mrow></mrow></mtd></mtr></mtable><mo>)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}]