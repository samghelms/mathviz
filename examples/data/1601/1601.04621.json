[{"file": "1601.04621.tex", "nexttext": "\n\nand learn the probability of following each feature account given an age category. Here $P(A)$ is the prior distribution of Twitter user ages, which we take from the survey by \\cite{Duggan2013} \\footnote{The survey only looked only looked at over-18s and so we inferred under-18s based on US census data}: \n\n\n\n", "itemtype": "equation", "pos": 19026, "prevtext": "\n\n\n\n\\title{Detecting the Age of Twitter Users}\n\n\\author{Benjamin Paul Chamberlain$^1$, Clive Humby$^2$, Marc Peter Deisenroth$^1$ \\\\ Department of Computing, Imperial College London, London SW7 2AZ$^1$ \\\\ Starcount Insights, 2 Riding House Street, London W1W 7FA$^2$}\n\n\\maketitle\n\n\\begin{abstract}\n\n\n\n\n\nTwitter provides an extremely rich and open source of data for studying human behaviour at scale. It has been used to advance our understanding of social network structure, the viral flow of information and how new ideas develop. Enriching Twitter with demographic information would permit more precise science and better generalisation to the real world. The only demographic indicators associated with a Twitter account are the free text name, location and description fields. We show how the age of most Twitter accounts can be inferred with high accuracy using the structure of the social graph. Besides classical social science applications, there are obvious privacy and child protection implications to this discovery.\n\nPrevious work on Twitter age detection has focussed on either user-name or linguistic features of tweets. A shortcoming of the user-name approach is that it requires real names (Twitter names are often false) and census data from each user's (unknown) birth country. Problems with linguistic approaches are that most Twitter users do not tweet (the median number of Tweets is 4) and a different model must be learnt for each language.\n\nTo address these issues, we devise a language-independent methodology for determining the age of Twitter users from data that is native to the Twitter ecosystem. Roughly 150,000 Twitter users specify an age in their free text description field. We generalize this to the entire Twitter network by showing that age can be predicted based on what/whom they follow.\n\n\nWe adopt a Bayesian classification paradigm, which offers a consistent framework for handling uncertainty in our data, e.g., inaccurate age descriptions or spurious edges in the graph.\n\nWorking within this paradigm we have successfully applied age detection to 700 million Twitter accounts with an F1 Score of 0.86.\n\n\n\n\n\n\n\n\n\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\nOwing to its popularity and openness, Twitter data is frequently used in scientific research. In its raw form it contains limited information about the people generating the data. Enriching Twitter data with age estimates is useful for scientific modelling and has commercial applications in areas such as marketing and personalised advertising. \n\n\n\\cite{Kosinski2013} have shown that people's interests are indicative of their age and that Facebook page likes can be used to predict age with high accuracy. Page likes play a similar role in Facebook as follows do in Twitter and so this result indicates that the same results could be achieved with Twitter. There are two principle causes of this effect: Firstly, people of similar ages tend to have similar interests as a result of age related life events (education, child birth, marriage, employment, retirement, wealth changes etc.). Secondly, social networks exhibit homophily with people tending to connect with others having similar attributes (age in this case), and because information about what else to follow flows through connected people \\citep{McPherson2001}.\n\n\nPrevious research into Twitter age detection has focussed on linguistic models of tweets or the free text ``user name'' field. Linguistic models require significant research efforts to be applied for every distinct language and has currently only been developed in Dutch \\citep{nguyen2013old}. In addition, any method that relies on tweets is unable to make predictions for the majority of Twitter users who do not generate tweets in adequate volumes. This is a problem since  the median number of tweets in our data set of 700 million users is four. Methods based on user-name rely on knowledge of the relative frequency of the user's true name in their country of birth \\citep{Oktay2014}. However, Twitter users are under no obligation to supply their real names and their country of birth is generally unknown.\n\n\nIn this paper, we take a different approach based on the hypothesis that (a) someone's interests depend on their age, and (b) what they follow describes (some of) their interests. We use this hypothesis to derive a machine learning model that predicts a user's age from what/whom they follow. We take the 133,000 accounts of users who specify their age in their description field as labelled ground truth. The features of the model are the 103,722 accounts that have greater than 10,000 followers and are followed by more than 10 labelled accounts. The features are subsequently used to infer the age of any Twitter user based on whom/what they follow within a Bayesian framework. Unlike methods that rely on linguistic style or first-name frequencies, our method generalises across national and linguistic boundaries. However, a major challenge with this data is that ages specified in the description field can be inaccurate and the graph contains many spurious edges that are not indicative of true interests. For this reason, we adopt a Bayesian classification paradigm, which offers a consistent framework for handling all forms of uncertainty.\n\n\n\\section{Related Work}\n\\label{sec:rel}\n\nOne of the first studies of Twitter demographics was conducted by \\cite{Mislove2011}. They used a large corpus of Tweets to derive demographic data for Twitter users located in the US. They did not study age, choosing instead to focus on the distributions of geography, race and gender. They were able to conclude that the Twitter population is not representative of the broader US populace.\n\n\nPrevious work on Twitter age detection has focussed on either aspects of language or user names. For instance,  \\cite{nguyen2013old} produced a lexical analysis of Dutch language tweets. They obtained ground truth through a manual tagging process employing several students. The principle features were simply unigrams, where the intuition was that older people use more positive language, fewer pronouns and longer words and sentences. Other features like intensifiers (emoticons), alphabetic lengthening (e.g.,  ``sweeeeeet'') and capitalisation were also useful. They found that age prediction works well for young people, but that above the age of 30, language tends to homogenise. They also noted a strong relationship between language and gender (women tend to identify more with language and use more first singular pronouns, while men use more links) implying that they need to be modelled together.  They used simple linear models in the study (linear and logistic regression). Age detection using lexical features is complicated by the existence of the concept of social age \\citep{DongNguyen142014}. A social age is determined by life stage (married, children, employment etc.) rather than years since birth and it has a strong affect on writing style. Social age and gender are fluid constructs and attempts to predict them from text oversimplify the problem. For instance, people interacting in groups of the opposite sex may adapt their language to mimic the perceived social norm. \n\n\n\\cite{Oktay2014} estimate the age of Twitter users from the first name supplied in the free-text account-name field. They use US Social Security data to generate probability distributions for birth year given age and show that for some names age distributions are quite sharply peaked. They also used second names to estimate ethnicity and found that both age and ethnicity have large affects on the patterns of Twitter usage.\n\n\n\\cite{Kosinski2013} used Facebook-likes to determine a broad range of attributes of users. They generated ground truth through responses to a Facebook app designed to measure a user's personality. They were particularly successful at identifying age (80\\% accuracy on a held out test set), and their approach has the benefit of generalising readily to different locales. Facebook likes are closely related to Twitter follows; both are indicators of user interest. However their work differs from ours for two principle reasons. Firstly Facebook contains a birthday field and so age can easily be calculated given access to this field. Secondly, they assumed that users accessing their app represented a stratified sample of Facebook ages and we are not able to make this assumption. \n\n\nFinally we make use of a survey of internet using Americans conducted by \\cite{Duggan2013}. They identified a sample of 1802 adults (speaking either English or Spanish) using random cold calling and recorded their demographic information and use of social media. 288 of their respondents were Twitter users. \n\n\n\\section{Data and Preliminaries}\n\\label{sec:prelim}\n\n\nOnly roughly 0.02\\% of Twitter profile descriptions contain age data and so working with a small sample of the network is not viable. We implemented a distributed web crawler to download the Twitter graph and associated account metadata using Twitter access tokens mined through an app. To optimize data throughput while remaining within Twitter's rate limits we developed an asynchronous system connected to an access token server using Python's Twisted library~\\citep{Wysocki2011}. We indexed each user's description field using Apache SOLR and searched the index for REGular EXpression (REGEX) patterns that were indicative of age data across Twitter's four major languages (English, Spanish, French and Portuguese). Initially, we attempted to match the multilingual equivalents of the English phrases, such as  ``I am \\textit{x}'', ``I am \\textit{x} y'', ``\\textit{x} years''. This resulted in significant issues with phrases of the form ``more than \\textit{x} years'', ``think I am \\textit{x} years'' and ``feel like \\textit{x} years'', which required further refinements of the REGEX matching string to logically exclude these forms, e.g.,. ``\\textit{x} years NOT(feel like \\textit{x} years, think I am \\textit{x} years)''. This process discovered 133,000 users who disclosed their age (i.e., 0.02 \\% of the 700 million indexed accounts) with a total of 15.9 million outgoing edges of which 7.14 million were to accounts with greater than 10,000 followers. \n\n\nWe bucket ages of users into ten categories chosen to suit our use case of targeted marketing. The age categories and number of accounts, relative frequency and average number of features per category are shown in Table \\ref{tab:age categories}.\n\n\\begin{table}[tb]\n  \\centering\n  \\caption{Age categories and counts. Mean features gives the average number of feature accounts followed.}\n    \\begin{tabular}{rrrrr}\n    \\toprule\n    idx & age range & count & freq & mean features\\\\\n    \\midrule\n    0     & under 12  & 7753 & 5.9\\% & 23.7\\\\ \n    1     & 12--13 & 20851 & 15.8\\% & 27.9\\\\\n    2     & 14--15 & 30570 & 23.1\\% & 30.8\\\\\n    3     & 16--17 & 23982 & 18.1\\% & 28.7\\\\\n    4     & 18--24 & 33331 & 25.2\\% & 26.0\\\\\n    5     & 25--34 & 9286 &  7.0\\% & 23.1\\\\\n    6     & 35--44 & 3046  & 2.3\\% & 22.6\\\\\n    7     & 45--54 & 1838 & 1.0\\% & 16.0\\\\\n    8     & 55--64 & 962 & 0.7\\% & 11.4\\\\\n    9     & over 65   & 596 & 0.5\\% & 11.2\\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:age categories}\n\\end{table}\n\nTable~\\ref{fig:input_bar} compares the relative counts of age classes reported in user descriptions to the counts that would be expected based on scaling up the survey conducted by~\\cite{Duggan2013}. Our data set appears to be highly skewed towards younger Twitter users. We believe this is because older users are less inclined to disclose their age. As a result, any model that attempts to use this sample to make broader predictions must take this skew into account. \n\n\\begin{figure}[tb]\n  \\centering\n    \\includegraphics[width=\\hsize]{input_age_bar.pdf}\n    \\caption{Counts of ages within each age category. The data is heavily skewed towards younger users when compared with an expectation based on user surveys~\\citep{Duggan2013}.}\n\\label{fig:input_bar}\n\\end{figure}\n\nTable~\\ref{fig:input_bar} also shows that data is relatively small for ages greater than 45, an effect that is compounded by older users tending to follow fewer features. To increase the data set size for improving the predictive performance for these age categories, we looked for terms that implicitly define older users. Using occupation terms to infer age bands induced new problematic biases (principally levels of education and affluence). However, we discovered that while older Twitter users do not disclose their age, they disclose that they are retired or that they are parents or grandparents in large numbers. To identify parents or grandparents it is necessary to exclude second person mentions (e.g., ``my grandmother''), and we found that any mention of both a male and female parent or grandparent tended to be erroneous. We assumed that retired people were over 65 years old and so it was necessary to exclude the large numbers of retired military personnel who are now working in civilian jobs. To ensure mutual exclusivity we allocated accounts that indicated multiple age bands to the most specific, assuming that ``retired'' is more specific than ``grandparent'', which in turn is more specific than ``parent''. Table~\\ref{tab:older_people} shows the number of users who describe themselves as parents, grandparents or retired and the age categories that we assumed for them. Counts are orders of magnitude greater than those for users who described themselves having a specific age with higher average features followed that for any class in Table \\ref{tab:counts}.\\footnote{We did not use parents in our work due to their low age specificity. We include the data as a sign post to future researchers who may wish to use parents as a starting point to efficiently sample older Twitter users for manual age coding.}\n\n\n\\begin{table}[tb]\n  \\centering\n  \\caption{Twitter users who disclose that they are parents, grandparents or retired in their descriptions. Data is selected to be mutually exclusive, i.e., ``parents'' is ``parents'' AND NOT (``grandparents'' OR ``retired'') and ``grandparents'' is ``grandparents'' AND NOT ``retired''. We did not calculate the average number of features exhibited by parents.}\n    \\begin{tabular}{rrrr}\n    \\toprule\n    index & age range & count & mean features \\\\\n    \\midrule\n    parents & over 18 & 1,485,164 & - \\\\\n    grandparents & over 45   & 63,895 & 45.0 \\\\\n    retired & over 65   & 176,748 & 36.1 \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:older_people}\n\\end{table}\n\n\\section{Age Detection based on Follows}\n\\label{sec:method}\nGiven a set of 133,000 labelled data points we wish to predict the age of the remaining 700 million Twitter users within our data set. The standard machine learning approach to achieve this task is to measure a set of features that define a space in which the labelled data can be separated, and then to learn specific parameters for each class. \n\n\\subsection{Feature Selection}\n\nOur hypothesis is that someone's age is predictive of what they are interested in and that Twitter-follows serve as a proxy for interests (similar to Facebook-likes). For this reason we selected Twitter accounts with greater than 10,000 followers as the features of our model. We only used features that are followed by more than 10 users who disclosed their age giving 103,722 features. Table \\ref{tab:counts} shows the 10 features with the highest support (number of followers who disclose their age) in our labelled data. The followers column gives the total number of followers of each feature across the Twitter network. There is a Pearson correlation of 0.86 between the support and the total follower count for our data set.\n\n\n\\begin{table*}[htbp]\n  \\centering\n  \\caption{The 10 accounts with the highest support within the labeled data set. Followers gives the total followers across all of Twitter}\n  \\scalebox{0.95}{\n    \\begin{tabular}{rrrrrrrrrrrrr}\n    \\toprule\n    \\textbf{twitter\\_handle} & \\textbf{support} & \\textbf{under 12} & \\textbf{12-13} & \\textbf{14-15} & \\textbf{16-17} & \\textbf{18-24} & \\textbf{25-34} & \\textbf{35-44} & \\textbf{45-54} & \\textbf{55-64} & \\textbf{65+} & \\textbf{followers} \\\\\n    \\midrule\n    \\textbf{justinbieber} & \\textbf{20359} & 1517  & 5179  & 5737  & 4202  & 3073  & 412   & 99    & 67    & 34    & 38    & 5.6$\\times 10^7$ \\\\\n    \\textbf{katyperry} & \\textbf{18395} & 1467  & 4180  & 4410  & 3604  & 3575  & 701   & 158   & 124   & 75    & 102   & 5.9$\\times 10^7$ \\\\\n    \\textbf{taylorswift13} & \\textbf{15199} & 1207  & 3417  & 3674  & 3045  & 2919  & 507   & 113   & 117   & 79    & 122   & 4.6$\\times 10^7$ \\\\\n    \\textbf{selenagomez} & \\textbf{14264} & 1270  & 3578  & 3691  & 2847  & 2339  & 367   & 76    & 43    & 26    & 27    & 2.4$\\times 10^7$ \\\\\n    \\textbf{ArianaGrande} & \\textbf{13512} & 1254  & 3404  & 3604  & 2631  & 2172  & 319   & 50    & 40    & 19    & 20    & 2.0$\\times 10^7$ \\\\\n    \\textbf{ddlovato} & \\textbf{13259} & 1099  & 3284  & 3562  & 2741  & 2135  & 301   & 53    & 37    & 19    & 28    & 2.5$\\times 10^7$ \\\\\n    \\textbf{onedirection} & \\textbf{12834} & 979   & 3472  & 3778  & 2767  & 1622  & 138   & 43    & 20    & 7     & 8     & 2.1$\\times 10^7$ \\\\\n    \\textbf{Harry\\_Styles} & \\textbf{12830} & 912   & 3468  & 3936  & 2751  & 1581  & 120   & 24    & 15    & 9     & 13    & 2.2$\\times 10^7$ \\\\\n    \\textbf{NiallOfficial} & \\textbf{12498} & 858   & 3431  & 3895  & 2702  & 1468  & 90    & 24    & 15    & 8     & 8     & 2.0$\\times 10^7$ \\\\\n    \\textbf{YouTube} & \\textbf{11688} & 926   & 2496  & 2687  & 2193  & 2287  & 495   & 183   & 154   & 99    & 169   & 4.6$\\times 10^7$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  \\label{tab:counts}\n\\end{table*}\n\n\\subsection{Bayesian Model}\n\nWe adopt the Bayesian modelling paradigm because it provides the only way to consistently model the various types of uncertainty (noisy labels, bad edges, survey estimates etc.) encountered in this problem.\n\nIn what follows we denote the feature vector of an unlabelled account by $X \\in \\{0,1\\}^M$, where $M$ is the number of features and $X_i = 1$ if and only if the user follows the $i^{th}$ feature, with the corresponding target vector of ages as $A \\in \\{0,1,...,9\\}$. We use $\\mathbf{X} \\in \\{0,1\\}^{N\\times M}$ to represent the whole labelled data set with $N$ the number of labelled data points and $\\mathbf{A} \\in \\{0,1,...,9\\}^N$ the known labels. \n\nWe wish to estimate $P(A| X,\\mathbf{X,A})$, the distribution of the age $A$ of an unseen user $X$ given the set of accounts $\\mathbf{X}$ that they follow and the labelled training set $\\mathbf{A}$. As older people are less likely to explicitly state their age, our labelled data is severely biased and we can not simply assign multinomial distributions $P(A | X_i)$ for each feature based on the relative frequencies of their followers (see Table \\ref{tab:counts}). To overcome this problem we make use of Bayes' law in the form\n\n\n", "index": 1, "text": "$$ P(A | X,\\mathbf{X,A}) \\propto P(X | A,\\mathbf{X,A}) P(A) $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"P(A|X,\\mathbf{X,A})\\propto P(X|A,\\mathbf{X,A})P(A)\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">|</mo><mi>X</mi><mo>,</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>A</mi><mo>,</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04621.tex", "nexttext": "\n\nOur model has $10^5$ features, which implies $5 \\times 10^{9}$ pairwise correlations. For tractability we make the Naive Bayes' assumption that the decision to follow each account is independent given the age of the user, leading to\n\n\n", "itemtype": "equation", "pos": 19399, "prevtext": "\n\nand learn the probability of following each feature account given an age category. Here $P(A)$ is the prior distribution of Twitter user ages, which we take from the survey by \\cite{Duggan2013} \\footnote{The survey only looked only looked at over-18s and so we inferred under-18s based on US census data}: \n\n\n\n", "index": 3, "text": "$$P(A) = [0.8, 2, 22, 3, 14, 23, 23, 22, 6, 4] \\times 10^{-2} $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"P(A)=[0.8,2,22,3,14,23,23,22,6,4]\\times 10^{-2}\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mn>0.8</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>22</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>14</mn><mo>,</mo><mn>23</mn><mo>,</mo><mn>23</mn><mo>,</mo><mn>22</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">]</mo></mrow><mo>\u00d7</mo><msup><mn>10</mn><mrow><mo>-</mo><mn>2</mn></mrow></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04621.tex", "nexttext": "\n\nwhere $i$ indexes the features. We only consider cases where $X_i = 1$ as the Twitter graph is very sparse\\footnote{We measured 700 million nodes having 50 billion edges, which implies a density of $1.6\\times 10^{-7}$}, and the default position of every account is to follow nobody. Therefore, the act of not following an account does not contain enough information to justify the additional computational cost. \nThe Maximum Likelihood Estimator (MLE) is given by\n\n\n", "itemtype": "equation", "pos": 19699, "prevtext": "\n\nOur model has $10^5$ features, which implies $5 \\times 10^{9}$ pairwise correlations. For tractability we make the Naive Bayes' assumption that the decision to follow each account is independent given the age of the user, leading to\n\n\n", "index": 5, "text": "$$ P(X | A,\\mathbf{X,A}) = \\prod_{X_i = 1} P(X_i | A,\\mathbf{A,X}) $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"P(X|A,\\mathbf{X,A})=\\prod_{X_{i}=1}P(X_{i}|A,\\mathbf{A,X})\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>A</mi><mo>,</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>A</mi><mo>,</mo><mi>\ud835\udc00</mi><mo>,</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04621.tex", "nexttext": "\n\nwhere $n_a$ is the number of labelled data points in age category $a$ and $n_{i,a}$ is the number that also follow account $i$. \n\n\nAfter applying Laplacian smoothing with parameter equal to 1~\\citep{Bishop2006} to $P(X_i = 1 | A,\\mathbf{A,X})$ the point estimate model produces many pathological distributions with high probability in incorrect categories or bi-modal class predictions of the form ``under 12 with 50\\% probability and over 65 with 50\\% probability''. The weakness of this model is that it assumes the same level of uncertainty for each $P(X_i = 1 | A,\\mathbf{A,X})$ when in reality the distributions for Twitter's most popular accounts are less uncertain than those with only a few thousand followers.\n\nTo resolve these issues we model $P(X_i | A,\\mathbf{A,X}, \\mu)$ as a Bernoulli random variable and place a conjugate Beta prior on the Bernoulli parameter $\\mu$. Figure \\ref{fig:pgm} shows the corresponding probabilistic graphical model that we use to infer the age of Twitter users. Probabilistic graphical models (PGMs) are tools used to visualise independencies inherent in a collection of random variables and are described in great detail in \\cite{Koller2009}. In Figure \\ref{fig:pgm} the labelled training data is represented as $\\mathbf{A}_{i,j}$ and $\\mathbf{X}_{i,j}$ and the target value to be predicted as $A$.  Given the learnt model parameters $\\mu_{i,a}$, $A$ only depends on the set of features $X_i$. Figure \\ref{fig:pgm} describes the following generative process for age data:\n\\begin{enumerate}\n\\item For each feature/age combination draw $\\mu_{a,i} \\sim Beta(b, c)$\n\\item Choose an age $A \\sim Mult(\\pi)$\n\\item For each feature $X_i \\sim Bernoulli(\\mu_{i,a})$\n\\end{enumerate}\n\nDue to conjugacy, the posterior distribution is also Beta distributed. By integrating out $\\mu$ in the usual Bayesian way we obtain \n\n\n", "itemtype": "equation", "pos": 20235, "prevtext": "\n\nwhere $i$ indexes the features. We only consider cases where $X_i = 1$ as the Twitter graph is very sparse\\footnote{We measured 700 million nodes having 50 billion edges, which implies a density of $1.6\\times 10^{-7}$}, and the default position of every account is to follow nobody. Therefore, the act of not following an account does not contain enough information to justify the additional computational cost. \nThe Maximum Likelihood Estimator (MLE) is given by\n\n\n", "index": 7, "text": "$$ P(X_i = 1 | A,\\mathbf{A,X}) = \\frac{n_{i,a}}{n_a} $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"P(X_{i}=1|A,\\mathbf{A,X})=\\frac{n_{i,a}}{n_{a}}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>A</mi><mo>,</mo><mi>\ud835\udc00</mi><mo>,</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><msub><mi>n</mi><mrow><mi>i</mi><mo>,</mo><mi>a</mi></mrow></msub><msub><mi>n</mi><mi>a</mi></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04621.tex", "nexttext": "\n\n\\begin{figure}[tb]\n  \\centering\n    \\includegraphics[width=0.5\\hsize]{age_pgm3.pdf}\n    \\caption{The probabilistic graphical model for the age estimation problem. Circles are random variables, with shading when they are observed. Small dots indicate model parameters and the plates denote replication.}\n\\label{fig:pgm}\n\\end{figure}\n\n\\begin{figure*}[htb]\n  \\centering\n  \\begin{subfigure}[t]{0.45\\hsize}\n    \\includegraphics[width=\\hsize]{confusion_matrix_age_labeled.pdf}\n    \\caption{Confusion matrix for a 10 \\% held out test set. Data included only accounts with explicit age labels. Very few predictions are made in older categories where labelled data is limited.}\n    \\label{fig:cm_age_labeled}\n   \\end{subfigure}\n   \\hspace{5mm}\n   \\begin{subfigure}[t]{0.45\\hsize}\n   \\includegraphics[width={\\hsize}]{confusion_matrix_all.pdf}\n    \\caption{Confusion matrix using retired people and grandparents.}\n\t\\label{fig:cm_all}\n   \\end{subfigure}\n    \\caption{Confusion matrices for a 10 \\% held out test set. (\\subref{fig:cm_age_labeled}) Data included only accounts with explicit age labels. Very few predictions are made in older categories where labelled data is limited. (\\subref{fig:cm_all}) By including data from retired people and grandparents, the predictions in the older age categories are improved substantially.}\n\\label{fig:cm_age_labeled_and_all}\n\\end{figure*}\n\nWe seek hyper-parameters $b,c$ of the prior $p(\\mu)$ that smooth out noisy observations of less popular feature accounts, but which do not have a large effect when ample data is available. To achieve this we set $c_i$ to be constant across all features (hence dropping the subscript) and proportional to the total number of observations in each age class. We then set $b_i$ so that $\\mathbb{E}[\\mu_i] = \\tfrac{n_i}{N} = \\tfrac{b_i}{b_i+c}$. This generates a prior that assumes that the a-priori probability of following an account is constant across age classes and varies in proportion to the number of followers across features.\n\n\n\\subsection{Data Cleaning}\n\nApplying REGEX matches to free-text fields inevitably leads to some false matches owing to unanticipated character combinations when working with large data sets. In addition, many Twitter accounts, while correctly labelled, may not represent the interests of human beings. This can occur when accounts are controlled by machines (bots), accounts are set up to look authentic to distribute spam (spam accounts) or account passwords are hacked in order to sell authentic looking followers. \n\nTo detect and remove spurious labelled data points we looked for accounts that had a large impact on the likelihood $P(\\mathbf{X}|\\mathbf{A})$. The Kullback-Leibler divergence (KL divergence) is the most commonly used to compare two probability distributions. We calculated an anomalousness score for the $i^{th}$ data point \n\n\n", "itemtype": "equation", "pos": 22142, "prevtext": "\n\nwhere $n_a$ is the number of labelled data points in age category $a$ and $n_{i,a}$ is the number that also follow account $i$. \n\n\nAfter applying Laplacian smoothing with parameter equal to 1~\\citep{Bishop2006} to $P(X_i = 1 | A,\\mathbf{A,X})$ the point estimate model produces many pathological distributions with high probability in incorrect categories or bi-modal class predictions of the form ``under 12 with 50\\% probability and over 65 with 50\\% probability''. The weakness of this model is that it assumes the same level of uncertainty for each $P(X_i = 1 | A,\\mathbf{A,X})$ when in reality the distributions for Twitter's most popular accounts are less uncertain than those with only a few thousand followers.\n\nTo resolve these issues we model $P(X_i | A,\\mathbf{A,X}, \\mu)$ as a Bernoulli random variable and place a conjugate Beta prior on the Bernoulli parameter $\\mu$. Figure \\ref{fig:pgm} shows the corresponding probabilistic graphical model that we use to infer the age of Twitter users. Probabilistic graphical models (PGMs) are tools used to visualise independencies inherent in a collection of random variables and are described in great detail in \\cite{Koller2009}. In Figure \\ref{fig:pgm} the labelled training data is represented as $\\mathbf{A}_{i,j}$ and $\\mathbf{X}_{i,j}$ and the target value to be predicted as $A$.  Given the learnt model parameters $\\mu_{i,a}$, $A$ only depends on the set of features $X_i$. Figure \\ref{fig:pgm} describes the following generative process for age data:\n\\begin{enumerate}\n\\item For each feature/age combination draw $\\mu_{a,i} \\sim Beta(b, c)$\n\\item Choose an age $A \\sim Mult(\\pi)$\n\\item For each feature $X_i \\sim Bernoulli(\\mu_{i,a})$\n\\end{enumerate}\n\nDue to conjugacy, the posterior distribution is also Beta distributed. By integrating out $\\mu$ in the usual Bayesian way we obtain \n\n\n", "index": 9, "text": "\\begin{align} \nP(X_i| A,\\mathbf{X,A}) &= \\int^1_0 P(X_i| \\mu_i,A) P(\\mu_i|b_i,c_i) d\\mu_i \\\\\n &= \\int^1_0 \\mu_i P(\\mu_i|b_i,c_i) d\\mu_i\\\\\n &= \\frac{n_{i,a} + b_i}{n_{a}+b_i+c_i} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(X_{i}|A,\\mathbf{X,A})\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>A</mi><mo>,</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int^{1}_{0}P(X_{i}|\\mu_{i},A)P(\\mu_{i}|b_{i},c_{i})d\\mu_{i}\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo>,</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>b</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><msub><mi>\u03bc</mi><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int^{1}_{0}\\mu_{i}P(\\mu_{i}|b_{i},c_{i})d\\mu_{i}\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup></mstyle><msub><mi>\u03bc</mi><mi>i</mi></msub><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>b</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><msub><mi>\u03bc</mi><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{n_{i,a}+b_{i}}{n_{a}+b_{i}+c_{i}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>n</mi><mrow><mi>i</mi><mo>,</mo><mi>a</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mrow><msub><mi>n</mi><mi>a</mi></msub><mo>+</mo><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><msub><mi>c</mi><mi>i</mi></msub></mrow></mfrac></mstyle></mrow></math>", "type": "latex"}, {"file": "1601.04621.tex", "nexttext": "\n\nwhere $P$ is the likelihood calculated from the full, labelled data set and $P_{\\setminus i}$ is the likelihood calculated from the labelled data set minus the $i^{th}$ data. We excluded any training examples that were more than three Median Absolute Deviations (MAD) from the mean score. This process excluded 246 accounts from our training data. Examples of invalid training data are shown in Table \\ref{tab:bad_ids}.\n\n\n\\begin{table*}[tb]\n  \\centering\n  \\caption{Spurious data points identified by taking the Median Absolute Deviation of the hold one out KL-Divergence.}\n  \\scalebox{0.92}{\n    \\begin{tabular}{rrrr}\n    \\toprule\n    Handle & Description & REGEX age & Notes \\\\\n    \\midrule\n    RIAMOpera & Opera at the Royal Irish ... Presenting: Ormindo Jan 11... & 11 & An Irish Opera \\\\\n    TiaKeough13 & My name Tia I'm 13 years old \u00e2\u0080\u00a6. & 13 & Hacked account \\\\\n    39yearoldvirgin & I'm 39 years old... if you're a woman, I want to meet you. & 39    & Probably not 39 \\\\\n    50Plushealths & Retired insurance Agent After 40 years of Services. & retired & Using reciprocation software \\\\\n    MrKRudd & Former PM of Australia... Proud granddad of Josie \\& McLean... & grandparent & Outlier. Former AUS PM \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  \\label{tab:bad_ids}\n\\end{table*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Experimental Evaluation}\n\\label{sec:experiments}\n\n\n\\begin{table*}[htbp]\n  \\centering\n  \\caption{Most discriminative features for all age categories.}\n  \\scalebox{0.9}{\n    \\begin{tabular}{llrrrrrrrrrr}\n    \\toprule\n    \\textbf{twitter\\_handle} & \\textbf{description} & \\textbf{under 12} & \\textbf{12--13} & \\textbf{14--15} & \\textbf{16--17} & \\textbf{18--24} & \\textbf{25--34} & \\textbf{35--44} & \\textbf{45--54} & \\textbf{55--64} & \\textbf{65+} \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{Under 12-year olds}}}}}\\\\\n    \\textbf{RosannaPansino} & vlogger & {\\bf{\\textcolor{blue}{{0.40}}}}  & 0.22  & 0.15  & 0.09  & 0.07  & 0.02  & 0.01  & 0.01  & 0.01  & 0.02 \\\\\n    \\textbf{AntVenom} & minecraft gamer & {\\bf{\\textcolor{blue}{{0.40}}}}  & 0.25  & 0.15  & 0.09  & 0.06  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{Bajan\\_Canadian} & internet personality & {\\bf{\\textcolor{blue}{{0.37}}}}  & 0.25  & 0.17  & 0.10  & 0.06  & 0.02  & 0.00  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{shaycarl} & vlogger & {\\bf{\\textcolor{blue}{{0.36}}}}  & 0.20  & 0.14  & 0.10  & 0.07  & 0.04  & 0.02  & 0.02  & 0.02  & 0.02 \\\\\n    \\textbf{InTheLittleWood} & gaming commentator & {\\bf{\\textcolor{blue}{{0.34}}}}  & 0.23  & 0.16  & 0.11  & 0.08  & 0.02  & 0.01  & 0.01  & 0.01  & 0.02 \\\\\n    \\midrule\n   \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{12--13 year olds}}}}}\\\\\n    \\textbf{ivandorschner} & child TV presenter & 0.18  & {\\bf{\\textcolor{blue}{{0.27}}}}  & 0.20  & 0.11  & 0.09  & 0.03  & 0.03  & 0.02  & 0.03  & 0.04 \\\\\n    \\textbf{Vikkstar123} & youtuber & 0.29  & {\\bf{\\textcolor{blue}{{0.26}}}}  & 0.20  & 0.14  & 0.07  & 0.02  & 0.01  & 0.01  & 0.01  & 0.02 \\\\\n    \\textbf{PeytonList} & child actress & 0.29  & {\\bf{\\textcolor{blue}{{0.25}}}}  & 0.20  & 0.14  & 0.07  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{G\\_Hannelius} & child actress & 0.31  & {\\bf{\\textcolor{blue}{{0.25}}}}  & 0.18  & 0.13  & 0.07  & 0.02  & 0.02  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{Cimorelliband} & girlband & 0.20  & {\\bf{\\textcolor{blue}{{0.25}}}}  & 0.23  & 0.17  & 0.09  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{14--15 year olds}}}}}\n    \\\\\n    \\textbf{therealsavannah} & child pop singer & 0.10  & 0.18  & {\\bf{\\textcolor{blue}{{0.27}}}}  & 0.21  & 0.12  & 0.02  & 0.01  & 0.03  & 0.03  & 0.03 \\\\\n    \\textbf{jessicajarrell} & child pop singer & 0.12  & 0.21  & {\\bf{\\textcolor{blue}{{0.26}}}}  & 0.24  & 0.10  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{TheDylanHolland} & child R\\&B singer & 0.12  & 0.22  & {\\bf{\\textcolor{blue}{{0.26}}}}  & 0.24  & 0.11  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{OfficialBirdy} & child singer & 0.10  & 0.17  & {\\bf{\\textcolor{blue}{{0.26}}}}  & 0.24  & 0.13  & 0.04  & 0.01  & 0.02  & 0.02  & 0.02 \\\\\n    \\textbf{officialjman} & child singer & 0.10  & 0.18  & {\\bf{\\textcolor{blue}{{0.26}}}}  & 0.28  & 0.13  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{16--17 year olds}}}}}\\\\\n    \\textbf{TannerPatrick} & singer & 0.05  & 0.13  & 0.25  & {\\bf{\\textcolor{blue}{{0.30}}}}  & 0.18  & 0.03  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{TheWordAlive} & metalcore band & 0.04  & 0.11  & 0.19  & {\\bf{\\textcolor{blue}{{0.29}}}}  & 0.22  & 0.09  & 0.02  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{MitchLuckerSS} & deathcore singer & 0.05  & 0.14  & 0.23  &{\\bf{\\textcolor{blue}{{0.29}}}}  & 0.20  & 0.04  & 0.01  & 0.01  & 0.01  & 0.02 \\\\\n    \\textbf{metrostation} & electronic band & 0.03  & 0.07  & 0.15  &{\\bf{\\textcolor{blue}{{0.29}}}}  & 0.18  & 0.10  & 0.04  & 0.06  & 0.06  & 0.03 \\\\\n    \\textbf{BreatheCarolina} & electronic band & 0.06  & 0.15  & 0.22  & {\\bf{\\textcolor{blue}{{0.29}}}}  & 0.19  & 0.06  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{18--24 year olds}}}}}\\\\\n    \\textbf{wecameasromans} & metalcore band & 0.05  & 0.13  & 0.22  & 0.28  & {\\bf{\\textcolor{blue}{{0.21}}}}  & 0.06  & 0.01  & 0.01  & 0.01  & 0.01 \\\\\n    \\textbf{Sum41} & rock band & 0.07  & 0.11  & 0.18  & 0.24  &{\\bf{\\textcolor{blue}{{0.21}}}}  & 0.09  & 0.02  & 0.02  & 0.03  & 0.03 \\\\\n    \\textbf{hopsin} & rapper & 0.04  & 0.09  & 0.13  & 0.19  & {\\bf{\\textcolor{blue}{{0.20}}}}  & 0.09  & 0.09  & 0.06  & 0.05  & 0.07 \\\\\n    \\textbf{Diablo} & computer game & 0.03  & 0.06  & 0.09  & 0.13  & {\\bf{\\textcolor{blue}{{0.20}}}}  & 0.17  & 0.09  & 0.05  & 0.06  & 0.12 \\\\\n    \\textbf{paparoach} & rock band & 0.04  & 0.09  & 0.14  & 0.19  & {\\bf{\\textcolor{blue}{{0.20}}}}  & 0.12  & 0.07  & 0.06  & 0.06  & 0.04 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{25--34 year olds}}}}}\\\\\n    \\textbf{icp} & hip hop duo & 0.02  & 0.04  & 0.05  & 0.09  & 0.19  & {\\bf{\\textcolor{blue}{{0.37}}}}  & 0.09  & 0.04  & 0.05  & 0.05 \\\\\n    \\textbf{kevinrichardson} & boyband member & 0.02  & 0.03  & 0.05  & 0.09  & 0.16  & {\\bf{\\textcolor{blue}{{0.35}}}}  & 0.12  & 0.07  & 0.06  & 0.04 \\\\\n    \\textbf{skulleeroz} & boyband member & 0.02  & 0.04  & 0.06  & 0.09  & 0.16  & {\\bf{\\textcolor{blue}{{0.33}}}}  & 0.12  & 0.07  & 0.06  & 0.05 \\\\\n    \\textbf{LeeEvansNews} & comedien & 0.02  & 0.03  & 0.06  & 0.07  & 0.17  & {\\bf{\\textcolor{blue}{{0.32}}}}  & 0.09  & 0.08  & 0.09  & 0.09 \\\\\n    \\textbf{miko\\_lee} & adult actress & 0.04  & 0.03  & 0.03  & 0.05  & 0.17  & {\\bf{\\textcolor{blue}{{0.31}}}}  & 0.08  & 0.07  & 0.08  & 0.14 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{35--44 year olds}}}}}\\\\\n    \\textbf{djspooky} & hip hop artist & 0.01  & 0.02  & 0.03  & 0.02  & 0.04  & 0.15  & {\\bf{\\textcolor{blue}{{0.45}}}}  & 0.14  & 0.06  & 0.08 \\\\\n    \\textbf{Mr\\_Mike\\_Jones} & rapper & 0.01  & 0.01  & 0.01  & 0.01  & 0.03  & 0.14  & {\\bf{\\textcolor{blue}{{0.44}}}}  & 0.16  & 0.09  & 0.10 \\\\\n    \\textbf{HISTORYTV18} & history  TV channel & 0.02  & 0.03  & 0.03  & 0.05  & 0.09  & 0.14  & {\\bf{\\textcolor{blue}{{0.36}}}}  & 0.10  & 0.06  & 0.13 \\\\\n    \\textbf{TopDawgEnt} & record label & 0.03  & 0.07  & 0.05  & 0.07  & 0.11  & 0.11  & {\\bf{\\textcolor{blue}{{0.36}}}}  & 0.09  & 0.03  & 0.07 \\\\\n    \\textbf{DannySwift} & boxer & 0.02  & 0.03  & 0.04  & 0.07  & 0.06  & 0.09  & {\\bf{\\textcolor{blue}{{0.33}}}}  & 0.12  & 0.08  & 0.16 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{45--54 and 55--64-year olds}}}} (identical most-discriminant features)}\\\\\n    \\textbf{JohnBevere} & evangelist & 0.00  & 0.00  & 0.00  & 0.00  & 0.01  & 0.01  & 0.07  & {\\bf{\\textcolor{blue}{{0.36}}}}  & {\\bf{\\textcolor{blue}{{0.39}}}}  & 0.15 \\\\\n    \\textbf{edstetzer} & evangelist & 0.00  & 0.00  & 0.00  & 0.00  & 0.00  & 0.01  & 0.07  & {\\bf{\\textcolor{blue}{{0.36}}}}  & {\\bf{\\textcolor{blue}{{0.39}}}}  & 0.16 \\\\\n    \\textbf{ChristineCaine} & evangelist & 0.00  & 0.00  & 0.01  & 0.00  & 0.01  & 0.01  & 0.07  & {\\bf{\\textcolor{blue}{{0.36}}}}  & {\\bf{\\textcolor{blue}{{0.38}}}}  & 0.15 \\\\\n    \\textbf{womenoffaith} & faith group & 0.00  & 0.00  & 0.00  & 0.00  & 0.00  & 0.02  & 0.08  & {\\bf{\\textcolor{blue}{{0.36}}}}  & {\\bf{\\textcolor{blue}{{0.38}}}}  & 0.16 \\\\\n    \\textbf{RELEVANT} & faith magazine & 0.00  & 0.01  & 0.00  & 0.01  & 0.01  & 0.01  & 0.07  & {\\bf{\\textcolor{blue}{{0.35}}}}  & {\\bf{\\textcolor{blue}{{0.38}}}}  & 0.17 \\\\\n    \\midrule\n    \\multicolumn{6}{l}{{\\bf{\\textcolor{blue}{{People over 65}}}}}\\\\\n    \\textbf{afneil} & political journalist & 0.00  & 0.00  & 0.01  & 0.01  & 0.02  & 0.02  & 0.04  & 0.17  & 0.25  & {\\bf{\\textcolor{blue}{{0.48}}}} \\\\\n    \\textbf{Chris\\_Boardman} & retired cyclist & 0.01  & 0.01  & 0.01  & 0.02  & 0.01  & 0.01  & 0.04  & 0.17  & 0.25  & {\\bf{\\textcolor{blue}{{0.47}}}} \\\\\n    \\textbf{SkySportsGolf} & golf TV channel & 0.01  & 0.02  & 0.02  & 0.02  & 0.03  & 0.01  & 0.04  & 0.16  & 0.22  & {\\bf{\\textcolor{blue}{{0.46}}}} \\\\\n    \\textbf{IamAustinHealey} & retired rugby player & 0.04  & 0.02  & 0.01  & 0.01  & 0.01  & 0.01  & 0.04  & 0.17  & 0.25  & {\\bf{\\textcolor{blue}{{0.45}}}} \\\\\n    \\textbf{anthonyfjoshua} & boxer & 0.02  & 0.03  & 0.03  & 0.04  & 0.09  & 0.06  & 0.03  & 0.08  & 0.15  & {\\bf{\\textcolor{blue}{{0.45}}}}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  \\label{tab:all age discriminant feature table}\n\\end{table*}\n\n\nWe report the performance of our ten category age model using the directly labelled age data described in Table \\ref{tab:counts}. For comparison with \\cite{nguyen2013old} we also report F1 scores, precision and recall for a three-category model with categories under 18, 18--45 and over 45. In this second set of experiments, we make use of the data labelled as grandparents and retirees described in Table \\ref{tab:older_people}. In all experiments, we trained our model using 90\\% of the data and evaluated performance using the remaining 10 \\% of the data.\n\n\n\n\nIn Table~\\ref{tab:all age discriminant feature table}, we report the five features with the highest posterior age values of $P(A  | X_i = 1)$ for each age category $a$ of $A$\\footnote{To aid with description generation we restricted the table entries to English language accounts with more than 100,000 followers}. The account descriptions are taken from the first line of the relevant Wikipedia page. The youngest Twitter users are characterised by an interest in internet celebrities and computer games players. Genres of music are important in differentiating all age groups from 12--45. 25--34 year olds are in part marked by entities that saw greater prominence in the past. This group is also distinguished by an interest in pornographic actors. Age categories 45--54 and 55--64 have the same top five and are differentiated by their interest in religious topics. Users over the age of 65 are identifiable through an interest in certain (retirement) sports and politics.\n\n\nFigure \\ref{fig:cm_age_labeled} shows a confusion matrix for classification using just the data of Table \\ref{tab:counts}. In the case of perfect predictions, the confusion matrix is diagonal. We can identify the diagonal structure in Figure~\\ref{fig:cm_age_labeled}, but the model based on this data set has huge difficulties to make predictions for ages greater then 45, and classifies these accounts largely in the range 25--44. This is because higher age classes have low counts and mean features (See Table \\ref{tab:counts}) and so many features have no support in these age classes.\n\n\\begin{table*}[tb]\n  \\centering\n  \\caption{Statistics for age prediction using only explicitly labelled data.}\n    \\begin{tabular}{rrrrrrrrrrr}\n    \\toprule\n    \\textbf{metric} & \\textbf{under 12} & \\textbf{12--13} & \\textbf{14--15} & \\textbf{16--17} & \\textbf{18--24} & \\textbf{25--34} & \\textbf{35--44} & \\textbf{45--54} & \\textbf{55--64} & \\textbf{65+}\\\\\n    \\midrule\n    \\textbf{support} & 672   & 1795  & 2669  & 2125  & 2752  & 776   & 254   & 140   & 75    & 47 \\\\\n    \\textbf{recall} & 0.20 & 0.18 & 0.39 & 0.23 & 0.37 & 0.45 & 0.27 & 0.01 & 0     & 0 \\\\\n    \\textbf{precision} & 0.25 & 0.35 & 0.36 & 0.29 & 0.36 & 0.19 & 0.11 & 0.10   & 0     & 0 \\\\\n    \\textbf{F1} & 0.22 & 0.24 & 0.38 & 0.26 & 0.37 & 0.27 & 0.16 & 0.01 & 0     & 0 \\\\\n    \\textbf{micro F1} & 0.30 &       &       &       &       &       &       &       &       &  \\\\\n    \\textbf{macro F1} & 0.21 &       &       &       &       &       &       &       &       &  \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:age_labeled_stats}\n\\end{table*}\nThis is supported by the corresponding performance statistics for each class, which are presented in Table  \\ref{tab:age_labeled_stats}, together with aggregates. The table shows that the predictive performance rapidly drops off for ages greater than 35. \n\n\n\\begin{figure}[tb]\n  \\centering\n    \\includegraphics[width=\\hsize]{learning_curve_Age.pdf}\n    \\caption{Learning curves showing the classification accuracy on the training and test set as the size of the training set is increased for a three-class model. The chart indicates that predictive accuracy on unseen data could benefit from additional training data. Error bars are one standard deviation on 5-fold cross-validation\n}\n\\label{fig:lc}\n\\end{figure}\nFigure \\ref{fig:lc} shows a learning curve used to estimate the extent our model could be improved by adding additional data. The horizontal axis shows the amount of data used for training. The remaining data is used to test the model. The large gap between the classification performance of the training data and the held out test data, combined with the upward trajectory of the test classification curve, indicates that our model would benefit from additional data.\n\n\n\n\n\n\n\n\nTo improve the predictive performance we added the data for grandparents and retired people described in Table \\ref{tab:older_people}. Retired people were all added to the 65+ category. Grandparents were randomly allocated to either 45--54, 55--64 or 65+. Figure \\ref{fig:cm_all} shows the confusion matrix using the enhanced data set. Predictive performance is greatly improved in the higher age categories, and the weight around the main diagonal of the matrix is improved for all categories. \n\n\n\\begin{table*}[tb]\n  \\centering\n  \\caption{Statistics for age prediction using explicitly labelled data and people described as retired or grandparents.}\n    \\begin{tabular}{rrrrrrrrrrr}\n    \\toprule\n    \\textbf{metric} & \\textbf{under 12} &  \\textbf{12--13} & \\textbf{14--15} & \\textbf{16--17} & \\textbf{18--24} & \\textbf{25--34} & \\textbf{35--44} & \\textbf{45--54} & \\textbf{55--64} & \\textbf{65+} \\\\\n    \\midrule\n    \\textbf{support} & 651   & 1731  & 2678  & 2036  & 2670  & 776   & 230   & 5058  & 5145  & 20487 \\\\\n    \\textbf{recall} & 0.19 & 0.20 & 0.38 & 0.23 & 0.33  & 0.25 & 0.18 & 0.32 & 0.41 & 0.30 \\\\\n    \\textbf{precision} & 0.22 & 0.33 & 0.36 & 0.24 & 0.31 & 0.15 & 0.07 & 0.14 & 0.19 & 0.79 \\\\\n    \\textbf{F1} & 0.21 & 0.27 & 0.37 & 0.24 & 0.32 & 0.19 & 0.10 & 0.20 & 0.26 & 0.43 \\\\\n    \\textbf{micro F1} & 0.31 &       &       &       &       &       &       &       &       &  \\\\\n    \\textbf{macro F1} & 0.25 &       &       &       &       &       &       &       &       &  \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:all_stats}\n\\end{table*}\nTable \\ref{tab:all_stats} shows the performance statistics for this augmented data set. F1 scores for the top three age classes are dramatically improved in comparison to Table \\ref{tab:age_labeled_stats}. As a result the macro F1 is 25 \\% higher, however the micro-F1 score is only marginally improved due to small degradations in other categories.\n\n\nFor comparison with the state-of-the-art work of \\cite{nguyen2013old} based on linguistic we consider the performance of our model as a three-class classifier using the following age bands: under 18, 18--44 and 45+. This choice allows the allocation of grandparents to a single class and has class boundaries that align with the ten-class model. \n\n\\begin{figure}\n  \\centering\n    \\includegraphics[width={0.3\\textwidth}]{confusion_matrix_all_3_class.pdf}\n    \\caption{Three-class confusion matrix using all explicitly labelled data and people described as grandparents of retired.}\n\\label{fig:cm_3_class}\n\\end{figure}\nFigure \\ref{fig:cm_3_class} shows the three-class confusion matrix. The mode for each row is on the main diagonal and the main cause of error is a misclassification of accounts in class 18--45 as under 18. \n\n\n\\begin{table}[htb]\n  \\centering\n  \\caption{Performance statistics for a three-class age model.}\n    \\begin{tabular}{rrrr}\n    \\toprule\n    \\textbf{metric} & \\textbf{under 18} & \\textbf{18--44} & \\textbf{over 45} \\\\\n    \\midrule\n    \\textbf{support} & 7096  & 3676  & 30690 \\\\\n    \\textbf{recall} & 0.68 & 0.50 & 0.95 \\\\\n    \\textbf{precision} & 0.76 & 0.39 & 0.96 \\\\\n    \\textbf{F1} & 0.72 & 0.44 & 0.96 \\\\\n    \\textbf{micro F1} & 0.86 &       &  \\\\\n    \\textbf{macro F1} & 0.58 &       &  \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:3_class_stats}\n\\end{table}\nThe corresponding performance statistics are shown in Table \\ref{tab:3_class_stats} are very good with a micro F1 score of 0.86; the same value was achieved by \\cite{nguyen2013old}. However, the major advantage of our model is that we have applied it to the entire Twitter graph, as opposed to being limited to a biased sample of Dutch Twitter users with a sufficient number of Tweets. \n\n\n\\begin{figure}[tb]\n\t\\centering\n    \t\\includegraphics[width=\\hsize]{multiclass_roc_3_class.pdf}\n        \\caption{Receiver operator characteristics for three class age detection (0 = under 18, 1 = 18--45, 2 = 45+). The dashed line indicates random performance.}\n\\label{fig:multiclass_roc_3_class}\n\\end{figure}\n\\begin{figure}[tb]\n\t\\centering\n    \t\\includegraphics[width=\\hsize]{1.pdf}\n        \\caption{Learning curves showing the classification accuracy on the training and test set as the size of the training set is increased for a three class model. Error bars are one standard deviation on 5-fold cross-validation}\n\\label{fig:lc_3_class}\n\\end{figure}\n\nFigure \\ref{fig:multiclass_roc_3_class} shows the areas under the receiver-operator characteristics (ROC) curves for the three-class model. The curves are generated by measuring the true positive and false positive rates for each class for a range of classification thresholds. A perfect classifier has an area under the curve (AUC) equal to one, while a completely random classifier follows the dashed line with an $AUC = 0.5$. Performance is excellent for classes under 18 and over 45, but weaker for 18--45 where training data was more limited. The three-class learning curve (Figure \\ref{fig:lc_3_class}) indicates that performance could still be improved by adding more labelled data. A promising way to do this would be to manually label accounts self-identifying as parents, however we lacked the resources to achieve this at meaningful scale.\n\n\n\\begin{figure}[tb]\n  \\centering\n    \\includegraphics[width=\\hsize]{results_age_bar.pdf}\n    \\caption{Predicted age proportions across the whole of Twitter compared to expectations based on survey data.}\n\\label{fig:result_bar}\n\\end{figure}\n\nWe have evaluated the performance of a 10-class age model trained on Twitter description data, which performs well with implicitly labelled age data. Our model performs as well as the current state of the art on a three-class problem for inferring the age of Twitter users without being limited to specific linguistic features. Finally, we have successfully applied our model to the entire Twitter network. Figure \\ref{fig:result_bar} shows aggregate classification results for 700 million Twitter accounts compared with expectations based on survey data \\citep{Duggan2013}. The figure shows that our model predicts that over 50\\% of Twitter users are between 18 and 35. Much of the bias of the original training set (Figure \\ref{fig:input_bar}) has been removed, largely due to Bayesian inference. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\\section{Conclusion and Future Work}\n\nWe have presented the first model that can classify the age of most Twitter users. Our work allows future researchers to build more complete models of social processes as measured on Twitter and to generalise phenomena on Twitter to the real world. The key idea behind our model is that a user's age can be predicted from what/whom they follow. Our Bayesian model delivers state-of-the-art performance equal to that achieved by the linguistic model of \\cite{nguyen2013old}, but with much broader applicability and more general assumptions. We are able to achieve these results using only the biased training data that is available natively within Twitter and so do not rely on manual labellers or an external application to produce training data.\n\nExtensions include the combination of our results with linguistic models to attain higher fidelity estimates for particular locales and relaxing the independence assumptions between features. It is computationally intractable to explicitly model all dependencies, but algorithms, such as probabilistic PCA, are able to discover the most significant correlations efficiently \\cite{Tipping1999}.\n\n\n\n\\section{Acknowledgements}\nThis work was partly funded through a Royal Commission for the Exhibition of 1851 Industrial Fellowship.\n\n\\bibliographystyle{aaai}\n\n\n\\begin{thebibliography}{}\n\n\\bibitem[\\protect\\citeauthoryear{Bishop}{2006}]{Bishop2006}\nBishop, C.~M.\n\\newblock 2006.\n\\newblock {\\em Pattern Recognition and Machine Learning}.\n\\newblock Springer Verlag.\n\n\\bibitem[\\protect\\citeauthoryear{Duggan and Brenner}{2013}]{Duggan2013}\nDuggan, M., and Brenner, J.\n\\newblock 2013.\n\\newblock {\\em The Demographics of Social Media Users-2012}.\n\n\\bibitem[\\protect\\citeauthoryear{Koller and Friedman}{2009}]{Koller2009}\nKoller, D., and Friedman, N.\n\\newblock 2009.\n\\newblock {\\em Probabilistic Graphical Models: Principles and Techniques}.\n\\newblock MIT Press.\n\n\\bibitem[\\protect\\citeauthoryear{Kosinski, Stillwell, and\n  Graepel}{2013}]{Kosinski2013}\nKosinski, M.; Stillwell, D.; and Graepel, T.\n\\newblock 2013.\n\\newblock Private traits and attributes are predictable from digital records of\n  human behavior.\n\\newblock {\\em Proceedings of the National Academy of Sciences of the United\n  States of America} 110(15):5802--5.\n\n\\bibitem[\\protect\\citeauthoryear{McPherson, Smith-Lovin, and\n  Cook}{2001}]{McPherson2001}\nMcPherson, M.; Smith-Lovin, L.; and Cook, J.~M.\n\\newblock 2001.\n\\newblock Birds of a feather: Homophily in social networks.\n\\newblock {\\em Annual Review of Sociology} 27(1):415--444.\n\n\\bibitem[\\protect\\citeauthoryear{Meder}{2011}]{DongNguyen142014}\nMeder, T.\n\\newblock 2011.\n\\newblock Why gender and age prediction from tweets is hard: Lessons from a\n  crowdsourcing experiment.\n\\newblock In {\\em International Conference on Computational Linguistics}.\n\n\\bibitem[\\protect\\citeauthoryear{Mislove, Lehmann and Ahn}{2011}]{Mislove2011}\nMislove, A.; Lehmann, S.; Ahn, Y.Y.\n\\newblock 2011.\n\\newblock Understanding the Demographics of Twitter Users.\n\\newblock In {\\em International Conference on Computational Linguistics}.\n\n\\bibitem[\\protect\\citeauthoryear{Nguyen \\bgroup et al\\mbox.\\egroup\n  }{2013}]{nguyen2013old}\nNguyen, D.; Gravel, R.; Trieschnigg, D.; and Meder, T.\n\\newblock 2013.\n\\newblock ``{H}ow old do you think {I} am?'' {A} study of language and\n  age in {T}witter.\n\\newblock In {\\em International Conference on Weblogs and Social Media},\n  439--448.\n  \n \\bibitem[\\protect\\citeauthoryear{Oktay, Firat, and Ertem}{2014}]{Oktay2014}\nOktay, H.; Firat, A.; and Ertem, Z.\n\\newblock 2014.\n\\newblock Demographic breakdown of {T}witter users: {A}n analysis based on\n  names.\n  \\newblock {\\em Conference on Big Data, Socialcom, Cybersecurity}\n\n\\bibitem[\\protect\\citeauthoryear{Tipping and Bishop}{1999}]{Tipping1999}\nTipping, M., and Bishop, C.\n\\newblock 1999.\n\\newblock Probabilistic principal component analysis.\n\\newblock {\\em Journal of the Royal Statistical Society}.\n\n\\bibitem[\\protect\\citeauthoryear{Wysocki and Zabierowski}{2011}]{Wysocki2011}\nWysocki, R., and Zabierowski, W.\n\\newblock 2011.\n\\newblock Twisted framework on game server example.\n\\newblock {\\em International Conference on Experience of Designing and Application of CAD Systems in Microelectronics}.\n\n\\end{thebibliography}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 25185, "prevtext": "\n\n\\begin{figure}[tb]\n  \\centering\n    \\includegraphics[width=0.5\\hsize]{age_pgm3.pdf}\n    \\caption{The probabilistic graphical model for the age estimation problem. Circles are random variables, with shading when they are observed. Small dots indicate model parameters and the plates denote replication.}\n\\label{fig:pgm}\n\\end{figure}\n\n\\begin{figure*}[htb]\n  \\centering\n  \\begin{subfigure}[t]{0.45\\hsize}\n    \\includegraphics[width=\\hsize]{confusion_matrix_age_labeled.pdf}\n    \\caption{Confusion matrix for a 10 \\% held out test set. Data included only accounts with explicit age labels. Very few predictions are made in older categories where labelled data is limited.}\n    \\label{fig:cm_age_labeled}\n   \\end{subfigure}\n   \\hspace{5mm}\n   \\begin{subfigure}[t]{0.45\\hsize}\n   \\includegraphics[width={\\hsize}]{confusion_matrix_all.pdf}\n    \\caption{Confusion matrix using retired people and grandparents.}\n\t\\label{fig:cm_all}\n   \\end{subfigure}\n    \\caption{Confusion matrices for a 10 \\% held out test set. (\\subref{fig:cm_age_labeled}) Data included only accounts with explicit age labels. Very few predictions are made in older categories where labelled data is limited. (\\subref{fig:cm_all}) By including data from retired people and grandparents, the predictions in the older age categories are improved substantially.}\n\\label{fig:cm_age_labeled_and_all}\n\\end{figure*}\n\nWe seek hyper-parameters $b,c$ of the prior $p(\\mu)$ that smooth out noisy observations of less popular feature accounts, but which do not have a large effect when ample data is available. To achieve this we set $c_i$ to be constant across all features (hence dropping the subscript) and proportional to the total number of observations in each age class. We then set $b_i$ so that $\\mathbb{E}[\\mu_i] = \\tfrac{n_i}{N} = \\tfrac{b_i}{b_i+c}$. This generates a prior that assumes that the a-priori probability of following an account is constant across age classes and varies in proportion to the number of followers across features.\n\n\n\\subsection{Data Cleaning}\n\nApplying REGEX matches to free-text fields inevitably leads to some false matches owing to unanticipated character combinations when working with large data sets. In addition, many Twitter accounts, while correctly labelled, may not represent the interests of human beings. This can occur when accounts are controlled by machines (bots), accounts are set up to look authentic to distribute spam (spam accounts) or account passwords are hacked in order to sell authentic looking followers. \n\nTo detect and remove spurious labelled data points we looked for accounts that had a large impact on the likelihood $P(\\mathbf{X}|\\mathbf{A})$. The Kullback-Leibler divergence (KL divergence) is the most commonly used to compare two probability distributions. We calculated an anomalousness score for the $i^{th}$ data point \n\n\n", "index": 11, "text": "\\begin{align}\nS_i = KL(P||P_{\\setminus i}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{i}=KL(P||P_{\\setminus i}),\" display=\"inline\"><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mi>K</mi><mi>L</mi><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><msub><mi>P</mi><mrow><mo>\u2216</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}]