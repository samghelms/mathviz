[{"file": "1601.04580.tex", "nexttext": "\nwhere $d_{i,j}$ is the distance between units $i$ and $j$, and $\\alpha > 0$ is a parameter of the model. Large values of $\\alpha$ induce more self-links and therefore more fine-grained partitionings. Since we are concerned with temporal covariates, we define the distance function as, $d_{i,j} = e^{\\frac{-|t_i - t_j|}{a}}$, which induces exponential decay as the time gap between the instances increases.\n\nThe text of each document $i$ is represented by a vector of word counts ${\\boldsymbol{{w}}}_i$. The likelihood distribution is multinomial, conditioned on a parameter $\\theta$ associated with the partition to which document $i$ belongs. By placing a Dirichlet prior on $\\theta$, we can analytically integrate it out. Writing ${\\boldsymbol{{z}}}^{({\\boldsymbol{{c}}})}$ for the cluster membership induced by the follower graph ${\\boldsymbol{{c}}}$, we have:\\\\\n\n\\begin{small}\n\n", "itemtype": "equation", "pos": 5840, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nNews events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying these storylines would enable many high-impact applications, such as tracking public interest and opinion in ongoing crisis events. However, this requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach. We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP). To ensure efficient linear-time inference, we employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate our baseline and proposed models on the TREC Twitter Timeline Generation task and show strong results.\n\\end{abstract}\n\n\\section{Introduction}\nA long-standing goal for information retrieval and extraction is to identify and group textual references to ongoing events in the world~\\cite{allan2002topic}. Success on this task would have applications in personalized news portals~\\cite{gabrilovich2004newsjunkie}, intelligence analysis, disaster relief~\\cite{vieweg2010microblogging}, and in understanding the properties of the news cycle~\\cite{leskovec2009meme}. This task attains a new importance in the era of social media, where citizen journalists can document events as they unfold~\\cite{lotan2011arab}, but where repetition and untrustworthy information can make the reader's task especially challenging~\\cite{becker2011beyond,marcus2011twitinfo,petrovic2010streaming}.\n\nThe main technical challenge is in fusing information from two heterogeneous data sources: textual content and time. Two different documents about a single event might use very different vocabulary, particularly in sparse social media data such as microblogs; conversely, two different sporting events might be described in nearly identical language, with differences only in the numerical outcome. Temporal information is therefore critical: in the first case, to find the commonalities across disparate writing styles, and in the second case, to identify the differences. A further challenge is that unlike in standard document clustering tasks, the number of events in a data stream is typically unknown in advance. Finally, the demand for scalability is intense, since online text is produced at a high rate, and ideally event detection systems should process as much of it as possible.\n\nDue to these challenges, existing approaches for combining these modalities have been somewhat heuristic, relying on tunable parameters to control the tradeoff between textual and temporal similarity. In contrast, the Bayesian setting provides elegant formalisms for reasoning about latent structures (e.g., events) and their stochastically-generated realizations across text and time. In this paper, we describe one such model, based on the distance-dependent Chinese Restaurant Process (dd-CRP; Blei and Frazier, 2011)\\nocite{blei2011distance}. This model is distinguished by the neat separation that it draws between textual content, which is treated as a stochastic emission from an unknown Multinomial distribution, and time, which is modeled as a prior on graphs over documents, through an arbitrary \\emph{distance function}. Na\\\"ive approaches to application of the dd-CRP are insufficiently scalable, which may be why the model has been relatively underutilized in the NLP literature~\\cite{titov2011bayesian,kim2011accounting,sirts2014pos}. We describe improvements to Bayesian inference that make the application of this model feasible. We also show how to estimate each of the hyperparameters of the model from unlabeled data, enabling it to be applied to new datasets without extensive re-tuning. Finally, we present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014~\\cite{lin2014overview}.\n\\section{Model}\nThe basic task that we address is to group short text documents into an unknown number of storylines, based on their textual content and their temporal signature. The textual content may be extremely sparse --- the typical Tweet is on the order of ten words long --- so leveraging temporal information is crucial. Moreover, the temporal signal is multiscale: in the 24-hour news cycle, some storylines last for less than an hour, while others, like the disappearance of the Malaysian Airlines 370 plane in 2014, continue for weeks or months. In some cases, the temporal distribution of references to a storyline will be unimodal and well-described by a parametric model~\\cite{marcus2011twitinfo}; in other cases, it may be irregular, with bursts of activity followed by periods of silence~\\cite{he2007analyzing}. Finally, it will be crucial to produce an implementation that scales to large numbers of event mentions, and we would prefer to minimize the number of heuristically tunable parameters.\n\nThe distance-dependent Chinese Restaurant Process (dd-CRP) meets many of these criteria~\\cite{blei2011distance}. In this model, the key idea is that each instance $i$  ``follows'' another instance $c_i$ (where it is possible that $c_i = i$). We can compute a partitioning over instances by considering the connected components in the undirected version of the follower graph; these partitions correspond to ``tables'' in the conventional ``Chinese Restaurant'' analogy~\\cite{aldous1985exchangeability}, or to clusters. The advantage of this approach is that it is fundamentally non-parametric, and it introduces a clean separation between the textual data and the covariates: the text is generated by a distribution associated with the partition, while the covariates are associated with the following links, which are conditioned on a distance function. \n\nSpecifically, we have the following form for the prior, \n\n", "index": 1, "text": "\\begin{equation}\nPr(c_i = j) \\propto\n\\begin{cases} \n  f(d_{i,j}), & i \\neq j\\\\\n  \\alpha, & i = j,\\\\\n\\end{cases}\n\\label{eq:prior}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Pr(c_{i}=j)\\propto\\begin{cases}f(d_{i,j}),&amp;i\\neq j\\\\&#10;\\alpha,&amp;i=j,\\\\&#10;\\end{cases}\" display=\"block\"><mrow><mi>P</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>\u03b1</mi><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>i</mi><mo>=</mo><mi>j</mi></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04580.tex", "nexttext": "\n\\end{small}\n\nGiven a multinomial likelihood $P({\\boldsymbol{{w}}} \\mid \\theta)$ and a (symmetric) Dirichlet prior $P(\\theta \\mid \\eta)$, this integral has a closed-form solution as the Dirichlet-Multinomial distribution (also known as the multivariate Polya distribution).\n\nThe joint probability is therefore equal to the product of \\autoref{eq:prior} and \\autoref{eq:likelihood}, \n\n", "itemtype": "equation", "pos": 6865, "prevtext": "\nwhere $d_{i,j}$ is the distance between units $i$ and $j$, and $\\alpha > 0$ is a parameter of the model. Large values of $\\alpha$ induce more self-links and therefore more fine-grained partitionings. Since we are concerned with temporal covariates, we define the distance function as, $d_{i,j} = e^{\\frac{-|t_i - t_j|}{a}}$, which induces exponential decay as the time gap between the instances increases.\n\nThe text of each document $i$ is represented by a vector of word counts ${\\boldsymbol{{w}}}_i$. The likelihood distribution is multinomial, conditioned on a parameter $\\theta$ associated with the partition to which document $i$ belongs. By placing a Dirichlet prior on $\\theta$, we can analytically integrate it out. Writing ${\\boldsymbol{{z}}}^{({\\boldsymbol{{c}}})}$ for the cluster membership induced by the follower graph ${\\boldsymbol{{c}}}$, we have:\\\\\n\n\\begin{small}\n\n", "index": 3, "text": "\\begin{align}\nP({\\boldsymbol{{w}}} \\mid {\\boldsymbol{{c}}}; \\eta) = & \\prod_k P(\\{ {\\boldsymbol{{w}}}_i : {z_i^{({\\boldsymbol{{c}}})}} = k \\}; \\eta)\\\\\n = &\\prod_k \\int_\\theta P(\\{ {\\boldsymbol{{w}}}_i : {z_i^{({\\boldsymbol{{c}}})}} = k \\} \\mid \\theta) P(\\theta ; \\eta) d\\theta\n\\label{eq:likelihood}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P({\\boldsymbol{{w}}}\\mid{\\boldsymbol{{c}}};\\eta)=\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>\u2223</mo><mi>\ud835\udc84</mi><mo>;</mo><mi>\u03b7</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\prod_{k}P(\\{{\\boldsymbol{{w}}}_{i}:{z_{i}^{({\\boldsymbol{{c}}})}%&#10;}=k\\};\\eta)\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>k</mi></munder></mstyle><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>i</mi></msub><mo>:</mo><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mi>k</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo>;</mo><mi>\u03b7</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\prod_{k}\\int_{\\theta}P(\\{{\\boldsymbol{{w}}}_{i}:{z_{i}^{({%&#10;\\boldsymbol{{c}}})}}=k\\}\\mid\\theta)P(\\theta;\\eta)d\\theta\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>k</mi></munder></mstyle><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>\u03b8</mi></msub></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>i</mi></msub><mo>:</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mi>k</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2223</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo>;</mo><mi>\u03b7</mi><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><mi>\u03b8</mi></mrow></math>", "type": "latex"}, {"file": "1601.04580.tex", "nexttext": "\n\nThe proposed model has three parameters: $\\alpha$, $a$, and $\\eta$. Estimation for these parameters is described in \\autoref{sec:hyper}.\n\\section{Inference}\n\\label{sec:inference}\n\\newcite{blei2011distance} describe a collapsed Gibbs sampling algorithm for the dd-CRP. We first describe this procedure, focusing on its application to our specific instantiation of the model. We then describe an approximate online inference procedure that enables this model to scale to larger datasets.\n\n\\subsection{Offline inference}\n\\label{sec:offline}\nThe key sampling equation for the dd-CRP is the posterior likelihood,\n\n", "itemtype": "equation", "pos": 7558, "prevtext": "\n\\end{small}\n\nGiven a multinomial likelihood $P({\\boldsymbol{{w}}} \\mid \\theta)$ and a (symmetric) Dirichlet prior $P(\\theta \\mid \\eta)$, this integral has a closed-form solution as the Dirichlet-Multinomial distribution (also known as the multivariate Polya distribution).\n\nThe joint probability is therefore equal to the product of \\autoref{eq:prior} and \\autoref{eq:likelihood}, \n\n", "index": 5, "text": "\\begin{align}\n\\notag\nP({\\boldsymbol{{w}}}, {\\boldsymbol{{c}}} ; \\eta, \\alpha, a) = & \\prod_i P(c_i ; \\alpha, a) \\\\ \n&\\times \\prod_k P(\\{{\\boldsymbol{{w}}}_i : {z_i^{({\\boldsymbol{{c}}})}} = k\\}; \\eta).\n\\label{eq:joint}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P({\\boldsymbol{{w}}},{\\boldsymbol{{c}}};\\eta,\\alpha,a)=\" display=\"inline\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>,</mo><mi>\ud835\udc84</mi><mo>;</mo><mi>\u03b7</mi><mo>,</mo><mi>\u03b1</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\prod_{i}P(c_{i};\\alpha,a)\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>i</mi></munder></mstyle><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>;</mo><mi>\u03b1</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\times\\prod_{k}P(\\{{\\boldsymbol{{w}}}_{i}:{z_{i}^{({\\boldsymbol{{%&#10;c}}})}}=k\\};\\eta).\" display=\"inline\"><mrow><mo>\u00d7</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>k</mi></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>i</mi></msub><mo>:</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mi>k</mi><mo stretchy=\"false\">}</mo></mrow><mo>;</mo><mi>\u03b7</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04580.tex", "nexttext": "\nThe prior is defined in Equation~\\ref{eq:prior}. Let $\\ell$ represent the likelihood under the partitioning induced when the link $c_i$ is cut. Now, the likelihood term has two cases: in the first case, $j$ is already in the same connected component as $i$ (even after cutting the link $c_i$), so no components are merged by setting $c_i = j$. In this case, the likelihood $P({\\boldsymbol{{w}}} \\mid c_i = j)$ is exactly equal to $\\ell$. In the second case, setting $c_i = j$ causes two clusters to be merged. This gives the likelihood,\n\n", "itemtype": "equation", "pos": 8398, "prevtext": "\n\nThe proposed model has three parameters: $\\alpha$, $a$, and $\\eta$. Estimation for these parameters is described in \\autoref{sec:hyper}.\n\\section{Inference}\n\\label{sec:inference}\n\\newcite{blei2011distance} describe a collapsed Gibbs sampling algorithm for the dd-CRP. We first describe this procedure, focusing on its application to our specific instantiation of the model. We then describe an approximate online inference procedure that enables this model to scale to larger datasets.\n\n\\subsection{Offline inference}\n\\label{sec:offline}\nThe key sampling equation for the dd-CRP is the posterior likelihood,\n\n", "index": 7, "text": "\\begin{align*}\nP(c_i = j \\mid {\\boldsymbol{{c}}}_{-i}, {\\boldsymbol{{w}}}) \\propto & P(c_i = j) P({\\boldsymbol{{w}}} \\mid {\\boldsymbol{{c}}}).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(c_{i}=j\\mid{\\boldsymbol{{c}}}_{-i},{\\boldsymbol{{w}}})\\propto\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo>\u2223</mo><msub><mi>\ud835\udc84</mi><mrow><mo>-</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>\ud835\udc98</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle P(c_{i}=j)P({\\boldsymbol{{w}}}\\mid{\\boldsymbol{{c}}}).\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>\u2223</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04580.tex", "nexttext": "\nwhere the constant of proportionality is exactly equal to $\\ell$. Each of the terms in the likelihood ratio is a Dirichlet Compound Multinomial likelihood. This likelihood function is itself a ratio of gamma functions; by eliminating constant terms and exploiting the identity $\\Gamma(x+1) = x \\Gamma(x)$, we can reduce the number of Gamma function evaluations required to compute this ratio to the number of words which appear in \\emph{both} clusters ${z^{({\\boldsymbol{{c}}})}}_i$ and ${z^{({\\boldsymbol{{c}}})}}_j$. Words that occur in neither cluster can safely be ignored, and the gamma functions for words which occur in exactly one of the two clusters cancel in the numerator and denominator of the ratio. Note also that we only need compute the likelihood for $c_i$ with respect to each existing partition, not for every possible follower link. \n\n\\subsection{Online inference}\n\\label{sec:online}\nWhile we make every effort to accelerate the computation of individual Gibbs samples, the complexity of the basic algorithm is superlinear in the number of instances. This is due to the fact that each sample requires computing the probability of instance $i$ joining every possible cluster, while the number of clusters itself grows with the number of instances (this growth is logarithmic in the Chinese Restaurant Process). Scalability to the streaming setting therefore requires more aggressive optimizations.\n\nTo get back to linear time complexity, we employ a fixed-lag sampling procedure~\\cite{doucet2000sequential}. After receiving instance $i$, we perform Gibbs sampling only within the fixed window $[t_i - \\tau, t_i]$, leaving $c_j$ fixed if $t_j < t_i - \\tau$. This approximate sampling procedure implicitly changes the underlying model, because there is no possibility of linking $i$ to a later message $j$ if the time gap $t_j - t_i > \\tau$. Another possibility for optimization would be to explicitly change the model so that the prior $P(c_i = j) = 0$ if the time gap $|t_i - t_j|$ is beyond some threshold. This would constrain the complexity of drawing each sample to be constant in the length of the data. We do not consider this possibility here. \n\nSince we are only interested in obtaining a single storyline clustering --- rather than a full Bayesian distribution over clusterings --- we perform annealing for samples towards the end of the sampling window. Specifically, we set the temperature to $\\gamma = 2.0$  and exponentiate the sampling likelihood by the inverse temperature~\\cite{geman1984stochastic}. This has the effect of interpolating between probabilistically-correct Gibbs sampling and a hard coordinate-ascent procedure.\n\n\\subsection{Hyperparameter estimation}\n\\label{sec:hyper}\nThe model has three parameters to estimate: \n\\begin{itemize}\n\\setlength\\parskip{0pt}\n\\setlength\\parsep{0pt}\n\\setlength\\itemsep{0pt}\n\\item $\\alpha$, the concentration parameter of the dd-CRP\n\\item $a$, the offset of the distance function\n\\item $\\eta$, the scale of the symmetric Dirichlet prior.\n\\end{itemize}\nWe interleave maximization-based updates to these parameters with sampling, in a procedure inspired by Monte Carlo Expectation Maximization~\\cite{wei1990monte}. Specifically, we compute gradients on the likelihood $P({\\boldsymbol{{c}}})$ with respect to $\\alpha$ and $a$, and take gradient steps after every fixed number of samples. For the symmetric Dirichlet parameter $\\eta$, we employ the fixed-point estimation from~\\newcite{minka2000estimating} by setting the parameter to $\\eta = \\frac{(K-1)/2}{\\sum_k\\log p_k}$, where $K$ is the number of singletons and $p_k$ is the probability of choosing the $k^{th}$ word from the vocabulary, assuming a bag of words model.\n\n\\section{TREC Evaluation}\nTo test the efficacy of this approach, we evaluate on the Twitter Timeline Generation (TTG) task in the Microblog track of TREC 2014. It involves taking tweets based on a query $Q$ at time $T$ and returning a summary that captures relevant information. We perform the task on 55 queries with different timestamps and compare our results with 13 groups that submitted 50 runs for this task in 2014. \n\n\\subsection{Systems}\nWe consider the following systems:\n\\begin{description}\n\\item[Baseline] In this system, we replace the distance-dependent prior with a standard Dirichlet prior. The number of clusters is heuristically set to 20. Annealed Gibbs sampling is employed for inference.\n\\item[Offline inference] Here we employ the dd-CRP model with the offline inference procedure described in \\autoref{sec:offline}.\n\\item[Online inference] We employ the dd-CRP model with the  online inference procedure described in \\autoref{sec:online}.\n\\end{description}\n\nFor the online inference implementation, we set the size of window and number of iterations to five days and 500 respectively. For the baseline, the smoothing parameter for time was set to 0.5. These values were chosen through 10-fold cross validation.\n\n\\subsection{Results}\nTo measure the quality of the clusterings obtained by these models, we compare the average weighted and unweighted F-measures for 55 TREC topics, using the evaluation scripts from the TREC TTG task. Overall results are shown in \\autoref{tab:sim}. The \\textsc{online model} has the best weighted F1 score, outperforming the offline version of the same model, even though its inference procedure is an approximation to the \\textsc{offline model}. It may be that its approximate inference procedure discourages long-range linkages, thus placing a greater emphasis on the temporal dimension. Both models were trained over 500 iterations, and the \\textsc{online model} was 30\\% faster to train than the offline model. \n\nCompared to the other 2014 TREC TTG systems, our dd-CRP models are competitive. Both models outperform all but one of the fourteen submissions on the unweighted $F_1$ metric, and would have placed fourth on the weighted $F^{w}_1$ metric. Note that the TREC evaluation scores both clustering quality and retrieval. We use only the baseline retrieval model, while many of the competing systems use customized retrieval models that perform better. In combination with the better retrieval models used by some of these systems, the dd-CRP performance might be better.\n\n\\begin{table*}\n\\centering\n\n\n\\begin{tabular}{l l l l l l}\n    \\toprule\n    Model & Recall & Recall$^{w}$ & Precision & $F_1$ & $F_1^{w}$ \\\\ \\midrule\n\\textsc{baseline} & 0.14 & 0.27 & 0.33 & 0.20 & 0.30 \\\\\n\\textsc{offline} & 0.32 & 0.47 & 0.27 & 0.29 & 0.34 \\\\\n\\textsc{online} & 0.34 & 0.55 & 0.26 & 0.29 & 0.35 \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption{Performance of Models in the TREC 2014 TTG Task. Weighted recall and $F_1$ are indicated as Recall$^{w}$ and $F_1^w$.}\n\\label{tab:sim}\n\\end{table*}\n\n\\iffalse\n\\begin{table*}\n\\centering\n\n\n\\begin{tabular}{l l l l l}\n    \\toprule\n    Model & Recall & Recall$^{w}$ & Precision & $F_1^{w}$ \\\\ \\midrule\n    $\\textsc{Baseline}$ & 0.14 & 0.27 & 0.33 & .223 \\\\\n   $\\textsc{Offline}$ & 0.32 & 0.47 & 0.27 & .246 \\\\\n   $\\textsc{Online}$ & 0.34 & 0.55 & 0.26 & \\textbf{0.278} \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption{Performance of Models in the TREC 2014 TTG Task. Weighted recall and $F_1$ are indicated as Recall$^{w}$ and $F_1^w$.}\n\\label{tab:sim}\n\\end{table*}\n\\fi\n\n\\section{Related work}\nTopic tracking and first-story detection are very well-studied tasks; space does not permit a complete analysis of the related work, but see~\\cite{allan2002topic} for a summary of ``first generation'' research. More recent non-Bayesian approaches have focused on string overlap~\\cite{suen2013nifty}, submodular optimization~\\cite{shahaf2012trains}, and locality-sensitive hashing~\\cite{petrovic2010streaming}. Two advantages of Bayesian methods are: (1) the probabilistic framework enables the application of principled techniques for hyperparameter optimization and inference, and (2) the modularity of Bayesian graphical models enables the incorporation of more complex subcomponents for modeling text, as well as the integration of storyline detection into larger-scale models, such as personalized article recommendation.\n\nIn Bayesian storyline analysis, the seminal models are Topics-Over-Time~\\cite{wang2006topics}, which associates a parametric distribution over time with each topic~\\cite{ihler2006adaptive}, and the Dynamic Topic Model~\\cite{blei2006dynamic}, which models topic evolution as a linear dynamical system~\\cite{nallapati2007multiscale}. Later work by \\newcite{diao2012finding} offers a model for identifying ``bursty'' topics, with inference requiring dynamic programming. All of these approaches require the number of topics to be identified in advance. \\newcite{kim2011accounting} apply a distance-dependent Chinese Restaurant \\emph{Franchise} for time-sensitive topic modeling; they evaluate using predictive likelihood rather than comparing against ground truth, and do not consider online inference.\n\nThe Infinite Topic-Cluster model~\\cite{ahmed2011unified} is particularly related to our work. This model is non-parametric over the number of storylines, through the use of the recurrent Chinese Restaurant Process (rCRP). The model is substantially more complex than our approach, with Latent Dirichlet Allocation as a sub-component, and with specialized distributions of words and entities. Unlike the dd-CRP, the rCRP is Markovian in nature, so that the topic distribution at each point in time is conditioned on the previous epoch (or, at best, the previous $K$ epochs, with complexity of inference increasing with $K$). This Markovian assumption creates probabilistic dependencies between the topic assignment for a given document and the documents that follow in subsequent epochs, necessitating an inference procedure that combines sequential Monte Carlo and Metropolis Hastings, and a custom data structure; this inference procedure was complex enough to warrant a companion paper~\\cite{ahmed2011online}. The rCRP is also employed by Diao and Jiang (2013, 2014)\\nocite{diao2013unified,diao2014recurrent}, who again resort to a complex approximate sampling procedure to obtain efficient inference. In contrast, the dd-CRP makes no Markovian assumptions, and efficient inference is possible through relatively straightforward Gibbs sampling in a fixed window.\n\n\n\\section{Conclusion}\nWe present a simple non-parametric model for clustering short documents (such as tweets) into ``storylines'', which are conceptually coherent and temporally focused. Unlike previous non-parametric models of temporal text data, inference can be performed using Gibbs sampling, and can be made more efficient through a fixed-window approximation. Performance on the TREC 2014 TTG task demonstrates the capabilities of the model, outperforming six of the thirteen submitted systems. In future work, we will consider learning more flexible temporal distance functions, which could potentially represent temporal periodicity or parametric models of content popularity.\n\\section*{Acknowledgments} We thank the EMNLP reviewers for their feedback, which we have tried to incorporate in this report. This research was supported by an award from the National Institutes for Health (award number R01GM112697-01), and by Google, through a Focused Research Award for Computational Journalism.\n\n\\small\n\\bibliographystyle{acl}\n{\\bibliography{cite-strings,cites,cite-definitions}}\n\n\n", "itemtype": "equation", "pos": 9091, "prevtext": "\nThe prior is defined in Equation~\\ref{eq:prior}. Let $\\ell$ represent the likelihood under the partitioning induced when the link $c_i$ is cut. Now, the likelihood term has two cases: in the first case, $j$ is already in the same connected component as $i$ (even after cutting the link $c_i$), so no components are merged by setting $c_i = j$. In this case, the likelihood $P({\\boldsymbol{{w}}} \\mid c_i = j)$ is exactly equal to $\\ell$. In the second case, setting $c_i = j$ causes two clusters to be merged. This gives the likelihood,\n\n", "index": 9, "text": "\\begin{align*}\nP&({\\boldsymbol{{w}}} \\mid c_i = j, {\\boldsymbol{{c}}}_{-i}) \\\\\n& \\propto \\frac{P(\\{{\\boldsymbol{{w}}}_k : {z^{({\\boldsymbol{{c}}})}}_k = {z^{({\\boldsymbol{{c}}})}}_j \\vee {z^{({\\boldsymbol{{c}}})}}_k = {z^{({\\boldsymbol{{c}}})}}_i\\})}\n{P(\\{{\\boldsymbol{{w}}}_k : {z^{({\\boldsymbol{{c}}})}}_k = {z^{({\\boldsymbol{{c}}})}}_i\\}) \nP(\\{{\\boldsymbol{{w}}}_k : {z^{({\\boldsymbol{{c}}})}}_k = {z^{({\\boldsymbol{{c}}})}}_j\\}) },\n\\label{eq:likelihood}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P\" display=\"inline\"><mi>P</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\boldsymbol{{w}}}\\mid c_{i}=j,{\\boldsymbol{{c}}}_{-i})\" display=\"inline\"><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo>\u2223</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo>,</mo><msub><mi>\ud835\udc84</mi><mrow><mo>-</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\frac{P(\\{{\\boldsymbol{{w}}}_{k}:{z^{({\\boldsymbol{{c}}})}%&#10;}_{k}={z^{({\\boldsymbol{{c}}})}}_{j}\\vee{z^{({\\boldsymbol{{c}}})}}_{k}={z^{({%&#10;\\boldsymbol{{c}}})}}_{i}\\})}{P(\\{{\\boldsymbol{{w}}}_{k}:{z^{({\\boldsymbol{{c}}%&#10;})}}_{k}={z^{({\\boldsymbol{{c}}})}}_{i}\\})P(\\{{\\boldsymbol{{w}}}_{k}:{z^{({%&#10;\\boldsymbol{{c}}})}}_{k}={z^{({\\boldsymbol{{c}}})}}_{j}\\})},\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>k</mi></msub><mo>:</mo><mrow><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi><none/></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>j</mi><none/></mmultiscripts><mo>\u2228</mo><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi><none/></mmultiscripts></mrow><mo>=</mo><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>i</mi><none/></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>k</mi></msub><mo>:</mo><mrow><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi><none/></mmultiscripts><mo>=</mo><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>i</mi><none/></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc98</mi><mi>k</mi></msub><mo>:</mo><mrow><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>k</mi><none/></mmultiscripts><mo>=</mo><mmultiscripts><mi>z</mi><none/><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc84</mi><mo stretchy=\"false\">)</mo></mrow><mi>j</mi><none/></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}]