[{"file": "1601.06000.tex", "nexttext": "\n\nwhere $g_0({\\mathbf{z}}_i)=g_{00}+\\sum_{j=1}^dg_{0j}(z_{ij})$, with $g_{00}\\in\n\\mathcal{R}$.\n\nIt is assumed that $g_{0j}$ satisfy $E(g_{0j}(z_{ij}))=0$ for\nidentification purposes.\nLet $\\varepsilon_i=Y_i-Q_{Y_i|{\\mathbf{x}}_i,{\\mathbf{z}}_i}(\\tau)$, then $\\varepsilon_i$\nsatisfies $P(\\varepsilon_i\\leq0|{\\mathbf{x}}_i,{\\mathbf{z}}_i)=\\tau$ and we may also write\n$Y_i={\\mathbf{x}}_i'{\\bolds{\\beta}}_0+g_0({\\mathbf{z}}_i)+\\varepsilon_i$.\nIn the rest of the paper, we will drop the dependence on $\\tau$\nin the notation for simplicity.\n\nModeling conditional quantiles in high dimension is of significant\nimportance for several reasons.\nFirst, it is well recognized that high dimensional data are often heterogeneous.\nHow the covariate influence the center of the conditional distribution\ncan be very different from how they influence the tails.\nAs a result, focusing on the conditional mean function alone can be misleading.\nBy estimating conditional quantiles at different quantile levels,\nwe are able to gain a more complete picture of the relationship between\nthe covariates and the response variable.\nSecond, in the high dimensional setting,\nthe quantile regression framework also allows a more realistic\ninterpretation of the sparsity\nof the covariate effects, which we refer to as quantile-adaptive sparsity.\nThat is, we assume a small subset of covariates influence the conditional\ndistribution. However, when we estimate different conditional\nquantiles, we allow the\nsubsets of active covariates to be different [\\citet{quantUltraHigh};\n\\citet{heWang}].\nFurthermore, the conditional quantiles are often of direct interest to\nthe researchers.\nFor example, for the birth weight data we analyzed in Section~\\ref{sec5}, low\nbirth weight, which corresponds to the low tail of the conditional\ndistribution, is of direct interest to the doctors. Another advantage\nof quantile regression is that it is naturally robust to outlier\ncontamination associated with heavy-tailed errors. For high dimensional\ndata, identifying outliers can be difficult. The robustness of quantile\nregression provides a certain degree of protection.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear quantile regression with high dimensional covariates was\ninvestigated by\n\\citeauthor{B1} [(\\citeyear{B1}), Lasso penalty] and\nWang, Wu and Li [(\\citeyear{quantUltraHigh}), nonconvex penalty].\nThe partially linear additive structure we consider in this paper is\nuseful for incorporating nonlinearity\nin the model while circumventing the curse of dimensionality.\nWe are interested in the case\n$p_n$ is of a similar order of $n$ or much larger than $n$.\nFor applications in microarray data analysis,\nthe vector ${\\mathbf{x}}_i$ often contains the measurements on thousands of\ngenes, while the\nvector ${\\mathbf{z}}_i$ contains the measurements of clinical or environment\nvariables, such as\nage and weight. For example, in the birth weight example of Section~\\ref{sec5},\nmother's age is modeled nonparametrically\nas exploratory analysis reveals a possible nonlinear effect. In\ngeneral, model specification\ncan be challenging in high dimension; see Section~\\ref{sec7} for some\nfurther discussion.\n\n\n\n\n\n\n\nWe approximate the nonparametric components using B-spline basis\nfunctions, which are computationally convenient\nand often accurate. First, we study the asymptotic theory\nof estimating the model (\\ref{model}) when $p_n$ diverges.\nIn our setting, this corresponds to the oracle model, that is, the\none we obtain if we know which covariates are important in advance.\nThis is along the line of the work\nof \\citet{Welsh}, \\citet{BaiWu} and \\citet{HeShao}\nfor $M$-regression with diverging number of parameters and\npossibly nonsmooth objective functions, which, however,\nwere restricted to linear regression. \\citet{partMeanVaryCoef}\nderived the asymptotic theory of profile kernel estimator\nfor general semiparametric models with diverging number\nof parameter while assuming a smooth quasi-likelihood function.\nSecond, we propose a nonconvex\npenalized regression estimator when $p_n$ is of an exponential order of\n$n$ and\nthe model has a sparse structure. For a general class of nonsmooth\npenalty functions, including the popular SCAD [\\citet{fanLi}]\nand MCP [\\citet{Zhang}] penalty, we derive the oracle property of the\nproposed estimator under\nrelaxed conditions. An interesting finding is that\nsolving the nonconvex penalized estimator can be achieved via\nsolving a series of weighted quantile regression problems, which can\nbe conveniently implemented using existing software packages.\n\nDeriving the asymptotic properties of the penalized estimator is very\nchallenging as\nwe need to simultaneously deal with the nonsmooth loss function, the\nnonconvex penalty\nfunction, approximation of nonlinear functions and very high \\mbox{dimensionality}.\nTo tackle these challenges, we combine a recently developed\nconvex-differencing method with \nmodern empirical process techniques. The method relies on a\nrepresentation of the penalized loss function as the difference\nof two convex functions, which leads to a sufficient local\noptimality condition [\\citet{TaoAn97}, \\citet{quantUltraHigh}].\nEmpirical process techniques are\nintroduced to derive various error bounds associated with\nthe nonsmooth objective function which contains both high dimensional\nlinear covariates and approximations of nonlinear components.\nIt is worth pointing out that our approach is different from what was\nused in the\nrecent literature for studying the theory of\nhigh dimensional semiparametric mean regression and is able to\nconsiderably weaken\nthe conditions required in the literature. In particular, we do not\nneed moment\nconditions for the random error and allow it to depend on the covariates.\n\nExisting work on penalized semiparametric regression has been largely\nlimited to mean regression with\nfixed $p$;\nsee, for example, \\citet{Bunea}, \\citet{LiangLi}, \\citet{WX},\n\\citet{scadPartLinMean}, \\citet{KLZ} and\n\\citet{WLLC}.\nImportant progress in the high dimensional $p$ setting\nhas been recently made by\n\\citeauthor{annalsMeanParLin} [(\\citeyear{annalsMeanParLin}), still\nassumes $p<n$] for\npartially linear regression, \\citet{HHW} for\nadditive models,\n\\citeauthor{Li} [(\\citeyear{Li}), $p=o(n)$] for semivarying coefficient models,\namong others. When $p$ is fixed, the semiparametric quantile regression\nmodel was considered by\n\\citet{heShi96}, \\citet{heQuantLong}, \\citet{quantVaryModel},\namong others. \\citet{Tang} considered a two-step procedure for a\nnonparametric varying\ncoefficients quantile regression model with a diverging number of\nnonparametric functional coefficients.\nThey required two separate tuning parameters and quite complex design\nconditions.\n\nThe rest of this article is organized as follows. In Section~\\ref\n{sec2}, we\npresent the partially linear additive quantile regression model and\ndiscuss the properties of the oracle estimator.\nIn Section~\\ref{sec3}, we present a nonconvex penalized method for simultaneous\nvariable selection and estimation and derive its\noracle property. In Section~\\ref{sec4}, we assess the performance of the\nproposed penalized\nestimator via Monte Carlo simulations.\nWe analyze a birth weight data set while accounting for gene expression\nmeasurements in Section~\\ref{sec5}.\nIn Section~\\ref{sec6}, we consider an extension to simultaneous\nestimation and\nvariable selection at multiple quantiles.\nSection~\\ref{sec7} concludes the paper with a discussion of related issues.\n\n\nThe proofs are given in the \\hyperref[appe]{Appendix}.\nSome of the technical details and additional numerical results are\nprovided in online supplementary material [\\citet{Supp}].\n\n\n\\section{Partially linear additive quantile regression with diverging\nnumber of parameters}\\label{sec2}\nFor high dimensional inference, it is often assumed that the vector of\ncoefficients\n${\\bolds{\\beta}}_0=(\\beta_{01},\\beta_{02},\\ldots,\\beta_{0p_n})'$ \n\nin model (\\ref{model}) is sparse, that is, most of its components are\nzero. Let $A = \\{1 \\leq j \\leq p_n: \\beta_{0j} \\neq0\\}$ be the index\nset of nonzero coefficients and $q_n = |A|$ be the cardinality of $A$.\nThe set $A$ is unknown and will be estimated.\n\n\nWithout loss of generality, we assume that the first $q_n$ components of\n${\\bolds{\\beta}}_0$ are nonzero and the remaining $p_n-q_n$ components are\nzero. \n\nHence, we can write ${\\bolds{\\beta}}_0 = ({\\bolds{\\beta}}_{01}',\\mathbf\n{0}_{p_n-q_n}' )'$, where $\\mathbf{0}_{p_n-q_n}$ denotes the\n$(p_n-q_n)$-vector of zeros.\nLet $X$ be the $n \\times p_n$ matrix of linear covariates and write it\nas $X = (X_1,\\ldots, X_{p_n})$.\nLet $X_A$ be the submatrix consisting of the first $q_n$ columns of $X$\ncorresponding to the active covariates.\n\n\n\n\n\n\n\nFor technical simplicity, we assume $x_{i}$ is centered to have mean zero;\nand $z_{ij} \\in[0,1]$, $\\forall i, j$.\n\n\n\\subsection{Oracle estimator}\\label{sec2.1}\nWe first study the estimator we would obtain when the index set $A$ is\nknown in advance, which we\nrefer to as the oracle estimator. Our asymptotic framework allows\n$q_n$, the size of $A$, to increase with $n$.\nThis resonates with the perspective that a more complex statistical\nmodel can be fit when more data are collected.\n\nWe use a linear combination of B-spline basis functions to approximate\nthe unknown nonlinear functions\n$g_0(\\cdot)$.\n\n\n\nTo introduce the B-spline functions, we start with two definitions.\n\n\\begin{defi*}\nLet $r\n\\equiv m + v$,\nwhere $m$ is a positive integer and $v\\in(0,1]$. Define $\\mathcal\n{H}_r$ as the collection of functions $h(\\cdot)$ on $[0,1]$ whose\n$m$th derivative $h^{(m)}(\\cdot)$ satisfies the H\\\"{o}lder condition\nof order $v$. That is, for any $h(\\cdot) \\in\\mathcal{H}_r$, there\nexists some positive constant $C$\nsuch that\n\n\n\n\n", "itemtype": "equation", "pos": 3666, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\\title{Partially linear additive quantile regression in~ultra-high dimension}\n\\runtitle{Ultra-high dimensional PLA quantile regression}\n\n\\begin{aug}\n\n\n\\author[A]{\\fnms{Ben}~\\snm{Sherwood}\\corref{}\\ead[label=e1]{bsherwo2@jhu.edu}}\n\\and\n\\author[B]{\\fnms{Lan}~\\snm{Wang}\\thanksref{T2}\\ead[label=e2]{wangx346@umn.edu}}\n\\runauthor{B. Sherwood and L. Wang}\n\\affiliation{Johns Hopkins University and University of Minnesota}\n\n\\address[A]{Department of Biostatistics\\\\\nJohns Hopkins University\\\\\nBaltimore, Maryland 21205\\\\\nUSA\\\\\n\\printead{e1}}\n\\address[B]{School of Statistics\\\\\nUniversity of Minnesota\\\\\nMinneapolis, Minnesota 55455\\\\\nUSA\\\\\n\\printead{e2}}\n\\end{aug}\n\n\\thankstext{T2}{Supported in part by NSF Grant DMS-13-08960.}\n\n\n\n\\received{\\smonth{9} \\syear{2014}}\n\n\n\\revised{\\smonth{7} \\syear{2015}}\n\n\n\n\n\\begin{abstract}\nWe consider a flexible semiparametric quantile regression model for analyzing\nhigh dimensional heterogeneous data. This model has\nseveral appealing features: (1) By considering different\nconditional quantiles, we may \\mbox{obtain} a more complete picture of the\nconditional distribution of a response variable given\nhigh dimensional covariates.\n(2) The sparsity level is allowed to be different at different quantile levels.\n(3) The partially linear additive structure accommodates nonlinearity\nand circumvents the curse of dimensionality. (4) It is naturally robust to\nheavy-tailed distributions. In this paper, we approximate the\nnonlinear components using B-spline basis functions. We first study\nestimation under this model when the nonzero components\nare known in advance and the number of covariates in the linear part diverges.\nWe then investigate a nonconvex penalized estimator for simultaneous\nvariable selection and estimation. We derive its oracle property for\na general class of nonconvex penalty functions\nin the presence of ultra-high dimensional covariates under relaxed conditions.\nTo tackle the challenges of nonsmooth loss function, nonconvex penalty function\nand the presence of nonlinear components, we combine a recently developed\nconvex-differencing method with \nmodern empirical process techniques. \n\n\n\nMonte Carlo simulations and an application to a microarray study\ndemonstrate the effectiveness of the proposed method. We also discuss\nhow the method\nfor a single quantile of interest can be extended to simultaneous\nvariable selection and estimation at multiple quantiles.\n\\end{abstract}\n\n\n\n\n\\begin{keyword}[class=AMS]\n\\kwd[Primary ]{62G35}\n\n\\kwd[; secondary ]{62G20}\n\\end{keyword}\n\n\\begin{keyword}\n\\kwd{Quantile regression}\n\\kwd{high dimensional data}\n\\kwd{nonconvex penalty}\n\\kwd{partial linear}\n\\kwd{variable selection}\n\\end{keyword}\n\n\\end{frontmatter}\n\n\n\\section{Introduction}\\label{sec1}\nIn this article, we study a flexible partially linear additive\nquantile regression model for analyzing high dimensional data.\nFor the $i$th subject, we observe $ \\{Y_i,{\\mathbf{x}}_i,{\\mathbf{z}}_i \\}$,\nwhere ${\\mathbf{x}}_i= (x_{i1},\\ldots,x_{ip_n})'$ is a $p_n$-dimensional\nvector of covariates\nand ${\\mathbf{z}}_i= (z_{i1},\\ldots,z_{id})'$ is a $d$-dimensional vector of\ncovariates, $i=1,\\ldots,n$.\nThe $\\tau$th ($0<\\tau<1$) conditional quantile of $Y_i$ given ${\\mathbf{x}}_i$,\n${\\mathbf{z}}_i$ is defined as $Q_{Y_i|{\\mathbf{x}}_i,{\\mathbf{z}}_i}(\\tau)=\\inf\\{t: F(t|{\\mathbf{x}}_i\n,{\\mathbf{z}}_i)\\geq\\tau\\}$,\nwhere $F(\\cdot|{\\mathbf{x}}_i,{\\mathbf{z}}_i)$ is the conditional distribution function\nof $Y_i$ given ${\\mathbf{x}}_i$\nand ${\\mathbf{z}}_i$. The case $\\tau=1/2$ corresponds to the conditional median.\nWe consider the following semiparametric model for the conditional\nquantile function\n\n\n\n\n", "index": 1, "text": "\\begin{equation}\n\\label{model} Q_{Y_i|{\\mathbf{x}}_i,{\\mathbf{z}}_i}(\\tau)={\\mathbf{x}}_i'\n{\\bolds{\\beta}}_0+g_0({\\mathbf{z}}_i),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Q_{Y_{i}|{\\mathbf{x}}_{i},{\\mathbf{z}}_{i}}(\\tau)={\\mathbf{x}}_{i}^{\\prime}{%&#10;\\bolds{\\beta}}_{0}+g_{0}({\\mathbf{z}}_{i}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>Q</mi><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc31</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>0</mn></msub></mrow><mo>+</mo><mrow><msub><mi>g</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\\end{defi*}\n\nAssume for some $r \\geq1.5$, the nonparametric\ncomponent $g_{0k}(\\cdot) \\in\\mathcal{H}_r$.\nLet ${\\bolds{\\pi}}(t)= (b_1(t),\\ldots,b_{k_n+l+1}(t) )'$ denote\na vector of normalized B-spline basis functions of order $l+1$\nwith $k_n$ quasi-uniform internal knots on $[0,1]$.\n\nThen $g_{0k}(\\cdot)$ can be approximated using a linear combination of\nB-spline basis functions in\n$\\bolds{\\Pi}({\\mathbf{z}}_i)=(1,{\\bolds{\\pi}}(z_{i1})',\\ldots,{\\bolds{\\pi}}(z_{id})')'$.\n\nWe refer to \\citet{Schumaker} for details of the B-spline construction,\nand the result\nthat there exists ${\\bolds{\\xi}}_{0}\\in\\mathcal{R}^{L_n}$, where\n$L_n=d(k_n+l+1)+1$, such that\n$\\sup_{{\\mathbf{z}}_i}|\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}_{0} - g_{0}({\\mathbf{z}}_i)| =\nO\n(k_n^{-r} )$.\nFor ease of notation and simplicity of proofs, we use the same number\nof basis functions for\nall nonlinear components in model (\\ref{model}). In practice, such\nrestrictions are not necessary.\n\nNow consider quantile regression with the oracle information that the\nlast $(p_n-q_n)$\nelements of ${\\bolds{\\beta}}_0$ are all zero. Let\n\n\n\n\n", "itemtype": "equation", "pos": 13515, "prevtext": "\n\nwhere $g_0({\\mathbf{z}}_i)=g_{00}+\\sum_{j=1}^dg_{0j}(z_{ij})$, with $g_{00}\\in\n\\mathcal{R}$.\n\nIt is assumed that $g_{0j}$ satisfy $E(g_{0j}(z_{ij}))=0$ for\nidentification purposes.\nLet $\\varepsilon_i=Y_i-Q_{Y_i|{\\mathbf{x}}_i,{\\mathbf{z}}_i}(\\tau)$, then $\\varepsilon_i$\nsatisfies $P(\\varepsilon_i\\leq0|{\\mathbf{x}}_i,{\\mathbf{z}}_i)=\\tau$ and we may also write\n$Y_i={\\mathbf{x}}_i'{\\bolds{\\beta}}_0+g_0({\\mathbf{z}}_i)+\\varepsilon_i$.\nIn the rest of the paper, we will drop the dependence on $\\tau$\nin the notation for simplicity.\n\nModeling conditional quantiles in high dimension is of significant\nimportance for several reasons.\nFirst, it is well recognized that high dimensional data are often heterogeneous.\nHow the covariate influence the center of the conditional distribution\ncan be very different from how they influence the tails.\nAs a result, focusing on the conditional mean function alone can be misleading.\nBy estimating conditional quantiles at different quantile levels,\nwe are able to gain a more complete picture of the relationship between\nthe covariates and the response variable.\nSecond, in the high dimensional setting,\nthe quantile regression framework also allows a more realistic\ninterpretation of the sparsity\nof the covariate effects, which we refer to as quantile-adaptive sparsity.\nThat is, we assume a small subset of covariates influence the conditional\ndistribution. However, when we estimate different conditional\nquantiles, we allow the\nsubsets of active covariates to be different [\\citet{quantUltraHigh};\n\\citet{heWang}].\nFurthermore, the conditional quantiles are often of direct interest to\nthe researchers.\nFor example, for the birth weight data we analyzed in Section~\\ref{sec5}, low\nbirth weight, which corresponds to the low tail of the conditional\ndistribution, is of direct interest to the doctors. Another advantage\nof quantile regression is that it is naturally robust to outlier\ncontamination associated with heavy-tailed errors. For high dimensional\ndata, identifying outliers can be difficult. The robustness of quantile\nregression provides a certain degree of protection.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear quantile regression with high dimensional covariates was\ninvestigated by\n\\citeauthor{B1} [(\\citeyear{B1}), Lasso penalty] and\nWang, Wu and Li [(\\citeyear{quantUltraHigh}), nonconvex penalty].\nThe partially linear additive structure we consider in this paper is\nuseful for incorporating nonlinearity\nin the model while circumventing the curse of dimensionality.\nWe are interested in the case\n$p_n$ is of a similar order of $n$ or much larger than $n$.\nFor applications in microarray data analysis,\nthe vector ${\\mathbf{x}}_i$ often contains the measurements on thousands of\ngenes, while the\nvector ${\\mathbf{z}}_i$ contains the measurements of clinical or environment\nvariables, such as\nage and weight. For example, in the birth weight example of Section~\\ref{sec5},\nmother's age is modeled nonparametrically\nas exploratory analysis reveals a possible nonlinear effect. In\ngeneral, model specification\ncan be challenging in high dimension; see Section~\\ref{sec7} for some\nfurther discussion.\n\n\n\n\n\n\n\nWe approximate the nonparametric components using B-spline basis\nfunctions, which are computationally convenient\nand often accurate. First, we study the asymptotic theory\nof estimating the model (\\ref{model}) when $p_n$ diverges.\nIn our setting, this corresponds to the oracle model, that is, the\none we obtain if we know which covariates are important in advance.\nThis is along the line of the work\nof \\citet{Welsh}, \\citet{BaiWu} and \\citet{HeShao}\nfor $M$-regression with diverging number of parameters and\npossibly nonsmooth objective functions, which, however,\nwere restricted to linear regression. \\citet{partMeanVaryCoef}\nderived the asymptotic theory of profile kernel estimator\nfor general semiparametric models with diverging number\nof parameter while assuming a smooth quasi-likelihood function.\nSecond, we propose a nonconvex\npenalized regression estimator when $p_n$ is of an exponential order of\n$n$ and\nthe model has a sparse structure. For a general class of nonsmooth\npenalty functions, including the popular SCAD [\\citet{fanLi}]\nand MCP [\\citet{Zhang}] penalty, we derive the oracle property of the\nproposed estimator under\nrelaxed conditions. An interesting finding is that\nsolving the nonconvex penalized estimator can be achieved via\nsolving a series of weighted quantile regression problems, which can\nbe conveniently implemented using existing software packages.\n\nDeriving the asymptotic properties of the penalized estimator is very\nchallenging as\nwe need to simultaneously deal with the nonsmooth loss function, the\nnonconvex penalty\nfunction, approximation of nonlinear functions and very high \\mbox{dimensionality}.\nTo tackle these challenges, we combine a recently developed\nconvex-differencing method with \nmodern empirical process techniques. The method relies on a\nrepresentation of the penalized loss function as the difference\nof two convex functions, which leads to a sufficient local\noptimality condition [\\citet{TaoAn97}, \\citet{quantUltraHigh}].\nEmpirical process techniques are\nintroduced to derive various error bounds associated with\nthe nonsmooth objective function which contains both high dimensional\nlinear covariates and approximations of nonlinear components.\nIt is worth pointing out that our approach is different from what was\nused in the\nrecent literature for studying the theory of\nhigh dimensional semiparametric mean regression and is able to\nconsiderably weaken\nthe conditions required in the literature. In particular, we do not\nneed moment\nconditions for the random error and allow it to depend on the covariates.\n\nExisting work on penalized semiparametric regression has been largely\nlimited to mean regression with\nfixed $p$;\nsee, for example, \\citet{Bunea}, \\citet{LiangLi}, \\citet{WX},\n\\citet{scadPartLinMean}, \\citet{KLZ} and\n\\citet{WLLC}.\nImportant progress in the high dimensional $p$ setting\nhas been recently made by\n\\citeauthor{annalsMeanParLin} [(\\citeyear{annalsMeanParLin}), still\nassumes $p<n$] for\npartially linear regression, \\citet{HHW} for\nadditive models,\n\\citeauthor{Li} [(\\citeyear{Li}), $p=o(n)$] for semivarying coefficient models,\namong others. When $p$ is fixed, the semiparametric quantile regression\nmodel was considered by\n\\citet{heShi96}, \\citet{heQuantLong}, \\citet{quantVaryModel},\namong others. \\citet{Tang} considered a two-step procedure for a\nnonparametric varying\ncoefficients quantile regression model with a diverging number of\nnonparametric functional coefficients.\nThey required two separate tuning parameters and quite complex design\nconditions.\n\nThe rest of this article is organized as follows. In Section~\\ref\n{sec2}, we\npresent the partially linear additive quantile regression model and\ndiscuss the properties of the oracle estimator.\nIn Section~\\ref{sec3}, we present a nonconvex penalized method for simultaneous\nvariable selection and estimation and derive its\noracle property. In Section~\\ref{sec4}, we assess the performance of the\nproposed penalized\nestimator via Monte Carlo simulations.\nWe analyze a birth weight data set while accounting for gene expression\nmeasurements in Section~\\ref{sec5}.\nIn Section~\\ref{sec6}, we consider an extension to simultaneous\nestimation and\nvariable selection at multiple quantiles.\nSection~\\ref{sec7} concludes the paper with a discussion of related issues.\n\n\nThe proofs are given in the \\hyperref[appe]{Appendix}.\nSome of the technical details and additional numerical results are\nprovided in online supplementary material [\\citet{Supp}].\n\n\n\\section{Partially linear additive quantile regression with diverging\nnumber of parameters}\\label{sec2}\nFor high dimensional inference, it is often assumed that the vector of\ncoefficients\n${\\bolds{\\beta}}_0=(\\beta_{01},\\beta_{02},\\ldots,\\beta_{0p_n})'$ \n\nin model (\\ref{model}) is sparse, that is, most of its components are\nzero. Let $A = \\{1 \\leq j \\leq p_n: \\beta_{0j} \\neq0\\}$ be the index\nset of nonzero coefficients and $q_n = |A|$ be the cardinality of $A$.\nThe set $A$ is unknown and will be estimated.\n\n\nWithout loss of generality, we assume that the first $q_n$ components of\n${\\bolds{\\beta}}_0$ are nonzero and the remaining $p_n-q_n$ components are\nzero. \n\nHence, we can write ${\\bolds{\\beta}}_0 = ({\\bolds{\\beta}}_{01}',\\mathbf\n{0}_{p_n-q_n}' )'$, where $\\mathbf{0}_{p_n-q_n}$ denotes the\n$(p_n-q_n)$-vector of zeros.\nLet $X$ be the $n \\times p_n$ matrix of linear covariates and write it\nas $X = (X_1,\\ldots, X_{p_n})$.\nLet $X_A$ be the submatrix consisting of the first $q_n$ columns of $X$\ncorresponding to the active covariates.\n\n\n\n\n\n\n\nFor technical simplicity, we assume $x_{i}$ is centered to have mean zero;\nand $z_{ij} \\in[0,1]$, $\\forall i, j$.\n\n\n\\subsection{Oracle estimator}\\label{sec2.1}\nWe first study the estimator we would obtain when the index set $A$ is\nknown in advance, which we\nrefer to as the oracle estimator. Our asymptotic framework allows\n$q_n$, the size of $A$, to increase with $n$.\nThis resonates with the perspective that a more complex statistical\nmodel can be fit when more data are collected.\n\nWe use a linear combination of B-spline basis functions to approximate\nthe unknown nonlinear functions\n$g_0(\\cdot)$.\n\n\n\nTo introduce the B-spline functions, we start with two definitions.\n\n\\begin{defi*}\nLet $r\n\\equiv m + v$,\nwhere $m$ is a positive integer and $v\\in(0,1]$. Define $\\mathcal\n{H}_r$ as the collection of functions $h(\\cdot)$ on $[0,1]$ whose\n$m$th derivative $h^{(m)}(\\cdot)$ satisfies the H\\\"{o}lder condition\nof order $v$. That is, for any $h(\\cdot) \\in\\mathcal{H}_r$, there\nexists some positive constant $C$\nsuch that\n\n\n\n\n", "index": 3, "text": "\\begin{equation}\n\\label{holder_cond} \\bigl{\\vert} h^{(m)}\\bigl(z'\\bigr) -\nh^{(m)}(z)\\bigr{\\vert}\\leq C \\bigl{\\vert} z' - z\\bigr\n{\\vert}^v\\qquad\\forall0 \\leq z', z \\leq1.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\bigl{|}h^{(m)}\\bigl{(}z^{\\prime}\\bigr{)}-h^{(m)}(z)\\bigr{|}\\leq C\\bigl{|}z^{%&#10;\\prime}-z\\bigr{|}^{v}\\qquad\\forall 0\\leq z^{\\prime},z\\leq 1.\" display=\"block\"><mrow><mrow><mrow><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>-</mo><mrow><msup><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow><mo>\u2264</mo><mrow><mi>C</mi><mo>\u2062</mo><msup><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>-</mo><mi>z</mi></mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow><mi>v</mi></msup></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mrow><mrow><mo>\u2200</mo><mn>0</mn></mrow><mo>\u2264</mo><msup><mi>z</mi><mo>\u2032</mo></msup></mrow><mo>,</mo><mrow><mi>z</mi><mo>\u2264</mo><mn>1</mn></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\nwhere\\vspace*{-2pt}\n\n\n$\\rho_\\tau(u) = u(\\tau- I(u<0))$ is the quantile loss function and\n${\\mathbf{x}}_{A_1}',\\ldots,{\\mathbf{x}}_{A_n}'$ denote the row vectors of $X_A$.\nThe\\vspace*{-2pt} oracle estimator for ${\\bolds{\\beta}}_0$ is $ (\\hat{{\\bolds{\\beta}}}_1',\n\\mathbf{0}_{p_n-q_n}' )'$.\nWrite $\\hat{{\\bolds{\\xi}}}=(\\hat{\\xi}_0,\\hat{{\\bolds{\\xi}}}_1',\\ldots,\\hat{{\\bolds{\\xi}}}_d')'$\nwhere $\\hat{\\xi}_0\\in\\mathcal{R}$ and $\\hat{{\\bolds{\\xi}}}_j\\in\\mathcal\n{R}^{k_n+l+1}$, $j=1\\ldots,d$.\nThe estimator\nfor the nonparametric function $g_{0j}$ is\n\n", "itemtype": "equation", "pos": 14800, "prevtext": "\n\\end{defi*}\n\nAssume for some $r \\geq1.5$, the nonparametric\ncomponent $g_{0k}(\\cdot) \\in\\mathcal{H}_r$.\nLet ${\\bolds{\\pi}}(t)= (b_1(t),\\ldots,b_{k_n+l+1}(t) )'$ denote\na vector of normalized B-spline basis functions of order $l+1$\nwith $k_n$ quasi-uniform internal knots on $[0,1]$.\n\nThen $g_{0k}(\\cdot)$ can be approximated using a linear combination of\nB-spline basis functions in\n$\\bolds{\\Pi}({\\mathbf{z}}_i)=(1,{\\bolds{\\pi}}(z_{i1})',\\ldots,{\\bolds{\\pi}}(z_{id})')'$.\n\nWe refer to \\citet{Schumaker} for details of the B-spline construction,\nand the result\nthat there exists ${\\bolds{\\xi}}_{0}\\in\\mathcal{R}^{L_n}$, where\n$L_n=d(k_n+l+1)+1$, such that\n$\\sup_{{\\mathbf{z}}_i}|\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}_{0} - g_{0}({\\mathbf{z}}_i)| =\nO\n(k_n^{-r} )$.\nFor ease of notation and simplicity of proofs, we use the same number\nof basis functions for\nall nonlinear components in model (\\ref{model}). In practice, such\nrestrictions are not necessary.\n\nNow consider quantile regression with the oracle information that the\nlast $(p_n-q_n)$\nelements of ${\\bolds{\\beta}}_0$ are all zero. Let\n\n\n\n\n", "index": 5, "text": "\\begin{equation}\n\\label{orcObjFun} (\\hat{{\\bolds{\\beta}}}_1, \\hat{{\\bolds{\\xi}}} ) = \\mathop{\\operatorname\n{argmin}}_{ ({\\bolds{\\beta}}\n_1, {\\bolds{\\xi}})} \\frac{1}{n} \\sum\n_{i=1}^n\\rho_\\tau\\bigl(Y_i\n- {\\mathbf{x}}_{A_i} '{\\bolds{\\beta}}_1 - \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}\\bigr),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"(\\hat{{\\bolds{\\beta}}}_{1},\\hat{{\\bolds{\\xi}}})=\\mathop{\\operatorname{argmin}}%&#10;_{({\\bolds{\\beta}}_{1},{\\bolds{\\xi}})}\\frac{1}{n}\\sum_{i=1}^{n}\\rho_{\\tau}%&#10;\\bigl{(}Y_{i}-{\\mathbf{x}}_{A_{i}}^{\\prime}{\\bolds{\\beta}}_{1}-\\bolds{\\Pi}({%&#10;\\mathbf{z}}_{i})^{\\prime}{\\bolds{\\xi}}\\bigr{)},\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><munder><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>a</mi><mo movablelimits=\"false\">\u2062</mo><mi>r</mi><mo movablelimits=\"false\">\u2062</mo><mi>g</mi><mo movablelimits=\"false\">\u2062</mo><mi>m</mi><mo movablelimits=\"false\">\u2062</mo><mi>i</mi><mo movablelimits=\"false\">\u2062</mo><mi>n</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>1</mn></msub></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udc31</mi><msub><mi>A</mi><mi>i</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nfor $j=1,\\ldots,d$; for $g_{00}$\nis $\\hat{g}_{0}=\\hat{\\xi}_0+n^{-1}\\sum_{i=1}^n\\sum_{j=1}^d{\\bolds{\\pi}}(z_{ij})\n'\\hat{{\\bolds{\\xi}}}_j$.\nThe centering of $\\hat{g}_{j}$ is the sample analog of the identifiability\ncondition $E[g_{0j}({\\mathbf{z}}_i)] = 0$. The estimator of $g_{0}({\\mathbf{z}}_i)$ is\n$\\hat{g}({\\mathbf{z}}_i)=\\hat{g}_{0}+\\sum_{j=1}^d\\hat{g}_{j}(z_{ij})$.\n\n\n\\subsection{Asymptotic properties}\\label{sec2.2}\nWe next present the asymptotic properties of the oracle estimators as\n$q_n$ diverges.\n\n\\begin{defi*}\nGiven ${\\mathbf{z}}\n= (z_1,\\ldots\n,z_d)'$, the function $g({\\mathbf{z}})$ is said to belong to the class of\nfunctions $\\mathcal{G}$ if it has the representation $g({\\mathbf{z}}) = \\alpha\n+\\sum_{k=1}^d g_k({\\mathbf{z}}_k)$, $\\alpha\\in\\mathcal{R}$,\n$g_k \\in\\mathcal{H}_r$ and $E[g_k({\\mathbf{z}}_{k})]=0$.\n\\end{defi*}\n\n\n\nLet\n\n", "itemtype": "equation", "pos": 15661, "prevtext": "\n\n\n\n\n\n\n\n\n\nwhere\\vspace*{-2pt}\n\n\n$\\rho_\\tau(u) = u(\\tau- I(u<0))$ is the quantile loss function and\n${\\mathbf{x}}_{A_1}',\\ldots,{\\mathbf{x}}_{A_n}'$ denote the row vectors of $X_A$.\nThe\\vspace*{-2pt} oracle estimator for ${\\bolds{\\beta}}_0$ is $ (\\hat{{\\bolds{\\beta}}}_1',\n\\mathbf{0}_{p_n-q_n}' )'$.\nWrite $\\hat{{\\bolds{\\xi}}}=(\\hat{\\xi}_0,\\hat{{\\bolds{\\xi}}}_1',\\ldots,\\hat{{\\bolds{\\xi}}}_d')'$\nwhere $\\hat{\\xi}_0\\in\\mathcal{R}$ and $\\hat{{\\bolds{\\xi}}}_j\\in\\mathcal\n{R}^{k_n+l+1}$, $j=1\\ldots,d$.\nThe estimator\nfor the nonparametric function $g_{0j}$ is\n\n", "index": 7, "text": "\n\\[\n\\hat{g}_j(z_{ij}) = {\\bolds{\\pi}}(z_{ij})'\n\\hat{{\\bolds{\\xi}}}_j-n^{-1}\\sum_{i=1}^n\n{\\bolds{\\pi}}(z_{ij})'\\hat{{\\bolds{\\xi}}}_j,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\hat{g}_{j}(z_{ij})={\\bolds{\\pi}}(z_{ij})^{\\prime}\\hat{{\\bolds{\\xi}}}_{j}-n^{-%&#10;1}\\sum_{i=1}^{n}{\\bolds{\\pi}}(z_{ij})^{\\prime}\\hat{{\\bolds{\\xi}}}_{j},\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub></mrow><mo>-</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhere $f_i(\\cdot)$ is the probability density function of $\\varepsilon\n_i$ given $({\\mathbf{x}}_i,{\\mathbf{z}}_i)$.\nLet $m_j({\\mathbf{z}}) = E [x_{ij} {|}{\\mathbf{z}}_i={\\mathbf{z}}]$, then it can\\vspace*{1pt}\nbe shown that\n$h^*_j(\\cdot)$ is the weighted projection of\n$m_j(\\cdot)$ into $\\mathcal{G}$ under the $L_2$ norm, where the\nweights $f_i(0)$ are included to account for the possibly heterogeneous errors.\nFurthermore, let $x_{A_{ij}}$ be the $(i,j)$th element of $X_A$. Define\n$\\delta_{ij} \\equiv x_{A_{ij}} - h^*_j({\\mathbf{z}}_i)$, ${\\bolds{\\delta}}_{i} =\n(\\delta_{i1},\\ldots,\\delta_{iq_n} )' \\in\\mathcal{R}^{q_n}$\nand $\\Delta_n = ({\\bolds{\\delta}}_1,\\ldots,{\\bolds{\\delta}}_n )' \\in\n{\\mathbb}{R}^{n \\times q_n}$. Let $H$ be the $n \\times q_n$ matrix with\nthe $(i,j)$th element $H_{ij}= h_j^*({\\mathbf{z}}_i)$, then $X_A = H + \\Delta\n_n$.\n\nThe following technical conditions are imposed for analyzing the\nasymptotic behavior of $\\hat{{\\bolds{\\beta}}}_1$ and $\\hat{g}$.\n\n\n\n\\begin{condition}[(Conditions on the random error)]\\label{cond_f}\nThe random error $\\varepsilon_i$ has the conditional distribution\nfunction $F_i$ and continuous\nconditional density function $f_i$, given ${\\mathbf{x}}_i$, ${\\mathbf{z}}_i$.\nThe $f_i$ are uniformly bounded away from 0 and\ninfinity in a neighborhood of zero, its first derivative $f_i'$ has a\nuniform upper bound in a neighborhood of zero,\nfor $1\\leq i\\leq n$.\n\\end{condition}\n\n\n\n\\begin{condition}[(Conditions on the covariates)]\\label{highd_cond_x}\nThere exist positive constants $M_1$ and $M_2$ such that $|x_{ij}| \\leq\nM_1$, $\\forall1\\leq i \\leq n, 1 \\leq j \\leq p_n$ and\n$E[\\delta_{ij}^{4}] \\leq M_2$, $\\forall1\\leq i \\leq n, 1 \\leq j\n\\leq q_n$.\n\n\nThere exist finite positive constants $C_1$ and $C_2$ such that with\nprobability one\n\n\n", "itemtype": "equation", "pos": 16644, "prevtext": "\n\nfor $j=1,\\ldots,d$; for $g_{00}$\nis $\\hat{g}_{0}=\\hat{\\xi}_0+n^{-1}\\sum_{i=1}^n\\sum_{j=1}^d{\\bolds{\\pi}}(z_{ij})\n'\\hat{{\\bolds{\\xi}}}_j$.\nThe centering of $\\hat{g}_{j}$ is the sample analog of the identifiability\ncondition $E[g_{0j}({\\mathbf{z}}_i)] = 0$. The estimator of $g_{0}({\\mathbf{z}}_i)$ is\n$\\hat{g}({\\mathbf{z}}_i)=\\hat{g}_{0}+\\sum_{j=1}^d\\hat{g}_{j}(z_{ij})$.\n\n\n\\subsection{Asymptotic properties}\\label{sec2.2}\nWe next present the asymptotic properties of the oracle estimators as\n$q_n$ diverges.\n\n\\begin{defi*}\nGiven ${\\mathbf{z}}\n= (z_1,\\ldots\n,z_d)'$, the function $g({\\mathbf{z}})$ is said to belong to the class of\nfunctions $\\mathcal{G}$ if it has the representation $g({\\mathbf{z}}) = \\alpha\n+\\sum_{k=1}^d g_k({\\mathbf{z}}_k)$, $\\alpha\\in\\mathcal{R}$,\n$g_k \\in\\mathcal{H}_r$ and $E[g_k({\\mathbf{z}}_{k})]=0$.\n\\end{defi*}\n\n\n\nLet\n\n", "index": 9, "text": "\n\\[\nh^*_j(\\cdot) = \\mathop{\\operatorname{arg}\\operatorname\n{inf}}_{h_j(\\cdot) \\in\\mathcal{G}} \\sum_{i=1}^nE \\bigl[\nf_i(0) \\bigl(x_{ij} - h_j({\\mathbf{z}}_i)\n\\bigr)^2 \\bigr],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"h^{*}_{j}(\\cdot)=\\mathop{\\operatorname{arg}\\operatorname{inf}}_{h_{j}(\\cdot)%&#10;\\in\\mathcal{G}}\\sum_{i=1}^{n}E\\bigl{[}f_{i}(0)\\bigl{(}x_{ij}-h_{j}({\\mathbf{z}%&#10;}_{i})\\bigr{)}^{2}\\bigr{]},\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>h</mi><mi>j</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>a</mi><mo movablelimits=\"false\">\u2062</mo><mi>r</mi><mo movablelimits=\"false\">\u2062</mo><mi>g</mi><mo movablelimits=\"false\">\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>i</mi><mo movablelimits=\"false\">\u2062</mo><mi>n</mi><mo movablelimits=\"false\">\u2062</mo><mi>f</mi></mrow><mrow><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>-</mo><mrow><msub><mi>h</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\\end{condition}\n\n\n\n\\begin{condition}[(Condition on the nonlinear functions)]\\label{cond_g_h}\nFor $r=m+v> 1.5$ $g_0 \\in\\mathcal{G}$.\n\n\n\\end{condition}\n\n\n\n\n\n\n\n\n\n\\begin{condition}[(Condition on the B-spline basis)]\n\\label{cond_j_n}\nThe\\vspace*{1pt} dimension of the spline basis $k_n$ has the following rate\n\n$k_n \\approx n^{1/(2r+1)}$.\n\n\\end{condition}\n\n\n\n\\begin{condition}[(Condition on model size)]\\label{cond_sigma_large_p}\n$q_n = O (n^{C_3} )$ for some $C_3< \\frac{1}{3}$.\n\n\\end{condition}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCondition \\ref{cond_f} is considerably more relaxed than what is\nusually imposed on the random error for\nthe theory of high dimensional mean regression, which often requires\nGaussian or sub-Gaussian tail condition.\nCondition~\\ref{highd_cond_x} is about the behavior of the covariates and the design\nmatrix under the oracle model, which\nis not restrictive.\nCondition \\ref{cond_g_h} is typical for the application of B-splines.\n\\citet{Stone85} showed that B-splines basis\nfunctions can be used to effectively approximate functions satisfying\nH\\\"{o}lder's condition. \n\n\nCondition \\ref{cond_j_n} provides the rate of $k_n$ needed for the\noptimal convergence rate of $\\hat{g}$.\nCondition~\\ref{cond_sigma_large_p} is standard for linear models with diverging number of parameters.\\vadjust{\\goodbreak}\n\n\n\n\n\n\nThe following theorem summarizes the asymptotic properties of the\noracle estimators.\n\n\n\n\\begin{theorem}\n\\label{large_q_oracle}\n\n\nAssume Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold.\nThen\n\n\\begin{eqnarray*}\n{\\Vert}\\hat{\\bolds{{\\bolds{\\beta}}}}_1-{\\bolds{\\beta}}_{01}{\\Vert}&=&\nO_p \\bigl(\\sqrt{n^{-1}q_n} \\bigr),\n\\\\\nn^{-1} \\sum_{i=1}^n \\bigl(\n\\hat{g}({\\mathbf{z}}_i) - g_0({\\mathbf{z}}_i)\n\\bigr)^2 &=& O_p \\bigl(n^{-1}(q_n+k_n)\n\\bigr).\n\\end{eqnarray*}\n\n\\end{theorem}\n\n\nAn interesting observation is that since we allow $q_n$ to diverge with\n$n$, it influences the rates for estimating both ${\\bolds{\\beta}}$ and $g$. As\n$q_n$ diverges, to investigate the asymptotic distribution of $\\hat\n{{\\bolds{\\beta}}}_1$, we consider estimating an arbitrary linear combination of\nthe components\nof ${\\bolds{\\beta}}_{01}$.\n\n\n\n\\begin{theorem}\n\\label{large_q_clt}\nAssume the conditions of Theorem \\ref{large_q_oracle} hold.\nLet $A_n$ be an $l\\times q_n$ matrix with $l$ fixed and\n$A_nA_n'\\rightarrow G$, a positive definite matrix, then\n\n", "itemtype": "equation", "pos": 18607, "prevtext": "\n\nwhere $f_i(\\cdot)$ is the probability density function of $\\varepsilon\n_i$ given $({\\mathbf{x}}_i,{\\mathbf{z}}_i)$.\nLet $m_j({\\mathbf{z}}) = E [x_{ij} {|}{\\mathbf{z}}_i={\\mathbf{z}}]$, then it can\\vspace*{1pt}\nbe shown that\n$h^*_j(\\cdot)$ is the weighted projection of\n$m_j(\\cdot)$ into $\\mathcal{G}$ under the $L_2$ norm, where the\nweights $f_i(0)$ are included to account for the possibly heterogeneous errors.\nFurthermore, let $x_{A_{ij}}$ be the $(i,j)$th element of $X_A$. Define\n$\\delta_{ij} \\equiv x_{A_{ij}} - h^*_j({\\mathbf{z}}_i)$, ${\\bolds{\\delta}}_{i} =\n(\\delta_{i1},\\ldots,\\delta_{iq_n} )' \\in\\mathcal{R}^{q_n}$\nand $\\Delta_n = ({\\bolds{\\delta}}_1,\\ldots,{\\bolds{\\delta}}_n )' \\in\n{\\mathbb}{R}^{n \\times q_n}$. Let $H$ be the $n \\times q_n$ matrix with\nthe $(i,j)$th element $H_{ij}= h_j^*({\\mathbf{z}}_i)$, then $X_A = H + \\Delta\n_n$.\n\nThe following technical conditions are imposed for analyzing the\nasymptotic behavior of $\\hat{{\\bolds{\\beta}}}_1$ and $\\hat{g}$.\n\n\n\n\\begin{condition}[(Conditions on the random error)]\\label{cond_f}\nThe random error $\\varepsilon_i$ has the conditional distribution\nfunction $F_i$ and continuous\nconditional density function $f_i$, given ${\\mathbf{x}}_i$, ${\\mathbf{z}}_i$.\nThe $f_i$ are uniformly bounded away from 0 and\ninfinity in a neighborhood of zero, its first derivative $f_i'$ has a\nuniform upper bound in a neighborhood of zero,\nfor $1\\leq i\\leq n$.\n\\end{condition}\n\n\n\n\\begin{condition}[(Conditions on the covariates)]\\label{highd_cond_x}\nThere exist positive constants $M_1$ and $M_2$ such that $|x_{ij}| \\leq\nM_1$, $\\forall1\\leq i \\leq n, 1 \\leq j \\leq p_n$ and\n$E[\\delta_{ij}^{4}] \\leq M_2$, $\\forall1\\leq i \\leq n, 1 \\leq j\n\\leq q_n$.\n\n\nThere exist finite positive constants $C_1$ and $C_2$ such that with\nprobability one\n\n\n", "index": 11, "text": "\n\\[\nC_1 \\leq\\lambda_{\\max} \\bigl( n^{-1}X_AX_A'\n\\bigr) \\leq C_2, \\qquad C_1 \\leq\\lambda_{\\max}\n\\bigl(n^{-1}\\Delta_n \\Delta_n'\n\\bigr) \\leq C_2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"C_{1}\\leq\\lambda_{\\max}\\bigl{(}n^{-1}X_{A}X_{A}^{\\prime}\\bigr{)}\\leq C_{2},%&#10;\\qquad C_{1}\\leq\\lambda_{\\max}\\bigl{(}n^{-1}\\Delta_{n}\\Delta_{n}^{\\prime}\\bigr%&#10;{)}\\leq C_{2}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>\u2264</mo><mrow><msub><mi>\u03bb</mi><mi>max</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>X</mi><mi>A</mi></msub><mo>\u2062</mo><msubsup><mi>X</mi><mi>A</mi><mo>\u2032</mo></msubsup></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>\u2264</mo><msub><mi>C</mi><mn>2</mn></msub></mrow><mo rspace=\"22.5pt\">,</mo><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>\u2264</mo><mrow><msub><mi>\u03bb</mi><mi>max</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi>n</mi></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi>n</mi><mo>\u2032</mo></msubsup></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>\u2264</mo><msub><mi>C</mi><mn>2</mn></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nin distribution, where $\\Sigma_n=K_n^{-1}S_n K_n^{-1}$ with\n$K_n=n^{-1}\\Delta_n'B_n \\Delta_n$,\n$S_n =\\break n^{-1}\\tau(1-\\tau) \\Delta_n'\\Delta_n$,\nand $B_n=\\operatorname{diag}(f_1(0),\\ldots,f_n(0))$ is an $n \\times n$\ndiagonal matrix with\n$f_i(0)$ denoting the conditional density function of $\\varepsilon_i$\ngiven $({\\mathbf{x}}_i,{\\mathbf{z}}_i)$ evaluated at zero.\n\\end{theorem}\n\nIf we consider the case where $q$ is fixed and finite, then we have the\nfollowing result\nregarding the behavior of the oracle estimator.\n\n\n\n\\begin{corollary}\n\\label{fixed_q_clt}\nAssume $q$ is a fixed positive integer,\n$n^{-1}\\Delta_n'B_n \\Delta_n\\rightarrow\\Sigma_1$ and\n\n\n$n^{-1}\\tau(1-\\tau)\\Delta_n'\\Delta_n\\rightarrow\\Sigma_2$, where\n$\\Sigma_1$ and $\\Sigma_2$\nare positive definite matrices.\nIf Conditions \\ref{cond_f}--\\ref{cond_j_n} hold, then\n\n\\begin{eqnarray*}\n\\sqrt{n} (\\hat{{\\bolds{\\beta}}}_1-{\\bolds{\\beta}}_{01} ) &\\stackrel{d}{\\rightarrow}& N\n\\bigl(\\mathbf{0}_q, \\Sigma_1^{-1}\n\\Sigma_2\\Sigma_1^{-1} \\bigr),\n\\\\\nn^{-1}\\sum_{i=1}^n \\bigl(\n\\hat{g}({\\mathbf{z}}_i) - g_0({\\mathbf{z}}_i)\n\\bigr)^2 &=& O_p \\bigl(n^{-2r/(2r+1)} \\bigr).\n\\end{eqnarray*}\n\n\\end{corollary}\n\nIn the case $q_n$ is fixed, the rates reduce to the classical\n$n^{-1/2}$ rate for estimating ${\\bolds{\\beta}}$ and $n^{-2r/(2r+1)}$ for\nestimating $g$, the latter\nwhich is consistent with \\citet{Stone85} for the optimal rate of\nconvergence.\n\n\n\\section{Nonconvex penalized estimation for partially linear additive\nquantile regression with ultra-high dimensional covariates}\\label{sec3}\n\n\n\\subsection{Nonconvex penalized estimator}\\label{sec3.1}\nIn real data analysis, we do not know which of the $p_n$ covariates in\n${\\mathbf{x}}_i$ are important.\nTo encourage sparse\\vadjust{\\goodbreak} estimation, we minimize the following penalized\nobjective function for estimating $({\\bolds{\\beta}}_0, {\\bolds{\\xi}}_0)$,\n\n\n\n\n", "itemtype": "equation", "pos": 21100, "prevtext": "\n\n\\end{condition}\n\n\n\n\\begin{condition}[(Condition on the nonlinear functions)]\\label{cond_g_h}\nFor $r=m+v> 1.5$ $g_0 \\in\\mathcal{G}$.\n\n\n\\end{condition}\n\n\n\n\n\n\n\n\n\n\\begin{condition}[(Condition on the B-spline basis)]\n\\label{cond_j_n}\nThe\\vspace*{1pt} dimension of the spline basis $k_n$ has the following rate\n\n$k_n \\approx n^{1/(2r+1)}$.\n\n\\end{condition}\n\n\n\n\\begin{condition}[(Condition on model size)]\\label{cond_sigma_large_p}\n$q_n = O (n^{C_3} )$ for some $C_3< \\frac{1}{3}$.\n\n\\end{condition}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCondition \\ref{cond_f} is considerably more relaxed than what is\nusually imposed on the random error for\nthe theory of high dimensional mean regression, which often requires\nGaussian or sub-Gaussian tail condition.\nCondition~\\ref{highd_cond_x} is about the behavior of the covariates and the design\nmatrix under the oracle model, which\nis not restrictive.\nCondition \\ref{cond_g_h} is typical for the application of B-splines.\n\\citet{Stone85} showed that B-splines basis\nfunctions can be used to effectively approximate functions satisfying\nH\\\"{o}lder's condition. \n\n\nCondition \\ref{cond_j_n} provides the rate of $k_n$ needed for the\noptimal convergence rate of $\\hat{g}$.\nCondition~\\ref{cond_sigma_large_p} is standard for linear models with diverging number of parameters.\\vadjust{\\goodbreak}\n\n\n\n\n\n\nThe following theorem summarizes the asymptotic properties of the\noracle estimators.\n\n\n\n\\begin{theorem}\n\\label{large_q_oracle}\n\n\nAssume Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold.\nThen\n\n\\begin{eqnarray*}\n{\\Vert}\\hat{\\bolds{{\\bolds{\\beta}}}}_1-{\\bolds{\\beta}}_{01}{\\Vert}&=&\nO_p \\bigl(\\sqrt{n^{-1}q_n} \\bigr),\n\\\\\nn^{-1} \\sum_{i=1}^n \\bigl(\n\\hat{g}({\\mathbf{z}}_i) - g_0({\\mathbf{z}}_i)\n\\bigr)^2 &=& O_p \\bigl(n^{-1}(q_n+k_n)\n\\bigr).\n\\end{eqnarray*}\n\n\\end{theorem}\n\n\nAn interesting observation is that since we allow $q_n$ to diverge with\n$n$, it influences the rates for estimating both ${\\bolds{\\beta}}$ and $g$. As\n$q_n$ diverges, to investigate the asymptotic distribution of $\\hat\n{{\\bolds{\\beta}}}_1$, we consider estimating an arbitrary linear combination of\nthe components\nof ${\\bolds{\\beta}}_{01}$.\n\n\n\n\\begin{theorem}\n\\label{large_q_clt}\nAssume the conditions of Theorem \\ref{large_q_oracle} hold.\nLet $A_n$ be an $l\\times q_n$ matrix with $l$ fixed and\n$A_nA_n'\\rightarrow G$, a positive definite matrix, then\n\n", "index": 13, "text": "\n\\[\n\\sqrt{n}A_n\\Sigma_n^{-1/2} (\\hat{\n{\\bolds{\\beta}}}_1-{\\bolds{\\beta}}_{01} ) \\rightarrow N(\\mathbf{0}_l,G)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\sqrt{n}A_{n}\\Sigma_{n}^{-1/2}(\\hat{{\\bolds{\\beta}}}_{1}-{\\bolds{\\beta}}_{01})%&#10;\\rightarrow N(\\mathbf{0}_{l},G)\" display=\"block\"><mrow><mrow><msqrt><mi>n</mi></msqrt><mo>\u2062</mo><msub><mi>A</mi><mi>n</mi></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>n</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>01</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2192</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mn/><mi>l</mi></msub><mo>,</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhere $p_\\lambda(\\cdot)$ is a penalty function with tuning parameter\n$\\lambda$.\n\n\n\n\n\nThe $L_1$ penalty or Lasso [\\citet{T1}] is a popular choice for\npenalized estimation. However,\nthe $L_1$ penalty is known to over-penalize large coefficients, tends\nto be biased\nand requires strong conditions on the design matrix to achieve\nselection consistency. \n\nThis is usually not \na concern for prediction, but can be undesirable if the goal is to\nidentify the underlying model. In comparison, an appropriate nonconvex\npenalty function can effectively overcome this problem [\\citet\n{fanLi}]. In this paper, we consider\ntwo such popular choices of penalty functions: the SCAD [\\citet\n{fanLi}] and MCP [\\citet{Zhang}] penalty functions. For the SCAD\npenalty function,\n\n\\begin{eqnarray*}\np_\\lambda\\bigl({\\vert}\\beta{\\vert}\\bigr) &=& \\lambda{\\vert}\\beta\n{\\vert} I\\bigl(0 \\leq{\\vert}\\beta{\\vert}< \\lambda\\bigr) + \\frac{a\n\\lambda{\\vert} \\beta{\\vert} - (\\beta^2 + \\lambda^2\n)/2}{a-1}I\n\\bigl(\\lambda\\leq{\\vert}\\beta{\\vert}\\leq a\\lambda\\bigr)\n\\\\\n&&{}+ \\frac{ (a + 1)\\lambda^2}{2}I\\bigl({\\vert}\\beta{\\vert}> a \\lambda\n\\bigr)\\qquad\\mbox{for some } a > 2,\n\\end{eqnarray*}\n\nand for the MCP penalty function,\n\n", "itemtype": "equation", "pos": 23083, "prevtext": "\n\nin distribution, where $\\Sigma_n=K_n^{-1}S_n K_n^{-1}$ with\n$K_n=n^{-1}\\Delta_n'B_n \\Delta_n$,\n$S_n =\\break n^{-1}\\tau(1-\\tau) \\Delta_n'\\Delta_n$,\nand $B_n=\\operatorname{diag}(f_1(0),\\ldots,f_n(0))$ is an $n \\times n$\ndiagonal matrix with\n$f_i(0)$ denoting the conditional density function of $\\varepsilon_i$\ngiven $({\\mathbf{x}}_i,{\\mathbf{z}}_i)$ evaluated at zero.\n\\end{theorem}\n\nIf we consider the case where $q$ is fixed and finite, then we have the\nfollowing result\nregarding the behavior of the oracle estimator.\n\n\n\n\\begin{corollary}\n\\label{fixed_q_clt}\nAssume $q$ is a fixed positive integer,\n$n^{-1}\\Delta_n'B_n \\Delta_n\\rightarrow\\Sigma_1$ and\n\n\n$n^{-1}\\tau(1-\\tau)\\Delta_n'\\Delta_n\\rightarrow\\Sigma_2$, where\n$\\Sigma_1$ and $\\Sigma_2$\nare positive definite matrices.\nIf Conditions \\ref{cond_f}--\\ref{cond_j_n} hold, then\n\n\\begin{eqnarray*}\n\\sqrt{n} (\\hat{{\\bolds{\\beta}}}_1-{\\bolds{\\beta}}_{01} ) &\\stackrel{d}{\\rightarrow}& N\n\\bigl(\\mathbf{0}_q, \\Sigma_1^{-1}\n\\Sigma_2\\Sigma_1^{-1} \\bigr),\n\\\\\nn^{-1}\\sum_{i=1}^n \\bigl(\n\\hat{g}({\\mathbf{z}}_i) - g_0({\\mathbf{z}}_i)\n\\bigr)^2 &=& O_p \\bigl(n^{-2r/(2r+1)} \\bigr).\n\\end{eqnarray*}\n\n\\end{corollary}\n\nIn the case $q_n$ is fixed, the rates reduce to the classical\n$n^{-1/2}$ rate for estimating ${\\bolds{\\beta}}$ and $n^{-2r/(2r+1)}$ for\nestimating $g$, the latter\nwhich is consistent with \\citet{Stone85} for the optimal rate of\nconvergence.\n\n\n\\section{Nonconvex penalized estimation for partially linear additive\nquantile regression with ultra-high dimensional covariates}\\label{sec3}\n\n\n\\subsection{Nonconvex penalized estimator}\\label{sec3.1}\nIn real data analysis, we do not know which of the $p_n$ covariates in\n${\\mathbf{x}}_i$ are important.\nTo encourage sparse\\vadjust{\\goodbreak} estimation, we minimize the following penalized\nobjective function for estimating $({\\bolds{\\beta}}_0, {\\bolds{\\xi}}_0)$,\n\n\n\n\n", "index": 15, "text": "\\begin{equation}\n\\label{penObj} Q^P({\\bolds{\\beta}},{\\bolds{\\xi}}) = n^{-1} \\sum\n_{i=1}^n\\rho_\\tau\\bigl(Y_i -\n{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}\\bigr) + \\sum_{j=1}^{p_n}\np_\\lambda\\bigl({\\vert}\\beta_j{\\vert}\\bigr),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"Q^{P}({\\bolds{\\beta}},{\\bolds{\\xi}})=n^{-1}\\sum_{i=1}^{n}\\rho_{\\tau}\\bigl{(}Y_%&#10;{i}-{\\mathbf{x}}_{i}^{\\prime}{\\bolds{\\beta}}-\\bolds{\\Pi}({\\mathbf{z}}_{i})^{%&#10;\\prime}{\\bolds{\\xi}}\\bigr{)}+\\sum_{j=1}^{p_{n}}p_{\\lambda}\\bigl{(}{|}\\beta_{j}%&#10;{|}\\bigr{)},\" display=\"block\"><mrow><mrow><mrow><msup><mi>Q</mi><mi>P</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>p</mi><mi>n</mi></msub></munderover><mrow><msub><mi>p</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nFor both penalty functions, the tuning parameter $\\lambda$ controls\nthe complexity\nof the selected model\nand goes to zero as $n$ increases to $\\infty$.\n\n\n\\subsection{Solving the penalized estimator}\\label{sec3.2}\nWe propose an effective algorithm to solve the above penalized\nestimation problem. The algorithm is largely based on\nthe idea of the local linear approximation (LLA) [\\citet{lla}].\nWe employ a new trick based on the observation ${\\vert} \\beta\n_j{\\vert} =\\rho_\\tau\n(\\beta_j) + \\rho_\\tau(-\\beta_j)$\nto transform the approximated objective function to a quantile\nregression objective function based\non an augmented data set, so that the penalized estimator can be\nobtained by iteratively\nsolving unpenalized weighted quantile regression problems.\n\nMore specifically, we initialize the algorithm by setting ${\\bolds{\\beta}}=0$\nand ${\\bolds{\\xi}}=0$. Then\nfor each step $t \\geq1$, we update the estimator by\n\n\n\n\n", "itemtype": "equation", "pos": 24539, "prevtext": "\n\nwhere $p_\\lambda(\\cdot)$ is a penalty function with tuning parameter\n$\\lambda$.\n\n\n\n\n\nThe $L_1$ penalty or Lasso [\\citet{T1}] is a popular choice for\npenalized estimation. However,\nthe $L_1$ penalty is known to over-penalize large coefficients, tends\nto be biased\nand requires strong conditions on the design matrix to achieve\nselection consistency. \n\nThis is usually not \na concern for prediction, but can be undesirable if the goal is to\nidentify the underlying model. In comparison, an appropriate nonconvex\npenalty function can effectively overcome this problem [\\citet\n{fanLi}]. In this paper, we consider\ntwo such popular choices of penalty functions: the SCAD [\\citet\n{fanLi}] and MCP [\\citet{Zhang}] penalty functions. For the SCAD\npenalty function,\n\n\\begin{eqnarray*}\np_\\lambda\\bigl({\\vert}\\beta{\\vert}\\bigr) &=& \\lambda{\\vert}\\beta\n{\\vert} I\\bigl(0 \\leq{\\vert}\\beta{\\vert}< \\lambda\\bigr) + \\frac{a\n\\lambda{\\vert} \\beta{\\vert} - (\\beta^2 + \\lambda^2\n)/2}{a-1}I\n\\bigl(\\lambda\\leq{\\vert}\\beta{\\vert}\\leq a\\lambda\\bigr)\n\\\\\n&&{}+ \\frac{ (a + 1)\\lambda^2}{2}I\\bigl({\\vert}\\beta{\\vert}> a \\lambda\n\\bigr)\\qquad\\mbox{for some } a > 2,\n\\end{eqnarray*}\n\nand for the MCP penalty function,\n\n", "index": 17, "text": "\n\\[\np_\\lambda\\bigl({\\vert}\\beta{\\vert}\\bigr) = \\lambda\\biggl( {\\vert}\n\\beta{\\vert}- \\frac{\\beta\n^2}{2a\\lambda} \\biggr) I\\bigl(0 \\leq{\\vert}\\beta{\\vert}< a\n\\lambda\\bigr) + \\frac\n{a\\lambda^2}{2}I \\bigl({\\vert}\\beta{\\vert}\\geq a \\lambda\n\\bigr)\\qquad\\mbox{for some } a > 1.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"p_{\\lambda}\\bigl{(}{|}\\beta{|}\\bigr{)}=\\lambda\\biggl{(}{|}\\beta{|}-\\frac{\\beta%&#10;^{2}}{2a\\lambda}\\biggr{)}I\\bigl{(}0\\leq{|}\\beta{|}&lt;a\\lambda\\bigr{)}+\\frac{a%&#10;\\lambda^{2}}{2}I\\bigl{(}{|}\\beta{|}\\geq a\\lambda\\bigr{)}\\qquad\\mbox{for some }%&#10;a&gt;1.\" display=\"block\"><mrow><msub><mi>p</mi><mi>\u03bb</mi></msub><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mo stretchy=\"false\">|</mo><mi>\u03b2</mi><mo stretchy=\"false\">|</mo><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mi>\u03bb</mi><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mo stretchy=\"false\">|</mo><mi>\u03b2</mi><mo stretchy=\"false\">|</mo><mo>-</mo><mfrac><msup><mi>\u03b2</mi><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mfrac><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mi>I</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mn>0</mn><mo>\u2264</mo><mo stretchy=\"false\">|</mo><mi>\u03b2</mi><mo stretchy=\"false\">|</mo><mo>&lt;</mo><mi>a</mi><mi>\u03bb</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>+</mo><mfrac><mrow><mi>a</mi><mo>\u2062</mo><msup><mi>\u03bb</mi><mn>2</mn></msup></mrow><mn>2</mn></mfrac><mi>I</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mo stretchy=\"false\">|</mo><mi>\u03b2</mi><mo stretchy=\"false\">|</mo><mo>\u2265</mo><mi>a</mi><mi>\u03bb</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>for some\u00a0</mtext><mi>a</mi><mo>&gt;</mo><mn>1</mn><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhere $\\hat{\\beta}_j^{t-1}$ is the value of $\\beta_j$ at step $t-1$.\\vspace*{1pt}\n\nBy observing that we can write ${\\vert} \\beta_j{\\vert} $ as $\\rho_\\tau\n(\\beta_j) +\n\\rho_\\tau(-\\beta_j)$,\nthe above minimization problem\ncan be framed as an unpenalized weighted quantile regression problem\nwith $n+2p_n$\naugmented observations. We denote these\naugmented observations by $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i^*)$, $i=1,\\ldots,\n(n+2p_n)$.\nThe first $n$ observations are those in the original data, that is,\n$(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i^*)=(Y_i, {\\mathbf{x}}_i, {\\mathbf{z}}_i)$,\n$i=1,\\ldots,n$;\nfor the next $p_n$ observations, we have $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i\n^*)=(0,1,0)$, $i=n+1,\\ldots, n+p_n$;\nand the last $p_n$ observations are given by $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i\n^*)=(0,-1,0)$,\n$i=n+p_n+1,\\ldots, n+2p_n$. We fit weighted\nlinear quantile regression model with the observations $(Y_i^*, {\\mathbf{x}}_i\n^*, {\\mathbf{z}}_i^*)$\nand corresponding weights $w_i^{t*}$, where $w_i^{t*}=1$, $i=1,\\ldots,n$;\n$w_{n+j}^{t*}=p'_\\lambda({\\vert} \\hat{\\beta}_j^{t-1}{\\vert} )$,\n$j=1,\\ldots,p_n$;\nand\n$w_{n+p_n+j}^{t*}=-p'_\\lambda({\\vert} \\hat{\\beta}_j^{t-1}{\\vert} )$,\n$j=1,\\ldots,p_n$.\\vspace*{2pt}\n\nThe above new algorithm is simple and convenient, as\nweighted quantile regression can be implemented using many existing\nsoftware packages.\nIn our simulations, we used the quantreg package in R and\ncontinue with the iterative procedure until ${\\Vert} \\hat{{\\bolds{\\beta}}}^t -\n\\hat\n{{\\bolds{\\beta}}}^{t-1}{\\Vert} _1 < 10^{-7}$.\n\n\n\n\\subsection{Asymptotic theory}\\label{sec3.3}\n\nIn addition to Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p},\nwe impose an additional condition on how quickly a nonzero signal can\ndecay, which is\nneeded to identify the underlying model.\n\n\n\n\\begin{condition}[(Condition on the signal)]\\label{cond_small_sig}\n\n\nThere exist positive constants $C_4$ and\n$C_5$ such that $2C_3 < C_4 < 1$ and $n^{(1-C_4)/2} \\mathop{\\min}_{1\n\\leq j \\leq q_n} {\\vert} \\beta_{0j}{\\vert} \\geq C_5$.\n\\end{condition}\n\nDue to the nonsmoothness and nonconvexity of the penalized objective\nfunction $Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$,\nthe classical KKT condition is not applicable to analyzing the\nasymptotic properties of the penalized estimator.\nTo investigate the asymptotic theory of the nonconvex estimator\nfor ultra-high dimensional partially linear additive quantile\nregression model,\nwe explore the necessary condition for the local minimizer\nof a convex differencing problem [\\citet{TaoAn97};\n\\citet{quantUltraHigh}] and extend it to the setting involving\nnonparametric\ncomponents.\n\n\n\n\n\n\n\n\n\n\n\nOur approach concerns a nonconvex\nobjective function that can be expressed as the difference of two\nconvex functions. Specifically, we consider objective functions\nbelonging to the class\n\n", "itemtype": "equation", "pos": 25732, "prevtext": "\n\nFor both penalty functions, the tuning parameter $\\lambda$ controls\nthe complexity\nof the selected model\nand goes to zero as $n$ increases to $\\infty$.\n\n\n\\subsection{Solving the penalized estimator}\\label{sec3.2}\nWe propose an effective algorithm to solve the above penalized\nestimation problem. The algorithm is largely based on\nthe idea of the local linear approximation (LLA) [\\citet{lla}].\nWe employ a new trick based on the observation ${\\vert} \\beta\n_j{\\vert} =\\rho_\\tau\n(\\beta_j) + \\rho_\\tau(-\\beta_j)$\nto transform the approximated objective function to a quantile\nregression objective function based\non an augmented data set, so that the penalized estimator can be\nobtained by iteratively\nsolving unpenalized weighted quantile regression problems.\n\nMore specifically, we initialize the algorithm by setting ${\\bolds{\\beta}}=0$\nand ${\\bolds{\\xi}}=0$. Then\nfor each step $t \\geq1$, we update the estimator by\n\n\n\n\n", "index": 19, "text": "\\begin{equation}\\label{q_scad_step}\n\\bigl(\\hat{{\\bolds{\\beta}}}^t, \\hat{{\\bolds{\\xi}}}^t \\bigr)\n= \\mathop{\\operatorname{argmin}}_{({\\bolds{\\beta}}, {\\bolds{\\xi}})} \\Biggl\\{n^{-1}\\sum\n_{i=1}^n\\rho_\\tau\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}\\bigr)+ \\sum_{j=1}^{p_n}\np'_\\lambda\\bigl(\\bigl{\\vert}\\hat{\\beta}_j^{t-1}\n\\bigr{\\vert}\\bigr){\\vert}\\beta_j{\\vert}\\Biggr\\},\\hspace*{-30pt}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\bigl{(}\\hat{{\\bolds{\\beta}}}^{t},\\hat{{\\bolds{\\xi}}}^{t}\\bigr{)}=\\mathop{%&#10;\\operatorname{argmin}}_{({\\bolds{\\beta}},{\\bolds{\\xi}})}\\Biggl{\\{}n^{-1}\\sum_{%&#10;i=1}^{n}\\rho_{\\tau}\\bigl{(}Y_{i}-{\\mathbf{x}}_{i}^{\\prime}{\\bolds{\\beta}}-%&#10;\\bolds{\\Pi}({\\mathbf{z}}_{i})^{\\prime}{\\bolds{\\xi}}\\bigr{)}+\\sum_{j=1}^{p_{n}}%&#10;p^{\\prime}_{\\lambda}\\bigl{(}\\bigl{|}\\hat{\\beta}_{j}^{t-1}\\bigr{|}\\bigr{)}{|}%&#10;\\beta_{j}{|}\\Biggr{\\}},\\hskip-30.0pt\" display=\"block\"><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msup><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mrow><munder><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>a</mi><mo movablelimits=\"false\">\u2062</mo><mi>r</mi><mo movablelimits=\"false\">\u2062</mo><mi>g</mi><mo movablelimits=\"false\">\u2062</mo><mi>m</mi><mo movablelimits=\"false\">\u2062</mo><mi>i</mi><mo movablelimits=\"false\">\u2062</mo><mi>n</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mrow><mo maxsize=\"260%\" minsize=\"260%\">{</mo><mrow><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>p</mi><mi>n</mi></msub></munderover><mrow><msubsup><mi>p</mi><mi>\u03bb</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><msubsup><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo maxsize=\"260%\" minsize=\"260%\">}</mo></mrow></mrow></mrow><mo rspace=\"0pt\">,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nThis is a very general formulation that incorporates many different forms\nof penalized objective functions.\nThe subdifferential of $k({\\bolds{\\eta}})$ at ${\\bolds{\\eta}}={\\bolds{\\eta}}_0$ is defined as\n\n", "itemtype": "equation", "pos": 29002, "prevtext": "\n\nwhere $\\hat{\\beta}_j^{t-1}$ is the value of $\\beta_j$ at step $t-1$.\\vspace*{1pt}\n\nBy observing that we can write ${\\vert} \\beta_j{\\vert} $ as $\\rho_\\tau\n(\\beta_j) +\n\\rho_\\tau(-\\beta_j)$,\nthe above minimization problem\ncan be framed as an unpenalized weighted quantile regression problem\nwith $n+2p_n$\naugmented observations. We denote these\naugmented observations by $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i^*)$, $i=1,\\ldots,\n(n+2p_n)$.\nThe first $n$ observations are those in the original data, that is,\n$(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i^*)=(Y_i, {\\mathbf{x}}_i, {\\mathbf{z}}_i)$,\n$i=1,\\ldots,n$;\nfor the next $p_n$ observations, we have $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i\n^*)=(0,1,0)$, $i=n+1,\\ldots, n+p_n$;\nand the last $p_n$ observations are given by $(Y_i^*, {\\mathbf{x}}_i^*, {\\mathbf{z}}_i\n^*)=(0,-1,0)$,\n$i=n+p_n+1,\\ldots, n+2p_n$. We fit weighted\nlinear quantile regression model with the observations $(Y_i^*, {\\mathbf{x}}_i\n^*, {\\mathbf{z}}_i^*)$\nand corresponding weights $w_i^{t*}$, where $w_i^{t*}=1$, $i=1,\\ldots,n$;\n$w_{n+j}^{t*}=p'_\\lambda({\\vert} \\hat{\\beta}_j^{t-1}{\\vert} )$,\n$j=1,\\ldots,p_n$;\nand\n$w_{n+p_n+j}^{t*}=-p'_\\lambda({\\vert} \\hat{\\beta}_j^{t-1}{\\vert} )$,\n$j=1,\\ldots,p_n$.\\vspace*{2pt}\n\nThe above new algorithm is simple and convenient, as\nweighted quantile regression can be implemented using many existing\nsoftware packages.\nIn our simulations, we used the quantreg package in R and\ncontinue with the iterative procedure until ${\\Vert} \\hat{{\\bolds{\\beta}}}^t -\n\\hat\n{{\\bolds{\\beta}}}^{t-1}{\\Vert} _1 < 10^{-7}$.\n\n\n\n\\subsection{Asymptotic theory}\\label{sec3.3}\n\nIn addition to Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p},\nwe impose an additional condition on how quickly a nonzero signal can\ndecay, which is\nneeded to identify the underlying model.\n\n\n\n\\begin{condition}[(Condition on the signal)]\\label{cond_small_sig}\n\n\nThere exist positive constants $C_4$ and\n$C_5$ such that $2C_3 < C_4 < 1$ and $n^{(1-C_4)/2} \\mathop{\\min}_{1\n\\leq j \\leq q_n} {\\vert} \\beta_{0j}{\\vert} \\geq C_5$.\n\\end{condition}\n\nDue to the nonsmoothness and nonconvexity of the penalized objective\nfunction $Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$,\nthe classical KKT condition is not applicable to analyzing the\nasymptotic properties of the penalized estimator.\nTo investigate the asymptotic theory of the nonconvex estimator\nfor ultra-high dimensional partially linear additive quantile\nregression model,\nwe explore the necessary condition for the local minimizer\nof a convex differencing problem [\\citet{TaoAn97};\n\\citet{quantUltraHigh}] and extend it to the setting involving\nnonparametric\ncomponents.\n\n\n\n\n\n\n\n\n\n\n\nOur approach concerns a nonconvex\nobjective function that can be expressed as the difference of two\nconvex functions. Specifically, we consider objective functions\nbelonging to the class\n\n", "index": 21, "text": "\n\\[\n\\mathbf{F} = \\bigl\\{ q({\\bolds{\\eta}}): q({\\bolds{\\eta}}) = k({\\bolds{\\eta}}) - l({\\bolds{\\eta}}), k(\\cdot),l(\n\\cdot) \\mbox{ are both convex} \\bigr\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{F}=\\bigl{\\{}q({\\bolds{\\eta}}):q({\\bolds{\\eta}})=k({\\bolds{\\eta}})-l({%&#10;\\bolds{\\eta}}),k(\\cdot),l(\\cdot)\\mbox{ are both convex}\\bigr{\\}}.\" display=\"block\"><mrow><mrow><mi>\ud835\udc05</mi><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:</mo><mrow><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0are both convex</mtext></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nSimilarly, we can define the subdifferential of $l({\\bolds{\\eta}})$.\nLet $\\operatorname{dom}(k) =\n\\{ {\\bolds{\\eta}}: k({\\bolds{\\eta}}) < \\infty\\}$ be the effective domain\nof $k$.\nA necessary condition for ${\\bolds{\\eta}}^*$ to be a local minimizer of\n$q({\\bolds{\\eta}})$ is that\n${\\bolds{\\eta}}^*$ has a neighborhood $U$ such that\n$\\partial l({\\bolds{\\eta}}) \\cap\\partial k({\\bolds{\\eta}}^*) \\neq\\varnothing, \\forall\n{\\bolds{\\eta}}\\in U \\cap\\operatorname{dom}(k)$\n(see Lemma \\ref{lem_diff_convex} in the \\hyperref[appe]{Appendix}).\n\n\n\n\nTo appeal to the above necessary condition for the convex differencing problem,\nit is noted that $Q^P({\\bolds{\\beta}}, {\\bolds{\\xi}})$ can be written as\n\n", "itemtype": "equation", "pos": 29363, "prevtext": "\n\nThis is a very general formulation that incorporates many different forms\nof penalized objective functions.\nThe subdifferential of $k({\\bolds{\\eta}})$ at ${\\bolds{\\eta}}={\\bolds{\\eta}}_0$ is defined as\n\n", "index": 23, "text": "\n\\[\n\\partial k({\\bolds{\\eta}}_0) = \\bigl\\{t: k({\\bolds{\\eta}}) \\geq k(\n{\\bolds{\\eta}}_0) + ({\\bolds{\\eta}}-{\\bolds{\\eta}}_0)'t, \\forall\n{\\bolds{\\eta}}\\bigr\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\partial k({\\bolds{\\eta}}_{0})=\\bigl{\\{}t:k({\\bolds{\\eta}})\\geq k({\\bolds{\\eta%&#10;}}_{0})+({\\bolds{\\eta}}-{\\bolds{\\eta}}_{0})^{\\prime}t,\\forall{\\bolds{\\eta}}%&#10;\\bigr{\\}}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>k</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b7</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mi>t</mi><mo>:</mo><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b7</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b7</mi><mn>0</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo>,</mo><mrow><mo>\u2200</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhere the two convex functions $k({\\bolds{\\beta}},{\\bolds{\\xi}}) =n^{-1} \\sum_{i=1}^n\\rho\n_\\tau(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}-\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}) + \\lambda\\sum_{j=1}^{p_n}\n{\\vert} \\beta_j{\\vert} $, and $l({\\bolds{\\beta}}, {\\bolds{\\xi}}) = \\sum_{j=1}^{p_n}\nL(\\beta_j)$.\nThe specific form of\n$L(\\beta_j)$ depends on the penalty function being used.\nFor the SCAD penalty function,\n\n\\begin{eqnarray*}\nL(\\beta_j) &=& \\bigl[ \\bigl(\\beta_j^2 + 2\n\\lambda{\\vert}\\beta_j{\\vert}+ \\lambda^2 \\bigr)/\n\\bigl(2(a-1)\\bigr) \\bigr]I\\bigl(\\lambda\\leq{\\vert}\\beta_j{\\vert}\\leq\na\\lambda\\bigr)\n\\\\\n&&{}+ \\bigl[ \\lambda{\\vert}\\beta_j{\\vert}- (a+1)\n\\lambda^2/2 \\bigr]I\\bigl({\\vert}\\beta_j{\\vert}> a\\lambda\n\\bigr);\n\\end{eqnarray*}\n\nwhile for the MCP penalty function,\n\n\\begin{eqnarray*}\n&& L(\\beta_j) = \\bigl[ \\beta_j^2 / (2a) \\bigr]I\n\\bigl(0 \\leq{\\vert}\\beta_j{\\vert}< a\\lambda\\bigr) + \\bigl[\n\\lambda{\\vert}\\beta_j{\\vert}- a \\lambda^2 / 2 \\bigr]I\n\\bigl({\\vert}\\beta_j{\\vert}\\geq a \\lambda\\bigr).\n\\end{eqnarray*}\n\n\n\n\nBuilding on the convex differencing structure, we show that\nwith probability approaching one that the oracle estimator\n$(\\hat{{\\bolds{\\beta}}}', \\hat{{\\bolds{\\xi}}}')'$, where\n$\\hat{{\\bolds{\\beta}}}= (\\hat{{\\bolds{\\beta}}}_1', \\mathbf{0}_{p_n-q_n}' )'$,\nis a local minimizer of $Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$.\n\n\n\n\nTo study the necessary optimality condition, we formally define\n$\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ and $\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})$, the\nsubdifferentials of $k({\\bolds{\\beta}}, {\\bolds{\\xi}})$ and $l({\\bolds{\\beta}}, {\\bolds{\\xi}})$, respectively.\nFirst, the function $l({\\bolds{\\beta}}, {\\bolds{\\xi}})$ does not depend on ${\\bolds{\\xi}}$ and\nis differentiable everywhere.\nHence, its subdifferential is simply the regular derivative. For any\nvalue of ${\\bolds{\\beta}}$ and ${\\bolds{\\xi}}$,\n\n\\begin{eqnarray*}\n\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& \\biggl\\{ \\mu = (\\mu_1,\n\\mu_2,\\ldots,\\mu_{p_n+L_n})' \\in\n{\\mathbb}{R}^{p_n+L_n}:\n\\\\\n&&{} \\mu_j = \\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_j}, 1 \\leq j \\leq p_n;\n\\mu_j=0, p_n+1 \\leq j \\leq p_n+L_n\n\\biggr\\}.\n\\end{eqnarray*}\n\n\n\n\nFor $1 \\leq j \\leq p_n$, for the SCAD penalty function,\n\n", "itemtype": "equation", "pos": 30215, "prevtext": "\n\nSimilarly, we can define the subdifferential of $l({\\bolds{\\eta}})$.\nLet $\\operatorname{dom}(k) =\n\\{ {\\bolds{\\eta}}: k({\\bolds{\\eta}}) < \\infty\\}$ be the effective domain\nof $k$.\nA necessary condition for ${\\bolds{\\eta}}^*$ to be a local minimizer of\n$q({\\bolds{\\eta}})$ is that\n${\\bolds{\\eta}}^*$ has a neighborhood $U$ such that\n$\\partial l({\\bolds{\\eta}}) \\cap\\partial k({\\bolds{\\eta}}^*) \\neq\\varnothing, \\forall\n{\\bolds{\\eta}}\\in U \\cap\\operatorname{dom}(k)$\n(see Lemma \\ref{lem_diff_convex} in the \\hyperref[appe]{Appendix}).\n\n\n\n\nTo appeal to the above necessary condition for the convex differencing problem,\nit is noted that $Q^P({\\bolds{\\beta}}, {\\bolds{\\xi}})$ can be written as\n\n", "index": 25, "text": "\n\\[\nQ^P({\\bolds{\\beta}},{\\bolds{\\xi}}) = k({\\bolds{\\beta}},{\\bolds{\\xi}}) - l({\\bolds{\\beta}}, {\\bolds{\\xi}}),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"Q^{P}({\\bolds{\\beta}},{\\bolds{\\xi}})=k({\\bolds{\\beta}},{\\bolds{\\xi}})-l({%&#10;\\bolds{\\beta}},{\\bolds{\\xi}}),\" display=\"block\"><mrow><mrow><mrow><msup><mi>Q</mi><mi>P</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03be</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhile for the MCP penalty function,\n\n", "itemtype": "equation", "pos": 32540, "prevtext": "\n\nwhere the two convex functions $k({\\bolds{\\beta}},{\\bolds{\\xi}}) =n^{-1} \\sum_{i=1}^n\\rho\n_\\tau(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}-\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}) + \\lambda\\sum_{j=1}^{p_n}\n{\\vert} \\beta_j{\\vert} $, and $l({\\bolds{\\beta}}, {\\bolds{\\xi}}) = \\sum_{j=1}^{p_n}\nL(\\beta_j)$.\nThe specific form of\n$L(\\beta_j)$ depends on the penalty function being used.\nFor the SCAD penalty function,\n\n\\begin{eqnarray*}\nL(\\beta_j) &=& \\bigl[ \\bigl(\\beta_j^2 + 2\n\\lambda{\\vert}\\beta_j{\\vert}+ \\lambda^2 \\bigr)/\n\\bigl(2(a-1)\\bigr) \\bigr]I\\bigl(\\lambda\\leq{\\vert}\\beta_j{\\vert}\\leq\na\\lambda\\bigr)\n\\\\\n&&{}+ \\bigl[ \\lambda{\\vert}\\beta_j{\\vert}- (a+1)\n\\lambda^2/2 \\bigr]I\\bigl({\\vert}\\beta_j{\\vert}> a\\lambda\n\\bigr);\n\\end{eqnarray*}\n\nwhile for the MCP penalty function,\n\n\\begin{eqnarray*}\n&& L(\\beta_j) = \\bigl[ \\beta_j^2 / (2a) \\bigr]I\n\\bigl(0 \\leq{\\vert}\\beta_j{\\vert}< a\\lambda\\bigr) + \\bigl[\n\\lambda{\\vert}\\beta_j{\\vert}- a \\lambda^2 / 2 \\bigr]I\n\\bigl({\\vert}\\beta_j{\\vert}\\geq a \\lambda\\bigr).\n\\end{eqnarray*}\n\n\n\n\nBuilding on the convex differencing structure, we show that\nwith probability approaching one that the oracle estimator\n$(\\hat{{\\bolds{\\beta}}}', \\hat{{\\bolds{\\xi}}}')'$, where\n$\\hat{{\\bolds{\\beta}}}= (\\hat{{\\bolds{\\beta}}}_1', \\mathbf{0}_{p_n-q_n}' )'$,\nis a local minimizer of $Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$.\n\n\n\n\nTo study the necessary optimality condition, we formally define\n$\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ and $\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})$, the\nsubdifferentials of $k({\\bolds{\\beta}}, {\\bolds{\\xi}})$ and $l({\\bolds{\\beta}}, {\\bolds{\\xi}})$, respectively.\nFirst, the function $l({\\bolds{\\beta}}, {\\bolds{\\xi}})$ does not depend on ${\\bolds{\\xi}}$ and\nis differentiable everywhere.\nHence, its subdifferential is simply the regular derivative. For any\nvalue of ${\\bolds{\\beta}}$ and ${\\bolds{\\xi}}$,\n\n\\begin{eqnarray*}\n\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& \\biggl\\{ \\mu = (\\mu_1,\n\\mu_2,\\ldots,\\mu_{p_n+L_n})' \\in\n{\\mathbb}{R}^{p_n+L_n}:\n\\\\\n&&{} \\mu_j = \\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_j}, 1 \\leq j \\leq p_n;\n\\mu_j=0, p_n+1 \\leq j \\leq p_n+L_n\n\\biggr\\}.\n\\end{eqnarray*}\n\n\n\n\nFor $1 \\leq j \\leq p_n$, for the SCAD penalty function,\n\n", "index": 27, "text": "\n\\[\n\\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_j} = \\cases{ 0, &\\quad$0 \\leq\n{\\vert}\\beta_j{\\vert}<\n\\lambda$,\n\\vspace*{3pt}\\cr\n\\bigl(\\beta_j-\\lambda\\operatorname{sgn}(\n\\beta_j)\\bigr)/(a-1), &\\quad$\\lambda\\leq{\\vert}\\beta_j\n{\\vert}\\leq a\\lambda$,\n\\vspace*{3pt}\\cr\n\\lambda\\operatorname{sgn}(\\beta_j), &\\quad$\n{\\vert}\\beta_j{\\vert}>a\\lambda$,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_{j}}=\\cases{0,&amp;\\quad$0\\leq{|}%&#10;\\beta_{j}{|}&lt;\\lambda$,\\cr\\bigl{(}\\beta_{j}-\\lambda\\operatorname{sgn}(\\beta_{j}%&#10;)\\bigr{)}/(a-1),&amp;\\quad$\\lambda\\leq{|}\\beta_{j}{|}\\leq a\\lambda$,\\cr\\lambda%&#10;\\operatorname{sgn}(\\beta_{j}),&amp;\\quad${|}\\beta_{j}{|}&gt;a\\lambda$,}\" display=\"block\"><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>l</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow/><mo separator=\"true\">\u2003</mo><mrow><mrow><mn>0</mn><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><mi>\u03bb</mi></mrow><mtext>,</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>\u03b2</mi><mi>j</mi></msub><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow/><mo separator=\"true\">\u2003</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mrow><mtext>,</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow/><mo separator=\"true\">\u2003</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>&gt;</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mrow><mtext>,</mtext></mrow></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nOn the other hand, the function $k({\\bolds{\\beta}}, {\\bolds{\\xi}})$ is not\ndifferentiable everywhere.\n\n\n\n\nIts subdifferential at $({\\bolds{\\beta}},{\\bolds{\\xi}})$ is a collection of $(p_n+L_n)$-vectors:\n\n\\begin{eqnarray*}\n\\partial k({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& \\Biggl\\{ {\\bolds{\\kappa}}= ( \\kappa_1, \\kappa\n_2,\\ldots,\\kappa_{p_n+L_n})' \\in\n{\\mathbb}{R}^{p_n+L_n}:\n\\\\\n\n&&{} \\kappa_j = -\\tau n^{-1}\\sum\n_{i=1}^nx_{ij}I\\bigl(Y_i -\n{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)\n'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} + (1-\\tau) n^{-1}\\sum_{i=1}^nx_{ij}I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- n^{-1}\\sum_{i=1}^nx_{ij}a_i\n+ \\lambda l_j,\\mbox{ for } 1 \\leq j \\leq p_n;\n\\\\\n&&{} \\kappa_j = -\\tau n^{-1}\\sum\n_{i=1}^n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\n\\bigl(Y_i - {\\mathbf{x}}_i '{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} +(1-\\tau)n^{-1}\\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i) I\\bigl(Y_i - {\\mathbf{x}}\n_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{} -n^{-1}\\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i)a_i, \\mbox{ for }\np_n + 1 \\leq j \\leq p_n +L_n \\Biggr\\},\n\\end{eqnarray*}\n\nwhere we write $\\bolds{\\Pi}({\\mathbf{z}}_i)=(1, \\Pi_1({\\mathbf{z}}_i), \\ldots, \\Pi\n_{L_n}({\\mathbf{z}}_i))'$;\n$a_i=0$ if $Y_i-{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}\\neq0$ and\n$a_i \\in[\\tau\n-1,\\tau]$ otherwise;\n\nfor $1 \\leq j \\leq p_n$,\n$l_j = \\operatorname{sgn}(\\beta_j)$ if $\\beta_j \\neq0$ and $l_j \\in[-1,1]$\notherwise.\n\nIn the following, we analyze the subgradient of the unpenalized\nobjective function, which plays an essential\nrole in checking the condition of the optimality condition. The\nsubgradient $s ({\\bolds{\\beta}},{\\bolds{\\xi}}) = (s_1({\\bolds{\\beta}},{\\bolds{\\xi}}\n),\\ldots,s_{p_n}({\\bolds{\\beta}},{\\bolds{\\xi}}),\\ldots,\\break s_{p_n+L_n}({\\bolds{\\beta}},{\\bolds{\\xi}}\n) )'$ is given by\n\n\\begin{eqnarray*}\ns_j({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& -\\frac{\\tau}{n}\\sum\n_{i=1}^nx_{ij}I\\bigl(Y_i -\n{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} + \\frac{1-\\tau}{n} \\sum_{i=1}^nx_{ij}I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i) '{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- \\frac{1}{n} \\sum_{i=1}^nx_{ij}a_i\\qquad\\mbox{for } 1 \\leq j \\leq p_n,\n\\\\\ns_j({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& -\\frac{\\tau}{n}\\sum\n_{i=1}^n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{}+ \\frac{1-\\tau}{n} \\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\\bigl(Y_i - {\\mathbf{x}}\n_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- \\frac{1}{n} \\sum_{i=1}^n\\Pi\n_{j-p_n}({\\mathbf{z}}_i)a_i\\qquad\\mbox{for } p_n\n+ 1 \\leq j \\leq p_n+L_n,\n\\end{eqnarray*}\n\nwhere $a_i$ is defined as before.\nThe following lemma states the behavior of $s_j(\\hat{{\\bolds{\\beta}}},\\hat\n{{\\bolds{\\xi}}})$ when being evaluated\nat the oracle estimator.\n\n\n\n\\begin{lemma}\n\\label{lem_sub_diff_q}\nAssume Conditions \\ref{cond_f}--\\ref{cond_small_sig} are satisfied,\n$\\lambda= o (n^{-(1-C_4)/2} )$,\\break  $n^{-1/2}q_n = o(\\lambda\n)$, $n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$. For\nthe oracle estimator $ (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )$ there\nexists $a_i^*$ with $a_i^*=0$ if $Y_i - {\\mathbf{x}}_i'\\hat{{\\bolds{\\beta}}} - \\bolds\n{\\Pi}({\\mathbf{z}}_i)\n'\\hat{{\\bolds{\\xi}}}\\neq0$ and $a_i^* \\in[\\tau-1,\\tau]$ otherwise, such\nthat for $s_j(\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})$ with $a_i=a_i^*$,\nwith probability approaching one\n\n\n\n\n\n\n\n\\begin{eqnarray}\ns_j (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} ) &=&0,\\qquad j=1,\\ldots,q_n\n\\mbox{ or } j = p_n+1,\\ldots,p_n+L_n,\n\\label\n{lem_sub_diff_q_1}\n\\\\\n{\\vert}\\hat{\\beta}_j{\\vert}&\\geq& (a+1/2)\\lambda,\\qquad j=1,\\ldots\n,q_n, \\label{lem_sub_diff_q_2}\n\\\\\n\\bigl{\\vert} s_j (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )\\bigr{\\vert}&\\leq& c\n\\lambda\\qquad\\forall c>0, j=q_n+1,\\ldots,p_n. \\label{lem_sub_diff_q_3}\n\\end{eqnarray}\n\n\\end{lemma}\n\n\\begin{rem*}\nNote that for $\\kappa_{j} \\in\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ and $l_j$ as\ndefined earlier\n\n\\begin{eqnarray*}\n\\kappa_j &=& s_j({\\bolds{\\beta}},{\\bolds{\\xi}}) + \\lambda\nl_j\\qquad\\mbox{for } 1 \\leq j \\leq p_n\n\\quad\\mbox{and}\n\\\\\n\\kappa_{j} &=& s_{j}({\\bolds{\\beta}},{\\bolds{\\xi}})\\qquad\\mbox{for }\np_n+1 \\leq j \\leq p_n+L_n.\n\\end{eqnarray*}\n\n\n\n\n\n\n\n\nThus, Lemma \\ref{lem_sub_diff_q} provides important insight on the\nasymptotic behavior of ${\\bolds{\\kappa}}\\in\\partial k(\\hat{{\\bolds{\\beta}}},\\hat\n{{\\bolds{\\xi}}})$. Consider a small neighborhood around the oracle estimator\n$ (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )$ with radius $\\lambda/ 2$.\nBuilding on Lemma \\ref{lem_sub_diff_q}, we prove in the \\hyperref[appe]{Appendix} that\nwith probability tending to one, for any $ ({\\bolds{\\beta}}, {\\bolds{\\xi}})\n\\in{\\mathbb}{R}^{p_n+L_n}$\nin this neighborhood, there exists ${\\bolds{\\kappa}}= (\\kappa_1,\\ldots\n,\\kappa_{p_n}, \\mathbf{0}_{L_n}' )' \\in\\partial k(\\hat\n{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})$ such that\n\n\\begin{eqnarray*}\n\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} &=& \\kappa_j,\\qquad j=1,\\ldots\n,p_n\\quad\\mbox{and}\n\\\\\n\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial{\\bolds{\\xi}}_j} &=& \\kappa_{p_n+j},\\qquad\nj=1,\\ldots,L_n.\n\\end{eqnarray*}\n\nThis leads to the main theorem of the paper.\nLet $\\mathcal{E}_n(\\lambda)$ be the set of local minima of\n$Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$. The theorem below shows that with probability\napproaching one, the oracle estimator belongs to the set $\\mathcal\n{E}_n(\\lambda)$.\n\n\n\n\\end{rem*}\n\n\n\n\\begin{theorem}\n\\label{scad_local_min}\nAssume Conditions \\ref{cond_f}--\\ref{cond_small_sig} are satisfied.\n\n\nConsider either the SCAD or the MCP penalty function with tuning\nparameter $\\lambda$. Let $\\hat{{\\bolds{\\eta}}} \\equiv(\\hat{{\\bolds{\\beta}}},\n\\hat{{\\bolds{\\xi}}} )$ be the oracle estimator. If \n\n$\\lambda= o (n^{-(1-C_4)/2} ), n^{-1/2}q_n = o(\\lambda)$,\n$n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$,\nthen\n\n", "itemtype": "equation", "pos": 32927, "prevtext": "\n\nwhile for the MCP penalty function,\n\n", "index": 29, "text": "\n\\[\n\\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_j} = \\cases{ \\beta_j/a, &\\quad\n$0 \\leq{\\vert}\n\\beta_j{\\vert}< a\\lambda$,\n\\vspace*{3pt}\\cr\n\\lambda\\operatorname{sgn}(\n\\beta_j), &\\quad${\\vert}\\beta_j{\\vert}\\geq a\\lambda$.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial l({\\bolds{\\beta}})}{\\partial\\beta_{j}}=\\cases{\\beta_{j}/a,&amp;%&#10;\\quad$0\\leq{|}\\beta_{j}{|}&lt;a\\lambda$,\\cr\\lambda\\operatorname{sgn}(\\beta_{j}),&amp;%&#10;\\quad${|}\\beta_{j}{|}\\geq a\\lambda$.}\" display=\"block\"><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>l</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>\u03b2</mi><mi>j</mi></msub><mo>/</mo><mi>a</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow/><mo separator=\"true\">\u2003</mo><mrow><mrow><mn>0</mn><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mrow><mtext>,</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow/><mo separator=\"true\">\u2003</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2265</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow></mrow><mtext>.</mtext></mrow></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\\end{theorem}\n\n\\begin{rem*}\n\n\n\n\n\n\n\n\nThe conditions for $\\lambda$ in the theorem are satisfied for $\\lambda\n= n^{-1/2+\\delta}$ where $\\delta\\in( \\max(1/(2r+1),C_3), C_4 )$. The\nfastest rate of $p_n$ allowed is\n$p_n = \\operatorname{exp}(n^{\\alpha})$ with $0 < \\alpha< 1/2 + 2\\delta$. Hence,\nwe allow for the ultra-high dimensional setting.\n\\end{rem*}\n\n\\begin{rem*}\nThe selection of the tuning parameter\n$\\lambda$ is important in practice.\nCross-validation is a common approach, but is known to often result in\noverfitting.\n\\citet{Lee}\nrecently proposed high dimensional BIC for linear quantile\nregression when $p$ is much larger than $n$. Motivated\nby their work, we\nchoose $\\lambda$ that minimizes the following high dimensional BIC criterion:\n\n\n\n\\begin{eqnarray}\\label{large_p_bic}\n\\mbox{QBIC}(\\lambda) &=& \\log\\Biggl( \\sum\n_{i=1}^n\\rho_\\tau\\bigl(Y_i\n- {\\mathbf{x}}_i'\\hat{{\\bolds{\\beta}}}_{\\lambda} - \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'\\hat{{\\bolds{\\xi}}}_{\\lambda\n} \\bigr) \\Biggr)\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} +\n\\nu_{\\lambda} \\frac{ \\log(p_n)\\log(\\log\n(n))}{2n}, \n\\end{eqnarray}\n\nwhere $p_n$ is the number of candidate linear covariates and $\\nu\n_\\lambda$ is the degrees of freedom of the\nfitted model, which is the number of interpolated fits for quantile regression.\n\n\n\n\n\n\n\n\n\n\\end{rem*}\n\n\n\\section{Simulation}\\label{sec4}\nWe investigate the performance of the penalized partially linear additive\nquantile regression estimator\nin high dimension. We focus on the SCAD penalty and referred to the new\nprocedure as Q-SCAD.\nAn alternative popular nonconvex penalty function is\nthe MCP penalty [\\citet{Zhang}], the simulation results for which are\nfound to be similar and reported in the online supplementary material\n[\\citet{Supp}].\nThe Q-SCAD is compared with three alternative procedures:\npartially linear additive quantile regression estimator with the LASSO\npenalty (Q-LASSO),\npartially linear additive mean regression with SCAD penalty (LS-SCAD) and\nLASSO penalty (LS-LASSO). It worth noting that for the mean\nregression case,\nthere appears to be no theory in the literature for the ultra-high\ndimensional case.\n\n\nWe first generate $\\tilde{X}=(\\tilde{X}_1,\\ldots, \\tilde{X}_{p+2})'$\nfrom the $N_{p+2}(\\mathbf{0}_{p+2},\\Sigma)$ multivariate normal distribution,\nwhere $\\Sigma= (\\sigma_{jk})_{ (p+2) \\times(p+2)}$ with $\\sigma\n_{jk} = 0.5^{|j-k|}$. Then we set $X_1 = \\sqrt{12}\\Phi(\\tilde{X}_1)$\nwhere $\\Phi(\\cdot)$ is\ndistribution function of $N(0,1)$ distribution\nand $\\sqrt{12}$ scales $X_1$ to have standard deviation one.\nFurthermore, we let $Z_{1} = \\Phi(\\tilde{X}_{25})$, $Z_2 = \\Phi\n(\\tilde{X}_{26})$, $X_i = \\tilde{X}_i$ for $i=2,\\ldots,24$ and $X_i\n= \\tilde{X}_{i-2}$ for $i = 27,\\ldots,p+2$.\nThe random responses are generated from the regression model\n\n\n\n\n", "itemtype": "equation", "pos": 39250, "prevtext": "\n\nOn the other hand, the function $k({\\bolds{\\beta}}, {\\bolds{\\xi}})$ is not\ndifferentiable everywhere.\n\n\n\n\nIts subdifferential at $({\\bolds{\\beta}},{\\bolds{\\xi}})$ is a collection of $(p_n+L_n)$-vectors:\n\n\\begin{eqnarray*}\n\\partial k({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& \\Biggl\\{ {\\bolds{\\kappa}}= ( \\kappa_1, \\kappa\n_2,\\ldots,\\kappa_{p_n+L_n})' \\in\n{\\mathbb}{R}^{p_n+L_n}:\n\\\\\n\n&&{} \\kappa_j = -\\tau n^{-1}\\sum\n_{i=1}^nx_{ij}I\\bigl(Y_i -\n{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)\n'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} + (1-\\tau) n^{-1}\\sum_{i=1}^nx_{ij}I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- n^{-1}\\sum_{i=1}^nx_{ij}a_i\n+ \\lambda l_j,\\mbox{ for } 1 \\leq j \\leq p_n;\n\\\\\n&&{} \\kappa_j = -\\tau n^{-1}\\sum\n_{i=1}^n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\n\\bigl(Y_i - {\\mathbf{x}}_i '{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} +(1-\\tau)n^{-1}\\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i) I\\bigl(Y_i - {\\mathbf{x}}\n_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{} -n^{-1}\\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i)a_i, \\mbox{ for }\np_n + 1 \\leq j \\leq p_n +L_n \\Biggr\\},\n\\end{eqnarray*}\n\nwhere we write $\\bolds{\\Pi}({\\mathbf{z}}_i)=(1, \\Pi_1({\\mathbf{z}}_i), \\ldots, \\Pi\n_{L_n}({\\mathbf{z}}_i))'$;\n$a_i=0$ if $Y_i-{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}\\neq0$ and\n$a_i \\in[\\tau\n-1,\\tau]$ otherwise;\n\nfor $1 \\leq j \\leq p_n$,\n$l_j = \\operatorname{sgn}(\\beta_j)$ if $\\beta_j \\neq0$ and $l_j \\in[-1,1]$\notherwise.\n\nIn the following, we analyze the subgradient of the unpenalized\nobjective function, which plays an essential\nrole in checking the condition of the optimality condition. The\nsubgradient $s ({\\bolds{\\beta}},{\\bolds{\\xi}}) = (s_1({\\bolds{\\beta}},{\\bolds{\\xi}}\n),\\ldots,s_{p_n}({\\bolds{\\beta}},{\\bolds{\\xi}}),\\ldots,\\break s_{p_n+L_n}({\\bolds{\\beta}},{\\bolds{\\xi}}\n) )'$ is given by\n\n\\begin{eqnarray*}\ns_j({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& -\\frac{\\tau}{n}\\sum\n_{i=1}^nx_{ij}I\\bigl(Y_i -\n{\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{} + \\frac{1-\\tau}{n} \\sum_{i=1}^nx_{ij}I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i) '{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- \\frac{1}{n} \\sum_{i=1}^nx_{ij}a_i\\qquad\\mbox{for } 1 \\leq j \\leq p_n,\n\\\\\ns_j({\\bolds{\\beta}}, {\\bolds{\\xi}}) &=& -\\frac{\\tau}{n}\\sum\n_{i=1}^n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\n\\bigl(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'{\\bolds{\\xi}}> 0\\bigr)\n\\\\\n&&{}+ \\frac{1-\\tau}{n} \\sum_{i=1}^n\n\\Pi_{j-p_n}({\\mathbf{z}}_i)I\\bigl(Y_i - {\\mathbf{x}}\n_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}< 0\\bigr)\n\\\\\n&&{}- \\frac{1}{n} \\sum_{i=1}^n\\Pi\n_{j-p_n}({\\mathbf{z}}_i)a_i\\qquad\\mbox{for } p_n\n+ 1 \\leq j \\leq p_n+L_n,\n\\end{eqnarray*}\n\nwhere $a_i$ is defined as before.\nThe following lemma states the behavior of $s_j(\\hat{{\\bolds{\\beta}}},\\hat\n{{\\bolds{\\xi}}})$ when being evaluated\nat the oracle estimator.\n\n\n\n\\begin{lemma}\n\\label{lem_sub_diff_q}\nAssume Conditions \\ref{cond_f}--\\ref{cond_small_sig} are satisfied,\n$\\lambda= o (n^{-(1-C_4)/2} )$,\\break  $n^{-1/2}q_n = o(\\lambda\n)$, $n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$. For\nthe oracle estimator $ (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )$ there\nexists $a_i^*$ with $a_i^*=0$ if $Y_i - {\\mathbf{x}}_i'\\hat{{\\bolds{\\beta}}} - \\bolds\n{\\Pi}({\\mathbf{z}}_i)\n'\\hat{{\\bolds{\\xi}}}\\neq0$ and $a_i^* \\in[\\tau-1,\\tau]$ otherwise, such\nthat for $s_j(\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})$ with $a_i=a_i^*$,\nwith probability approaching one\n\n\n\n\n\n\n\n\\begin{eqnarray}\ns_j (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} ) &=&0,\\qquad j=1,\\ldots,q_n\n\\mbox{ or } j = p_n+1,\\ldots,p_n+L_n,\n\\label\n{lem_sub_diff_q_1}\n\\\\\n{\\vert}\\hat{\\beta}_j{\\vert}&\\geq& (a+1/2)\\lambda,\\qquad j=1,\\ldots\n,q_n, \\label{lem_sub_diff_q_2}\n\\\\\n\\bigl{\\vert} s_j (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )\\bigr{\\vert}&\\leq& c\n\\lambda\\qquad\\forall c>0, j=q_n+1,\\ldots,p_n. \\label{lem_sub_diff_q_3}\n\\end{eqnarray}\n\n\\end{lemma}\n\n\\begin{rem*}\nNote that for $\\kappa_{j} \\in\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ and $l_j$ as\ndefined earlier\n\n\\begin{eqnarray*}\n\\kappa_j &=& s_j({\\bolds{\\beta}},{\\bolds{\\xi}}) + \\lambda\nl_j\\qquad\\mbox{for } 1 \\leq j \\leq p_n\n\\quad\\mbox{and}\n\\\\\n\\kappa_{j} &=& s_{j}({\\bolds{\\beta}},{\\bolds{\\xi}})\\qquad\\mbox{for }\np_n+1 \\leq j \\leq p_n+L_n.\n\\end{eqnarray*}\n\n\n\n\n\n\n\n\nThus, Lemma \\ref{lem_sub_diff_q} provides important insight on the\nasymptotic behavior of ${\\bolds{\\kappa}}\\in\\partial k(\\hat{{\\bolds{\\beta}}},\\hat\n{{\\bolds{\\xi}}})$. Consider a small neighborhood around the oracle estimator\n$ (\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}} )$ with radius $\\lambda/ 2$.\nBuilding on Lemma \\ref{lem_sub_diff_q}, we prove in the \\hyperref[appe]{Appendix} that\nwith probability tending to one, for any $ ({\\bolds{\\beta}}, {\\bolds{\\xi}})\n\\in{\\mathbb}{R}^{p_n+L_n}$\nin this neighborhood, there exists ${\\bolds{\\kappa}}= (\\kappa_1,\\ldots\n,\\kappa_{p_n}, \\mathbf{0}_{L_n}' )' \\in\\partial k(\\hat\n{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})$ such that\n\n\\begin{eqnarray*}\n\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} &=& \\kappa_j,\\qquad j=1,\\ldots\n,p_n\\quad\\mbox{and}\n\\\\\n\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial{\\bolds{\\xi}}_j} &=& \\kappa_{p_n+j},\\qquad\nj=1,\\ldots,L_n.\n\\end{eqnarray*}\n\nThis leads to the main theorem of the paper.\nLet $\\mathcal{E}_n(\\lambda)$ be the set of local minima of\n$Q^P({\\bolds{\\beta}},{\\bolds{\\xi}})$. The theorem below shows that with probability\napproaching one, the oracle estimator belongs to the set $\\mathcal\n{E}_n(\\lambda)$.\n\n\n\n\\end{rem*}\n\n\n\n\\begin{theorem}\n\\label{scad_local_min}\nAssume Conditions \\ref{cond_f}--\\ref{cond_small_sig} are satisfied.\n\n\nConsider either the SCAD or the MCP penalty function with tuning\nparameter $\\lambda$. Let $\\hat{{\\bolds{\\eta}}} \\equiv(\\hat{{\\bolds{\\beta}}},\n\\hat{{\\bolds{\\xi}}} )$ be the oracle estimator. If \n\n$\\lambda= o (n^{-(1-C_4)/2} ), n^{-1/2}q_n = o(\\lambda)$,\n$n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$,\nthen\n\n", "index": 31, "text": "\n\\[\nP \\bigl(\\hat{{\\bolds{\\eta}}} \\in\\mathcal{E}_n(\\lambda) \\bigr) \\rightarrow1\n\\qquad\\mbox{as } n\\rightarrow\\infty.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"P\\bigl{(}\\hat{{\\bolds{\\eta}}}\\in\\mathcal{E}_{n}(\\lambda)\\bigr{)}\\rightarrow 1%&#10;\\qquad\\mbox{as }n\\rightarrow\\infty.\" display=\"block\"><mrow><mi>P</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">^</mo></mover><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2192</mo><mn>1</mn><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>as\u00a0</mtext><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nwhere $\\beta_j \\sim U[0.5,1.5]$ for $1 \\leq j \\leq4$. We consider\nthree different distributions of the error term $\\varepsilon_i$: (1)\nstandard normal distribution; (2) $t$ distribution with 3 degrees of\nfreedom; and (3) heteroscedastic\nnormal distribution $\\varepsilon_i = \\tilde{X}_{i1}\\zeta_i$ where\n$\\zeta_i \\sim N(0,\\sigma=0.7)$\nare independent of the $X_i$'s. \n\n\n\nWe perform 100 simulations for each setting with sample size $n=300$,\nand $p=100$, $300$, $600$. \n\n\nResults for additional simulations with sample sizes of $50$, 100 and\n200 are provided in the online supplementary material [\\citet\n{Supp}]. For the heteroscedastic error case, we model $\\tau= 0.7$\nand $0.9$; otherwise, we model the conditional median.\nNote that at $\\tau= 0.7$ or 0.9, when the error has the\naforementioned heteroscedastic distribution, $X_1$ is part of the true\nmodel. At these two quantiles, the true model consists of 5 linear\ncovariates. In all simulations, the number of basis functions is set to three,\nwhich we find to work satisfactorily in a variety of settings. For the\nLASSO method, we select the tuning parameters $\\lambda$\n\nby using five-fold cross validation. For the Q-SCAD model, we select\n$\\lambda$ that minimizes (\\ref{large_p_bic}) while for LS-SCAD we use\na least squares equivalent. The tuning parameter $a$ in the SCAD\npenalty function\nis set to 3.7 as recommended in \\citet{fanLi}. To assess the\nperformance of different methods, we adopt the following criteria:\n\n\\begin{longlist}\n\n\n\n\\item[1.] False Variables (FV): average number of nonzero linear covariates\nincorrectly included in the model.\n\n\\item[2.] True Variables (TV): \naverage number of nonzero linear covariates correctly included in the model.\n\n\\item[3.] True: proportion of times the true model is exactly identified.\n\n\\item[4.] P: proportion of times $X_{1}$ is selected.\n\n\\item[5.] AADE: average of the \\textit{average absolute deviation} (ADE) of\nthe fit of the nonlinear components,\nwhere the ADE is defined as $n^{-1}\\sum_{i=1}^n|\\hat{g}({\\mathbf{z}}_i) -\ng_0({\\mathbf{z}}_i)|$.\n\n\\item[6.] MSE: average of the mean squared error for estimating ${\\bolds{\\beta}}\n_0$, that is, the average of ${\\Vert} \\hat{{\\bolds{\\beta}}}-{\\bolds{\\beta}}_0{\\Vert} ^2$\nacross all simulation runs.\n\\end{longlist}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.5$) and mean regression\nusing SCAD and LASSO penalty functions for $\\varepsilon\\sim N(0,1)$}\\label{tab1}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.20 & 4.00 & 0.88 & 0.00 & 0.16 & 0.03 \\\\\n\nQ-LASSO & 300 & 100 & 12.88 & 4.00 & 0.00 & 0.13 & 0.16 & 0.13 \\\\\n\nLS-SCAD & 300 & 100 & 0.32 & 4.00 & 0.85 & 0.00 & 0.13 & 0.02 \\\\\n\nLS-LASSO & 300 & 100 & 11.63 & 4.00 & 0.00 & 0.12 & 0.13 & 0.07\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.04 & 4.00 & 0.96 & 0.00 & 0.15 & 0.02 \\\\\n\nQ-LASSO & 300 & 300 & 15.93 & 4.00 & 0.00 & 0.07 & 0.16 & 0.14 \\\\\n\nLS-SCAD & 300 & 300 & 0.33 & 4.00 & 0.78 & 0.00 & 0.12 & 0.02 \\\\\n\nLS-LASSO & 300 & 300 & 15.00 & 4.00 & 0.00 & 0.04 & 0.13 & 0.09\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.06 & 4.00 & 0.94 & 0.00 & 0.15 & 0.02 \\\\\n\nQ-LASSO & 300 & 600 & 21.86 & 4.00 & 0.01 & 0.06 & 0.16 & 0.16 \\\\\n\nLS-SCAD & 300 & 600 & 2.57 & 4.00 & 0.69 & 0.01 & 0.13 & 0.06 \\\\\n\nLS-LASSO & 300 & 600 & 17.11 & 4.00 & 0.00 & 0.04 & 0.13 & 0.09 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.5$) and mean regression\nusing SCAD and LASSO penalty functions for $\\varepsilon\\sim T_3$}\\label{tab2}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.07 & 4.00 & 0.95 & 0.00 & 0.16 & 0.03 \\\\\n\nQ-LASSO & 300 & 100 & 13.09 & 4.00 & 0.01 & 0.17 & 0.17 & 0.15 \\\\\n\nLS-SCAD & 300 & 100 & 1.08 & 3.99 & 0.45 & 0.02 & 0.19 & 0.11 \\\\\n\nLS-LASSO & 300 & 100 & 10.15 & 3.94 & 0.02 & 0.08 & 0.19 & 0.31\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.05 & 4.00 & 0.97 & 0.00 & 0.17 & 0.03 \\\\\n\nQ-LASSO & 300 & 300 & 18.42 & 4.00 & 0.00 & 0.08 & 0.18 & 0.18 \\\\\n\nLS-SCAD & 300 & 300 & 1.22 & 4.00 & 0.46 & 0.00 & 0.20 & 0.11 \\\\\n\nLS-LASSO & 300 & 300 & 15.15 & 3.99 & 0.01 & 0.08 & 0.21 & 0.26\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.06 & 3.98 & 0.94 & 0.00 & 0.16 & 0.04 \\\\\n\nQ-LASSO & 300 & 600 & 20.81 & 4.00 & 0.01 & 0.03 & 0.18 & 0.23 \\\\\n\nLS-SCAD & 300 & 600 & 1.33 & 4.00 & 0.45 & 0.00 & 0.19 & 0.14 \\\\\n\nLS-LASSO & 300 & 600 & 17.40 & 4.00 & 0.01 & 0.01 & 0.20 & 0.28 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.7$) and mean regression\nusing SCAD and LASSO penalty functions for heteroscedastic errors}\\label{tab3}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.21 & 4.84 & 0.70 & 0.84 & 0.17 & 0.05 \\\\\n\nQ-LASSO & 300 & 100 & 13.86 & 4.97 & 0.00 & 0.97 & 0.24 & 0.15 \\\\\n\nLS-SCAD & 300 & 100 & 1.09 & 4.06 & 0.01 & 0.06 & 0.16 & 0.69 \\\\\n\nLS-LASSO & 300 & 100 & 11.48 & 4.13 & 0.00 & 0.13 & 0.17 & 0.78\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.20 & 4.77 & 0.61 & 0.77 & 0.20 & 0.06 \\\\\n\nQ-LASSO & 300 & 300 & 18.54 & 4.97 & 0.00 & 0.97 & 0.27 & 0.18 \\\\\n\nLS-SCAD & 300 & 300 & 3.28 & 4.00 & 0.00 & 0.00 & 0.16 & 0.68 \\\\\n\nLS-LASSO & 300 & 300 & 15.85 & 4.08 & 0.00 & 0.08 & 0.16 & 0.79\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.16 & 4.59 & 0.48 & 0.59 & 0.26 & 0.08 \\\\\n\nQ-LASSO & 300 & 600 & 23.26 & 4.89 & 0.00 & 0.89 & 0.31 & 0.24 \\\\\nL\nLS-SCAD & 300 & 600 & 6.31 & 4.02 & 0.00 & 0.02 & 0.16 & 0.69 \\\\\n\nLS-LASSO & 300 & 600 & 18.50 & 4.09 & 0.00 & 0.09 & 0.16 & 0.83 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.9$) and mean regression\nusing SCAD and LASSO penalty functions for heteroscedastic errors}\\label{tab4}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.06 & 4.93 & 0.91 & 0.98 & 0.24 & 0.30 \\\\\n\nQ-LASSO & 300 & 100 & 12.94 & 5.00 & 0.00 & 1.00 & 0.49 & 0.73 \\\\\n\nLS-SCAD & 300 & 100 & 1.09 & 4.06 & 0.01 & 0.06 & 0.16 & 4.72 \\\\\n\nLS-LASSO & 300 & 100 & 11.48 & 4.13 & 0.00 & 0.13 & 0.17 & 4.73\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.26 & 5.00 & 0.81 & 1.00 & 0.19 & 0.24 \\\\\n\nQ-LASSO & 300 & 300 & 16.33 & 5.00 & 0.00 & 1.00 & 0.62 & 0.92 \\\\\n\nLS-SCAD & 300 & 300 & 3.28 & 4.00 & 0.00 & 0.00 & 0.16 & 4.63 \\\\\n\nLS-LASSO & 300 & 300 & 15.85 & 4.08 & 0.00 & 0.08 & 0.16 & 4.67\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.34 & 4.94 & 0.77 & 1.00 & 0.21 & 0.29 \\\\\n\nQ-LASSO & 300 & 600 & 19.79 & 4.97 & 0.00 & 1.00 & 0.74 & 1.15 \\\\\n\nLS-SCAD & 300 & 600 & 6.31 & 4.02 & 0.00 & 0.02 & 0.16 & 4.64 \\\\\n\nLS-LASSO & 300 & 600 & 18.50 & 4.09 & 0.00 & 0.09 & 0.16 & 4.74 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\nThe simulation results are summarized in Tables~\\ref{tab1}--\\ref{tab4}.\nTables \\ref{tab1}~and~\\ref{tab2} correspond to $\\tau= 0.5$, $N(0,1)$ and $T_3$ error\ndistribution, respectively.\nTables \\ref{tab3}~and~\\ref{tab4} are for the heteroscedastic error, $\\tau= 0.7$ and\n$0.9$, respectively. Least squares based estimates of $\\hat{{\\bolds{\\beta}}}$\nfor $\\tau=0.7$ or $0.9$ are obtained by assuming $\\varepsilon_i \\sim\nN(0,\\sigma)$, with estimates of $\\sigma$ being used in each\nsimulation. An extension of Table~\\ref{tab3} for $p = 1200$ and 2400 is included\nin the online supplementary material [\\citet{Supp}].\n\n\n\n\nWe observe that the method with the SCAD penalty tends to pick a\nsmaller and more accurate model. The advantages of quantile regression\ncan be seen by its stronger performance\nat the presence of heavy-tailed distribution or heteroscedastic errors.\nFor the latter case,\nthe least squared based methods perform poorly in identifying the\nactive variables in the\ndispersion function. \n\n\n\n\nEstimation of the nonlinear terms is similar across different error\ndistributions and\ndifferent values of~$p$.\n\n\n\n\n\n\\section{An application to birth weight data}\\label{sec5}\n\nVotavova et~al. (\\citeyear{AppliedExample}) collected blood samples\nfrom peripheral blood,\ncord blood and the placenta from 20 pregnant smokers and 52 pregnant\nwomen without significant exposure to smoking. Their main objective was\nto identify the difference in transcriptome alterations between the two\ngroups. Birth weight of the baby (in kilograms) was recorded along with\nage of the mother, gestational age, parity, measurement of the amount\nof cotinine, a chemical found in tobacco, in the blood and mother's\nBMI. Low birth weight is known to be associated with both short-term\nand long-term health complications. Scientists are interested in which\ngenes are associated with low birth weight\n[\\citet{birthWt2}].\n\n\n\nWe consider modeling the 0.1, 0.3 and 0.5 conditional quantiles of\ninfant birth weight.\nWe use the genetic data from the peripheral blood sample which include\n64 subjects after dropping those with incomplete information. The blood\nsamples were assayed using HumanRef-8 v3 Expression BeadChips with\n24,539 probes. For each quantile, the top 200 probes are selected using\nthe quantile-adaptive screening method [\\citet{heWang}]. The\ngene expression values of the 200 probes are included as linear\ncovariates for the semiparametric quantile regression model. The\nclinical variables parity, gestational age, cotinine level and BMI\nare also included as linear covariates. The age of the mother is\nmodeled nonparametrically as exploratory analysis reveals potential\nnonlinear effect.\n\n\n\nWe consider the semiparametric quantile regression model with the SCAD\nand LASSO penalty functions.\n\nLeast squares based semiparametric models with the SCAD and LASSO\npenalty functions are also considered. Results for the MCP penalty are\nreported in the online supplementary material [\\citet{Supp}].\n\nThe tuning parameter $\\lambda$ is selected by minimizing (\\ref\n{large_p_bic}) for the SCAD estimator and by five-fold cross validation\nfor LASSO as discussed in Section~\\ref{sec4}. The third column of\nTable~\\ref{applied_models} reports the number of nonzero elements, ``Original\nNZ,'' for each model. As expected, the LASSO method selects a larger\nmodel than the SCAD penalty does. The number of nonzero variables\nvaries with the quantile level, providing evidence that mean regression\nalone would provide a limited view of the conditional distribution.\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\n\\caption{Quantile ($\\tau=0.1$, 0.3 and 0.5) and mean regression\nanalysis of birth weight based on the original data and the random partitioned data}\\label{tab5}\\label{applied_models}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lcd{2.0}cd{2.2}@{}}\n\\hline\n$\\bolds{\\tau}$ & \\textbf{Method} & \\multicolumn{1}{c}{\\textbf{Original NZ}} & \\textbf{Prediction error} & \\multicolumn{1}{c@{}}{\\textbf{Randomized NZ}}\\\\\n\\hline\n0.10 & Q-SCAD & 2 & 0.07 (0.03) & 2.27 \\\\\n0.10 & Q-LASSO & 10 & 0.08 (0.02) & 3.09\n\\[3pt]\n0.30 & Q-SCAD & 7 & 0.18 (0.04) & 6.74 \\\\\n0.30 & Q-LASSO & 22 & 0.16 (0.03) & 12.39\n\\[3pt]\n0.50 & Q-SCAD & 5 & 0.21 (0.04) & 5.80 \\\\\n0.50 & Q-LASSO & 6 & 0.20 (0.04) & 14.25\n\\[3pt]\nMean & LS-SCAD &12 & 0.20 (0.04) & 5.43 \\\\\nMean & LS-LASSO & 12 & 0.20 (0.04) & 3.77 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\nNext, we compare different models on 100 random partitions of the data set.\nFor each partition, we randomly select 50 subjects for the training\ndata and 14 subjects for the test data.\nThe fourth column of Table~\\ref{applied_models} reports the prediction\nerror evaluated on the test data, defined as\n$14^{-1}\\sum_{i=1}^{14} \\rho_\\tau(Y_i - \\hat{Y}_i)$; while the\nfifth column reports the average number of linear covariates included\nin each model (denoted by ``Randomized NZ''). Standard errors for the\nprediction error is reported in parentheses. We note that the SCAD\nmethod produces notably smaller models than the Lasso method does\nwithout sacrificing much prediction accuracy.\n\n\n\n\\begin{figure}[b]\n\n\\includegraphics{1367f01.eps}\n\n\\caption{Lack-of-fit diagnostic QQ plot for the birth weight data\nexample.}\\label{fig:qq_diag}\\label{fig1}\n\\end{figure}\n\nModel checking in high dimension is challenging. In the following, we\nconsider a simulation-based diagnostic plot to help visually assess the\noverall lack-of-fit for the quantile regression model\n[Wei and He (\\citeyear{WH06})] to assess the overall lack-of-fit for the quantile regression\nmodel. First, we randomly generate $\\tilde{\\tau}$ from the uniform\n$[0,1]$ distribution. Then we fit the proposed semiparametric quantile\nregression model using the SCAD penalty for the quantile $\\tilde{\\tau\n}$. Next, we generate a response variable $\\tilde{Y} = {\\mathbf{x}}'\\hat\n{{\\bolds{\\beta}}}(\\tilde{\\tau}) + \\hat{g}(z,\\tilde{\\tau})$,\nwhere $({\\mathbf{x}},z)$ is randomly sampled from the set of observed\ncovariates, with $z$ denoting mother's age and\n${\\mathbf{x}}$ denoting the vector of other covariates. The process is repeated\n100 times and produces a sample of 100 simulated birth weights based on\nthe model. Figure~\\ref{fig1} shows the QQ plot comparing the simulated and\nobserved birth weights. Overall, the QQ plot is close to the 45 degree\nline and does not suggest gross lack-of-fit. Figure~\\ref{fig2} displays the\nestimated nonlinear effects of mother's age $\\hat{g}(z)$ at the three\nquantiles [standardized to satisfy the constraint $\\sum_{i=1}^n \\hat\n{g}(z_i) = 0$]. At the 0.1 and 0.3 quantiles, the estimated mother's\nage effects are similar except for some deviations at the tails of the\nmother's age distribution. At these two quantiles, after age 30,\nmother's age is observed to have a positive effect. The effect of\nmother's age at the median is nonmonotone: the effect is first\nincreasing (up to age 25), then decreasing (to about age 33), and\nincreasing again.\n\n\n\n\n\nWe observe that different models are often selected for different\nrandom partitions. Table~\\ref{tab6} summarizes the variables selected by Q-SCAD\nfor $\\tau= 0.1$, 0.3 and 0.5 and the frequency these variables are\nselected in the 100 random partitions. Probes are listed by their\nidentification number along with corresponding gene in parentheses. The\nSCAD models tend to produce sparser models while the LASSO models\nprovide slightly better predictive performance.\n\n\n\n\\begin{figure}[t]\n\n\\includegraphics{1367f02.eps}\n\n\\caption{Estimated nonlinear effects of mother's age (denoted by $z$)\nat three different quantiles.}\\label{fig:nl_plots}\\label{fig2}\n\\end{figure}\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Frequency of covariates selected at three quantiles among 100 random partitions}\\label{table_partition_selections}\\label{tab6}\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lclclc@{}}\n\\hline\n\\multicolumn{2}{@{}c}{\\textbf{Q-SCAD 0.1}} &\n\\multicolumn{2}{c}{\\textbf{Q-SCAD 0.3}} &\n\\multicolumn{2}{c@{}}{\\textbf{Q-SCAD 0.5}} \\[-6pt]\n\\multicolumn{2}{@{}c}{\\hrulefill} &\n\\multicolumn{2}{c}{\\hrulefill} &\n\\multicolumn{2}{c@{}}{\\hrulefill} \\\\\n & \\textbf{Fre-} &  & \\textbf{Fre-} &  & \\textbf{Fre-} \\\\\n\\textbf{Covariate} & \\textbf{quency} & \\multicolumn{1}{c}{\\textbf{Covariate}} & \\textbf{quency} & \\multicolumn{1}{c}{\\textbf{Covariate}} & \\textbf{quency} \\\\\n\\hline\nGestational age & 82 & Gestational age & 86 & Gestational age & 69 \\\\\n1,687,073 (SOGA1) & 24 & 1,804,451 (LEO1) & 33 & 2,334,204 (ERCC6L) & 57 \\\\\n& & 1,755,657 (RASIP1) & 27 & 1,732,467 (OR2AG1) & 52 \\\\\n& & 1,658,821 (SAMD1) & 23 & 1,656,361 (LOC201175) & 31 \\\\\n& & 2,059,464 (OR5P2) & 14 & 1,747,184 (PUS7L) & \\phantom{0}5 \\\\\n& & 2,148,497 (C20orf107) & \\phantom{0}6 & & \\\\\n& & 2,280,960 (DEPDC7) & \\phantom{0}3 & &\\\\\n\\hline\n\\end{tabular*}\\vspace*{-3pt}\n\\end{table}\n\nGestational age is identified to be important with high frequency at\nall three quantiles under consideration.\nThis is not surprising given the known important relationship between\nbirth weight and\ngestational age. Premature birth is often strongly associated with low\nbirth weight.\nThe genes selected at the three different quantiles are not\noverlapping. This is an\nindication of the heterogeneity in the data. The variation in frequency\nis likely\ndue to the relatively small sample size.\n\n\n\n\n\n\n\nHowever, examining the selected genes does provide some interesting\ninsights. The gene SOGA1 is a suppressor of glucose, which is\ninteresting because maternal gestational diabetes is known to have a\nsignificant effect on birth weight [\\citet{gestDiab}]. The genes\nOR2AG1, OR5P2 and DEPDC7 are all located on chromosome 11,\nthe chromosome with the most selected genes. Chromosome 11 also\ncontains PHLDA2, a gene that has been reported to be highly expressed\nin mothers that have children with lower birth weight [\\citet{phlda2}].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Estimation and variable selection for multiple quantiles}\\label{sec6}\nMotivated by referees' suggestions, we consider an extension for\nsimultaneous variable selection at multiple quantiles. Let $\\tau_1 <\n\\tau_2 < \\cdots< \\tau_M$ be the set of quantiles of interest, where\n$M > 0$ is a positive integer. We assume that\n\n\n\n\n", "itemtype": "equation", "pos": 42144, "prevtext": "\n\n\\end{theorem}\n\n\\begin{rem*}\n\n\n\n\n\n\n\n\nThe conditions for $\\lambda$ in the theorem are satisfied for $\\lambda\n= n^{-1/2+\\delta}$ where $\\delta\\in( \\max(1/(2r+1),C_3), C_4 )$. The\nfastest rate of $p_n$ allowed is\n$p_n = \\operatorname{exp}(n^{\\alpha})$ with $0 < \\alpha< 1/2 + 2\\delta$. Hence,\nwe allow for the ultra-high dimensional setting.\n\\end{rem*}\n\n\\begin{rem*}\nThe selection of the tuning parameter\n$\\lambda$ is important in practice.\nCross-validation is a common approach, but is known to often result in\noverfitting.\n\\citet{Lee}\nrecently proposed high dimensional BIC for linear quantile\nregression when $p$ is much larger than $n$. Motivated\nby their work, we\nchoose $\\lambda$ that minimizes the following high dimensional BIC criterion:\n\n\n\n\\begin{eqnarray}\\label{large_p_bic}\n\\mbox{QBIC}(\\lambda) &=& \\log\\Biggl( \\sum\n_{i=1}^n\\rho_\\tau\\bigl(Y_i\n- {\\mathbf{x}}_i'\\hat{{\\bolds{\\beta}}}_{\\lambda} - \\bolds{\\Pi}(\n{\\mathbf{z}}_i)'\\hat{{\\bolds{\\xi}}}_{\\lambda\n} \\bigr) \\Biggr)\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} +\n\\nu_{\\lambda} \\frac{ \\log(p_n)\\log(\\log\n(n))}{2n}, \n\\end{eqnarray}\n\nwhere $p_n$ is the number of candidate linear covariates and $\\nu\n_\\lambda$ is the degrees of freedom of the\nfitted model, which is the number of interpolated fits for quantile regression.\n\n\n\n\n\n\n\n\n\n\\end{rem*}\n\n\n\\section{Simulation}\\label{sec4}\nWe investigate the performance of the penalized partially linear additive\nquantile regression estimator\nin high dimension. We focus on the SCAD penalty and referred to the new\nprocedure as Q-SCAD.\nAn alternative popular nonconvex penalty function is\nthe MCP penalty [\\citet{Zhang}], the simulation results for which are\nfound to be similar and reported in the online supplementary material\n[\\citet{Supp}].\nThe Q-SCAD is compared with three alternative procedures:\npartially linear additive quantile regression estimator with the LASSO\npenalty (Q-LASSO),\npartially linear additive mean regression with SCAD penalty (LS-SCAD) and\nLASSO penalty (LS-LASSO). It worth noting that for the mean\nregression case,\nthere appears to be no theory in the literature for the ultra-high\ndimensional case.\n\n\nWe first generate $\\tilde{X}=(\\tilde{X}_1,\\ldots, \\tilde{X}_{p+2})'$\nfrom the $N_{p+2}(\\mathbf{0}_{p+2},\\Sigma)$ multivariate normal distribution,\nwhere $\\Sigma= (\\sigma_{jk})_{ (p+2) \\times(p+2)}$ with $\\sigma\n_{jk} = 0.5^{|j-k|}$. Then we set $X_1 = \\sqrt{12}\\Phi(\\tilde{X}_1)$\nwhere $\\Phi(\\cdot)$ is\ndistribution function of $N(0,1)$ distribution\nand $\\sqrt{12}$ scales $X_1$ to have standard deviation one.\nFurthermore, we let $Z_{1} = \\Phi(\\tilde{X}_{25})$, $Z_2 = \\Phi\n(\\tilde{X}_{26})$, $X_i = \\tilde{X}_i$ for $i=2,\\ldots,24$ and $X_i\n= \\tilde{X}_{i-2}$ for $i = 27,\\ldots,p+2$.\nThe random responses are generated from the regression model\n\n\n\n\n", "index": 33, "text": "\\begin{equation}\nY_i = X_{i6}\\beta_1 + X_{i12}\n\\beta_2 + X_{i15}\\beta_3 + X_{i20}\n\\beta_4 + \\sin(2 \\pi Z_{i1}) + Z_{i2}^3\n+ \\varepsilon_i, \n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"Y_{i}=X_{i6}\\beta_{1}+X_{i12}\\beta_{2}+X_{i15}\\beta_{3}+X_{i20}\\beta_{4}+\\sin(%&#10;2\\pi Z_{i1})+Z_{i2}^{3}+\\varepsilon_{i},\\par&#10;\" display=\"block\"><mrow><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>6</mn></mrow></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>12</mn></mrow></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>15</mn></mrow></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>3</mn></msub></mrow><mo>+</mo><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>20</mn></mrow></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mn>4</mn></msub></mrow><mo>+</mo><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>2</mn></mrow><mn>3</mn></msubsup><mo>+</mo><msub><mi>\u03b5</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\n\n\n\\noindent  where $g_0^{(m)}({\\mathbf{z}}_i) = g_{00}^{(m)} + \\sum_{j=1}^d\ng_{0j}^{(m)}(z_{ij})$, with $g_{00}^{(m)} \\in\\mathcal{R}$. We assume\nthat functions $g_{0j}^{(m)}$ satisfy $E\n[g_{0j}^{(m)}(z_{ij}) ]=0$ for the purpose of identification. The\nnonlinear functions\nare allowed to vary with the quantiles. We are interested in the high\ndimensional case where\nmost of the linear covariates have zero coefficients across all $M$\nquantiles, for which group selection will help us combine information\nacross quantiles.\n\n\n\n\n\nWe write ${\\bolds{\\beta}}_0^{(m)} = (\\beta_{01}^{(m)},\\beta_{02}^{(m)},\n\\ldots, \\beta_{0p_n}^{(m)} )'$, $m = 1,\\ldots,M$.\nLet $\\bar{{\\bolds{\\beta}}}^{0j}$ be the \\mbox{$M$-}vec\\-tor $ (\\beta\n_{0j}^{(1)},\\ldots, \\beta_{0j}^{(M)} )'$, $1 \\leq j \\leq p_n$.\nLet $\\bar{A} = \\{j: {\\Vert} \\bar{{\\bolds{\\beta}}}^{0j}{\\Vert} \\neq0, 1 \\leq j\n\\leq\np_n \\}$ be the index set of variables\nthat are active at least one quantile level of interest, where ${\\Vert}\n\\cdot{\\Vert} $ denotes the $L_2$ norm. Let $\\bar{q}_n = |\\bar{A}|$ be the\ncardinality of $A$. Without loss of generality, we assume $\\bar{A} = \\{\n1,\\ldots,\\bar{q}_n\\}$. Let $X_{\\bar{A}}$ and ${\\mathbf{x}}_{\\bar\n{A}_1}',\\ldots,{\\mathbf{x}}_{\\bar{A}_n}$ be defined as before.\nBy the result of \\citet{Schumaker}, there exists ${\\bolds{\\xi}}_0^{(m)} \\in\n\\mathcal{R}^{L_n}$, where \\mbox{$L_n = d(k_n+l+1)+1$}, such that\n$\\mathop{\\operatorname{sup}}_{{\\mathbf{z}}_i} |\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}\n_0^{(m)} -\ng_0^{(m)}({\\mathbf{z}}_i)| = O(k_n^{-r})$, $m=1,\\ldots,M$.\n\nWe write the $(Mp_n)$-vector ${\\bolds{\\beta}}= ({{\\bolds{\\beta}}^{(1)}}',\\ldots\n,{{\\bolds{\\beta}}^{(M)}}' )'$,\nwhere for $k=1,\\ldots, M$, ${\\bolds{\\beta}}^{(k)}=(\\beta^{(k)}_1,\\ldots\n,\\beta^{(k)}_{p_n})'$;\nand we write the $(ML_n)$-vector ${\\bolds{\\xi}}= ({{\\bolds{\\xi}}^{(1)}}',\\ldots\n,{{\\bolds{\\xi}}^{(M)}}' )$. Let $\\bar{{\\bolds{\\beta}}}^j$ be the $M$-vector\n$ (\\beta_j^{(1)},\\ldots, \\beta_j^{(M)} )'$, $1 \\leq j\n\\leq p_n$. For simultaneous variable selection and estimation, we\nestimate $ ({\\bolds{\\beta}}_0^{(m)},\\xi_0^{(m)} )$, $m = 1,\\ldots\n,M$, by minimizing the following penalized objective function\n\n\n\n\\begin{eqnarray}\\label{groupPen}\n\\bar{Q}^P({\\bolds{\\beta}},{\\bolds{\\xi}}) &=& n^{-1} \\sum\n_{i=1}^n \\sum_{m=1}^{M}\n\\rho_{\\tau_m}\\bigl(Y_i - {\\mathbf{x}}_i'\n{\\bolds{\\beta}}^{(m)} - \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}^{(m)}\\bigr)\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} + \\sum_{j=1}^{p_n}\np_\\lambda\\bigl(\\bigl{\\Vert}\\bar{{\\bolds{\\beta}}}^{j}\\bigr{\\Vert}\n_1\\bigr),\n\\end{eqnarray}\n\nwhere $p_\\lambda(\\cdot)$ is a penalty function with tuning parameter\n$\\lambda$, ${\\Vert} \\cdot{\\Vert} _1$ denotes the $L_1$ norm, which\nwas used in\n\\citet{YuanLin} for group penalty; see also \\citet\n{HuangEtAl}. The penalty function encourages group-wise sparsity and forces\nthe covariates that have no effect on any of the $M$ quantiles to be\nexcluded together. Similarly penalty functions have been used in\n\\citet{zouYuan}, \\citet{LiuWu}\nfor variable selection at multiple quantiles. The above estimator can\nbe computed similarly as in Section~\\ref{sec3.2}.\n\nIn the oracle case, the estimator would be obtained by considering the\nunpenalized part of (\\ref{groupPen}), but with ${\\mathbf{x}}_i$ replaced by\n${\\mathbf{x}}_{\\bar{A}_i}$. \n\n\nThat is, we let\n\n\n\n\\begin{eqnarray}\n\\label{mm} && \\bigl\\{\\hat{{\\bolds{\\beta}}}_1^{(m)},\\hat{\n\\xi}^{(m)}: 1\\leq m\\leq M \\bigr\\}\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&\\qquad = \\mathop{\\operatorname{argmin}}_{{\\bolds{\\beta}}_1^{(m)},{\\bolds{\\xi}}^{(m)},\n1\\leq m\\leq M} n^{-1} \\sum\n_{i=1}^n \\sum_{m=1}^M\n\\rho_{\\tau_{m}}\\bigl(Y_i - {\\mathbf{x}}_{\\bar{A}_i}'\n{\\bolds{\\beta}}_1^{(m)} - \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}^{(m)}\\bigr).\n\\end{eqnarray}\n\nThe oracle estimator for ${\\bolds{\\beta}}_0^{(m)}$ is $\\hat{{\\bolds{\\beta}}\n}^{(m)}= (\\hat{{\\bolds{\\beta}}}_1^{(m)'},\\mathbf{0}_{p_n-q_n}' )'$,\nand across all quantiles is $\\hat{\\bar{{\\bolds{\\beta}}}} = (\\hat\n{{\\bolds{\\beta}}}^{(1)},\\ldots,\\hat{{\\bolds{\\beta}}}^{(M)} )$ and $\\hat{\\bar\n{{\\bolds{\\xi}}}} = (\\hat{{\\bolds{\\xi}}}^{(1)},\\ldots,\\hat{{\\bolds{\\xi}}}^{(M)}\n)$. The oracle estimator for the nonparametric function $g_{0j}^{(m)}$\nis $\\hat{g}_j^{(m)}(z_{ij}) = {\\bolds{\\pi}}(z_{ij})'\\hat{{\\bolds{\\xi}}_j}^{(m)} -\nn^{-1}\\* \\sum_{i=1}^n \\pi(z_{ij})'\\hat{{\\bolds{\\xi}}}_j^{(m)}$ for $j=1,\\ldots\n,d$; for $g_{00}^{(m)}$ is $\\hat{g}_0^{(m)} = \\hat{\\xi}_0^{(m)} +\nn^{-1} \\sum_{i=1}^n \\times\\break \\sum_{j=1}^d \\pi(z_{ij})'\\hat{{\\bolds{\\xi}}}_j^{(m)}$.\nThe oracle estimator of $g_0^{(m)}({\\mathbf{z}}_i)$ is $\\hat{g}^{(m)}({\\mathbf{z}}_i)\n= \\hat{g}_0^{(m)} +\\break \\sum_{j=1}^d \\hat{g}_j^{(m)}(z_{ij})$.\n\n\n\nAs the next theorem suggests, Theorem \\ref{scad_local_min} can be extended to the\nmultiple quantile case. To save space, we present the regularity\nconditions and the technical derivations in the online supplementary\nmaterial [\\citet{Supp}].\n\n\n\n\\begin{theorem}\n\\label{scad_local_min_mult}\nAssume Conditions \\textup{B1--B6} in the online supplementary material [\\citet{Supp}] are satisfied.\nLet\\vspace*{1pt} $\\bar{\\mathcal{E}}_n(\\lambda)$ be the set of local minima of the\npenalized objective function $\\bar{Q}^P({\\bolds{\\beta}},\\gamma)$.\nConsider either the SCAD or the MCP penalty\\vspace*{1pt} function with tuning\nparameter $\\lambda$. Let $\\hat{\\bar{{\\bolds{\\eta}}}} \\equiv(\\hat\n{\\bar{{\\bolds{\\beta}}}},\\hat{\\bar{{\\bolds{\\xi}}}} )$ be the oracle estimator\nthat solves (\\ref{mm}). If \n$\\lambda= o (n^{-(1-C_4)/2} )$, $n^{-1/2}\\bar{q}_n =\no(\\lambda)$, $n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$,\nthen\n\n", "itemtype": "equation", "pos": 59887, "prevtext": "\n\nwhere $\\beta_j \\sim U[0.5,1.5]$ for $1 \\leq j \\leq4$. We consider\nthree different distributions of the error term $\\varepsilon_i$: (1)\nstandard normal distribution; (2) $t$ distribution with 3 degrees of\nfreedom; and (3) heteroscedastic\nnormal distribution $\\varepsilon_i = \\tilde{X}_{i1}\\zeta_i$ where\n$\\zeta_i \\sim N(0,\\sigma=0.7)$\nare independent of the $X_i$'s. \n\n\n\nWe perform 100 simulations for each setting with sample size $n=300$,\nand $p=100$, $300$, $600$. \n\n\nResults for additional simulations with sample sizes of $50$, 100 and\n200 are provided in the online supplementary material [\\citet\n{Supp}]. For the heteroscedastic error case, we model $\\tau= 0.7$\nand $0.9$; otherwise, we model the conditional median.\nNote that at $\\tau= 0.7$ or 0.9, when the error has the\naforementioned heteroscedastic distribution, $X_1$ is part of the true\nmodel. At these two quantiles, the true model consists of 5 linear\ncovariates. In all simulations, the number of basis functions is set to three,\nwhich we find to work satisfactorily in a variety of settings. For the\nLASSO method, we select the tuning parameters $\\lambda$\n\nby using five-fold cross validation. For the Q-SCAD model, we select\n$\\lambda$ that minimizes (\\ref{large_p_bic}) while for LS-SCAD we use\na least squares equivalent. The tuning parameter $a$ in the SCAD\npenalty function\nis set to 3.7 as recommended in \\citet{fanLi}. To assess the\nperformance of different methods, we adopt the following criteria:\n\n\\begin{longlist}\n\n\n\n\\item[1.] False Variables (FV): average number of nonzero linear covariates\nincorrectly included in the model.\n\n\\item[2.] True Variables (TV): \naverage number of nonzero linear covariates correctly included in the model.\n\n\\item[3.] True: proportion of times the true model is exactly identified.\n\n\\item[4.] P: proportion of times $X_{1}$ is selected.\n\n\\item[5.] AADE: average of the \\textit{average absolute deviation} (ADE) of\nthe fit of the nonlinear components,\nwhere the ADE is defined as $n^{-1}\\sum_{i=1}^n|\\hat{g}({\\mathbf{z}}_i) -\ng_0({\\mathbf{z}}_i)|$.\n\n\\item[6.] MSE: average of the mean squared error for estimating ${\\bolds{\\beta}}\n_0$, that is, the average of ${\\Vert} \\hat{{\\bolds{\\beta}}}-{\\bolds{\\beta}}_0{\\Vert} ^2$\nacross all simulation runs.\n\\end{longlist}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.5$) and mean regression\nusing SCAD and LASSO penalty functions for $\\varepsilon\\sim N(0,1)$}\\label{tab1}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.20 & 4.00 & 0.88 & 0.00 & 0.16 & 0.03 \\\\\n\nQ-LASSO & 300 & 100 & 12.88 & 4.00 & 0.00 & 0.13 & 0.16 & 0.13 \\\\\n\nLS-SCAD & 300 & 100 & 0.32 & 4.00 & 0.85 & 0.00 & 0.13 & 0.02 \\\\\n\nLS-LASSO & 300 & 100 & 11.63 & 4.00 & 0.00 & 0.12 & 0.13 & 0.07\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.04 & 4.00 & 0.96 & 0.00 & 0.15 & 0.02 \\\\\n\nQ-LASSO & 300 & 300 & 15.93 & 4.00 & 0.00 & 0.07 & 0.16 & 0.14 \\\\\n\nLS-SCAD & 300 & 300 & 0.33 & 4.00 & 0.78 & 0.00 & 0.12 & 0.02 \\\\\n\nLS-LASSO & 300 & 300 & 15.00 & 4.00 & 0.00 & 0.04 & 0.13 & 0.09\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.06 & 4.00 & 0.94 & 0.00 & 0.15 & 0.02 \\\\\n\nQ-LASSO & 300 & 600 & 21.86 & 4.00 & 0.01 & 0.06 & 0.16 & 0.16 \\\\\n\nLS-SCAD & 300 & 600 & 2.57 & 4.00 & 0.69 & 0.01 & 0.13 & 0.06 \\\\\n\nLS-LASSO & 300 & 600 & 17.11 & 4.00 & 0.00 & 0.04 & 0.13 & 0.09 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.5$) and mean regression\nusing SCAD and LASSO penalty functions for $\\varepsilon\\sim T_3$}\\label{tab2}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.07 & 4.00 & 0.95 & 0.00 & 0.16 & 0.03 \\\\\n\nQ-LASSO & 300 & 100 & 13.09 & 4.00 & 0.01 & 0.17 & 0.17 & 0.15 \\\\\n\nLS-SCAD & 300 & 100 & 1.08 & 3.99 & 0.45 & 0.02 & 0.19 & 0.11 \\\\\n\nLS-LASSO & 300 & 100 & 10.15 & 3.94 & 0.02 & 0.08 & 0.19 & 0.31\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.05 & 4.00 & 0.97 & 0.00 & 0.17 & 0.03 \\\\\n\nQ-LASSO & 300 & 300 & 18.42 & 4.00 & 0.00 & 0.08 & 0.18 & 0.18 \\\\\n\nLS-SCAD & 300 & 300 & 1.22 & 4.00 & 0.46 & 0.00 & 0.20 & 0.11 \\\\\n\nLS-LASSO & 300 & 300 & 15.15 & 3.99 & 0.01 & 0.08 & 0.21 & 0.26\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.06 & 3.98 & 0.94 & 0.00 & 0.16 & 0.04 \\\\\n\nQ-LASSO & 300 & 600 & 20.81 & 4.00 & 0.01 & 0.03 & 0.18 & 0.23 \\\\\n\nLS-SCAD & 300 & 600 & 1.33 & 4.00 & 0.45 & 0.00 & 0.19 & 0.14 \\\\\n\nLS-LASSO & 300 & 600 & 17.40 & 4.00 & 0.01 & 0.01 & 0.20 & 0.28 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.7$) and mean regression\nusing SCAD and LASSO penalty functions for heteroscedastic errors}\\label{tab3}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.21 & 4.84 & 0.70 & 0.84 & 0.17 & 0.05 \\\\\n\nQ-LASSO & 300 & 100 & 13.86 & 4.97 & 0.00 & 0.97 & 0.24 & 0.15 \\\\\n\nLS-SCAD & 300 & 100 & 1.09 & 4.06 & 0.01 & 0.06 & 0.16 & 0.69 \\\\\n\nLS-LASSO & 300 & 100 & 11.48 & 4.13 & 0.00 & 0.13 & 0.17 & 0.78\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.20 & 4.77 & 0.61 & 0.77 & 0.20 & 0.06 \\\\\n\nQ-LASSO & 300 & 300 & 18.54 & 4.97 & 0.00 & 0.97 & 0.27 & 0.18 \\\\\n\nLS-SCAD & 300 & 300 & 3.28 & 4.00 & 0.00 & 0.00 & 0.16 & 0.68 \\\\\n\nLS-LASSO & 300 & 300 & 15.85 & 4.08 & 0.00 & 0.08 & 0.16 & 0.79\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.16 & 4.59 & 0.48 & 0.59 & 0.26 & 0.08 \\\\\n\nQ-LASSO & 300 & 600 & 23.26 & 4.89 & 0.00 & 0.89 & 0.31 & 0.24 \\\\\nL\nLS-SCAD & 300 & 600 & 6.31 & 4.02 & 0.00 & 0.02 & 0.16 & 0.69 \\\\\n\nLS-LASSO & 300 & 600 & 18.50 & 4.09 & 0.00 & 0.09 & 0.16 & 0.83 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Simulation results comparing quantile ($\\tau=0.9$) and mean regression\nusing SCAD and LASSO penalty functions for heteroscedastic errors}\\label{tab4}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lccd{2.2}ccccc@{}}\n\\hline\n\\textbf{Method} & $\\bolds{n}$ & $\\bolds{p}$ & \\multicolumn{1}{c}{\\textbf{FV}} & \\textbf{TV} & \\textbf{True} & \\textbf{P} & \\textbf{AADE} & \\textbf{MSE}\\\\\n\\hline\n\nQ-SCAD & 300 & 100 & 0.06 & 4.93 & 0.91 & 0.98 & 0.24 & 0.30 \\\\\n\nQ-LASSO & 300 & 100 & 12.94 & 5.00 & 0.00 & 1.00 & 0.49 & 0.73 \\\\\n\nLS-SCAD & 300 & 100 & 1.09 & 4.06 & 0.01 & 0.06 & 0.16 & 4.72 \\\\\n\nLS-LASSO & 300 & 100 & 11.48 & 4.13 & 0.00 & 0.13 & 0.17 & 4.73\n\\[3pt]\n\nQ-SCAD & 300 & 300 & 0.26 & 5.00 & 0.81 & 1.00 & 0.19 & 0.24 \\\\\n\nQ-LASSO & 300 & 300 & 16.33 & 5.00 & 0.00 & 1.00 & 0.62 & 0.92 \\\\\n\nLS-SCAD & 300 & 300 & 3.28 & 4.00 & 0.00 & 0.00 & 0.16 & 4.63 \\\\\n\nLS-LASSO & 300 & 300 & 15.85 & 4.08 & 0.00 & 0.08 & 0.16 & 4.67\n\\[3pt]\n\nQ-SCAD & 300 & 600 & 0.34 & 4.94 & 0.77 & 1.00 & 0.21 & 0.29 \\\\\n\nQ-LASSO & 300 & 600 & 19.79 & 4.97 & 0.00 & 1.00 & 0.74 & 1.15 \\\\\n\nLS-SCAD & 300 & 600 & 6.31 & 4.02 & 0.00 & 0.02 & 0.16 & 4.64 \\\\\n\nLS-LASSO & 300 & 600 & 18.50 & 4.09 & 0.00 & 0.09 & 0.16 & 4.74 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\n\n\nThe simulation results are summarized in Tables~\\ref{tab1}--\\ref{tab4}.\nTables \\ref{tab1}~and~\\ref{tab2} correspond to $\\tau= 0.5$, $N(0,1)$ and $T_3$ error\ndistribution, respectively.\nTables \\ref{tab3}~and~\\ref{tab4} are for the heteroscedastic error, $\\tau= 0.7$ and\n$0.9$, respectively. Least squares based estimates of $\\hat{{\\bolds{\\beta}}}$\nfor $\\tau=0.7$ or $0.9$ are obtained by assuming $\\varepsilon_i \\sim\nN(0,\\sigma)$, with estimates of $\\sigma$ being used in each\nsimulation. An extension of Table~\\ref{tab3} for $p = 1200$ and 2400 is included\nin the online supplementary material [\\citet{Supp}].\n\n\n\n\nWe observe that the method with the SCAD penalty tends to pick a\nsmaller and more accurate model. The advantages of quantile regression\ncan be seen by its stronger performance\nat the presence of heavy-tailed distribution or heteroscedastic errors.\nFor the latter case,\nthe least squared based methods perform poorly in identifying the\nactive variables in the\ndispersion function. \n\n\n\n\nEstimation of the nonlinear terms is similar across different error\ndistributions and\ndifferent values of~$p$.\n\n\n\n\n\n\\section{An application to birth weight data}\\label{sec5}\n\nVotavova et~al. (\\citeyear{AppliedExample}) collected blood samples\nfrom peripheral blood,\ncord blood and the placenta from 20 pregnant smokers and 52 pregnant\nwomen without significant exposure to smoking. Their main objective was\nto identify the difference in transcriptome alterations between the two\ngroups. Birth weight of the baby (in kilograms) was recorded along with\nage of the mother, gestational age, parity, measurement of the amount\nof cotinine, a chemical found in tobacco, in the blood and mother's\nBMI. Low birth weight is known to be associated with both short-term\nand long-term health complications. Scientists are interested in which\ngenes are associated with low birth weight\n[\\citet{birthWt2}].\n\n\n\nWe consider modeling the 0.1, 0.3 and 0.5 conditional quantiles of\ninfant birth weight.\nWe use the genetic data from the peripheral blood sample which include\n64 subjects after dropping those with incomplete information. The blood\nsamples were assayed using HumanRef-8 v3 Expression BeadChips with\n24,539 probes. For each quantile, the top 200 probes are selected using\nthe quantile-adaptive screening method [\\citet{heWang}]. The\ngene expression values of the 200 probes are included as linear\ncovariates for the semiparametric quantile regression model. The\nclinical variables parity, gestational age, cotinine level and BMI\nare also included as linear covariates. The age of the mother is\nmodeled nonparametrically as exploratory analysis reveals potential\nnonlinear effect.\n\n\n\nWe consider the semiparametric quantile regression model with the SCAD\nand LASSO penalty functions.\n\nLeast squares based semiparametric models with the SCAD and LASSO\npenalty functions are also considered. Results for the MCP penalty are\nreported in the online supplementary material [\\citet{Supp}].\n\nThe tuning parameter $\\lambda$ is selected by minimizing (\\ref\n{large_p_bic}) for the SCAD estimator and by five-fold cross validation\nfor LASSO as discussed in Section~\\ref{sec4}. The third column of\nTable~\\ref{applied_models} reports the number of nonzero elements, ``Original\nNZ,'' for each model. As expected, the LASSO method selects a larger\nmodel than the SCAD penalty does. The number of nonzero variables\nvaries with the quantile level, providing evidence that mean regression\nalone would provide a limited view of the conditional distribution.\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\n\\caption{Quantile ($\\tau=0.1$, 0.3 and 0.5) and mean regression\nanalysis of birth weight based on the original data and the random partitioned data}\\label{tab5}\\label{applied_models}\n\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lcd{2.0}cd{2.2}@{}}\n\\hline\n$\\bolds{\\tau}$ & \\textbf{Method} & \\multicolumn{1}{c}{\\textbf{Original NZ}} & \\textbf{Prediction error} & \\multicolumn{1}{c@{}}{\\textbf{Randomized NZ}}\\\\\n\\hline\n0.10 & Q-SCAD & 2 & 0.07 (0.03) & 2.27 \\\\\n0.10 & Q-LASSO & 10 & 0.08 (0.02) & 3.09\n\\[3pt]\n0.30 & Q-SCAD & 7 & 0.18 (0.04) & 6.74 \\\\\n0.30 & Q-LASSO & 22 & 0.16 (0.03) & 12.39\n\\[3pt]\n0.50 & Q-SCAD & 5 & 0.21 (0.04) & 5.80 \\\\\n0.50 & Q-LASSO & 6 & 0.20 (0.04) & 14.25\n\\[3pt]\nMean & LS-SCAD &12 & 0.20 (0.04) & 5.43 \\\\\nMean & LS-LASSO & 12 & 0.20 (0.04) & 3.77 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\nNext, we compare different models on 100 random partitions of the data set.\nFor each partition, we randomly select 50 subjects for the training\ndata and 14 subjects for the test data.\nThe fourth column of Table~\\ref{applied_models} reports the prediction\nerror evaluated on the test data, defined as\n$14^{-1}\\sum_{i=1}^{14} \\rho_\\tau(Y_i - \\hat{Y}_i)$; while the\nfifth column reports the average number of linear covariates included\nin each model (denoted by ``Randomized NZ''). Standard errors for the\nprediction error is reported in parentheses. We note that the SCAD\nmethod produces notably smaller models than the Lasso method does\nwithout sacrificing much prediction accuracy.\n\n\n\n\\begin{figure}[b]\n\n\\includegraphics{1367f01.eps}\n\n\\caption{Lack-of-fit diagnostic QQ plot for the birth weight data\nexample.}\\label{fig:qq_diag}\\label{fig1}\n\\end{figure}\n\nModel checking in high dimension is challenging. In the following, we\nconsider a simulation-based diagnostic plot to help visually assess the\noverall lack-of-fit for the quantile regression model\n[Wei and He (\\citeyear{WH06})] to assess the overall lack-of-fit for the quantile regression\nmodel. First, we randomly generate $\\tilde{\\tau}$ from the uniform\n$[0,1]$ distribution. Then we fit the proposed semiparametric quantile\nregression model using the SCAD penalty for the quantile $\\tilde{\\tau\n}$. Next, we generate a response variable $\\tilde{Y} = {\\mathbf{x}}'\\hat\n{{\\bolds{\\beta}}}(\\tilde{\\tau}) + \\hat{g}(z,\\tilde{\\tau})$,\nwhere $({\\mathbf{x}},z)$ is randomly sampled from the set of observed\ncovariates, with $z$ denoting mother's age and\n${\\mathbf{x}}$ denoting the vector of other covariates. The process is repeated\n100 times and produces a sample of 100 simulated birth weights based on\nthe model. Figure~\\ref{fig1} shows the QQ plot comparing the simulated and\nobserved birth weights. Overall, the QQ plot is close to the 45 degree\nline and does not suggest gross lack-of-fit. Figure~\\ref{fig2} displays the\nestimated nonlinear effects of mother's age $\\hat{g}(z)$ at the three\nquantiles [standardized to satisfy the constraint $\\sum_{i=1}^n \\hat\n{g}(z_i) = 0$]. At the 0.1 and 0.3 quantiles, the estimated mother's\nage effects are similar except for some deviations at the tails of the\nmother's age distribution. At these two quantiles, after age 30,\nmother's age is observed to have a positive effect. The effect of\nmother's age at the median is nonmonotone: the effect is first\nincreasing (up to age 25), then decreasing (to about age 33), and\nincreasing again.\n\n\n\n\n\nWe observe that different models are often selected for different\nrandom partitions. Table~\\ref{tab6} summarizes the variables selected by Q-SCAD\nfor $\\tau= 0.1$, 0.3 and 0.5 and the frequency these variables are\nselected in the 100 random partitions. Probes are listed by their\nidentification number along with corresponding gene in parentheses. The\nSCAD models tend to produce sparser models while the LASSO models\nprovide slightly better predictive performance.\n\n\n\n\\begin{figure}[t]\n\n\\includegraphics{1367f02.eps}\n\n\\caption{Estimated nonlinear effects of mother's age (denoted by $z$)\nat three different quantiles.}\\label{fig:nl_plots}\\label{fig2}\n\\end{figure}\n\n\n\n\\begin{table}[t]\n\\tabcolsep=0pt\n\\caption{Frequency of covariates selected at three quantiles among 100 random partitions}\\label{table_partition_selections}\\label{tab6}\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lclclc@{}}\n\\hline\n\\multicolumn{2}{@{}c}{\\textbf{Q-SCAD 0.1}} &\n\\multicolumn{2}{c}{\\textbf{Q-SCAD 0.3}} &\n\\multicolumn{2}{c@{}}{\\textbf{Q-SCAD 0.5}} \\[-6pt]\n\\multicolumn{2}{@{}c}{\\hrulefill} &\n\\multicolumn{2}{c}{\\hrulefill} &\n\\multicolumn{2}{c@{}}{\\hrulefill} \\\\\n & \\textbf{Fre-} &  & \\textbf{Fre-} &  & \\textbf{Fre-} \\\\\n\\textbf{Covariate} & \\textbf{quency} & \\multicolumn{1}{c}{\\textbf{Covariate}} & \\textbf{quency} & \\multicolumn{1}{c}{\\textbf{Covariate}} & \\textbf{quency} \\\\\n\\hline\nGestational age & 82 & Gestational age & 86 & Gestational age & 69 \\\\\n1,687,073 (SOGA1) & 24 & 1,804,451 (LEO1) & 33 & 2,334,204 (ERCC6L) & 57 \\\\\n& & 1,755,657 (RASIP1) & 27 & 1,732,467 (OR2AG1) & 52 \\\\\n& & 1,658,821 (SAMD1) & 23 & 1,656,361 (LOC201175) & 31 \\\\\n& & 2,059,464 (OR5P2) & 14 & 1,747,184 (PUS7L) & \\phantom{0}5 \\\\\n& & 2,148,497 (C20orf107) & \\phantom{0}6 & & \\\\\n& & 2,280,960 (DEPDC7) & \\phantom{0}3 & &\\\\\n\\hline\n\\end{tabular*}\\vspace*{-3pt}\n\\end{table}\n\nGestational age is identified to be important with high frequency at\nall three quantiles under consideration.\nThis is not surprising given the known important relationship between\nbirth weight and\ngestational age. Premature birth is often strongly associated with low\nbirth weight.\nThe genes selected at the three different quantiles are not\noverlapping. This is an\nindication of the heterogeneity in the data. The variation in frequency\nis likely\ndue to the relatively small sample size.\n\n\n\n\n\n\n\nHowever, examining the selected genes does provide some interesting\ninsights. The gene SOGA1 is a suppressor of glucose, which is\ninteresting because maternal gestational diabetes is known to have a\nsignificant effect on birth weight [\\citet{gestDiab}]. The genes\nOR2AG1, OR5P2 and DEPDC7 are all located on chromosome 11,\nthe chromosome with the most selected genes. Chromosome 11 also\ncontains PHLDA2, a gene that has been reported to be highly expressed\nin mothers that have children with lower birth weight [\\citet{phlda2}].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Estimation and variable selection for multiple quantiles}\\label{sec6}\nMotivated by referees' suggestions, we consider an extension for\nsimultaneous variable selection at multiple quantiles. Let $\\tau_1 <\n\\tau_2 < \\cdots< \\tau_M$ be the set of quantiles of interest, where\n$M > 0$ is a positive integer. We assume that\n\n\n\n\n", "index": 35, "text": "\\begin{equation}\n\\label{multTau} Q_{Y_i {|}{\\mathbf{x}}_i, {\\mathbf{z}}_i}(\\tau_m) = {\\mathbf{x}}_i'\n{\\bolds{\\beta}}_0^{(m)} + g_0^{(m)}(\n{\\mathbf{z}}_i),\\qquad m = 1,\\ldots,M,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"Q_{Y_{i}{|}{\\mathbf{x}}_{i},{\\mathbf{z}}_{i}}(\\tau_{m})={\\mathbf{x}}_{i}^{%&#10;\\prime}{\\bolds{\\beta}}_{0}^{(m)}+g_{0}^{(m)}({\\mathbf{z}}_{i}),\\qquad m=1,%&#10;\\ldots,M,\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>Q</mi><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi>\ud835\udc31</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msubsup><mi>\u03b2</mi><mn>0</mn><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>g</mi><mn>0</mn><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mi>m</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>M</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\\end{theorem}\n\n\n\n\n\n\n\n\n\\subsection*{A numerical example}\nTo assess the multiple quantile\nestimator, we ran 100 simulations using the setting presented in\nSection~\\ref{sec4} with $\\varepsilon_i \\sim T_3$, and consider $\\tau=\n0.5$, 0.7\nand 0.9. We compare the variable selection performance of\nthe multiple-quantile estimator (denoted by Q-group) in this section\nwith the method that estimates each quantile separately (denoted by\nQ-ind). For both approaches, we use the SCAD penalty function. Results\nfor the MCP penalty are included in the online supplementary material\n[\\citet{Supp}]. We also report results from the\nmultiple-quantile oracle estimator (denotes by Q-oracle) which assumes\nthe knowledge of the underlying model and serves as a benchmark.\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Comparison of group and individual penalty functions for\nmultiple quantile estimation with $\\varepsilon\\sim T_3$}\\label{tab7}\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lcd{1.2}d{1.2}d{1.2}c@{}}\n\\hline\n\\textbf{Method} & \\multicolumn{1}{c}{$\\bolds{p}$} & \\multicolumn{1}{c}{\\textbf{FV}} & \\multicolumn{1}{c}{\\textbf{TV}} & \\multicolumn{1}{c}{\\textbf{True}}\n& \\multicolumn{1}{c@{}}{$\\bolds{L_2}$ \\textbf{error}}\n\\\\\n\\hline\nQ-group-SCAD & 300 & 1.01 & 4 & 0.49 & 0.14 \\\\\nQ-ind-SCAD & 300 & 0.98 & 4 & 0.45 & 0.17 \\\\\nQ-oracle & 300 & 0 & 4 & 1 & 0.06\n\\[3pt]\nQ-group-SCAD & 600 & 1.2 & 4 & 0.56 & 0.15 \\\\\nQ-ind-SCAD & 600 & 1.51 & 3.99 & 0.34 & 0.17 \\\\\nQ-oracle & 600 & 0 & 4 & 1 & 0.07 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\nTable~\\ref{tab7} summarizes the simulation results for $n=50$, $p=300$ and 600.\nAs in \\citet{zouYuan}, when evaluating the Q-ind method,\nat quantile level $\\tau_m$, we define $A_m=\\{j: \\hat{\\beta\n}_j^{(m)}\\neq0\\}$ be\nthe index set of estimated nonzero coefficients at this quantile level.\nLet $\\bigcup_{m=1}^MA_m$ be the set of the selected variables using Q-ind.\nAs the simulations results in Section~\\ref{sec4}, we report FV, TV and TRUE.\nWe also report the error for estimating the linear coefficients ($L_2$ error),\nwhich is defined as the average of $M^{-1} \\sum_{m=1}^M (\\hat\n{{\\bolds{\\beta}}}^{(m)}-{\\bolds{\\beta}}_{0}^{(m)} )^2$\nover all simulation runs. The results demonstrate that\ncomparing with Q-ind, the new method Q-group has lower false discovery rate,\nhigher probability of identifying the true underlying model and smaller\nestimation error.\n\n\n\\section{Discussion}\\label{sec7}\nWe considered nonconvex penalized estimation for partially linear\nadditive quantile regression models\nwith high dimensional linear covariates. We derive the oracle theory\nunder mild conditions.\nWe have focused on estimating a particular quantile of interest and\nalso considered an extension\nto simultaneous variable selection at multiple quantiles.\n\nA problem of important practical interest is how to identify which\ncovariates should be modeled linearly and which\ncovariates should be modeled nonlinearly. Usually, we do not have such\nprior knowledge in real data analysis.\nThis is a challenging problem in high dimension. Recently, important\nprogresses have been made by\n\\citet{ZCL}; \\citet{HWM}; \\citet{Lian} for\nsemiparametric mean\nregression models. We plan on addressing this question for high\ndimensional semiparametric quantile regression in our future research.\n\nAnother relevant problem of practical interest is to estimate the\nconditional quantile function itself.\nGiven ${\\mathbf{x}}^*$, ${\\mathbf{z}}^*$, we can estimate $Q_{Y_i {|}{\\mathbf{x}}^*, {\\mathbf{z}}\n^*}(\\tau)$\nby ${{\\mathbf{x}}^*}'\\hat{{\\bolds{\\beta}}}_{1} + \\hat{g}({\\mathbf{z}}^*)$, where\n$\\hat{{\\bolds{\\beta}}}$ and $\\hat{g}$ are obtained from penalized quantile regression.\nWe conjecture that the consistency of estimating the conditional quantile\nfunction can be derived under somewhat weaker conditions\nin the current paper, as motivated by the results on \\textit{persistency}\nfor linear mean regression in\nhigh dimension [\\citet{Green}]. The details will also be\nfurther investigated\nin the future.\n\n\n\\begin{appendix}\\label{sec8}\\label{appe}\n\n\\section*{Appendix}\n\n\nThroughout the appendix, we use $C$ to denote a positive constant which\ndoes not depend on $n$ and may vary from\nline to line. For a vector ${\\mathbf{x}}$, ${\\Vert} {\\mathbf{x}}{\\Vert} $\ndenotes its Euclidean norm.\nFor a matrix $A$, ${\\Vert} A{\\Vert} = \\sqrt{\\lambda_{\\max}(A'A)}$\ndenotes its\nspectral norm. For a function $h(\\cdot)$ on $[0,1]$, ${\\Vert}\nh{\\Vert} _{\\infty}\n= \\mathop{\\operatorname{\\sup}}_{x} |h(x)|$\ndenotes the uniform norm. Let $I_n$ denote an $n\\times n$ identity matrix.\n\n\n\\subsection{Derivation of the results in Section~\\texorpdfstring{\\protect\\ref{sec2}}{2}}\\label{sec8.1}\n\n\n\\subsubsection{Notation}\\label{sec8.1.1}\nTo facilitate the proof, we will make use of the theoretically centered\nB-spline basis functions\nsimilar to the approach used by \\citet{XY}. More specifically,\nwe consider the B-spline basis functions $b_j(\\cdot)$ in\\vspace*{2pt} Section~\\ref{sec2.1}\nand let $B_j(z_{ik}) = b_{j+1}(z_{ik}) - \\frac{E\n[b_{j+1}(z_{ik}) ]}{E [b_1(z_{ik}) ]}b_1(z_{ik})$\nfor $j=1,\\ldots, k_n+l$. Then $E(B_j(z_{ik}))=0$.\nFor a given covariate $z_{ik}$,\nlet\n${\\mathbf w}(z_{ik})= (B_1(z_{ik}),\\ldots,B_{k_n+l}(z_{ik}) )'$\nbe the vector of basis functions, and ${\\mathbf{W}}({\\mathbf{z}}_i)$ denote the\n$J_n$-dimensional vector\n$ (k_n^{-1/2},{\\mathbf w}(z_{i1})',\\ldots,{\\mathbf w}(z_{id})' )'$,\nwhere $J_n=d(k_n+l)+1$.\n\nBy the result of \\citeauthor{Schumaker} [(\\citeyear{Schumaker}),\npage~227], there exists a\nvector ${\\bolds{\\gamma}}_0\\in\\mathcal{R}^{J_n}$ and a\npositive constant\n$C_0$, such that $\\sup_{t\\in[0,1]^{d}}|g_0({\\mathbf{t}})-{\\mathbf{W}}({\\mathbf{t}})'{\\bolds{\\gamma}}\n_0|\\leq C_0k_n^{-r}$.\nLet\n\n\n\n\n", "itemtype": "equation", "pos": 65636, "prevtext": "\n\n\n\n\n\\noindent  where $g_0^{(m)}({\\mathbf{z}}_i) = g_{00}^{(m)} + \\sum_{j=1}^d\ng_{0j}^{(m)}(z_{ij})$, with $g_{00}^{(m)} \\in\\mathcal{R}$. We assume\nthat functions $g_{0j}^{(m)}$ satisfy $E\n[g_{0j}^{(m)}(z_{ij}) ]=0$ for the purpose of identification. The\nnonlinear functions\nare allowed to vary with the quantiles. We are interested in the high\ndimensional case where\nmost of the linear covariates have zero coefficients across all $M$\nquantiles, for which group selection will help us combine information\nacross quantiles.\n\n\n\n\n\nWe write ${\\bolds{\\beta}}_0^{(m)} = (\\beta_{01}^{(m)},\\beta_{02}^{(m)},\n\\ldots, \\beta_{0p_n}^{(m)} )'$, $m = 1,\\ldots,M$.\nLet $\\bar{{\\bolds{\\beta}}}^{0j}$ be the \\mbox{$M$-}vec\\-tor $ (\\beta\n_{0j}^{(1)},\\ldots, \\beta_{0j}^{(M)} )'$, $1 \\leq j \\leq p_n$.\nLet $\\bar{A} = \\{j: {\\Vert} \\bar{{\\bolds{\\beta}}}^{0j}{\\Vert} \\neq0, 1 \\leq j\n\\leq\np_n \\}$ be the index set of variables\nthat are active at least one quantile level of interest, where ${\\Vert}\n\\cdot{\\Vert} $ denotes the $L_2$ norm. Let $\\bar{q}_n = |\\bar{A}|$ be the\ncardinality of $A$. Without loss of generality, we assume $\\bar{A} = \\{\n1,\\ldots,\\bar{q}_n\\}$. Let $X_{\\bar{A}}$ and ${\\mathbf{x}}_{\\bar\n{A}_1}',\\ldots,{\\mathbf{x}}_{\\bar{A}_n}$ be defined as before.\nBy the result of \\citet{Schumaker}, there exists ${\\bolds{\\xi}}_0^{(m)} \\in\n\\mathcal{R}^{L_n}$, where \\mbox{$L_n = d(k_n+l+1)+1$}, such that\n$\\mathop{\\operatorname{sup}}_{{\\mathbf{z}}_i} |\\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}}\n_0^{(m)} -\ng_0^{(m)}({\\mathbf{z}}_i)| = O(k_n^{-r})$, $m=1,\\ldots,M$.\n\nWe write the $(Mp_n)$-vector ${\\bolds{\\beta}}= ({{\\bolds{\\beta}}^{(1)}}',\\ldots\n,{{\\bolds{\\beta}}^{(M)}}' )'$,\nwhere for $k=1,\\ldots, M$, ${\\bolds{\\beta}}^{(k)}=(\\beta^{(k)}_1,\\ldots\n,\\beta^{(k)}_{p_n})'$;\nand we write the $(ML_n)$-vector ${\\bolds{\\xi}}= ({{\\bolds{\\xi}}^{(1)}}',\\ldots\n,{{\\bolds{\\xi}}^{(M)}}' )$. Let $\\bar{{\\bolds{\\beta}}}^j$ be the $M$-vector\n$ (\\beta_j^{(1)},\\ldots, \\beta_j^{(M)} )'$, $1 \\leq j\n\\leq p_n$. For simultaneous variable selection and estimation, we\nestimate $ ({\\bolds{\\beta}}_0^{(m)},\\xi_0^{(m)} )$, $m = 1,\\ldots\n,M$, by minimizing the following penalized objective function\n\n\n\n\\begin{eqnarray}\\label{groupPen}\n\\bar{Q}^P({\\bolds{\\beta}},{\\bolds{\\xi}}) &=& n^{-1} \\sum\n_{i=1}^n \\sum_{m=1}^{M}\n\\rho_{\\tau_m}\\bigl(Y_i - {\\mathbf{x}}_i'\n{\\bolds{\\beta}}^{(m)} - \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}^{(m)}\\bigr)\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} + \\sum_{j=1}^{p_n}\np_\\lambda\\bigl(\\bigl{\\Vert}\\bar{{\\bolds{\\beta}}}^{j}\\bigr{\\Vert}\n_1\\bigr),\n\\end{eqnarray}\n\nwhere $p_\\lambda(\\cdot)$ is a penalty function with tuning parameter\n$\\lambda$, ${\\Vert} \\cdot{\\Vert} _1$ denotes the $L_1$ norm, which\nwas used in\n\\citet{YuanLin} for group penalty; see also \\citet\n{HuangEtAl}. The penalty function encourages group-wise sparsity and forces\nthe covariates that have no effect on any of the $M$ quantiles to be\nexcluded together. Similarly penalty functions have been used in\n\\citet{zouYuan}, \\citet{LiuWu}\nfor variable selection at multiple quantiles. The above estimator can\nbe computed similarly as in Section~\\ref{sec3.2}.\n\nIn the oracle case, the estimator would be obtained by considering the\nunpenalized part of (\\ref{groupPen}), but with ${\\mathbf{x}}_i$ replaced by\n${\\mathbf{x}}_{\\bar{A}_i}$. \n\n\nThat is, we let\n\n\n\n\\begin{eqnarray}\n\\label{mm} && \\bigl\\{\\hat{{\\bolds{\\beta}}}_1^{(m)},\\hat{\n\\xi}^{(m)}: 1\\leq m\\leq M \\bigr\\}\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&\\qquad = \\mathop{\\operatorname{argmin}}_{{\\bolds{\\beta}}_1^{(m)},{\\bolds{\\xi}}^{(m)},\n1\\leq m\\leq M} n^{-1} \\sum\n_{i=1}^n \\sum_{m=1}^M\n\\rho_{\\tau_{m}}\\bigl(Y_i - {\\mathbf{x}}_{\\bar{A}_i}'\n{\\bolds{\\beta}}_1^{(m)} - \\bolds{\\Pi}({\\mathbf{z}}_i)'\n{\\bolds{\\xi}}^{(m)}\\bigr).\n\\end{eqnarray}\n\nThe oracle estimator for ${\\bolds{\\beta}}_0^{(m)}$ is $\\hat{{\\bolds{\\beta}}\n}^{(m)}= (\\hat{{\\bolds{\\beta}}}_1^{(m)'},\\mathbf{0}_{p_n-q_n}' )'$,\nand across all quantiles is $\\hat{\\bar{{\\bolds{\\beta}}}} = (\\hat\n{{\\bolds{\\beta}}}^{(1)},\\ldots,\\hat{{\\bolds{\\beta}}}^{(M)} )$ and $\\hat{\\bar\n{{\\bolds{\\xi}}}} = (\\hat{{\\bolds{\\xi}}}^{(1)},\\ldots,\\hat{{\\bolds{\\xi}}}^{(M)}\n)$. The oracle estimator for the nonparametric function $g_{0j}^{(m)}$\nis $\\hat{g}_j^{(m)}(z_{ij}) = {\\bolds{\\pi}}(z_{ij})'\\hat{{\\bolds{\\xi}}_j}^{(m)} -\nn^{-1}\\* \\sum_{i=1}^n \\pi(z_{ij})'\\hat{{\\bolds{\\xi}}}_j^{(m)}$ for $j=1,\\ldots\n,d$; for $g_{00}^{(m)}$ is $\\hat{g}_0^{(m)} = \\hat{\\xi}_0^{(m)} +\nn^{-1} \\sum_{i=1}^n \\times\\break \\sum_{j=1}^d \\pi(z_{ij})'\\hat{{\\bolds{\\xi}}}_j^{(m)}$.\nThe oracle estimator of $g_0^{(m)}({\\mathbf{z}}_i)$ is $\\hat{g}^{(m)}({\\mathbf{z}}_i)\n= \\hat{g}_0^{(m)} +\\break \\sum_{j=1}^d \\hat{g}_j^{(m)}(z_{ij})$.\n\n\n\nAs the next theorem suggests, Theorem \\ref{scad_local_min} can be extended to the\nmultiple quantile case. To save space, we present the regularity\nconditions and the technical derivations in the online supplementary\nmaterial [\\citet{Supp}].\n\n\n\n\\begin{theorem}\n\\label{scad_local_min_mult}\nAssume Conditions \\textup{B1--B6} in the online supplementary material [\\citet{Supp}] are satisfied.\nLet\\vspace*{1pt} $\\bar{\\mathcal{E}}_n(\\lambda)$ be the set of local minima of the\npenalized objective function $\\bar{Q}^P({\\bolds{\\beta}},\\gamma)$.\nConsider either the SCAD or the MCP penalty\\vspace*{1pt} function with tuning\nparameter $\\lambda$. Let $\\hat{\\bar{{\\bolds{\\eta}}}} \\equiv(\\hat\n{\\bar{{\\bolds{\\beta}}}},\\hat{\\bar{{\\bolds{\\xi}}}} )$ be the oracle estimator\nthat solves (\\ref{mm}). If \n$\\lambda= o (n^{-(1-C_4)/2} )$, $n^{-1/2}\\bar{q}_n =\no(\\lambda)$, $n^{-1/2}k_n = o(\\lambda)$ and $\\log(p_n) = o(n\\lambda^2)$,\nthen\n\n", "index": 37, "text": "\n\\[\nP \\bigl(\\hat{\\bar{{\\bolds{\\eta}}}} \\in\\bar{\\mathcal{E}}_n(\\lambda) \\bigr)\n\\rightarrow1\\qquad\\mbox{as } n \\rightarrow\\infty.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"P\\bigl{(}\\hat{\\bar{{\\bolds{\\eta}}}}\\in\\bar{\\mathcal{E}}_{n}(\\lambda)\\bigr{)}%&#10;\\rightarrow 1\\qquad\\mbox{as }n\\rightarrow\\infty.\" display=\"block\"><mrow><mi>P</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mover accent=\"true\"><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">^</mo></mover><mo>\u2208</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2192</mo><mn>1</mn><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>as\u00a0</mtext><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nWe write ${\\bolds{\\gamma}}=(\\gamma_0,{\\bolds{\\gamma}}_1',\\ldots,{\\bolds{\\gamma}}_d')'$, where\n$\\gamma_0\\in\\mathcal{R}$, ${\\bolds{\\gamma}}_j\\in\\mathcal{R}^{k_n+l}$,\n$j=1,\\ldots,d$;\nand we write $\\hat{{\\bolds{\\gamma}}}=(\\hat{\\gamma}_0,\\hat{{\\bolds{\\gamma}}\n}_1',\\ldots,\\hat{{\\bolds{\\gamma}}}_d')'$\nthe same fashion.\nIt can be shown that (see the supplemental material) $\\hat{\\bolds\n{c}}_1=\\hat{\\bolds{{\\bolds{\\beta}}}}_1$.\nSo the change of the basis functions for the nonlinear part does not\nalter the estimator for the linear part.\nLet\n$\\tilde{g}_j({\\mathbf{z}}_i) = w(z_{ij})'\\hat{{\\bolds{\\gamma}}}_j$\nbe the estimator of $g_{0j}$,\n$j=1,\\ldots,d$. The estimator for $g_{00}$ is\n$\\tilde{g}_{0}=k_n^{-1/2}\\hat{{\\bolds{\\gamma}}}_0$.\nThe estimator for $g_{0}({\\mathbf{z}}_i)$ is $\\tilde{g}({\\mathbf{z}}_i)={\\mathbf{W}}({\\mathbf{z}}\n_i)'\\hat\n{{\\bolds{\\gamma}}}=\\tilde{g}_0+\n\\sum_{j=1}^{d}\\tilde{g}_j({\\mathbf{z}}_i)$. It can be derived that (see the\nsupplemental material)\n$\\hat{g}_j({\\mathbf{z}}_i)=\\tilde{g}_j({\\mathbf{z}}_i)-n^{-1}\\sum_{i=1}^n\\tilde\n{g}_j({\\mathbf{z}}_i)$\nand $\\hat{g}_0=\\tilde{g}_{0}+n^{-1}\\sum_{i=1}^n\\sum_{j=1}^d\\tilde\n{g}_j({\\mathbf{z}}_i)$.\nHence, $\\hat{g}=\\hat{g}_0+\\sum_{j=1}^d\\hat{g}_j=\\tilde{g}$.\n\nLater, we will show\n$n^{-1} \\sum_{i=1}^n (\\tilde{g}({\\mathbf{z}}_i) - g_{0}({\\mathbf{z}}_i)\n)^2 =\nO_p (n^{-1}(q_n+dJ_n) )$.\n\n\n\n\n\nThroughout the proof, we will also use the following notation:\n\n\\begin{eqnarray*}\n\n\\psi_\\tau(\\varepsilon_i) &=& \\tau-I(\\varepsilon_i < 0),\n\\\\\nW&=& \\bigl({\\mathbf{W}}({\\mathbf{z}}_1),\\ldots,{\\mathbf{W}}({\\mathbf{z}}_n)\n\\bigr)' \\in{\\mathbb}{R}^{n \\times\nJ_n},\n\\\\\nP &=& W\\bigl(W'B_nW\\bigr)^{-1}W'B_n\n\\in{\\mathbb}{R}^{n \\times n},\n\\\\\nX^* &=&\\bigl({\\mathbf{x}}_1^*,\\ldots,{\\mathbf{x}}_n^*\n\\bigr)'=(I_n-P)X_A \\in{\\mathbb}{R}^{n\n\\times q_n},\n\\\\\nW_B^2 &=& W'B_nW \\in\n{\\mathbb}{R}^{J_n \\times J_n},\n\\\\\n\n\n{\\bolds{\\theta}}_1 &=& \\sqrt{n} ({\\mathbf{c}}_1-\n{\\bolds{\\beta}}_{10} )\\in{\\mathbb}{R}^{q_n},\n\\\\\n{\\bolds{\\theta}}_2 &=& W_B ({\\bolds{\\gamma}}-{\\bolds{\\gamma}}_0 ) +\nW_B^{-1}W'B_nX_A(\n{\\mathbf{c}}_1-{\\bolds{\\beta}}_{10}) \\in{\\mathbb}{R}^{J_n},\n\\\\\n\\tilde{{\\mathbf{x}}}_i &=& n^{-1/2}{\\mathbf{x}}_i^* \\in\n{\\mathbb}{R}^{q_n},\n\\\\\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)&=& W_B^{-1}{\\mathbf{W}}(\n{\\mathbf{z}}_i)\\in{\\mathbb}{R}^{J_n},\n\\\\\n\\tilde{{\\mathbf{s}}}_i &=& \\bigl(\\tilde{{\\mathbf{x}}}_i',\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i) \\bigr)' \\in{\\mathbb}{R}^{q_n+J_n},\n\\\\\nu_{ni}&=&{\\mathbf{W}}({\\mathbf{z}}_i)'{\\bolds{\\gamma}}_{0}\n- g_{0}({\\mathbf{z}}_i).\n\\end{eqnarray*}\n\nNotice that\n\n", "itemtype": "equation", "pos": 71469, "prevtext": "\n\n\\end{theorem}\n\n\n\n\n\n\n\n\n\\subsection*{A numerical example}\nTo assess the multiple quantile\nestimator, we ran 100 simulations using the setting presented in\nSection~\\ref{sec4} with $\\varepsilon_i \\sim T_3$, and consider $\\tau=\n0.5$, 0.7\nand 0.9. We compare the variable selection performance of\nthe multiple-quantile estimator (denoted by Q-group) in this section\nwith the method that estimates each quantile separately (denoted by\nQ-ind). For both approaches, we use the SCAD penalty function. Results\nfor the MCP penalty are included in the online supplementary material\n[\\citet{Supp}]. We also report results from the\nmultiple-quantile oracle estimator (denotes by Q-oracle) which assumes\nthe knowledge of the underlying model and serves as a benchmark.\n\n\n\n\\begin{table}[b]\n\\tabcolsep=0pt\n\\caption{Comparison of group and individual penalty functions for\nmultiple quantile estimation with $\\varepsilon\\sim T_3$}\\label{tab7}\n\\begin{tabular*}{\\tablewidth}{@{\\extracolsep{\\fill}}@{}lcd{1.2}d{1.2}d{1.2}c@{}}\n\\hline\n\\textbf{Method} & \\multicolumn{1}{c}{$\\bolds{p}$} & \\multicolumn{1}{c}{\\textbf{FV}} & \\multicolumn{1}{c}{\\textbf{TV}} & \\multicolumn{1}{c}{\\textbf{True}}\n& \\multicolumn{1}{c@{}}{$\\bolds{L_2}$ \\textbf{error}}\n\\\\\n\\hline\nQ-group-SCAD & 300 & 1.01 & 4 & 0.49 & 0.14 \\\\\nQ-ind-SCAD & 300 & 0.98 & 4 & 0.45 & 0.17 \\\\\nQ-oracle & 300 & 0 & 4 & 1 & 0.06\n\\[3pt]\nQ-group-SCAD & 600 & 1.2 & 4 & 0.56 & 0.15 \\\\\nQ-ind-SCAD & 600 & 1.51 & 3.99 & 0.34 & 0.17 \\\\\nQ-oracle & 600 & 0 & 4 & 1 & 0.07 \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\nTable~\\ref{tab7} summarizes the simulation results for $n=50$, $p=300$ and 600.\nAs in \\citet{zouYuan}, when evaluating the Q-ind method,\nat quantile level $\\tau_m$, we define $A_m=\\{j: \\hat{\\beta\n}_j^{(m)}\\neq0\\}$ be\nthe index set of estimated nonzero coefficients at this quantile level.\nLet $\\bigcup_{m=1}^MA_m$ be the set of the selected variables using Q-ind.\nAs the simulations results in Section~\\ref{sec4}, we report FV, TV and TRUE.\nWe also report the error for estimating the linear coefficients ($L_2$ error),\nwhich is defined as the average of $M^{-1} \\sum_{m=1}^M (\\hat\n{{\\bolds{\\beta}}}^{(m)}-{\\bolds{\\beta}}_{0}^{(m)} )^2$\nover all simulation runs. The results demonstrate that\ncomparing with Q-ind, the new method Q-group has lower false discovery rate,\nhigher probability of identifying the true underlying model and smaller\nestimation error.\n\n\n\\section{Discussion}\\label{sec7}\nWe considered nonconvex penalized estimation for partially linear\nadditive quantile regression models\nwith high dimensional linear covariates. We derive the oracle theory\nunder mild conditions.\nWe have focused on estimating a particular quantile of interest and\nalso considered an extension\nto simultaneous variable selection at multiple quantiles.\n\nA problem of important practical interest is how to identify which\ncovariates should be modeled linearly and which\ncovariates should be modeled nonlinearly. Usually, we do not have such\nprior knowledge in real data analysis.\nThis is a challenging problem in high dimension. Recently, important\nprogresses have been made by\n\\citet{ZCL}; \\citet{HWM}; \\citet{Lian} for\nsemiparametric mean\nregression models. We plan on addressing this question for high\ndimensional semiparametric quantile regression in our future research.\n\nAnother relevant problem of practical interest is to estimate the\nconditional quantile function itself.\nGiven ${\\mathbf{x}}^*$, ${\\mathbf{z}}^*$, we can estimate $Q_{Y_i {|}{\\mathbf{x}}^*, {\\mathbf{z}}\n^*}(\\tau)$\nby ${{\\mathbf{x}}^*}'\\hat{{\\bolds{\\beta}}}_{1} + \\hat{g}({\\mathbf{z}}^*)$, where\n$\\hat{{\\bolds{\\beta}}}$ and $\\hat{g}$ are obtained from penalized quantile regression.\nWe conjecture that the consistency of estimating the conditional quantile\nfunction can be derived under somewhat weaker conditions\nin the current paper, as motivated by the results on \\textit{persistency}\nfor linear mean regression in\nhigh dimension [\\citet{Green}]. The details will also be\nfurther investigated\nin the future.\n\n\n\\begin{appendix}\\label{sec8}\\label{appe}\n\n\\section*{Appendix}\n\n\nThroughout the appendix, we use $C$ to denote a positive constant which\ndoes not depend on $n$ and may vary from\nline to line. For a vector ${\\mathbf{x}}$, ${\\Vert} {\\mathbf{x}}{\\Vert} $\ndenotes its Euclidean norm.\nFor a matrix $A$, ${\\Vert} A{\\Vert} = \\sqrt{\\lambda_{\\max}(A'A)}$\ndenotes its\nspectral norm. For a function $h(\\cdot)$ on $[0,1]$, ${\\Vert}\nh{\\Vert} _{\\infty}\n= \\mathop{\\operatorname{\\sup}}_{x} |h(x)|$\ndenotes the uniform norm. Let $I_n$ denote an $n\\times n$ identity matrix.\n\n\n\\subsection{Derivation of the results in Section~\\texorpdfstring{\\protect\\ref{sec2}}{2}}\\label{sec8.1}\n\n\n\\subsubsection{Notation}\\label{sec8.1.1}\nTo facilitate the proof, we will make use of the theoretically centered\nB-spline basis functions\nsimilar to the approach used by \\citet{XY}. More specifically,\nwe consider the B-spline basis functions $b_j(\\cdot)$ in\\vspace*{2pt} Section~\\ref{sec2.1}\nand let $B_j(z_{ik}) = b_{j+1}(z_{ik}) - \\frac{E\n[b_{j+1}(z_{ik}) ]}{E [b_1(z_{ik}) ]}b_1(z_{ik})$\nfor $j=1,\\ldots, k_n+l$. Then $E(B_j(z_{ik}))=0$.\nFor a given covariate $z_{ik}$,\nlet\n${\\mathbf w}(z_{ik})= (B_1(z_{ik}),\\ldots,B_{k_n+l}(z_{ik}) )'$\nbe the vector of basis functions, and ${\\mathbf{W}}({\\mathbf{z}}_i)$ denote the\n$J_n$-dimensional vector\n$ (k_n^{-1/2},{\\mathbf w}(z_{i1})',\\ldots,{\\mathbf w}(z_{id})' )'$,\nwhere $J_n=d(k_n+l)+1$.\n\nBy the result of \\citeauthor{Schumaker} [(\\citeyear{Schumaker}),\npage~227], there exists a\nvector ${\\bolds{\\gamma}}_0\\in\\mathcal{R}^{J_n}$ and a\npositive constant\n$C_0$, such that $\\sup_{t\\in[0,1]^{d}}|g_0({\\mathbf{t}})-{\\mathbf{W}}({\\mathbf{t}})'{\\bolds{\\gamma}}\n_0|\\leq C_0k_n^{-r}$.\nLet\n\n\n\n\n", "index": 39, "text": "\\begin{equation}\n\\label{orcObjFun2} (\\hat{\\bolds{{\\mathbf{c}}}}_1, \\hat{{\\bolds{\\gamma}}} ) = \\mathop{\n\\operatorname{argmin}}_{ ({\\mathbf{c}}_1, {\\bolds{\\gamma}})} \\frac\n{1}{n} \\sum\n_{i=1}^n\\rho_\\tau\\bigl(Y_i\n- {\\mathbf{x}}_{A_i}'{\\mathbf{c}}_1 - {\\mathbf{W}}({\\mathbf{z}}\n_{i})'{\\bolds{\\gamma}}\\bigr).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"(\\hat{\\bolds{{\\mathbf{c}}}}_{1},\\hat{{\\bolds{\\gamma}}})=\\mathop{\\operatorname{%&#10;argmin}}_{({\\mathbf{c}}_{1},{\\bolds{\\gamma}})}\\frac{1}{n}\\sum_{i=1}^{n}\\rho_{%&#10;\\tau}\\bigl{(}Y_{i}-{\\mathbf{x}}_{A_{i}}^{\\prime}{\\mathbf{c}}_{1}-{\\mathbf{W}}(%&#10;{\\mathbf{z}}_{i})^{\\prime}{\\bolds{\\gamma}}\\bigr{)}.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><munder><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>a</mi><mo movablelimits=\"false\">\u2062</mo><mi>r</mi><mo movablelimits=\"false\">\u2062</mo><mi>g</mi><mo movablelimits=\"false\">\u2062</mo><mi>m</mi><mo movablelimits=\"false\">\u2062</mo><mi>i</mi><mo movablelimits=\"false\">\u2062</mo><mi>n</mi></mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1c</mi><mn>1</mn></msub><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udc31</mi><msub><mi>A</mi><mi>i</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><mi>\ud835\udc16</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b3</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\n\nDefine the minimizers under the transformation as\n\n", "itemtype": "equation", "pos": 74308, "prevtext": "\n\nWe write ${\\bolds{\\gamma}}=(\\gamma_0,{\\bolds{\\gamma}}_1',\\ldots,{\\bolds{\\gamma}}_d')'$, where\n$\\gamma_0\\in\\mathcal{R}$, ${\\bolds{\\gamma}}_j\\in\\mathcal{R}^{k_n+l}$,\n$j=1,\\ldots,d$;\nand we write $\\hat{{\\bolds{\\gamma}}}=(\\hat{\\gamma}_0,\\hat{{\\bolds{\\gamma}}\n}_1',\\ldots,\\hat{{\\bolds{\\gamma}}}_d')'$\nthe same fashion.\nIt can be shown that (see the supplemental material) $\\hat{\\bolds\n{c}}_1=\\hat{\\bolds{{\\bolds{\\beta}}}}_1$.\nSo the change of the basis functions for the nonlinear part does not\nalter the estimator for the linear part.\nLet\n$\\tilde{g}_j({\\mathbf{z}}_i) = w(z_{ij})'\\hat{{\\bolds{\\gamma}}}_j$\nbe the estimator of $g_{0j}$,\n$j=1,\\ldots,d$. The estimator for $g_{00}$ is\n$\\tilde{g}_{0}=k_n^{-1/2}\\hat{{\\bolds{\\gamma}}}_0$.\nThe estimator for $g_{0}({\\mathbf{z}}_i)$ is $\\tilde{g}({\\mathbf{z}}_i)={\\mathbf{W}}({\\mathbf{z}}\n_i)'\\hat\n{{\\bolds{\\gamma}}}=\\tilde{g}_0+\n\\sum_{j=1}^{d}\\tilde{g}_j({\\mathbf{z}}_i)$. It can be derived that (see the\nsupplemental material)\n$\\hat{g}_j({\\mathbf{z}}_i)=\\tilde{g}_j({\\mathbf{z}}_i)-n^{-1}\\sum_{i=1}^n\\tilde\n{g}_j({\\mathbf{z}}_i)$\nand $\\hat{g}_0=\\tilde{g}_{0}+n^{-1}\\sum_{i=1}^n\\sum_{j=1}^d\\tilde\n{g}_j({\\mathbf{z}}_i)$.\nHence, $\\hat{g}=\\hat{g}_0+\\sum_{j=1}^d\\hat{g}_j=\\tilde{g}$.\n\nLater, we will show\n$n^{-1} \\sum_{i=1}^n (\\tilde{g}({\\mathbf{z}}_i) - g_{0}({\\mathbf{z}}_i)\n)^2 =\nO_p (n^{-1}(q_n+dJ_n) )$.\n\n\n\n\n\nThroughout the proof, we will also use the following notation:\n\n\\begin{eqnarray*}\n\n\\psi_\\tau(\\varepsilon_i) &=& \\tau-I(\\varepsilon_i < 0),\n\\\\\nW&=& \\bigl({\\mathbf{W}}({\\mathbf{z}}_1),\\ldots,{\\mathbf{W}}({\\mathbf{z}}_n)\n\\bigr)' \\in{\\mathbb}{R}^{n \\times\nJ_n},\n\\\\\nP &=& W\\bigl(W'B_nW\\bigr)^{-1}W'B_n\n\\in{\\mathbb}{R}^{n \\times n},\n\\\\\nX^* &=&\\bigl({\\mathbf{x}}_1^*,\\ldots,{\\mathbf{x}}_n^*\n\\bigr)'=(I_n-P)X_A \\in{\\mathbb}{R}^{n\n\\times q_n},\n\\\\\nW_B^2 &=& W'B_nW \\in\n{\\mathbb}{R}^{J_n \\times J_n},\n\\\\\n\n\n{\\bolds{\\theta}}_1 &=& \\sqrt{n} ({\\mathbf{c}}_1-\n{\\bolds{\\beta}}_{10} )\\in{\\mathbb}{R}^{q_n},\n\\\\\n{\\bolds{\\theta}}_2 &=& W_B ({\\bolds{\\gamma}}-{\\bolds{\\gamma}}_0 ) +\nW_B^{-1}W'B_nX_A(\n{\\mathbf{c}}_1-{\\bolds{\\beta}}_{10}) \\in{\\mathbb}{R}^{J_n},\n\\\\\n\\tilde{{\\mathbf{x}}}_i &=& n^{-1/2}{\\mathbf{x}}_i^* \\in\n{\\mathbb}{R}^{q_n},\n\\\\\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)&=& W_B^{-1}{\\mathbf{W}}(\n{\\mathbf{z}}_i)\\in{\\mathbb}{R}^{J_n},\n\\\\\n\\tilde{{\\mathbf{s}}}_i &=& \\bigl(\\tilde{{\\mathbf{x}}}_i',\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i) \\bigr)' \\in{\\mathbb}{R}^{q_n+J_n},\n\\\\\nu_{ni}&=&{\\mathbf{W}}({\\mathbf{z}}_i)'{\\bolds{\\gamma}}_{0}\n- g_{0}({\\mathbf{z}}_i).\n\\end{eqnarray*}\n\nNotice that\n\n", "index": 41, "text": "\n\\[\nn^{-1}\\sum_{i=1}^n\n\\rho_\\tau\\bigl(Y_i - {\\mathbf{x}}_{A_i}'\n{\\mathbf{c}}_1 - {\\mathbf{W}}({\\mathbf{z}}_i)'{\\bolds{\\gamma}}\\bigr) =\nn^{-1}\\sum_{i=1}^n\n\\rho_\\tau\\bigl(\\varepsilon_i - \\tilde{{\\mathbf{x}}}_i'\n{\\bolds{\\theta}}_1 - \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'\n{\\bolds{\\theta}}_2 - u_{ni}\\bigr).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"n^{-1}\\sum_{i=1}^{n}\\rho_{\\tau}\\bigl{(}Y_{i}-{\\mathbf{x}}_{A_{i}}^{\\prime}{%&#10;\\mathbf{c}}_{1}-{\\mathbf{W}}({\\mathbf{z}}_{i})^{\\prime}{\\bolds{\\gamma}}\\bigr{)%&#10;}=n^{-1}\\sum_{i=1}^{n}\\rho_{\\tau}\\bigl{(}\\varepsilon_{i}-\\tilde{{\\mathbf{x}}}_%&#10;{i}^{\\prime}{\\bolds{\\theta}}_{1}-\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_{i})^{%&#10;\\prime}{\\bolds{\\theta}}_{2}-u_{ni}\\bigr{)}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mi>\ud835\udc31</mi><msub><mi>A</mi><mi>i</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><mi>\ud835\udc16</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b3</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>\u03b5</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc16</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>2</mn></msub></mrow><mo>-</mo><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nLet $a_n$ be a sequence of positive numbers and define\n\n\\begin{eqnarray*}\nQ_i(a_n) &\\equiv& Q_i(a_n\n{\\bolds{\\theta}}_1,a_n{\\bolds{\\theta}}_2) =\n\\rho_\\tau\\bigl(\\varepsilon_i - a_n\\tilde{\n{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 - a_n\\tilde{\n{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 - u_{ni}\n\\bigr),\n\\\\\n\n\nE_s[Q_i] &=& E [Q_i\n{|}{\\mathbf{x}}_i,{\\mathbf{z}}_i ].\n\\end{eqnarray*}\n\nLet ${\\bolds{\\theta}}=({\\bolds{\\theta}}_1', {\\bolds{\\theta}}_2')'$. Define\n\n\n\n\\begin{eqnarray}\\label{d_i_defined}\nD_i({\\bolds{\\theta}},a_n) &=&\nQ_i(a_n) - Q_{i}(0) - E_s\n\\bigl[Q_i(a_n)-Q_i(0) \\bigr]\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} + a_n \\bigl(\\tilde{{\\mathbf{x}}}_i'\n{\\bolds{\\theta}}_1 + \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}\n_2 \\bigr)\\psi_\\tau(\\varepsilon_i).\n\\end{eqnarray}\n\n\nNoting that $\\rho_\\tau(u) = \\frac{1}{2}|u| + (\\tau- \\frac\n{1}{2} )u$, we have\n\n\n\n\\begin{eqnarray}\n\\label{q_i_equality} Q_i(a_n) - Q_i(0) &=&\n\\tfrac{1}{2} \\bigl[ \\bigl{\\vert}\\varepsilon_i - a_n\n\\tilde{{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 - a_n\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 -\nu_{ni} \\bigr{\\vert}- {\\vert}\\varepsilon_i -\nu_{ni} {\\vert}\\bigr]\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{}- a_n \\bigl(\\tau- \\tfrac{1}{2} \\bigr) \\bigl(\\tilde{\n{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 + \\tilde{{\\mathbf{W}}}(\n{\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 \\bigr).\n\\end{eqnarray}\n\nDefine\n\n", "itemtype": "equation", "pos": 74667, "prevtext": "\n\n\n\nDefine the minimizers under the transformation as\n\n", "index": 43, "text": "\n\\[\n(\\hat{{\\bolds{\\theta}}}_1,\\hat{{\\bolds{\\theta}}}_2 ) = \\mathop{\n\\operatorname{arg}\\operatorname{min}}_{({\\bolds{\\theta}}_1,{\\bolds{\\theta}}_2)} n^{-1}\n\\sum\n_{i=1}^n\\rho_\\tau\\bigl(\n\\varepsilon_i - \\tilde{{\\mathbf{x}}}_i'{\\bolds{\\theta}}\n_1 - \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'\n{\\bolds{\\theta}}_2 - u_{ni}\\bigr).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"(\\hat{{\\bolds{\\theta}}}_{1},\\hat{{\\bolds{\\theta}}}_{2})=\\mathop{\\operatorname{%&#10;arg}\\operatorname{min}}_{({\\bolds{\\theta}}_{1},{\\bolds{\\theta}}_{2})}n^{-1}%&#10;\\sum_{i=1}^{n}\\rho_{\\tau}\\bigl{(}\\varepsilon_{i}-\\tilde{{\\mathbf{x}}}_{i}^{%&#10;\\prime}{\\bolds{\\theta}}_{1}-\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_{i})^{\\prime}{%&#10;\\bolds{\\theta}}_{2}-u_{ni}\\bigr{)}.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><munder><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>a</mi><mo movablelimits=\"false\">\u2062</mo><mi>r</mi><mo movablelimits=\"false\">\u2062</mo><mi>g</mi><mo movablelimits=\"false\">\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo movablelimits=\"false\">\u2062</mo><mi>m</mi><mo movablelimits=\"false\">\u2062</mo><mi>i</mi><mo movablelimits=\"false\">\u2062</mo><mi>n</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>1</mn></msub></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mrow><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c1</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>\u03b5</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc16</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>2</mn></msub></mrow><mo>-</mo><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nThen by combining (\\ref{d_i_defined}) and (\\ref{q_i_equality}),\n\n\n\n\n", "itemtype": "equation", "pos": 76349, "prevtext": "\n\nLet $a_n$ be a sequence of positive numbers and define\n\n\\begin{eqnarray*}\nQ_i(a_n) &\\equiv& Q_i(a_n\n{\\bolds{\\theta}}_1,a_n{\\bolds{\\theta}}_2) =\n\\rho_\\tau\\bigl(\\varepsilon_i - a_n\\tilde{\n{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 - a_n\\tilde{\n{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 - u_{ni}\n\\bigr),\n\\\\\n\n\nE_s[Q_i] &=& E [Q_i\n{|}{\\mathbf{x}}_i,{\\mathbf{z}}_i ].\n\\end{eqnarray*}\n\nLet ${\\bolds{\\theta}}=({\\bolds{\\theta}}_1', {\\bolds{\\theta}}_2')'$. Define\n\n\n\n\\begin{eqnarray}\\label{d_i_defined}\nD_i({\\bolds{\\theta}},a_n) &=&\nQ_i(a_n) - Q_{i}(0) - E_s\n\\bigl[Q_i(a_n)-Q_i(0) \\bigr]\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{} + a_n \\bigl(\\tilde{{\\mathbf{x}}}_i'\n{\\bolds{\\theta}}_1 + \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}\n_2 \\bigr)\\psi_\\tau(\\varepsilon_i).\n\\end{eqnarray}\n\n\nNoting that $\\rho_\\tau(u) = \\frac{1}{2}|u| + (\\tau- \\frac\n{1}{2} )u$, we have\n\n\n\n\\begin{eqnarray}\n\\label{q_i_equality} Q_i(a_n) - Q_i(0) &=&\n\\tfrac{1}{2} \\bigl[ \\bigl{\\vert}\\varepsilon_i - a_n\n\\tilde{{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 - a_n\n\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 -\nu_{ni} \\bigr{\\vert}- {\\vert}\\varepsilon_i -\nu_{ni} {\\vert}\\bigr]\n\\nonumber\\[-8pt]\\[-8pt]\\nonumber\n&&{}- a_n \\bigl(\\tau- \\tfrac{1}{2} \\bigr) \\bigl(\\tilde{\n{\\mathbf{x}}}_i'{\\bolds{\\theta}}_1 + \\tilde{{\\mathbf{W}}}(\n{\\mathbf{z}}_i)'{\\bolds{\\theta}}_2 \\bigr).\n\\end{eqnarray}\n\nDefine\n\n", "index": 45, "text": "\n\\[\nQ_i^*(a_n) = \\tfrac{1}{2} \\bigl[ \\bigl{\\vert}\n\\varepsilon_i - \\tilde{{\\mathbf{x}}}_i'\n{\\bolds{\\theta}}_1a_n - \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'\n{\\bolds{\\theta}}_2a_n - u_{ni} \\bigr{\\vert}- {\\vert}\n\\varepsilon_i - u_{ni} {\\vert}\\bigr].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"Q_{i}^{*}(a_{n})=\\tfrac{1}{2}\\bigl{[}\\bigl{|}\\varepsilon_{i}-\\tilde{{\\mathbf{x%&#10;}}}_{i}^{\\prime}{\\bolds{\\theta}}_{1}a_{n}-\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_{i%&#10;})^{\\prime}{\\bolds{\\theta}}_{2}a_{n}-u_{ni}\\bigr{|}-{|}\\varepsilon_{i}-u_{ni}{%&#10;|}\\bigr{]}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>Q</mi><mi>i</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\tfrac</mtext></merror><mo>\u2062</mo><mn>12</mn><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><msub><mi>\u03b5</mi><mi>i</mi></msub><mo>-</mo><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc16</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo>-</mo><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>\u03b5</mi><mi>i</mi></msub><mo>-</mo><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\n\n\n\n\\subsubsection{Some technical lemmas}\\label{sec8.1.2}\n\nThe proofs of Lemmas \\ref{spline}--\\ref{lem_g_hat_rate} below are\ngiven in the supplemental material [\\citet{Supp}].\n\n\n\n\\begin{lemma}\\label{spline}\nWe have the following properties for the spline basis vector:\n\\begin{longlist}[(3)]\n\\item[(1)] $E({\\Vert} {\\mathbf{W}}({\\mathbf{z}}_i){\\Vert} )\\leq b_1$, $\\forall i$, for some\npositive constant\n$b_1$ for\nall $n$ sufficiently large.\n\n\\item[(2)] There exists positive constant $b_2$ and $b_2^*$ such that for\nall $n$ sufficiently large\n$E(\\lambda_{\\min}({\\mathbf{W}}({\\mathbf{z}}_i){\\mathbf{W}}({\\mathbf{z}}_i)^T))\\geq b_2k_n^{-1}$ and\n$E(\\lambda_{\\max}({\\mathbf{W}}({\\mathbf{z}}_i){\\mathbf{W}}({\\mathbf{z}}_i)^T))\\leq b_2^*k_n^{-1}$.\n\n\\item[(3)] $E({\\Vert} W_B^{-1}{\\Vert} )\\geq b_3\\sqrt{k_nn^{-1}}$,\nfor some positive constant $b_3$, for all $n$ sufficiently large.\n\n\\item[(4)]\n$\\mathop{\\max}_{i} {\\Vert} \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i){\\Vert} =O_p(\\sqrt{\\frac\n{k_n}{n}})$.\n\n\\item[(5)] $\\sum_{i=1}^nf_i(0)\\tilde{{\\mathbf{x}}}_i\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'={\\mathbf0}$.\n\\end{longlist}\n\\end{lemma}\n\n\n\n\\begin{lemma}\n\\label{x_star_big_q}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} are satisfied,\nthen:\n\\begin{longlist}[(3)]\n\\item[(1)] There exists a positive constant $C$ such that\n$\\lambda_{\\max} (n^{-1} {X^*}'X^* ) \\leq C$, with\nprobability one.\n\n\\item[(2)] $n^{-1/2}X^* = n^{-1/2}\\Delta_n + o_p(1)$.\nFurthermore, $n^{-1}{X^*}'B_nX^* = K_n + o_p(1)$, where $B_n$ and $K_n$\nare defined as in Theorem \\ref{large_q_clt}.\n\\end{longlist}\n\\end{lemma}\n\n\n\n\\begin{lemma}\n\\label{lem_g_hat_rate}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold, then\n\n$n^{-1}\\sum_{i=1}^n(\\tilde{g}({\\mathbf{z}}_i)-g_0({\\mathbf{z}}_i))^2 = O_p\n(d_n/n )$.\n\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}\n\\label{theta_tilde_rate}\nAssume Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold.\nLet $\\tilde{{\\bolds{\\theta}}}_1 = \\sqrt{n} ({X^*}'B_nX^*\n)^{-1}\\* {X^*}'\\psi_\\tau(\\varepsilon)$,\nwhere $\\psi_\\tau(\\varepsilon)=(\\psi_\\tau(\\varepsilon_1),\\ldots,\\psi\n_\\tau(\\varepsilon_n))'$,\nthen:\n\\begin{longlist}[(3)]\n\\item[(1)] ${\\Vert} \\tilde{{\\bolds{\\theta}}}_1{\\Vert} = O_p (\\sqrt{q_n} )$.\n\n\\item[(2)]\n\n\n\n$A_n\\Sigma_n^{-1/2}\\tilde{{\\bolds{\\theta}}}_1 \\stackrel{d}{\\rightarrow} N(0, G)$,\nwhere $A_n$, $\\Sigma_n$ and $G$ are defined in Theorem \\ref{large_q_clt}.\n\\end{longlist}\n\\end{lemma}\n\n\\begin{pf}\n(1) The result follows from the observation that, by Lemma \\ref{x_star_big_q},\n\n", "itemtype": "equation", "pos": 76663, "prevtext": "\n\nThen by combining (\\ref{d_i_defined}) and (\\ref{q_i_equality}),\n\n\n\n\n", "index": 47, "text": "\\begin{equation}\n\\label{d_i_defined_2} D_i({\\bolds{\\theta}},a_n) = Q_i^*(a_n)\n- E_s\\bigl[Q_i^*(a_n)\\bigr] +\na_n \\bigl(\\tilde{{\\mathbf{x}}}_i'\n{\\bolds{\\theta}}_1 + \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'\n{\\bolds{\\theta}}_2 \\bigr)\\psi_\\tau(\\varepsilon_i).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"D_{i}({\\bolds{\\theta}},a_{n})=Q_{i}^{*}(a_{n})-E_{s}\\bigl{[}Q_{i}^{*}(a_{n})%&#10;\\bigr{]}+a_{n}\\bigl{(}\\tilde{{\\mathbf{x}}}_{i}^{\\prime}{\\bolds{\\theta}}_{1}+%&#10;\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_{i})^{\\prime}{\\bolds{\\theta}}_{2}\\bigr{)}%&#10;\\psi_{\\tau}(\\varepsilon_{i}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msubsup><mi>Q</mi><mi>i</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>E</mi><mi>s</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><msubsup><mi>Q</mi><mi>i</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>a</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc31</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><mover accent=\"true\"><mi>\ud835\udc16</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b8</mi><mn>2</mn></msub></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b5</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nand $n^{-1/2}{\\Vert} H-PX_A{\\Vert} =o_{p}(1)$.\n\n(2)\n\n\n\n\n\n\n\\begin{eqnarray*}\nA_n\\Sigma_n^{-1/2}\\tilde{{\\bolds{\\theta}}}_1\n&=& A_n\\Sigma_n^{-1/2}K_n^{-1}\n\\bigl[n^{-1/2}\\Delta_n'\\psi_\\tau(\n\\varepsilon) \\bigr]\\bigl(1+o_p(1)\\bigr)\n\\\\\n&&{} +A_n\\Sigma_n^{-1/2}K_n^{-1}\n\\bigl[n^{-1/2}(H-PX_A) \\bigr]\\psi_\\tau(\\varepsilon)\n\\bigl(1+o_p(1)\\bigr),\n\\end{eqnarray*}\n\nwhere the second term is $o_p(1)$ because $n^{-1/2}{\\Vert}\nH-PX_A{\\Vert} =o(1)$.\nWe write $A_n\\Sigma_n^{-1/2}K_n^{-1} [n^{-1/2}\\Delta_n'\\psi_\\tau\n(\\varepsilon) ]\n=\\sum_{i=1}^nD_{ni}$, where\n", "itemtype": "equation", "pos": 79407, "prevtext": "\n\n\n\n\n\n\\subsubsection{Some technical lemmas}\\label{sec8.1.2}\n\nThe proofs of Lemmas \\ref{spline}--\\ref{lem_g_hat_rate} below are\ngiven in the supplemental material [\\citet{Supp}].\n\n\n\n\\begin{lemma}\\label{spline}\nWe have the following properties for the spline basis vector:\n\\begin{longlist}[(3)]\n\\item[(1)] $E({\\Vert} {\\mathbf{W}}({\\mathbf{z}}_i){\\Vert} )\\leq b_1$, $\\forall i$, for some\npositive constant\n$b_1$ for\nall $n$ sufficiently large.\n\n\\item[(2)] There exists positive constant $b_2$ and $b_2^*$ such that for\nall $n$ sufficiently large\n$E(\\lambda_{\\min}({\\mathbf{W}}({\\mathbf{z}}_i){\\mathbf{W}}({\\mathbf{z}}_i)^T))\\geq b_2k_n^{-1}$ and\n$E(\\lambda_{\\max}({\\mathbf{W}}({\\mathbf{z}}_i){\\mathbf{W}}({\\mathbf{z}}_i)^T))\\leq b_2^*k_n^{-1}$.\n\n\\item[(3)] $E({\\Vert} W_B^{-1}{\\Vert} )\\geq b_3\\sqrt{k_nn^{-1}}$,\nfor some positive constant $b_3$, for all $n$ sufficiently large.\n\n\\item[(4)]\n$\\mathop{\\max}_{i} {\\Vert} \\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i){\\Vert} =O_p(\\sqrt{\\frac\n{k_n}{n}})$.\n\n\\item[(5)] $\\sum_{i=1}^nf_i(0)\\tilde{{\\mathbf{x}}}_i\\tilde{{\\mathbf{W}}}({\\mathbf{z}}_i)'={\\mathbf0}$.\n\\end{longlist}\n\\end{lemma}\n\n\n\n\\begin{lemma}\n\\label{x_star_big_q}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} are satisfied,\nthen:\n\\begin{longlist}[(3)]\n\\item[(1)] There exists a positive constant $C$ such that\n$\\lambda_{\\max} (n^{-1} {X^*}'X^* ) \\leq C$, with\nprobability one.\n\n\\item[(2)] $n^{-1/2}X^* = n^{-1/2}\\Delta_n + o_p(1)$.\nFurthermore, $n^{-1}{X^*}'B_nX^* = K_n + o_p(1)$, where $B_n$ and $K_n$\nare defined as in Theorem \\ref{large_q_clt}.\n\\end{longlist}\n\\end{lemma}\n\n\n\n\\begin{lemma}\n\\label{lem_g_hat_rate}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold, then\n\n$n^{-1}\\sum_{i=1}^n(\\tilde{g}({\\mathbf{z}}_i)-g_0({\\mathbf{z}}_i))^2 = O_p\n(d_n/n )$.\n\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{lemma}\n\\label{theta_tilde_rate}\nAssume Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold.\nLet $\\tilde{{\\bolds{\\theta}}}_1 = \\sqrt{n} ({X^*}'B_nX^*\n)^{-1}\\* {X^*}'\\psi_\\tau(\\varepsilon)$,\nwhere $\\psi_\\tau(\\varepsilon)=(\\psi_\\tau(\\varepsilon_1),\\ldots,\\psi\n_\\tau(\\varepsilon_n))'$,\nthen:\n\\begin{longlist}[(3)]\n\\item[(1)] ${\\Vert} \\tilde{{\\bolds{\\theta}}}_1{\\Vert} = O_p (\\sqrt{q_n} )$.\n\n\\item[(2)]\n\n\n\n$A_n\\Sigma_n^{-1/2}\\tilde{{\\bolds{\\theta}}}_1 \\stackrel{d}{\\rightarrow} N(0, G)$,\nwhere $A_n$, $\\Sigma_n$ and $G$ are defined in Theorem \\ref{large_q_clt}.\n\\end{longlist}\n\\end{lemma}\n\n\\begin{pf}\n(1) The result follows from the observation that, by Lemma \\ref{x_star_big_q},\n\n", "index": 49, "text": "\n\\[\n\n\n\\tilde{{\\bolds{\\theta}}}_1 = \\bigl(K_n+o_p(1)\n\\bigr)^{-1} \\bigl[n^{-1/2}\\Delta_n'\n\\psi_\\tau(\\varepsilon)+n^{-1/2}(H-PX_A)\\psi\n_\\tau(\\varepsilon) \\bigr],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\par&#10;\\tilde{{\\bolds{\\theta}}}_{1}=\\bigl{(}K_{n}+o_{p}(1)\\bigr{)}^{-1}%&#10;\\bigl{[}n^{-1/2}\\Delta_{n}^{\\prime}\\psi_{\\tau}(\\varepsilon)+n^{-1/2}(H-PX_{A})%&#10;\\psi_{\\tau}(\\varepsilon)\\bigr{]},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">~</mo></mover><mn>1</mn></msub><mo>=</mo><mrow><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>K</mi><mi>n</mi></msub><mo>+</mo><mrow><msub><mi>o</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><msup><mi>n</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi>n</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>H</mi><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><msub><mi>X</mi><mi>A</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\nTo verify asymptotic normality, we first note that $E(D_{ni})=0$ and\n\n\\begin{eqnarray*}\n\\sum_{i=1}^nE\\bigl(D_{ni}D_{ni}'\n\\bigr) &=&A_n\\Sigma_n^{-1/2}K_n^{-1}S_nK_n^{-1}\n\\Sigma_n^{-1/2}A_n'=\nA_nA_n'\\rightarrow G.\n\\end{eqnarray*}\n\nThe proof is complete by\nchecking the Lindeberg--Feller condition. For any\n$\\varepsilon> 0$ and using Conditions \\ref{cond_f}, \\ref{highd_cond_x}\nand \\ref{cond_sigma_large_p}\n\n\\begin{eqnarray*}\n&&\\sum_{i=1}^nE \\bigl[{\\Vert}\nD_{ni}{\\Vert}^2 I\\bigl({\\Vert} D_{ni}{\\Vert}>\n\\varepsilon\\bigr) \\bigr]\n\\\\\n&&\\qquad \\leq\\varepsilon^{-2} \\sum\n_{i=1}^nE{\\Vert} D_{ni}{\\Vert}\n^4\n\\\\\n&&\\qquad\\leq(n\\varepsilon)^{-2} \\sum_{i=1}^nE\n\\bigl(\\psi^4_\\tau(\\varepsilon_i) \\bigl(\n{\\bolds{\\delta}}_i'K_n^{-1}\n\\Sigma_n^{-1/2} A_n'A_n\n\\Sigma_n^{-1/2}K_n^{-1}\n{\\bolds{\\delta}}_i \\bigr)^2 \\bigr)\n\\\\\n&&\\qquad\\leq C n^{-2}\\varepsilon^{-2}\\sum\n_{i=1}^nE\\bigl({\\Vert}{\\bolds{\\delta}}_i{\\Vert}\n^4\\bigr)= O_p\\bigl(q_n^2/n\n\\bigr)= o_p(1),\n\\end{eqnarray*}\n\nwhere the last inequality follows by observing that\n$\\lambda_{\\max}(A_n'A_n)=\\lambda_{\\max}(A_nA_n')\\rightarrow c$ for some finite\npositive constant $c$.\n\\end{pf}\n\n\n\n\n\n\\begin{lemma}\n\\label{lem_theta_tilde_hat}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold, then\n\n", "itemtype": "equation", "pos": 80112, "prevtext": "\n\nand $n^{-1/2}{\\Vert} H-PX_A{\\Vert} =o_{p}(1)$.\n\n(2)\n\n\n\n\n\n\n\\begin{eqnarray*}\nA_n\\Sigma_n^{-1/2}\\tilde{{\\bolds{\\theta}}}_1\n&=& A_n\\Sigma_n^{-1/2}K_n^{-1}\n\\bigl[n^{-1/2}\\Delta_n'\\psi_\\tau(\n\\varepsilon) \\bigr]\\bigl(1+o_p(1)\\bigr)\n\\\\\n&&{} +A_n\\Sigma_n^{-1/2}K_n^{-1}\n\\bigl[n^{-1/2}(H-PX_A) \\bigr]\\psi_\\tau(\\varepsilon)\n\\bigl(1+o_p(1)\\bigr),\n\\end{eqnarray*}\n\nwhere the second term is $o_p(1)$ because $n^{-1/2}{\\Vert}\nH-PX_A{\\Vert} =o(1)$.\nWe write $A_n\\Sigma_n^{-1/2}K_n^{-1} [n^{-1/2}\\Delta_n'\\psi_\\tau\n(\\varepsilon) ]\n=\\sum_{i=1}^nD_{ni}$, where\n", "index": 51, "text": "\n\\[\nD_{ni}=n^{-1/2}A_n\\Sigma_n^{-1/2}K_n^{-1}{\\bolds{\\delta}}_i \\psi_\\tau\n(\\varepsilon_i).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"D_{ni}=n^{-1/2}A_{n}\\Sigma_{n}^{-1/2}K_{n}^{-1}{\\bolds{\\delta}}_{i}\\psi_{\\tau}%&#10;(\\varepsilon_{i}).\" display=\"block\"><mrow><mrow><msub><mi>D</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msup><mi>n</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msub><mi>A</mi><mi>n</mi></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>n</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><msubsup><mi>K</mi><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msub><mi>\u03b4</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b5</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\n\\end{lemma}\n\n\\begin{pf}\nProof provided in online supplementary material [\\citet{Supp}].\n\\end{pf}\n\n\n\\subsubsection{Proof of Theorems \\texorpdfstring{\\protect\\ref{large_q_oracle}}{2.1},\n\\texorpdfstring{\\protect\\ref{large_q_clt}}{2.2} and Corollary \\texorpdfstring{\\protect\\ref{fixed_q_clt}}{1}}\\label\n{sec8.1.3}\n\n\n\n\nBy the observation $\\hat{g}=\\tilde{g}$, Lemma \\ref{lem_g_hat_rate}\nimplies the second result of Theorem~\\ref{large_q_oracle}. The first result\nof Theorem~\\ref{large_q_oracle} follows by observing\n$\\hat{{\\mathbf{c}}}_1=\\hat{{\\bolds{\\beta}}}_1$ and Lemmas \\ref{theta_tilde_rate} and\n\\ref{lem_theta_tilde_hat}.\nThe proof of Theorem \\ref{large_q_clt} follows from Lemmas \\ref\n{theta_tilde_rate} and \\ref{lem_theta_tilde_hat}.\n\nSet $A_n = I_{q}$, then the proof of Corollary \\ref{fixed_q_clt}\nfollows from the fact that $q$ being constant and Theorems \\ref\n{large_q_oracle} and \\ref{large_q_clt}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Derivation of the results in Section~\\texorpdfstring{\\protect\\ref{sec3.3}}{3.3}}\\label{sec8.2}\n\n\n\n\\begin{lemma}\n\\label{lem_diff_convex}\nConsider the function $k(\\eta)-l(\\eta)$ where both $k$ and $l$ are\nconvex with subdifferential functions $\\partial k(\\eta)$ and $\\partial\nl(\\eta)$.\nLet $\\eta^*$ be a point that has neighborhood $U$ such that $\\partial\nl(\\eta) \\cap\\partial k(\\eta^*) \\neq\\varnothing, \\forall \\eta\\in\nU \\cap\\operatorname{dom}(k)$.\nThen $\\eta^*$ is a local minimizer of $k(\\eta) - l(\\eta)$.\n\\end{lemma}\n\n\\begin{pf}\nThe proof is available in \\citet{TaoAn97}.\n\\end{pf}\n\n\n\n\n\\subsubsection{Proof of Lemma \\texorpdfstring{\\protect\\ref{lem_sub_diff_q}}{1}}\\label{sec8.2.1}\n\\mbox{}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_1})}\nBy convex optimization theory $\\mathbf{0} \\in\\partial\\sum_{i=1}^n\\rho\n_\\tau\n(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}})$.\nThus, there exists\n$a_j^*$ as described in the lemma such that with the choice\n$a_j=a_j^*$, we have $s_j(\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})=0$ for\n$j=1,\\ldots,q_n$ or $j=p_n+1,\\ldots,p_n+J_n$.\n\n\\end{pf*}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_2})}\nIt is sufficient to show\n\n$\nP ({\\vert} \\hat{\\beta}_j{\\vert} \\geq(a+1/2)\\lambda$,\nfor $j= 1,\\ldots,q_n ) \\rightarrow1\n$\nas $n,p\\rightarrow\\infty$.\n\nNote that\n\n\n\n\n", "itemtype": "equation", "pos": 81429, "prevtext": "\nTo verify asymptotic normality, we first note that $E(D_{ni})=0$ and\n\n\\begin{eqnarray*}\n\\sum_{i=1}^nE\\bigl(D_{ni}D_{ni}'\n\\bigr) &=&A_n\\Sigma_n^{-1/2}K_n^{-1}S_nK_n^{-1}\n\\Sigma_n^{-1/2}A_n'=\nA_nA_n'\\rightarrow G.\n\\end{eqnarray*}\n\nThe proof is complete by\nchecking the Lindeberg--Feller condition. For any\n$\\varepsilon> 0$ and using Conditions \\ref{cond_f}, \\ref{highd_cond_x}\nand \\ref{cond_sigma_large_p}\n\n\\begin{eqnarray*}\n&&\\sum_{i=1}^nE \\bigl[{\\Vert}\nD_{ni}{\\Vert}^2 I\\bigl({\\Vert} D_{ni}{\\Vert}>\n\\varepsilon\\bigr) \\bigr]\n\\\\\n&&\\qquad \\leq\\varepsilon^{-2} \\sum\n_{i=1}^nE{\\Vert} D_{ni}{\\Vert}\n^4\n\\\\\n&&\\qquad\\leq(n\\varepsilon)^{-2} \\sum_{i=1}^nE\n\\bigl(\\psi^4_\\tau(\\varepsilon_i) \\bigl(\n{\\bolds{\\delta}}_i'K_n^{-1}\n\\Sigma_n^{-1/2} A_n'A_n\n\\Sigma_n^{-1/2}K_n^{-1}\n{\\bolds{\\delta}}_i \\bigr)^2 \\bigr)\n\\\\\n&&\\qquad\\leq C n^{-2}\\varepsilon^{-2}\\sum\n_{i=1}^nE\\bigl({\\Vert}{\\bolds{\\delta}}_i{\\Vert}\n^4\\bigr)= O_p\\bigl(q_n^2/n\n\\bigr)= o_p(1),\n\\end{eqnarray*}\n\nwhere the last inequality follows by observing that\n$\\lambda_{\\max}(A_n'A_n)=\\lambda_{\\max}(A_nA_n')\\rightarrow c$ for some finite\npositive constant $c$.\n\\end{pf}\n\n\n\n\n\n\\begin{lemma}\n\\label{lem_theta_tilde_hat}\nIf Conditions \\ref{cond_f}--\\ref{cond_sigma_large_p} hold, then\n\n", "index": 53, "text": "\n\\[\n{\\Vert}\\hat{{\\bolds{\\theta}}}_1 - \\tilde{{\\bolds{\\theta}}}_1{\\Vert}=\no_p(1).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"{\\|}\\hat{{\\bolds{\\theta}}}_{1}-\\tilde{{\\bolds{\\theta}}}_{1}{\\|}=o_{p}(1).\" display=\"block\"><mrow><mrow><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">~</mo></mover><mn>1</mn></msub></mrow><mo>\u2225</mo></mrow><mo>=</mo><mrow><msub><mi>o</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nBy Condition \\ref{cond_small_sig}, $\\mathop{\\min}_{1 \\leq j \\leq\nq_n} {\\vert} \\beta_{0j}{\\vert} \\geq C_5 n^{-(1-C_4)/2}$.\nBy Theorem \\ref{large_q_oracle} and Conditions \\ref\n{cond_sigma_large_p} and \\ref{cond_small_sig},\n$\\mathop{\\max}_{1 \\leq j \\leq q_n} {\\vert} \\hat{\\beta}_j - \\beta\n_{0j}{\\vert} = O_p (\\sqrt{\\frac{q_n}{n}} ) = o_p\n(n^{-(1-C_4)/2} )$.\n(\\ref{lem_sub_diff_q_1}) holds by noting $\\lambda= o\n(n^{-(1-C_4)/2} )$.\n\\end{pf*}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_3})}\nProof provided in the online supplementary material [\\citet{Supp}].\n\n\n\n\\end{pf*}\n\n\n\\subsubsection{Proof of Theorem \\texorpdfstring{\\protect\\ref{scad_local_min}}{3.1}}\\label{sec8.2.2}\n\n\nRecall that for $\\kappa_{j} \\in\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ \n\n\n\n\\begin{eqnarray*}\n\\kappa_j &=& s_j({\\bolds{\\beta}},{\\bolds{\\xi}}) + \\lambda l_j\n\\qquad\\mbox{for } 1 \\leq j \\leq p_n,\n\\\\\n\n\\kappa_{j} &=& s_{j}({\\bolds{\\beta}},{\\bolds{\\xi}}) \\qquad\\mbox{for }\np_n+1 \\leq j \\leq p_n+J_n.\n\\end{eqnarray*}\n\nDefine the set\n\n\\begin{eqnarray*}\n\\mathcal{G} &=& \\bigl\\{ {\\bolds{\\kappa}}= (\\kappa_1, \\kappa_2,\n\\ldots,\\kappa_{p_n+J_n})': \\kappa_j = \\lambda\n\\operatorname{sgn}(\\hat{\\beta}_j), j=1,\\ldots,q_n;\n\\\\\n&&{} \\kappa_j = s_j(\\hat{{\\bolds{\\beta}}}, \\hat{{\\bolds{\\xi}}}) + \\lambda\nl_j, j=q_n+1,\\ldots,p_n;\n\\\\\n&&{} \\kappa_j = 0, j=p_n+1,\\ldots,p_n+J_n,\n\\bigr\\},\n\\end{eqnarray*}\n\nwhere $l_j$ ranges over $[-1,1]$ for $j= q_n+1,\\ldots,p_n$.\nBy Lemma \\ref{lem_sub_diff_q}, we have $P(\\mathcal{G} \\subset\\partial k(\\hat{{\\bolds{\\beta}}\n},\\hat{{\\bolds{\\xi}}}))\\rightarrow1$.\n\n\n\n\n\n\n\n\nConsider any $({\\bolds{\\beta}}', {\\bolds{\\xi}}')'$ in a ball with the center $\n(\\hat{{\\bolds{\\beta}}}',\\hat{{\\bolds{\\xi}}}' )'$ and radius\n$\\lambda/ 2$.\nBy Lemma \\ref{lem_diff_convex}, to prove the theorem it is sufficient\nto show that there exists ${\\bolds{\\kappa}}^* = (\\kappa_1^*,\\ldots\n,\\kappa^*_{p_n+J_n} )' \\in\\mathcal{G}$\nsuch that\n\n\n\n\n\\begin{eqnarray}\nP \\biggl( \\kappa_j^* = \\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\n\\beta_j}, j=1,\n\\ldots,p_n \\biggr) &\\rightarrow&1; \\label{xi_star_j}\n\\\\\nP \\biggl( \\kappa_{p_n+j}^* = \\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}}\n)}{\\partial\\xi_j}, j=1,\n\\ldots,J_n \\biggr) &\\rightarrow&1. \\label\n{xi_star_j2}\n\\end{eqnarray}\n\nSince $\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\xi_j} = 0$ for\n$j=1,\\ldots,J_n$,\n(\\ref{xi_star_j2}) is satisfied by Lemma \\ref{lem_sub_diff_q}.\n\nWe outline how $\\kappa_j^*$ can be selected to satisfy (\\ref\n{xi_star_j}). \n\n\n\n\n\\begin{longlist}[2.]\n\n\n\n\n\n\n\\item[1.] For $1 \\leq j \\leq q_n$,\nwe have $\\kappa^*_j=\\lambda\\operatorname{sgn}(\\hat{\\beta}_j)$ for $\\beta\n_j \\neq0$. \n\nFor either SCAD or MCP penalty function, $\\frac{\\partial l({\\bolds{\\beta}},\n{\\bolds{\\xi}})}{\\partial\\beta_j} = \\lambda\\operatorname{sgn}(\\beta_j)$ for\n${\\vert} \\beta_j{\\vert} > a\\lambda$. By Lemma \\ref{lem_sub_diff_q},\nwe have\n\n\\begin{eqnarray*}\n\\min_{1\\leq j \\leq q_n}{\\vert}\\beta_j{\\vert}&\\geq& \\min\n_{1 \\leq j \\leq\nq_n}{\\vert}\\hat{\\beta}_j{\\vert}- \\max\n_{1\\leq j \\leq q_n}{\\vert}\\hat{\\beta}_j - \\beta_j\n{\\vert}\n\\geq (a+1/2)\\lambda- \\lambda/2 = a\\lambda,\n\\end{eqnarray*}\n\nwith probability approaching one.\nThus, $P(\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} =\n\\lambda\\operatorname{sgn}(\\beta_j))\\rightarrow1$.\nFor any $1 \\leq j \\leq q_n$, ${\\Vert} \\hat{\\beta}_j-\\beta_{0j}{\\Vert} =\nO_p (n^{-1/2}q_n^{1/2} ) = o(\\lambda)$. Therefore, for\nsufficiently large $n$, $\\hat{\\beta}_j$ and $\\beta_j$ have the same\nsign. This implies\n$P(\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} = \\kappa_j^*,\n1 \\leq j \\leq q_n)\\rightarrow1$ as $n\\rightarrow\\infty$.\n\n\\item[2.]\n\n\n\n\n\nFor $j=q_n+1,\\ldots,p_n$,\n$\\hat{\\beta}_j=0$ by the definition of the oracle estimator and\n$\\kappa_j = \\lambda l_j$ with $l_j \\in[-1,1]$. Therefore,\n\n", "itemtype": "equation", "pos": 83785, "prevtext": "\n\n\\end{lemma}\n\n\\begin{pf}\nProof provided in online supplementary material [\\citet{Supp}].\n\\end{pf}\n\n\n\\subsubsection{Proof of Theorems \\texorpdfstring{\\protect\\ref{large_q_oracle}}{2.1},\n\\texorpdfstring{\\protect\\ref{large_q_clt}}{2.2} and Corollary \\texorpdfstring{\\protect\\ref{fixed_q_clt}}{1}}\\label\n{sec8.1.3}\n\n\n\n\nBy the observation $\\hat{g}=\\tilde{g}$, Lemma \\ref{lem_g_hat_rate}\nimplies the second result of Theorem~\\ref{large_q_oracle}. The first result\nof Theorem~\\ref{large_q_oracle} follows by observing\n$\\hat{{\\mathbf{c}}}_1=\\hat{{\\bolds{\\beta}}}_1$ and Lemmas \\ref{theta_tilde_rate} and\n\\ref{lem_theta_tilde_hat}.\nThe proof of Theorem \\ref{large_q_clt} follows from Lemmas \\ref\n{theta_tilde_rate} and \\ref{lem_theta_tilde_hat}.\n\nSet $A_n = I_{q}$, then the proof of Corollary \\ref{fixed_q_clt}\nfollows from the fact that $q$ being constant and Theorems \\ref\n{large_q_oracle} and \\ref{large_q_clt}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Derivation of the results in Section~\\texorpdfstring{\\protect\\ref{sec3.3}}{3.3}}\\label{sec8.2}\n\n\n\n\\begin{lemma}\n\\label{lem_diff_convex}\nConsider the function $k(\\eta)-l(\\eta)$ where both $k$ and $l$ are\nconvex with subdifferential functions $\\partial k(\\eta)$ and $\\partial\nl(\\eta)$.\nLet $\\eta^*$ be a point that has neighborhood $U$ such that $\\partial\nl(\\eta) \\cap\\partial k(\\eta^*) \\neq\\varnothing, \\forall \\eta\\in\nU \\cap\\operatorname{dom}(k)$.\nThen $\\eta^*$ is a local minimizer of $k(\\eta) - l(\\eta)$.\n\\end{lemma}\n\n\\begin{pf}\nThe proof is available in \\citet{TaoAn97}.\n\\end{pf}\n\n\n\n\n\\subsubsection{Proof of Lemma \\texorpdfstring{\\protect\\ref{lem_sub_diff_q}}{1}}\\label{sec8.2.1}\n\\mbox{}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_1})}\nBy convex optimization theory $\\mathbf{0} \\in\\partial\\sum_{i=1}^n\\rho\n_\\tau\n(Y_i - {\\mathbf{x}}_i'{\\bolds{\\beta}}- \\bolds{\\Pi}({\\mathbf{z}}_i)'{\\bolds{\\xi}})$.\nThus, there exists\n$a_j^*$ as described in the lemma such that with the choice\n$a_j=a_j^*$, we have $s_j(\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})=0$ for\n$j=1,\\ldots,q_n$ or $j=p_n+1,\\ldots,p_n+J_n$.\n\n\\end{pf*}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_2})}\nIt is sufficient to show\n\n$\nP ({\\vert} \\hat{\\beta}_j{\\vert} \\geq(a+1/2)\\lambda$,\nfor $j= 1,\\ldots,q_n ) \\rightarrow1\n$\nas $n,p\\rightarrow\\infty$.\n\nNote that\n\n\n\n\n", "index": 55, "text": "\\begin{equation}\n\\mathop{\\min}_{1\\leq j \\leq q_n} {\\vert}\\hat{\\beta}_j{\\vert}\\geq\n\\mathop{\\min}_{1 \\leq j \\leq q_n} {\\vert}\\beta_{0j}{\\vert}- \\mathop{\n\\max}_{1\n\\leq j \\leq q_n} {\\vert}\\hat{\\beta}_j - \\beta_{0j}\n{\\vert}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\mathop{\\min}_{1\\leq j\\leq q_{n}}{|}\\hat{\\beta}_{j}{|}\\geq\\mathop{\\min}_{1\\leq&#10;j%&#10;\\leq q_{n}}{|}\\beta_{0j}{|}-\\mathop{\\max}_{1\\leq j\\leq q_{n}}{|}\\hat{\\beta}_{j%&#10;}-\\beta_{0j}{|}.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><msub><mi>q</mi><mi>n</mi></msub></mrow></munder><mrow><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><munder><mo movablelimits=\"false\">min</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><msub><mi>q</mi><mi>n</mi></msub></mrow></munder><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>-</mo><mrow><munder><mo movablelimits=\"false\">max</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><msub><mi>q</mi><mi>n</mi></msub></mrow></munder><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub><mo>-</mo><msub><mi>\u03b2</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06000.tex", "nexttext": "\n\nFor ${\\vert} \\beta_j{\\vert} < \\lambda$, $\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}}\n)}{\\partial\\beta_j} = 0$ for the SCAD penalty and $\\frac{\\partial\nl({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} = \\beta_j / a$ for MCP,\n$j=q_n+1,\\ldots,p_n$.\n\n\n\n\n\nNote that for both penalty functions, we have $ |\\frac{l({\\bolds{\\beta}},\n{\\bolds{\\xi}})}{\\partial\\beta_j} | \\leq\\lambda$,\n$j= q_n+1,\\ldots,p_n$. By Lemma \\ref{lem_sub_diff_q}, $|s_j(\\hat\n{\\beta}_j)| \\leq\\lambda/2$ with probability approaching one for $j =\nq_n+1,\\ldots, p_n$. Therefore, for both penalty functions, there\nexists $l_j^* \\in[-1,1]$ such that $P(s_j(\\hat{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}})\n+ \\lambda l_j^* = \\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\beta_j}, j =\nq_n+1,\\ldots, p_n) \\rightarrow1$. Define $\\kappa_j^* = s_j(\\hat\n{{\\bolds{\\beta}}},\\hat{{\\bolds{\\xi}}}) + \\lambda l_j^*$.\nThen $P(\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} = \\kappa\n_j^*, q_n+1 \\leq j \\leq p_n)\\rightarrow1$ as $n\\rightarrow\\infty$.\n\n\n\n\\end{longlist}\n\nThis completes the proof.\n\n\\end{appendix}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section*{Acknowledgments}\nWe thank the Editor, the Associate Editor and the anonymous referees\nfor their careful reading\nand constructive comments which have helped us to significantly improve\nthe paper.\n\n\n\n\n\\begin{supplement}[id=suppA]\n\n\\stitle{Supplemental Material to ``Partially linear additive quantile regression in~ultra-high dimension''}\n\\slink[doi]{10.1214/15-AOS1367SUPP} \n\n\\sdatatype{.pdf}\n\\sfilename{aos1367\\_supp.pdf}\n\\sdescription{We provide technical details for some of the proofs and additional simulation results.}\n\\end{supplement}\n\n\n\n\n\n\n\n\\begin{thebibliography}{43}\n\n\n\\bibitem[\\protect\\citeauthoryear{Bai and Wu}{1994}]{BaiWu}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bai},~\\bfnm{Z.~D.}\\binits{Z.~D.}} \\AND\n\\bauthor{\\bsnm{Wu},~\\bfnm{Y.}\\binits{Y.}}\n(\\byear{1994}).\n\\btitle{Limiting behavior of {$M$}-estimators of regression\ncoefficients in high-dimensional linear models. I. {S}cale-dependent case}.\n\\bjournal{J. Multivariate Anal.}\n\\bvolume{51}\n\\bpages{211--239}.\n\\bid{doi={10.1006/jmva.1994.1059}, issn={0047-259X}, mr={1321295}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Bai, Z.} and \\textsc{Wu, Y.} (1994).\nLimiting behavior of M-estimators of regression coefficients in high dimensional\nlinear models, I. Scale-dependent case. \\emph{Journal of Multivariate\nAnalysis}, \\textbf{51}, 211-239.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Belloni and Chernozhukov}{2011}]{B1}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Belloni},~\\bfnm{Alexandre}\\binits{A.}} \\AND\n\\bauthor{\\bsnm{Chernozhukov},~\\bfnm{Victor}\\binits{V.}}\n(\\byear{2011}).\n\\btitle{{$\\ell\\sb1$}-penalized quantile regression in high-dimensional\nsparse models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{39}\n\\bpages{82--130}.\n\\bid{doi={10.1214/10-AOS827}, issn={0090-5364}, mr={2797841}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Belloni, A.} and \\textsc{Chernozhukov, V.} (2011).\nL1-Penalized quantile regression in high-dimensional sparse\nmodels. \\emph{Annals of Statistics}, {\\bf39}, 82-130.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Bunea}{2004}]{Bunea}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bunea},~\\bfnm{Florentina}\\binits{F.}}\n(\\byear{2004}).\n\\btitle{Consistent covariate selection and post model selection\ninference in semiparametric regression}.\n\\bjournal{Ann. Statist.}\n\\bvolume{32}\n\\bpages{898--927}.\n\\bid{doi={10.1214/009053604000000247}, issn={0090-5364}, mr={2065193}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Bunea, F.} (2004).\nConsistent covariate selection and post model selection inference in\nsemiparametric regression.\n\\emph{Annals of Statistics}, \\textbf{32}, 898--927.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Fan and Li}{2001}]{fanLi}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Fan},~\\bfnm{Jianqing}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Li},~\\bfnm{Runze}\\binits{R.}}\n(\\byear{2001}).\n\\btitle{Variable selection via nonconcave penalized likelihood and its\noracle properties}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{96}\n\\bpages{1348--1360}.\n\\bid{doi={10.1198/016214501753382273}, issn={0162-1459}, mr={1946581}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Fan, J.} and \\textsc{Li, R.} (2001).\nVariable selection via nonconcave penalized likelihood and its\noracle properties. \\emph{Journal of the American Statistical\nAssociation}, \\textbf{96}, 1348-1360.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Gilliam et~al.}{2003}]{gestDiab}\n\n\\begin{barticle}[auto:parserefs-M02]\n\\bauthor{\\bsnm{Gilliam},~\\bfnm{M.}\\binits{M.}},\n\\bauthor{\\bsnm{Rifas-Shiman},~\\bfnm{S.}\\binits{S.}},\n\\bauthor{\\bsnm{Berkey},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Field},~\\bfnm{A.}\\binits{A.}} \\AND\n\\bauthor{\\bsnm{Colditz},~\\bfnm{G.}\\binits{G.}}\n(\\byear{2003}).\n\\btitle{Maternal gestational diabetes, birth weight and adolescent obesity}.\n\\bjournal{Pediatrics}\n\\bvolume{111}\n\\bpages{221--226}.\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Gilliam, M.}, \\textsc{Rifas-Shiman, S.},\n\\textsc{Berkey, C.}, \\textsc{Field, A.} and \\textsc{Colditz, G.}\n(2003). Maternal gestational diabetes, birth weight and adolescent\nobesity. \\emph{Pediatrics}, \\textbf{111}, 221-226.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Greenshtein and Ritov}{2004}]{Green}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Greenshtein},~\\bfnm{Eitan}\\binits{E.}} \\AND\n\\bauthor{\\bsnm{Ritov},~\\bfnm{Ya'acov}\\binits{Y.}}\n(\\byear{2004}).\n\\btitle{Persistence in high-dimensional linear predictor selection and\nthe virtue of overparametrization}.\n\\bjournal{Bernoulli}\n\\bvolume{10}\n\\bpages{971--988}.\n\\bid{doi={10.3150/bj/1106314846}, issn={1350-7265}, mr={2108039}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Greenshtein, E.} and \\textsc{Ritov, Y. A.} (2004).\nPersistence in high-dimensional linear predictor selection and the\nvirtue of overparametrization. \\emph{Bernoulli}, \\textbf{10}, 971-988.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{He and Shao}{2000}]{HeShao}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{He},~\\bfnm{Xuming}\\binits{X.}} \\AND\n\\bauthor{\\bsnm{Shao},~\\bfnm{Qi-Man}\\binits{Q.-M.}}\n(\\byear{2000}).\n\\btitle{On parameters of increasing dimensions}.\n\\bjournal{J. Multivariate Anal.}\n\\bvolume{73}\n\\bpages{120--135}.\n\\bid{doi={10.1006/jmva.1999.1873}, issn={0047-259X}, mr={1766124}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{He, X.} and \\textsc{Shao, Q.} (2000). On\nparameters of increasing dimensions. \\emph{Journal of Multivariate Analysis},\n\\textbf{73}, 120-135.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{He and Shi}{1996}]{heShi96}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{He},~\\bfnm{Xuming}\\binits{X.}} \\AND\n\\bauthor{\\bsnm{Shi},~\\bfnm{Peide}\\binits{P.}}\n(\\byear{1996}).\n\\btitle{Bivariate tensor-product {$B$}-splines in a partly linear model}.\n\\bjournal{J. Multivariate Anal.}\n\\bvolume{58}\n\\bpages{162--181}.\n\\bid{doi={10.1006/jmva.1996.0045}, issn={0047-259X}, mr={1405586}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{He, X.} and \\textsc{Shi, P.} (1996).\nBivariate tensor-product B-splines in a partly linear model. \\emph\n{Journal of Multivariate Analysis}, \\textbf{58(2)}, 162-181.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{He, Wang and Hong}{2013}]{heWang}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{He},~\\bfnm{Xuming}\\binits{X.}},\n\\bauthor{\\bsnm{Wang},~\\bfnm{Lan}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Hong},~\\bfnm{Hyokyoung~Grace}\\binits{H.~G.}}\n(\\byear{2013}).\n\\btitle{Quantile-adaptive model-free variable screening for\nhigh-dimensional heterogeneous data}.\n\\bjournal{Ann. Statist.}\n\\bvolume{41}\n\\bpages{342--369}.\n\\bid{doi={10.1214/13-AOS1087}, issn={0090-5364}, mr={3059421}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{He, X.}, \\textsc{Wang, L.} and \\textsc\n{Hong, H.} (2013). Quantile-adaptive model-free nonlinear feature\nscreening for high-dimensional heterogeneous data. \\emph{Annals of\nStatistics}, \\textbf{41}, 342-369.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{He, Zhu and Fung}{2002}]{heQuantLong}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{He},~\\bfnm{Xuming}\\binits{X.}},\n\\bauthor{\\bsnm{Zhu},~\\bfnm{Zhong-Yi}\\binits{Z.-Y.}} \\AND\n\\bauthor{\\bsnm{Fung},~\\bfnm{Wing-Kam}\\binits{W.-K.}}\n(\\byear{2002}).\n\\btitle{Estimation in a semiparametric model for longitudinal data with\nunspecified dependence structure}.\n\\bjournal{Biometrika}\n\\bvolume{89}\n\\bpages{579--590}.\n\\bid{doi={10.1093/biomet/89.3.579}, issn={0006-3444}, mr={1929164}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{He, X.}, \\textsc{Zhu, Z.} and \\textsc\n{Fung, W.} (2002). Estimation in a semiparametric model for\nlongitudinal data with unspecified dependence structure. \\emph\n{Biometrika}, \\textbf{89}, 579-590.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Huang, Breheny and Ma}{2012}]{HuangEtAl}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Huang},~\\bfnm{Jian}\\binits{J.}},\n\\bauthor{\\bsnm{Breheny},~\\bfnm{Patrick}\\binits{P.}} \\AND\n\\bauthor{\\bsnm{Ma},~\\bfnm{Shuangge}\\binits{S.}}\n(\\byear{2012}).\n\\btitle{A selective review of group selection in high-dimensional models}.\n\\bjournal{Statist. Sci.}\n\\bvolume{27}\n\\bpages{481--499}.\n\\bid{doi={10.1214/12-STS392}, issn={0883-4237}, mr={3025130}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Huang, J.}, \\textsc{Breheny, P.} and\n\\textsc{Ma, S.} (2012). A Selective Review of Group Selection in\nHigh-Dimensional Models. \\emph{Statistical Science}, \\textbf{27}, 481-499.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Huang, Horowitz and Wei}{2010}]{HHW}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Huang},~\\bfnm{Jian}\\binits{J.}},\n\\bauthor{\\bsnm{Horowitz},~\\bfnm{Joel~L.}\\binits{J.~L.}} \\AND\n\\bauthor{\\bsnm{Wei},~\\bfnm{Fengrong}\\binits{F.}}\n(\\byear{2010}).\n\\btitle{Variable selection in nonparametric additive models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{38}\n\\bpages{2282--2313}.\n\\bid{doi={10.1214/09-AOS781}, issn={0090-5364}, mr={2676890}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Huang, J.}, \\textsc{Horowitz, J. L.}, and \\textsc{Wei, F.}\n(2010). Variable selection in nonparametric\nadditive models. \\emph{Annals of Statistics}, \\textbf{38}, 2282-2313.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Huang, Wei and Ma}{2012}]{HWM}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Huang},~\\bfnm{Jian}\\binits{J.}},\n\\bauthor{\\bsnm{Wei},~\\bfnm{Fengrong}\\binits{F.}} \\AND\n\\bauthor{\\bsnm{Ma},~\\bfnm{Shuangge}\\binits{S.}}\n(\\byear{2012}).\n\\btitle{Semiparametric regression pursuit}.\n\\bjournal{Statist. Sinica}\n\\bvolume{22}\n\\bpages{1403--1426}.\n\\bid{issn={1017-0405}, mr={3027093}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Huang, J.}, \\textsc{Wei, F.}, and \\textsc{Ma, S.} (2012).\nSemiparametric regression pursuit.\n\\emph{Statistica Sinica}, \\textbf{22}, 1403-1426.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Ishida et~al.}{2012}]{phlda2}\n\n\\begin{barticle}[pbm]\n\\bauthor{\\bsnm{Ishida},~\\bfnm{Miho}\\binits{M.}},\n\\bauthor{\\bsnm{Monk},~\\bfnm{David}\\binits{D.}},\n\\bauthor{\\bsnm{Duncan},~\\bfnm{Andrew~J.}\\binits{A.~J.}},\n\\bauthor{\\bsnm{Abu-Amero},~\\bfnm{Sayeda}\\binits{S.}},\n\\bauthor{\\bsnm{Chong},~\\bfnm{Jiehan}\\binits{J.}},\n\\bauthor{\\bsnm{Ring},~\\bfnm{Susan~M.}\\binits{S.~M.}},\n\\bauthor{\\bsnm{Pembrey},~\\bfnm{Marcus~E.}\\binits{M.~E.}},\n\\bauthor{\\bsnm{Hindmarsh},~\\bfnm{Peter~C.}\\binits{P.~C.}},\n\\bauthor{\\bsnm{Whittaker},~\\bfnm{John~C.}\\binits{J.~C.}},\n\\bauthor{\\bsnm{Stanier},~\\bfnm{Philip}\\binits{P.}} \\AND\n\\bauthor{\\bsnm{Moore},~\\bfnm{Gudrun~E.}\\binits{G.~E.}}\n(\\byear{2012}).\n\\btitle{Maternal inheritance of a promoter variant in the imprinted\nPHLDA2 gene significantly increases birth weight}.\n\\bjournal{Am. J. Hum. Genet.}\n\\bvolume{90}\n\\bpages{715--719}.\n\\bid{doi={10.1016/j.ajhg.2012.02.021}, issn={1537-6605},\npii={S0002-9297(12)00106-1}, pmcid={3322226}, pmid={22444668}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Ishida, M.}, \\textsc{Monk, D.}, \\textsc{Duncan, A.}, \\textsc\n{Abu-Amero, S.}, \\textsc{Chong, J.}, \\textsc{Ring, S.}, \\textsc\n{Pembrey, M.}, \\textsc{Hindmarsh, P.}, \\textsc{Whittaker, J.},\n\\textsc{Stanier, P.} and \\textsc{Moore, G.} (2012).\nMaternal inheritance of a promoter variant in the imprinted PHLDA2 gene\nsignificantly increases birth weight. \\emph{The American Journal of\nHuman Genetics}, \\textbf{90}, 715-719.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Kai, Li and Zou}{2011}]{KLZ}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Kai},~\\bfnm{Bo}\\binits{B.}},\n\\bauthor{\\bsnm{Li},~\\bfnm{Runze}\\binits{R.}} \\AND\n\\bauthor{\\bsnm{Zou},~\\bfnm{Hui}\\binits{H.}}\n(\\byear{2011}).\n\\btitle{New efficient estimation and variable selection methods for\nsemiparametric varying-coefficient partially linear models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{39}\n\\bpages{305--332}.\n\\bid{doi={10.1214/10-AOS842}, issn={0090-5364}, mr={2797848}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Kai, B.}, \\textsc{Li, R.} and \\textsc{Zou, H.} (2011).\nNew efficient estimation and variable selection methods for\nsemiparametric varying-coefficient partially linear models.\n\\emph{Annals of Statistics}, \\textbf{39},\n305-332.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Lam and Fan}{2008}]{partMeanVaryCoef}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Lam},~\\bfnm{Clifford}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Fan},~\\bfnm{Jianqing}\\binits{J.}}\n(\\byear{2008}).\n\\btitle{Profile-kernel likelihood inference with diverging number of\nparameters}.\n\\bjournal{Ann. Statist.}\n\\bvolume{36}\n\\bpages{2232--2260}.\n\\bid{doi={10.1214/07-AOS544}, issn={0090-5364}, mr={2458186}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Lam, C.} and \\textsc{Fan, J.}\n(2008). Profile-kernel likelihood inference with diverging number of\nparameters. \\emph{Annals of Statistics}, \\textbf{36}, 2232-2260.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Lee, Noh and Park}{2014}]{Lee}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Lee},~\\bfnm{Eun~Ryung}\\binits{E.~R.}},\n\\bauthor{\\bsnm{Noh},~\\bfnm{Hohsuk}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Park},~\\bfnm{Byeong~U.}\\binits{B.~U.}}\n(\\byear{2014}).\n\\btitle{Model selection via {B}ayesian information criterion for\nquantile regression models}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{109}\n\\bpages{216--229}.\n\\bid{doi={10.1080/01621459.2013.836975}, issn={0162-1459}, mr={3180558}}\n\\bptnote{check year}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Lee, E. R., Noh, H. }and \\textsc{Park, B. U.} (2013).\nModel Selection via Bayesian Information Criterion for\nQuantile Regression Models. \\textit{Journal of the American\nStatistical Association}, \\textbf{109}, 216-229.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Li, Xue and Lian}{2011}]{Li}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Li},~\\bfnm{Gaorong}\\binits{G.}},\n\\bauthor{\\bsnm{Xue},~\\bfnm{Liugen}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Lian},~\\bfnm{Heng}\\binits{H.}}\n(\\byear{2011}).\n\\btitle{Semi-varying coefficient models with a diverging number of components}.\n\\bjournal{J. Multivariate Anal.}\n\\bvolume{102}\n\\bpages{1166--1174}.\n\\bid{doi={10.1016/j.jmva.2011.03.010}, issn={0047-259X}, mr={2805656}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Li, G.}, \\textsc{Xue, L.} and \\textsc{Lian, H.}\n(2011). Semi-varying coefficient models with a diverging number of components.\n\\emph{Journal of Multivariate Analysis}, \\textbf{102}, 1166-1174.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Lian, Liang and Ruppert}{2015}]{Lian}\n\n\\begin{barticle}[auto:parserefs-M02]\n\\bauthor{\\bsnm{Lian},~\\bfnm{H.}\\binits{H.}},\n\\bauthor{\\bsnm{Liang},~\\bfnm{H.}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Ruppert},~\\bfnm{D.}\\binits{D.}}\n(\\byear{2015}).\n\\btitle{Separation of covariates into nonparametric and parametric\nparts in high-dimensional partially linear additive models}.\n\\bjournal{Statist. Sinica}\n\\bvolume{25}\n\\bpages{591--607}.\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Lian, H.}, \\textsc{Liang, H.} and \\textsc{Ruppert, D.}\n(2015). Separation of covariates into nonparametric and parametric\nparts in\nhigh-dimensional partially linear additive models. \\emph{Statistica\nSinica}, \\textbf{25}, 591-607.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Liang and Li}{2009}]{LiangLi}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Liang},~\\bfnm{Hua}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Li},~\\bfnm{Runze}\\binits{R.}}\n(\\byear{2009}).\n\\btitle{Variable selection for partially linear models with measurement errors}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{104}\n\\bpages{234--248}.\n\\bid{doi={10.1198/jasa.2009.0127}, issn={0162-1459}, mr={2504375}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Liang, H.} and \\textsc{Li, R.} (2009).\nVariable selection for partially linear models with measurement errors.\n\\emph{Journal of the American Statistical Association}, \\textbf\n{104(485)}, 234-248.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Liu, Wang and Liang}{2011}]{scadPartLinMean}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Liu},~\\bfnm{Xiang}\\binits{X.}},\n\\bauthor{\\bsnm{Wang},~\\bfnm{Li}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Liang},~\\bfnm{Hua}\\binits{H.}}\n(\\byear{2011}).\n\\btitle{Estimation and variable selection for semiparametric additive\npartial linear models}.\n\\bjournal{Statist. Sinica}\n\\bvolume{21}\n\\bpages{1225--1248}.\n\\bid{doi={10.5705/ss.2009.140}, issn={1017-0405}, mr={2827522}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Liu, X.}, \\textsc{Wang, L.} and\n\\textsc{Liang, H.} (2011). Estimation and variable selection for\nsemiparametric additive partial linear models. \\emph{Statistica\nSinica}, \\textbf{21}, 1225-1248.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Liu and Wu}{2011}]{LiuWu}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Liu},~\\bfnm{Yufeng}\\binits{Y.}} \\AND\n\\bauthor{\\bsnm{Wu},~\\bfnm{Yichao}\\binits{Y.}}\n(\\byear{2011}).\n\\btitle{Simultaneous multiple non-crossing quantile regression\nestimation using kernel constraints}.\n\\bjournal{J. Nonparametr. Stat.}\n\\bvolume{23}\n\\bpages{415--437}.\n\\bid{doi={10.1080/10485252.2010.537336}, issn={1048-5252}, mr={2801302}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Liu, Y.} and \\textsc{Wu, Y.} (2011).\nSimultaneous multiple non-crossing quantile regression estimation using\nkernel constraints. \\emph{Journal of Nonparametric Statistics},\n\\textbf{23}, 415-437.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Schumaker}{1981}]{Schumaker}\n\n\\begin{bbook}[mr]\n\\bauthor{\\bsnm{Schumaker},~\\bfnm{Larry~L.}\\binits{L.~L.}}\n(\\byear{1981}).\n\\btitle{Spline Functions: Basic Theory}.\n\\bpublisher{Wiley},\n\\blocation{New York}.\n\\bid{mr={0606200}}\n\\end{bbook}\n\n\\iffalse\\OrigBibText\n\\textsc{Schumaker, L.} (1981). \\emph{Spline\nFunctions: Basic Theory}, Wiley: New York.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Sherwood and Wang}{2015}]{Supp}\n\n\\begin{bmisc}[author]\n\\bauthor{\\bsnm{Sherwood},~\\bfnm{B.}\\binits{B.}} \\AND\n\\bauthor{\\bsnm{Wang},~\\bfnm{L.}\\binits{L.}}\n(\\byear{2015}).\n\\bhowpublished{Supplement to ``Partially linear additive quantile\nregression in~ultra-high dimension.''\nDOI:\\doiurl{10.1214/15-AOS1367SUPP}}.\n\\bptok{imsref}\n\\end{bmisc}\n\n\\iffalse\\OrigBibText\n\\textsc{Sherwood, B.} and \\textsc{Wang, L.} (2015).\nSupplement to ``Partially Linear Additive Quantile Regression in\nultra-high Dimension''.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Stone}{1985}]{Stone85}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Stone},~\\bfnm{Charles~J.}\\binits{C.~J.}}\n(\\byear{1985}).\n\\btitle{Additive regression and other nonparametric models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{13}\n\\bpages{689--705}.\n\\bid{doi={10.1214/aos/1176349548}, issn={0090-5364}, mr={0790566}}\n\\bptnote{check pages}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Stone, C.} (1985). Additive regression and\nother nonparametric models. \\emph{Annals of Statistics}, \\textbf{13}, 689-706.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Tang et~al.}{2013}]{Tang}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Tang},~\\bfnm{Yanlin}\\binits{Y.}},\n\\bauthor{\\bsnm{Song},~\\bfnm{Xinyuan}\\binits{X.}},\n\\bauthor{\\bsnm{Wang},~\\bfnm{Huixia~Judy}\\binits{H.~J.}} \\AND\n\\bauthor{\\bsnm{Zhu},~\\bfnm{Zhongyi}\\binits{Z.}}\n(\\byear{2013}).\n\\btitle{Variable selection in high-dimensional quantile varying\ncoefficient models}.\n\\bjournal{J. Multivariate Anal.}\n\\bvolume{122}\n\\bpages{115--132}.\n\\bid{doi={10.1016/j.jmva.2013.07.015}, issn={0047-259X}, mr={3189311}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Tang, Y. L.}, \\textsc{Song, X. Y.}, \\textsc\n{Wang, H. X.} and \\textsc{Zhu, Z. Y.} (2013).\nVariable selection in high-dimensional quantile varying coefficient models.\n\\emph{Journal of Multivariate Analysis}, \\textbf{122}, 115-132.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Tao and An}{1997}]{TaoAn97}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Tao},~\\bfnm{Pham~Dinh}\\binits{P.~D.}} \\AND\n\\bauthor{\\bsnm{An},~\\bfnm{Le~Thi~Hoai}\\binits{L.~T.~H.}}\n(\\byear{1997}).\n\\btitle{Convex analysis approach to d.c. programming: Theory,\nalgorithms and applications}.\n\\bjournal{Acta Math. Vietnam.}\n\\bvolume{22}\n\\bpages{289--355}.\n\\bid{issn={0251-4184}, mr={1479751}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Tao, P.} and \\textsc{An, L.} (1997).\nConvex analysis approach to D.C. programming: theory, algorithms and\napplications. \\emph{Acta Mathematica Vietnamica}, \\textbf{22}, 289-355.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Tibshirani}{1996}]{T1}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Tibshirani},~\\bfnm{Robert}\\binits{R.}}\n(\\byear{1996}).\n\\btitle{Regression shrinkage and selection via the lasso}.\n\\bjournal{J. Roy. Statist. Soc. Ser. B}\n\\bvolume{58}\n\\bpages{267--288}.\n\\bid{issn={0035-9246}, mr={1379242}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Tibshirani, R. J.} (1996). Regression shrinkage and selection\nvia the\nLasso. \\emph{Journal of the Royal Statistical Society, Series B},\n\\textbf{58}, 267--288.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Turan et~al.}{2012}]{birthWt2}\n\n\\begin{barticle}[auto:parserefs-M02]\n\\bauthor{\\bsnm{Turan},~\\bfnm{N.}\\binits{N.}},\n\\bauthor{\\bsnm{Ghalwash},~\\bfnm{M.}\\binits{M.}},\n\\bauthor{\\bsnm{Kataril},~\\bfnm{S.}\\binits{S.}},\n\\bauthor{\\bsnm{Coutifaris},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Obradovic},~\\bfnm{Z.}\\binits{Z.}} \\AND\n\\bauthor{\\bsnm{Sapienza},~\\bfnm{C.}\\binits{C.}}\n(\\byear{2012}).\n\\btitle{DNA methylation differences at growth related genes correlate\nwith birth weight: A~molecular signature linked to developmental\norigins of adult disease?}\n\\bjournal{BMC Medical Genomics}\n\\bvolume{5}\n\\bpages{10}.\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Turan, N.}, \\textsc{Ghalwash, M.},\n\\textsc{Kataril, S.}, \\textsc{Coutifaris, C.}, \\textsc{Obradovic,\nZ.} and \\textsc{Sapienza, C.} (2012). DNA methylation differences at\ngrowth related genes correlate with birth weight: a molecular signature\nlinked to developmental origins of adult disease? \\emph{BMC Medical\nGenomics}, \\textbf{5(1)}, 10.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Votavova et~al.}{2011}]{AppliedExample}\n\n\\begin{barticle}[pbm]\n\\bauthor{\\bsnm{Votavova},~\\bfnm{H.}\\binits{H.}},\n\\bauthor{\\bsnm{Dostalova Merkerova},~\\bfnm{M.}\\binits{M.}},\n\\bauthor{\\bsnm{Fejglova},~\\bfnm{K.}\\binits{K.}},\n\\bauthor{\\bsnm{Vasikova},~\\bfnm{A.}\\binits{A.}},\n\\bauthor{\\bsnm{Krejcik},~\\bfnm{Z.}\\binits{Z.}},\n\\bauthor{\\bsnm{Pastorkova},~\\bfnm{A.}\\binits{A.}},\n\\bauthor{\\bsnm{Tabashidze},~\\bfnm{N.}\\binits{N.}},\n\\bauthor{\\bsnm{Topinka},~\\bfnm{J.}\\binits{J.}},\n\\bauthor{\\bsnm{Veleminsky},~\\bfnm{M.}\\binits{M.}, \\bsuffix{Jr.}},\n\\bauthor{\\bsnm{Sram},~\\bfnm{R.~J.}\\binits{R.~J.}} \\AND\n\\bauthor{\\bsnm{Brdicka},~\\bfnm{R.}\\binits{R.}}\n(\\byear{2011}).\n\\btitle{Transcriptome alterations in maternal and fetal cells induced\nby tobacco smoke}.\n\\bjournal{Placenta}\n\\bvolume{32}\n\\bpages{763--770}.\n\\bid{doi={10.1016/j.placenta.2011.06.022}, issn={1532-3102},\npii={S0143-4004(11)00243-8}, pmid={21803418}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Votavova, H.}, \\textsc{Dostalova Merkerova, M.}, \\textsc{Fejglova, K.}, \\textsc{Vasikova, A.}, \\textsc\n{Krejcik, Z.}, \\textsc{Pastorkova, A.}, \\textsc{Tabashidze, N.},\n\\textsc{Topinka, J.}, \\textsc{Veleminsky, M. Jr.}, \\textsc{Sram, R.}\nand \\textsc{Brdicka, R.} (2011). Transcriptome alterations in maternal\nand fetal cells induced by tobacco smoke. \\emph{Placenta}, \\textbf\n{32}, 763-770.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Wang, Wu and Li}{2012}]{quantUltraHigh}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Wang},~\\bfnm{Lan}\\binits{L.}},\n\\bauthor{\\bsnm{Wu},~\\bfnm{Yichao}\\binits{Y.}} \\AND\n\\bauthor{\\bsnm{Li},~\\bfnm{Runze}\\binits{R.}}\n(\\byear{2012}).\n\\btitle{Quantile regression for analyzing heterogeneity in ultra-high\ndimension}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{107}\n\\bpages{214--222}.\n\\bid{doi={10.1080/01621459.2012.656014}, issn={0162-1459}, mr={2949353}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Wang, L.}, \\textsc{Wu, Y.} and\n\\textsc{Li, R.} (2012). Quantile regression for analyzing\nheterogeneity in ultra-high dimension. \\emph{Journal of the American\nStatistical Association}, \\textbf{107}, 214-222.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Wang and Xia}{2009}]{WX}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Wang},~\\bfnm{Hansheng}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Xia},~\\bfnm{Yingcun}\\binits{Y.}}\n(\\byear{2009}).\n\\btitle{Shrinkage estimation of the varying coefficient model}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{104}\n\\bpages{747--757}.\n\\bid{doi={10.1198/jasa.2009.0138}, issn={0162-1459}, mr={2541592}}\n\\bptnote{check volume}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Wang, H. S.} and \\textsc{Xia, Y. C.} (2009).\nShrinkage estimation of the varying coefficient model.\n\\emph{Journal of the American Statistical Association}, \\textbf{486},\n747-757.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Wang, Zhu and Zhou}{2009}]{quantVaryModel}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Wang},~\\bfnm{Huixia~Judy}\\binits{H.~J.}},\n\\bauthor{\\bsnm{Zhu},~\\bfnm{Zhongyi}\\binits{Z.}} \\AND\n\\bauthor{\\bsnm{Zhou},~\\bfnm{Jianhui}\\binits{J.}}\n(\\byear{2009}).\n\\btitle{Quantile regression in partially linear varying coefficient models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{37}\n\\bpages{3841--3866}.\n\\bid{doi={10.1214/09-AOS695}, issn={0090-5364}, mr={2572445}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Wang H.}, \\textsc{Zhu, Z.} and\n\\textsc{Zhou, J.} (2009). Quantile regression in partially linear\nvarying coefficient models. \\emph{Annals of Statistics}, \\textbf{37},\n3841-3866.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Wang et~al.}{2011}]{WLLC}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Wang},~\\bfnm{Li}\\binits{L.}},\n\\bauthor{\\bsnm{Liu},~\\bfnm{Xiang}\\binits{X.}},\n\\bauthor{\\bsnm{Liang},~\\bfnm{Hua}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Carroll},~\\bfnm{Raymond~J.}\\binits{R.~J.}}\n(\\byear{2011}).\n\\btitle{Estimation and variable selection for generalized additive\npartial linear models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{39}\n\\bpages{1827--1851}.\n\\bid{doi={10.1214/11-AOS885}, issn={0090-5364}, mr={2893854}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Wang, L.}, \\textsc{Liu, X.}, \\textsc{Liang,\nH.} and \\textsc{Carroll, R.} (2011). Estimation and variable selection for\ngeneralized additive partial linear models. \\emph{Annals of\nStatistics}, \\textbf{39}, 1827-1851.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Wei and He}{2006}]{WH06}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Wei},~\\bfnm{Ying}\\binits{Y.}} \\AND\n\\bauthor{\\bsnm{He},~\\bfnm{Xuming}\\binits{X.}}\n(\\byear{2006}).\n\\btitle{Conditional growth charts}.\n\\bjournal{Ann. Statist.}\n\\bvolume{34}\n\\bpages{2069--2131}.\n\\bnote{With discussions and a rejoinder by the authors}.\n\\bid{doi={10.1214/009053606000000623}, issn={0090-5364}, mr={2291494}}\n\\bptnote{check pages}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\nWei, Y and He, X. (2006). Conditional Growth Charts (with discussions). Annals of Statistics, 34, 2069-2097 and 2126-2131.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Welsh}{1989}]{Welsh}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Welsh},~\\bfnm{A.~H.}\\binits{A.~H.}}\n(\\byear{1989}).\n\\btitle{On {$M$}-processes and {$M$}-estimation}.\n\\bjournal{Ann. Statist.}\n\\bvolume{17}\n\\bpages{337--361}.\n\\bid{doi={10.1214/aos/1176347021}, issn={0090-5364}, mr={0981455}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Welsh, A.} (1989). On M-Processes and\nM-Estimation. \\emph{Annals of Statistics}, \\textbf{17}, 337-361.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Xie and Huang}{2009}]{annalsMeanParLin}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Xie},~\\bfnm{Huiliang}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Huang},~\\bfnm{Jian}\\binits{J.}}\n(\\byear{2009}).\n\\btitle{S{CAD}-penalized regression in high-dimensional partially\nlinear models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{37}\n\\bpages{673--696}.\n\\bid{doi={10.1214/07-AOS580}, issn={0090-5364}, mr={2502647}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Xie, H.} and \\textsc{Huang, J.}\n(2009). SCAD-penalized regression in high-dimensional partially linear\nmodels. \\emph{Annals of Statistics}, \\textbf{37}, 673-696.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Xue and Yang}{2006}]{XY}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Xue},~\\bfnm{Lan}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Yang},~\\bfnm{Lijian}\\binits{L.}}\n(\\byear{2006}).\n\\btitle{Additive coefficient modeling via polynomial spline}.\n\\bjournal{Statist. Sinica}\n\\bvolume{16}\n\\bpages{1423--1446}.\n\\bid{issn={1017-0405}, mr={2327498}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Xue, L.} and \\textsc{Yang, L. J.} (2006).\nAdditive coefficient modeling via polynomial spline.\n\\emph{Statistica Sinica}, \\textbf{16}, 1423-1446.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Yuan and Lin}{2006}]{YuanLin}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Yuan},~\\bfnm{Ming}\\binits{M.}} \\AND\n\\bauthor{\\bsnm{Lin},~\\bfnm{Yi}\\binits{Y.}}\n(\\byear{2006}).\n\\btitle{Model selection and estimation in regression with grouped variables}.\n\\bjournal{J. R. Stat. Soc. Ser. B. Stat. Methodol.}\n\\bvolume{68}\n\\bpages{49--67}.\n\\bid{doi={10.1111/j.1467-9868.2005.00532.x}, issn={1369-7412}, mr={2212574}}\n\\bptnote{check year}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Yuan, M.} and \\textsc{Lin, Y.} (2007).\nModel selection and estimation in regression with grouped\nvariables. \\emph{Journal of the Royal Statistical Society, Series B},\n\\textbf{68}, 49-67.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Zhang}{2010}]{Zhang}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Zhang},~\\bfnm{Cun-Hui}\\binits{C.-H.}}\n(\\byear{2010}).\n\\btitle{Nearly unbiased variable selection under minimax concave penalty}.\n\\bjournal{Ann. Statist.}\n\\bvolume{38}\n\\bpages{894--942}.\n\\bid{doi={10.1214/09-AOS729}, issn={0090-5364}, mr={2604701}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Zhang, C.} (2010). Nearly unbiased variable\nselection under minimax concave penalty. \\emph{Annals of Statistics},\n\\textbf{38}, 894-942.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Zhang, Cheng and Liu}{2011}]{ZCL}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Zhang},~\\bfnm{Hao~Helen}\\binits{H.~H.}},\n\\bauthor{\\bsnm{Cheng},~\\bfnm{Guang}\\binits{G.}} \\AND\n\\bauthor{\\bsnm{Liu},~\\bfnm{Yufeng}\\binits{Y.}}\n(\\byear{2011}).\n\\btitle{Linear or nonlinear? {A}utomatic structure discovery for\npartially linear models}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{106}\n\\bpages{1099--1112}.\n\\bid{doi={10.1198/jasa.2011.tm10281}, issn={0162-1459}, mr={2894767}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Zhang, H.}, \\textsc{Cheng, G.} and \\textsc\n{Liu, Y.} (2011).\nLinear or nonlinear? Automatic structure discovery for partially linear models.\n\\emph{Journal of American Statistical Association}, \\textbf{106}, 1099-1112.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Zou and Li}{2008}]{lla}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Zou},~\\bfnm{Hui}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Li},~\\bfnm{Runze}\\binits{R.}}\n(\\byear{2008}).\n\\btitle{One-step sparse estimates in nonconcave penalized likelihood models}.\n\\bjournal{Ann. Statist.}\n\\bvolume{36}\n\\bpages{1509--1533}.\n\\bid{doi={10.1214/009053607000000802}, issn={0090-5364}, mr={2435443}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Zou, H.} and \\textsc{Li, R.} (2008). One-step\nsparse estimates in nonconcave penalized likelihood models. \\emph\n{Annals of Statistics}, \\textbf{36}, 1509-1533.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\\bibitem[\\protect\\citeauthoryear{Zou and Yuan}{2008}]{zouYuan}\n\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Zou},~\\bfnm{Hui}\\binits{H.}} \\AND\n\\bauthor{\\bsnm{Yuan},~\\bfnm{Ming}\\binits{M.}}\n(\\byear{2008}).\n\\btitle{Regularized simultaneous model selection in multiple quantiles\nregression}.\n\\bjournal{Comput. Statist. Data Anal.}\n\\bvolume{52}\n\\bpages{5296--5304}.\n\\bid{doi={10.1016/j.csda.2008.05.013}, issn={0167-9473}, mr={2526595}}\n\\end{barticle}\n\n\\iffalse\\OrigBibText\n\\textsc{Zou, H.} and \\textsc{Yuan, M.} (2008).\nRegularized simultaneous model selection in multiple quantiles\nregression. \\emph{Computational Statistics and Data Analysis}, \\textbf\n{52}, 5296-5304.\n\\endOrigBibText\\fi\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\\end{thebibliography}\n\n\n\\printaddresses\n\n", "itemtype": "equation", "pos": 87803, "prevtext": "\n\nBy Condition \\ref{cond_small_sig}, $\\mathop{\\min}_{1 \\leq j \\leq\nq_n} {\\vert} \\beta_{0j}{\\vert} \\geq C_5 n^{-(1-C_4)/2}$.\nBy Theorem \\ref{large_q_oracle} and Conditions \\ref\n{cond_sigma_large_p} and \\ref{cond_small_sig},\n$\\mathop{\\max}_{1 \\leq j \\leq q_n} {\\vert} \\hat{\\beta}_j - \\beta\n_{0j}{\\vert} = O_p (\\sqrt{\\frac{q_n}{n}} ) = o_p\n(n^{-(1-C_4)/2} )$.\n(\\ref{lem_sub_diff_q_1}) holds by noting $\\lambda= o\n(n^{-(1-C_4)/2} )$.\n\\end{pf*}\n\n\\begin{pf*}{Proof of (\\ref{lem_sub_diff_q_3})}\nProof provided in the online supplementary material [\\citet{Supp}].\n\n\n\n\\end{pf*}\n\n\n\\subsubsection{Proof of Theorem \\texorpdfstring{\\protect\\ref{scad_local_min}}{3.1}}\\label{sec8.2.2}\n\n\nRecall that for $\\kappa_{j} \\in\\partial k({\\bolds{\\beta}},{\\bolds{\\xi}})$ \n\n\n\n\\begin{eqnarray*}\n\\kappa_j &=& s_j({\\bolds{\\beta}},{\\bolds{\\xi}}) + \\lambda l_j\n\\qquad\\mbox{for } 1 \\leq j \\leq p_n,\n\\\\\n\n\\kappa_{j} &=& s_{j}({\\bolds{\\beta}},{\\bolds{\\xi}}) \\qquad\\mbox{for }\np_n+1 \\leq j \\leq p_n+J_n.\n\\end{eqnarray*}\n\nDefine the set\n\n\\begin{eqnarray*}\n\\mathcal{G} &=& \\bigl\\{ {\\bolds{\\kappa}}= (\\kappa_1, \\kappa_2,\n\\ldots,\\kappa_{p_n+J_n})': \\kappa_j = \\lambda\n\\operatorname{sgn}(\\hat{\\beta}_j), j=1,\\ldots,q_n;\n\\\\\n&&{} \\kappa_j = s_j(\\hat{{\\bolds{\\beta}}}, \\hat{{\\bolds{\\xi}}}) + \\lambda\nl_j, j=q_n+1,\\ldots,p_n;\n\\\\\n&&{} \\kappa_j = 0, j=p_n+1,\\ldots,p_n+J_n,\n\\bigr\\},\n\\end{eqnarray*}\n\nwhere $l_j$ ranges over $[-1,1]$ for $j= q_n+1,\\ldots,p_n$.\nBy Lemma \\ref{lem_sub_diff_q}, we have $P(\\mathcal{G} \\subset\\partial k(\\hat{{\\bolds{\\beta}}\n},\\hat{{\\bolds{\\xi}}}))\\rightarrow1$.\n\n\n\n\n\n\n\n\nConsider any $({\\bolds{\\beta}}', {\\bolds{\\xi}}')'$ in a ball with the center $\n(\\hat{{\\bolds{\\beta}}}',\\hat{{\\bolds{\\xi}}}' )'$ and radius\n$\\lambda/ 2$.\nBy Lemma \\ref{lem_diff_convex}, to prove the theorem it is sufficient\nto show that there exists ${\\bolds{\\kappa}}^* = (\\kappa_1^*,\\ldots\n,\\kappa^*_{p_n+J_n} )' \\in\\mathcal{G}$\nsuch that\n\n\n\n\n\\begin{eqnarray}\nP \\biggl( \\kappa_j^* = \\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\n\\beta_j}, j=1,\n\\ldots,p_n \\biggr) &\\rightarrow&1; \\label{xi_star_j}\n\\\\\nP \\biggl( \\kappa_{p_n+j}^* = \\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}}\n)}{\\partial\\xi_j}, j=1,\n\\ldots,J_n \\biggr) &\\rightarrow&1. \\label\n{xi_star_j2}\n\\end{eqnarray}\n\nSince $\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\xi_j} = 0$ for\n$j=1,\\ldots,J_n$,\n(\\ref{xi_star_j2}) is satisfied by Lemma \\ref{lem_sub_diff_q}.\n\nWe outline how $\\kappa_j^*$ can be selected to satisfy (\\ref\n{xi_star_j}). \n\n\n\n\n\\begin{longlist}[2.]\n\n\n\n\n\n\n\\item[1.] For $1 \\leq j \\leq q_n$,\nwe have $\\kappa^*_j=\\lambda\\operatorname{sgn}(\\hat{\\beta}_j)$ for $\\beta\n_j \\neq0$. \n\nFor either SCAD or MCP penalty function, $\\frac{\\partial l({\\bolds{\\beta}},\n{\\bolds{\\xi}})}{\\partial\\beta_j} = \\lambda\\operatorname{sgn}(\\beta_j)$ for\n${\\vert} \\beta_j{\\vert} > a\\lambda$. By Lemma \\ref{lem_sub_diff_q},\nwe have\n\n\\begin{eqnarray*}\n\\min_{1\\leq j \\leq q_n}{\\vert}\\beta_j{\\vert}&\\geq& \\min\n_{1 \\leq j \\leq\nq_n}{\\vert}\\hat{\\beta}_j{\\vert}- \\max\n_{1\\leq j \\leq q_n}{\\vert}\\hat{\\beta}_j - \\beta_j\n{\\vert}\n\\geq (a+1/2)\\lambda- \\lambda/2 = a\\lambda,\n\\end{eqnarray*}\n\nwith probability approaching one.\nThus, $P(\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} =\n\\lambda\\operatorname{sgn}(\\beta_j))\\rightarrow1$.\nFor any $1 \\leq j \\leq q_n$, ${\\Vert} \\hat{\\beta}_j-\\beta_{0j}{\\Vert} =\nO_p (n^{-1/2}q_n^{1/2} ) = o(\\lambda)$. Therefore, for\nsufficiently large $n$, $\\hat{\\beta}_j$ and $\\beta_j$ have the same\nsign. This implies\n$P(\\frac{\\partial l({\\bolds{\\beta}}, {\\bolds{\\xi}})}{\\partial\\beta_j} = \\kappa_j^*,\n1 \\leq j \\leq q_n)\\rightarrow1$ as $n\\rightarrow\\infty$.\n\n\\item[2.]\n\n\n\n\n\nFor $j=q_n+1,\\ldots,p_n$,\n$\\hat{\\beta}_j=0$ by the definition of the oracle estimator and\n$\\kappa_j = \\lambda l_j$ with $l_j \\in[-1,1]$. Therefore,\n\n", "index": 57, "text": "\n\\[\n{\\vert}\\beta_j{\\vert}\\leq{\\vert}\\hat{\\beta}_j{\\vert}\n+ {\\vert}\\hat{\\beta}_j - \\beta_j{\\vert}< \\lambda/2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"{|}\\beta_{j}{|}\\leq{|}\\hat{\\beta}_{j}{|}+{|}\\hat{\\beta}_{j}-\\beta_{j}{|}&lt;%&#10;\\lambda/2.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03b2</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mi>j</mi></msub><mo>-</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>\u03bb</mi><mo>/</mo><mn>2</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]