[{"file": "1601.00740.tex", "nexttext": "  \nwhere $f$ is a non-linear function applied element-wise, and $\\mathbf{y}_t$ is the \\texttt{softmax} probabilities of the events having seen the {observations} up to $\\mathbf{x}_t$.  $\\mathbf{W}$, $\\mathbf{H}$,  $\\mathbf{b}$, $\\mathbf{W}_y$,  $\\mathbf{b}_y$ are the parameters that are learned. Matrices are denoted with bold, capital letters, and vectors are denoted with bold, lower-case letters.  In a standard RNN a common choice for $f$ is \\texttt{tanh} or \\texttt{sigmoid}. RNNs with this choice of $f$ suffer from a well-studied problem of \\textit{vanishing gradients}~\\citep{Pascanu12}, and hence are poor at capturing long temporal dependencies which are essential for anticipation. A common remedy to vanishing gradients is to replace \\texttt{tanh} non-linearities by Long Short-Term Memory cells~\\citep{Hochreiter97}.  We now give an overview of LSTM and then describe our model for anticipation. \n\n\\subsection{Long-Short Term Memory Cells}\n\nLSTM is a network of neurons that implements a memory cell~\\citep{Hochreiter97}. The central idea behind LSTM is that the memory cell can maintain its state over time. When combined with RNN, LSTM units allow the recurrent network to remember long term context dependencies.\n\nLSTM consists of three gates -- input gate $\\mathbf{i}$, output gate $\\mathbf{o}$, and forget gate $\\mathbf{f}$ -- and a memory cell $\\mathbf{c}$. {See Figure~\\ref{fig:lstm} for an illustration.} \nAt each time step $t$, LSTM first computes its gates' activations \\{$\\mathbf{i}_t$,$\\mathbf{f}_t$\\}~\\eqref{eq:i-lstm}\\eqref{eq:f-lstm} and updates its memory cell from $\\mathbf{c}_{t-1}$ to $\\mathbf{c}_t$~\\eqref{eq:c-lstm}, it then computes the output gate activation $\\mathbf{o}_t$~\\eqref{eq:o-lstm}, and finally outputs a hidden representation $\\mathbf{h}_t$~\\eqref{eq:h-lstm}. The inputs into LSTM are the {observations} $\\mathbf{x}_t$ and the hidden representation from the previous time step $\\mathbf{h}_{t-1}$. LSTM applies the following set of update operations:  \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\title{Brain4Cars: Car That Knows Before You Do \\\\ via Sensory-Fusion Deep Learning\nArchitecture}\n\\author[1,2]{Ashesh Jain}\n\\author[1,2]{Hema S Koppula}\n\\author[2]{Shane Soh}\n\\author[2]{Bharad Raghavan}\n\\author[1]{Avi Singh}\n\\author[3]{Ashutosh Saxena}\n\\affil[ ]{Cornell University$^1$, Stanford University$^2$, Brain Of Things Inc.$^3$}\n\\affil[ ]{{\\{ashesh,hema\\}@cs.cornell.edu, avisingh@iitk.ac.in, \\{shanesoh,bharadr,asaxena\\}@cs.stanford.edu}}\n\n\\maketitle\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\begin{abstract}\n\n\nAdvanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for  unsafe road conditions and alert drivers if they perform a dangerous maneuver.  However, many accidents are unavoidable because by the time drivers are alerted, it is already too late.  Anticipating maneuvers beforehand can alert drivers before they perform the  maneuver and also give ADAS more time to avoid or prepare for the danger. \n\nIn this work we propose a vehicular sensor-rich platform and learning algorithms for  maneuver anticipation. For this purpose we equip a car with cameras, Global Positioning System (GPS), and a computing device to capture the driving context from both inside and outside of the car. In order to anticipate maneuvers, we propose a sensory-fusion deep learning architecture which jointly learns to anticipate and fuse  multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We propose a novel training procedure which allows the network to predict the future given only a partial temporal context. We introduce a  diverse data set with 1180 miles of natural freeway and city driving, and show that we can anticipate  maneuvers 3.5 seconds before they occur in real-time with a precision and recall of 90.5\\% and 87.4\\% respectively.\n\n\\iffalse\nAdvanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for  unsafe road conditions and alert drivers if they perform a dangerous maneuver.  However, many accidents are unavoidable because by the time drivers are alerted, it is already too late.  Anticipating maneuvers beforehand can alert drivers before they perform the  maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we present a vehicular sensory platform and deep learning architectures for anticipating driving maneuvers several seconds in advance. For this purpose we equip a car with cameras, Global Positioning System (GPS), and a computing device to capture the driving context from both inside and outside of the car. \n\nAnticipating the future actions of a human is a widely studied problem in robotics that requires spatio-temporal reasoning. We propose a generic deep learning approach for anticipation in sensory-rich robotics applications. We introduce a sensory-fusion architecture which jointly learns to anticipate and fuse information from multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We propose a novel training procedure which allows the network to predict the future given only a partial temporal context. We evaluate our approach on a  diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate  maneuvers 3.5 seconds before they occur in real-time with a precision and recall of 90.5\\% and 87.4\\% respectively.\n\\fi\n\n\n\\iffalse\nUnderstanding human activities is a widely studied problem in robotics that requires spatio-temporal reasoning. Previous works have modeled them as structured prediction using graphical models such as Conditional Random Field (CRF). In this work we represent such spatio-temporal graphical models with deep recurrent neural network (RNN) architectures. Our RNN architectures capture both the node and edge interactions of the graphical models, and their depth captures long term temporal interactions. We show representation of two spatio-temporal graphical models as deep RNN, and use it to demonstrate state-of-the-art results on human activity detection and anticipation data sets.  \n\\fi\n\n\n\\end{abstract}\n\n\n\n\n\\section{Introduction}\nOver the last decade\ncars have been equipped with various assistive technologies in order to\nprovide a safe driving experience. Technologies such as lane keeping, blind spot check,\npre-crash systems etc.,  are successful in alerting drivers whenever they commit a\ndangerous maneuver~\\citep{Laugier11}. Still in the US alone more than 33,000 people die in road\naccidents every year, the majority of which are due to inappropriate\nmaneuvers~\\cite{road_accidents}. \nWe therefore need mechanisms that\t can alert drivers \\textit{before} they perform a dangerous maneuver \nin order to avert many such accidents~\\citep{Rueda04}.  \n\n\nIn this work we address the problem of anticipating maneuvers that a driver is likely to perform in the next few seconds. Figure~\\ref{fig:intro} shows our system anticipating a left turn maneuver a few seconds before the car reaches the intersection. Our system also outputs probabilities over the  maneuvers the driver can perform. With this prior knowledge of maneuvers, the driver assistance systems can alert drivers about possible dangers before they perform the maneuver, \nthereby giving them more time to react. Some previous works~\\citep{Frohlich14,Kumar13,Morris11} also predict a driver's  future maneuver.  However, as we show in the following sections, these methods use limited context and/or do not accurately model the anticipation problem.\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.8\\linewidth]{intropic.pdf}\n\\caption{\\textbf{Anticipating maneuvers.} Our algorithm anticipates driving maneuvers performed a few seconds in the future. It uses information from multiple sources including videos, vehicle dynamics, GPS, and street maps to anticipate the probability of different future maneuvers.}\n\\label{fig:intro}\n\\end{figure}\n\n\nIn order to anticipate maneuvers, we reason with the contextual information from \nthe surrounding events, which we refer to as the \\textit{driving context}. \nWe obtain this driving context from multiple sources. We use videos\nof the driver inside the car and the road in front, the vehicle's dynamics,\nglobal position coordinates (GPS), and street maps; from this we extract a\ntime series of multi-modal data from both inside and outside the vehicle. The\nchallenge lies in modeling the temporal aspects of driving and fusing the multiple sensory streams. \nIn this work we propose a specially tailored approach for anticipation in such sensory-rich settings.\n\n\nAnticipation of the future actions of a human is an important perception task with applications in robotics and computer vision~\\citep{Kuderer12,Ziebart09,Kitani12,Koppula13,Wang13}. It requires the prediction of future events from  a limited temporal context. This differentiates anticipation from \\textit{activity recognition}~\\citep{Wang13}, where the complete temporal context is available for prediction.  Furthermore, in sensory-rich robotics settings like ours, the context for anticipation comes from multiple sensors. In such scenarios the end performance of the application largely depends on how the information from different sensors are fused. Previous works on anticipation~\\citep{Kitani12,Koppula13,Kuderer12} usually deal with single-data modality and do not address anticipation for sensory-rich robotics applications. Additionally, they learn representations using shallow architectures~\\citep{Jain15,Kitani12,Koppula13,Kuderer12} that cannot handle long temporal dependencies~\\citep{Bengio11}. \n\n\\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=\\linewidth]{rnn_anticipation.pdf}\n\\caption{(\\textbf{Left}) Shows training RNN for anticipation in a sequence-to-sequence prediction manner. The network explicitly learns to map the partial context $({\\mathbf{{x}}}_1,..,{\\mathbf{{x}}}_t)\\;\\forall t$ to the future event ${\\mathbf{{y}}}$. (\\textbf{Right}) At test time the network's goal is to anticipate the future event as soon as possible, i.e. by observing only a partial temporal context.}\n\\label{fig:introfig}\n\\end{figure}\n\nIn order to address the anticipation problem more generally, we propose a Recurrent Neural Network (RNN) based architecture which learns rich representations for anticipation. We focus on sensory-rich robotics applications, and our  architecture learns how to optimally fuse information from different sensors. Our approach  captures temporal dependencies by using Long Short-Term Memory (LSTM) units.  We train our architecture in a sequence-to-sequence prediction manner (Figure~\\ref{fig:introfig}) such that it explicitly learns to anticipate given a partial context, and we introduce a novel loss layer which helps anticipation by preventing over-fitting. \n\nWe evaluate our approach on a driving data set with 1180 miles of natural freeway and\ncity driving collected across two states -- from 10 drivers and with different kinds of driving maneuvers. The data set is challenging because of the variations in routes and traffic conditions, and the driving styles of the drivers (Figure~\\ref{fig:dataset}). We demonstrate that our deep learning sensory-fusion approach anticipates \nmaneuvers 3.5 seconds before they occur with {84.5\\%} precision and {77.1\\%} recall while using out-of-the-box face tracker. With more sophesticated 3D pose estimation of the face, our precision and recall increases to \\textbf{90.5\\%} and \\textbf{87.4\\%} respectively. We believe that our work creates scope for new ADAS features to make roads safer. In summary our key contributions are as follows: \n\n\n \\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=.9\\linewidth]{dataset.pdf}\n\\caption{\\textbf{Variations in the data set.} {Images from the data set~\\citep{Jain15} for} a left lane change. (\\textbf{Left}) Views from the road facing camera. (\\textbf{Right}) Driving style of the drivers vary for the same maneuver.}\n\\label{fig:dataset}\n\\end{figure}\n\n\\begin{itemize}\n\\itemsep0em \n\\item We propose an approach for anticipating driving maneuvers several seconds in advance.\n\\item We propose a generic sensory-fusion RNN-LSTM architecture for anticipation in robotics applications.\n\\item We release the first data set of natural driving with videos from both inside and outside the car, GPS, and speed information.\n\\item We release an open-source deep learning package \\href{https://github.com/asheshjain399/NeuralModels}{\\texttt{NeuralModels}} which is especially designed for robotics applications with multiple sensory streams. \n\\end{itemize}\nOur data set and deep learning code are publicly available at: \\url{http://www.brain4cars.com}\n\n\n\\section{Related Work}\n\n\nOur work builds upon the previous works on assisitive vehicular technologies, anticipating human activities, learning temporal models, and computer vision methods for analyzing human face. \n\n\\noindent \\textbf{Assistive features for vehicles.} \nLatest cars available in market comes equipped with cameras and sensors to monitor the surrounding environment. Through multi-sensory fusion they provide assisitive features like lane keeping, forward collision avoidance, adaptive cruise control etc. These systems warn drivers when they perform a potentially dangerous maneuver~\\citep{Shia14,Vasudevan12}. Driver monitoring for distraction and drowsiness has also been extensively researched~\\citep{Fletcher05,Rezaei14}. Techniques like eye-gaze tracking are now commercially available (Seeing Machines Ltd.) and has been effective in detecting distraction. Our work complements existing ADAS and driver monitoring techniques by anticipating maneuvers several seconds before they occur.\n\n\n\nClosely related to us are previous works on predicting the driver's intent. Vehicle trajectory has been used to predict the intent for lane change or turn maneuver~\\citep{Berndt08,Frohlich14,Kumar13,Liebner12}. Most of these works ignore the rich context available from cameras, GPS, and street maps. Previous works have addressed maneuver anticipation~\\citep{BoschURBAN,Morris11,Doshi11,Trivedi07} through sensory-fusion from multiple cameras, GPS, and vehicle dynamics. In particular, Morris et al.~\\citep{Morris11} and Trivedi et al.~\\citep{Trivedi07} used Relevance Vector Machine (RVM) for intent prediction and performed sensory fusion by concatenating feature vectors. We will show that such hand designed concatenation of features does not work well. Furthermore, these works do not model the temporal aspect of the problem properly. They assume that informative contextual cues always appear at a fixed time before the maneuver. We show that this assumption is not true, and in fact the temporal aspect of the problem should be carefully modeled. In contrast to these works, our RNN-LSTM based sensory-fusion architecture captures long temporal dependencies through its memory cell and learns rich representations for anticipation through a hierarchy of non-linear transformations of input data.  {Our work is also \nrelated to works on driver} behavior prediction with different sensors~\\citep{Jabon10,Fletcher05,Fletcher03}, and vehicular controllers which act on these predictions~\\citep{Shia14,Vasudevan12,Driggs15}.\n\n\\noindent \\textbf{Anticipation and Modeling Humans.} Modeling of human motion has given rise to many applications, anticipation being one of them. Anticipating human activities has  shown to improve human-robot collaboration~\\citep{Wang13,Koppula15,Mainprice13,Koppula14,Dragan12}. Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans~\\citep{Kitani12,Bennewitz05,Kuderer12,Jain15_icra}. Feature matching techniques have been proposed for anticipating human activities from videos~\\citep{Ryoo11}. Modeling human preferences has enabled robots to plan good trajectories~\\citep{Dragan13b,Sisbot07,Jain13b,Jain15_ijrr}. Similar to these works, we anticipate human actions, which are driving maneuvers in our case. However, the algorithms proposed in the previous works do not apply in our setting. In our case, anticipating maneuvers requires modeling the interaction between the driving context and the driver's intention. Such interactions are absent in the previous works, and they use shallow architectures~\\citep{Bengio11} that do not properly model temporal aspects of human activities. They further deal with a single data modality and do not tackle the challenges of sensory-fusion. Our problem setup involves all these challenges, for which we propose a deep learning approach which efficiently handles temporal dependencies and learns to fuse multiple sensory streams.\n\n\n\\noindent \\textbf{Analyzing the human face.} \nThe vision approaches related to our work are face detection and tracking~\\citep{Viola04,Zhang10}, statistical models of face~\\citep{Cootes01} and pose estimation methods for face~\\citep{Xiong14}. Active Appearance Model (AAM)~\\citep{Cootes01} and its variants~\\citep{Matthews04,Xiong13} statistically model the shape and texture of the face. AAMs have also been used to estimate the 3D-pose of a face from a single image~\\citep{Xiong14} and in design of assistive features for driver monitoring~\\citep{Rezaei14,Tawari14b}. In our approach we adapt off-the-shelf available face detection~\\citep{Viola04} and tracking algorithms~\\citep{Shi94} (see Section~\\ref{sec:features}). Our approach allows us to easily experiment with more advanced face detection and tracking algorithms. We demonstrate this by using the Constrained Local Neural Field (CLNF) model~\\citep{Baltrusaitis13} and tracking 68 fixed landmark points on the driver's face and estimating the 3D head-pose.\n\n\\noindent \\textbf{Learning temporal models.} Temporal models are commonly used to model human activities~\\citep{Koppula13c,Morency07,Wang06,Wang05}. These models have been used in both discriminative and generative fashions. The discriminative temporal models are mostly inspired by the Conditional Random Field (CRF)~\\citep{Lafferty01} which  captures the temporal structure of the problem. Wang et al.~\\citep{Wang05} and Morency et al.~\\citep{Morency07} propose dynamic extensions of the CRF for image segmentation and gesture recognition respectively. On the other hand, generative approaches for temporal modeling include various filtering methods, such as Kalman and particle filters~\\citep{Thrun05}, Hidden Markov Models, and many types of Dynamic Bayesian Networks~\\citep{Murphy12}. Some previous works~\\citep{Berndt08,Kuge00,Oliver00} used HMMs to model different aspects of the driver's behaviour. Most of these generative approaches model how latent (hidden) states influence the observations. However, in our problem both the latent states and the observations influence each other. \nIn the following sections, we will describe the Autoregressive Input-Output HMM (AIO-HMM) for maneuver anticipation~\\citep{Jain15} and will use it as a baseline to compare our deep learning approach. Unlike AIO-HMM our deep architecture have internal memory which allows it to handle long temporal dependencies~\\citep{Hihi95}. Furthermore, the input features undergo a hierarchy of  non-linear transformation through the deep architecture which allows learning rich representations. \n\nTwo building blocks of our architecture are Recurrent Neural Networks (RNNs)~\\citep{Pascanu12} and Long Short-Term Memory (LSTM) units~\\citep{Hochreiter97}. Our work draws upon ideas from previous works on RNNs and LSTM from the language~\\citep{Sutskever14}, speech~\\citep{Hannun14}, and vision~\\citep{Donahue15} communities. \nOur approach to the joint training of multiple RNNs is related to the recent work on hierarchical RNNs~\\citep{Du15}. We consider RNNs in multi-modal setting, which is related to the recent use of RNNs in image-captioning~\\citep{Donahue15}. Our contribution lies in formulating activity anticipation in a deep learning framework using RNNs with LSTM units. We focus on sensory-rich robotics applications, and our architecture extends previous works doing sensory-fusion with feed-forward networks~\\citep{Ngiam11,Sung15} to the fusion of temporal streams. Using our architecture we demonstrate state-of-the-art on maneuver anticipation.\n\n\n         \n\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{sys_ijrr.pdf}\n\\caption{\\textbf{System Overview.} Our system anticipating a left lane\nchange maneuver. (a) We process multi-modal data including \nGPS, speed, street maps, and events inside and\noutside of the vehicle using video cameras. (b) Vision pipeline extracts visual cues such as driver's head movements.\n (c) The inside and outside driving context is processed to extract expressive\n features. (d,e) Using our deep learning architecture we fuse the information from outside and inside the vehicle and anticipate the probability of each maneuver.}\n\\label{fig:system}\n\\end{figure*}\n\n\\section{Overview}\n\\label{sec:overview}\n\nWe first give an overview of the maneuver anticipation problem and then describe our system. \n\\subsection{Problem Overview}\n\nOur goal is to anticipate  driving maneuvers a few seconds before they occur. This includes anticipating a lane change before the wheels touch the lane markings or anticipating if the driver keeps straight or makes a turn when approaching an intersection. This is a challenging problem for multiple reasons. First, it requires the modeling of context from different sources. Information from a single source, such as a camera  capturing events outside the car, is not sufficiently rich.  Additional visual information from within the car can also be used.  For example, the driver's head movements are useful for anticipation -- drivers typically check for the side traffic while changing lanes and scan the cross traffic at intersections.\n\nSecond, reasoning about maneuvers should take into account the driving context at both local and global levels. Local context requires modeling events in vehicle's vicinity such as the surrounding vision, GPS, and speed information. On the other hand, factors that influence the overall route contributes to the global context, such as the driver's final destination. Third, the informative cues necessary for anticipation appear at variable times before the maneuver, as illustrated in Figure~\\ref{fig:time_variance}. In particular, the time interval between the driver's head movement and the occurrence of the maneuver depends on many factors such as the speed, traffic conditions, etc.\n\nIn addition, appropriately fusing the information from multiple sensors is crucial for anticipation. Simple sensory fusion approaches like concatenation of feature vectors performs poorly, as we demonstrate through experiments. In our proposed approach we learn a neural network layer for fusing the temporal streams of data coming from different sensors. Our resulting architecture is end-to-end trainable via back propagation, and we jointly train it to: (i) model the temporal aspects of the problem; (ii) fuse multiple sensory streams; and (iii) anticipate maneuvers.\n\n\\iffalse\nWe obtain the driving context from different sources as shown in Figure~\\ref{fig:system}. Our system includes: (1) a driver-facing camera inside the vehicle, (2) a road-facing camera outside the vehicle, (3) a speed logger, and (4) a GPS and map \nlogger. The information from these sources constitute the \\textit{driving context}.  We use the face camera to track the driver's head movements. The video from the road camera enables additional reasoning on maneuvers. For example, when the vehicle is in the left-most lane, the only safe maneuvers are a right-lane change or keeping straight, unless the vehicle is approaching an intersection. Maneuvers also correlate with the vehicle's speed, e.g., turns usually happen at  lower speeds than lane changes. Additionally, the GPS data augmented with the street map enables us to detect upcoming road artifacts such as intersections, highway exits, etc. In the following sections, we describe our deep learning approach to anticipation for sensory-rich applications like ours. \n\\fi\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.85\\linewidth]{variable_time.pdf}\n\\caption{\\textbf{Variable time occurrence of events.} \\textit{Left}: The events inside the vehicle before the maneuvers. We track the driver's face along with many facial points. \\textit{Right}: The trajectories generated by the horizontal motion of facial points (pixels) `t' seconds before the maneuver. X-axis is the time and Y-axis is the pixels' horizontal coordinates. Informative cues appear during the shaded time interval. Such cues occur at variable times before the maneuver, and the order in which the cues appear is also important.}\n\n\\label{fig:time_variance}\n\\end{figure}\n\n\n\\subsection{System Overview}\n\nFor maneuver anticipation our vehicular sensory platform includes the following (as shown in Figure~\\ref{fig:system}): \n\\begin{enumerate}\n\\item A driver-facing camera inside the vehicle. We mount this camera on the dashboard and use it to track the driver's head movements. This camera operates at 25 fps.\n\\item A camera facing the road is mounted on the dashboard to capture the (outside) view in front of the car. This camera operates at 30 fps. The video from this camera enables additional reasoning on maneuvers. For example, when the vehicle is in the left-most lane, the only safe maneuvers are a right-lane change or keeping straight, unless the vehicle is approaching an intersection.\n\\item A speed logger for vehicle dynamics because maneuvers correlate with the vehicle's speed, e.g., turns usually happen at  lower speeds than lane changes.\n\\item A Global Positioning System (GPS) for localizing the vehicle on the map. This enables us to detect upcoming road artifacts such as intersections, highway exits, etc.\n\\end{enumerate}\n\nUsing this system we collect 1180 miles of natural city and freeway driving data from 10 drivers. We denote the information from sensors with feature vector ${\\mathbf{{x}}}$. Our vehicular systems gives a temporal sequence of feature vectors $\\{({\\mathbf{{x}}}_1,{\\mathbf{{x}}}_2,...,{\\mathbf{{x}}}_t,...)\\}$.  For now we do not distinguish between the information from different sensors, later in Section~\\ref{sec:sensor-fusion-rnn} we introduce sensory fusion. In Section~\\ref{sec:features} we formally define our feature representations and describe our data set in Section~\\ref{subsec:dataset}. \nWe now formally define anticipation and present our deep learning architecture. \n\n\n\n\n\\iffalse\nFor maneuver anticipation our vehicular sensory platform includes: (1) a driver-facing camera inside the vehicle, (2) a road-facing camera outside the vehicle, (3) a speed logger, and (4) a GPS and map \nlogger. The information from these sources constitute the \\textit{driving context}.  Figure~\\ref{fig:system} shows the overall anticipation pipeline. We use the face camera to track the driver's head movements. The video from the road camera enables additional reasoning on maneuvers. For example, when the vehicle is in the left-most lane, the only safe maneuvers are a right-lane change or keeping straight, unless the vehicle is approaching an intersection. Maneuvers also correlate with the vehicle's speed, e.g., turns usually happen at  lower speeds than lane changes. Additionally, the GPS data augmented with the street map enables us to detect upcoming road artifacts such as intersections, highway exits, etc. In Section~\\ref{sec:features} we formally define our feature representations. Using this system we collect 1180 miles of natural city and freeway driving data from 10 drivers. In Section~\\ref{subsec:dataset} we describe our data set. In the following sections, we describe our deep learning approach to anticipation for sensory-rich applications like ours.\n\\fi\n\n\n\n\n\n\\section{Preliminaries}\n\nWe now formally define anticipation and then present our Recurrent Neural\nNetwork architecture. The goal of anticipation is to predict an event several\nseconds before it happens given the contextual information up to the present\ntime. The future event can be one of multiple possibilities. At training\ntime a set of temporal sequences of {observations} and events\n$\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)_j,{\\mathbf{{y}}}_j\\}_{j=1}^N$ is provided\nwhere $\\mathbf{x}_t$ is the {observation} at time $t$, ${\\mathbf{{y}}}$ is the representation of the event (described below) that happens at the end of the sequence  at $t= T$,\n\nand $j$ is the sequence index. At test\ntime, however, the algorithm receives an {observation} $\\mathbf{x}_t$ at each time step,\nand its goal is to predict the future event as early as possible, i.e. by\nobserving only a partial sequence of {observations} $\\{(\\mathbf{x}_1,...,\\mathbf{x}_t)\n| t < T\\}$. This differentiates anticipation from \\textit{activity recognition}~\\citep{Wang13b,Koppula13b} where in the latter  the complete {observation} sequence is available at  test time. In this paper, ${\\mathbf{{x}}}_t$ is a real-valued feature vector and ${\\mathbf{{y}}} =[y^1,...,y^K]$ is a vector of size $K$ (the number of events), where $y^k$ denotes the probability of the temporal sequence belonging to event the $k$ such that $\\sum_{k=1}^K y^k = 1$. At the time of training, ${\\mathbf{{y}}}$ takes the form of a one-hot vector with the entry in ${\\mathbf{{y}}}$ corresponding to the ground truth event as $1$ and the rest $0$. \n\n\nIn this work we propose a deep RNN architecture with Long Short-Term Memory\n(LSTM) units~\\citep{Hochreiter97} for anticipation. Below we give an overview of\nthe standard RNN and LSTM which form the building blocks of our architecture.\n\n\n\\subsection{Recurrent Neural Networks}\n\nA standard RNN~\\citep{Pascanu12} takes in a temporal sequence of vectors $(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)$ as input, and outputs a sequence of vectors $(\\mathbf{h}_1,\\mathbf{h}_2,...,\\mathbf{h}_T)$ also known as high-level representations. The representations are generated by non-linear transformation of the input sequence from $t=1\t$ to $T$, as described in the equations below.\n\n", "index": 1, "text": "\\begin{align}\n\\label{eq:h-rnn} \\mathbf{h}_t &= f(\\mathbf{W}\\mathbf{x}_t + \\mathbf{H}\\mathbf{h}_{t-1} + \\mathbf{b})\\\\\n\\label{eq:y-rnn} \\mathbf{y}_t &= \\texttt{softmax} (\\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{h}_{t}\" display=\"inline\"><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=f(\\mathbf{W}\\mathbf{x}_{t}+\\mathbf{H}\\mathbf{h}_{t-1}+\\mathbf{b})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc16\ud835\udc31</mi><mi>t</mi></msub><mo>+</mo><msub><mi>\ud835\udc07\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>\ud835\udc1b</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{y}_{t}\" display=\"inline\"><msub><mi>\ud835\udc32</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\texttt{softmax}(\\mathbf{W}_{y}\\mathbf{h}_{t}+\\mathbf{b}_{y})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>\ud835\ude9c\ud835\ude98\ud835\ude8f\ud835\ude9d\ud835\ude96\ud835\ude8a\ud835\udea1</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>y</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\nwhere $\\odot$ is an element-wise product and $\\sigma$ is the logistic function. $\\sigma$ and \\texttt{tanh} are applied element-wise. ${\\mathbf{{W}}}_*$, ${\\mathbf{{V}}}_*$, ${\\mathbf{{U}}}_*$, and ${\\mathbf{{b}}}_*$ are the parameters, further the weight matrices ${\\mathbf{{V}}}_*$ are diagonal. The input and forget gates of LSTM participate in updating the memory cell~\\eqref{eq:c-lstm}. More specifically, forget gate controls the part of memory to forget, and the input gate computes new values based on the current {observation} that are written to the memory cell. The output gate together with the memory cell computes the hidden representation~\\eqref{eq:h-lstm}. Since  LSTM cell activation involves \\textit{summation} over time~\\eqref{eq:c-lstm} and derivatives distribute over sums, the gradient in LSTM gets propagated over a longer time before vanishing. In the standard RNN, we replace the non-linear $f$ in equation~\\eqref{eq:h-rnn} by the LSTM equations given above in order to capture long temporal dependencies. We use the following shorthand notation to denote the recurrent LSTM operation.\n\n", "itemtype": "equation", "pos": 30470, "prevtext": "  \nwhere $f$ is a non-linear function applied element-wise, and $\\mathbf{y}_t$ is the \\texttt{softmax} probabilities of the events having seen the {observations} up to $\\mathbf{x}_t$.  $\\mathbf{W}$, $\\mathbf{H}$,  $\\mathbf{b}$, $\\mathbf{W}_y$,  $\\mathbf{b}_y$ are the parameters that are learned. Matrices are denoted with bold, capital letters, and vectors are denoted with bold, lower-case letters.  In a standard RNN a common choice for $f$ is \\texttt{tanh} or \\texttt{sigmoid}. RNNs with this choice of $f$ suffer from a well-studied problem of \\textit{vanishing gradients}~\\citep{Pascanu12}, and hence are poor at capturing long temporal dependencies which are essential for anticipation. A common remedy to vanishing gradients is to replace \\texttt{tanh} non-linearities by Long Short-Term Memory cells~\\citep{Hochreiter97}.  We now give an overview of LSTM and then describe our model for anticipation. \n\n\\subsection{Long-Short Term Memory Cells}\n\nLSTM is a network of neurons that implements a memory cell~\\citep{Hochreiter97}. The central idea behind LSTM is that the memory cell can maintain its state over time. When combined with RNN, LSTM units allow the recurrent network to remember long term context dependencies.\n\nLSTM consists of three gates -- input gate $\\mathbf{i}$, output gate $\\mathbf{o}$, and forget gate $\\mathbf{f}$ -- and a memory cell $\\mathbf{c}$. {See Figure~\\ref{fig:lstm} for an illustration.} \nAt each time step $t$, LSTM first computes its gates' activations \\{$\\mathbf{i}_t$,$\\mathbf{f}_t$\\}~\\eqref{eq:i-lstm}\\eqref{eq:f-lstm} and updates its memory cell from $\\mathbf{c}_{t-1}$ to $\\mathbf{c}_t$~\\eqref{eq:c-lstm}, it then computes the output gate activation $\\mathbf{o}_t$~\\eqref{eq:o-lstm}, and finally outputs a hidden representation $\\mathbf{h}_t$~\\eqref{eq:h-lstm}. The inputs into LSTM are the {observations} $\\mathbf{x}_t$ and the hidden representation from the previous time step $\\mathbf{h}_{t-1}$. LSTM applies the following set of update operations:  \n\n", "index": 3, "text": "\\begin{align}\n\\label{eq:i-lstm} {\\mathbf{{i}}}_t &= \\sigma({\\mathbf{{W}}}_{i}{\\mathbf{{x}}}_t + {\\mathbf{{U}}}_i {\\mathbf{{h}}}_{t-1} + {\\mathbf{{V}}}_i {\\mathbf{{c}}}_{t-1} + {\\mathbf{{b}}}_i)\\\\\n\\label{eq:f-lstm} {\\mathbf{{f}}}_t &= \\sigma({\\mathbf{{W}}}_{f}{\\mathbf{{x}}}_t + {\\mathbf{{U}}}_f {\\mathbf{{h}}}_{t-1} + {\\mathbf{{V}}}_f {\\mathbf{{c}}}_{t-1} +{\\mathbf{{b}}}_f)\\\\\n\\label{eq:c-lstm} {\\mathbf{{c}}}_t &= {\\mathbf{{f}}}_t \\odot {\\mathbf{{c}}}_{t-1} + {\\mathbf{{i}}}_t \\odot\t \\texttt{tanh} ({\\mathbf{{W}}}_{c}{\\mathbf{{x}}}_t + {\\mathbf{{U}}}_c {\\mathbf{{h}}}_{t-1} +{\\mathbf{{b}}}_c)\\\\\n\\label{eq:o-lstm} {\\mathbf{{o}}}_t &= \\sigma({\\mathbf{{W}}}_{o}{\\mathbf{{x}}}_t + {\\mathbf{{U}}}_o {\\mathbf{{h}}}_{t-1} + {\\mathbf{{V}}}_o {\\mathbf{{c}}}_{t} +{\\mathbf{{b}}}_o)\\\\\n\\label{eq:h-lstm} {\\mathbf{{h}}}_t &= {\\mathbf{{o}}}_t \\odot \\texttt{tanh}({\\mathbf{{c}}}_{t}) \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{i}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc22</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\mathbf{{W}}}_{i}{\\mathbf{{x}}}_{t}+{\\mathbf{{U}}}_{i}{%&#10;\\mathbf{{h}}}_{t-1}+{\\mathbf{{V}}}_{i}{\\mathbf{{c}}}_{t-1}+{\\mathbf{{b}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc14</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc15</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{f}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc1f</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\mathbf{{W}}}_{f}{\\mathbf{{x}}}_{t}+{\\mathbf{{U}}}_{f}{%&#10;\\mathbf{{h}}}_{t-1}+{\\mathbf{{V}}}_{f}{\\mathbf{{c}}}_{t-1}+{\\mathbf{{b}}}_{f})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>f</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc14</mi><mi>f</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc15</mi><mi>f</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>f</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{c}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{{f}}}_{t}\\odot{\\mathbf{{c}}}_{t-1}+{\\mathbf{{i}}}_{t}%&#10;\\odot\\texttt{tanh}({\\mathbf{{W}}}_{c}{\\mathbf{{x}}}_{t}+{\\mathbf{{U}}}_{c}{%&#10;\\mathbf{{h}}}_{t-1}+{\\mathbf{{b}}}_{c})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc1f</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mrow><msub><mi>\ud835\udc22</mi><mi>t</mi></msub><mo>\u2299</mo><mtext>\ud835\ude9d\ud835\ude8a\ud835\ude97\ud835\ude91</mtext></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>c</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc14</mi><mi>c</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>c</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{o}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc28</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\mathbf{{W}}}_{o}{\\mathbf{{x}}}_{t}+{\\mathbf{{U}}}_{o}{%&#10;\\mathbf{{h}}}_{t-1}+{\\mathbf{{V}}}_{o}{\\mathbf{{c}}}_{t}+{\\mathbf{{b}}}_{o})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>o</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc14</mi><mi>o</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc15</mi><mi>o</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{h}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc21</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{{o}}}_{t}\\odot\\texttt{tanh}({\\mathbf{{c}}}_{t})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc28</mi><mi>t</mi></msub><mo>\u2299</mo><mtext>\ud835\ude9d\ud835\ude8a\ud835\ude97\ud835\ude91</mtext></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\nWe now describe our RNN architecture with LSTM units for anticipation. Following which we will describe a particular instantiation of our architecture for maneuver anticipation where the {observations} ${\\mathbf{{x}}}$ come from multiple sources.        \n\n\n\\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=.8\\linewidth]{lstm.png}\n\\caption{\\textbf{ Internal working of an LSTM unit.}}\n\\label{fig:lstm}\n\\end{figure}\n\n\n\n\n\\section{Network Architecture for Anticipation}\n\\label{sec:network}\nIn order to anticipate, an algorithm must learn to predict the future {given only} a partial temporal context. This makes anticipation challenging and also differentiates it from activity recognition. Previous works treat anticipation as a recognition problem~\\citep{Koppula13,Morris11,Ryoo11} and train discriminative classifiers (such as SVM or CRF) on the complete temporal context. However, at test time {these} classifiers only observe  a partial temporal context and make predictions within a filtering framework. We model anticipation with a recurrent architecture which unfolds through time. This lets us train a single classifier that learns to handle partial temporal context of varying lengths.\n\n\nFurthermore, anticipation in robotics applications is challenging because the contextual information can come from multiple sensors with different data modalities. Examples include autonomous vehicles that reason from multiple sensors~\\citep{Geiger12} or robots that jointly reason over perception and language instructions~\\citep{Misra14}. In such applications the way information from different sensors is fused is critical to the application's final performance. We therefore build an end-to-end deep learning architecture which jointly learns to anticipate and fuse information from different sensors.   \n\n\\subsection{RNN with LSTM units for anticipation}\n\nAt the time of training, we observe the complete temporal {observation sequence} and the event  $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T),{\\mathbf{{y}}}\\}$. Our goal is to train a network which predicts the future event given a partial temporal {observation sequence} $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_t) \t| t < T\\}$. We do so by training an RNN in a sequence-to-sequence prediction manner. Given training examples $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)_j,{\\mathbf{{y}}}_j\\}_{j=1}^N$ we train an RNN with LSTM units to map the sequence of {observations} $(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)$ to the sequence of events  $({\\mathbf{{y}}}_1,...,{\\mathbf{{y}}}_T)$ such that ${\\mathbf{{y}}}_{t} = {\\mathbf{{y}}}, \\forall t$, as shown in Fig.~\\ref{fig:introfig}. Trained in this manner, our RNN will attempt to map all sequences of partial {observations} $(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_t)$~$\\forall t \\leq~T$ to the future event ${\\mathbf{{y}}}$. This way our model explicitly learns to anticipate. We additionally use LSTM units which prevents the gradients from vanishing and allows our model to capture long temporal dependencies in human activities.\\footnote{Driving maneuvers can take up to 6 seconds and the value of T can go up to 150 with a camera frame rate of 25 fps.}  \n\n\n\n\n\n\n\\subsection{Fusion-RNN: Sensory fusion RNN for anticipation}\n\\label{sec:sensor-fusion-rnn}\n\n\nWe now present an instantiation of our RNN architecture for fusing two sensory streams: $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T), \\;({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$. In the next section we will describe these streams for maneuver anticipation.\n\n\n\\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=.9\\linewidth]{fusionRNN.pdf}\n\\caption{\\textbf{Sensory fusion RNN for anticipation.} (\\textbf{Bottom}) In the Fusion-RNN each sensory stream is passed through their independent RNN. (\\textbf{Middle}) High-level representations from RNNs are then combined through a fusion layer. (\\textbf{Top}) In order to prevent over-fitting early in time the loss exponentially increases with time.}\n\\label{fig:fusion}\n\\end{figure}\n\nAn obvious way to allow sensory fusion in the RNN is by concatenating the streams, i.e. using $([{\\mathbf{{x}}}_1;{\\mathbf{{z}}}_1],...,[{\\mathbf{{x}}}_T;{\\mathbf{{z}}}_T])$ as input to the RNN. However, we found that this sort of simple concatenation performs poorly. We instead learn a sensory fusion layer which combines the high-level representations of sensor data. Our proposed architecture first passes the two sensory streams $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T), \\;({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$ independently through separate  RNNs~\\eqref{eq:f-rnn1} and~\\eqref{eq:f-rnn2}.  \nThe high level representations from both RNNs $\\{({\\mathbf{{h}}}_1^x,...,{\\mathbf{{h}}}_T^x), \\;({\\mathbf{{h}}}_1^z,...,{\\mathbf{{h}}}_T^z)$ are then concatenated at each time step $t$ and passed through a fully connected (fusion) layer which fuses the two representations~\\eqref{eq:f-fusion}, {as shown in Figure~\\ref{fig:fusion}}. The output representation from the fusion layer is then passed to the softmax layer for anticipation~\\eqref{eq:f-output}. The following operations are performed from $t=1$ to $T$.\n\n", "itemtype": "equation", "pos": 32464, "prevtext": "\n\nwhere $\\odot$ is an element-wise product and $\\sigma$ is the logistic function. $\\sigma$ and \\texttt{tanh} are applied element-wise. ${\\mathbf{{W}}}_*$, ${\\mathbf{{V}}}_*$, ${\\mathbf{{U}}}_*$, and ${\\mathbf{{b}}}_*$ are the parameters, further the weight matrices ${\\mathbf{{V}}}_*$ are diagonal. The input and forget gates of LSTM participate in updating the memory cell~\\eqref{eq:c-lstm}. More specifically, forget gate controls the part of memory to forget, and the input gate computes new values based on the current {observation} that are written to the memory cell. The output gate together with the memory cell computes the hidden representation~\\eqref{eq:h-lstm}. Since  LSTM cell activation involves \\textit{summation} over time~\\eqref{eq:c-lstm} and derivatives distribute over sums, the gradient in LSTM gets propagated over a longer time before vanishing. In the standard RNN, we replace the non-linear $f$ in equation~\\eqref{eq:h-rnn} by the LSTM equations given above in order to capture long temporal dependencies. We use the following shorthand notation to denote the recurrent LSTM operation.\n\n", "index": 5, "text": "\\begin{equation}\n({\\mathbf{{h}}}_t,{\\mathbf{{c}}}_t) = \\text{LSTM}({\\mathbf{{x}}}_t,{\\mathbf{{h}}}_{t-1},{\\mathbf{{c}}}_{t-1})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"({\\mathbf{{h}}}_{t},{\\mathbf{{c}}}_{t})=\\text{LSTM}({\\mathbf{{x}}}_{t},{%&#10;\\mathbf{{h}}}_{t-1},{\\mathbf{{c}}}_{t-1})\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc21</mi><mi>t</mi></msub><mo>,</mo><msub><mi>\ud835\udc1c</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mtext>LSTM</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo>,</mo><msub><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\t \nwhere ${\\mathbf{{W}}}_*$ and ${\\mathbf{{b}}}_*$ are model parameters, and $\\text{LSTM}_x$ and $\\text{LSTM}_z$ process the sensory streams $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T)$ and $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)$ respectively. {The same framework can be extended to handle more sensory streams.} \n\n\\subsection{Exponential loss-layer for anticipation.} \nWe propose a new loss layer which encourages the architecture to anticipate early while also ensuring that the architecture does not over-fit the training data early enough in time when there is not enough context for anticipation. \nWhen using the standard softmax loss, the architecture suffers a loss of $-\\log(y_t^k)$ for the mistakes it makes at each time step, where $y_t^k$ is the probability of the ground truth event $k$ computed by the architecture using Eq.~\\eqref{eq:f-output}. We propose to modify this loss by multiplying it with an exponential term as illustrated in Figure~\\ref{fig:fusion}. Under this new scheme, the loss exponentially grows with time as shown below. \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nWe now describe our RNN architecture with LSTM units for anticipation. Following which we will describe a particular instantiation of our architecture for maneuver anticipation where the {observations} ${\\mathbf{{x}}}$ come from multiple sources.        \n\n\n\\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=.8\\linewidth]{lstm.png}\n\\caption{\\textbf{ Internal working of an LSTM unit.}}\n\\label{fig:lstm}\n\\end{figure}\n\n\n\n\n\\section{Network Architecture for Anticipation}\n\\label{sec:network}\nIn order to anticipate, an algorithm must learn to predict the future {given only} a partial temporal context. This makes anticipation challenging and also differentiates it from activity recognition. Previous works treat anticipation as a recognition problem~\\citep{Koppula13,Morris11,Ryoo11} and train discriminative classifiers (such as SVM or CRF) on the complete temporal context. However, at test time {these} classifiers only observe  a partial temporal context and make predictions within a filtering framework. We model anticipation with a recurrent architecture which unfolds through time. This lets us train a single classifier that learns to handle partial temporal context of varying lengths.\n\n\nFurthermore, anticipation in robotics applications is challenging because the contextual information can come from multiple sensors with different data modalities. Examples include autonomous vehicles that reason from multiple sensors~\\citep{Geiger12} or robots that jointly reason over perception and language instructions~\\citep{Misra14}. In such applications the way information from different sensors is fused is critical to the application's final performance. We therefore build an end-to-end deep learning architecture which jointly learns to anticipate and fuse information from different sensors.   \n\n\\subsection{RNN with LSTM units for anticipation}\n\nAt the time of training, we observe the complete temporal {observation sequence} and the event  $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T),{\\mathbf{{y}}}\\}$. Our goal is to train a network which predicts the future event given a partial temporal {observation sequence} $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_t) \t| t < T\\}$. We do so by training an RNN in a sequence-to-sequence prediction manner. Given training examples $\\{(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)_j,{\\mathbf{{y}}}_j\\}_{j=1}^N$ we train an RNN with LSTM units to map the sequence of {observations} $(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T)$ to the sequence of events  $({\\mathbf{{y}}}_1,...,{\\mathbf{{y}}}_T)$ such that ${\\mathbf{{y}}}_{t} = {\\mathbf{{y}}}, \\forall t$, as shown in Fig.~\\ref{fig:introfig}. Trained in this manner, our RNN will attempt to map all sequences of partial {observations} $(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_t)$~$\\forall t \\leq~T$ to the future event ${\\mathbf{{y}}}$. This way our model explicitly learns to anticipate. We additionally use LSTM units which prevents the gradients from vanishing and allows our model to capture long temporal dependencies in human activities.\\footnote{Driving maneuvers can take up to 6 seconds and the value of T can go up to 150 with a camera frame rate of 25 fps.}  \n\n\n\n\n\n\n\\subsection{Fusion-RNN: Sensory fusion RNN for anticipation}\n\\label{sec:sensor-fusion-rnn}\n\n\nWe now present an instantiation of our RNN architecture for fusing two sensory streams: $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T), \\;({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$. In the next section we will describe these streams for maneuver anticipation.\n\n\n\\begin{figure}[t]\n\\centering\t\n\\includegraphics[width=.9\\linewidth]{fusionRNN.pdf}\n\\caption{\\textbf{Sensory fusion RNN for anticipation.} (\\textbf{Bottom}) In the Fusion-RNN each sensory stream is passed through their independent RNN. (\\textbf{Middle}) High-level representations from RNNs are then combined through a fusion layer. (\\textbf{Top}) In order to prevent over-fitting early in time the loss exponentially increases with time.}\n\\label{fig:fusion}\n\\end{figure}\n\nAn obvious way to allow sensory fusion in the RNN is by concatenating the streams, i.e. using $([{\\mathbf{{x}}}_1;{\\mathbf{{z}}}_1],...,[{\\mathbf{{x}}}_T;{\\mathbf{{z}}}_T])$ as input to the RNN. However, we found that this sort of simple concatenation performs poorly. We instead learn a sensory fusion layer which combines the high-level representations of sensor data. Our proposed architecture first passes the two sensory streams $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T), \\;({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$ independently through separate  RNNs~\\eqref{eq:f-rnn1} and~\\eqref{eq:f-rnn2}.  \nThe high level representations from both RNNs $\\{({\\mathbf{{h}}}_1^x,...,{\\mathbf{{h}}}_T^x), \\;({\\mathbf{{h}}}_1^z,...,{\\mathbf{{h}}}_T^z)$ are then concatenated at each time step $t$ and passed through a fully connected (fusion) layer which fuses the two representations~\\eqref{eq:f-fusion}, {as shown in Figure~\\ref{fig:fusion}}. The output representation from the fusion layer is then passed to the softmax layer for anticipation~\\eqref{eq:f-output}. The following operations are performed from $t=1$ to $T$.\n\n", "index": 7, "text": "\\begin{align}\n\\label{eq:f-rnn1} ({\\mathbf{{h}}}_t^x,{\\mathbf{{c}}}_t^x) &= \\text{LSTM}_x({\\mathbf{{x}}}_t,{\\mathbf{{h}}}_{t-1}^x,{\\mathbf{{c}}}_{t-1}^x)\\\\\n\\label{eq:f-rnn2} ({\\mathbf{{h}}}_t^z,{\\mathbf{{c}}}_t^z) &= \\text{LSTM}_z({\\mathbf{{z}}}_t,{\\mathbf{{h}}}_{t-1}^z,{\\mathbf{{c}}}_{t-1}^z)\\\\\n\\label{eq:f-fusion} \\text{Sensory fusion:  } {\\mathbf{{e}}}_t &= \\texttt{tanh}({\\mathbf{{W}}}_f[{\\mathbf{{h}}}_t^x;{\\mathbf{{h}}}_t^z] + {\\mathbf{{b}}}_f)\\\\\n\\label{eq:f-output} {\\mathbf{{y}}}_t &= \\texttt{softmax}({\\mathbf{{W}}}_y{\\mathbf{{e}}}_t + {\\mathbf{{b}}}_y)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\mathbf{{h}}}_{t}^{x},{\\mathbf{{c}}}_{t}^{x})\" display=\"inline\"><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc1c</mi><mi>t</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{LSTM}_{x}({\\mathbf{{x}}}_{t},{\\mathbf{{h}}}_{t-1}^{x},{%&#10;\\mathbf{{c}}}_{t-1}^{x})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mtext>LSTM</mtext><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\mathbf{{h}}}_{t}^{z},{\\mathbf{{c}}}_{t}^{z})\" display=\"inline\"><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>z</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc1c</mi><mi>t</mi><mi>z</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{LSTM}_{z}({\\mathbf{{z}}}_{t},{\\mathbf{{h}}}_{t-1}^{z},{%&#10;\\mathbf{{c}}}_{t-1}^{z})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mtext>LSTM</mtext><mi>z</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>z</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc1c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>z</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{Sensory fusion: }{\\mathbf{{e}}}_{t}\" display=\"inline\"><mrow><mtext>Sensory fusion:\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udc1e</mi><mi>t</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\texttt{tanh}({\\mathbf{{W}}}_{f}[{\\mathbf{{h}}}_{t}^{x};{\\mathbf%&#10;{{h}}}_{t}^{z}]+{\\mathbf{{b}}}_{f})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>\ud835\ude9d\ud835\ude8a\ud835\ude97\ud835\ude91</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>x</mi></msubsup><mo>;</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>z</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>f</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{y}}}_{t}\" display=\"inline\"><msub><mi>\ud835\udc32</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\texttt{softmax}({\\mathbf{{W}}}_{y}{\\mathbf{{e}}}_{t}+{\\mathbf{{%&#10;b}}}_{y})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>\ud835\ude9c\ud835\ude98\ud835\ude8f\ud835\ude9d\ud835\ude96\ud835\ude8a\ud835\udea1</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udc16</mi><mi>y</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1e</mi><mi>t</mi></msub></mrow><mo>+</mo><msub><mi>\ud835\udc1b</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nThis loss penalizes the RNN exponentially more for the mistakes it makes as it sees more {observations}. This encourages the model to fix mistakes as early as it can in time. The loss in equation~\\ref{eq:loss} also penalizes the network less on mistakes made early in time when there is not enough context available. This way it acts like a regularizer and reduces the risk to over-fit very early in time. \n\n\\iffalse\n\\subsection{Model training and data augmentation}\n\\label{subsec:augmentation}\n\nOur architecture for maneuver anticipation has more than 25,000 parameters that need to be learned (Section~\\ref{sec:maneuver}). With such a large number of parameters on a non-convex manifold, over-fitting becomes a major challenge. We therefore introduce redundancy in the training data which acts like a regularizer and reduces  over-fitting~\\citep{Krizhevsky12,Hannun14}. \n In order to augment training data, we extract sub-sequences of temporal {observations}. Given a training example with two temporal sensor streams $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T),({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T),{\\mathbf{{y}}}\\}$, we uniformly randomly sample multiple sub-sequences $\\{({\\mathbf{{x}}}_i,...,{\\mathbf{{x}}}_j),({\\mathbf{{z}}}_i,...,{\\mathbf{{z}}}_j),{\\mathbf{{y}}} | 1 \\leq i < j \\leq T\\}$ as additional training examples. It is important to note that data augmentation only adds redundancy and \\textit{does not} rely on any external source of new information. \n\n\nOn the augmented data set, we train the network described in Section~\\ref{sec:sensor-fusion-rnn}. We use RMSprop gradients which have been shown to work well on training deep networks~\\citep{Dauphin15}, and we keep the step size fixed at $10^{-4}$. We experimented with different variants of softmax loss, and our proposed loss-layer with exponential growth Eq.~\\eqref{eq:loss} works best for anticipation (see Section~\\ref{experiments} for details).\n\\fi\n\n\n\n\n\\section{Features}\n\\label{sec:features}\n\n\n\\begin{figure}[t]\n\\centering\n\n\\includegraphics[width=.9\\linewidth]{feature.pdf}\n\\caption{\\textbf{Inside vehicle feature extraction.} The angular histogram features extracted at three different time steps for a left turn maneuver. \\textit{Bottom}: Trajectories for the horizontal motion of tracked facial pixels `t' seconds before the maneuver. At t=5 seconds before the maneuver the driver is looking straight, at t=3 looks (left) in the direction of maneuver, and at t=2 looks (right) in opposite direction for the crossing traffic. \\textit{Middle}: Average motion vector of tracked facial pixels in polar coordinates. $r$ is the average movement of pixels and arrow indicates the direction in which the face moves when looking from the camera. \\textit{Top}: Normalized angular histogram features. }\n\\label{fig:face_feature}\n\\end{figure}\n\n\n\n\n\n\n\nWe extract features by processing the inside and outside driving contexts. We do this by  grouping the overall contextual information from the sensors into: (i) the context from inside the vehicle, which comes from the driver facing camera and is represented as temporal sequence of features $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)$; and (ii) the context from outside the vehicle, which comes from the remaining sensors: GPS, road facing camera, and street maps. We represent the outside context with $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T)$. In order to anticipate maneuvers, our RNN architecture (Figure~\\ref{fig:fusion}) processes the temporal context $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_t),\\; ({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_t)\\}$  at every time step $t$, and outputs softmax probabilities ${\\mathbf{{y}}}_t$ for the following five maneuvers:  $\\mathcal{M}=$~\\{\\textit{left turn}, \\textit{right turn}, \\textit{left lane change}, \\textit{right lane change}, \\textit{straight driving}\\}.\n\n\n\\subsection{Inside-vehicle features.}\\label{subsec:inside_features} \nThe inside features ${\\mathbf{{z}}}_t$ capture the driver's head movements at each time instant $t$.  Our vision pipeline consists of face detection, tracking, and feature extraction modules. We extract head motion features per-frame,  denoted by  $\\phi(\\text{face})$. We compute ${\\mathbf{{z}}}_t$ by aggregating  $\\phi(\\text{face})$ for every 20 frames, i.e., ${\\mathbf{{z}}}_t = \\sum_{i=1}^{20}\\phi(\\text{face}_i)/\\|\\sum_{i=1}^{20}\\phi(\\text{face}_i)\\|$. \n\n\\noindent \\textit{Face detection and tracking.} We detect\nthe driver's face using a trained Viola-Jones face detector~\\citep{Viola04}. From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner\ndetector \\citep{Shi94} and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker \\citep{Lucas81,Shi94,Tomasi91}. However, the tracking may accumulate errors over time because of changes in illumination due to the shadows of trees,  traffic, etc. We therefore constrain the tracked facial points to follow a projective transformation and remove the incorrectly tracked points using the RANSAC algorithm. While tracking the facial points, we  lose some of the tracked  points with every new frame. To address this problem, we  re-initialize the tracker with new discriminative facial points once the number of tracked  points falls below a threshold \\citep{Kalal10}. \n\\vspace{0.05in}\n\n\\noindent \\textit{Head motion features.} \nFor maneuver anticipation the horizontal movement of the face and its angular rotation (\\textit{yaw}) are particularly important.  From the face tracking we obtain \\textit{face tracks}, which are 2D trajectories of the tracked facial points in the image plane. Figure~\\ref{fig:face_feature} (bottom) shows how the horizontal coordinates of the tracked facial points vary with time before a left turn maneuver. We represent the driver's face movements and rotations with histogram features. In particular, we take matching facial points between  successive frames and create histograms of their corresponding horizontal motions (in pixels) and angular motions in the image plane (Figure~\\ref{fig:face_feature}). We bin the horizontal and angular motions using $[\\leq-2,\\;-2\\;\\text{to}\\;0,\\;0\\;\\text{to}\\;2,\\;\\geq2]$ and  $[0\\;\\text{to}\\;\\frac{\\pi}{2},\\;\\frac{\\pi}{2}\\;\\text{to}\\;\\pi,\\;\\pi\\;\\text{to}\\;\\frac{3\\pi}{2},\\;\\frac{3\\pi}{2}\\;\\text{to}\\;2\\pi]$,\nrespectively. We also calculate the mean movement of the driver's face center. This gives us $\\phi(\\text{face})\\in\\mathbb{R}^9$ facial features per-frame. The driver's eye-gaze is also useful a feature. However, robustly estimating 3D eye-gaze in outside environment is still a topic of research, and orthogonal to this work on anticipation. We therefore do not consider eye-gaze features.  \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.9\\linewidth]{feat_compare_3.pdf}\n\\caption{\\textbf{Improved features for maneuver anticipation.} We track facial landmark points using the CLNF tracker~\\citep{Baltrusaitis13} which results in more consistent 2D trajectories as compared to the KLT tracker~\\citep{Shi94} used by Jain et al.~\\citep{Jain15}. Furthermore, the CLNF also gives an estimate of the driver's 3D head pose.}\n\\label{fig:feature_compare}\n\\end{figure}\n\n\\noindent \\textit{3D head pose and facial landmark features.}\nOur framework is flexible and allows incorporating more advanced face detection and tracking algorithms. For example we replace the KLT tracker described above with  the Constrained Local Neural Field (CLNF) model~\\citep{Baltrusaitis13} and track 68 fixed  landmark points on the driver's face. CLNF is particularly well suited for driving scenarios due its ability to handle a wide range of head pose and illumination variations. As shown in Figure~\\ref{fig:feature_compare}, CLNF offers us two distinct benefits over the features from KLT (i) while discriminative facial points may change from situation to situation, tracking fixed landmarks results in consistent optical flow trajectories which adds to robustness; and (ii) CLNF also allows us to estimate the 3D head pose of the driver's face by minimizing  error in the projection of a generic 3D mesh model of the face w.r.t. the 2D location of landmarks in the image. The histogram features generated from the optical flow trajectories along with the 3D head pose features (yaw, pitch and row), give us $\\phi(\\text{face})\\in\\mathbb{R}^{12}$ when using the CLNF tracker. \n\nIn Section~\\ref{sec:experiments} we present results with the features from KLT, as well as the results with richer features obtained from the CLNF model.\n\n\n\\subsection{Outside-vehicle features.}\\label{subsec:outside_features} \nThe outside feature vector ${\\mathbf{{x}}}_t$ encodes the information about the outside environment such as the road conditions, vehicle dynamics, etc. In order to get this information, we use the road-facing camera together with the vehicle's GPS coordinates, its speed, and the street maps. More specifically, we obtain two binary features from the road-facing camera   indicating whether a lane exists on the left side and on the right side of the  vehicle. We also augment the vehicle's GPS coordinates with the street maps and extract a binary feature indicating if the vehicle is within 15 meters of a road artifact  such as intersections, turns, highway exists, etc. We also encode the average,  maximum, and  minimum speeds of the vehicle over the last 5 seconds as features. This results in a ${\\mathbf{{x}}}_t \\in\\mathbb{R}^6$ dimensional feature vector. \n\n\n\n\n\n\n\n\\section{Bayesian networks for\\\\ maneuver anticipation}\n\\label{sec:approach}\n\nIn this section we propose alternate Bayesian networks~\\citep{Jain15} based on Hidden Markov Model (HMM) for maneuver anticipation. These models form a strong baseline to compare our sensory-fusion deep learning architecture. \n\nDriving maneuvers are influenced by multiple interactions involving the vehicle, its driver, outside traffic, and occasionally global factors like the driver's destination. These interactions influence the driver's intention, i.e. their state of mind before the maneuver, which is not directly observable.  In our Bayesian network formulation, we represent the driver's intention with discrete states that are  \\textit{latent} (or hidden). In order to anticipate maneuvers, we  jointly model the driving context and the  \\textit{latent} states in a tractable manner. We represent the driving context as a set of features described in Section~\\ref{sec:features}. We now present the motivation for the Bayesian networks and then discuss our key model Autoregressive Input-Output HMM (AIO-HMM). \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.9\\linewidth]{model_aiohmm.pdf}\n\\caption{\\textbf{AIO-HMM.} The model has three layers: (i) Input (top): this  layer represents outside vehicle features ${\\mathbf{{x}}}$; (ii) Hidden (middle): this  layer represents driver's latent states ${h}$; and (iii) Output (bottom): this  layer represents inside vehicle features ${\\mathbf{{z}}}$. This layer also captures temporal dependencies of inside vehicle features. $T$ represents time.}\n\\label{fig:model}\n\\end{figure}\n\n\n\\subsection{Modeling driving maneuvers}\nModeling maneuvers require temporal modeling of the driving context. Discriminative methods, such as the Support Vector Machine and the Relevance Vector Machine~\\citep{Tipping01}, which do not model the temporal aspect perform poorly on anticipation tasks, as we show in Section~\\ref{sec:experiments}. Therefore, a temporal model such as the Hidden Markov Model (HMM) is better suited to model maneuver anticipation. \n\nAn HMM models how the driver's \\textit{latent} states generate both the inside driving context (${\\mathbf{{z}}}_t$) and the outside driving context (${\\mathbf{{x}}}_t$). However, a more accurate model should capture how events \\textit{outside} the vehicle (i.e. the outside driving context) affect the driver's state of mind, which then generates the observations \\textit{inside} the vehicle (i.e. the inside driving context). \n\nSuch interactions can be modeled by an Input-Output HMM (IOHMM)~\\citep{Bengio95}. However, modeling the problem with IOHMM does not capture the temporal dependencies of the inside driving context. These dependencies are critical to capture the smooth and temporally correlated behaviours such as the driver's face movements. We therefore present Autoregressive Input-Output HMM (AIO-HMM) which extends IOHMM to model these observation dependencies. Figure~\\ref{fig:model} shows the AIO-HMM graphical model for modeling maneuvers. We learn separate AIO-HMM model for each maneuver. In order to anticipate maneuvers, during inference we determine which model best explains the past several seconds of the driving context based on the data log-likelihood.  In Appendix~\\ref{sec:aiohmm} we describe the training and inference procedure for AIO-HMM. \n\n\n\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.9\\linewidth]{data.pdf}\n\\caption{\\textbf{Our data set} is diverse in drivers and landscape.}\n\\label{fig:diverse_data}\n\\end{figure}\n\nIn this section we first give an overview of our data set and then present the quantitative results. We also demonstrate our system and algorithm on real-world driving scenarios. \\textbf{Our video demonstrations are available at}: \\hbox{\\url{http://www.brain4cars.com}. }\n\n\\subsection{Driving data set}\n\\label{subsec:dataset}\nOur data set consists of natural driving videos with both inside and outside views of the car, its speed, and the global position system (GPS) coordinates.\\footnote{The inside and outside cameras operate at 25 and 30 frames/sec.} The outside car video captures the view of the road ahead. We collected this driving data set under fully natural settings without any intervention.\\footnote{\\textbf{Protocol:} We set up cameras, GPS and speed recording device in subject's personal vehicles and left it to record the data. The subjects were asked to ignore our setup and drive as they would normally.} \nIt consists of 1180 miles of freeway and city driving and encloses 21,000 square miles across two states. We collected this data set from 10 drivers over a period of two months. The complete data set has a total of 2 million video frames and includes diverse landscapes. Figure~\\ref{fig:diverse_data} shows a few samples from our data set. We annotated the driving videos with a total of 700 events containing 274 lane changes, 131 turns, and 295 randomly sampled instances of driving straight. Each lane change or turn annotation marks the start time of the maneuver, i.e., before the car touches the lane or yaws, respectively. For all annotated events, we also annotated the lane information, i.e., the number of lanes on the road and the current lane of the car. Our data set is publicly available at \\hbox{\\url{http://www.brain4cars.com}. }\n\n\\subsection{Baseline algorithms}\nWe compare the following algorithms:\n\n\\begin{itemize}\n \\setlength{\\itemsep}{3pt}\n\n  \\setlength{\\parsep}{0pt}\n\\item \\textit{Chance}: Uniformly randomly anticipates a maneuver.\n\\item \\textit{SVM}~\\citep{Morris11}: Support Vector Machine is a discriminative classifier~\\citep{Cortes95}. Morris et al.~\\citep{Morris11} takes this approach  for anticipating maneuvers.\\footnote{Morries et al.~\\citep{Morris11} considered binary \nclassification problem (lane change vs driving straight) and used RVM~\\citep{Tipping01}.}\n\n\nWe train the SVM on 5 seconds of driving context by concatenating all frame features \nto get a $\\mathbb{R}^{3840}$ dimensional feature vector. \n\\item \\textit{Random-Forest}~\\citep{Criminisi11}: This is also a discriminative classifier that\nlearns many decision trees from the training data, and at test time it\naverages the prediction of the individual decision trees. We train\nit on the same features as SVM with 150 trees of depth ten each.\n\\item \\textit{HMM}: This is the Hidden Markov Model. \nWe train the HMM on a temporal sequence of feature vectors that we extract every 0.8 seconds, \ni.e., every 20 video frames.  \nWe consider three versions of the HMM: (i) HMM $E$: with only outside features from the road camera, the vehicle's speed, GPS and street maps (Section~\\ref{subsec:outside_features});\n(ii) HMM $F$: with only inside features from the driver's face (Section~\\ref{subsec:inside_features}); \nand (ii) HMM $E+F$: with both inside and outside features. \n\\item \\textit{IOHMM}: Jain et al.~\\cite{Jain15} modeled driving maneuvers with this Bayesian network. It is trained on the same features as HMM $E + F$.\n\\item \\textit{AIO-HMM}: Jain et al.~\\citep{Jain15} proposed this Bayesian network for modeling maneuvers. It is trained on the same features as HMM $E + F$. \n\\item \\textit{Simple-RNN} (S-RNN): In this architecture sensor streams are fused by simple concatenation and then passed through a single RNN with LSTM units. \n\\item \\textit{Fusion-RNN-Uniform-Loss} (F-RNN-UL): In this architecture sensor streams are passed through separate RNNs, and the high-level representations from RNNs are then fused via a fully-connected layer. The loss at each time step takes the form $-\\log(y_t^k)$. \n\\item \\textit{Fusion-RNN-Exp-Loss} (F-RNN-EL): This architecture is similar to F-RNN-UL, except that the loss exponentially grows with time $-e^{-(T-t)}\\log(y_t^k)$. \n\\end{itemize}\n\nOur RNN and LSTM implementations are open-sourced and available at \\texttt{NeuralModels}~\\citep{Neuralmodels}. For the RNNs in our Fusion-RNN architecture we use a single layer LSTM of size 64 with sigmoid gate activations and tanh activation for hidden representation. Our fully connected fusion layer uses tanh activation and outputs a 64 dimensional vector. Our overall architecture (F-RNN-EL and F-RNN-UL) have nearly 25,000 parameters that are learned using RMSprop~\\citep{Dauphin15}. \n\n\\begin{algorithm}[t]\\caption{Maneuver anticipation}\n\\begin{algorithmic}\n\\STATE \\textbf{Initialize} $m^* = \\textit{driving straight}$\\\\\n\n\\STATE \\textbf{Input} Features $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T),({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$ and prediction threshold $p_{th}$\n\\STATE \\textbf{Output} Predicted maneuver $m^*$\n\\WHILE{$t=1$ to $T$}\n        \\STATE Observe features $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_t)$ and $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_t)$\n        \\STATE Estimate probability $ {\\mathbf{{y}}}_t$ of each maneuver in $\\mathcal{M}$\n        \\STATE $m_t^*={\\operatorname{arg\\,max}}_{m\\in\\mathcal{M}}{\\mathbf{{y}}}_t$\n\t\t\\IF{$m_t^* \\neq \\textit{driving straight}$ \\& ${\\mathbf{{y}}}_t\\{m_t^*\\} > p_{th} $}        \n\t\t\t\\STATE $m^* = m_t^*$\n\t\t\t\n\t\t\t\\STATE \\textbf{break}\n\t\t\\ENDIF\n        \n\n\\ENDWHILE\n\\STATE \\textbf{Return} $m^*$\n\\end{algorithmic}\n\\label{alg:inference}\n\\end{algorithm}\n\n\\begin{table*}[t!]\n\\centering\n\\caption{{\\textbf{Maneuver Anticipation Results.} Average \\textit{precision}, \\textit{recall} and \\textit{time-to-maneuver} are computed from 5-fold cross-validation. Standard error is also shown. Algorithms are compared on the features from Jain et al.~\\citep{Jain15}.}}\n\\resizebox{1\\textwidth}{!}{\n\\centering\n\\begin{tabular}{cr|ccc|ccc|ccc}\n\n&  &\\multicolumn{3}{c}{Lane change}&\\multicolumn{3}{|c}{Turns}&\\multicolumn{3}{|c}{All maneuvers}\\\\\n\\cline{1-11}\n\\multicolumn{2}{c|}{\\multirow{2}{*}{Method}} & \\multirow{2}{*}{$Pr$ (\\%)}  & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  & \\multirow{2}{*}{$Pr$ (\\%)} & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  & \\multirow{2}{*}{$Pr$ (\\%)} & \\multirow{2}{*}{$Re$ (\\%)}  & Time-to- \\\\ \n& & & &  maneuver (s) &  & &  maneuver (s) &  & & maneuver (s)\\\\\\hline\n&Chance\t&\t33.3\t\t&\t33.3\t\t&\t-\t&\t33.3\t\t&\t33.3\t\t&\t-\t&\t20.0\t\t&\t20.0\t\t&\t-\\\\\n&Morris et al.~\\citep{Morris11} SVM\t&\t73.7 $\\pm$ 3.4\t&\t57.8 $\\pm$ 2.8\t&\t2.40 \t\t&\t64.7 $\\pm$ 6.5\t&\t47.2 $\\pm$ 7.6\t&\t2.40 \t\t&\t43.7 $\\pm$ 2.4\t&\t37.7 $\\pm$ 1.8\t& 1.20\\\\\n&Random-Forest &   71.2 $\\pm$ 2.4  &   53.4 $\\pm$ 3.2  &   3.00    &   68.6 $\\pm$ 3.5  & 44.4 $\\pm$ 3.5  &   1.20      &   51.9 $\\pm$ 1.6 &   27.7 $\\pm$ 1.1  & 1.20\\\\\n\n\n\n\n&HMM $E$ & 75.0 $\\pm$ 2.2 & 60.4 $\\pm$ 5.7 & 3.46  &  74.4 $\\pm$ 0.5 & 66.6 $\\pm$ 3.0 & 4.04  & 63.9 $\\pm$ 2.6 & 60.2 $\\pm$ 4.2 & 3.26 \\\\\n&HMM $F$\t\t&\t76.4 $\\pm$ 1.4\t&\t75.2 $\\pm$ 1.6\t&\t3.62\t\t&\t75.6 $\\pm$ 2.7\t&\t\t60.1 $\\pm$ 1.7\t&\t3.58 \t\t&\t64.2 $\\pm$ 1.5\t&\t36.8 $\\pm$ 1.3\t&\t2.61\\\\\n&HMM $E+F$\t&\t80.9 $\\pm$ 0.9\t&\t79.6 $\\pm$ 1.3\t&\t3.61 \t\t&\t73.5 $\\pm$ 2.2\t&\n75.3 $\\pm$ 3.1\t&\t4.53 \t\t&\t67.8 $\\pm$ 2.0\t&\t67.7 $\\pm$ 2.5\t&\t3.72 \\\\\\hline\n&IOHMM\t\t&\t81.6 $\\pm$ 1.0\t&\t{79.6 $\\pm$ 1.9}\t&\t3.98  \t&\t77.6 $\\pm$ 3.3\t&\t\t{75.9 $\\pm$ 2.5}\t&\t4.42 \t\t&\t74.2 $\\pm$ 1.7\t&\t71.2 $\\pm$ 1.6\t&\t3.83 \\\\\n&(\\textit{Our final Bayesian network}) AIO-HMM\t\t&\t{83.8 $\\pm$ 1.3}\t&\t79.2 $\\pm$ 2.9\t&\t3.80  \t\t&\t{80.8\t $\\pm$ 3.4}\t&\t\t75.2 $\\pm$ 2.4\t&\t4.16 \t\t&\t{77.4 $\\pm$ 2.3}\t&\t{71.2 $\\pm$ 1.3}\t&\t3.53 \\\\\n& S-RNN & 85.4 $\\pm$ 0.7 & 86.0 $\\pm$ 1.4 & 3.53 & 75.2 $\\pm$ 1.4 & 75.3 $\\pm$ 2.1 & 3.68 & 78.0 $\\pm$ 1.5 & 71.1 $\\pm$ 1.0 & 3.15 \\\\\n\n&F-RNN-UL & \\textbf{92.7} $\\pm$ 2.1 & 84.4 $\\pm$ 2.8 & 3.46 & 81.2 $\\pm$ 3.5 & 78.6 $\\pm$ 2.8 & 3.94 & 82.2 $\\pm$ 1.0 & 75.9 $\\pm$ 1.5 & 3.75 \\\\\n&(\\textit{Our final deep architecture})  F-RNN-EL & 88.2 $\\pm$ 1.4 & \\textbf{86.0} $\\pm$ 0.7 & 3.42 & \\textbf{83.8} $\\pm$ 2.1 & \\textbf{79.9} $\\pm$ 3.5 & 3.78 & \\textbf{84.5} $\\pm$ 1.0 & \\textbf{77.1} $\\pm$ 1.3 & 3.58\\\\\n\n\\end{tabular}\n}\n\\label{tab:prscore}\n\\end{table*}\n\n\\subsection{Evaluation protocol}\n\nWe evaluate an algorithm based on its correctness in predicting future maneuvers. We anticipate maneuvers every 0.8 seconds where the algorithm processes the recent context and assigns a probability to each of the four maneuvers: \\{\\textit{left lane change, right lane change, left turn, right turn}\\} and a probability to the event of \\textit{driving straight}. These five probabilities together sum to one.\nAfter anticipation, i.e. when the algorithm has computed all five probabilities, the algorithm predicts a  maneuver if its probability is above a threshold $p_{th}$. If none of the maneuvers' probabilities are above this threshold, the algorithm does not make a maneuver prediction and predicts \\textit{driving  straight}. However, when it predicts one of the four maneuvers, it sticks with this prediction and makes no further predictions for next 5 seconds or until a maneuver occurs, whichever happens earlier. After 5 seconds or a maneuver has occurred, it returns to anticipating  future maneuvers. Algorithm~\\ref{alg:inference} shows the inference steps for maneuver anticipation.\n\nDuring this process of anticipation and prediction, the algorithm makes (i) true predictions ($tp$): when it predicts the correct maneuver; (ii) false predictions ($fp$): when it predicts a maneuver but the driver performs a different  maneuver; (iii) false positive predictions ($fpp$): when it predicts a maneuver but the driver does not perform any maneuver (i.e. \\textit{driving straight}); and (iv) missed predictions ($mp$): when it predicts \\textit{driving straight} but the driver performs a maneuver. We evaluate the algorithms using their precision and recall scores:\n\n", "itemtype": "equation", "pos": 39361, "prevtext": "\t \nwhere ${\\mathbf{{W}}}_*$ and ${\\mathbf{{b}}}_*$ are model parameters, and $\\text{LSTM}_x$ and $\\text{LSTM}_z$ process the sensory streams $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T)$ and $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)$ respectively. {The same framework can be extended to handle more sensory streams.} \n\n\\subsection{Exponential loss-layer for anticipation.} \nWe propose a new loss layer which encourages the architecture to anticipate early while also ensuring that the architecture does not over-fit the training data early enough in time when there is not enough context for anticipation. \nWhen using the standard softmax loss, the architecture suffers a loss of $-\\log(y_t^k)$ for the mistakes it makes at each time step, where $y_t^k$ is the probability of the ground truth event $k$ computed by the architecture using Eq.~\\eqref{eq:f-output}. We propose to modify this loss by multiplying it with an exponential term as illustrated in Figure~\\ref{fig:fusion}. Under this new scheme, the loss exponentially grows with time as shown below. \n\n", "index": 9, "text": "\\begin{align}\n\\label{eq:loss}\nloss = \\sum_{j=1}^N \\sum_{t=1}^T  - e^{-(T-t)}\\log (y_t^k)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle loss=\\sum_{j=1}^{N}\\sum_{t=1}^{T}-e^{-(T-t)}\\log(y_{t}^{k})\" display=\"inline\"><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle></mrow><mo>-</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>-</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mi>t</mi><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nThe precision measures the fraction of the predicted maneuvers that are correct and recall measures the \nfraction of the maneuvers that are correctly predicted. For true predictions ($tp$) we also compute the \naverage \\textit{time-to-maneuver}, where time-to-maneuver is  the interval between the  time of algorithm's prediction and the start of the maneuver.\n\nWe perform cross validation to choose the number of the driver's latent states in the AIO-HMM and the threshold on  probabilities for maneuver prediction. For \\textit{SVM} we cross-validate for the parameter $C$ and the choice of kernel from Gaussian and polynomial kernels. The parameters are chosen as the ones giving the highest F1-score on a validation set. \nThe F1-score is the harmonic mean of the precision and recall, defined as $F1 = 2*Pr*Re/(Pr+Re)$.\n\n\n\n\\subsection{Quantitative results}\n\nWe evaluate the algorithms on maneuvers that were not seen during training  and report the results using 5-fold cross validation. Table \\ref{tab:prscore} reports the precision and recall scores \nunder three settings: (i) \\textit{Lane change}: when the algorithms only predict for the left and right lane changes. This setting is relevant for highway driving where the prior probabilities of turns are low; (ii) \\textit{Turns}: when the algorithms only predict for the left and right turns; and (iii) \\textit{All maneuvers}: here the algorithms jointly predict all four maneuvers. All three settings include the instances of \\textit{driving straight}. \n\nTable~\\ref{tab:prscore} compares the performance of the baseline anticipation algorithms, Bayesian networks, and the variants of our deep learning model. All algorithms in Table~\\ref{tab:prscore} use same feature vectors and KLT face tracker which ensures a fair comparison. As shown in the table, overall the best algorithm for maneuver anticipation is F-RNN-EL, and the best performing Bayesian network is AIO-HMM. F-RNN-EL significantly outperforms AIO-HMM in every setting. This improvement in performance is because RNNs with LSTM units are very expressive models with an internal memory. This allows them to model the much needed long temporal dependencies for anticipation. Additionally, unlike AIO-HMM,  F-RNN-EL is a discriminative model that does not make any assumptions about the generative nature of the problem.  The results also highlight the importance of modeling the temporal nature in the data. Classifiers like SVM and Random Forest do not model the temporal aspects and hence performs poorly. \n\nThe performance of several variants of our deep architecture, reported in Table~\\ref{tab:prscore},  justifies our design decisions to reach the final fusion architecture. When predicting all maneuvers, F-RNN-EL gives 6\\%  higher precision and recall than S-RNN, which performs a simple fusion by concatenating the two sensor streams. On the other hand, F-RNN models each sensor stream with a separate RNN and then uses a fully connected layer to fuse the high-level representations at each time step. This form of sensory fusion is  more principled since the sensor streams represent different data modalities. \nIn addition, exponentially growing the loss further improves the performance. Our new loss scheme  penalizes the network proportional to the length of context it has seen.  When predicting all maneuvers, we observe that F-RNN-EL shows an improvement of 2\\% in precision and recall over F-RNN-UL. We conjecture that exponentially growing the loss acts like a regularizer. It reduces the risk of our network over-fitting early in time when there is not enough context available. Furthermore, the time-to-maneuver remains comparable for F-RNN  with and without exponential loss. \n\nThe Bayesian networks AIO-HMM and HMM $E+F$ adopt different sensory fusion strategies. AIO-HMM fuses the two sensory streams using an input-output model, on the other hand HMM $E+F$ performs early fusion by concatenation. As a result, AIO-HMM gives 10\\% higher precision than HMM $E+F$ for jointly predicting all the maneuvers. AIO-HMM further extends IOHMM by modeling the temporal dependencies of events inside the vehicle. This results in better performance: on average AIO-HMM precision is 3\\% higher than IOHMM, as shown in Table~\\ref{tab:prscore}. Another important aspect of anticipation is the joint modeling of the inside and outside driving contexts. HMM $F$ learns only from the inside driving context, while HMM $E$ learns only from the outside driving context. The performances of both the models is therefore less than HMM $E+F$, which learns jointly both the contexts. \n\nTable~\\ref{tab:fpp} compares the $fpp$ of different algorithms.  False positive predictions ($fpp$) happen when an algorithm predicts a maneuver but the driver does not perform any maneuver (i.e. drives straight). Therefore low value of $fpp$ is preferred. HMM $F$ performs best on this metric at 11\\%\nas it mostly assigns a high probability to \\textit{driving straight}. \nHowever, due to this reason, it incorrectly predicts \\textit{driving straight} even when maneuvers happen. This results in the low recall of \\hbox{HMM $F$} at 36\\%, as shown in Table~\\ref{tab:prscore}. AIO-HMM's \n$fpp$ is 10\\% less than that of IOHMM and HMM $E+F$, and F-RNN-EL is 3\\% less than AIO-HMM. The primary reason for false positive predictions is distracted driving. Drivers interactions with fellow passengers or their looking at the surrounding scenes are sometimes wrongly interpreted by the algorithms. Understanding driver distraction is still an open problem, and orthogonal to the objective of this work. \n\n\\begin{table}[t]\n\\centering\n\\caption{\\textbf{False positive prediction} ($fpp$) of different algorithms. The number inside parenthesis is the standard error.}\n\\resizebox{.85\\linewidth}{!}{\n\\begin{tabular}{r|ccc}\nAlgorithm\t&\tLane change\t&\tTurns\t&\tAll\\\\\\hline\nMorris et al.~\\citep{Morris11} SVM\t&\t15.3 (0.8) &\t\t13.3 (5.6)\t&\t24.0 (3.5)\\\\\nRandom-Forest & 16.2 (3.3) & 12.9 (3.7) & 17.5 (4.0) \\\\\nHMM $E$ & 36.2 (6.6) & 33.3 (0.0) & 63.8 (9.4) \\\\\nHMM $F$\t&\t23.1 (2.1)\t&\t23.3 (3.1)\t&\t11.5 (0.1)\\\\\nHMM $E+F$\t&\t30.0 (4.8)\t&\t21.2 (3.3)\t&\t40.7\t (4.9)\\\\\nIOHMM\t&\t28.4 (1.5)\t&\t25.0 (0.1)\t&\t40.0 (1.5)\\\\\nAIO-HMM\t&\t24.6 (1.5)\t&\t20.0 (2.0)\t&\t30.7 (3.4)\\\\\nS-RNN & 16.2 (1.3) & 16.7 (0.0) & 19.2 (0.0)\\\\\nF-RNN-UL & 19.2 (2.4) & 25.0 (2.4) & 21.5 (2.1) \\\\\nF-RNN-EL & 10.8 (0.7) & 23.3 (1.5) & 27.7 (3.8)\n\\end{tabular}\n}\n\\label{tab:fpp}\n\\end{table}\n\n\n\\begin{table}[h]\n\\centering\n\\caption{\\textbf{3D head-pose features.} In this table we study the effect of better features with best performing algorithm from Table~\\ref{tab:prscore} in `All maneuvers' setting. We use~\\citep{Baltrusaitis13} to track 68 facial landmark points and estimate 3D head-pose.}\n{\n\\newcolumntype{P}[2]{>{\\footnotesize#1\\hspace{0pt}\\arraybackslash}p{#2}}\n\\setlength{\\tabcolsep}{2pt}\n\\centering\n\\resizebox{\\hsize}{!}{\n\\begin{tabular}\n{@{}p{0.40\\linewidth}| P{\\centering}{16mm}P{\\centering}{16mm}P{\\centering}{16mm}@{}}\n\n\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{$Pr$ (\\%)}  & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  \\\\ \n & & &  maneuver (s)\\\\\\hline\nF-RNN-EL &  {84.5} $\\pm$ 1.0 & {77.1} $\\pm$ 1.3 & 3.58\\\\\n\nF-RNN-EL w/ 3D head-pose &  \\textbf{90.5} $\\pm$ 1.0 & \\textbf{87.4} $\\pm$ 0.5 & 3.16\\\\\n\\end{tabular}\n}}\n\\label{tab:features}\n\\end{table}\n\n\\textbf{3D head-pose features.} The modularity of our approach allows experimenting with more advanced head tracking algorithms. We replace the pipeline for extracting features from the driver's face~\\citep{Jain15} by a Constrained Local Neural Field (CLNF) model~\\citep{Baltrusaitis13}. The new vision pipeline tracks 68 facial landmark points and estimates the driver's 3D head pose as described in Section~\\ref{sec:features}. As shown in Table~\\ref{tab:features}, we see a significant, 6\\% increase in precision and 10\\% increase in recall of F-RNN-EL when using features from our new vision pipeline. This increase in performance is attributed to the following reasons: (i) robustness of CLNF model to variations in illumination and head pose; (ii) 3D head-pose features are very informative for understanding the driver's intention; and (iii) optical flow trajectories generated by tracking facial landmark points represent head movements better, as shown in Figure~\\ref{fig:feature_compare}. The confusion matrix in Figure~\\ref{fig:confmat} shows the precision for each maneuver. F-RNN-EL gives a higher precision than AIO-HMM on every maneuver when both algorithms are trained on same features (Fig.~\\ref{fig:confmat}c). The new vision pipeline with CLNF tracker further improves the precision of F-RNN-EL on all maneuvers (Fig.~\\ref{fig:confmat}d). \n\n\\textbf{Effect of prediction threshold.} In Figure~\\ref{fig:f1score} we study how F1-score varies as we change the prediction threshold $p_{th}$.  We make the following observations: (i) The F1-score does not undergo large variations with changes to the prediction threshold. Hence, it allows practitioners  to fairly trade-off between the precision and recall without hurting the F1-score by much; and (ii)   the maximum F1-score attained by F-RNN-EL is 4\\% more than AIO-HMM when compared on the same features and 13\\% more with our new vision pipeline.\nIn {Tables~\\ref{tab:prscore},~\\ref{tab:fpp} and \\ref{tab:features}}, we used the threshold values which gave the highest F1-score.\n\n\\noindent\\textbf{Anticipation complexity.} The F-RNN-EL anticipates maneuvers every 0.8 seconds using the previous 5 seconds of the driving context. The complexity mainly comprises of feature extraction and the model inference in Algorithm~\\ref{alg:inference}. Fortunately both these steps can be performed as a dynamic program by storing the computation of the most recent anticipation. Therefore, for every anticipation we only process the incoming 0.8 seconds and not complete 5 seconds of the driving context. On average we predict a maneuver under 0.20~milliseconds using Theano~\\citep{Bastien12} on Nvidia K40 GPU on Ubuntu~12.04. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{thresh_f1.pdf}\n\\caption{\\textbf{Effect of prediction threshold $p_{th}$.} At test time an algorithm makes a prediction only when it is at least $p_{th}$ confident in its prediction. This plot shows how F1-score vary with change in prediction threshold.}\n\\label{fig:f1score}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{IOHMM_I_O_precision.png}\n\t\\caption{IOHMM}\n\t\\end{subfigure}\n\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{AIOHMM_I_O_precision.png}\n\t\\caption{AIO-HMM}\n\t\\end{subfigure}\t\n\t\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{fusionrnn_old_features.png}\n\t\\caption{F-RNN-EL}\n\t\\end{subfigure}\t\n\t\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{fusionrnn_new_features.png}\n\t\\caption{F-RNN-EL w/ 3D-pose}\n\t\\end{subfigure}\n\t\\caption{\\textbf{Confusion matrix} of different algorithms when jointly predicting all the maneuvers. Predictions made by algorithms are represented by rows and actual maneuvers are represented by columns. Numbers on the diagonal represent precision.}\n\t\\label{fig:confmat}\t\n\\end{figure*}\n\n\n\n\\section{Conclusion}\n\nIn this paper we considered the problem of anticipating driving maneuvers a few seconds before the driver performs them.  This problem requires the modeling of long temporal dependencies and the fusion of multiple sensory streams. We proposed a novel deep learning architecture based on Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units for anticipation. Our architecture learns to fuse multiple sensory streams, and by training it in a sequence-to-sequence prediction manner, it explicitly learns to anticipate using only a partial temporal context. We also proposed a novel loss layer for anticipation which prevents over-fitting. \n\nWe release an open-source data set of 1180 miles of natural driving. We performed an extensive evaluation and showed improvement over many baseline algorithms. Our sensory fusion deep learning approach gives a precision of 84.5\\% and recall of 77.1\\%, and anticipates maneuvers \\hbox{3.5 seconds} (on average) before they happen. By incorporating the driver's 3D head-pose our precision and recall improves to 90.5\\% and 87.4\\% respectively. Potential application of our work is enabling advanced driver assistance systems (ADAS) to alert drivers before they perform a dangerous maneuver, thereby giving drivers more time to react. We believe that our deep learning architecture is widely applicable to many activity anticipation problems. Our code and data set are publicly available on the project web-page. \n\\\\~\\\\~\n\\textbf{Acknowledgement.} We thank NVIDIA for the donation of K40 GPUs used in\nthis research. We also thank Silvio Savarese for useful discussions. This work\nwas supported by National Robotics Initiative (NRI) award 1426452, Office of Naval\nResearch (ONR) award N00014-14-1-0156, and by Microsoft Faculty Fellowship and NSF Career\nAward to Saxena.\n\n\\appendices\n\n\\section{Modeling Maneuvers with AIO-HMM}\n\\label{sec:aiohmm}\n\nGiven $T$ seconds long driving context $\\mathcal{C}$  before the maneuver $\\mathcal{M}$, we learn a generative model for the context $P(\\mathcal{C}|\\mathcal{M})$. The driving context $\\mathcal{C}$ consists of the outside driving context and the inside driving context. The outside and inside contexts are temporal sequences represented by the outside features ${\\mathbf{{x}}}_1^T = \\{{\\mathbf{{x}}}_1,..,{\\mathbf{{x}}}_T\\}$ and the inside features ${\\mathbf{{z}}}_1^T = \\{{\\mathbf{{z}}}_1,..,{\\mathbf{{z}}}_T\\}$ respectively. The corresponding sequence of the driver's latent states is ${h}_1^T = \\{{h}_1,..,{h}_T\\}$. ${\\mathbf{{x}}}$ and ${\\mathbf{{z}}}$ are vectors and ${h}$ is a discrete state.\n\n", "itemtype": "equation", "pos": 62358, "prevtext": "\nThis loss penalizes the RNN exponentially more for the mistakes it makes as it sees more {observations}. This encourages the model to fix mistakes as early as it can in time. The loss in equation~\\ref{eq:loss} also penalizes the network less on mistakes made early in time when there is not enough context available. This way it acts like a regularizer and reduces the risk to over-fit very early in time. \n\n\\iffalse\n\\subsection{Model training and data augmentation}\n\\label{subsec:augmentation}\n\nOur architecture for maneuver anticipation has more than 25,000 parameters that need to be learned (Section~\\ref{sec:maneuver}). With such a large number of parameters on a non-convex manifold, over-fitting becomes a major challenge. We therefore introduce redundancy in the training data which acts like a regularizer and reduces  over-fitting~\\citep{Krizhevsky12,Hannun14}. \n In order to augment training data, we extract sub-sequences of temporal {observations}. Given a training example with two temporal sensor streams $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T),({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T),{\\mathbf{{y}}}\\}$, we uniformly randomly sample multiple sub-sequences $\\{({\\mathbf{{x}}}_i,...,{\\mathbf{{x}}}_j),({\\mathbf{{z}}}_i,...,{\\mathbf{{z}}}_j),{\\mathbf{{y}}} | 1 \\leq i < j \\leq T\\}$ as additional training examples. It is important to note that data augmentation only adds redundancy and \\textit{does not} rely on any external source of new information. \n\n\nOn the augmented data set, we train the network described in Section~\\ref{sec:sensor-fusion-rnn}. We use RMSprop gradients which have been shown to work well on training deep networks~\\citep{Dauphin15}, and we keep the step size fixed at $10^{-4}$. We experimented with different variants of softmax loss, and our proposed loss-layer with exponential growth Eq.~\\eqref{eq:loss} works best for anticipation (see Section~\\ref{experiments} for details).\n\\fi\n\n\n\n\n\\section{Features}\n\\label{sec:features}\n\n\n\\begin{figure}[t]\n\\centering\n\n\\includegraphics[width=.9\\linewidth]{feature.pdf}\n\\caption{\\textbf{Inside vehicle feature extraction.} The angular histogram features extracted at three different time steps for a left turn maneuver. \\textit{Bottom}: Trajectories for the horizontal motion of tracked facial pixels `t' seconds before the maneuver. At t=5 seconds before the maneuver the driver is looking straight, at t=3 looks (left) in the direction of maneuver, and at t=2 looks (right) in opposite direction for the crossing traffic. \\textit{Middle}: Average motion vector of tracked facial pixels in polar coordinates. $r$ is the average movement of pixels and arrow indicates the direction in which the face moves when looking from the camera. \\textit{Top}: Normalized angular histogram features. }\n\\label{fig:face_feature}\n\\end{figure}\n\n\n\n\n\n\n\nWe extract features by processing the inside and outside driving contexts. We do this by  grouping the overall contextual information from the sensors into: (i) the context from inside the vehicle, which comes from the driver facing camera and is represented as temporal sequence of features $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)$; and (ii) the context from outside the vehicle, which comes from the remaining sensors: GPS, road facing camera, and street maps. We represent the outside context with $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T)$. In order to anticipate maneuvers, our RNN architecture (Figure~\\ref{fig:fusion}) processes the temporal context $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_t),\\; ({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_t)\\}$  at every time step $t$, and outputs softmax probabilities ${\\mathbf{{y}}}_t$ for the following five maneuvers:  $\\mathcal{M}=$~\\{\\textit{left turn}, \\textit{right turn}, \\textit{left lane change}, \\textit{right lane change}, \\textit{straight driving}\\}.\n\n\n\\subsection{Inside-vehicle features.}\\label{subsec:inside_features} \nThe inside features ${\\mathbf{{z}}}_t$ capture the driver's head movements at each time instant $t$.  Our vision pipeline consists of face detection, tracking, and feature extraction modules. We extract head motion features per-frame,  denoted by  $\\phi(\\text{face})$. We compute ${\\mathbf{{z}}}_t$ by aggregating  $\\phi(\\text{face})$ for every 20 frames, i.e., ${\\mathbf{{z}}}_t = \\sum_{i=1}^{20}\\phi(\\text{face}_i)/\\|\\sum_{i=1}^{20}\\phi(\\text{face}_i)\\|$. \n\n\\noindent \\textit{Face detection and tracking.} We detect\nthe driver's face using a trained Viola-Jones face detector~\\citep{Viola04}. From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner\ndetector \\citep{Shi94} and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker \\citep{Lucas81,Shi94,Tomasi91}. However, the tracking may accumulate errors over time because of changes in illumination due to the shadows of trees,  traffic, etc. We therefore constrain the tracked facial points to follow a projective transformation and remove the incorrectly tracked points using the RANSAC algorithm. While tracking the facial points, we  lose some of the tracked  points with every new frame. To address this problem, we  re-initialize the tracker with new discriminative facial points once the number of tracked  points falls below a threshold \\citep{Kalal10}. \n\\vspace{0.05in}\n\n\\noindent \\textit{Head motion features.} \nFor maneuver anticipation the horizontal movement of the face and its angular rotation (\\textit{yaw}) are particularly important.  From the face tracking we obtain \\textit{face tracks}, which are 2D trajectories of the tracked facial points in the image plane. Figure~\\ref{fig:face_feature} (bottom) shows how the horizontal coordinates of the tracked facial points vary with time before a left turn maneuver. We represent the driver's face movements and rotations with histogram features. In particular, we take matching facial points between  successive frames and create histograms of their corresponding horizontal motions (in pixels) and angular motions in the image plane (Figure~\\ref{fig:face_feature}). We bin the horizontal and angular motions using $[\\leq-2,\\;-2\\;\\text{to}\\;0,\\;0\\;\\text{to}\\;2,\\;\\geq2]$ and  $[0\\;\\text{to}\\;\\frac{\\pi}{2},\\;\\frac{\\pi}{2}\\;\\text{to}\\;\\pi,\\;\\pi\\;\\text{to}\\;\\frac{3\\pi}{2},\\;\\frac{3\\pi}{2}\\;\\text{to}\\;2\\pi]$,\nrespectively. We also calculate the mean movement of the driver's face center. This gives us $\\phi(\\text{face})\\in\\mathbb{R}^9$ facial features per-frame. The driver's eye-gaze is also useful a feature. However, robustly estimating 3D eye-gaze in outside environment is still a topic of research, and orthogonal to this work on anticipation. We therefore do not consider eye-gaze features.  \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.9\\linewidth]{feat_compare_3.pdf}\n\\caption{\\textbf{Improved features for maneuver anticipation.} We track facial landmark points using the CLNF tracker~\\citep{Baltrusaitis13} which results in more consistent 2D trajectories as compared to the KLT tracker~\\citep{Shi94} used by Jain et al.~\\citep{Jain15}. Furthermore, the CLNF also gives an estimate of the driver's 3D head pose.}\n\\label{fig:feature_compare}\n\\end{figure}\n\n\\noindent \\textit{3D head pose and facial landmark features.}\nOur framework is flexible and allows incorporating more advanced face detection and tracking algorithms. For example we replace the KLT tracker described above with  the Constrained Local Neural Field (CLNF) model~\\citep{Baltrusaitis13} and track 68 fixed  landmark points on the driver's face. CLNF is particularly well suited for driving scenarios due its ability to handle a wide range of head pose and illumination variations. As shown in Figure~\\ref{fig:feature_compare}, CLNF offers us two distinct benefits over the features from KLT (i) while discriminative facial points may change from situation to situation, tracking fixed landmarks results in consistent optical flow trajectories which adds to robustness; and (ii) CLNF also allows us to estimate the 3D head pose of the driver's face by minimizing  error in the projection of a generic 3D mesh model of the face w.r.t. the 2D location of landmarks in the image. The histogram features generated from the optical flow trajectories along with the 3D head pose features (yaw, pitch and row), give us $\\phi(\\text{face})\\in\\mathbb{R}^{12}$ when using the CLNF tracker. \n\nIn Section~\\ref{sec:experiments} we present results with the features from KLT, as well as the results with richer features obtained from the CLNF model.\n\n\n\\subsection{Outside-vehicle features.}\\label{subsec:outside_features} \nThe outside feature vector ${\\mathbf{{x}}}_t$ encodes the information about the outside environment such as the road conditions, vehicle dynamics, etc. In order to get this information, we use the road-facing camera together with the vehicle's GPS coordinates, its speed, and the street maps. More specifically, we obtain two binary features from the road-facing camera   indicating whether a lane exists on the left side and on the right side of the  vehicle. We also augment the vehicle's GPS coordinates with the street maps and extract a binary feature indicating if the vehicle is within 15 meters of a road artifact  such as intersections, turns, highway exists, etc. We also encode the average,  maximum, and  minimum speeds of the vehicle over the last 5 seconds as features. This results in a ${\\mathbf{{x}}}_t \\in\\mathbb{R}^6$ dimensional feature vector. \n\n\n\n\n\n\n\n\\section{Bayesian networks for\\\\ maneuver anticipation}\n\\label{sec:approach}\n\nIn this section we propose alternate Bayesian networks~\\citep{Jain15} based on Hidden Markov Model (HMM) for maneuver anticipation. These models form a strong baseline to compare our sensory-fusion deep learning architecture. \n\nDriving maneuvers are influenced by multiple interactions involving the vehicle, its driver, outside traffic, and occasionally global factors like the driver's destination. These interactions influence the driver's intention, i.e. their state of mind before the maneuver, which is not directly observable.  In our Bayesian network formulation, we represent the driver's intention with discrete states that are  \\textit{latent} (or hidden). In order to anticipate maneuvers, we  jointly model the driving context and the  \\textit{latent} states in a tractable manner. We represent the driving context as a set of features described in Section~\\ref{sec:features}. We now present the motivation for the Bayesian networks and then discuss our key model Autoregressive Input-Output HMM (AIO-HMM). \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.9\\linewidth]{model_aiohmm.pdf}\n\\caption{\\textbf{AIO-HMM.} The model has three layers: (i) Input (top): this  layer represents outside vehicle features ${\\mathbf{{x}}}$; (ii) Hidden (middle): this  layer represents driver's latent states ${h}$; and (iii) Output (bottom): this  layer represents inside vehicle features ${\\mathbf{{z}}}$. This layer also captures temporal dependencies of inside vehicle features. $T$ represents time.}\n\\label{fig:model}\n\\end{figure}\n\n\n\\subsection{Modeling driving maneuvers}\nModeling maneuvers require temporal modeling of the driving context. Discriminative methods, such as the Support Vector Machine and the Relevance Vector Machine~\\citep{Tipping01}, which do not model the temporal aspect perform poorly on anticipation tasks, as we show in Section~\\ref{sec:experiments}. Therefore, a temporal model such as the Hidden Markov Model (HMM) is better suited to model maneuver anticipation. \n\nAn HMM models how the driver's \\textit{latent} states generate both the inside driving context (${\\mathbf{{z}}}_t$) and the outside driving context (${\\mathbf{{x}}}_t$). However, a more accurate model should capture how events \\textit{outside} the vehicle (i.e. the outside driving context) affect the driver's state of mind, which then generates the observations \\textit{inside} the vehicle (i.e. the inside driving context). \n\nSuch interactions can be modeled by an Input-Output HMM (IOHMM)~\\citep{Bengio95}. However, modeling the problem with IOHMM does not capture the temporal dependencies of the inside driving context. These dependencies are critical to capture the smooth and temporally correlated behaviours such as the driver's face movements. We therefore present Autoregressive Input-Output HMM (AIO-HMM) which extends IOHMM to model these observation dependencies. Figure~\\ref{fig:model} shows the AIO-HMM graphical model for modeling maneuvers. We learn separate AIO-HMM model for each maneuver. In order to anticipate maneuvers, during inference we determine which model best explains the past several seconds of the driving context based on the data log-likelihood.  In Appendix~\\ref{sec:aiohmm} we describe the training and inference procedure for AIO-HMM. \n\n\n\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.9\\linewidth]{data.pdf}\n\\caption{\\textbf{Our data set} is diverse in drivers and landscape.}\n\\label{fig:diverse_data}\n\\end{figure}\n\nIn this section we first give an overview of our data set and then present the quantitative results. We also demonstrate our system and algorithm on real-world driving scenarios. \\textbf{Our video demonstrations are available at}: \\hbox{\\url{http://www.brain4cars.com}. }\n\n\\subsection{Driving data set}\n\\label{subsec:dataset}\nOur data set consists of natural driving videos with both inside and outside views of the car, its speed, and the global position system (GPS) coordinates.\\footnote{The inside and outside cameras operate at 25 and 30 frames/sec.} The outside car video captures the view of the road ahead. We collected this driving data set under fully natural settings without any intervention.\\footnote{\\textbf{Protocol:} We set up cameras, GPS and speed recording device in subject's personal vehicles and left it to record the data. The subjects were asked to ignore our setup and drive as they would normally.} \nIt consists of 1180 miles of freeway and city driving and encloses 21,000 square miles across two states. We collected this data set from 10 drivers over a period of two months. The complete data set has a total of 2 million video frames and includes diverse landscapes. Figure~\\ref{fig:diverse_data} shows a few samples from our data set. We annotated the driving videos with a total of 700 events containing 274 lane changes, 131 turns, and 295 randomly sampled instances of driving straight. Each lane change or turn annotation marks the start time of the maneuver, i.e., before the car touches the lane or yaws, respectively. For all annotated events, we also annotated the lane information, i.e., the number of lanes on the road and the current lane of the car. Our data set is publicly available at \\hbox{\\url{http://www.brain4cars.com}. }\n\n\\subsection{Baseline algorithms}\nWe compare the following algorithms:\n\n\\begin{itemize}\n \\setlength{\\itemsep}{3pt}\n\n  \\setlength{\\parsep}{0pt}\n\\item \\textit{Chance}: Uniformly randomly anticipates a maneuver.\n\\item \\textit{SVM}~\\citep{Morris11}: Support Vector Machine is a discriminative classifier~\\citep{Cortes95}. Morris et al.~\\citep{Morris11} takes this approach  for anticipating maneuvers.\\footnote{Morries et al.~\\citep{Morris11} considered binary \nclassification problem (lane change vs driving straight) and used RVM~\\citep{Tipping01}.}\n\n\nWe train the SVM on 5 seconds of driving context by concatenating all frame features \nto get a $\\mathbb{R}^{3840}$ dimensional feature vector. \n\\item \\textit{Random-Forest}~\\citep{Criminisi11}: This is also a discriminative classifier that\nlearns many decision trees from the training data, and at test time it\naverages the prediction of the individual decision trees. We train\nit on the same features as SVM with 150 trees of depth ten each.\n\\item \\textit{HMM}: This is the Hidden Markov Model. \nWe train the HMM on a temporal sequence of feature vectors that we extract every 0.8 seconds, \ni.e., every 20 video frames.  \nWe consider three versions of the HMM: (i) HMM $E$: with only outside features from the road camera, the vehicle's speed, GPS and street maps (Section~\\ref{subsec:outside_features});\n(ii) HMM $F$: with only inside features from the driver's face (Section~\\ref{subsec:inside_features}); \nand (ii) HMM $E+F$: with both inside and outside features. \n\\item \\textit{IOHMM}: Jain et al.~\\cite{Jain15} modeled driving maneuvers with this Bayesian network. It is trained on the same features as HMM $E + F$.\n\\item \\textit{AIO-HMM}: Jain et al.~\\citep{Jain15} proposed this Bayesian network for modeling maneuvers. It is trained on the same features as HMM $E + F$. \n\\item \\textit{Simple-RNN} (S-RNN): In this architecture sensor streams are fused by simple concatenation and then passed through a single RNN with LSTM units. \n\\item \\textit{Fusion-RNN-Uniform-Loss} (F-RNN-UL): In this architecture sensor streams are passed through separate RNNs, and the high-level representations from RNNs are then fused via a fully-connected layer. The loss at each time step takes the form $-\\log(y_t^k)$. \n\\item \\textit{Fusion-RNN-Exp-Loss} (F-RNN-EL): This architecture is similar to F-RNN-UL, except that the loss exponentially grows with time $-e^{-(T-t)}\\log(y_t^k)$. \n\\end{itemize}\n\nOur RNN and LSTM implementations are open-sourced and available at \\texttt{NeuralModels}~\\citep{Neuralmodels}. For the RNNs in our Fusion-RNN architecture we use a single layer LSTM of size 64 with sigmoid gate activations and tanh activation for hidden representation. Our fully connected fusion layer uses tanh activation and outputs a 64 dimensional vector. Our overall architecture (F-RNN-EL and F-RNN-UL) have nearly 25,000 parameters that are learned using RMSprop~\\citep{Dauphin15}. \n\n\\begin{algorithm}[t]\\caption{Maneuver anticipation}\n\\begin{algorithmic}\n\\STATE \\textbf{Initialize} $m^* = \\textit{driving straight}$\\\\\n\n\\STATE \\textbf{Input} Features $\\{({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_T),({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_T)\\}$ and prediction threshold $p_{th}$\n\\STATE \\textbf{Output} Predicted maneuver $m^*$\n\\WHILE{$t=1$ to $T$}\n        \\STATE Observe features $({\\mathbf{{x}}}_1,...,{\\mathbf{{x}}}_t)$ and $({\\mathbf{{z}}}_1,...,{\\mathbf{{z}}}_t)$\n        \\STATE Estimate probability $ {\\mathbf{{y}}}_t$ of each maneuver in $\\mathcal{M}$\n        \\STATE $m_t^*={\\operatorname{arg\\,max}}_{m\\in\\mathcal{M}}{\\mathbf{{y}}}_t$\n\t\t\\IF{$m_t^* \\neq \\textit{driving straight}$ \\& ${\\mathbf{{y}}}_t\\{m_t^*\\} > p_{th} $}        \n\t\t\t\\STATE $m^* = m_t^*$\n\t\t\t\n\t\t\t\\STATE \\textbf{break}\n\t\t\\ENDIF\n        \n\n\\ENDWHILE\n\\STATE \\textbf{Return} $m^*$\n\\end{algorithmic}\n\\label{alg:inference}\n\\end{algorithm}\n\n\\begin{table*}[t!]\n\\centering\n\\caption{{\\textbf{Maneuver Anticipation Results.} Average \\textit{precision}, \\textit{recall} and \\textit{time-to-maneuver} are computed from 5-fold cross-validation. Standard error is also shown. Algorithms are compared on the features from Jain et al.~\\citep{Jain15}.}}\n\\resizebox{1\\textwidth}{!}{\n\\centering\n\\begin{tabular}{cr|ccc|ccc|ccc}\n\n&  &\\multicolumn{3}{c}{Lane change}&\\multicolumn{3}{|c}{Turns}&\\multicolumn{3}{|c}{All maneuvers}\\\\\n\\cline{1-11}\n\\multicolumn{2}{c|}{\\multirow{2}{*}{Method}} & \\multirow{2}{*}{$Pr$ (\\%)}  & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  & \\multirow{2}{*}{$Pr$ (\\%)} & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  & \\multirow{2}{*}{$Pr$ (\\%)} & \\multirow{2}{*}{$Re$ (\\%)}  & Time-to- \\\\ \n& & & &  maneuver (s) &  & &  maneuver (s) &  & & maneuver (s)\\\\\\hline\n&Chance\t&\t33.3\t\t&\t33.3\t\t&\t-\t&\t33.3\t\t&\t33.3\t\t&\t-\t&\t20.0\t\t&\t20.0\t\t&\t-\\\\\n&Morris et al.~\\citep{Morris11} SVM\t&\t73.7 $\\pm$ 3.4\t&\t57.8 $\\pm$ 2.8\t&\t2.40 \t\t&\t64.7 $\\pm$ 6.5\t&\t47.2 $\\pm$ 7.6\t&\t2.40 \t\t&\t43.7 $\\pm$ 2.4\t&\t37.7 $\\pm$ 1.8\t& 1.20\\\\\n&Random-Forest &   71.2 $\\pm$ 2.4  &   53.4 $\\pm$ 3.2  &   3.00    &   68.6 $\\pm$ 3.5  & 44.4 $\\pm$ 3.5  &   1.20      &   51.9 $\\pm$ 1.6 &   27.7 $\\pm$ 1.1  & 1.20\\\\\n\n\n\n\n&HMM $E$ & 75.0 $\\pm$ 2.2 & 60.4 $\\pm$ 5.7 & 3.46  &  74.4 $\\pm$ 0.5 & 66.6 $\\pm$ 3.0 & 4.04  & 63.9 $\\pm$ 2.6 & 60.2 $\\pm$ 4.2 & 3.26 \\\\\n&HMM $F$\t\t&\t76.4 $\\pm$ 1.4\t&\t75.2 $\\pm$ 1.6\t&\t3.62\t\t&\t75.6 $\\pm$ 2.7\t&\t\t60.1 $\\pm$ 1.7\t&\t3.58 \t\t&\t64.2 $\\pm$ 1.5\t&\t36.8 $\\pm$ 1.3\t&\t2.61\\\\\n&HMM $E+F$\t&\t80.9 $\\pm$ 0.9\t&\t79.6 $\\pm$ 1.3\t&\t3.61 \t\t&\t73.5 $\\pm$ 2.2\t&\n75.3 $\\pm$ 3.1\t&\t4.53 \t\t&\t67.8 $\\pm$ 2.0\t&\t67.7 $\\pm$ 2.5\t&\t3.72 \\\\\\hline\n&IOHMM\t\t&\t81.6 $\\pm$ 1.0\t&\t{79.6 $\\pm$ 1.9}\t&\t3.98  \t&\t77.6 $\\pm$ 3.3\t&\t\t{75.9 $\\pm$ 2.5}\t&\t4.42 \t\t&\t74.2 $\\pm$ 1.7\t&\t71.2 $\\pm$ 1.6\t&\t3.83 \\\\\n&(\\textit{Our final Bayesian network}) AIO-HMM\t\t&\t{83.8 $\\pm$ 1.3}\t&\t79.2 $\\pm$ 2.9\t&\t3.80  \t\t&\t{80.8\t $\\pm$ 3.4}\t&\t\t75.2 $\\pm$ 2.4\t&\t4.16 \t\t&\t{77.4 $\\pm$ 2.3}\t&\t{71.2 $\\pm$ 1.3}\t&\t3.53 \\\\\n& S-RNN & 85.4 $\\pm$ 0.7 & 86.0 $\\pm$ 1.4 & 3.53 & 75.2 $\\pm$ 1.4 & 75.3 $\\pm$ 2.1 & 3.68 & 78.0 $\\pm$ 1.5 & 71.1 $\\pm$ 1.0 & 3.15 \\\\\n\n&F-RNN-UL & \\textbf{92.7} $\\pm$ 2.1 & 84.4 $\\pm$ 2.8 & 3.46 & 81.2 $\\pm$ 3.5 & 78.6 $\\pm$ 2.8 & 3.94 & 82.2 $\\pm$ 1.0 & 75.9 $\\pm$ 1.5 & 3.75 \\\\\n&(\\textit{Our final deep architecture})  F-RNN-EL & 88.2 $\\pm$ 1.4 & \\textbf{86.0} $\\pm$ 0.7 & 3.42 & \\textbf{83.8} $\\pm$ 2.1 & \\textbf{79.9} $\\pm$ 3.5 & 3.78 & \\textbf{84.5} $\\pm$ 1.0 & \\textbf{77.1} $\\pm$ 1.3 & 3.58\\\\\n\n\\end{tabular}\n}\n\\label{tab:prscore}\n\\end{table*}\n\n\\subsection{Evaluation protocol}\n\nWe evaluate an algorithm based on its correctness in predicting future maneuvers. We anticipate maneuvers every 0.8 seconds where the algorithm processes the recent context and assigns a probability to each of the four maneuvers: \\{\\textit{left lane change, right lane change, left turn, right turn}\\} and a probability to the event of \\textit{driving straight}. These five probabilities together sum to one.\nAfter anticipation, i.e. when the algorithm has computed all five probabilities, the algorithm predicts a  maneuver if its probability is above a threshold $p_{th}$. If none of the maneuvers' probabilities are above this threshold, the algorithm does not make a maneuver prediction and predicts \\textit{driving  straight}. However, when it predicts one of the four maneuvers, it sticks with this prediction and makes no further predictions for next 5 seconds or until a maneuver occurs, whichever happens earlier. After 5 seconds or a maneuver has occurred, it returns to anticipating  future maneuvers. Algorithm~\\ref{alg:inference} shows the inference steps for maneuver anticipation.\n\nDuring this process of anticipation and prediction, the algorithm makes (i) true predictions ($tp$): when it predicts the correct maneuver; (ii) false predictions ($fp$): when it predicts a maneuver but the driver performs a different  maneuver; (iii) false positive predictions ($fpp$): when it predicts a maneuver but the driver does not perform any maneuver (i.e. \\textit{driving straight}); and (iv) missed predictions ($mp$): when it predicts \\textit{driving straight} but the driver performs a maneuver. We evaluate the algorithms using their precision and recall scores:\n\n", "index": 11, "text": "$$Pr = \\frac{tp}{\\underbrace{tp+fp+fpp}_\\text{Total \\# of maneuver predictions}};\\;\\;\\;Re=\\frac{tp}{\\underbrace{tp+fp+mp}_\\text{Total \\# of maneuvers}}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"Pr=\\frac{tp}{\\underbrace{tp+fp+fpp}_{\\text{Total \\# of maneuver predictions}}}%&#10;;\\;\\;\\;Re=\\frac{tp}{\\underbrace{tp+fp+mp}_{\\text{Total \\# of maneuvers}}}\" display=\"block\"><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>r</mi></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><mi>p</mi></mrow><munder><munder accentunder=\"true\"><mrow><mrow><mi>t</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow><mo movablelimits=\"false\">+</mo><mrow><mi>f</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow><mo movablelimits=\"false\">+</mo><mrow><mi>f</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mtext>Total # of maneuver predictions</mtext></munder></mfrac></mrow><mo rspace=\"10.9pt\">;</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>e</mi></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><mi>p</mi></mrow><munder><munder accentunder=\"true\"><mrow><mrow><mi>t</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow><mo movablelimits=\"false\">+</mo><mrow><mi>f</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow><mo movablelimits=\"false\">+</mo><mrow><mi>m</mi><mo movablelimits=\"false\">\u2062</mo><mi>p</mi></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mtext>Total # of maneuvers</mtext></munder></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nWe model the correlations between ${\\mathbf{{x}}}$, $h$ and ${\\mathbf{{z}}}$ with an AIO-HMM as shown in Figure~\\ref{fig:model}. The AIO-HMM models the distribution in equation~\\eqref{eq:aio-hmm}. It does not assume any generative process for the outside features $P({\\mathbf{{x}}}_1^T|\\mathcal{M})$. It instead models them in a discriminative manner. \n\nThe top (input) layer of the AIO-HMM consists of  outside features ${\\mathbf{{x}}}_1^T$. The outside features then affect the driver's latent states $h_1^T$, represented by the middle (hidden) layer, which then generates the inside features ${\\mathbf{{z}}}_1^T$ at the bottom (output) layer. The events inside the vehicle such as the driver's head movements are temporally correlated because they are generally smooth. The AIO-HMM handles these dependencies with autoregressive connections in the output layer.  \n\n\\noindent \\textbf{Model Parameters.}\nAIO-HMM has two types of parameters: (i) state transition parameters $\\mathbf{w}$; and (ii) observation emission parameters ($\\boldsymbol\\mu$,$\\mathbf{\\Sigma}$). We use set $\\mathcal{S}$ to denote the possible latent states of the driver. For each state $h=i\\in\\mathcal{S}$, we parametrize transition probabilities of leaving the state with log-linear functions, and parametrize the output layer feature emissions with normal distributions. \n\n", "itemtype": "equation", "pos": 76352, "prevtext": "\nThe precision measures the fraction of the predicted maneuvers that are correct and recall measures the \nfraction of the maneuvers that are correctly predicted. For true predictions ($tp$) we also compute the \naverage \\textit{time-to-maneuver}, where time-to-maneuver is  the interval between the  time of algorithm's prediction and the start of the maneuver.\n\nWe perform cross validation to choose the number of the driver's latent states in the AIO-HMM and the threshold on  probabilities for maneuver prediction. For \\textit{SVM} we cross-validate for the parameter $C$ and the choice of kernel from Gaussian and polynomial kernels. The parameters are chosen as the ones giving the highest F1-score on a validation set. \nThe F1-score is the harmonic mean of the precision and recall, defined as $F1 = 2*Pr*Re/(Pr+Re)$.\n\n\n\n\\subsection{Quantitative results}\n\nWe evaluate the algorithms on maneuvers that were not seen during training  and report the results using 5-fold cross validation. Table \\ref{tab:prscore} reports the precision and recall scores \nunder three settings: (i) \\textit{Lane change}: when the algorithms only predict for the left and right lane changes. This setting is relevant for highway driving where the prior probabilities of turns are low; (ii) \\textit{Turns}: when the algorithms only predict for the left and right turns; and (iii) \\textit{All maneuvers}: here the algorithms jointly predict all four maneuvers. All three settings include the instances of \\textit{driving straight}. \n\nTable~\\ref{tab:prscore} compares the performance of the baseline anticipation algorithms, Bayesian networks, and the variants of our deep learning model. All algorithms in Table~\\ref{tab:prscore} use same feature vectors and KLT face tracker which ensures a fair comparison. As shown in the table, overall the best algorithm for maneuver anticipation is F-RNN-EL, and the best performing Bayesian network is AIO-HMM. F-RNN-EL significantly outperforms AIO-HMM in every setting. This improvement in performance is because RNNs with LSTM units are very expressive models with an internal memory. This allows them to model the much needed long temporal dependencies for anticipation. Additionally, unlike AIO-HMM,  F-RNN-EL is a discriminative model that does not make any assumptions about the generative nature of the problem.  The results also highlight the importance of modeling the temporal nature in the data. Classifiers like SVM and Random Forest do not model the temporal aspects and hence performs poorly. \n\nThe performance of several variants of our deep architecture, reported in Table~\\ref{tab:prscore},  justifies our design decisions to reach the final fusion architecture. When predicting all maneuvers, F-RNN-EL gives 6\\%  higher precision and recall than S-RNN, which performs a simple fusion by concatenating the two sensor streams. On the other hand, F-RNN models each sensor stream with a separate RNN and then uses a fully connected layer to fuse the high-level representations at each time step. This form of sensory fusion is  more principled since the sensor streams represent different data modalities. \nIn addition, exponentially growing the loss further improves the performance. Our new loss scheme  penalizes the network proportional to the length of context it has seen.  When predicting all maneuvers, we observe that F-RNN-EL shows an improvement of 2\\% in precision and recall over F-RNN-UL. We conjecture that exponentially growing the loss acts like a regularizer. It reduces the risk of our network over-fitting early in time when there is not enough context available. Furthermore, the time-to-maneuver remains comparable for F-RNN  with and without exponential loss. \n\nThe Bayesian networks AIO-HMM and HMM $E+F$ adopt different sensory fusion strategies. AIO-HMM fuses the two sensory streams using an input-output model, on the other hand HMM $E+F$ performs early fusion by concatenation. As a result, AIO-HMM gives 10\\% higher precision than HMM $E+F$ for jointly predicting all the maneuvers. AIO-HMM further extends IOHMM by modeling the temporal dependencies of events inside the vehicle. This results in better performance: on average AIO-HMM precision is 3\\% higher than IOHMM, as shown in Table~\\ref{tab:prscore}. Another important aspect of anticipation is the joint modeling of the inside and outside driving contexts. HMM $F$ learns only from the inside driving context, while HMM $E$ learns only from the outside driving context. The performances of both the models is therefore less than HMM $E+F$, which learns jointly both the contexts. \n\nTable~\\ref{tab:fpp} compares the $fpp$ of different algorithms.  False positive predictions ($fpp$) happen when an algorithm predicts a maneuver but the driver does not perform any maneuver (i.e. drives straight). Therefore low value of $fpp$ is preferred. HMM $F$ performs best on this metric at 11\\%\nas it mostly assigns a high probability to \\textit{driving straight}. \nHowever, due to this reason, it incorrectly predicts \\textit{driving straight} even when maneuvers happen. This results in the low recall of \\hbox{HMM $F$} at 36\\%, as shown in Table~\\ref{tab:prscore}. AIO-HMM's \n$fpp$ is 10\\% less than that of IOHMM and HMM $E+F$, and F-RNN-EL is 3\\% less than AIO-HMM. The primary reason for false positive predictions is distracted driving. Drivers interactions with fellow passengers or their looking at the surrounding scenes are sometimes wrongly interpreted by the algorithms. Understanding driver distraction is still an open problem, and orthogonal to the objective of this work. \n\n\\begin{table}[t]\n\\centering\n\\caption{\\textbf{False positive prediction} ($fpp$) of different algorithms. The number inside parenthesis is the standard error.}\n\\resizebox{.85\\linewidth}{!}{\n\\begin{tabular}{r|ccc}\nAlgorithm\t&\tLane change\t&\tTurns\t&\tAll\\\\\\hline\nMorris et al.~\\citep{Morris11} SVM\t&\t15.3 (0.8) &\t\t13.3 (5.6)\t&\t24.0 (3.5)\\\\\nRandom-Forest & 16.2 (3.3) & 12.9 (3.7) & 17.5 (4.0) \\\\\nHMM $E$ & 36.2 (6.6) & 33.3 (0.0) & 63.8 (9.4) \\\\\nHMM $F$\t&\t23.1 (2.1)\t&\t23.3 (3.1)\t&\t11.5 (0.1)\\\\\nHMM $E+F$\t&\t30.0 (4.8)\t&\t21.2 (3.3)\t&\t40.7\t (4.9)\\\\\nIOHMM\t&\t28.4 (1.5)\t&\t25.0 (0.1)\t&\t40.0 (1.5)\\\\\nAIO-HMM\t&\t24.6 (1.5)\t&\t20.0 (2.0)\t&\t30.7 (3.4)\\\\\nS-RNN & 16.2 (1.3) & 16.7 (0.0) & 19.2 (0.0)\\\\\nF-RNN-UL & 19.2 (2.4) & 25.0 (2.4) & 21.5 (2.1) \\\\\nF-RNN-EL & 10.8 (0.7) & 23.3 (1.5) & 27.7 (3.8)\n\\end{tabular}\n}\n\\label{tab:fpp}\n\\end{table}\n\n\n\\begin{table}[h]\n\\centering\n\\caption{\\textbf{3D head-pose features.} In this table we study the effect of better features with best performing algorithm from Table~\\ref{tab:prscore} in `All maneuvers' setting. We use~\\citep{Baltrusaitis13} to track 68 facial landmark points and estimate 3D head-pose.}\n{\n\\newcolumntype{P}[2]{>{\\footnotesize#1\\hspace{0pt}\\arraybackslash}p{#2}}\n\\setlength{\\tabcolsep}{2pt}\n\\centering\n\\resizebox{\\hsize}{!}{\n\\begin{tabular}\n{@{}p{0.40\\linewidth}| P{\\centering}{16mm}P{\\centering}{16mm}P{\\centering}{16mm}@{}}\n\n\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{$Pr$ (\\%)}  & \\multirow{2}{*}{$Re$ (\\%)} & Time-to-  \\\\ \n & & &  maneuver (s)\\\\\\hline\nF-RNN-EL &  {84.5} $\\pm$ 1.0 & {77.1} $\\pm$ 1.3 & 3.58\\\\\n\nF-RNN-EL w/ 3D head-pose &  \\textbf{90.5} $\\pm$ 1.0 & \\textbf{87.4} $\\pm$ 0.5 & 3.16\\\\\n\\end{tabular}\n}}\n\\label{tab:features}\n\\end{table}\n\n\\textbf{3D head-pose features.} The modularity of our approach allows experimenting with more advanced head tracking algorithms. We replace the pipeline for extracting features from the driver's face~\\citep{Jain15} by a Constrained Local Neural Field (CLNF) model~\\citep{Baltrusaitis13}. The new vision pipeline tracks 68 facial landmark points and estimates the driver's 3D head pose as described in Section~\\ref{sec:features}. As shown in Table~\\ref{tab:features}, we see a significant, 6\\% increase in precision and 10\\% increase in recall of F-RNN-EL when using features from our new vision pipeline. This increase in performance is attributed to the following reasons: (i) robustness of CLNF model to variations in illumination and head pose; (ii) 3D head-pose features are very informative for understanding the driver's intention; and (iii) optical flow trajectories generated by tracking facial landmark points represent head movements better, as shown in Figure~\\ref{fig:feature_compare}. The confusion matrix in Figure~\\ref{fig:confmat} shows the precision for each maneuver. F-RNN-EL gives a higher precision than AIO-HMM on every maneuver when both algorithms are trained on same features (Fig.~\\ref{fig:confmat}c). The new vision pipeline with CLNF tracker further improves the precision of F-RNN-EL on all maneuvers (Fig.~\\ref{fig:confmat}d). \n\n\\textbf{Effect of prediction threshold.} In Figure~\\ref{fig:f1score} we study how F1-score varies as we change the prediction threshold $p_{th}$.  We make the following observations: (i) The F1-score does not undergo large variations with changes to the prediction threshold. Hence, it allows practitioners  to fairly trade-off between the precision and recall without hurting the F1-score by much; and (ii)   the maximum F1-score attained by F-RNN-EL is 4\\% more than AIO-HMM when compared on the same features and 13\\% more with our new vision pipeline.\nIn {Tables~\\ref{tab:prscore},~\\ref{tab:fpp} and \\ref{tab:features}}, we used the threshold values which gave the highest F1-score.\n\n\\noindent\\textbf{Anticipation complexity.} The F-RNN-EL anticipates maneuvers every 0.8 seconds using the previous 5 seconds of the driving context. The complexity mainly comprises of feature extraction and the model inference in Algorithm~\\ref{alg:inference}. Fortunately both these steps can be performed as a dynamic program by storing the computation of the most recent anticipation. Therefore, for every anticipation we only process the incoming 0.8 seconds and not complete 5 seconds of the driving context. On average we predict a maneuver under 0.20~milliseconds using Theano~\\citep{Bastien12} on Nvidia K40 GPU on Ubuntu~12.04. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{thresh_f1.pdf}\n\\caption{\\textbf{Effect of prediction threshold $p_{th}$.} At test time an algorithm makes a prediction only when it is at least $p_{th}$ confident in its prediction. This plot shows how F1-score vary with change in prediction threshold.}\n\\label{fig:f1score}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{IOHMM_I_O_precision.png}\n\t\\caption{IOHMM}\n\t\\end{subfigure}\n\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{AIOHMM_I_O_precision.png}\n\t\\caption{AIO-HMM}\n\t\\end{subfigure}\t\n\t\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{fusionrnn_old_features.png}\n\t\\caption{F-RNN-EL}\n\t\\end{subfigure}\t\n\t\\begin{subfigure}[b]{.23\\textwidth}\n\t\\includegraphics[width=\\linewidth]{fusionrnn_new_features.png}\n\t\\caption{F-RNN-EL w/ 3D-pose}\n\t\\end{subfigure}\n\t\\caption{\\textbf{Confusion matrix} of different algorithms when jointly predicting all the maneuvers. Predictions made by algorithms are represented by rows and actual maneuvers are represented by columns. Numbers on the diagonal represent precision.}\n\t\\label{fig:confmat}\t\n\\end{figure*}\n\n\n\n\\section{Conclusion}\n\nIn this paper we considered the problem of anticipating driving maneuvers a few seconds before the driver performs them.  This problem requires the modeling of long temporal dependencies and the fusion of multiple sensory streams. We proposed a novel deep learning architecture based on Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units for anticipation. Our architecture learns to fuse multiple sensory streams, and by training it in a sequence-to-sequence prediction manner, it explicitly learns to anticipate using only a partial temporal context. We also proposed a novel loss layer for anticipation which prevents over-fitting. \n\nWe release an open-source data set of 1180 miles of natural driving. We performed an extensive evaluation and showed improvement over many baseline algorithms. Our sensory fusion deep learning approach gives a precision of 84.5\\% and recall of 77.1\\%, and anticipates maneuvers \\hbox{3.5 seconds} (on average) before they happen. By incorporating the driver's 3D head-pose our precision and recall improves to 90.5\\% and 87.4\\% respectively. Potential application of our work is enabling advanced driver assistance systems (ADAS) to alert drivers before they perform a dangerous maneuver, thereby giving drivers more time to react. We believe that our deep learning architecture is widely applicable to many activity anticipation problems. Our code and data set are publicly available on the project web-page. \n\\\\~\\\\~\n\\textbf{Acknowledgement.} We thank NVIDIA for the donation of K40 GPUs used in\nthis research. We also thank Silvio Savarese for useful discussions. This work\nwas supported by National Robotics Initiative (NRI) award 1426452, Office of Naval\nResearch (ONR) award N00014-14-1-0156, and by Microsoft Faculty Fellowship and NSF Career\nAward to Saxena.\n\n\\appendices\n\n\\section{Modeling Maneuvers with AIO-HMM}\n\\label{sec:aiohmm}\n\nGiven $T$ seconds long driving context $\\mathcal{C}$  before the maneuver $\\mathcal{M}$, we learn a generative model for the context $P(\\mathcal{C}|\\mathcal{M})$. The driving context $\\mathcal{C}$ consists of the outside driving context and the inside driving context. The outside and inside contexts are temporal sequences represented by the outside features ${\\mathbf{{x}}}_1^T = \\{{\\mathbf{{x}}}_1,..,{\\mathbf{{x}}}_T\\}$ and the inside features ${\\mathbf{{z}}}_1^T = \\{{\\mathbf{{z}}}_1,..,{\\mathbf{{z}}}_T\\}$ respectively. The corresponding sequence of the driver's latent states is ${h}_1^T = \\{{h}_1,..,{h}_T\\}$. ${\\mathbf{{x}}}$ and ${\\mathbf{{z}}}$ are vectors and ${h}$ is a discrete state.\n\n", "index": 13, "text": "\\begin{align}\n\\nonumber P(\\mathcal{C}|\\mathcal{M}) &= \\sum_{h_1^T} P({\\mathbf{{z}}}_1^T,{\\mathbf{{x}}}_1^T,h_1^T|\\mathcal{M})\\\\\n\\nonumber &= P({\\mathbf{{x}}}_1^T|\\mathcal{M})\\sum_{h_1^T} P({\\mathbf{{z}}}_1^T,h_1^T|{\\mathbf{{x}}}_1^T,\\mathcal{M})\\\\\n\\label{eq:aio-hmm} & \\propto \\sum_{h_1^T} P({\\mathbf{{z}}}_1^T,h_1^T|{\\mathbf{{x}}}_1^T,\\mathcal{M})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(\\mathcal{C}|\\mathcal{M})\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{h_{1}^{T}}P({\\mathbf{{z}}}_{1}^{T},{\\mathbf{{x}}}_{1}^{T},%&#10;h_{1}^{T}|\\mathcal{M})\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=P({\\mathbf{{x}}}_{1}^{T}|\\mathcal{M})\\sum_{h_{1}^{T}}P({\\mathbf{%&#10;{z}}}_{1}^{T},h_{1}^{T}|{\\mathbf{{x}}}_{1}^{T},\\mathcal{M})\" display=\"inline\"><mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\sum_{h_{1}^{T}}P({\\mathbf{{z}}}_{1}^{T},h_{1}^{T}|{%&#10;\\mathbf{{x}}}_{1}^{T},\\mathcal{M})\" display=\"inline\"><mrow><mo>\u221d</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\nThe inside (vehicle) features represented by the output layer are jointly influenced by all three layers. These interactions are modeled by the mean and variance of the normal distribution. We model the mean of the distribution using the outside and inside features from the vehicle as follows:\n\n", "itemtype": "equation", "pos": 78060, "prevtext": "\nWe model the correlations between ${\\mathbf{{x}}}$, $h$ and ${\\mathbf{{z}}}$ with an AIO-HMM as shown in Figure~\\ref{fig:model}. The AIO-HMM models the distribution in equation~\\eqref{eq:aio-hmm}. It does not assume any generative process for the outside features $P({\\mathbf{{x}}}_1^T|\\mathcal{M})$. It instead models them in a discriminative manner. \n\nThe top (input) layer of the AIO-HMM consists of  outside features ${\\mathbf{{x}}}_1^T$. The outside features then affect the driver's latent states $h_1^T$, represented by the middle (hidden) layer, which then generates the inside features ${\\mathbf{{z}}}_1^T$ at the bottom (output) layer. The events inside the vehicle such as the driver's head movements are temporally correlated because they are generally smooth. The AIO-HMM handles these dependencies with autoregressive connections in the output layer.  \n\n\\noindent \\textbf{Model Parameters.}\nAIO-HMM has two types of parameters: (i) state transition parameters $\\mathbf{w}$; and (ii) observation emission parameters ($\\boldsymbol\\mu$,$\\mathbf{\\Sigma}$). We use set $\\mathcal{S}$ to denote the possible latent states of the driver. For each state $h=i\\in\\mathcal{S}$, we parametrize transition probabilities of leaving the state with log-linear functions, and parametrize the output layer feature emissions with normal distributions. \n\n", "index": 15, "text": "\\begin{align*}\n\\text{Transition: }& P(h_t = j | h_{t-1} = i,\n{\\mathbf{{x}}}_t;\\mathbf{w}_{ij}) = \\frac{e^{\\mathbf{w}_{ij} \\cdot {\\mathbf{{x}}}_t}}{\\sum_{l \\in \\mathcal{S}}\ne^{\\mathbf{w}_{il} \\cdot {\\mathbf{{x}}}_t}}\\\\\n\\text{Emission: }&P({\\mathbf{{z}}}_t | h_t=i,{\\mathbf{{x}}}_t,{\\mathbf{{z}}}_{t-1};\\boldsymbol\\mu_{it},\\mathbf{\\Sigma}_i) =\n\\mathcal{N}({\\mathbf{{z}}}_t|\\boldsymbol\\mu_{it},\\mathbf{\\Sigma}_i) \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle P(h_{t}=j|h_{t-1}=i,{\\mathbf{{x}}}_{t};\\mathbf{w}_{ij})=\\frac{e^%&#10;{\\mathbf{w}_{ij}\\cdot{\\mathbf{{x}}}_{t}}}{\\sum_{l\\in\\mathcal{S}}e^{\\mathbf{w}_%&#10;{il}\\cdot{\\mathbf{{x}}}_{t}}}\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>j</mi><mo stretchy=\"false\">|</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo>;</mo><msub><mi>\ud835\udc30</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>e</mi><mrow><msub><mi>\ud835\udc30</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u22c5</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow></msup><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></msub><msup><mi>e</mi><mrow><msub><mi>\ud835\udc30</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u22c5</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow></msup></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle P({\\mathbf{{z}}}_{t}|h_{t}=i,{\\mathbf{{x}}}_{t},{\\mathbf{{z}}}_{%&#10;t-1};\\bm{\\mu}_{it},\\mathbf{\\Sigma}_{i})=\\mathcal{N}({\\mathbf{{z}}}_{t}|\\bm{\\mu%&#10;}_{it},\\mathbf{\\Sigma}_{i})\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>i</mi><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msub><mi>\ud835\udf41</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udeba</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udf41</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udeba</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nIn the equation above, $\\mathbf{a}_i$ and $\\mathbf{b}_i$ are parameters that we learn for every state $i \\in \\mathcal{S}$. Therefore, the parameters we learn for state $i \\in \\mathcal{S}$ are $\\boldsymbol\\theta_i = \\{\\boldsymbol\\mu_i$, $\\mathbf{a}_i$, $\\mathbf{b}_i$, $\\mathbf{\\Sigma}_i$ and $\\mathbf{w}_{ij} | j \\in \\mathcal{S}\\}$, and the overall model parameters are $\\boldsymbol\\Theta = \\{\\boldsymbol\\theta_i|i\\in\\mathcal{S}\\}$.\n\n\\subsection{Learning AIO-HMM parameters}\n\nThe training data $\\mathcal{D} = \\{({\\mathbf{{x}}}_{1,n}^{T_n},{\\mathbf{{z}}}_{1,n}^{T_n})|n=1,..,N\\}$ consists of $N$ instances of a maneuver $\\mathcal{M}$. The goal is to maximize the data log-likelihood.\n\n", "itemtype": "equation", "pos": 78780, "prevtext": "\n\nThe inside (vehicle) features represented by the output layer are jointly influenced by all three layers. These interactions are modeled by the mean and variance of the normal distribution. We model the mean of the distribution using the outside and inside features from the vehicle as follows:\n\n", "index": 17, "text": "$$\\boldsymbol\\mu_{it} = (1 + \\mathbf{a}_i \\cdot {\\mathbf{{x}}}_t + \\mathbf{b}_i \\cdot {\\mathbf{{z}}}_{t-1})\n\\boldsymbol\\mu_i$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\bm{\\mu}_{it}=(1+\\mathbf{a}_{i}\\cdot{\\mathbf{{x}}}_{t}+\\mathbf{b}_{i}\\cdot{%&#10;\\mathbf{{z}}}_{t-1})\\bm{\\mu}_{i}\" display=\"block\"><mrow><msub><mi>\ud835\udf41</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>\ud835\udc1a</mi><mi>i</mi></msub><mo>\u22c5</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc1b</mi><mi>i</mi></msub><mo>\u22c5</mo><msub><mi>\ud835\udc33</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udf41</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nDirectly optimizing  equation~\\eqref{eq:likelihood}  is challenging because parameters $h$ representing the driver's states are \\textit{latent}. We therefore use the iterative EM procedure to learn the model parameters. In EM, instead of directly maximizing equation~\\eqref{eq:likelihood}, we maximize its simpler lower bound. We estimate the lower bound in the E-step and then maximize that estimate in the M-step. These two steps are repeated iteratively. \n\n\\noindent \\textbf{E-step.} In the E-step we get the lower bound of equation~\\eqref{eq:likelihood} by calculating the expected\nvalue of the \\textit{complete} data log-likelihood using the current estimate of the parameter $\\hat{\\boldsymbol\\Theta}$. \n\n", "itemtype": "equation", "pos": 79590, "prevtext": "\nIn the equation above, $\\mathbf{a}_i$ and $\\mathbf{b}_i$ are parameters that we learn for every state $i \\in \\mathcal{S}$. Therefore, the parameters we learn for state $i \\in \\mathcal{S}$ are $\\boldsymbol\\theta_i = \\{\\boldsymbol\\mu_i$, $\\mathbf{a}_i$, $\\mathbf{b}_i$, $\\mathbf{\\Sigma}_i$ and $\\mathbf{w}_{ij} | j \\in \\mathcal{S}\\}$, and the overall model parameters are $\\boldsymbol\\Theta = \\{\\boldsymbol\\theta_i|i\\in\\mathcal{S}\\}$.\n\n\\subsection{Learning AIO-HMM parameters}\n\nThe training data $\\mathcal{D} = \\{({\\mathbf{{x}}}_{1,n}^{T_n},{\\mathbf{{z}}}_{1,n}^{T_n})|n=1,..,N\\}$ consists of $N$ instances of a maneuver $\\mathcal{M}$. The goal is to maximize the data log-likelihood.\n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq:likelihood}\nl(\\boldsymbol\\Theta;\\mathcal{D}) = \\sum_{n=1}^N\n\\log P({\\mathbf{{z}}}_{1,n}^{T_n}|{\\mathbf{{x}}}_{1,n}^{T_n};\\boldsymbol\\Theta)\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"l(\\bm{\\Theta};\\mathcal{D})=\\sum_{n=1}^{N}\\log P({\\mathbf{{z}}}_{1,n}^{T_{n}}|{%&#10;\\mathbf{{x}}}_{1,n}^{T_{n}};\\bm{\\Theta})\" display=\"block\"><mrow><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc31</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo>;</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\nwhere $l_c(\\boldsymbol\\Theta;\\mathcal{D}_c)$ is the  log-likelihood \n of  the \\textit{complete} data $\\mathcal{D}_c$ defined as:  \n\n", "itemtype": "equation", "pos": 80482, "prevtext": "\nDirectly optimizing  equation~\\eqref{eq:likelihood}  is challenging because parameters $h$ representing the driver's states are \\textit{latent}. We therefore use the iterative EM procedure to learn the model parameters. In EM, instead of directly maximizing equation~\\eqref{eq:likelihood}, we maximize its simpler lower bound. We estimate the lower bound in the E-step and then maximize that estimate in the M-step. These two steps are repeated iteratively. \n\n\\noindent \\textbf{E-step.} In the E-step we get the lower bound of equation~\\eqref{eq:likelihood} by calculating the expected\nvalue of the \\textit{complete} data log-likelihood using the current estimate of the parameter $\\hat{\\boldsymbol\\Theta}$. \n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:estep}\n\\text{E-step: }Q(\\boldsymbol\\Theta;\\hat{\\boldsymbol\\Theta}) =\nE[l_c(\\boldsymbol\\Theta;\\mathcal{D}_c)|\\hat{\\boldsymbol\\Theta},\\mathcal{D}] \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\text{E-step: }Q(\\bm{\\Theta};\\hat{\\bm{\\Theta}})=E[l_{c}(\\bm{\\Theta};\\mathcal{D%&#10;}_{c})|\\hat{\\bm{\\Theta}},\\mathcal{D}]\" display=\"block\"><mrow><mtext>E-step:\u00a0</mtext><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>;</mo><mover accent=\"true\"><mi>\ud835\udeaf</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>E</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>l</mi><mi>c</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>c</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi>\ud835\udeaf</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\nWe should note that the occurrences of hidden variables $h$ in $l_c(\\boldsymbol\\Theta;\\mathcal{D}_c)$ are marginalized in equation~\\eqref{eq:estep}, and hence $h$ need not be known.\nWe efficiently estimate $Q(\\boldsymbol\\Theta;\\hat{\\boldsymbol\\Theta})$ using the forward-backward algorithm~\\citep{Murphy12}. \n\\iffalse\n{\\textcolor{blue}{\\textbf{{[Remove the next line and equations.]}}}} It essentially computes the following two posterior distributions for each instance of maneuver:  \n\n", "itemtype": "equation", "pos": 80801, "prevtext": "\nwhere $l_c(\\boldsymbol\\Theta;\\mathcal{D}_c)$ is the  log-likelihood \n of  the \\textit{complete} data $\\mathcal{D}_c$ defined as:  \n\n", "index": 23, "text": "\\begin{align}\n\\mathcal{D}_c &= \\{({\\mathbf{{x}}}_{1,n}^{T_n},{\\mathbf{{z}}}_{1,n}^{T_n},h_{1,n}^{T_n})|n=1,..,N\\} \\label{eq:complete-data}\\\\\nl_c(\\boldsymbol\\Theta;\\mathcal{D}_c) &= \\sum_{n=1}^N\n\\log P({\\mathbf{{z}}}_{1,n}^{T_n},h_{1,n}^{T_n}|{\\mathbf{{x}}}_{1,n}^{T_n};\\boldsymbol\\Theta) \\label{eq:complete-likelihood}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{D}_{c}\" display=\"inline\"><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>c</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\{({\\mathbf{{x}}}_{1,n}^{T_{n}},{\\mathbf{{z}}}_{1,n}^{T_{n}},h_{%&#10;1,n}^{T_{n}})|n=1,..,N\\}\" display=\"inline\"><mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc31</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo>,</mo><msubsup><mi>\ud835\udc33</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo>,</mo><msubsup><mi>h</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>,</mo><mi>N</mi><mo stretchy=\"false\">}</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle l_{c}(\\bm{\\Theta};\\mathcal{D}_{c})\" display=\"inline\"><mrow><msub><mi>l</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>c</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{n=1}^{N}\\log P({\\mathbf{{z}}}_{1,n}^{T_{n}},h_{1,n}^{T_{n}%&#10;}|{\\mathbf{{x}}}_{1,n}^{T_{n}};\\bm{\\Theta})\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo>,</mo><msubsup><mi>h</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc31</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><msub><mi>T</mi><mi>n</mi></msub></msubsup><mo>;</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\\fi\n\n\\noindent \\textbf{M-step.} In the M-step we maximize the expected value  of the complete data log-likelihood $Q(\\boldsymbol\\Theta;\\hat{\\boldsymbol\\Theta})$ and update the model parameter as follows:\n\n", "itemtype": "equation", "pos": 81619, "prevtext": "\n\nWe should note that the occurrences of hidden variables $h$ in $l_c(\\boldsymbol\\Theta;\\mathcal{D}_c)$ are marginalized in equation~\\eqref{eq:estep}, and hence $h$ need not be known.\nWe efficiently estimate $Q(\\boldsymbol\\Theta;\\hat{\\boldsymbol\\Theta})$ using the forward-backward algorithm~\\citep{Murphy12}. \n\\iffalse\n{\\textcolor{blue}{\\textbf{{[Remove the next line and equations.]}}}} It essentially computes the following two posterior distributions for each instance of maneuver:  \n\n", "index": 25, "text": "\\begin{align*}\n\\gamma_{jt} &= P(h_{t}=j|{\\mathbf{{z}}}_{1}^{K},{\\mathbf{{x}}}_{1}^{K};\\hat{\\boldsymbol\\Theta})\\\\\n\\xi_{ijt} &= P(h_{t}=j,h_{t-1}=i|{\\mathbf{{z}}}_{1}^{K},{\\mathbf{{x}}}_{1}^{K};\\hat{\\boldsymbol\\Theta})\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\gamma_{jt}\" display=\"inline\"><msub><mi>\u03b3</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=P(h_{t}=j|{\\mathbf{{z}}}_{1}^{K},{\\mathbf{{x}}}_{1}^{K};\\hat{\\bm%&#10;{\\Theta}})\" display=\"inline\"><mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>j</mi><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>K</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>K</mi></msubsup><mo>;</mo><mover accent=\"true\"><mi>\ud835\udeaf</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\xi_{ijt}\" display=\"inline\"><msub><mi>\u03be</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=P(h_{t}=j,h_{t-1}=i|{\\mathbf{{z}}}_{1}^{K},{\\mathbf{{x}}}_{1}^{K%&#10;};\\hat{\\bm{\\Theta}})\" display=\"inline\"><mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>j</mi><mo>,</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>i</mi><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>K</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>K</mi></msubsup><mo>;</mo><mover accent=\"true\"><mi>\ud835\udeaf</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\nSolving  equation~\\eqref{eq:mstep} requires us to optimize for the parameters \n$\\boldsymbol\\mu$, $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{\\Sigma}$ and $\\mathbf{w}$. We optimize all parameters expect $\\mathbf{w}$ exactly by deriving their closed form update expressions. We optimize $\\mathbf{w}$ using the gradient descent. \n\n\\subsection{Inference of Maneuvers}\n\n\n\nOur learning algorithm trains separate AIO-HMM models for each maneuver. The goal during inference is to determine which model best explains the past $T$ seconds of the driving context not seen during training. We evaluate the likelihood of the inside and outside feature sequences (${\\mathbf{{z}}}_1^T$ and ${\\mathbf{{x}}}_1^T$)  for each maneuver, and anticipate the probability $P_\\mathcal{M}$ of each maneuver $\\mathcal{M}$ as follows:\n\n\n", "itemtype": "equation", "pos": 82053, "prevtext": "\n\\fi\n\n\\noindent \\textbf{M-step.} In the M-step we maximize the expected value  of the complete data log-likelihood $Q(\\boldsymbol\\Theta;\\hat{\\boldsymbol\\Theta})$ and update the model parameter as follows:\n\n", "index": 27, "text": "\\begin{equation}\n\\label{eq:mstep}\n\\text{M-step: }\\boldsymbol\\Theta = {\\operatorname{arg\\,max}}_{\\boldsymbol\\Theta}\nQ(\\boldsymbol\\Theta;\\boldsymbol\\hat{\\boldsymbol\\Theta})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\text{M-step: }\\bm{\\Theta}={\\operatorname{arg\\,max}}_{\\bm{\\Theta}}Q(\\bm{\\Theta%&#10;};\\bm{\\hat{}}{\\bm{\\Theta}})\" display=\"block\"><mrow><mrow><mtext>M-step:\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udeaf</mi></mrow><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mi>\ud835\udeaf</mi></msub><mo>\u2061</mo><mi>Q</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo>;</mo><mrow><mover accent=\"true\"><mi/><mo mathvariant=\"bold\" stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mi>\ud835\udeaf</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\\iffalse\n\n", "itemtype": "equation", "pos": 83041, "prevtext": "\n\nSolving  equation~\\eqref{eq:mstep} requires us to optimize for the parameters \n$\\boldsymbol\\mu$, $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{\\Sigma}$ and $\\mathbf{w}$. We optimize all parameters expect $\\mathbf{w}$ exactly by deriving their closed form update expressions. We optimize $\\mathbf{w}$ using the gradient descent. \n\n\\subsection{Inference of Maneuvers}\n\n\n\nOur learning algorithm trains separate AIO-HMM models for each maneuver. The goal during inference is to determine which model best explains the past $T$ seconds of the driving context not seen during training. We evaluate the likelihood of the inside and outside feature sequences (${\\mathbf{{z}}}_1^T$ and ${\\mathbf{{x}}}_1^T$)  for each maneuver, and anticipate the probability $P_\\mathcal{M}$ of each maneuver $\\mathcal{M}$ as follows:\n\n\n", "index": 29, "text": "\\begin{align}\n\\label{eq:infer} P_\\mathcal{M} &= P(\\mathcal{M}|{\\mathbf{{z}}}_1^T, {\\mathbf{{x}}}_1^T) \\propto P({\\mathbf{{z}}}_1^T, {\\mathbf{{x}}}_1^T | \\mathcal{M})P(\\mathcal{M})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P_{\\mathcal{M}}\" display=\"inline\"><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=P(\\mathcal{M}|{\\mathbf{{z}}}_{1}^{T},{\\mathbf{{x}}}_{1}^{T})%&#10;\\propto P({\\mathbf{{z}}}_{1}^{T},{\\mathbf{{x}}}_{1}^{T}|\\mathcal{M})P(\\mathcal%&#10;{M})\" display=\"inline\"><mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00740.tex", "nexttext": "\n\\fi\nAlgorithm~\\ref{alg:coactive} shows the complete inference procedure. The inference in equation~\\eqref{eq:infer} simply requires a forward-pass~\\citep{Murphy12} of the AIO-HMM, the complexity of which is \n\\hbox{$\\mathcal{O}(T(|\\mathcal{S}|^2 + |\\mathcal{S}||{\\mathbf{{z}}}|^3 + |\\mathcal{S}||{\\mathbf{{x}}}|))$}. However, in practice it is only $\\mathcal{O}(T|\\mathcal{S}||{\\mathbf{{z}}}|^3)$ because $|{\\mathbf{{z}}}|^3 \\gg |S|$ and $|{\\mathbf{{z}}}|^3 \\gg |{\\mathbf{{x}}}|$. Here $|\\mathcal{S}|$\nis the number of discrete states representing the driver's intention, while $|{\\mathbf{{z}}}|$ and $|{\\mathbf{{x}}}|$ are the dimensions of the inside and outside feature vectors respectively. In equation~\\eqref{eq:infer} $P(\\mathcal{M})$ is the prior probability of maneuver $\\mathcal{M}$. We assume an uninformative uniform prior over the maneuvers.\n  \\begin{algorithm}[H]\\caption{Anticipating maneuvers }\n\\begin{algorithmic}\n\\INPUT Driving videos, GPS, Maps and Vehicle Dynamics\n\\OUTPUT Probability of each maneuver\n\\STATE Initialize the face tracker with the driver's face\n\\WHILE{$driving$}\n\t\\STATE Track the driver's face~\\citep{Viola04}\n\t\\STATE Extract features ${\\mathbf{{z}}}_1^T$ and ${\\mathbf{{x}}}_1^T$ (Sec.~\\ref{sec:features})\n\t\\STATE Inference $P_\\mathcal{M} = P(\\mathcal{M}|{\\mathbf{{z}}}_1^T, {\\mathbf{{x}}}_1^T)$ (Eq.~\\eqref{eq:infer})\n\t\\STATE Send the inferred probability of each maneuver to ADAS\n\\ENDWHILE\n\\end{algorithmic}\n\\label{alg:coactive}\n\\end{algorithm}\n\n\n\n{\n\\bibliographystyle{plainnat}\n\\bibliography{longstrings,references}\n}\n\n\n", "itemtype": "equation", "pos": 83242, "prevtext": "\n\\iffalse\n\n", "index": 31, "text": "\\begin{align}\n\\nonumber \\mathcal{M}^* &= {\\operatorname{arg\\,max}}_{\\mathcal{M}}P(\\mathcal{M}|{\\mathbf{{z}}}_1^T, {\\mathbf{{x}}}_1^T) \\\\\n\\label{eq:infer} &= {\\operatorname{arg\\,max}}_{\\mathcal{M}}P({\\mathbf{{z}}}_1^T, {\\mathbf{{x}}}_1^T | \\mathcal{M})P(\\mathcal{M})\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{M}^{*}\" display=\"inline\"><msup><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo>*</mo></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{arg\\,max}}_{\\mathcal{M}}P(\\mathcal{M}|{\\mathbf{{z%&#10;}}}_{1}^{T},{\\mathbf{{x}}}_{1}^{T})\" display=\"inline\"><mrow><mo>=</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{arg\\,max}}_{\\mathcal{M}}P({\\mathbf{{z}}}_{1}^{T},%&#10;{\\mathbf{{x}}}_{1}^{T}|\\mathcal{M})P(\\mathcal{M})\" display=\"inline\"><mrow><mo>=</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc33</mi><mn>1</mn><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc31</mi><mn>1</mn><mi>T</mi></msubsup><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}]