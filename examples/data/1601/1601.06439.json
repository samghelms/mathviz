[{"file": "1601.06439.tex", "nexttext": "\nTags not in the groundtruth ordered set have zero relevance.\nMore common metrics, such as precision, recall, and average precision, assume\nthat all tags are equally relevant, so we do not utilize these metrics in this\npaper. Instead we use the more appropriate \\emph{normalized discounted \ncumulative gain} (nDCG), a common metric used in evaluating\nsearch engine results. For an ordered set $T = \\{t_1,\\cdots,t_k\\}$, such that\n$i < j \\rightarrow t_i \\succ t_j$, we define the DCG with respect to the \nground-truth as:\n\n\n", "itemtype": "equation", "pos": 11744, "prevtext": "\n\n\n\n\n\n\n\n\n\\title{Who Ordered This?: Exploiting Implicit User Tag Order Preferences for Personalized Image Tagging}\n\\name{Amandianeze O. Nwana and Tsuhan Chen}\n\n\n\n\n\n\n\n\n\n\\address{School of Electrical and Computer Engineering, Cornell University, Ithaca, NY 14850, U.S.A.}\n\n\n\n\\maketitle\n\n\\begin{abstract}\nWhat makes a person pick certain tags over others when tagging an \nimage? Does the order that a person presents tags for a given image follow\nan implicit bias that is personal? Can these biases be used to improve existing\nautomated image tagging systems? We show that tag ordering, which has been \nlargely overlooked by the image tagging community, is an important cue in \nunderstanding user tagging behavior and can be used to improve auto-tagging\nsystems. Inspired by the assumption that people order their tags, we propose a\nnew way of measuring tag preferences, and also propose a new personalized \ntagging objective function that explicitly considers a user's preferred tag\norderings. We also provide a (partially) greedy algorithm that produces good\nsolutions to our new objective and under certain conditions produces an optimal\nsolution. We validate our method on a subset of Flickr images that spans 5000 \nusers, over 5200 tags, and over 90,000 images. Our experiments show that \nexploiting personalized tag orders improves the average performance of \nstate-of-art approaches both on per-image and per-user bases.\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{keywords}\nordered tags, personalized tagging, user behavior, personalization,\ntag preferences\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sect:intro}\n\nNearly every work on image tagging to date has treated the tags that accompany\nan image as a bag-of-words with no inherent ordering, most especially works \nwhich use the nearest neighbor approach for tagging. Some recent work\n\\cite{nwana:15}, has shown that this bag-of-words assumption is actually not \nthe reality. They show that if a user is asked to tag an image with a fixed set\nof tags multiple times, the order of the tags they choose tends to stay the \nsame rather than random.\nMore so, a recent design change on Flickr~\\cite{web:flickr} that reversed the\nusers' tag input ordering created a backlash from the community that caused\nFlickr to revert back to the original design with the following apology:\n``We thought it would be more intuitive for newest tags to appear at the top,\nso that when you add a tag and refresh, it's in the place you expect it. \nHowever, thanks to your points about meticulous tag ordering, we've decided we\nshould leave it as is. You should now see your tags in the correct order''\n\\cite{web:flickr_order}.\n\nThese insights lead us to believe that tag ordering is inherently personal, and\nshould be incorportated into tagging systems. Hence, we propose a new model\nthat seeks to exploit the history of a user's tag order to improve personalized\nimage tagging. We also compare the same model which instead treats tag order as\na global phenomenon, and show that indeed such a model underperforms the \npersonalized ordering model significantly.\n\n\n\n\\subsection{Related Work}\n\\label{sec:related}\n\nThere has been a lot of work in the area of automatic image tagging\n\\cite{li:11, weston:11, lindstaedt:08, mcauley:12, sawant:10, \nsigurbjornsson:08,stone:08, belem:11}, much of which has treated tagging as\nmore of a labelling problem, thereby implicitly imposing that tagging is a \nglobal task rather than a user-specific task. Most notably the work by Li et\nal.~\\cite{li:11} has tried to treat the tagging problem from the point of\nview of personalization by using a cross-entropy method to decide how to \nweight different image tagging functions for each user.\n\nThere has been work seeking to tag based on object importance, \nbut there the focus has been an explicitly categorical approach to \nimportance by measuring properties of objects in the image (eg, size, salience)\nto estimate their relative importances \\cite{berg:12}. These object-property\nbased approaches to importance typically ignore particular user preferences, and\ntreat importance as a global phenomenon \\cite{berg:12, spain:11}. \n\nNearest neighbor approaches are usually more \ncommon than their explicit classification counterparts in tagging because one\ndoes not have to learn how to recognize or detect specific objects\nin the image, which is not scalable, nor are all concepts one\nwould like to describe an image visual\n\\cite{smeulders:00, schreiber:01, weston:11}. \nIn many cases, an initial set of tags for a given query\nimage is provided and the aim is to refine or propagate these tags through some, \nsimilarity measure (image, user, or tag co-similarity) \\cite{lindstaedt:08, \nrattenbury:07, schmitz:06}. Whereas their goal is to create global tags from\nother tags, our goal is to find the words most suitable to describe an\nimage for specific individuals.\nThere has also been some work that looks at the recommendation perspective \n\\cite{adomavicius:05, koren:09,rendle:09,\nrendle:10, pan:13, peng:14, sahoo:12, sen:09, song:08},\nwith~\\cite{peng:14} having an explicit goal of personalized tagging.\n\nIn this work, we employ the nearest neighbor approach to tagging because we would like to\nuse a relatively large tagging vocabulary, which introduces scalability issues\nwith the explicit/classification approaches, and we also do not want to restrict\nour vocabulary to words that describe concrete visual concepts. Instead we\nwould like to have more abstract tags because we are primarily interested in \npersonalized tagging. Finally, we also do not want to consider tag importances\nsolely on a global scale, but per user preference.  \n\n\n\n\\section{Model}\n\\label{sec:model}\n\nIn this section we discuss our model for the tagging behavior of users\nand derive an objective function based on the model of tagging behavior. We\nthen discuss a practical representation of this model and how it relates to\nother known problems.\n\n\n\n\\subsection{User Tagging Behavior}\n\\label{model:assumption}\n\nThe main assumption, following the claims from~\\cite{nwana:15},\nis that given an image, the order that \nusers present tags to an online tagging system, say Flickr \\cite{web:flickr}, \nimplies an underlying preference order for those tags.  For example, if a user\npresents the tags $B,\\ A,\\ C$ in that order, it would imply that for that image,\nthe user found it more preferable (or important) to mention $B$ before $A$,\n$B$ before $C$, and $A$ before $C$. \n\nOur main idea is that with enough observed instances of pairs of tags mentioned \ntogether by a particular user, we can estimate these implicit biases, and in turn \nimprove the task of automatic image tagging for new images on a personal level.\n\n\n\n\\subsection{Tagging Objective}\n\\label{model:objective}\n\nOur main tagging goal is to output a list of tags for an image in order of\npreference for the target user. To that end, given a set of pairwise \ntag-preferences (as probabilities) for a given user and a query image, we want\nto find the ordering of candidate tags (generated by some baseline tag \ngenerator), for that query image, that is maximal. \nDetails are provided in the supplementary appendix.\n\n\\subsubsection*{Parameter Estimation}\n\\label{model: parameter_estimation}\n\nFor the objective described above, we represent the tag-preferences\nas probabilities. That is, $p_{ab}$ is the strength of preference for tag $a$ over\ntag $b$. We estimate this quantity as the number of times $a$ occurs before $b$,\ndivided by the number of times they occur together, regardless of order. This\nallows for anti-symmetry: $p_{ba} = 1 -p_{ab}$. In this representation, $p_{ab}=.5$\nimplies there is no preference. Also note the we only need to keep track of the strongest\npreference (either $p_{ab}$, or $p_{ba}$, but not both).\n\n\n\\subsection{Maximizing Objective -- PrioritizedTopoSort}\n\nTo maximize our tagging objective, we came up with a modified version of the\ntopological sort algorithm. Since we can represent the preferences as directed\nedges with weighs as the probabilities, we can use topological sort~\\cite{toposort}\nto produce an ordering that is faithful to these preferences.\n\nTo deal with cycles, which are assumed not to exist for the topological sort\nalgorithm to work, when we find a cycle, we delete the edge of least strength.\nUnder certain mild conditions, this is guaranteed to still be optimal. Also,\nsince there could be multiple correct topological sort orderings, in order to\nbreak ties, we use a baseline ordering of the candidate tags \n(gotten from the baseline tag generator as mentioned in section\n\\ref{model:objective}), as we run the modified topological sort algorithm,\nwhenever there is an ambiguous ordering.\n\nDetails and proofs of claims made here are provided in the appendix.\n\n\n\n\n\n\\section{Experimental Setup}\n\nIn this section, we will discuss how we go about verifying our model, from \nthe choice of the dataset, choice of baseline and choice of evaluation metric. \n\n\\subsection{Dataset}\n\nFor this work we chose to work with the NUS-WIDE dataset \\cite{nus-wide-civr09}.\nThe NUS-WIDE dataset is a subset of 269,648 images from Flickr. For each image in\nthe dataset, we know, via the Flickr API, the corresponding user that uploaded\nthat image, and the sequence of tags that user chose to annotate the image with. \nSince we are particularly concerned about personalization, we only select images\nfrom this database which satisfy the following criteria: the users who uploaded\nthe image must have at least 6 images in this dataset, similar to the setting in\n\\cite{li:11}. This results in about 91,400 images from 5000 users. We split this dataset\ninto a training and test partition, by randomly assigning half of each users' images\nto the training, and half to the test set. \nFor each image we only retain the tags that occurred frequently enough across\nthe dataset, in order to make some sort of meaningful inference on the\ntags. In this work, we made the design choice of working with tags that occurred\nat least 50  times in the dataset. This results in a vocabulary of $5,326$ unique tags.\n\n\\subsubsection*{Image Similarity}\nAs we mentioned in section \\ref{sec:related}, we go the nearest neighbor route for the\ntask of image tagging. To that end our image features are a\n500-D bag of words based on SIFT descriptors \\cite{lowe:04}, and we use the\neuclidean distance between feature vectors to encode the notion of closeness or\nsimilarity.\n\n\\subsection{Baseline}\n\\label{exp:baseline}\n\nWe take as our baseline the work of Li et al. \\cite{li:11}, which\nwe considered the reproducable state of the art for \\emph{personalized}\nimage tagging and evaluated their claims on the NUS-WIDE dataset as well. \nTheir main idea\nis that for a given tag, each user has 2 weighting variables, one weighting\nhow much to rank that tag according to its frequency independent of visual\ncontent, and the other how much to rank the frequency of the tag according\nto its frequency in visually similar images. We re-implemented\ntheir method using the two tagging functions described in their paper (\\textit{PersonalPreference}\n\\cite{sawant:10} function and \\textit{Visual}\\cite{li:10} function). For the visual \nfeatures, we also use the 500-D bag of words based on SIFT descriptors for\nconsistency.\n\nFor a given image query $q$, we define the ordered set of tags returned by the\nbaseline method as $XE(q)$.\n\n\\subsection{Metrics}\n\\label{exp:metrics}\nSince our assumption is that the order that a user tags an image important for\nautomatic tagging, our metric should take the groundtruth user order into account. \n\nMore concretely, given an ordered set of tags,$\\{t_1,\n\\cdots,t_k\\}$, we define the relevance of each tag according to it's reciprocal\nrank in the ordered set:\n", "index": 1, "text": "\n\\[\n\trel(t_i) = \\frac{1}{rank^*(t_i)},\\ \\forall t_ i\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"rel(t_{i})=\\frac{1}{rank^{*}(t_{i})},\\ \\forall t_{i}\" display=\"block\"><mrow><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><msup><mi>k</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo rspace=\"7.5pt\">,</mo><mrow><mo>\u2200</mo><msub><mi>t</mi><mi>i</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06439.tex", "nexttext": "\n\nThis metric is called \\emph{discounted} because the later we include a tag in our ranking,\nthe less gain we get from it (i.e. its relevance is discounted by the inverse of\nthe log of its position in the ranking, not the groundtruth). Note that this metric is\nmaximal when the most relevant items are listed first.\n\nWe also define for a given ranked list, $T$, its ideal ranking $\\bar{\\sigma}(T)$,\nsuch that for $x,y \\in T$, if $\\bar{\\sigma}^{-1}(x) < \\bar{\\sigma}^{-1}(y)$, \nthen $x \\succ y$ in the groundtruth (i.e., the ideal ranking is ranked from most \nrelevant to least). Then the normalized DCG is defined as:\n", "itemtype": "equation", "pos": 12320, "prevtext": "\nTags not in the groundtruth ordered set have zero relevance.\nMore common metrics, such as precision, recall, and average precision, assume\nthat all tags are equally relevant, so we do not utilize these metrics in this\npaper. Instead we use the more appropriate \\emph{normalized discounted \ncumulative gain} (nDCG), a common metric used in evaluating\nsearch engine results. For an ordered set $T = \\{t_1,\\cdots,t_k\\}$, such that\n$i < j \\rightarrow t_i \\succ t_j$, we define the DCG with respect to the \nground-truth as:\n\n\n", "index": 3, "text": "\\begin{equation}\n\tDCG(T) = rel(t_1) + \\sum\\limits_{t_i \\in T, i\\not=1} \\frac{rel(t_i)}{\\log_2(i)}\n\t\\label{eqtn:dcg}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"DCG(T)=rel(t_{1})+\\sum\\limits_{t_{i}\\in T,i\\not=1}\\frac{rel(t_{i})}{\\log_{2}(i)}\" display=\"block\"><mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>\u2208</mo><mi>T</mi></mrow><mo>,</mo><mrow><mi>i</mi><mo>\u2260</mo><mn>1</mn></mrow></mrow></munder><mfrac><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06439.tex", "nexttext": "\n\nNote that the nDCG is maximal (equal to 1) when $T=\\bar{\\sigma}(T)$. We can also \nparameterize the DCG, and nDCG to calculate the $DCG@k$ and $nDCG@k$. That is, \ncalculate the metrics evaluated only for the first $k$ entries of the ranked lists. Let\n$T[:k]$ be the first k entries of $T$, then:\n", "itemtype": "equation", "pos": 13067, "prevtext": "\n\nThis metric is called \\emph{discounted} because the later we include a tag in our ranking,\nthe less gain we get from it (i.e. its relevance is discounted by the inverse of\nthe log of its position in the ranking, not the groundtruth). Note that this metric is\nmaximal when the most relevant items are listed first.\n\nWe also define for a given ranked list, $T$, its ideal ranking $\\bar{\\sigma}(T)$,\nsuch that for $x,y \\in T$, if $\\bar{\\sigma}^{-1}(x) < \\bar{\\sigma}^{-1}(y)$, \nthen $x \\succ y$ in the groundtruth (i.e., the ideal ranking is ranked from most \nrelevant to least). Then the normalized DCG is defined as:\n", "index": 5, "text": "\n\\[\n\tnDCG(T) = \\dfrac{DCG(T)}{DCG(\\bar{\\sigma}(T))}\n\t\\label{eqtn:ndcg}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"nDCG(T)=\\dfrac{DCG(T)}{DCG(\\bar{\\sigma}(T))}\" display=\"block\"><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>\u03c3</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.06439.tex", "nexttext": "\n\n\n\n\n\\section{Results}\n\n\n\n\n\n\n\n\\subsection{Task}\n\nGiven a query image $q$ from \nthe test set, we find the $N$ nearest neighbors (visual similarity) from the \ntraining set, and given some initial rank $\\tilde{T}$ on the set of tags used to \ndescribe these $N$ visually similar images, we find optimal ranking, \n$\\sigma^*(\\tilde{T})$, as described in section~\\ref{model:objective}, \nusing the preference probabilities derived\nfrom the training set according to section~\\ref{model: parameter_estimation}.\nWe set the baseline order of candidate tags as $\\tilde{T} = XE(q)$, which is the\nresult of our baseline (and the current state-of-art for personalization)\ngiven the query $q$, from section \\ref{exp:baseline}. \n\n\n\\subsection{Design Decisions}\n\\label{res:design_dec}\n\nIn evaluating our approach, we made a few \ndesign decisions which are parameters in our model. The first\nis the minimum number of times tags $a$ and $b$ occur together, which we\ndenote as $C$. If any pair of tags don't co-occur at least $C$ times,\nwe do not have a high confidence in the order bias that we estimate from\nthe occurrences because of noise and overfitting. The second which we denote\nas $R$ is the minimum strength factor between a pair of tags. That is,\nwe ensure that, $p_{ab} \\geq R.p_{ba}$, for $p_{ab}$ to be an edge to be\nconsidered by the PrioritizedTopoSort. The purpose of these parameters\nis to control for noise and overfitting to insufficient data.\n\n\n\\subsection{Evaluations}\n \n In this section, we describe the tests with which we evaluate our method.\n For the evaluation metrics discussed in section \\ref{exp:metrics}, we\n run evaluations under the following settings:\n \n \\begin{itemize}\n \t\\item Number of visually similar neighbors: $\\{50,100\\}$\n \t\\item Minimum co-occurrence, C: $\\{5,10,30\\}$\n \t\\item Minimum strength factor, R: $\\{5,10,30\\}$\n \\end{itemize}\n \n We compare our performance (personalized pairwise preference), to the\n state-of-art~\\cite{li:11}, and also to global pairwise preferences. Global\n pairwise preferences is similar to what we have proposed so far in this \n paper with the exception of ignoring the users, so the images are treated\n as though they all come from the ``average user''. \n \n We provide evaluation for our metrics on entire ranks, and also the top\n $k$ tags in the rank for $k \\in \\{1,5,10,15,30\\}$. \n \n We show the mean performance averaged both over each image (included in the \n appendix), and also average across its mean performance per user since we \n are interested in personalization.\n \n We also calculate the number of times our approach produces better rankings\n (in terms of NDCG), than the baseline, and the number of times a user prefers\n our approach on average over the baseline. We report these numbers in Table \n\\ref{tab:duels}.\n \n \n\n\\subsection{Observations}\n\n\\begin{table}[t]\n  \\begin{center}\n    \\begin{tabular}{| l || c | c |}\n    \\hline\n              & NN = 100  & NN = 50 \\\\\n    \\hline\\hline\n    Per User  & 5.5\\%     & 4.8\\%       \\\\\n    \\hline\n    Per Image & 7.3\\%     &  6.2\\%  \\\\\n    \\hline\n    \\end{tabular}\n  \\end{center}\n  \\caption{Average NDCG percentage improvement over the baseline using pairwise\n  personal preferences}\n  \\label{tab:avg_stats}\n\\end{table}\n\n\\begin{table}[t]\n  \\begin{center}\n    \\begin{tabular}{| l || c | c |}\n    \\hline\n              & NN = 100  & NN = 50 \\\\\n    \\hline\\hline\n    Per User  & 1.11    & 1.10       \\\\\n    \\hline\n    Per Image & 1.13     & 1.09  \\\\\n    \\hline\n    \\end{tabular}\n  \\end{center}\n  \\caption{Average number of times our approach is preferred to the baseline. Our\n  approach is preferred about 10\\% more of the time than the baseline (state-of-art).\n  }\n  \\label{tab:duels}\n\\end{table}\n\nFrom \\figurename~\\ref{fig:usr_ndcg_all} and \\ref{fig:ndcg_k_usr}, we notice that\nour algorithm which personalizes via pairwise tag preferences (red bar/line)\nalways outperforms the state-of-the-art baseline \\cite{li:11} (blue bar/line).\n\n\n\n\\figurename~\\ref{fig:usr_ndcg_all} shows this increase in\nperformance for all parameters $R$ (the minimum strength factor described in \nsection \\ref{res:design_dec}), and $C$ (the minimum number of co-occurrences), \nthat we considered. We notice that holding $C$ constant\nas the parameter $R$ increases, the performance of our personalization method\ndecreases. This is likely due to a lack of enough data per user so that very\nfew tag pairs can meet our relatively aggressive over-fitting criteria. The\nsame trend is noticed when we hold $R$ constant and increase $C$.\n\n \\begin{figure*}[ht!]\n \\centering\n  \\includegraphics[height=.27\\textheight,width=.9\\textwidth]{user_ndcg_all.eps}\n  \\caption{This figure shows the mean NDCG per \\emph{user}, for 100 visual neighbors\n  on the left, and 50 visual neighbors on the right. The performance is similar\n  for both. Each bar on the x-axis corresponds to specific settings of the R and C\n  parameters. The default ordering $DO$ is the baseline $XE$~\\cite{li:11}}\n  \\label{fig:usr_ndcg_all}\n \\end{figure*} \n \n \\begin{figure*}[ht!]\n \\centering\n  \\includegraphics[height=.27\\textheight,width=.9\\textwidth]{ndcg_at_k_user.eps}\n  \\caption{This figure shows the mean NDCG@K per \\emph{user}, for 100 visual neighbors\n  on the left, and 50 visual neighbors on the right. The performance is similar\n  for both. The x-axis corresponds to K.}\n  \\label{fig:ndcg_k_usr}\n \\end{figure*}\n\nWe also evaluate the performance of the global (or ``average user'', yellow bar/line), pairwise\npreferences, and we see that enforcing pairwise orders based on a global/average\nnotion of preference actually degrades the performance. Except for $K=1$,\nthe $nDCG@K$ metric for the global preferences is worse than the baseline.\nThis might be because, users tend to use more global/popular tags in the\nbeginning of their ranked lists, so that we are actually able to learn with\nenough data, certain global biases among the popular tags. Beyond that, because\nusers tag differently, enforcing a global preference is actually counterproductive.\nWe also observe the reverse trend as we fix $C$ and vary $R$ (and vice-versa) to that\nof the personal preferences. This is because to estimate global preferences\nwe need to observe a lot more data than for a single user, and so estimating\nfor pairs without a lot of co-occurrences will invariably lead to over-fitting.\n\n\\figurename~\\ref{fig:ndcg_k_usr}, shows the \n$nDCG@K$ as $K$ varies under the best choices of parameters\n$R$ and $C$, for both the global and personal preferences. The best choices\nfor the global were, $R=C=30$, and for the personal, $R=C=5$. We see, averaging\nover both users and images(included in the appendix), that the baseline outperforms the global (except at\n$K=1$), and the personal outperforms the baseline. These observations imply that\npicking lower parameters is usually better for the personal preferences, while\nhigher values of $R$ and $C$ are necessary for the global preferences to be useful.\n\nTable \\ref{tab:avg_stats} shows that our method on average is $6\\%$ better\nthan the baseline method, and this improvement goes up to about $30\\%$,\nfor $nDCG@1$, as can be seen from Figure~\\ref{fig:ndcg_k_usr}.\n\nThe above observations are true for both the average across all the images\nin our test set, and across all the users' mean $nDCG$. This shows that the\ngains we get are relatively consistent for each user. This combined with\nthe fact that the global pairwise preferences under-perform, imply that \ntrue personal pairwise preferences indeed exist, can be estimated, and can\nbe used to improve the performance of automatic image tagging. \n\nWe include more detailed plots in the supplemntary appendix.\n\n \n\n\n\\section{Conclusion}\n\nIn this work we proposed a new measurement of tag preferences, and\ndemonstrated that there is indeed a tag-order bias, that is, when a user\nmentions tag $a$ before tag $b$, in a list of tags for a given image, the\nuser is implying that he prefers, or considers $a$ to be of greater importance\nthan $b$. We showed that this bias can be learned from historical data using\nthe maximum likelihood estimate based on a pair's co-occurrence, and subsequently \nshowed that such information can be exploited to improve the performance of \ncurrent state-of-art automated image tagging systems. \n\nWe also defined a new tagging objective function that assumes the inherent\npairwise bias between tags, and provided an algorithm which optimizes the\nnew objective (under some mild conditions), and helped verify our claims and\nassumptions.\n\nThis leads us to conclude that although there are many visual factors\nthat may affect what tags a user will provide for an image, it is useful\nto characterize instead (or rather in conjunction) the users' tagging\nhabits to learn what tags are of more importance to the users, whether\nthey are visually motivated or not, and automatic tagging systems should\nemploy this technique to improve their overall performance.\n\n\n\n\\section{Future Work}\n\nWe believe that there are several ways that this work could be extended or\nextend other works. One direction we see is working on algorithms that \nprovide tight guarantees for solving our tagging objective, or even solves it\noptimally, even in the presence of dependent cycles. Another direction comes\nfrom the observation that most tag pairs never occur, but it may be possible\nto learn a function that maps a pair of tags to a real number indicating the \ndirection and strength of the preference. It would also be interesting to \nsee how the assumptions made in this paper can be used to extend the \nworks on learning for personalized ranking \\cite{rendle:09, rendle:10, pan:13}.\n\nWe think it would also be interesting to explore the cognitive dimensions that\ndrive tag ordering, and how these cognitive dimensions contribute to the tagging\nchoices both independently and collectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEbib}\n\\small{\n\\bibliography{icme16_bib}  \n}\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 13436, "prevtext": "\n\nNote that the nDCG is maximal (equal to 1) when $T=\\bar{\\sigma}(T)$. We can also \nparameterize the DCG, and nDCG to calculate the $DCG@k$ and $nDCG@k$. That is, \ncalculate the metrics evaluated only for the first $k$ entries of the ranked lists. Let\n$T[:k]$ be the first k entries of $T$, then:\n", "index": 7, "text": "\n\\[\n  nDCG@k(T) = \\dfrac{DCG(T[:k])}{DCG(\\bar{\\sigma}(T)[:k])}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"nDCG@k(T)=\\dfrac{DCG(T[:k])}{DCG(\\bar{\\sigma}(T)[:k])}\" display=\"block\"><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mi mathvariant=\"normal\">@</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>D</mi><mi>C</mi><mi>G</mi><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mrow><mo stretchy=\"false\">[</mo><mo>:</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>D</mi><mi>C</mi><mi>G</mi><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03c3</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">[</mo><mo>:</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}]