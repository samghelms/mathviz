[{"file": "1601.02698.tex", "nexttext": "\n\nHere $p(\\cdot)$ a prior distribution for parameter vector $\\theta$, which may itself have one or more levels of stochastic interdependence. The distribution of each HMM initial state $x_{i1}$ is $f_{i1}(\\cdot {\\, | \\,} \\theta)$.  Markov state transition probabilities are given by $f_{it}(\\cdot {\\, | \\,} \\theta, x_{i, t-1})$ and observation probabilities by $g_{it}(\\cdot {\\, | \\,} \\theta, x_{it})$.\n\nDiscrete HMMs have long been applied in the area of ecological capture-recapture \\citep[\\emph{e.g.},][]{Gimenez2007, king2012review, langrock2012flexible}. In this context, a set of $n$ distinct animals is monitored for $k$ sampling occasions.  Each $y_i$ represents the observation history of animal $i$, for $i = 1, \\ldots, n$, which can be modeled using HMMs as in~(\\ref{eqn:HMM}).  The set of possible observations $\\mathcal{Y}$ may include a state to represent ``unobserved\".  Since all $n$ animals are not typically observed on occasion $t=1$, each embedded HMM will ``begin\" at the sampling period corresponding to the first genuine observation of that animal. \n\n\n\\subsection{Model Likelihood}\n\nWe now provide the model likelihood for the general HMM formulation in~(\\ref{eqn:HMM}), which is used in the Bayesian estimation procedures described next.  We begin with the likelihood contribution from a single observation history,\n\n", "itemtype": "equation", "pos": 8463, "prevtext": "\n\n\\title{Efficient Markov Chain Monte Carlo Sampling for Hierarchical Hidden Markov Models}\n\\author{Daniel Turek$^{*}$, Perry de Valpine, and Christopher J. Paciorek}\n\\date{}\n\\maketitle\n\\thispagestyle{empty}\n\n\\vspace{0.2in}\n\n\\begin{center}\n$^*$Corresponding author \\\\\n\nUniversity of California, Berkeley \\\\\n493 Evans Hall, Berkeley, CA 94720, USA \\\\\ndturek@berkeley.edu \\\\\n\\end{center}\n\n\\vspace{0.5in}\n\n\\begin{abstract}   \nTraditional Markov chain Monte Carlo (MCMC) sampling of hidden Markov models (HMMs) involves latent states underlying an imperfect observation process, and generates posterior samples for top-level parameters concurrently with nuisance latent variables.  When potentially many HMMs are embedded within a hierarchical model, this can result in prohibitively long MCMC runtimes.  We study combinations of existing methods, which are shown to vastly improve computational efficiency for these hierarchical models while maintaining the modeling flexibility provided by embedded HMMs.  The methods include discrete filtering of the HMM likelihood to remove latent states, reduced data representations, and a novel procedure for dynamic block sampling of posterior dimensions.  The first two methods have been used in isolation in existing application-specific software, but are not generally available for incorporation in arbitrary model structures.  Using the NIMBLE package for R, we develop and test combined computational approaches using three examples from ecological capture-recapture, although our methods are generally applicable to any embedded discrete HMMs.  These combinations provide several orders of magnitude improvement in MCMC sampling efficiency, defined as the rate of generating effectively independent posterior samples.  In addition to being computationally significant for this class of hierarchical models, this result underscores the potential for vast improvements to MCMC sampling efficiency which can result from combinations of known algorithms.\n\\end{abstract}   \n\n\\vspace{0.5in}\n\n\\textbf{Keywords:} \\\\\n\\indent \\indent Capture-recapture, Effective sample size, Hidden Markov model, Hierarchical model, MCMC, NIMBLE, Sampling efficiency\n\n\\thispagestyle{empty}\n\\newpage\n\n\n\n\\section{Introduction}\n\nHidden Markov models (HMMs) are widely applied for the analysis of time series data with incomplete or noisy observations together with stochastic system dynamics \\citep{cappe2006inference, elliott2008hidden}.  HMMs are used in a diverse range of application domains, with recent attention in areas of speech recognition and natural language processing \\citep{gales2008application}.  See \\citet{macdonald1997hidden} for a broad review of HMM applications in disciplines such as as medicine, finance, sociology, and climatology.\n\nFor a single discrete HMM, likelihood calculation involves summing over the distribution of a sequence of unknown latent states.  This can be implemented either using standard direct filtering summations \\citep[\\emph{e.g.},][chapter 2]{elliott2008hidden} as part of either maximum likelihood or Bayesian analysis, or using Markov chain Monte Carlo \\citep[MCMC;][]{Gilks2005, Brooks2011} for Bayesian analysis.  In the case of MCMC, the unknown state variables are included in MCMC sampling.  However, it is often the case that one or more HMMs are embedded in a larger hierarchical model, perhaps accounting for explanatory variables of state transition probabilities or shared variation among multiple time series.  In such cases practitioners may rely on MCMC to perform a Bayesian analysis, but they face a quandary of computational efficiency.  If they use standard MCMC software, they often have no choice to but to include the unknown latent state variables in MCMC sampling.  For large models this can contribute hundreds or thousands of dimensions which require MCMC sampling, to the point of rendering this approach computationally impractical.\n\nIn theory there are computational tradeoffs between using MCMC and direct filtering summation when embedding HMMs in a larger hierarchical model, but these tradeoffs have not been explored to date.  Here we do so, by considering combinations of several existing computational methods for fitting HMMs.  These methods include direct filtering to remove latent variables, using a reduced representation of observational data, and dynamic blocking of model parameters to achieve efficient MCMC sampling.  We demonstrate that for large models, a combination of these techniques can yield several orders of magnitude improvement in sampling efficiency.  This can make the analysis of such models practical, opening new possibilities for fitting complex hierarchical models.\n\nAs examples we draw upon capture-recapture and from ecological statistics \\citep[for a broad review, see][]{lebreton2009modeling}.  In capture-recapture, each animal in a study generates a capture history over multiple observational periods.  These data can be modeled using discrete HMMs, where latent states may simply represent ``alive\" or ``dead\", or in the case of multistate capture-recapture, are more detailed such as including reproductive status or location.  We present a series of three examples of increasing complexity to study the tradeoffs in computational cost and MCMC mixing of several methodological approaches.  Our examples include a simple Cormack-Jolly-Seber capture-recapture model (``Dipper\"), a simple multistate model (``Orchid\"), and a larger multistate model with thousands of embedded HMMs (``Goose\").\n\nSome of the techniques we study are already supported in existing software, however only for specific applications or particular hierarchical structures.  The standalone program MARK \\citep{white1999program} is perhaps the industry leader for applied capture-recapture.  MARK provides an application-specific MCMC algorithm for fitting multistate random effects capture-recapture models, which implements filtering over latent states to directly calculate model likelihoods.  MARK also supports a reduced representation of datasets with repeated observations -- known as an ``m-array\" in capture-recapture -- however only for band-recovery analyses \\citep{brownie1985statistical}.  More recently, M-SURGE \\citep{choquet2004m} was developed specifically for multistate capture-recapture.  M-SURGE supports numerical integration to remove latent states, although this is used exclusively for maximum likelihood estimation, and never in combination with MCMC.  Furthermore, neither of these software programs expose these computational techniques for user control, nor are they applicable outside the domain of ecological capture-recapture.\n\nWe make use of the NIMBLE software for specifying hierarchical models and statistical algorithms \\citep{nimble-software:2015} to generalize these computational approaches for embedded HMMs.  We consider particular combinations of techniques using the flexible and transparent algorithmic control provided by NIMBLE.  Although we draw upon capture-recapture for examples, our advances in efficient handling of HMMs can be embedded in any larger hierarchical model structure using NIMBLE.  However, we focus attention on the computational methodologies rather than implementation details.  For comparisons of interest we also include the widely used JAGS package \\citep{Plummer2003} for MCMC.\n\n\n\n\\section{Computational Approaches to Discrete HMMs}\n\nWe begin with a general specification of discrete HMMs, and explain how multistate capture-recapture models may be framed in this context.  We then provide the model likelihood, and present a variety of approaches to computing it in the context of MCMC estimation.\n\n\\subsection{Discrete HMMs and Multistate Capture-Recapture}\n\nLet $y_i = (y_{i1}, \\ldots, y_{ik})$ represent the $i^\\text{th}$ sequence of observations taken over sampling occasions $t = 1, \\ldots, k$.  Each $y_{it} \\in \\mathcal{Y}$, where $\\mathcal{Y}$ is the finite set of possible observations.  Similarly, let $x_i = (x_{i1}, \\ldots, x_{ik})$ be the sequence of true underlying states at occasions $t = 1, \\ldots, k$, with $x_{it} \\in \\mathcal{X}$ for finite set of states $\\mathcal{X}$.  We will consider a total of $n$ observed sequences, hence the full data set is $y = (y_1, \\ldots, y_n)$.  Finally, let $\\theta$ be a vector of all model parameters, which may also include random effects.  Letting $i$ take all values in $1, \\ldots, n$, the general hierarchical model is\n\n", "index": 1, "text": "\\begin{equation} \\label{eqn:HMM}\n\\begin{split}\n\\Theta & \\sim p(\\theta) \\\\\nX_{i1} & \\sim f_{i1}(x_{i1} {\\, | \\,} \\theta) \\\\\nX_{it} {\\, | \\,} X_{i,t-1} & \\sim f_{it}(x_{it} {\\, | \\,} \\theta, x_{i, t-1}), \\hspace{12pt}  t = 2, \\ldots, k \\\\\nY_{it} {\\, | \\,} X_{it} & \\sim g_{it}(y_{it} {\\, | \\,} \\theta, x_{it}),          \\hspace{26pt} t = 1, \\ldots, k \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\Theta&amp;\\displaystyle\\sim p(\\theta)\\\\&#10;\\displaystyle X_{i1}&amp;\\displaystyle\\sim f_{i1}(x_{i1}{\\,|\\,}\\theta)\\\\&#10;\\displaystyle X_{it}{\\,|\\,}X_{i,t-1}&amp;\\displaystyle\\sim f_{it}(x_{it}{\\,|\\,}%&#10;\\theta,x_{i,t-1}),\\hskip 12.0ptt=2,\\ldots,k\\\\&#10;\\displaystyle Y_{it}{\\,|\\,}X_{it}&amp;\\displaystyle\\sim g_{it}(y_{it}{\\,|\\,}\\theta%&#10;,x_{it}),\\hskip 26.0ptt=1,\\ldots,k\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mi mathvariant=\"normal\">\u0398</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>\u223c</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><msub><mi>f</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><msub><mi>f</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo rspace=\"14.5pt\">,</mo><mi>t</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><msub><mi>g</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo rspace=\"28.5pt\">,</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nwhere $\\mathcal{X}^k$ denotes the standard $k$-fold Cartesian product of $\\mathcal{X}$.  Using the likelihood components in~(\\ref{eqn:Li}), the total model likelihood of $y$ is\n\n", "itemtype": "equation", "pos": 10181, "prevtext": "\n\nHere $p(\\cdot)$ a prior distribution for parameter vector $\\theta$, which may itself have one or more levels of stochastic interdependence. The distribution of each HMM initial state $x_{i1}$ is $f_{i1}(\\cdot {\\, | \\,} \\theta)$.  Markov state transition probabilities are given by $f_{it}(\\cdot {\\, | \\,} \\theta, x_{i, t-1})$ and observation probabilities by $g_{it}(\\cdot {\\, | \\,} \\theta, x_{it})$.\n\nDiscrete HMMs have long been applied in the area of ecological capture-recapture \\citep[\\emph{e.g.},][]{Gimenez2007, king2012review, langrock2012flexible}. In this context, a set of $n$ distinct animals is monitored for $k$ sampling occasions.  Each $y_i$ represents the observation history of animal $i$, for $i = 1, \\ldots, n$, which can be modeled using HMMs as in~(\\ref{eqn:HMM}).  The set of possible observations $\\mathcal{Y}$ may include a state to represent ``unobserved\".  Since all $n$ animals are not typically observed on occasion $t=1$, each embedded HMM will ``begin\" at the sampling period corresponding to the first genuine observation of that animal. \n\n\n\\subsection{Model Likelihood}\n\nWe now provide the model likelihood for the general HMM formulation in~(\\ref{eqn:HMM}), which is used in the Bayesian estimation procedures described next.  We begin with the likelihood contribution from a single observation history,\n\n", "index": 3, "text": "\\begin{equation} \\label{eqn:Li}\n\\begin{split}\nL(\\theta {\\, | \\,} y_i) & = \\sum_{x_i \\in \\mathcal{X}^k} f_{i1}(x_{i1} {\\, | \\,} \\theta) \\left(\\prod_{t=2}^{k} f_{it}(x_{it} {\\, | \\,} \\theta, x_{i,t-1})\\right) \\left(\\prod_{t=1}^{k} g_{it}(y_{it} {\\, | \\,} \\theta, x_{it})\\right), \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle L(\\theta{\\,|\\,}y_{i})&amp;\\displaystyle=\\sum_{x_{i}\\in%&#10;\\mathcal{X}^{k}}f_{i1}(x_{i1}{\\,|\\,}\\theta)\\left(\\prod_{t=2}^{k}f_{it}(x_{it}{%&#10;\\,|\\,}\\theta,x_{i,t-1})\\right)\\left(\\prod_{t=1}^{k}g_{it}(y_{it}{\\,|\\,}\\theta,%&#10;x_{it})\\right),\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>L</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mi>k</mi></msup></mrow></munder><msub><mi>f</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><msub><mi>f</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>g</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\n\n\\subsection{Computational Approaches}\n\nWe now describe several computational approaches to applying Bayesian estimation to embedded HMMs.  These strategies will form the basis for our comparisons, using examples from capture-recapture.\n\n\\subsubsection*{MCMC for latent states and parameters}\n\nOne approach to Bayesian estimation is to perform MCMC sampling of both the model parameters and latent states; that is, to sample from the full posterior distribution $p(\\theta, x {\\, | \\,} y)$.  Doing so makes use of Bayes law in the form:\n\n", "itemtype": "equation", "pos": 10665, "prevtext": "\nwhere $\\mathcal{X}^k$ denotes the standard $k$-fold Cartesian product of $\\mathcal{X}$.  Using the likelihood components in~(\\ref{eqn:Li}), the total model likelihood of $y$ is\n\n", "index": 5, "text": "\\begin{equation*} \nL(\\theta {\\, | \\,} y) = \\prod_{i = 1}^n L(\\theta {\\, | \\,} y_i).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"L(\\theta{\\,|\\,}y)=\\prod_{i=1}^{n}L(\\theta{\\,|\\,}y_{i}).\" display=\"block\"><mrow><mi>L</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\nUsing this approach the dimension of the MCMC sampling problem can be very large, since there can be up to $nk$ latent state variables.  Although we expect the MCMC update of each individual variable will be fast, since the algorithmic complexity is limited to that of standard MCMC sampling algorithms (\\emph{e.g.}, Metropolis-Hastings), there can be a large number of latent states.  In addition to the computational cost, this can result in slow MCMC mixing for latent states and parameters.\n\n\\subsubsection*{Filtering over latent states with MCMC for parameters}\n\nAn alternate approach makes use of direct filtering to calculate the likelihood contribution of each observation history.  This approach relies on the discrete HMM structure underlying each observed sequence $y_i$ in~(\\ref{eqn:HMM}).  Doing so, we may perform MCMC sampling of the posterior distribution of $\\theta$ only, rather than $(\\theta, x)$ as in the latent state MCMC, and use filtering to calculate each $p(y_i {\\, | \\,} \\theta)$ as described in \\citet{elliott2008hidden}.  The filtering MCMC approach makes use of Bayes law in the form:\n\n", "itemtype": "equation", "pos": 11302, "prevtext": "\n\n\n\\subsection{Computational Approaches}\n\nWe now describe several computational approaches to applying Bayesian estimation to embedded HMMs.  These strategies will form the basis for our comparisons, using examples from capture-recapture.\n\n\\subsubsection*{MCMC for latent states and parameters}\n\nOne approach to Bayesian estimation is to perform MCMC sampling of both the model parameters and latent states; that is, to sample from the full posterior distribution $p(\\theta, x {\\, | \\,} y)$.  Doing so makes use of Bayes law in the form:\n\n", "index": 7, "text": "\\begin{equation*}\n\\begin{split}\np(\\theta, x {\\, | \\,} y) & \\propto p(\\theta) \\; \\prod_{i = 1}^n \\; p(x_i {\\, | \\,} \\theta) \\; p(y_i {\\, | \\,} \\theta, x_i) \\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle p(\\theta,x{\\,|\\,}y)&amp;\\displaystyle\\propto p(\\theta)%&#10;\\;\\prod_{i=1}^{n}\\;p(x_{i}{\\,|\\,}\\theta)\\;p(y_{i}{\\,|\\,}\\theta,x_{i})\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo>,</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u221d</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mpadded width=\"+2.8pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mpadded><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mi>i</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>i</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\nFor a general discrete HMM as specified in~(\\ref{eqn:HMM}), the filtering likelihood calculation proceeds as follows.  Everything pertains to the $i^{th}$ observation history $y_i$ and we omit subscripts~$i$.  All probabilities are conditional on $\\theta$, and we use $y_{1:t}$ to represent $y_1, \\ldots, y_t$.  We begin by defining distributions for the latent state at each time step, and the conditional likelihood:\n\n", "itemtype": "equation", "pos": 12604, "prevtext": "\n\nUsing this approach the dimension of the MCMC sampling problem can be very large, since there can be up to $nk$ latent state variables.  Although we expect the MCMC update of each individual variable will be fast, since the algorithmic complexity is limited to that of standard MCMC sampling algorithms (\\emph{e.g.}, Metropolis-Hastings), there can be a large number of latent states.  In addition to the computational cost, this can result in slow MCMC mixing for latent states and parameters.\n\n\\subsubsection*{Filtering over latent states with MCMC for parameters}\n\nAn alternate approach makes use of direct filtering to calculate the likelihood contribution of each observation history.  This approach relies on the discrete HMM structure underlying each observed sequence $y_i$ in~(\\ref{eqn:HMM}).  Doing so, we may perform MCMC sampling of the posterior distribution of $\\theta$ only, rather than $(\\theta, x)$ as in the latent state MCMC, and use filtering to calculate each $p(y_i {\\, | \\,} \\theta)$ as described in \\citet{elliott2008hidden}.  The filtering MCMC approach makes use of Bayes law in the form:\n\n", "index": 9, "text": "\\begin{equation} \\label{eqn:filterBayes}\n\\begin{split}\np(\\theta {\\, | \\,} y) & \\propto p(\\theta) \\; \\prod_{i = 1}^n \\; p(y_i {\\, | \\,} \\theta) \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle p(\\theta{\\,|\\,}y)&amp;\\displaystyle\\propto p(\\theta)\\;%&#10;\\prod_{i=1}^{n}\\;p(y_{i}{\\,|\\,}\\theta)\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u221d</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mpadded width=\"+2.8pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mpadded><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>i</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\nMapping the elements of $\\mathcal{X}$ to the indices $\\{1, 2, \\ldots, |\\mathcal{X}|\\}$, a bijection, we express each $P_t$ and $Q_t$ as column vectors of length $|\\mathcal{X}|$.  Define $|\\mathcal{X}| \\times |\\mathcal{X}|$ state transition matrices $T_t$ as having $(i,j)$ element $\\text{Pr}(X_t=i {\\, | \\,} X_{t-1}=j)$.  Similarly, define $|\\mathcal{Y}| \\times |\\mathcal{X}|$ observation matrices $Z_t$ with $(i,j)$ element $\\text{Pr}(Y_t=i {\\, | \\,} X_{t}=j)$.  The elements of each $T_t$ and $Z_t$ are defined by $f_t$ and $g_t$, respectively, from~(\\ref{eqn:HMM}). We rewrite~(\\ref{eqn:filterMCMC}) in matrix form as\n\n", "itemtype": "equation", "pos": 13197, "prevtext": "\n\nFor a general discrete HMM as specified in~(\\ref{eqn:HMM}), the filtering likelihood calculation proceeds as follows.  Everything pertains to the $i^{th}$ observation history $y_i$ and we omit subscripts~$i$.  All probabilities are conditional on $\\theta$, and we use $y_{1:t}$ to represent $y_1, \\ldots, y_t$.  We begin by defining distributions for the latent state at each time step, and the conditional likelihood:\n\n", "index": 11, "text": "\\begin{equation} \\label{eqn:filterMCMC}\n\\begin{split}\nP_t(x) & = \\text{Pr}(X_t=x {\\, | \\,} y_{1:t-1}) \\\\\n& = \\sum_{x_{t-1} \\in \\mathcal{X}} \\, \\text{Pr}(X_t=x {\\, | \\,} X_{t-1}=x_{t-1}) \\, \\text{Pr}(X_{t-1}=x_{t-1} {\\, | \\,} y_{1:t-1}) \\\\\n\\\\\nQ_t(x) &= \\text{Pr}(X_t=x {\\, | \\,} y_{1:t}) \\\\\n& = \\text{Pr}(X_t=x {\\, | \\,} y_{1:t-1}) \\, \\text{Pr}(Y_t=y_t {\\, | \\,} X_t=x) / \\text{Pr}(Y_t=y_t {\\, | \\,} y_{1:t-1}) \\\\\n\\\\\nL_t & = \\text{Pr}(Y_t=y_t {\\, | \\,} y_{1:t-1}) \\\\\n& = \\sum_{x_{t} \\in \\mathcal{X}} \\, \\text{Pr}(Y_t=y_t {\\, | \\,} X_t=x_t) \\, \\text{Pr}(X_t=x_t {\\, | \\,} y_{1:t-1}) \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle P_{t}(x)&amp;\\displaystyle=\\text{Pr}(X_{t}=x{\\,|\\,}y_{1%&#10;:t-1})\\\\&#10;&amp;\\displaystyle=\\sum_{x_{t-1}\\in\\mathcal{X}}\\,\\text{Pr}(X_{t}=x{\\,|\\,}X_{t-1}=x%&#10;_{t-1})\\,\\text{Pr}(X_{t-1}=x_{t-1}{\\,|\\,}y_{1:t-1})\\\\&#10;\\\\&#10;\\displaystyle Q_{t}(x)&amp;\\displaystyle=\\text{Pr}(X_{t}=x{\\,|\\,}y_{1:t})\\\\&#10;&amp;\\displaystyle=\\text{Pr}(X_{t}=x{\\,|\\,}y_{1:t-1})\\,\\text{Pr}(Y_{t}=y_{t}{\\,|\\,%&#10;}X_{t}=x)/\\text{Pr}(Y_{t}=y_{t}{\\,|\\,}y_{1:t-1})\\\\&#10;\\\\&#10;\\displaystyle L_{t}&amp;\\displaystyle=\\text{Pr}(Y_{t}=y_{t}{\\,|\\,}y_{1:t-1})\\\\&#10;&amp;\\displaystyle=\\sum_{x_{t}\\in\\mathcal{X}}\\,\\text{Pr}(Y_{t}=y_{t}{\\,|\\,}X_{t}=x%&#10;_{t})\\,\\text{Pr}(X_{t}=x_{t}{\\,|\\,}y_{1:t-1})\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><msub><mi>P</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mpadded><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/></mtr><mtr><mtd columnalign=\"right\"><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>t</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>t</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/></mtr><mtr><mtd columnalign=\"right\"><msub><mi>L</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>t</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mpadded width=\"+1.7pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mpadded><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>t</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mtext>Pr</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>x</mi><mi>t</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nwhere $A(i)$ is the $i^{th}$ row of $A$, $A^\\prime$ denotes matrix transposition, and $*$ represents element-wise multiplication.  The initial latent state distribution $P_1$ is specified by $f_{1}$ from the model specification~(\\ref{eqn:HMM}), and all other $P_t$, $Q_t$, and $L_t$ terms are iteratively calculated using~(\\ref{eqn:filterMCMCmatrix}).  The desired likelihood is calculated as $L(\\theta {\\, | \\,} y) = L_{1} L_{2} \\cdots L_{k}$.  In related works \\citep[\\emph{e.g.},][]{kery_bayesian_2012corrected} $T_t$ and $Z_t$ may be transposed, resulting only in notational changes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simplification of this filtering algorithm is possible for the case of single-state capture-recapture with one absorbing state.  Once an animal is deceased, it is guaranteed to remain in that state thereafter, where $\\mathcal{X} = \\{\\text{``alive\"}, \\text{``dead\"}\\}$ and $\\mathcal{Y} = \\{\\text{``seen\"}, \\text{``not seen\"}\\}$.  In this context we can express the likelihood of a capture history in terms of survival probabilities $\\phi_t = \\text{Pr}(X_t=\\text{``alive\"} {\\, | \\,} X_{t-1}=\\text{``alive\"})$ and detection probabilities $p_t = \\text{Pr}(Y_t=\\text{``seen\"} {\\, | \\,} X_t=\\text{``alive\"})$ as\n\n", "itemtype": "equation", "pos": 14430, "prevtext": "\n\nMapping the elements of $\\mathcal{X}$ to the indices $\\{1, 2, \\ldots, |\\mathcal{X}|\\}$, a bijection, we express each $P_t$ and $Q_t$ as column vectors of length $|\\mathcal{X}|$.  Define $|\\mathcal{X}| \\times |\\mathcal{X}|$ state transition matrices $T_t$ as having $(i,j)$ element $\\text{Pr}(X_t=i {\\, | \\,} X_{t-1}=j)$.  Similarly, define $|\\mathcal{Y}| \\times |\\mathcal{X}|$ observation matrices $Z_t$ with $(i,j)$ element $\\text{Pr}(Y_t=i {\\, | \\,} X_{t}=j)$.  The elements of each $T_t$ and $Z_t$ are defined by $f_t$ and $g_t$, respectively, from~(\\ref{eqn:HMM}). We rewrite~(\\ref{eqn:filterMCMC}) in matrix form as\n\n", "index": 13, "text": "\\begin{equation} \\label{eqn:filterMCMCmatrix}\n\\begin{split}\nP_t & = T_t \\, Q_{t-1}, \\hspace{56pt} t \\geq 2 \\\\\nQ_t & = Z_t(y_t)^{\\prime} * P_t \\, / \\, L_t, \\hspace{15pt} t \\geq 1 \\\\\nL_t & = Z_t(y_t) \\, P_t, \\hspace{49pt} t \\geq 1, \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle P_{t}&amp;\\displaystyle=T_{t}\\,Q_{t-1},\\hskip 56.0ptt%&#10;\\geq 2\\\\&#10;\\displaystyle Q_{t}&amp;\\displaystyle=Z_{t}(y_{t})^{\\prime}*P_{t}\\,/\\,L_{t},\\hskip&#10;1%&#10;5.0ptt\\geq 1\\\\&#10;\\displaystyle L_{t}&amp;\\displaystyle=Z_{t}(y_{t})\\,P_{t},\\hskip 49.0ptt\\geq 1,\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>P</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>T</mi><mi>t</mi></msub></mpadded><mo>\u2062</mo><msub><mi>Q</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo rspace=\"58.5pt\">,</mo><mrow><mi>t</mi><mo>\u2265</mo><mn>2</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>Q</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mrow><msub><mi>Z</mi><mi>t</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow><mo>*</mo><mpadded width=\"+1.7pt\"><msub><mi>P</mi><mi>t</mi></msub></mpadded></mrow><mo rspace=\"4.2pt\">/</mo><msub><mi>L</mi><mi>t</mi></msub></mrow></mrow><mo rspace=\"17.5pt\">,</mo><mrow><mi>t</mi><mo>\u2265</mo><mn>1</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>L</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>Z</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>P</mi><mi>t</mi></msub></mrow></mrow><mo rspace=\"51.5pt\">,</mo><mrow><mi>t</mi><mo>\u2265</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nwhere we numerically assign $y_t=\\text{``seen\"}$ as $y_t=1$ and $y_t=\\text{``not seen\"}$ as $y_t=0$, $t_{\\text{final}}$ is the time index of the final observed sighting (\\emph{i.e.}, $t_{\\text{final}} = \\max \\{t {\\, | \\,} y_t = 1\\}$), $\\chi_k = 1$, and $\\chi_{t} = 1 - \\phi_t + \\phi_t (1-p_t) \\chi_{t+1}$ for $t < k$ \\citep{Lebreton1992}.  Use of this simplified calculation for single-state capture-recapture will dramatically speed up likelihood evaluations relative to~(\\ref{eqn:filterMCMCmatrix}), since the likelihood is expressed in closed form.\n\nThese filtering algorithms numerically integrate over sequences of latent states to directly calculate model likelihoods, removing the need to perform MCMC sampling of these latent variables.  However, the MCMC sampling step for each component of $\\theta$ now requires application of a filtering algorithm for each observed history $y_i$.  Thus, this approach reduces the dimensionality of the MCMC sampling problem, but at the cost of increased computational complexity of each MCMC iteration.\n\n\\subsubsection*{Filtering MCMC with a reduced representation of the dataset}\n\nA further specialized approach arises when there are repeated instances of identical observation histories in the full observed dataset $y$.  That is, multiple distinct individuals exhibited identical observation histories over the $k$ observational periods.  Let $n^*$ be the number of unique observation histories in the original dataset $y$.  We define a reduced representation $(y^*, m^*)$, where $y^*$ contains the $n^*$ unique histories appearing in $y$.  An accompanying vector of multiplicities $m^*$ indicates how many times each unique history appears in the original dataset, where history $y^*_i$ occurs in $y$ a total of $m^*_i$ times, for $i = 1, \\ldots, n^*$.  \n\nUsing this reduced representation, we can express~(\\ref{eqn:filterBayes}) such that the likelihood of each unique observation history is calculated only once.  This computational approach makes use of Bayes law in the form:\n\n", "itemtype": "equation", "pos": 15906, "prevtext": "\nwhere $A(i)$ is the $i^{th}$ row of $A$, $A^\\prime$ denotes matrix transposition, and $*$ represents element-wise multiplication.  The initial latent state distribution $P_1$ is specified by $f_{1}$ from the model specification~(\\ref{eqn:HMM}), and all other $P_t$, $Q_t$, and $L_t$ terms are iteratively calculated using~(\\ref{eqn:filterMCMCmatrix}).  The desired likelihood is calculated as $L(\\theta {\\, | \\,} y) = L_{1} L_{2} \\cdots L_{k}$.  In related works \\citep[\\emph{e.g.},][]{kery_bayesian_2012corrected} $T_t$ and $Z_t$ may be transposed, resulting only in notational changes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simplification of this filtering algorithm is possible for the case of single-state capture-recapture with one absorbing state.  Once an animal is deceased, it is guaranteed to remain in that state thereafter, where $\\mathcal{X} = \\{\\text{``alive\"}, \\text{``dead\"}\\}$ and $\\mathcal{Y} = \\{\\text{``seen\"}, \\text{``not seen\"}\\}$.  In this context we can express the likelihood of a capture history in terms of survival probabilities $\\phi_t = \\text{Pr}(X_t=\\text{``alive\"} {\\, | \\,} X_{t-1}=\\text{``alive\"})$ and detection probabilities $p_t = \\text{Pr}(Y_t=\\text{``seen\"} {\\, | \\,} X_t=\\text{``alive\"})$ as\n\n", "index": 15, "text": "\\begin{equation} \\label{eqn:filterMCMCsimplified}\nL(\\theta {\\, | \\,} y) = \\left( \\prod_{t=1}^{t_{\\text{final}}-1} \\phi_t \\right) \\left( \\prod_{t=2}^{t_{\\text{final}}} p_t^{y_t}(1-p_t)^{1-y_t} \\right) \\chi_{t_{\\text{final}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"L(\\theta{\\,|\\,}y)=\\left(\\prod_{t=1}^{t_{\\text{final}}-1}\\phi_{t}\\right)\\left(%&#10;\\prod_{t=2}^{t_{\\text{final}}}p_{t}^{y_{t}}(1-p_{t})^{1-y_{t}}\\right)\\chi_{t_{%&#10;\\text{final}}},\" display=\"block\"><mrow><mi>L</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><msub><mi>t</mi><mtext>final</mtext></msub><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>)</mo></mrow><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>t</mi><mtext>final</mtext></msub></munderover><msubsup><mi>p</mi><mi>t</mi><msub><mi>y</mi><mi>t</mi></msub></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>p</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi><mi>t</mi></msub></mrow></msup><mo>)</mo></mrow><msub><mi>\u03c7</mi><msub><mi>t</mi><mtext>final</mtext></msub></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nComputing according to~(\\ref{eqn:reducedFilterMCMC}) requires only $n^*$ applications of the filtering likelihood calculation, rather than $n$ applications when using the filtering MCMC approach on the full dataset.  We expect to this provide an approximate factor of $n/n^*$ improvement in computational efficiency relative to the filtering MCMC on the original dataset.\n\n\n\\subsubsection*{Filtering MCMC with block sampling}\n\nAs a final approach, we consider joint (a.k.a. block) MCMC sampling of model parameters \\citep{Roberts1997}.  In the case of correlated posteriors, it is well known that block sampling of highly-correlated parameter dimensions can result in improved MCMC mixing \\citep[\\emph{e.g.},][]{Liu1994}.  The general problem of determining posterior dimensions for block sampling is difficult, as a practitioner cannot reliably guess what blocking arrangement will result in efficient MCMC sampling.  Further, existing literature on the efficiency of block sampling generally only considers the mixing properties of univariate versus block sampling, and fails to consider computational demands \\citep[among others]{Mengersen1996, Roberts1996a, Roberts1997a}.\n\nWe make use of NIMBLE's automated procedure for determining an efficient problem-specific block sampling MCMC algorithm, which exemplifies how the flexibility and programmability of NIMBLE facilitates a higher level of algorithmic control than other statistical software packages.  This procedure dynamically determines a partition of the model parameters which results in efficient MCMC sampling.  MCMC efficiency is defined as the number of effectively independent posterior samples generated per second of algorithm runtime, which balances improvements in MCMC mixing with computational requirements.  This automated blocking procedure is described in detail in \\citet{turek2015automated}.\n\nThe use of a block sampling strategy can be combined with filtering over latent states.  Under this approach we use the filtering algorithms already described to integrate out the latent states, and require MCMC sampling for the model parameters.  We use a dynamically determined block sampling strategy for the MCMC sampling of these parameters.\n\n\n\\section{Capture-Recapture Example Models}\n\nWe use three capture-recapture examples representing different levels of complexity to asses performance of the various computational approaches to MCMC estimation.  The first is the well-studied European Dipper dataset, demonstrating single-state capture-recapture.  The second is a multistate capture-recapture dataset of observations of a flowering orchid.  This is considered multistate data since the orchids may be observed in multiple distinct states, in addition to the possibility of ``not seen\".  The third and largest dataset is also a multistate example, representing observations of Canadian Geese at various locations.\n\n\\subsection{Dipper Model}\n\nThe European Dipper (\\emph{Cinclus cinclus}) dataset has been analyzed extensively in the literature \\citep[][among numerous others]{marzolin1988polygynie, Lebreton1992, Gimenez2007, royle_modeling_2008, amstrup_handbook_2010}, and may be considered a canonical example of capture-recapture.  For simplicity, we do not make use of a covariate reflecting gender or the distinction of flood years as in \\citet{Lebreton1992}.\n\nThe dataset consists of $n=294$ sighting histories collected over $k=7$ annual sighting occasions.  The set of latent states is $\\mathcal{X} = \\{\\text{``alive\"}, \\text{``dead\"}\\}$ and the set of observable states is $\\mathcal{Y} = \\{\\text{``seen\"}, \\text{``not seen\"}\\}$.  For computation, we use the numerical assignments $x=1$ for ``alive\", $x=0$ for ``dead\", $y=1$ for ``seen\", and $y=0$ for ``not seen\".\n\nThe model is parameterized by annual probability of survival, $\\phi$, and probability of detection, $p$, which are assumed to be constant among all sampling occasions and individuals.  This reflects the most basic Cormack-Jolly-Seber model structure \\citep{jolly1965explicit, seber1965note}, typically denoted as $\\phi(.)~ p(.)$ to imply constant probabilities of survival and detection \\citep[\\emph{e.g.},][]{nichols1983estimation}.  The hierarchical model specification is given below, which is a realization of the general structure provided in~(\\ref{eqn:HMM}), where $i$ assumes all values in $1, \\ldots, n$.\n\n\n", "itemtype": "equation", "pos": 18175, "prevtext": "\nwhere we numerically assign $y_t=\\text{``seen\"}$ as $y_t=1$ and $y_t=\\text{``not seen\"}$ as $y_t=0$, $t_{\\text{final}}$ is the time index of the final observed sighting (\\emph{i.e.}, $t_{\\text{final}} = \\max \\{t {\\, | \\,} y_t = 1\\}$), $\\chi_k = 1$, and $\\chi_{t} = 1 - \\phi_t + \\phi_t (1-p_t) \\chi_{t+1}$ for $t < k$ \\citep{Lebreton1992}.  Use of this simplified calculation for single-state capture-recapture will dramatically speed up likelihood evaluations relative to~(\\ref{eqn:filterMCMCmatrix}), since the likelihood is expressed in closed form.\n\nThese filtering algorithms numerically integrate over sequences of latent states to directly calculate model likelihoods, removing the need to perform MCMC sampling of these latent variables.  However, the MCMC sampling step for each component of $\\theta$ now requires application of a filtering algorithm for each observed history $y_i$.  Thus, this approach reduces the dimensionality of the MCMC sampling problem, but at the cost of increased computational complexity of each MCMC iteration.\n\n\\subsubsection*{Filtering MCMC with a reduced representation of the dataset}\n\nA further specialized approach arises when there are repeated instances of identical observation histories in the full observed dataset $y$.  That is, multiple distinct individuals exhibited identical observation histories over the $k$ observational periods.  Let $n^*$ be the number of unique observation histories in the original dataset $y$.  We define a reduced representation $(y^*, m^*)$, where $y^*$ contains the $n^*$ unique histories appearing in $y$.  An accompanying vector of multiplicities $m^*$ indicates how many times each unique history appears in the original dataset, where history $y^*_i$ occurs in $y$ a total of $m^*_i$ times, for $i = 1, \\ldots, n^*$.  \n\nUsing this reduced representation, we can express~(\\ref{eqn:filterBayes}) such that the likelihood of each unique observation history is calculated only once.  This computational approach makes use of Bayes law in the form:\n\n", "index": 17, "text": "\\begin{equation} \\label{eqn:reducedFilterMCMC}\n\\begin{split}\np(\\theta {\\, | \\,} y) = p(\\theta) \\; \\prod_{i = 1}^{n^*} \\; p(y^*_i {\\, | \\,} \\theta)^{m^*_i} \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle p(\\theta{\\,|\\,}y)=p(\\theta)\\;\\prod_{i=1}^{n^{*}}\\;p%&#10;(y^{*}_{i}{\\,|\\,}\\theta)^{m^{*}_{i}}\\\\&#10;\\end{split}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\u03b8</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mpadded width=\"+2.8pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>n</mi><mo>*</mo></msup></munderover></mpadded><mi>p</mi><msup><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><msubsup><mi>y</mi><mi>i</mi><mo>*</mo></msubsup></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>m</mi><mi>i</mi><mo>*</mo></msubsup></msup></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\n\n\\subsection{Orchid Model}\n\nOur second example models sighting histories of the showy lady's slipper (\\emph{Cypripedium reginae}), a flowering variety of orchid which is native to north America.  Here, the concept of ``capture\" has been generalized to observational sightings.  One cannot observe these orchids with certainty due to a dormant state, in which the orchid is alive but not observable.\n\nThe Orchid model data consist of observational sighting histories of $n=250$ unique flowers, collected over $k=11$ annual observational periods.  There are four latent states, $\\mathcal{X} = \\{ \\text{``vegetative\"}, \\text{``flowering\"}, \\text{``dormant\"}, \\text{``dead\"} \\}$, but only three distinct observable states, $\\mathcal{Y} = \\{ \\text{``seen vegetative\"}, \\text{``seen flowering\"}, \\text{``not seen\"} \\}$ as we cannot distinguish between dormant and deceased flowers.  The presence of multiple distinct observable states (in addition to ``not seen\") classifies this as multistate capture-recapture.  The full dataset is available in the supplementary material of  \\citet{kery_bayesian_2012corrected}.\n\nFollowing \\citet{kery2004demographicCorrected} we include time-dependent survival probabilities $\\phi_t$, and state transition probabilities $\\psi_{rs}$ between the three living states.  We use an uninformative Dirichlet prior distribution for each set $\\{ \\psi_{1s}, \\psi_{2s}, \\psi_{3s} \\}$, implemented using elemental $\\text{Gamma}(1, 1)$ hyperpriors as in \\citet{royle2008hierarchical}.  As flowers in the dormant state are never observed and there is no mis-identification of flowers in the vegetative or flowering states, the observation matrix $Z$ is deterministic.  In the model specification below, latent states $x_{it}$ are represented as binary column vectors, and $i$ assumes all values in $1, \\ldots, n$.\n\n\n", "itemtype": "equation", "pos": 22733, "prevtext": "\nComputing according to~(\\ref{eqn:reducedFilterMCMC}) requires only $n^*$ applications of the filtering likelihood calculation, rather than $n$ applications when using the filtering MCMC approach on the full dataset.  We expect to this provide an approximate factor of $n/n^*$ improvement in computational efficiency relative to the filtering MCMC on the original dataset.\n\n\n\\subsubsection*{Filtering MCMC with block sampling}\n\nAs a final approach, we consider joint (a.k.a. block) MCMC sampling of model parameters \\citep{Roberts1997}.  In the case of correlated posteriors, it is well known that block sampling of highly-correlated parameter dimensions can result in improved MCMC mixing \\citep[\\emph{e.g.},][]{Liu1994}.  The general problem of determining posterior dimensions for block sampling is difficult, as a practitioner cannot reliably guess what blocking arrangement will result in efficient MCMC sampling.  Further, existing literature on the efficiency of block sampling generally only considers the mixing properties of univariate versus block sampling, and fails to consider computational demands \\citep[among others]{Mengersen1996, Roberts1996a, Roberts1997a}.\n\nWe make use of NIMBLE's automated procedure for determining an efficient problem-specific block sampling MCMC algorithm, which exemplifies how the flexibility and programmability of NIMBLE facilitates a higher level of algorithmic control than other statistical software packages.  This procedure dynamically determines a partition of the model parameters which results in efficient MCMC sampling.  MCMC efficiency is defined as the number of effectively independent posterior samples generated per second of algorithm runtime, which balances improvements in MCMC mixing with computational requirements.  This automated blocking procedure is described in detail in \\citet{turek2015automated}.\n\nThe use of a block sampling strategy can be combined with filtering over latent states.  Under this approach we use the filtering algorithms already described to integrate out the latent states, and require MCMC sampling for the model parameters.  We use a dynamically determined block sampling strategy for the MCMC sampling of these parameters.\n\n\n\\section{Capture-Recapture Example Models}\n\nWe use three capture-recapture examples representing different levels of complexity to asses performance of the various computational approaches to MCMC estimation.  The first is the well-studied European Dipper dataset, demonstrating single-state capture-recapture.  The second is a multistate capture-recapture dataset of observations of a flowering orchid.  This is considered multistate data since the orchids may be observed in multiple distinct states, in addition to the possibility of ``not seen\".  The third and largest dataset is also a multistate example, representing observations of Canadian Geese at various locations.\n\n\\subsection{Dipper Model}\n\nThe European Dipper (\\emph{Cinclus cinclus}) dataset has been analyzed extensively in the literature \\citep[][among numerous others]{marzolin1988polygynie, Lebreton1992, Gimenez2007, royle_modeling_2008, amstrup_handbook_2010}, and may be considered a canonical example of capture-recapture.  For simplicity, we do not make use of a covariate reflecting gender or the distinction of flood years as in \\citet{Lebreton1992}.\n\nThe dataset consists of $n=294$ sighting histories collected over $k=7$ annual sighting occasions.  The set of latent states is $\\mathcal{X} = \\{\\text{``alive\"}, \\text{``dead\"}\\}$ and the set of observable states is $\\mathcal{Y} = \\{\\text{``seen\"}, \\text{``not seen\"}\\}$.  For computation, we use the numerical assignments $x=1$ for ``alive\", $x=0$ for ``dead\", $y=1$ for ``seen\", and $y=0$ for ``not seen\".\n\nThe model is parameterized by annual probability of survival, $\\phi$, and probability of detection, $p$, which are assumed to be constant among all sampling occasions and individuals.  This reflects the most basic Cormack-Jolly-Seber model structure \\citep{jolly1965explicit, seber1965note}, typically denoted as $\\phi(.)~ p(.)$ to imply constant probabilities of survival and detection \\citep[\\emph{e.g.},][]{nichols1983estimation}.  The hierarchical model specification is given below, which is a realization of the general structure provided in~(\\ref{eqn:HMM}), where $i$ assumes all values in $1, \\ldots, n$.\n\n\n", "index": 19, "text": "\\begin{equation*} \n\\begin{split}\n\\phi & \\sim \\text{Uniform}(0, 1) \\\\\np & \\sim \\text{Uniform}(0, 1) \\\\\nX_{i1} = Y_{i1} & = 1  \\\\\nX_{it} {\\, | \\,} X_{i,t-1} & \\sim \\text{Bernoulli}(\\phi \\; x_{i,t-1}) \\hspace{20pt} t = 2, \\ldots, k \\\\\nY_{it} {\\, | \\,} X_{it} & \\sim \\text{Bernoulli}(p \\; x_{it}) \\hspace{34pt} t = 2, \\ldots, k\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\phi&amp;\\displaystyle\\sim\\text{Uniform}(0,1)\\\\&#10;\\displaystyle p&amp;\\displaystyle\\sim\\text{Uniform}(0,1)\\\\&#10;\\displaystyle X_{i1}=Y_{i1}&amp;\\displaystyle=1\\\\&#10;\\displaystyle X_{it}{\\,|\\,}X_{i,t-1}&amp;\\displaystyle\\sim\\text{Bernoulli}(\\phi\\;x%&#10;_{i,t-1})\\hskip 20.0ptt=2,\\ldots,k\\\\&#10;\\displaystyle Y_{it}{\\,|\\,}X_{it}&amp;\\displaystyle\\sim\\text{Bernoulli}(p\\;x_{it})%&#10;\\hskip 34.0ptt=2,\\ldots,k\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mi>\u03d5</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>\u223c</mo><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>p</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>\u223c</mo><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>Bernoulli</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"+2.8pt\"><mi>\u03d5</mi></mpadded><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mi>t</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>Bernoulli</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"+2.8pt\"><mi>p</mi></mpadded><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2003</mo><mrow><mi>t</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nwhich makes use of state transition matrices\n\n", "itemtype": "equation", "pos": 24917, "prevtext": "\n\n\n\\subsection{Orchid Model}\n\nOur second example models sighting histories of the showy lady's slipper (\\emph{Cypripedium reginae}), a flowering variety of orchid which is native to north America.  Here, the concept of ``capture\" has been generalized to observational sightings.  One cannot observe these orchids with certainty due to a dormant state, in which the orchid is alive but not observable.\n\nThe Orchid model data consist of observational sighting histories of $n=250$ unique flowers, collected over $k=11$ annual observational periods.  There are four latent states, $\\mathcal{X} = \\{ \\text{``vegetative\"}, \\text{``flowering\"}, \\text{``dormant\"}, \\text{``dead\"} \\}$, but only three distinct observable states, $\\mathcal{Y} = \\{ \\text{``seen vegetative\"}, \\text{``seen flowering\"}, \\text{``not seen\"} \\}$ as we cannot distinguish between dormant and deceased flowers.  The presence of multiple distinct observable states (in addition to ``not seen\") classifies this as multistate capture-recapture.  The full dataset is available in the supplementary material of  \\citet{kery_bayesian_2012corrected}.\n\nFollowing \\citet{kery2004demographicCorrected} we include time-dependent survival probabilities $\\phi_t$, and state transition probabilities $\\psi_{rs}$ between the three living states.  We use an uninformative Dirichlet prior distribution for each set $\\{ \\psi_{1s}, \\psi_{2s}, \\psi_{3s} \\}$, implemented using elemental $\\text{Gamma}(1, 1)$ hyperpriors as in \\citet{royle2008hierarchical}.  As flowers in the dormant state are never observed and there is no mis-identification of flowers in the vegetative or flowering states, the observation matrix $Z$ is deterministic.  In the model specification below, latent states $x_{it}$ are represented as binary column vectors, and $i$ assumes all values in $1, \\ldots, n$.\n\n\n", "index": 21, "text": "\\begin{equation*} \n\\begin{split}\n\\phi_t & \\sim \\text{Uniform}(0, 1) \\hspace{100pt} t = 2, \\ldots, 11 \\\\\n\\{ \\psi_{1s}, \\psi_{2s}, \\psi_{3s} \\} & \\sim \\text{Dirichlet}(\\alpha = \\{1,1,1\\} ) \\hspace{52pt} s = 1, 2, 3 \\\\\n\nX_{i1} & = y_{i1}  \\\\\nX_{it} {\\, | \\,} X_{i,t-1} & \\sim \\text{Categorical}(p = T_{t} \\; x_{i,t-1}) \\hspace{39pt} t = 2, \\ldots, k \\\\\nY_{it} {\\, | \\,} X_{it} & \\sim \\text{Categorical}(p = Z \\; x_{it}) \\hspace{54pt} t = 1, \\ldots, k\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\phi_{t}&amp;\\displaystyle\\sim\\text{Uniform}(0,1)\\hskip 1%&#10;00.0ptt=2,\\ldots,11\\\\&#10;\\displaystyle\\{\\psi_{1s},\\psi_{2s},\\psi_{3s}\\}&amp;\\displaystyle\\sim\\text{%&#10;Dirichlet}(\\alpha=\\{1,1,1\\})\\hskip 52.0pts=1,2,3\\\\&#10;\\displaystyle\\par&#10; X_{i1}&amp;\\displaystyle=y_{i1}\\\\&#10;\\displaystyle X_{it}{\\,|\\,}X_{i,t-1}&amp;\\displaystyle\\sim\\text{Categorical}(p=T_{%&#10;t}\\;x_{i,t-1})\\hskip 39.0ptt=2,\\ldots,k\\\\&#10;\\displaystyle Y_{it}{\\,|\\,}X_{it}&amp;\\displaystyle\\sim\\text{Categorical}(p=Z\\;x_{%&#10;it})\\hskip 54.0ptt=1,\\ldots,k\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\u03d5</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003</mo><mrow><mi>t</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mn>11</mn></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03c8</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>\u03c8</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>\u03c8</mi><mrow><mn>3</mn><mo>\u2062</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Dirichlet</mtext><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2006</mo><mi>s</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Categorical</mtext><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>=</mo><mpadded width=\"+2.8pt\"><msub><mi>T</mi><mi>t</mi></msub></mpadded><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2002\u2003</mo><mi>t</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Categorical</mtext><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>=</mo><mpadded width=\"+2.8pt\"><mi>Z</mi></mpadded><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2003</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nand constant observation matrix\n\n", "itemtype": "equation", "pos": 25438, "prevtext": "\nwhich makes use of state transition matrices\n\n", "index": 23, "text": "\\begin{equation*}\nT_t = \\left[\n\\begin{array}{cccc}\n\\phi_t \\psi_{11} & \\phi_t \\psi_{12} & \\phi_t \\psi_{13} & 0 \\\\\n\\phi_t \\psi_{21} & \\phi_t \\psi_{22} & \\phi_t \\psi_{23} & 0 \\\\\n\\phi_t \\psi_{31} & \\phi_t \\psi_{32} & \\phi_t \\psi_{33} & 0 \\\\\n1-\\phi_t & 1-\\phi_t & 1-\\phi_t & 1\n\\end{array}\n\\right]\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"T_{t}=\\left[\\begin{array}[]{cccc}\\phi_{t}\\psi_{11}&amp;\\phi_{t}\\psi_{12}&amp;\\phi_{t}%&#10;\\psi_{13}&amp;0\\\\&#10;\\phi_{t}\\psi_{21}&amp;\\phi_{t}\\psi_{22}&amp;\\phi_{t}\\psi_{23}&amp;0\\\\&#10;\\phi_{t}\\psi_{31}&amp;\\phi_{t}\\psi_{32}&amp;\\phi_{t}\\psi_{33}&amp;0\\\\&#10;1-\\phi_{t}&amp;1-\\phi_{t}&amp;1-\\phi_{t}&amp;1\\end{array}\\right]\" display=\"block\"><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>11</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>12</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>13</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>21</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>22</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>23</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>31</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>32</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>33</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mi>t</mi></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mi>t</mi></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mi>t</mi></msub></mrow></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\n\\subsection{Goose Model}\n\nThe multistate Goose model tracks $n=11,200$ Canadian Geese (\\emph{Branta canadensis}) between three distinct locations over $k=4$ years.  Latent states $\\mathcal{X} = \\{ \\text{``site A\"}, \\text{``site B\"}, \\text{``site C\"}, \\text{``dead\"} \\}$, with observable states $\\mathcal{Y} = \\{ \\text{``seen at A\"}, \\text{``seen at B\"}, \\text{``seen at C\"}, \\text{``not seen\"} \\}$.  There exists a large number of identical sighting histories among the 11,200 geese, allowing a reduced representation using only the $n^*=153$ unique sighting histories.  The complete dataset can be found in \\citet{amstrup_handbook_2010}.\n\nFollowing \\citet{amstrup_handbook_2010}, we include site-dependent survival probabilities, and both time- and site-dependent geographic transition probabilities and probabilities of detection.  We use uninformative priors for all parameters, including Dirichlet priors for each set of geographic transition probabilities.  Subsequent works \\citep[\\emph{e.g.},][]{mccrea2011multistate} have shown improved fits using more elaborate models for these data, but our purpose is to compare computational efficiency.  We desire high efficiency regardless of model fit, so the particular choice of model is tangential to our main points.  $i$ assumes all values in $1, \\ldots, n$ in the hierarchical specification below.\n\n", "itemtype": "equation", "pos": 25778, "prevtext": "\nand constant observation matrix\n\n", "index": 25, "text": "\\begin{equation*}\nZ = \\left[\n\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n\\end{array}\n\\right].\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"Z=\\left[\\begin{array}[]{cccc}1&amp;0&amp;0&amp;0\\\\&#10;0&amp;1&amp;0&amp;0\\\\&#10;0&amp;0&amp;1&amp;1\\\\&#10;\\end{array}\\right].\" display=\"block\"><mrow><mrow><mi>Z</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nwhich makes use of state transition matrices\n\n", "itemtype": "equation", "pos": 27269, "prevtext": "\n\n\\subsection{Goose Model}\n\nThe multistate Goose model tracks $n=11,200$ Canadian Geese (\\emph{Branta canadensis}) between three distinct locations over $k=4$ years.  Latent states $\\mathcal{X} = \\{ \\text{``site A\"}, \\text{``site B\"}, \\text{``site C\"}, \\text{``dead\"} \\}$, with observable states $\\mathcal{Y} = \\{ \\text{``seen at A\"}, \\text{``seen at B\"}, \\text{``seen at C\"}, \\text{``not seen\"} \\}$.  There exists a large number of identical sighting histories among the 11,200 geese, allowing a reduced representation using only the $n^*=153$ unique sighting histories.  The complete dataset can be found in \\citet{amstrup_handbook_2010}.\n\nFollowing \\citet{amstrup_handbook_2010}, we include site-dependent survival probabilities, and both time- and site-dependent geographic transition probabilities and probabilities of detection.  We use uninformative priors for all parameters, including Dirichlet priors for each set of geographic transition probabilities.  Subsequent works \\citep[\\emph{e.g.},][]{mccrea2011multistate} have shown improved fits using more elaborate models for these data, but our purpose is to compare computational efficiency.  We desire high efficiency regardless of model fit, so the particular choice of model is tangential to our main points.  $i$ assumes all values in $1, \\ldots, n$ in the hierarchical specification below.\n\n", "index": 27, "text": "\\begin{equation*} \n\\begin{split}\n\\phi_r & \\sim \\text{Uniform}(0, 1) \\hspace{100pt} r = 1, 2, 3 \\\\\n\\{ \\psi_{1st}, \\psi_{2st}, \\psi_{3st} \\} & \\sim \\text{Dirichlet}(\\alpha = \\{1,1,1\\} ) \\hspace{52pt} s = 1, 2, 3, \\hspace{10pt} t=2,3,4 \\\\\np_{rt} & \\sim \\text{Uniform}(0, 1) \\hspace{101pt} r = 1, 2, 3, \\hspace{10pt} t = 1, 2, 3, 4 \\\\\n\nX_{i1} & = y_{i1} \\\\\nX_{it} {\\, | \\,} X_{i,t-1} & \\sim \\text{Categorical}(p = T_{t} \\; x_{i,t-1}) \\hspace{39pt} t = 2, \\ldots, k \\\\\nY_{it} {\\, | \\,} X_{it} & \\sim \\text{Categorical}(p = Z_t \\; x_{it}) \\hspace{51pt} t = 1, \\ldots, k\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\phi_{r}&amp;\\displaystyle\\sim\\text{Uniform}(0,1)\\hskip 1%&#10;00.0ptr=1,2,3\\\\&#10;\\displaystyle\\{\\psi_{1st},\\psi_{2st},\\psi_{3st}\\}&amp;\\displaystyle\\sim\\text{%&#10;Dirichlet}(\\alpha=\\{1,1,1\\})\\hskip 52.0pts=1,2,3,\\hskip 10.0ptt=2,3,4\\\\&#10;\\displaystyle p_{rt}&amp;\\displaystyle\\sim\\text{Uniform}(0,1)\\hskip 101.0ptr=1,2,3%&#10;,\\hskip 10.0ptt=1,2,3,4\\\\&#10;\\displaystyle\\par&#10; X_{i1}&amp;\\displaystyle=y_{i1}\\\\&#10;\\displaystyle X_{it}{\\,|\\,}X_{i,t-1}&amp;\\displaystyle\\sim\\text{Categorical}(p=T_{%&#10;t}\\;x_{i,t-1})\\hskip 39.0ptt=2,\\ldots,k\\\\&#10;\\displaystyle Y_{it}{\\,|\\,}X_{it}&amp;\\displaystyle\\sim\\text{Categorical}(p=Z_{t}%&#10;\\;x_{it})\\hskip 51.0ptt=1,\\ldots,k\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\u03d5</mi><mi>r</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003</mo><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03c8</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>\u03c8</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>\u03c8</mi><mrow><mn>3</mn><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Dirichlet</mtext><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2006</mo><mi>s</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo rspace=\"12.5pt\">,</mo><mi>t</mi><mo>=</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>p</mi><mrow><mi>r</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mtext>Uniform</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo separator=\"true\">\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003</mo><mrow><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Categorical</mtext><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>=</mo><mpadded width=\"+2.8pt\"><msub><mi>T</mi><mi>t</mi></msub></mpadded><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2002\u2003</mo><mi>t</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mpadded width=\"+1.7pt\"><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>\u223c</mo><mtext>Categorical</mtext><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>=</mo><mpadded width=\"+2.8pt\"><msub><mi>Z</mi><mi>t</mi></msub></mpadded><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2003\u2003</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\nand observation matrices\n\n", "itemtype": "equation", "pos": 27906, "prevtext": "\nwhich makes use of state transition matrices\n\n", "index": 29, "text": "\\begin{equation*}\nT_t = \\left[\n\\begin{array}{cccc}\n\\phi_1 \\psi_{11t} & \\phi_2 \\psi_{12t} & \\phi_3 \\psi_{13t} & 0 \\\\\n\\phi_1 \\psi_{21t} & \\phi_2 \\psi_{22t} & \\phi_3 \\psi_{23t} & 0 \\\\\n\\phi_1 \\psi_{31t} & \\phi_2 \\psi_{32t} & \\phi_3 \\psi_{33t} & 0 \\\\\n1-\\phi_1 & 1-\\phi_2 & 1-\\phi_3 & 1\n\\end{array}\n\\right]\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"T_{t}=\\left[\\begin{array}[]{cccc}\\phi_{1}\\psi_{11t}&amp;\\phi_{2}\\psi_{12t}&amp;\\phi_{3%&#10;}\\psi_{13t}&amp;0\\\\&#10;\\phi_{1}\\psi_{21t}&amp;\\phi_{2}\\psi_{22t}&amp;\\phi_{3}\\psi_{23t}&amp;0\\\\&#10;\\phi_{1}\\psi_{31t}&amp;\\phi_{2}\\psi_{32t}&amp;\\phi_{3}\\psi_{33t}&amp;0\\\\&#10;1-\\phi_{1}&amp;1-\\phi_{2}&amp;1-\\phi_{3}&amp;1\\end{array}\\right]\" display=\"block\"><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>11</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>12</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>3</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>13</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>21</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>22</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>3</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>23</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>31</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>32</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03d5</mi><mn>3</mn></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mrow><mn>33</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mn>2</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03d5</mi><mn>3</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02698.tex", "nexttext": "\n\n\n\n\\section{Performance Results}\n\nWe now present the performance of various computational strategies for MCMC estimation applied to the three example capture-recapture models.  We do not present posterior results, but instead only the algorithmic efficiencies of each computational approach to generating these.  For each, the posterior results of top-level parameters closely agree with existing published analyses of the same datasets and models \\citep{Lebreton1992, kery_bayesian_2012corrected, amstrup_handbook_2010}, which provides validation of our computational methodologies.\n\nWe include results for the following computational strategies MCMC estimation: latent state MCMC (``Latent State\") where model parameters and latent states undergo MCMC sampling, filtering MCMC (``Filtering\") in which we filter over latent states and only top-level parameters undergo MCMC sampling, and a combination of filtering and blocking (``Filtering \\& Blocking\") in which a customized blocking strategy is used for MCMC sampling of top-level parameters.  When appropriate, we also use a reduced representation (``RR\") of the dataset.\n\nWe use the NIMBLE package for R to generate and execute MCMC algorithms, as the algorithmic flexibility it provides facilitates these computational approaches.  The use of user-defined distribution functions in NIMBLE allows us to incorporate the filtering algorithms (\\ref{eqn:filterMCMCmatrix}) and (\\ref{eqn:filterMCMCsimplified}) directly into a hierarchical model specification.  The generic discrete HMM filtering procedure described in (\\ref{eqn:filterMCMCmatrix}) is used for filtering, or when permitted by the model structure we instead use the closed form likelihood calculation given in (\\ref{eqn:filterMCMCsimplified}).  NIMBLE also provides the automated parameter blocking procedure \\citep{turek2015automated} we use to generate problem-specific parameter blocking strategies for MCMC sampling.\n\nWe define the efficiency of an MCMC algorithm in terms of the number of effectively independent posterior samples produced per second of algorithm runtime.  This metric is denoted as effective samples per second (ESPS), and we will present both the minimum and mean ESPS among all model parameters.  This metric balances the tradeoff between computationally fast algorithms which generate highly autocorrelated chains of posterior samples, versus algorithms which are more computationally demanding but result in lower posterior autocorrelation, which provides stronger inferential power.\n\nAll algorithm runtimes represent the time required to generate 100,000 posterior samples.  When possible, we also provide comparisons with MCMC algorithms from the JAGS software package for R.  All calculations are produced using single-threaded execution on an Intel Xeon E5-2609 processor (2.40 GHz), running under the Ubuntu Linux operating system.\n\n\\subsection{Dipper Model}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the Dipper model, use of the filtering MCMC compared to MCMC sampling of all discrete latent states yielded a 60-fold improvement in sampling efficiency in NIMBLE and a 15-fold improvement in JAGS (Figure~\\ref{fig:plotDipper}).  The sampling efficiencies of both top-level parameters are quite similar under each algorithm (although vary greatly between algorithms), hence the mean and the minimum summary statistics shown in Figure~\\ref{fig:plotDipper} are similar as well.\n\n\n\n\\begin{figure}[h]\n\\centerline{\\includegraphics[scale=1.0]{plot_dipper}}\n\n\\caption{Minimum and mean parameter sampling efficiencies for the Dipper model.}\n\\label{fig:plotDipper}\n\\end{figure}\n\nThe latent state MCMC requires MCMC sampling of 848 latent variables, in addition to the two top-level model parameters of interest.  The performance of JAGS is slightly better, although both result in sampling efficiencies of roughly 100 ESPS for both parameters.  NIMBLE and JAGS each require approximately four minutes to generate 100,000 samples.  The filtering MCMC is implemented in NIMBLE according to~(\\ref{eqn:filterMCMCsimplified}), where only the two top-level parameters undergo MCMC sampling and runtime is reduced to 5 seconds.  The mixing also improves relative to the latent state MCMC, yielding a sampling efficiency of roughly 6,000 ESPS for both parameters, a 60-fold improvement.\n\nFor the Dipper model alone, we can also implement the filtering MCMC in JAGS.  This is possible because (\\ref{eqn:filterMCMCsimplified}) provides a closed form expression for the likelihood of each sighting history.  This allows use of the ``zeros-trick\" \\citep[][p. 204-206]{Lunn2012} where a general log-likelihood expression is incorporated into a model through the mean parameter of a Poisson distribution, using an artificial zero-valued observation.  Using this technique reduces JAGS runtime to 30 seconds and increases sampling efficiency of both parameters to approximately 1,500 ESPS, a 15-fold improvement relative to the latent state MCMC.  Although the underlying calculations are similar to those of NIMBLE's filtering MCMC, this approach requires the additional overhead of artificial model variables and observations.\n\n\n\n\\subsection{Orchid Model}\n\nFor the multistate Orchid model, a combination of filtering over latent states and dynamic block sampling of parameters yielded a 3-fold improvement in sampling efficiency of the slowest mixing parameter, relative to the latent state MCMC (Figure~\\ref{fig:plotOrchid}).\n\n\n\n\\begin{figure}[h]\n\\centerline{\\includegraphics[scale=1.0]{plot_orchid}}\n\n\\caption{Minimum and mean parameter sampling efficiencies for the Orchid model.}\n\\label{fig:plotOrchid}\n\\end{figure}\n\nThe latent state MCMC samples 2,157 latent variables in addition to 19 top-level parameters, which required 42 minutes to generate 100,000 samples.  Efficiency results for the latent state MCMC are quite similar to the filtering MCMC, which required 36 minutes but with slightly inferior mixing.  Both of these algorithms struggle to achieve good mixing among the nine state transition probabilities.  We might expect triplets of these parameters to be highly correlated due to the Dirichlet prior imposing a sum-to-one constraint, and indeed, examining the posterior correlations we find several instances of absolute pairwise posterior correlation greater than 0.9.  Under the latent state and filtering MCMC algorithms, several state transition probabilities have sampling efficiencies between 0.1 and 0.3 ESPS, which dictates the minimum efficiencies shown in Figure~\\ref{fig:plotOrchid}.\n\nFor the 19 parameters undergoing MCMC sampling, NIMBLE's automated parameter blocking procedure converges on two blocks each consisting of two state transition probabilities, and univariate sampling for the other 15 parameters.  We observe that these pairs of transition probabilities have absolute posterior correlations of 0.98 and 0.97, the highest among all 19 parameters.  Joint sampling according to this blocking scheme in combination with filtering over latent states results in a minimum sampling efficiency of 0.6 ESPS, representing a 3-fold improvement over the latent state MCMC.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Goose Model}\n\nAs the Goose model includes a large number of repeated sighting histories among the 11,200 geese, this model benefits from a reduced representation of the data using the 153 unique sighting histories.  Applying the filtering MCMC to a reduced data representation produced a 70-fold improvement in sampling efficiency of the slowest mixing parameter, compared to the latent state MCMC (Figure~\\ref{fig:plotGoose}).  An additional order of magnitude improvement was gained by applying dynamic blocking of model parameters.\n\n\n\n\\begin{figure}[h]\n\\centerline{\\includegraphics[scale=1.0]{plot_goose}}\n\n\\caption{Minimum and mean parameter sampling efficiencies for the Goose model.}\n\\label{fig:plotGoose}\n\\end{figure}\n\nThe latent state MCMC requires sampling of 14,437 latent variables in addition to 21 top-level parameters.  We cannot use a reduced data representation under the latent state approach, since for correct inference each of the 11,200 sighting histories must have a corresponding sequence of latent state variables.  The latent state MCMC required approximately 24 hours to generate 100,000 samples, yielding a minimum sampling efficiency of 0.0027 ESPS and a mean of 0.028 ESPS.  This approach can be deemed impractical, as this translates to generating ten effective samples (for the slowest mixing parameter) per hour.\n\nApplying the filtering MCMC to a reduced data representation using the 153 unique sighting histories, the complete model likelihood is calculated according to~(\\ref{eqn:reducedFilterMCMC}), using~(\\ref{eqn:filterMCMCmatrix}) to calculate the likelihood of each unique history.  Computation time is reduced to 20 minutes, which agrees with the expected speedup factor of $\\frac{11,200}{153} \\approx 73.2$.  Mixing also improves to produce a minimum sampling efficiency of 0.20 ESPS, a 70-fold improvement relative to the latent state MCMC.  This translates to 720 effective samples per hour, which may be considered practical.\n\nNIMBLE's automated blocking procedure converges on seven blocks of parameters, ranging between two and five parameters each.  These seven blocks include 20 of the 21 parameters, leaving only one parameter for univariate sampling.  It is realistically unlikely that a practitioner would discover this blocking scheme through expert opinion or trial and error.  Runtime is comparable using this approach, but the joint sampling of correlated parameters gives a dramatic improvement in MCMC mixing.  The minimum sampling efficiency improves by an additional order of magnitude to 2.4 ESPS, or generating over 8,600 effective samples per hour.  This represents nearly a 1000-fold improvement over the latent state MCMC.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Discussion}\n\nWe have studied alternate computational approaches for MCMC sampling of hierarchical models which include embedded discrete HMMs.  Traditional MCMC analysis of such models involves sampling the unknown (nuisance) latent states, whereas we propose filtering over latent states to calculate model likelihoods and limiting MCMC sampling to top-level parameters.  This introduces a computational trade-off: simplified MCMC sampling with the additional expense of filtering.  Through examples, we observe that worthwhile gains in sampling efficiency result from this approach.\n\nFurthermore, the filtering MCMC permits a reduced representation of datasets with repeated observations.  This simplification is not possible when using traditional latent state MCMC, since each (possibly duplicated) observational history requires its own sequence of latent states.  When appropriate, combining our filtering MCMC with this reduced data representation provides an additional echelon of improvement in MCMC sampling efficiency, the extent of which is limited only by the degree of repetition in the initial data.\n\nWe note that the filtering MCMC approach forgoes generating posterior samples for latent states.  In some analyses the distribution of latent variables at a particular observational periods may be of interest, or otherwise may be used (for example) to estimate longevity distributions.  The inclusion of latent variables would also be necessary when used as explanatory variables in other parts of a hierarchical model \\citep[\\emph{e.g.},][]{risk2011robust}, or in the case of individual-specific covariates.  Our suggested approaches would not be appropriate in these analysis scenarios.\n\nThe analyses presented herein are facilitated by the NIMBLE package for R.  NIMBLE allows user-defined distribution functions to be used directly in hierarchical model specifications.  We define a multivariate distribution function parametrized by state transition and observation matrices, where the probability density evaluation routine implements discrete filtering to calculate likelihood values.  Models are specified using this distribution, which effectively embeds filtering into the model for the purposes of likelihood calculation.  NIMBLE's MCMC engine may then be applied to the resulting model to achieve the filtering MCMC.  We make use of NIMBLE's default MCMC as well as that resulting from automated parameter blocking.  The distinction of allowing programmable models and statistical algorithms, as compared to other statistical software, makes such analyses possible in NIMBLE.\n\n\n\n\\section*{Acknowledgements}\nThis work was supported by the NSF under grant DBI-1147230 and by support to DT from the Berkeley Institute for Data Science.  We thank Marc K\\'{e}ry, Byron Morgan, and Michael Schaub for reviewing earlier versions of the manuscript.\n\n\n\\newpage\n\\printbibliography\n\n\n\n\n", "itemtype": "equation", "pos": 28248, "prevtext": "\nand observation matrices\n\n", "index": 31, "text": "\\begin{equation*}\nZ_t = \\left[\n\\begin{array}{cccc}\np_{1t}        & 0              & 0              & 0 \\\\\n0               & p_{2t}       & 0              & 0 \\\\\n0               & 0              & p_{3t}       & 0 \\\\\n1-p_{1t}     & 1-p_{2t}   & 1-p_{3t}   & 1 \\\\\n\\end{array}\n\\right].\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"Z_{t}=\\left[\\begin{array}[]{cccc}p_{1t}&amp;0&amp;0&amp;0\\\\&#10;0&amp;p_{2t}&amp;0&amp;0\\\\&#10;0&amp;0&amp;p_{3t}&amp;0\\\\&#10;1-p_{1t}&amp;1-p_{2t}&amp;1-p_{3t}&amp;1\\\\&#10;\\end{array}\\right].\" display=\"block\"><mrow><mrow><msub><mi>Z</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>p</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><msub><mi>p</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><msub><mi>p</mi><mrow><mn>3</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>p</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>p</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>-</mo><msub><mi>p</mi><mrow><mn>3</mn><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]