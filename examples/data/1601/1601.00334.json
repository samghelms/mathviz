[{"file": "1601.00334.tex", "nexttext": "\n\nIn general, a system of N variables results in ${N \\choose 1}$\nunivariate entropy constraints, ${N \\choose 2}$ mutual information\nconstraints, and $A = \\sum_{k=1}^N {N \\choose k} = 2^N -1$ atoms to be\ndetermined. In the simplest case of $N = 3$ variables we have six\nconstraints and $A = 7$ regions to specify, see\nFig~\\ref{Fig:InfoDiag}. This means we only have one free parameter,\nmaking the maximization process particularly easy in this case; in\ngeneral there are $\\sum_{k=3}^N {N \\choose k}$ free parameters.\n\n\n\n\nApart from the chosen constraints defined above, there are also\ngeneral constraints on the values of the subregions ensuring they\ndefine a valid information diagram, i.e. that there exists a\nprobability distribution with corresponding information-theoretic\nquantities.  A family of such constraints (so-called Shannon\ninequalities) can be inferred from the fundamental requirement that,\nfor discrete variables, (conditional) entropies and mutual\ninformations are necessarily non-negative: A) $H(X_i | \\{X\\}_N - X_i)\n\\geq 0$; B) $I(X_i,X_j| \\{X\\}_K) \\geq 0$, where $i \\neq j$ and\n$\\{X\\}_K \\subseteq \\{X\\}_N - \\{X_i,X_j\\}$\n\n\\footnote{This set of equalities is minimal in the sense that no\ninequality is implied by any combination of the\nothers~\\cite{Yeung2008information}.}.\n\nEach inequality can also be\nrepresented as a sum of atomic regions, e.g. \n\n\n", "itemtype": "equation", "pos": 10587, "prevtext": "\n \n\n\n\n\\title{Pairwise Network Information and Nonlinear Correlations}\n\n\n\n\\author{Elliot A. Martin}\n\\affiliation{Complexity Science Group, Department of Physics and\n  Astronomy, University of Calgary, Calgary, Alberta, Canada, T2N 1N4}\n\\author{Jaroslav  Hlinka}\n\\affiliation{Institute of Computer Science, The Czech Academy of Sciences, Pod vodarenskou vezi 2, 18207 Prague, Czech\n  Republic}\n\\affiliation{National Institute of Mental Health, Topolov\\'{a} 748, 250 67 Klecany, Czech Republic}\n\\author{J\\\"{o}rn Davidsen} \n\\email[]{davidsen@phas.ucalgary.ca}\n\\affiliation{Complexity Science Group, Department of Physics and\n  Astronomy, University of Calgary, Calgary, Alberta, Canada, T2N 1N4}\n\n\\date{\\today}\n\n\n\\begin{abstract}  \n  Reconstructing the structural connectivity between interacting units\n  from observed activity is a challenge across many different\n  disciplines. The fundamental first step is to establish whether or\n  to what extent the interactions between the units can be considered\n  pairwise and, thus, can be modeled as an interaction network with\n  simple links corresponding to pairwise interactions.  In principle\n  this can be determined by comparing the maximum entropy given the\n  bivariate probability distributions to the true joint entropy. In\n  many practical cases this is not an option since the bivariate\n  distributions needed may not be reliably estimated, or the\n  optimization is too computationally expensive.  Here we present an\n  approach that allows one to use mutual informations as a proxy for\n  the bivariate distributions. This has the advantage of being less\n  computationally expensive and easier to estimate.  We achieve this\n  by introducing a novel entropy maximization scheme that is based on\n  conditioning on entropies and mutual informations. This renders our\n  approach typically superior to other methods based on linear\n  approximations. The advantages of the proposed method are documented\n  using oscillator networks and a resting-state human brain network as\n  generic relevant examples.\n\\end{abstract}\n\n\\pacs{89.75.Hc, \n89.70.Cf, \n05.45.Tp, \t\n87.18.Sn \n}\n\n\n\\maketitle\n\n\n\n\nPairwise measures of dependence such as cross-correlations (as\nmeasured by the Pearson correlation coefficient or covariance matrix)\nand mutual information are widely used to characterize the\ninteractions within complex systems. They are a key ingredient to\ntechniques such as principal component analysis, empirical orthogonal\nfunctions, and functional\nnetworks~\\cite{Donges2015::CD,haimovici13,timme14}. These techniques are\nwidespread since they provide greatly simplified descriptions of\ncomplex systems, and allow for the analysis of what might otherwise be\nintractable problems~\\cite{bullmore09}.\n\nIn this paper we study how faithfully these measures alone can\nrepresent a given system. With the increasing use of functional\nnetworks this topic has received much attention recently, and many\ntechnical concerns have been brought to light dealing with the\ninference of these networks. Previous studies have shown that the\nestimates of the functional networks can be negatively affected by\nproperties of the time\nseries~\\cite{Martin2013::EPL,Palu2011::NPG,bialonski11}, as well as\nproperties of the measure of association,\ne.g. cross-correlations~\\cite{Tirabassi2015::SciRep,Hlinka2012::Chaos,martin2014::NPG,mader15}. In\nthis work however, we address a more fundamental question: How well do\npairwise measurements represent a system?\n\n\nIn principle this can be evaluated using the framework laid out\nin~\\cite{Schneidman2003::PRL} as later applied\nin~\\cite{Schneidman2006::Nat}, where they assessed the rationale of\nonly looking at the pairwise relationships between neurons.  They\nexamined how well the maximum entropy distribution, consistent with\nall the pairwise probability distributions, described the system. If\nthe system is not well described by this maximum entropy distribution\nthen we know from the work of Jaynes~\\cite{Jaynes1957::PRa} that other\ninformation beyond pairwise relationships would need to be taken into\naccount. Similar analyses have since been applied in\nneuroscience~\\cite{Watanabe2013::NatCom,Yu2011::JNeurosci,ohiorhenuan10},\nas well as in genetics~\\cite{Lezon2006::PNAS},\nlinguistics~\\cite{stephens10}, economics~\\cite{Xi2014::PhysicaA}, and\nto the supreme court of the United States~\\cite{Lee2013::JStatPhys}.\n\n\nHowever, the data to accurately estimate the needed bivariate\nprobability distributions may not be available. To get around this\nsome researchers have used the first two moments of the variables as\nconstraints instead of the full bivariate\ndistributions~\\cite{Bialek2012::PNAS,Wood2012::PNAS} --- effectively\nusing the cross-correlations as their constraints. In the case of\nbinary variables, as in the original work~\\cite{Schneidman2006::Nat},\nthis is equivalent to conditioning on the bivariate distributions. For\nlarger cardinality variables this is only an approximation though, as\nthe cross-correlation is only sensitive to linear\nrelationships~\\cite{kantz}. Systems where larger cardinalities and\nnonlinear behaviour are thought to play a significant role such as in\ncoupled oscillators --- which have been used to model systems as\ndiverse as pacemaker cells and crickets~\\cite{Pikovsky2003} --- are,\nhowever, rather the norm than an exception~\\cite{kantz}. In\nparticular, we show here that this plays a significant role in a\nresting-state human brain network.\n\n\nIn order to retain the attractive properties of the cross-correlation\nand simultaneously capture a much wider range of relationships we\npropose using the mutual information. Mutual information can detect\narbitrary pairwise relationships between variables, and is only\nnon-zero when the variables are pairwise independent, making it the\nideal measure~\\cite{cover}. However, while calculating the maximum\nentropy given the moments of a distribution results in simple\nequations in the probabilities, using mutual informations as\nconstraints results in transcendental equations which are much harder\nto solve.  We circumvent this problem here using the set theoretic\nformulation of information theory~\\cite{Yeung2008information}, which\ngives us an upper bound on the maximum entropy that is saturated in\nmany cases.\n\n\n\nThe set theoretic formulation of information theory allows us to map\ninformation theoretic quantities to the regions of an information\ndiagram, a variation of a Venn diagram. The information diagram for\nthree variables is shown in Fig.~\\ref{Fig:InfoDiag} with the\nassociated information theoretic quantities labeled\n\n\\footnote{We use the convention $p(x,y,z) = P(X = x, Y = y, Z = z)$.}: \n\nentropy, $H(X) = \\sum p(x)\n\\log(p(x))$; conditional entropy, $H(X|Y,Z) = \\sum p(x,y,z) \\log(p(x |\ny, z))$; mutual information, $I(X,Y) = \\sum p(x,y)\n\\log(p(x,y)/(p(x)p(y)))$; conditional mutual information, $I(X;Y|Z) =\n\\sum p(x,y,z) \\log \\left(p(x;y|z)/[p(x|z)p(y|z) ] \\right)$;\nmultivariate mutual information, $I(X;Y;Z) = I(X;Y) - I(X;Y|Z)$.\n\n\n\n\n\n\\begin{figure}[!h]\n\n  \\begin{center}\n    \\includegraphics*[width=0.6\\columnwidth]{InfoDiag1.pdf}\n   \\end{center}\n   \\caption{\\label{Fig:InfoDiag} (Color online) The information\n     diagram for 3 variables. It contains 7 regions corresponding to\n     the possible combinations of 3 variables, with their\n     corresponding information theoretic quantities defined in the\n     text. The univariate entropy $H(X)$ is the sum of all the regions\n     in the red circle, and the mutual information $I(Y;Z)$ is the sum\n     of all the regions in the blue oval.}\n\\end{figure}\n\n\n\nWe illustrate our method using systems of coupled oscillators, as they\ncommonly occur in nature and are used to model a large variety of\nsystems~\\cite{Pikovsky2003}.  In particular we look at the Kuramoto\nmodel~\\cite{Kuramoto1975,Acebron2005::RMP} as a paradigmatic example\nthat is capable of a wide range of dynamics from synchronization to\nchaos~\\cite{Maistrenko2004::PRL}, and hence provides an excellent test\nbed for our method.\n\n\n\n\n\n\n\\paragraph{Method:}\n\nGiven a set of $N$ variables ($\\{X\\}_N$), we want to know how well the\ncross-correlation or mutual information between all pairs of variables\ncan encode the state of the system. To do this we first determine the\nmaximum entropy consistent with the given measure of similarity,\n$H_m(\\{X\\}_N)$.  This means that any model of the system consistent\nwith the ${N \\choose 2}$ values of the similarity measure can have an\nentropy of at most $H_m(\\{X\\}_N)$. From the work of\nJaynes~\\cite{Jaynes1957::PRa}, we also know that any model of the\nsystem with a smaller entropy must implicitly or explicitly include\ninformation beyond these values. As a result the true joint entropy,\n$H(\\{X\\}_N)$, will always be less than or equal to $H_m(\\{X\\}_N)$.\n\n\nAs the next step, we consider the information gained by using a given\nmeasure compared to the information gained by knowing the full joint\nprobability distribution as opposed to the univariate\ndistributions. The latter information gain is given by the\nmulti-information (also called total\ncorrelation~\\cite{Watanabe1960::IBM}), $I_N(\\{X\\}_N) = H_I(\\{X\\}_N) -\nH(\\{X\\}_N) \\geq 0$, where $H_I(\\{X\\}_N) = \\sum_i H(X_i)$ is the\nentropy if all variables were independent.  We similarly define the\nmeasure information $I^m(\\{X\\}_N) = H_I(\\{X\\}_N) - H_m(\\{X\\}_N)$, to\nbe the shared information between a set of variables given a measure.\nThe fraction of information retained by describing the system with a\ngiven measure is then $0 \\leq I^m/I_N \\leq 1$.\n\n\nWhen using the cross-correlation, estimating $H_m$ is\nstraightforward. Estimates of the first two moments of the variables\nuniquely determine the cross-correlations,\nand can be used as constraints in a Lagrange multiplier problem\nsolving for $H_m$. The resulting probability distribution\n$P_m({\\{X\\}_N})$ is the Boltzmann distribution\n$  P_m(\\{X\\}_N)  = \\exp{ \\left( \\sum_i h_i x_i + \\sum_{i \\geq j}J_{i,j}x_ix_j \\right)},$\nwhere $h_i$ and $J_{i,j}$ are the Lagrange multipliers~\\cite{timme14}.\n\n\n\nWhen using mutual information, estimating $H_m$ with Lagrange\nmultipliers is much harder as the derivatives of the Lagrange function\nare transcendental functions in $P_m(\\{X\\}_N)$. Instead, we use the\nmutual informations and univariate entropies as constraints, and draw\non the structure of information diagrams. Each univariate entropy and\nmutual information corresponds to a region in the information diagram\nthat can be written as a sum of a number of {\\it atomic regions\n  (atoms)}. Thus, as seen in Fig.~\\ref{Fig:InfoDiag}, we obtain\nconstraints of the form:\n\n\n", "index": 1, "text": "\\begin{align}\n\\label{Eq:IConst}\n\\text{const} = I(Y;Z) =&  I(Y;Z|X) + I(X;Y;Z),\\\\\n\\nonumber\n\\text{const} =  H(X) =&  H(X|Y,Z) + I(X;Y|Z) \\\\\n&+ I(X;Z|Y) + I(X;Y;Z).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{const}=I(Y;Z)=\" display=\"inline\"><mrow><mtext>const</mtext><mo>=</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle I(Y;Z|X)+I(X;Y;Z),\" display=\"inline\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy=\"false\">|</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{const}=H(X)=\" display=\"inline\"><mrow><mtext>const</mtext><mo>=</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle H(X|Y,Z)+I(X;Y|Z)\" display=\"inline\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+I(X;Z|Y)+I(X;Y;Z).\" display=\"inline\"><mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Z</mi><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo>;</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00334.tex", "nexttext": "\n\n\n\n\nNot so-well known, for $N\\geq 4$, there are also inequalities that are\nnot deducible from the Shannon inequalities, so called non-Shannon\ninequalities~\\cite{Yeung2008information}.  In principle, these\ninequalities may be included in our maximization problem; however,\nthey have not yet been fully described. Therefore, we suggest\nconstructing the diagram with the maximum entropy that satisfies the\nproblem specific constraints and is consistent with the Shannon\ninequalities.  As it may violate the non-Shannon inequalities, it may\nnot represent a valid distribution. However, the sum of the atomic\nregions would still be an upper bound on the entropy $H_m$, and thus\nprovide a \\emph{lower} bound on $I^m/I_N$.  Notably, for the\nparticular (and in our simulations common) case where all $A$ elements\nare non-negative --- which is always true for $N=3$ --- one can prove\nthat the bound is attainable~\\cite{Martin2015::PRE}.\n\n\n\n\n\nTo summarize, the task of finding the maximum entropy conditioned on the\nunivariate entropies, mutual informations, and elemental Shannon\ninequalities, can be solved using linear optimization:  Each\nconstraint will take the form of a linear (in-)equality, as\nin Eqs.~\\eqref{Eq:IConst},~\\eqref{Eq:ShannonIneq}, and we maximize the\nN-variate entropy by maximizing the sum over all $A$ atoms of the\ninformation diagram.\n\n\n\n\\paragraph{Example of an Nonlinear Pairwise Distribution:}\n\nWe now give an example illustrating how the mutual information can\nbetter detect pairwise relationships then the\ncross-correlation. Consider a set of variables $\\{X\\}_N$: each\nvariable is drawn uniformly from the set $\\{-1,0,1\\}$, and all\nvariables are simultaneously $0$ or independently distributed among\n$\\{-1,1\\}$. The cross-correlation between any pair of variables is\nzero, and therefore consistent with the hypothesis that all variables\nare independent. Therefore, the fraction of information captured by\nthe cross-correlation is $I^m/I_N = 0$. However, there is a\nsignificant amount of mutual information between the variables.\n\nSince $P(X_i|\\{X\\}_N - X_i) = P(X_i|X_{j\\neq i}) \\forall i$ and $j$,\nall the conditional mutual informations are zero. Therefore, the only\nnonzero atoms in the information diagram will be the N-variate mutual\ninformation $I(X_1;...;X_N) = I(X_1;X_2)$, and the conditional\nentropies $H(X_i| \\{X\\}_N - X_i) = 2/3$ bits. This is the maximum\nentropy diagram consistent with the pairwise mutual informations and\nunivariate entropies, so the expected result using the mutual\ninformation is $I^m/I_N = 1$.  We can see why this is the case by\nstarting with the information diagram for 2 variables (which is fixed\nfrom our conditions), and successively adding new variables. The\naddition of each new variable adds $2/3$ bits to the total entropy ---\nwhich is the maximal amount consistent with the mutual informations.\n\n\n\n\n\\paragraph{Kuramoto Model:}\n\n\nThe Kuramoto model is a dynamical system of $N$ phase oscillators with\nall to all coupling proportional to\n$K$~\\cite{Kuramoto1975,Acebron2005::RMP}. The $ith$ oscillator has an\nintrinsic frequency of $\\omega_i$, a phase of $\\theta_i$, and its\ndynamics is given by\n\n$  \\frac{\\partial \\theta_i }{\\partial t} = \\omega_i + \\frac{K}{N} \\sum_{j=1}^N\\sin(\\theta_j - \\theta_i) + \\eta_i(t).\n$\nHere, we have followed~\\cite{Sakaguchi1988::PTP} and added a dynamical\nnoise term to mimic natural fluctuations and environmental effects;\n$\\eta_i(t)$ is drawn from a Gaussian distribution with correlation\nfunction $\\langle \\eta_i(t)\\eta_j(t') \\rangle = G\n\\delta_{i,j}\\delta(t-t')$, where $G$ determines the amplitude of the\nnoise.\n\n\nFor values of $K$ above a critical threshold, $K > K_{c}$,\nsynchronization occurs~\\cite{Pikovsky2003}. In the limit of constant\nphase differences the dynamics are trivial, and knowledge of one\noscillator will specify the phase of all others. Therefore, pairwise\ninformation is sufficient to describe the system in this case. Yet,\nthe presence of noise results in random\nperturbations of the phases and typically prevents constant phase\ndifferences~\\cite{Sakaguchi1988::PTP} such that only $I^m/I_N\n\\lesssim 1$ is expected. In the weak coupling regime when\nsynchronization is absent, it is nontrivial what $I^m/I_N$ should be.\n\n\nTo estimate $I^m/I_N$ and to establish the importance of the level of\ndiscretization or cardinality, we first discretize the phase of each\noscillator into $n$ states. The region from zero to $2\\pi$ is divided\ninto $n$ intervals, such that the oscillator is equally likely to be\nfound in each one, and all values of $\\theta$ are mapped to the\naverage value of the points in their interval. Discretizing the system\nusing $n$ equally sized intervals, each having a length $2\\pi/n$, did\nnot noticeably alter our results.\n\n\nTo provide clear proofs of principle, we focus on three-oscillator\nsystems in the following as this is the smallest system size at which\nthe results are non-trivial. Similar results hold for larger systems\nand when only a subset of oscillators is observed as shown in the\nSupplementary material~\\cite{SuppMat}. Specifically, we consider three\ndifferent cases: (i) all oscillators have the same intrinsic\nfrequency, (ii) all oscillators have unique intrinsic frequencies and\nare still synchronized, and (iii) all oscillators have unique\nintrinsic frequencies and the entire system and all subsystems are\nunsynchronized (``weak coupling regime''). For three-oscillator\nsystems, the corresponding parameter regimes in the absence of noise\nhave been carefully documented in Ref.~\\cite{Maistrenko2004::PRL}.\n\n\n\n\nFor each of the three cases examined we created ensembles of\n100 three-oscillator systems, where each element of the ensemble will\nhave randomly sampled frequencies\n\n\\footnote{For the numerical simulations, we use the Euler-Maruyama\n  method, with a time step $dt = 2^{-6}$, and unless otherwise stated\n  we use an integration time of $T=2000$ in all of our results. We\n  also discard times up to $50$ to remove transient effects, and\n  resample the data only taking every $8th$ data point.}.\n\nThese ensembles are studied in two different noise regimes, $G =\n0.001$ and $G = 0.5$. The same ensemble of frequencies is used in both\nnoise regimes.\n\nIn the first case all oscillators are synchronized with $\\omega_1 =\n\\omega_2 = \\omega_3$, and $K = 1.65$. Recall, in the synchronized case\nwe expect $E[I^m/I_N] \\approx 1$. This is indeed what we see in the low\nnoise case, $G = 0.001$, Fig.~\\ref{Fig:Completeness}~A; though the\nmutual information preserves slightly more information at larger\ncardinalities. However, for increased noise, $G=0.5$, the\ncross-correlation performs poorly at larger cardinalities, while the\nmutual information behaves robustly, Fig.~\\ref{Fig:Completeness}~D.\n\n\n\\begin{figure}[!h]\n\n  \\begin{center}\n    \\includegraphics*[width=0.85\\columnwidth]{ExpectedCompleteness6Panel.pdf}\n   \\end{center}\n   \\caption{\\label{Fig:Completeness} (Color online) The fraction of\n     shared information coded by the mutual information (blue\n     diamonds) and the cross-correlation (green squares). Panels A),\n     B), and C) correspond to low noise cases, $G = 0.001$, and panels\n     D), E), F) to higher noise, $G= 0.5$.  Panels A) and D)\n     correspond to the synchronized case (i) in the text; Panels B)\n     and E) to the synchronized case (ii); Panels C) and F) to the\n     unsynchronized case (iii). The estimated expectations,\n     $\\hat{E}[...]$, are averages over the ensemble of 100\n     realizations where we draw $\\omega_3$ from a normal distribution\n     with zero mean and unit variance.}\n\\end{figure}\n\n\nFor the second case, where the oscillators are synchronized with\ndifferent intrinsic frequencies, we use $\\Delta_1/ \\Delta_2 = 1.11$,\n$K/ \\Delta_2 = 4$, and $K = 2.20$, where $\\Delta_1 = \\omega_2 -\n\\omega_1$, and $\\Delta_2 = \\omega_3 - \\omega_2$. Now at both noise\nlevels, at cardinalities greater than 2, the cross-correlation fails\nto capture a significant portion of the available information --- as\n$\\hat{E}[I^m/I_N]$ is significantly less than one ---\nFig.~\\ref{Fig:Completeness}~B and E. This indicates that even small\namplitude noise can prevent the cross-correlation from accurately\nencoding information about the system in this case.  The mutual\ninformation again robustly encodes almost all of the possible\ninformation, $\\hat{E}[I^m/I_N] \\approx 1$, in both noise regimes and\nacross all discretizations analyzed.\n\n\nIn the final case, the weak coupling regime ($K/ \\Delta_2 = 0.99$, all\nother parameters as in the second case), we do not have a strong\nhypothesis for what $E[I^m/I_N]$ should be. In\nFig.~\\ref{Fig:Completeness}~C and F we can see that the\ncross-correlation encodes virtually no information about the system\nfor cardinalities greater than 2, $\\hat{E}[I^m/I_3] \\approx 0$.  The\nmutual information again robustly encodes the vast majority of the\nmulti-information, with $\\hat{E}[I^m/I_3] > 0.8$ for all noise levels\nand discretizations.\n\n\n\\paragraph{Resting-State Human Brain Networks:}\n\nTo illustrate the applicability of our methodology in real-world data\nsituations, we apply it to neuroimaging data, in a similar context as\nin~\\cite{Watanabe2013::NatCom}. In particular, we want to assess to\nwhat extent the multivariate activity distribution is determined by\npurely bivariate dependence patterns. The used data consist of time\nseries of functional magnetic resonance imaging signal from 96 healthy\nvolunteers measured using a 3T Siemens Magnetom Trio scanner in IKEM\n(Institute for Clinical and Experimental Medicine) in Prague, Czech\nRepublic. Average signals from 12 regions of the fronto-parietal\nnetwork were extracted using a brain atlas~\\cite{Shirer2011}. After\npreprocessing and denoising as in~\\cite{Hlinka2011Neuroimage}, the\ndata were temporally concatenated. Each variable was further\ndiscretized to 2 or 3 states using equiquantal binning.\n\nUsing our approach, we find $I^m/I_N=0.77$ for both the 2-state and\n3-state discretizations, suggesting that bivariate dependence patterns\ncapture the dominant proportion of the information. For 2-state\ndiscretization, this is smaller then\nin~\\cite{Watanabe2013::NatCom}. However, for the 3-state\ndiscretization it provides a much higher estimate of the bivariate\ndependence role than the method taking into account only correlations,\nas in the case of the Kuramoto model.  This suggests that only when\naccounting also for non-linear coupling, the bivariate dependences\nprovide sufficient data structure approximation resolving the apparent\ninconsistency of the results in~\\cite{Watanabe2013::NatCom}. This is\nalso true for other brain networks~\\cite{Martin2015::PRE}.\n\n\n\n\\paragraph{Discussion:}\n\nOur method allows for potential speedups over the maximum entropy\ncalculation when conditioning on the bivariate distributions, as well\nas when conditioning on the cross-correlations. In both of these cases\nsolving the associated Lagrange multiplier equations are non-linear\noptimization problems. The maximum entropy distribution could also be\nfound using iterative fitting routines\nlike~\\cite{Darroch1972::AnMatStat}, but in these cases the problem\nwill still scale like $n^N$ ($n$ is the cardinality of the\nvariables). While there are pathological linear optimization problems\nthat scale exponentially with $N$, there will always be a slightly\nperturbed problem such that our method will scale\npolynomially~\\cite{Vershynin2009::SJC}.\n\n\nResearchers have so far relied on conditioning on the\ncross-correlations when insufficient data is available to estimate the\nbivariate distributions. They either coarse grain to binary variables\nwhere it is equivalent to conditioning on the\ndistributions~\\cite{Watanabe2013::NatCom} --- potentially loosing\nimportant information --- or use higher cardinality variables where it\nis only a linear approximation~\\cite{Bialek2012::PNAS,Wood2012::PNAS}.\nOur approach based on mutual information can be applied in these\ncases; the associated entropies can be estimated with as few as\n$2^{H/2}$ data points~\\cite{Nemenman2011::Ent} ($H$ is measured in\nbits). While this maximization has previously been prohibitively\ndifficult, our work shows that it is feasible allowing it to become\nwidely applicable and serve as a starting point before considering\nmultivariate information\nmeasures~\\cite{timme14a,Kralemann2014::NJP}. Additionally, if our\nmethod returns a small $I^m/I_N$ this suggests both that the\nfaithfulness assumption used in causal inference is\nviolated~\\cite{Eichler2013causal,Sun2014::Ent,Runge2012::PRL,Schindler2007::PR},\nand that there is synergy among the variables~\\cite{Griffith2014}.\n\n\n\nOur calculation of $H_m$ for the mutual information is free of\ndistributional assumptions, computing the maximum entropy in the\ngeneral space of arbitrary cardinality variables. This may result in\nhigher entropy estimates than methods that consider predefined\ncardinality, e.g., binary variables. Notably, our simulations suggest\nthat estimating $H_m$ in this way provides comparable, or\nsubstantially lower, entropy estimates than $H_m$ for the\ncross-correlation, which explicitly constrains the cardinality.  This\nmakes the technique competitive even when a specific cardinality could\nbe reasonably assumed.\n\n\n\\paragraph{Conclusions:}\n\nIn this work we introduced a novel method to determine the importance\nof pairwise relationships by estimating the maximum entropy\nconditioned on the mutual informations. We showed that by mapping this\nproblem to a linear optimization problem it could also be efficiently\ncomputed.  Using the generic case of coupled oscillators we gave a\nproof of principle example where our method was able to widely\noutperform conditioning on the cross-correlations. The example of the\nresting-state brain network showed that this also carries over to real\nworld applications, highlighting the potential of the method when\ncardinalities larger than two and nonlinear behavior are important.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{acknowledgments}\n  This project was financially supported by NSERC (EM and JD) and by\n  the Czech Science Foundation project No. 13-23940S and the Czech\n  Health Research Council project NV15-29835A (JH). All authors would\n  like to thank the MPIPKS for its hospitality and hosting the\n  international seminar program ``Causality, Information Transfer and\n  Dynamical Networks'', which stimulated some of the involved\n  research. We also would like to thank P. Grassberger for many\n  helpful discussions.\n\\end{acknowledgments}\n\n\n\\appendix\n\\section{Supplementary Material}\n\nWe simulated a system of 100 Kuramoto oscillators, as described in our\nmanuscript, in two regimes: i) All oscillators are synchronized,\n$K=4$; ii) the oscillators are partially synchronized with more than\n20 different synchronized clusters, $K=1.75$. In both cases we use the\nsame set of intrinsic frequencies (drawn from a normal distribution\nwith mean zero and unit variance), and a noise level $G = 0.001$.\n\n\nSimilar to the analysis done in Ref.~\\cite{Schneidman2006::Nat}, we\nanalyzed the effects of sampling from a larger system by randomly\nselecting $T$ of the 100 oscillators and calculating $I^m/I_T$ for\nthose oscillators. For each tuple size, $T$, we repeated this one\nhundred times, using the same sets of tuples in both regimes, and\ncomputed $\\hat{E}[I^m/I_T]$ as the average of these values. As in our\nprevious examples, our method outperforms the cross-correlation in the\nsynchronized case (see Fig.~\\ref{Fig:SubSampSynch}), as well as for\nweaker coupling (see Fig.~\\ref{Fig:SubSampWeak}). Our method results\nin $\\hat{E}[I^m/I_T] \\lesssim 1$ in both regimes, and across all\ndiscritizations and tuple sizes, while the cross-correlation only does\nso for binary variables.\n\n\n\n\\begin{figure}[!h]\n\n  \\begin{center}                         \n    \\includegraphics*[width=\\columnwidth]{FracInfo-Reg-Avg-357tuples-Sampled100-N100G0001K4.pdf}\n   \\end{center}\n   \\caption{\\label{Fig:SubSampSynch} (Color online) The fraction of\n     shared information coded by the mutual information (MI) and the\n     cross-correlation (CC) for a tuple of size T.  We simulated 100\n     nodes with $K = 4$, $G=0.001$, and the estimated expectations,\n     $\\hat{E}[...]$, are averages over 100 randomly selected tuples of\n     the given size. All oscillators are synchronized, and their\n     intrinsic frequencies are drawn from a normal distribution with\n     zero mean and unit variance. Error bars are 25\\% and 75\\%\n     quantiles.}\n\\end{figure}\n\n\n\n\n\\begin{figure}[!h]\n\n  \\begin{center}                         \n    \\includegraphics*[width=\\columnwidth]{FracInfo-Reg-Avg-357tuples-Sampled100-N100G0001K175.pdf}\n   \\end{center}\n   \\caption{\\label{Fig:SubSampWeak} (Color online) The fraction of\n     shared information coded by the mutual information (MI) and the\n     cross-correlation (CC) for a tuple of size T.  We simulated 100\n     nodes with $K = 1.75$, $G=0.001$, and the estimated expectations,\n     $\\hat{E}[...]$, are averages over 100 randomly selected tuples of\n     the given size. The intrinsic frequencies and tuples are the same\n     as in Fig.~\\ref{Fig:SubSampSynch}. The oscillators are partially\n     synchronized with more than 20 different clusters. Error bars are\n     25\\% and 75\\% quantiles.}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 12130, "prevtext": "\n\nIn general, a system of N variables results in ${N \\choose 1}$\nunivariate entropy constraints, ${N \\choose 2}$ mutual information\nconstraints, and $A = \\sum_{k=1}^N {N \\choose k} = 2^N -1$ atoms to be\ndetermined. In the simplest case of $N = 3$ variables we have six\nconstraints and $A = 7$ regions to specify, see\nFig~\\ref{Fig:InfoDiag}. This means we only have one free parameter,\nmaking the maximization process particularly easy in this case; in\ngeneral there are $\\sum_{k=3}^N {N \\choose k}$ free parameters.\n\n\n\n\nApart from the chosen constraints defined above, there are also\ngeneral constraints on the values of the subregions ensuring they\ndefine a valid information diagram, i.e. that there exists a\nprobability distribution with corresponding information-theoretic\nquantities.  A family of such constraints (so-called Shannon\ninequalities) can be inferred from the fundamental requirement that,\nfor discrete variables, (conditional) entropies and mutual\ninformations are necessarily non-negative: A) $H(X_i | \\{X\\}_N - X_i)\n\\geq 0$; B) $I(X_i,X_j| \\{X\\}_K) \\geq 0$, where $i \\neq j$ and\n$\\{X\\}_K \\subseteq \\{X\\}_N - \\{X_i,X_j\\}$\n\n\\footnote{This set of equalities is minimal in the sense that no\ninequality is implied by any combination of the\nothers~\\cite{Yeung2008information}.}.\n\nEach inequality can also be\nrepresented as a sum of atomic regions, e.g. \n\n\n", "index": 3, "text": "\\begin{equation}\n  \\label{Eq:ShannonIneq}\n  I(X_1;X_2|X_3) = I(X_1;X_2|X_3,X_4) + I(X_1;X_2;X_4|X_3) \\geq 0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"I(X_{1};X_{2}|X_{3})=I(X_{1};X_{2}|X_{3},X_{4})+I(X_{1};X_{2};X_{4}|X_{3})\\geq&#10;0.\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>;</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msub><mi>X</mi><mn>3</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>;</mo><msub><mi>X</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo><msub><mi>X</mi><mn>3</mn></msub><mo>,</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mn>1</mn></msub><mo>;</mo><msub><mi>X</mi><mn>2</mn></msub><mo>;</mo><msub><mi>X</mi><mn>4</mn></msub><mo stretchy=\"false\">|</mo><msub><mi>X</mi><mn>3</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mn>0</mn><mo>.</mo></mrow></math>", "type": "latex"}]