[{"file": "1601.06884.tex", "nexttext": "\nwhere $g(t)$ is a smooth functional, and $f_{1}$ and $f_{2}$ are\nsmooth $d$-dimensional probability densities. If $g\\left(f_{1}(x),f_{2}(x)\\right)=g\\left(\\frac{f_{1}(x)}{f_{2}(x)}\\right),$\nis convex, and $g(1)=0$, then $G\\left(f_{1},f_{2}\\right)$ defines\nthe family of $f$-divergences. Some common divergences that belong\nto this family include the KL divergence ($g(t)=-\\ln t$), the R\\'{e}nyi-$\\alpha$\ndivergence ($g(t)=t^{\\alpha}$), and the total variation distance\n($g(t)=|t-1|$). In this work, we do not require $g$ to be convex\nnor $g(1)=0$ which allows us to consider a broader class of functionals.\n\nWe use a kernel density plug-in estimator of the divergence functional\nin (\\ref{eq:fdiv}). Assume that $N$ i.i.d. realizations $\\left\\{ \\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N}\\right\\} $\nare available from $f_{1}$ and $N$ i.i.d. realizations $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}\\right\\} $\nare available from $f_{2}$. Let $h>0$ be the kernel bandwidth for\nthe density estimators. Let $K(\\cdot)$ be a symmetric, product kernel\nfunction with bounded support in each dimension and $||K||_{\\infty}<\\infty$.\nWe focus on finite support kernels for simplicity in the proofs although\nit is likely that our results extend to some infinitely supported\nkernels as well. The KDEs are \n\\begin{eqnarray*}\n\\fth 1(\\mathbf{X}_{j}) & = & \\frac{1}{Nh^{d}}\\sum_{i=1}^{N}K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{i}}{h}\\right),\\\\\n\\fth 2(\\mathbf{X}_{j}) & = & \\frac{1}{Mh^{d}}\\sum_{\\substack{i=1\\\\\ni\\neq j\n}\n}^{N}K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{i}}{h}\\right),\n\\end{eqnarray*}\nwhere $M=N-1$. $G\\left(f_{1},f_{2}\\right)$ is then approximated\nas \n\n", "itemtype": "equation", "pos": 9380, "prevtext": "\n\\global\\long\\def\\et#1{\\tilde{\\mathbf{e}}_{#1,h_{#1}}}\n\\global\\long\\def\\gt{\\tilde{\\mathbf{G}}_{h_{1},h_{2}}}\n\\global\\long\\def\\gh{\\tilde{\\mathbf{G}}_{h}}\n\n\n\\global\\long\\def\\ft#1{\\tilde{\\mathbf{f}}_{#1,h_{#1}}}\n\\global\\long\\def\\fth#1{\\tilde{\\mathbf{f}}_{#1,h}}\n\\global\\long\\def\\ftl#1{\\tilde{\\mathbf{f}}_{#1,h(l)}}\n\\global\\long\\def\\et#1{\\tilde{\\mathbf{e}}_{#1,h_{#1}}}\n\\global\\long\\def\\eth#1{\\tilde{\\mathbf{e}}_{#1,h}}\n\n\n\\global\\long\\def\\bE{\\mathbb{E}}\n\\global\\long\\def\\ez{\\mathbb{E}_{\\mathbf{Z}}}\n\\global\\long\\def\\var{\\mathbb{V}}\n\\global\\long\\def\\bias{\\mathbb{B}}\n\\global\\long\\def\\S{\\mathcal{S}}\n\n\n\n\n\n\n\\author{\\IEEEauthorblockN{Kevin R. Moon\\IEEEauthorrefmark{1}, Kumar Sricharan\\IEEEauthorrefmark{2}, Kristjan Greenewald\\IEEEauthorrefmark{1}, Alfred O. Hero III\\IEEEauthorrefmark{1}} \\IEEEauthorblockA{\\IEEEauthorrefmark{1}EECS Dept., University of Michigan, \\{krmoon,greenewk,hero\\}@umich.edu} \\IEEEauthorblockA{\\IEEEauthorrefmark{2}Xerox PARC, sricharan.kumar@parc.com} \\thanks{This work was partially supported by NSF grant CCF-1217880 and a NSF Graduate Research Fellowship to the first author under Grant No. F031543.}}\n\n\n\\title{Improving Convergence of Divergence Functional Ensemble Estimators}\n\\maketitle\n\\begin{abstract}\nRecent work has focused on the problem of nonparametric estimation\nof divergence functionals. Many existing approaches are restrictive\nin their assumptions on the density support or require difficult calculations\nat the support boundary which must be known \\emph{a priori}. We derive\nthe MSE convergence rate of a leave-one-out kernel density plug-in\ndivergence functional estimator for general bounded density support\nsets where knowledge of the support boundary is not required. We then\ngeneralize the theory of optimally weighted ensemble estimation to\nderive two estimators that achieve the parametric rate when the densities\nare sufficiently smooth. The asymptotic distribution of these estimators\nand some guidelines for tuning parameter selection are provided. Based\non the theory, we propose an empirical estimator of R\\'enyi-$\\alpha$\ndivergence that outperforms the standard kernel density plug-in estimator,\nespecially in high dimension.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\nInformation divergence is a measure of the difference between probability\ndistributions and has many applications in the fields of information\ntheory, statistics, signal processing, and machine learning. Some\napplications involving divergences include estimating the decay rates\nof error probabilities~\\cite{cover2012elements}, estimating bounds\non the Bayes error~\\cite{avi1996bound,hashlamoun1994bound,moon2015Bayes,chernoff1952measure,berisha2014bound,moon2014nips},\ntesting the hypothesis that two sets of samples come from the same\nprobability distribution~\\cite{moon2015partI}, clustering~\\cite{dhillon2003cluster,banerjee2005clustering,lewi2006real},\nfeature selection and classification~\\cite{bruzzone1995feature,guorong1996feature,sakate2014variable},\nblind source separation~\\cite{hild2001blind,mihoko2002blind}, image\nsegmentation~\\cite{vemuri2011segment,hamza2003segmentation,liu2014segment},\nextending machine learning to algorithms to distributional features~\\cite{poczos2011estimation,oliva2013distribution,szabo2014distribution,moon2015partII},\nand steganography~\\cite{korzhik2015steganographic}. Additionally,\nmutual information and entropy are both special cases of divergences\nand have been used in some of the above applications as well as others\nsuch as determining channel capacity~\\cite{cover2012elements}, fMRI\ndata processing~\\cite{chai2009fMRI}, intrinsic dimension estimation~\\cite{carter2010local,moon2014icip},\nand texture classification and image registration~\\cite{hero2002applications}.\nFor many more applications of divergence measures, see~\\cite{basseville2013divergence}.\n\nAn important subset of information divergences is the family of $f$-divergences~\\cite{csiszar1967div,ali1966div}.\nThis family includes the well-known Kullback-Leibler (KL) divergence~\\cite{kullback1951divergence},\nthe R\\'{e}nyi-$\\alpha$ divergence~\\cite{renyi1961divergence},\nthe Hellinger-Bhattacharyya distance~\\cite{hellinger1909,bhattacharyya1946div},\nthe Chernoff-$\\alpha$ divergence~\\cite{chernoff1952measure}, and\nthe total variation distance. \n\nWe consider the problem of estimating divergence functionals when\nonly two finite populations of independent and identically distributed\n(i.i.d.) samples are available from two $d$-dimensional distributions\nthat are unknown, nonparametric, and smooth. This paper paper greatly\nimproves upon the results reported in \\cite{moon2014isit,moon2014nips}\nwhich explored $k$-nearest neighbor ($k$-nn) based estimators of\n$f$-divergences. We focus on plug-in kernel density estimators (KDEs)\nof general divergence functionals. We establish a central limit theorem\nfor the divergence estimator distribution and derive more general\nconditions on the required densities' smoothness for the mean squared\nerror (MSE) convergence rates. Specifically, we derive an ensemble\nestimator that achieves the parametric rate when the densities are\nat least $s\\geq(d+1)/2$ times differentiable, whereas \\cite{moon2014isit}\nrequires $s\\geq d$. We extend these estimators to more general bounded\ndensity support sets in $\\mathbb{R}^{d}$, whereas the proofs in \\cite{moon2014isit}\nrestricted the estimator to compactly supported densities with no\nboundary conditions (e.g. a support set equal to the surface of a\ntorus), which is unnecessarily restrictive. Finally, we use a leave-one-out\napproach that uses all of the data for both density estimation and\nintegral approximation in contrast to~\\cite{moon2014isit,moon2014nips,sricharan2013ensemble,sricharan2012estimation,krishnamurthy2014divergence,kandasamy2015nonparametric}\nwhich use a less efficient data-splitting approach.\n\nOther divergence functional estimators with parametric convergence\nrates have recently been published. Nguyen et al~\\cite{nguyen2010div}\nproposed a $f$-divergence estimator that estimates the likelihood\nratio of the two densities by solving a convex optimization problem\nand then plugs it into the divergence formulas. For this method they\nprove that the minimax MSE convergence rate is parametric ($O\\left(\\frac{1}{N}\\right)$)\nwhen the likelihood ratio is at least $d/2$ times differentiable.\nHowever, this estimator is restricted to true $f$-divergences instead\nof the broader class of divergence functionals considered in this\npaper. Furthermore, solving the convex problem of~\\cite{nguyen2010div}\ncan be more computationally demanding than our approach when $N$\nis large. \n\nOther recent work~\\cite{krishnamurthy2014divergence,kandasamy2015nonparametric,singh2014exponential,singh2014renyi}\nhas focused on estimating various divergence functionals by using\nan optimal KDE. These estimators achieve parametric convergence rates\nwhen the densities have at least $d$~ \\cite{singh2014renyi,singh2014exponential}\nor $d/2$~\\cite{krishnamurthy2014divergence,kandasamy2015nonparametric}\nderivatives. However, these optimal KDEs are difficult to construct\nwhen the density support set is bounded as they require knowledge\nof the support boundary and difficult computations at the boundary.\nAdditionally, numerical integration is required for some functionals,\nwhich can be computationally difficult. Other important functionals\nare excluded from the estimation framework in~\\cite{krishnamurthy2014divergence,singh2014renyi},\nincluding some that bound the Bayes error~\\cite{moon2015Bayes,berisha2014bound,avi1996bound}.\nIn contrast, our method applies to a large class of divergence functionals,\ndoes not require knowledge of the support boundary, and avoids numerical\nintegration.\n\nThe asymptotic distributions of the estimators in~\\cite{krishnamurthy2014divergence,singh2014exponential,singh2014renyi,nguyen2010div}\nare currently unknown. Kandasamy et al~\\cite{kandasamy2015nonparametric}\nprove a central limit theorem for their data-splitting estimator but\ndo not prove similar results for their leave-one-out estimator.\n\nIn Section~\\ref{sec:base_est}, we derive MSE convergence rates for\nkernel density plug-in divergence functional estimators. We derive\nthe asymptotic distribution of the weighted ensemble estimators in\nSection~\\ref{sec:weighted} which enables us to perform hypothesis\ntesting. We then generalize the theory of optimally weighted ensemble\nentropy estimation developed in~\\cite{sricharan2013ensemble} to\nobtain two divergence functional estimators with a MSE convergence\nrate of $O\\left(\\frac{1}{N}\\right)$, where $N$ is the sample size,\nwhen the densities are at least $d$ or $(d+1)/2$ times differentiable.\nThese estimators apply to general divergence functionals and are simpler\nto implement than other estimators that also achieve the parametric\nrate. Based on the theory derived in these sections, we then provide\nsome guidelines for selecting the tuning parameters in Section~\\ref{sec:tuning}\nfollowed by experimental validation for the special case of R\\'enyi-$\\alpha$\ndivergence estimation in Section~\\ref{sec:experiments}. Bold face\ntype is used for random variables and random vectors. The conditional\nexpectation given a random variable $\\mathbf{Z}$ is denoted $\\mathbb{E}_{\\mathbf{Z}}$. \n\n\n\\section{The Divergence Functional Weak Estimator}\n\n\\label{sec:base_est}We focus on estimating functionals of the form\n\n", "index": 1, "text": "\\begin{equation}\nG\\left(f_{1},f_{2}\\right)=\\int g\\left(f_{1}(x),f_{2}(x)\\right)f_{2}(x)dx,\\label{eq:fdiv}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"G\\left(f_{1},f_{2}\\right)=\\int g\\left(f_{1}(x),f_{2}(x)\\right)f_{2}(x)dx,\" display=\"block\"><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>x</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\nSimilar to~\\cite{sricharan2013ensemble,moon2014isit,moon2014nips},\nthe principal assumptions we make on the densities $f_{1}$ and $f_{2}$\nand the functional $g$ are that: 1) $f_{1}$, $f_{2},$ are $s$\ntimes differentiable and $g$ is infinitely differentiable; 2) $f_{1}$\nand $f_{2}$ have common bounded support sets $\\mathcal{S}$; 3) $f_{1}$\nand $f_{2}$ are strictly lower bounded on $\\S$. The smoothness assumptions\non the densities are relaxed compared to~\\cite{sricharan2013ensemble,moon2014isit,moon2014nips}.\nHowever, we assume stricter conditions on the smoothness of $g$ to\nenable us to achieve good rates without knowledge of the boundary\nof the support set. We also assume 4) that the support is smooth with\nrespect to the kernel $K(u)$ in the sense that the expectation of\nthe area outside of $\\mathcal{S}$ wrt any random variable $u$ with\nsmooth distribution is a smooth function of the bandwidth $h$. It\nis not necessary for $\\mathcal{S}$ to have smooth contours with no\nedges as this assumption is satisfied when $\\mathcal{S}=[-1,1]^{d}$\nand when $K(x)$ is the uniform rectangular kernel. See Appendix~\\ref{sec:assumptions}\nfor more details on all of the assumptions.\n\n\\begin{theorem} \\label{thm:bias} \\emph{For infinitely differentiable\n$g$, the bias of the plug-in estimator $\\gh$ is given by \n\n", "itemtype": "equation", "pos": 11138, "prevtext": "\nwhere $g(t)$ is a smooth functional, and $f_{1}$ and $f_{2}$ are\nsmooth $d$-dimensional probability densities. If $g\\left(f_{1}(x),f_{2}(x)\\right)=g\\left(\\frac{f_{1}(x)}{f_{2}(x)}\\right),$\nis convex, and $g(1)=0$, then $G\\left(f_{1},f_{2}\\right)$ defines\nthe family of $f$-divergences. Some common divergences that belong\nto this family include the KL divergence ($g(t)=-\\ln t$), the R\\'{e}nyi-$\\alpha$\ndivergence ($g(t)=t^{\\alpha}$), and the total variation distance\n($g(t)=|t-1|$). In this work, we do not require $g$ to be convex\nnor $g(1)=0$ which allows us to consider a broader class of functionals.\n\nWe use a kernel density plug-in estimator of the divergence functional\nin (\\ref{eq:fdiv}). Assume that $N$ i.i.d. realizations $\\left\\{ \\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N}\\right\\} $\nare available from $f_{1}$ and $N$ i.i.d. realizations $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}\\right\\} $\nare available from $f_{2}$. Let $h>0$ be the kernel bandwidth for\nthe density estimators. Let $K(\\cdot)$ be a symmetric, product kernel\nfunction with bounded support in each dimension and $||K||_{\\infty}<\\infty$.\nWe focus on finite support kernels for simplicity in the proofs although\nit is likely that our results extend to some infinitely supported\nkernels as well. The KDEs are \n\\begin{eqnarray*}\n\\fth 1(\\mathbf{X}_{j}) & = & \\frac{1}{Nh^{d}}\\sum_{i=1}^{N}K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{i}}{h}\\right),\\\\\n\\fth 2(\\mathbf{X}_{j}) & = & \\frac{1}{Mh^{d}}\\sum_{\\substack{i=1\\\\\ni\\neq j\n}\n}^{N}K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{i}}{h}\\right),\n\\end{eqnarray*}\nwhere $M=N-1$. $G\\left(f_{1},f_{2}\\right)$ is then approximated\nas \n\n", "index": 3, "text": "\\begin{equation}\n\\gh=\\frac{1}{N}\\sum_{i=1}^{N}g\\left(\\fth 1\\left(\\mathbf{X}_{i}\\right),\\fth 2\\left(\\mathbf{X}_{i}\\right)\\right).\\label{eq:estimator}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\gh=\\frac{1}{N}\\sum_{i=1}^{N}g\\left(\\fth 1\\left(\\mathbf{X}_{i}\\right),\\fth 2%&#10;\\left(\\mathbf{X}_{i}\\right)\\right).\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gh</mtext></merror><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\fth</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\fth</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n}\n\n\\emph{If $g(x,y)$ has $k,l$-th order mixed derivatives $\\frac{\\partial^{k+l}g(x,y)}{\\partial x^{k}\\partial y^{l}}$\nthat depend on $x,y$ only through $x^{\\alpha}y^{\\beta}$ for some\n$\\alpha,\\beta\\in\\mathbb{R}$, then for any positive integer $\\lambda\\geq2$,\nthe bias is \n\\begin{eqnarray}\n\\bias\\left[\\gh\\right] & = & \\sum_{j=1}^{s}c_{10,j}h^{j}+\\sum_{q=1}^{\\lambda/2}\\sum_{j=0}^{s}c_{11,q,j}\\frac{h^{j}}{\\left(Nh^{d}\\right)^{q}}\\nonumber \\\\\n &  & +O\\left(h^{s}+1/\\left(Nh^{d}\\right)^{\\frac{\\lambda}{2}}\\right).\\label{eq:bias2}\n\\end{eqnarray}\n}\n\n\\end{theorem}\n\nDivergence functionals that satisfy the mixed derivatives condition\nrequired for (\\ref{eq:bias2}) include the KL divergence and the R\\'{e}nyi-$\\alpha$\ndivergence. Obtaining higher order terms for other divergence functionals\nis left for future work.\n\n\\begin{theorem}\\label{thm:variance}\\emph{Assume that the functional\n$g$ is Lipschitz continuous in both arguments with constant $C_{g}$.\nThen the variance of the plug-in estimator $\\gh$ is bounded by \n", "itemtype": "equation", "pos": 12612, "prevtext": "\n\n\nSimilar to~\\cite{sricharan2013ensemble,moon2014isit,moon2014nips},\nthe principal assumptions we make on the densities $f_{1}$ and $f_{2}$\nand the functional $g$ are that: 1) $f_{1}$, $f_{2},$ are $s$\ntimes differentiable and $g$ is infinitely differentiable; 2) $f_{1}$\nand $f_{2}$ have common bounded support sets $\\mathcal{S}$; 3) $f_{1}$\nand $f_{2}$ are strictly lower bounded on $\\S$. The smoothness assumptions\non the densities are relaxed compared to~\\cite{sricharan2013ensemble,moon2014isit,moon2014nips}.\nHowever, we assume stricter conditions on the smoothness of $g$ to\nenable us to achieve good rates without knowledge of the boundary\nof the support set. We also assume 4) that the support is smooth with\nrespect to the kernel $K(u)$ in the sense that the expectation of\nthe area outside of $\\mathcal{S}$ wrt any random variable $u$ with\nsmooth distribution is a smooth function of the bandwidth $h$. It\nis not necessary for $\\mathcal{S}$ to have smooth contours with no\nedges as this assumption is satisfied when $\\mathcal{S}=[-1,1]^{d}$\nand when $K(x)$ is the uniform rectangular kernel. See Appendix~\\ref{sec:assumptions}\nfor more details on all of the assumptions.\n\n\\begin{theorem} \\label{thm:bias} \\emph{For infinitely differentiable\n$g$, the bias of the plug-in estimator $\\gh$ is given by \n\n", "index": 5, "text": "\\begin{equation}\n\\bias\\left[\\gh\\right]=\\sum_{j=1}^{s}c_{10,j}h^{j}+c_{11}\\frac{1}{Nh^{d}}+O\\left(h^{s}+\\frac{1}{Nh^{d}}\\right).\\label{eq:bias1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\bias\\left[\\gh\\right]=\\sum_{j=1}^{s}c_{10,j}h^{j}+c_{11}\\frac{1}{Nh^{d}}+O%&#10;\\left(h^{s}+\\frac{1}{Nh^{d}}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bias</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gh</mtext></merror><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mi>c</mi><mrow><mn>10</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msup><mi>h</mi><mi>j</mi></msup></mrow></mrow><mo>+</mo><mrow><msub><mi>c</mi><mn>11</mn></msub><mo>\u2062</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mi>h</mi><mi>s</mi></msup><mo>+</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $\\|K\\|_{\\infty}$ is the $\\ell_{\\infty}$ norm of the kernel\n$K$ in the KDE.}\n\n\\end{theorem}\n\nFrom Theorems~\\ref{thm:bias} and~\\ref{thm:variance}, it is clear\nthat we require $h\\rightarrow0$ and $Nh^{d}\\rightarrow\\infty$ for\n$\\gh$ to be unbiased while the variance of the plug-in estimator\ndepends primarily on the number of samples. The dominating terms in\nthe bias are $\\Theta\\left(h\\right)$ and $\\Theta\\left(\\frac{1}{Nh^{d}}\\right)$.\nIf no attempt is made to correct the bias, the optimal choice of $h$\nin terms of minimizing the MSE is $h^{*}=\\Theta\\left(N^{-1/(d+1)}\\right)$,\nresulting in a dominant bias term of $\\Theta\\left(N^{-1/(d+1)}\\right)$.\nNote that this differs from the standard result for the optimal KDE\nbandwidth which is $\\Theta\\left(N^{-1/\\left(d+4\\right)}\\right)$ for\nthe symmetric uniform kernel~\\cite{hansen2009lecture}.\n\n\n\\subsection{Proof Sketches of Theorems~\\ref{thm:bias} and~\\ref{thm:variance}}\n\n\\label{sec:Proofs}To prove the expressions for the bias, the bias\nis first decomposed into two parts by adding and subtracting $g\\left(\\ez\\fth 1(\\mathbf{Z}),\\ez\\fth 2(\\mathbf{Z})\\right)$\nwithin the expectation: a ``bias'' term and a ``variance'' term.\nApplying a Taylor series expansion on these terms results in expressions\nthat depend on powers of $\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]:=\\ez\\fth i(\\mathbf{Z})-f_{i}(\\mathbf{Z})$\nand $\\eth i(\\mathbf{Z}):=\\fth i(\\mathbf{Z})-\\ez\\fth i(\\mathbf{Z})$,\nrespectively. A point $X\\in\\mathcal{S}$ is defined to be in the interior\nof $\\mathcal{S}$ (denoted $\\mathcal{S}_{I}$) if for all $Y\\notin\\mathcal{S}$,\n$K\\left(\\frac{X-Y}{h}\\right)=0$. Applying KDE properties and a Taylor\nseries expansion of the densities gives for $\\mathbf{Z}\\in\\mathcal{S}_{I}$,\n\n", "itemtype": "equation", "pos": 13781, "prevtext": "\n}\n\n\\emph{If $g(x,y)$ has $k,l$-th order mixed derivatives $\\frac{\\partial^{k+l}g(x,y)}{\\partial x^{k}\\partial y^{l}}$\nthat depend on $x,y$ only through $x^{\\alpha}y^{\\beta}$ for some\n$\\alpha,\\beta\\in\\mathbb{R}$, then for any positive integer $\\lambda\\geq2$,\nthe bias is \n\\begin{eqnarray}\n\\bias\\left[\\gh\\right] & = & \\sum_{j=1}^{s}c_{10,j}h^{j}+\\sum_{q=1}^{\\lambda/2}\\sum_{j=0}^{s}c_{11,q,j}\\frac{h^{j}}{\\left(Nh^{d}\\right)^{q}}\\nonumber \\\\\n &  & +O\\left(h^{s}+1/\\left(Nh^{d}\\right)^{\\frac{\\lambda}{2}}\\right).\\label{eq:bias2}\n\\end{eqnarray}\n}\n\n\\end{theorem}\n\nDivergence functionals that satisfy the mixed derivatives condition\nrequired for (\\ref{eq:bias2}) include the KL divergence and the R\\'{e}nyi-$\\alpha$\ndivergence. Obtaining higher order terms for other divergence functionals\nis left for future work.\n\n\\begin{theorem}\\label{thm:variance}\\emph{Assume that the functional\n$g$ is Lipschitz continuous in both arguments with constant $C_{g}$.\nThen the variance of the plug-in estimator $\\gh$ is bounded by \n", "index": 7, "text": "\n\\[\n\\var\\left[\\gh\\right]\\leq\\frac{11C_{g}^{2}||K||_{\\infty}^{2}}{N},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\gh\\right]\\leq\\frac{11C_{g}^{2}||K||_{\\infty}^{2}}{N},\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gh</mtext></merror><mo>]</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mn>11</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow><mi>N</mi></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n A point $X\\in\\mathcal{S}$ is near the boundary of $\\mathcal{S}$\n(denoted $\\mathcal{S}_{B}$) if it is not in $\\mathcal{S}_{I}$. If\nthe boundary is sufficiently smooth, then for $\\mathbf{Z}$ restricted\nto $\\mathcal{S}_{B}$ and arbitrary $\\gamma$ with $\\sup_{x,y}|\\gamma(x,y)|<\\infty,$\n(Appendix~\\ref{sec:BiasProof}) \n", "itemtype": "equation", "pos": 15590, "prevtext": "\nwhere $\\|K\\|_{\\infty}$ is the $\\ell_{\\infty}$ norm of the kernel\n$K$ in the KDE.}\n\n\\end{theorem}\n\nFrom Theorems~\\ref{thm:bias} and~\\ref{thm:variance}, it is clear\nthat we require $h\\rightarrow0$ and $Nh^{d}\\rightarrow\\infty$ for\n$\\gh$ to be unbiased while the variance of the plug-in estimator\ndepends primarily on the number of samples. The dominating terms in\nthe bias are $\\Theta\\left(h\\right)$ and $\\Theta\\left(\\frac{1}{Nh^{d}}\\right)$.\nIf no attempt is made to correct the bias, the optimal choice of $h$\nin terms of minimizing the MSE is $h^{*}=\\Theta\\left(N^{-1/(d+1)}\\right)$,\nresulting in a dominant bias term of $\\Theta\\left(N^{-1/(d+1)}\\right)$.\nNote that this differs from the standard result for the optimal KDE\nbandwidth which is $\\Theta\\left(N^{-1/\\left(d+4\\right)}\\right)$ for\nthe symmetric uniform kernel~\\cite{hansen2009lecture}.\n\n\n\\subsection{Proof Sketches of Theorems~\\ref{thm:bias} and~\\ref{thm:variance}}\n\n\\label{sec:Proofs}To prove the expressions for the bias, the bias\nis first decomposed into two parts by adding and subtracting $g\\left(\\ez\\fth 1(\\mathbf{Z}),\\ez\\fth 2(\\mathbf{Z})\\right)$\nwithin the expectation: a ``bias'' term and a ``variance'' term.\nApplying a Taylor series expansion on these terms results in expressions\nthat depend on powers of $\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]:=\\ez\\fth i(\\mathbf{Z})-f_{i}(\\mathbf{Z})$\nand $\\eth i(\\mathbf{Z}):=\\fth i(\\mathbf{Z})-\\ez\\fth i(\\mathbf{Z})$,\nrespectively. A point $X\\in\\mathcal{S}$ is defined to be in the interior\nof $\\mathcal{S}$ (denoted $\\mathcal{S}_{I}$) if for all $Y\\notin\\mathcal{S}$,\n$K\\left(\\frac{X-Y}{h}\\right)=0$. Applying KDE properties and a Taylor\nseries expansion of the densities gives for $\\mathbf{Z}\\in\\mathcal{S}_{I}$,\n\n", "index": 9, "text": "\\begin{equation}\n\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]=\\sum_{j=1}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h^{2j}+O\\left(h^{s}\\right).\\label{eq:bias_interior}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]=\\sum_{j=1}^{\\left\\lfloor s/2%&#10;\\right\\rfloor}c_{i,j}(\\mathbf{Z})h^{2j}+O\\left(h^{s}\\right).\" display=\"block\"><mrow><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bias</mtext></merror><mi>\ud835\udc19</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\fth</mtext></merror><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo>\u230a</mo><mrow><mi>s</mi><mo>/</mo><mn>2</mn></mrow><mo>\u230b</mo></mrow></munderover><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>j</mi></mrow></msup></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>h</mi><mi>s</mi></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nCombining this result with (\\ref{eq:bias_interior}) gives an expression\nfor the terms involving $\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]$. \n\nFor the terms involving $\\et i(\\mathbf{Z})$, define the random variable\n$\\mathbf{V}_{i}(\\mathbf{Z})=K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h}\\right)-\\ez K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h}\\right)$.\nThen by the binomial theorem and properties of KDEs, \n", "itemtype": "equation", "pos": 16101, "prevtext": "\n A point $X\\in\\mathcal{S}$ is near the boundary of $\\mathcal{S}$\n(denoted $\\mathcal{S}_{B}$) if it is not in $\\mathcal{S}_{I}$. If\nthe boundary is sufficiently smooth, then for $\\mathbf{Z}$ restricted\nto $\\mathcal{S}_{B}$ and arbitrary $\\gamma$ with $\\sup_{x,y}|\\gamma(x,y)|<\\infty,$\n(Appendix~\\ref{sec:BiasProof}) \n", "index": 11, "text": "\n\\[\n\\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{Z}}^{t}\\left[\\fth i(\\mathbf{Z})\\right]\\right]=\\sum_{j=1}^{s}c_{4,i,j,t}h^{j}+o\\left(h^{s}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{%&#10;Z}}^{t}\\left[\\fth i(\\mathbf{Z})\\right]\\right]=\\sum_{j=1}^{s}c_{4,i,j,t}h^{j}+o%&#10;\\left(h^{s}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bias</mtext></merror><mi>\ud835\udc19</mi><mi>t</mi></msubsup><mo>\u2062</mo><mrow><mo>[</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\fth</mtext></merror><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mi>c</mi><mrow><mn>4</mn><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><msup><mi>h</mi><mi>j</mi></msup></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>h</mi><mi>s</mi></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThese expressions can then be used to simplify $\\ez\\left[\\eth 2^{q}(\\mathbf{Z})\\right]$.\nFor example, $\\ez\\left[\\eth 2^{2}(\\mathbf{Z})\\right]=\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})/\\left(Nh^{2d}\\right)$.\nA similar approach can be used for $\\ez\\left[\\eth 1^{q}(\\mathbf{Z})\\right]$.\nFor general $g$, we only have that $\\partial^{k+l}g\\left(\\ez\\fth 1(\\mathbf{Z}),\\ez\\fth 2(\\mathbf{Z})\\right)/\\left(\\partial x^{k}\\partial y^{l}\\right)=\\partial^{k+l}g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)/\\left(\\partial x^{k}\\partial y^{l}\\right)+o(1)$.\nThus we can only gaurantee the bias expansion up to the $O\\left(1/\\left(Nh^{d}\\right)\\right)$\nterm in (\\ref{eq:bias1}). However, if the mixed derivatives of $g(x,y)$\nhave the form of $x^{\\alpha}y^{\\beta}$, then we can apply the generalized\nbinomial theorem to the derivatives of $g$ evaluated at $\\ez\\fth i(\\mathbf{Z})$\nto obtain (\\ref{eq:bias2}).\n\nThe proof of the variance bound uses the Efron-Stein inequality which\nbounds the variance by analyzing the expected squared difference between\nthe plug-in estimator when one sample is allowed to differ. It can\nbe shown that if $\\mathbf{X}_{1}$ and $\\mathbf{X}_{1}^{'}$ are drawn\nindependently from $f_{2}$, then \n\\begin{eqnarray*}\n\\bE\\left[\\left|\\fth i(\\mathbf{X}_{1})-\\fth i(\\mathbf{X}_{1}^{'})\\right|^{2}\\right] & \\leq & 2||K||_{\\infty}^{2},\\\\\n\\bE\\left[\\left|\\fth i(\\mathbf{X}_{j})-\\fth i^{'}(\\mathbf{X}_{j})\\right|^{2}\\right] & \\leq & \\frac{2||K||_{\\infty}^{2}}{(N-1)^{2}},\n\\end{eqnarray*}\nwhere we denote $\\fth i^{'}$ and $\\gh^{'}$ as the estimators with\n$\\mathbf{X}_{1}$ replaced with $\\mathbf{X}_{1}^{'}$. Applying the\nLipschitz condition and Jensen's inequality with these results gives\n", "itemtype": "equation", "pos": 16694, "prevtext": "\nCombining this result with (\\ref{eq:bias_interior}) gives an expression\nfor the terms involving $\\bias_{\\mathbf{Z}}\\left[\\fth i(\\mathbf{Z})\\right]$. \n\nFor the terms involving $\\et i(\\mathbf{Z})$, define the random variable\n$\\mathbf{V}_{i}(\\mathbf{Z})=K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h}\\right)-\\ez K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h}\\right)$.\nThen by the binomial theorem and properties of KDEs, \n", "index": 13, "text": "\n\\[\n\\ez\\left[\\mathbf{V}_{i}^{j}(\\mathbf{Z})\\right]=h^{d}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,j,m}(\\mathbf{Z})h^{2m}+O\\left(h^{2d}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\ez\\left[\\mathbf{V}_{i}^{j}(\\mathbf{Z})\\right]=h^{d}\\sum_{m=0}^{\\left\\lfloor s%&#10;/2\\right\\rfloor}c_{3,2,j,m}(\\mathbf{Z})h^{2m}+O\\left(h^{2d}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><msubsup><mi>\ud835\udc15</mi><mi>i</mi><mi>j</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>h</mi><mi>d</mi></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mo>\u230a</mo><mrow><mi>s</mi><mo>/</mo><mn>2</mn></mrow><mo>\u230b</mo></mrow></munderover><mrow><msub><mi>c</mi><mrow><mn>3</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi></mrow></msup></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>d</mi></mrow></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nA similar result can be derived when $\\mathbf{Y}_{1}$ and $\\mathbf{Y}_{1}^{'}$\nare instead considered. The Efron-Stein inequality finishes the proof.\nThe full proofs of both theorems are given in Appendix~\\ref{sec:BiasProof}\nand \\ref{sec:VarProof}.\n\n\n\\section{Weighted Ensemble Estimation}\n\n\\label{sec:weighted}Theorem~\\ref{thm:bias} shows that when the\ndimension of the data is not small, the bias of the plug-in estimator\n$\\gh$ decreases very slowly as a function of sample size, resulting\nin large MSE. However, by applying the theory of optimally weighted\nensemble estimation, developed in~\\cite{sricharan2013ensemble} for\nentropy estimation, we can take a weighted sum of an ensemble of estimators\nand decrease the bias via an appropriate choice of weights.\n\nWe form an ensemble of estimators by choosing different values of\n$h$. Choose $\\bar{l}=\\left\\{ l_{1},\\dots,l_{L}\\right\\} $ to be real\npositive numbers that index $h(l_{i})$. Thus the parameter $l$ indexes\nover different neighborhood sizes for the KDEs. Define $w:=\\left\\{ w\\left(l_{1}\\right),\\dots,w\\left(l_{L}\\right)\\right\\} $\nand $\\tilde{\\mathbf{G}}_{w}:=\\sum_{l\\in\\bar{l}}w(l)\\tilde{\\mathbf{G}}_{h(l)}.$\nThe key to reducing the MSE is to choose the weight vector $w$ to\nreduce the lower order terms in the bias without substantially increasing\nthe variance. \n\nWe first present the asymptotic distribution of $\\tilde{\\mathbf{G}}_{w}$\nwhich enables us to perform inference tasks like hypothesis testing\non the divergence functional. The proof is based on the Efron-Stein\ninequality and an application of Slutsky's Theorem. Details are given\nin~\\cite{moon2016isitlong}.\n\n\\begin{theorem}\\label{thm:clt}\\emph{Assume that the functional $g$\nis Lipschitz in both arguments with constant $C_{g}$. Further assume\nthat $h=o(1)$, $N\\rightarrow\\infty$, and $Nh^{d}\\rightarrow\\infty$.\nThen for fixed $L$, the asymptotic distribution of the weighted ensemble\nestimator $\\tilde{\\mathbf{G}}_{w}$ is \n", "itemtype": "equation", "pos": 18529, "prevtext": "\nThese expressions can then be used to simplify $\\ez\\left[\\eth 2^{q}(\\mathbf{Z})\\right]$.\nFor example, $\\ez\\left[\\eth 2^{2}(\\mathbf{Z})\\right]=\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})/\\left(Nh^{2d}\\right)$.\nA similar approach can be used for $\\ez\\left[\\eth 1^{q}(\\mathbf{Z})\\right]$.\nFor general $g$, we only have that $\\partial^{k+l}g\\left(\\ez\\fth 1(\\mathbf{Z}),\\ez\\fth 2(\\mathbf{Z})\\right)/\\left(\\partial x^{k}\\partial y^{l}\\right)=\\partial^{k+l}g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)/\\left(\\partial x^{k}\\partial y^{l}\\right)+o(1)$.\nThus we can only gaurantee the bias expansion up to the $O\\left(1/\\left(Nh^{d}\\right)\\right)$\nterm in (\\ref{eq:bias1}). However, if the mixed derivatives of $g(x,y)$\nhave the form of $x^{\\alpha}y^{\\beta}$, then we can apply the generalized\nbinomial theorem to the derivatives of $g$ evaluated at $\\ez\\fth i(\\mathbf{Z})$\nto obtain (\\ref{eq:bias2}).\n\nThe proof of the variance bound uses the Efron-Stein inequality which\nbounds the variance by analyzing the expected squared difference between\nthe plug-in estimator when one sample is allowed to differ. It can\nbe shown that if $\\mathbf{X}_{1}$ and $\\mathbf{X}_{1}^{'}$ are drawn\nindependently from $f_{2}$, then \n\\begin{eqnarray*}\n\\bE\\left[\\left|\\fth i(\\mathbf{X}_{1})-\\fth i(\\mathbf{X}_{1}^{'})\\right|^{2}\\right] & \\leq & 2||K||_{\\infty}^{2},\\\\\n\\bE\\left[\\left|\\fth i(\\mathbf{X}_{j})-\\fth i^{'}(\\mathbf{X}_{j})\\right|^{2}\\right] & \\leq & \\frac{2||K||_{\\infty}^{2}}{(N-1)^{2}},\n\\end{eqnarray*}\nwhere we denote $\\fth i^{'}$ and $\\gh^{'}$ as the estimators with\n$\\mathbf{X}_{1}$ replaced with $\\mathbf{X}_{1}^{'}$. Applying the\nLipschitz condition and Jensen's inequality with these results gives\n", "index": 15, "text": "\n\\[\n\\bE\\left[\\left|\\gh-\\gh^{'}\\right|^{2}\\right]\\leq20C_{g}^{2}||K||_{\\infty}^{2}/N^{2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left|\\gh-\\gh^{{}^{\\prime}}\\right|^{2}\\right]\\leq 20C_{g}^{2}||K||_{%&#10;\\infty}^{2}/N^{2}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>|</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gh</mtext></merror><mo>-</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gh</mtext></merror><msup><mi/><mo>\u2032</mo></msup></msup></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mn>20</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow><mo>/</mo><msup><mi>N</mi><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $\\mathbf{S}$ is a standard normal random variable.}\\end{theorem}\n\nThe theory of optimally weighted ensemble estimation is a general\ntheory originally presented by Sricharan et al~\\cite{sricharan2013ensemble}\nthat can be applied to many estimation problems as long as the bias\nand variance of the estimator can be expressed in a specific way.\nWe generalize the conditions required to apply the theory. Let $\\bar{l}=\\left\\{ l_{1},\\dots,l_{L}\\right\\} $\nbe a set of index values, e.g., kernel widths, and $N$ the number\nof samples available. For an indexed ensemble of estimators $\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$\nof a parameter $E$, the weighted ensemble estimator with weights\n$w=\\left\\{ w\\left(l_{1}\\right),\\dots,w\\left(l_{L}\\right)\\right\\} $\nsatisfying $\\sum_{l\\in\\bar{l}}w(l)=1$ is defined as $\\hat{\\mathbf{E}}_{w}=\\sum_{l\\in\\bar{l}}w\\left(l\\right)\\hat{\\mathbf{E}}_{l}.$\n$\\hat{\\mathbf{E}}_{w}$ is asyptotically unbiased if the estimators\n$\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$ are asymptotically\nunbiased. Consider: \n\\begin{itemize}\n\\item $\\mathcal{C}.1$ For each $l\\in\\bar{l}$, the bias of $\\hat{\\mathbf{E}}_{l}$\nis given by \n", "itemtype": "equation", "pos": 20571, "prevtext": "\nA similar result can be derived when $\\mathbf{Y}_{1}$ and $\\mathbf{Y}_{1}^{'}$\nare instead considered. The Efron-Stein inequality finishes the proof.\nThe full proofs of both theorems are given in Appendix~\\ref{sec:BiasProof}\nand \\ref{sec:VarProof}.\n\n\n\\section{Weighted Ensemble Estimation}\n\n\\label{sec:weighted}Theorem~\\ref{thm:bias} shows that when the\ndimension of the data is not small, the bias of the plug-in estimator\n$\\gh$ decreases very slowly as a function of sample size, resulting\nin large MSE. However, by applying the theory of optimally weighted\nensemble estimation, developed in~\\cite{sricharan2013ensemble} for\nentropy estimation, we can take a weighted sum of an ensemble of estimators\nand decrease the bias via an appropriate choice of weights.\n\nWe form an ensemble of estimators by choosing different values of\n$h$. Choose $\\bar{l}=\\left\\{ l_{1},\\dots,l_{L}\\right\\} $ to be real\npositive numbers that index $h(l_{i})$. Thus the parameter $l$ indexes\nover different neighborhood sizes for the KDEs. Define $w:=\\left\\{ w\\left(l_{1}\\right),\\dots,w\\left(l_{L}\\right)\\right\\} $\nand $\\tilde{\\mathbf{G}}_{w}:=\\sum_{l\\in\\bar{l}}w(l)\\tilde{\\mathbf{G}}_{h(l)}.$\nThe key to reducing the MSE is to choose the weight vector $w$ to\nreduce the lower order terms in the bias without substantially increasing\nthe variance. \n\nWe first present the asymptotic distribution of $\\tilde{\\mathbf{G}}_{w}$\nwhich enables us to perform inference tasks like hypothesis testing\non the divergence functional. The proof is based on the Efron-Stein\ninequality and an application of Slutsky's Theorem. Details are given\nin~\\cite{moon2016isitlong}.\n\n\\begin{theorem}\\label{thm:clt}\\emph{Assume that the functional $g$\nis Lipschitz in both arguments with constant $C_{g}$. Further assume\nthat $h=o(1)$, $N\\rightarrow\\infty$, and $Nh^{d}\\rightarrow\\infty$.\nThen for fixed $L$, the asymptotic distribution of the weighted ensemble\nestimator $\\tilde{\\mathbf{G}}_{w}$ is \n", "index": 17, "text": "\n\\[\nPr\\left(\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]\\right)/\\sqrt{\\var\\left[\\tilde{\\mathbf{G}}_{w}\\right]}\\leq t\\right)\\rightarrow Pr(\\mathbf{S}\\leq t),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"Pr\\left(\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]%&#10;\\right)/\\sqrt{\\var\\left[\\tilde{\\mathbf{G}}_{w}\\right]}\\leq t\\right)\\rightarrow&#10;Pr%&#10;(\\mathbf{S}\\leq t),\" display=\"block\"><mrow><mi>P</mi><mi>r</mi><mrow><mo>(</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\ud835\udc06</mi><mo stretchy=\"false\">~</mo></mover><mi>w</mi></msub><mo>-</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc06</mi><mo stretchy=\"false\">~</mo></mover><mi>w</mi></msub><mo>]</mo></mrow><mo>)</mo></mrow><mo>/</mo><msqrt><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc06</mi><mo stretchy=\"false\">~</mo></mover><mi>w</mi></msub><mo>]</mo></mrow></mrow></msqrt><mo>\u2264</mo><mi>t</mi><mo>)</mo></mrow><mo>\u2192</mo><mi>P</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc12</mi><mo>\u2264</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $c_{i}$ are constants depending on the underlying density,\n$J=\\left\\{ i_{1},\\dots,i_{I}\\right\\} $ is a finite index set with\n$I<L$, and $\\psi_{i}(l)$ are basis functions depending only on the\nparameter $l$ and not the sample size. \n\\item $\\mathcal{C}.2$ For each $l\\in\\bar{l}$, the variance of $\\hat{\\mathbf{E}}_{l}$\nis given by \n", "itemtype": "equation", "pos": 21922, "prevtext": "\nwhere $\\mathbf{S}$ is a standard normal random variable.}\\end{theorem}\n\nThe theory of optimally weighted ensemble estimation is a general\ntheory originally presented by Sricharan et al~\\cite{sricharan2013ensemble}\nthat can be applied to many estimation problems as long as the bias\nand variance of the estimator can be expressed in a specific way.\nWe generalize the conditions required to apply the theory. Let $\\bar{l}=\\left\\{ l_{1},\\dots,l_{L}\\right\\} $\nbe a set of index values, e.g., kernel widths, and $N$ the number\nof samples available. For an indexed ensemble of estimators $\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$\nof a parameter $E$, the weighted ensemble estimator with weights\n$w=\\left\\{ w\\left(l_{1}\\right),\\dots,w\\left(l_{L}\\right)\\right\\} $\nsatisfying $\\sum_{l\\in\\bar{l}}w(l)=1$ is defined as $\\hat{\\mathbf{E}}_{w}=\\sum_{l\\in\\bar{l}}w\\left(l\\right)\\hat{\\mathbf{E}}_{l}.$\n$\\hat{\\mathbf{E}}_{w}$ is asyptotically unbiased if the estimators\n$\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$ are asymptotically\nunbiased. Consider: \n\\begin{itemize}\n\\item $\\mathcal{C}.1$ For each $l\\in\\bar{l}$, the bias of $\\hat{\\mathbf{E}}_{l}$\nis given by \n", "index": 19, "text": "\n\\[\n\\bias\\left[\\hat{\\mathbf{E}}_{l}\\right]=\\sum_{i\\in J}c_{i}\\psi_{i}(l)\\phi_{i,d}(N)+O\\left(\\frac{1}{\\sqrt{N}}\\right),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\bias\\left[\\hat{\\mathbf{E}}_{l}\\right]=\\sum_{i\\in J}c_{i}\\psi_{i}(l)\\phi_{i,d}%&#10;(N)+O\\left(\\frac{1}{\\sqrt{N}}\\right),\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bias</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc04</mi><mo stretchy=\"false\">^</mo></mover><mi>l</mi></msub><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>J</mi></mrow></munder><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03d5</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><msqrt><mi>N</mi></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\\end{itemize}\n\\begin{theorem}\\label{thm:ensemble}\\emph{Assume conditions $\\mathcal{C}.1$\nand $\\mathcal{C}.2$ hold for an ensemble of estimators $\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$.\nThen there exists a weight vector $w_{0}$ such that \n", "itemtype": "equation", "pos": 22380, "prevtext": "\nwhere $c_{i}$ are constants depending on the underlying density,\n$J=\\left\\{ i_{1},\\dots,i_{I}\\right\\} $ is a finite index set with\n$I<L$, and $\\psi_{i}(l)$ are basis functions depending only on the\nparameter $l$ and not the sample size. \n\\item $\\mathcal{C}.2$ For each $l\\in\\bar{l}$, the variance of $\\hat{\\mathbf{E}}_{l}$\nis given by \n", "index": 21, "text": "\n\\[\n\\var\\left[\\hat{\\mathbf{E}}_{l}\\right]=c_{v}\\left(\\frac{1}{N}\\right)+o\\left(\\frac{1}{N}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\hat{\\mathbf{E}}_{l}\\right]=c_{v}\\left(\\frac{1}{N}\\right)+o\\left(%&#10;\\frac{1}{N}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc04</mi><mo stretchy=\"false\">^</mo></mover><mi>l</mi></msub><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThe weight vector $w_{0}$ is the solution to the following convex\noptimization problem:\n\n", "itemtype": "equation", "pos": 22734, "prevtext": "\n\n\\end{itemize}\n\\begin{theorem}\\label{thm:ensemble}\\emph{Assume conditions $\\mathcal{C}.1$\nand $\\mathcal{C}.2$ hold for an ensemble of estimators $\\left\\{ \\hat{\\mathbf{E}}_{l}\\right\\} _{l\\in\\bar{l}}$.\nThen there exists a weight vector $w_{0}$ such that \n", "index": 23, "text": "\n\\[\n\\mathbb{E}\\left[\\left(\\hat{\\mathbf{E}}_{w_{0}}-E\\right)^{2}\\right]=O\\left(\\frac{1}{N}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{E}\\left[\\left(\\hat{\\mathbf{E}}_{w_{0}}-E\\right)^{2}\\right]=O\\left(%&#10;\\frac{1}{N}\\right).\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc04</mi><mo stretchy=\"false\">^</mo></mover><msub><mi>w</mi><mn>0</mn></msub></msub><mo>-</mo><mi>E</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n}\n\n\\end{theorem}\n\\begin{IEEEproof}\nA more restrictive version of Theorem~\\ref{thm:ensemble} was originally\npresented in~\\cite{sricharan2013ensemble} with the stricter condition\nof $\\phi_{i,d}(N)=N^{-1/(2d)}$. The proof of our generalized version\n(Theorem~\\ref{thm:ensemble}) is sketched here. From condition $\\mathcal{C}.1$,\nthe bias of the weighted estimator is \n", "itemtype": "equation", "pos": 22923, "prevtext": "\nThe weight vector $w_{0}$ is the solution to the following convex\noptimization problem:\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{array}{rl}\n\\min_{w} & ||w||_{2}\\\\\nsubject\\, to & \\sum_{l\\in\\bar{l}}w(l)=1,\\\\\n & \\gamma_{w}(i)=\\sum_{l\\in\\bar{l}}w(l)\\psi_{i}(l)=0,\\, i\\in J.\n\\end{array}\\label{eq:optimize}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{rl}\\min_{w}&amp;||w||_{2}\\\\&#10;subject\\,to&amp;\\sum_{l\\in\\bar{l}}w(l)=1,\\\\&#10;&amp;\\gamma_{w}(i)=\\sum_{l\\in\\bar{l}}w(l)\\psi_{i}(l)=0,\\,i\\in J.\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mi>w</mi></munder></mtd><mtd columnalign=\"left\"><msub><mrow><mo fence=\"true\">||</mo><mi>w</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>t</mi></mpadded><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>o</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mover accent=\"true\"><mi>l</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></munder><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><msub><mi>\u03b3</mi><mi>w</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mover accent=\"true\"><mi>l</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></munder><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>J</mi></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThe variance of the weighted estimator is bounded as \n\n", "itemtype": "equation", "pos": 23497, "prevtext": "\n}\n\n\\end{theorem}\n\\begin{IEEEproof}\nA more restrictive version of Theorem~\\ref{thm:ensemble} was originally\npresented in~\\cite{sricharan2013ensemble} with the stricter condition\nof $\\phi_{i,d}(N)=N^{-1/(2d)}$. The proof of our generalized version\n(Theorem~\\ref{thm:ensemble}) is sketched here. From condition $\\mathcal{C}.1$,\nthe bias of the weighted estimator is \n", "index": 27, "text": "\n\\[\n\\bias\\left[\\hat{\\mathbf{E}}_{w}\\right]=\\sum_{i\\in J}c_{i}\\gamma_{w}(i)\\phi_{i,d}(N)+O\\left(\\frac{\\sqrt{L}||w||_{2}}{\\sqrt{N}}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\bias\\left[\\hat{\\mathbf{E}}_{w}\\right]=\\sum_{i\\in J}c_{i}\\gamma_{w}(i)\\phi_{i,%&#10;d}(N)+O\\left(\\frac{\\sqrt{L}||w||_{2}}{\\sqrt{N}}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bias</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc04</mi><mo stretchy=\"false\">^</mo></mover><mi>w</mi></msub><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>J</mi></mrow></munder><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03b3</mi><mi>w</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03d5</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><msqrt><mi>L</mi></msqrt><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>w</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow><msqrt><mi>N</mi></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThe optimization problem in Eq.~(\\ref{eq:optimize}) zeroes out the\nlower-order bias terms and limits the $\\ell_{2}$ norm of the weight\nvector $w$ to limit the variance contribution. This results in an\nMSE rate of $O_{w}(1/N)$ when the dimension $d$ is fixed and when\n$L$ is fixed independently of the sample size $N$. Furthermore,\na solution to Eq.~(\\ref{eq:optimize}) is guaranteed to exist as\nlong as $L>I$ and the vectors $a_{i}=\\left[\\psi_{i}(l_{1}),\\dots,\\psi_{i}(l_{L})\\right]$\nare linearly independent. \n\\end{IEEEproof}\nTo achieve the parametric rate $O\\left(1/N\\right)$ in MSE convergence\nit is not necessary that $\\gamma_{w}(i)=0,\\, i\\in J$. Solving the\nfollowing convex optimization problem in place of the optimization\nproblem in Theorem~\\ref{thm:ensemble} retains the $O(1/N)$ rate:\n\n", "itemtype": "equation", "pos": 23692, "prevtext": "\nThe variance of the weighted estimator is bounded as \n\n", "index": 29, "text": "\\begin{equation}\n\\var\\left[\\hat{\\mathbf{E}}_{w}\\right]\\leq\\frac{L||w||_{2}^{2}}{N}.\\label{eq:ens_var}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\hat{\\mathbf{E}}_{w}\\right]\\leq\\frac{L||w||_{2}^{2}}{N}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mover accent=\"true\"><mi>\ud835\udc04</mi><mo stretchy=\"false\">^</mo></mover><mi>w</mi></msub><mo>]</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mi>L</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>w</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mi>N</mi></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere the parameter $\\eta$ is chosen to achieve trade-off between\nbias and variance. Instead of forcing $\\gamma_{w}(i)=0$, the relaxed\noptimization problem uses the weights to decrease the bias terms at\nthe rate of $O_{w}\\left(1/\\sqrt{N}\\right)$ yielding an MSE of $O_{w}(1/N).$\n\nWe refer to the distributional functional estimators obtained using\nthis theory as \\textbf{O}ptimally Weighted \\textbf{Di}stributional\nFu\\textbf{n}ctional (ODin) estimators. Sricharan et al~\\cite{sricharan2013ensemble}\napplied a more restrictive version of Theorem~\\ref{thm:ensemble}\nto obtain an entropy estimator with convergence rate $O_{w}(1/N)$.\nWe apply the same theory to obtain a divergence functional estimator\nwith the same asymptotic rate. Let $h(l)=lN^{-1/(2d)}$. From Theorem~\\ref{thm:bias},\n$\\psi_{i}(l)=l^{i}$, $i=1,\\dots,d$. Note that if $s\\geq d$, then\nwe are left with $O\\left(\\frac{1}{l^{d}\\sqrt{N}}\\right)$ in addition\nto the terms in the sum. To obtain a uniform bound on the bias with\nrespect to $w$ and $\\bar{l}$, we also include the function $\\psi_{d+1}(l)=l^{-d}$\nin the optimization problem. The bias of the resulting base estimator\nsatisfies condition $\\mathcal{C}.1$ with $\\phi_{i,d}(N)=N^{-i/(2d)}$\nfor $i=1,\\dots,d$ and $\\phi_{d+1,d}(N)=N^{-1/2}.$ The variance also\nsatisfies condition $\\mathcal{C}.2$. The optimal weight $w_{0}$\nis found by using Eq.~(\\ref{eq:relaxed}) to obtain a plug-in divergence\nfunctional estimator $\\tilde{\\mathbf{G}}_{w_{0},1}$ with an MSE convergence\nrate of $O_{w_{0}}\\left(\\frac{1}{N}\\right)$ as long as $L\\geq d$\nand $s\\geq d$. Otherwise, we can only guarantee the rate up to $O_{w_{0}}\\left(\\frac{1}{N^{s/d}}\\right)$.\nWe refer to this estimator as the ODin1 estimator.\n\nWe define another weighted ensemble estimator that imposes less restrictive\nassumptions on the densities' smoothness by letting $h(l)$ decrease\nat a faster rate. Let $h(l)=lN^{\\frac{-1}{d+1}}$. From Theorem~\\ref{thm:bias},\nif $g(x,y)$ has mixed partial derivatives of the form of $x^{\\alpha}y^{\\beta}$,\nthen the bias has terms proportional to $l^{j-dq}N^{-\\frac{j+q}{d+1}}$\nwhere $j,q\\geq0$ and $j+q>0$. Theorem~\\ref{thm:ensemble} can then\nbe applied to this ensemble of estimators. Let $\\phi_{j,q,d}(N)=N^{-\\frac{j+q}{d+1}}$\nand $\\psi_{j,q}(l)=l^{j-dq}$. Let \n\n", "itemtype": "equation", "pos": 24604, "prevtext": "\nThe optimization problem in Eq.~(\\ref{eq:optimize}) zeroes out the\nlower-order bias terms and limits the $\\ell_{2}$ norm of the weight\nvector $w$ to limit the variance contribution. This results in an\nMSE rate of $O_{w}(1/N)$ when the dimension $d$ is fixed and when\n$L$ is fixed independently of the sample size $N$. Furthermore,\na solution to Eq.~(\\ref{eq:optimize}) is guaranteed to exist as\nlong as $L>I$ and the vectors $a_{i}=\\left[\\psi_{i}(l_{1}),\\dots,\\psi_{i}(l_{L})\\right]$\nare linearly independent. \n\\end{IEEEproof}\nTo achieve the parametric rate $O\\left(1/N\\right)$ in MSE convergence\nit is not necessary that $\\gamma_{w}(i)=0,\\, i\\in J$. Solving the\nfollowing convex optimization problem in place of the optimization\nproblem in Theorem~\\ref{thm:ensemble} retains the $O(1/N)$ rate:\n\n", "index": 31, "text": "\\begin{equation}\n\\begin{array}{rl}\n\\min_{w} & \\epsilon\\\\\nsubject\\, to & \\sum_{l\\in\\bar{l}}w(l)=1,\\,\\left\\Vert w\\right\\Vert _{2}^{2}\\leq\\eta,\\\\\n & \\left|\\gamma_{w}(i)N^{\\frac{1}{2}}\\phi_{i,d}(N)\\right|\\leq\\epsilon,\\,\\, i\\in J,\n\\end{array}\\label{eq:relaxed}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{rl}\\min_{w}&amp;\\epsilon\\\\&#10;subject\\,to&amp;\\sum_{l\\in\\bar{l}}w(l)=1,\\,\\left\\|w\\right\\|_{2}^{2}\\leq\\eta,\\\\&#10;&amp;\\left|\\gamma_{w}(i)N^{\\frac{1}{2}}\\phi_{i,d}(N)\\right|\\leq\\epsilon,\\,\\,i\\in J%&#10;,\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mi>w</mi></munder></mtd><mtd columnalign=\"left\"><mi>\u03f5</mi></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>t</mi></mpadded><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>o</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mover accent=\"true\"><mi>l</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></munder><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo rspace=\"4.2pt\">,</mo><mrow><msubsup><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mi>\u03b7</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mo>|</mo><mrow><msub><mi>\u03b3</mi><mi>w</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>N</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mi>\u03d5</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>|</mo></mrow><mo>\u2264</mo><mi>\u03f5</mi></mrow><mo rspace=\"5.9pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>J</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n Then from Eq.~(\\ref{eq:bias2}), the bias of $\\tilde{\\mathbf{G}}_{h(l)}$\nsatisfies condition $\\mathcal{C}.1$. If $L>|J|$, then Theorem~\\ref{thm:ensemble}\ncan be applied to obtain the optimal weight vector. The corresponding\nestimator $\\tilde{\\mathbf{G}}_{w_{0},2}$ achieves the parametric\nconvergence rate if $\\lambda\\geq d+1$ and if $s\\geq(d+1)/2$. This\nis more in line with existing estimators that achieve parametric rates\nof MSE convergence when $s\\geq\\frac{d}{2}$~\\cite{krishnamurthy2014divergence}.\n$\\tilde{\\mathbf{G}}_{w_{0},2}$ is referred to as the ODin2 estimator\nand is summarized in Algorithm~\\ref{alg:estimator}.\n\n\\begin{algorithm*}\n\\begin{algorithmic}[1]\n\n\\REQUIRE $\\eta$, $L$ positive real numbers $\\bar{l}$, samples\n$\\left\\{ \\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N}\\right\\} $ from $f_{1}$,\nsamples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}\\right\\} $ from\n$f_{2}$, dimension $d$, function $g$, kernel $K$\n\n\\ENSURE The optimally weighted divergence estimator $\\tilde{\\mathbf{G}}_{w_{0},2}$\n\n\\STATE Solve for $w_{0}$ using Eq.~\\ref{eq:relaxed} with $\\phi_{j,q,d}(N)=N^{-\\frac{j+q}{d+1}}$\nand basis functions $\\psi_{j,q}(l)=l^{j-dq}$, $l\\in\\bar{l}$, and\n$\\{i,j\\}\\in J$ defined in Eq.~\\ref{eq:J}\n\n\\FORALL{$l\\in\\bar{l}$}\n\n\\STATE $h(l)\\leftarrow lN^{\\frac{-1}{d+1}}$\n\n\\FOR{$i=1$ to $N$}\n\n\\STATE $\\ftl 1(\\mathbf{X}_{i})\\leftarrow\\frac{1}{Nh(l)^{d}}\\sum_{j=1}^{N}K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Y}_{j}}{h(l)}\\right)$,\n$\\ftl 2(\\mathbf{X}_{i})\\leftarrow\\frac{1}{(N-1)h(l)^{d}}\\sum_{\\substack{j=1\\\\\nj\\neq i\n}\n}^{N}K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{X}_{j}}{h(l)}\\right)$\n\n\\ENDFOR \n\n\\STATE $\\tilde{\\mathbf{G}}_{h(l)}\\leftarrow\\frac{1}{N}\\sum_{i=1}^{N}g\\left(\\ftl 1(\\mathbf{X}_{i}),\\ftl 2(\\mathbf{X}_{i})\\right)$\n\n\\ENDFOR \n\n\\STATE $\\tilde{\\mathbf{G}}_{w_{0},2}\\leftarrow\\sum_{l\\in\\bar{l}}w_{0}(l)\\tilde{\\mathbf{G}}_{h(l)}$\n\n\\end{algorithmic}\n\n\\caption{Optimally weighted ensemble estimator of divergence functionals \\label{alg:estimator}}\n\\end{algorithm*}\n\n\nThese improvements come at a cost in the number of kernel parameters\n$L$. If $s\\geq\\frac{d+1}{2}$ then the size of $J$ is on the order\nof $d^{2}/8$. This may lead to increased variance of the ensemble\nestimator for high dimensions (see Eq.~(\\ref{eq:ens_var})). Future\nwork is also required to extend ODin2 to other functionals of interest.\n\n\n\\section{Tuning Parameter Selection}\n\n\\label{sec:tuning}The optimization problem in Eq.~(\\ref{eq:relaxed})\nhas the parameters $\\eta$, $L$, and $\\bar{l}$. The parameter $\\eta$\nprovides an upper bound on the norm of the weight vector which gives\nan upper bound on the constant in the variance of the ensemble estimator.\nIf all the constants in Eq.~(\\ref{eq:bias1}) or Eq.~(\\ref{eq:bias2})\nand an exact expression for the variance of the ensemble estimator\nwere known, then $\\eta$ could be chosen to minimize the MSE. Since\nthe constants are unknown, by applying Eq.~(\\ref{eq:relaxed}), the\nresulting MSE of the ensemble estimator is $O\\left(\\epsilon^{2}/N\\right)+O\\left(L\\eta^{2}/N\\right),$\nwhere each term in the sum comes from the bias and variance, respectively.\nSince there is a tradeoff between $\\eta$ and $\\epsilon$, setting\n$\\eta=\\epsilon/\\sqrt{L}$ would minimize these terms in theory. In\npractice, we find that the variance of the ensemble estimator is less\nthan the upper bound of $L\\eta^{2}/N$ and setting $\\eta=\\epsilon/\\sqrt{L}$\nis therefore too restrictive. Setting $\\eta=\\epsilon$ instead works\nwell in practice.\n\nFor fixed $L$, the set of kernel widths $\\bar{l}$ can in theory\nbe chosen by minimizing $\\epsilon$ in Eq.~(\\ref{eq:relaxed}) over\n$\\bar{l}$ in addition to $w$. However, this results in a nonconvex\noptimization problem since $w$ does not lie in the non-negative orthant.\nA parameter search may not be practical as $\\epsilon$ generally decreases\nas the size and spread of $\\bar{l}$ increases. This decrease in $\\epsilon$\ndoes not always correspond to a decrease in MSE as high and low values\nof $h(l)$ can lead to inaccurate density estimates. Denote the value\nof the minimum value of $l$ so that $\\tilde{\\mathbf{f}}_{i,h(l_{min})}(\\mathbf{X}_{j})>0$\n$\\forall i=1,2$ as $l_{min}$ and the diameter of the support $\\mathcal{S}$\nas $D$. To ensure the density estimates are bounded away from zero,\nwe require that $\\min(\\bar{l})\\geq l_{min}$. The weights in $w_{0}$\nare generally largest for the smallest values of $\\bar{l}$ so $\\min(\\bar{l})$\nshould also be sufficiently larger than $l_{min}$ to render an adequate\nestimate. Similarly, $\\max(\\bar{l})$ should be sufficiently smaller\nthan $D$ as high bandwidth values lead to high bias. The remaining\n$\\bar{l}$ values are chosen to be equally spaced between $\\min(\\bar{l})$\nand $\\max(\\bar{l})$.\n\nLarge $L$ leads to increase in the similarity of bandwidth values\n$h(l)$ and basis functions $\\psi_{i,d}(l)$, which results in a negligible\ndecrease in the bias. Hence $L$ should be chosen large enough to\ndecrease the bias but small enough so that the $h(l)$ values are\nsufficiently distinct (typically $30\\leq L\\leq60$). \n\n\n\\section{Experiments: R\\'enyi-$\\alpha$ divergence }\n\n\\label{sec:experiments}To validate our theory, we estimated the R\\'{e}nyi-$\\alpha$\ndivergence integral between two truncated multivariate Gaussian distributions\nwith varying dimension and sample sizes. The densities have means\n$\\bar{\\mu}_{1}=0.7*\\bar{1}_{d}$, $\\bar{\\mu}_{2}=0.3*\\bar{1}_{d}$\nand covariance matrices $0.1*I_{d}$ where $\\bar{1}_{d}$ is a $d$-dimensional\nvector of ones, and $I_{d}$ is a $d$-dimensional identity matrix.\nWe used $\\alpha=0.5$ and restricted the Gaussians to the unit cube.\n\nThe top figure in Fig.~\\ref{fig:mse} shows the MSE (200 trials)\nof the uniform kernel plug-in estimator, the two proposed optimally\nweighted estimators, and a linear combination of ODin1 and ODin2,\n$\\tilde{\\mathbf{G}}_{\\rho}=(1-\\rho)\\tilde{\\mathbf{G}}_{w_{0},1}+\\rho\\tilde{\\mathbf{G}}_{w_{0},2}$,\nfor various dimensions and sample sizes. $\\bar{l}$, $L$, and $\\rho$\nare tuned to minimize the MSE. The bandwidth for the plug-in estimator\nis the value from those used in the ODin2 ensemble that minimizes\nthe MSE. Note that for $d=4$, the plug-in estimator performs comparably\nwith the optimally weighted estimators. However, for $d=7,10$, the\nplug-in estimator performs considerably worse. This demonstrates the\nstrength of ensemble estimation as the weighted sum of these poor\nestimators results in a very good estimator. \n\nThe bottom figure in Fig.~\\ref{fig:mse} shows the corresponding\naverage estimates with standard error bars compared to the true values.\nODin1 has smaller variance than ODin2 when $d=4$ and slightly larger\nvariance when $d=10$. The average values for $\\rho$ are $0.16,\\,0.59,\\,0.61$\nwhen $d=4,\\,7,\\,10$, which indicates a preference for ODin1 for low\ndimension and a slight preference for ODin2 for higher dimensions.\nPaired t-tests on the MSE (125 trials) of the two methods indicate\nthat the differences are statistically significant (see Table~\\ref{tab:-ttests}). \n\n\\begin{figure}\n\\centering\n\n\\includegraphics[width=0.5\\textwidth]{mse_v2}\\includegraphics[width=0.5\\textwidth]{errorbars}\n\n\\caption{\\label{fig:mse}(Top) Log-log plot of MSE of the uniform kernel plug-in\n(``Kernel''), the two proposed optimally weighted estimators (ODin1\nand ODin2), and the optimal linear combination of ODin1 and ODin2\nfor various dimensions and sample sizes. (Bottom) Plot of the average\nvalue of the same estimators with standard error bars compared to\nthe true values being estimated when $d=4,\\,10$. The proposed weighted\nensemble estimators match the theoretical rate (average log-log slope\nis $-0.98$ and $-0.99$ for ODin1 and ODin2, respectively) and perform\nbetter than the plug-in estimator for high dimensions.}\n\\end{figure}\n\n\n\\begin{table}\n\\centering\n\n\\caption{\\label{tab:-ttests}$p$-values of paired t-tests of ODin1 vs. ODin2\nMSE ($N=1300$).}\n\n\n\\begin{tabular}{|c|c|c|c|}\n\\hline \nDim. & ODin1$>$ODin2 & ODin1$<$ODin2 & ODin1$=$ODin2\\tabularnewline\n\\hline \n\\hline \n4 & 1 & 0 & $1.8\\times10^{-58}$\\tabularnewline\n\\hline \n10 & 0 & 1 & $1.0\\times10^{-52}$\\tabularnewline\n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\n\\section{Conclusion}\n\nWe derived convergence rates for a kernel density plug-in estimator\nof divergence functionals. We generalized the theory of optimally\nweighted ensemble estimation and derived an estimator that achieves\nthe parametric rate when the densities are only $(d+1)/2$ times differentiable.\nThe estimators we derive apply to general bounded density support\nsets and do not require knowledge of the support which is a distinct\nadvantage over other estimators. We also derived the asymptotic distribution\nof the estimator, provided some guidelines for tuning parameter selection,\nand validated our theory for the case of empirical estimation of the\nR\\'enyi-$\\alpha$ divergence. \n\n\\appendices\n\n\n\\section{Assumptions}\n\n\\label{sec:assumptions}Our assumptions are as follows:\n\\begin{itemize}\n\\item $(\\mathcal{A}.0)$: Assume that the kernel $K$ is symmetric, is a\nproduct kernel, and has bounded support in each dimension. Also assume\nthat it has order $\\nu$ which means that the $j$th moment of the\nkernel $K_{i}$ defined as $\\int t^{j}K_{i}(t)dt$ is zero for all\n$j=1,\\dots,\\nu-1$ and $i=1,\\dots,d$ where $K_{i}$ is the kernel\nin the $i$th coordinate.\n\\item $(\\mathcal{A}.1)$: Assume there exist constants $\\epsilon_{0},\\epsilon_{\\infty}$\nsuch that $0<\\epsilon_{0}\\leq f_{i}(x)\\leq\\epsilon_{\\infty}<\\infty,\\,\\forall x\\in S.$ \n\\item $(\\mathcal{A}.2)$: Assume that the densities $f_{i}\\in\\Sigma(s,K)$\nin the interior of $\\mathcal{S}$ with $s\\geq2$.\n\\item $(\\mathcal{A}.3)$: Assume that $g$ is infinitely differentiable.\n\\item $(\\mathcal{A}.4$): Assume that $\\left|g^{(j)}\\left(f_{1}(x)/f_{2}(x)\\right)\\right|$,\n$j=0,1,\\ldots$ are strictly upper bounded for $\\epsilon_{0}\\leq f_{i}(x)\\leq\\epsilon_{\\infty}.$ \n\\item $(\\mathcal{A}.5)$: Assume the following boundary smoothness condition:\nLet $p_{x}(u):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a polynomial\nof order $q\\leq r=\\left\\lfloor s\\right\\rfloor $ whose coefficients\nare a function of $x$ and are $r-q$ times differentiable. Then assume\nthat \n", "itemtype": "equation", "pos": 27147, "prevtext": "\nwhere the parameter $\\eta$ is chosen to achieve trade-off between\nbias and variance. Instead of forcing $\\gamma_{w}(i)=0$, the relaxed\noptimization problem uses the weights to decrease the bias terms at\nthe rate of $O_{w}\\left(1/\\sqrt{N}\\right)$ yielding an MSE of $O_{w}(1/N).$\n\nWe refer to the distributional functional estimators obtained using\nthis theory as \\textbf{O}ptimally Weighted \\textbf{Di}stributional\nFu\\textbf{n}ctional (ODin) estimators. Sricharan et al~\\cite{sricharan2013ensemble}\napplied a more restrictive version of Theorem~\\ref{thm:ensemble}\nto obtain an entropy estimator with convergence rate $O_{w}(1/N)$.\nWe apply the same theory to obtain a divergence functional estimator\nwith the same asymptotic rate. Let $h(l)=lN^{-1/(2d)}$. From Theorem~\\ref{thm:bias},\n$\\psi_{i}(l)=l^{i}$, $i=1,\\dots,d$. Note that if $s\\geq d$, then\nwe are left with $O\\left(\\frac{1}{l^{d}\\sqrt{N}}\\right)$ in addition\nto the terms in the sum. To obtain a uniform bound on the bias with\nrespect to $w$ and $\\bar{l}$, we also include the function $\\psi_{d+1}(l)=l^{-d}$\nin the optimization problem. The bias of the resulting base estimator\nsatisfies condition $\\mathcal{C}.1$ with $\\phi_{i,d}(N)=N^{-i/(2d)}$\nfor $i=1,\\dots,d$ and $\\phi_{d+1,d}(N)=N^{-1/2}.$ The variance also\nsatisfies condition $\\mathcal{C}.2$. The optimal weight $w_{0}$\nis found by using Eq.~(\\ref{eq:relaxed}) to obtain a plug-in divergence\nfunctional estimator $\\tilde{\\mathbf{G}}_{w_{0},1}$ with an MSE convergence\nrate of $O_{w_{0}}\\left(\\frac{1}{N}\\right)$ as long as $L\\geq d$\nand $s\\geq d$. Otherwise, we can only guarantee the rate up to $O_{w_{0}}\\left(\\frac{1}{N^{s/d}}\\right)$.\nWe refer to this estimator as the ODin1 estimator.\n\nWe define another weighted ensemble estimator that imposes less restrictive\nassumptions on the densities' smoothness by letting $h(l)$ decrease\nat a faster rate. Let $h(l)=lN^{\\frac{-1}{d+1}}$. From Theorem~\\ref{thm:bias},\nif $g(x,y)$ has mixed partial derivatives of the form of $x^{\\alpha}y^{\\beta}$,\nthen the bias has terms proportional to $l^{j-dq}N^{-\\frac{j+q}{d+1}}$\nwhere $j,q\\geq0$ and $j+q>0$. Theorem~\\ref{thm:ensemble} can then\nbe applied to this ensemble of estimators. Let $\\phi_{j,q,d}(N)=N^{-\\frac{j+q}{d+1}}$\nand $\\psi_{j,q}(l)=l^{j-dq}$. Let \n\n", "index": 33, "text": "\\begin{equation}\nJ=\\left\\{ \\{j,q\\}|0<j+q\\leq(d+1)/2,\\, q\\in\\{0,1,\\dots,\\lambda/2\\},\\, j\\in\\{0,1,\\dots,s\\}\\right\\} .\\label{eq:J}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"J=\\left\\{\\{j,q\\}|0&lt;j+q\\leq(d+1)/2,\\,q\\in\\{0,1,\\dots,\\lambda/2\\},\\,j\\in\\{0,1,%&#10;\\dots,s\\}\\right\\}.\" display=\"block\"><mrow><mrow><mi>J</mi><mo>=</mo><mrow><mo>{</mo><mrow><mo stretchy=\"false\">{</mo><mi>j</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mn>0</mn><mo>&lt;</mo><mrow><mi>j</mi><mo>+</mo><mi>q</mi></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mi>q</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mi>\u03bb</mi><mo>/</mo><mn>2</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $v_{t}(h)$ admits the expansion \n", "itemtype": "equation", "pos": 37369, "prevtext": "\n Then from Eq.~(\\ref{eq:bias2}), the bias of $\\tilde{\\mathbf{G}}_{h(l)}$\nsatisfies condition $\\mathcal{C}.1$. If $L>|J|$, then Theorem~\\ref{thm:ensemble}\ncan be applied to obtain the optimal weight vector. The corresponding\nestimator $\\tilde{\\mathbf{G}}_{w_{0},2}$ achieves the parametric\nconvergence rate if $\\lambda\\geq d+1$ and if $s\\geq(d+1)/2$. This\nis more in line with existing estimators that achieve parametric rates\nof MSE convergence when $s\\geq\\frac{d}{2}$~\\cite{krishnamurthy2014divergence}.\n$\\tilde{\\mathbf{G}}_{w_{0},2}$ is referred to as the ODin2 estimator\nand is summarized in Algorithm~\\ref{alg:estimator}.\n\n\\begin{algorithm*}\n\\begin{algorithmic}[1]\n\n\\REQUIRE $\\eta$, $L$ positive real numbers $\\bar{l}$, samples\n$\\left\\{ \\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N}\\right\\} $ from $f_{1}$,\nsamples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N}\\right\\} $ from\n$f_{2}$, dimension $d$, function $g$, kernel $K$\n\n\\ENSURE The optimally weighted divergence estimator $\\tilde{\\mathbf{G}}_{w_{0},2}$\n\n\\STATE Solve for $w_{0}$ using Eq.~\\ref{eq:relaxed} with $\\phi_{j,q,d}(N)=N^{-\\frac{j+q}{d+1}}$\nand basis functions $\\psi_{j,q}(l)=l^{j-dq}$, $l\\in\\bar{l}$, and\n$\\{i,j\\}\\in J$ defined in Eq.~\\ref{eq:J}\n\n\\FORALL{$l\\in\\bar{l}$}\n\n\\STATE $h(l)\\leftarrow lN^{\\frac{-1}{d+1}}$\n\n\\FOR{$i=1$ to $N$}\n\n\\STATE $\\ftl 1(\\mathbf{X}_{i})\\leftarrow\\frac{1}{Nh(l)^{d}}\\sum_{j=1}^{N}K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Y}_{j}}{h(l)}\\right)$,\n$\\ftl 2(\\mathbf{X}_{i})\\leftarrow\\frac{1}{(N-1)h(l)^{d}}\\sum_{\\substack{j=1\\\\\nj\\neq i\n}\n}^{N}K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{X}_{j}}{h(l)}\\right)$\n\n\\ENDFOR \n\n\\STATE $\\tilde{\\mathbf{G}}_{h(l)}\\leftarrow\\frac{1}{N}\\sum_{i=1}^{N}g\\left(\\ftl 1(\\mathbf{X}_{i}),\\ftl 2(\\mathbf{X}_{i})\\right)$\n\n\\ENDFOR \n\n\\STATE $\\tilde{\\mathbf{G}}_{w_{0},2}\\leftarrow\\sum_{l\\in\\bar{l}}w_{0}(l)\\tilde{\\mathbf{G}}_{h(l)}$\n\n\\end{algorithmic}\n\n\\caption{Optimally weighted ensemble estimator of divergence functionals \\label{alg:estimator}}\n\\end{algorithm*}\n\n\nThese improvements come at a cost in the number of kernel parameters\n$L$. If $s\\geq\\frac{d+1}{2}$ then the size of $J$ is on the order\nof $d^{2}/8$. This may lead to increased variance of the ensemble\nestimator for high dimensions (see Eq.~(\\ref{eq:ens_var})). Future\nwork is also required to extend ODin2 to other functionals of interest.\n\n\n\\section{Tuning Parameter Selection}\n\n\\label{sec:tuning}The optimization problem in Eq.~(\\ref{eq:relaxed})\nhas the parameters $\\eta$, $L$, and $\\bar{l}$. The parameter $\\eta$\nprovides an upper bound on the norm of the weight vector which gives\nan upper bound on the constant in the variance of the ensemble estimator.\nIf all the constants in Eq.~(\\ref{eq:bias1}) or Eq.~(\\ref{eq:bias2})\nand an exact expression for the variance of the ensemble estimator\nwere known, then $\\eta$ could be chosen to minimize the MSE. Since\nthe constants are unknown, by applying Eq.~(\\ref{eq:relaxed}), the\nresulting MSE of the ensemble estimator is $O\\left(\\epsilon^{2}/N\\right)+O\\left(L\\eta^{2}/N\\right),$\nwhere each term in the sum comes from the bias and variance, respectively.\nSince there is a tradeoff between $\\eta$ and $\\epsilon$, setting\n$\\eta=\\epsilon/\\sqrt{L}$ would minimize these terms in theory. In\npractice, we find that the variance of the ensemble estimator is less\nthan the upper bound of $L\\eta^{2}/N$ and setting $\\eta=\\epsilon/\\sqrt{L}$\nis therefore too restrictive. Setting $\\eta=\\epsilon$ instead works\nwell in practice.\n\nFor fixed $L$, the set of kernel widths $\\bar{l}$ can in theory\nbe chosen by minimizing $\\epsilon$ in Eq.~(\\ref{eq:relaxed}) over\n$\\bar{l}$ in addition to $w$. However, this results in a nonconvex\noptimization problem since $w$ does not lie in the non-negative orthant.\nA parameter search may not be practical as $\\epsilon$ generally decreases\nas the size and spread of $\\bar{l}$ increases. This decrease in $\\epsilon$\ndoes not always correspond to a decrease in MSE as high and low values\nof $h(l)$ can lead to inaccurate density estimates. Denote the value\nof the minimum value of $l$ so that $\\tilde{\\mathbf{f}}_{i,h(l_{min})}(\\mathbf{X}_{j})>0$\n$\\forall i=1,2$ as $l_{min}$ and the diameter of the support $\\mathcal{S}$\nas $D$. To ensure the density estimates are bounded away from zero,\nwe require that $\\min(\\bar{l})\\geq l_{min}$. The weights in $w_{0}$\nare generally largest for the smallest values of $\\bar{l}$ so $\\min(\\bar{l})$\nshould also be sufficiently larger than $l_{min}$ to render an adequate\nestimate. Similarly, $\\max(\\bar{l})$ should be sufficiently smaller\nthan $D$ as high bandwidth values lead to high bias. The remaining\n$\\bar{l}$ values are chosen to be equally spaced between $\\min(\\bar{l})$\nand $\\max(\\bar{l})$.\n\nLarge $L$ leads to increase in the similarity of bandwidth values\n$h(l)$ and basis functions $\\psi_{i,d}(l)$, which results in a negligible\ndecrease in the bias. Hence $L$ should be chosen large enough to\ndecrease the bias but small enough so that the $h(l)$ values are\nsufficiently distinct (typically $30\\leq L\\leq60$). \n\n\n\\section{Experiments: R\\'enyi-$\\alpha$ divergence }\n\n\\label{sec:experiments}To validate our theory, we estimated the R\\'{e}nyi-$\\alpha$\ndivergence integral between two truncated multivariate Gaussian distributions\nwith varying dimension and sample sizes. The densities have means\n$\\bar{\\mu}_{1}=0.7*\\bar{1}_{d}$, $\\bar{\\mu}_{2}=0.3*\\bar{1}_{d}$\nand covariance matrices $0.1*I_{d}$ where $\\bar{1}_{d}$ is a $d$-dimensional\nvector of ones, and $I_{d}$ is a $d$-dimensional identity matrix.\nWe used $\\alpha=0.5$ and restricted the Gaussians to the unit cube.\n\nThe top figure in Fig.~\\ref{fig:mse} shows the MSE (200 trials)\nof the uniform kernel plug-in estimator, the two proposed optimally\nweighted estimators, and a linear combination of ODin1 and ODin2,\n$\\tilde{\\mathbf{G}}_{\\rho}=(1-\\rho)\\tilde{\\mathbf{G}}_{w_{0},1}+\\rho\\tilde{\\mathbf{G}}_{w_{0},2}$,\nfor various dimensions and sample sizes. $\\bar{l}$, $L$, and $\\rho$\nare tuned to minimize the MSE. The bandwidth for the plug-in estimator\nis the value from those used in the ODin2 ensemble that minimizes\nthe MSE. Note that for $d=4$, the plug-in estimator performs comparably\nwith the optimally weighted estimators. However, for $d=7,10$, the\nplug-in estimator performs considerably worse. This demonstrates the\nstrength of ensemble estimation as the weighted sum of these poor\nestimators results in a very good estimator. \n\nThe bottom figure in Fig.~\\ref{fig:mse} shows the corresponding\naverage estimates with standard error bars compared to the true values.\nODin1 has smaller variance than ODin2 when $d=4$ and slightly larger\nvariance when $d=10$. The average values for $\\rho$ are $0.16,\\,0.59,\\,0.61$\nwhen $d=4,\\,7,\\,10$, which indicates a preference for ODin1 for low\ndimension and a slight preference for ODin2 for higher dimensions.\nPaired t-tests on the MSE (125 trials) of the two methods indicate\nthat the differences are statistically significant (see Table~\\ref{tab:-ttests}). \n\n\\begin{figure}\n\\centering\n\n\\includegraphics[width=0.5\\textwidth]{mse_v2}\\includegraphics[width=0.5\\textwidth]{errorbars}\n\n\\caption{\\label{fig:mse}(Top) Log-log plot of MSE of the uniform kernel plug-in\n(``Kernel''), the two proposed optimally weighted estimators (ODin1\nand ODin2), and the optimal linear combination of ODin1 and ODin2\nfor various dimensions and sample sizes. (Bottom) Plot of the average\nvalue of the same estimators with standard error bars compared to\nthe true values being estimated when $d=4,\\,10$. The proposed weighted\nensemble estimators match the theoretical rate (average log-log slope\nis $-0.98$ and $-0.99$ for ODin1 and ODin2, respectively) and perform\nbetter than the plug-in estimator for high dimensions.}\n\\end{figure}\n\n\n\\begin{table}\n\\centering\n\n\\caption{\\label{tab:-ttests}$p$-values of paired t-tests of ODin1 vs. ODin2\nMSE ($N=1300$).}\n\n\n\\begin{tabular}{|c|c|c|c|}\n\\hline \nDim. & ODin1$>$ODin2 & ODin1$<$ODin2 & ODin1$=$ODin2\\tabularnewline\n\\hline \n\\hline \n4 & 1 & 0 & $1.8\\times10^{-58}$\\tabularnewline\n\\hline \n10 & 0 & 1 & $1.0\\times10^{-52}$\\tabularnewline\n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\n\\section{Conclusion}\n\nWe derived convergence rates for a kernel density plug-in estimator\nof divergence functionals. We generalized the theory of optimally\nweighted ensemble estimation and derived an estimator that achieves\nthe parametric rate when the densities are only $(d+1)/2$ times differentiable.\nThe estimators we derive apply to general bounded density support\nsets and do not require knowledge of the support which is a distinct\nadvantage over other estimators. We also derived the asymptotic distribution\nof the estimator, provided some guidelines for tuning parameter selection,\nand validated our theory for the case of empirical estimation of the\nR\\'enyi-$\\alpha$ divergence. \n\n\\appendices\n\n\n\\section{Assumptions}\n\n\\label{sec:assumptions}Our assumptions are as follows:\n\\begin{itemize}\n\\item $(\\mathcal{A}.0)$: Assume that the kernel $K$ is symmetric, is a\nproduct kernel, and has bounded support in each dimension. Also assume\nthat it has order $\\nu$ which means that the $j$th moment of the\nkernel $K_{i}$ defined as $\\int t^{j}K_{i}(t)dt$ is zero for all\n$j=1,\\dots,\\nu-1$ and $i=1,\\dots,d$ where $K_{i}$ is the kernel\nin the $i$th coordinate.\n\\item $(\\mathcal{A}.1)$: Assume there exist constants $\\epsilon_{0},\\epsilon_{\\infty}$\nsuch that $0<\\epsilon_{0}\\leq f_{i}(x)\\leq\\epsilon_{\\infty}<\\infty,\\,\\forall x\\in S.$ \n\\item $(\\mathcal{A}.2)$: Assume that the densities $f_{i}\\in\\Sigma(s,K)$\nin the interior of $\\mathcal{S}$ with $s\\geq2$.\n\\item $(\\mathcal{A}.3)$: Assume that $g$ is infinitely differentiable.\n\\item $(\\mathcal{A}.4$): Assume that $\\left|g^{(j)}\\left(f_{1}(x)/f_{2}(x)\\right)\\right|$,\n$j=0,1,\\ldots$ are strictly upper bounded for $\\epsilon_{0}\\leq f_{i}(x)\\leq\\epsilon_{\\infty}.$ \n\\item $(\\mathcal{A}.5)$: Assume the following boundary smoothness condition:\nLet $p_{x}(u):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a polynomial\nof order $q\\leq r=\\left\\lfloor s\\right\\rfloor $ whose coefficients\nare a function of $x$ and are $r-q$ times differentiable. Then assume\nthat \n", "index": 35, "text": "\n\\[\n\\int_{x\\in\\mathcal{S}}\\left(\\int_{u:K(u)>0,\\, x+uh\\notin\\mathcal{S}}K(u)p_{x}(u)du\\right)^{t}dx=v_{t}(h),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\int_{x\\in\\mathcal{S}}\\left(\\int_{u:K(u)&gt;0,\\,x+uh\\notin\\mathcal{S}}K(u)p_{x}(u%&#10;)du\\right)^{t}dx=v_{t}(h),\" display=\"block\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi>x</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></msub><mrow><msup><mrow><mo>(</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi>u</mi><mo>:</mo><mrow><mrow><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mi>x</mi><mo>+</mo><mrow><mi>u</mi><mo>\u2062</mo><mi>h</mi></mrow></mrow><mo>\u2209</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></mrow></mrow></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>p</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow><mo>)</mo></mrow><mi>t</mi></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>x</mi></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nfor some constants $e_{i,q,t}$.\n\\end{itemize}\nAssumption $\\mathcal{A}.5$ appears to be complex but is actually\nsatisfied by a simple case:\n\n\\begin{theorem}\\label{thm:boundary}Assumption $\\mathcal{A}.5$ is\nsatisfied when $\\mathcal{S}=[-1,1]^{d}$ and when $K(x)$ is the uniform\nrectangular kernel; that is $K(x)=1$ for all $x:\\,||x||_{1}\\leq1/2$.\n\n\\end{theorem}\n\n\n\\section{Proof of Theorem~\\ref{thm:boundary}}\n\n\\label{sec:boundaryProof}Consider a uniform rectangular kernel $K(x)$\nthat satisfies $K(x)=1$ for all $x$ such that $||x||_{1}\\leq1/2$.\nAlso consider the family of probability densities $f$ with rectangular\nsupport $\\mathcal{S}=[-1,1]^{d}$. We will show that $\\S$ satisfies\nthe following smoothness condition $(\\mathcal{A}.5$): for any polynomial\n$p_{x}(u):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ of order $q\\leq r=\\left\\lfloor s\\right\\rfloor $\nwith coefficients that are $r-q$ times differentiable wrt $x$, \n\n", "itemtype": "equation", "pos": 37520, "prevtext": "\nwhere $v_{t}(h)$ admits the expansion \n", "index": 37, "text": "\n\\[\nv_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"v_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right),\" display=\"block\"><mrow><mrow><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>r</mi><mo>-</mo><mi>q</mi></mrow></munderover><mrow><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><msup><mi>h</mi><mi>i</mi></msup></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>h</mi><mrow><mi>r</mi><mo>-</mo><mi>q</mi></mrow></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $v_{t}(h)$has the expansion \n", "itemtype": "equation", "pos": 38505, "prevtext": "\nfor some constants $e_{i,q,t}$.\n\\end{itemize}\nAssumption $\\mathcal{A}.5$ appears to be complex but is actually\nsatisfied by a simple case:\n\n\\begin{theorem}\\label{thm:boundary}Assumption $\\mathcal{A}.5$ is\nsatisfied when $\\mathcal{S}=[-1,1]^{d}$ and when $K(x)$ is the uniform\nrectangular kernel; that is $K(x)=1$ for all $x:\\,||x||_{1}\\leq1/2$.\n\n\\end{theorem}\n\n\n\\section{Proof of Theorem~\\ref{thm:boundary}}\n\n\\label{sec:boundaryProof}Consider a uniform rectangular kernel $K(x)$\nthat satisfies $K(x)=1$ for all $x$ such that $||x||_{1}\\leq1/2$.\nAlso consider the family of probability densities $f$ with rectangular\nsupport $\\mathcal{S}=[-1,1]^{d}$. We will show that $\\S$ satisfies\nthe following smoothness condition $(\\mathcal{A}.5$): for any polynomial\n$p_{x}(u):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ of order $q\\leq r=\\left\\lfloor s\\right\\rfloor $\nwith coefficients that are $r-q$ times differentiable wrt $x$, \n\n", "index": 39, "text": "\\begin{equation}\n\\int_{x\\in\\S}\\left(\\int_{u:||u||_{1}\\leq\\frac{1}{2},\\, x+uh\\notin\\S}p_{x}(u)du\\right)^{t}dx=v_{t}(h),\\label{eq:smoothness}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\int_{x\\in\\S}\\left(\\int_{u:||u||_{1}\\leq\\frac{1}{2},\\,x+uh\\notin\\S}p_{x}(u)du%&#10;\\right)^{t}dx=v_{t}(h),\" display=\"block\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi>x</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u00a7</mi></mrow></msub><mrow><msup><mrow><mo>(</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi>u</mi><mo>:</mo><mrow><mrow><msub><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mi>u</mi><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mn>1</mn></msub><mo>\u2264</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mi>x</mi><mo>+</mo><mrow><mi>u</mi><mo>\u2062</mo><mi>h</mi></mrow></mrow><mo>\u2209</mo><mi mathvariant=\"normal\">\u00a7</mi></mrow></mrow></mrow></msub><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow><mo>)</mo></mrow><mi>t</mi></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>x</mi></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nNote that the inner integral forces the $x$'s under consideration\nto be boundary points via the constraint $x+uh\\notin\\S$.\n\n\n\\subsection{Single Coordinate Boundary Point}\n\nWe begin by focussing on points $x$ which are boundary points by\nvirtue of a single coordinate $x_{i}$ such that $x_{i}+u_{i}h\\notin\\S$.\nWLOG, assume that $x_{i}+u_{i}h>1$. The inner integral in Eq.~\\ref{eq:smoothness}\ncan then be evaluated first wrt all coordinates other than $i$. Since\nall of these coordinates lie within the support, the inner integral\nover these coordinates will amount to integration of the polynomial\n$p_{x}(u)$ over a symmetric $d-1$ dimensional rectangular region\n$\\left|u_{j}\\right|\\leq\\frac{1}{2}$ for all $j\\neq i$. This yields\na function $\\sum_{m=1}^{q}\\tilde{p}_{m}(x)u_{i}^{m}$ where the coefficients\n$\\tilde{p}_{m}(x)$ are each $r-q$ times differentiable wrt $x$.\n\nWith respect to the $u_{i}$ coordinate, the inner integral will have\nlimits from $\\frac{1-x_{i}}{h}$ to $\\frac{1}{2}$ for some $1>x_{i}>1-\\frac{h}{2}$.\nConsider the $\\tilde{p}_{q}(x)u_{i}^{q}$ monomial term. The inner\nintegral wrt this term yields \n\n", "itemtype": "equation", "pos": 38694, "prevtext": "\nwhere $v_{t}(h)$has the expansion \n", "index": 41, "text": "\n\\[\nv_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"v_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right).\" display=\"block\"><mrow><mrow><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>r</mi><mo>-</mo><mi>q</mi></mrow></munderover><mrow><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><msup><mi>h</mi><mi>i</mi></msup></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>h</mi><mrow><mi>r</mi><mo>-</mo><mi>q</mi></mrow></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nRaising the right hand side of Eq.~\\ref{eq:poly1} to the power of\n$t$ results in an expression of the form \n\n", "itemtype": "equation", "pos": 39883, "prevtext": "\nNote that the inner integral forces the $x$'s under consideration\nto be boundary points via the constraint $x+uh\\notin\\S$.\n\n\n\\subsection{Single Coordinate Boundary Point}\n\nWe begin by focussing on points $x$ which are boundary points by\nvirtue of a single coordinate $x_{i}$ such that $x_{i}+u_{i}h\\notin\\S$.\nWLOG, assume that $x_{i}+u_{i}h>1$. The inner integral in Eq.~\\ref{eq:smoothness}\ncan then be evaluated first wrt all coordinates other than $i$. Since\nall of these coordinates lie within the support, the inner integral\nover these coordinates will amount to integration of the polynomial\n$p_{x}(u)$ over a symmetric $d-1$ dimensional rectangular region\n$\\left|u_{j}\\right|\\leq\\frac{1}{2}$ for all $j\\neq i$. This yields\na function $\\sum_{m=1}^{q}\\tilde{p}_{m}(x)u_{i}^{m}$ where the coefficients\n$\\tilde{p}_{m}(x)$ are each $r-q$ times differentiable wrt $x$.\n\nWith respect to the $u_{i}$ coordinate, the inner integral will have\nlimits from $\\frac{1-x_{i}}{h}$ to $\\frac{1}{2}$ for some $1>x_{i}>1-\\frac{h}{2}$.\nConsider the $\\tilde{p}_{q}(x)u_{i}^{q}$ monomial term. The inner\nintegral wrt this term yields \n\n", "index": 43, "text": "\\begin{equation}\n\\sum_{m=1}^{q}\\tilde{p}_{m}(x)\\int_{\\frac{1-x_{i}}{h}}^{\\frac{1}{2}}u_{i}^{m}du_{i}=\\sum_{m=1}^{q}\\tilde{p}_{m}(x)\\frac{1}{m+1}\\left(\\frac{1}{2^{m+1}}-\\left(\\frac{1-x_{i}}{h}\\right)^{m+1}\\right).\\label{eq:poly1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\sum_{m=1}^{q}\\tilde{p}_{m}(x)\\int_{\\frac{1-x_{i}}{h}}^{\\frac{1}{2}}u_{i}^{m}%&#10;du_{i}=\\sum_{m=1}^{q}\\tilde{p}_{m}(x)\\frac{1}{m+1}\\left(\\frac{1}{2^{m+1}}-%&#10;\\left(\\frac{1-x_{i}}{h}\\right)^{m+1}\\right).\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">~</mo></mover><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mfrac><mn>1</mn><mn>2</mn></mfrac></msubsup><mrow><msubsup><mi>u</mi><mi>i</mi><mi>m</mi></msubsup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">~</mo></mover><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><msup><mn>2</mn><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msup></mfrac><mo>-</mo><msup><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere the coefficients $\\check{p}_{j}(x)$ are $r-q$ times differentiable\nwrt $x$. Integrating Eq.~\\ref{eq:poly2} over all the coordinates\nin $x$ other than $x_{i}$ results in an expression of the form \n\n", "itemtype": "equation", "pos": 40235, "prevtext": "\nRaising the right hand side of Eq.~\\ref{eq:poly1} to the power of\n$t$ results in an expression of the form \n\n", "index": 45, "text": "\\begin{equation}\n\\sum_{j=0}^{qt}\\check{p}_{j}(x)\\left(\\frac{1-x_{i}}{h}\\right)^{j},\\label{eq:poly2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\sum_{j=0}^{qt}\\check{p}_{j}(x)\\left(\\frac{1-x_{i}}{h}\\right)^{j},\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>q</mi><mo>\u2062</mo><mi>t</mi></mrow></munderover><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">\u02c7</mo></mover><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow><mi>j</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n where again the coefficients$\\bar{p}_{j}(x_{i})$ are $r-q$ times\ndifferentiable wrt $x_{i}$. Note that since the other cooordinates\nof $x$ other than $x_{i}$ are far away from the boundary, the coefficients\n$\\bar{p}_{j}(x_{i})$ are independent of $h$. To evaluate the integral\nof Eq.~\\ref{eq:poly3}, consider the $r-q$ term Taylor series expansion\nof $\\bar{p}_{j}(x_{i})$ around $x_{i}=1$. This will yield terms\nof the form \n\\begin{eqnarray*}\n\\int_{1-h/2}^{1}\\frac{\\left(1-x_{i}\\right)^{j+k}}{h^{k}}dx_{i} & = & \\left.-\\frac{\\left(1-x_{i}\\right)^{j+k+1}}{h^{k}(j+k+1)}\\right|_{x_{i}=1-h/2}^{x_{i}=1}\\\\\n & = & \\frac{h^{j+1}}{(j+k+1)2^{j+k+1}},\n\\end{eqnarray*}\nfor $0\\leq j\\leq r-q$, and $0\\leq k\\leq qt$. Combining terms results\nin the expansion $v_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right)$.\n\n\n\\subsection{Multiple Coordinate Boundary Point}\n\nThe case where multiple coordinates of the point $x$ are near the\nboundary is a straightforward extension of the single boundary point\ncase so we only sketch the main ideas here. As an example, consider\nthe case where 2 of the coordinates are near the boundary. Assume\nWLOG they are $x_{1}$ and $x_{2}$ and that $x_{1}+u_{1}h>1$ and\n$x_{2}+u_{2}h>1$. The inner integral in Eq.~\\ref{eq:smoothness}\ncan again be evaluated first wrt all coordinates other than 1 and\n2. This yields a function $\\sum_{m,j=1}^{q}\\tilde{p}_{m,j}(x)u_{1}^{m}u_{2}^{j}$\nwhere the coefficients $\\tilde{p}_{m,j}(x)$ are each $r-q$ times\ndifferentiable wrt $x$. Integrating this wrt $x_{1}$and $x_{2}$\nand then raising the result to the power of $t$ yields a double sum\nsimilar to Eq.~\\ref{eq:poly2}. Integrating this over all the coordinates\nin $x$ other than $x_{1}$ and $x_{2}$ gives a double sum similar\nto Eq.~\\ref{eq:poly3}. Then a Taylor series expansion of the coefficients\nand integration over $x_{1}$ and $x_{2}$ yields the result.\n\n\n\\section{Proof of Theorem~\\ref{thm:bias}}\n\n\\label{sec:BiasProof}We prove generalized versions of Theorems~\\ref{thm:bias},\n\\ref{thm:variance}, and \\ref{thm:clt} where the densities may have\ndifferent bandwidths $h_{1}$ and $h_{2}$ and number of samples $N_{1}$\nand $N_{2}$. The bias of the base estimator $\\gt$ can be expressed\nas \n\\begin{eqnarray}\n\\bias\\left[\\gt\\right] & = & \\mathbb{E}\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right]\\nonumber \\\\\n & = & \\mathbb{E}\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right]\\nonumber \\\\\n &  & +\\mathbb{E}\\left[g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)-g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right],\\label{eq:gsplit}\n\\end{eqnarray}\nwhere $\\mathbf{Z}$ is drawn from $f_{2}$. We find bounds for these\nterms by using Taylor series expansions. \n\nThe Taylor series expansion of $g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)$\naround $f_{1}(\\mathbf{Z})$ and $f_{2}(\\mathbf{Z})$ is\n\\begin{eqnarray}\ng\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right) & = & \\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty}\\left(\\left.\\frac{\\partial^{i+j}g(x,y)}{\\partial x^{i}\\partial y^{j}}\\right|_{\\substack{x=f_{1}(\\mathbf{Z})\\\\\ny=f_{2}(\\mathbf{Z})\n}\n}\\right)\\frac{\\bias_{\\mathbf{Z}}^{i}\\left[\\ft 1(\\mathbf{Z})\\right]\\bias_{\\mathbf{Z}}^{j}\\left[\\ft 2(\\mathbf{Z})\\right]}{i!j!}\\label{eq:gtaylor1}\n\\end{eqnarray}\nwhere $\\bias_{\\mathbf{Z}}^{j}\\left[\\ft i(\\mathbf{Z})\\right]=\\left(\\ez\\ft i(\\mathbf{Z})-f_{i}(\\mathbf{Z})\\right)^{j}$.\nThis expansion can be used to control the second term in Eq.~\\ref{eq:gsplit}.\nTo accomplish this, we require an expression for $\\ez\\ft i(\\mathbf{Z})-f_{i}(\\mathbf{Z})=\\bias_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]$.\n\nTo obtain an expression for $\\bias_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]$,\nwe consider separately the cases when $\\mathbf{Z}$ is in the interior\nof the support $\\mathcal{S}$ or when $\\mathbf{Z}$ is near the boundary\nof the support. A point $X\\in\\mathcal{S}$ is defined to be in the\ninterior of $\\mathcal{S}$ if for all $Y\\notin\\mathcal{S}$, $K\\left(\\frac{X-Y}{h_{i}}\\right)=0$.\nA point $X\\in\\mathcal{S}$ is near the boundary of the support if\nit is not in the interior. Denote the region in the interior and near\nthe boundary wrt $h_{i}$ as $\\mathcal{S}_{I_{i}}$ and $\\mathcal{S}_{B_{i}}$,\nrespectively.\n\n\\begin{lemma}\\label{lem:interior}Let $\\mathbf{Z}$ be a realization\nof the density $f_{2}$ independent of $\\ft i$ for $i=1,2$. Assume\nthat the densities $f_{1}$ and $f_{2}$ belong to $\\Sigma(s,L)$.\nThen for $\\mathbf{Z}\\in\\mathcal{S}_{I_{i}}$, \n\n", "itemtype": "equation", "pos": 40552, "prevtext": "\nwhere the coefficients $\\check{p}_{j}(x)$ are $r-q$ times differentiable\nwrt $x$. Integrating Eq.~\\ref{eq:poly2} over all the coordinates\nin $x$ other than $x_{i}$ results in an expression of the form \n\n", "index": 47, "text": "\\begin{equation}\n\\sum_{j=0}^{qt}\\bar{p}_{j}(x_{i})\\left(\\frac{1-x_{i}}{h}\\right)^{j},\\label{eq:poly3}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\sum_{j=0}^{qt}\\bar{p}_{j}(x_{i})\\left(\\frac{1-x_{i}}{h}\\right)^{j},\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>q</mi><mo>\u2062</mo><mi>t</mi></mrow></munderover><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow><mi>j</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\n\\end{lemma}\n\\begin{IEEEproof}\nObtaining the lower order terms in Eq.~\\ref{eq:interior} is a common\nresult in kernel density estimation. However, since we also require\nthe higher order terms, we present the proof here. Additionally, some\nof the results in the proof will be useful later. From the linearity\nof the KDE, we have that if $\\mathbf{X}$ is drawn from $f_{i}$ and\nis independent of $\\mathbf{Z}$, then \n\\begin{eqnarray}\n\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z}) & = & \\ez\\left[\\frac{1}{h_{i}^{d}}K\\left(\\frac{\\mathbf{X}-\\mathbf{Z}}{h_{i}}\\right)\\right]\\nonumber \\\\\n & = & \\int\\frac{1}{h_{i}^{d}}K\\left(\\frac{x-\\mathbf{Z}}{h_{i}}\\right)f_{i}(x)dx\\nonumber \\\\\n & = & \\int K\\left(t\\right)f_{i}(th_{i}+\\mathbf{Z})dt,\\label{eq:substitution}\n\\end{eqnarray}\nwhere the last step follows from the substitution $t=\\frac{x-\\mathbf{Z}}{h_{i}}$.\nSince the density $f_{i}$ belongs to $\\Sigma(s,K)$, using multi-index\nnotation we can expand it as \n\n", "itemtype": "equation", "pos": 45226, "prevtext": "\n where again the coefficients$\\bar{p}_{j}(x_{i})$ are $r-q$ times\ndifferentiable wrt $x_{i}$. Note that since the other cooordinates\nof $x$ other than $x_{i}$ are far away from the boundary, the coefficients\n$\\bar{p}_{j}(x_{i})$ are independent of $h$. To evaluate the integral\nof Eq.~\\ref{eq:poly3}, consider the $r-q$ term Taylor series expansion\nof $\\bar{p}_{j}(x_{i})$ around $x_{i}=1$. This will yield terms\nof the form \n\\begin{eqnarray*}\n\\int_{1-h/2}^{1}\\frac{\\left(1-x_{i}\\right)^{j+k}}{h^{k}}dx_{i} & = & \\left.-\\frac{\\left(1-x_{i}\\right)^{j+k+1}}{h^{k}(j+k+1)}\\right|_{x_{i}=1-h/2}^{x_{i}=1}\\\\\n & = & \\frac{h^{j+1}}{(j+k+1)2^{j+k+1}},\n\\end{eqnarray*}\nfor $0\\leq j\\leq r-q$, and $0\\leq k\\leq qt$. Combining terms results\nin the expansion $v_{t}(h)=\\sum_{i=1}^{r-q}e_{i,q,t}h^{i}+o\\left(h^{r-q}\\right)$.\n\n\n\\subsection{Multiple Coordinate Boundary Point}\n\nThe case where multiple coordinates of the point $x$ are near the\nboundary is a straightforward extension of the single boundary point\ncase so we only sketch the main ideas here. As an example, consider\nthe case where 2 of the coordinates are near the boundary. Assume\nWLOG they are $x_{1}$ and $x_{2}$ and that $x_{1}+u_{1}h>1$ and\n$x_{2}+u_{2}h>1$. The inner integral in Eq.~\\ref{eq:smoothness}\ncan again be evaluated first wrt all coordinates other than 1 and\n2. This yields a function $\\sum_{m,j=1}^{q}\\tilde{p}_{m,j}(x)u_{1}^{m}u_{2}^{j}$\nwhere the coefficients $\\tilde{p}_{m,j}(x)$ are each $r-q$ times\ndifferentiable wrt $x$. Integrating this wrt $x_{1}$and $x_{2}$\nand then raising the result to the power of $t$ yields a double sum\nsimilar to Eq.~\\ref{eq:poly2}. Integrating this over all the coordinates\nin $x$ other than $x_{1}$ and $x_{2}$ gives a double sum similar\nto Eq.~\\ref{eq:poly3}. Then a Taylor series expansion of the coefficients\nand integration over $x_{1}$ and $x_{2}$ yields the result.\n\n\n\\section{Proof of Theorem~\\ref{thm:bias}}\n\n\\label{sec:BiasProof}We prove generalized versions of Theorems~\\ref{thm:bias},\n\\ref{thm:variance}, and \\ref{thm:clt} where the densities may have\ndifferent bandwidths $h_{1}$ and $h_{2}$ and number of samples $N_{1}$\nand $N_{2}$. The bias of the base estimator $\\gt$ can be expressed\nas \n\\begin{eqnarray}\n\\bias\\left[\\gt\\right] & = & \\mathbb{E}\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right]\\nonumber \\\\\n & = & \\mathbb{E}\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right]\\nonumber \\\\\n &  & +\\mathbb{E}\\left[g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)-g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right],\\label{eq:gsplit}\n\\end{eqnarray}\nwhere $\\mathbf{Z}$ is drawn from $f_{2}$. We find bounds for these\nterms by using Taylor series expansions. \n\nThe Taylor series expansion of $g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)$\naround $f_{1}(\\mathbf{Z})$ and $f_{2}(\\mathbf{Z})$ is\n\\begin{eqnarray}\ng\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right) & = & \\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty}\\left(\\left.\\frac{\\partial^{i+j}g(x,y)}{\\partial x^{i}\\partial y^{j}}\\right|_{\\substack{x=f_{1}(\\mathbf{Z})\\\\\ny=f_{2}(\\mathbf{Z})\n}\n}\\right)\\frac{\\bias_{\\mathbf{Z}}^{i}\\left[\\ft 1(\\mathbf{Z})\\right]\\bias_{\\mathbf{Z}}^{j}\\left[\\ft 2(\\mathbf{Z})\\right]}{i!j!}\\label{eq:gtaylor1}\n\\end{eqnarray}\nwhere $\\bias_{\\mathbf{Z}}^{j}\\left[\\ft i(\\mathbf{Z})\\right]=\\left(\\ez\\ft i(\\mathbf{Z})-f_{i}(\\mathbf{Z})\\right)^{j}$.\nThis expansion can be used to control the second term in Eq.~\\ref{eq:gsplit}.\nTo accomplish this, we require an expression for $\\ez\\ft i(\\mathbf{Z})-f_{i}(\\mathbf{Z})=\\bias_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]$.\n\nTo obtain an expression for $\\bias_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]$,\nwe consider separately the cases when $\\mathbf{Z}$ is in the interior\nof the support $\\mathcal{S}$ or when $\\mathbf{Z}$ is near the boundary\nof the support. A point $X\\in\\mathcal{S}$ is defined to be in the\ninterior of $\\mathcal{S}$ if for all $Y\\notin\\mathcal{S}$, $K\\left(\\frac{X-Y}{h_{i}}\\right)=0$.\nA point $X\\in\\mathcal{S}$ is near the boundary of the support if\nit is not in the interior. Denote the region in the interior and near\nthe boundary wrt $h_{i}$ as $\\mathcal{S}_{I_{i}}$ and $\\mathcal{S}_{B_{i}}$,\nrespectively.\n\n\\begin{lemma}\\label{lem:interior}Let $\\mathbf{Z}$ be a realization\nof the density $f_{2}$ independent of $\\ft i$ for $i=1,2$. Assume\nthat the densities $f_{1}$ and $f_{2}$ belong to $\\Sigma(s,L)$.\nThen for $\\mathbf{Z}\\in\\mathcal{S}_{I_{i}}$, \n\n", "index": 49, "text": "\\begin{equation}\n\\mathbb{E}_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]=f_{i}(\\mathbf{Z})+\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h_{i}^{s}\\right).\\label{eq:interior}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{E}_{\\mathbf{Z}}\\left[\\ft i(\\mathbf{Z})\\right]=f_{i}(\\mathbf{Z})+\\sum_{%&#10;j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor}c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h%&#10;_{i}^{s}\\right).\" display=\"block\"><mrow><mrow><mrow><msub><mi>\ud835\udd3c</mi><mi>\ud835\udc19</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>\u03bd</mi><mo>/</mo><mn>2</mn></mrow></mrow><mrow><mo>\u230a</mo><mrow><mi>s</mi><mo>/</mo><mn>2</mn></mrow><mo>\u230b</mo></mrow></munderover><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>h</mi><mi>i</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>j</mi></mrow></msubsup></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>h</mi><mi>i</mi><mi>s</mi></msubsup><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nwhere $\\alpha!=\\alpha_{1}!\\alpha_{2}!\\dots\\alpha_{d}!$ and $Y^{\\alpha}=Y_{1}^{\\alpha_{1}}Y_{2}^{\\alpha_{2}}\\dots Y_{d}^{\\alpha_{d}}$.\nCombining Eqs.~\\ref{eq:substitution} and \\ref{eq:fTaylor} give\n\\begin{eqnarray*}\n\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z}) & = & f_{i}(\\mathbf{Z})+\\sum_{|\\alpha|\\leq\\left\\lfloor s\\right\\rfloor }\\frac{D^{\\alpha}f_{i}(\\mathbf{Z})}{\\alpha!}h_{i}^{|\\alpha|}\\int t^{\\alpha}K(t)dt+O(h_{i}^{s})\\\\\n & = & f_{i}(\\mathbf{Z})+\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O(h_{i}^{s}),\n\\end{eqnarray*}\nwhere the last step follows from the fact that $K$ is symmetric and\nof order $\\nu$.\n\\end{IEEEproof}\nTo obtain a similar result for the case when $\\mathbf{Z}$ is near\nthe boundary of $\\mathcal{S}$, we use assumption $\\mathcal{A}.5$.\n\n\\begin{lemma}\\label{lem:boundary}Let $\\gamma(x,y)$ be an arbitrary\nfunction satisfying $\\sup_{x,y}|\\gamma(x,y)|<\\infty$. Let $\\mathcal{S}$\nsatisfy the boundary smoothness conditions. Assume that the densities\n$f_{1}$ and $f_{2}$ belong to $\\Sigma(s,L)$ and let $\\mathbf{Z}$\nbe a realization of the density $f_{2}$ independent of $\\ft i$ for\n$i=1,2$. Let $h^{'}=\\min\\left(h_{1},h_{2}\\right)$. Then \n\\begin{eqnarray}\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B_{i}}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{Z}}^{t}\\left[\\ft i(\\mathbf{Z})\\right]\\right] & = & \\sum_{j=1}^{r}c_{4,i,j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right)\\label{eq:boundary_single}\\\\\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B_{1}}\\cap\\mathcal{S}_{B_{2}}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{Z}}^{t}\\left[\\ft 1(\\mathbf{Z})\\right]\\bias_{\\mathbf{Z}}^{q}\\left[\\ft 2(\\mathbf{Z})\\right]\\right] & = & \\sum_{j=0}^{r-1}\\sum_{i=0}^{r-1}c_{4,j,i,q,t}h_{1}^{j}h_{2}^{i}h^{'}+o\\left(\\left(h^{'}\\right)^{r}\\right)\\label{eq:boundary_both}\n\\end{eqnarray}\n\n\n\\end{lemma}\n\\begin{IEEEproof}\nFor fixed $X$ near the boundary of $\\mathcal{S}$, we have \n\\begin{eqnarray*}\n\\mathbb{E}\\left[\\ft i(X)\\right]-f_{i}(X) & = & \\frac{1}{h_{i}^{d}}\\int_{Y:Y\\in\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY-f_{i}(X)\\\\\n & = & \\left[\\frac{1}{h_{i}^{d}}\\int_{Y:K\\left(\\frac{X-Y}{h_{i}}\\right)>0}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY-f_{i}(X)\\right]\\\\\n &  & -\\left[\\frac{1}{h_{i}^{d}}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\right]\\\\\n & = & T_{1,i}(X)-T_{2,i}(X).\n\\end{eqnarray*}\n\n\nNote that in $T_{1,i}(X)$, we are extending the integral beyond the\nsupport of the density $f_{i}$. However, by using the same Taylor\nseries expansion method as in the proof of Lemma~\\ref{lem:interior},\nwe always evaluate $f_{i}$ and its derivatives at the point $X$\nwhich is within the support of $f_{i}$. Thus it does not matter how\nwe define an extension of $f_{i}$ since the Taylor series will remain\nthe same. Thus $T_{1,i}(X)$ results in an identical expression to\nthat obtained from Eq.~\\ref{eq:interior}.\n\nFor the $T_{2,i}(X)$ term, we expand it as follows using multi-index\nnotation as \n\\begin{eqnarray*}\nT_{2,i}(X) & = & \\frac{1}{h_{i}^{d}}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\\\\n & = & \\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)f_{i}(X+h_{i}u)du\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}du+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\nRecognizing that the $|\\alpha|$th derivative of $f_{i}$ is $r-|\\alpha|$\ntimes differentiable, we can apply assumption $\\mathcal{A}.5$ to\nobtain the expectation of $T_{2,i}(X)$ wrt $X$: \n\\begin{eqnarray*}\n\\bE\\left[T_{2,i}(\\mathbf{X})\\right] & = & \\frac{1}{h_{i}^{d}}\\int_{X}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dYf_{2}(X)dx\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{X}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}duf_{2}(X)dX+o\\left(h_{i}^{r}\\right)\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\left[\\sum_{1\\leq|\\beta|\\leq r-|\\alpha|}e_{\\beta,r-|\\alpha|}h_{i}^{|\\beta|}+o\\left(h_{i}^{r-|\\alpha|}\\right)\\right]+o\\left(h_{i}^{r}\\right)\\\\\n & = & \\sum_{j=1}^{r}e_{j}h_{i}^{j}+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\n\n\nSimilarly, we find that \n\\begin{eqnarray*}\n\\bE\\left[\\left(T_{2,i}(\\mathbf{X})\\right)^{t}\\right] & = & \\frac{1}{h_{i}^{dt}}\\int_{X}\\left(\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\right)^{t}f_{2}(X)dx\\\\\n & = & \\int_{X}\\left(\\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}du\\right)^{t}f_{2}(X)dX\\\\\n & = & \\sum_{j=1}^{r}e_{j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\n\n\nCombining these results gives \n\\begin{eqnarray*}\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\left(\\ez\\left[\\ft i(\\mathbf{Z})\\right]-f_{i}(\\mathbf{Z})\\right)^{t}\\right] & = & \\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\left(T_{1,i}(\\mathbf{Z})-T_{2,i}(\\mathbf{Z})\\right)^{t}\\right]\\\\\n & = & \\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\sum_{j=0}^{t}\\binom{t}{j}\\left(T_{1,i}(\\mathbf{Z})\\right)^{j}\\left(-T_{2,i}(\\mathbf{Z})\\right)^{t-j}\\right]\\\\\n & = & \\sum_{j=1}^{r}c_{4,i,j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right),\n\\end{eqnarray*}\nwhere the constants are functionals of the kernel, $\\gamma$, and\nthe densities.\n\nEquation~\\ref{eq:boundary_both} can be proved in a similar manner. \n\\end{IEEEproof}\nApplying Lemmas~\\ref{lem:interior} and \\ref{lem:boundary} to Eq.~\\ref{eq:gtaylor1}\ngives \n\n", "itemtype": "equation", "pos": 46393, "prevtext": "\n\n\n\\end{lemma}\n\\begin{IEEEproof}\nObtaining the lower order terms in Eq.~\\ref{eq:interior} is a common\nresult in kernel density estimation. However, since we also require\nthe higher order terms, we present the proof here. Additionally, some\nof the results in the proof will be useful later. From the linearity\nof the KDE, we have that if $\\mathbf{X}$ is drawn from $f_{i}$ and\nis independent of $\\mathbf{Z}$, then \n\\begin{eqnarray}\n\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z}) & = & \\ez\\left[\\frac{1}{h_{i}^{d}}K\\left(\\frac{\\mathbf{X}-\\mathbf{Z}}{h_{i}}\\right)\\right]\\nonumber \\\\\n & = & \\int\\frac{1}{h_{i}^{d}}K\\left(\\frac{x-\\mathbf{Z}}{h_{i}}\\right)f_{i}(x)dx\\nonumber \\\\\n & = & \\int K\\left(t\\right)f_{i}(th_{i}+\\mathbf{Z})dt,\\label{eq:substitution}\n\\end{eqnarray}\nwhere the last step follows from the substitution $t=\\frac{x-\\mathbf{Z}}{h_{i}}$.\nSince the density $f_{i}$ belongs to $\\Sigma(s,K)$, using multi-index\nnotation we can expand it as \n\n", "index": 51, "text": "\\begin{equation}\nf_{i}(th_{i}+\\mathbf{Z})=f_{i}(\\mathbf{Z})+\\sum_{|\\alpha|\\leq\\left\\lfloor s\\right\\rfloor }\\frac{D^{\\alpha}f_{i}(\\mathbf{Z})}{\\alpha!}\\left(th_{i}\\right)^{\\alpha}+O\\left(\\left\\Vert th_{i}\\right\\Vert ^{s}\\right),\\label{eq:fTaylor}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"f_{i}(th_{i}+\\mathbf{Z})=f_{i}(\\mathbf{Z})+\\sum_{|\\alpha|\\leq\\left\\lfloor s%&#10;\\right\\rfloor}\\frac{D^{\\alpha}f_{i}(\\mathbf{Z})}{\\alpha!}\\left(th_{i}\\right)^{%&#10;\\alpha}+O\\left(\\left\\|th_{i}\\right\\|^{s}\\right),\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>\ud835\udc19</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mo>\u230a</mo><mi>s</mi><mo>\u230b</mo></mrow></mrow></munder><mrow><mfrac><mrow><msup><mi>D</mi><mi>\u03b1</mi></msup><mo>\u2062</mo><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03b1</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mi>\u03b1</mi></msup></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><msup><mrow><mo>\u2225</mo><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mi>s</mi></msup><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\nFor the first term in Eq.~\\ref{eq:gsplit}, the truncated Taylor\nseries expansion of $g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)$\naround $\\mathbb{E}_{\\mathbf{Z}}\\ft 1(\\mathbf{Z})$ and $\\ez\\ft 2(\\mathbf{Z})$\ngives\n\\begin{eqnarray}\ng\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right) & = & \\sum_{i=0}^{\\lambda}\\sum_{j=0}^{\\lambda}\\left(\\left.\\frac{\\partial^{i+j}g(x,y)}{\\partial x^{i}\\partial y^{j}}\\right|_{\\substack{x=\\ez\\ft 1(\\mathbf{Z})\\\\\ny=\\ez\\ft 2(\\mathbf{Z})\n}\n}\\right)\\frac{\\et 1^{i}(\\mathbf{Z})\\et 2^{j}(\\mathbf{Z})}{i!j!}+o\\left(\\et 1^{\\lambda}(\\mathbf{Z})+\\et 2^{\\lambda}(\\mathbf{Z})\\right)\\label{eq:g_taylor2}\n\\end{eqnarray}\nwhere $\\et i(\\mathbf{Z}):=\\ft i(\\mathbf{Z})-\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z})$.\nTo control the first term in Eq.~\\ref{eq:gsplit}, we require expressions\nfor $\\ez\\left[\\et i^{j}(\\mathbf{Z})\\right]$. \n\n\\begin{lemma}\\label{lem:ekhat} Let $\\mathbf{Z}$ be a realization\nof the density $f_{2}$ that is in the interior of the support and\nis independent of $\\ft i$ for $i=1,2$. Let $n(q)$ be the set of\ninteger divisors of q including $1$ but excluding $q$. Then,\n\\begin{eqnarray}\n\\ez\\left[\\et i^{q}(\\mathbf{Z})\\right] & = & \\begin{cases}\n\\sum_{j\\in n(q)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{q-j}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,i,q,j,m}(\\mathbf{Z})h_{i}^{2m}+O\\left(\\frac{1}{N_{i}}\\right), & q\\geq2\\\\\n0, & q=1,\n\\end{cases}\\label{eq:moment}\\\\\n\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\et 2^{l}(\\mathbf{Z})\\right] & = & \\begin{cases}\n\\left(\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,1,q,i,m}(\\mathbf{Z})h_{1}^{2m}\\right)\\times, & q,\\, l\\geq2\\\\\n\\left(\\sum_{j\\in n(l)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{l-j}}\\sum_{t=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,l,j,t}(\\mathbf{Z})h_{2}^{2t}\\right)+O\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right)\\\\\n0, & q=1\\,\\text{or }l=1\n\\end{cases}\\label{eq:cross_moment}\n\\end{eqnarray}\nwhere $c_{6,i,q,j,m}$ is a functional of $\\gamma$, $f_{1},$ and\n$f_{2}.$\\end{lemma}\n\\begin{IEEEproof}\nDefine the random variable $\\mathbf{V}_{i}(\\mathbf{Z})=K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)-\\ez K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)$.\nThis gives \n\\begin{eqnarray*}\n\\et 2(\\mathbf{Z}) & = & \\ft 2(\\mathbf{Z})-\\ez\\ft 2(\\mathbf{Z})\\\\\n & = & \\frac{1}{N_{2}h_{2}^{d}}\\sum_{i=1}^{N_{2}}\\mathbf{V}_{i}(\\mathbf{Z}).\n\\end{eqnarray*}\nClearly, $\\ez\\mathbf{V}_{i}(\\mathbf{Z})=0$. From Eq.~\\ref{eq:substitution},\nwe have for integer $j\\geq1$ \n\\begin{eqnarray*}\n\\ez\\left[K^{j}\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right] & = & \\int K^{j}\\left(t\\right)f_{2}(th_{2}+\\mathbf{Z})dt\\\\\n & = & h_{2}^{d}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,j,m}(\\mathbf{Z})h_{2}^{2m},\n\\end{eqnarray*}\nwhere the constants $c_{3,2,j,m}$ depend on the density $f_{2}$,\nits derivatives, and the moments of the kernel $K^{j}$. Note that\nsince $K$ is symmetric, the odd moments of $K^{j}$ are zero for\n$\\mathbf{Z}$ in the interior of the support. However, all even moments\nmay now be nonzero since $K^{j}$ may now be nonnegative. By the binomial\ntheorem, \n\\begin{eqnarray*}\n\\ez\\left[\\mathbf{V}_{i}^{j}(\\mathbf{Z})\\right] & = & \\sum_{k=0}^{j}\\binom{j}{k}\\ez\\left[K^{k}\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right]\\ez\\left[K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right]^{j-k}\\\\\n & = & \\sum_{k=0}^{j}\\binom{j}{k}h_{2}^{d}O\\left(h_{2}^{d(j-k)}\\right)\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,k,m}(\\mathbf{Z})h_{2}^{2m}\\\\\n & = & h_{2}^{d}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,j,m}(\\mathbf{Z})h_{2}^{2m}+O\\left(h^{2d}\\right).\n\\end{eqnarray*}\nWe can use these expressions to simplify $\\ez\\left[\\et 2^{q}(\\mathbf{Z})\\right]$.\nAs an example, let $q=2$. Then since the $\\mathbf{X}_{i}s$ are independent,\n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{2}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}h_{2}^{2d}}\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})\\\\\n & = & \\frac{1}{N_{2}h_{2}^{d}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,2,m}(\\mathbf{Z})h_{2}^{2m}+O\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nSimilarly, we find that \n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{3}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}^{2}h_{2}^{3d}}\\ez\\mathbf{V}_{i}^{3}(\\mathbf{Z})\\\\\n & = & \\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{2}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,3,m}(\\mathbf{Z})h_{2}^{2m}+o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nFor $q=4$, we have \n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{4}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}^{3}h_{2}^{4d}}\\ez\\mathbf{V}_{i}^{4}(\\mathbf{Z})+\\frac{N_{2}-1}{N_{2}^{3}h_{2}^{4d}}\\left(\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})\\right)^{2}\\\\\n & = & \\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{3}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,4,m}(\\mathbf{Z})h_{2}^{2m}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{2}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,2,m}(\\mathbf{Z})h_{2}^{2m}+o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nThe pattern is then for $q\\geq2$, \n", "itemtype": "equation", "pos": 52274, "prevtext": "\nwhere $\\alpha!=\\alpha_{1}!\\alpha_{2}!\\dots\\alpha_{d}!$ and $Y^{\\alpha}=Y_{1}^{\\alpha_{1}}Y_{2}^{\\alpha_{2}}\\dots Y_{d}^{\\alpha_{d}}$.\nCombining Eqs.~\\ref{eq:substitution} and \\ref{eq:fTaylor} give\n\\begin{eqnarray*}\n\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z}) & = & f_{i}(\\mathbf{Z})+\\sum_{|\\alpha|\\leq\\left\\lfloor s\\right\\rfloor }\\frac{D^{\\alpha}f_{i}(\\mathbf{Z})}{\\alpha!}h_{i}^{|\\alpha|}\\int t^{\\alpha}K(t)dt+O(h_{i}^{s})\\\\\n & = & f_{i}(\\mathbf{Z})+\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O(h_{i}^{s}),\n\\end{eqnarray*}\nwhere the last step follows from the fact that $K$ is symmetric and\nof order $\\nu$.\n\\end{IEEEproof}\nTo obtain a similar result for the case when $\\mathbf{Z}$ is near\nthe boundary of $\\mathcal{S}$, we use assumption $\\mathcal{A}.5$.\n\n\\begin{lemma}\\label{lem:boundary}Let $\\gamma(x,y)$ be an arbitrary\nfunction satisfying $\\sup_{x,y}|\\gamma(x,y)|<\\infty$. Let $\\mathcal{S}$\nsatisfy the boundary smoothness conditions. Assume that the densities\n$f_{1}$ and $f_{2}$ belong to $\\Sigma(s,L)$ and let $\\mathbf{Z}$\nbe a realization of the density $f_{2}$ independent of $\\ft i$ for\n$i=1,2$. Let $h^{'}=\\min\\left(h_{1},h_{2}\\right)$. Then \n\\begin{eqnarray}\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B_{i}}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{Z}}^{t}\\left[\\ft i(\\mathbf{Z})\\right]\\right] & = & \\sum_{j=1}^{r}c_{4,i,j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right)\\label{eq:boundary_single}\\\\\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B_{1}}\\cap\\mathcal{S}_{B_{2}}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\bias_{\\mathbf{Z}}^{t}\\left[\\ft 1(\\mathbf{Z})\\right]\\bias_{\\mathbf{Z}}^{q}\\left[\\ft 2(\\mathbf{Z})\\right]\\right] & = & \\sum_{j=0}^{r-1}\\sum_{i=0}^{r-1}c_{4,j,i,q,t}h_{1}^{j}h_{2}^{i}h^{'}+o\\left(\\left(h^{'}\\right)^{r}\\right)\\label{eq:boundary_both}\n\\end{eqnarray}\n\n\n\\end{lemma}\n\\begin{IEEEproof}\nFor fixed $X$ near the boundary of $\\mathcal{S}$, we have \n\\begin{eqnarray*}\n\\mathbb{E}\\left[\\ft i(X)\\right]-f_{i}(X) & = & \\frac{1}{h_{i}^{d}}\\int_{Y:Y\\in\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY-f_{i}(X)\\\\\n & = & \\left[\\frac{1}{h_{i}^{d}}\\int_{Y:K\\left(\\frac{X-Y}{h_{i}}\\right)>0}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY-f_{i}(X)\\right]\\\\\n &  & -\\left[\\frac{1}{h_{i}^{d}}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\right]\\\\\n & = & T_{1,i}(X)-T_{2,i}(X).\n\\end{eqnarray*}\n\n\nNote that in $T_{1,i}(X)$, we are extending the integral beyond the\nsupport of the density $f_{i}$. However, by using the same Taylor\nseries expansion method as in the proof of Lemma~\\ref{lem:interior},\nwe always evaluate $f_{i}$ and its derivatives at the point $X$\nwhich is within the support of $f_{i}$. Thus it does not matter how\nwe define an extension of $f_{i}$ since the Taylor series will remain\nthe same. Thus $T_{1,i}(X)$ results in an identical expression to\nthat obtained from Eq.~\\ref{eq:interior}.\n\nFor the $T_{2,i}(X)$ term, we expand it as follows using multi-index\nnotation as \n\\begin{eqnarray*}\nT_{2,i}(X) & = & \\frac{1}{h_{i}^{d}}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\\\\n & = & \\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)f_{i}(X+h_{i}u)du\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}du+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\nRecognizing that the $|\\alpha|$th derivative of $f_{i}$ is $r-|\\alpha|$\ntimes differentiable, we can apply assumption $\\mathcal{A}.5$ to\nobtain the expectation of $T_{2,i}(X)$ wrt $X$: \n\\begin{eqnarray*}\n\\bE\\left[T_{2,i}(\\mathbf{X})\\right] & = & \\frac{1}{h_{i}^{d}}\\int_{X}\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dYf_{2}(X)dx\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{X}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}duf_{2}(X)dX+o\\left(h_{i}^{r}\\right)\\\\\n & = & \\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\left[\\sum_{1\\leq|\\beta|\\leq r-|\\alpha|}e_{\\beta,r-|\\alpha|}h_{i}^{|\\beta|}+o\\left(h_{i}^{r-|\\alpha|}\\right)\\right]+o\\left(h_{i}^{r}\\right)\\\\\n & = & \\sum_{j=1}^{r}e_{j}h_{i}^{j}+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\n\n\nSimilarly, we find that \n\\begin{eqnarray*}\n\\bE\\left[\\left(T_{2,i}(\\mathbf{X})\\right)^{t}\\right] & = & \\frac{1}{h_{i}^{dt}}\\int_{X}\\left(\\int_{Y:Y\\notin\\mathcal{S}}K\\left(\\frac{X-Y}{h_{i}}\\right)f_{i}(Y)dY\\right)^{t}f_{2}(X)dx\\\\\n & = & \\int_{X}\\left(\\sum_{|\\alpha|\\leq r}\\frac{h_{i}^{|\\alpha|}}{\\alpha!}\\int_{u:h_{i}u+X\\notin\\mathcal{S},K(u)>0}K\\left(u\\right)D^{\\alpha}f_{i}(X)u^{\\alpha}du\\right)^{t}f_{2}(X)dX\\\\\n & = & \\sum_{j=1}^{r}e_{j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right).\n\\end{eqnarray*}\n\n\nCombining these results gives \n\\begin{eqnarray*}\n\\bE\\left[1_{\\left\\{ \\mathbf{Z}\\in\\mathcal{S}_{B}\\right\\} }\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\left(\\ez\\left[\\ft i(\\mathbf{Z})\\right]-f_{i}(\\mathbf{Z})\\right)^{t}\\right] & = & \\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\left(T_{1,i}(\\mathbf{Z})-T_{2,i}(\\mathbf{Z})\\right)^{t}\\right]\\\\\n & = & \\bE\\left[\\gamma\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\sum_{j=0}^{t}\\binom{t}{j}\\left(T_{1,i}(\\mathbf{Z})\\right)^{j}\\left(-T_{2,i}(\\mathbf{Z})\\right)^{t-j}\\right]\\\\\n & = & \\sum_{j=1}^{r}c_{4,i,j,t}h_{i}^{j}+o\\left(h_{i}^{r}\\right),\n\\end{eqnarray*}\nwhere the constants are functionals of the kernel, $\\gamma$, and\nthe densities.\n\nEquation~\\ref{eq:boundary_both} can be proved in a similar manner. \n\\end{IEEEproof}\nApplying Lemmas~\\ref{lem:interior} and \\ref{lem:boundary} to Eq.~\\ref{eq:gtaylor1}\ngives \n\n", "index": 53, "text": "\\begin{equation}\n\\mathbb{E}\\left[g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)-g\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right]=\\sum_{j=1}^{r}\\left(c_{4,1,j}h_{1}^{j}+c_{4,2,j}h_{2}^{j}\\right)+\\sum_{j=0}^{r-1}\\sum_{i=0}^{r-1}c_{5,i,j}h_{1}^{j}h_{2}^{i}h^{'}+o\\left(h_{1}^{r}+h_{2}^{r}\\right).\\label{eq:biasresult1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{E}\\left[g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)-g\\left%&#10;(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\right]=\\sum_{j=1}^{r}\\left(c_{4,1,%&#10;j}h_{1}^{j}+c_{4,2,j}h_{2}^{j}\\right)+\\sum_{j=0}^{r-1}\\sum_{i=0}^{r-1}c_{5,i,j%&#10;}h_{1}^{j}h_{2}^{i}h^{{}^{\\prime}}+o\\left(h_{1}^{r}+h_{2}^{r}\\right).\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mrow><mo>(</mo><mrow><mrow><msub><mi>c</mi><mrow><mn>4</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>j</mi></msubsup></mrow><mo>+</mo><mrow><msub><mi>c</mi><mrow><mn>4</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>j</mi></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>c</mi><mrow><mn>5</mn><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>j</mi></msubsup><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>i</mi></msubsup><mo>\u2062</mo><msup><mi>h</mi><msup><mi/><mo>\u2032</mo></msup></msup></mrow></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>h</mi><mn>1</mn><mi>r</mi></msubsup><mo>+</mo><msubsup><mi>h</mi><mn>2</mn><mi>r</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n For any integer $q$, the largest possible factor is $q/2$. Thus\nfor given $q$, the smallest possible exponent on the $N_{2}h_{2}^{d}$\nterm is $q/2$. This increases as $q$ increases. A similar expression\nholds for $\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\right]$ except the $\\mathbf{X}_{i}$s\nare replaced with $\\mathbf{Y}_{i}$, $f_{2}$ is replaced with $f_{1}$,\nand $N_{2}$ and $h_{2}$ are replaced with $N_{1}$ and $h_{1}$,\nrespectively, all resulting in different constants. Then since $\\et 1(\\mathbf{Z})$\nand $\\et 2(\\mathbf{Z})$ are conditionally independent given $\\mathbf{Z}$,\n\\begin{eqnarray*}\n\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\et 2^{l}(\\mathbf{Z})\\right] & = & \\left(\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,1,q,i,m}(\\mathbf{Z})h_{1}^{2m}\\right)\\left(\\sum_{j\\in n(l)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{l-j}}\\sum_{t=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,l,j,t}(\\mathbf{Z})h_{2}^{2t}\\right)\\\\\n &  & +O\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\n\n\\end{IEEEproof}\nApplying Lemma~\\ref{lem:ekhat} to Eq.~\\ref{eq:g_taylor2} when\ntaking the conditional expectation given $\\mathbf{Z}$ in the interior\ngives an expression of the form\n\\begin{eqnarray}\n\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\left(c_{7,1,j,m}\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{1}^{2m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{7,2,j,m}\\left(\\ez\\ft 2(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{2}^{2m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n+\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{7,j,i,m,n}\\left(\\ez\\ft 2(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{1}^{2m}h_{2}^{2n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n+O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}\\right).\\label{eq:interior_ez}\n\\end{eqnarray}\n\n\nNote that the functionals $c_{7,i,j,m}$ and $c_{7,j,i,m,n}$ depend\non the derivatives of $g$ and $\\ez\\ft i(\\mathbf{Z})$ which depends\non $h_{i}$. To apply ensemble estimation, we need to separate the\ndependence on $h_{i}$ from the constants. If we use ODin1, then it\nis sufficient to note that in the interior of the support, $\\ez\\ft i(\\mathbf{Z})=f_{i}(\\mathbf{Z})+o(1)$\nand therefore $c\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)=c\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)+o(1)$\nfor some functional $c$. The terms in Eq.~\\ref{eq:interior_ez}\nreduce to \n", "itemtype": "equation", "pos": 57568, "prevtext": "\n\n\nFor the first term in Eq.~\\ref{eq:gsplit}, the truncated Taylor\nseries expansion of $g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)$\naround $\\mathbb{E}_{\\mathbf{Z}}\\ft 1(\\mathbf{Z})$ and $\\ez\\ft 2(\\mathbf{Z})$\ngives\n\\begin{eqnarray}\ng\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right) & = & \\sum_{i=0}^{\\lambda}\\sum_{j=0}^{\\lambda}\\left(\\left.\\frac{\\partial^{i+j}g(x,y)}{\\partial x^{i}\\partial y^{j}}\\right|_{\\substack{x=\\ez\\ft 1(\\mathbf{Z})\\\\\ny=\\ez\\ft 2(\\mathbf{Z})\n}\n}\\right)\\frac{\\et 1^{i}(\\mathbf{Z})\\et 2^{j}(\\mathbf{Z})}{i!j!}+o\\left(\\et 1^{\\lambda}(\\mathbf{Z})+\\et 2^{\\lambda}(\\mathbf{Z})\\right)\\label{eq:g_taylor2}\n\\end{eqnarray}\nwhere $\\et i(\\mathbf{Z}):=\\ft i(\\mathbf{Z})-\\mathbb{E}_{\\mathbf{Z}}\\ft i(\\mathbf{Z})$.\nTo control the first term in Eq.~\\ref{eq:gsplit}, we require expressions\nfor $\\ez\\left[\\et i^{j}(\\mathbf{Z})\\right]$. \n\n\\begin{lemma}\\label{lem:ekhat} Let $\\mathbf{Z}$ be a realization\nof the density $f_{2}$ that is in the interior of the support and\nis independent of $\\ft i$ for $i=1,2$. Let $n(q)$ be the set of\ninteger divisors of q including $1$ but excluding $q$. Then,\n\\begin{eqnarray}\n\\ez\\left[\\et i^{q}(\\mathbf{Z})\\right] & = & \\begin{cases}\n\\sum_{j\\in n(q)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{q-j}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,i,q,j,m}(\\mathbf{Z})h_{i}^{2m}+O\\left(\\frac{1}{N_{i}}\\right), & q\\geq2\\\\\n0, & q=1,\n\\end{cases}\\label{eq:moment}\\\\\n\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\et 2^{l}(\\mathbf{Z})\\right] & = & \\begin{cases}\n\\left(\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,1,q,i,m}(\\mathbf{Z})h_{1}^{2m}\\right)\\times, & q,\\, l\\geq2\\\\\n\\left(\\sum_{j\\in n(l)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{l-j}}\\sum_{t=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,l,j,t}(\\mathbf{Z})h_{2}^{2t}\\right)+O\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right)\\\\\n0, & q=1\\,\\text{or }l=1\n\\end{cases}\\label{eq:cross_moment}\n\\end{eqnarray}\nwhere $c_{6,i,q,j,m}$ is a functional of $\\gamma$, $f_{1},$ and\n$f_{2}.$\\end{lemma}\n\\begin{IEEEproof}\nDefine the random variable $\\mathbf{V}_{i}(\\mathbf{Z})=K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)-\\ez K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)$.\nThis gives \n\\begin{eqnarray*}\n\\et 2(\\mathbf{Z}) & = & \\ft 2(\\mathbf{Z})-\\ez\\ft 2(\\mathbf{Z})\\\\\n & = & \\frac{1}{N_{2}h_{2}^{d}}\\sum_{i=1}^{N_{2}}\\mathbf{V}_{i}(\\mathbf{Z}).\n\\end{eqnarray*}\nClearly, $\\ez\\mathbf{V}_{i}(\\mathbf{Z})=0$. From Eq.~\\ref{eq:substitution},\nwe have for integer $j\\geq1$ \n\\begin{eqnarray*}\n\\ez\\left[K^{j}\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right] & = & \\int K^{j}\\left(t\\right)f_{2}(th_{2}+\\mathbf{Z})dt\\\\\n & = & h_{2}^{d}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,j,m}(\\mathbf{Z})h_{2}^{2m},\n\\end{eqnarray*}\nwhere the constants $c_{3,2,j,m}$ depend on the density $f_{2}$,\nits derivatives, and the moments of the kernel $K^{j}$. Note that\nsince $K$ is symmetric, the odd moments of $K^{j}$ are zero for\n$\\mathbf{Z}$ in the interior of the support. However, all even moments\nmay now be nonzero since $K^{j}$ may now be nonnegative. By the binomial\ntheorem, \n\\begin{eqnarray*}\n\\ez\\left[\\mathbf{V}_{i}^{j}(\\mathbf{Z})\\right] & = & \\sum_{k=0}^{j}\\binom{j}{k}\\ez\\left[K^{k}\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right]\\ez\\left[K\\left(\\frac{\\mathbf{X}_{i}-\\mathbf{Z}}{h_{2}}\\right)\\right]^{j-k}\\\\\n & = & \\sum_{k=0}^{j}\\binom{j}{k}h_{2}^{d}O\\left(h_{2}^{d(j-k)}\\right)\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,k,m}(\\mathbf{Z})h_{2}^{2m}\\\\\n & = & h_{2}^{d}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,j,m}(\\mathbf{Z})h_{2}^{2m}+O\\left(h^{2d}\\right).\n\\end{eqnarray*}\nWe can use these expressions to simplify $\\ez\\left[\\et 2^{q}(\\mathbf{Z})\\right]$.\nAs an example, let $q=2$. Then since the $\\mathbf{X}_{i}s$ are independent,\n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{2}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}h_{2}^{2d}}\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})\\\\\n & = & \\frac{1}{N_{2}h_{2}^{d}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,2,m}(\\mathbf{Z})h_{2}^{2m}+O\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nSimilarly, we find that \n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{3}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}^{2}h_{2}^{3d}}\\ez\\mathbf{V}_{i}^{3}(\\mathbf{Z})\\\\\n & = & \\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{2}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,3,m}(\\mathbf{Z})h_{2}^{2m}+o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nFor $q=4$, we have \n\\begin{eqnarray*}\n\\ez\\left[\\et 2^{4}(\\mathbf{Z})\\right] & = & \\frac{1}{N_{2}^{3}h_{2}^{4d}}\\ez\\mathbf{V}_{i}^{4}(\\mathbf{Z})+\\frac{N_{2}-1}{N_{2}^{3}h_{2}^{4d}}\\left(\\ez\\mathbf{V}_{i}^{2}(\\mathbf{Z})\\right)^{2}\\\\\n & = & \\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{3}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{3,2,4,m}(\\mathbf{Z})h_{2}^{2m}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{2}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,2,m}(\\mathbf{Z})h_{2}^{2m}+o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\nThe pattern is then for $q\\geq2$, \n", "index": 55, "text": "\n\\[\n\\ez\\left[\\et 2^{q}(\\mathbf{Z})\\right]=\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,q,i,m}(\\mathbf{Z})h_{2}^{2m}+O\\left(\\frac{1}{N_{2}}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\ez\\left[\\et 2^{q}(\\mathbf{Z})\\right]=\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{2}h_{2%&#10;}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor}c_{6,2,q,i,m}(%&#10;\\mathbf{Z})h_{2}^{2m}+O\\left(\\frac{1}{N_{2}}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\et</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><mi>q</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mfrac><mn>1</mn><msup><mrow><mo>(</mo><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow><mo>)</mo></mrow><mrow><mi>q</mi><mo>-</mo><mi>i</mi></mrow></msup></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mo>\u230a</mo><mrow><mi>s</mi><mo>/</mo><mn>2</mn></mrow><mo>\u230b</mo></mrow></munderover><mrow><msub><mi>c</mi><mrow><mn>6</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>q</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>m</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi></mrow></msubsup></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><msub><mi>N</mi><mn>2</mn></msub></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nFor ODin2, we need the higher order terms. To separate the dependence\non $h_{i}$ from the constants, we need more information about the\nfunctional $g$ and its derivatives. Consider the special case where\nthe functional $g(x,y)$ has derivatives of the form of $x^{\\alpha}y^{\\beta}$\nwith $\\alpha,\\beta<0.$ This corresponds to the important cases of\nthe KL divergence and the Renyi divergence. The generalized binomial\ntheorem states that if $\\binom{\\alpha}{m}:=\\frac{\\alpha(\\alpha-1)\\dots(\\alpha-m+1)}{m!}$\nand if $q$ and $t$ are real numbers with $|q|>|t|$, then for any\ncomplex number $\\alpha$, \n\n", "itemtype": "equation", "pos": 60376, "prevtext": "\n For any integer $q$, the largest possible factor is $q/2$. Thus\nfor given $q$, the smallest possible exponent on the $N_{2}h_{2}^{d}$\nterm is $q/2$. This increases as $q$ increases. A similar expression\nholds for $\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\right]$ except the $\\mathbf{X}_{i}$s\nare replaced with $\\mathbf{Y}_{i}$, $f_{2}$ is replaced with $f_{1}$,\nand $N_{2}$ and $h_{2}$ are replaced with $N_{1}$ and $h_{1}$,\nrespectively, all resulting in different constants. Then since $\\et 1(\\mathbf{Z})$\nand $\\et 2(\\mathbf{Z})$ are conditionally independent given $\\mathbf{Z}$,\n\\begin{eqnarray*}\n\\ez\\left[\\et 1^{q}(\\mathbf{Z})\\et 2^{l}(\\mathbf{Z})\\right] & = & \\left(\\sum_{i\\in n(q)}\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{q-i}}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,1,q,i,m}(\\mathbf{Z})h_{1}^{2m}\\right)\\left(\\sum_{j\\in n(l)}\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{l-j}}\\sum_{t=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{6,2,l,j,t}(\\mathbf{Z})h_{2}^{2t}\\right)\\\\\n &  & +O\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\n\n\\end{IEEEproof}\nApplying Lemma~\\ref{lem:ekhat} to Eq.~\\ref{eq:g_taylor2} when\ntaking the conditional expectation given $\\mathbf{Z}$ in the interior\ngives an expression of the form\n\\begin{eqnarray}\n\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\left(c_{7,1,j,m}\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{1}^{2m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{7,2,j,m}\\left(\\ez\\ft 2(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{2}^{2m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n+\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{7,j,i,m,n}\\left(\\ez\\ft 2(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\frac{h_{1}^{2m}h_{2}^{2n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n+O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}\\right).\\label{eq:interior_ez}\n\\end{eqnarray}\n\n\nNote that the functionals $c_{7,i,j,m}$ and $c_{7,j,i,m,n}$ depend\non the derivatives of $g$ and $\\ez\\ft i(\\mathbf{Z})$ which depends\non $h_{i}$. To apply ensemble estimation, we need to separate the\ndependence on $h_{i}$ from the constants. If we use ODin1, then it\nis sufficient to note that in the interior of the support, $\\ez\\ft i(\\mathbf{Z})=f_{i}(\\mathbf{Z})+o(1)$\nand therefore $c\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)=c\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)+o(1)$\nfor some functional $c$. The terms in Eq.~\\ref{eq:interior_ez}\nreduce to \n", "index": 57, "text": "\n\\[\nc_{7,1,1,0}\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\frac{1}{N_{1}h_{1}^{d}}+c_{7,2,1,0}\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\frac{1}{N_{2}h_{2}^{d}}+o\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2}h_{2}^{d}}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"c_{7,1,1,0}\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\frac{1}{N_{1}h_{1}%&#10;^{d}}+c_{7,2,1,0}\\left(f_{1}(\\mathbf{Z}),f_{2}(\\mathbf{Z})\\right)\\frac{1}{N_{2%&#10;}h_{2}^{d}}+o\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2}h_{2}^{d}}\\right).\" display=\"block\"><mrow><mrow><mrow><msub><mi>c</mi><mrow><mn>7</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>+</mo><mrow><msub><mi>c</mi><mrow><mn>7</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nSince the densities are bounded away from zero, for sufficiently small\n$h_{i}$, we have that $f_{i}(\\mathbf{Z})>\\left|\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h_{i}^{s}\\right)\\right|.$\nApplying the generalized binomial theorem and Lemma~\\ref{lem:interior}\ngives that \n", "itemtype": "equation", "pos": 61212, "prevtext": "\nFor ODin2, we need the higher order terms. To separate the dependence\non $h_{i}$ from the constants, we need more information about the\nfunctional $g$ and its derivatives. Consider the special case where\nthe functional $g(x,y)$ has derivatives of the form of $x^{\\alpha}y^{\\beta}$\nwith $\\alpha,\\beta<0.$ This corresponds to the important cases of\nthe KL divergence and the Renyi divergence. The generalized binomial\ntheorem states that if $\\binom{\\alpha}{m}:=\\frac{\\alpha(\\alpha-1)\\dots(\\alpha-m+1)}{m!}$\nand if $q$ and $t$ are real numbers with $|q|>|t|$, then for any\ncomplex number $\\alpha$, \n\n", "index": 59, "text": "\\begin{equation}\n(q+t)^{\\alpha}=\\sum_{m=0}^{\\infty}\\binom{\\alpha}{m}q^{\\alpha-m}t^{m}.\\label{eq:general_binomial}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"(q+t)^{\\alpha}=\\sum_{m=0}^{\\infty}\\binom{\\alpha}{m}q^{\\alpha-m}t^{m}.\" display=\"block\"><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>q</mi><mo>+</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>\u03b1</mi><mi>m</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msup><mi>q</mi><mrow><mi>\u03b1</mi><mo>-</mo><mi>m</mi></mrow></msup><mo>\u2062</mo><msup><mi>t</mi><mi>m</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nSince $m$ is an integer, the exponents of the $h_{i}$ terms are\nalso integers. Thus Eq.~\\ref{eq:interior_ez} gives in this case\n\\begin{eqnarray}\n\\ez\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right] & = & \\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\left(c_{8,1,j,m}\\left(\\mathbf{Z}\\right)\\frac{h_{1}^{2m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{8,2,j,m}\\left(\\mathbf{Z}\\right)\\frac{h_{2}^{2m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n &  & +\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{8,j,i,m,n}\\left(\\mathbf{Z}\\right)\\frac{h_{1}^{2m}h_{2}^{2n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n &  & +O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}+h_{1}^{s}+h_{2}^{s}\\right).\\label{eq:interior_ez2}\n\\end{eqnarray}\n\n\nAs before, the case for $\\mathbf{Z}$ close to the boundary of the\nsupport is more complicated. However, by using a similar technique\nto the proof of Lemma~\\ref{lem:boundary} for $\\mathbf{Z}$ at the\nboundary and combining with the previous results, we find that for\ngeneral $g$, \n\n", "itemtype": "equation", "pos": 61651, "prevtext": "\nSince the densities are bounded away from zero, for sufficiently small\n$h_{i}$, we have that $f_{i}(\\mathbf{Z})>\\left|\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h_{i}^{s}\\right)\\right|.$\nApplying the generalized binomial theorem and Lemma~\\ref{lem:interior}\ngives that \n", "index": 61, "text": "\n\\[\n\\left(\\ez\\ft 1(\\mathbf{Z})\\right)^{\\alpha}=\\sum_{m=0}^{\\infty}\\binom{\\alpha}{m}f_{i}^{\\alpha-m}(\\mathbf{Z})\\left(\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right\\rfloor }c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h_{i}^{s}\\right)\\right)^{m}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\left(\\ez\\ft 1(\\mathbf{Z})\\right)^{\\alpha}=\\sum_{m=0}^{\\infty}\\binom{\\alpha}{m%&#10;}f_{i}^{\\alpha-m}(\\mathbf{Z})\\left(\\sum_{j=\\nu/2}^{\\left\\lfloor s/2\\right%&#10;\\rfloor}c_{i,j}(\\mathbf{Z})h_{i}^{2j}+O\\left(h_{i}^{s}\\right)\\right)^{m}.\" display=\"block\"><mrow><mrow><msup><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mi>\u03b1</mi></msup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>\u03b1</mi><mi>m</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>f</mi><mi>i</mi><mrow><mi>\u03b1</mi><mo>-</mo><mi>m</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mrow><mi>\u03bd</mi><mo>/</mo><mn>2</mn></mrow></mrow><mrow><mo>\u230a</mo><mrow><mi>s</mi><mo>/</mo><mn>2</mn></mrow><mo>\u230b</mo></mrow></munderover><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>h</mi><mi>i</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>j</mi></mrow></msubsup></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>h</mi><mi>i</mi><mi>s</mi></msubsup><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mi>m</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nIf $g(x,y)$ has derivatives of the form of $x^{\\alpha}y^{\\beta}$\nwith $\\alpha,\\beta<0$, then we can similarly obtain \n\\begin{eqnarray}\n\\bE\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right] & = & \\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{r}\\left(c_{9,1,j,m}\\frac{h_{1}^{m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{9,2,j,m}\\frac{h_{2}^{m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n &  & +\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{r}\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{r}c_{9,j,i,m,n}\\frac{h_{1}^{m}h_{2}^{n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n &  & +O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}+h_{1}^{s}+h_{2}^{s}\\right).\\label{eq:bias_result3}\n\\end{eqnarray}\nCombining Eq.~\\ref{eq:biasresult1} with either Eq.~\\ref{eq:bias_result2}\nor \\ref{eq:bias_result3} completes the proof.\n\n\n\\section{Proof of Theorem~\\ref{thm:variance}}\n\n\\label{sec:VarProof}To bound the variance, we will use the Efron-Stein\ninequality:\n\n\\begin{lemma}[Efron-Stein Inequality] Let $\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n},\\mathbf{X}_{1}^{'},\\dots,\\mathbf{X}_{n}^{'}$\nbe independent random variables on the space $\\mathcal{S}$. Then\nif $f:\\mathcal{S}\\times\\dots\\times\\mathcal{S}\\rightarrow\\mathbb{R}$,\nwe have that \n", "itemtype": "equation", "pos": 63171, "prevtext": "\nSince $m$ is an integer, the exponents of the $h_{i}$ terms are\nalso integers. Thus Eq.~\\ref{eq:interior_ez} gives in this case\n\\begin{eqnarray}\n\\ez\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right] & = & \\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\left(c_{8,1,j,m}\\left(\\mathbf{Z}\\right)\\frac{h_{1}^{2m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{8,2,j,m}\\left(\\mathbf{Z}\\right)\\frac{h_{2}^{2m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n &  & +\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{\\left\\lfloor s/2\\right\\rfloor }\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{\\left\\lfloor s/2\\right\\rfloor }c_{8,j,i,m,n}\\left(\\mathbf{Z}\\right)\\frac{h_{1}^{2m}h_{2}^{2n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n &  & +O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}+h_{1}^{s}+h_{2}^{s}\\right).\\label{eq:interior_ez2}\n\\end{eqnarray}\n\n\nAs before, the case for $\\mathbf{Z}$ close to the boundary of the\nsupport is more complicated. However, by using a similar technique\nto the proof of Lemma~\\ref{lem:boundary} for $\\mathbf{Z}$ at the\nboundary and combining with the previous results, we find that for\ngeneral $g$, \n\n", "index": 63, "text": "\\begin{equation}\n\\bE\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right]=c_{9,1}\\frac{1}{N_{1}h_{1}^{d}}+c_{9,2}\\frac{1}{N_{2}h_{2}^{d}}+o\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2}h_{2}^{d}}\\right).\\label{eq:bias_result2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(%&#10;\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right]=c_{9,1}\\frac{1}{N_{1}h_{1}^{d}}%&#10;+c_{9,2}\\frac{1}{N_{2}h_{2}^{d}}+o\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2%&#10;}h_{2}^{d}}\\right).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ez</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>c</mi><mrow><mn>9</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>+</mo><mrow><msub><mi>c</mi><mrow><mn>9</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>\u2062</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\n\\end{lemma}\n\nSuppose we have samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1}^{'},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand denote the respective estimators as $\\gt$ and $\\gt^{'}$. We\nhave that \n\\begin{eqnarray}\n\\left|\\gt-\\gt^{'}\\right| & \\leq & \\frac{1}{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right|\\nonumber \\\\\n &  & +\\frac{1}{N_{2}}\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|.\\label{eq:triangle}\n\\end{eqnarray}\nSince $g$ is Lipschitz continuous with constant $C_{g}$, we have\n\\begin{eqnarray}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right| & \\leq & C_{g}\\left(\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right|+\\left|\\ft 2(\\mathbf{X}_{1})-\\ft 2(\\mathbf{X}_{1}^{'})\\right|\\right),\\label{eq:lipschitz}\n\\end{eqnarray}\n\\begin{eqnarray}\n\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right| & = & \\frac{1}{N_{1}h_{1}^{d}}\\left|\\sum_{i=1}^{N_{1}}\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)\\right|\\nonumber \\\\\n & \\leq & \\frac{1}{N_{1}h_{1}^{d}}\\sum_{i=1}^{N_{1}}\\left|K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right|\\nonumber \\\\\n\\implies\\bE\\left[\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right|^{2}\\right] & \\leq & \\frac{1}{N_{1}h_{1}^{2d}}\\sum_{i=1}^{N_{1}}\\bE\\left[\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\right],\\label{eq:densityBound}\n\\end{eqnarray}\nwhere the last step follows from Jensen's inequality. By making the\nsubstitution $\\mathbf{u}_{i}=\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}$\nand $\\mathbf{u}_{i}^{'}=\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}$,\nthis gives \n\\begin{eqnarray*}\n\\frac{1}{h_{1}^{2d}}\\bE\\left[\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\right] & = & \\frac{1}{h^{2d}}\\int\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\times\\\\\n &  & f_{2}(\\mathbf{X}_{1})f_{2}(\\mathbf{X}_{1}^{'})f_{1}(\\mathbf{Y}_{i})d\\mathbf{X}_{1}d\\mathbf{X}_{1}^{'}d\\mathbf{Y}_{i}\\\\\n & \\leq & 2||K||_{\\infty}^{2}.\n\\end{eqnarray*}\nCombining this with Eq.~\\ref{eq:densityBound} gives \n", "itemtype": "equation", "pos": 64836, "prevtext": "\nIf $g(x,y)$ has derivatives of the form of $x^{\\alpha}y^{\\beta}$\nwith $\\alpha,\\beta<0$, then we can similarly obtain \n\\begin{eqnarray}\n\\bE\\left[g\\left(\\ft 1(\\mathbf{Z}),\\ft 2(\\mathbf{Z})\\right)-g\\left(\\ez\\ft 1(\\mathbf{Z}),\\ez\\ft 2(\\mathbf{Z})\\right)\\right] & = & \\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{r}\\left(c_{9,1,j,m}\\frac{h_{1}^{m}}{\\left(N_{1}h_{1}^{d}\\right)^{j}}+c_{9,2,j,m}\\frac{h_{2}^{m}}{\\left(N_{2}h_{2}^{d}\\right)^{j}}\\right)\\nonumber \\\\\n &  & +\\sum_{j=1}^{\\lambda/2}\\sum_{m=0}^{r}\\sum_{i=1}^{\\lambda/2}\\sum_{n=0}^{r}c_{9,j,i,m,n}\\frac{h_{1}^{m}h_{2}^{n}}{\\left(N_{1}h_{1}^{d}\\right)^{j}\\left(N_{2}h_{2}^{d}\\right)^{i}}\\nonumber \\\\\n &  & +O\\left(\\frac{1}{\\left(N_{1}h_{1}^{d}\\right)^{\\frac{\\lambda}{2}}}+\\frac{1}{\\left(N_{2}h_{2}^{d}\\right)^{\\frac{\\lambda}{2}}}+h_{1}^{s}+h_{2}^{s}\\right).\\label{eq:bias_result3}\n\\end{eqnarray}\nCombining Eq.~\\ref{eq:biasresult1} with either Eq.~\\ref{eq:bias_result2}\nor \\ref{eq:bias_result3} completes the proof.\n\n\n\\section{Proof of Theorem~\\ref{thm:variance}}\n\n\\label{sec:VarProof}To bound the variance, we will use the Efron-Stein\ninequality:\n\n\\begin{lemma}[Efron-Stein Inequality] Let $\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n},\\mathbf{X}_{1}^{'},\\dots,\\mathbf{X}_{n}^{'}$\nbe independent random variables on the space $\\mathcal{S}$. Then\nif $f:\\mathcal{S}\\times\\dots\\times\\mathcal{S}\\rightarrow\\mathbb{R}$,\nwe have that \n", "index": 65, "text": "\n\\[\n\\var\\left[f(\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n})\\right]\\leq\\frac{1}{2}\\sum_{i=1}^{n}\\bE\\left[\\left(f(\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n})-f(\\mathbf{X}_{1},\\dots,\\mathbf{X}_{i}^{'},\\dots,\\mathbf{X}_{n})\\right)^{2}\\right].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[f(\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n})\\right]\\leq\\frac{1}{2}\\sum_{i=%&#10;1}^{n}\\bE\\left[\\left(f(\\mathbf{X}_{1},\\dots,\\mathbf{X}_{n})-f(\\mathbf{X}_{1},%&#10;\\dots,\\mathbf{X}_{i}^{{}^{\\prime}},\\dots,\\mathbf{X}_{n})\\right)^{2}\\right].\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msubsup><mi>\ud835\udc17</mi><mi>i</mi><msup><mi/><mo>\u2032</mo></msup></msubsup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc17</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nSimilarly, \n", "itemtype": "equation", "pos": 67839, "prevtext": "\n\n\n\\end{lemma}\n\nSuppose we have samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1}^{'},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand denote the respective estimators as $\\gt$ and $\\gt^{'}$. We\nhave that \n\\begin{eqnarray}\n\\left|\\gt-\\gt^{'}\\right| & \\leq & \\frac{1}{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right|\\nonumber \\\\\n &  & +\\frac{1}{N_{2}}\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|.\\label{eq:triangle}\n\\end{eqnarray}\nSince $g$ is Lipschitz continuous with constant $C_{g}$, we have\n\\begin{eqnarray}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right| & \\leq & C_{g}\\left(\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right|+\\left|\\ft 2(\\mathbf{X}_{1})-\\ft 2(\\mathbf{X}_{1}^{'})\\right|\\right),\\label{eq:lipschitz}\n\\end{eqnarray}\n\\begin{eqnarray}\n\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right| & = & \\frac{1}{N_{1}h_{1}^{d}}\\left|\\sum_{i=1}^{N_{1}}\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)\\right|\\nonumber \\\\\n & \\leq & \\frac{1}{N_{1}h_{1}^{d}}\\sum_{i=1}^{N_{1}}\\left|K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right|\\nonumber \\\\\n\\implies\\bE\\left[\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right|^{2}\\right] & \\leq & \\frac{1}{N_{1}h_{1}^{2d}}\\sum_{i=1}^{N_{1}}\\bE\\left[\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\right],\\label{eq:densityBound}\n\\end{eqnarray}\nwhere the last step follows from Jensen's inequality. By making the\nsubstitution $\\mathbf{u}_{i}=\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}$\nand $\\mathbf{u}_{i}^{'}=\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}$,\nthis gives \n\\begin{eqnarray*}\n\\frac{1}{h_{1}^{2d}}\\bE\\left[\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\right] & = & \\frac{1}{h^{2d}}\\int\\left(K\\left(\\frac{\\mathbf{X}_{1}-\\mathbf{Y}_{i}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{1}^{'}-\\mathbf{Y}_{i}}{h_{1}}\\right)\\right)^{2}\\times\\\\\n &  & f_{2}(\\mathbf{X}_{1})f_{2}(\\mathbf{X}_{1}^{'})f_{1}(\\mathbf{Y}_{i})d\\mathbf{X}_{1}d\\mathbf{X}_{1}^{'}d\\mathbf{Y}_{i}\\\\\n & \\leq & 2||K||_{\\infty}^{2}.\n\\end{eqnarray*}\nCombining this with Eq.~\\ref{eq:densityBound} gives \n", "index": 67, "text": "\n\\[\n\\bE\\left[\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{'})\\right|^{2}\\right]\\leq2||K||_{\\infty}^{2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left|\\ft 1(\\mathbf{X}_{1})-\\ft 1(\\mathbf{X}_{1}^{{}^{\\prime}})\\right%&#10;|^{2}\\right]\\leq 2||K||_{\\infty}^{2}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>|</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc17</mi><mn>1</mn><msup><mi/><mo>\u2032</mo></msup></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nCombining these results with Eq.~\\ref{eq:lipschitz} gives \n\n", "itemtype": "equation", "pos": 67962, "prevtext": "\nSimilarly, \n", "index": 69, "text": "\n\\[\n\\bE\\left[\\left|\\ft 2(\\mathbf{X}_{1})-\\ft 2(\\mathbf{X}_{1}^{'})\\right|^{2}\\right]\\leq2||K||_{\\infty}^{2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left|\\ft 2(\\mathbf{X}_{1})-\\ft 2(\\mathbf{X}_{1}^{{}^{\\prime}})\\right%&#10;|^{2}\\right]\\leq 2||K||_{\\infty}^{2}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>|</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc17</mi><mn>1</mn><msup><mi/><mo>\u2032</mo></msup></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\nThe second term in Eq.~\\ref{eq:triangle} is controlled in a similar\nway. From the Lipschitz condition, \n\\begin{eqnarray*}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2} & \\leq & C_{g}^{2}\\left|\\ft 2(\\mathbf{X}_{j})-\\ft 2^{'}(\\mathbf{X}_{j})\\right|^{2}\\\\\n & = & \\frac{C_{g}^{2}}{M_{2}^{2}h_{2}^{2d}}\\left(K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h}\\right)-K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}^{'}}{h}\\right)\\right)^{2}.\n\\end{eqnarray*}\nThe $h_{2}^{2d}$ terms are eliminated by making the substitutions\nof $\\mathbf{u}_{j}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h_{2}}$ and\n$\\mathbf{u}_{j}^{'}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}^{'}}{h_{2}}$\nwithin the expectation to obtain \n\\begin{eqnarray}\n\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2}\\right] & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{M_{2}^{2}}\\label{eq:gsquared}\n\\end{eqnarray}\n", "itemtype": "equation", "pos": 68133, "prevtext": "\nCombining these results with Eq.~\\ref{eq:lipschitz} gives \n\n", "index": 71, "text": "\\begin{equation}\n\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right)^{2}\\right]\\leq8C_{g}^{2}||K||_{\\infty}^{2}.\\label{eq:1stTermFinal}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g%&#10;\\left(\\ft 1(\\mathbf{X}_{1}^{{}^{\\prime}}),\\ft 2(\\mathbf{X}_{1}^{{}^{\\prime}})%&#10;\\right)\\right)^{2}\\right]\\leq 8C_{g}^{2}||K||_{\\infty}^{2}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc17</mi><mn>1</mn><msup><mi/><mo>\u2032</mo></msup></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc17</mi><mn>1</mn><msup><mi/><mo>\u2032</mo></msup></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>8</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\\begin{eqnarray}\n & = & \\sum_{j=2}^{N_{2}}\\sum_{i=2}^{N_{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2^{'}(\\mathbf{X}_{i})\\right)\\right|\\right]\\nonumber \\\\\n & \\leq & M_{2}^{2}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2}\\right]\\nonumber \\\\\n & \\leq & 2C_{g}^{2}||K||_{\\infty}^{2},\\label{eq:2ndtermFinal}\n\\end{eqnarray}\nwhere we use the Cauchy Schwarz inequality to bound the expectation\nwithin each summand. Finally, applying Jensen's inequality and Eqs.~\\ref{eq:1stTermFinal}\nand \\ref{eq:2ndtermFinal} gives \n\\begin{eqnarray*}\n\\bE\\left[\\left|\\gt-\\gt^{'}\\right|^{2}\\right] & \\leq & \\frac{2}{N_{2}^{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right|^{2}\\right]\\\\\n &  & +\\frac{2}{N_{2}^{2}}\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\\\\\n & \\leq & \\frac{20C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}^{2}}.\n\\end{eqnarray*}\n\n\nNow suppose we have samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1}^{'},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand denote the respective estimators as $\\gt$ and $\\gt^{'}$. Then\n\\begin{eqnarray*}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right| & \\leq & C_{g}\\left|\\ft 1(\\mathbf{X}_{j})-\\ft 1^{'}(\\mathbf{X}_{j})\\right|\\\\\n & = & \\frac{C_{g}}{N_{1}h_{1}^{d}}\\left|K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{1}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{1}^{'}}{h_{1}}\\right)\\right|\\\\\n\\implies\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right|^{2}\\right] & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{N_{1}^{2}}.\n\\end{eqnarray*}\nThus using a similar argument as was used to obtain Eq.~\\ref{eq:2ndtermFinal},\n\\begin{eqnarray*}\n\\bE\\left[\\left|\\gt-\\gt^{'}\\right|^{2}\\right] & \\leq & \\frac{1}{N_{2}^{2}}\\bE\\left[\\left(\\sum_{j=1}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\\\\\n & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}^{2}}.\n\\end{eqnarray*}\nApplying the Efron-Stein inequality gives \n", "itemtype": "equation", "pos": 69410, "prevtext": "\n\n\nThe second term in Eq.~\\ref{eq:triangle} is controlled in a similar\nway. From the Lipschitz condition, \n\\begin{eqnarray*}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2} & \\leq & C_{g}^{2}\\left|\\ft 2(\\mathbf{X}_{j})-\\ft 2^{'}(\\mathbf{X}_{j})\\right|^{2}\\\\\n & = & \\frac{C_{g}^{2}}{M_{2}^{2}h_{2}^{2d}}\\left(K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h}\\right)-K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}^{'}}{h}\\right)\\right)^{2}.\n\\end{eqnarray*}\nThe $h_{2}^{2d}$ terms are eliminated by making the substitutions\nof $\\mathbf{u}_{j}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h_{2}}$ and\n$\\mathbf{u}_{j}^{'}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}^{'}}{h_{2}}$\nwithin the expectation to obtain \n\\begin{eqnarray}\n\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2}\\right] & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{M_{2}^{2}}\\label{eq:gsquared}\n\\end{eqnarray}\n", "index": 73, "text": "\n\\[\n\\implies\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\implies\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),%&#10;\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime}}(%&#10;\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\" display=\"block\"><mrow><mi/><mo>\u27f9</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\n\n\n\\section{Proof of Theorem~\\ref{thm:clt}}\n\n\\label{sec:cltProof}We are interested in the asymptotic distribution\nof \n\\begin{eqnarray*}\n\\sqrt{N_{2}}\\left(\\gt-\\bE\\left[\\gt\\right]\\right) & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]\\right)\\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]-\\bE\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]\\right).\n\\end{eqnarray*}\nNote that by the standard central limit theorem, the second term converges\nin distribution to a Gaussian random variable. If the first term converges\nin probability to a constant (specifically, 0), then we can use Slutsky's\ntheorem to find the asymptotic distribution. So now we focus on the\nfirst term which we denote as $\\mathbf{V}_{N_{2}}$.\n\nTo prove convergence in probability, we will use Chebyshev's inequality.\nNote that $\\bE\\left[\\mathbf{V}_{N_{2}}\\right]=0$. To bound the variance\nof $\\mathbf{V}_{N_{2}}$, we again use the Efron-Stein inequality.\nLet $\\mathbf{X}_{1}^{'}$ be drawn from $f_{2}$ and denote $\\mathbf{V}_{N_{2}}$\nand $\\mathbf{V}_{N_{2}}^{'}$ as the sequences using $\\mathbf{X}_{1}$\nand $\\mathbf{X}_{1}^{'}$, respectively. Then \n\\begin{eqnarray}\n\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'} & = & \\frac{1}{\\sqrt{N_{2}}}\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right)\\nonumber \\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\left(g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)-\\bE_{\\mathbf{X}_{1}^{'}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right]\\right)\\nonumber \\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=2}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right).\\label{eq:VnDiff}\n\\end{eqnarray}\nNote that \n", "itemtype": "equation", "pos": 72358, "prevtext": "\n\\begin{eqnarray}\n & = & \\sum_{j=2}^{N_{2}}\\sum_{i=2}^{N_{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2^{'}(\\mathbf{X}_{i})\\right)\\right|\\right]\\nonumber \\\\\n & \\leq & M_{2}^{2}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2}\\right]\\nonumber \\\\\n & \\leq & 2C_{g}^{2}||K||_{\\infty}^{2},\\label{eq:2ndtermFinal}\n\\end{eqnarray}\nwhere we use the Cauchy Schwarz inequality to bound the expectation\nwithin each summand. Finally, applying Jensen's inequality and Eqs.~\\ref{eq:1stTermFinal}\nand \\ref{eq:2ndtermFinal} gives \n\\begin{eqnarray*}\n\\bE\\left[\\left|\\gt-\\gt^{'}\\right|^{2}\\right] & \\leq & \\frac{2}{N_{2}^{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right|^{2}\\right]\\\\\n &  & +\\frac{2}{N_{2}^{2}}\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\\\\\n & \\leq & \\frac{20C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}^{2}}.\n\\end{eqnarray*}\n\n\nNow suppose we have samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1}^{'},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand denote the respective estimators as $\\gt$ and $\\gt^{'}$. Then\n\\begin{eqnarray*}\n\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right| & \\leq & C_{g}\\left|\\ft 1(\\mathbf{X}_{j})-\\ft 1^{'}(\\mathbf{X}_{j})\\right|\\\\\n & = & \\frac{C_{g}}{N_{1}h_{1}^{d}}\\left|K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{1}}{h_{1}}\\right)-K\\left(\\frac{\\mathbf{X}_{j}-\\mathbf{Y}_{1}^{'}}{h_{1}}\\right)\\right|\\\\\n\\implies\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right|^{2}\\right] & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{N_{1}^{2}}.\n\\end{eqnarray*}\nThus using a similar argument as was used to obtain Eq.~\\ref{eq:2ndtermFinal},\n\\begin{eqnarray*}\n\\bE\\left[\\left|\\gt-\\gt^{'}\\right|^{2}\\right] & \\leq & \\frac{1}{N_{2}^{2}}\\bE\\left[\\left(\\sum_{j=1}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\\\\\n & \\leq & \\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}^{2}}.\n\\end{eqnarray*}\nApplying the Efron-Stein inequality gives \n", "index": 75, "text": "\n\\[\n\\var\\left[\\gt\\right]\\leq\\frac{10C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}}+\\frac{C_{g}^{2}||K||_{\\infty}^{2}N_{1}}{N_{2}^{2}}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\gt\\right]\\leq\\frac{10C_{g}^{2}||K||_{\\infty}^{2}}{N_{2}}+\\frac{C_{g%&#10;}^{2}||K||_{\\infty}^{2}N_{1}}{N_{2}^{2}}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\gt</mtext></merror><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mfrac><mrow><mn>10</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow><msub><mi>N</mi><mn>2</mn></msub></mfrac><mo>+</mo><mfrac><mrow><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>N</mi><mn>1</mn></msub></mrow><msubsup><mi>N</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nIf we condition on $\\mathbf{X}_{1}$, then by the standard central\nlimit theorem $\\sqrt{N_{i}h_{i}^{d}}\\left(\\ft i(\\mathbf{X}_{1})-\\bE_{\\mathbf{X}_{1}}\\left[\\ft i(\\mathbf{X}_{1})\\right]\\right)$\nconverges in distribution to a zero mean Gaussian random variable\nwith variance $\\sigma_{i}^{2}(\\mathbf{X}_{1})=O(1).$ This is true\neven if $\\mathbf{X}_{1}$ is close to the boundary of the support\nof the densities. The KDEs $\\ft 1(\\mathbf{X}_{1})$ and $\\ft 2(\\mathbf{X}_{1})$\nare conditionally independent given $\\mathbf{X}_{1}$ as are their\nlimiting distributions. Thus the KDEs converge jointly in distribution\nto a Gaussian random vector with zero mean, zero covariance, and their\nrespective variances. By the delta method, we have that if $g(x,y)$\nis continuously differentiable with respect to both $x$ and $y$\nat $\\bE_{\\mathbf{X}_{1}}\\left[\\ft i(\\mathbf{X}_{1})\\right]$ for $i=1,2$,\nrespectively, then \n", "itemtype": "equation", "pos": 74579, "prevtext": "\n\n\n\n\\section{Proof of Theorem~\\ref{thm:clt}}\n\n\\label{sec:cltProof}We are interested in the asymptotic distribution\nof \n\\begin{eqnarray*}\n\\sqrt{N_{2}}\\left(\\gt-\\bE\\left[\\gt\\right]\\right) & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]\\right)\\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]-\\bE\\left[g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right]\\right).\n\\end{eqnarray*}\nNote that by the standard central limit theorem, the second term converges\nin distribution to a Gaussian random variable. If the first term converges\nin probability to a constant (specifically, 0), then we can use Slutsky's\ntheorem to find the asymptotic distribution. So now we focus on the\nfirst term which we denote as $\\mathbf{V}_{N_{2}}$.\n\nTo prove convergence in probability, we will use Chebyshev's inequality.\nNote that $\\bE\\left[\\mathbf{V}_{N_{2}}\\right]=0$. To bound the variance\nof $\\mathbf{V}_{N_{2}}$, we again use the Efron-Stein inequality.\nLet $\\mathbf{X}_{1}^{'}$ be drawn from $f_{2}$ and denote $\\mathbf{V}_{N_{2}}$\nand $\\mathbf{V}_{N_{2}}^{'}$ as the sequences using $\\mathbf{X}_{1}$\nand $\\mathbf{X}_{1}^{'}$, respectively. Then \n\\begin{eqnarray}\n\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'} & = & \\frac{1}{\\sqrt{N_{2}}}\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right)\\nonumber \\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\left(g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)-\\bE_{\\mathbf{X}_{1}^{'}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right]\\right)\\nonumber \\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=2}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right).\\label{eq:VnDiff}\n\\end{eqnarray}\nNote that \n", "index": 77, "text": "\n\\[\n\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right)^{2}\\right]=\\bE\\left[\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{%&#10;\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)%&#10;\\right]\\right)^{2}\\right]=\\bE\\left[\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(%&#10;\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right].\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><msub><mi>\ud835\udc17</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><msub><mi>\ud835\udc17</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nprovided that $N_{i}h_{i}^{d}\\rightarrow\\infty$. Thus $\\bE\\left[\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right]=o(1)$.\nA similar result holds when we replace $\\mathbf{X}_{1}$ with $\\mathbf{X}_{1}^{'}$. \n\nFor the third term in Eq.~\\ref{eq:VnDiff}, \n", "itemtype": "equation", "pos": 75778, "prevtext": "\nIf we condition on $\\mathbf{X}_{1}$, then by the standard central\nlimit theorem $\\sqrt{N_{i}h_{i}^{d}}\\left(\\ft i(\\mathbf{X}_{1})-\\bE_{\\mathbf{X}_{1}}\\left[\\ft i(\\mathbf{X}_{1})\\right]\\right)$\nconverges in distribution to a zero mean Gaussian random variable\nwith variance $\\sigma_{i}^{2}(\\mathbf{X}_{1})=O(1).$ This is true\neven if $\\mathbf{X}_{1}$ is close to the boundary of the support\nof the densities. The KDEs $\\ft 1(\\mathbf{X}_{1})$ and $\\ft 2(\\mathbf{X}_{1})$\nare conditionally independent given $\\mathbf{X}_{1}$ as are their\nlimiting distributions. Thus the KDEs converge jointly in distribution\nto a Gaussian random vector with zero mean, zero covariance, and their\nrespective variances. By the delta method, we have that if $g(x,y)$\nis continuously differentiable with respect to both $x$ and $y$\nat $\\bE_{\\mathbf{X}_{1}}\\left[\\ft i(\\mathbf{X}_{1})\\right]$ for $i=1,2$,\nrespectively, then \n", "index": 79, "text": "\n\\[\n\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]=O\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2}h_{2}^{d}}\\right)=o(1),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})%&#10;\\right)\\right]=O\\left(\\frac{1}{N_{1}h_{1}^{d}}+\\frac{1}{N_{2}h_{2}^{d}}\\right)%&#10;=o(1),\" display=\"block\"><mrow><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><msub><mi>\ud835\udc17</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mi>d</mi></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n", "itemtype": "equation", "pos": 76245, "prevtext": "\nprovided that $N_{i}h_{i}^{d}\\rightarrow\\infty$. Thus $\\bE\\left[\\var_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right]=o(1)$.\nA similar result holds when we replace $\\mathbf{X}_{1}$ with $\\mathbf{X}_{1}^{'}$. \n\nFor the third term in Eq.~\\ref{eq:VnDiff}, \n", "index": 81, "text": "\n\\[\n\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(%&#10;\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime}}(%&#10;\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]\" display=\"block\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThere are $M_{2}$ terms where $i=j$ and we have from Appendix~\\ref{sec:VarProof}\n(see Eq.~\\ref{eq:gsquared}) that \n", "itemtype": "equation", "pos": 76435, "prevtext": "\n", "index": 83, "text": "\n\\[\n=\\sum_{j=2}^{N_{2}}\\sum_{i=2}^{N_{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2^{'}(\\mathbf{X}_{i})\\right)\\right|\\right].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"=\\sum_{j=2}^{N_{2}}\\sum_{i=2}^{N_{2}}\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j%&#10;}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime%&#10;}}(\\mathbf{X}_{j})\\right)\\right|\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(%&#10;\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2^{{}^{\\prime}}(%&#10;\\mathbf{X}_{i})\\right)\\right|\\right].\" display=\"block\"><mrow><mrow><mi/><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThus these terms are $O\\left(\\frac{1}{M_{2}}\\right)$. There are $M_{2}^{2}-M_{2}$\nterms when $i\\neq j$. In this case, we can do four substitutions\nof the form $\\mathbf{u}_{j}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h_{2}}$\nto obtain \n", "itemtype": "equation", "pos": 76875, "prevtext": "\nThere are $M_{2}$ terms where $i=j$ and we have from Appendix~\\ref{sec:VarProof}\n(see Eq.~\\ref{eq:gsquared}) that \n", "index": 85, "text": "\n\\[\n\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|^{2}\\right]\\leq\\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{M_{2}^{2}}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g%&#10;\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime}}(\\mathbf{X}_{j})\\right)\\right|^%&#10;{2}\\right]\\leq\\frac{2C_{g}^{2}||K||_{\\infty}^{2}}{M_{2}^{2}}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup></mrow><msubsup><mi>M</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThen since $h_{2}^{d}=o(1)$, we get \n\n", "itemtype": "equation", "pos": 77316, "prevtext": "\nThus these terms are $O\\left(\\frac{1}{M_{2}}\\right)$. There are $M_{2}^{2}-M_{2}$\nterms when $i\\neq j$. In this case, we can do four substitutions\nof the form $\\mathbf{u}_{j}=\\frac{\\mathbf{X}_{j}-\\mathbf{X}_{1}}{h_{2}}$\nto obtain \n", "index": 87, "text": "\n\\[\n\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2^{'}(\\mathbf{X}_{i})\\right)\\right|\\right]\\leq\\frac{4C_{g}^{2}||K||_{\\infty}^{2}h_{2}^{2d}}{M_{2}^{2}}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g%&#10;\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime}}(\\mathbf{X}_{j})\\right)\\right|%&#10;\\left|g\\left(\\ft 1(\\mathbf{X}_{i}),\\ft 2(\\mathbf{X}_{i})\\right)-g\\left(\\ft 1(%&#10;\\mathbf{X}_{i}),\\ft 2^{{}^{\\prime}}(\\mathbf{X}_{i})\\right)\\right|\\right]\\leq%&#10;\\frac{4C_{g}^{2}||K||_{\\infty}^{2}h_{2}^{2d}}{M_{2}^{2}}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><msubsup><mi>C</mi><mi>g</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>K</mi><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mi>h</mi><mn>2</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>d</mi></mrow></msubsup></mrow><msubsup><mi>M</mi><mn>2</mn><mn>2</mn></msubsup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n\\begin{eqnarray*}\n\\implies\\bE\\left[\\left(\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'}\\right)^{2}\\right] & \\leq & \\frac{3}{N_{2}}\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right)^{2}\\right]\\\\\n &  & +\\frac{3}{N_{2}}\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)-\\bE_{\\mathbf{X}_{1}^{'}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right]\\right)^{2}\\right]\\\\\n &  & +\\frac{3}{N_{2}}\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right)\\right)^{2}\\right]\\\\\n & = & o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\n\n\nNow consider samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1}^{'},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand the respective sequences $\\mathbf{V}_{N_{2}}$ and $\\mathbf{V}_{N_{2}}^{'}$.\nThen \n\\begin{eqnarray*}\n\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'} & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right).\n\\end{eqnarray*}\nUsing a similar argument as that used to obtain Eq.~\\ref{eq:CLTSum},\nwe have that if $h_{1}^{d}=o(1)$ and $N_{1}\\rightarrow\\infty$, then\n", "itemtype": "equation", "pos": 77702, "prevtext": "\nThen since $h_{2}^{d}=o(1)$, we get \n\n", "index": 89, "text": "\\begin{equation}\n\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]=o(1),\\label{eq:CLTSum}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(%&#10;\\mathbf{X}_{j})\\right)-g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2^{{}^{\\prime}}(%&#10;\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]=o(1),\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>2</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n", "itemtype": "equation", "pos": 79449, "prevtext": "\n\\begin{eqnarray*}\n\\implies\\bE\\left[\\left(\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'}\\right)^{2}\\right] & \\leq & \\frac{3}{N_{2}}\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)-\\bE_{\\mathbf{X}_{1}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}),\\ft 2(\\mathbf{X}_{1})\\right)\\right]\\right)^{2}\\right]\\\\\n &  & +\\frac{3}{N_{2}}\\bE\\left[\\left(g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)-\\bE_{\\mathbf{X}_{1}^{'}}\\left[g\\left(\\ft 1(\\mathbf{X}_{1}^{'}),\\ft 2(\\mathbf{X}_{1}^{'})\\right)\\right]\\right)^{2}\\right]\\\\\n &  & +\\frac{3}{N_{2}}\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2^{'}(\\mathbf{X}_{j})\\right)\\right)\\right)^{2}\\right]\\\\\n & = & o\\left(\\frac{1}{N_{2}}\\right).\n\\end{eqnarray*}\n\n\nNow consider samples $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand $\\left\\{ \\mathbf{X}_{1},\\dots,\\mathbf{X}_{N_{2}},\\mathbf{Y}_{1}^{'},\\dots,\\mathbf{Y}_{N_{1}}\\right\\} $\nand the respective sequences $\\mathbf{V}_{N_{2}}$ and $\\mathbf{V}_{N_{2}}^{'}$.\nThen \n\\begin{eqnarray*}\n\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'} & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right).\n\\end{eqnarray*}\nUsing a similar argument as that used to obtain Eq.~\\ref{eq:CLTSum},\nwe have that if $h_{1}^{d}=o(1)$ and $N_{1}\\rightarrow\\infty$, then\n", "index": 91, "text": "\n\\[\n\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{'}(\\mathbf{X}_{j}),\\ft 2(\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]=o(1)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"\\bE\\left[\\left(\\sum_{j=2}^{N_{2}}\\left|g\\left(\\ft 1(\\mathbf{X}_{j}),\\ft 2(%&#10;\\mathbf{X}_{j})\\right)-g\\left(\\ft 1^{{}^{\\prime}}(\\mathbf{X}_{j}),\\ft 2(%&#10;\\mathbf{X}_{j})\\right)\\right|\\right)^{2}\\right]=o(1)\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><msub><mi>N</mi><mn>2</mn></msub></munderover><mrow><mo>|</mo><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><msup><mn>1</mn><msup><mi/><mo>\u2032</mo></msup></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc17</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nApplying the Efron-Stein inequality gives \n", "itemtype": "equation", "pos": 79644, "prevtext": "\n", "index": 93, "text": "\n\\[\n\\implies\\bE\\left[\\left(\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{'}\\right)^{2}\\right]=o\\left(\\frac{1}{N_{2}}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\implies\\bE\\left[\\left(\\mathbf{V}_{N_{2}}-\\mathbf{V}_{N_{2}}^{{}^{\\prime}}%&#10;\\right)^{2}\\right]=o\\left(\\frac{1}{N_{2}}\\right).\" display=\"block\"><mrow><mrow><mi/><mo>\u27f9</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><msub><mi>\ud835\udc15</mi><msub><mi>N</mi><mn>2</mn></msub></msub><mo>-</mo><msubsup><mi>\ud835\udc15</mi><msub><mi>N</mi><mn>2</mn></msub><msup><mi/><mo>\u2032</mo></msup></msubsup></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mn>1</mn><msub><mi>N</mi><mn>2</mn></msub></mfrac><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThus by Chebyshev's inequality, \n", "itemtype": "equation", "pos": 79807, "prevtext": "\nApplying the Efron-Stein inequality gives \n", "index": 95, "text": "\n\\[\n\\var\\left[\\mathbf{V}_{N_{2}}\\right]=o\\left(\\frac{N_{2}+N_{1}}{N_{2}}\\right)=o(1).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\mathbf{V}_{N_{2}}\\right]=o\\left(\\frac{N_{2}+N_{1}}{N_{2}}\\right)=o(%&#10;1).\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>\ud835\udc15</mi><msub><mi>N</mi><mn>2</mn></msub></msub><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>+</mo><msub><mi>N</mi><mn>1</mn></msub></mrow><msub><mi>N</mi><mn>2</mn></msub></mfrac><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nand therefore $\\mathbf{V}_{N_{2}}$ converges to zero in probability.\nBy Slutsky's theorem, $\\sqrt{N_{2}}\\left(\\gt-\\bE\\left[\\gt\\right]\\right)$\nconverges in distribution to a zero mean Gaussian random variable\nwith variance \n", "itemtype": "equation", "pos": 79928, "prevtext": "\nThus by Chebyshev's inequality, \n", "index": 97, "text": "\n\\[\n\\Pr\\left(\\left|\\mathbf{V}_{N_{2}}\\right|>\\epsilon\\right)\\leq\\frac{\\var\\left[\\mathbf{V}_{N_{2}}\\right]}{\\epsilon^{2}}=o(1),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"\\Pr\\left(\\left|\\mathbf{V}_{N_{2}}\\right|&gt;\\epsilon\\right)\\leq\\frac{\\var\\left[%&#10;\\mathbf{V}_{N_{2}}\\right]}{\\epsilon^{2}}=o(1),\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><mo>|</mo><msub><mi>\ud835\udc15</mi><msub><mi>N</mi><mn>2</mn></msub></msub><mo>|</mo></mrow><mo>&gt;</mo><mi>\u03f5</mi></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>\ud835\udc15</mi><msub><mi>N</mi><mn>2</mn></msub></msub><mo>]</mo></mrow></mrow><msup><mi>\u03f5</mi><mn>2</mn></msup></mfrac><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\n where $\\mathbf{X}$ is drawn from $f_{2}$.\n\nFor the weighted ensemble estimator, we wish to know the asymptotic\ndistribution of $\\sqrt{N_{2}}\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]\\right)$\nwhere $\\tilde{\\mathbf{G}}_{w}=\\sum_{l\\in\\bar{l}}w(l)\\tilde{\\mathbf{G}}_{h(l)}$.\nWe have that \n\\begin{eqnarray*}\n\\sqrt{N_{2}}\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]\\right) & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\sum_{l\\in\\bar{l}}w(l)\\left(g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)-\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]\\right)\\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(\\bE_{\\mathbf{X}_{j}}\\left[\\sum_{l\\in\\bar{l}}w(l)g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]-\\bE\\left[\\sum_{l\\in\\bar{l}}w(l)g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]\\right).\n\\end{eqnarray*}\nThe second term again converges in distribution to a Gaussian random\nvariable by the central limit theorem. The mean and variance are,\nrespectively, zero and \n", "itemtype": "equation", "pos": 80280, "prevtext": "\nand therefore $\\mathbf{V}_{N_{2}}$ converges to zero in probability.\nBy Slutsky's theorem, $\\sqrt{N_{2}}\\left(\\gt-\\bE\\left[\\gt\\right]\\right)$\nconverges in distribution to a zero mean Gaussian random variable\nwith variance \n", "index": 99, "text": "\n\\[\n\\var\\left[\\bE_{\\mathbf{X}}\\left[g\\left(\\ft 1(\\mathbf{X}),\\ft 2(\\mathbf{X})\\right)\\right]\\right],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\bE_{\\mathbf{X}}\\left[g\\left(\\ft 1(\\mathbf{X}),\\ft 2(\\mathbf{X})%&#10;\\right)\\right]\\right],\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mi>\ud835\udc17</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ft</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06884.tex", "nexttext": "\nThe first term is equal to \n\\begin{eqnarray*}\n\\sum_{l\\in\\bar{l}}w(l)\\left(\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)-\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]\\right)\\right) & = & \\sum_{l\\in\\bar{l}}w(l)o_{P}(1)\\\\\n & = & o_{P}(1),\n\\end{eqnarray*}\nwhere $o_{P}(1)$ denotes convergence to zero in probability. In the\nlast step, we used the fact that if two random variables converge\nin probability to constants, then their linear combination converges\nin probability to the linear combination of the constants. Combining\nthis result with Slutsky's theorem completes the proof.\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{References}\n\n\n", "itemtype": "equation", "pos": 81484, "prevtext": "\n where $\\mathbf{X}$ is drawn from $f_{2}$.\n\nFor the weighted ensemble estimator, we wish to know the asymptotic\ndistribution of $\\sqrt{N_{2}}\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]\\right)$\nwhere $\\tilde{\\mathbf{G}}_{w}=\\sum_{l\\in\\bar{l}}w(l)\\tilde{\\mathbf{G}}_{h(l)}$.\nWe have that \n\\begin{eqnarray*}\n\\sqrt{N_{2}}\\left(\\tilde{\\mathbf{G}}_{w}-\\bE\\left[\\tilde{\\mathbf{G}}_{w}\\right]\\right) & = & \\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\sum_{l\\in\\bar{l}}w(l)\\left(g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)-\\bE_{\\mathbf{X}_{j}}\\left[g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]\\right)\\\\\n &  & +\\frac{1}{\\sqrt{N_{2}}}\\sum_{j=1}^{N_{2}}\\left(\\bE_{\\mathbf{X}_{j}}\\left[\\sum_{l\\in\\bar{l}}w(l)g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]-\\bE\\left[\\sum_{l\\in\\bar{l}}w(l)g\\left(\\ftl 1(\\mathbf{X}_{j}),\\ftl 2(\\mathbf{X}_{j})\\right)\\right]\\right).\n\\end{eqnarray*}\nThe second term again converges in distribution to a Gaussian random\nvariable by the central limit theorem. The mean and variance are,\nrespectively, zero and \n", "index": 101, "text": "\n\\[\n\\var\\left[\\sum_{l\\in\\bar{l}}w(l)\\bE_{\\mathbf{X}}\\left[g\\left(\\ftl 1(\\mathbf{X}),\\ftl 2(\\mathbf{X})\\right)\\right]\\right].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\var\\left[\\sum_{l\\in\\bar{l}}w(l)\\bE_{\\mathbf{X}}\\left[g\\left(\\ftl 1(\\mathbf{X}%&#10;),\\ftl 2(\\mathbf{X})\\right)\\right]\\right].\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\var</mtext></merror><mo>\u2062</mo><mrow><mo>[</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mover accent=\"true\"><mi>l</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow></munder><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bE</mtext></merror><mi>\ud835\udc17</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ftl</mtext></merror><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ftl</mtext></merror><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]