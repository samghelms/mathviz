[{"file": "1601.00732.tex", "nexttext": "\nHowever rank minimisation is an intractable problem. Therefore LRR actually uses the nuclear norm $\\| \\cdot \\|_*$ (sum of the matrix's singular values) as the closest convex relation\n\n", "itemtype": "equation", "pos": 8237, "prevtext": "\n\n\\title{Low-Rank Representation over the Manifold of Curves}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Stephen Tierney$^1$, Junbin Gao$^2$, Yi Guo$^3$ and Zhengwu Zhang$^4$\\\\\n{\\small $^1$School of Computing and Mathematics,\nCharles Sturt University,\nBathurst, NSW 2795, Australia}\\\\\n{\\small $^2$Business Analytics Discipline, The University of Sydney Business School, Camperdown NSW 2006, Australia}\\\\\n{\\small $^3$Digital Productivity \\& Services Flagship,\nCSIRO, North Ryde, NSW 2113, Australia} \\\\\n{\\small $^4$Department of Statistics,\nStatistical and Applied Mathematical Sciences Institute, NC 27709-4006, USA}\\\\\n{\\tt\\small stierney@csu.edu.au; junbin.gao@sydney.edu.au; yi.guo@csiro.au; zhengwustat@gmail.com}\n}\n\n \n\\maketitle\n\n\\begin{abstract}\nIn machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e.\\ each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data  can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a method to analyse subspace structure of the functional data by using the state of the art Low-Rank Representation (LRR). Experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional LRR in tasks concerning functional data.\n\\end{abstract}\n\n\\section{Introduction}\n\nIn machine learning it is common to interpret each data point as a vector in Euclidean space \\cite{Bishop2006}. Such a discretisation is chosen because it allows for easy closed form solutions and fast computation, even with large datasets. However these methods ignore the fact that the data may not naturally fit into this assumption. In fact much of the data collected for practical machine learning are actually functions i.e. curves. For example financial data such as stock or commodity prices are functions of monetary value over time. Functional data have become increasingly important in many scientific and engineering research areas such as ECG (electrocardiogram) or EEG (Electroencephalography) in healthcare, biology data analysis, weather or climate data and motion trajectories from computer vision.  \n\nAnalyzing functional data has been an emerging topic in statistical research \\cite{FerratyRomain2011,Mueller2011,SrivastavaShantanuJermyn2011,SrivastavaWuKurtekKlassenMarron2011}  and  has attracted great attention from machine learning community in recent years \\cite{BahadoriKaleFanLiu2015,PetitjeanForestierWebbNicholsonChenKeogh2014}.   One of important challenges in analyzing functional data for machine learning is to efficiently cluster and to learn better representations for functional data. Theoretically the underlying process for functional data is of infinite dimension, thus it is difficult to work with them with only finite samples available.  A desired model for functional data is expected to properly and parsimoniously characterize the nature and variability hidden in the data. The classic functional principal component analysis (fPCA) \\cite{RamsaySilverman2005} is one of such examples to discover dominant modes of variation in the data. However fPCA may fail to capture patterns if the functional data are not well aligned in its domain.  For time series, a special type of functional data, dynamic time warping (DTW) has long been proposed to compare time series based on shape and distortions (e.g., shifting and stretching) along the temporal axis \\cite{Rakthanmanon2013,TuckerWuSrivastava2013}.\n\nAnother important type of functional data is shape \\cite{SuSrivastavaHuffer2013,SrivastavaShantanuJermyn2011}.   Shape is an important characterizing feature for objects and in computer vision shape has been widely used for the purpose of object detection, tracking, classification, and recognition. In fact, a natural and popular representation for shape analysis is to parametrize boundaries of planar objects as 2D curves. In object recognition, images of the same object should be similar regardless of resolution, lighting, or orientation. Hence an efficient shape representation or shape analysis scheme must be invariant to scale, translation and rotation. A very useful shape representation is the square-root velocity function (SRVF) representation \\cite{JoshiKlassenSrivastavaJermyn2007,SrivastavaShantanuJermyn2011}. In general, the resulting SRVF of a continuous shape is square integrable, a well-defined Hilbert space where appropriate measurement can be applied, refer to Section~\\ref{Sec:2} for more details. By acknowledging the true nature of the data we can develop more powerful methods that exploit features that would otherwise be ignored or lead to erroneous results with simple linear models. \n\n\n\n\nOur intention in this study is to consider functional data clustering by accounting for the possible invariance in scaling/stretching, translation and rotation of functional data to help maintain shape characteristics. The focus of this paper is upon functional data where data sets consist of continuous real curves including shapes in Euclidean spaces. More specifically we propose a method of subspace analysis for functional data based on the idea developed in recent subspace clustering. The idea is to apply a feature mapping such as the aforementioned SRVF to the curves so that they are transformed onto the curve manifold, where the subspace analysis can be conducted based on the geometry on the manifold. In particular, we adapt the well known low-rank representation (LRR) framework \\cite{LiuLinYanSunYuMa2013} to deal with data that lie on the manifold of open curves by implementing the classical LRR in tangent spaces of the manifold \\cite{FuGaoHongTien2015,WangHuGaoSunYin2015,YinGaoGuo2015}. \n\nLRR on Euclidean spaces \\cite{LiuLinYanSunYuMa2013} is closely related to several state-of-the-art subspace analysis approaches such as Sparse Subspace Clustering (SSC) \\cite{ElhamifarVidal2013}, Robust PCA (RPCA) \\cite{CandesLiMaWright2010} and low-rank Matrix Completion (MC) \\cite{WuGaneshShiMatsushitaWangMa2012} methods.   \nThis mixture of subspaces model has naturally led to the development of subspace segmentation methods. Such methods aim to segment the data into clusters with each cluster corresponding to a unique subspace. More formally, given a data matrix of observed column-wise data samples $\\mathbf A = [\\mathbf{ a_1,a_2,\\dots,a_N}] \\in \\mathbb{R}^{D \\times N}$, the objective of subspace clustering is to assign each data sample to its underlying subspace. The basic assumption is that the data within $\\mathbf A$ is drawn from a union of $c$ subspaces $\\{S_i\\}^c_{i=1}$ of dimensions $\\{d_i\\}^c_{i=1}$. \n\n\nThe core of both SSC and LRR is to learn an affinity matrix for the given dataset and the learned affinity matrix will be pipelined to a spectral clustering method like nCUT \\cite{ShiMalik2000} to obtain the final subspace labels. \n\n\n\n\n\nTo learn the affinity matrix, SSC relies on the self expressive property \\cite{ElhamifarVidal2013}, which is that\\begin{quote}\n{\\it{each data point in a union of subspaces can be efficiently reconstructed by a linear combination of other points in the data}}.\n\\end{quote}\nIn other words, each point can be written as a linear combination of the other points i.e.\\ $\\mathbf{A = A Z}$, where $\\mathbf Z \\in \\mathbb{R}^{N \\times N}$ is a matrix of coefficients. Most methods however assume the data generation model $\\mathbf{X = A + N}$, where $\\mathbf X$ is the observed data and $\\mathbf N$ is noise.\nSince it is difficult to separate the noise from the data the solution is to relax the self-expressive model to  $\\mathbf{X = X Z + E}$, where $\\mathbf E$ is a fitting error and is different from $\\mathbf N$.\n\nSimilarly LRR \\cite{LiuLinYanSunYuMa2013} exploits the self expressive property but attempts to learn the global subspace structure by computing the lowest-rank representation of the set of data points. In other words, data points belonging to the same subspace should have similar coefficient patterns. In the presence of noise LRR attempts to minimise the following objective\n\n", "index": 1, "text": "\\begin{align}\n\\min_{\\mathbf{Z, E}} \\; \\frac{1}{2}\\| \\mathbf E \\|_{\\ell} + \\textrm{rank}( \\mathbf{Z} ), \\quad\n\\text{s.t.} \\quad \\mathbf{X = XZ + E}.  \\label{(1)}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{\\mathbf{Z,E}}\\;\\frac{1}{2}\\|\\mathbf{E}\\|_{\\ell}+\\textrm{%&#10;rank}(\\mathbf{Z}),\\quad\\text{s.t.}\\quad\\mathbf{X=XZ+E}.\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+2.8pt\"><munder><mi>min</mi><mrow><mi>\ud835\udc19</mi><mo>,</mo><mi>\ud835\udc04</mi></mrow></munder></mpadded><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc04</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msub></mrow><mo>+</mo><mrow><mtext>rank</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>\ud835\udc17</mi></mrow><mo>=</mo><mrow><mi>\ud835\udc17\ud835\udc19</mi><mo>+</mo><mi>\ud835\udc04</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\| \\cdot \\|_{\\ell}$ is a placeholder for the norm most appropriate to the expected noise type. For example in the case of Gaussian noise the best choice is the $\\ell_2$ norm i.e.\\ $\\| \\cdot \\|_F^2$ and for sparse noise the $\\ell_1$ norm should be used.\n\nBoth SSC and LRR rely on the linear self expressive property. This property is no longer available in the nonlinear manifold, e.g. the manifold of open curves as mentioned previously. To generalize LRR or SSC for data in the manifold space, we explicitly explore the underlying nonlinear data structure and utilize the techniques of exponential and logarithm mappings to bring data to a local linear space. \n\nThe rest of the paper is organized as follows. In Section \\ref{Sec:2}, we review the preliminaries about the manifold of open curves and introduce the curve Low-Rank Representation (cLRR) model. Section \\ref{Sec:3} is dedicated to  explaining an efficient algorithm for solving the optimization proposed in cLRR based on the linearized alternative direction method with adaptive penalty (LADMAP) and the algorithm convergence and complexity are also analyzed.  In Section \\ref{Sec:4}, the proposed model is assessed    on both synthetic and real world databases against several state-of-the-art methods. Finally, conclusions are discussed in Section \\ref{Sec:5}.\n\n\\section{LRR over the Curve Manifold}\\label{Sec:2}\n\nAs previously discussed LRR is limited to a linear model and its current version can only be applied to vector data from a Euclidean space. Matrix $\\mathbf Z$ in \\eqref{(1)} or \\eqref{(2)} encodes the affinity/similarity between data points. However this assumption is often unnatural and quite limiting. Much of the data encountered in real world is functional. In other words it exhibits a curve like structure over a domain. Euclidean linear models are unable to capture the nonlinear invariance embedded in each data point. For example in thermal infra-red data of geological substances a curve may contain a key identifying feature such as a dip near a particular frequency. This dip may shift or vary position over time even for the same substance due to impurities. Under a linear vector model this variation may cause the vector to drastically move in the ambient Euclidean space and cause poor results. Or in other cases the feature may be elongated, shrunk or be subject to some non-uniformly warping or scaling. In all these cases the linear model will fail to accurately represent the non-linear affinity in the data.\n\nExploring these unique non-linear invariance in functional data is the focus of this paper. We now discuss how to adapt LRR (similar approach appliable to SSC) such that it easily accepts curve data and nonlinear relationships within clusters can be easily discovered.\n\\subsection{The Curve Manifold}\nGiven a smooth parameterized $n$-dimension curve $\\beta : D = [0, 1] \\to \\mathbb R^n$, we represent it using he square-root velocity function (SRVF) representation  \\cite{JoshiKlassenSrivastavaJermyn2007,SrivastavaShantanuJermyn2011}, which is given by\n\n", "itemtype": "equation", "pos": 8593, "prevtext": "\nHowever rank minimisation is an intractable problem. Therefore LRR actually uses the nuclear norm $\\| \\cdot \\|_*$ (sum of the matrix's singular values) as the closest convex relation\n\n", "index": 3, "text": "\\begin{align}\n\\min_{\\mathbf{Z, E}} \\; \\frac{1}{2}\\| \\mathbf E \\|_{\\ell} + \\| \\mathbf{Z} \\|_*, \\quad\n\\text{s.t.} \\quad \\mathbf{X = XZ + E},  \\label{(2)}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{\\mathbf{Z,E}}\\;\\frac{1}{2}\\|\\mathbf{E}\\|_{\\ell}+\\|\\mathbf{Z%&#10;}\\|_{*},\\quad\\text{s.t.}\\quad\\mathbf{X=XZ+E},\" display=\"inline\"><mrow><mrow><mrow><mrow><mrow><mrow><mpadded width=\"+2.8pt\"><munder><mi>min</mi><mrow><mi>\ud835\udc19</mi><mo>,</mo><mi>\ud835\udc04</mi></mrow></munder></mpadded><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc04</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msub></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc19</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo rspace=\"12.5pt\">,</mo><mtext>s.t.</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi>\ud835\udc17</mi></mrow><mo>=</mo><mrow><mi>\ud835\udc17\ud835\udc19</mi><mo>+</mo><mi>\ud835\udc04</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nThe SRVF mapping transforms the original curve $\\beta(t)$ into a gradient based representation, which facilitates the comparing of the shape information. \n\nIn this paper, we focus on the set of open curves, e.g. the curves do not form a loop ($\\beta(0) \\neq \\beta(1)$). For handling general curves, we refer readers to  \\cite{SrivastavaShantanuJermyn2011}. The SRVF facilitates a measure and geometry bearing invariance to scaling, shifting and reparameterization in the curves domain. For example, all the translated curves from a curve $\\beta(t)$ will have the same SRVF.  Robinson \\cite{Robinson2012} proved that if the curve $\\beta(t)$ is absolutely continuous, then its SRVF $q(t)$ is square-integrable, i.e., $q(t)$ is in a functional Hilbert space $L^2(D, \\mathbb{R}^n)$ .  Conversely for each $q(t)\\in L^2(D, \\mathbb{R}^n)$, there exists a curve $\\beta(t)$ whose SRVF corresponds to $q(t)$. Thus the set $L^2(D, \\mathbb{R}^n)$ is a well-defined representation space of all the curves.  The most important advantage offered by the SRVF framework is that the natural and widely used $L^2$-measure on $L^2(D, \\mathbb{R}^n)$ is invariant to the reparameterization. That is, for any two SRVFs $q_1$ and $q_2$ and a randomly chosen reparametrization function (non-decreasing) $t = \\gamma(\\tau)$, we have \n", "itemtype": "equation", "pos": 11828, "prevtext": "\nwhere $\\| \\cdot \\|_{\\ell}$ is a placeholder for the norm most appropriate to the expected noise type. For example in the case of Gaussian noise the best choice is the $\\ell_2$ norm i.e.\\ $\\| \\cdot \\|_F^2$ and for sparse noise the $\\ell_1$ norm should be used.\n\nBoth SSC and LRR rely on the linear self expressive property. This property is no longer available in the nonlinear manifold, e.g. the manifold of open curves as mentioned previously. To generalize LRR or SSC for data in the manifold space, we explicitly explore the underlying nonlinear data structure and utilize the techniques of exponential and logarithm mappings to bring data to a local linear space. \n\nThe rest of the paper is organized as follows. In Section \\ref{Sec:2}, we review the preliminaries about the manifold of open curves and introduce the curve Low-Rank Representation (cLRR) model. Section \\ref{Sec:3} is dedicated to  explaining an efficient algorithm for solving the optimization proposed in cLRR based on the linearized alternative direction method with adaptive penalty (LADMAP) and the algorithm convergence and complexity are also analyzed.  In Section \\ref{Sec:4}, the proposed model is assessed    on both synthetic and real world databases against several state-of-the-art methods. Finally, conclusions are discussed in Section \\ref{Sec:5}.\n\n\\section{LRR over the Curve Manifold}\\label{Sec:2}\n\nAs previously discussed LRR is limited to a linear model and its current version can only be applied to vector data from a Euclidean space. Matrix $\\mathbf Z$ in \\eqref{(1)} or \\eqref{(2)} encodes the affinity/similarity between data points. However this assumption is often unnatural and quite limiting. Much of the data encountered in real world is functional. In other words it exhibits a curve like structure over a domain. Euclidean linear models are unable to capture the nonlinear invariance embedded in each data point. For example in thermal infra-red data of geological substances a curve may contain a key identifying feature such as a dip near a particular frequency. This dip may shift or vary position over time even for the same substance due to impurities. Under a linear vector model this variation may cause the vector to drastically move in the ambient Euclidean space and cause poor results. Or in other cases the feature may be elongated, shrunk or be subject to some non-uniformly warping or scaling. In all these cases the linear model will fail to accurately represent the non-linear affinity in the data.\n\nExploring these unique non-linear invariance in functional data is the focus of this paper. We now discuss how to adapt LRR (similar approach appliable to SSC) such that it easily accepts curve data and nonlinear relationships within clusters can be easily discovered.\n\\subsection{The Curve Manifold}\nGiven a smooth parameterized $n$-dimension curve $\\beta : D = [0, 1] \\to \\mathbb R^n$, we represent it using he square-root velocity function (SRVF) representation  \\cite{JoshiKlassenSrivastavaJermyn2007,SrivastavaShantanuJermyn2011}, which is given by\n\n", "index": 5, "text": "\\begin{align*}\nq(t) = \\frac{\\dot{\\beta}(t)}{\\sqrt{\\| \\dot{\\beta}(t) \\|}}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle q(t)=\\frac{\\dot{\\beta}(t)}{\\sqrt{\\|\\dot{\\beta}(t)\\|}}.\" display=\"inline\"><mrow><mrow><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mover accent=\"true\"><mi>\u03b2</mi><mo>\u02d9</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msqrt><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><mi>\u03b2</mi><mo>\u02d9</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow></msqrt></mfrac></mstyle></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\nThis property has been exploited in \\cite{BahadoriKaleFanLiu2015} for functional data clustering under the subspace clustering framework. Different from the work proposed in \\cite{BahadoriKaleFanLiu2015}, we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see \\cite{FuGaoHongTien2015,WangHuGaoSunYin2015,YinGaoGuo2015}. To see this, we introduce some more notation.  Let $\\Gamma$ be the set of all diffeomorphisms from $D=[0,1]$ to $D=[0,1]$. This set collects all the reparametrization mappings. $\\Gamma$ is a Lie group with the composition as the group operation and the identity mapping as the identity element. Then all the orbits $[q] = \\{ q\\circ \\gamma = q(\\gamma(t)) \\;|\\; \\forall \\gamma\\in \\Gamma\\}$ together define the quotient manifold $L^2(D, \\mathbb{R}^n)/\\Gamma$. \n\nWithout loss of generality, all curves are normalized to have unit length, i.e., $\\int_{D}\\|\\dot{\\beta}(t)\\|dt = 1$. The SRVFs associated with these curves are elements of a unit hypersphere in the Hilbert space $L^2(D, \\mathbb{R}^n)$, i.e., $\\int_D \\| q(t) \\|^2 dt = 1$.  Therefore, under the curve normalization assumption, instead of $L^2(D, \\mathbb{R}^n)$, we consider the following unit  hypersphere manifold\n\n\n", "itemtype": "equation", "pos": 13221, "prevtext": "\nThe SRVF mapping transforms the original curve $\\beta(t)$ into a gradient based representation, which facilitates the comparing of the shape information. \n\nIn this paper, we focus on the set of open curves, e.g. the curves do not form a loop ($\\beta(0) \\neq \\beta(1)$). For handling general curves, we refer readers to  \\cite{SrivastavaShantanuJermyn2011}. The SRVF facilitates a measure and geometry bearing invariance to scaling, shifting and reparameterization in the curves domain. For example, all the translated curves from a curve $\\beta(t)$ will have the same SRVF.  Robinson \\cite{Robinson2012} proved that if the curve $\\beta(t)$ is absolutely continuous, then its SRVF $q(t)$ is square-integrable, i.e., $q(t)$ is in a functional Hilbert space $L^2(D, \\mathbb{R}^n)$ .  Conversely for each $q(t)\\in L^2(D, \\mathbb{R}^n)$, there exists a curve $\\beta(t)$ whose SRVF corresponds to $q(t)$. Thus the set $L^2(D, \\mathbb{R}^n)$ is a well-defined representation space of all the curves.  The most important advantage offered by the SRVF framework is that the natural and widely used $L^2$-measure on $L^2(D, \\mathbb{R}^n)$ is invariant to the reparameterization. That is, for any two SRVFs $q_1$ and $q_2$ and a randomly chosen reparametrization function (non-decreasing) $t = \\gamma(\\tau)$, we have \n", "index": 7, "text": "\n\\[\n\\|q_1(t) - q_2(t)\\|_{L^2} = \\|q_1(\\gamma(\\tau)) - q_2(\\gamma(\\tau))\\|_{L^2}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\|q_{1}(t)-q_{2}(t)\\|_{L^{2}}=\\|q_{1}(\\gamma(\\tau))-q_{2}(\\gamma(\\tau))\\|_{L^{%&#10;2}}.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>q</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><msup><mi>L</mi><mn>2</mn></msup></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>q</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><msup><mi>L</mi><mn>2</mn></msup></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\nThe manifold $\\mathcal C^o$ has some nice properties, see \\cite{AbsilMahonySepulchre2008}.  For any two points $q_0$ and $q_1$ in $\\mathcal C^o$, a\ngeodesic connecting them is given by $\\alpha: [0, 1] \\rightarrow \\mathcal C^o$,\n\n", "itemtype": "equation", "pos": 14538, "prevtext": "\n\nThis property has been exploited in \\cite{BahadoriKaleFanLiu2015} for functional data clustering under the subspace clustering framework. Different from the work proposed in \\cite{BahadoriKaleFanLiu2015}, we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see \\cite{FuGaoHongTien2015,WangHuGaoSunYin2015,YinGaoGuo2015}. To see this, we introduce some more notation.  Let $\\Gamma$ be the set of all diffeomorphisms from $D=[0,1]$ to $D=[0,1]$. This set collects all the reparametrization mappings. $\\Gamma$ is a Lie group with the composition as the group operation and the identity mapping as the identity element. Then all the orbits $[q] = \\{ q\\circ \\gamma = q(\\gamma(t)) \\;|\\; \\forall \\gamma\\in \\Gamma\\}$ together define the quotient manifold $L^2(D, \\mathbb{R}^n)/\\Gamma$. \n\nWithout loss of generality, all curves are normalized to have unit length, i.e., $\\int_{D}\\|\\dot{\\beta}(t)\\|dt = 1$. The SRVFs associated with these curves are elements of a unit hypersphere in the Hilbert space $L^2(D, \\mathbb{R}^n)$, i.e., $\\int_D \\| q(t) \\|^2 dt = 1$.  Therefore, under the curve normalization assumption, instead of $L^2(D, \\mathbb{R}^n)$, we consider the following unit  hypersphere manifold\n\n\n", "index": 9, "text": "\\begin{align*}\n\\mathcal C^o = \\bigg\\{ q \\in L^2(D, \\mathbb R^n): \\int_D \\| q(t) \\|^2 dt = 1 \\bigg\\}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{C}^{o}=\\bigg{\\{}q\\in L^{2}(D,\\mathbb{R}^{n}):\\int_{D}\\|q%&#10;(t)\\|^{2}dt=1\\bigg{\\}}.\" display=\"inline\"><mrow><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mi>o</mi></msup><mo>=</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">{</mo><mrow><mi>q</mi><mo>\u2208</mo><mrow><msup><mi>L</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo>,</mo><msup><mi>\u211d</mi><mi>n</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>D</mi></msub></mstyle><mrow><msup><mrow><mo>\u2225</mo><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo maxsize=\"210%\" minsize=\"210%\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\theta = \\cos^{-1}(\\langle q_0 , q_1 \\rangle)$ is the length of the geodesic. If we take derivative of $\\alpha$ w.r.t to $q_1$, the tangent vector\nat $q_0$ is\n\n", "itemtype": "equation", "pos": 14881, "prevtext": "\n\nThe manifold $\\mathcal C^o$ has some nice properties, see \\cite{AbsilMahonySepulchre2008}.  For any two points $q_0$ and $q_1$ in $\\mathcal C^o$, a\ngeodesic connecting them is given by $\\alpha: [0, 1] \\rightarrow \\mathcal C^o$,\n\n", "index": 11, "text": "\\begin{align}\n\\alpha (\\tau)  = \\frac1{\\sin(\\theta)} (\\sin(\\theta(1 -\\tau))q_0 + \\sin(\\theta\\tau)q_1), \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\alpha(\\tau)=\\frac{1}{\\sin(\\theta)}(\\sin(\\theta(1-\\tau))q_{0}+%&#10;\\sin(\\theta\\tau)q_{1}),\" display=\"inline\"><mrow><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msub><mi>q</mi><mn>0</mn></msub></mrow><mo>+</mo><mrow><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msub><mi>q</mi><mn>1</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": " \nThe above formula is regarded as the {\\t Logarithm} mapping $\\log_{q_0} (q_1)$ on the manifold $\\mathcal C^o$.\n\nAs we are concerned with the shape invariance, i.e., we need to additionally remove the shape-preserving transformations: rotation and curve reparametrization. The manifold concerning us is the quotient space of the manifold $\\mathcal S^o = \\mathcal C^o/(SO(n) \\times \\Gamma)$, where $SO(n)$ is the rotation group. Each element $[q]\\in \\mathcal{S}^o$ is an equivalent class defined by \n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $\\theta = \\cos^{-1}(\\langle q_0 , q_1 \\rangle)$ is the length of the geodesic. If we take derivative of $\\alpha$ w.r.t to $q_1$, the tangent vector\nat $q_0$ is\n\n", "index": 13, "text": "\\begin{align}\nv = \\frac{\\theta}{\\sin(\\theta)}[q_1 - \\langle q_0 , q_1 \\rangle q_0]. \\label{Tangent1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle v=\\frac{\\theta}{\\sin(\\theta)}[q_{1}-\\langle q_{0},q_{1}\\rangle q%&#10;_{0}].\" display=\"inline\"><mrow><mrow><mi>v</mi><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b8</mi><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>q</mi><mn>1</mn></msub><mo>-</mo><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>q</mi><mn>0</mn></msub><mo>,</mo><msub><mi>q</mi><mn>1</mn></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msub><mi>q</mi><mn>0</mn></msub></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\nGiven any two points $[q_0]$ and $[q_1]$ in $\\mathcal S^o$,  a  tangent representative \\cite{AbsilMahonySepulchre2008} in the tangent space $T_{[q_0]} (\\mathcal S^o)$ can be calculated in the following way, as suggested in \\cite{ZhangSuKlassenLeSrivastava2015,SuSrivastava2014} based on \\eqref{Tangent1},\n\n", "itemtype": "equation", "pos": 15773, "prevtext": " \nThe above formula is regarded as the {\\t Logarithm} mapping $\\log_{q_0} (q_1)$ on the manifold $\\mathcal C^o$.\n\nAs we are concerned with the shape invariance, i.e., we need to additionally remove the shape-preserving transformations: rotation and curve reparametrization. The manifold concerning us is the quotient space of the manifold $\\mathcal S^o = \\mathcal C^o/(SO(n) \\times \\Gamma)$, where $SO(n)$ is the rotation group. Each element $[q]\\in \\mathcal{S}^o$ is an equivalent class defined by \n", "index": 15, "text": "\n\\[\n[q] = \\left\\{O q(\\gamma(t))\\sqrt{\\dot{\\gamma}(t)}\\; |\\; O\\in SO(n) \\text{ and } \\gamma \\in\\Gamma \\right\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"[q]=\\left\\{Oq(\\gamma(t))\\sqrt{\\dot{\\gamma}(t)}\\;|\\;O\\in SO(n)\\text{ and }%&#10;\\gamma\\in\\Gamma\\right\\}.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">[</mo><mi>q</mi><mo stretchy=\"false\">]</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mi>O</mi><mo>\u2062</mo><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+2.8pt\"><msqrt><mrow><mover accent=\"true\"><mi>\u03b3</mi><mo>\u02d9</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mpadded></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">|</mo><mrow><mi>O</mi><mo>\u2208</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo>\u2208</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\widetilde{q}_1$ is the representative of $[q_1]$ given by the well-defined algorithm in \\cite{ZhangSuKlassenLeSrivastava2015,SuSrivastava2014} and $\\widetilde{\\theta} = \\cos^{-1}(\\langle q_0 , \\widetilde{q}_1 \\rangle)$. In fact, $\\widetilde{v}$ is the lifting representation of abstract tangent vector $\\log_{[q_0]}([q_1])$ on $T_{[q_0]}(\\mathcal{S}^o)$ at $q_1$.\n\n\\subsection{The Proposed Curve LRR}\nGiven a set of $N$ unit-length curves $\\{\\beta_1(t), ..., \\beta_N(t)\\}$, denote their SRVFs by $\\{q_1(t), ..., q_N(t)\\}$ such that $[q_i]\\in \\mathcal S^o$ and $q_i(t)$ is a representative of the equivalent class $[q_i]$. We cannot apply the standard LRR model \\eqref{(2)} directly on the quotient manifold $\\mathcal S^o$. This is because \\eqref{(2)} indeed relies on the following individual linear combination\n\n", "itemtype": "equation", "pos": 16193, "prevtext": "\n\nGiven any two points $[q_0]$ and $[q_1]$ in $\\mathcal S^o$,  a  tangent representative \\cite{AbsilMahonySepulchre2008} in the tangent space $T_{[q_0]} (\\mathcal S^o)$ can be calculated in the following way, as suggested in \\cite{ZhangSuKlassenLeSrivastava2015,SuSrivastava2014} based on \\eqref{Tangent1},\n\n", "index": 17, "text": "\\begin{align}\n\\widetilde{v} = \\log_{q_0} (\\widetilde{q}_1) = \\frac{\\widetilde{\\theta}}{\\sin(\\widetilde{\\theta})}[\\widetilde{q}_1 - \\langle q_0 , \\widetilde{q}_1 \\rangle q_0]. \\label{Tangent2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widetilde{v}=\\log_{q_{0}}(\\widetilde{q}_{1})=\\frac{\\widetilde{%&#10;\\theta}}{\\sin(\\widetilde{\\theta})}[\\widetilde{q}_{1}-\\langle q_{0},\\widetilde{%&#10;q}_{1}\\rangle q_{0}].\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>v</mi><mo>~</mo></mover><mo>=</mo><mrow><msub><mi>log</mi><msub><mi>q</mi><mn>0</mn></msub></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>q</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mover accent=\"true\"><mi>\u03b8</mi><mo>~</mo></mover><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03b8</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mover accent=\"true\"><mi>q</mi><mo>~</mo></mover><mn>1</mn></msub><mo>-</mo><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>q</mi><mn>0</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>q</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msub><mi>q</mi><mn>0</mn></msub></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhich is invalid for $[q_i]$'s on $\\mathcal S^o$.   Note that $z_{ij}$ can be explained as the affinity or similarity between data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n\nOn any manifold, the tangent space at a given point is linearly local approximation to the manifold around the point and the linear combination is valid in the tangent space. This prompts us to replace the affinity relation in \\eqref{linear} by the following \n\n", "itemtype": "equation", "pos": 17217, "prevtext": "\nwhere $\\widetilde{q}_1$ is the representative of $[q_1]$ given by the well-defined algorithm in \\cite{ZhangSuKlassenLeSrivastava2015,SuSrivastava2014} and $\\widetilde{\\theta} = \\cos^{-1}(\\langle q_0 , \\widetilde{q}_1 \\rangle)$. In fact, $\\widetilde{v}$ is the lifting representation of abstract tangent vector $\\log_{[q_0]}([q_1])$ on $T_{[q_0]}(\\mathcal{S}^o)$ at $q_1$.\n\n\\subsection{The Proposed Curve LRR}\nGiven a set of $N$ unit-length curves $\\{\\beta_1(t), ..., \\beta_N(t)\\}$, denote their SRVFs by $\\{q_1(t), ..., q_N(t)\\}$ such that $[q_i]\\in \\mathcal S^o$ and $q_i(t)$ is a representative of the equivalent class $[q_i]$. We cannot apply the standard LRR model \\eqref{(2)} directly on the quotient manifold $\\mathcal S^o$. This is because \\eqref{(2)} indeed relies on the following individual linear combination\n\n", "index": 19, "text": "\\begin{align}\n\\mathbf x_i = \\sum^N_{j=1} z_{ij} \\mathbf x_j + \\mathbf e_i, \\label{linear}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{x}_{i}=\\sum^{N}_{j=1}z_{ij}\\mathbf{x}_{j}+\\mathbf{e}_{i},\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow></mrow><mo>+</mo><msub><mi>\ud835\udc1e</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwith the constraint $\\sum_{j=1}^N w_{ij} = 1, i = 1, 2, \\dots, N$ to maintain consistency at different locations. The meaning of $w_{ij}$ in \\eqref{tangentLinear} is the similarity between curves $\\beta_i(t)$ and $\\beta_j(t)$ via the ``affinity'' between tangent vectors $\\log_{[q_i]}([q_i]) $ and $\\log_{[q_i]}([q_j])$ at the first order approximation accuracy.  Each $\\log_{[q_i]}([q_j])$ can be calculated by \\eqref{Tangent2} and it is obvious that $\\log_{[q_i]}([q_i])=0$ for any $i$.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith all the ingredients at hand, we are fully equipped to propose the curve LRR (cLRR) model as follows \n\n", "itemtype": "equation", "pos": 17753, "prevtext": "\nwhich is invalid for $[q_i]$'s on $\\mathcal S^o$.   Note that $z_{ij}$ can be explained as the affinity or similarity between data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n\nOn any manifold, the tangent space at a given point is linearly local approximation to the manifold around the point and the linear combination is valid in the tangent space. This prompts us to replace the affinity relation in \\eqref{linear} by the following \n\n", "index": 21, "text": "\\begin{align}\n\\log_{[q_i]}([q_i]) = \\sum_{j=1}^N w_{ij} \\log_{[q_i]} ([q_j]) + \\mathbf{e}_i\\label{tangentLinear}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\log_{[q_{i}]}([q_{i}])=\\sum_{j=1}^{N}w_{ij}\\log_{[q_{i}]}([q_{j}%&#10;])+\\mathbf{e}_{i}\" display=\"inline\"><mrow><mrow><msub><mi>log</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><msub><mi>log</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>j</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><msub><mi>\ud835\udc1e</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\|\\cdot\\|_{[q]}$ is the metric defined on the manifold, which is defined by the classic $L^2$ Hilbert metric on the tangent space.  \n\nDenote $\\mathbf w_i$ the $i$-th row of matrix $\\mathbf W$ and define\n\n", "itemtype": "equation", "pos": 18492, "prevtext": "\nwith the constraint $\\sum_{j=1}^N w_{ij} = 1, i = 1, 2, \\dots, N$ to maintain consistency at different locations. The meaning of $w_{ij}$ in \\eqref{tangentLinear} is the similarity between curves $\\beta_i(t)$ and $\\beta_j(t)$ via the ``affinity'' between tangent vectors $\\log_{[q_i]}([q_i]) $ and $\\log_{[q_i]}([q_j])$ at the first order approximation accuracy.  Each $\\log_{[q_i]}([q_j])$ can be calculated by \\eqref{Tangent2} and it is obvious that $\\log_{[q_i]}([q_i])=0$ for any $i$.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith all the ingredients at hand, we are fully equipped to propose the curve LRR (cLRR) model as follows \n\n", "index": 23, "text": "\\begin{align}\n\\begin{aligned}\n\\min_{\\mathbf W} \\lambda \\| \\mathbf W \\|_* + \\sum_{i = 1}^N \\frac{1}{2} \\| \\sum_{j=1}^N w_{ij} \\log_{[q_i]} ([q_j]) \\|_{[q_i]}^2, \\\\\n\\textrm{s.t.} \\; \\sum_{j=1}^N w_{ij} = 1, i = 1, 2, \\dots, N.\n\\end{aligned}\\label{Model}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{aligned} \\displaystyle\\min_{\\mathbf{W}}\\lambda\\|\\mathbf{W}%&#10;\\|_{*}+\\sum_{i=1}^{N}\\frac{1}{2}\\|\\sum_{j=1}^{N}w_{ij}\\log_{[q_{i}]}([q_{j}])%&#10;\\|_{[q_{i}]}^{2},\\\\&#10;\\displaystyle\\textrm{s.t.}\\;\\sum_{j=1}^{N}w_{ij}=1,i=1,2,\\dots,N.\\end{aligned}\" display=\"inline\"><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2061</mo><mi>\u03bb</mi></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><msub><mi>log</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>j</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><mrow><mpadded width=\"+2.8pt\"><mtext>s.t.</mtext></mpadded><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nThen with some algebraic manipulation we can re-write the model \\eqref{Model} into the following simplified form, \n\n", "itemtype": "equation", "pos": 18966, "prevtext": "\nwhere $\\|\\cdot\\|_{[q]}$ is the metric defined on the manifold, which is defined by the classic $L^2$ Hilbert metric on the tangent space.  \n\nDenote $\\mathbf w_i$ the $i$-th row of matrix $\\mathbf W$ and define\n\n", "index": 25, "text": "\\begin{align} \n\\label{b_create}\nB^i_{jk} = \\langle \\log_{[q_i]} ([q_j]), \\log_{[q_i]} ([q_k]) \\rangle.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle B^{i}_{jk}=\\langle\\log_{[q_{i}]}([q_{j}]),\\log_{[q_{i}]}([q_{k}]%&#10;)\\rangle.\" display=\"inline\"><mrow><mrow><msubsup><mi>B</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow><mi>i</mi></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msub><mi>log</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>j</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>log</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>q</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\mathbf B^i  = (B^i_{jk})$.\n \nEffectively this objective allows for similarity between curves to be measured in their tangent spaces. Our highly accurate segmentation results in Section \\ref{Sec:4} have demonstrated that this is an effective way to learn  non-linear similarity.\n\n\\section{Optimisation}\\label{Sec:3}\n\\subsection{Algorithm}\nTo solve the cLRR objective we use the Linearized Alternative Direction Method with Adaptive Penalty (LADMAP) \\cite{LinLiuLi2015,LinLiuSu2011} . First take the Augmented Lagrangian of the objective \\eqref{obj_curve}\n\n", "itemtype": "equation", "pos": 19196, "prevtext": "\nThen with some algebraic manipulation we can re-write the model \\eqref{Model} into the following simplified form, \n\n", "index": 27, "text": "\\begin{align}\n\\begin{aligned}\n\\min_{\\mathbf W} \\lambda \\| \\mathbf W \\|_* + \\sum_{i = 1}^N \\mathbf w_i \\mathbf B^i \\mathbf w_i^T, \\\\\n\\textrm{s.t.} \\; \\sum_{j=1}^N w_{ij} = 1, i = 1, 2, \\dots, N.\n\\end{aligned}\\label{obj_curve}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{aligned} \\displaystyle\\min_{\\mathbf{W}}\\lambda\\|\\mathbf{W}%&#10;\\|_{*}+\\sum_{i=1}^{N}\\mathbf{w}_{i}\\mathbf{B}^{i}\\mathbf{w}_{i}^{T},\\\\&#10;\\displaystyle\\textrm{s.t.}\\;\\sum_{j=1}^{N}w_{ij}=1,i=1,2,\\dots,N.\\end{aligned}\" display=\"inline\"><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2061</mo><mi>\u03bb</mi></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>\u2062</mo><msup><mi>\ud835\udc01</mi><mi>i</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc30</mi><mi>i</mi><mi>T</mi></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><mrow><mpadded width=\"+2.8pt\"><mtext>s.t.</mtext></mpadded><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\mathbf y$ is the Lagrangian multiplier (vector) corresponding to the equality constraint $\\mathbf W\\mathbf 1 = \\mathbf 1$, $\\|\\cdot\\|_{F}$ is the matrix Frobebius-norm, and we will update $\\beta$ as well in the iterative algorithm to be introduced.\n\nDenote by $F(\\mathbf W)$ the function defined by \\eqref{25August2014-5} except for the first term $\\lambda \\|\\mathbf W\\|_*$. To solve \\eqref{25August2014-5}, we adopt a linearization of $F(\\mathbf{W})$ at the current location $\\mathbf W^{(k)}$ in the iteration process, that is, we approximate $F(\\mathbf W)$ by the following linearization with a proximal term\n\n", "itemtype": "equation", "pos": 19995, "prevtext": "\nwhere $\\mathbf B^i  = (B^i_{jk})$.\n \nEffectively this objective allows for similarity between curves to be measured in their tangent spaces. Our highly accurate segmentation results in Section \\ref{Sec:4} have demonstrated that this is an effective way to learn  non-linear similarity.\n\n\\section{Optimisation}\\label{Sec:3}\n\\subsection{Algorithm}\nTo solve the cLRR objective we use the Linearized Alternative Direction Method with Adaptive Penalty (LADMAP) \\cite{LinLiuLi2015,LinLiuSu2011} . First take the Augmented Lagrangian of the objective \\eqref{obj_curve}\n\n", "index": 29, "text": "\\begin{equation}\n\\begin{aligned}\nL = &\\lambda \\|\\mathbf W\\|_*  + \\frac12\\sum^N_{i=1}\\mathbf w_i \\mathbf{B}^i\\mathbf w^T_i + \\langle {\\mathbf y}, \\mathbf W\\mathbf 1 - \\mathbf 1\\rangle \\\\\n&+ \\frac{\\beta}2\\|\\mathbf W\\mathbf 1 - \\mathbf 1\\|^2_F\n\\end{aligned}\\label{25August2014-5}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle L=\" display=\"inline\"><mrow><mi>L</mi><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lambda\\|\\mathbf{W}\\|_{*}+\\frac{1}{2}\\sum^{N}_{i=1}\\mathbf{w}_{i}%&#10;\\mathbf{B}^{i}\\mathbf{w}^{T}_{i}+\\langle{\\mathbf{y}},\\mathbf{W}\\mathbf{1}-%&#10;\\mathbf{1}\\rangle\" display=\"inline\"><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>\u2062</mo><msup><mi>\ud835\udc01</mi><mi>i</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc30</mi><mi>i</mi><mi>T</mi></msubsup></mrow></mrow></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\ud835\udc32</mi><mo>,</mo><mrow><mi>\ud835\udc16\ud835\udfcf</mi><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{\\beta}{2}\\|\\mathbf{W}\\mathbf{1}-\\mathbf{1}\\|^{2}_{F}\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc16\ud835\udfcf</mi><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $\\eta_W$ is an approximate constant with a suggested value given by $\\eta_W = \\max\\{\\|B_i\\|^2\\}+N+1$, and $\\partial F(\\mathbf W^{(k)})$ is a gradient matrix of $F(\\mathbf W)$ at $\\mathbf W^{(k)}$. Denote by $\\mathbf B$ the 3-order tensor whose $i$-th front slice is given by $\\mathbf B^i$. Let us define $\\mathbf W\\odot \\mathbf B$ the matrix whose $i$-row is given by $\\mathbf w_i \\mathbf B^i$, then it is easy to show\n\n", "itemtype": "equation", "pos": 20906, "prevtext": "\nwhere $\\mathbf y$ is the Lagrangian multiplier (vector) corresponding to the equality constraint $\\mathbf W\\mathbf 1 = \\mathbf 1$, $\\|\\cdot\\|_{F}$ is the matrix Frobebius-norm, and we will update $\\beta$ as well in the iterative algorithm to be introduced.\n\nDenote by $F(\\mathbf W)$ the function defined by \\eqref{25August2014-5} except for the first term $\\lambda \\|\\mathbf W\\|_*$. To solve \\eqref{25August2014-5}, we adopt a linearization of $F(\\mathbf{W})$ at the current location $\\mathbf W^{(k)}$ in the iteration process, that is, we approximate $F(\\mathbf W)$ by the following linearization with a proximal term\n\n", "index": 31, "text": "\\begin{align*}\nF(\\mathbf W)\\approx & F(\\mathbf W^{(k)}) + \\langle \\partial F(\\mathbf W^{(k)}), \\mathbf W-\\mathbf W^{(k)}\\rangle \\\\\n&+ \\frac{\\eta_{W}\\beta_k}2\\|\\mathbf W-\\mathbf W^{(k)}\\|^2_F,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle F(\\mathbf{W})\\approx\" display=\"inline\"><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc16</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle F(\\mathbf{W}^{(k)})+\\langle\\partial F(\\mathbf{W}^{(k)}),\\mathbf{%&#10;W}-\\mathbf{W}^{(k)}\\rangle\" display=\"inline\"><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>F</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>\ud835\udc16</mi><mo>-</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{\\eta_{W}\\beta_{k}}{2}\\|\\mathbf{W}-\\mathbf{W}^{(k)}\\|^{2}_{%&#10;F},\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03b7</mi><mi>W</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc16</mi><mo>-</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\n\nThen \\eqref{25August2014-5} can be approximated by linearization and $\\mathbf w$ will be updated by the following\n\n", "itemtype": "equation", "pos": 21536, "prevtext": "\nwhere $\\eta_W$ is an approximate constant with a suggested value given by $\\eta_W = \\max\\{\\|B_i\\|^2\\}+N+1$, and $\\partial F(\\mathbf W^{(k)})$ is a gradient matrix of $F(\\mathbf W)$ at $\\mathbf W^{(k)}$. Denote by $\\mathbf B$ the 3-order tensor whose $i$-th front slice is given by $\\mathbf B^i$. Let us define $\\mathbf W\\odot \\mathbf B$ the matrix whose $i$-row is given by $\\mathbf w_i \\mathbf B^i$, then it is easy to show\n\n", "index": 33, "text": "\\begin{equation} \n\\partial F(\\mathbf W^{(k)}) = \\mathbf W\\odot \\mathbf B + \\mathbf y\\mathbf 1^T + \\beta_k (\\mathbf W\\mathbf 1 - \\mathbf 1)\\mathbf 1^T.\n\\label{Eq:14October2014-4}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\partial F(\\mathbf{W}^{(k)})=\\mathbf{W}\\odot\\mathbf{B}+\\mathbf{y}\\mathbf{1}^{T%&#10;}+\\beta_{k}(\\mathbf{W}\\mathbf{1}-\\mathbf{1})\\mathbf{1}^{T}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>F</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\ud835\udc16</mi><mo>\u2299</mo><mi>\ud835\udc01</mi></mrow><mo>+</mo><msup><mi>\ud835\udc32\ud835\udfcf</mi><mi>T</mi></msup><mo>+</mo><mrow><msub><mi>\u03b2</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc16\ud835\udfcf</mi><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mn>\ud835\udfcf</mn><mi>T</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\\begin{algorithm}[]\n\\caption{{\\bf Solving \\eqref{obj_curve} by LADMAP}}\n\\label{curve_lrr_alg}\n\\begin{algorithmic}[1]\n\n\\REQUIRE $\\{\\mathbf X_i\\}_{i=1}^N$, $\\lambda$\n\n\\STATE Initialise: $\\mathbf W = \\mathbf 0$, $\\mathbf y = \\mathbf 0$, $\\beta = 0.1$, $\\beta_{\\text{max}} = 10$, $\\rho^0 = 1.1$, $\\eta = \\max \\{ \\| \\mathbf B^i \\|_F \\} + N + 1$, $\\epsilon_1 = 1e^{-4}$, $\\epsilon_2 = 1e^{-4}$\n\n\\STATE Construct each $\\mathbf B^i$ as per \\eqref{b_create}\n\n\\WHILE{not converged}\n\n\\STATE Update $\\mathbf W$ using \\eqref{SolutionWk}\n\n\\STATE Check convergence criteria\n\n", "itemtype": "equation", "pos": 21846, "prevtext": "\n\n\n\nThen \\eqref{25August2014-5} can be approximated by linearization and $\\mathbf w$ will be updated by the following\n\n", "index": 35, "text": "\\begin{align}\n \\mathbf W^{(k+1)}   \n= &\\arg\\min_{\\mathbf W} \\lambda \\|\\mathbf W\\|_* \\label{SolutionW}\\\\\n+&\\frac{\\eta_W\\beta_k}{2} \\bigg\\|\\mathbf W - \\left(\\mathbf W^{(k)} - \\frac{1}{\\eta_W\\beta_k} \\partial F(\\mathbf W^{(k)})\\right)\\bigg\\|^2_F. \\notag\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{W}^{(k+1)}=\" display=\"inline\"><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\arg\\min_{\\mathbf{W}}\\lambda\\|\\mathbf{W}\\|_{*}\" display=\"inline\"><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2061</mo><mi>\u03bb</mi></mrow></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\" display=\"inline\"><mo>+</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\eta_{W}\\beta_{k}}{2}\\bigg{\\|}\\mathbf{W}-\\left(\\mathbf{W}^{%&#10;(k)}-\\frac{1}{\\eta_{W}\\beta_{k}}\\partial F(\\mathbf{W}^{(k)})\\right)\\bigg{\\|}^{%&#10;2}_{F}.\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03b7</mi><mi>W</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo maxsize=\"210%\" minsize=\"210%\">\u2225</mo><mrow><mi>\ud835\udc16</mi><mo>-</mo><mrow><mo>(</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mi>\u03b7</mi><mi>W</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>F</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\\STATE Update Lagrangian Multiplier\n\n", "itemtype": "equation", "pos": 22669, "prevtext": "\n\n\\begin{algorithm}[]\n\\caption{{\\bf Solving \\eqref{obj_curve} by LADMAP}}\n\\label{curve_lrr_alg}\n\\begin{algorithmic}[1]\n\n\\REQUIRE $\\{\\mathbf X_i\\}_{i=1}^N$, $\\lambda$\n\n\\STATE Initialise: $\\mathbf W = \\mathbf 0$, $\\mathbf y = \\mathbf 0$, $\\beta = 0.1$, $\\beta_{\\text{max}} = 10$, $\\rho^0 = 1.1$, $\\eta = \\max \\{ \\| \\mathbf B^i \\|_F \\} + N + 1$, $\\epsilon_1 = 1e^{-4}$, $\\epsilon_2 = 1e^{-4}$\n\n\\STATE Construct each $\\mathbf B^i$ as per \\eqref{b_create}\n\n\\WHILE{not converged}\n\n\\STATE Update $\\mathbf W$ using \\eqref{SolutionWk}\n\n\\STATE Check convergence criteria\n\n", "index": 37, "text": "\\begin{align*}\n\\beta^{(k)} \\| \\mathbf W^{(k+1)} - \\mathbf W^{(k)} \\|_F \\leq \\epsilon_1\\\\\n\\| \\mathbf W \\mathbf 1 - \\mathbf 1 \\|_F \\leq \\epsilon_2 \\\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\beta^{(k)}\\|\\mathbf{W}^{(k+1)}-\\mathbf{W}^{(k)}\\|_{F}\\leq%&#10;\\epsilon_{1}\" display=\"inline\"><mrow><mrow><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>-</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>\u2264</mo><msub><mi>\u03f5</mi><mn>1</mn></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mathbf{W}\\mathbf{1}-\\mathbf{1}\\|_{F}\\leq\\epsilon_{2}\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc16\ud835\udfcf</mi><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub><mo>\u2264</mo><msub><mi>\u03f5</mi><mn>2</mn></msub></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\\STATE Update $\\rho$\n\n", "itemtype": "equation", "pos": 22866, "prevtext": "\n\n\\STATE Update Lagrangian Multiplier\n\n", "index": 39, "text": "\\begin{align*}\n\\mathbf y^{(k+1)} = \\mathbf y^k + \\beta^{(k)} ( \\mathbf W \\mathbf 1 - \\mathbf 1 )^T\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{y}^{(k+1)}=\\mathbf{y}^{k}+\\beta^{(k)}(\\mathbf{W}\\mathbf{1%&#10;}-\\mathbf{1})^{T}\" display=\"inline\"><mrow><msup><mi>\ud835\udc32</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>\ud835\udc32</mi><mi>k</mi></msup><mo>+</mo><mrow><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc16\ud835\udfcf</mi><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\\STATE Update $\\beta$\n\n", "itemtype": "equation", "pos": 23000, "prevtext": "\n\n\\STATE Update $\\rho$\n\n", "index": 41, "text": "\\begin{align*}\n\\rho = \n\\begin{cases}\n\\rho_0 & \\text{if} \\;\\; \\beta^{(k)} \\| \\mathbf W^{(k+1)} - \\mathbf W^{(k)} \\|_F \\leq \\epsilon_1 \\\\\n1 & \\text{otherwise,}\n\\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\rho=\\begin{cases}\\rho_{0}&amp;\\text{if}\\;\\;\\beta^{(k)}\\|\\mathbf{W}^{%&#10;(k+1)}-\\mathbf{W}^{(k)}\\|_{F}\\leq\\epsilon_{1}\\\\&#10;1&amp;\\text{otherwise,}\\end{cases}\" display=\"inline\"><mrow><mi>\u03c1</mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>\u03c1</mi><mn>0</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+5.6pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>-</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>\u2264</mo><msub><mi>\u03f5</mi><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n \n\\ENDWHILE\n\n\\RETURN $\\mathbf W$\n\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{figure*}[!th]\n\\centering\n\t\\subfloat[Cluster 1]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_1}}\n\t\\subfloat[Cluster 2]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_2}}\n\t\\subfloat[Cluster 3]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_3}}\n\t\n\\caption{Example plots of curves generated in the Synthetic Data Experiment. Each cluster has a base sine curve (the left most blue curve) which is progressively warped with each successive instantiation.}\n\\label{fig_syn_data}\n\\end{figure*}\n\nProblem \\eqref{SolutionW} admits a closed form solution by using SVD thresholding operator \\cite{CaiCandesShen2008}, given by\n\n", "itemtype": "equation", "pos": 23206, "prevtext": "\n\n\\STATE Update $\\beta$\n\n", "index": 43, "text": "\\begin{align*}\n\\beta^{(k+1)} = \\textrm{min}(\\beta_{\\textrm{max}}, \\rho \\beta^{(k)})\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\beta^{(k+1)}=\\textrm{min}(\\beta_{\\textrm{max}},\\rho\\beta^{(k)})\" display=\"inline\"><mrow><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><mtext>min</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b2</mi><mtext>max</mtext></msub><mo>,</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere $U_{W}\\Sigma_{W}V_{W}^{T}$ is the SVD of $\\mathbf W^{(k)} - \\frac{1}{\\eta_W\\beta_k} \\partial F(\\mathbf W^{(k)})$ and $S_\\tau(\\cdot)$ is the Singular Value Thresholding (SVT) \\cite{CaiCandesShen2008,parikh2013proximal} operator defined by\n\n", "itemtype": "equation", "pos": 24009, "prevtext": "\n \n\\ENDWHILE\n\n\\RETURN $\\mathbf W$\n\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{figure*}[!th]\n\\centering\n\t\\subfloat[Cluster 1]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_1}}\n\t\\subfloat[Cluster 2]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_2}}\n\t\\subfloat[Cluster 3]{\n\t\\includegraphics[width=0.3\\linewidth]{syn_cluster_3}}\n\t\n\\caption{Example plots of curves generated in the Synthetic Data Experiment. Each cluster has a base sine curve (the left most blue curve) which is progressively warped with each successive instantiation.}\n\\label{fig_syn_data}\n\\end{figure*}\n\nProblem \\eqref{SolutionW} admits a closed form solution by using SVD thresholding operator \\cite{CaiCandesShen2008}, given by\n\n", "index": 45, "text": "\\begin{equation}\\label{SolutionWk}\n\\begin{aligned}\n\\mathbf W^{(k+1)} = U_{W} S_{\\frac{\\lambda}{\\eta_W\\beta_k}}(\\Sigma_{W})V_{W}^{T},\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbf{W}^{(k+1)}=U_{W}S_{\\frac{\\lambda}{\\eta_{W}\\beta_{k}}}(%&#10;\\Sigma_{W})V_{W}^{T},\" display=\"inline\"><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msub><mi>U</mi><mi>W</mi></msub><mo>\u2062</mo><msub><mi>S</mi><mfrac><mi>\u03bb</mi><mrow><msub><mi>\u03b7</mi><mi>W</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow></mfrac></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>W</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>V</mi><mi>W</mi><mi>T</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\nThe updating rule for $\\mathbf y$\n\n", "itemtype": "equation", "pos": 24415, "prevtext": "\nwhere $U_{W}\\Sigma_{W}V_{W}^{T}$ is the SVD of $\\mathbf W^{(k)} - \\frac{1}{\\eta_W\\beta_k} \\partial F(\\mathbf W^{(k)})$ and $S_\\tau(\\cdot)$ is the Singular Value Thresholding (SVT) \\cite{CaiCandesShen2008,parikh2013proximal} operator defined by\n\n", "index": 47, "text": "\\begin{equation}\n\\begin{aligned}\nS_{\\tau}(\\Sigma) = \\text{diag}(\\max\\{|\\Sigma_{ii}|-\\tau, 0\\}).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\tau}(\\Sigma)=\\text{diag}(\\max\\{|\\Sigma_{ii}|-\\tau,0\\}).\" display=\"inline\"><mrow><mrow><mrow><msub><mi>S</mi><mi>\u03c4</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>diag</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03c4</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nand the updating rule for $\\beta_k$\n\n", "itemtype": "equation", "pos": 24575, "prevtext": "\n\nThe updating rule for $\\mathbf y$\n\n", "index": 49, "text": "\\begin{equation} \n\\mathbf y^{(k+1)} = \\mathbf y^{(k)} + \\beta_k (\\mathbf W^{(k)}\\mathbf 1 -\\mathbf 1) \n\\label{Eq:14October2014-5}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}^{(k+1)}=\\mathbf{y}^{(k)}+\\beta_{k}(\\mathbf{W}^{(k)}\\mathbf{1}-%&#10;\\mathbf{1})\" display=\"block\"><mrow><msup><mi>\ud835\udc32</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>\ud835\udc32</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>+</mo><mrow><msub><mi>\u03b2</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo>-</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 24756, "prevtext": "\nand the updating rule for $\\beta_k$\n\n", "index": 51, "text": "\\begin{equation}\n\\begin{aligned}\n\\beta_{k+1} = \\min\\{\\beta_{\\text{max}}, \\rho \\beta_k\\},\n\\end{aligned}\\label{UpdateBeta}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\beta_{k+1}=\\min\\{\\beta_{\\text{max}},\\rho\\beta_{k}\\},\" display=\"inline\"><mrow><mrow><msub><mi>\u03b2</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03b2</mi><mtext>max</mtext></msub><mo>,</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\nWe summarize the above as Algorithm~\\ref{curve_lrr_alg}.  Once the coefficient matrix $\\mathbf W$ is found, a spectral clustering like nCUT \\cite{ShiMalik2000} is applied on the affinity matrix $\\frac{|\\mathbf W|+|\\mathbf W|^T}{2}$ to obtain the segmentation of the data.\n\n\n\n\n\n\\subsection{Complexity Analysis}\nFor ease of analysis, we firstly define some symbols used in the following. Let $K$ and $r$ denote the total number of iterations and the lowest rank of the matrix $\\mathbf W$, respectively. The size of $\\mathbf W$ is $N\\times N$. The major computation cost of our proposed method contains two parts, calculating all $\\mathbf B^i$'s and updating $\\mathbf W$. In terms of the formula \\eqref{b_create} through \\eqref{Tangent1} and \\eqref{Tangent2}, the computational complexity of Log algorithm is $O(T^2)$ where $T$ is the number of terms in a discretized curves; therefore, the complexity of $B_{jk}^i$ is at most $O(T^2)$ and $\\mathbf B^i$'s computational complexity is $O(N^2T^2)$. Thus the total for all the $\\mathbf B^i$ is $O(N^3)$. In each iteration of the Algorithm, the singular value thresholding is adopted to update the low rank matrix $\\mathbf W$ whose complexity is $O(rN^2)$~\\cite{LiuLinYanSunYuMa2013}. Suppose the algorithm is terminated after $K$ iterations, the overall computational complexity is given by\n", "itemtype": "equation", "pos": 24899, "prevtext": "\nwhere\n\n", "index": 53, "text": "\\[\n \\rho = \\begin{cases} \\rho_0 & \\beta_k \\|\\mathbf W^{k+1} - \\mathbf W^k\\| \\leq \\varepsilon_1,\\\\\n 1 & \\text{otherwise}.\n \\end{cases}\n \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\rho=\\begin{cases}\\rho_{0}&amp;\\beta_{k}\\|\\mathbf{W}^{k+1}-\\mathbf{W}^{k}\\|\\leq%&#10;\\varepsilon_{1},\\\\&#10;1&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><mi>\u03c1</mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>\u03c1</mi><mn>0</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mi>\u03b2</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo>\u2225</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>-</mo><msup><mi>\ud835\udc16</mi><mi>k</mi></msup></mrow><mo>\u2225</mo></mrow></mrow><mo>\u2264</mo><msub><mi>\u03b5</mi><mn>1</mn></msub></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\n\n\n\\subsection{Convergence Analysis}\nAlgorithm~\\ref{curve_lrr_alg} is adopted from the algorithm proposed in~\\cite{LinLiuSu2011}. However due to the terms of $\\mathbf B^i$'s in the objective function~\\eqref{25August2014-5}, the convergence theorem proved in~\\cite{LinLiuSu2011} cannot be directly applied to this case as the linearization is implemented on both the augmented Lagrangian terms and the term involving $\\mathbf B^i$'s. Fortunately we can employ the revised approach, presented in \\cite{YinGaoLinShiGuo2015}, to prove the convergence for the algorithm. Without repeating all the details, we present the convergence theorem for Algorithm~\\ref{curve_lrr_alg} as follows.\n\\begin{theorem}[Convergence of Algorithm~\\ref{curve_lrr_alg}] If $\\eta_W\\geq \\max\\{\\|B_i\\|^2\\}+N+1$, $\\displaystyle\\sum^{+\\infty}_{k=1}\\beta^{-1}_k = +\\infty$, $\\displaystyle\\beta_{k+1}-\\beta_k > C_0 \\frac{\\sum_i \\|B_i\\|^2}{\\eta_W - \\max\\{\\|B_i\\|^2\\}-N}$, where $C_0$ is a given constant and $\\|\\cdot\\|$ is the matrix spectral norm, then the sequence $\\{W^{k}\\}$ generated by Algorithm~\\ref{curve_lrr_alg} converges to an optimal solution to problem~\\eqref{obj_curve}.\n\\end{theorem}\n\nIn all the experiments we have conducted, the algorithm converges very fast with $K<100$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Experiments}\\label{Sec:4}\nIn this section we show three sets of experiments to evaluate the newly proposed cLRR. The performance of the proposed method is compared with the same type of subspace clustering algorithm LRR \\cite{LiuLinYanSunYuMa2013}. To compare segmentation accuracy we use the subspace clustering accuracy (SCA) metric \\cite{ElhamifarVidal2013}, which is defined as\n\n", "itemtype": "equation", "pos": 26372, "prevtext": "\n\nWe summarize the above as Algorithm~\\ref{curve_lrr_alg}.  Once the coefficient matrix $\\mathbf W$ is found, a spectral clustering like nCUT \\cite{ShiMalik2000} is applied on the affinity matrix $\\frac{|\\mathbf W|+|\\mathbf W|^T}{2}$ to obtain the segmentation of the data.\n\n\n\n\n\n\\subsection{Complexity Analysis}\nFor ease of analysis, we firstly define some symbols used in the following. Let $K$ and $r$ denote the total number of iterations and the lowest rank of the matrix $\\mathbf W$, respectively. The size of $\\mathbf W$ is $N\\times N$. The major computation cost of our proposed method contains two parts, calculating all $\\mathbf B^i$'s and updating $\\mathbf W$. In terms of the formula \\eqref{b_create} through \\eqref{Tangent1} and \\eqref{Tangent2}, the computational complexity of Log algorithm is $O(T^2)$ where $T$ is the number of terms in a discretized curves; therefore, the complexity of $B_{jk}^i$ is at most $O(T^2)$ and $\\mathbf B^i$'s computational complexity is $O(N^2T^2)$. Thus the total for all the $\\mathbf B^i$ is $O(N^3)$. In each iteration of the Algorithm, the singular value thresholding is adopted to update the low rank matrix $\\mathbf W$ whose complexity is $O(rN^2)$~\\cite{LiuLinYanSunYuMa2013}. Suppose the algorithm is terminated after $K$ iterations, the overall computational complexity is given by\n", "index": 55, "text": "\n\\[\nO(N^3)+O(KrN^2)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"O(N^{3})+O(KrN^{2})\" display=\"block\"><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>3</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00732.tex", "nexttext": "\nTherefore a higher SCA $\\%$ means greater clustering accuracy.\n\nThe parameters used were fixed across all experiments with $\\lambda$ for LRR set at $1$ and $0.1$ for cLRR. A wide range of parameters were tested for each algorithm. Overall we found that the segmentation accuracy of LRR did not vary that much with changes in $\\lambda$.\n\n\\subsection{Synthetic Data}\n\n\n\n\\begin{figure}[!h]\n\\centering\n\t\\subfloat[LRR]{\n\t\\includegraphics[width=1\\linewidth]{syn_seg_lrr}}\\\\\n\t\\subfloat[Curve LRR]{\n\t\\includegraphics[width=1\\linewidth]{syn_seg_curve}}\n\t\n\\caption{The segmentation results from the data in Figure \\ref{fig_syn_data}.}\n\\label{fig_syn_clusters}\n\\end{figure}\n\n\\begin{table}[!h]\n\\centering\n\n\\begin{tabular}{c c c c c}\n\\hline\n & Mean & Median & Min & Max \\\\\n\\hline\n\nLRR\t\t\t& 80.4\\%\t\t& 83.33\\%\t\t& 60\\%\t\t& 91.67\\% \\\\\nCurveLRR\t& \\bf 96.77\\%\t\t& \\bf 98.33\\%\t\t& \\bf 73.33\\%\t\t& \\bf 100\\%\n\t\n\\end{tabular}\n\n\\caption{Synthetic Results}\n\\label{table_syn_results}\n\\end{table}\n\n\\begin{figure*}[htb]\n\\centering\n\t\\subfloat[Cluster 1]{\n\t\\includegraphics[width=0.24\\linewidth]{tir_cluster_1}}\n\t\\subfloat[Cluster 2]{\n\t\\includegraphics[width=0.24\\linewidth]{tir_cluster_2}}\n\t\\subfloat[Cluster 3]{\n\t\\includegraphics[width=0.24\\linewidth]{tir_cluster_3}}\n\t\\subfloat[Base Curves]{\n\t\\includegraphics[width=0.24\\linewidth]{tir_base_curves}}\n\t\n\\caption{Example plots of curves used in the Semi-synthetic TIR Data Experiment. Each cluster has a base curve from the TIR library. The curves for each cluster have been shifted and stretched randomly from the base.}\n\\label{fig_tir_data}\n\\end{figure*}\n\n\nTo evaluate and confirm the effectiveness of the curve LRR method we first perform experimental evaluation using synthetic data. In this test three clusters were created consisting of twenty 1-D curves of length 100. The curves in each cluster were sine waves, with each cluster corresponding to a unique frequency. Within each cluster progressive amounts of warping were applied. See Figure \\ref{fig_syn_data} for an example of data from three syntheticly generated clusters. Clustering was then performed on the data by applying curve LRR and segmenting the affinity matrix with nCUT. This experiment was repeated $50$ times with new data generated each time to obtain basic statistics. We compare against the baseline: LRR. Results are reported using subspace clustering accuracy and can be found in Table \\ref{table_syn_results}. Overall in this experiment Curve LRR outperforms conventional LRR by a significant margin.\n\n\\subsection{Semi-synthetic TIR Data}\n\n\n\n\nWe assemble synthetic data from a library of pure infrared hyper spectral mineral data. For each cluster we pick one spectra sample from the library as a basis. Each curve basis is then randomly shifted and stretched in a random portion. This random warping is performed $20$ times to produce the curves for each cluster. See Figure \\ref{fig_tir_data} for an example of data used in this experiment. In this experiment we used three clusters. Again as in the previous experiment we repeated the test $50$ times. Results are reported in Table \\ref{table_tir_results} and Figure \\ref{fig_tir_clusters}.\n\nThe results show that LRR cannot accurately cluster data with this sort of nonlinear invariance, which is commonly found in this type of data due to impurities in the mineral samples. On the other hand cLRR perfectly clustered the data.\n\n\\begin{figure}[H]\n\\centering\n\t\\subfloat[LRR]{\n\t\\includegraphics[width=1\\linewidth]{tir_seg_lrr}}\\\\\n\t\\subfloat[Curve LRR]{\n\t\\includegraphics[width=1\\linewidth]{tir_seg_curve}}\n\t\\caption{The segmentation results from the data in Figure \\ref{fig_tir_data}.}\n\\label{fig_tir_clusters}\n\\end{figure}\n\n\\begin{table}[!h]\n\\centering\n\n\\begin{tabular}{c c c c c}\n\\hline\n & Mean & Median & Min & Max \\\\\n\\hline\n\nLRR\t\t\t& 60.13\\%\t\t& 60\\%\t\t& 50\\%\t\t& 71.67\\% \\\\\nCurveLRR\t& \\bf 100\\%\t\t& \\bf 100\\%\t\t& \\bf 100\\%\t\t& \\bf 100\\%\n\t\n\\end{tabular}\n\n\\caption{Semi-Synthetic TIR Results}\n\\label{table_tir_results}\n\\end{table}\n\n\\subsection{Character Classification}\n\n\\begin{figure*} \n\\centering\n\t\\subfloat[Trajectories for ``a'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_plot_a}}\n\t\\subfloat[Trajectories for ``b'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_plot_b}}\n\t\\subfloat[Trajectories for ``c'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_plot_c}}\\\\\n\t\\subfloat[Reconstructed ``a'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_scatter_a}}\n\t\\subfloat[Reconstructed ``b'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_scatter_b}}\n\t\\subfloat[Reconstructed ``c'']{\n\t\\includegraphics[width=0.25\\linewidth]{char_scatter_c}}\n\\caption{Example data from the character classification dataset. The top row plots the x and y pen tip velocities over time for three sample characters. The bottom row shows the corresponding character reconstruction by integrating the pen tip velocity data (for visualisation only).}\n\\label{fig_char_examples}\n\\end{figure*}\n\n\\begin{figure*}\n\\centering\n\t\\subfloat[Cluster 1 - X]{\n\t\\includegraphics[width=0.27\\linewidth]{char_cluster_x_1}}\n\t\\subfloat[Cluster 2 - X]{\n\t\\includegraphics[width=0.27\\linewidth]{char_cluster_x_2}}\n\t\\subfloat[Cluster 3 - X]{\n\t\\includegraphics[width=0.27\\linewidth]{char_cluster_x_3}}\\\\\n\t\\subfloat[Cluster 1 - Y]{\n\t\\includegraphics[width=0.27\\linewidth]{char_cluster_y_1}}\n\t\\subfloat[Cluster 2 - Y]{\n\t\\includegraphics[width=0.27\\linewidth]{char_cluster_y_2}}\n\t\\subfloat[Cluster 3 - Y]{\n\t\\includegraphics[width=0.25\\linewidth]{char_cluster_y_3}}\n\\caption{Example plots of curves used in the Character Classification Experiment. Each cluster consists of randomly selected characters from each class that are then subject to a combination of shifting, warping, stretching or shrinking and scaling. The top row shows the curves from the pen tip velocity in the X direction over time and the bottom row shows the same but for the Y direction.}\n\\label{fig_char_data}\n\\end{figure*}\n\n\n\n\n\nIn this experiment a collection of handwritten English characters were used to evaluate performance on a real world data set. The dataset consists of pen position data collected by a digitisation tablet at 200Hz, which is then converted to horizontal and vertical velocities \\cite{williams2006extracting, williams2008modelling}. These 2-D trajectory curves are normalised such that the mean of each curve is close to zero. See Figure \\ref{fig_char_examples} for some examples of this data. Figure \\ref{fig_char_data} shows the example plots of curves used in the character classification experiment.\n\nTo evaluate performance twenty characters were randomly selected from three character classes. The data as originally released has been carefully produced and processed so that trajectories for each characters are extremely similar. Far more so than is realistic. For example the start time for each character has been aligned furthermore the writing speed, character size and variance in velocity over time is extremely consistent. Therefore to make the data more realistic we randomly globally shift each character so that their start times vary. Furthermore we randomly globally stretch and shrink each trajectory to account for different writing speeds, we also scale the trajectories by applying constant factors to account for character size and we lastly perform local warping (as done in the semi-synthetic experiment) to account for variance in speed over time. \n\nSince the data consists of multidimensional curves the X and Y trajectory curves were concatenated to form data usable for conventional LRR since it can only handle vectors. Results can be found in Table \\ref{table_char_results} and Figure \\ref{fig_char_clusters}. Once again, the cLRR clearly outperforms LRR in all metrics. Furthermore cLRR shows excellent performance with a median accuracy of over $90\\%$ on an extremely challenging dataset.\n\n\\begin{figure}[]\n\\centering\n\t\\subfloat[LRR]{\n\t\\includegraphics[width=1\\linewidth]{char_seg_lrr}}\\\\\n\t\\subfloat[Curve LRR]{\n\t\\includegraphics[width=1\\linewidth]{char_seg_curve}}\n\t\n\\caption{The segmentation results from the data in Figure \\ref{fig_char_data}.}\n\\label{fig_char_clusters}\n\\end{figure}\n\n\n\\begin{table}\n\\centering\n\n\\begin{tabular}{c c c c c}\n\\hline\n & Mean & Median & Min & Max \\\\\n\\hline\n\nLRR\t\t\t& 52.33\\%\t\t\t& 51.67\\%\t\t& 43.33\\%\t\t& 63.33\\% \\\\\nCurveLRR\t& \\bf 86.33\\%\t\t& \\bf 91.67\\%\t\t& \\bf 70\\%\t\t& \\bf 100\\%\n\t\n\\end{tabular}\n\\caption{Character Classification Results}\n\\label{table_char_results}\n\\end{table}\n\n\\section{Conclusion}\\label{Sec:5}\nIn this paper, we extended the conventional LRR model on Euclidean space to a new LRR model for the manifold of open curves. The new LRR formulation is based on the tangent space approximation to the manifold so that the classic data self expressive can be well preserved for the manifold of curves at relevant high accuracy.  The resulting optimization problem can be solved using the LADMAP technique and algorithm convergence and complexity were presented. Finally we tested the new model by conducting experiments on synthetic, semi-synthetic and real world data, and the experimental results show the outstanding performance against the conventional LRR. Our next work is further extended the LRR model to the manifold of general closed curves.  \n\n\\section*{Acknowledgments}\nFunding information hidden for the review process.\n\n{\\small\n\\bibliographystyle{ieee}\n\n\\begin{thebibliography}{10}\\itemsep=-1pt\n\n\\bibitem{AbsilMahonySepulchre2008}\nP.-A. Absil, R.~Mahony, and R.~Sepulchre.\n\\newblock {\\em Optimization algorithms on matrix manifolds}.\n\\newblock Princeton University Press, 2008.\n\n\\bibitem{BahadoriKaleFanLiu2015}\nM.~T. Bahadori, D.~Kale, Y.~Fan, and Y.~Liu.\n\\newblock Functional subspace clustering with application to time series.\n\\newblock In {\\em Proceedings of The 32nd International Conference on Machine\n  Learning}, pages 228--237, 2015.\n\n\\bibitem{Bishop2006}\nC.~Bishop.\n\\newblock {\\em Pattern Recognition and Machine Learning}.\n\\newblock Information Science and Statistics. Springer, 2006.\n\n\\bibitem{CaiCandesShen2008}\nJ.~F. Cai, E.~J. Cand\\`{e}s, and Z.~Shen.\n\\newblock A singular value thresholding algorithm for matrix completion.\n\\newblock {\\em SIAM J. on Optimization}, 20(4):1956--1982, 2008.\n\n\\bibitem{CandesLiMaWright2010}\nE.~J. Cand\\`{e}s, X.~Li, Y.~Ma, and J.~Wright.\n\\newblock Robust principal component analysis?\n\\newblock Submitted for publication, Stanford University, 2010.\n\\newblock \\url{http://www-stat.stanford.edu/~candes/papers/RobustPCA.pdf}.\n\n\\bibitem{ElhamifarVidal2013}\nE.~Elhamifar and R.~Vidal.\n\\newblock Sparse subspace clustering: {A}lgorithm, theory, and applications.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  35(11):2765--2781, 2013.\n\n\\bibitem{FerratyRomain2011}\nF.~Ferraty and Y.~Romain, editors.\n\\newblock {\\em The Oxford Handbook of Functional Data Analysis}.\n\\newblock Oxford University Press, 2011.\n\n\\bibitem{FuGaoHongTien2015}\nY.~Fu, J.~Gao, X.~Hong, and D.~Tien.\n\\newblock Low rank representation on {R}iemannian manifold of symmetrical\n  positive definite matrices.\n\\newblock In {\\em SIAM Conferences on Data Mining (SDM)}, pages 316--324, 2015.\n\n\\bibitem{JoshiKlassenSrivastavaJermyn2007}\nS.~H. Joshi, E.~Klassen, A.~Srivastava, and I.~Jermyn.\n\\newblock A novel representation for {R}iemannian analysis of elastic curves in\n  $r^n$.\n\\newblock In {\\em IEEE Conference on Computer Vision and Pattern Recognition},\n  pages 1--7, 2007.\n\n\\bibitem{LinLiuLi2015}\nZ.~Lin, R.~Liu, and H.~Li.\n\\newblock Linearized alternating direction method with parallel splitting and\n  adaptive penalty for separable convex programs in machine learning.\n\\newblock {\\em Machine Learning}, 99:287--325, 2015.\n\n\\bibitem{LinLiuSu2011}\nZ.~Lin, R.~Liu, and Z.~Su.\n\\newblock Linearized alternating direction method with adaptive penalty for low\n  rank representation.\n\\newblock In {\\em Proceedings of NIPS}, 2011.\n\n\\bibitem{LiuLinYanSunYuMa2013}\nG.~Liu, Z.~Lin, S.~Yan, J.~Sun, Y.~Yu, and Y.~Ma.\n\\newblock Robust recovery of subspace structures by low-rank representation.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  35(1):171--184, 2013.\n\n\\bibitem{Mueller2011}\nH.-G. M\\\"{u}ller.\n\\newblock {\\em International Encyclopedia of Statistical Science}, chapter\n  Functional data analysis, pages 554--555.\n\\newblock Springer, 2011.\n\n\\bibitem{parikh2013proximal}\nN.~Parikh and S.~Boyd.\n\\newblock Proximal algorithms.\n\\newblock {\\em Foundations and Trends in Optimization}, 1(3):123--231, 2013.\n\n\\bibitem{PetitjeanForestierWebbNicholsonChenKeogh2014}\nF.~Petitjean, G.~Forestier, G.~I. Webb, A.~E. Nicholson, Y.~Chen, and E.~Keogh.\n\\newblock Dynamic time warping averaging of time series allows faster and more\n  accurate classification. in icdm, 2014.\n\\newblock In {\\em International Conference on Data Mining}, 2014.\n\n\\bibitem{Rakthanmanon2013}\nT.~Rakthanmanon.\n\\newblock Addressing big data time series: Mining trillions of time series\n  subsequences under dynamic time warping.\n\\newblock {\\em ACM Transactions on Knowledge Discovery from Data}, 7(3):1--31,\n  2013.\n\n\\bibitem{RamsaySilverman2005}\nJ.~Ramsay and B.~W. Silverman.\n\\newblock {\\em Functional Data Analysis}.\n\\newblock Springer Series in Statistics. Springer, 2005.\n\n\\bibitem{Robinson2012}\nD.~Robinson.\n\\newblock {\\em Functional analysis and partial matching in the square root\n  velocity framework}.\n\\newblock PhD thesis, Florida State University, 2012.\n\n\\bibitem{ShiMalik2000}\nJ.~Shi and J.~Malik.\n\\newblock Normalized cuts and image segmentation.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  22:888--905, 2000.\n\n\\bibitem{SrivastavaShantanuJermyn2011}\nA.~Srivastava, E.~Klassen, S.~H. Joshi, and I.~H. Jermyn.\n\\newblock Shape analysis of elastic curves in {E}uclidean spaces.\n\\newblock {\\em IEEE Transactionson Pattern Analysis and Machine Intelligence},\n  33(7):1415--1428, 2011.\n\n\\bibitem{SrivastavaWuKurtekKlassenMarron2011}\nA.~Srivastava, W.~Wu, S.~Kurtek, E.~Klassen, and J.~S. Marron.\n\\newblock Registration of functional data using {Fisher-Rao} metric.\n\\newblock {\\em varXiv:1103.3817}, 2011.\n\n\\bibitem{SuSrivastava2014}\nJ.~Su and A.~Srivastava.\n\\newblock Rate-invariant analysus of trajectories on {Riemannian} manifolds\n  with application in visual speech recognition.\n\\newblock In {\\em Proceedings of International Conference on Computer Vision\n  and Pattern Recognition}, 2014.\n\n\\bibitem{SuSrivastavaHuffer2013}\nJ.~Su, A.~Srivastava, and F.~W. Huffer.\n\\newblock Detection, classification and estimation of individual shapes in {2D}\n  and {3D} point clouds.\n\\newblock {\\em Computational Statistics \\& Data Analysis}, 58:227--241, 2013.\n\n\\bibitem{TuckerWuSrivastava2013}\nJ.~D. Tucker, W.~Wu, and A.~Srivastava.\n\\newblock Generative models for functional data using phase and amplitude\n  separation.\n\\newblock {\\em Computational Statistics and Data Analysis}, 61:50--66, 2013.\n\n\\bibitem{WangHuGaoSunYin2015}\nB.~Wang, Y.~Hu, J.~Gao, Y.~Sun, and B.~Yin.\n\\newblock Low rank representation on grassmann manifolds: An extrinsic\n  perspective.\n\\newblock {\\em arXiv:1301.3529}, 1:1--9, 2015.\n\n\\bibitem{williams2008modelling}\nB.~Williams, M.~Toussaint, and A.~J. Storkey.\n\\newblock Modelling motion primitives and their timing in biologically executed\n  movements.\n\\newblock In {\\em Advances in neural information processing systems}, pages\n  1609--1616, 2008.\n\n\\bibitem{williams2006extracting}\nB.~H. Williams, M.~Toussaint, and A.~J. Storkey.\n\\newblock {\\em Extracting motion primitives from natural handwriting data}.\n\\newblock Springer, 2006.\n\n\\bibitem{WuGaneshShiMatsushitaWangMa2012}\nL.~Wu, A.~Ganesh, B.~Shi, Y.~Matsushita, Y.~Wang, and Y.~Ma.\n\\newblock Convex optimization based low-rank matrix completion and recovery for\n  photometric stereo and factor classification.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  XX:XXX--XXX, August 2012.\n\n\\bibitem{YinGaoGuo2015}\nM.~Yin, J.~Gao, and Y.~Guo.\n\\newblock A nonlinear low-rank representation on {S}tiefel manifold.\n\\newblock {\\em Electronics Letters}, 51(10):749--751, 2015.\n\n\\bibitem{YinGaoLinShiGuo2015}\nM.~Yin, J.~Gao, Z.~Lin, Q.~Shi, and Y.~Guo.\n\\newblock Graph dual regularized low-rank matrix approximation for data\n  representation.\n\\newblock {\\em IEEE Transactions on Image Processing}, 24(12):4918--4933, 2015.\n\n\\bibitem{ZhangSuKlassenLeSrivastava2015}\nZ.~Zhang, J.~Su, E.~Klassen, H.~Le, and A.~Srivastava.\n\\newblock Video-based action recognition using rate-invariant analysis of\n  covariance trajectories.\n\\newblock {\\em arXiv:1503.06699v1}, 1, 2015.\n\n\\end{thebibliography}\n\n}\n\n\n\n", "itemtype": "equation", "pos": 28102, "prevtext": "\n\n\n\\subsection{Convergence Analysis}\nAlgorithm~\\ref{curve_lrr_alg} is adopted from the algorithm proposed in~\\cite{LinLiuSu2011}. However due to the terms of $\\mathbf B^i$'s in the objective function~\\eqref{25August2014-5}, the convergence theorem proved in~\\cite{LinLiuSu2011} cannot be directly applied to this case as the linearization is implemented on both the augmented Lagrangian terms and the term involving $\\mathbf B^i$'s. Fortunately we can employ the revised approach, presented in \\cite{YinGaoLinShiGuo2015}, to prove the convergence for the algorithm. Without repeating all the details, we present the convergence theorem for Algorithm~\\ref{curve_lrr_alg} as follows.\n\\begin{theorem}[Convergence of Algorithm~\\ref{curve_lrr_alg}] If $\\eta_W\\geq \\max\\{\\|B_i\\|^2\\}+N+1$, $\\displaystyle\\sum^{+\\infty}_{k=1}\\beta^{-1}_k = +\\infty$, $\\displaystyle\\beta_{k+1}-\\beta_k > C_0 \\frac{\\sum_i \\|B_i\\|^2}{\\eta_W - \\max\\{\\|B_i\\|^2\\}-N}$, where $C_0$ is a given constant and $\\|\\cdot\\|$ is the matrix spectral norm, then the sequence $\\{W^{k}\\}$ generated by Algorithm~\\ref{curve_lrr_alg} converges to an optimal solution to problem~\\eqref{obj_curve}.\n\\end{theorem}\n\nIn all the experiments we have conducted, the algorithm converges very fast with $K<100$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Experiments}\\label{Sec:4}\nIn this section we show three sets of experiments to evaluate the newly proposed cLRR. The performance of the proposed method is compared with the same type of subspace clustering algorithm LRR \\cite{LiuLinYanSunYuMa2013}. To compare segmentation accuracy we use the subspace clustering accuracy (SCA) metric \\cite{ElhamifarVidal2013}, which is defined as\n\n", "index": 57, "text": "\\begin{align}\n\\text{SCA} = 1 - \\frac{\\text{num. of misclassified points}}{\\text{total num. of points}}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{SCA}=1-\\frac{\\text{num. of misclassified points}}{\\text{%&#10;total num. of points}}.\" display=\"inline\"><mrow><mrow><mtext>SCA</mtext><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mtext>num. of misclassified points</mtext><mtext>total num. of points</mtext></mfrac></mstyle></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]