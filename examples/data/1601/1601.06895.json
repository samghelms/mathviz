[{"file": "1601.06895.tex", "nexttext": "\n \n", "itemtype": "equation", "pos": 11748, "prevtext": "\n\n\\clearpage\n\n\n\n\\title{\\LARGE Echo State Networks for Self-Organizing Resource Allocation in LTE-U with Uplink-Downlink Decoupling}\n\n\n\n\\author{{Mingzhe Chen\\IEEEauthorrefmark{1}}, Walid Saad\\IEEEauthorrefmark{2}, and Changchuan Yin\\IEEEauthorrefmark{1}\\vspace*{0em}\\\\\n\\authorblockA{\\small \\IEEEauthorrefmark{1}Beijing Key Laboratory of Network System Architecture and Convergence,\\\\ \nBeijing University of Posts and Telecommunications, Beijing, China 100876\\\\\nEmail: \\protect\\url{chenmingzhe@bupt.edu.cn} and \\protect\\url{ccyin@ieee.org.} \\\\\n\\IEEEauthorrefmark{2}Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA, Email: \\protect\\url{walids@vt.edu}\\\\\n\n\n\n\n\n}\\vspace*{0em}\n   \n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\\maketitle\n\\thispagestyle{empty}\n\n\\vspace{0cm}\n\\begin{abstract}\nUplink-downlink decoupling in which users can be associated to different base stations in the uplink and downlink of heterogeneous small cell networks (SCNs) has attracted significant attention recently. However, most existing works focus on simple association mechanisms in LTE SCNs that operate\nonly in the licensed band. In contrast, in this paper, the problem of resource allocation with uplink-downlink decoupling is studied for an\nSCN that incorporates LTE in the unlicensed band (LTE-U). Here, the users can access both licensed and unlicensed bands while being associated to different base stations. This problem is formulated as a noncooperative game that incorporates user association, spectrum allocation, and load balancing.\nTo solve this problem, a distributed algorithm based on the machine learning framework of \\emph{echo state networks} is proposed using which the small base stations autonomously choose their optimal bands allocation strategies while having only limited information on the network's and users' states. It is shown that the proposed algorithm converges to a stationary mixed-strategy distribution which constitutes a mixed strategy Nash equilibrium for the studied game. Simulation results show that the proposed approach yields significant gains, in terms of the sum-rate of the $50$th percentile of users, that reach up to $60\\%$ and $78\\%$, respectively, compared to Q-learning and Q-learning without decoupling. The results also show that ESN significantly provides a considerable reduction of information exchange for the wireless network.\n\\end{abstract}\n\n\\vspace{0cm}\n{\\small \\emph{Index Terms}--- game theory; resource allocation; heterogeneous networks; reinforcement learning; LTE-U.}\n\n\\vspace{-0cm}\n\\section{Introduction}\n\\label{sec:intro}\nThe recent surge in wireless services has led to significant changes in existing cellular systems \\cite{1}. In particular, the next-generation of cellular systems will be based on small cell networks (SCNs) that rely on low-cost, low-power small base stations (SBSs). The ultra dense nature of SCNs coupled with the transmit power disparity between SBSs, further motivates the use of uplink-downlink decoupling techniques \\cite{2,3} in which users can associate to different SBSs in the uplink and downlink, respectively. Such techniques have become recently very popular, particularly with the emergence of uplink-intensive applications such as machine-to-machine communications or even social networks. \n\nExisting literature has studied a number of problems related to uplink-downlink decoupling \\cite{2,3}. In \\cite{2}, the authors delineate the main benefits of decoupling the uplink and downlink, and propose an optimal decoupling association strategy that maximizes data rate. The work in \\cite{3} investigates the throughput and outage gains of uplink-downlink decoupling using a simulation approach. Despite the promising results, these existing works \\cite{2,3} are restricted to performance analysis and centralized optimization approaches that may not scale well in a dense and heterogeneous SCN. Moreover, these existing works are restricted to classical LTE networks in which the devices and SBSs can access only a single, licensed band. \n\nRecently, there has been significant interest in studying how LTE-based SCNs can operate in the unlicensed band (LTE-U) \\cite{5,6,7,8,9,10,11,12}. LTE-U presents many challenges in terms of spectrum allocation, user association, and interference management \\cite{5,6,7}.   \nIn \\cite{5}, optimal resource allocation algorithms are proposed for both dual band femtocell and integrated femto-WiFi networks. The authors in \\cite{6} develop a hybrid method to perform both traffic offloading and resource sharing in an LTE-U scenario with a single base station (BS). In \\cite{7}, a novel performance analysis that accounts for system-level dynamics is performed and an enabling architecture that captures the tight interaction between different radio access technologies is proposed. However, most existing works on LTE-U \\cite{5,6,7,8,9,10,11,12} have focused on the performance analysis and resource allocation with conventional association methods. Indeed, none of these works analyzed the potential of uplink-downlink decoupling in LTE-U. LTE-U provides an ideal setting to perform uplink-downlink decoupling, since beyond the spatial dimension, the possibility of uplink-downlink decoupling exists not only between base stations but also between the licensed and unlicensed bands.   \n\nMore recently, reinforcement learning (RL) has gained significant attention for developing distributed approaches for resource allocation in LTE and heterogeneous SCNs \\cite{13,14,15,16,17,18,19,20,21}. In \\cite{14,15,16,17} and \\cite{21}, channel selection, network selection, and interference management were addressed using the framework of Q-learning and smoothed best response. In \\cite{18,19,20}, regret-based learning approaches are developed to address the problem of interference management, dynamic clustering, and SBSs' on/off. However, none of the existing works on RL \\cite{13,14,15,16,17,18,19,20,21} have focused on the LTE-U network and downlink-uplink decoupling. Moreover, most existing algorithms \\cite{13,14,15,16,17,18,19,20,21}, require agents to obtain the other agents' value functions \\cite{22} and state information, which is not practical for scenarios in which agents are distributed. In contrast, here, our goal is to develop a novel and efficient multi-agent RL algorithm based on recurrent neural networks (RNNs) \\cite{23,24,25,26,27,28} whose advantage is that they can store the state informations of users and have no need to share value functions between agents. \n\n\n\n\n\nSince RNNs have the ability to retain state over time, because of their recurrent connections, they are promising candidates for compactly storing moments of series of observations. \\emph{Echo state networks}, an emerging RNN framework \\cite{23,24,25,26,27,28}, are a promising candidate for wireless network modeling, as they are known to be relatively easy to train. Existing literature has studied a number of problems related to ESNs \\cite{23,24,25,26}. In \\cite{23,24,25}, ESNs are proposed to model reward function, characterize wireless channels, and equalize the non-linear satellite communication channel. The authors in \\cite{26}, prove the convergence of ESNs-RL algorithm under the condition of Markov decision problems and exploits ESNs-RL algorithm to settle the maze and tiger problems. However, most existing works on ESNs \\cite{23,24,25,26} have just focused on the problems of traditional RNNs and wireless channel. In addition, none of these works exploited the use of ESNs for resource allocation problem in a wireless network. \n\nThe main contribution of this paper is to develop a novel, self-organizing framework to optimize resource allocation with uplink-downlink decoupling in an LTE-U system. We formulate the problem as a noncooperative game in which the players are the SBSs and the macrocell base station (MBS). Each player seeks to find an optimal spectrum allocation scheme to optimize a utility function that captures the sum-rate in terms of downlink and uplink, and balances the licensed and unlicensed spectrums between users. To solve this resource allocation game, we propose a self-organizing algorithm based on the powerful framework of ESNs \\cite{26,27,28}. Unlike previous studies \\cite{5,6}, which rely on the coordination among SBSs and on the knowledge of the entire users' state informations, the proposed approach requires minimum information to learn the mixed strategy Nash equilibrium. The use of ESNs enables the LTE-U SCN to quickly learn its resource allocation parameters without requiring significant training data. \nThe proposed algorithm enables \\emph{dual-mode} SBSs to autonomously learn and decide on the allocation of the licensed and unlicensed bands in the uplink and downlink to each user depending on the network environment. Moreover, we show that the proposed ESN algorithm can converge to a mixed strategy Nash equilibrium.\nTo our best knowledge, \\emph{this is the first work that exploits the framework of ESNs to optimize resource allocation with uplink-downlink decoupling in LTE-U systems}. \n\n\nSimulation results show that the proposed approach yields a performance improvement, in terms of the sum-rate of the $50$th percentile of users, reaching up to $60\\%$ and $78\\%$, respectively, compared to Q-learning and Q-learning without decoupling.  \n\n\nThe rest of this paper is organized as follows. The system model is described in Section \\uppercase\\expandafter{\\romannumeral2}. The ESN-based resource allocation algorithm is proposed in Section \\uppercase\\expandafter{\\romannumeral3}. In Section \\uppercase\\expandafter{\\romannumeral4}, numerical simulation results are presented and analyzed. Finally, conclusions are drawn in Section \\uppercase\\expandafter{\\romannumeral5}.\n\n\n\n\n\\vspace{-0cm}\n\\section{System Model and Problem Formulation}\n\\label{sec:SM}\nConsider the downlink and uplink of an SCN that encompasses LTE-U, WiFi access points, and a macrocell network. Here, the macrocell tier operates using only the licensed band. The MBS is located at the center of a geographical area. Within this area, we consider a set $\\mathcal{N} = \\{1,2,\\ldots,N_s\\}$ of \\emph{dual-mode} SBSs that are able to access both the licensed and unlicensed bands. Co-located with this cellular network is a WiFi network that consists of $W$ WiFi access points. In addition, we consider a set $\\mathcal{U} = \\left\\{ {1,2, \\cdots ,U} \\right\\}$ of $U$ LTE-U users which are distributed uniformly over the area of interest. All users can access different SBSs as well as the MBS for transmitting in the downlink or uplink. For this system, we consider a frequency division duplexing (FDD) mode for LTE on the licensed band, which splits the licensed band into equal spectrum bands for the downlink and uplink. Time division duplexing (TDD) mode LTE with listen before talk mechanism is considered on the unlicensed band \\cite{12}. Here, the unlicensed band is used for both the downlink and uplink, just like in a conventional LTE TDD system. For LTE-U operating in the unlicensed band, TDD offers the flexibility to adjust the resource allocation between the downlink and uplink. The WiFi access points will transmit using a standard carrier sense multiple access with collision avoidance (CSMA/CA) protocol with its corresponding RTS/CTS access mechanism. \n\n\\subsection{LTE data rate analysis}\nWe denote by $\\mathcal{B}_l$ the set of the MBS and SBSs on the licensed band. During the connection period, we denote by $c_{lij}^\\textrm{DL}$ the downlink capacity and $c_{lji}^\\textrm{UL}$ the uplink capacity on the licensed band. Thus, the overall long-term downlink and uplink rates of the LTE-U user $i$ on the licensed band are given by:\n \n", "index": 1, "text": "\\begin{equation}\n  {R_{lij}^\\textrm{DL}} = {d_{ij}}{c_{lij}^\\textrm{DL}}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{R_{lij}^{\\textrm{DL}}}={d_{ij}}{c_{lij}^{\\textrm{DL}}},\" display=\"block\"><mrow><mrow><msubsup><mi>R</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup><mo>=</mo><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere\n \n", "itemtype": "equation", "pos": 11839, "prevtext": "\n \n", "index": 3, "text": "\\begin{equation}\n{R_{lji}^\\textrm{UL}} = {v_{ji}}{c_{lji}^\\textrm{UL}}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{R_{lji}^{\\textrm{UL}}}={v_{ji}}{c_{lji}^{\\textrm{UL}}},\" display=\"block\"><mrow><mrow><msubsup><mi>R</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup><mo>=</mo><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n \n", "itemtype": "equation", "pos": 11934, "prevtext": "\nwhere\n \n", "index": 5, "text": "\\begin{equation*}\n c_{lij}^\\textrm{DL}\\!=\\! F_l^\\textrm{DL}{\\log _2}\\left(\\!1\\!+\\! {\\frac{{{P_j}{h_{ij}}}}{{\\sum\\limits_{k \\in \\mathcal{B}_l,k \\ne j} {{P_k}{h_{ik}}}  \\!+\\! {\\sigma ^2}}}} \\!\\right)\\!\\!,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"c_{lij}^{\\textrm{DL}}\\!=\\!F_{l}^{\\textrm{DL}}{\\log_{2}}\\left(\\!1\\!+\\!{\\frac{{{%&#10;P_{j}}{h_{ij}}}}{{\\sum\\limits_{k\\in\\mathcal{B}_{l},k\\neq j}{{P_{k}}{h_{ik}}}\\!%&#10;+\\!{\\sigma^{2}}}}}\\!\\right)\\!\\!,\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mpadded><mo rspace=\"0.8pt\">=</mo><mrow><msubsup><mi>F</mi><mi>l</mi><mtext>DL</mtext></msubsup><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo rspace=\"0.8pt\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">+</mo><mpadded width=\"-1.7pt\"><mfrac><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>k</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>l</mi></msub></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow></munder><mrow><msub><mi>P</mi><mi>k</mi></msub><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mpadded></mrow></mrow><mo rspace=\"0.8pt\">+</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mpadded></mrow><mo rspace=\"0pt\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n $d_{ij}$ and $v_{ji}$ are the fraction of the downlink and uplink licensed bands allocated from SBS $j$ to user $i$, respectively, \n \n$F_l^\\textrm{DL}$ and $F_l^\\textrm{UL}$ denote, respectively, the downlink and uplink bandwidths on the licensed band, $P_{j}$ is the transmit power of the BS $j$, $P_{u}$ is the transmit power of LTE-U users, ${h_{ij}}$ is the channel gain between user $i$ and BS $j$, and ${\\sigma ^2}$ is the power of the Gaussian noise. Note that, hereinafter, we use the term BS to refer to either an SBS or the MBS and denote by $\\mathcal{B}$ the set of BSs.\n\nSimilarly, the downlink and uplink rates of user $i$ that is transmitting over the unlicensed band are given by:\n\n", "itemtype": "equation", "pos": 12154, "prevtext": "\n \n", "index": 7, "text": "\\begin{equation*} \n c_{lji}^\\textrm{UL}\\! =\\! F_l^\\textrm{UL}{\\log _2}\\!\\left(\\!\\!1\\!+\\!\\! {\\frac{{{P_u}{h_{ij}}}}{{\\sum\\limits_{k \\in \\mathcal{U}, k \\ne i} {{P_u}{h_{kj}}}  \\!+\\! {\\sigma ^2}}}}\\! \\right)\\!,\n \\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"c_{lji}^{\\textrm{UL}}\\!=\\!F_{l}^{\\textrm{UL}}{\\log_{2}}\\!\\left(\\!\\!1\\!+\\!\\!{%&#10;\\frac{{{P_{u}}{h_{ij}}}}{{\\sum\\limits_{k\\in\\mathcal{U},k\\neq i}{{P_{u}}{h_{kj}%&#10;}}\\!+\\!{\\sigma^{2}}}}}\\!\\right)\\!,\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup></mpadded><mo rspace=\"0.8pt\">=</mo><mrow><msubsup><mi>F</mi><mi>l</mi><mtext>UL</mtext></msubsup><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mi>log</mi><mn>2</mn></msub></mpadded><mo>\u2061</mo><mrow><mo rspace=\"0pt\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0pt\">+</mo><mpadded width=\"-1.7pt\"><mfrac><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>k</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>i</mi></mrow></mrow></munder><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mpadded></mrow></mrow><mo rspace=\"0.8pt\">+</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mpadded></mrow><mo rspace=\"0.8pt\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 13076, "prevtext": "\n $d_{ij}$ and $v_{ji}$ are the fraction of the downlink and uplink licensed bands allocated from SBS $j$ to user $i$, respectively, \n \n$F_l^\\textrm{DL}$ and $F_l^\\textrm{UL}$ denote, respectively, the downlink and uplink bandwidths on the licensed band, $P_{j}$ is the transmit power of the BS $j$, $P_{u}$ is the transmit power of LTE-U users, ${h_{ij}}$ is the channel gain between user $i$ and BS $j$, and ${\\sigma ^2}$ is the power of the Gaussian noise. Note that, hereinafter, we use the term BS to refer to either an SBS or the MBS and denote by $\\mathcal{B}$ the set of BSs.\n\nSimilarly, the downlink and uplink rates of user $i$ that is transmitting over the unlicensed band are given by:\n\n", "index": 9, "text": "\\begin{equation}\nR_{uij}^\\textrm{DL} = {\\kappa _{ij}}c_{uij}^\\textrm{DL},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"R_{uij}^{\\textrm{DL}}={\\kappa_{ij}}c_{uij}^{\\textrm{DL}},\" display=\"block\"><mrow><mrow><msubsup><mi>R</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup><mo>=</mo><mrow><msub><mi>\u03ba</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere\n \n", "itemtype": "equation", "pos": 13165, "prevtext": "\n\n", "index": 11, "text": "\\begin{equation}\nR_{uji}^\\textrm{UL} = {\\tau_{ji}}c_{uji}^\\textrm{UL},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"R_{uji}^{\\textrm{UL}}={\\tau_{ji}}c_{uji}^{\\textrm{UL}},\" display=\"block\"><mrow><mrow><msubsup><mi>R</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup><mo>=</mo><mrow><msub><mi>\u03c4</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n \n", "itemtype": "equation", "pos": 13258, "prevtext": "\nwhere\n \n", "index": 13, "text": "\\begin{equation*}\n c_{uij}^\\textrm{DL}={{LF_u}{{\\log }_2}\\left( {1 + \\frac{{{P_j}{h_{ij}}}}{{\\sum\\limits_{k \\in \\mathcal{B}_u, k \\ne j} {{P_w}{h_{ik}} + {\\sigma ^2}} }}} \\right)} ,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"c_{uij}^{\\textrm{DL}}={{LF_{u}}{{\\log}_{2}}\\left({1+\\frac{{{P_{j}}{h_{ij}}}}{{%&#10;\\sum\\limits_{k\\in\\mathcal{B}_{u},k\\neq j}{{P_{w}}{h_{ik}}+{\\sigma^{2}}}}}}%&#10;\\right)},\" display=\"block\"><mrow><mrow><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup><mo>=</mo><mrow><mi>L</mi><mo>\u2062</mo><msub><mi>F</mi><mi>u</mi></msub><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>k</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi>u</mi></msub></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow></munder><mrow><msub><mi>P</mi><mi>w</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>+</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n$L$ denotes the SBSs occupy time slots of the unlicensed band, ${\\kappa _{ij}}$ and ${\\tau_{ji}}$ denote, respectively, the downlink and uplink time slots during which user $i$ transmits on the unlicensed band. Note that, the SBSs adopt a TDD mode on the unlicensed band and the LTE-U users on the uplink and downlink share the time slots of the unlicensed band. \n\n$F_u$ denotes the bandwidth of the unlicensed band, $\\mathcal{B}_u$ denotes the SBSs on the unlicensed band, and $L$ $\\in [0,1]$ is the time slots during which the LTE SCN uses the unlicensed band.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{WiFi data rate analysis}\nFor the WiFi network, we assume that the WiFi access points will adopt a CSMA/CA scheme with binary slotted exponential backoff. Therefore, the saturation capacity of $N_w$ users sharing the same unlicensed band can be expressed by \\cite{29}: \n \n", "itemtype": "equation", "pos": 13457, "prevtext": "\n\n \n", "index": 15, "text": "\\begin{equation*}\nc_{uji}^\\textrm{UL}\\! =\\! LF_u{\\log _2}\\!\\left(\\!\\!1\\!+\\!\\! {\\frac{{{P_u}{h_{ij}}}}{{\\sum\\limits_{k \\in \\mathcal{U}, k \\ne i} {{P_u}{h_{kj}}}  \\!+\\! {\\sigma ^2}}}}\\! \\right)\\!,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"c_{uji}^{\\textrm{UL}}\\!=\\!LF_{u}{\\log_{2}}\\!\\left(\\!\\!1\\!+\\!\\!{\\frac{{{P_{u}}{%&#10;h_{ij}}}}{{\\sum\\limits_{k\\in\\mathcal{U},k\\neq i}{{P_{u}}{h_{kj}}}\\!+\\!{\\sigma^%&#10;{2}}}}}\\!\\right)\\!,\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow><mtext>UL</mtext></msubsup></mpadded><mo rspace=\"0.8pt\">=</mo><mrow><mi>L</mi><mo>\u2062</mo><msub><mi>F</mi><mi>u</mi></msub><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mi>log</mi><mn>2</mn></msub></mpadded><mo>\u2061</mo><mrow><mo rspace=\"0pt\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0pt\">+</mo><mpadded width=\"-1.7pt\"><mfrac><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>k</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>i</mi></mrow></mrow></munder><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mpadded></mrow></mrow><mo rspace=\"0.8pt\">+</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mpadded></mrow><mo rspace=\"0.8pt\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere ${P_{tr}} = 1 - {\\left( {1 - \\varsigma } \\right)^{{N_w}}}$, ${P_{tr}}$ is the probability that there is at least one transmission in a time slot and $\\varsigma$ is the transmission probability of each user. ${P_s} =\\left. {N_w}\\varsigma {\\left( {1 - \\varsigma } \\right)^{{N_w} - 1}}\\small/{P_{tr}}\\right.$,\n\n \n \nis the successful transmission on the channel, $T_s$ is the average time that the channel is sensed busy because of a successful transmission, $T_c$ is the average time that the channel is sensed busy by each station during a collision, $T_\\sigma$ is the duration of an empty slot time, and $E\\left[ P \\right]$ is the average packet size. \n\nIn our model, the WiFi network adopts conventional distributed coordination function (DCF) access and RTS/CTS access mechanisms. Thus, $T_c$ and $T_s$ are given by \\cite{29}:\n\n", "itemtype": "equation", "pos": 14532, "prevtext": "\n$L$ denotes the SBSs occupy time slots of the unlicensed band, ${\\kappa _{ij}}$ and ${\\tau_{ji}}$ denote, respectively, the downlink and uplink time slots during which user $i$ transmits on the unlicensed band. Note that, the SBSs adopt a TDD mode on the unlicensed band and the LTE-U users on the uplink and downlink share the time slots of the unlicensed band. \n\n$F_u$ denotes the bandwidth of the unlicensed band, $\\mathcal{B}_u$ denotes the SBSs on the unlicensed band, and $L$ $\\in [0,1]$ is the time slots during which the LTE SCN uses the unlicensed band.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{WiFi data rate analysis}\nFor the WiFi network, we assume that the WiFi access points will adopt a CSMA/CA scheme with binary slotted exponential backoff. Therefore, the saturation capacity of $N_w$ users sharing the same unlicensed band can be expressed by \\cite{29}: \n \n", "index": 17, "text": "\\begin{equation}\n R\\left( {{N_w}} \\right) = \\frac{{{P_{tr}}{P_s}E\\left[ P \\right]}}{{\\left( {1 - {P_{tr}}} \\right){T_\\sigma } + {P_{tr}}{P_s}{T_s} + {P_{tr}}\\left( {1 - {P_s}} \\right){T_c}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"R\\left({{N_{w}}}\\right)=\\frac{{{P_{tr}}{P_{s}}E\\left[P\\right]}}{{\\left({1-{P_{%&#10;tr}}}\\right){T_{\\sigma}}+{P_{tr}}{P_{s}}{T_{s}}+{P_{tr}}\\left({1-{P_{s}}}%&#10;\\right){T_{c}}}},\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>N</mi><mi>w</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></msub><mo>\u2062</mo><msub><mi>P</mi><mi>s</mi></msub><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo>[</mo><mi>P</mi><mo>]</mo></mrow></mrow><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>T</mi><mi>\u03c3</mi></msub></mrow><mo>+</mo><mrow><msub><mi>P</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></msub><mo>\u2062</mo><msub><mi>P</mi><mi>s</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><msub><mi>P</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>P</mi><mi>s</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>T</mi><mi>c</mi></msub></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n \n \n", "itemtype": "equation", "pos": 15573, "prevtext": "\nwhere ${P_{tr}} = 1 - {\\left( {1 - \\varsigma } \\right)^{{N_w}}}$, ${P_{tr}}$ is the probability that there is at least one transmission in a time slot and $\\varsigma$ is the transmission probability of each user. ${P_s} =\\left. {N_w}\\varsigma {\\left( {1 - \\varsigma } \\right)^{{N_w} - 1}}\\small/{P_{tr}}\\right.$,\n\n \n \nis the successful transmission on the channel, $T_s$ is the average time that the channel is sensed busy because of a successful transmission, $T_c$ is the average time that the channel is sensed busy by each station during a collision, $T_\\sigma$ is the duration of an empty slot time, and $E\\left[ P \\right]$ is the average packet size. \n\nIn our model, the WiFi network adopts conventional distributed coordination function (DCF) access and RTS/CTS access mechanisms. Thus, $T_c$ and $T_s$ are given by \\cite{29}:\n\n", "index": 19, "text": "\\begin{equation}\n \\begin{aligned}\n{T_s} = &{{RTS} \\mathord{\\left/\n {\\vphantom {{RTS} C}} \\right.\n \\kern-\\nulldelimiterspace} C} + {{CTS} \\mathord{\\left/\n {\\vphantom {{CTS} C}} \\right.\n \\kern-\\nulldelimiterspace} C} + {{\\left( {H + E\\left[ P \\right]} \\right)} \\mathord{\\left/\n {\\vphantom {{\\left( {H + E\\left[ P \\right]} \\right)} C}} \\right.\n \\kern-\\nulldelimiterspace} C} \\\\\n &+ {{ACK} \\mathord{\\left/\n {\\vphantom {{ACK} C}} \\right.\n \\kern-\\nulldelimiterspace} C} + 3SIFS + DIFS + 4\\delta ,\n \\end{aligned}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{T_{s}}=\" display=\"inline\"><mrow><msub><mi>T</mi><mi>s</mi></msub><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle{{RTS}\\mathord{\\left/{\\vphantom{{RTS}C}}\\right.\\kern-1.2pt}C}+{{%&#10;CTS}\\mathord{\\left/{\\vphantom{{CTS}C}}\\right.\\kern-1.2pt}C}+{{\\left({H+E\\left[%&#10;P\\right]}\\right)}\\mathord{\\left/{\\vphantom{{\\left({H+E\\left[P\\right]}\\right)}C%&#10;}}\\right.\\kern-1.2pt}C}\" display=\"inline\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>C</mi></mrow><mo>+</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>C</mi></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mi>H</mi><mo>+</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo>[</mo><mi>P</mi><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>C</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+{{ACK}\\mathord{\\left/{\\vphantom{{ACK}C}}\\right.\\kern-1.2pt}C}+3%&#10;SIFS+DIFS+4\\delta,\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>C</mi></mrow></mrow><mo>+</mo><mrow><mn>3</mn><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi></mrow><mo>+</mo><mrow><mn>4</mn><mo>\u2062</mo><mi>\u03b4</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $T_s$ denotes the average time the channel is sensed busy because of a successful transmission, $T_c$ is the average time the channel is sensed busy by each station during a collision, $H = PH{Y_{hdr}} + MA{C_{hdr}}$, $C$ is the channel bit rate, and $ACK$, $DIFS$, $\\delta$, $RTS$, and $CTS$ are WiFi parameters.\n \n\nIn our model, the SBSs occupy $L$ time slots on the unlicensed band. Thus, the WiFi users occupy the $\\left(1-L\\right)$ time slots and the per WiFi user rate is:\n\n", "itemtype": "equation", "pos": 16098, "prevtext": "\n \n \n", "index": 21, "text": "\\begin{equation}\n {T_c} = {{RTS} \\mathord{\\left/\n {\\vphantom {{RTS} C}} \\right.\n \\kern-\\nulldelimiterspace} C} + DIFS + \\delta,\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{T_{c}}={{RTS}\\mathord{\\left/{\\vphantom{{RTS}C}}\\right.\\kern-1.2pt}C}+DIFS+\\delta,\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>c</mi></msub><mo>=</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi mathvariant=\"normal\">/</mi><mo>\u2062</mo><mi>C</mi></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi></mrow><mo>+</mo><mi>\u03b4</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "   \nwhere $N_w$ is the number of WiFi users on the unlicensed band.\n\n\\subsection{Problem formulation}\nGiven this system model, our goal is to develop an effective spectrum allocation scheme with uplink-downlink decoupling that can allocate the appropriate bandwidth on the licensed band and time slots on the unlicensed band to each user, simultaneously. However, the rate of each SBS depends not only on its own choice of the allocation action but also on remaining SBSs' actions. In this regard, we formulate a noncooperative game $\\mathcal{G} = \\left[ {\\mathcal{B},\\left\\{ {{\\mathcal{A}_n}} \\right\\}_{n \\in \\mathcal{B}},\\left\\{ {{u_n}} \\right\\}}_{n \\in \\mathcal{B}} \\right]$ in which the set of BSs $\\mathcal{B}$ are the players including SBSs and the MBS and $u_n$ is the utility function for BS $n$. Each player $n$ has a set ${\\mathcal{A}_n} = \\left\\{ {\\boldsymbol{a}_{n,1}, \\ldots, \\boldsymbol{a}_{n,{\\left| {{\\mathcal{A}_n}} \\right|}}} \\right\\}$  of actions where $\\left| {{\\mathcal{A}_n}} \\right|$ is the total number of actions. For an SBS $n$, each action ${\\boldsymbol{a}_n} = \\left( {{\\boldsymbol{d}_n},{\\boldsymbol{v}_n},{\\boldsymbol{\\rho}_n}} \\right)$, is composed of: (i) the downlink licensed bandwidth allocation ${ \\boldsymbol{d}_n} = \\left[  {d_{n,1}, \\ldots, d_{n,{K_n}}} \\right]$, where $K_n$ is the number of all users in the coverage area $\\mathcal{L}_b$ of SBS $n$, (ii) the uplink licensed bandwidth allocation ${ \\boldsymbol{v}_n} = \\left[  {v_{n,1}, \\ldots v_{n,{{K_n}}}} \\right]$, and, (iii) the time slots allocation on the unlicensed band ${ \\boldsymbol{\\rho }_n} = \\left[  {\\kappa_{1,n}, \\ldots, \\kappa_{{K_n},n}, \\tau_{n,1}, \\ldots, \\tau_{n,{{K_n}}}} \\right]$. ${\\boldsymbol{d}_n}$, ${\\boldsymbol{v}_n}$ and ${\\boldsymbol{\\rho}_n}$ must satisfy:\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $T_s$ denotes the average time the channel is sensed busy because of a successful transmission, $T_c$ is the average time the channel is sensed busy by each station during a collision, $H = PH{Y_{hdr}} + MA{C_{hdr}}$, $C$ is the channel bit rate, and $ACK$, $DIFS$, $\\delta$, $RTS$, and $CTS$ are WiFi parameters.\n \n\nIn our model, the SBSs occupy $L$ time slots on the unlicensed band. Thus, the WiFi users occupy the $\\left(1-L\\right)$ time slots and the per WiFi user rate is:\n\n", "index": 23, "text": "\\begin{equation}\n{R_w} = \\frac{{R\\left( {{N_w}} \\right)\\left( {1 - L} \\right)}}{{{N_w}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"{R_{w}}=\\frac{{R\\left({{N_{w}}}\\right)\\left({1-L}\\right)}}{{{N_{w}}}},\" display=\"block\"><mrow><mrow><msub><mi>R</mi><mi>w</mi></msub><mo>=</mo><mfrac><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>N</mi><mi>w</mi></msub><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>L</mi></mrow><mo>)</mo></mrow></mrow><msub><mi>N</mi><mi>w</mi></msub></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 18610, "prevtext": "   \nwhere $N_w$ is the number of WiFi users on the unlicensed band.\n\n\\subsection{Problem formulation}\nGiven this system model, our goal is to develop an effective spectrum allocation scheme with uplink-downlink decoupling that can allocate the appropriate bandwidth on the licensed band and time slots on the unlicensed band to each user, simultaneously. However, the rate of each SBS depends not only on its own choice of the allocation action but also on remaining SBSs' actions. In this regard, we formulate a noncooperative game $\\mathcal{G} = \\left[ {\\mathcal{B},\\left\\{ {{\\mathcal{A}_n}} \\right\\}_{n \\in \\mathcal{B}},\\left\\{ {{u_n}} \\right\\}}_{n \\in \\mathcal{B}} \\right]$ in which the set of BSs $\\mathcal{B}$ are the players including SBSs and the MBS and $u_n$ is the utility function for BS $n$. Each player $n$ has a set ${\\mathcal{A}_n} = \\left\\{ {\\boldsymbol{a}_{n,1}, \\ldots, \\boldsymbol{a}_{n,{\\left| {{\\mathcal{A}_n}} \\right|}}} \\right\\}$  of actions where $\\left| {{\\mathcal{A}_n}} \\right|$ is the total number of actions. For an SBS $n$, each action ${\\boldsymbol{a}_n} = \\left( {{\\boldsymbol{d}_n},{\\boldsymbol{v}_n},{\\boldsymbol{\\rho}_n}} \\right)$, is composed of: (i) the downlink licensed bandwidth allocation ${ \\boldsymbol{d}_n} = \\left[  {d_{n,1}, \\ldots, d_{n,{K_n}}} \\right]$, where $K_n$ is the number of all users in the coverage area $\\mathcal{L}_b$ of SBS $n$, (ii) the uplink licensed bandwidth allocation ${ \\boldsymbol{v}_n} = \\left[  {v_{n,1}, \\ldots v_{n,{{K_n}}}} \\right]$, and, (iii) the time slots allocation on the unlicensed band ${ \\boldsymbol{\\rho }_n} = \\left[  {\\kappa_{1,n}, \\ldots, \\kappa_{{K_n},n}, \\tau_{n,1}, \\ldots, \\tau_{n,{{K_n}}}} \\right]$. ${\\boldsymbol{d}_n}$, ${\\boldsymbol{v}_n}$ and ${\\boldsymbol{\\rho}_n}$ must satisfy:\n\n", "index": 25, "text": "\\begin{equation}\\label{eq:c1}\n\\sum\\nolimits_{j=1}^{K_n}\\! {d_{n,j}} \\le 1,\\:\\: \\sum\\nolimits_{j=1}^{K_n} \\!{v_{n,j}} \\le 1,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\sum\\nolimits_{j=1}^{K_{n}}\\!{d_{n,j}}\\leq 1,\\&gt;\\&gt;\\sum\\nolimits_{j=1}^{K_{n}}\\!%&#10;{v_{n,j}}\\leq 1,\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>n</mi></msub></msubsup></mpadded><msub><mi>d</mi><mrow><mi>n</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo rspace=\"6.9pt\">,</mo><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>n</mi></msub></msubsup></mpadded><msub><mi>v</mi><mrow><mi>n</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>\u2264</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 18749, "prevtext": "\n\n", "index": 27, "text": "\\begin{equation}\\label{eq:c2}\n\\sum\\nolimits_{j = 1}^{K_n}\\! \\left( {\\kappa_{n,j}+\\tau_{j,n}} \\right) \\le 1,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\sum\\nolimits_{j=1}^{K_{n}}\\!\\left({\\kappa_{n,j}+\\tau_{j,n}}\\right)\\leq 1,\" display=\"block\"><mrow><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>n</mi></msub></msubsup></mpadded><mrow><mo>(</mo><mrow><msub><mi>\u03ba</mi><mrow><mi>n</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>\u03c4</mi><mrow><mi>j</mi><mo>,</mo><mi>n</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $\\mathcal{Z} = \\left\\{ {{1}, \\ldots ,{Z}} \\right\\}$ is a finite set of $Z$ level fractions of spectrum. For example, one can separate the fractions of spectrum into $Z=10$ equally spaced intervals. Then, we will have $\\mathcal{Z} = \\left\\{ {{0}, {0.1}, \\ldots ,{0.9}, {1}} \\right\\}$. For the MBS, each action ${\\boldsymbol{a}_m} = \\left( {\\boldsymbol{d}_m},{\\boldsymbol{v}_m}\\right)$ is composed of its downlink licensed bandwidth allocation ${ \\boldsymbol{d}_m}$ and uplink licensed bandwidth allocation ${ \\boldsymbol{v}_m}$. ${\\boldsymbol{a}} = \\left( {{\\boldsymbol{a}_{1}},{\\boldsymbol{a}_{2}}, \\ldots ,{\\boldsymbol{a}_{N_b}}} \\right) \\in \\mathcal{A}$, represents the action profile of all players where $N_b=N_s+1$, expresses the number of BSs including one MBS and $N_s$ SBSs, and $ \\mathcal{A} = \\prod\\nolimits_{n \\in N_b} {{\\mathcal{A}_{n}}}$. \n\n\nTo maximize the downlink and uplink rates simultaneously while maintaining load balancing, for each SBS $n$, the utility function needs to capture both the sum data rate and load balancing. Here, load balancing implies that each BS will balance its spectrum allocation between users, while taking into account their capacity. Therefore, we define a utility function $u_{n}$ for SBS $n$ as follows:\n\n", "itemtype": "equation", "pos": 18872, "prevtext": "\n\n", "index": 29, "text": "\\begin{equation}\\label{eq:c3}\nd_{jn},v_{nj},\\kappa_{nj}, \\tau_{jn} \\in \\mathcal{Z}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"d_{jn},v_{nj},\\kappa_{nj},\\tau_{jn}\\in\\mathcal{Z}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>d</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>v</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>\u03ba</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>\u03c4</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $\\boldsymbol{a}_{-n}$ denotes the action profile of all the BSs other than SBS $n$ and $\\eta$ is a scaling factor that adjusts the number of users to use the unlicensed band in the downlink due to the high outage probability on the unlicensed band. For example, for $\\eta=0.5$, the downlink rates of users on the unlicensed band decreased by half, which, in turn, will decrease the value of utility function, which will then require the SBSs to change their other spectrum allocation schemes in a way to achieve a higher value of utility function. Since the MBS only has the licensed spectrum to allocate to users, the utility function $u_{m}$ for the MBS can be expressed by: \n\n", "itemtype": "equation", "pos": 20230, "prevtext": "\nwhere $\\mathcal{Z} = \\left\\{ {{1}, \\ldots ,{Z}} \\right\\}$ is a finite set of $Z$ level fractions of spectrum. For example, one can separate the fractions of spectrum into $Z=10$ equally spaced intervals. Then, we will have $\\mathcal{Z} = \\left\\{ {{0}, {0.1}, \\ldots ,{0.9}, {1}} \\right\\}$. For the MBS, each action ${\\boldsymbol{a}_m} = \\left( {\\boldsymbol{d}_m},{\\boldsymbol{v}_m}\\right)$ is composed of its downlink licensed bandwidth allocation ${ \\boldsymbol{d}_m}$ and uplink licensed bandwidth allocation ${ \\boldsymbol{v}_m}$. ${\\boldsymbol{a}} = \\left( {{\\boldsymbol{a}_{1}},{\\boldsymbol{a}_{2}}, \\ldots ,{\\boldsymbol{a}_{N_b}}} \\right) \\in \\mathcal{A}$, represents the action profile of all players where $N_b=N_s+1$, expresses the number of BSs including one MBS and $N_s$ SBSs, and $ \\mathcal{A} = \\prod\\nolimits_{n \\in N_b} {{\\mathcal{A}_{n}}}$. \n\n\nTo maximize the downlink and uplink rates simultaneously while maintaining load balancing, for each SBS $n$, the utility function needs to capture both the sum data rate and load balancing. Here, load balancing implies that each BS will balance its spectrum allocation between users, while taking into account their capacity. Therefore, we define a utility function $u_{n}$ for SBS $n$ as follows:\n\n", "index": 31, "text": "\\begin{equation}\\label{eq:un}\n\\begin{split}\nu_{n} \\left(\\boldsymbol{a}_{n}, {\\boldsymbol{a}_{-n}}\\right) =&\\sum\\limits_{n = 1}^{K_j} {\\log _2\\left(1+d_{nj}c_{lnj}^{\\textrm{DL}}+\\eta \\kappa_{nj}c_{unj}^{\\textrm{DL}}\\right)} \\! +\\sum\\limits_{n = 1}^{K_j} {\\log _2\\left(1+v_{jn}c_{ljn}^{\\textrm{UL}}+\\tau_{jn}c_{ujn}^{\\textrm{UL}}\\right)},\n\\end{split}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle u_{n}\\left(\\boldsymbol{a}_{n},{\\boldsymbol{a}_{-n}}%&#10;\\right)=&amp;\\displaystyle\\sum\\limits_{n=1}^{K_{j}}{\\log_{2}\\left(1+d_{nj}c_{lnj}^%&#10;{\\textrm{DL}}+\\eta\\kappa_{nj}c_{unj}^{\\textrm{DL}}\\right)}\\!+\\sum\\limits_{n=1}%&#10;^{K_{j}}{\\log_{2}\\left(1+v_{jn}c_{ljn}^{\\textrm{UL}}+\\tau_{jn}c_{ujn}^{\\textrm%&#10;{UL}}\\right)},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mi/></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>j</mi></msub></munderover><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>d</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mrow><mo>+</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msub><mi>\u03ba</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mrow></mrow><mo rspace=\"0.8pt\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>j</mi></msub></munderover><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow><mtext>UL</mtext></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03c4</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>n</mi></mrow><mtext>UL</mtext></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nNote that, hereinafter, we use the utility function $u_n$ to refer to either the utility function $u_n$ of SBS $n$ or the utility function $u_m$ of the MBS.\n\nGiven a finite set $\\mathcal{A}$, $\\Delta\\left( \\mathcal{A} \\right)$ represents the set of all probability distributions over the elements of the finite set $\\mathcal{A}$. Let ${\\boldsymbol{\\pi} _n}= \\left[ {{\\pi _{n,\\boldsymbol{a}_{1}}}, \\ldots ,{\\pi _{n,\\boldsymbol{a}_{\\left| {{A_n}} \\right|}}}} \\right]$ be a probability distribution using which BS $n$ selects a given action from $\\mathcal{A}_n$. Consequently, ${\\pi _{n,\\boldsymbol{a}_{i}}} = {\\ensuremath{\\operatorname{Pr}}} \\left( {{\\boldsymbol{a}_n} = {\\boldsymbol{a}_{n,i}}} \\right)$ is BS $n$'s mixed strategy where $\\boldsymbol{a}_n$ is the action of BS $n$ adopts. Then, the expected reward that BS $n$ adopts the spectrum allocation scheme $i$ given the mixed strategy   long-term performance metric can be written as follows:\n\n", "itemtype": "equation", "pos": 21279, "prevtext": "\nwhere $\\boldsymbol{a}_{-n}$ denotes the action profile of all the BSs other than SBS $n$ and $\\eta$ is a scaling factor that adjusts the number of users to use the unlicensed band in the downlink due to the high outage probability on the unlicensed band. For example, for $\\eta=0.5$, the downlink rates of users on the unlicensed band decreased by half, which, in turn, will decrease the value of utility function, which will then require the SBSs to change their other spectrum allocation schemes in a way to achieve a higher value of utility function. Since the MBS only has the licensed spectrum to allocate to users, the utility function $u_{m}$ for the MBS can be expressed by: \n\n", "index": 33, "text": "\\begin{equation}\\label{eq:um}\n\\begin{split}\nu_{m} \\left(\\boldsymbol{a}_{m}, {\\boldsymbol{a}_{-m}}\\right) =&\\sum\\limits_{n = 1}^{K_m} {\\log _2\\left(1+d_{mj}c_{lmj}^{\\textrm{DL}}\\right)}+\\sum\\limits_{n = 1}^{K_m} {\\log _2\\left(1+v_{jm}c_{ljm}^{\\textrm{UL}}\\right)}.\n\\end{split}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle u_{m}\\left(\\boldsymbol{a}_{m},{\\boldsymbol{a}_{-m}}%&#10;\\right)=&amp;\\displaystyle\\sum\\limits_{n=1}^{K_{m}}{\\log_{2}\\left(1+d_{mj}c_{lmj}^%&#10;{\\textrm{DL}}\\right)}+\\sum\\limits_{n=1}^{K_{m}}{\\log_{2}\\left(1+v_{jm}c_{ljm}^%&#10;{\\textrm{UL}}\\right)}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mrow><msub><mi>u</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>m</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>m</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mi/></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>m</mi></msub></munderover><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>d</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>j</mi></mrow><mtext>DL</mtext></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>m</mi></msub></munderover><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msub><mi>v</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>m</mi></mrow><mtext>UL</mtext></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere ${\\pi _{ - n,{\\boldsymbol{a}_{ - n}}}} =\\sum\\nolimits_{{\\boldsymbol{a}_n} \\in { \\mathcal{A}_n}} {\\pi \\left( {{\\boldsymbol{a}_n},{\\boldsymbol{a}_{ - n}}} \\right)} $ denotes the marginal probabilities distribution over the action set of BS $n$.   \n\n \\section{Echo State Networks for Self-Organizing Resource Allocation} \nGiven the proposed wireless model of Section \\uppercase\\expandafter{\\romannumeral2}, our next goal is to solve the proposed resource allocation game. To solve the game, our goal is to find the mixed-strategy Nash equilibrium (NE). The mixed NE is formally defined as follows:\n\n\\begin{definition}(Mixed Nash equilibrium): A mixed strategy profile ${\\boldsymbol{\\pi} ^*} = \\left( {\\boldsymbol{\\pi} _1^*, \\ldots ,\\boldsymbol{\\pi} _{N_b}^*} \\right) = \\left( {\\boldsymbol{\\pi} _n^*,\\boldsymbol{\\pi}_{ - n}^*} \\right)$ is a mixed strategy Nash equilibrium if, $\\forall n \\in \\mathcal{B}$ and $\\forall \\boldsymbol{\\pi} _n \\in \\Delta \\left( {{\\mathcal{A}_n}} \\right)$, it holds that:\n\n", "itemtype": "equation", "pos": 22520, "prevtext": "\nNote that, hereinafter, we use the utility function $u_n$ to refer to either the utility function $u_n$ of SBS $n$ or the utility function $u_m$ of the MBS.\n\nGiven a finite set $\\mathcal{A}$, $\\Delta\\left( \\mathcal{A} \\right)$ represents the set of all probability distributions over the elements of the finite set $\\mathcal{A}$. Let ${\\boldsymbol{\\pi} _n}= \\left[ {{\\pi _{n,\\boldsymbol{a}_{1}}}, \\ldots ,{\\pi _{n,\\boldsymbol{a}_{\\left| {{A_n}} \\right|}}}} \\right]$ be a probability distribution using which BS $n$ selects a given action from $\\mathcal{A}_n$. Consequently, ${\\pi _{n,\\boldsymbol{a}_{i}}} = {\\ensuremath{\\operatorname{Pr}}} \\left( {{\\boldsymbol{a}_n} = {\\boldsymbol{a}_{n,i}}} \\right)$ is BS $n$'s mixed strategy where $\\boldsymbol{a}_n$ is the action of BS $n$ adopts. Then, the expected reward that BS $n$ adopts the spectrum allocation scheme $i$ given the mixed strategy   long-term performance metric can be written as follows:\n\n", "index": 35, "text": "\\begin{equation}\n{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right] = \\sum\\limits_{{\\boldsymbol{a}_{ - n}} \\in {\\mathcal{A}_{ - n}}} {{u_n}\\left( {{\\boldsymbol{a}_n},{\\boldsymbol{a}_{ - n}}} \\right){\\pi _{ - n, \\boldsymbol{a}_{ - n}}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}\\left[{{u_{n}}\\left({{\\boldsymbol{a}_{n}}}\\right)}\\right]=\\sum%&#10;\\limits_{{\\boldsymbol{a}_{-n}}\\in{\\mathcal{A}_{-n}}}{{u_{n}}\\left({{%&#10;\\boldsymbol{a}_{n}},{\\boldsymbol{a}_{-n}}}\\right){\\pi_{-n,\\boldsymbol{a}_{-n}}%&#10;}},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></munder><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mrow><mrow><mo>-</mo><mi>n</mi></mrow><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere \n\n", "itemtype": "equation", "pos": 23792, "prevtext": "\nwhere ${\\pi _{ - n,{\\boldsymbol{a}_{ - n}}}} =\\sum\\nolimits_{{\\boldsymbol{a}_n} \\in { \\mathcal{A}_n}} {\\pi \\left( {{\\boldsymbol{a}_n},{\\boldsymbol{a}_{ - n}}} \\right)} $ denotes the marginal probabilities distribution over the action set of BS $n$.   \n\n \\section{Echo State Networks for Self-Organizing Resource Allocation} \nGiven the proposed wireless model of Section \\uppercase\\expandafter{\\romannumeral2}, our next goal is to solve the proposed resource allocation game. To solve the game, our goal is to find the mixed-strategy Nash equilibrium (NE). The mixed NE is formally defined as follows:\n\n\\begin{definition}(Mixed Nash equilibrium): A mixed strategy profile ${\\boldsymbol{\\pi} ^*} = \\left( {\\boldsymbol{\\pi} _1^*, \\ldots ,\\boldsymbol{\\pi} _{N_b}^*} \\right) = \\left( {\\boldsymbol{\\pi} _n^*,\\boldsymbol{\\pi}_{ - n}^*} \\right)$ is a mixed strategy Nash equilibrium if, $\\forall n \\in \\mathcal{B}$ and $\\forall \\boldsymbol{\\pi} _n \\in \\Delta \\left( {{\\mathcal{A}_n}} \\right)$, it holds that:\n\n", "index": 37, "text": "\\begin{equation}\\label{eq:mNE}\n{\\tilde u_n}\\left( {\\boldsymbol{\\pi} _n^*,\\boldsymbol{\\pi} _{ - n}^*} \\right) \\ge {\\tilde u_n}\\left( {{\\boldsymbol{\\pi} _n},\\boldsymbol{\\pi} _{ - n}^*} \\right),\n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"{\\tilde{u}_{n}}\\left({\\boldsymbol{\\pi}_{n}^{*},\\boldsymbol{\\pi}_{-n}^{*}}%&#10;\\right)\\geq{\\tilde{u}_{n}}\\left({{\\boldsymbol{\\pi}_{n}},\\boldsymbol{\\pi}_{-n}^%&#10;{*}}\\right),\\par&#10;\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udf45</mi><mi>n</mi><mo>*</mo></msubsup><mo>,</mo><msubsup><mi>\ud835\udf45</mi><mrow><mo>-</mo><mi>n</mi></mrow><mo>*</mo></msubsup><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><msub><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udf45</mi><mi>n</mi></msub><mo>,</mo><msubsup><mi>\ud835\udf45</mi><mrow><mo>-</mo><mi>n</mi></mrow><mo>*</mo></msubsup><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nis the expected utility of BS $n$ when selecting the mixed strategy $\\boldsymbol{\\pi} _n$.  \n\\end{definition}\n\nFor our game, the mixed NE represents each BS maximizes its data rate and balances the licensed and unlicensed spectrum between users. \nHowever, in a dense SCN, each SBS may not know the entire users' states information including interference, location, and path loss, which makes it challenging to solve the proposed game in the presence of limited information. To find the mixed NE, we need to develop a novel learning algorithm based on the powerful framework of \\emph{echo state networks}.\n\nESNs are novel kind of recurrent neural networks \\cite{26,27,28} that are easy to train and can track the state of a network over time. Learning algorithms based on ESNs can learn to mimic a target system with arbitrary accuracy and can automatically adapt spectrum allocation to the change of network states. Consequently, ESNs are promising candidates for solving wireless resource allocation games in SCNs in which each SBS can use an ESN approach to simulate the users' states, estimate the value of the aggregate utility function, and find the mixed strategy NE of the game. Here, finding the mixed strategy NE of the proposed game refers to the process of allocating appropriate bandwidth on the licensed band and time slots on the unlicensed band to each user, simultaneously. \n\nCompared to traditional RL approaches such as Q-learning \\cite{30}, an approach based on the ESNs can quickly learn the resource allocation parameter without requiring significant training data and it has the ability to adapt the optimal spectrum allocation scheme over time, due to the use of recurrent neural network concepts. \n\nIn order to find the mixed strategy NE of the proposed game, we begin by describing how to use ESNs for learning resource allocation parameters. Then, we propose an ESN-based approach to find the mixed strategy NE. Finally, we prove the convergence of the proposed learning algorithm with different learning rules. \n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=16cm]{newfigure1}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig6} An SCN using ESNs with uplink-downlink decoupling in LTE-U. The left hand shows the proposed ESNs learning algorithm running in SBS. The right hand shows the architecture of ESNs.}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}\n\n\\subsection{ESNs formulation}\nAs shown in Fig. 1, the two ESNs are used in our proposed algorithm. Each such algorithm consists of four components: a) agents, b) inputs, c) actions, and d) reward function. The agents are the players who using ESNs learning algorithm in our proposed game. The inputs, actions, and reward functions are used to find the mixed strategy NE. In our proposed algorithm, the first ESN is proposed to approximate the utility function of BSs and the second ESN is used to choose the optimal spectrum allocation action. Thus, they have different inputs and reward functions but the same agents and actions. Here, the reward function of each ESN that captures the gain of the spectrum allocation scheme to the users of each SBS has the similar form to the utility function in (\\ref{eq:un})-(\\ref{eq:um}). Hereinafter, we use ESNs $\\alpha$ and ESNs $\\beta$ to refer to the first ESNs and the second ESNs, respectively. The ESN models are thus defined as follows: \n\n$\\bullet$ \\textbf{Agents}: The agents are the BSs in $\\mathcal{B}$.  \n\n$\\bullet$ \\textbf{Inputs:} $\\boldsymbol{x}_{t,n}^\\alpha=\\left[ {\\boldsymbol{a}_{1}\\left(t\\right), \\cdots, \\boldsymbol{a}_{n-1}\\left(t\\right), \\boldsymbol{a}_{n+1}\\left(t\\right), \\cdots, \\boldsymbol{a}_{N_b}\\left(t\\right) } \\right]^{\\mathrm{T}}$ which represents the state of network at time $t$, $\\boldsymbol{a}_{i}\\left(t\\right)$ is the spectrum allocation scheme that BS $i$ adopts at time $t$.  \n\n$\\boldsymbol{x}_{t,n}^\\beta=\\left[ {x_{t,n1}^\\textrm{DL}, \\cdots , x_{t,K_nj}^\\textrm{DL}, x_{t,n1}^\\textrm{UL}, \\cdots , x_{t,K_nj}^\\textrm{UL}} \\right]^{\\mathrm{T}}$ which represents the association of the LTE-U users of BS $n$ at time $t$ ( i.e., $x_{t,ni}^\\textrm{DL}=1$ when user $i$ connects to BS $n$ in the downlink). Actually, at each time, all the users send the connection requests to all BSs. Therefore, inputs only have the request state and stable state, i.e., $\\left[1, \\dots, 1\\right]$ and $\\left[1, \\dots, 0\\right]$. \n\n$\\bullet$ \\textbf{Actions:} For each SBS $n$, it can only adopts one spectrum allocation action at time $t$, i.e., ${{\\boldsymbol{a}_n}\\left( t \\right) = {\\boldsymbol{a}_{n,i}}}$, where ${\\boldsymbol{a}_{n,i}} \\in {\\mathcal{A}_n}$. Therefore, ${\\boldsymbol{a}_{n,i}}$ is specified as follows:\n\n", "itemtype": "equation", "pos": 24007, "prevtext": "\nwhere \n\n", "index": 39, "text": "\\begin{equation}\n{\\tilde u_n}\\left( {{\\boldsymbol{\\pi} _n},{\\boldsymbol{\\pi} _{ - n}}} \\right) =\\sum\\limits_{{\\boldsymbol{a}} \\in {\\mathcal{A}}} {{u_n}\\left( {{\\boldsymbol{a}}} \\right)\\prod\\limits_{n \\in \\mathcal{B}} {{\\pi _{n,{a_n}}}}} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"{\\tilde{u}_{n}}\\left({{\\boldsymbol{\\pi}_{n}},{\\boldsymbol{\\pi}_{-n}}}\\right)=%&#10;\\sum\\limits_{{\\boldsymbol{a}}\\in{\\mathcal{A}}}{{u_{n}}\\left({{\\boldsymbol{a}}}%&#10;\\right)\\prod\\limits_{n\\in\\mathcal{B}}{{\\pi_{n,{a_{n}}}}}}\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udf45</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udf45</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc82</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></munder><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>\ud835\udc82</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>n</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi></mrow></munder><msub><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>a</mi><mi>n</mi></msub></mrow></msub></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $d_{j,n}^{i}$ is the fraction of the downlink licensed band that SBS $n$ allocates to user $j$ adopting the spectrum allocation scheme $i$, $v_{n,j}^{i}$ denotes the fraction of the uplink licensed band that SBS $n$ allocates to user $j$ adopting spectrum allocation scheme $i$, $\\kappa_{j,n}^{i}$ and $\\tau_{n,j}^{i}$ respectively, denote the fractions of the unlicensed band that SBS $n$ allocates to user $j$ in the downlink and uplink. We consider the case in which each user can access the licensed and unlicensed bands at the same time. The licensed bandwidths and time slots on the unlicensed band must satisfy (\\ref{eq:c1})-(\\ref{eq:c3}). Thus, ${\\boldsymbol{a}_{n,i}}$ represents SBS $n$ adopting spectrums allocation action $i$ to the users. Since the MBS only has the licensed spectrum to allocate to users, the action of the MBS in ${\\mathcal{A}_n}$ is given by:\n\n", "itemtype": "equation", "pos": 28953, "prevtext": "\nis the expected utility of BS $n$ when selecting the mixed strategy $\\boldsymbol{\\pi} _n$.  \n\\end{definition}\n\nFor our game, the mixed NE represents each BS maximizes its data rate and balances the licensed and unlicensed spectrum between users. \nHowever, in a dense SCN, each SBS may not know the entire users' states information including interference, location, and path loss, which makes it challenging to solve the proposed game in the presence of limited information. To find the mixed NE, we need to develop a novel learning algorithm based on the powerful framework of \\emph{echo state networks}.\n\nESNs are novel kind of recurrent neural networks \\cite{26,27,28} that are easy to train and can track the state of a network over time. Learning algorithms based on ESNs can learn to mimic a target system with arbitrary accuracy and can automatically adapt spectrum allocation to the change of network states. Consequently, ESNs are promising candidates for solving wireless resource allocation games in SCNs in which each SBS can use an ESN approach to simulate the users' states, estimate the value of the aggregate utility function, and find the mixed strategy NE of the game. Here, finding the mixed strategy NE of the proposed game refers to the process of allocating appropriate bandwidth on the licensed band and time slots on the unlicensed band to each user, simultaneously. \n\nCompared to traditional RL approaches such as Q-learning \\cite{30}, an approach based on the ESNs can quickly learn the resource allocation parameter without requiring significant training data and it has the ability to adapt the optimal spectrum allocation scheme over time, due to the use of recurrent neural network concepts. \n\nIn order to find the mixed strategy NE of the proposed game, we begin by describing how to use ESNs for learning resource allocation parameters. Then, we propose an ESN-based approach to find the mixed strategy NE. Finally, we prove the convergence of the proposed learning algorithm with different learning rules. \n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=16cm]{newfigure1}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig6} An SCN using ESNs with uplink-downlink decoupling in LTE-U. The left hand shows the proposed ESNs learning algorithm running in SBS. The right hand shows the architecture of ESNs.}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}\n\n\\subsection{ESNs formulation}\nAs shown in Fig. 1, the two ESNs are used in our proposed algorithm. Each such algorithm consists of four components: a) agents, b) inputs, c) actions, and d) reward function. The agents are the players who using ESNs learning algorithm in our proposed game. The inputs, actions, and reward functions are used to find the mixed strategy NE. In our proposed algorithm, the first ESN is proposed to approximate the utility function of BSs and the second ESN is used to choose the optimal spectrum allocation action. Thus, they have different inputs and reward functions but the same agents and actions. Here, the reward function of each ESN that captures the gain of the spectrum allocation scheme to the users of each SBS has the similar form to the utility function in (\\ref{eq:un})-(\\ref{eq:um}). Hereinafter, we use ESNs $\\alpha$ and ESNs $\\beta$ to refer to the first ESNs and the second ESNs, respectively. The ESN models are thus defined as follows: \n\n$\\bullet$ \\textbf{Agents}: The agents are the BSs in $\\mathcal{B}$.  \n\n$\\bullet$ \\textbf{Inputs:} $\\boldsymbol{x}_{t,n}^\\alpha=\\left[ {\\boldsymbol{a}_{1}\\left(t\\right), \\cdots, \\boldsymbol{a}_{n-1}\\left(t\\right), \\boldsymbol{a}_{n+1}\\left(t\\right), \\cdots, \\boldsymbol{a}_{N_b}\\left(t\\right) } \\right]^{\\mathrm{T}}$ which represents the state of network at time $t$, $\\boldsymbol{a}_{i}\\left(t\\right)$ is the spectrum allocation scheme that BS $i$ adopts at time $t$.  \n\n$\\boldsymbol{x}_{t,n}^\\beta=\\left[ {x_{t,n1}^\\textrm{DL}, \\cdots , x_{t,K_nj}^\\textrm{DL}, x_{t,n1}^\\textrm{UL}, \\cdots , x_{t,K_nj}^\\textrm{UL}} \\right]^{\\mathrm{T}}$ which represents the association of the LTE-U users of BS $n$ at time $t$ ( i.e., $x_{t,ni}^\\textrm{DL}=1$ when user $i$ connects to BS $n$ in the downlink). Actually, at each time, all the users send the connection requests to all BSs. Therefore, inputs only have the request state and stable state, i.e., $\\left[1, \\dots, 1\\right]$ and $\\left[1, \\dots, 0\\right]$. \n\n$\\bullet$ \\textbf{Actions:} For each SBS $n$, it can only adopts one spectrum allocation action at time $t$, i.e., ${{\\boldsymbol{a}_n}\\left( t \\right) = {\\boldsymbol{a}_{n,i}}}$, where ${\\boldsymbol{a}_{n,i}} \\in {\\mathcal{A}_n}$. Therefore, ${\\boldsymbol{a}_{n,i}}$ is specified as follows:\n\n", "index": 41, "text": "\\begin{equation}\n\\boldsymbol{a}_{n,i}= {\\left[ {\\begin{array}{*{20}{c}}\n{d_{1,n}^{i} \\cdots d_{{K_n,n}}^{i}, v_{n,1}^{i} \\cdots v_{{n,K_n}}^{i}}\\\\\n{\\kappa_{1,n}^{i} \\cdots \\kappa_{{K_n,n}}^{i}, \\tau_{n,1}^{i} \\cdots \\tau_{{n,K_n}}^{i}}\n\\end{array}} \\right]^\\mathrm{T}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\boldsymbol{a}_{n,i}={\\left[{\\begin{array}[]{*{20}{c}}{d_{1,n}^{i}\\cdots d_{{K%&#10;_{n},n}}^{i},v_{n,1}^{i}\\cdots v_{{n,K_{n}}}^{i}}\\\\&#10;{\\kappa_{1,n}^{i}\\cdots\\kappa_{{K_{n},n}}^{i},\\tau_{n,1}^{i}\\cdots\\tau_{{n,K_{%&#10;n}}}^{i}}\\end{array}}\\right]^{\\mathrm{T}}},\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><msup><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><msubsup><mi>d</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>d</mi><mrow><msub><mi>K</mi><mi>n</mi></msub><mo>,</mo><mi>n</mi></mrow><mi>i</mi></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>v</mi><mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>v</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>K</mi><mi>n</mi></msub></mrow><mi>i</mi></msubsup></mrow></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><msubsup><mi>\u03ba</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>\u03ba</mi><mrow><msub><mi>K</mi><mi>n</mi></msub><mo>,</mo><mi>n</mi></mrow><mi>i</mi></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>\u03c4</mi><mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>\u03c4</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>K</mi><mi>n</mi></msub></mrow><mi>i</mi></msubsup></mrow></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable><mo>]</mo></mrow><mi mathvariant=\"normal\">T</mi></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\n\n\n\n$\\bullet$ \\textbf{Reward function:} In our model, the vector of reward functions are the functions to which ESNs approximate. The reward function of ESNs $\\alpha$ is used to store the reward of spectrum allocation action. However, the reward function of ESNs $\\beta$ is used to choose the optimal spectrum allocation action. Therefore, the reward function is given by $\\boldsymbol{r}_{t,n}^\\alpha \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\mathcal{A}_n}\\right)=\\left[r_{t,n}^{\\alpha,1} \\!\\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,1}}\\right), \\cdots , r_{t,n}^{\\alpha, {\\left| {{\\mathcal{A}_n}} \\right|}}\\!\\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,{{\\left| {{\\mathcal{A}_n}} \\right|}}}}\\right)\\! \\right]^{\\mathrm{T}}$. $\\boldsymbol{r}_{t,n}^\\alpha \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\mathcal{A}_n}\\right)$ represents the set of spectrum allocation rewards achieved by spectrum allocation schemes for each BS $n$ at time $t$. According to (5)-(8), constructing the reward function needs to consider the instability of the unlicensed band that the WiFi user would occupy at any time. Therefore, the reward function of action $i$ on SBS $n$ is given by:\n\n", "itemtype": "equation", "pos": 30119, "prevtext": "\nwhere $d_{j,n}^{i}$ is the fraction of the downlink licensed band that SBS $n$ allocates to user $j$ adopting the spectrum allocation scheme $i$, $v_{n,j}^{i}$ denotes the fraction of the uplink licensed band that SBS $n$ allocates to user $j$ adopting spectrum allocation scheme $i$, $\\kappa_{j,n}^{i}$ and $\\tau_{n,j}^{i}$ respectively, denote the fractions of the unlicensed band that SBS $n$ allocates to user $j$ in the downlink and uplink. We consider the case in which each user can access the licensed and unlicensed bands at the same time. The licensed bandwidths and time slots on the unlicensed band must satisfy (\\ref{eq:c1})-(\\ref{eq:c3}). Thus, ${\\boldsymbol{a}_{n,i}}$ represents SBS $n$ adopting spectrums allocation action $i$ to the users. Since the MBS only has the licensed spectrum to allocate to users, the action of the MBS in ${\\mathcal{A}_n}$ is given by:\n\n", "index": 43, "text": "\\begin{equation}\n\n\n\\boldsymbol{a}_{m,i}= {\\left[ {\\begin{array}{*{20}{c}}\n{d_{1,m}^{i} \\cdots d_{{K_m,m}}^{i}, v_{m,1}^{i} \\cdots v_{{m,K_m}}^{i}}\\\\\n\\end{array}} \\right]^\\mathrm{T}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\par&#10;\\boldsymbol{a}_{m,i}={\\left[{\\begin{array}[]{*{20}{c}}{d_{1,m}^{i}%&#10;\\cdots d_{{K_{m},m}}^{i},v_{m,1}^{i}\\cdots v_{{m,K_{m}}}^{i}}\\\\&#10;\\end{array}}\\right]^{\\mathrm{T}}}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc82</mi><mrow><mi>m</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><msup><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"center\"><mrow><mrow><msubsup><mi>d</mi><mrow><mn>1</mn><mo>,</mo><mi>m</mi></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>d</mi><mrow><msub><mi>K</mi><mi>m</mi></msub><mo>,</mo><mi>m</mi></mrow><mi>i</mi></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>v</mi><mrow><mi>m</mi><mo>,</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msubsup><mi>v</mi><mrow><mi>m</mi><mo>,</mo><msub><mi>K</mi><mi>m</mi></msub></mrow><mi>i</mi></msubsup></mrow></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable><mo>]</mo></mrow><mi mathvariant=\"normal\">T</mi></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $\\boldsymbol{x}_{t,n}^\\alpha={\\boldsymbol{a}_{-n}}$ expresses the input at time $t$ of ESN $\\alpha$ is the actions that other BSs adopt at time $t$. Since the MBS only has the licensed spectrum to allocate to users, the reward function of the MBS is given by:\n\n\n", "itemtype": "equation", "pos": 31498, "prevtext": "\n\n\n\n\n\n\n\n\n\n\n\n\n$\\bullet$ \\textbf{Reward function:} In our model, the vector of reward functions are the functions to which ESNs approximate. The reward function of ESNs $\\alpha$ is used to store the reward of spectrum allocation action. However, the reward function of ESNs $\\beta$ is used to choose the optimal spectrum allocation action. Therefore, the reward function is given by $\\boldsymbol{r}_{t,n}^\\alpha \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\mathcal{A}_n}\\right)=\\left[r_{t,n}^{\\alpha,1} \\!\\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,1}}\\right), \\cdots , r_{t,n}^{\\alpha, {\\left| {{\\mathcal{A}_n}} \\right|}}\\!\\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,{{\\left| {{\\mathcal{A}_n}} \\right|}}}}\\right)\\! \\right]^{\\mathrm{T}}$. $\\boldsymbol{r}_{t,n}^\\alpha \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\mathcal{A}_n}\\right)$ represents the set of spectrum allocation rewards achieved by spectrum allocation schemes for each BS $n$ at time $t$. According to (5)-(8), constructing the reward function needs to consider the instability of the unlicensed band that the WiFi user would occupy at any time. Therefore, the reward function of action $i$ on SBS $n$ is given by:\n\n", "index": 45, "text": "\\begin{equation}\n\\begin{split}\nr_{t,n}^{\\alpha,i} \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,i}}\\right) =u_{n} \\left(\\boldsymbol{a}_{n,i}, {\\boldsymbol{a}_{-n}}\\right),\n\n\n\n\n\n\\end{split}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle r_{t,n}^{\\alpha,i}\\left(\\boldsymbol{x}_{t,n}^{%&#10;\\alpha},{\\boldsymbol{a}_{n,i}}\\right)=u_{n}\\left(\\boldsymbol{a}_{n,i},{%&#10;\\boldsymbol{a}_{-n}}\\right),\\par&#10;\\par&#10;\\par&#10;\\par&#10;\\par&#10;\\end{split}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>\u03b1</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b1</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\nThe reward function in the ESNs $\\beta$ is used to choose the optimal spectrum allocation action based on the expected reward under the actions that other BSs adopt. It can be expressed by:\n\n", "itemtype": "equation", "pos": 31978, "prevtext": "\nwhere $\\boldsymbol{x}_{t,n}^\\alpha={\\boldsymbol{a}_{-n}}$ expresses the input at time $t$ of ESN $\\alpha$ is the actions that other BSs adopt at time $t$. Since the MBS only has the licensed spectrum to allocate to users, the reward function of the MBS is given by:\n\n\n", "index": 47, "text": "\\begin{equation}\n\nr_{t,m}^{\\alpha,i} \\left(\\boldsymbol{x}_{t,m}^\\alpha, {\\boldsymbol{a}_{m,i}}\\right) =u_{m} \\left(\\boldsymbol{a}_{m}, {\\boldsymbol{a}_{-m}}\\right).\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\par&#10; r_{t,m}^{\\alpha,i}\\left(\\boldsymbol{x}_{t,m}^{\\alpha},{\\boldsymbol{a}_{m%&#10;,i}}\\right)=u_{m}\\left(\\boldsymbol{a}_{m},{\\boldsymbol{a}_{-m}}\\right).\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>m</mi></mrow><mrow><mi>\u03b1</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>m</mi></mrow><mi>\u03b1</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>m</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>u</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>m</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>m</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere (a) is obtained from the fact that the input of the first ESNs $\\boldsymbol{x}_{t,n}^\\alpha$ denotes the actions that all BSs take at time $t$ other than BS $n$, i.e., $\\boldsymbol{x}_{t,n}^\\alpha=\\boldsymbol{a}_{t,-n}$.\n\n\n\\subsection{ESNs Update}\nIn this subsection, we first introduce the ESNs update that each BS $n$ uses to store and estimate the reward of each spectrum allocation scheme. Then, we represent the proposed ESN-based approach that each BS $n$ uses to choose the optimal spectrum allocation scheme.  \nAs shown in Fig. 1, the internal structure of ESNs for BS $n$ consists of three components: a) input weight matrix $\\boldsymbol{W}_n^{in}$, b) recurrent matrix $\\boldsymbol{W}_n$, and c) output weight matrix $\\boldsymbol{W}_n^{out}$. Given these basic definitions, for each BS $n$, an ESN model is essentially a dynamic neural network, known as the dynamic reservoir, which will be combined with the input $\\boldsymbol{x}_{t,n}$ representing network state. Therefore, we first explain how an ESN model can be generated. Mathematically, the dynamic reservoir consists of the input weight matrix $\\boldsymbol{W}_n^{in} \\in {\\mathbb{R}^{N \\times 2K_n}}$, and the recurrent matrix $\\boldsymbol{W}_n \\in {\\mathbb{R}^{N \\times N}}$, where $N$ is the number of units of the dynamic reservoir that each BS $n$ uses to store the users' states. The output weight matrix $\\boldsymbol{W}_n^{out} \\in {\\mathbb{R}^{{\\left| {{\\mathcal{A}_n}} \\right|} \\times \\left(N+2K_j\\right)}}$ is the linear readout weights which is trained to approximate the reward function of each BS which essentially reflects the rate achieved by that BS. The dynamic reservoir of BS $n$ is therefore given by the pair $\\left( \\boldsymbol{W}_n^{in}, \\boldsymbol{W}_n \\right)$ which is initially generated randomly by uniform distribution and $\\boldsymbol{W}_n$ is defined as a sparse matrix with a spectral radius less than one \\cite{27}. $\\boldsymbol{W}_n^{out}$ is also initialized randomly via a uniform distribution. In this ESN model, one needs to only train $\\boldsymbol{W}_n^{out}$ to approximate the reward function which illustrates that ESNs are easy to train \\cite{26,27,28}. Even though the dynamic reservoir is initially generated randomly, it will be combined with input to store the users' states and it will also be combined with the trained output matrix to approximate the reward function.\n\nSince the users' associations change depending on the spectrum allocation scheme that each SBS adopts, the ESN model of each BS $n$ needs to update its input $\\boldsymbol{x}_{t,n}$ and store the users' states, which is done by the dynamic reservoir state ${\\boldsymbol{\\mu  }_{t,n}}$. Here, ${\\boldsymbol{\\mu}_{t,n}}$ denotes the users' association results and the users' states for each BS $n$ at time $t$. The dynamic reservoir state for each BS $n$ can be computed as follows:\n\n", "itemtype": "equation", "pos": 32350, "prevtext": "\n\nThe reward function in the ESNs $\\beta$ is used to choose the optimal spectrum allocation action based on the expected reward under the actions that other BSs adopt. It can be expressed by:\n\n", "index": 49, "text": "\\begin{equation}\n\\begin{split}\nr_{t,n}^{\\beta,i}\\left( {{\\boldsymbol{x}_{t,n}^\\beta},{\\boldsymbol{a}_{n,i}}} \\right) &=\\sum\\limits_{{\\boldsymbol{x}_{t,n}^\\alpha} \\in {A_{ - n}}} {r_{t,n}^{\\alpha,i} \\left(\\boldsymbol{x}_{t,n}^\\alpha, {\\boldsymbol{a}_{n,i}}\\right) {\\pi _{ - n, \\boldsymbol{x}_{t,n}^\\alpha}}}\\\\\n&\\mathop  = \\limits^{\\left( a \\right)} \\sum\\limits_{{\\boldsymbol{a}_{ - n}} \\in {\\mathcal{A}_{ - n}}} {{r_{t,n}^{\\alpha,i} }\\left( {{\\boldsymbol{a}_{n,i}},{\\boldsymbol{a}_{ - n}}} \\right){\\pi _{ - n, \\boldsymbol{a}_{ - n}}}}\\\\\n&={\\mathbb{E}}\\left[ {{r_{t,n}^{\\alpha,i} }\\left( {{\\boldsymbol{a}_{n,i}}} \\right)} \\right],\\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle r_{t,n}^{\\beta,i}\\left({{\\boldsymbol{x}_{t,n}^{%&#10;\\beta}},{\\boldsymbol{a}_{n,i}}}\\right)&amp;\\displaystyle=\\sum\\limits_{{\\boldsymbol%&#10;{x}_{t,n}^{\\alpha}}\\in{A_{-n}}}{r_{t,n}^{\\alpha,i}\\left(\\boldsymbol{x}_{t,n}^{%&#10;\\alpha},{\\boldsymbol{a}_{n,i}}\\right){\\pi_{-n,\\boldsymbol{x}_{t,n}^{\\alpha}}}}%&#10;\\\\&#10;&amp;\\displaystyle\\mathop{=}\\limits^{\\left(a\\right)}\\sum\\limits_{{\\boldsymbol{a}_{%&#10;-n}}\\in{\\mathcal{A}_{-n}}}{{r_{t,n}^{\\alpha,i}}\\left({{\\boldsymbol{a}_{n,i}},{%&#10;\\boldsymbol{a}_{-n}}}\\right){\\pi_{-n,\\boldsymbol{a}_{-n}}}}\\\\&#10;&amp;\\displaystyle={\\mathbb{E}}\\left[{{r_{t,n}^{\\alpha,i}}\\left({{\\boldsymbol{a}_{%&#10;n,i}}}\\right)}\\right],\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>\u03b2</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b2</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b1</mi></msubsup><mo>\u2208</mo><msub><mi>A</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></munder><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>\u03b1</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b1</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mrow><mrow><mo>-</mo><mi>n</mi></mrow><mo>,</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b1</mi></msubsup></mrow></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mover><mo movablelimits=\"false\">=</mo><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></munder><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>\u03b1</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c0</mi><mrow><mrow><mo>-</mo><mi>n</mi></mrow><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>\u03b1</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $f\\left(  \\cdot  \\right)$ is the tanh function and $j \\in \\left\\{ {\\alpha ,\\beta } \\right\\}$. Suppose that, each BS $n$, has ${{\\left| {{\\mathcal{A}_n}} \\right|}}$ spectrum allocation actions, $\\boldsymbol{a}_{n,1}, \\ldots, {\\boldsymbol{a}_{n,{{\\left| {{\\mathcal{A}_n}} \\right|}} }}$, to choose from. Then, the ESNs will have ${{\\left| {{\\mathcal{A}_n}} \\right|}} $ outputs, one corresponding to each one of those actions. We must train the output matrix $\\boldsymbol{W}_n^{j,out}$, so that the output $i$ yields the value of the reward function $r_{t,n}^{j,i} \\left(\\boldsymbol{x}_{t,n}^j, {\\boldsymbol{a}_{n,i}}\\right)$ due to action ${\\boldsymbol{a}_{n,i}}$ in the input $\\boldsymbol{x}_{t,n}^j$:\n\n", "itemtype": "equation", "pos": 35882, "prevtext": "\nwhere (a) is obtained from the fact that the input of the first ESNs $\\boldsymbol{x}_{t,n}^\\alpha$ denotes the actions that all BSs take at time $t$ other than BS $n$, i.e., $\\boldsymbol{x}_{t,n}^\\alpha=\\boldsymbol{a}_{t,-n}$.\n\n\n\\subsection{ESNs Update}\nIn this subsection, we first introduce the ESNs update that each BS $n$ uses to store and estimate the reward of each spectrum allocation scheme. Then, we represent the proposed ESN-based approach that each BS $n$ uses to choose the optimal spectrum allocation scheme.  \nAs shown in Fig. 1, the internal structure of ESNs for BS $n$ consists of three components: a) input weight matrix $\\boldsymbol{W}_n^{in}$, b) recurrent matrix $\\boldsymbol{W}_n$, and c) output weight matrix $\\boldsymbol{W}_n^{out}$. Given these basic definitions, for each BS $n$, an ESN model is essentially a dynamic neural network, known as the dynamic reservoir, which will be combined with the input $\\boldsymbol{x}_{t,n}$ representing network state. Therefore, we first explain how an ESN model can be generated. Mathematically, the dynamic reservoir consists of the input weight matrix $\\boldsymbol{W}_n^{in} \\in {\\mathbb{R}^{N \\times 2K_n}}$, and the recurrent matrix $\\boldsymbol{W}_n \\in {\\mathbb{R}^{N \\times N}}$, where $N$ is the number of units of the dynamic reservoir that each BS $n$ uses to store the users' states. The output weight matrix $\\boldsymbol{W}_n^{out} \\in {\\mathbb{R}^{{\\left| {{\\mathcal{A}_n}} \\right|} \\times \\left(N+2K_j\\right)}}$ is the linear readout weights which is trained to approximate the reward function of each BS which essentially reflects the rate achieved by that BS. The dynamic reservoir of BS $n$ is therefore given by the pair $\\left( \\boldsymbol{W}_n^{in}, \\boldsymbol{W}_n \\right)$ which is initially generated randomly by uniform distribution and $\\boldsymbol{W}_n$ is defined as a sparse matrix with a spectral radius less than one \\cite{27}. $\\boldsymbol{W}_n^{out}$ is also initialized randomly via a uniform distribution. In this ESN model, one needs to only train $\\boldsymbol{W}_n^{out}$ to approximate the reward function which illustrates that ESNs are easy to train \\cite{26,27,28}. Even though the dynamic reservoir is initially generated randomly, it will be combined with input to store the users' states and it will also be combined with the trained output matrix to approximate the reward function.\n\nSince the users' associations change depending on the spectrum allocation scheme that each SBS adopts, the ESN model of each BS $n$ needs to update its input $\\boldsymbol{x}_{t,n}$ and store the users' states, which is done by the dynamic reservoir state ${\\boldsymbol{\\mu  }_{t,n}}$. Here, ${\\boldsymbol{\\mu}_{t,n}}$ denotes the users' association results and the users' states for each BS $n$ at time $t$. The dynamic reservoir state for each BS $n$ can be computed as follows:\n\n", "index": 51, "text": "\\begin{equation}\\label{eq:utn}\n{\\boldsymbol{\\mu}_{t,n}^j} ={\\mathop{f}\\nolimits}\\!\\left( {\\boldsymbol{W}_n^j{\\boldsymbol{\\mu}_{t - 1,n}^j} + \\boldsymbol{W}_n^{j,in}{\\boldsymbol{x}_{t,n}^j}} \\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mu}_{t,n}^{j}}={\\mathop{f}\\nolimits}\\!\\left({\\boldsymbol{W}_{n}^%&#10;{j}{\\boldsymbol{\\mu}_{t-1,n}^{j}}+\\boldsymbol{W}_{n}^{j,in}{\\boldsymbol{x}_{t,%&#10;n}^{j}}}\\right),\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udf41</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>=</mo><mrow><mo rspace=\"0.8pt\">\ud835\udc53</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\ud835\udc7e</mi><mi>n</mi><mi>j</mi></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udf41</mi><mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $\\boldsymbol{W}_{t,in}^{j,out}$ denotes the $i$th row of $\\boldsymbol{W}_{t,n}^{j,out}$. (\\ref{eq:rtn}) is used to estimate the reward of each BS $n$ that adopts any spectrum allocation action after training $\\boldsymbol{W}_n^{j,out}$. To train $\\boldsymbol{W}_n^{j,out}$, a linear gradient descent approach can be used to derive the following update rule:\n\n", "itemtype": "equation", "pos": 36802, "prevtext": "\nwhere $f\\left(  \\cdot  \\right)$ is the tanh function and $j \\in \\left\\{ {\\alpha ,\\beta } \\right\\}$. Suppose that, each BS $n$, has ${{\\left| {{\\mathcal{A}_n}} \\right|}}$ spectrum allocation actions, $\\boldsymbol{a}_{n,1}, \\ldots, {\\boldsymbol{a}_{n,{{\\left| {{\\mathcal{A}_n}} \\right|}} }}$, to choose from. Then, the ESNs will have ${{\\left| {{\\mathcal{A}_n}} \\right|}} $ outputs, one corresponding to each one of those actions. We must train the output matrix $\\boldsymbol{W}_n^{j,out}$, so that the output $i$ yields the value of the reward function $r_{t,n}^{j,i} \\left(\\boldsymbol{x}_{t,n}^j, {\\boldsymbol{a}_{n,i}}\\right)$ due to action ${\\boldsymbol{a}_{n,i}}$ in the input $\\boldsymbol{x}_{t,n}^j$:\n\n", "index": 53, "text": "\\begin{equation}\\label{eq:rtn}\nr_{t,n}^{j,i} \\left(\\boldsymbol{x}_{t,n}^j, {\\boldsymbol{a}_{n,i}}\\right) = {\\boldsymbol{W}_{t,in}^{j,out}}\\left[ {{\\boldsymbol{\\mu}_{t,n}^j};{\\boldsymbol{x}_{t,n}^j}} \\right],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"r_{t,n}^{j,i}\\left(\\boldsymbol{x}_{t,n}^{j},{\\boldsymbol{a}_{n,i}}\\right)={%&#10;\\boldsymbol{W}_{t,in}^{j,out}}\\left[{{\\boldsymbol{\\mu}_{t,n}^{j}};{\\boldsymbol%&#10;{x}_{t,n}^{j}}}\\right],\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msubsup><mo>\u2062</mo><mrow><mo>[</mo><msubsup><mi>\ud835\udf41</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>;</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere ${\\lambda_j}$ is the learning rate for ESNs $j$ and $e_{t,n}^{j,i}$ is the $j$th actual reward at action $i$ of BS $n$ at time $t$, i.e., $e_{t,n}^{\\alpha,i}=u_n^i\\left( {{\\boldsymbol{a}_{n,i}},{\\boldsymbol{a}_{ - n}}} \\right)$ and $e_{t,n}^{\\beta,i}={\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right] $. Note that ${\\boldsymbol{a}_{ - n}}$ denotes the actions that all BSs other than BS $n$ adopt now.\n\n\\subsection{Reinforcement learning with ESNs algorithm}\n\nTo solve the game, we introduce the proposed ESN-based reinforcement learning approach to find the mixed strategy NE. The proposed ESN-based reinforcement learning approach consists of ESN $\\alpha$ and ESN $\\beta$. ESN $\\alpha$ is used to approximate the utility function of our proposed game. ESN $\\alpha$ stores the reward of utility function at any case which can be used by ESN $\\beta$. ESN $\\beta$ uses the reward stored in ESN $\\alpha$ to find the mixed strategy NE in a reinforcement learning approach. In our proposed algorithm, each SBS $n$ needs to calculate the number of time slots $L$ during which SBSs could allocate the unlicensed spectrum based on (8), update the users' associations and store users' states based on (\\ref{eq:utn}), estimate the rewards of spectrum allocation actions based on (\\ref{eq:rtn}), choose the optimal allocation scheme, and update the output matrix $\\boldsymbol{W}_n^{out}$ based on (\\ref{eq:w}) at each time. \n\nIn order to guarantee that any action always has a non-null probability to play in any case, the $\\varepsilon$-greedy exploration \\cite{30} is adopted in the proposed algorithm. The mechanism is responsible for selecting the actions that the agent will perform during the learning process. Its purpose is to harmonize the tradeoff between exploitation and exploration. Therefore, the probability of BS $n$ playing action $i$ is given by: \n\n", "itemtype": "equation", "pos": 37388, "prevtext": "\nwhere $\\boldsymbol{W}_{t,in}^{j,out}$ denotes the $i$th row of $\\boldsymbol{W}_{t,n}^{j,out}$. (\\ref{eq:rtn}) is used to estimate the reward of each BS $n$ that adopts any spectrum allocation action after training $\\boldsymbol{W}_n^{j,out}$. To train $\\boldsymbol{W}_n^{j,out}$, a linear gradient descent approach can be used to derive the following update rule:\n\n", "index": 55, "text": "\\begin{equation}\\label{eq:w}\n{\\boldsymbol{W}_{t + 1,in}^{j,out}} = {\\boldsymbol{W}_{t,in}^{j,out}} + {\\lambda_j} \\left( {e_{t,n}^{j,i} -r_{t,n}^{j,i} \\left(\\boldsymbol{x}_{t,n}^j, {\\boldsymbol{a}_{n,i}} \\right)} \\right)\\left[ {{\\boldsymbol{\\mu}_{t,n}^j};{\\boldsymbol{x}_{t,n}^j}} \\right]^{\\mathrm{T}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{W}_{t+1,in}^{j,out}}={\\boldsymbol{W}_{t,in}^{j,out}}+{\\lambda_{j}%&#10;}\\left({e_{t,n}^{j,i}-r_{t,n}^{j,i}\\left(\\boldsymbol{x}_{t,n}^{j},{\\boldsymbol%&#10;{a}_{n,i}}\\right)}\\right)\\left[{{\\boldsymbol{\\mu}_{t,n}^{j}};{\\boldsymbol{x}_{%&#10;t,n}^{j}}}\\right]^{\\mathrm{T}},\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msubsup><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>e</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>-</mo><mrow><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>[</mo><msubsup><mi>\ud835\udf41</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>;</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>j</mi></msubsup><mo>]</mo></mrow><mi mathvariant=\"normal\">T</mi></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nThe $\\varepsilon$-greedy mechanism decides the probability distribution of action set for each BS. It can be easy to conclude that the probability distribution of each SBS's action set consists of a large probability of the action that results in the optimal reward and a uniform equal probability for other actions. Thus, based on $\\varepsilon$-greedy mechanism, each BS can obtain the probability distribution of other BSs' action sets by only obtaining the spectrum allocation action that results in the optimal reward.\n\nThe learning rates in ESNs have two different rules: a) fixed value and b) the Robbins-Monro conditions \\cite{26}. The Robbins-Monro conditions can be given by: \n\n", "itemtype": "equation", "pos": 39586, "prevtext": "\nwhere ${\\lambda_j}$ is the learning rate for ESNs $j$ and $e_{t,n}^{j,i}$ is the $j$th actual reward at action $i$ of BS $n$ at time $t$, i.e., $e_{t,n}^{\\alpha,i}=u_n^i\\left( {{\\boldsymbol{a}_{n,i}},{\\boldsymbol{a}_{ - n}}} \\right)$ and $e_{t,n}^{\\beta,i}={\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right] $. Note that ${\\boldsymbol{a}_{ - n}}$ denotes the actions that all BSs other than BS $n$ adopt now.\n\n\\subsection{Reinforcement learning with ESNs algorithm}\n\nTo solve the game, we introduce the proposed ESN-based reinforcement learning approach to find the mixed strategy NE. The proposed ESN-based reinforcement learning approach consists of ESN $\\alpha$ and ESN $\\beta$. ESN $\\alpha$ is used to approximate the utility function of our proposed game. ESN $\\alpha$ stores the reward of utility function at any case which can be used by ESN $\\beta$. ESN $\\beta$ uses the reward stored in ESN $\\alpha$ to find the mixed strategy NE in a reinforcement learning approach. In our proposed algorithm, each SBS $n$ needs to calculate the number of time slots $L$ during which SBSs could allocate the unlicensed spectrum based on (8), update the users' associations and store users' states based on (\\ref{eq:utn}), estimate the rewards of spectrum allocation actions based on (\\ref{eq:rtn}), choose the optimal allocation scheme, and update the output matrix $\\boldsymbol{W}_n^{out}$ based on (\\ref{eq:w}) at each time. \n\nIn order to guarantee that any action always has a non-null probability to play in any case, the $\\varepsilon$-greedy exploration \\cite{30} is adopted in the proposed algorithm. The mechanism is responsible for selecting the actions that the agent will perform during the learning process. Its purpose is to harmonize the tradeoff between exploitation and exploration. Therefore, the probability of BS $n$ playing action $i$ is given by: \n\n", "index": 57, "text": "\\begin{equation}\n{\\ensuremath{\\operatorname{Pr}}} \\left( {{\\boldsymbol{a}_n}\\left( t \\right) = {\\boldsymbol{a}_{n,i}}} \\right) \\!=\\! \\left\\{ {\\begin{array}{*{20}{c}}\n{1 - \\varepsilon  + \\frac{\\varepsilon }{{\\left| {{\\mathcal{A}_n}} \\right|}},\\;\\;{\\boldsymbol{a}_{n,i}} = \\arg\\! \\mathop {\\max }\\limits_{{\\boldsymbol{a}_n} \\in {\\mathcal{A}_n}}\\! {{r}_{t,n}^{\\beta}}\\!\\left( {{\\boldsymbol{x}_{t,n}^\\beta},{\\boldsymbol{a}_n}} \\right),}\\\\\n{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{\\varepsilon }{{\\left| {{\\mathcal{A}_n}} \\right|}},\\;\\;\\;\\text{otherwise}.\\;\\,\\,\\,\\;\\;\\;\\;\\;\\;\\;\\;\\;}\n\\end{array}} \\right. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{Pr}}\\left({{\\boldsymbol{a}_{n}}\\left(t\\right)={\\boldsymbol{a}_{%&#10;n,i}}}\\right)\\!=\\!\\left\\{{\\begin{array}[]{*{20}{c}}{1-\\varepsilon+\\frac{%&#10;\\varepsilon}{{\\left|{{\\mathcal{A}_{n}}}\\right|}},\\;\\;{\\boldsymbol{a}_{n,i}}=%&#10;\\arg\\!\\mathop{\\max}\\limits_{{\\boldsymbol{a}_{n}}\\in{\\mathcal{A}_{n}}}\\!{{r}_{t%&#10;,n}^{\\beta}}\\!\\left({{\\boldsymbol{x}_{t,n}^{\\beta}},{\\boldsymbol{a}_{n}}}%&#10;\\right),}\\\\&#10;{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{\\varepsilon}{{\\left|{{\\mathcal{A}_{n}%&#10;}}\\right|}},\\;\\;\\;\\text{otherwise}.\\;\\,\\,\\,\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}}\\right.\" display=\"block\"><mrow><mrow><mo>Pr</mo><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow><mo>=</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo rspace=\"0.8pt\">)</mo></mrow></mrow><mo rspace=\"0.8pt\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><mrow><mrow><mn>1</mn><mo>-</mo><mi>\u03b5</mi></mrow><mo>+</mo><mfrac><mi>\u03b5</mi><mrow><mo>|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub><mo>|</mo></mrow></mfrac></mrow><mo rspace=\"8.1pt\">,</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>=</mo><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><munder><mo movablelimits=\"false\">max</mo><mrow><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub></mrow></munder></mpadded><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b2</mi></msubsup></mpadded><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc99</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mi>\u03b2</mi></msubsup><mo>,</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mpadded lspace=\"-28.9pt\" width=\"-28.9pt\"><mfrac><mi>\u03b5</mi><mrow><mo>|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub><mo>|</mo></mrow></mfrac></mpadded><mo rspace=\"10.9pt\">,</mo><mtext>otherwise</mtext></mrow><mo>.</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2003\u2005</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $j \\in \\left\\{ {\\alpha ,\\beta } \\right\\}$. The learning rate has an effect on the speed of the convergence of our proposed algorithm. However, the two learning rules are all converged with time increases which we would provide the proof next.   \n\nWe assume that all BSs transmit simultaneously and each user can only connect to one BS in the uplink or downlink at each time. We further assume that each SBS knows the time slots that occupies the unlicensed band $L$. Based on the above formulations, the distributed RL approach based on ESNs performed by every BS $n$ is shown in Algorithm 1. In line 8 of Algorithm 1, we capture the fact that each SBS broadcasts the action that it adopts now and the probability distribution of action profiles to other BSs.\n\n\\begin{algorithm}[!t]\n\\caption{Reinforcement learning with ESNs algorithm }   \n\\label{alg:Framwork}   \n\\begin{algorithmic} [1] \n\\REQUIRE The set of users' association states, $\\boldsymbol{x}_{t,n}^\\alpha$ and $\\boldsymbol{x}_{t,n}^\\beta$;\\\\ \n\n\\ENSURE initialize $\\boldsymbol{W}_n^{\\alpha,in}$, $\\boldsymbol{W}_n^\\alpha$, $\\boldsymbol{W}_n^{\\alpha,out}$, $\\boldsymbol{W}_n^{\\beta,in}$, $\\boldsymbol{W}_n^\\beta$, $\\boldsymbol{W}_n^{\\beta,out}$ $\\boldsymbol{r}_{0,n}^\\alpha=0$, $\\boldsymbol{r}_{0,n}^\\beta=0$  \\\\ \n\n\\STATE calculate the time slots $L$ based on (8)\n\n\\FOR {time $t$} \n\n\\IF{$rand(.) < \\varepsilon$}\n\n\\STATE randomly choose one action \n\n\\ELSE\n\n\\STATE choose action $\\boldsymbol{a}_{n,i}\\left(t\\right) = \\mathop {\\arg \\max }\\limits_{\\boldsymbol{a}_{n,i}\\left(t\\right) } \\left( {r_{t,j}^{\\beta,i}\\left( {{\\boldsymbol{x}_{t,n}^{\\beta}},\\boldsymbol{a}_{n,i}\\left(t\\right)} \\right)} \\right)$ \n\n\\ENDIF \n\n\\STATE broadcast the action $\\boldsymbol{a}_{n,i}\\left(t\\right)$ and the optimal action $\\boldsymbol{a}_{n}^*$ that results in the maximal value of reward function\n\n\\STATE calculate the reward $r^\\alpha$ and $r^\\beta$ based on (\\ref{eq:utn})-(\\ref{eq:rtn}) \n\n\\STATE update the output weight matrix $\\boldsymbol{W}_{t,ij}^{\\alpha,out}$ and $\\boldsymbol{W}_{t,ij}^{\\beta,out}$ based on (\\ref{eq:w})\n\\ENDFOR  \n\\end{algorithmic}\n\\end{algorithm}  \n\nIn essence, at every time instant, every BS allocates its spectrum to the users and maximizes its own rate. The users would send connection request to all BSs at each time. After iterations, each user could get the best rate and each BS maximizes its total rate. Note that the performance of the proposed algorithm can be improved by incorporating a training sequence to update the output weight matrix $\\boldsymbol{W}^{out}$. Adjusting the input weight matrix $\\boldsymbol{W}^{in}$ and recurrent matrix $\\boldsymbol{W}$ appropriately will also improve the accuracy of the algorithm. Algorithm 1 continues to iterate until each user achieves maximal rate and the users' association states remain unchanged. \n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Convergence of the ESNs-based algorithm}\nIn the proposed algorithm, we use ESNs $\\alpha$ to store and estimate the reward of each spectrum allocation scheme. Then, we train ESNs $\\beta$ as RL algorithm to solve the proposed game. Since ESNs have two different learning rules to update the output matrix, in this subsection, we prove the convergence of the two learning phases of our proposed algorithm. We first prove the convergence of ESNs with the Robbins-Monro learning rule. Next, we use the continuous time version to prove the convergence of ESNs with the fixed value learning rule. Finally, we prove the proposed algorithm reaches to the mixed strategy NE.\n\n\\begin{theorem} ESN $\\alpha$ and ESN $\\beta$ for each BS $n$ converge to the utility function $u_n$ and ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]$ with probability 1 when $\\lambda_j \\ne {1 \\mathord{\\left/\n {\\vphantom {1 t}} \\right.\n \\kern-\\nulldelimiterspace} t}$.\n\\end{theorem}\n\n\\begin{proof} In order to prove this theorem, we first need to prove the ESNs converges. Then, we formulate the exact value to which the ESNs converge. \n\nBased on the Gordon's Theorem \\cite{26}, the ESNs converge with probability 1 must satisfy: a) a finite MDP, b) Sarsa learning \\cite{31} is being used with a linear function approximator, and c) learning rates satisfy the Robbins-Monro conditions (${\\lambda} > 0,\\sum\\nolimits_{t = 0}^\\infty  {{\\lambda\\left(t\\right)} = +\\infty ,\\sum\\nolimits_{t = 0}^\\infty  {\\lambda^2\\left(t\\right) <+ \\infty } } $). The proposed game only has two states that are request state and stable state, and the number of actions is finite which satisfies a).\n\nFrom (\\ref{eq:rtn}) and (\\ref{eq:w}), we can formulate the update equation for ESNs as follows:   \n\n", "itemtype": "equation", "pos": 40885, "prevtext": "\nThe $\\varepsilon$-greedy mechanism decides the probability distribution of action set for each BS. It can be easy to conclude that the probability distribution of each SBS's action set consists of a large probability of the action that results in the optimal reward and a uniform equal probability for other actions. Thus, based on $\\varepsilon$-greedy mechanism, each BS can obtain the probability distribution of other BSs' action sets by only obtaining the spectrum allocation action that results in the optimal reward.\n\nThe learning rates in ESNs have two different rules: a) fixed value and b) the Robbins-Monro conditions \\cite{26}. The Robbins-Monro conditions can be given by: \n\n", "index": 59, "text": "\\begin{equation}\n\\left\\{ {\\begin{array}{*{20}{c}}\n{\\left( i \\right)\\;\\;\\;\\;\\;\\;\\;\\;{\\lambda _j }\\left( n \\right) > 0,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\\n{\\left( {ii} \\right)\\;\\;\\;\\;\\;\\;\\;\\mathop {\\lim }\\limits_{t \\to \\infty } \\sum\\limits_{n = 1}^t {{\\lambda _j }\\left( n \\right)}  =  + \\infty },\\\\\n{\\left( {iii} \\right)\\;\\;\\;\\;\\;\\;\\;\\mathop {\\lim }\\limits_{t \\to \\infty } \\sum\\limits_{n = 1}^t {{\\lambda _j^2 }\\left( n \\right)}  <  + \\infty },\n\\end{array}} \\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\left\\{{\\begin{array}[]{*{20}{c}}{\\left(i\\right)\\;\\;\\;\\;\\;\\;\\;\\;{\\lambda_{j}}%&#10;\\left(n\\right)&gt;0,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\\\&#10;{\\left({ii}\\right)\\;\\;\\;\\;\\;\\;\\;\\mathop{\\lim}\\limits_{t\\to\\infty}\\sum\\limits_{%&#10;n=1}^{t}{{\\lambda_{j}}\\left(n\\right)}=+\\infty},\\\\&#10;{\\left({iii}\\right)\\;\\;\\;\\;\\;\\;\\;\\mathop{\\lim}\\limits_{t\\to\\infty}\\sum\\limits_%&#10;{n=1}^{t}{{\\lambda_{j}^{2}}\\left(n\\right)}&lt;+\\infty},\\end{array}}\\right.\" display=\"block\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo rspace=\"47.3pt\">,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>)</mo></mrow><mo separator=\"true\">\u2003\u2002\u2003</mo><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>t</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>)</mo></mrow><mo separator=\"true\">\u2003\u2002\u2003</mo><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>t</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msubsup><mi>\u03bb</mi><mi>j</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>&lt;</mo><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable><mi/></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nActually, (\\ref{eq:rc}) is a special form of Sarsa(0) learning \\cite{31}. Condition c) is trivially satisfied via our learning scheme's definition. Therefore, we can conclude that the ESNs in our game with the Robbins-Monro learning rule satisfy Gordon's Theorem and converge with probability 1. However, Gordon's Theorem dose not formulate the exact value to which the ESNs converge. Therefore, we use the continuous time version of (\\ref{eq:rc}) to formulate the exact value to which the ESNs converges.\n\nTo obtain the continuous time version, consider $\\Delta t \\in [0,1]$ to be a small amount of time and $r_{t +\\Delta t ,n}^i - r_{t,n}^i  \\approx \\Delta t  \\times {\\alpha _t}\\left( {u_{t+\\Delta t,n}^i - r_{t,n}^i} \\right)$ to be the approximate growth in $r_{n}^i$ during $\\Delta t$. Dividing both sides of the equation by $\\Delta t$ and taking the limit for $\\Delta t \\to 0$, (\\ref{eq:rc}) can be expressed as follows:\n\n", "itemtype": "equation", "pos": 45981, "prevtext": "\nwhere $j \\in \\left\\{ {\\alpha ,\\beta } \\right\\}$. The learning rate has an effect on the speed of the convergence of our proposed algorithm. However, the two learning rules are all converged with time increases which we would provide the proof next.   \n\nWe assume that all BSs transmit simultaneously and each user can only connect to one BS in the uplink or downlink at each time. We further assume that each SBS knows the time slots that occupies the unlicensed band $L$. Based on the above formulations, the distributed RL approach based on ESNs performed by every BS $n$ is shown in Algorithm 1. In line 8 of Algorithm 1, we capture the fact that each SBS broadcasts the action that it adopts now and the probability distribution of action profiles to other BSs.\n\n\\begin{algorithm}[!t]\n\\caption{Reinforcement learning with ESNs algorithm }   \n\\label{alg:Framwork}   \n\\begin{algorithmic} [1] \n\\REQUIRE The set of users' association states, $\\boldsymbol{x}_{t,n}^\\alpha$ and $\\boldsymbol{x}_{t,n}^\\beta$;\\\\ \n\n\\ENSURE initialize $\\boldsymbol{W}_n^{\\alpha,in}$, $\\boldsymbol{W}_n^\\alpha$, $\\boldsymbol{W}_n^{\\alpha,out}$, $\\boldsymbol{W}_n^{\\beta,in}$, $\\boldsymbol{W}_n^\\beta$, $\\boldsymbol{W}_n^{\\beta,out}$ $\\boldsymbol{r}_{0,n}^\\alpha=0$, $\\boldsymbol{r}_{0,n}^\\beta=0$  \\\\ \n\n\\STATE calculate the time slots $L$ based on (8)\n\n\\FOR {time $t$} \n\n\\IF{$rand(.) < \\varepsilon$}\n\n\\STATE randomly choose one action \n\n\\ELSE\n\n\\STATE choose action $\\boldsymbol{a}_{n,i}\\left(t\\right) = \\mathop {\\arg \\max }\\limits_{\\boldsymbol{a}_{n,i}\\left(t\\right) } \\left( {r_{t,j}^{\\beta,i}\\left( {{\\boldsymbol{x}_{t,n}^{\\beta}},\\boldsymbol{a}_{n,i}\\left(t\\right)} \\right)} \\right)$ \n\n\\ENDIF \n\n\\STATE broadcast the action $\\boldsymbol{a}_{n,i}\\left(t\\right)$ and the optimal action $\\boldsymbol{a}_{n}^*$ that results in the maximal value of reward function\n\n\\STATE calculate the reward $r^\\alpha$ and $r^\\beta$ based on (\\ref{eq:utn})-(\\ref{eq:rtn}) \n\n\\STATE update the output weight matrix $\\boldsymbol{W}_{t,ij}^{\\alpha,out}$ and $\\boldsymbol{W}_{t,ij}^{\\beta,out}$ based on (\\ref{eq:w})\n\\ENDFOR  \n\\end{algorithmic}\n\\end{algorithm}  \n\nIn essence, at every time instant, every BS allocates its spectrum to the users and maximizes its own rate. The users would send connection request to all BSs at each time. After iterations, each user could get the best rate and each BS maximizes its total rate. Note that the performance of the proposed algorithm can be improved by incorporating a training sequence to update the output weight matrix $\\boldsymbol{W}^{out}$. Adjusting the input weight matrix $\\boldsymbol{W}^{in}$ and recurrent matrix $\\boldsymbol{W}$ appropriately will also improve the accuracy of the algorithm. Algorithm 1 continues to iterate until each user achieves maximal rate and the users' association states remain unchanged. \n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Convergence of the ESNs-based algorithm}\nIn the proposed algorithm, we use ESNs $\\alpha$ to store and estimate the reward of each spectrum allocation scheme. Then, we train ESNs $\\beta$ as RL algorithm to solve the proposed game. Since ESNs have two different learning rules to update the output matrix, in this subsection, we prove the convergence of the two learning phases of our proposed algorithm. We first prove the convergence of ESNs with the Robbins-Monro learning rule. Next, we use the continuous time version to prove the convergence of ESNs with the fixed value learning rule. Finally, we prove the proposed algorithm reaches to the mixed strategy NE.\n\n\\begin{theorem} ESN $\\alpha$ and ESN $\\beta$ for each BS $n$ converge to the utility function $u_n$ and ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]$ with probability 1 when $\\lambda_j \\ne {1 \\mathord{\\left/\n {\\vphantom {1 t}} \\right.\n \\kern-\\nulldelimiterspace} t}$.\n\\end{theorem}\n\n\\begin{proof} In order to prove this theorem, we first need to prove the ESNs converges. Then, we formulate the exact value to which the ESNs converge. \n\nBased on the Gordon's Theorem \\cite{26}, the ESNs converge with probability 1 must satisfy: a) a finite MDP, b) Sarsa learning \\cite{31} is being used with a linear function approximator, and c) learning rates satisfy the Robbins-Monro conditions (${\\lambda} > 0,\\sum\\nolimits_{t = 0}^\\infty  {{\\lambda\\left(t\\right)} = +\\infty ,\\sum\\nolimits_{t = 0}^\\infty  {\\lambda^2\\left(t\\right) <+ \\infty } } $). The proposed game only has two states that are request state and stable state, and the number of actions is finite which satisfies a).\n\nFrom (\\ref{eq:rtn}) and (\\ref{eq:w}), we can formulate the update equation for ESNs as follows:   \n\n", "index": 61, "text": "\\begin{equation}\\label{eq:rc}\nr_{t + 1,n}^{j,i} - r_{t,n}^{j,i} = {\\lambda _j}\\left(u_{n}^{j,i} - r_{t,n}^{j,i} \\right), \\hspace{1em} j \\in \\left\\{ \\alpha, \\beta \\right\\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"r_{t+1,n}^{j,i}-r_{t,n}^{j,i}={\\lambda_{j}}\\left(u_{n}^{j,i}-r_{t,n}^{j,i}%&#10;\\right),\\hskip 10.0ptj\\in\\left\\{\\alpha,\\beta\\right\\}.\" display=\"block\"><mrow><mrow><mrow><mrow><msubsup><mi>r</mi><mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>-</mo><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mo>=</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>u</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>-</mo><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mo>{</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nThe general solution for (\\ref{eq:dr}) can be found by integration:\n\n", "itemtype": "equation", "pos": 47094, "prevtext": "\nActually, (\\ref{eq:rc}) is a special form of Sarsa(0) learning \\cite{31}. Condition c) is trivially satisfied via our learning scheme's definition. Therefore, we can conclude that the ESNs in our game with the Robbins-Monro learning rule satisfy Gordon's Theorem and converge with probability 1. However, Gordon's Theorem dose not formulate the exact value to which the ESNs converge. Therefore, we use the continuous time version of (\\ref{eq:rc}) to formulate the exact value to which the ESNs converges.\n\nTo obtain the continuous time version, consider $\\Delta t \\in [0,1]$ to be a small amount of time and $r_{t +\\Delta t ,n}^i - r_{t,n}^i  \\approx \\Delta t  \\times {\\alpha _t}\\left( {u_{t+\\Delta t,n}^i - r_{t,n}^i} \\right)$ to be the approximate growth in $r_{n}^i$ during $\\Delta t$. Dividing both sides of the equation by $\\Delta t$ and taking the limit for $\\Delta t \\to 0$, (\\ref{eq:rc}) can be expressed as follows:\n\n", "index": 63, "text": "\\begin{equation}\\label{eq:dr}\n\\frac{{dr_{t,n}^{j,i}}}{{dt}} \\approx {\\lambda _j}\\left( {u_n^{j,i} - r_{t,n}^{j,i}} \\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\frac{{dr_{t,n}^{j,i}}}{{dt}}\\approx{\\lambda_{j}}\\left({u_{n}^{j,i}-r_{t,n}^{j%&#10;,i}}\\right).\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>\u2248</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>u</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>-</mo><msubsup><mi>r</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere $C$ is the constant of integration. As $\\exp\\left(-x\\right)$ is monotonic function and ${\\lim _{t \\to \\infty }}\\exp \\left( { - \\alpha_tt} \\right) = 0$, when $\\lambda_j \\ne {1 \\mathord{\\left/\n {\\vphantom {1 t}} \\right.\n \\kern-\\nulldelimiterspace} t}$. It is easy to observe that, when $t \\to \\infty $, the limit of (\\ref{eq:rC}) is given by:\n\n", "itemtype": "equation", "pos": 47301, "prevtext": "\nThe general solution for (\\ref{eq:dr}) can be found by integration:\n\n", "index": 65, "text": "\\begin{equation}\\label{eq:rC}\nr_n^{j,i} = C\\exp \\left( { - {\\lambda _j}t} \\right) + u_n^{j,i}, \\;\\; j \\in \\left\\{ \\alpha, \\beta \\right\\},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"r_{n}^{j,i}=C\\exp\\left({-{\\lambda_{j}}t}\\right)+u_{n}^{j,i},\\;\\;j\\in\\left\\{%&#10;\\alpha,\\beta\\right\\},\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>r</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo>=</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>+</mo><msubsup><mi>u</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></mrow><mo rspace=\"8.1pt\">,</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mo>{</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nFrom (\\ref{eq:limr}), we can conclude that the ESNs converge to the utility function as ${\\lambda _j} \\ne \\frac{1}{t}$, however, as ${\\lambda _j} =\\frac{1}{t}$, the ESNs converge to the utility function with a constant $C\\exp \\left( { - 1} \\right)$. Moreover, we can see that the convergence of ESNs actually has no relationship with learning rate, it only needs enough time to update. \nThis completes the proof.\n\\end{proof} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem} The ESN-based algorithm converges to a mixed Nash equilibrium, with the mixed strategy probability ${\\boldsymbol{\\pi}_n^*} \\in \\Delta\\left(\\mathcal{A}_n\\right)$, $\\forall n \\in \\mathcal{B}$.\n\\end{theorem}\n\\begin{proof} In order to prove Theorem 2, we need to establish the mixed NE conditions in (\\ref{eq:mNE}). We assume that the spectrum allocation action $\\boldsymbol{a}_{n}^*$ results in the optimal reward given the optimal mixed strategy $(\\boldsymbol{\\pi}_{n}^*,\\boldsymbol{\\pi}_{-n}^*)$, which means that $\\pi_{n,\\boldsymbol{a}_n^*}^*=1 - \\varepsilon  + \\frac{\\varepsilon }{\\left| \\mathcal{A}_n \\right|}$ and $\\pi_{n,\\boldsymbol{a}_n'}^*= \\frac{\\varepsilon }{{{\\left| \\mathcal{A}_n \\right|}}}$. We also assume that $\\boldsymbol{a}_{n} \\in \\mathcal{A}_n/\\boldsymbol{a}_{n}^*$ results in the optimal reward given the optimal mixed strategy $(\\boldsymbol{\\pi}_{n}, \\boldsymbol{\\pi}_{-n}^*)$. Based on the Theorem 1, ESN $\\beta$ of the proposed algorithm converges to ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]$. Thus, (\\ref{eq:mNE}) can be rewritten as follows:\n\n", "itemtype": "equation", "pos": 47801, "prevtext": "\nwhere $C$ is the constant of integration. As $\\exp\\left(-x\\right)$ is monotonic function and ${\\lim _{t \\to \\infty }}\\exp \\left( { - \\alpha_tt} \\right) = 0$, when $\\lambda_j \\ne {1 \\mathord{\\left/\n {\\vphantom {1 t}} \\right.\n \\kern-\\nulldelimiterspace} t}$. It is easy to observe that, when $t \\to \\infty $, the limit of (\\ref{eq:rC}) is given by:\n\n", "index": 67, "text": "\\begin{equation}\\label{eq:limr}\n{\\lim _{t \\to \\infty }}r_n^{j,i}=\\left\\{ {\\begin{array}{*{20}{c}}\n{C\\exp \\left( { - 1} \\right) + u_n^{j,i},\\;\\;\\;\\;{\\lambda _j} = \\frac{1}{t}},\\\\\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{u_n^{j,i},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{\\lambda _j} \\ne \\frac{1}{t}}.\n\\end{array}} \\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"{\\lim_{t\\to\\infty}}r_{n}^{j,i}=\\left\\{{\\begin{array}[]{*{20}{c}}{C\\exp\\left({-%&#10;1}\\right)+u_{n}^{j,i},\\;\\;\\;\\;{\\lambda_{j}}=\\frac{1}{t}},\\\\&#10;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{u_{n}^{j,i},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{\\lambda_{j}}%&#10;\\neq\\frac{1}{t}}.\\end{array}}\\right.\" display=\"block\"><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>t</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><msubsup><mi>r</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow></mrow><mo>+</mo><msubsup><mi>u</mi><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow><mo rspace=\"13.7pt\">,</mo><msub><mi>\u03bb</mi><mi>j</mi></msub></mrow><mo>=</mo><mfrac><mn>1</mn><mi>t</mi></mfrac></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><msubsup><mpadded lspace=\"33.6pt\" width=\"+33.6pt\"><mi>u</mi></mpadded><mi>n</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msubsup><mo rspace=\"38.9pt\">,</mo><msub><mi>\u03bb</mi><mi>j</mi></msub></mrow><mo>\u2260</mo><mfrac><mn>1</mn><mi>t</mi></mfrac></mrow><mo>.</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\n\n\n\n\n\n\n\nwhere (a) is obtained from the fact that $\\pi_{n,\\boldsymbol{a}_n^*}^*=\\pi_{n,\\boldsymbol{a}_n} =1 - \\varepsilon  + \\frac{\\varepsilon }{\\left| \\mathcal{A}_n \\right|}$ and $\\pi_{n,\\boldsymbol{a}_n'}^*=\\pi_{n,\\boldsymbol{a}_n''}= \\frac{\\varepsilon }{{{\\left| \\mathcal{A}_n \\right|}}}$, ${{\\boldsymbol{a}_{ n}'} \\in{{{\\mathcal{A}_n}} \\mathord{\\left/\n {\\vphantom {{{A_n}} a}} \\right.\n \\kern-\\nulldelimiterspace} \\boldsymbol{a}_n^*}}$, ${{\\boldsymbol{a}_{ n}''} \\in{{{\\mathcal{A}_n}} \\mathord{\\left/\n {\\vphantom {{{A_n}} a}} \\right.\n \\kern-\\nulldelimiterspace} \\boldsymbol{a}_n}}$. Since in our proposed algorithm, the optimal action ${{\\boldsymbol{a}_n^*}}$ results in the optimal ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n^*}} \\right)} \\right]$, we can conclude that ${{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n^*}} \\right)} \\right]-{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]}\\ge 0$. This completes the proof. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \\end{proof}\n\n\\section{Simulation results} \nIn this section, we evaluate the performance of the proposed ESN algorithm using simulations. We first introduce the simulation parameters. Further, we evaluate the  performance of ESNs' approximation and estimation in our proposed algorithm. Finally, we show the improvements in terms of sum-rate of all users and the $50$th percentile of users by comparing the proposed algorithm with three Q-learning algorithms.\n\\subsection{System Parameters}\nFor our simulations, we use the following parameters. One macrocell with a radius $r_M=500$ meters is considered with $N_s$ uniformly distributed picocells, $W=2$ uniformly distributed WiFi access points, and $U$ uniformly distributed LTE-U users. The picocells share the licensed band with MBS and the unlicensed band with WiFi. Each WAP has 4 WiFi users. The channel gain is generated as Rayleigh variables with unit variance. The WiFi network is set up based on the IEEE 802.11n protocol working at the $5$ GHz band with a RTS/CTS mechanism. Other parameters are listed in Table  \\uppercase\\expandafter{\\romannumeral1}. The results are compared to three schemes: a) Q-learning with uplink-downlink decoupling, b) Q-learning with uplink-downlink decoupling within an LTE system, and c) Q-learning without uplink-downlink decoupling. All statistical results are averaged over a large number of independent runs. Hereinafter, we use the term ``sum-rate'' to refer to the total downlink and uplink rates and ``Q-learning\" to refer to the Q-learning with uplink-downlink decoupling within an LTE-U system. \n\nWe assume Q-learning in our simulations has knowledge of the action that each BS takes and the entire users' information including interference and location. The update rule of Q-learning can be expressed as follows:\n\n", "itemtype": "equation", "pos": 49684, "prevtext": "\nFrom (\\ref{eq:limr}), we can conclude that the ESNs converge to the utility function as ${\\lambda _j} \\ne \\frac{1}{t}$, however, as ${\\lambda _j} =\\frac{1}{t}$, the ESNs converge to the utility function with a constant $C\\exp \\left( { - 1} \\right)$. Moreover, we can see that the convergence of ESNs actually has no relationship with learning rate, it only needs enough time to update. \nThis completes the proof.\n\\end{proof} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem} The ESN-based algorithm converges to a mixed Nash equilibrium, with the mixed strategy probability ${\\boldsymbol{\\pi}_n^*} \\in \\Delta\\left(\\mathcal{A}_n\\right)$, $\\forall n \\in \\mathcal{B}$.\n\\end{theorem}\n\\begin{proof} In order to prove Theorem 2, we need to establish the mixed NE conditions in (\\ref{eq:mNE}). We assume that the spectrum allocation action $\\boldsymbol{a}_{n}^*$ results in the optimal reward given the optimal mixed strategy $(\\boldsymbol{\\pi}_{n}^*,\\boldsymbol{\\pi}_{-n}^*)$, which means that $\\pi_{n,\\boldsymbol{a}_n^*}^*=1 - \\varepsilon  + \\frac{\\varepsilon }{\\left| \\mathcal{A}_n \\right|}$ and $\\pi_{n,\\boldsymbol{a}_n'}^*= \\frac{\\varepsilon }{{{\\left| \\mathcal{A}_n \\right|}}}$. We also assume that $\\boldsymbol{a}_{n} \\in \\mathcal{A}_n/\\boldsymbol{a}_{n}^*$ results in the optimal reward given the optimal mixed strategy $(\\boldsymbol{\\pi}_{n}, \\boldsymbol{\\pi}_{-n}^*)$. Based on the Theorem 1, ESN $\\beta$ of the proposed algorithm converges to ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]$. Thus, (\\ref{eq:mNE}) can be rewritten as follows:\n\n", "index": 69, "text": "\\begin{equation}\n\\begin{split}\n&{\\tilde u_n}\\left( {\\boldsymbol{\\pi} _n^*,\\boldsymbol{\\pi} _{ - n}^*} \\right)-{\\tilde u_n}\\left( {{\\boldsymbol{\\pi} _n},\\boldsymbol{\\pi} _{ - n}^*} \\right)\\\\\n&=\\sum\\limits_{{\\boldsymbol{a}_{ n}} \\in {\\mathcal{A}_{ n}}}\\left[{\\pi_{n,\\boldsymbol{a}_n}^* \\sum\\limits_{{\\boldsymbol{a}_{ - n}} \\in {\\mathcal{A}_{ - n}}}{{u_n}\\left( {{\\boldsymbol{a}_n},{\\boldsymbol{a}_{ - n}}} \\right)\\pi_{n,{\\boldsymbol{a}_{- n}}}^*}-{\\pi_{n,\\boldsymbol{a}_n} \\sum\\limits_{{\\boldsymbol{a}_{ - n}} \\in {\\mathcal{A}_{ - n}}}{{u_n}\\left( {{\\boldsymbol{a}_n} ,{\\boldsymbol{a}_{ - n}}} \\right)\\pi_{n,{\\boldsymbol{a}_{- n}}}^*} }} \\right]\\\\\n&=\\sum\\limits_{{\\boldsymbol{a}_{ n}} \\in {\\mathcal{A}_{ n}}}\\left[{\\pi_{n,\\boldsymbol{a}_n}^*{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]-\\pi_{n,\\boldsymbol{a}_n}{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]} \\right]\\\\\n&\\mathop = \\limits^{\\left( a \\right)}\\left(1-\\epsilon\\right)\\left({{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n^*}} \\right)} \\right]-{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]} \\right)\\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle{\\tilde{u}_{n}}\\left({\\boldsymbol{\\pi}_{n}^{*},%&#10;\\boldsymbol{\\pi}_{-n}^{*}}\\right)-{\\tilde{u}_{n}}\\left({{\\boldsymbol{\\pi}_{n}}%&#10;,\\boldsymbol{\\pi}_{-n}^{*}}\\right)\\\\&#10;&amp;\\displaystyle=\\sum\\limits_{{\\boldsymbol{a}_{n}}\\in{\\mathcal{A}_{n}}}\\left[{%&#10;\\pi_{n,\\boldsymbol{a}_{n}}^{*}\\sum\\limits_{{\\boldsymbol{a}_{-n}}\\in{\\mathcal{A%&#10;}_{-n}}}{{u_{n}}\\left({{\\boldsymbol{a}_{n}},{\\boldsymbol{a}_{-n}}}\\right)\\pi_{%&#10;n,{\\boldsymbol{a}_{-n}}}^{*}}-{\\pi_{n,\\boldsymbol{a}_{n}}\\sum\\limits_{{%&#10;\\boldsymbol{a}_{-n}}\\in{\\mathcal{A}_{-n}}}{{u_{n}}\\left({{\\boldsymbol{a}_{n}},%&#10;{\\boldsymbol{a}_{-n}}}\\right)\\pi_{n,{\\boldsymbol{a}_{-n}}}^{*}}}}\\right]\\\\&#10;&amp;\\displaystyle=\\sum\\limits_{{\\boldsymbol{a}_{n}}\\in{\\mathcal{A}_{n}}}\\left[{%&#10;\\pi_{n,\\boldsymbol{a}_{n}}^{*}{\\mathbb{E}}\\left[{{u_{n}}\\left({{\\boldsymbol{a}%&#10;_{n}}}\\right)}\\right]-\\pi_{n,\\boldsymbol{a}_{n}}{\\mathbb{E}}\\left[{{u_{n}}%&#10;\\left({{\\boldsymbol{a}_{n}}}\\right)}\\right]}\\right]\\\\&#10;&amp;\\displaystyle\\mathop{=}\\limits^{\\left(a\\right)}\\left(1-\\epsilon\\right)\\left({%&#10;{\\mathbb{E}}\\left[{{u_{n}}\\left({{\\boldsymbol{a}_{n}^{*}}}\\right)}\\right]-{%&#10;\\mathbb{E}}\\left[{{u_{n}}\\left({{\\boldsymbol{a}_{n}}}\\right)}\\right]}\\right)\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><msub><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udf45</mi><mi>n</mi><mo>*</mo></msubsup><mo>,</mo><msubsup><mi>\ud835\udf45</mi><mrow><mo>-</mo><mi>n</mi></mrow><mo>*</mo></msubsup><mo>)</mo></mrow></mrow><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udf45</mi><mi>n</mi></msub><mo>,</mo><msubsup><mi>\ud835\udf45</mi><mrow><mo>-</mo><mi>n</mi></mrow><mo>*</mo></msubsup><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub></mrow></munder><mrow><mo>[</mo><mrow><mrow><msubsup><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></munder><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow><mo>*</mo></msubsup></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub></mrow></msub><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow></munder><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow></msub></mrow><mo>*</mo></msubsup></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>n</mi></msub></mrow></munder><mrow><mo>[</mo><mrow><mrow><msubsup><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c0</mi><mrow><mi>n</mi><mo>,</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub></mrow></msub><mo>\u2062</mo><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mover><mo movablelimits=\"false\">=</mo><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mover><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc82</mi><mi>n</mi><mo>*</mo></msubsup><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>u</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mi>n</mi></msub><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06895.tex", "nexttext": "\nwhere ${\\boldsymbol{a}_{ - n}^*}$ represents the optimal action profile of all BSs other than SBS $n$.\n\n\\begin{table}\\footnotesize\n \\caption{\n    \\vspace*{-0em}SYSTEM PARAMETERS \\cite{29}}\\vspace*{-1em}\n\\centering  \n\\begin{tabular}{|c|c|c|c|}\n\\hline\n\\textbf{Parameters} & \\textbf{Values} & \\textbf{Parameters} & \\textbf{Values} \\\\\n\\hline\n$P_M $ & 43 dBm & $P_P$ & 35 dBm\\\\\n\\hline\n$P_u$ & 20 dBm & $\\alpha$ & 0.05\\\\\n\\hline\n$\\varepsilon$ & 0.7 & $N$ & 1000\\\\\n\\hline\n $\\eta$ & 0.7 & $\\delta $ & 0 us\\\\\n \\hline\n $F_l^\\textrm{UL}$ & 10 MHz & $F_l^\\textrm{DL}$ & 10 MHz \\\\\n\\hline\n$F_u$ & 20 MHz & $\\mathcal{L}_b$ & 100 m \\\\\n\\hline\n$E\\left[P\\right] $ & 1500 byte & SIFS & 16 us\\\\\n\\hline\n CTS & 304 us&DIFS & 50 us\\\\\n\\hline\nACK & 304 us & RTS & 352 us\\\\ \n\\hline\n$C$ & 130 Mbps& $\\lambda_q, \\lambda_\\beta$ & 0.06, 0.06 \\\\\n\\hline\n$\\lambda_\\alpha$ & 0.08 & $Z$ & 10 \\\\\n\\hline\nPath loss (unlicensed) &$15.3+50\\log_{10}(m)$& Path loss (licensed) &$15.3+37.5\\log_{10}(m)$\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\\subsection{ESNs Approximation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=10cm]{newfirstESN}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig2} The ESNs $\\alpha$ approximation of the reward function ($N_b=5, U=40, R_w=4$ Mbps).}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}\n\nFig. \\ref{fig2} shows how the first ESNs approximate to the reward function as the number of iteration varies. In Fig. \\ref{fig2}, we can see that, as the number of iteration increases, the approximations of BSs improve. This demonstrates the effectiveness of ESN as an approximation tool for utility functions. Moreover, the network state stored in the ESNs improves the approximation which records the value of reward function according to the network state. Fig. \\ref{fig2} also shows that the ESNs only need less than $100$ iterations to finish the approximation of the reward function. This is due to the fact that ESNs only need to train the output matrix which reduces the training process of ESNs.  \n\n\\begin{figure}\n\\centerline{\\subfigure[$\\lambda_\\alpha=0.08$ ]{\\includegraphics[width=6cm]{alpha1}\n\\label{fig3a}}\\hspace{-0.65cm}\n\\subfigure[$\\lambda_\\alpha=0.04$ and $\\lambda_\\alpha=0.01$]{\\includegraphics[width=6cm]{alpha24}\n\\label{fig3b}}\\hspace{-0.65cm}\n\\subfigure[$\\lambda_\\alpha=0.15$]{\\includegraphics[width=6cm]{alpha13}\n\\label{fig3c}}}\n\\caption{\\label{fig3}The ESNs $\\alpha$ approximation of the reward function vs. the learning rate $\\lambda_\\alpha$ varies ($N_b=5, U=40, R_w=4$ Mbps).}\n\\end{figure}\n\nIn Fig. \\ref{fig3}, we show how ESNs $\\alpha$ can approximate the reward function as the learning rate $\\lambda_\\alpha$ varies. Fig. \\ref{fig3a} and Fig. \\ref{fig3b} show that, even if the learning rate $\\lambda_\\alpha$ just changes from $0.04$ to $0.08$, the ESNs achieves more than $100\\%$ improvement in terms of the approximation speed. This is due to the fact that the learning rate decides the step length of the adjustment of ESNs. However, by comparing Fig. \\ref{fig3a} with Fig. \\ref{fig3c}, we can see that the approximation of ESNs $\\alpha$ with $\\lambda_\\alpha=0.15$ needs more than 1000 iterations to approximate the reward function, while, for $\\lambda_\\alpha=0.08$, it only needs 200 iterations. Clearly, when the learning rate $\\lambda_\\alpha$ is too large, the update value for the output matrix of ESNs $\\alpha$ is also large, which results in a low speed of convergence. Therefore, we can conclude that it is important to choose an appropriate learning rate which decides the convergence speed of ESNs $\\alpha$. \n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=10cm]{newsecondESNs}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig4} The update of ESNs $\\beta$ of the proposed algorithms. Different lines denote different actions that SBS adopts. ($N_b=5, U=40, R_w=4$ Mbps).}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}\n\nIn Fig. \\ref{fig4}, we show how ESNs $\\beta$ of the proposed algorithm updates the expected reward for each BS when it adopts different spectrum allocation actions as the number of iteration varies. In this figure, each line of ESNs $\\beta$ stands for one spectrum allocation action of each BS. We can see that each line of ESNs $\\beta$ converges to a stable value as the number of iteration increases which implies that by using ESNs, each BS can estimate the reward before BS takes any action. This is due to the fact that, as iteration increases, the approximation of ESNs $\\alpha$ provides the reward that ESNs $\\beta$ can be used to calculate the expected reward. Fig. \\ref{fig4} also shows that, as below iteration $100$, the expected reward of each spectrum allocation action changes quickly. However, each curve exhibits only small oscillations as the number of iterations is more than $100$. The main reason behind this is that, at the beginning, ESN $\\alpha$ spends some iterations to approximate the reward function. Since the ESNs $\\alpha$ has not yet approximated the reward function well, ESNs $\\beta$ can not calculate the expected reward for each action accurately. As the number of iterations is above 100, ESNs $\\alpha$ finishes the approximation of reward function, which results in the accurate calculation of expected reward for ESNs $\\beta$.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\vspace{0cm}\n\\subfigure[]{\n\\label{fig7a} \n\\includegraphics[width=10cm]{uplinkcdf}}\n\\subfigure[]{\n\\label{fig7b} \n\\includegraphics[width=10cm]{downlinkcdf}}\n  \\vspace{-0.3cm}\n\n \\caption{\\label{fig7} Downlink and uplink rates CDFs resulting from the different algorithms ($U=40, N_s=5, R_w=4$ Mbps).}\n\\end{figure}\n\nFig. \\ref{fig7} shows the cumulative distribution function (CDF) of rate in both the uplink and downlink for all the considered schemes. In Fig. \\ref{fig7a}, we can see that, the uplink rates of $6\\%$, $9\\%$, $10\\%$, and $15\\%$ users from all the considered algorithm are below $0.001$ Mbps. This is due to the fact that, in all considered algorithms, each SBS has a limited coverage which limits users' associations. The coverage results in the users who are far away from the MBS but not in the coverage of any SBS associating with the MBS. Fig. \\ref{fig7a} also shows that the proposed approach improves the uplink CDF at low rate yielding up to $50\\%$ and $130\\%$ gains for 0.001 Mbps rate compared to Q-learning and Q-learning without decoupling, respectively. This is due to the fact that the proposed algorithm chooses the action based on the expected reward which is calculated by two ESNs to update but Q-learning only uses the maximal reward to choose action and both the proposed approach and Q-learning adopt uplink-downlink decoupling to improve their uplink rates. Fig. \\ref{fig7b} also shows that the proposed approach improves the downlink CDF at low rate showing $150\\%$, $25\\%$, and $5\\%$ gains for 0.05, 0.5 and 5 Mbps rates compared to the Q-learning. This is because the proposed approach uses the estimated expected value of the reward function to choose the optimal allocation scheme that results in the optimal reward. \n\n\\begin{figure}\n\\centering\n\\vspace{0cm}\n\\subfigure[]{\n\\label{fig8a} \n\\includegraphics[width=10cm]{bsrate}}\n\\subfigure[]{\n\\label{fig8b} \n\\includegraphics[width=10cm]{Pbs50rate}}\n  \\vspace{-0.3cm}\n\n \\caption{\\label{fig8} Sum-rate as the number of SBSs varies ($U=40, R_w=4$ Mbps).}\n\\end{figure}\n\nIn Fig. \\ref{fig8}, we show how the total sum-rate varies with the number of SBSs. In Fig. \\ref{fig8a}, we can see that, as the number of SBSs increases, all algorithms result in increasing sum-rates because the users have more choices to choose an SBS and the distances from the SBSs to the users decrease. Fig. \\ref{fig8a} also shows that Q-learning achieves, respectively, up to $7\\%$ and $20\\%$ improvements in the sum-rate compared to Q-learning within an LTE system and Q-learning without decoupling for the case with $8$ SBSs. Clearly, downlink-uplink decoupling and LTE utilizing unlicensed band improve the network performance. However, Fig. \\ref{fig8b} shows that the rates of the $50$th percentile of users decreases as the number of SBSs increases. This is due to the fact that, in our simulations, each SBS has a limited coverage area that restricts the access of the users who are located far away from the MBS and not in the coverage of any SBS. Thus, as the number of SBSs increase, the interference to the limited users are increased which results in the decrease of the $50$th percentile of users' throughput. By comparing Fig. \\ref{fig8a} with Fig. \\ref{fig8b}, we can also see that the proposed approach achieves, respectively, $3\\%$ and $19.5\\%$ gains of sum-rate, and $60\\%$ and $78\\%$ gains of the sum-rate of the $50$th percentile of users compared to Q-learning for the case with $8$ SBSs. This demonstrates that the proposed algorithm balances the load better than Q-learning, which allocates appropriate spectrum to the users who have low SINRs. Moreover,  Fig. \\ref{fig8a} and Fig. \\ref{fig8b} also show that the Q-learning in LTE has a higher sum rate of the $50$th percentile of users but lower sum-rate of all users than Q-learning without decoupling. It is obvious that the downlink-uplink decoupling improves the rate of edge users.\n\n\\begin{figure}\n\\centering\n\\vspace{0cm}\n\\subfigure[Sum-rates of all users]{\n\\label{fig9a} \n\\includegraphics[width=10cm]{sumrateusernew}}\n\\subfigure[Sum-rates of the $50$th percentile of users]{\n\\label{fig9b} \n\\includegraphics[width=10cm]{users}}\n  \\vspace{-0.3cm}\n\n \\caption{\\label{fig9} Downlink and uplink sum-rates of users vs. the number of users ($N_s=4, R_w=4$ Mbps).}\n\\end{figure}\n\nFig. \\ref{fig9} shows how the total users' sum-rate of the network in both the uplink and downlink changes as the number of users varies. In Fig. \\ref{fig9a}, we can see that sum-rate increases, then decreases as the number of users increases. That is because each SBS has a relatively small load on the average as the number of users is below $40$. However, as the number of users is above $40$, sum-rate also decreases because each SBS needs to allocate more spectrum to the users who have small SINRs. It can be seen from the Fig. \\ref{fig9b}, as the number of users increases, the sum-rate of the $50$th percentile of users decreases. But this decrease is much slower for all considered algorithms as the number of users is below $60$. Fig. \\ref{fig9b} also shows that, as the number of users is above $60$, the sum-rates of the $50$th percentile of users for all considered algorithms decrease much faster than the case as the number of users is below 60. This is due to the fact that SCNs are over-loaded. Fig. \\ref{fig9b} also shows that the proposed algorithm achieves, respectively, up to $35\\%$ and $60\\%$ improvements in the sum-rate compared to Q-learning for the cases with 60 users and 80 users. This implies that by using ESN, each BS can learn and decide on the spectrum allocation scheme better than Q-learning and reach to the mixed strategy NE. Moreover, in Fig. \\ref{fig9b}, we also can see that the distinguish between the comparing Q-learning algorithms is little as the number of users varies. The main reason behind this is that, the over-loaded BSs limit the function of decoupling and the number of edge users who want to access the unlicensed band.\n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=10cm]{WiFiuser}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig10} Downlink and uplink sum-rates of users vs. the rate requirement of WiFi users ($U=40, N_s=5$).}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}  \n \n \\begin{figure}\n\\centering\n\\vspace{0cm}\n\\subfigure[The case with convergence of Q-learning]{\n\\label{fig5a} \n\\includegraphics[width=10cm]{convergence}}\n\\subfigure[The case with divergence of Q-learning]{\n\\label{fig5b} \n\\includegraphics[width=10cm]{fakeconverge}}\n  \\vspace{-0.3cm}\n\n \\caption{\\label{fig5} The convergence of the algorithms ($N_s=5, U=40, R_w=4$ Mbps).}\n\\end{figure}\n   \nFig. \\ref{fig10} shows the total users' sum-rate of the network in both the uplink and downlink changes as the rate requirement of WiFi users varies. In Fig. \\ref{fig10}, we can see that the sum-rate of all considered algorithms other than the Q-learning in LTE decrease as the rate requirement of WiFi users increases.  That is because the time of unlicensed spectrum that BSs can occupy decreases as the rate requirement of WiFi users increases. Fig. \\ref{fig10} also shows that Q-learning without decoupling has lower sum-rate compared to Q-learning in LTE as the case with the rate requirement of WiFi users is above $5$ Mbps. This is due to the fact that Q-learning in LTE-U without decoupling becomes Q-learning in LTE without decoupling. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!t]\n  \\begin{center}\n   \\vspace{0cm}\n    \\includegraphics[width=10cm]{convergeT}\n    \\vspace{-0.6cm}\n    \\caption{\\label{fig6} The convergence time of the proposed algorithm as a function of the number of SBSs ($R_w=4$ Mbps).}\n  \\end{center}\\vspace{-1cm}\n\\end{figure}\n\nFig. \\ref{fig5} shows the number of iterations needed till convergence for both the proposed approach and Q-learning. In this figure, we can see that, as time elapses, the total value of reward function increase until convergence to their final values. In Fig. \\ref{fig5a} , we can see that, the proposed approach needs 300 iterations to reach convergence and exhibits an acceptable increase of iterations compared to Q-learning. The main reason behind this is because Q-learning exploits the entire informations of all users and BSs to update the Q-table, but the proposed algorithm only needs the action informations of BSs to update output matrix. However, Fig. \\ref{fig5b}  shows that the proposed algorithm reaches convergence as the iteration increases but the Q-learning diverges. Moreover, the proposed algorithm achieves up to 60\\% improvement compared to Q-learning. This is due to the fact that ESN-based learning stores the users' states and actions of BSs to calculate the expected value of each action of BSs which guarantees the proposed algorithm to reach the mixed strategy NE. Note that, since Q-learning would diverge in some cases, we choose the maximal value as the converge point in the case that Q-learning diverges. \n\n\n\n\n\n\n\n\n\n\nIn Fig. \\ref{fig6}, we show the convergence time of the proposed approach as the number of SBSs varies for 40 and 80 users. In this figure, we can see that, as the network size increases, the average number of iterations till convergence increases. Fig. \\ref{fig6} also shows that reducing the number of users leads to a faster convergence time. Although users are not players in the game, they affect spectrum allocation action selection for each BS. As the number of users increases, the spectrum allocation action for each BS increases, and, thus, a longer convergence time is observed.\n\n\n\n\n\n\n\n\n\n\\section{CONCLUSION}\nIn this paper, we have developed a novel resource allocation framework for optimizing the use of uplink-downlink decoupling in an LTE-U system. We have formulated the problem as a noncooperative game between the BSs that seeks to maximize the total uplink and downlink rates while balancing the load among one another. To solve this game, we have developed a novel algorithm based on the machine learning tools of echo state networks. The proposed algorithm enables each BS to decide on its spectrum allocation scheme autonomously with limited information on the network state. Simulation results have shown that the proposed approach yields significant performance gains in terms of rate and load balancing compared to conventional approaches. Moreover, the results have also shown that the use of ESN can significantly reduce the information exchange for the wireless networks.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEbib}\n\\bibliography{references}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 53647, "prevtext": "\n\n\n\n\n\n\n\nwhere (a) is obtained from the fact that $\\pi_{n,\\boldsymbol{a}_n^*}^*=\\pi_{n,\\boldsymbol{a}_n} =1 - \\varepsilon  + \\frac{\\varepsilon }{\\left| \\mathcal{A}_n \\right|}$ and $\\pi_{n,\\boldsymbol{a}_n'}^*=\\pi_{n,\\boldsymbol{a}_n''}= \\frac{\\varepsilon }{{{\\left| \\mathcal{A}_n \\right|}}}$, ${{\\boldsymbol{a}_{ n}'} \\in{{{\\mathcal{A}_n}} \\mathord{\\left/\n {\\vphantom {{{A_n}} a}} \\right.\n \\kern-\\nulldelimiterspace} \\boldsymbol{a}_n^*}}$, ${{\\boldsymbol{a}_{ n}''} \\in{{{\\mathcal{A}_n}} \\mathord{\\left/\n {\\vphantom {{{A_n}} a}} \\right.\n \\kern-\\nulldelimiterspace} \\boldsymbol{a}_n}}$. Since in our proposed algorithm, the optimal action ${{\\boldsymbol{a}_n^*}}$ results in the optimal ${\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n^*}} \\right)} \\right]$, we can conclude that ${{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n^*}} \\right)} \\right]-{\\mathbb{E}}\\left[ {{u_n}\\left( {{\\boldsymbol{a}_n}} \\right)} \\right]}\\ge 0$. This completes the proof. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \\end{proof}\n\n\\section{Simulation results} \nIn this section, we evaluate the performance of the proposed ESN algorithm using simulations. We first introduce the simulation parameters. Further, we evaluate the  performance of ESNs' approximation and estimation in our proposed algorithm. Finally, we show the improvements in terms of sum-rate of all users and the $50$th percentile of users by comparing the proposed algorithm with three Q-learning algorithms.\n\\subsection{System Parameters}\nFor our simulations, we use the following parameters. One macrocell with a radius $r_M=500$ meters is considered with $N_s$ uniformly distributed picocells, $W=2$ uniformly distributed WiFi access points, and $U$ uniformly distributed LTE-U users. The picocells share the licensed band with MBS and the unlicensed band with WiFi. Each WAP has 4 WiFi users. The channel gain is generated as Rayleigh variables with unit variance. The WiFi network is set up based on the IEEE 802.11n protocol working at the $5$ GHz band with a RTS/CTS mechanism. Other parameters are listed in Table  \\uppercase\\expandafter{\\romannumeral1}. The results are compared to three schemes: a) Q-learning with uplink-downlink decoupling, b) Q-learning with uplink-downlink decoupling within an LTE system, and c) Q-learning without uplink-downlink decoupling. All statistical results are averaged over a large number of independent runs. Hereinafter, we use the term ``sum-rate'' to refer to the total downlink and uplink rates and ``Q-learning\" to refer to the Q-learning with uplink-downlink decoupling within an LTE-U system. \n\nWe assume Q-learning in our simulations has knowledge of the action that each BS takes and the entire users' information including interference and location. The update rule of Q-learning can be expressed as follows:\n\n", "index": 71, "text": "\\begin{equation}\n{Q_{t + 1}^i} = \\left( {1 - \\lambda_q } \\right){Q_t^i} +{\\lambda_q }\\left( {{\\boldsymbol{a}_{n,i}}, {\\boldsymbol{a}_{ - n}^*}} \\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"{Q_{t+1}^{i}}=\\left({1-\\lambda_{q}}\\right){Q_{t}^{i}}+{\\lambda_{q}}\\left({{%&#10;\\boldsymbol{a}_{n,i}},{\\boldsymbol{a}_{-n}^{*}}}\\right),\" display=\"block\"><mrow><mrow><msubsup><mi>Q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03bb</mi><mi>q</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>Q</mi><mi>t</mi><mi>i</mi></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>q</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\ud835\udc82</mi><mrow><mi>n</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msubsup><mi>\ud835\udc82</mi><mrow><mo>-</mo><mi>n</mi></mrow><mo>*</mo></msubsup><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]