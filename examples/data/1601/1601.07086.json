[{"file": "1601.07086.tex", "nexttext": "\n  where the only thing we know is that \n  \n", "itemtype": "equation", "pos": 8547, "prevtext": "\n\n\\title{Sequence assembly from corrupted shotgun reads}\n\\author{\n\tShirshendu Ganguly\n\t\\thanks{University of Washington; \\texttt{sganguly@math.washington.edu}.}\n\t\\and\n\tElchanan Mossel\n\t\\thanks{University of Pennsylvania and University of California, Berkeley; \\texttt{mossel@wharton.upenn.edu}.}\n\t\\and \n\tMikl\\'os Z.\\ R\\'acz\n\t\\thanks{Microsoft Research; \\texttt{miracz@microsoft.com}.}\n}\n\\date{\\today}\n\n\\maketitle\n\n\n\n\n\n\\begin{abstract}\nThe prevalent technique for DNA sequencing consists of two main steps:\nshotgun sequencing,  \nwhere many randomly located fragments, called reads, are extracted from the overall sequence, \nfollowed by an assembly algorithm that aims to reconstruct the original sequence. \nThere are many different technologies that generate the reads: \nwidely-used second-generation methods create short reads with low error rates, \nwhile emerging third-generation methods create long reads with high error rates. \nBoth error \\emph{rates} and error \\emph{profiles} differ among methods, \nso reconstruction algorithms are often tailored to specific shotgun sequencing technologies. \nAs these methods change over time, \na fundamental question is whether there exist reconstruction algorithms which are \\emph{robust}, \ni.e., which perform well under a wide range of error distributions. \n\n\nHere we study this question of sequence assembly from corrupted reads. \nWe make no assumption on the \\emph{types} of errors in the reads, \nbut only assume a bound on their \\emph{magnitude}.  \nMore precisely, for each read we assume that \ninstead of receiving the true read with no errors, \nwe receive a corrupted read which has edit distance at most \n$\\varepsilon$ times the length of the read \nfrom the true read. \nWe show that if the reads are long enough and there are sufficiently many of them, \nthen approximate reconstruction is possible: \nwe construct a simple algorithm such that \nfor almost all original sequences the output of the algorithm is \na sequence whose edit distance from the original one is at most \n$O(\\varepsilon)$ times the length of the original sequence. \n\\end{abstract}\n\n\n\n\n\n\n\\section{Introduction} \\label{sec:intro} \n\n\n\nDNA sequencing is by now an essential element of a variety of biological and clinical studies. \nCurrent de novo sequencing technologies typically have two main stages. \nFirst, many randomly located fragments, called reads, are extracted from the DNA sequence in a process called shotgun sequencing. \nNext, an assembly algorithm aims to reconstruct the original sequence based on overlaps between the reads. \n\n\nThe rise of second-generation sequencing methods, such as Illumina, have resulted in many advances in the past decade because they generate high-throughput data cheaply and quickly. \nHowever, these methods produce short reads (a few hundred basepairs long) in order to have a low error rate ($1$-$3\\%$), which results in incomplete and fragmented assemblies~\\cite{Salzberg:10}. \nEmerging technologies, such as PacBio's Single Molecule Real-Time sequencing technology and Oxford Nanopore Technologies, were developed in part to solve this problem. \nThey produce long reads (over ten thousand basepairs long), but currently suffer from a high error rate ($10$-$22\\%$) (see, e.g.,~\\cite{Chin_et_al:13,LoQuSi:15,Li:15}). \n\n\nNot only do these different shotgun sequencing methods produce reads with different error \\emph{rates}, \nthey also have different error \\emph{profiles}, \ne.g., due to various systematic errors. \nConsequently assembly algorithms are often tailored to specific sequencing technologies \nto exploit their unique properties. \nAs these technologies will inevitably change and new ones will arise, \na fundamental question is the \\emph{robustness} of reconstruction algorithms. \nWill the current ones still be useful a decade from now? \nAre there algorithms which perform well under a wide variety of error distributions? \nThis is the question we study in this paper. \n\n\nSeveral recent papers \nhave taken an \\emph{information-theoretic} point of view \nto the sequence assembly problem. \nThe basic question is: what are the fundamental limits to \\emph{any} assembly algorithm? \nGiven a sequencing technology and the statistics of the DNA sequence, \nhow long do the reads need to be and \nhow many are required for reconstruction? \nMotahari, Bresler, and Tse~\\cite{MoBrTs:13} study this question assuming an i.i.d.\\ DNA sequence and error-free reads, \nand show a sharp phase transition: \nif the reads are short enough to have repeats, then reconstruction is impossible, \nwhile as long as the reads are long enough to have no repeats, \nthe necessary condition of having enough reads to cover the whole DNA sequence is essentially sufficient. \nPreviously Dyer, Frieze, and Suen~\\cite{DyFrSu:94} obtained the same phase transition in the length of the reads \nassuming sequencing by hybridization, i.e., that a copy of every read is available. \nSeveral extensions and variations of this problem have been studied. \nAdding some amount of i.i.d.\\ noise to the reads still allows for reconstruction of the perfect layout of the reads~\\cite{MoRaTsMa:13}. \nIn~\\cite{BrBrTs:13} the authors give a sufficient condition for reconstruction for any sequence based on its repeat statistics, assuming error-free reads. \nThis was later extended to allow the reads to come from an erasure error model~\\cite{ShCoTs:15}. \nThese papers are discussed in more detail later. \n\n\nIn this paper we continue this line of work, assuming an i.i.d.\\ DNA sequence as in~\\cite{MoBrTs:13,MoRaTsMa:13}. \nThe main novelty in the model we consider is a strong \\emph{adversarial corruption/error model} on the reads. \nMore precisely, for each read we assume that instead of receiving the true read with no errors, \nwe receive a corrupted read with the edit distance between the true and the corrupted reads being at most $\\varepsilon$ times the length of the true read. \nGiven such a strong adversarial error model, we relax our goal from perfect reconstruction to approximate reconstruction. \nOur main contribution is to show that if the reads are long enough and there are sufficiently many of them, \nthen approximate reconstruction is possible: \nwe present a simple sequential algorithm for which \nthe edit distance between \nthe original sequence and \nthe output of the algorithm \nis at most \n$O(\\varepsilon)$ times the length of the original sequence. \n\n\n\n\n\\section{Problem setting} \\label{sec:problem} \n\n\n\nWe are interested in approximately recovering a long sequence of interest \nfrom a set of randomly chosen shorter reads which are arbitrarily corrupted up to a certain extent. \nConsequently the problem has four main parameters: \nsequence length $n$, \nread length $L$, \nnumber of reads $N$, and \nerror/corruption rate ${\\varepsilon}$; \na fifth parameter, $\\delta$, measures the probability of unsuccessful approximate reconstruction. \n\n\nBefore defining the problem precisely, we introduce some notation. \nLet $\\Sigma$ be a finite alphabet from which the entries of the sequence come from; \nin the case of DNA sequencing we have $\\Sigma = \\left\\{ A, C, G, T \\right\\}$. \nFor a sequence $x = \\left( x_1, x_2, \\dots, x_n \\right) \\in \\Sigma^n$ \nand integers $i$ and $j$, let $x\\left[i,j\\right]$ denote the substring $\\left( x_i, x_{i+1}, \\dots, x_j \\right)$. \nLet $\\Sigma^{*} = \\cup_n \\Sigma^n$. \nFor $x,y \\in \\Sigma^{*}$, \nlet ${\\mathrm{ed}} \\left( x, y \\right)$ denote the edit distance between $x$ and $y$, \ni.e., the minimum number of deletion, insertion, or substitution operations necessary to go from $x$ to $y$. \n\n\nThe approximate reconstruction problem with parameters $\\left( n, L, N, {\\varepsilon}, \\delta \\right)$ is then defined as follows; \nsee Fig.~\\ref{fig:problem} for an illustration. \n\\begin{itemize}\n \\item The sequence of interest, $X \\in \\Sigma^n$, is chosen uniformly at random among all possible sequences; \n  i.e., the entries of $X$ are i.i.d.\\ chosen uniformly from the alphabet $\\Sigma$.\\footnote{We focus on the uniform distribution for simplicity; extensions to certain other distributions should also be tractable.} \n \\item The data are corrupted reads of $X$, defined as follows. \n  Let $\\left\\{ T_i \\right\\}_{i=1}^{N}$ be i.i.d.\\ uniform in $\\left\\{1,2, \\dots, n - L + 1 \\right\\}$ (the starting positions of the reads), \n  and let $R_i = X \\left[ T_i, T_i + L - 1 \\right]$ be the $i^{\\text{th}}$ (uncorrupted) read. \n  Instead of receiving the (multi)set of (uncorrupted) reads $\\mathcal{R} = \\left\\{ R_1, R_2, \\dots R_N \\right\\}$, we receive a (multi)set of corrupted reads \n \n", "index": 1, "text": "\\[\n    {\\widetilde}{\\mathcal{R}} = \\left\\{ {\\widetilde}{R}_1, {\\widetilde}{R}_2, \\dots, {\\widetilde}{R}_N \\right\\},\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\widetilde{}}{\\mathcal{R}}=\\left\\{{\\widetilde{}}{R}_{1},{\\widetilde{}}{R}_{2}%&#10;,\\dots,{\\widetilde{}}{R}_{N}\\right\\},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u211b</mi></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><msub><mi>R</mi><mn>1</mn></msub></mrow><mo>,</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><msub><mi>R</mi><mn>2</mn></msub></mrow><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><msub><mi>R</mi><mi>N</mi></msub></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  but otherwise the ${\\widetilde}{R}_i$ can be arbitrary.\\footnote{The choice of edit distance arises naturally from DNA sequencing; one can consider the problem with other notions of distance as well.}\n \\item The goal of an approximate reconstruction algorithm is to output a sequence ${\\widehat}{X} \\in \\Sigma^{*}$ such that \n    \n", "itemtype": "equation", "pos": 8710, "prevtext": "\n  where the only thing we know is that \n  \n", "index": 3, "text": "\\begin{equation}\\label{eq:error_rate}\n    {\\mathrm{ed}} \\left( R_i, {\\widetilde}{R}_i \\right) \\leq {\\varepsilon} L, \\qquad \\text{ for every } i \\in \\left[N\\right],\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(R_{i},{\\widetilde{}}{R}_{i}\\right)\\leq{\\varepsilon}L,\\qquad%&#10;\\text{ for every }i\\in\\left[N\\right],\" display=\"block\"><mrow><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><msub><mi>R</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mtext>\u00a0for every\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><mrow><mo>[</mo><mi>N</mi><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  for some absolute constant $C$, with probability at least $1 - \\delta$ (for all $n$ large enough).\n\\end{itemize}\n\n\\begin{figure}[h!]\n  \\centering\n  \\includegraphics[width=\\linewidth]{schematic.png}\n  \\caption{A schematic of the approximate reconstruction problem.}\n  \\label{fig:problem}\n\\end{figure}\n\nWhen ${\\varepsilon} = 0$ (i.e., there are no errors in the reads), this amounts to exact reconstruction of the original sequence, \nwhich was studied and solved in~\\cite{MoBrTs:13}. \n\nIf an algorithm achieves~\\eqref{eq:goal} for a given error rate ${\\varepsilon} > 0$ with a given constant $C$, \nthen we say that it is an \\emph{approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$}. \nNote that for the empty string $\\emptyset$ we have ${\\mathrm{ed}} \\left( X, \\emptyset \\right) = n$, \nso for a given approximation factor $C$ the problem is interesting only for ${\\varepsilon} < 1 / C$; \nof course the goal is minimize the approximation factor $C$. \n\n\n\n\n\n\\section{Results} \\label{sec:results} \n\n\nBefore presenting our results on the approximate reconstruction problem described above, \nwe first recall the main results of~\\cite{MoBrTs:13} characterizing the case of ${\\varepsilon} = 0$, i.e., error-free reads. \nIn both cases the interesting regime is when the read length $L$ scales as the logarithm of the sequence length $n$, \nso in the following we let \n$L = \\overline{L} \\ln \\left( n \\right)$, \nwhere $\\overline{L}$ is constant.\\footnote{For the sake of readability, we refrain from rounding non-integer values that are meant to be integers, such as $\\overline{L} \\ln \\left( n \\right)$.} \n\n\nWhen there are no errors in the reads, \nthen there are two obstructions to reconstruction.\nFirst, if the reads are too short, \nthen there will be repeats coming from different parts of the original sequence, \nwhich create ambiguity in reconstruction, \neven if all substrings of length $L$ of the sequence are given. \nThis observation goes back to the work of Ukkonen~\\cite{Ukkonen:92}, \nwho characterized the patterns that preclude exact reconstruction of the sequence. \nThis is often referred to as the \n\\emph{repeat-limited regime}. \n\n\nSecond, even if $L$ is large enough, it is necessary to have enough reads to cover the original sequence; \notherwise the data does not contain enough information for exact reconstruction. \nDefine \n$N_{{\\mathrm{cov}}} = N_{{\\mathrm{cov}}} \\left( n, L, \\delta \\right)$ \nas the minimum number of reads necessary such that with probability at least $1 - \\delta$ \nthe randomly located reads cover the entire original sequence. \nLander and Waterman~\\cite{LanderWaterman:88} first studied the coverage properties of shotgun sequencing \nand showed that \n$N_{{\\mathrm{cov}}} \\approx \\frac{n}{L} \\ln \\left( \\frac{n}{L \\delta} \\right)$. \nThis is often referred to as the \n\\emph{coverage-limited regime}. \n\n\nIn the main result of~\\cite{MoBrTs:13}, Motahari, Bresler, and Tse show that when there are no errors in the reads, \nthese are the only two obstructions to reconstruction. \n\n\\begin{theorem}[Exact reconstruction from error-free reads~\\cite{MoBrTs:13}] \\label{thm:MBT13}\n  If $\\overline{L} < 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then no algorithm can reconstruct the original sequence exactly with probability greater than $1/2 + o(1)$ (as $n \\to \\infty$). \n\n  If $\\overline{L} > 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then exact reconstruction is possible, \nand the necessary condition of coverage is essentially sufficient. \n  More precisely, let $N_{\\min} \\left( n, L, \\delta \\right)$ denote the minimum number of reads needed to reconstruct the original sequence exactly with probability at least $1 - \\delta$. \n  If $\\overline{L} > 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then for every fixed $\\delta \\in (0,1/2)$, \n \n", "itemtype": "equation", "pos": 9223, "prevtext": "\n  but otherwise the ${\\widetilde}{R}_i$ can be arbitrary.\\footnote{The choice of edit distance arises naturally from DNA sequencing; one can consider the problem with other notions of distance as well.}\n \\item The goal of an approximate reconstruction algorithm is to output a sequence ${\\widehat}{X} \\in \\Sigma^{*}$ such that \n    \n", "index": 5, "text": "\\begin{equation}\\label{eq:goal}\n      {\\mathrm{ed}} \\left( X, {\\widehat}{X} \\right) \\leq C {\\varepsilon} n\n    \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X,{\\widehat{}}{X}\\right)\\leq C{\\varepsilon}n\" display=\"block\"><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>X</mi><mo>,</mo><mrow><mover accent=\"true\"><mi/><mo>^</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n\\end{theorem}\n\nWhen the reads are corrupted, the two obstructions to reconstruction discussed above remain. \nIn fact, the repeat-limited regime is slightly larger in the presence of corruption. \nIf \n$\\overline{L} < 2 / \\left\\{  \\left( 1 - {\\varepsilon} \\right) \\ln \\left( \\left| \\Sigma \\right| \\right) \\right\\}$ \nthen consider the corruption process which deletes the last ${\\varepsilon} L$ coordinates of every read. \nThe reads then have normalized length $\\left( 1 - {\\varepsilon} \\right) \\overline{L} < 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, \nso by Theorem~\\ref{thm:MBT13} exact reconstruction is impossible. \n\n\n\nThe picture describing successful algorithms is less clean when the reads are corrupted. \nThis is primarily due to the desideratum of approximate reconstruction. \nWhen there are no errors, the success of an algorithm is binary: either it reconstructs the original sequence exactly or it does not. \nHere, however, an algorithm can have varying degrees of success based on the approximation factor $C$ that it achieves in~\\eqref{eq:goal}. \nEven for those parameters $\\left( \\overline{L}, N \\right)$ for which an algorithm with finite approximation factor $C$ exists, the best achievable $C$ might depend on $\\left( \\overline{L}, N \\right)$. \n\n\nWith this in mind, our goal in this paper is to show that \nwhen $\\overline{L}$ and $N$ are large enough, \nthen there exists an approximate reconstruction algorithm with finite approximation factor. \n\n\n\\begin{theorem}[Approximate reconstruction from corrupted reads]\\label{thm:main_eps}\n  There exist constants $C$ and $\\overline{C}$, depending only on $\\left| \\Sigma \\right|$, such that \n  for every ${\\varepsilon} > 0$, \n  if $\\overline{L} > \\overline{C} / {\\varepsilon}$ and $N > N_{{\\mathrm{cov}}} / {\\varepsilon}$, \n  then there exists an approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$. \n\\end{theorem}\n\n\nWe show that a simple sequential algorithm achieves this result. \nStarting with an arbitrary read, \nat each step of the algorithm we find a read that overlaps with the current partially reconstructed sequence \nand extend the sequence using this read. \nIf $\\overline{L}$ and $N$ are large enough, \nthen in each step of this process we extend the sequence by at least $cL$ for some positive constant $c$, \nwhile incurring an error in edit distance of at most $O({\\varepsilon}) L$. \nEstimates on the edit distance between random strings with some overlap are crucially used in the analysis. \nThe algorithm terminates when it has approximately reached both ends of the original sequence, \nand results in an estimate with the guarantee given by Theorem~\\ref{thm:main_eps}. \n\n\nSeveral variants of such an algorithm can be considered, \nand it can be shown using one of them that \nthe dependence of $\\overline{L}$ and $N$ on ${\\varepsilon}$ in Theorem~\\ref{thm:main_eps} is not necessary. \nHowever, we decided to focus on one particular variant \nbecause it results in a small approximation factor (close to $3$) \nwhen ${\\varepsilon}$ is small enough, as stated in the following theorem. \n\n\n\\begin{theorem}[Approximate reconstruction from corrupted reads]\\label{thm:main_eps_3}\n  For every $C > 3$ \n  there exist constants $\\overline{C} = \\overline{C} \\left( \\Sigma \\right)$, ${\\varepsilon}_0 = {\\varepsilon}_0 \\left( \\Sigma, C \\right)$ and $C' = C' \\left( \\Sigma, C \\right)$ \n  such that for every ${\\varepsilon}\\in \\left( 0, {\\varepsilon}_0 \\right)$ \n  if $\\overline{L} \\geq \\overline{C} / {\\varepsilon}$ and $N \\geq C' N_{{\\mathrm{cov}}} / {\\varepsilon}$, \n  then there exists an approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$. \n\\end{theorem}\n\n\nThe closest results to Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3} in the literature are those of~\\cite{MoRaTsMa:13} and~\\cite{ShCoTs:15}. \nIn~\\cite{MoRaTsMa:13} the authors consider i.i.d.\\ noise affecting the reads of an i.i.d.\\ sequence, \nand show that perfect layout (where all the reads are mapped correctly to their true locations) is possible even \nwhen the noise level is relatively high. \nThe main reason for this positive result is that the independent noise assumption allows error correction of the reads by averaging; \nin the adversarial error model considered here such averaging is not always possible, hence the weaker goal of approximate reconstruction and the results of Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3}. \n\n\nIn the recent follow-up work~\\cite{ShCoTs:15}, the authors show positive results for a more realistic adversarial error model. \nThey consider arbitrary sequences and give a bound on the read length, as a function of the repeat statistics of the sequence and the error rate, above which perfect assembly is possible. \nHowever, the model they consider simplifies several aspects of the problem. \nThe authors mention several possible extensions as avenues for future work, two of which we consider in this paper:\nmore general errors, and a shotgun read model. \nFor one, they specifically consider \\emph{erasure errors}, where symbols in a read are erased, but the locations of the erasures are known. \nFurthermore they assume bounds not only on the number of erasures in a read, but also on the number of reads in which a given base is erased. \nThis means that the reads contain information about the whole sequence. \nThe error model we consider is much more adversarial, \ne.g., it can happen that even all the reads together contain no information at all about ${\\varepsilon} n$ bases of the sequence due to deletions. \nAlso, they consider a dense-read model, where all reads of length $L$ of the original sequence are provided, \ntherefore bypassing the question of coverage depth necessary for assembly. \nHere we instead consider the more realistic shotgun read model and provide a sufficient bridging condition for approximate reconstruction. \n\n\n\nIn summary, while our reconstruction results are weaker than previous ones, \nthis is due to the much stricter adversarial error model we consider. \nGoing forward, the main challenge is to bridge the gap between these models and results. \n\n\n\n\n\n\n\\section{Sequential reconstruction algorithm and its analysis} \\label{sec:aofa} \n\n\nWe first present some results on the edit distance between random strings in Section~\\ref{sec:edit} \nthat then allow us to present a simple sequential reconstruction algorithm in Section~\\ref{sec:algo}. \nThis algorithm is then analyzed and shown to have the desired performance in Section~\\ref{sec:analysis}. \n\n\n\n\\subsection{Results on the edit distance between random strings} \\label{sec:edit} \n\n\n\nSince the only information we have about the corrupted reads is that their edit distance from the actual reads is not too large (see~\\eqref{eq:error_rate}), \nit is essential for any reconstruction algorithm to have a good understanding of and to make use of the edit distance between pairs of reads. \nAccordingly, we now present results on the edit distance between random strings with overlap; \nthe proofs of these results are in Appendix~\\ref{sec:edit_distance_proofs}. \n\n\nConsider two random strings with overlap.  \nSimulations show (see Figure~\\ref{fig:edit}) \nthat there is a phase transition in the edit distance between the two as a function of the overlap. \nIf the overlap is above a certain threshold (which is linear in the length of the strings), \nthen the edit distance is exactly twice the length of the overhang; \nif the overlap is below this threshold, \nthen the edit distance between the two strings is as if they were completely independent. \nThe results below rigorously verify certain aspects of this picture.\n\n\n\\begin{figure}[h!]\n  \\centering\n  \\includegraphics[width=0.45\\linewidth]{avg_new.png}\n  \\qquad  \n  \\includegraphics[width=0.45\\linewidth]{avg_overlap_s4_new.png}\n  \\caption{The left plot shows the empirical average edit distance between two independent strings of length $n$, where $n$ ranges from $250$ to $10^4$, and $\\left|\\Sigma\\right|$ is $2$ (black) or $4$ (blue). \n  The lines $0.29 n$ (for $\\left|\\Sigma\\right| = 2$) and $0.518 n$ (for $\\left| \\Sigma \\right| = 4$) show a good fit to the data, though the limiting slope appears to be somewhat smaller than $0.29$ and $0.518$, respectively. \n  The right plot shows the empirical average edit distance between two strings of length $n = 10^3$ and with $\\left| \\Sigma \\right| = 4$, where one string is a shift of the other, as in the setup of Lemma~\\ref{lem:large_overlap}. \n  The function $f$ is piecewise linear with two pieces: \n  it is equal to twice the length of the shift when the shift percentage is less than $26.25\\%$, \n  and it is equal to the constant $525$ ($52.5\\%$ of the length $n = 10^3$) when the shift percentage is more than $26.25\\%$.\n  In both plots the average is over $10^3$ runs and the error bars show plus/minus $3$ times the empirical standard deviation.}\n  \\label{fig:edit}\n\\end{figure}\n\n\n \n\\begin{lemma}\\label{lem:indpt}\n  Let $X_m, Y_m \\in \\Sigma^m$ be two independent uniformly random strings. \n  There exists an absolute constant $c_{{\\mathrm{ind}}} = c_{{\\mathrm{ind}}} \\left( \\Sigma \\right) > 0$ such that almost surely\n \n", "itemtype": "equation", "pos": 13171, "prevtext": "\n  for some absolute constant $C$, with probability at least $1 - \\delta$ (for all $n$ large enough).\n\\end{itemize}\n\n\\begin{figure}[h!]\n  \\centering\n  \\includegraphics[width=\\linewidth]{schematic.png}\n  \\caption{A schematic of the approximate reconstruction problem.}\n  \\label{fig:problem}\n\\end{figure}\n\nWhen ${\\varepsilon} = 0$ (i.e., there are no errors in the reads), this amounts to exact reconstruction of the original sequence, \nwhich was studied and solved in~\\cite{MoBrTs:13}. \n\nIf an algorithm achieves~\\eqref{eq:goal} for a given error rate ${\\varepsilon} > 0$ with a given constant $C$, \nthen we say that it is an \\emph{approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$}. \nNote that for the empty string $\\emptyset$ we have ${\\mathrm{ed}} \\left( X, \\emptyset \\right) = n$, \nso for a given approximation factor $C$ the problem is interesting only for ${\\varepsilon} < 1 / C$; \nof course the goal is minimize the approximation factor $C$. \n\n\n\n\n\n\\section{Results} \\label{sec:results} \n\n\nBefore presenting our results on the approximate reconstruction problem described above, \nwe first recall the main results of~\\cite{MoBrTs:13} characterizing the case of ${\\varepsilon} = 0$, i.e., error-free reads. \nIn both cases the interesting regime is when the read length $L$ scales as the logarithm of the sequence length $n$, \nso in the following we let \n$L = \\overline{L} \\ln \\left( n \\right)$, \nwhere $\\overline{L}$ is constant.\\footnote{For the sake of readability, we refrain from rounding non-integer values that are meant to be integers, such as $\\overline{L} \\ln \\left( n \\right)$.} \n\n\nWhen there are no errors in the reads, \nthen there are two obstructions to reconstruction.\nFirst, if the reads are too short, \nthen there will be repeats coming from different parts of the original sequence, \nwhich create ambiguity in reconstruction, \neven if all substrings of length $L$ of the sequence are given. \nThis observation goes back to the work of Ukkonen~\\cite{Ukkonen:92}, \nwho characterized the patterns that preclude exact reconstruction of the sequence. \nThis is often referred to as the \n\\emph{repeat-limited regime}. \n\n\nSecond, even if $L$ is large enough, it is necessary to have enough reads to cover the original sequence; \notherwise the data does not contain enough information for exact reconstruction. \nDefine \n$N_{{\\mathrm{cov}}} = N_{{\\mathrm{cov}}} \\left( n, L, \\delta \\right)$ \nas the minimum number of reads necessary such that with probability at least $1 - \\delta$ \nthe randomly located reads cover the entire original sequence. \nLander and Waterman~\\cite{LanderWaterman:88} first studied the coverage properties of shotgun sequencing \nand showed that \n$N_{{\\mathrm{cov}}} \\approx \\frac{n}{L} \\ln \\left( \\frac{n}{L \\delta} \\right)$. \nThis is often referred to as the \n\\emph{coverage-limited regime}. \n\n\nIn the main result of~\\cite{MoBrTs:13}, Motahari, Bresler, and Tse show that when there are no errors in the reads, \nthese are the only two obstructions to reconstruction. \n\n\\begin{theorem}[Exact reconstruction from error-free reads~\\cite{MoBrTs:13}] \\label{thm:MBT13}\n  If $\\overline{L} < 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then no algorithm can reconstruct the original sequence exactly with probability greater than $1/2 + o(1)$ (as $n \\to \\infty$). \n\n  If $\\overline{L} > 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then exact reconstruction is possible, \nand the necessary condition of coverage is essentially sufficient. \n  More precisely, let $N_{\\min} \\left( n, L, \\delta \\right)$ denote the minimum number of reads needed to reconstruct the original sequence exactly with probability at least $1 - \\delta$. \n  If $\\overline{L} > 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, then for every fixed $\\delta \\in (0,1/2)$, \n \n", "index": 7, "text": "\\[\n    \\lim_{n \\to \\infty, L = \\overline{L} \\ln \\left(n\\right)} \\frac{N_{\\min} \\left( n, L, \\delta \\right)}{N_{{\\mathrm{cov}}} \\left( n, L, \\delta \\right)} = 1.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\lim_{n\\to\\infty,L=\\overline{L}\\ln\\left(n\\right)}\\frac{N_{\\min}\\left(n,L,%&#10;\\delta\\right)}{N_{{\\mathrm{cov}}}\\left(n,L,\\delta\\right)}=1.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mrow><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mo>,</mo><mrow><mi>L</mi><mo>=</mo><mrow><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></munder><mo>\u2061</mo><mfrac><mrow><msub><mi>N</mi><mi>min</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>n</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>\u03b4</mi><mo>)</mo></mrow></mrow><mrow><msub><mi>N</mi><mi>cov</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>n</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>\u03b4</mi><mo>)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n\\end{lemma}\n\nDetermining the value of the limiting constant $c_{{\\mathrm{ind}}}$ is a challenging open problem. \nWhen $\\left| \\Sigma \\right| = 4$, as in the case of DNA sequencing, \nsimulations suggest that $c_{{\\mathrm{ind}}} \\approx 0.51$, \nwhile we show a simple lower bound of $c_{{\\mathrm{ind}}} > 0.338$. \n\n\n\\begin{lemma}\\label{lem:small_overlap}\n  Let $X \\in \\Sigma^{2m}$ be a uniformly random string. \n  For every $d \\in \\left( 0, 1 \\right)$ there exist  positive constants $\\gamma = \\gamma \\left( d, \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that \n \n", "itemtype": "equation", "pos": 22523, "prevtext": "\n\\end{theorem}\n\nWhen the reads are corrupted, the two obstructions to reconstruction discussed above remain. \nIn fact, the repeat-limited regime is slightly larger in the presence of corruption. \nIf \n$\\overline{L} < 2 / \\left\\{  \\left( 1 - {\\varepsilon} \\right) \\ln \\left( \\left| \\Sigma \\right| \\right) \\right\\}$ \nthen consider the corruption process which deletes the last ${\\varepsilon} L$ coordinates of every read. \nThe reads then have normalized length $\\left( 1 - {\\varepsilon} \\right) \\overline{L} < 2 / \\ln \\left( \\left| \\Sigma \\right| \\right)$, \nso by Theorem~\\ref{thm:MBT13} exact reconstruction is impossible. \n\n\n\nThe picture describing successful algorithms is less clean when the reads are corrupted. \nThis is primarily due to the desideratum of approximate reconstruction. \nWhen there are no errors, the success of an algorithm is binary: either it reconstructs the original sequence exactly or it does not. \nHere, however, an algorithm can have varying degrees of success based on the approximation factor $C$ that it achieves in~\\eqref{eq:goal}. \nEven for those parameters $\\left( \\overline{L}, N \\right)$ for which an algorithm with finite approximation factor $C$ exists, the best achievable $C$ might depend on $\\left( \\overline{L}, N \\right)$. \n\n\nWith this in mind, our goal in this paper is to show that \nwhen $\\overline{L}$ and $N$ are large enough, \nthen there exists an approximate reconstruction algorithm with finite approximation factor. \n\n\n\\begin{theorem}[Approximate reconstruction from corrupted reads]\\label{thm:main_eps}\n  There exist constants $C$ and $\\overline{C}$, depending only on $\\left| \\Sigma \\right|$, such that \n  for every ${\\varepsilon} > 0$, \n  if $\\overline{L} > \\overline{C} / {\\varepsilon}$ and $N > N_{{\\mathrm{cov}}} / {\\varepsilon}$, \n  then there exists an approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$. \n\\end{theorem}\n\n\nWe show that a simple sequential algorithm achieves this result. \nStarting with an arbitrary read, \nat each step of the algorithm we find a read that overlaps with the current partially reconstructed sequence \nand extend the sequence using this read. \nIf $\\overline{L}$ and $N$ are large enough, \nthen in each step of this process we extend the sequence by at least $cL$ for some positive constant $c$, \nwhile incurring an error in edit distance of at most $O({\\varepsilon}) L$. \nEstimates on the edit distance between random strings with some overlap are crucially used in the analysis. \nThe algorithm terminates when it has approximately reached both ends of the original sequence, \nand results in an estimate with the guarantee given by Theorem~\\ref{thm:main_eps}. \n\n\nSeveral variants of such an algorithm can be considered, \nand it can be shown using one of them that \nthe dependence of $\\overline{L}$ and $N$ on ${\\varepsilon}$ in Theorem~\\ref{thm:main_eps} is not necessary. \nHowever, we decided to focus on one particular variant \nbecause it results in a small approximation factor (close to $3$) \nwhen ${\\varepsilon}$ is small enough, as stated in the following theorem. \n\n\n\\begin{theorem}[Approximate reconstruction from corrupted reads]\\label{thm:main_eps_3}\n  For every $C > 3$ \n  there exist constants $\\overline{C} = \\overline{C} \\left( \\Sigma \\right)$, ${\\varepsilon}_0 = {\\varepsilon}_0 \\left( \\Sigma, C \\right)$ and $C' = C' \\left( \\Sigma, C \\right)$ \n  such that for every ${\\varepsilon}\\in \\left( 0, {\\varepsilon}_0 \\right)$ \n  if $\\overline{L} \\geq \\overline{C} / {\\varepsilon}$ and $N \\geq C' N_{{\\mathrm{cov}}} / {\\varepsilon}$, \n  then there exists an approximate reconstruction algorithm for error rate ${\\varepsilon}$ with approximation factor $C$. \n\\end{theorem}\n\n\nThe closest results to Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3} in the literature are those of~\\cite{MoRaTsMa:13} and~\\cite{ShCoTs:15}. \nIn~\\cite{MoRaTsMa:13} the authors consider i.i.d.\\ noise affecting the reads of an i.i.d.\\ sequence, \nand show that perfect layout (where all the reads are mapped correctly to their true locations) is possible even \nwhen the noise level is relatively high. \nThe main reason for this positive result is that the independent noise assumption allows error correction of the reads by averaging; \nin the adversarial error model considered here such averaging is not always possible, hence the weaker goal of approximate reconstruction and the results of Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3}. \n\n\nIn the recent follow-up work~\\cite{ShCoTs:15}, the authors show positive results for a more realistic adversarial error model. \nThey consider arbitrary sequences and give a bound on the read length, as a function of the repeat statistics of the sequence and the error rate, above which perfect assembly is possible. \nHowever, the model they consider simplifies several aspects of the problem. \nThe authors mention several possible extensions as avenues for future work, two of which we consider in this paper:\nmore general errors, and a shotgun read model. \nFor one, they specifically consider \\emph{erasure errors}, where symbols in a read are erased, but the locations of the erasures are known. \nFurthermore they assume bounds not only on the number of erasures in a read, but also on the number of reads in which a given base is erased. \nThis means that the reads contain information about the whole sequence. \nThe error model we consider is much more adversarial, \ne.g., it can happen that even all the reads together contain no information at all about ${\\varepsilon} n$ bases of the sequence due to deletions. \nAlso, they consider a dense-read model, where all reads of length $L$ of the original sequence are provided, \ntherefore bypassing the question of coverage depth necessary for assembly. \nHere we instead consider the more realistic shotgun read model and provide a sufficient bridging condition for approximate reconstruction. \n\n\n\nIn summary, while our reconstruction results are weaker than previous ones, \nthis is due to the much stricter adversarial error model we consider. \nGoing forward, the main challenge is to bridge the gap between these models and results. \n\n\n\n\n\n\n\\section{Sequential reconstruction algorithm and its analysis} \\label{sec:aofa} \n\n\nWe first present some results on the edit distance between random strings in Section~\\ref{sec:edit} \nthat then allow us to present a simple sequential reconstruction algorithm in Section~\\ref{sec:algo}. \nThis algorithm is then analyzed and shown to have the desired performance in Section~\\ref{sec:analysis}. \n\n\n\n\\subsection{Results on the edit distance between random strings} \\label{sec:edit} \n\n\n\nSince the only information we have about the corrupted reads is that their edit distance from the actual reads is not too large (see~\\eqref{eq:error_rate}), \nit is essential for any reconstruction algorithm to have a good understanding of and to make use of the edit distance between pairs of reads. \nAccordingly, we now present results on the edit distance between random strings with overlap; \nthe proofs of these results are in Appendix~\\ref{sec:edit_distance_proofs}. \n\n\nConsider two random strings with overlap.  \nSimulations show (see Figure~\\ref{fig:edit}) \nthat there is a phase transition in the edit distance between the two as a function of the overlap. \nIf the overlap is above a certain threshold (which is linear in the length of the strings), \nthen the edit distance is exactly twice the length of the overhang; \nif the overlap is below this threshold, \nthen the edit distance between the two strings is as if they were completely independent. \nThe results below rigorously verify certain aspects of this picture.\n\n\n\\begin{figure}[h!]\n  \\centering\n  \\includegraphics[width=0.45\\linewidth]{avg_new.png}\n  \\qquad  \n  \\includegraphics[width=0.45\\linewidth]{avg_overlap_s4_new.png}\n  \\caption{The left plot shows the empirical average edit distance between two independent strings of length $n$, where $n$ ranges from $250$ to $10^4$, and $\\left|\\Sigma\\right|$ is $2$ (black) or $4$ (blue). \n  The lines $0.29 n$ (for $\\left|\\Sigma\\right| = 2$) and $0.518 n$ (for $\\left| \\Sigma \\right| = 4$) show a good fit to the data, though the limiting slope appears to be somewhat smaller than $0.29$ and $0.518$, respectively. \n  The right plot shows the empirical average edit distance between two strings of length $n = 10^3$ and with $\\left| \\Sigma \\right| = 4$, where one string is a shift of the other, as in the setup of Lemma~\\ref{lem:large_overlap}. \n  The function $f$ is piecewise linear with two pieces: \n  it is equal to twice the length of the shift when the shift percentage is less than $26.25\\%$, \n  and it is equal to the constant $525$ ($52.5\\%$ of the length $n = 10^3$) when the shift percentage is more than $26.25\\%$.\n  In both plots the average is over $10^3$ runs and the error bars show plus/minus $3$ times the empirical standard deviation.}\n  \\label{fig:edit}\n\\end{figure}\n\n\n \n\\begin{lemma}\\label{lem:indpt}\n  Let $X_m, Y_m \\in \\Sigma^m$ be two independent uniformly random strings. \n  There exists an absolute constant $c_{{\\mathrm{ind}}} = c_{{\\mathrm{ind}}} \\left( \\Sigma \\right) > 0$ such that almost surely\n \n", "index": 9, "text": "\\[\n   \\lim_{m \\to \\infty} \\frac{1}{m} {\\mathrm{ed}} \\left( X_m, Y_m \\right) = c_{{\\mathrm{ind}}}.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\lim_{m\\to\\infty}\\frac{1}{m}{\\mathrm{ed}}\\left(X_{m},Y_{m}\\right)=c_{{\\mathrm{%&#10;ind}}}.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>m</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>X</mi><mi>m</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo>)</mo></mrow></mrow></mrow><mo>=</mo><msub><mi>c</mi><mi>ind</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  for all $k \\ge d m$ with probability at least $1 - e^{-c' d m}$. \n\\end{lemma}\n\nIn particular, when $\\left| \\Sigma \\right| = 4$, then we can take \n$\\gamma = 0.338 \\times d$. \n\n\\begin{lemma}\\label{lem:large_overlap}\n  Let $X \\in \\Sigma^{2m}$ be a uniformly random string. \n  There exist positive constants $c = c \\left( \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that \n \n", "itemtype": "equation", "pos": 23200, "prevtext": "\n\\end{lemma}\n\nDetermining the value of the limiting constant $c_{{\\mathrm{ind}}}$ is a challenging open problem. \nWhen $\\left| \\Sigma \\right| = 4$, as in the case of DNA sequencing, \nsimulations suggest that $c_{{\\mathrm{ind}}} \\approx 0.51$, \nwhile we show a simple lower bound of $c_{{\\mathrm{ind}}} > 0.338$. \n\n\n\\begin{lemma}\\label{lem:small_overlap}\n  Let $X \\in \\Sigma^{2m}$ be a uniformly random string. \n  For every $d \\in \\left( 0, 1 \\right)$ there exist  positive constants $\\gamma = \\gamma \\left( d, \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that \n \n", "index": 11, "text": "\\[\n    {\\mathrm{ed}} \\left( X \\left[ 1 , m \\right], X \\left[k + 1, k + m  \\right] \\right) \\geq \\gamma m\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X\\left[1,m\\right],X\\left[k+1,k+m\\right]\\right)\\geq\\gamma m\" display=\"block\"><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mi>m</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi>m</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  for all $k \\leq c m$ with probability at least $1 - e^{-c' m}$. \n\\end{lemma}\n\nWe denote by $\\kappa_{{\\mathrm{ed}}} = \\kappa_{{\\mathrm{ed}}} \\left( \\Sigma \\right)$ the supremum of all constants $c = c \\left( \\Sigma \\right)$ for which Lemma~\\ref{lem:large_overlap} holds (with some $c' = c' \\left( \\Sigma \\right)$). \nFigure~\\ref{fig:edit} suggests that $\\kappa_{{\\mathrm{ed}}} = c_{{\\mathrm{ind}}} / 2$.  \nWe show that for $\\left| \\Sigma \\right| = 4$, $\\kappa_{{\\mathrm{ed}}} > 0.0846$.\n\n\nAs a corollary of the lemmas we obtain the following result. \n\\begin{corollary}\\label{cor:shift} \n  Let $X \\in \\Sigma^n$ be uniformly random and let $L = \\overline{L} \\ln \\left( n \\right)$. \n  If the constant $\\overline{L} = \\overline{L} \\left( \\Sigma \\right)$ is large enough, \n  then there exists a positive constant $c = c \\left( \\Sigma \\right)$ such that \n  with probability going to $1$ as $n \\to \\infty$ \n  the following holds for all $i,j \\in \\left[ n - L + 1 \\right]$: \n  \\begin{enumerate}[(a)]\n   \\item if $\\left| i - j \\right| \\leq c L$, then ${\\mathrm{ed}} \\left( X \\left[ i, i + L - 1 \\right], X \\left[ j, j + L - 1 \\right] \\right) = 2 \\left| i - j \\right|$;\n   \\item otherwise ${\\mathrm{ed}} \\left( X \\left[ i, i + L - 1 \\right], X \\left[ j, j + L - 1 \\right] \\right) \\geq 2 c L$.\n  \\end{enumerate}\n\\end{corollary}\n\n\\begin{proof}\nThis follows directly from Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap}, \ntogether with a union bound over all possible pairs $i, j \\in \\left[ n - L + 1 \\right]$. \nThe constant $\\overline{L}$ needs to be chosen large enough so that the error probabilities in Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap} are $o \\left( n^{-2} \\right)$. \n\\end{proof}\n\n\n\n\n\n\n\\subsection{Sequential reconstruction algorithm} \\label{sec:algo} \n\n\n\nWe present now a simple sequential approximate reconstruction algorithm. \nThe algorithm takes as a parameter the number of reads; \nthis guarantees a certain amount of coverage. \nLet $N = C' N_{{\\mathrm{cov}}} / {\\varepsilon}$ and $c' = 1/C'$, where we assume that $C' \\geq 1$; \nstandard results~\\cite{LanderWaterman:88} imply that then with probability at least $1 - \\delta / 2$ \nthere is no gap greater than $1.1\\times c' {\\varepsilon} L$ in between subsequent starting points of the (yet uncorrupted) reads. \nWe fix $\\alpha = 4 / c \\left( \\Sigma \\right)$, where $c \\left( \\Sigma \\right)$ is given by Corollary~\\ref{cor:shift}. \nFor the purposes of Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3} we may and will assume that ${\\varepsilon}$ is small enough; \nin particular we assume that ${\\varepsilon} \\leq 1 / \\left( 3 \\alpha \\right)$. \n\n\nBefore specifying the algorithm we introduce further notation. \nLet negative integers denote counting coordinates from the opposite end of a sequence, \ne.g., for $x \\in \\Sigma^m$, $x \\left[ - k, - 1 \\right]$ denotes the suffix of $x$ of length $k$. \nFurthermore let $I : {\\widetilde}{\\mathcal{R}} \\to \\left[ N \\right]$ denote the map that takes a corrupted read to its index, i.e., $I \\left( {\\widetilde}{R}_i \\right) = i$. \n\n\nThe algorithm is as follows:\n\\begin{algorithmic}[1]\n\\State Let $Y = {\\widetilde}{R}_1$ and set $k = 1$.\n\\While{there exists ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ such that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_k \\left[ - \\alpha {\\varepsilon} L, - 1 \\right], {\\widetilde}{R} \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$}\\label{alg:right}\n\\State choose any such ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$;\n\\State $Y \\gets$ the concatenation of $Y$ and ${\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, - 1 \\right]$;\n\\State $k \\gets I \\left( {\\widetilde}{R} \\right)$.\n\\EndWhile\n\\State Set $k = 1$. \n\\While{there exists ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ such that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ - \\alpha {\\varepsilon} L, - 1 \\right], {\\widetilde}{R}_k \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$}\\label{alg:left}\n\\State choose any such ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$;\n\\State $Y \\gets$ the concatenation of ${\\widetilde}{R} \\left[ 1, - \\alpha {\\varepsilon} L - 1 \\right]$ and $Y$;\n\\State $k \\gets I \\left( {\\widetilde}{R} \\right)$.\n\\EndWhile\n\\end{algorithmic}\n\n\nIn words: we take an arbitrary read, extend it to the right until we possibly can (first while loop), \nand then extend it to the left until we can (second while loop). \n\n\n\n\n\n\\subsection{Analysis of the sequential algorithm} \\label{sec:analysis} \n\n\nWe now analyze the algorithm presented above and as a consequence prove our results:  \nTheorem~\\ref{thm:main_eps} follows by taking $C' = 1$, \nwhile Theorem~\\ref{thm:main_eps_3} follows by taking $C'$ large enough. \n\n\nWe first recall a fact that follows directly from the dynamic programming algorithm for computing the edit distance. \nFor any $m$, sequences $x, y \\in \\Sigma^m$, and $i,j < m$, we have\n\n", "itemtype": "equation", "pos": 23694, "prevtext": "\n  for all $k \\ge d m$ with probability at least $1 - e^{-c' d m}$. \n\\end{lemma}\n\nIn particular, when $\\left| \\Sigma \\right| = 4$, then we can take \n$\\gamma = 0.338 \\times d$. \n\n\\begin{lemma}\\label{lem:large_overlap}\n  Let $X \\in \\Sigma^{2m}$ be a uniformly random string. \n  There exist positive constants $c = c \\left( \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that \n \n", "index": 13, "text": "\\[\n    {\\mathrm{ed}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = 2k\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X\\left[1,m\\right],X\\left[1+k,m+k\\right]\\right)=2k\" display=\"block\"><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>k</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nIn words: deleting a coordinate from the end of both $x$ and $y$ cannot increase their edit distance. \n\n\nThe first thing we have to understand is the set of corrupted reads that satisfy the conditions of the while loops in \nthe algorithm; \nwe focus on the first while loop as the second one is analogous. \nThe following lemma says that if two corrupted reads are such that the length $\\alpha {\\varepsilon} L$ prefix of one and suffix of the other are close in edit distance, \nthen the starting points of these reads are approximately $\\left( 1 - \\alpha {\\varepsilon} \\right) L$ apart. \n\n\n\\begin{lemma}\\label{lem:close_reads}\n  Let $X \\in \\Sigma^n$ be a uniformly random string, \n  let $L = \\left( \\overline{C} / {\\varepsilon} \\right) \\ln \\left( n \\right)$, \n  and let ${\\widetilde}{R}_1, {\\widetilde}{R}_2 \\in {\\widetilde}{\\mathcal{R}}$ be two corrupted reads of $X$ of length $L$.\n  Suppose that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R}_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2 c' \\right) {\\varepsilon} L$. \n  If $\\overline{C}$ is large enough, then with probability $1 - o \\left( n^{-2} \\right)$ we have that \n  \n", "itemtype": "equation", "pos": 28728, "prevtext": "\n  for all $k \\leq c m$ with probability at least $1 - e^{-c' m}$. \n\\end{lemma}\n\nWe denote by $\\kappa_{{\\mathrm{ed}}} = \\kappa_{{\\mathrm{ed}}} \\left( \\Sigma \\right)$ the supremum of all constants $c = c \\left( \\Sigma \\right)$ for which Lemma~\\ref{lem:large_overlap} holds (with some $c' = c' \\left( \\Sigma \\right)$). \nFigure~\\ref{fig:edit} suggests that $\\kappa_{{\\mathrm{ed}}} = c_{{\\mathrm{ind}}} / 2$.  \nWe show that for $\\left| \\Sigma \\right| = 4$, $\\kappa_{{\\mathrm{ed}}} > 0.0846$.\n\n\nAs a corollary of the lemmas we obtain the following result. \n\\begin{corollary}\\label{cor:shift} \n  Let $X \\in \\Sigma^n$ be uniformly random and let $L = \\overline{L} \\ln \\left( n \\right)$. \n  If the constant $\\overline{L} = \\overline{L} \\left( \\Sigma \\right)$ is large enough, \n  then there exists a positive constant $c = c \\left( \\Sigma \\right)$ such that \n  with probability going to $1$ as $n \\to \\infty$ \n  the following holds for all $i,j \\in \\left[ n - L + 1 \\right]$: \n  \\begin{enumerate}[(a)]\n   \\item if $\\left| i - j \\right| \\leq c L$, then ${\\mathrm{ed}} \\left( X \\left[ i, i + L - 1 \\right], X \\left[ j, j + L - 1 \\right] \\right) = 2 \\left| i - j \\right|$;\n   \\item otherwise ${\\mathrm{ed}} \\left( X \\left[ i, i + L - 1 \\right], X \\left[ j, j + L - 1 \\right] \\right) \\geq 2 c L$.\n  \\end{enumerate}\n\\end{corollary}\n\n\\begin{proof}\nThis follows directly from Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap}, \ntogether with a union bound over all possible pairs $i, j \\in \\left[ n - L + 1 \\right]$. \nThe constant $\\overline{L}$ needs to be chosen large enough so that the error probabilities in Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap} are $o \\left( n^{-2} \\right)$. \n\\end{proof}\n\n\n\n\n\n\n\\subsection{Sequential reconstruction algorithm} \\label{sec:algo} \n\n\n\nWe present now a simple sequential approximate reconstruction algorithm. \nThe algorithm takes as a parameter the number of reads; \nthis guarantees a certain amount of coverage. \nLet $N = C' N_{{\\mathrm{cov}}} / {\\varepsilon}$ and $c' = 1/C'$, where we assume that $C' \\geq 1$; \nstandard results~\\cite{LanderWaterman:88} imply that then with probability at least $1 - \\delta / 2$ \nthere is no gap greater than $1.1\\times c' {\\varepsilon} L$ in between subsequent starting points of the (yet uncorrupted) reads. \nWe fix $\\alpha = 4 / c \\left( \\Sigma \\right)$, where $c \\left( \\Sigma \\right)$ is given by Corollary~\\ref{cor:shift}. \nFor the purposes of Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3} we may and will assume that ${\\varepsilon}$ is small enough; \nin particular we assume that ${\\varepsilon} \\leq 1 / \\left( 3 \\alpha \\right)$. \n\n\nBefore specifying the algorithm we introduce further notation. \nLet negative integers denote counting coordinates from the opposite end of a sequence, \ne.g., for $x \\in \\Sigma^m$, $x \\left[ - k, - 1 \\right]$ denotes the suffix of $x$ of length $k$. \nFurthermore let $I : {\\widetilde}{\\mathcal{R}} \\to \\left[ N \\right]$ denote the map that takes a corrupted read to its index, i.e., $I \\left( {\\widetilde}{R}_i \\right) = i$. \n\n\nThe algorithm is as follows:\n\\begin{algorithmic}[1]\n\\State Let $Y = {\\widetilde}{R}_1$ and set $k = 1$.\n\\While{there exists ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ such that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_k \\left[ - \\alpha {\\varepsilon} L, - 1 \\right], {\\widetilde}{R} \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$}\\label{alg:right}\n\\State choose any such ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$;\n\\State $Y \\gets$ the concatenation of $Y$ and ${\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, - 1 \\right]$;\n\\State $k \\gets I \\left( {\\widetilde}{R} \\right)$.\n\\EndWhile\n\\State Set $k = 1$. \n\\While{there exists ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ such that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ - \\alpha {\\varepsilon} L, - 1 \\right], {\\widetilde}{R}_k \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$}\\label{alg:left}\n\\State choose any such ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$;\n\\State $Y \\gets$ the concatenation of ${\\widetilde}{R} \\left[ 1, - \\alpha {\\varepsilon} L - 1 \\right]$ and $Y$;\n\\State $k \\gets I \\left( {\\widetilde}{R} \\right)$.\n\\EndWhile\n\\end{algorithmic}\n\n\nIn words: we take an arbitrary read, extend it to the right until we possibly can (first while loop), \nand then extend it to the left until we can (second while loop). \n\n\n\n\n\n\\subsection{Analysis of the sequential algorithm} \\label{sec:analysis} \n\n\nWe now analyze the algorithm presented above and as a consequence prove our results:  \nTheorem~\\ref{thm:main_eps} follows by taking $C' = 1$, \nwhile Theorem~\\ref{thm:main_eps_3} follows by taking $C'$ large enough. \n\n\nWe first recall a fact that follows directly from the dynamic programming algorithm for computing the edit distance. \nFor any $m$, sequences $x, y \\in \\Sigma^m$, and $i,j < m$, we have\n\n", "index": 15, "text": "\\begin{equation}\\label{eq:ed_mon}\n  {\\mathrm{ed}} \\left( x \\left[ 1, i \\right], y \\left[ 1, j \\right] \\right) \n  \\leq \n  {\\mathrm{ed}} \\left( x \\left[ 1, i + 1 \\right], y \\left[ 1, j + 1 \\right] \\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(x\\left[1,i\\right],y\\left[1,j\\right]\\right)\\leq{\\mathrm{ed}}%&#10;\\left(x\\left[1,i+1\\right],y\\left[1,j+1\\right]\\right).\" display=\"block\"><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>i</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n\\end{lemma}\n\\begin{proof}\n  Suppose that~\\eqref{eq:shift} does not hold.  \n  Then the overlap between $R_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right]$ and $R_2 \\left[ 1, \\alpha {\\varepsilon} L \\right]$ is less than \n  $\\left( \\alpha - \\left( 2 + 2c'\\right) \\right) {\\varepsilon} L$. \n  Recall the definition of $c = c\\left( \\Sigma \\right)$ from Corollary~\\ref{cor:shift}. \n  Since $c \\alpha {\\varepsilon} L = 4 {\\varepsilon} L \\geq \\left( 2 + 2c' \\right) {\\varepsilon} L$, we can apply Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap} to get that \n  with probability $1 - o \\left( n^{-2} \\right)$ we have \n  ${\\mathrm{ed}} \\left( R_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], R_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\geq 2 \\left( 2 + 2 c' \\right) {\\varepsilon} L$. \n  By the triangle inequality this implies that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R}_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\geq \\left( 2 + 4 c' \\right) {\\varepsilon} L$, \n  which is a contradiction.  \n\\end{proof}\nSince the probability that the conclusion of the lemma does not hold is $o \\left( n^{-2} \\right)$, \nwe can take a union bound over all pairs of corrupted reads and have the conclusion of the lemma apply to all of them with probability $1 - o \\left( 1 \\right)$. \n\n\nWe are now ready to analyze the algorithm step by step. \nWe show by induction that after each extension step the partially reconstructed sequence is a good approximation of \na substring of the original sequence. \n\n\\begin{lemma}\\label{lem:analysis}\nLet $X \\in \\Sigma^n$ be a uniformly random string, \nlet $L = \\left( \\overline{C} / {\\varepsilon} \\right) \\ln \\left( n \\right)$ where $\\overline{C}$ is a large enough constant, \nand let $N = C' N_{{\\mathrm{cov}}} / {\\varepsilon}$. \nLet $\\alpha = 4 / c \\left( \\Sigma \\right)$, where $c \\left( \\Sigma \\right)$ is given by Corollary~\\ref{cor:shift}. \nLet $Y_i$ be the state of the partially reconstructed sequence $Y$ after $i$ corrupted reads have been processed by the algorithm; \nwe have $Y_1 = {\\widetilde}{R}_1$. \nLet $\\tau_1$ be the number of reads processed in the first while loop of the algorithm, \nand let $\\tau_2$ be the number of reads processed in the second while loop.  \nAlso let $\\tau = \\tau_1 + \\tau_2$, the total number of reads processed during the algorithm. \nWith probability at least $1 - \\delta$ \n(over the choice of $X$ and the starting points of the reads in $\\mathcal{R}$) \nwe have the following: \n\\begin{enumerate}[(a)]\n \\item\\label{lem:analysis_i} For every $i \\leq \\tau$ \n  there exist $a_i, b_i \\in \\left[ n \\right]$ such that \n  $\\left| a_i - b_i \\right| \\geq \\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) i L$ and \n  \n", "itemtype": "equation", "pos": 30152, "prevtext": "\nIn words: deleting a coordinate from the end of both $x$ and $y$ cannot increase their edit distance. \n\n\nThe first thing we have to understand is the set of corrupted reads that satisfy the conditions of the while loops in \nthe algorithm; \nwe focus on the first while loop as the second one is analogous. \nThe following lemma says that if two corrupted reads are such that the length $\\alpha {\\varepsilon} L$ prefix of one and suffix of the other are close in edit distance, \nthen the starting points of these reads are approximately $\\left( 1 - \\alpha {\\varepsilon} \\right) L$ apart. \n\n\n\\begin{lemma}\\label{lem:close_reads}\n  Let $X \\in \\Sigma^n$ be a uniformly random string, \n  let $L = \\left( \\overline{C} / {\\varepsilon} \\right) \\ln \\left( n \\right)$, \n  and let ${\\widetilde}{R}_1, {\\widetilde}{R}_2 \\in {\\widetilde}{\\mathcal{R}}$ be two corrupted reads of $X$ of length $L$.\n  Suppose that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R}_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2 c' \\right) {\\varepsilon} L$. \n  If $\\overline{C}$ is large enough, then with probability $1 - o \\left( n^{-2} \\right)$ we have that \n  \n", "index": 17, "text": "\\begin{equation}\\label{eq:shift}\n    T_2 \\in \\left[ T_1 + \\left( 1 - \\alpha {\\varepsilon} \\right) L - \\left( 2 + 2 c' \\right) {\\varepsilon} L, T_1 + \\left( 1 - \\alpha {\\varepsilon} \\right) L + \\left( 2 + 2 c' \\right) {\\varepsilon} L \\right].\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"T_{2}\\in\\left[T_{1}+\\left(1-\\alpha{\\varepsilon}\\right)L-\\left(2+2c^{\\prime}%&#10;\\right){\\varepsilon}L,T_{1}+\\left(1-\\alpha{\\varepsilon}\\right)L+\\left(2+2c^{%&#10;\\prime}\\right){\\varepsilon}L\\right].\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mn>2</mn></msub><mo>\u2208</mo><mrow><mo>[</mo><mrow><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>,</mo><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>L</mi></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n \\item\\label{lem:analysis_tau} $\\tau \\leq \\frac{n}{\\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) L}$;\n \\item\\label{lem:analysis_X_tau} ${\\mathrm{ed}} \\left( X \\left[ a_{\\tau}, b_{\\tau} \\right], X \\right) \\leq 2 L$.\n\\end{enumerate}\n\\end{lemma}\n\n\n\n\\begin{proof}\n  Part~\\eqref{lem:analysis_i} of the lemma holds for $i = 1$ by choosing $a_1 = T_1$ and $b_1 = T_1 + L - 1$. \n  For larger $i$ we prove the statement by induction on $i$. \n\n  Suppose we are in the first while loop of the algorithm, i.e., $i \\leq \\tau_1$. \n  We set $a_i = a_1$ for all $i \\leq \\tau_1$ and only change $b_i$. \n  Let ${\\widetilde}{T}_i = T \\left( {\\widetilde}{R}_{k \\left( i \\right)} \\right)$, \n  where $k \\left( i \\right)$ is the index of the read chosen at the $i^{\\text{th}}$ round, \n  and set $b_i := {\\widetilde}{T}_i + L - 1$.\n  As mentioned before, we may assume that there is no gap greater than $2c' {\\varepsilon} L$ in between subsequent starting points of the reads. \n  Therefore if ${\\widetilde}{T}_i \\leq n - 2L$, \n  then there must exist $R \\in \\mathcal{R}$ such that \n  $T \\left( R \\right) - {\\widetilde}{T}_i \\in \\left[ \\left( 1 - \\alpha {\\varepsilon} \\right) L - c'{\\varepsilon} L, \\left( 1 - \\alpha {\\varepsilon} \\right) L + c'{\\varepsilon} L \\right]$. \n  By the triangle inequality this implies that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_{k \\left( i \\right)} \\left[ -\\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R} \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$, \n  i.e., ${\\widetilde}{R}$ satisfies the condition of the while loop. \n  Thus ${\\widetilde}{T}_{\\tau_1} > n - 2L$. \n  Now take any ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ that satisfies the condition of the while loop. \n  By Lemma~\\ref{lem:close_reads} we know that \n  $T \\left( {\\widetilde}{R} \\right) - {\\widetilde}{T}_i - \\left( 1 - \\alpha {\\varepsilon} \\right) L \\in \\left[ - \\left( 2 + 2c' \\right) {\\varepsilon} L, \\left( 2 + 2c' \\right) {\\varepsilon} L \\right]$. \n  By subadditivity and the induction hypothesis we have that \n  \n", "itemtype": "equation", "pos": 33172, "prevtext": "\n\\end{lemma}\n\\begin{proof}\n  Suppose that~\\eqref{eq:shift} does not hold.  \n  Then the overlap between $R_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right]$ and $R_2 \\left[ 1, \\alpha {\\varepsilon} L \\right]$ is less than \n  $\\left( \\alpha - \\left( 2 + 2c'\\right) \\right) {\\varepsilon} L$. \n  Recall the definition of $c = c\\left( \\Sigma \\right)$ from Corollary~\\ref{cor:shift}. \n  Since $c \\alpha {\\varepsilon} L = 4 {\\varepsilon} L \\geq \\left( 2 + 2c' \\right) {\\varepsilon} L$, we can apply Lemmas~\\ref{lem:small_overlap} and~\\ref{lem:large_overlap} to get that \n  with probability $1 - o \\left( n^{-2} \\right)$ we have \n  ${\\mathrm{ed}} \\left( R_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], R_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\geq 2 \\left( 2 + 2 c' \\right) {\\varepsilon} L$. \n  By the triangle inequality this implies that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_1 \\left[ - \\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R}_2 \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\geq \\left( 2 + 4 c' \\right) {\\varepsilon} L$, \n  which is a contradiction.  \n\\end{proof}\nSince the probability that the conclusion of the lemma does not hold is $o \\left( n^{-2} \\right)$, \nwe can take a union bound over all pairs of corrupted reads and have the conclusion of the lemma apply to all of them with probability $1 - o \\left( 1 \\right)$. \n\n\nWe are now ready to analyze the algorithm step by step. \nWe show by induction that after each extension step the partially reconstructed sequence is a good approximation of \na substring of the original sequence. \n\n\\begin{lemma}\\label{lem:analysis}\nLet $X \\in \\Sigma^n$ be a uniformly random string, \nlet $L = \\left( \\overline{C} / {\\varepsilon} \\right) \\ln \\left( n \\right)$ where $\\overline{C}$ is a large enough constant, \nand let $N = C' N_{{\\mathrm{cov}}} / {\\varepsilon}$. \nLet $\\alpha = 4 / c \\left( \\Sigma \\right)$, where $c \\left( \\Sigma \\right)$ is given by Corollary~\\ref{cor:shift}. \nLet $Y_i$ be the state of the partially reconstructed sequence $Y$ after $i$ corrupted reads have been processed by the algorithm; \nwe have $Y_1 = {\\widetilde}{R}_1$. \nLet $\\tau_1$ be the number of reads processed in the first while loop of the algorithm, \nand let $\\tau_2$ be the number of reads processed in the second while loop.  \nAlso let $\\tau = \\tau_1 + \\tau_2$, the total number of reads processed during the algorithm. \nWith probability at least $1 - \\delta$ \n(over the choice of $X$ and the starting points of the reads in $\\mathcal{R}$) \nwe have the following: \n\\begin{enumerate}[(a)]\n \\item\\label{lem:analysis_i} For every $i \\leq \\tau$ \n  there exist $a_i, b_i \\in \\left[ n \\right]$ such that \n  $\\left| a_i - b_i \\right| \\geq \\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) i L$ and \n  \n", "index": 19, "text": "\\begin{equation}\\label{eq:Y_i}\n   {\\mathrm{ed}} \\left( Y_i, X \\left[ a_i, b_i \\right] \\right) \\leq  \\left( 3+2c' \\right) {\\varepsilon} i L; \n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(Y_{i},X\\left[a_{i},b_{i}\\right]\\right)\\leq\\left(3+2c^{%&#10;\\prime}\\right){\\varepsilon}iL;\" display=\"block\"><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>b</mi><mi>i</mi></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo>(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  so it suffices to estimate the latter term. By~\\eqref{eq:ed_mon} we have that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], R \\left[ 1 + \\alpha {\\varepsilon} L , L \\right] \\right) \\leq {\\mathrm{ed}} \\left( {\\widetilde}{R}, R \\right) \\leq {\\varepsilon} L$. \n  Using the definition of $b_i$ we have that \n \n", "itemtype": "equation", "pos": 35402, "prevtext": "\n \\item\\label{lem:analysis_tau} $\\tau \\leq \\frac{n}{\\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) L}$;\n \\item\\label{lem:analysis_X_tau} ${\\mathrm{ed}} \\left( X \\left[ a_{\\tau}, b_{\\tau} \\right], X \\right) \\leq 2 L$.\n\\end{enumerate}\n\\end{lemma}\n\n\n\n\\begin{proof}\n  Part~\\eqref{lem:analysis_i} of the lemma holds for $i = 1$ by choosing $a_1 = T_1$ and $b_1 = T_1 + L - 1$. \n  For larger $i$ we prove the statement by induction on $i$. \n\n  Suppose we are in the first while loop of the algorithm, i.e., $i \\leq \\tau_1$. \n  We set $a_i = a_1$ for all $i \\leq \\tau_1$ and only change $b_i$. \n  Let ${\\widetilde}{T}_i = T \\left( {\\widetilde}{R}_{k \\left( i \\right)} \\right)$, \n  where $k \\left( i \\right)$ is the index of the read chosen at the $i^{\\text{th}}$ round, \n  and set $b_i := {\\widetilde}{T}_i + L - 1$.\n  As mentioned before, we may assume that there is no gap greater than $2c' {\\varepsilon} L$ in between subsequent starting points of the reads. \n  Therefore if ${\\widetilde}{T}_i \\leq n - 2L$, \n  then there must exist $R \\in \\mathcal{R}$ such that \n  $T \\left( R \\right) - {\\widetilde}{T}_i \\in \\left[ \\left( 1 - \\alpha {\\varepsilon} \\right) L - c'{\\varepsilon} L, \\left( 1 - \\alpha {\\varepsilon} \\right) L + c'{\\varepsilon} L \\right]$. \n  By the triangle inequality this implies that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R}_{k \\left( i \\right)} \\left[ -\\alpha {\\varepsilon} L, -1 \\right], {\\widetilde}{R} \\left[ 1, \\alpha {\\varepsilon} L \\right] \\right) \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L$, \n  i.e., ${\\widetilde}{R}$ satisfies the condition of the while loop. \n  Thus ${\\widetilde}{T}_{\\tau_1} > n - 2L$. \n  Now take any ${\\widetilde}{R} \\in {\\widetilde}{\\mathcal{R}}$ that satisfies the condition of the while loop. \n  By Lemma~\\ref{lem:close_reads} we know that \n  $T \\left( {\\widetilde}{R} \\right) - {\\widetilde}{T}_i - \\left( 1 - \\alpha {\\varepsilon} \\right) L \\in \\left[ - \\left( 2 + 2c' \\right) {\\varepsilon} L, \\left( 2 + 2c' \\right) {\\varepsilon} L \\right]$. \n  By subadditivity and the induction hypothesis we have that \n  \n", "index": 21, "text": "\\begin{align*}\n   {\\mathrm{ed}} \\left( Y_{i+1}, X \\left[ a_{i+1}, b_{i+1} \\right] \\right) \n    &\\leq \n    {\\mathrm{ed}} \\left( Y_i, X \\left[ a_i, b_i \\right] \\right) + {\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], X \\left[ b_{i} + 1, b_{i+1} \\right] \\right) \\\\\n    &\\leq \\left(3+2c'\\right) {\\varepsilon} i L + {\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], X \\left[ b_{i} + 1, b_{i+1} \\right] \\right),\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathrm{ed}}\\left(Y_{i+1},X\\left[a_{i+1},b_{i+1}\\right]\\right)\" display=\"inline\"><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\mathrm{ed}}\\left(Y_{i},X\\left[a_{i},b_{i}\\right]\\right)+{%&#10;\\mathrm{ed}}\\left({\\widetilde{}}{R}\\left[1+\\alpha{\\varepsilon}L,-1\\right],X%&#10;\\left[b_{i}+1,b_{i+1}\\right]\\right)\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>b</mi><mi>i</mi></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>,</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\left(3+2c^{\\prime}\\right){\\varepsilon}iL+{\\mathrm{ed}}\\left(%&#10;{\\widetilde{}}{R}\\left[1+\\alpha{\\varepsilon}L,-1\\right],X\\left[b_{i}+1,b_{i+1}%&#10;\\right]\\right),\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>L</mi></mrow><mo>+</mo><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>,</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  and so by the triangle inequality we have that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], X \\left[ b_{i} + 1, b_{i+1} \\right] \\right) \\leq \\left( 3 + 2c' \\right) {\\varepsilon} L$,\n  proving~\\eqref{eq:Y_i} for all $i \\leq \\tau_1$. The proof for $i \\in \\left[ \\tau_1, \\tau_2 \\right]$ is similar, except now $b_i = b_{\\tau_1}$ and $a_i$ changes. \n\n  We proved that for all $i \\leq \\tau_1$ we have \n  ${\\widetilde}{T}_{i+1} - {\\widetilde}{T}_i \\geq \\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) L$. \n  A similar statement holds for $i \\in \\left[ \\tau_1, \\tau_2 \\right]$, and together these imply part~\\eqref{lem:analysis_tau} of the lemma.\n\n  Since we have $a_{\\tau} \\leq L$ and $b_{\\tau} \\geq n - L$, this implies part~\\eqref{lem:analysis_X_tau} of the lemma.   \n\\end{proof}\n\nPutting everything together and using the triangle inequality we get that the algorithm outputs an estimate ${\\widehat}{X}$ which satisfies \n", "itemtype": "equation", "pos": 36243, "prevtext": "\n  so it suffices to estimate the latter term. By~\\eqref{eq:ed_mon} we have that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], R \\left[ 1 + \\alpha {\\varepsilon} L , L \\right] \\right) \\leq {\\mathrm{ed}} \\left( {\\widetilde}{R}, R \\right) \\leq {\\varepsilon} L$. \n  Using the definition of $b_i$ we have that \n \n", "index": 23, "text": "\\[\n   {\\mathrm{ed}} \\left( R \\left[ 1 + \\alpha {\\varepsilon} L, L \\right], X \\left[ b_{i} + 1, b_{i+1} \\right] \\right) = \\left| \\left( {\\widetilde}{T}_i + L - 1 \\right) - \\left( T \\left( R \\right) + \\alpha {\\varepsilon} L - 1 \\right) \\right| \\leq \\left( 2 + 2c' \\right) {\\varepsilon} L,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(R\\left[1+\\alpha{\\varepsilon}L,L\\right],X\\left[b_{i}+1,b_{i+%&#10;1}\\right]\\right)=\\left|\\left({\\widetilde{}}{T}_{i}+L-1\\right)-\\left(T\\left(R%&#10;\\right)+\\alpha{\\varepsilon}L-1\\right)\\right|\\leq\\left(2+2c^{\\prime}\\right){%&#10;\\varepsilon}L,\" display=\"block\"><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>,</mo><mi>L</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mrow><mo>(</mo><mrow><mrow><mrow><mover accent=\"true\"><mi/><mo>~</mo></mover><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>L</mi></mrow><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo>-</mo><mrow><mo>(</mo><mrow><mrow><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>R</mi><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow><mo>|</mo></mrow><mo>\u2264</mo><mrow><mrow><mo>(</mo><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>L</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nwhich proves Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3}. \n\n\n\n\n\\section{Discussion and future work} \\label{sec:discussion} \n\n\nWe introduced an adversarial error model for the problem of sequence assembly from shotgun reads. \nOur main result shows that if the reads are long enough and there is high enough coverage, \nthen approximate reconstruction of the original sequence is possible for almost all sequences. \nThe main question our work leaves open is: \nwhat are the fundamental information-theoretic limits to approximate reconstruction? \nGiven $\\overline{L}$ and $N$, is approximate reconstruction possible? \nIf so, what is the best approximation factor achievable? \nWhat is the best ``strategy'' for an adversary that can corrupt the reads? \n\n\nThe probabilistic model we consider for the sequence of interest is simplistic, \nand it would be worthwhile to consider more general distributions, such as a Markov chain model. \nHowever, in many genomes there are long repeats, which are not captured by a Markov model. \nA direction for future research is to understand the fundamental limits to approximate reconstruction for arbitrary sequences as a function of their (approximate) repeat statistics. \n\n\nOur adversarial error model also contains a simplification: \nsequencing technologies typically do not have a uniform error rate. \nInstead, while the error rate is reasonably small for most reads, \nthere are some where the error rate is large and the resulting reads are useless.  \nPractitioners can often detect these bad reads and thus throw them away. \nA variant of our algorithm can also handle very bad reads if the quality of good and very bad reads are sufficiently separated: \nthe very bad reads simply will not align anywhere and so will be thrown out. \nHowever, if there is a continuous spectrum of quality from good to very bad reads, the algorithm runs into issues due to the reads in the middle of the spectrum. \nWe leave addressing this issue as a future challenge. \n \n\n\n\n\n\n\n\\section*{Acknowledgements}\n\nThe research of E.M.\\ is supported by \nNSF grant CCF-1320105, DOD ONR grant N00014-14-1-0823, and Simons Foundation grant 328025. \nM.Z.R.\\ thanks Jasmine Nirody and Rachel Wang for helpful discussions. \n\n\n\n\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{bib}\n\n\n\n\n\n\n\n\\appendix\n\n\n\n\n\\section{Proofs of edit distance results} \\label{sec:edit_distance_proofs} \n\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:indpt}]\n For any $m$ and $n$ we clearly have \n\n", "itemtype": "equation", "pos": 37514, "prevtext": "\n  and so by the triangle inequality we have that \n  ${\\mathrm{ed}} \\left( {\\widetilde}{R} \\left[ 1 + \\alpha {\\varepsilon} L, -1 \\right], X \\left[ b_{i} + 1, b_{i+1} \\right] \\right) \\leq \\left( 3 + 2c' \\right) {\\varepsilon} L$,\n  proving~\\eqref{eq:Y_i} for all $i \\leq \\tau_1$. The proof for $i \\in \\left[ \\tau_1, \\tau_2 \\right]$ is similar, except now $b_i = b_{\\tau_1}$ and $a_i$ changes. \n\n  We proved that for all $i \\leq \\tau_1$ we have \n  ${\\widetilde}{T}_{i+1} - {\\widetilde}{T}_i \\geq \\left( 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} \\right) L$. \n  A similar statement holds for $i \\in \\left[ \\tau_1, \\tau_2 \\right]$, and together these imply part~\\eqref{lem:analysis_tau} of the lemma.\n\n  Since we have $a_{\\tau} \\leq L$ and $b_{\\tau} \\geq n - L$, this implies part~\\eqref{lem:analysis_X_tau} of the lemma.   \n\\end{proof}\n\nPutting everything together and using the triangle inequality we get that the algorithm outputs an estimate ${\\widehat}{X}$ which satisfies \n", "index": 25, "text": "\n\\[\n {\\mathrm{ed}} \\left( X, {\\widehat}{X} \\right) \\leq \\frac{3+2c'}{ 1 - \\left( \\alpha + 2 + 2c' \\right) {\\varepsilon} } {\\varepsilon} n + 2 L,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X,{\\widehat{}}{X}\\right)\\leq\\frac{3+2c^{\\prime}}{1-\\left(%&#10;\\alpha+2+2c^{\\prime}\\right){\\varepsilon}}{\\varepsilon}n+2L,\" display=\"block\"><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>X</mi><mo>,</mo><mrow><mover accent=\"true\"><mi/><mo>^</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mfrac><mrow><mn>3</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mrow><mn>1</mn><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mi>\u03b1</mi><mo>+</mo><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>c</mi><mo>\u2032</mo></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03b5</mi></mrow></mrow></mfrac><mo>\u2062</mo><mi>\u03b5</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>L</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nThus Kingman's subadditive ergodic theorem implies that \n$\\lim_{m \\to \\infty} \\frac{1}{m} {\\mathrm{ed}} \\left( X_m, Y_m \\right) =: c_{{\\mathrm{ind}}}$ \nexists almost surely. \nClearly $c_{{\\mathrm{ind}}} \\geq 0$; \nwhat remains to show is that $c_{{\\mathrm{ind}}} > 0$. \nWe do this via a volume argument; \nwe first present a simple argument and then refine it to get a better lower bound on $c_{{\\mathrm{ind}}}$. \n\n\nIf ${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq r$, \nthen one can get from $X_m$ to $Y_m$ using at most $r$ deletions, insertions and substitutions. \nThe locations of the at most $r$ deletions and substitutions can be chosen in at most $\\binom{m}{r}$ ways, \nand the same holds for the locations of the at most $r$ insertions and substitutions. \nGiven the locations of these, there can be at most $\\left| \\Sigma \\right|^r$ subsequences in these locations. \nThat is, the edit distance ball of radius $r$ around any point $x \\in \\Sigma^m$ contains at most \n$\\binom{m}{r}^2 \\left| \\Sigma \\right|^{r}$ \npoints of $\\Sigma^m$. \nFor $r = \\delta m$ we get \n", "itemtype": "equation", "pos": 40132, "prevtext": "\nwhich proves Theorems~\\ref{thm:main_eps} and~\\ref{thm:main_eps_3}. \n\n\n\n\n\\section{Discussion and future work} \\label{sec:discussion} \n\n\nWe introduced an adversarial error model for the problem of sequence assembly from shotgun reads. \nOur main result shows that if the reads are long enough and there is high enough coverage, \nthen approximate reconstruction of the original sequence is possible for almost all sequences. \nThe main question our work leaves open is: \nwhat are the fundamental information-theoretic limits to approximate reconstruction? \nGiven $\\overline{L}$ and $N$, is approximate reconstruction possible? \nIf so, what is the best approximation factor achievable? \nWhat is the best ``strategy'' for an adversary that can corrupt the reads? \n\n\nThe probabilistic model we consider for the sequence of interest is simplistic, \nand it would be worthwhile to consider more general distributions, such as a Markov chain model. \nHowever, in many genomes there are long repeats, which are not captured by a Markov model. \nA direction for future research is to understand the fundamental limits to approximate reconstruction for arbitrary sequences as a function of their (approximate) repeat statistics. \n\n\nOur adversarial error model also contains a simplification: \nsequencing technologies typically do not have a uniform error rate. \nInstead, while the error rate is reasonably small for most reads, \nthere are some where the error rate is large and the resulting reads are useless.  \nPractitioners can often detect these bad reads and thus throw them away. \nA variant of our algorithm can also handle very bad reads if the quality of good and very bad reads are sufficiently separated: \nthe very bad reads simply will not align anywhere and so will be thrown out. \nHowever, if there is a continuous spectrum of quality from good to very bad reads, the algorithm runs into issues due to the reads in the middle of the spectrum. \nWe leave addressing this issue as a future challenge. \n \n\n\n\n\n\n\n\\section*{Acknowledgements}\n\nThe research of E.M.\\ is supported by \nNSF grant CCF-1320105, DOD ONR grant N00014-14-1-0823, and Simons Foundation grant 328025. \nM.Z.R.\\ thanks Jasmine Nirody and Rachel Wang for helpful discussions. \n\n\n\n\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{bib}\n\n\n\n\n\n\n\n\\appendix\n\n\n\n\n\\section{Proofs of edit distance results} \\label{sec:edit_distance_proofs} \n\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:indpt}]\n For any $m$ and $n$ we clearly have \n\n", "index": 27, "text": "\\begin{multline*}\n  {\\mathrm{ed}} \\left( X_{m+n}, Y_{m+n} \\right) \\\\ \n  \\leq \n  {\\mathrm{ed}} \\left( X_{m+n} \\left[ 1 , m \\right], Y_{m+n} \\left[ 1 , m \\right] \\right) \n  +\n  {\\mathrm{ed}} \\left( X_{m+n} \\left[ m + 1 , m + n \\right], Y_{m+n} \\left[ m + 1 , m + n \\right] \\right).\n\\end{multline*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"p14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathrm{ed}}\\left(X_{m+n},Y_{m+n}\\right)\\\\&#10;\\displaystyle\\leq{\\mathrm{ed}}\\left(X_{m+n}\\left[1,m\\right],Y_{m+n}\\left[1,m%&#10;\\right]\\right)+{\\mathrm{ed}}\\left(X_{m+n}\\left[m+1,m+n\\right],Y_{m+n}\\left[m+1%&#10;,m+n\\right]\\right).\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>X</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>Y</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>X</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>Y</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>X</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>Y</mi><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>n</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nwhere \n$H \\left( x \\right) = - x \\log_2 \\left( x \\right) - \\left( 1 - x \\right) \\log_2 \\left( 1 - x \\right)$ \nis the binary entropy function. \nNote that the total number of sequences of length $m$ is $\\left| \\Sigma \\right|^m$.\nLet $\\delta^{*} = \\delta^{*} \\left( \\Sigma \\right)$ be the unique solution in $\\left( 0, 1 \\right)$ of \n$4^{H\\left( \\delta \\right)} \\left| \\Sigma \\right|^{\\delta} = \\left| \\Sigma \\right|$. \nBy the volume argument above we have that \nfor any $\\delta < \\delta^{*}$  \nthe probability that\n${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq \\delta m$ \nis exponentially small in $m$. \nThus \n$c_{{\\mathrm{ind}}} \\geq \\delta^{*} > 0$. \nIn particular, \nfor $\\left| \\Sigma \\right| = 2$, we have $\\delta^* \\approx 0.09488$, and \nfor $\\left| \\Sigma \\right| = 4$, we have $\\delta^* \\approx 0.22709$.   \n\n\nWe can obtain a better bound by a slightly more careful argument. \nAgain, if ${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq r$, \nthen one can get from $X_m$ to $Y_m$ using at most $r$ deletions, insertions and substitutions. \nSuppose that \nthe number of deletions is $D$, \nthe number of insertions is $I$, and \nthe number of substitutions is $S$. \nSince $X_m$ and $Y_m$ have the same length, \nwe have $D = I$ \nand also $D + I + S \\leq r$, i.e., $S \\leq r - 2D$. \nThe locations of the $D$ deletions can be chosen in at most $\\binom{m}{D}$ ways, \nthe locations of the $I$ insertions can be chosen in at most $\\binom{m}{I}$ ways, \nwhile the locations of the $S$ substitutions can be chosen in at most $\\binom{m}{S}$ ways. \nGiven the locations of these, \nthere can be at most $\\left| \\Sigma \\right|^I$ subsequences in the locations of the insertions, \nand at most $\\left| \\Sigma \\right|^S$ subsequences in the locations of the substitutions. \nTherefore the edit distance ball of radius $r$ around any point $x \\in \\Sigma^m$ contains at most \n", "itemtype": "equation", "pos": 41488, "prevtext": "\nThus Kingman's subadditive ergodic theorem implies that \n$\\lim_{m \\to \\infty} \\frac{1}{m} {\\mathrm{ed}} \\left( X_m, Y_m \\right) =: c_{{\\mathrm{ind}}}$ \nexists almost surely. \nClearly $c_{{\\mathrm{ind}}} \\geq 0$; \nwhat remains to show is that $c_{{\\mathrm{ind}}} > 0$. \nWe do this via a volume argument; \nwe first present a simple argument and then refine it to get a better lower bound on $c_{{\\mathrm{ind}}}$. \n\n\nIf ${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq r$, \nthen one can get from $X_m$ to $Y_m$ using at most $r$ deletions, insertions and substitutions. \nThe locations of the at most $r$ deletions and substitutions can be chosen in at most $\\binom{m}{r}$ ways, \nand the same holds for the locations of the at most $r$ insertions and substitutions. \nGiven the locations of these, there can be at most $\\left| \\Sigma \\right|^r$ subsequences in these locations. \nThat is, the edit distance ball of radius $r$ around any point $x \\in \\Sigma^m$ contains at most \n$\\binom{m}{r}^2 \\left| \\Sigma \\right|^{r}$ \npoints of $\\Sigma^m$. \nFor $r = \\delta m$ we get \n", "index": 29, "text": "\n\\[\n  \\binom{m}{\\delta m}^2 \\left| \\Sigma \\right|^{\\delta m} \n  \\approx \n  2^{2 H \\left( \\delta \\right) m} \\left| \\Sigma \\right|^{\\delta m},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\binom{m}{\\delta m}^{2}\\left|\\Sigma\\right|^{\\delta m}\\approx 2^{2H\\left(\\delta%&#10;\\right)m}\\left|\\Sigma\\right|^{\\delta m},\" display=\"block\"><mrow><mrow><mrow><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>m</mi></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>m</mi></mrow></msup></mrow><mo>\u2248</mo><mrow><msup><mn>2</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03b4</mi><mo>)</mo></mrow><mo>\u2062</mo><mi>m</mi></mrow></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\npoints of $\\Sigma^m$. \nFor $r = \\delta m$ and $D = \\delta_D m$ we have\n", "itemtype": "equation", "pos": 43479, "prevtext": "\nwhere \n$H \\left( x \\right) = - x \\log_2 \\left( x \\right) - \\left( 1 - x \\right) \\log_2 \\left( 1 - x \\right)$ \nis the binary entropy function. \nNote that the total number of sequences of length $m$ is $\\left| \\Sigma \\right|^m$.\nLet $\\delta^{*} = \\delta^{*} \\left( \\Sigma \\right)$ be the unique solution in $\\left( 0, 1 \\right)$ of \n$4^{H\\left( \\delta \\right)} \\left| \\Sigma \\right|^{\\delta} = \\left| \\Sigma \\right|$. \nBy the volume argument above we have that \nfor any $\\delta < \\delta^{*}$  \nthe probability that\n${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq \\delta m$ \nis exponentially small in $m$. \nThus \n$c_{{\\mathrm{ind}}} \\geq \\delta^{*} > 0$. \nIn particular, \nfor $\\left| \\Sigma \\right| = 2$, we have $\\delta^* \\approx 0.09488$, and \nfor $\\left| \\Sigma \\right| = 4$, we have $\\delta^* \\approx 0.22709$.   \n\n\nWe can obtain a better bound by a slightly more careful argument. \nAgain, if ${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq r$, \nthen one can get from $X_m$ to $Y_m$ using at most $r$ deletions, insertions and substitutions. \nSuppose that \nthe number of deletions is $D$, \nthe number of insertions is $I$, and \nthe number of substitutions is $S$. \nSince $X_m$ and $Y_m$ have the same length, \nwe have $D = I$ \nand also $D + I + S \\leq r$, i.e., $S \\leq r - 2D$. \nThe locations of the $D$ deletions can be chosen in at most $\\binom{m}{D}$ ways, \nthe locations of the $I$ insertions can be chosen in at most $\\binom{m}{I}$ ways, \nwhile the locations of the $S$ substitutions can be chosen in at most $\\binom{m}{S}$ ways. \nGiven the locations of these, \nthere can be at most $\\left| \\Sigma \\right|^I$ subsequences in the locations of the insertions, \nand at most $\\left| \\Sigma \\right|^S$ subsequences in the locations of the substitutions. \nTherefore the edit distance ball of radius $r$ around any point $x \\in \\Sigma^m$ contains at most \n", "index": 31, "text": "\n\\[\n\\max_{0 \\leq D \\leq r/2} \\left\\{ \\binom{m}{D}^2 \\binom{m}{r - 2D} \\left| \\Sigma \\right|^{ r - D } \\right\\}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\max_{0\\leq D\\leq r/2}\\left\\{\\binom{m}{D}^{2}\\binom{m}{r-2D}\\left|\\Sigma\\right%&#10;|^{r-D}\\right\\}\" display=\"block\"><mrow><munder><mi>max</mi><mrow><mn>0</mn><mo>\u2264</mo><mi>D</mi><mo>\u2264</mo><mrow><mi>r</mi><mo>/</mo><mn>2</mn></mrow></mrow></munder><mo>\u2061</mo><mrow><mo>{</mo><mrow><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mi>D</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mrow><mi>r</mi><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>D</mi></mrow></mrow></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mi>r</mi><mo>-</mo><mi>D</mi></mrow></msup></mrow><mo>}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nLet $\\delta^{**} = \\delta^{**} \\left( \\Sigma \\right)$ be the unique solution in $\\left( 0, 1 \\right)$ of \n", "itemtype": "equation", "pos": 43663, "prevtext": "\npoints of $\\Sigma^m$. \nFor $r = \\delta m$ and $D = \\delta_D m$ we have\n", "index": 33, "text": "\n\\[\n  \\binom{m}{\\delta_D m}^2 \\binom{m}{\\left( \\delta - 2\\delta_D \\right) m} \\left| \\Sigma \\right|^{\\left( \\delta - \\delta_D \\right) m} \n  \\approx \n  2^{\\left( 2 H \\left( \\delta_D \\right) + H \\left( \\delta - 2\\delta_D \\right)  - \\delta_D \\log_2 \\left( \\left| \\Sigma \\right| \\right) \\right) m} \n  \\left| \\Sigma \\right|^{\\delta m}. \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\binom{m}{\\delta_{D}m}^{2}\\binom{m}{\\left(\\delta-2\\delta_{D}\\right)m}\\left|%&#10;\\Sigma\\right|^{\\left(\\delta-\\delta_{D}\\right)m}\\approx 2^{\\left(2H\\left(\\delta%&#10;_{D}\\right)+H\\left(\\delta-2\\delta_{D}\\right)-\\delta_{D}\\log_{2}\\left(\\left|%&#10;\\Sigma\\right|\\right)\\right)m}\\left|\\Sigma\\right|^{\\delta m}.\" display=\"block\"><mrow><mrow><mrow><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mrow><msub><mi>\u03b4</mi><mi>D</mi></msub><mo>\u2062</mo><mi>m</mi></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mrow><mrow><mo>(</mo><mrow><mi>\u03b4</mi><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>\u03b4</mi><mi>D</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>m</mi></mrow></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mrow><mo>(</mo><mrow><mi>\u03b4</mi><mo>-</mo><msub><mi>\u03b4</mi><mi>D</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>m</mi></mrow></msup></mrow><mo>\u2248</mo><mrow><msup><mn>2</mn><mrow><mrow><mo>(</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03b4</mi><mi>D</mi></msub><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\u03b4</mi><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>\u03b4</mi><mi>D</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03b4</mi><mi>D</mi></msub><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>m</mi></mrow></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nThe volume argument thus tells us that for every $\\delta < \\delta^{**}$ \nthe probability that \n${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq \\delta m$ \nis exponentially small in $m$. \nThus \n$c_{{\\mathrm{ind}}} \\geq \\delta^{**}$. \nIn particular, \nfor $\\left| \\Sigma \\right| = 2$, we have $\\delta^{**} \\approx 0.15776$, and \nfor $\\left| \\Sigma \\right| = 4$, we have $\\delta^{**} \\approx 0.33832$.   \n\\end{proof}\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:small_overlap}]\nBy~\\eqref{eq:ed_mon} we have that \n", "itemtype": "equation", "pos": 44102, "prevtext": "\nLet $\\delta^{**} = \\delta^{**} \\left( \\Sigma \\right)$ be the unique solution in $\\left( 0, 1 \\right)$ of \n", "index": 35, "text": "\n\\[\n  2^{\\max_{0 \\leq x \\leq \\delta / 2} \\left\\{ 2 H \\left( x \\right) + H \\left( \\delta - 2x \\right) - x \\log_2 \\left( \\left| \\Sigma \\right| \\right) \\right\\}} \\left| \\Sigma \\right|^{\\delta} \n  =\n  \\left| \\Sigma \\right|.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"2^{\\max_{0\\leq x\\leq\\delta/2}\\left\\{2H\\left(x\\right)+H\\left(\\delta-2x\\right)-x%&#10;\\log_{2}\\left(\\left|\\Sigma\\right|\\right)\\right\\}}\\left|\\Sigma\\right|^{\\delta}=%&#10;\\left|\\Sigma\\right|.\" display=\"block\"><mrow><mrow><mrow><msup><mn>2</mn><mrow><msub><mi>max</mi><mrow><mn>0</mn><mo>\u2264</mo><mi>x</mi><mo>\u2264</mo><mrow><mi>\u03b4</mi><mo>/</mo><mn>2</mn></mrow></mrow></msub><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\u03b4</mi><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>x</mi><mo>\u2062</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mi>\u03b4</mi></msup></mrow><mo>=</mo><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nSince $k + 1 > dm$, the strings \n$X \\left[ 1 , dm \\right]$ \nand \n$X \\left[ k + 1 , k + dm \\right]$\nare independent uniformly random strings of length $dm$. \nRecall the definition of $\\delta^{**}$ from the proof of Lemma~\\ref{lem:indpt} \nand let $\\delta \\in \\left( 0, \\delta^{**} \\right)$. \nIn the proof of Lemma~\\ref{lem:indpt} we showed that \nthe probability that \n${\\mathrm{ed}} \\left( X \\left[ 1 , dm \\right], X \\left[k + 1, k + dm  \\right] \\right) \n\\leq \\delta d m$\nis exponentially small in $dm$. \nBy taking a union bound over $k \\in \\left[ dm, m \\right]$ \nwe arrive at the desired result \nwith, e.g., $\\gamma \\left( d, \\Sigma \\right) = 0.9 \\times \\delta^{**} \\left( \\Sigma \\right) d$, \nand an appropriate constant $c'$. \n\\end{proof}\n\n\nBefore proving Lemma~\\ref{lem:large_overlap} we introduce a variant of the edit distance which is simpler to understand theoretically \nand for which we state and prove a result similar to Lemma~\\ref{lem:large_overlap}. \nWe denote by ${\\underline{\\mathrm{ed}}} \\left( x, y \\right)$ the minimum number of deletion or insertion operations necessary to go from $x$ to $y$;  \nthat is, compared to the edit distance, substitutions are not allowed. \nSince a substitution can be simulated by a deletion followed by an insertion, we have that \n${\\mathrm{ed}} \\left( x, y \\right) \\leq {\\underline{\\mathrm{ed}}} \\left( x, y \\right) \\leq 2 {\\mathrm{ed}} \\left( x, y \\right)$ \nfor all $x, y \\in \\Sigma^*$. \nThe nice property of this distance is that \n${\\underline{\\mathrm{ed}}} \\left( x, y \\right) \n= \\left| x \\right| + \\left| y \\right| - 2 {\\mathrm{LCS}} \\left( x, y \\right)$,\nwhere ${\\mathrm{LCS}} \\left( x, y \\right)$ is the length of the longest common subsequence (LCS) of $x$ and $y$. \nThe following result is about the longest common subsequence of two random strings and is similar to Lemma~\\ref{lem:large_overlap}. \n\n\\begin{lemma}\\label{lem:LCS}\nLet $X \\in \\Sigma^{2m}$ be a uniformly random string. \nThere exist positive constants $c = c \\left( \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that    \n", "itemtype": "equation", "pos": 44823, "prevtext": "\nThe volume argument thus tells us that for every $\\delta < \\delta^{**}$ \nthe probability that \n${\\mathrm{ed}} \\left( X_m, Y_m \\right) \\leq \\delta m$ \nis exponentially small in $m$. \nThus \n$c_{{\\mathrm{ind}}} \\geq \\delta^{**}$. \nIn particular, \nfor $\\left| \\Sigma \\right| = 2$, we have $\\delta^{**} \\approx 0.15776$, and \nfor $\\left| \\Sigma \\right| = 4$, we have $\\delta^{**} \\approx 0.33832$.   \n\\end{proof}\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:small_overlap}]\nBy~\\eqref{eq:ed_mon} we have that \n", "index": 37, "text": "\n\\[\n{\\mathrm{ed}} \\left( X \\left[ 1 , m \\right], X \\left[k + 1, k + m  \\right] \\right) \n\\geq \n{\\mathrm{ed}} \\left( X \\left[ 1 , dm \\right], X \\left[k + 1, k + dm  \\right] \\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X\\left[1,m\\right],X\\left[k+1,k+m\\right]\\right)\\geq{\\mathrm{%&#10;ed}}\\left(X\\left[1,dm\\right],X\\left[k+1,k+dm\\right]\\right).\" display=\"block\"><mrow><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mi>m</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>m</mi></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>m</mi></mrow></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nfor all $k \\le c m$ with probability at least $1 - e^{-c' m}$.\n\\end{lemma}\n\n\n\\begin{proof}\n  It is immediate that \n \n", "itemtype": "equation", "pos": 47054, "prevtext": "\nSince $k + 1 > dm$, the strings \n$X \\left[ 1 , dm \\right]$ \nand \n$X \\left[ k + 1 , k + dm \\right]$\nare independent uniformly random strings of length $dm$. \nRecall the definition of $\\delta^{**}$ from the proof of Lemma~\\ref{lem:indpt} \nand let $\\delta \\in \\left( 0, \\delta^{**} \\right)$. \nIn the proof of Lemma~\\ref{lem:indpt} we showed that \nthe probability that \n${\\mathrm{ed}} \\left( X \\left[ 1 , dm \\right], X \\left[k + 1, k + dm  \\right] \\right) \n\\leq \\delta d m$\nis exponentially small in $dm$. \nBy taking a union bound over $k \\in \\left[ dm, m \\right]$ \nwe arrive at the desired result \nwith, e.g., $\\gamma \\left( d, \\Sigma \\right) = 0.9 \\times \\delta^{**} \\left( \\Sigma \\right) d$, \nand an appropriate constant $c'$. \n\\end{proof}\n\n\nBefore proving Lemma~\\ref{lem:large_overlap} we introduce a variant of the edit distance which is simpler to understand theoretically \nand for which we state and prove a result similar to Lemma~\\ref{lem:large_overlap}. \nWe denote by ${\\underline{\\mathrm{ed}}} \\left( x, y \\right)$ the minimum number of deletion or insertion operations necessary to go from $x$ to $y$;  \nthat is, compared to the edit distance, substitutions are not allowed. \nSince a substitution can be simulated by a deletion followed by an insertion, we have that \n${\\mathrm{ed}} \\left( x, y \\right) \\leq {\\underline{\\mathrm{ed}}} \\left( x, y \\right) \\leq 2 {\\mathrm{ed}} \\left( x, y \\right)$ \nfor all $x, y \\in \\Sigma^*$. \nThe nice property of this distance is that \n${\\underline{\\mathrm{ed}}} \\left( x, y \\right) \n= \\left| x \\right| + \\left| y \\right| - 2 {\\mathrm{LCS}} \\left( x, y \\right)$,\nwhere ${\\mathrm{LCS}} \\left( x, y \\right)$ is the length of the longest common subsequence (LCS) of $x$ and $y$. \nThe following result is about the longest common subsequence of two random strings and is similar to Lemma~\\ref{lem:large_overlap}. \n\n\\begin{lemma}\\label{lem:LCS}\nLet $X \\in \\Sigma^{2m}$ be a uniformly random string. \nThere exist positive constants $c = c \\left( \\Sigma \\right)$ and $c' = c' \\left( \\Sigma \\right)$ such that    \n", "index": 39, "text": "\n\\[\n    {\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = m - k\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{LCS}}\\left(X\\left[1,m\\right],X\\left[1+k,m+k\\right]\\right)=m-k\" display=\"block\"><mrow><mrow><mi>LCS</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>k</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>m</mi><mo>-</mo><mi>k</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  since the last $m - k$ coordinates of $X\\left[1,m\\right]$ \n  and the first $m-k$ coordinates of $X \\left[ 1 + k , m + k \\right]$ are the same. \n  What remains is to show that the probability of\n  \n", "itemtype": "equation", "pos": 47275, "prevtext": "\nfor all $k \\le c m$ with probability at least $1 - e^{-c' m}$.\n\\end{lemma}\n\n\n\\begin{proof}\n  It is immediate that \n \n", "index": 41, "text": "\\[\n   {\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) \\geq m - k,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{LCS}}\\left(X\\left[1,m\\right],X\\left[1+k,m+k\\right]\\right)\\geq m-k,\" display=\"block\"><mrow><mrow><mrow><mi>LCS</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>k</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>m</mi><mo>-</mo><mi>k</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  is exponentially small in $m$. \n  Note that if a common subsequence of $X \\left[ 1 , m \\right]$ and $X \\left[ 1 + k , m + k \\right]$\n  is such that the $\\ell^{\\text{th}}$ coordinate of $X \\left[ 1 , m \\right]$ \n  is mapped to the $\\left( \\ell - k \\right)^{\\text{th}}$ coordinate of $X \\left[ 1 + k , m + k \\right]$ (the ``trivial'' map), \n  then this subsequence can have length at most $m - k$, \n  since in $X \\left[ 1, m \\right]$ there are only $m - \\ell$ coordinates to the right of this coordinate, \n  while in $X \\left[ 1 + k , m + k \\right]$ there are only $\\ell - k - 1$ coordinates to the left of this coordinate. \n  So if a common subsequence has length greater than $m - k$, then every coordinate is mapped ``nontrivially''. \n  This then creates many constraints on the pair of sequences and so there will not be many of them, as we now argue. \n\n\n  Suppose that \n  ${\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = m - \\ell$\n  for some $\\ell \\in \\left\\{ 0, 1, \\dots, k - 1 \\right\\}$. \n  A noncrossing matching between $m - \\ell$ coordinates of the two sequences \n  is characterized by the $\\ell$ coordinates in each sequence that are not part of the matching; \n  the remaining pairs of the matching are determined due to the noncrossing property. \n  Thus there are $\\binom{m}{\\ell}^2$ such matchings. \n  The coordinates of a LCS between the two sequences corresponds to a noncrossing matching between the two, \n  and it imposes conditions on the values of these coordinates. \n  If \n  ${\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = m - \\ell$\n  then all but $\\ell$ coordinates of $X \\left[ 1 + k , m + k \\right]$ are determined by $X \\left[ 1 , m \\right]$. \n  Furthermore, there are at least $m - k - \\ell$ coordinates $j \\in \\left[ 1 + k, m \\right]$ such that \n  the $\\left( j - k \\right)^{\\text{th}}$ coordinate of $X \\left[ 1 + k , m + k \\right]$ is in the matching, \n  and thus the value of $X \\left[ j \\right]$ is determined by a previous value $X \\left[ i \\right]$ for some $i < j$. \n  This means that at most $k + \\ell$ coordinates of $X \\left[ 1, m \\right]$ are not determined by the value of a previous coordinate. \n  So the probability of any given noncrossing matching being a common subsequence is at most \n  $\\left| \\Sigma \\right|^{\\left( k + 2\\ell \\right) - \\left( m + k \\right)} = \\left| \\Sigma \\right|^{2\\ell - m}$. \n  We have thus shown that the probability of~\\eqref{eq:large_LCS} is at most \n \n", "itemtype": "equation", "pos": 47581, "prevtext": "\n  since the last $m - k$ coordinates of $X\\left[1,m\\right]$ \n  and the first $m-k$ coordinates of $X \\left[ 1 + k , m + k \\right]$ are the same. \n  What remains is to show that the probability of\n  \n", "index": 43, "text": "\\begin{equation}\\label{eq:large_LCS}\n  {\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) > m - k \n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{LCS}}\\left(X\\left[1,m\\right],X\\left[1+k,m+k\\right]\\right)&gt;m-k\" display=\"block\"><mrow><mrow><mi>LCS</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>k</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>m</mi><mo>-</mo><mi>k</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\n  When $k = dm$, then this is approximately \n  $\\left( d m \\right) \\times 2^{\\left\\{ 2 H \\left( d \\right) + \\left( 2d - 1 \\right) \\log_2 \\left( \\left| \\Sigma \\right| \\right) \\right\\} m}$,\n  which goes to zero exponentially in $m$ if $d \\in \\left( 0, 1 \\right)$ is small enough. \n  Taking a union bound over $k$ we have that this holds for all $k \\in \\left\\{ 0, 1, \\dots,  dm \\right\\}$ simultaneously. \n\\end{proof}\n\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:large_overlap}] \nThe proof is very similar to the one above. \nIt is again immediate that \n${\\mathrm{ed}} \\left( X \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\leq 2k$, \nsince one can obtain $X \\left[ 1 + k, m + k \\right]$ from $X \\left[ 1, m \\right]$\nby first deleting the first $k$ coordinates of $X\\left[1,m\\right]$ \nand then inserting $X \\left[ m + 1 , m + k \\right]$ at the end of the sequence. \nWhat remains is to show that the probability of\n\n", "itemtype": "equation", "pos": 50233, "prevtext": "\n  is exponentially small in $m$. \n  Note that if a common subsequence of $X \\left[ 1 , m \\right]$ and $X \\left[ 1 + k , m + k \\right]$\n  is such that the $\\ell^{\\text{th}}$ coordinate of $X \\left[ 1 , m \\right]$ \n  is mapped to the $\\left( \\ell - k \\right)^{\\text{th}}$ coordinate of $X \\left[ 1 + k , m + k \\right]$ (the ``trivial'' map), \n  then this subsequence can have length at most $m - k$, \n  since in $X \\left[ 1, m \\right]$ there are only $m - \\ell$ coordinates to the right of this coordinate, \n  while in $X \\left[ 1 + k , m + k \\right]$ there are only $\\ell - k - 1$ coordinates to the left of this coordinate. \n  So if a common subsequence has length greater than $m - k$, then every coordinate is mapped ``nontrivially''. \n  This then creates many constraints on the pair of sequences and so there will not be many of them, as we now argue. \n\n\n  Suppose that \n  ${\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = m - \\ell$\n  for some $\\ell \\in \\left\\{ 0, 1, \\dots, k - 1 \\right\\}$. \n  A noncrossing matching between $m - \\ell$ coordinates of the two sequences \n  is characterized by the $\\ell$ coordinates in each sequence that are not part of the matching; \n  the remaining pairs of the matching are determined due to the noncrossing property. \n  Thus there are $\\binom{m}{\\ell}^2$ such matchings. \n  The coordinates of a LCS between the two sequences corresponds to a noncrossing matching between the two, \n  and it imposes conditions on the values of these coordinates. \n  If \n  ${\\mathrm{LCS}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) = m - \\ell$\n  then all but $\\ell$ coordinates of $X \\left[ 1 + k , m + k \\right]$ are determined by $X \\left[ 1 , m \\right]$. \n  Furthermore, there are at least $m - k - \\ell$ coordinates $j \\in \\left[ 1 + k, m \\right]$ such that \n  the $\\left( j - k \\right)^{\\text{th}}$ coordinate of $X \\left[ 1 + k , m + k \\right]$ is in the matching, \n  and thus the value of $X \\left[ j \\right]$ is determined by a previous value $X \\left[ i \\right]$ for some $i < j$. \n  This means that at most $k + \\ell$ coordinates of $X \\left[ 1, m \\right]$ are not determined by the value of a previous coordinate. \n  So the probability of any given noncrossing matching being a common subsequence is at most \n  $\\left| \\Sigma \\right|^{\\left( k + 2\\ell \\right) - \\left( m + k \\right)} = \\left| \\Sigma \\right|^{2\\ell - m}$. \n  We have thus shown that the probability of~\\eqref{eq:large_LCS} is at most \n \n", "index": 45, "text": "\\[\n   \\sum_{\\ell=0}^{k-1} \\binom{m}{\\ell}^2 \\left| \\Sigma \\right|^{2\\ell - m} \\leq k \\binom{m}{k}^2 \\left| \\Sigma \\right|^{2k - m}.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\sum_{\\ell=0}^{k-1}\\binom{m}{\\ell}^{2}\\left|\\Sigma\\right|^{2\\ell-m}\\leq k%&#10;\\binom{m}{k}^{2}\\left|\\Sigma\\right|^{2k-m}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mi mathvariant=\"normal\">\u2113</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo>-</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>\u2264</mo><mrow><mi>k</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mi>k</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi></mrow><mo>-</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nis exponentially small in $m$. \nIf~\\eqref{eq:large_ed} holds then one can get \nfrom $X \\left[ 1 , m \\right]$ to $X \\left[ 1 + k , m + k \\right]$ \nby first performing $S$ substitutions to get $X' \\left[ 1, m \\right]$, \nthen performing $D$ deletions and finally $I$ insertions. \nThese quantities have to satisfy $S + D + I \\leq 2k - 1$ and $D = I$. \nWe thus have that \n${\\underline{\\mathrm{ed}}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\leq 2k - 1 - S$, \nand so \n${\\mathrm{LCS}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\geq m - k + \\left( 1 + S \\right) / 2$. \nLet $m - \\ell := {\\mathrm{LCS}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right)$. \nAgain, the number of such noncrossing matchings is $\\binom{m}{\\ell}^2$. \nAs in the proof of the previous lemma, \nthe noncrossing matching corresponding to such a LCS has to map every coordinate ``nontrivially''. \nEvery such matching imposes constraints on $X' \\left[ 1, m \\right]$ and $X \\left[ 1 + k, m + k \\right]$, \nand while there are less constraints as before---since $X' \\left[ 1, m \\right]$ differs from $X \\left[ 1, m \\right]$ in $S$ substitutions---we can still show that the probability of~\\eqref{eq:large_ed} is at most\n", "itemtype": "equation", "pos": 51288, "prevtext": "\n  When $k = dm$, then this is approximately \n  $\\left( d m \\right) \\times 2^{\\left\\{ 2 H \\left( d \\right) + \\left( 2d - 1 \\right) \\log_2 \\left( \\left| \\Sigma \\right| \\right) \\right\\} m}$,\n  which goes to zero exponentially in $m$ if $d \\in \\left( 0, 1 \\right)$ is small enough. \n  Taking a union bound over $k$ we have that this holds for all $k \\in \\left\\{ 0, 1, \\dots,  dm \\right\\}$ simultaneously. \n\\end{proof}\n\n\n\n\\begin{proof}[Proof of Lemma~\\ref{lem:large_overlap}] \nThe proof is very similar to the one above. \nIt is again immediate that \n${\\mathrm{ed}} \\left( X \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\leq 2k$, \nsince one can obtain $X \\left[ 1 + k, m + k \\right]$ from $X \\left[ 1, m \\right]$\nby first deleting the first $k$ coordinates of $X\\left[1,m\\right]$ \nand then inserting $X \\left[ m + 1 , m + k \\right]$ at the end of the sequence. \nWhat remains is to show that the probability of\n\n", "index": 47, "text": "\\begin{equation}\\label{eq:large_ed}\n  {\\mathrm{ed}} \\left( X \\left[ 1 , m \\right], X \\left[ 1 + k , m + k \\right] \\right) < 2k \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{ed}}\\left(X\\left[1,m\\right],X\\left[1+k,m+k\\right]\\right)&lt;2k\" display=\"block\"><mrow><mrow><mi>ed</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mn>1</mn><mo>,</mo><mi>m</mi><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo>,</mo><mrow><mi>m</mi><mo>+</mo><mi>k</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>&lt;</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07086.tex", "nexttext": "\nWhen $k = dm$, then this is approximately\n$\\left( d m \\right) \\times 2^{\\left\\{ 2 H \\left( d \\right) + H \\left( 2 d \\right) + \\left( 3d - 1 \\right) \\log_2 \\left( \\left| \\Sigma \\right| \\right) \\right\\} m}$,\nwhich goes to zero exponentially in $m$ if $d \\in \\left( 0, 1 \\right)$ is small enough; \nin particular this happens when $d \\leq 0.0846092$. \nTaking a union bound over $k$ we have that this holds for all $k \\in \\left\\{ 0, 1, \\dots,  dm \\right\\}$ simultaneously. \n\\end{proof}\n\n\n\n", "itemtype": "equation", "pos": 52682, "prevtext": "\nis exponentially small in $m$. \nIf~\\eqref{eq:large_ed} holds then one can get \nfrom $X \\left[ 1 , m \\right]$ to $X \\left[ 1 + k , m + k \\right]$ \nby first performing $S$ substitutions to get $X' \\left[ 1, m \\right]$, \nthen performing $D$ deletions and finally $I$ insertions. \nThese quantities have to satisfy $S + D + I \\leq 2k - 1$ and $D = I$. \nWe thus have that \n${\\underline{\\mathrm{ed}}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\leq 2k - 1 - S$, \nand so \n${\\mathrm{LCS}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right) \\geq m - k + \\left( 1 + S \\right) / 2$. \nLet $m - \\ell := {\\mathrm{LCS}} \\left( X' \\left[ 1, m \\right], X \\left[ 1 + k, m + k \\right] \\right)$. \nAgain, the number of such noncrossing matchings is $\\binom{m}{\\ell}^2$. \nAs in the proof of the previous lemma, \nthe noncrossing matching corresponding to such a LCS has to map every coordinate ``nontrivially''. \nEvery such matching imposes constraints on $X' \\left[ 1, m \\right]$ and $X \\left[ 1 + k, m + k \\right]$, \nand while there are less constraints as before---since $X' \\left[ 1, m \\right]$ differs from $X \\left[ 1, m \\right]$ in $S$ substitutions---we can still show that the probability of~\\eqref{eq:large_ed} is at most\n", "index": 49, "text": "\n\\[\n k \\binom{m}{2k} \\binom{m}{k}^2 \\left| \\Sigma \\right|^{3k - m}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"k\\binom{m}{2k}\\binom{m}{k}^{2}\\left|\\Sigma\\right|^{3k-m}.\" display=\"block\"><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>k</mi></mrow></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>m</mi><mi>k</mi></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo>|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>|</mo></mrow><mrow><mrow><mn>3</mn><mo>\u2062</mo><mi>k</mi></mrow><mo>-</mo><mi>m</mi></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}]