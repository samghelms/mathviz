[{"file": "1601.01981.tex", "nexttext": "\nwhere for observation $j$ in cluster $i$, ${\\mathbf}{r}_{ij}$ is a vector of $r$ predictors of primary interest (e.g., policy variables) and any additional controls, ${\\mathbf}{s}_{ij}$ is a vector of $s$ fixed effects that vary across clusters, and ${\\mathbf}{t}_{ij}$ is a vector of $t$ fixed effects that are identified within clusters. In the state-policy example described in the introduction, the ${\\mathbf}{r}_{ij}$ would include indicator variables for each policy change, as well as additional demographic controls; ${\\mathbf}{s}_{ij}$ would include year fixed effects; and ${\\mathbf}{t}_{ij}$ would indicate state fixed effects. Interest would center on testing hypotheses regarding the coefficients in ${\\boldsymbol}\\beta$ that correspond to the policy indicators, while ${\\boldsymbol}\\gamma$ and ${\\boldsymbol}\\mu$ would be treated as incidental. \n\nFor developing theory, it is often easier to work with the matrix version of this model, in which\n\n", "itemtype": "equation", "pos": 11732, "prevtext": "\n\n\\def\\spacingset#1{ \\renewcommand{\\baselinestretch} {#1}\\small\\normalsize} \\spacingset{1}\n\n\n\n\n\\if0{0}\n{\n  \\title{\\bf Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models}\n  \\author{\\\\James E. Pustejovsky\\thanks{\n    The authors thank Dan Knopf for helpful discussions about the linear algebra behind the cluster-robust variance estimator.}\\hspace{.2cm}\\\\\n    Department of Educational Psychology \\\\ \n    University of Texas at Austin\\\\ \\\\\n    and \\\\ \\\\\n    Elizabeth Tipton \\\\\n    Department of Human Development \\\\ \n    Teachers College, Columbia University}\n  \\maketitle\n} \\fi\n\n\\if1{0}\n{\n  \\bigskip\n  \\bigskip\n  \\bigskip\n  \\begin{center}\n    {\\LARGE\\bf Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed-effect models}\n\\end{center}\n  \\medskip\n} \\fi\n\n\\bigskip\n\\begin{abstract}\nIn longitudinal panels and other regression models with unobserved effects, fixed effects estimation is often paired with cluster-robust variance estimation (CRVE) in order to account for heteroskedasticity and un-modeled dependence among the errors. CRVE is asymptotically consistent as the number of independent clusters increases, but can be biased downward for sample sizes often found in applied work, leading to hypothesis tests with overly liberal rejection rates. One solution is to use bias-reduced linearization (BRL), which corrects the CRVE so that it is unbiased under a working model, and t-tests with Satterthwaite degrees of freedom. We propose a generalization of BRL that can be applied in models with arbitrary sets of fixed effects, where the original BRL method is undefined, and describe how to apply the method when the regression is estimated after absorbing the fixed effects. We also propose a small-sample test for multiple-parameter hypotheses, which generalizes the Satterthwaite approximation for t-tests. In simulations covering a variety of study designs, we find that conventional cluster-robust Wald tests can severely under-reject while the proposed small-sample test maintains Type I error very close to nominal levels. \n\\end{abstract}\n\n\\noindent\n{\\it Keywords:} Robust Standard Errors, Clustering, Fixed-Effects, Small Samples\n\\vfill\n\n\\newpage\n\\spacingset{1.45} \n\n\\section{INTRODUCTION}\n\\label{sec:intro}\n\nIn a wide array of economic analyses, interest centers on the parameters of linear regression models, estimated by ordinary or weighted least squares (OLS/WLS) from a sample of units that are correlated. \nSuch correlation among units can arise from sampling aggregate units (e.g., countries, districts, villages), each of which contains multiple observations; from repeated measurement of an outcome on a common set of units, as in panel data; or from model misspecification, as in analysis of regression discontinuity designs \\citep[e.g.,][]{Lee2008regression}. \nA common approach to inference in these settings is to use a cluster-robust variance estimator \\citep[CRVE;][]{Arellano1987computing, Liang1986longitudinal, white1984asymptotic}.\nThe advantage of CRVEs is that they produce consistent standard errors and test statistics without imposing strong parametric assumptions about the dependence structure of the errors in the model.\nInstead, the method relies on the weaker assumption that units can be grouped into clusters that are mutually independent. \nCRVEs are an extension to another economic mainstay, heteroskedasticity-robust variance estimators \\citep{Huber1967behavior, White1980heteroskedasticity}, which are used to account for non-constant variance in regression models with independent errors.\nIn the past decade, use of CRVE has become standard practice for applied micro-economic analysis, as evidenced by coverage in major textbooks and review articles \\citep[e.g.,][]{Wooldridge2010econometric, Angrist2009mostly, Cameron2015practitioners}.\n\nAs a leading example of the application of CRVEs, consider a study of the effects on employment outcomes of several state-level policy shifts, where the policies were implemented at different time-points in each state. \nIn a difference-in-differences analysis of state-by-year panel data, the policy effects would be parameterized in a regression model that includes indicator variables for each policy shift and perhaps additional demographic controls. It is also common to include fixed effects for states and time-periods in order to control for unobserved confounding in each dimension. \nThe model could be estimated by OLS, with the fixed effects included as indicator variables; more commonly, the effects of the policy indicators would be estimated after absorbing the fixed effects, a computational technique that is also known as the fixed effects or within transformation \\citep{Wooldridge2010econometric}. \nStandard errors would then be clustered by state to account for residual dependence in the errors from a given state, and these clustered standard errors would be used to test hypotheses regarding each policy or the set of policies.\nThe need to cluster the standard errors by state, even when including state fixed effects, was highlighted by \\citet{Bertrand2004how}, who showed that to do otherwise can lead to inappropriately small standard errors and hypothesis tests with incorrect rejection rates. \n\nThe consistency property of CRVEs is asymptotic in the number of independent clusters \\citep{Wooldridge2003cluster}.\nRecent methodological work has demonstrated that CRVEs can be biased downward and associated hypothesis tests can have Type-I error rates considerably in excess of nominal levels when based on samples with only a small or moderate number of clusters \\citep[e.g.,][]{Webb2013wild}.\n\\citet{Cameron2015practitioners} provide a thorough review of this literature, including a discussion of current practice, possible solutions, and open problems. \nIn particular, they demonstrate that small-sample corrections for t-tests implemented in common software packages such as Stata and SAS do not provide adequate control of Type-I error. \n\n\\citet[see also \\citealp{McCaffrey2001generalizations}]{Bell2002bias} proposed a method that improves the small-sample properties of CRVEs. \nTheir method, called bias-reduced linearization (BRL), entails adjusting the CRVE so that it is exactly unbiased under a working model specified by the analyst, while also remaining asymptotically consistent under arbitrary true variance structures. \nSimulations reported by \\citet{Bell2002bias} demonstrate that the BRL correction serves to reduce the bias of the CRVE even when the working model is misspecified. \nThe same authors also proposed and studied small-sample corrections to single-parameter hypothesis tests using the BRL variance estimator, based on Satterthwaite \\citep{Bell2002bias} or saddlepoint approximations \\citep{McCaffrey2006improved}. \n\\citet{Angrist2009effects} applied the BRL correction in an analysis of a longitudinal cluster-randomized trial with 35 clusters, observing that the bias correction makes a difference for inferences. \n\nDespite a growing body of simulation evidence that BRL performs well \\citep[e.g.,][]{Imbens2015robust}, several problems with the method hinder its wider application. \nFirst, \\citet{Angrist2009mostly} noted that the BRL correction breaks down (i.e., cannot be calculated) in some highly parameterized models, such as state-by-year panels that include fixed effects for states and for years.\nSecond, in models with fixed effects, the magnitude of the BRL adjustment depends on whether it is computed based on the full design matrix used in OLS estimation (i.e., including fixed effect dummies) or after absorbing the fixed effects. \n\\citet{Cameron2015practitioners} noted that other methods of small-sample correction suffer from the same subtle problem of depending on arbitrary computational details.  \nThird, extant methods for hypothesis testing based on BRL are limited to single-parameter constraints \\citep{Bell2002bias, McCaffrey2006improved} and small-sample methods for multiple-parameter hypothesis tests remain lacking.\nMultiple-parameter tests are used in a range of applications, including in panel data settings (e.g., Hausman tests for consistency of random effects estimators), seemingly unrelated regression models, and analysis of field experiments with multiple treatment groups.\n\nThis paper addresses each of these concerns in turn, with the aim of extending the BRL method so that is suitable for everyday econometric practice, including models with fixed effects. \nFirst, we describe an extension to the BRL adjustment that is well-defined in models with arbitrary sets of fixed effects, where existing BRL adjustments break down. \nSecond, we demonstrate how to calculate the BRL adjustments so that they are invariant to whether the regression model is estimated including dummy fixed effects or after absorbing the fixed effects (i.e., using the within estimator) and identify conditions under which first-stage absorption of the fixed effects can be safely ignored. \nFinally, we propose a procedure for testing multiple-parameter hypotheses by approximating the sampling distribution of the Wald statistic by Hotelling's $T^2$ distribution with estimated degrees of freedom. The method is a generalization of the Satterthwaite correction proposed by \\citet{Bell2002bias} for single parameter constraints.\n\nOur work is related to a stream of recent literature that has examined methods for cluster-robust inference with a small number of clusters. \n\\citet{Conley2011inference} proposed methods for hypothesis testing in a difference-in-differences setting where the number of treated units is small and fixed, while the number of untreated units increases asymptotically. \n\\citet{Ibragimov2010tstatistic} proposed a method for constructing robust tests of scalar parameters that maintains the nominal Type-I error rate; however, their method requires that the target parameter be identified within each independent cluster and so it is not always applicable.  \n\\citet{Cameron2008bootstrap} investigated a range of bootstrapping procedures that provide improved Type-I error control in small samples, finding that a cluster wild-bootstrap technique was particularly accurate in small samples. \nNearly all of this work has focused on single-parameter hypothesis tests only. \nFor multiple-parameter constraints, \\citet{Cameron2015practitioners} suggest an ad-hoc degrees of freedom adjustment and note, as an alternative, that bootstrapping techniques can in principle be applied to multiple-parameter tests. \nHowever, little methodological work has examined the accuracy of multiple-parameter tests.\n\nThe paper is organized as follows. The remainder of this section introduces our econometric framework and reviews the standard CRVE methods, as implemented in most software applications.\nSection \\ref{sec:BRL} reviews the original BRL correction and describes modifications that make it possible to implement BRL in a broad class of models with fixed effects.\nSection \\ref{sec:testing} discusses methods for hypothesis testing based on the BRL-adjusted CRVE. \nSection \\ref{sec:simulation} reports a simulation study examining the null rejection rates of our proposed test for multiple-parameter constraints, where we find that the small-sample test offers drastic improvements over commonly implemented alternatives. \nSection \\ref{sec:examples} illustrates the use of the proposed hypothesis tests in three examples that cover a variety of contexts in which CRVE is commonly used. \nSection \\ref{sec:conclusion} concludes and discusses avenues for future work. \n\n\\subsection{Econometric framework}\n\nWe consider a linear regression model of the form,\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:fixed_effects_ij}\n\\ {y}_{ij} = {\\mathbf}{r}_{ij}' {\\boldsymbol}\\beta + {\\mathbf}{s}_{ij}' {\\boldsymbol}\\gamma + {\\mathbf}{t}_{ij}' {\\boldsymbol}\\mu + \\epsilon_{ij} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\ {y}_{ij}={\\mathbf{}}{r}_{ij}^{\\prime}{\\boldsymbol{}}\\beta+{\\mathbf{}}{s}_{ij%&#10;}^{\\prime}{\\boldsymbol{}}\\gamma+{\\mathbf{}}{t}_{ij}^{\\prime}{\\boldsymbol{}}\\mu%&#10;+\\epsilon_{ij}\" display=\"block\"><mrow><msub><mpadded lspace=\"5pt\" width=\"+5pt\"><mi>y</mi></mpadded><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mrow><msubsup><mi>r</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo>+</mo><mrow><msubsup><mi>t</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\u03bc</mi></mrow><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere for cluster $i$, ${\\mathbf}{R}_i$ is an $n_i \\times r$ matrix of focal predictors and controls; ${\\mathbf}{S}_i$ is an $n_i \\times s$ matrix describing fixed effects that vary across clusters, and ${\\mathbf}{T}_i$ is an $n_i \\times t$ matrix describing fixed effects that are identified only within clusters. The distinction between the covariates ${\\mathbf}{R}_i$ versus the fixed effects ${\\mathbf}{S}_i$ is arbitrary and depends on the analyst's inferential goals. However, the distinction between the two fixed effect matrices ${\\mathbf}{S}_i$ and ${\\mathbf}{T}_i$ is unambiguous, in that the within-cluster fixed effects satisfy ${\\mathbf}{T}_h {\\mathbf}{T}_i' = {\\mathbf}{0}$ for $h \\neq i$. \n\nWe shall assume that ${\\text{E}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i, {\\mathbf}{T}_i\\right.\\right) = {\\mathbf}{0}$ and ${\\text{Var}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i,{\\mathbf}{T}_i\\right.\\right) = {\\boldsymbol}\\Sigma_i$, for $i = 1,...,m$, where the form of ${\\boldsymbol}\\Sigma_1,...,{\\boldsymbol}\\Sigma_m$ may be unknown but the errors are independent across clusters. \nFor notational convenience, let ${\\mathbf}{U}_i = \\left[{\\mathbf}{R}_i \\ {\\mathbf}{S}_i \\right]$ denote the set of predictors that vary across clusters, ${\\mathbf}{X}_i = \\left[{\\mathbf}{R}_i \\ {\\mathbf}{S}_i \\ {\\mathbf}{T}_i \\right]$ denote the full set of predictors, ${\\boldsymbol}\\alpha = \\left({\\boldsymbol}\\beta', {\\boldsymbol}\\gamma', {\\boldsymbol}\\mu' \\right)'$, and $p = r + s + t$.\nDenote the total number of individual observations by $N = \\sum_{i=1}^m n_i$.\nLet ${\\mathbf}{y}$, ${\\mathbf}{R}$, ${\\mathbf}{S}$, ${\\mathbf}{T}$, ${\\mathbf}{U}$, ${\\mathbf}{X}$, and ${\\boldsymbol}\\epsilon$ denote the matrices obtained by stacking their corresponding components, as in ${\\mathbf}{R} = \\left({\\mathbf}{R}_1' \\ {\\mathbf}{R}_2' \\ \\cdots \\ {\\mathbf}{R}_m'\\right)'$. \n\nWe assume that ${\\boldsymbol}\\beta$ is estimated by weighted least squares (WLS) using symmetric, full rank weighting matrices ${\\mathbf}{W}_1,...,{\\mathbf}{W}_m$. \nClearly, the WLS estimator includes OLS as a special case (where ${\\mathbf}{W}_i = {\\mathbf}{I}_i$, an identity matrix), as well as feasible GLS.\\footnote{\nThe WLS estimator also encompasses the estimator proposed by \\citet{Ibragimov2010tstatistic} for clustered data. \nAssuming that ${\\mathbf}{X}_i$ has rank $p$ for $i = 1,...,m$, their proposed approach involves estimating ${\\boldsymbol}\\beta$ separately within each cluster and taking the simple average of these estimates. \nThe resulting average is equivalent to the WLS estimator with weights ${\\mathbf}{W}_i = {\\mathbf}{X}_i \\left({\\mathbf}{X}_i'{\\mathbf}{X}_i\\right)^{-2} {\\mathbf}{X}_i$.} \nIn the latter case, it is assumed that ${\\text{Var}}\\left({\\mathbf}{e}_i\\left|{\\mathbf}{X}_i\\right.\\right) = {\\boldsymbol}\\Phi_i$, where ${\\boldsymbol}\\Phi_i$ is a known function of a low-dimensional parameter. \nFor example, an auto-regressive error structure might be posited to describe repeated measures on an individual over time. \nThe weighting matrices are then taken to be ${\\mathbf}{W}_i = \\hat{{\\boldsymbol}\\Phi}_i^{-1}$, where the $\\hat{{\\boldsymbol}\\Phi}_i$ are constructed from estimates of the variance parameter.\nFinally, for analysis of data from complex survey designs, WLS may be used with sampling weights in order to account for unequal selection probabilities.\n\n\\subsection{Absorption}\n\nThe goal of most analyses is to estimate and test hypotheses regarding the parameters in ${\\boldsymbol}\\beta$, while the fixed effects ${\\boldsymbol}\\gamma$ and ${\\boldsymbol}\\mu$ are not of inferential interest. Moreover, estimating all of the parameters by WLS becomes computationally intensive and numerically inaccurate if the model includes a large number of fixed effects (i.e., $s + t$ large). \nA commonly implemented solution is to first absorb the fixed effects, which leaves only the $r$ parameters in ${\\boldsymbol}\\beta$ to be estimated. \nSection \\ref{sec:BRL} examines the implications of absorption for application of the BRL adjustment. \nIn order to do, we now formalize the absorption method.\n\nTo begin, denote the full block-diagonal weighting matrix as ${\\mathbf}{W} = \\text{diag}\\left({\\mathbf}{W}_1,...,{\\mathbf}{W}_m\\right)$.\nLet ${\\mathbf}{K}$ be the $x \\times r$ matrix that selects the covariates of interest, so that ${\\mathbf}{X} {\\mathbf}{K} = {\\mathbf}{R}$ and ${\\mathbf}{K}'{\\boldsymbol}\\alpha = {\\boldsymbol}\\beta$.\nFor a generic matrix ${\\mathbf}{Z}$ of full column rank, let ${\\mathbf}{M_Z} = \\left({\\mathbf}{Z}'{\\mathbf}{W}{\\mathbf}{Z}\\right)^{-1}$ and ${\\mathbf}{H_Z} = {\\mathbf}{Z}{\\mathbf}{M_Z}{\\mathbf}{Z}'{\\mathbf}{W}$. \n\nThe absorption technique involves obtaining the residuals from the regression of ${\\mathbf}{y}$ on ${\\mathbf}{T}$ and from the multivariate regressions of ${\\mathbf}{U} = [{\\mathbf}{R}\\ {\\mathbf}{S}]$ on ${\\mathbf}{T}$. \nThe ${\\mathbf}{y}$ residuals and ${\\mathbf}{R}$ residuals are then regressed on the ${\\mathbf}{S}$ residuals. \nFinally, these twice-regressed ${\\mathbf}{y}$ residuals are regressed on the twice-regressed ${\\mathbf}{R}$ residuals to obtain the WLS estimates of ${\\boldsymbol}\\beta$. \nLet ${\\mathbf}{\\ddot{S}} = \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{S}$, ${\\mathbf}{\\ddot{R}} = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right)\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{R}$, and ${\\mathbf}{\\ddot{y}} = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right)\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{y}$. \nIn what follows, subscripts on ${\\mathbf}{\\ddot{R}}$, ${\\mathbf}{\\ddot{S}}$,  ${\\mathbf}{\\ddot{U}}$, and ${\\mathbf}{\\ddot{y}}$ refer to the rows of these matrices corresponding to a specific cluster. \nThe WLS estimator of ${\\boldsymbol}\\beta$ can then be written as\n\n", "itemtype": "equation", "pos": 12898, "prevtext": "\nwhere for observation $j$ in cluster $i$, ${\\mathbf}{r}_{ij}$ is a vector of $r$ predictors of primary interest (e.g., policy variables) and any additional controls, ${\\mathbf}{s}_{ij}$ is a vector of $s$ fixed effects that vary across clusters, and ${\\mathbf}{t}_{ij}$ is a vector of $t$ fixed effects that are identified within clusters. In the state-policy example described in the introduction, the ${\\mathbf}{r}_{ij}$ would include indicator variables for each policy change, as well as additional demographic controls; ${\\mathbf}{s}_{ij}$ would include year fixed effects; and ${\\mathbf}{t}_{ij}$ would indicate state fixed effects. Interest would center on testing hypotheses regarding the coefficients in ${\\boldsymbol}\\beta$ that correspond to the policy indicators, while ${\\boldsymbol}\\gamma$ and ${\\boldsymbol}\\mu$ would be treated as incidental. \n\nFor developing theory, it is often easier to work with the matrix version of this model, in which\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:fixed_effects}\n{\\mathbf}{y}_i = {\\mathbf}{R}_i {\\boldsymbol}\\beta + {\\mathbf}{S}_i {\\boldsymbol}\\gamma + {\\mathbf}{T}_i {\\boldsymbol}\\mu + {\\boldsymbol}\\epsilon_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{y}_{i}={\\mathbf{}}{R}_{i}{\\boldsymbol{}}\\beta+{\\mathbf{}}{S}_{i}{%&#10;\\boldsymbol{}}\\gamma+{\\mathbf{}}{T}_{i}{\\boldsymbol{}}\\mu+{\\boldsymbol{}}%&#10;\\epsilon_{i},\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo>+</mo><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\u03bc</mi></mrow><mo>+</mo><msub><mi>\u03f5</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nThis estimator is algebraically identical to the direct WLS estimator based on the full set of predictors,\n", "itemtype": "equation", "pos": 18916, "prevtext": "\nwhere for cluster $i$, ${\\mathbf}{R}_i$ is an $n_i \\times r$ matrix of focal predictors and controls; ${\\mathbf}{S}_i$ is an $n_i \\times s$ matrix describing fixed effects that vary across clusters, and ${\\mathbf}{T}_i$ is an $n_i \\times t$ matrix describing fixed effects that are identified only within clusters. The distinction between the covariates ${\\mathbf}{R}_i$ versus the fixed effects ${\\mathbf}{S}_i$ is arbitrary and depends on the analyst's inferential goals. However, the distinction between the two fixed effect matrices ${\\mathbf}{S}_i$ and ${\\mathbf}{T}_i$ is unambiguous, in that the within-cluster fixed effects satisfy ${\\mathbf}{T}_h {\\mathbf}{T}_i' = {\\mathbf}{0}$ for $h \\neq i$. \n\nWe shall assume that ${\\text{E}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i, {\\mathbf}{T}_i\\right.\\right) = {\\mathbf}{0}$ and ${\\text{Var}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i,{\\mathbf}{T}_i\\right.\\right) = {\\boldsymbol}\\Sigma_i$, for $i = 1,...,m$, where the form of ${\\boldsymbol}\\Sigma_1,...,{\\boldsymbol}\\Sigma_m$ may be unknown but the errors are independent across clusters. \nFor notational convenience, let ${\\mathbf}{U}_i = \\left[{\\mathbf}{R}_i \\ {\\mathbf}{S}_i \\right]$ denote the set of predictors that vary across clusters, ${\\mathbf}{X}_i = \\left[{\\mathbf}{R}_i \\ {\\mathbf}{S}_i \\ {\\mathbf}{T}_i \\right]$ denote the full set of predictors, ${\\boldsymbol}\\alpha = \\left({\\boldsymbol}\\beta', {\\boldsymbol}\\gamma', {\\boldsymbol}\\mu' \\right)'$, and $p = r + s + t$.\nDenote the total number of individual observations by $N = \\sum_{i=1}^m n_i$.\nLet ${\\mathbf}{y}$, ${\\mathbf}{R}$, ${\\mathbf}{S}$, ${\\mathbf}{T}$, ${\\mathbf}{U}$, ${\\mathbf}{X}$, and ${\\boldsymbol}\\epsilon$ denote the matrices obtained by stacking their corresponding components, as in ${\\mathbf}{R} = \\left({\\mathbf}{R}_1' \\ {\\mathbf}{R}_2' \\ \\cdots \\ {\\mathbf}{R}_m'\\right)'$. \n\nWe assume that ${\\boldsymbol}\\beta$ is estimated by weighted least squares (WLS) using symmetric, full rank weighting matrices ${\\mathbf}{W}_1,...,{\\mathbf}{W}_m$. \nClearly, the WLS estimator includes OLS as a special case (where ${\\mathbf}{W}_i = {\\mathbf}{I}_i$, an identity matrix), as well as feasible GLS.\\footnote{\nThe WLS estimator also encompasses the estimator proposed by \\citet{Ibragimov2010tstatistic} for clustered data. \nAssuming that ${\\mathbf}{X}_i$ has rank $p$ for $i = 1,...,m$, their proposed approach involves estimating ${\\boldsymbol}\\beta$ separately within each cluster and taking the simple average of these estimates. \nThe resulting average is equivalent to the WLS estimator with weights ${\\mathbf}{W}_i = {\\mathbf}{X}_i \\left({\\mathbf}{X}_i'{\\mathbf}{X}_i\\right)^{-2} {\\mathbf}{X}_i$.} \nIn the latter case, it is assumed that ${\\text{Var}}\\left({\\mathbf}{e}_i\\left|{\\mathbf}{X}_i\\right.\\right) = {\\boldsymbol}\\Phi_i$, where ${\\boldsymbol}\\Phi_i$ is a known function of a low-dimensional parameter. \nFor example, an auto-regressive error structure might be posited to describe repeated measures on an individual over time. \nThe weighting matrices are then taken to be ${\\mathbf}{W}_i = \\hat{{\\boldsymbol}\\Phi}_i^{-1}$, where the $\\hat{{\\boldsymbol}\\Phi}_i$ are constructed from estimates of the variance parameter.\nFinally, for analysis of data from complex survey designs, WLS may be used with sampling weights in order to account for unequal selection probabilities.\n\n\\subsection{Absorption}\n\nThe goal of most analyses is to estimate and test hypotheses regarding the parameters in ${\\boldsymbol}\\beta$, while the fixed effects ${\\boldsymbol}\\gamma$ and ${\\boldsymbol}\\mu$ are not of inferential interest. Moreover, estimating all of the parameters by WLS becomes computationally intensive and numerically inaccurate if the model includes a large number of fixed effects (i.e., $s + t$ large). \nA commonly implemented solution is to first absorb the fixed effects, which leaves only the $r$ parameters in ${\\boldsymbol}\\beta$ to be estimated. \nSection \\ref{sec:BRL} examines the implications of absorption for application of the BRL adjustment. \nIn order to do, we now formalize the absorption method.\n\nTo begin, denote the full block-diagonal weighting matrix as ${\\mathbf}{W} = \\text{diag}\\left({\\mathbf}{W}_1,...,{\\mathbf}{W}_m\\right)$.\nLet ${\\mathbf}{K}$ be the $x \\times r$ matrix that selects the covariates of interest, so that ${\\mathbf}{X} {\\mathbf}{K} = {\\mathbf}{R}$ and ${\\mathbf}{K}'{\\boldsymbol}\\alpha = {\\boldsymbol}\\beta$.\nFor a generic matrix ${\\mathbf}{Z}$ of full column rank, let ${\\mathbf}{M_Z} = \\left({\\mathbf}{Z}'{\\mathbf}{W}{\\mathbf}{Z}\\right)^{-1}$ and ${\\mathbf}{H_Z} = {\\mathbf}{Z}{\\mathbf}{M_Z}{\\mathbf}{Z}'{\\mathbf}{W}$. \n\nThe absorption technique involves obtaining the residuals from the regression of ${\\mathbf}{y}$ on ${\\mathbf}{T}$ and from the multivariate regressions of ${\\mathbf}{U} = [{\\mathbf}{R}\\ {\\mathbf}{S}]$ on ${\\mathbf}{T}$. \nThe ${\\mathbf}{y}$ residuals and ${\\mathbf}{R}$ residuals are then regressed on the ${\\mathbf}{S}$ residuals. \nFinally, these twice-regressed ${\\mathbf}{y}$ residuals are regressed on the twice-regressed ${\\mathbf}{R}$ residuals to obtain the WLS estimates of ${\\boldsymbol}\\beta$. \nLet ${\\mathbf}{\\ddot{S}} = \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{S}$, ${\\mathbf}{\\ddot{R}} = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right)\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{R}$, and ${\\mathbf}{\\ddot{y}} = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right)\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{y}$. \nIn what follows, subscripts on ${\\mathbf}{\\ddot{R}}$, ${\\mathbf}{\\ddot{S}}$,  ${\\mathbf}{\\ddot{U}}$, and ${\\mathbf}{\\ddot{y}}$ refer to the rows of these matrices corresponding to a specific cluster. \nThe WLS estimator of ${\\boldsymbol}\\beta$ can then be written as\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:WLS}\n{\\boldsymbol}{\\hat\\beta} = {\\mathbf}{M_{\\ddot{R}}} \\sum_{i=1}^m {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{y}}_i. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{}}{\\hat{\\beta}}={\\mathbf{}}{M_{\\ddot{R}}}\\sum_{i=1}^{m}{\\mathbf{}%&#10;}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{y}}_{i}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>y</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nbut avoids the need to solve a system of $p$ linear equations.\n\nIn the remainder, we focus on the more general case in which fixed effects are absorbed before estimation of ${\\boldsymbol}\\beta$. For models that do not include within-cluster fixed effects, so that the full covariate matrix is ${\\mathbf}{U} = \\left[{\\mathbf}{R} \\ {\\mathbf}{S}\\right]$, all of the results hold after substituting ${\\mathbf}{U}$ for ${\\mathbf}{\\ddot{R}}$. \n\n\\subsection{Standard CRVE}\n\nThe WLS estimator ${\\boldsymbol}{\\hat\\beta}$, has true variance\n\n", "itemtype": "equation", "pos": 19196, "prevtext": "\nThis estimator is algebraically identical to the direct WLS estimator based on the full set of predictors,\n", "index": 7, "text": "\\[\n{\\boldsymbol}{\\hat\\beta} = {\\mathbf}{K}'{\\mathbf}{M_X} \\sum_{i=1}^m {\\mathbf}{X}_i' {\\mathbf}{W}_i {\\mathbf}{y}_i,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{}}{\\hat{\\beta}}={\\mathbf{}}{K}^{\\prime}{\\mathbf{}}{M_{X}}\\sum_{i=%&#10;1}^{m}{\\mathbf{}}{X}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{y}_{i},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msup><mi>K</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>M</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msubsup><mi>X</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhich depends upon the unknown variance matrices ${\\boldsymbol}\\Sigma_i$. \nA model-based approach to estimating this variance would involve assuming that ${\\mathbf}\\Sigma_i$ follows a structure defined by some low-dimensional parameter; for example, it may be assumed that the structure was hierarchical or auto-regressive. \nThe model-based variance estimator would substitute estimates of ${\\boldsymbol}\\Sigma_i$ into (\\ref{eq:var_WLS}).\nHowever, if the model is misspecified, this estimator will be inconsistent and inferences based upon it will be invalid.\n\nThe CRVE involves estimating ${\\text{Var}}\\left({\\boldsymbol}{\\hat\\beta}\\right)$ empirically, without imposing structural assumptions on ${\\boldsymbol}\\Sigma_i$. \nWhile there are several versions of this approach, all can be written in the form\n\n", "itemtype": "equation", "pos": 19848, "prevtext": "\nbut avoids the need to solve a system of $p$ linear equations.\n\nIn the remainder, we focus on the more general case in which fixed effects are absorbed before estimation of ${\\boldsymbol}\\beta$. For models that do not include within-cluster fixed effects, so that the full covariate matrix is ${\\mathbf}{U} = \\left[{\\mathbf}{R} \\ {\\mathbf}{S}\\right]$, all of the results hold after substituting ${\\mathbf}{U}$ for ${\\mathbf}{\\ddot{R}}$. \n\n\\subsection{Standard CRVE}\n\nThe WLS estimator ${\\boldsymbol}{\\hat\\beta}$, has true variance\n\n", "index": 9, "text": "\\begin{equation}\n\\label{eq:var_WLS}\n{\\text{Var}}\\left({\\boldsymbol}{\\hat\\beta}\\right) = {\\mathbf}{M_{\\ddot{R}}}\\left(\\sum_{i=1}^m {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\boldsymbol}\\Sigma_i {\\mathbf}{W}_i{\\mathbf}{\\ddot{R}}_i\\right) {\\mathbf}{M_{\\ddot{R}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"{\\text{Var}}\\left({\\boldsymbol{}}{\\hat{\\beta}}\\right)={\\mathbf{}}{M_{\\ddot{R}}%&#10;}\\left(\\sum_{i=1}^{m}{\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{%&#10;\\boldsymbol{}}\\Sigma_{i}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}\\right){%&#10;\\mathbf{}}{M_{\\ddot{R}}},\" display=\"block\"><mrow><mrow><mrow><mtext>Var</mtext><mo>\u2062</mo><mrow><mo>(</mo><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{e}_i = {\\mathbf}{Y}_i - {\\mathbf}{X}_i {\\boldsymbol}{\\hat\\beta}$ is the vector of residuals from cluster $i$ and ${\\mathbf}{A}_i$ is some $n_i$ by $n_i$ adjustment matrix. \n\nThe form of these adjustment matrices parallels those of the heteroskedasticity-consistent (HC) variance estimators proposed by \\citet*{MacKinnon1985some}. \nThe most basic CRVE, described by \\citet{Liang1986longitudinal}, uses ${\\mathbf}{A}_i = {\\mathbf}{I}_i$, an $n_i \\times n_i$ identity matrix. \nFollowing \\citet{Cameron2015practitioners}, we refer to this estimator as CR0. \nThis estimator is biased towards zero because the cross-product of the residuals ${\\mathbf}{e}_i {\\mathbf}{e}_i'$ tends to under-estimate the true variance ${\\boldsymbol}\\Sigma_i$ in cluster $i$.\nA rough bias adjustment is to take ${\\mathbf}{A}_i = c{\\mathbf}{I}_i$, where $c = \\sqrt{(m/(m-1))}$; we denote this adjusted estimator as CR1. Some functions in Stata use a slightly different correction factor $c_S = \\sqrt{(m N)/[(m - 1)(N - p)]}$; we will refer to the adjusted estimator using $c_S$ as CR1S. When $N >> p$, $c_S \\approx \\sqrt{m/(m-1)}$ and so CR1 and CR1S will be very similar.\nThe CR1 or CR1S estimator is now commonly used in empirical applications.\n\nUse of these adjustments still tends to under-estimate the true variance of $\\hat{{\\boldsymbol}\\beta}$ \\citep{Cameron2015practitioners}. \nAnalytic work and simulation studies indicate that the degree of bias depends not only on the number of clusters $m$, but also on features of the covariates in ${\\mathbf}{X}$. Specifically, \nthe bias tends to be larger when the covariates are skewed or unbalanced across clusters, or when clusters vary in size \\citep{Carter2013asymptotic, MacKinnon2013thirty}. \nA more principled approach to bias correction would therefore take into account these features of the covariates. \nOne such estimator uses adjustment matrices given by ${\\mathbf}{A}_i = \\left({\\mathbf}{I} - {\\mathbf}{\\ddot{R}}_i {\\mathbf}{M_{\\ddot{R}}}{\\mathbf}{\\ddot{R}}_i'{\\mathbf}{W}_i\\right)^{-1}$. This estimator, denoted CR3, closely approximates the jackknife re-sampling variance estimator \\citep{Bell2002bias, Mancl2001covariance}.  \nHowever, CR3 tends to over-correct the bias of CR0, while the CR1 estimator tends to under-correct. \nThe next section describes in detail the BRL approach, which makes adjustments that are intermediate in magnitude between CR1 and CR3. \n\n\n\\section{BIAS REDUCED LINEARIZATION}\n\\label{sec:BRL}\n\nIn contrast to the CR1 or CR3 estimators, the BRL correction for CRVE is premised on a ``working'' model for the structure of the errors, which must be specified by the analyst. \nUnder a given working model, adjustment matrices ${\\mathbf}{A}_i$ are defined so that the variance estimator is exactly unbiased.\nWe refer to this correction as CR2 because it is an extension of the HC2 variance estimator for regressions with uncorrelated errors, which is exactly unbiased when the errors are homoskedastic \\citep{MacKinnon1985some}.\nThe idea of specifying a model may seem antithetical to the purpose of using CRVE, yet extensive simulation studies have demonstrated that the method performs better in small samples than any of the other adjustments, even when the working model is incorrect. (See Section \\ref{sec:simulation} for a review of this literature.) \nAlthough the CR2 estimator may no longer be exactly unbiased when the working model is misspecified, its bias still tends to be greatly reduced compared to CR1 or CR0 (thus the name ``bias reduced linearization''). Furthermore, as the number of clusters increases, reliance on the working model diminishes. \nIn a sense, CR2 provides necessary scaffolding in the small-sample case, which falls away when there is sufficient data.\n\nLet ${\\boldsymbol}\\Phi_i$ denote a working model for the covariance of the errors in cluster $i$ (up to a scalar constant), with ${\\boldsymbol}\\Phi = \\text{diag}\\left({\\boldsymbol}\\Phi_1,...,{\\boldsymbol}\\Phi_m\\right)$. \nFor example, following \\citet{Bell2002bias} we might assume ${\\boldsymbol}\\Phi_i = {\\mathbf}{I}_i$, i.e., that the errors are uncorrelated and homoskedastic. \nAlternatively, \\citet{Imbens2015robust} suggested using a basic random effects (i.e., compound symmetric) structure, in which ${\\boldsymbol}\\Phi_i$ has unit diagonal entries and off-diagonal entries of $\\rho$, with $\\rho$ estimated using the OLS residuals \\citep[see][p. 16]{Imbens2015robust}.\n\nBased on a given working model, in the original formulation of \\citet{Bell2002bias}, the BRL adjustment matrices are chosen to satisfy the criterion\n\n", "itemtype": "equation", "pos": 20927, "prevtext": "\nwhich depends upon the unknown variance matrices ${\\boldsymbol}\\Sigma_i$. \nA model-based approach to estimating this variance would involve assuming that ${\\mathbf}\\Sigma_i$ follows a structure defined by some low-dimensional parameter; for example, it may be assumed that the structure was hierarchical or auto-regressive. \nThe model-based variance estimator would substitute estimates of ${\\boldsymbol}\\Sigma_i$ into (\\ref{eq:var_WLS}).\nHowever, if the model is misspecified, this estimator will be inconsistent and inferences based upon it will be invalid.\n\nThe CRVE involves estimating ${\\text{Var}}\\left({\\boldsymbol}{\\hat\\beta}\\right)$ empirically, without imposing structural assumptions on ${\\boldsymbol}\\Sigma_i$. \nWhile there are several versions of this approach, all can be written in the form\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:V_small}\n{\\mathbf}{V}^{CR} = {\\mathbf}{M_{\\ddot{R}}}\\left(\\sum_{i=1}^m {\\mathbf}{\\ddot{R}}_i'{\\mathbf}{W}_i {\\mathbf}{A}_i {\\mathbf}{e}_i {\\mathbf}{e}_i' {\\mathbf}{A}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i\\right) {\\mathbf}{M_{\\ddot{R}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{V}^{CR}={\\mathbf{}}{M_{\\ddot{R}}}\\left(\\sum_{i=1}^{m}{\\mathbf{}}{%&#10;\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{A}_{i}{\\mathbf{}}{e}_{i}{%&#10;\\mathbf{}}{e}_{i}^{\\prime}{\\mathbf{}}{A}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{%&#10;\\mathbf{}}{\\ddot{R}}_{i}\\right){\\mathbf{}}{M_{\\ddot{R}}},\" display=\"block\"><mrow><mrow><msup><mi>V</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi></mrow></msup><mo>=</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>e</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>e</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>A</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$ denotes the rows of ${\\mathbf}{I} - {\\mathbf}{H_X}$ corresponding to cluster $i$.\nIf the working model and weight matrices are both taken to be identity matrices, then the adjustment matrices simplify to ${\\mathbf}{A}_i = \\left({\\mathbf}{I}_i - {\\mathbf}{X}_i{\\mathbf}{M_X} {\\mathbf}{X}_i'\\right)^{-1/2}$, where ${\\mathbf}{Z}^{-1/2}$ denotes the symmetric square-root of the matrix ${\\mathbf}{Z}$. \nThis formulation of ${\\mathbf}{A}_i$ is problematic because, for some fixed effects models that are common in economic applications, Equation \\ref{eq:CR2_criterion_BM} does not have a solution. \nIn the next two subsections, we address two problems that arise in models with fixed effects, thereby articulating a BRL methodology that is suitable for a wide range of applications.\n\n\\subsection{Generalized Inverse}\n\nThe equality defining the ${\\mathbf}{A}_i$ matrices cannot always be solved because it is possible that some of the matrices involved are not of full rank, and thus cannot be inverted. \n\\citet{Angrist2009mostly} note that this problem arises in balanced state-by-year panel models that include fixed effects for states and for years. \nIn order to address this concern, we provide an alternative criterion for the adjustment matrices that can always be satisfied. \nInstead of criterion (\\ref{eq:CR2_criterion_BM}), we seek adjustment matrices ${\\mathbf}{A}_i$ that satisfy:\n\n", "itemtype": "equation", "pos": 25796, "prevtext": "\nwhere ${\\mathbf}{e}_i = {\\mathbf}{Y}_i - {\\mathbf}{X}_i {\\boldsymbol}{\\hat\\beta}$ is the vector of residuals from cluster $i$ and ${\\mathbf}{A}_i$ is some $n_i$ by $n_i$ adjustment matrix. \n\nThe form of these adjustment matrices parallels those of the heteroskedasticity-consistent (HC) variance estimators proposed by \\citet*{MacKinnon1985some}. \nThe most basic CRVE, described by \\citet{Liang1986longitudinal}, uses ${\\mathbf}{A}_i = {\\mathbf}{I}_i$, an $n_i \\times n_i$ identity matrix. \nFollowing \\citet{Cameron2015practitioners}, we refer to this estimator as CR0. \nThis estimator is biased towards zero because the cross-product of the residuals ${\\mathbf}{e}_i {\\mathbf}{e}_i'$ tends to under-estimate the true variance ${\\boldsymbol}\\Sigma_i$ in cluster $i$.\nA rough bias adjustment is to take ${\\mathbf}{A}_i = c{\\mathbf}{I}_i$, where $c = \\sqrt{(m/(m-1))}$; we denote this adjusted estimator as CR1. Some functions in Stata use a slightly different correction factor $c_S = \\sqrt{(m N)/[(m - 1)(N - p)]}$; we will refer to the adjusted estimator using $c_S$ as CR1S. When $N >> p$, $c_S \\approx \\sqrt{m/(m-1)}$ and so CR1 and CR1S will be very similar.\nThe CR1 or CR1S estimator is now commonly used in empirical applications.\n\nUse of these adjustments still tends to under-estimate the true variance of $\\hat{{\\boldsymbol}\\beta}$ \\citep{Cameron2015practitioners}. \nAnalytic work and simulation studies indicate that the degree of bias depends not only on the number of clusters $m$, but also on features of the covariates in ${\\mathbf}{X}$. Specifically, \nthe bias tends to be larger when the covariates are skewed or unbalanced across clusters, or when clusters vary in size \\citep{Carter2013asymptotic, MacKinnon2013thirty}. \nA more principled approach to bias correction would therefore take into account these features of the covariates. \nOne such estimator uses adjustment matrices given by ${\\mathbf}{A}_i = \\left({\\mathbf}{I} - {\\mathbf}{\\ddot{R}}_i {\\mathbf}{M_{\\ddot{R}}}{\\mathbf}{\\ddot{R}}_i'{\\mathbf}{W}_i\\right)^{-1}$. This estimator, denoted CR3, closely approximates the jackknife re-sampling variance estimator \\citep{Bell2002bias, Mancl2001covariance}.  \nHowever, CR3 tends to over-correct the bias of CR0, while the CR1 estimator tends to under-correct. \nThe next section describes in detail the BRL approach, which makes adjustments that are intermediate in magnitude between CR1 and CR3. \n\n\n\\section{BIAS REDUCED LINEARIZATION}\n\\label{sec:BRL}\n\nIn contrast to the CR1 or CR3 estimators, the BRL correction for CRVE is premised on a ``working'' model for the structure of the errors, which must be specified by the analyst. \nUnder a given working model, adjustment matrices ${\\mathbf}{A}_i$ are defined so that the variance estimator is exactly unbiased.\nWe refer to this correction as CR2 because it is an extension of the HC2 variance estimator for regressions with uncorrelated errors, which is exactly unbiased when the errors are homoskedastic \\citep{MacKinnon1985some}.\nThe idea of specifying a model may seem antithetical to the purpose of using CRVE, yet extensive simulation studies have demonstrated that the method performs better in small samples than any of the other adjustments, even when the working model is incorrect. (See Section \\ref{sec:simulation} for a review of this literature.) \nAlthough the CR2 estimator may no longer be exactly unbiased when the working model is misspecified, its bias still tends to be greatly reduced compared to CR1 or CR0 (thus the name ``bias reduced linearization''). Furthermore, as the number of clusters increases, reliance on the working model diminishes. \nIn a sense, CR2 provides necessary scaffolding in the small-sample case, which falls away when there is sufficient data.\n\nLet ${\\boldsymbol}\\Phi_i$ denote a working model for the covariance of the errors in cluster $i$ (up to a scalar constant), with ${\\boldsymbol}\\Phi = \\text{diag}\\left({\\boldsymbol}\\Phi_1,...,{\\boldsymbol}\\Phi_m\\right)$. \nFor example, following \\citet{Bell2002bias} we might assume ${\\boldsymbol}\\Phi_i = {\\mathbf}{I}_i$, i.e., that the errors are uncorrelated and homoskedastic. \nAlternatively, \\citet{Imbens2015robust} suggested using a basic random effects (i.e., compound symmetric) structure, in which ${\\boldsymbol}\\Phi_i$ has unit diagonal entries and off-diagonal entries of $\\rho$, with $\\rho$ estimated using the OLS residuals \\citep[see][p. 16]{Imbens2015robust}.\n\nBased on a given working model, in the original formulation of \\citet{Bell2002bias}, the BRL adjustment matrices are chosen to satisfy the criterion\n\n", "index": 13, "text": "\\begin{equation}\n\\label{eq:CR2_criterion_BM}\n{\\mathbf}{A}_i \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i' {\\mathbf}{A}_i'  =  {\\boldsymbol}\\Phi_i \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{A}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}{%&#10;\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}^{\\prime}{%&#10;\\mathbf{}}{A}_{i}^{\\prime}={\\boldsymbol{}}\\Phi_{i}\" display=\"block\"><mrow><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>A</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>=</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nA variance estimator that uses such adjustment matrices will be exactly unbiased when the working model is correctly specified.\n\nThe above criterion (\\ref{eq:CR2_criterion}) does not uniquely define ${\\mathbf}{A}_i$. One solution, which produces symmetric adjustment matrices, uses\n\n", "itemtype": "equation", "pos": 27460, "prevtext": "\nwhere $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$ denotes the rows of ${\\mathbf}{I} - {\\mathbf}{H_X}$ corresponding to cluster $i$.\nIf the working model and weight matrices are both taken to be identity matrices, then the adjustment matrices simplify to ${\\mathbf}{A}_i = \\left({\\mathbf}{I}_i - {\\mathbf}{X}_i{\\mathbf}{M_X} {\\mathbf}{X}_i'\\right)^{-1/2}$, where ${\\mathbf}{Z}^{-1/2}$ denotes the symmetric square-root of the matrix ${\\mathbf}{Z}$. \nThis formulation of ${\\mathbf}{A}_i$ is problematic because, for some fixed effects models that are common in economic applications, Equation \\ref{eq:CR2_criterion_BM} does not have a solution. \nIn the next two subsections, we address two problems that arise in models with fixed effects, thereby articulating a BRL methodology that is suitable for a wide range of applications.\n\n\\subsection{Generalized Inverse}\n\nThe equality defining the ${\\mathbf}{A}_i$ matrices cannot always be solved because it is possible that some of the matrices involved are not of full rank, and thus cannot be inverted. \n\\citet{Angrist2009mostly} note that this problem arises in balanced state-by-year panel models that include fixed effects for states and for years. \nIn order to address this concern, we provide an alternative criterion for the adjustment matrices that can always be satisfied. \nInstead of criterion (\\ref{eq:CR2_criterion_BM}), we seek adjustment matrices ${\\mathbf}{A}_i$ that satisfy:\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eq:CR2_criterion}\n{\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{A}_i \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i' {\\mathbf}{A}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i = {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\boldsymbol}\\Phi_i {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{A}_{i}\\left({%&#10;\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}%&#10;}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}^{\\prime}{\\mathbf{}}{A}_{i}^{\\prime}{\\mathbf%&#10;{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}={\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf%&#10;{}}{W}_{i}{\\boldsymbol{}}\\Phi_{i}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>A</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow><mo>=</mo><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{D}_i$ is the upper-right triangular Cholesky factorization of ${\\boldsymbol}\\Phi_i$, \n\n", "itemtype": "equation", "pos": 28112, "prevtext": "\nA variance estimator that uses such adjustment matrices will be exactly unbiased when the working model is correctly specified.\n\nThe above criterion (\\ref{eq:CR2_criterion}) does not uniquely define ${\\mathbf}{A}_i$. One solution, which produces symmetric adjustment matrices, uses\n\n", "index": 17, "text": "\\begin{equation}\n\\label{eq:CR2_adjustment}\n{\\mathbf}{A}_i = {\\mathbf}{D}_i' {\\mathbf}{B}_i^{+1/2} {\\mathbf}{D}_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{A}_{i}={\\mathbf{}}{D}_{i}^{\\prime}{\\mathbf{}}{B}_{i}^{+1/2}{%&#10;\\mathbf{}}{D}_{i},\" display=\"block\"><mrow><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mrow><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>B</mi><mi>i</mi><mrow><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><msub><mi>D</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nand ${\\mathbf}{B}_i^{+1/2}$ is the symmetric square root of the Moore-Penrose inverse of ${\\mathbf}{B}_i $. \nThe Moore-Penrose inverse is well-defined and unique even when ${\\mathbf}{B}_i$ is not of full rank \\citep[][Thm. 9.18]{Banerjee2014linear}. These adjustment matrices satisfy criterion (\\ref{eq:CR2_criterion}), as stated in the following theorem.\n\n\\begin{thm}\n\\label{thm:BRL_FE}\nLet ${\\mathbf}{L}_i = \\left({\\mathbf}{\\ddot{U}}'{\\mathbf}{\\ddot{U}} - {\\mathbf}{\\ddot{U}}_i'{\\mathbf}{\\ddot{U}}_i\\right)$, where ${\\mathbf}{\\ddot{U}} = \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{U}$, and assume that ${\\mathbf}{L}_1,...,{\\mathbf}{L}_m$ have full rank $r + s$. Further assume that ${\\text{Var}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i,{\\mathbf}{T}_i\\right.\\right) = \\sigma^2 {\\boldsymbol}\\Phi_i$, for $i = 1,...,m$. Then the adjustment matrix ${\\mathbf}{A}_i$ defined in (\\ref{eq:CR2_adjustment}) and (\\ref{eq:CR2_Bmatrix}) satisfies criterion (\\ref{eq:CR2_criterion}) and the CR2 variance estimator is exactly unbiased.\n\\end{thm}\n\nProof is given in Appendix \\ref{app:theorems}. If ${\\mathbf}{B}_i$ is of full rank, then the adjustment matrices also satisfy the original criterion (\\ref{eq:CR2_criterion_BM}). The main implication of Theorem \\ref{thm:BRL_FE} is that the CR2 variance estimator remains well-defined, even in models with large sets of fixed effects.\n\n\\subsection{Absorption and Dummy Equivalence}\n\nA problem with existing small-sample adjustments to CRVEs is that they can result in a different estimator depending upon if the fixed effects are estimated by OLS or are first absorbed. \nFor example, this problem arises with the CR1S estimator because it uses a multiplicative correction to the residuals that depends on the total number of covariates estimated in the model. \nWhen fixed effects are included as indicators, the constant is calculated as $c_S = \\sqrt{(mN) / [(m - 1)(N - p)]}$, where $p$ is the total number of covariates, including fixed effects. \nIn contrast, if the fixed effects are absorbed, the constant is calculated as $c_S = \\sqrt{(mN) / [(m - 1)(N - r)]}$, where $r$ is the number of covariates that are not absorbed. \n\\citet{Cameron2015practitioners} highlight that this discrepancy can be substantial if the clusters are small. For instance, if each cluster includes $n_i = 2$ units, then the CR1S estimator based on estimating the fixed effects by OLS is over twice as large as the estimator based on the absorbed model.\nSuch differences between the correction based on OLS estimation of the fixed effects and the correction based on the absorbed model are problematic because the magnitude of the variance estimator should not depend on how the model estimates are calculated. \n\nSimilar inconsistencies can arise when applying the BRL method in models with fixed effects. \nConsider the scenario in which absorption is used to estimate ${\\boldsymbol}\\beta$; here, the analyst might choose to calculate the CR2 correction based on the absorbed covariate matrix ${\\mathbf}{\\ddot{R}}$---that is, by substituting ${\\mathbf}{H_{\\ddot{R}}}$ for ${\\mathbf}{H_X}$ in (\\ref{eq:CR2_Bmatrix})---in order to avoid calculating the full projection matrix ${\\mathbf}{H_X}$. \nHowever, this approach can lead to differences in the adjustment matrices compared to when the full model is estimated by OLS because it is based on a subtly different working model. \nEssentially, calculating CR2 based on the absorbed model amounts to assuming that the working model ${\\boldsymbol}\\Phi$ applies not to the model errors ${\\boldsymbol}\\epsilon$, but rather to the errors from the regression of ${\\mathbf}{\\ddot{y}}$ on ${\\mathbf}{\\ddot{R}}$.\nWe find this method of specifying the working model to be incoherent, and therefore recommend against taking it.\nRather, the CR2 adjustment matrices should be calculated based on a working model for the errors in the full regression model, following Equations (\\ref{eq:CR2_adjustment}) and (\\ref{eq:CR2_Bmatrix}) as stated. \n\nA drawback of using the CR2 adjustment matrices based on the full regression model is that it entails calculating the projection matrix ${\\mathbf}{H_X}$ for the full set of $p$ covariates (i.e., including fixed effect indicators). Given that the entire advantage of using absorption to calculate $\\hat{{\\boldsymbol}\\beta}$ is to avoid computations involving large, sparse matrices, it is of interest to find methods for more efficiently calculating the CR2 adjustment matrices. Some efficiency can be gained by using the fact that the residual projection matrix ${\\mathbf}{I} - {\\mathbf}{H_X}$ can be factored into components as $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{R}}}\\right)_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right) \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right)$.\n\nIn certain cases, further computational efficiency can be achieved by computing the adjustment matrices after absorbing the within-cluster fixed effects ${\\mathbf}{T}$ (but not the between-cluster fixed effects ${\\mathbf}{S}$). \nSpecifically, if the weights used for WLS estimation are the inverses of the working covariance model, so that ${\\mathbf}{W}_i = {\\boldsymbol}\\Phi_i^{-1}$ for $i = 1,...,m$, then the adjustment matrices can be calculated without accounting for the within-cluster fixed effects. \nThis result is formalized in the following theorem.  \n\n\\begin{thm}\n\\label{thm:absorb}\nLet ${\\mathbf}{\\tilde{A}}_i = {\\mathbf}{D}_i'{\\mathbf}{\\tilde{B}}_i^{+1/2} {\\mathbf}{D}_i$, where \n\n", "itemtype": "equation", "pos": 28344, "prevtext": "\nwhere ${\\mathbf}{D}_i$ is the upper-right triangular Cholesky factorization of ${\\boldsymbol}\\Phi_i$, \n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq:CR2_Bmatrix}\n{\\mathbf}{B}_i = {\\mathbf}{D}_i\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)' {\\mathbf}{D}_i',\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{B}_{i}={\\mathbf{}}{D}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}%&#10;\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)^{%&#10;\\prime}{\\mathbf{}}{D}_{i}^{\\prime},\" display=\"block\"><mrow><mrow><msub><mi>B</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIf ${\\mathbf}{T}_i {\\mathbf}{T}_k' = {\\mathbf}{0}$ for $j \\neq k$ and ${\\mathbf}{W} = {\\boldsymbol}\\Phi^{-1}$, then ${\\mathbf}{A}_i = {\\mathbf}{\\tilde{A}}_i$. \n\\end{thm}\n\nProof is given in Appendix A.\nThe main implication of Theorem \\ref{thm:absorb} is that the more computationally convenient formula ${\\mathbf}{\\tilde{B}}_i$ can be applied in the common case that the weighting matrices are the inverse of the working covariance model.\n\nFollowing the working model suggested by \\citet{Bell2002bias}, in which ${\\boldsymbol}\\Phi = {\\mathbf}{I}$, the above theorem shows that the adjustment method is invariant to the choice of method for dealing with fixed effects so long as the model is estimated by OLS (i.e., ${\\mathbf}{W} = {\\mathbf}{I}$).\nIn this case, the CR2 adjustment matrices then simplify further to\n", "itemtype": "equation", "pos": 34098, "prevtext": "\nand ${\\mathbf}{B}_i^{+1/2}$ is the symmetric square root of the Moore-Penrose inverse of ${\\mathbf}{B}_i $. \nThe Moore-Penrose inverse is well-defined and unique even when ${\\mathbf}{B}_i$ is not of full rank \\citep[][Thm. 9.18]{Banerjee2014linear}. These adjustment matrices satisfy criterion (\\ref{eq:CR2_criterion}), as stated in the following theorem.\n\n\\begin{thm}\n\\label{thm:BRL_FE}\nLet ${\\mathbf}{L}_i = \\left({\\mathbf}{\\ddot{U}}'{\\mathbf}{\\ddot{U}} - {\\mathbf}{\\ddot{U}}_i'{\\mathbf}{\\ddot{U}}_i\\right)$, where ${\\mathbf}{\\ddot{U}} = \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right){\\mathbf}{U}$, and assume that ${\\mathbf}{L}_1,...,{\\mathbf}{L}_m$ have full rank $r + s$. Further assume that ${\\text{Var}}\\left({\\boldsymbol}\\epsilon_i\\left|{\\mathbf}{R}_i,{\\mathbf}{S}_i,{\\mathbf}{T}_i\\right.\\right) = \\sigma^2 {\\boldsymbol}\\Phi_i$, for $i = 1,...,m$. Then the adjustment matrix ${\\mathbf}{A}_i$ defined in (\\ref{eq:CR2_adjustment}) and (\\ref{eq:CR2_Bmatrix}) satisfies criterion (\\ref{eq:CR2_criterion}) and the CR2 variance estimator is exactly unbiased.\n\\end{thm}\n\nProof is given in Appendix \\ref{app:theorems}. If ${\\mathbf}{B}_i$ is of full rank, then the adjustment matrices also satisfy the original criterion (\\ref{eq:CR2_criterion_BM}). The main implication of Theorem \\ref{thm:BRL_FE} is that the CR2 variance estimator remains well-defined, even in models with large sets of fixed effects.\n\n\\subsection{Absorption and Dummy Equivalence}\n\nA problem with existing small-sample adjustments to CRVEs is that they can result in a different estimator depending upon if the fixed effects are estimated by OLS or are first absorbed. \nFor example, this problem arises with the CR1S estimator because it uses a multiplicative correction to the residuals that depends on the total number of covariates estimated in the model. \nWhen fixed effects are included as indicators, the constant is calculated as $c_S = \\sqrt{(mN) / [(m - 1)(N - p)]}$, where $p$ is the total number of covariates, including fixed effects. \nIn contrast, if the fixed effects are absorbed, the constant is calculated as $c_S = \\sqrt{(mN) / [(m - 1)(N - r)]}$, where $r$ is the number of covariates that are not absorbed. \n\\citet{Cameron2015practitioners} highlight that this discrepancy can be substantial if the clusters are small. For instance, if each cluster includes $n_i = 2$ units, then the CR1S estimator based on estimating the fixed effects by OLS is over twice as large as the estimator based on the absorbed model.\nSuch differences between the correction based on OLS estimation of the fixed effects and the correction based on the absorbed model are problematic because the magnitude of the variance estimator should not depend on how the model estimates are calculated. \n\nSimilar inconsistencies can arise when applying the BRL method in models with fixed effects. \nConsider the scenario in which absorption is used to estimate ${\\boldsymbol}\\beta$; here, the analyst might choose to calculate the CR2 correction based on the absorbed covariate matrix ${\\mathbf}{\\ddot{R}}$---that is, by substituting ${\\mathbf}{H_{\\ddot{R}}}$ for ${\\mathbf}{H_X}$ in (\\ref{eq:CR2_Bmatrix})---in order to avoid calculating the full projection matrix ${\\mathbf}{H_X}$. \nHowever, this approach can lead to differences in the adjustment matrices compared to when the full model is estimated by OLS because it is based on a subtly different working model. \nEssentially, calculating CR2 based on the absorbed model amounts to assuming that the working model ${\\boldsymbol}\\Phi$ applies not to the model errors ${\\boldsymbol}\\epsilon$, but rather to the errors from the regression of ${\\mathbf}{\\ddot{y}}$ on ${\\mathbf}{\\ddot{R}}$.\nWe find this method of specifying the working model to be incoherent, and therefore recommend against taking it.\nRather, the CR2 adjustment matrices should be calculated based on a working model for the errors in the full regression model, following Equations (\\ref{eq:CR2_adjustment}) and (\\ref{eq:CR2_Bmatrix}) as stated. \n\nA drawback of using the CR2 adjustment matrices based on the full regression model is that it entails calculating the projection matrix ${\\mathbf}{H_X}$ for the full set of $p$ covariates (i.e., including fixed effect indicators). Given that the entire advantage of using absorption to calculate $\\hat{{\\boldsymbol}\\beta}$ is to avoid computations involving large, sparse matrices, it is of interest to find methods for more efficiently calculating the CR2 adjustment matrices. Some efficiency can be gained by using the fact that the residual projection matrix ${\\mathbf}{I} - {\\mathbf}{H_X}$ can be factored into components as $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{R}}}\\right)_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right) \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right)$.\n\nIn certain cases, further computational efficiency can be achieved by computing the adjustment matrices after absorbing the within-cluster fixed effects ${\\mathbf}{T}$ (but not the between-cluster fixed effects ${\\mathbf}{S}$). \nSpecifically, if the weights used for WLS estimation are the inverses of the working covariance model, so that ${\\mathbf}{W}_i = {\\boldsymbol}\\Phi_i^{-1}$ for $i = 1,...,m$, then the adjustment matrices can be calculated without accounting for the within-cluster fixed effects. \nThis result is formalized in the following theorem.  \n\n\\begin{thm}\n\\label{thm:absorb}\nLet ${\\mathbf}{\\tilde{A}}_i = {\\mathbf}{D}_i'{\\mathbf}{\\tilde{B}}_i^{+1/2} {\\mathbf}{D}_i$, where \n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:CR2_B_tilde}\n{\\mathbf}{\\tilde{B}}_i = {\\mathbf}{D}_i\\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{R}}}\\right)_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right) {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{S}}}\\right)' \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{R}}}\\right)_i' {\\mathbf}{D}_i'.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{\\tilde{B}}_{i}={\\mathbf{}}{D}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{%&#10;H_{\\ddot{R}}}\\right)_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{S}}}\\right){%&#10;\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{S}}}\\right)^{%&#10;\\prime}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{R}}}\\right)_{i}^{\\prime}{%&#10;\\mathbf{}}{D}_{i}^{\\prime}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>B</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>S</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>S</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIn contrast, if the working model proposed by \\citet{Imbens2015robust} is instead used, then the above theorem implies that the CR2 adjustments will differ if the model is estimated by OLS with dummies for fixed effects versus by using absorption.\n\nThe two theorems of this section extend the BRL methodology described by \\citet{Bell2002bias}, demonstrating how the CR2 adjustment can be computed efficiently---and from a coherent working model---for a broad range of commonly used regression models, including those with within- and between-cluster fixed effects.\nThe next section addresses a final set of concerns: how to conduct single- and multiple-parameter hypothesis tests using the CR2 estimator. \n\n\\section{HYPOTHESIS TESTING}\n\\label{sec:testing}\n\nThe CR2 correction produces a CRVE that has reduced bias (compared to other CRVEs) when the number of clusters is small, leading to more accurate standard errors. However, standard errors are of limited inherent interest---rather, their main use is for the construction of hypothesis tests and confidence intervals.\nCluster-robust Wald-type test statistics are a function of the parameter estimates ${\\boldsymbol}{\\hat\\beta}$ and the corresponding CRVE.\nConventional Wald tests are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., as $m \\to \\infty$). \n\nLike the research on the bias of the CRVE estimator, evidence from a wide variety of contexts indicates that the asymptotic limiting distribution of these statistics may be a poor approximation when the number of clusters is small, even if corrections such as CR2 or CR3 are employed \\citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. \nLike the bias of the CRVE estimator itself, the accuracy of the asymptotic approximations depends on design features such as the degree of imbalance across clusters, skewness or leverage of the covariates, and the similarity of cluster sizes \\citep{McCaffrey2001generalizations, Tipton2015small-F, Webb2013wild, Carter2013asymptotic}. \nThis provides motivation for development of general-purpose hypothesis testing procedures that have accurate rejection rates in small samples.\n\nIn this section, we develop a general method for conducting hypothesis tests based on CRVEs. We consider linear constraints on ${\\boldsymbol}\\beta$, where the null hypothesis has the form $H_0: {\\mathbf}{C}{\\boldsymbol}\\beta = {\\mathbf}{d}$ for fixed $q \\times r$ matrix ${\\mathbf}{C}$ and $q \\times 1$ vector ${\\mathbf}{d}$. \nThe cluster-robust Wald statistic is then\n\n", "itemtype": "equation", "pos": 35254, "prevtext": "\nIf ${\\mathbf}{T}_i {\\mathbf}{T}_k' = {\\mathbf}{0}$ for $j \\neq k$ and ${\\mathbf}{W} = {\\boldsymbol}\\Phi^{-1}$, then ${\\mathbf}{A}_i = {\\mathbf}{\\tilde{A}}_i$. \n\\end{thm}\n\nProof is given in Appendix A.\nThe main implication of Theorem \\ref{thm:absorb} is that the more computationally convenient formula ${\\mathbf}{\\tilde{B}}_i$ can be applied in the common case that the weighting matrices are the inverse of the working covariance model.\n\nFollowing the working model suggested by \\citet{Bell2002bias}, in which ${\\boldsymbol}\\Phi = {\\mathbf}{I}$, the above theorem shows that the adjustment method is invariant to the choice of method for dealing with fixed effects so long as the model is estimated by OLS (i.e., ${\\mathbf}{W} = {\\mathbf}{I}$).\nIn this case, the CR2 adjustment matrices then simplify further to\n", "index": 23, "text": "\\[\n{\\mathbf}{A}_i = \\left({\\mathbf}{I}_i - {\\mathbf}{\\ddot{U}}_i\\left({\\mathbf}{\\ddot{U}}'{\\mathbf}{\\ddot{U}}\\right)^{-1}{\\mathbf}{\\ddot{U}}_i'\\right)^{+1/2}. \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{A}_{i}=\\left({\\mathbf{}}{I}_{i}-{\\mathbf{}}{\\ddot{U}}_{i}\\left({%&#10;\\mathbf{}}{\\ddot{U}}^{\\prime}{\\mathbf{}}{\\ddot{U}}\\right)^{-1}{\\mathbf{}}{%&#10;\\ddot{U}}_{i}^{\\prime}\\right)^{+1/2}.\" display=\"block\"><mrow><mrow><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><msub><mi>I</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mrow><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{V}^{CR}$ is one of the cluster-robust estimators described in previous sections. \nThe asymptotic Wald test rejects $H_0$ if $Q$ exceeds the $\\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom. \nIn large samples, it can be shown that this test has level $\\alpha$. \nHowever, in practice it is rarely clear how large a sample is needed for the asymptotic approximation to be accurate. \n\n\\subsection{Small-sample corrections for t-tests}\n\\label{subsec:t-tests}\n\nConsider testing the hypothesis $H_0: {\\mathbf}{c}'{\\boldsymbol}\\beta = 0$ for a fixed $r \\times 1$ contrast vector ${\\mathbf}{c}$. \nFor this one-dimensional constraint, an equivalent to the Wald statistic given in (\\ref{eq:Wald_stat}) is to use the test statistic $Z = {\\mathbf}{c}'{\\boldsymbol}{\\hat\\beta} / \\sqrt{{\\mathbf}{c}'{\\mathbf}{V}^{CR}{\\mathbf}{c}}$, which follows a standard normal distribution in large samples. \nIn small samples, it is common to use the CR1 or CR1S estimator and to approximate the distribution of $Z$ by a $t(m - 1)$ distribution. \n\\citet{Hansen2007asymptotic} provided one justification for the use of this reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \\citep[see also][]{Donald2007inference}. \n\\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values are conservative (leading to rejection rates less than or equal to $\\alpha$).\nHowever, both of these arguments require that ${\\mathbf}{c}'{\\boldsymbol}\\beta$ be separately identified within each cluster. \nOutside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \\citep{Cameron2015practitioners}. \nFurthermore, using these critical values does not take into account that the distribution of ${\\mathbf}{V}^{CR}$ is affected by the structure of the covariate matrix. \n\nAn alternative t-test developed by \\citet{Bell2002bias} involves using a $t(\\nu)$ references distribution, with degrees of freedom $\\nu$ estimated by a Satterthwaite approximation.\nThe Satterthwaite approximation \\citep{Satterthwaite1946approximate} entails using degrees of freedom that are a function of the the first two moments of the sampling distribution of ${\\mathbf}{c}' {\\mathbf}{V}^{CR} {\\mathbf}{c}$.\nTheoretically, these degrees of freedom should be \n\n", "itemtype": "equation", "pos": 37986, "prevtext": "\nIn contrast, if the working model proposed by \\citet{Imbens2015robust} is instead used, then the above theorem implies that the CR2 adjustments will differ if the model is estimated by OLS with dummies for fixed effects versus by using absorption.\n\nThe two theorems of this section extend the BRL methodology described by \\citet{Bell2002bias}, demonstrating how the CR2 adjustment can be computed efficiently---and from a coherent working model---for a broad range of commonly used regression models, including those with within- and between-cluster fixed effects.\nThe next section addresses a final set of concerns: how to conduct single- and multiple-parameter hypothesis tests using the CR2 estimator. \n\n\\section{HYPOTHESIS TESTING}\n\\label{sec:testing}\n\nThe CR2 correction produces a CRVE that has reduced bias (compared to other CRVEs) when the number of clusters is small, leading to more accurate standard errors. However, standard errors are of limited inherent interest---rather, their main use is for the construction of hypothesis tests and confidence intervals.\nCluster-robust Wald-type test statistics are a function of the parameter estimates ${\\boldsymbol}{\\hat\\beta}$ and the corresponding CRVE.\nConventional Wald tests are justified based on the asymptotic behavior of robust Wald statistics as the number of clusters grows large (i.e., as $m \\to \\infty$). \n\nLike the research on the bias of the CRVE estimator, evidence from a wide variety of contexts indicates that the asymptotic limiting distribution of these statistics may be a poor approximation when the number of clusters is small, even if corrections such as CR2 or CR3 are employed \\citep{Bell2002bias, Bertrand2004how, Cameron2008bootstrap}. \nLike the bias of the CRVE estimator itself, the accuracy of the asymptotic approximations depends on design features such as the degree of imbalance across clusters, skewness or leverage of the covariates, and the similarity of cluster sizes \\citep{McCaffrey2001generalizations, Tipton2015small-F, Webb2013wild, Carter2013asymptotic}. \nThis provides motivation for development of general-purpose hypothesis testing procedures that have accurate rejection rates in small samples.\n\nIn this section, we develop a general method for conducting hypothesis tests based on CRVEs. We consider linear constraints on ${\\boldsymbol}\\beta$, where the null hypothesis has the form $H_0: {\\mathbf}{C}{\\boldsymbol}\\beta = {\\mathbf}{d}$ for fixed $q \\times r$ matrix ${\\mathbf}{C}$ and $q \\times 1$ vector ${\\mathbf}{d}$. \nThe cluster-robust Wald statistic is then\n\n", "index": 25, "text": "\\begin{equation}\n\\label{eq:Wald_stat}\nQ = \\left({\\mathbf}{C}{\\boldsymbol}{\\hat\\beta} - {\\mathbf}{d}\\right)'\\left({\\mathbf}{C} {\\mathbf}{V}^{CR} {\\mathbf}{C}'\\right)^{-1}\\left({\\mathbf}{C}{\\boldsymbol}{\\hat\\beta} - {\\mathbf}{d}\\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"Q=\\left({\\mathbf{}}{C}{\\boldsymbol{}}{\\hat{\\beta}}-{\\mathbf{}}{d}\\right)^{%&#10;\\prime}\\left({\\mathbf{}}{C}{\\mathbf{}}{V}^{CR}{\\mathbf{}}{C}^{\\prime}\\right)^{%&#10;-1}\\left({\\mathbf{}}{C}{\\boldsymbol{}}{\\hat{\\beta}}-{\\mathbf{}}{d}\\right),\" display=\"block\"><mrow><mrow><mi>Q</mi><mo>=</mo><mrow><msup><mrow><mo>(</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>-</mo><mi>d</mi></mrow><mo>)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi>C</mi><mo>\u2062</mo><msup><mi>V</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi></mrow></msup><mo>\u2062</mo><msup><mi>C</mi><mo>\u2032</mo></msup></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>-</mo><mi>d</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nExpressions for the first two moments of ${\\mathbf}{c}'{\\mathbf}{V}^{CR2}{\\mathbf}{c}$ can be derived under the assumption that the errors ${\\boldsymbol}\\epsilon_1,...,{\\boldsymbol}\\epsilon_m$ are normally distributed. \n\nIn practice, both moments involve the variance structure ${\\boldsymbol}\\Sigma$, which is unknown. \n\\citet{Bell2002bias} proposed to estimate the moments based on the same working model that is used to derive the adjustment matrices. \nThis ``model-assisted'' estimate of the degrees of freedom is then calculated as \n\n", "itemtype": "equation", "pos": 40667, "prevtext": "\nwhere ${\\mathbf}{V}^{CR}$ is one of the cluster-robust estimators described in previous sections. \nThe asymptotic Wald test rejects $H_0$ if $Q$ exceeds the $\\alpha$ critical value from a chi-squared distribution with $q$ degrees of freedom. \nIn large samples, it can be shown that this test has level $\\alpha$. \nHowever, in practice it is rarely clear how large a sample is needed for the asymptotic approximation to be accurate. \n\n\\subsection{Small-sample corrections for t-tests}\n\\label{subsec:t-tests}\n\nConsider testing the hypothesis $H_0: {\\mathbf}{c}'{\\boldsymbol}\\beta = 0$ for a fixed $r \\times 1$ contrast vector ${\\mathbf}{c}$. \nFor this one-dimensional constraint, an equivalent to the Wald statistic given in (\\ref{eq:Wald_stat}) is to use the test statistic $Z = {\\mathbf}{c}'{\\boldsymbol}{\\hat\\beta} / \\sqrt{{\\mathbf}{c}'{\\mathbf}{V}^{CR}{\\mathbf}{c}}$, which follows a standard normal distribution in large samples. \nIn small samples, it is common to use the CR1 or CR1S estimator and to approximate the distribution of $Z$ by a $t(m - 1)$ distribution. \n\\citet{Hansen2007asymptotic} provided one justification for the use of this reference distribution by identifying conditions under which $Z$ converges in distribution to $t(m-1)$ as the within-cluster sample sizes grow large, with $m$ fixed \\citep[see also][]{Donald2007inference}. \n\\citet{Ibragimov2010tstatistic} proposed a weighting technique derived so that that $t(m-1)$ critical values are conservative (leading to rejection rates less than or equal to $\\alpha$).\nHowever, both of these arguments require that ${\\mathbf}{c}'{\\boldsymbol}\\beta$ be separately identified within each cluster. \nOutside of these circumstances, using $t(m-1)$ critical values can still lead to over-rejection \\citep{Cameron2015practitioners}. \nFurthermore, using these critical values does not take into account that the distribution of ${\\mathbf}{V}^{CR}$ is affected by the structure of the covariate matrix. \n\nAn alternative t-test developed by \\citet{Bell2002bias} involves using a $t(\\nu)$ references distribution, with degrees of freedom $\\nu$ estimated by a Satterthwaite approximation.\nThe Satterthwaite approximation \\citep{Satterthwaite1946approximate} entails using degrees of freedom that are a function of the the first two moments of the sampling distribution of ${\\mathbf}{c}' {\\mathbf}{V}^{CR} {\\mathbf}{c}$.\nTheoretically, these degrees of freedom should be \n\n", "index": 27, "text": "\\begin{equation}\n\\label{eq:nu_Satterthwaite}\n\\nu = \\frac{2\\left[{\\text{E}}\\left({\\mathbf}{c}'{\\mathbf}{V}^{CR2}{\\mathbf}{c}\\right)\\right]^2}{{\\text{Var}}\\left({\\mathbf}{c}'{\\mathbf}{V}^{CR2}{\\mathbf}{c}\\right)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\nu=\\frac{2\\left[{\\text{E}}\\left({\\mathbf{}}{c}^{\\prime}{\\mathbf{}}{V}^{CR2}{%&#10;\\mathbf{}}{c}\\right)\\right]^{2}}{{\\text{Var}}\\left({\\mathbf{}}{c}^{\\prime}{%&#10;\\mathbf{}}{V}^{CR2}{\\mathbf{}}{c}\\right)}.\" display=\"block\"><mrow><mrow><mi>\u03bd</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo>\u2062</mo><msup><mrow><mo>[</mo><mrow><mtext>E</mtext><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mi>c</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>V</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><mi>c</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mn>2</mn></msup></mrow><mrow><mtext>Var</mtext><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mi>c</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>V</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><mi>c</mi></mrow><mo>)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{p}_i = \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i'{\\mathbf}{A}_i {\\mathbf}{W}_i{\\mathbf}{\\ddot{R}}_i{\\mathbf}{M_{\\ddot{R}}} {\\mathbf}{c}$.\n\\todo{Can we use ${\\mathbf}{H_{\\ddot{U}}}$ here instead?} \nAlternately, for any of the CRVEs one could instead use an ``empirical'' estimate of the degrees of freedom, constructed by substituting ${\\mathbf}{e}_i {\\mathbf}{e}_i'$ in place of ${\\boldsymbol}\\Sigma_i$. \nHowever, \\citet{Bell2002bias} found using simulation that this plug-in degrees of freedom estimate led to very conservative rejection rates. \n\nThe \\citet{Bell2002bias} Satterthwaite approximation has been shown to perform well in a variety of conditions (see Section 4). \nThese studies encompass a variety of data generation processes, covariate types, and weighting procedures. \nA key finding is that the degrees of freedom depend not only on the number of clusters $m$, but also on features of the covariates. \nWhen the covariate is balanced across clusters---as occurs in balanced panels with a dichotomous covariate with the same proportion of ones in each cluster---the degrees of freedom are $m - 1$ even in small samples. \nHowever, when the covariate is highly unbalanced---as occurs when the panel is not balanced or if the proportion of ones varies from cluster to cluster---the degrees of freedom can be considerably smaller. \nSimilarly, covariates with large leverage points will tend to exhibit lower of degrees of freedom. \n\nBy adjusting the degrees of freedom to account for these features, the Type I error rate of the test is nearly always less than or equal to the nominal $\\alpha$, so long as the degrees of freedom are larger than 4 or 5 \\citep{Bell2002bias, Tipton2015small-t}.\nThis is because when the degrees of freedom are smaller, the t-distribution approximation to the sampling distribution does not hold, and the Type I error can be higher than the stated $\\alpha$ level.\\footnote{When the degrees of freedom are smaller than 4 or 5, \\citet{Tipton2015small-t} suggested using a smaller ${\\boldsymbol}\\alpha$ level for hypothesis testing in order to partially compensate. \nAlthough degrees of freedom this small may seem unlikely, they can easily arise in practice even when the number of clusters is moderate. For example, in a state-by-year panel with $m = 48$ states, the degrees of freedom can be quite small if a policy is only implemented in 10 percent of states.}\nIn comparison, the CR1 degrees of freedom (i.e., $m - 1$) are constant, and the test only performs well when in the cases in which the covariates are balanced.\nBecause the degrees of freedom are covariate-dependent, it is not possible to assess whether a small-sample correction is needed based solely on the total number of clusters in the data. \nConsequently, \\citet{Tipton2015small-t} argued that t-tests based on CRVE should routinely use the CR2 variance estimator and the Satterthwaite degrees of freedom, even when $m$ appears to be large.\n\n\n\\subsection{Small-sample corrections for F-tests}\n\\label{subsec:F-tests}\n\nLittle research has considered small-sample corrections for multiple-constraint hypothesis tests based on cluster-robust Wald statistics.\nCameron and Miller highlight this problem, noting that some form of adjustment is clearly needed in light of the extensive work on single-parameter tests.\nWe now describe an approach to multi-parameter testing that closely parallels the Satterthwaite correction for t-tests.\n\nOur approach is to approximate the sampling distribution of $Q$ by Hotelling's $T^2$ distribution (a multiple of an F distribution) with estimated degrees of freedom. To motivate the approximation, let ${\\mathbf}{G} = {\\mathbf}{C} {\\mathbf}{M_{\\ddot{R}}}{\\mathbf}{\\ddot{R}}'{\\mathbf}{W}{\\boldsymbol}\\Phi{\\mathbf}{W}{\\mathbf}{\\ddot{R}}{\\mathbf}{M_{\\ddot{R}}} {\\mathbf}{C}'$ denote the variance of ${\\mathbf}{C}{\\boldsymbol}{\\hat\\beta}$ under the working model and observe that $Q$ can be written as\n", "itemtype": "equation", "pos": 41431, "prevtext": "\nExpressions for the first two moments of ${\\mathbf}{c}'{\\mathbf}{V}^{CR2}{\\mathbf}{c}$ can be derived under the assumption that the errors ${\\boldsymbol}\\epsilon_1,...,{\\boldsymbol}\\epsilon_m$ are normally distributed. \n\nIn practice, both moments involve the variance structure ${\\boldsymbol}\\Sigma$, which is unknown. \n\\citet{Bell2002bias} proposed to estimate the moments based on the same working model that is used to derive the adjustment matrices. \nThis ``model-assisted'' estimate of the degrees of freedom is then calculated as \n\n", "index": 29, "text": "\\begin{equation}\n\\label{eq:nu_model}\n\\nu_{M} = \\frac{\\left(\\sum_{i=1}^m {\\mathbf}{p}_i' {\\boldsymbol}\\Phi {\\mathbf}{p}_i\\right)^2}{\\sum_{i=1}^m \\sum_{j=1}^m \\left({\\mathbf}{p}_i' {\\boldsymbol}\\Phi {\\mathbf}{p}_j\\right)^2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\nu_{M}=\\frac{\\left(\\sum_{i=1}^{m}{\\mathbf{}}{p}_{i}^{\\prime}{\\boldsymbol{}}%&#10;\\Phi{\\mathbf{}}{p}_{i}\\right)^{2}}{\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\left({\\mathbf{%&#10;}}{p}_{i}^{\\prime}{\\boldsymbol{}}\\Phi{\\mathbf{}}{p}_{j}\\right)^{2}},\" display=\"block\"><mrow><mrow><msub><mi>\u03bd</mi><mi>M</mi></msub><mo>=</mo><mfrac><msup><mrow><mo>(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><msubsup><mi>p</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msup><mrow><mo>(</mo><mrow><msubsup><mi>p</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{z} = {\\mathbf}{G}^{-1/2}\\left({\\mathbf}{C}{\\boldsymbol}{\\hat\\beta} - {\\mathbf}{d}\\right)$ and ${\\boldsymbol}\\Omega = {\\mathbf}{G}^{-1/2} {\\mathbf}{C} {\\mathbf}{V}^{CR2}{\\mathbf}{C}'{\\mathbf}{G}^{-1/2}$. \nNow suppose that $\\eta \\times {\\boldsymbol}\\Omega$ follows a Wishart distribution with $\\eta$ degrees of freedom and a $q$-dimensional identity scale matrix. It then follows that\n\n", "itemtype": "equation", "pos": 45626, "prevtext": "\nwhere ${\\mathbf}{p}_i = \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i'{\\mathbf}{A}_i {\\mathbf}{W}_i{\\mathbf}{\\ddot{R}}_i{\\mathbf}{M_{\\ddot{R}}} {\\mathbf}{c}$.\n\\todo{Can we use ${\\mathbf}{H_{\\ddot{U}}}$ here instead?} \nAlternately, for any of the CRVEs one could instead use an ``empirical'' estimate of the degrees of freedom, constructed by substituting ${\\mathbf}{e}_i {\\mathbf}{e}_i'$ in place of ${\\boldsymbol}\\Sigma_i$. \nHowever, \\citet{Bell2002bias} found using simulation that this plug-in degrees of freedom estimate led to very conservative rejection rates. \n\nThe \\citet{Bell2002bias} Satterthwaite approximation has been shown to perform well in a variety of conditions (see Section 4). \nThese studies encompass a variety of data generation processes, covariate types, and weighting procedures. \nA key finding is that the degrees of freedom depend not only on the number of clusters $m$, but also on features of the covariates. \nWhen the covariate is balanced across clusters---as occurs in balanced panels with a dichotomous covariate with the same proportion of ones in each cluster---the degrees of freedom are $m - 1$ even in small samples. \nHowever, when the covariate is highly unbalanced---as occurs when the panel is not balanced or if the proportion of ones varies from cluster to cluster---the degrees of freedom can be considerably smaller. \nSimilarly, covariates with large leverage points will tend to exhibit lower of degrees of freedom. \n\nBy adjusting the degrees of freedom to account for these features, the Type I error rate of the test is nearly always less than or equal to the nominal $\\alpha$, so long as the degrees of freedom are larger than 4 or 5 \\citep{Bell2002bias, Tipton2015small-t}.\nThis is because when the degrees of freedom are smaller, the t-distribution approximation to the sampling distribution does not hold, and the Type I error can be higher than the stated $\\alpha$ level.\\footnote{When the degrees of freedom are smaller than 4 or 5, \\citet{Tipton2015small-t} suggested using a smaller ${\\boldsymbol}\\alpha$ level for hypothesis testing in order to partially compensate. \nAlthough degrees of freedom this small may seem unlikely, they can easily arise in practice even when the number of clusters is moderate. For example, in a state-by-year panel with $m = 48$ states, the degrees of freedom can be quite small if a policy is only implemented in 10 percent of states.}\nIn comparison, the CR1 degrees of freedom (i.e., $m - 1$) are constant, and the test only performs well when in the cases in which the covariates are balanced.\nBecause the degrees of freedom are covariate-dependent, it is not possible to assess whether a small-sample correction is needed based solely on the total number of clusters in the data. \nConsequently, \\citet{Tipton2015small-t} argued that t-tests based on CRVE should routinely use the CR2 variance estimator and the Satterthwaite degrees of freedom, even when $m$ appears to be large.\n\n\n\\subsection{Small-sample corrections for F-tests}\n\\label{subsec:F-tests}\n\nLittle research has considered small-sample corrections for multiple-constraint hypothesis tests based on cluster-robust Wald statistics.\nCameron and Miller highlight this problem, noting that some form of adjustment is clearly needed in light of the extensive work on single-parameter tests.\nWe now describe an approach to multi-parameter testing that closely parallels the Satterthwaite correction for t-tests.\n\nOur approach is to approximate the sampling distribution of $Q$ by Hotelling's $T^2$ distribution (a multiple of an F distribution) with estimated degrees of freedom. To motivate the approximation, let ${\\mathbf}{G} = {\\mathbf}{C} {\\mathbf}{M_{\\ddot{R}}}{\\mathbf}{\\ddot{R}}'{\\mathbf}{W}{\\boldsymbol}\\Phi{\\mathbf}{W}{\\mathbf}{\\ddot{R}}{\\mathbf}{M_{\\ddot{R}}} {\\mathbf}{C}'$ denote the variance of ${\\mathbf}{C}{\\boldsymbol}{\\hat\\beta}$ under the working model and observe that $Q$ can be written as\n", "index": 31, "text": "\\[\nQ = {\\mathbf}{z}' {\\boldsymbol}\\Omega^{-1} {\\mathbf}{z},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"Q={\\mathbf{}}{z}^{\\prime}{\\boldsymbol{}}\\Omega^{-1}{\\mathbf{}}{z},\" display=\"block\"><mrow><mrow><mi>Q</mi><mo>=</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a9</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>z</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nWe will refer to this as the approximate Hotelling's $T^2$ (AHT) test.\nWe consider how to estimate $\\eta$ below.\nThis approximation is conceptually similar to the Satterthwaite approximation for one-dimensional constraints, and in fact reduces to the Satterthwaite approximation when $q = 1$. \nFor $q > 1$, the test depends on the multivariate distribution of ${\\mathbf}{V}^{CR2}$, including both variance and covariance terms. \n\n\\citet{Tipton2015small-F} recently introduced this test for application in the special case of CRVE for meta-regression models.\nWishart approximations have been considered as approximations in several simpler models where special cases of CRVE are used.\n\\citet{Nel1986solution} proposed an AHT-type test for testing equality of multivariate means across two samples with unequal variance-covariance matrices \\citep[i.e., the multivariate Behrens-Fisher problem; see also][]{Krishnamoorthy2004modified}.\n\\citet{Zhang2012twowayANOVA} followed a similar approach in developing a test for contrasts in analysis of variance models with unequal within-cell variance, which are particularly simple cases of linear models with heteroskedastic error terms. \n\\citet{Zhang2012MANOVA} extended the method to multivariate analysis of variance models where the covariance of the errors differs across groups, a special case of model (\\ref{eq:fixed_effects}) where the CR2 variance estimator has a particularly simple form. \nIn each of these special cases, the robust variance estimator is a mixture of Wishart distributions that is well-approximated by a Wishart distribution with estimated degrees of freedom.\nAdditionally, \\citet{Pan2002small} described an F-test for use in GEE models, which uses the Wishart approximation to the distribution of ${\\mathbf}{V}^{CR0}$ but estimates the degrees of freedom using a different method than the one we describe below.\n\nThe contribution of the present paper is to extend the AHT test to the general setting of linear models with fixed effects and clustered errors. \nThe remaining question is how to estimate the parameter $\\eta$, which determines scalar multiplier and denominator degrees of freedom of the AHT test. \nTo do so, we match the mean and variance of ${\\boldsymbol}\\Omega$ to that of the approximating Wishart distribution under the working variance model ${\\boldsymbol}\\Phi$, just as in the degrees of freedom for the t-test. \nThe problem that arises in doing so is that it is not possible to exactly match both moments if $q > 1$.\nFollowing \\citet{Tipton2015small-F}, we instead match the mean and total variance of ${\\boldsymbol}\\Omega$---i.e., the sum of the variances of its entries.\n\nLet ${\\mathbf}{g}_1,...,{\\mathbf}{g}_q$ denote the $q \\times 1$ column vectors of ${\\mathbf}{G}^{-1/2}$. \nLet\n", "itemtype": "equation", "pos": 46089, "prevtext": "\nwhere ${\\mathbf}{z} = {\\mathbf}{G}^{-1/2}\\left({\\mathbf}{C}{\\boldsymbol}{\\hat\\beta} - {\\mathbf}{d}\\right)$ and ${\\boldsymbol}\\Omega = {\\mathbf}{G}^{-1/2} {\\mathbf}{C} {\\mathbf}{V}^{CR2}{\\mathbf}{C}'{\\mathbf}{G}^{-1/2}$. \nNow suppose that $\\eta \\times {\\boldsymbol}\\Omega$ follows a Wishart distribution with $\\eta$ degrees of freedom and a $q$-dimensional identity scale matrix. It then follows that\n\n", "index": 33, "text": "\\begin{equation}\n\\label{eq:AHT}\n\\left(\\frac{\\eta - q + 1}{\\eta q}\\right) Q \\ \\dot\\sim \\ F(q, \\eta - q + 1).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\left(\\frac{\\eta-q+1}{\\eta q}\\right)Q\\ \\dot{\\sim}\\ F(q,\\eta-q+1).\" display=\"block\"><mrow><mrow><mrow><mo>(</mo><mfrac><mrow><mrow><mi>\u03b7</mi><mo>-</mo><mi>q</mi></mrow><mo>+</mo><mn>1</mn></mrow><mrow><mi>\u03b7</mi><mo>\u2062</mo><mi>q</mi></mrow></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>Q</mi></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><mover accent=\"true\"><mo>\u223c</mo><mo>\u02d9</mo></mover></mpadded><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mrow><mrow><mi>\u03b7</mi><mo>-</mo><mi>q</mi></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nfor $s = 1,...,q$ and $i = 1,...,m$.\\todo{Possible to simplify $p_{si}$ by ignoring within-cluster fixed effects?} \nThe degrees of freedom are then estimated under the working model as\n\n", "itemtype": "equation", "pos": 48984, "prevtext": "\nWe will refer to this as the approximate Hotelling's $T^2$ (AHT) test.\nWe consider how to estimate $\\eta$ below.\nThis approximation is conceptually similar to the Satterthwaite approximation for one-dimensional constraints, and in fact reduces to the Satterthwaite approximation when $q = 1$. \nFor $q > 1$, the test depends on the multivariate distribution of ${\\mathbf}{V}^{CR2}$, including both variance and covariance terms. \n\n\\citet{Tipton2015small-F} recently introduced this test for application in the special case of CRVE for meta-regression models.\nWishart approximations have been considered as approximations in several simpler models where special cases of CRVE are used.\n\\citet{Nel1986solution} proposed an AHT-type test for testing equality of multivariate means across two samples with unequal variance-covariance matrices \\citep[i.e., the multivariate Behrens-Fisher problem; see also][]{Krishnamoorthy2004modified}.\n\\citet{Zhang2012twowayANOVA} followed a similar approach in developing a test for contrasts in analysis of variance models with unequal within-cell variance, which are particularly simple cases of linear models with heteroskedastic error terms. \n\\citet{Zhang2012MANOVA} extended the method to multivariate analysis of variance models where the covariance of the errors differs across groups, a special case of model (\\ref{eq:fixed_effects}) where the CR2 variance estimator has a particularly simple form. \nIn each of these special cases, the robust variance estimator is a mixture of Wishart distributions that is well-approximated by a Wishart distribution with estimated degrees of freedom.\nAdditionally, \\citet{Pan2002small} described an F-test for use in GEE models, which uses the Wishart approximation to the distribution of ${\\mathbf}{V}^{CR0}$ but estimates the degrees of freedom using a different method than the one we describe below.\n\nThe contribution of the present paper is to extend the AHT test to the general setting of linear models with fixed effects and clustered errors. \nThe remaining question is how to estimate the parameter $\\eta$, which determines scalar multiplier and denominator degrees of freedom of the AHT test. \nTo do so, we match the mean and variance of ${\\boldsymbol}\\Omega$ to that of the approximating Wishart distribution under the working variance model ${\\boldsymbol}\\Phi$, just as in the degrees of freedom for the t-test. \nThe problem that arises in doing so is that it is not possible to exactly match both moments if $q > 1$.\nFollowing \\citet{Tipton2015small-F}, we instead match the mean and total variance of ${\\boldsymbol}\\Omega$---i.e., the sum of the variances of its entries.\n\nLet ${\\mathbf}{g}_1,...,{\\mathbf}{g}_q$ denote the $q \\times 1$ column vectors of ${\\mathbf}{G}^{-1/2}$. \nLet\n", "index": 35, "text": "\\[\n{\\mathbf}{p}_{si} = \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i' {\\mathbf}{A}_i {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i {\\mathbf}{M_{\\ddot{R}}}{\\mathbf}{C} {\\mathbf}{g}_s \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{p}_{si}=\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}^{\\prime%&#10;}{\\mathbf{}}{A}_{i}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}{\\mathbf{}}{M_{%&#10;\\ddot{R}}}{\\mathbf{}}{C}{\\mathbf{}}{g}_{s}\" display=\"block\"><mrow><msub><mi>p</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><msub><mi>g</mi><mi>s</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIf $q = 1$, then $\\eta_M$ reduces to $\\nu_M$ from Equation (\\ref{eq:nu_model}).\n\nThis AHT F-test shares several features with the t-test developed by Bell and McCaffrey. As with the t-test, the degrees of freedom of this F-test depend not only on the number of clusters, but also on features of the covariates being tested. \nThe degrees of freedom can be much lower than $m - 1$, particularly when the covariates being tested exhibit high leverage or are unbalanced across clusters. \nFor example, if the goal is to test if there are differences across a three-arm, block-randomized experiment with clustering by block, the degrees of freedom will be largest (approaching $m - 1$) when the treatment is allocated equally across the three groups within each block. \nWhen the proportion varies across clusters, the degrees of freedom are reduced, potentially into ``small sample'' territory even when the number of clusters is large. \n\nA primary difference between the AHT test and the standard test is in the degrees of freedom. We expect that using the AHT degrees of freedom, which take into account features of the covariate distribution, will improve the accuracy of the rejection rates in small samples. \nWe have also claimed that the choice of working model used in the CR2 correction does not have a strong influence on performance. \nIn the next section, we provide evidence for these claims through a careful review of prior simulation study results and through the results of a new simulation study based upon the conditions commonly found in economic applications.\n\n\\section{Simulation evidence}\n\\label{sec:simulation}\n\nEvidence from several large simulation studies indicates that hypothesis tests based on the CR2 adjustment and estimated degrees of freedom substantially out-perform the procedures that are most commonly used in empirical applications.\nHowever, existing simulations have focused almost entirely on single-parameter tests. \nIn this section, we first review findings from previous simulations, with particular emphasis on the role of covariate features and sample size on the Type I error rates of these tests.\nWe then describe the design and results of a new simulation study, which focused on the rejection rates of multiple-parameter tests.\n  Throughout, we refer to tests employing the CR2-corrected CRVE and estimated degrees of freedom as the ``AHT'' test; for t-tests, the estimated degrees of freedom are equivalent to the Satterthwaite approximation given in Equation (\\ref{eq:nu_model}). \nWe refer to tests employing the CR1 correction and $m - 1$ degrees of freedom as the ``standard'' test. \n\n\\subsection{Review of previous simulation studies}\n\nTo date, four simulation studies have examined the performance of the CR2 t-test, across a total of nearly 100 parameter combinations and a range of application contexts.\n\\citet{Cameron2015practitioners} and \\citet{Imbens2015robust} focused on conditions common in economics, while \\citet{Bell2002bias} focused on those common in complex surveys and \\citet{Tipton2015small-t} on those in meta-analysis. \nTable \\ref{tab:simulation_summary} summarizes the results of these studies.\nSome of the studies focused on policy dummies in the balanced case, while others varied the degree of balance; still others examined continuous covariates that are symmetrically distributed, as well as those with high skew and leverage.\nThese studies also examined the role of the number of clusters, with values ranging from 6 to 50, as well as the number of observations per clusters (from 1 to roughly 260).\nFinally, the studies used a range of both true error structures (including various combinations of heteroskedasticity and clustering) and estimation strategies (including different 'working' models), including scenarios in which the working model differed from the true error structure.\nFinally, while most previous studies focused on OLS estimation, one study \\citep{Tipton2015small-t} examined the performance of t-tests based on WLS estimation.\n\n\\begin{sidewaystable}\n\\small\n\\caption{Type I error rates of t-tests based on CRVE}\n\\label{tab:simulation_summary}\n\\include{simulation_table}\n\\caption*{Table refers to the table within the relevant article. $m$ is the number of clusters; $n$ is the number of observations within each cluster; c indicates cluster-level covariate, while o indicates observation-level covariate; \\% = percent taking value of one; M = symmetric continuous; K = skewed continuous; H = heteroskedastic; RE = random effects (Moulton factor); C = correlated errors; LN = log-normal errors; (\\#) indicates number of different models tested.}\n\\end{sidewaystable}\n\nTable \\ref{tab:simulation_summary} also indicates the range of Type I error rates observed across the conditions studied in each of the simulation studies, with values given for both the standard and AHT tests.\nAcross studies, the Type I error for the standard t-test ranges from .01 to .34 for a stated $\\alpha$ level of .05.\nThese values are particularly far above nominal when the covariate tested is unbalanced, skewed (i.e., high leverage), or when the number of observations per cluster varies. \nAlthough not reported in the table, high Type I error rates occur not only when the number of clusters is very small, but also at moderate sizes when the covariate is unbalanced or skewed. \n\nIn comparison, the AHT t-test performs considerably better across the range of conditions studied, with Type I error rates ranging between 0.01 and 0.13.\nNotably, the largest value observed here is from \\citet{Imbens2015robust}, who do not break results out by degrees of freedom. \nGiven the condition studied (30 clusters, with only 3 having a policy dummy), it is quite possible that the degrees of freedom are below the cut-off of 4 or 5 at which others have shown the t-test approximation can fail \\citep{Tipton2015small-t}. \nPutting this value aside, the maximum Type I error observed in these conditions is 0.06, only slightly higher than nominal.\nCrucially, these nearly nominal Type I error rates hold even when the working model is far from the true error structure, and for various types of covariates.\nThis is because the AHT test takes into account covariate features in the degrees of freedom, which can be far less than $m - 1$.\n\nIn comparison to the t-test, the AHT F-test has only been studied in a single simulation focused on the meta-analytic case \\citep{Tipton2015small-F}.\nAlthough this study focused only on the use of CRVE with WLS estimation, it was comprehensive in other regards.\nIn particular, it examined the effects of the number of covariates in the model (up to $p = 5$) and the number of constraints tested ($q = 2,3,4,5$), including cases in which $p = q$ and in which $q < p$. \nThe simulations also examined models with various combinations of covariate types, including both balanced and unbalanced indicator variables, as well as symmetric or skewed continuous covariates.\nLike \\citet{Tipton2015small-t}, these simulations focused on true correlation structures that included heteroskedasticity, clustering (i.e., a cluster specific random effect), and correlated errors.\nThe working models were then chosen to be far from the true error structure (i.e., an independent-errors working model).\nFinally, the number of clusters was varied from 10 to 100, each with between 1 and 10 observations. \nType I error rates of the standard test and the AHT F-test were compared for nominal $\\alpha$ levels of .01, .05, and .10.\n\nThe results of the simulations by \\citet{Tipton2015small-F} indicate that the AHT F-test always has Type I error less than or equal to the stated $\\alpha$ level, except in cases with extreme model misspecification. \nHowever, even under such conditions, the Type I error was in line with rates observed for t-tests; for example, for $\\alpha = 0.05$ the error was not above 0.06. \nIn comparison, the Type I error of the standard test was often very high, with maximum rejection rates ranging from .17 to .22, depending on the dimension of the constraint being tested.\nLike the t-test, the degrees of freedom of the AHT F-test were driven by covariate features, with particularly low degrees of freedom resulting from covariates that are unbalanced or skewed.\n\nWhile the simulation study by \\citet{Tipton2015small-F} included a variety of conditions, its design was focused on the types of data found in meta-analytic applications. \nThese differ from the economic context in two ways.\nFirst, in meta-analysis, it is common to have heteroskedasticity of a known form and for analysts to incorporate weights in the analysis (typically inverse-variance weights).\nIn comparison, unweighted, OLS estimation is more common in economic applications.\nSecond, meta-analytic regressions often involve testing a variety of types of covariates, including continuous regressors.\nIn comparison, many economic applications are focused on testing binary indicator variables that represent differences between policy regimes. \nTests for policy effects can involve cluster-level comparisons (e.g., comparisons across states) or observation-level comparisons (e.g., pre/post comparisons within each state), or a combination of both observation-level and cluster-level comparisons (as in difference-in-differences analysis).\nIn light of these differences, we conducted a new study to evaluate the performance of the standard and AHT tests under conditions that more closely resemble economic applications. \n\n\\subsection{Simulation Design}\n\nThe simulation study focused on testing hypotheses about the relative effects of three policy conditions, while varying the manner in which the policy indicators are assigned following one of three distinct designs. \nFirst, we considered a randomized block (RB) design in which every policy condition is observed in every cluster. \nSecond, we considered a cluster-randomized (CR) design in which each cluster is observed under a single policy condition. \nThird, we considered a difference-in-differences (DD) design in which some clusters are observed under all three policy conditions while other clusters are observed under a single condition. \nFor each design, we simulated both balanced and unbalanced configurations, for a total of six distinct study designs, across which the performance of CRVEs is expected to vary. \nAppendix \\ref{app:simulations} describes the exact specification of each design. For each design, we simulated studies with $m = 15$, 30, or 50 clusters, each with $n = 18$ or 30 units.\n\nFor a given study design, we simulated multivariate outcome data so that we could examine the performance of the proposed testing procedures for constraints of varying dimension. \nSpecifically, we simulated a tri-variate, equi-correlated outcome from a data-generating process in which all three policy conditions produce identical average outcomes, so that all tested null hypotheses hold. \nLet $y_{hijk}$ denote the measurement of outcome $k$ at time point $j$ for unit $i$ under condition $h$, for $h = 1,...,3$, $i = 1,...,m$, $j = 1,...,n$, and $k = 1,...,3$. \nThe data-generating model is then \n\n", "itemtype": "equation", "pos": 49344, "prevtext": "\nfor $s = 1,...,q$ and $i = 1,...,m$.\\todo{Possible to simplify $p_{si}$ by ignoring within-cluster fixed effects?} \nThe degrees of freedom are then estimated under the working model as\n\n", "index": 37, "text": "\\begin{equation}\n\\label{eq:eta_model}\n\\eta_M = \\frac{q(q + 1)}{\\sum_{s,t=1}^q \\sum_{i,j=1}^m {\\mathbf}{p}_{si}'{\\boldsymbol}\\Phi{\\mathbf}{p}_{tj} {\\mathbf}{p}_{ti}'{\\boldsymbol}\\Phi{\\mathbf}{p}_{sj} + {\\mathbf}{p}_{si}'{\\boldsymbol}\\Phi{\\mathbf}{p}_{sj} {\\mathbf}{p}_{ti}'{\\boldsymbol}\\Phi{\\mathbf}{p}_{tj}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\eta_{M}=\\frac{q(q+1)}{\\sum_{s,t=1}^{q}\\sum_{i,j=1}^{m}{\\mathbf{}}{p}_{si}^{%&#10;\\prime}{\\boldsymbol{}}\\Phi{\\mathbf{}}{p}_{tj}{\\mathbf{}}{p}_{ti}^{\\prime}{%&#10;\\boldsymbol{}}\\Phi{\\mathbf{}}{p}_{sj}+{\\mathbf{}}{p}_{si}^{\\prime}{\\boldsymbol%&#10;{}}\\Phi{\\mathbf{}}{p}_{sj}{\\mathbf{}}{p}_{ti}^{\\prime}{\\boldsymbol{}}\\Phi{%&#10;\\mathbf{}}{p}_{tj}}.\" display=\"block\"><mrow><mrow><msub><mi>\u03b7</mi><mi>M</mi></msub><mo>=</mo><mfrac><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>q</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><msubsup><mi>p</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>p</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>+</mo><mrow><msubsup><mi>p</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>p</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msub><mi>p</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere $\\nu_{hi}$ is a random effect for unit $i$ under condition $h$ and $\\epsilon_{ijk}$ is the idiosyncratic error for unit $i$ at time point $j$ on outcome $k$. \nThe random effects for unit $i$ are taken to have variance ${\\text{Var}}\\left(\\nu_{hi}\\right) = \\tau^2$. \nWe further assumed that the random effects are correlated, which has the effect of inducing variability in the cluster-specific treatment effects and thus a degree of misspecification into the analytic models described below. Letting $\\sigma_\\delta^2$ denote the degree of treatment effect variability relative to the total variability in a given outcome measurement, we simulated the random effects $\\nu_{1i},\\nu_{2i},\\nu_{3i}$ to satisfy $\\text{Var}\\left(\\nu_{gi} - \\nu_{hi}\\right) = \\sigma_\\delta^2$ for $g \\neq h, g,h = 1,2,3$. \nThe errors at a given time point are assumed to be correlated, with ${\\text{Var}}\\left(\\epsilon_{ijk}\\right) = 1 - \\tau^2$ and $\\text{corr}\\left(\\epsilon_{ijk}, \\epsilon_{ijl}\\right) = \\rho$ for $k\\neq l, k,l = 1,2,3$. \n\nUnder this data-generating process, we simulated data based on parameter values of $\\tau^2 = .05$, .15, or .25 for the intra-class correlation; outcomes that were either weakly ($\\rho = .2$) or strongly correlated ($\\rho = .8$); and values of $\\sigma_\\delta^2 = .00$, .01, or .04 for treatment effect variability.\nEach combination of sample sizes and parameter levels was simulated under each of the six study designs, yielding a total of 648 simulation conditions.\n\nGiven a set of simulated data, we estimated the effects of the second and third policy conditions (relative to the first) on each outcome, using a seemingly unrelated regression framework. \nThe general analytic model for the difference-in-differences design was\n\n", "itemtype": "equation", "pos": 60805, "prevtext": "\nIf $q = 1$, then $\\eta_M$ reduces to $\\nu_M$ from Equation (\\ref{eq:nu_model}).\n\nThis AHT F-test shares several features with the t-test developed by Bell and McCaffrey. As with the t-test, the degrees of freedom of this F-test depend not only on the number of clusters, but also on features of the covariates being tested. \nThe degrees of freedom can be much lower than $m - 1$, particularly when the covariates being tested exhibit high leverage or are unbalanced across clusters. \nFor example, if the goal is to test if there are differences across a three-arm, block-randomized experiment with clustering by block, the degrees of freedom will be largest (approaching $m - 1$) when the treatment is allocated equally across the three groups within each block. \nWhen the proportion varies across clusters, the degrees of freedom are reduced, potentially into ``small sample'' territory even when the number of clusters is large. \n\nA primary difference between the AHT test and the standard test is in the degrees of freedom. We expect that using the AHT degrees of freedom, which take into account features of the covariate distribution, will improve the accuracy of the rejection rates in small samples. \nWe have also claimed that the choice of working model used in the CR2 correction does not have a strong influence on performance. \nIn the next section, we provide evidence for these claims through a careful review of prior simulation study results and through the results of a new simulation study based upon the conditions commonly found in economic applications.\n\n\\section{Simulation evidence}\n\\label{sec:simulation}\n\nEvidence from several large simulation studies indicates that hypothesis tests based on the CR2 adjustment and estimated degrees of freedom substantially out-perform the procedures that are most commonly used in empirical applications.\nHowever, existing simulations have focused almost entirely on single-parameter tests. \nIn this section, we first review findings from previous simulations, with particular emphasis on the role of covariate features and sample size on the Type I error rates of these tests.\nWe then describe the design and results of a new simulation study, which focused on the rejection rates of multiple-parameter tests.\n  Throughout, we refer to tests employing the CR2-corrected CRVE and estimated degrees of freedom as the ``AHT'' test; for t-tests, the estimated degrees of freedom are equivalent to the Satterthwaite approximation given in Equation (\\ref{eq:nu_model}). \nWe refer to tests employing the CR1 correction and $m - 1$ degrees of freedom as the ``standard'' test. \n\n\\subsection{Review of previous simulation studies}\n\nTo date, four simulation studies have examined the performance of the CR2 t-test, across a total of nearly 100 parameter combinations and a range of application contexts.\n\\citet{Cameron2015practitioners} and \\citet{Imbens2015robust} focused on conditions common in economics, while \\citet{Bell2002bias} focused on those common in complex surveys and \\citet{Tipton2015small-t} on those in meta-analysis. \nTable \\ref{tab:simulation_summary} summarizes the results of these studies.\nSome of the studies focused on policy dummies in the balanced case, while others varied the degree of balance; still others examined continuous covariates that are symmetrically distributed, as well as those with high skew and leverage.\nThese studies also examined the role of the number of clusters, with values ranging from 6 to 50, as well as the number of observations per clusters (from 1 to roughly 260).\nFinally, the studies used a range of both true error structures (including various combinations of heteroskedasticity and clustering) and estimation strategies (including different 'working' models), including scenarios in which the working model differed from the true error structure.\nFinally, while most previous studies focused on OLS estimation, one study \\citep{Tipton2015small-t} examined the performance of t-tests based on WLS estimation.\n\n\\begin{sidewaystable}\n\\small\n\\caption{Type I error rates of t-tests based on CRVE}\n\\label{tab:simulation_summary}\n\\include{simulation_table}\n\\caption*{Table refers to the table within the relevant article. $m$ is the number of clusters; $n$ is the number of observations within each cluster; c indicates cluster-level covariate, while o indicates observation-level covariate; \\% = percent taking value of one; M = symmetric continuous; K = skewed continuous; H = heteroskedastic; RE = random effects (Moulton factor); C = correlated errors; LN = log-normal errors; (\\#) indicates number of different models tested.}\n\\end{sidewaystable}\n\nTable \\ref{tab:simulation_summary} also indicates the range of Type I error rates observed across the conditions studied in each of the simulation studies, with values given for both the standard and AHT tests.\nAcross studies, the Type I error for the standard t-test ranges from .01 to .34 for a stated $\\alpha$ level of .05.\nThese values are particularly far above nominal when the covariate tested is unbalanced, skewed (i.e., high leverage), or when the number of observations per cluster varies. \nAlthough not reported in the table, high Type I error rates occur not only when the number of clusters is very small, but also at moderate sizes when the covariate is unbalanced or skewed. \n\nIn comparison, the AHT t-test performs considerably better across the range of conditions studied, with Type I error rates ranging between 0.01 and 0.13.\nNotably, the largest value observed here is from \\citet{Imbens2015robust}, who do not break results out by degrees of freedom. \nGiven the condition studied (30 clusters, with only 3 having a policy dummy), it is quite possible that the degrees of freedom are below the cut-off of 4 or 5 at which others have shown the t-test approximation can fail \\citep{Tipton2015small-t}. \nPutting this value aside, the maximum Type I error observed in these conditions is 0.06, only slightly higher than nominal.\nCrucially, these nearly nominal Type I error rates hold even when the working model is far from the true error structure, and for various types of covariates.\nThis is because the AHT test takes into account covariate features in the degrees of freedom, which can be far less than $m - 1$.\n\nIn comparison to the t-test, the AHT F-test has only been studied in a single simulation focused on the meta-analytic case \\citep{Tipton2015small-F}.\nAlthough this study focused only on the use of CRVE with WLS estimation, it was comprehensive in other regards.\nIn particular, it examined the effects of the number of covariates in the model (up to $p = 5$) and the number of constraints tested ($q = 2,3,4,5$), including cases in which $p = q$ and in which $q < p$. \nThe simulations also examined models with various combinations of covariate types, including both balanced and unbalanced indicator variables, as well as symmetric or skewed continuous covariates.\nLike \\citet{Tipton2015small-t}, these simulations focused on true correlation structures that included heteroskedasticity, clustering (i.e., a cluster specific random effect), and correlated errors.\nThe working models were then chosen to be far from the true error structure (i.e., an independent-errors working model).\nFinally, the number of clusters was varied from 10 to 100, each with between 1 and 10 observations. \nType I error rates of the standard test and the AHT F-test were compared for nominal $\\alpha$ levels of .01, .05, and .10.\n\nThe results of the simulations by \\citet{Tipton2015small-F} indicate that the AHT F-test always has Type I error less than or equal to the stated $\\alpha$ level, except in cases with extreme model misspecification. \nHowever, even under such conditions, the Type I error was in line with rates observed for t-tests; for example, for $\\alpha = 0.05$ the error was not above 0.06. \nIn comparison, the Type I error of the standard test was often very high, with maximum rejection rates ranging from .17 to .22, depending on the dimension of the constraint being tested.\nLike the t-test, the degrees of freedom of the AHT F-test were driven by covariate features, with particularly low degrees of freedom resulting from covariates that are unbalanced or skewed.\n\nWhile the simulation study by \\citet{Tipton2015small-F} included a variety of conditions, its design was focused on the types of data found in meta-analytic applications. \nThese differ from the economic context in two ways.\nFirst, in meta-analysis, it is common to have heteroskedasticity of a known form and for analysts to incorporate weights in the analysis (typically inverse-variance weights).\nIn comparison, unweighted, OLS estimation is more common in economic applications.\nSecond, meta-analytic regressions often involve testing a variety of types of covariates, including continuous regressors.\nIn comparison, many economic applications are focused on testing binary indicator variables that represent differences between policy regimes. \nTests for policy effects can involve cluster-level comparisons (e.g., comparisons across states) or observation-level comparisons (e.g., pre/post comparisons within each state), or a combination of both observation-level and cluster-level comparisons (as in difference-in-differences analysis).\nIn light of these differences, we conducted a new study to evaluate the performance of the standard and AHT tests under conditions that more closely resemble economic applications. \n\n\\subsection{Simulation Design}\n\nThe simulation study focused on testing hypotheses about the relative effects of three policy conditions, while varying the manner in which the policy indicators are assigned following one of three distinct designs. \nFirst, we considered a randomized block (RB) design in which every policy condition is observed in every cluster. \nSecond, we considered a cluster-randomized (CR) design in which each cluster is observed under a single policy condition. \nThird, we considered a difference-in-differences (DD) design in which some clusters are observed under all three policy conditions while other clusters are observed under a single condition. \nFor each design, we simulated both balanced and unbalanced configurations, for a total of six distinct study designs, across which the performance of CRVEs is expected to vary. \nAppendix \\ref{app:simulations} describes the exact specification of each design. For each design, we simulated studies with $m = 15$, 30, or 50 clusters, each with $n = 18$ or 30 units.\n\nFor a given study design, we simulated multivariate outcome data so that we could examine the performance of the proposed testing procedures for constraints of varying dimension. \nSpecifically, we simulated a tri-variate, equi-correlated outcome from a data-generating process in which all three policy conditions produce identical average outcomes, so that all tested null hypotheses hold. \nLet $y_{hijk}$ denote the measurement of outcome $k$ at time point $j$ for unit $i$ under condition $h$, for $h = 1,...,3$, $i = 1,...,m$, $j = 1,...,n$, and $k = 1,...,3$. \nThe data-generating model is then \n\n", "index": 39, "text": "\\begin{equation}\n\\label{eq:data_generating_model}\ny_{hijk} = \\nu_{hi} + \\epsilon_{ijk},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"y_{hijk}=\\nu_{hi}+\\epsilon_{ijk},\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><msub><mi>\u03bd</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere $\\mu_{hk}$ is the mean of outcome $k$ under condition $h$, $\\alpha_i$ is a fixed effect for each cluster, $\\gamma_j$ is a fixed effect for each unit within the cluster (i.e., per time-point), and $\\epsilon_{ijk}$ is residual error. \nFor the cluster-randomized designs, fixed effects for clusters were omitted because the clusters are nested within treatment conditions. For the randomized block designs, the fixed effects for time-points were omitted for simplicity.\nThe model is estimated by OLS after absorbing any fixed effects, and so the ``working'' model amounts to assuming that the residuals are all independent and identically distributed. Note that the working model departs from the true data generating model both because of correlation among the outcomes ($\\rho > 0$) and because of treatment effect variability ($\\sigma_\\delta^2 > 0$). The range of parameter combinations used in the true data generating model thus allow us to examine the performance of the AHT test under both small and large degrees of working model misspecification. \n\nAnalytic model (\\ref{eq:sim_analytic_model}) provided opportunities to test a range of single- and multi-parameter constraints. \nWe first tested the single-dimensional null hypotheses that a given policy condition had no average effect on the first outcome ($H_0: \\mu_{11} = \\mu_{12}$ or $H_0: \\mu_{11} = \\mu_{13}$). \nWe also tested the null hypothesis of no differences among policy conditions on the first outcome ($H_0: \\mu_{11} = \\mu_{12} = \\mu_{13}$), which has dimension $q = 2$. \nWe then tested the multi-variate versions of the above tests, which involve all three outcome measures jointly. Namely, we tested the null hypotheses that a given policy condition had no average effects on any outcome (i.e., $H_0: \\mu_{11} = \\mu_{1h}, \\mu_{21} = \\mu_{2h}, \\mu_{31} = \\mu_{3h}$, for $h = 2$ or $h = 3$) which has dimension $q = 3$, and the null hypothesis of no differences among policy conditions on any outcome ($H_0: \\mu_{11} = \\mu_{12} = \\mu_{13}, \\mu_{21} = \\mu_{22} = \\mu_{23}, \\mu_{31} = \\mu_{32} = \\mu_{33}$), which has dimension $q = 6$. \nFor a given combination of sample sizes, parameter levels, and study design, we simulated 10,000 datasets from model (\\ref{eq:data_generating_model}), estimated model (\\ref{eq:sim_analytic_model}) on each dataset, and tested all of the hypotheses described above. \nSimulated Type I error rates therefore have standard errors of approximately 0.001 for $\\alpha = .01$, 0.0022 for $\\alpha = .05$, and 0.003 for $\\alpha = .10$. \n\n\\subsection{Simulation Results}\n\nOur discussion of the simulation results is focused on four trends, each of which is depicted visually in a figure and described in the text. \nAll of the trends are similar to the findings from Tipton and Pustejovsky (2015), which provides further support that the AHT F-test performs well across a wide range of data generating mechanisms and parameter combinations.\n\n\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{overview-1.pdf} \n\n}\n\n\\caption[Rejection rates of Standard and AHT tests, by dimension of hypothesis (]{Rejection rates of Standard and AHT tests, by dimension of hypothesis ($q$) and nominal type I error ($\\alpha$).}\\label{fig:overview}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nThe first finding is that the AHT test has Type I error close to the stated $\\alpha$ level for all parameter combinations studied, whereas the standard test (based on the CR1 variance estimator and $m - 1$ degrees of freedom) does not. \nFigure \\ref{fig:overview} illustrates this pattern, for constraints of varying dimension (from $q = 1$, in the first column, to $q = 6$, in the final column) and nominal $\\alpha$ level (from .01, in the first row, to .10, in the last row). \nIn each of these figures, the number of clusters varies from 15 to 50 (on the horizontal axis), the solid horizontal line indicates the stated $\\alpha$ level and the dashed line indicates an upper confidence bound on simulation error.\nIt can be seen that the AHT test has Type I error near the stated $\\alpha$ level, even with a small number of clusters.\nWhen the number of clusters is very small, the Type I error can be smaller than the stated $\\alpha$ level. \nAlthough there exist situations in which the error is above the simulation bound, the departures are typically small. For example, when $m = 15$ the rejection rates do not exceed 0.021 for $\\alpha = .01$, 0.073 for $\\alpha = .05$, and 0.134 for $\\alpha = .10$.\nThe rejection rates are even closer to nominal for lower-dimensional constraints.\nIn comparison, the Type I error for the standard test can be markedly higher than the stated $\\alpha$ level, particularly when the number of clusters is small or the dimension of the hypothesis is large. \nFor example, for nominal $\\alpha = .05$, the maximum Type I error ranges from 0.124 ($q = 1$) to 0.686 ($q = 6$) for data sets with 15 clusters.\nPerhaps even more important for practice, even when there are 50 clusters, the rejection rate of the standard test can be far above the stated $\\alpha$ level.\nAgain, focusing on the $\\alpha = 0.05$ case, the maximum error ranges from 0.072 ($q = 1$) to 0.183 ($q = 6$).\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{balance-1.pdf} \n\n}\n\n\\caption[Rejection rates of Standard and AHT tests, by study design and dimension of hypothesis (]{Rejection rates of Standard and AHT tests, by study design and dimension of hypothesis ($q$). CR = cluster-randomized design; DD = difference-in-differences design; RB = randomized block design; B = balanced; U = unbalanced.}\\label{fig:balance}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nIn order to better understand the effects of different parameter combinations on the performance of both tests, Figure \\ref{fig:balance} focuses on the $\\alpha = 0.05$ case and breaks out the results by study design. The top row of the figure depicts the standard test, while the bottom row depicts the AHT test; columns correspond to the number of clusters.\nWithin each graph, results are given by study design (on the horizontal axis), with colors corresponding to the dimension of the test.\nIn the top panel, it can be seen that the rejection rate of the standard test increases with the dimension of the test ($q$) and the degree of unbalance in the study design.\nDifferences between the balanced and unbalanced designs are largest for the CR and DD designs, with smaller discrepancies in RB designs.\nThe bottom row of Figure \\ref{fig:balance} displays results for the AHT test; here we focus on three trends.\nFirst, the rejection rate of the AHT test usually increases as the dimension of the test increases, though never above 0.073.\nSecond, unbalanced designs led to rejection rates that were usually below the nominal $\\alpha$---just the opposite of how the standard test is affected by unbalance. \nThis trend is the strongest for CR and DD designs, where Type I error can be close to 0 at its minimum. \nThird, for studies with at least 30 clusters, rejection rates are very closes to nominal (between 0.032 and 0.057) across all conditions studied.\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{misspecification-1.pdf} \n\n}\n\n\\caption[Rejection rates of AHT test, by treatment effect variance and intra-class correlation]{Rejection rates of AHT test, by treatment effect variance and intra-class correlation.}\\label{fig:misspecification}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nNext, by simulating the errors across a variety of parameter combinations, we were also able to test the impact of misspecification of the working model on Type I error.\nBecause the CR2 correction and AHT degrees of freedom are both based on a working model with independent, homoskedastic errors, model misspecification increases with the true level of treatment effect variance ($\\sigma_\\delta^2$) and intra-class correlation ($\\tau^2$).\nFigure \\ref{fig:misspecification} depicts Type I error rates for $\\alpha = 0.05$ for the AHT test, with separate graphs according to the dimension $q$ of the test (columns) and the number of clusters (rows).\nWithin each panel, results are separated by the 9 combinations of $\\sigma_\\delta^2$ and $\\tau^2$. \nIt can be seen that the range of rejection rates remains very similar across the 9 error structures, with no clear pattern to the small differences that emerge.\nThese results follow closely those from Tipton and Pustejovsky (2015), which also found that even with extreme model misspecification the Type I error of the CR2S test was close to nominal. \n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{figure}\n\n{\\centering \\includegraphics[width=\\linewidth]{df-1.pdf} \n\n}\n\n\\caption[Range of denominator degrees of freedom for AHT test, by number of clusters and study design]{Range of denominator degrees of freedom for AHT test, by number of clusters and study design. CR = cluster-randomized design; DD = difference-in-differences design; RB = randomized block design; B = balanced; U = unbalanced.}\\label{fig:df}\n\\end{figure}\n\n\n\\end{knitrout}\n\nFinally, Figure \\ref{fig:df} depicts the range of estimated degrees of freedom for the AHT test as a function of the simulated study design and number of clusters ($m$). \nWithin each box plot, the degrees of freedom vary depending on the hypothesis tested, with constraints of larger dimension having lower degrees of freedom. \nIt can be seen that the AHT degrees of freedom are often far less than $m - 1$ and that they are strongly affected by the pattern of treatment assignment and the degree of balance. The balanced and unbalanced RB designs generally had AHT degrees of freedom closest to $m - 1$ because the treatment effects being tested are all identified within each cluster. \nThe balanced DD design usually had the next largest degrees of freedom because it involved contrasts between two patterns of treatment configuration, followed by the balanced CR design, which involved contrasts between three patterns of treatment configurations. \nFor both of these designs, unbalance led to sharply reduced degrees of freedom.\n\nThese new simulation results have demonstrated that the standard robust Wald test, using the CR1 correction and $m - 1$ degrees of freedom, produces a wide range of rejection rates, often far in excess of the nominal Type I error. \nIn contrast, the rejection rates of the AHT test are below or at most slightly above nominal, across the conditions that we have examined.\nThis is because the AHT test incorporates information about the covariate features into its estimated degrees of freedom, whereas the standard test does not.\nAn important question that remains then is how much the AHT and standard tests diverge in actual application.\nIn the next section, we compare the two tests in several examples, drawn from a range of recent empirical research.\n\n\\section{EXAMPLES}\n\\label{sec:examples}\n\nThis section presents three short examples that illustrate the performance of CRVE across a variety of applied contexts. \nIn the first example, the effects of substantive interest involve between-cluster contrasts. \nThe second example involves a cluster-robust Hausman test for differences between within- and across-cluster information. \nIn the final example, the effects are identified within each cluster. \nIn each example, we demonstrate the proposed AHT test for single- and multiple-parameter hypotheses and compare the results to the standard test based on the CR1 variance estimator and $m - 1$ degrees of freedom. \nThe focus here is on providing insight into the conditions under which the AHT and standard estimators diverge in terms of three quantities of interest: the standard error estimates, the degrees of freedom estimates, and the stated p-values. \nData files and replication code (in R) are available for each analysis as an online supplement.\n\n\\subsection{Achievement Awards demonstration} \n\n\\citet{Angrist2009effects} reported results from a randomized trial in Israel that aimed to increase completion rates of the Bagrut, the national matriculation certificate for post-secondary education, among low-achieving high school students. \nIn the Achievement Awards demonstration, 40 non-vocational high schools with low rates of Bagrut completion were selected from across Israel, including 10 Arab and 10 Jewish religious schools and 20 Jewish secular schools. \nThe schools were then pair-matched based on 1999 rates of Bagrut completion, and within each pair one school was randomized to receive a cash-transfer program. \nIn these treatment schools, seniors who completed certification were eligible for payments of approximately \\$1,500. \nStudent-level covariate and outcome data were drawn from administrative records for the school years ending in June of 2000, 2001, and 2002. \nThe incentive program was in effect for the group of seniors in treatment schools taking the Bagrut exams in Spring of 2001, but the program was discontinued for the following year. \nWe therefore treat completion rates for 2000 and 2002 as being unaffected by treatment assignment.\nThe primary outcome of interest is Bagrut completion. \n\nThis study provides an opportunity to examine the AHT test in a situation in which the treatment was assigned at the cluster level, with a smaller number of clusters.\nFor simplicity, we restrict our analysis to the sample of female students, which reduces the total sample to 35 schools.\nFollowing the original analysis of \\citet{Angrist2009effects}, we allow the program's effects to vary depending on whether a students was in the upper or lower half of the distribution of prior-year academic performance. \nLetting $h = 1,2,3$ index the sector of each school (Arab religious, Jewish religious, or Jewish secular), we consider the following analytic model: \n\n", "itemtype": "equation", "pos": 62662, "prevtext": "\nwhere $\\nu_{hi}$ is a random effect for unit $i$ under condition $h$ and $\\epsilon_{ijk}$ is the idiosyncratic error for unit $i$ at time point $j$ on outcome $k$. \nThe random effects for unit $i$ are taken to have variance ${\\text{Var}}\\left(\\nu_{hi}\\right) = \\tau^2$. \nWe further assumed that the random effects are correlated, which has the effect of inducing variability in the cluster-specific treatment effects and thus a degree of misspecification into the analytic models described below. Letting $\\sigma_\\delta^2$ denote the degree of treatment effect variability relative to the total variability in a given outcome measurement, we simulated the random effects $\\nu_{1i},\\nu_{2i},\\nu_{3i}$ to satisfy $\\text{Var}\\left(\\nu_{gi} - \\nu_{hi}\\right) = \\sigma_\\delta^2$ for $g \\neq h, g,h = 1,2,3$. \nThe errors at a given time point are assumed to be correlated, with ${\\text{Var}}\\left(\\epsilon_{ijk}\\right) = 1 - \\tau^2$ and $\\text{corr}\\left(\\epsilon_{ijk}, \\epsilon_{ijl}\\right) = \\rho$ for $k\\neq l, k,l = 1,2,3$. \n\nUnder this data-generating process, we simulated data based on parameter values of $\\tau^2 = .05$, .15, or .25 for the intra-class correlation; outcomes that were either weakly ($\\rho = .2$) or strongly correlated ($\\rho = .8$); and values of $\\sigma_\\delta^2 = .00$, .01, or .04 for treatment effect variability.\nEach combination of sample sizes and parameter levels was simulated under each of the six study designs, yielding a total of 648 simulation conditions.\n\nGiven a set of simulated data, we estimated the effects of the second and third policy conditions (relative to the first) on each outcome, using a seemingly unrelated regression framework. \nThe general analytic model for the difference-in-differences design was\n\n", "index": 41, "text": "\\begin{equation}\n\\label{eq:sim_analytic_model}\ny_{hijk} = \\mu_{hk} + \\alpha_i + \\gamma_j + \\epsilon_{ijk},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"y_{hijk}=\\mu_{hk}+\\alpha_{i}+\\gamma_{j}+\\epsilon_{ijk},\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><msub><mi>\u03bc</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>+</mo><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03b3</mi><mi>j</mi></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIn this model for student $j$ in year $t$ in school $i$ in sector $h$, $z_{hit}$ is an indicator equal to one in the treatment schools for the 2001 school year and otherwise equal to zero; ${\\mathbf}{r}_{hitj}$ is a vector of indicators for whether the student is in the lower or upper half of the distribution of prior academic performance; and ${\\boldsymbol}\\beta_h = \\left(\\beta_{1h}, \\beta_{2h}\\right)$ is a vector of average treatment effects for schools in sector $h$. \nThe vector ${\\mathbf}{s}_{hitj}$ includes the following individual student demographic measures: mother's and father's education, immigration status, number of siblings, and indicators for each quartile in the distribution of prior-year academic performance. \nThe model also includes fixed effects $\\gamma_{ht}$ for each sector in each year and $\\mu_{hi}$ for each school. \n\nBased on Model (\\ref{eq:AL_ATE}), we test four hypotheses, again with the goal of exploring the use of the AHT tests under a range of conditions. \nFirst, we assume that the program effects are constant across sector (i.e., ${\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3 = {\\boldsymbol}\\beta$) and test for whether the program affected completion rates for students in the upper half of the prior achievement distribution ($H_0: \\beta_2 = 0$, with $q = 1$).\nSecond, we test for whether the program was effective in either half of the prior academic performance ($H_0: {\\boldsymbol}\\beta = 0$, with $q = 2$), still assuming that program effects are constant across sector. \nThird, we test for whether program effects in the upper half of the prior achievement distribution are moderated by school sector ($H_0: \\beta_{21} = \\beta_{22} = \\beta_{23}$, with $q = 3$). \nFinally, we conduct a joint test for whether program effects in either half of the prior achievement distribution are moderated by school sector ($H_0: {\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3$, with $q = 4$). \n\n\n\n\n\n\\begin{table}[bth]\n\\centering\n\\begin{tabular}{lcrrr}\n  \\toprule\nHypothesis & Test & F & df & p \\\\ \n  \\midrule\nATE - upper half (q = 1) & Standard & 5.746 & 34.00 & 0.02217 \\\\ \n   & AHT & 5.169 & 15.86 & 0.03726 \\\\ \n  ATE - joint (q = 2) & Standard & 3.848 & 34.00 & 0.03116 \\\\ \n   & AHT & 3.371 & 15.46 & 0.06096 \\\\ \n   \\midrule\nModeration - upper half (q = 2) & Standard & 3.186 & 34.00 & 0.05393 \\\\ \n   & AHT & 0.091 & 3.19 & 0.91520 \\\\ \n  Moderation - joint (q = 4) & Standard & 8.213 & 34.00 & 0.00010 \\\\ \n   & AHT & 2.895 & 3.21 & 0.19446 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of treatment effects in the Achievement Awards Demonstration} \n\\label{tab:AAD}\n\\end{table}\n\n\nTable \\ref{tab:AAD} reports the results of all four hypothesis tests. \nThese results indicate three important trends.\nFirst, in the case of the first two hypotheses, the AHT test statistics are only slightly smaller than their standard counterparts, but the degrees of freedom are considerably smaller. \nThese differences in degrees of freedom arise because the treatment was assigned at the cluster level, while the subgroups varied within each cluster. \nSecond, the third and fourth hypotheses tests, which compared treatment effects across sectors and subgroups, are cases in which the AHT and standard tests diverge markedly.\nFor these cases, the AHT test statistic and degrees of freedom are both considerably smaller than those from the standard test. \nThis reflects the degree of unbalance in allocations across sectors (19 Jewish secular, 7 Jewish religious, and 9 Arab religious schools), combined with cluster-level randomization. \nIn combination, these smaller test statistics and degrees of freedom result in larger p-values for the AHT test when compared to the standard test.\n\n\\subsection{Effects of minimum legal drinking age on mortality} \n\nOur second example focues on panel data, using an example described in \\citet[see also \\citealp{Carpenter2011minimum}]{Angrist2014mastering}.\nBased on data from the Fatal Accident Reporting System maintained by the National Highway Traffic Safety Administration, we estimated the effects of changes in the minimum legal drinking age over the time period of 1970-1983 on state-level death rates resulting from motor vehicle crashes.\nA standard difference-in-differences specification for such a state-by-year panel is\n\n", "itemtype": "equation", "pos": 76872, "prevtext": "\nwhere $\\mu_{hk}$ is the mean of outcome $k$ under condition $h$, $\\alpha_i$ is a fixed effect for each cluster, $\\gamma_j$ is a fixed effect for each unit within the cluster (i.e., per time-point), and $\\epsilon_{ijk}$ is residual error. \nFor the cluster-randomized designs, fixed effects for clusters were omitted because the clusters are nested within treatment conditions. For the randomized block designs, the fixed effects for time-points were omitted for simplicity.\nThe model is estimated by OLS after absorbing any fixed effects, and so the ``working'' model amounts to assuming that the residuals are all independent and identically distributed. Note that the working model departs from the true data generating model both because of correlation among the outcomes ($\\rho > 0$) and because of treatment effect variability ($\\sigma_\\delta^2 > 0$). The range of parameter combinations used in the true data generating model thus allow us to examine the performance of the AHT test under both small and large degrees of working model misspecification. \n\nAnalytic model (\\ref{eq:sim_analytic_model}) provided opportunities to test a range of single- and multi-parameter constraints. \nWe first tested the single-dimensional null hypotheses that a given policy condition had no average effect on the first outcome ($H_0: \\mu_{11} = \\mu_{12}$ or $H_0: \\mu_{11} = \\mu_{13}$). \nWe also tested the null hypothesis of no differences among policy conditions on the first outcome ($H_0: \\mu_{11} = \\mu_{12} = \\mu_{13}$), which has dimension $q = 2$. \nWe then tested the multi-variate versions of the above tests, which involve all three outcome measures jointly. Namely, we tested the null hypotheses that a given policy condition had no average effects on any outcome (i.e., $H_0: \\mu_{11} = \\mu_{1h}, \\mu_{21} = \\mu_{2h}, \\mu_{31} = \\mu_{3h}$, for $h = 2$ or $h = 3$) which has dimension $q = 3$, and the null hypothesis of no differences among policy conditions on any outcome ($H_0: \\mu_{11} = \\mu_{12} = \\mu_{13}, \\mu_{21} = \\mu_{22} = \\mu_{23}, \\mu_{31} = \\mu_{32} = \\mu_{33}$), which has dimension $q = 6$. \nFor a given combination of sample sizes, parameter levels, and study design, we simulated 10,000 datasets from model (\\ref{eq:data_generating_model}), estimated model (\\ref{eq:sim_analytic_model}) on each dataset, and tested all of the hypotheses described above. \nSimulated Type I error rates therefore have standard errors of approximately 0.001 for $\\alpha = .01$, 0.0022 for $\\alpha = .05$, and 0.003 for $\\alpha = .10$. \n\n\\subsection{Simulation Results}\n\nOur discussion of the simulation results is focused on four trends, each of which is depicted visually in a figure and described in the text. \nAll of the trends are similar to the findings from Tipton and Pustejovsky (2015), which provides further support that the AHT F-test performs well across a wide range of data generating mechanisms and parameter combinations.\n\n\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{overview-1.pdf} \n\n}\n\n\\caption[Rejection rates of Standard and AHT tests, by dimension of hypothesis (]{Rejection rates of Standard and AHT tests, by dimension of hypothesis ($q$) and nominal type I error ($\\alpha$).}\\label{fig:overview}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nThe first finding is that the AHT test has Type I error close to the stated $\\alpha$ level for all parameter combinations studied, whereas the standard test (based on the CR1 variance estimator and $m - 1$ degrees of freedom) does not. \nFigure \\ref{fig:overview} illustrates this pattern, for constraints of varying dimension (from $q = 1$, in the first column, to $q = 6$, in the final column) and nominal $\\alpha$ level (from .01, in the first row, to .10, in the last row). \nIn each of these figures, the number of clusters varies from 15 to 50 (on the horizontal axis), the solid horizontal line indicates the stated $\\alpha$ level and the dashed line indicates an upper confidence bound on simulation error.\nIt can be seen that the AHT test has Type I error near the stated $\\alpha$ level, even with a small number of clusters.\nWhen the number of clusters is very small, the Type I error can be smaller than the stated $\\alpha$ level. \nAlthough there exist situations in which the error is above the simulation bound, the departures are typically small. For example, when $m = 15$ the rejection rates do not exceed 0.021 for $\\alpha = .01$, 0.073 for $\\alpha = .05$, and 0.134 for $\\alpha = .10$.\nThe rejection rates are even closer to nominal for lower-dimensional constraints.\nIn comparison, the Type I error for the standard test can be markedly higher than the stated $\\alpha$ level, particularly when the number of clusters is small or the dimension of the hypothesis is large. \nFor example, for nominal $\\alpha = .05$, the maximum Type I error ranges from 0.124 ($q = 1$) to 0.686 ($q = 6$) for data sets with 15 clusters.\nPerhaps even more important for practice, even when there are 50 clusters, the rejection rate of the standard test can be far above the stated $\\alpha$ level.\nAgain, focusing on the $\\alpha = 0.05$ case, the maximum error ranges from 0.072 ($q = 1$) to 0.183 ($q = 6$).\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{balance-1.pdf} \n\n}\n\n\\caption[Rejection rates of Standard and AHT tests, by study design and dimension of hypothesis (]{Rejection rates of Standard and AHT tests, by study design and dimension of hypothesis ($q$). CR = cluster-randomized design; DD = difference-in-differences design; RB = randomized block design; B = balanced; U = unbalanced.}\\label{fig:balance}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nIn order to better understand the effects of different parameter combinations on the performance of both tests, Figure \\ref{fig:balance} focuses on the $\\alpha = 0.05$ case and breaks out the results by study design. The top row of the figure depicts the standard test, while the bottom row depicts the AHT test; columns correspond to the number of clusters.\nWithin each graph, results are given by study design (on the horizontal axis), with colors corresponding to the dimension of the test.\nIn the top panel, it can be seen that the rejection rate of the standard test increases with the dimension of the test ($q$) and the degree of unbalance in the study design.\nDifferences between the balanced and unbalanced designs are largest for the CR and DD designs, with smaller discrepancies in RB designs.\nThe bottom row of Figure \\ref{fig:balance} displays results for the AHT test; here we focus on three trends.\nFirst, the rejection rate of the AHT test usually increases as the dimension of the test increases, though never above 0.073.\nSecond, unbalanced designs led to rejection rates that were usually below the nominal $\\alpha$---just the opposite of how the standard test is affected by unbalance. \nThis trend is the strongest for CR and DD designs, where Type I error can be close to 0 at its minimum. \nThird, for studies with at least 30 clusters, rejection rates are very closes to nominal (between 0.032 and 0.057) across all conditions studied.\n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{sidewaysfigure}\n\n{\\centering \\includegraphics[width=\\linewidth]{misspecification-1.pdf} \n\n}\n\n\\caption[Rejection rates of AHT test, by treatment effect variance and intra-class correlation]{Rejection rates of AHT test, by treatment effect variance and intra-class correlation.}\\label{fig:misspecification}\n\\end{sidewaysfigure}\n\n\n\\end{knitrout}\n\nNext, by simulating the errors across a variety of parameter combinations, we were also able to test the impact of misspecification of the working model on Type I error.\nBecause the CR2 correction and AHT degrees of freedom are both based on a working model with independent, homoskedastic errors, model misspecification increases with the true level of treatment effect variance ($\\sigma_\\delta^2$) and intra-class correlation ($\\tau^2$).\nFigure \\ref{fig:misspecification} depicts Type I error rates for $\\alpha = 0.05$ for the AHT test, with separate graphs according to the dimension $q$ of the test (columns) and the number of clusters (rows).\nWithin each panel, results are separated by the 9 combinations of $\\sigma_\\delta^2$ and $\\tau^2$. \nIt can be seen that the range of rejection rates remains very similar across the 9 error structures, with no clear pattern to the small differences that emerge.\nThese results follow closely those from Tipton and Pustejovsky (2015), which also found that even with extreme model misspecification the Type I error of the CR2S test was close to nominal. \n\n\\begin{knitrout}\n\\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\\color{fgcolor}\\begin{figure}\n\n{\\centering \\includegraphics[width=\\linewidth]{df-1.pdf} \n\n}\n\n\\caption[Range of denominator degrees of freedom for AHT test, by number of clusters and study design]{Range of denominator degrees of freedom for AHT test, by number of clusters and study design. CR = cluster-randomized design; DD = difference-in-differences design; RB = randomized block design; B = balanced; U = unbalanced.}\\label{fig:df}\n\\end{figure}\n\n\n\\end{knitrout}\n\nFinally, Figure \\ref{fig:df} depicts the range of estimated degrees of freedom for the AHT test as a function of the simulated study design and number of clusters ($m$). \nWithin each box plot, the degrees of freedom vary depending on the hypothesis tested, with constraints of larger dimension having lower degrees of freedom. \nIt can be seen that the AHT degrees of freedom are often far less than $m - 1$ and that they are strongly affected by the pattern of treatment assignment and the degree of balance. The balanced and unbalanced RB designs generally had AHT degrees of freedom closest to $m - 1$ because the treatment effects being tested are all identified within each cluster. \nThe balanced DD design usually had the next largest degrees of freedom because it involved contrasts between two patterns of treatment configuration, followed by the balanced CR design, which involved contrasts between three patterns of treatment configurations. \nFor both of these designs, unbalance led to sharply reduced degrees of freedom.\n\nThese new simulation results have demonstrated that the standard robust Wald test, using the CR1 correction and $m - 1$ degrees of freedom, produces a wide range of rejection rates, often far in excess of the nominal Type I error. \nIn contrast, the rejection rates of the AHT test are below or at most slightly above nominal, across the conditions that we have examined.\nThis is because the AHT test incorporates information about the covariate features into its estimated degrees of freedom, whereas the standard test does not.\nAn important question that remains then is how much the AHT and standard tests diverge in actual application.\nIn the next section, we compare the two tests in several examples, drawn from a range of recent empirical research.\n\n\\section{EXAMPLES}\n\\label{sec:examples}\n\nThis section presents three short examples that illustrate the performance of CRVE across a variety of applied contexts. \nIn the first example, the effects of substantive interest involve between-cluster contrasts. \nThe second example involves a cluster-robust Hausman test for differences between within- and across-cluster information. \nIn the final example, the effects are identified within each cluster. \nIn each example, we demonstrate the proposed AHT test for single- and multiple-parameter hypotheses and compare the results to the standard test based on the CR1 variance estimator and $m - 1$ degrees of freedom. \nThe focus here is on providing insight into the conditions under which the AHT and standard estimators diverge in terms of three quantities of interest: the standard error estimates, the degrees of freedom estimates, and the stated p-values. \nData files and replication code (in R) are available for each analysis as an online supplement.\n\n\\subsection{Achievement Awards demonstration} \n\n\\citet{Angrist2009effects} reported results from a randomized trial in Israel that aimed to increase completion rates of the Bagrut, the national matriculation certificate for post-secondary education, among low-achieving high school students. \nIn the Achievement Awards demonstration, 40 non-vocational high schools with low rates of Bagrut completion were selected from across Israel, including 10 Arab and 10 Jewish religious schools and 20 Jewish secular schools. \nThe schools were then pair-matched based on 1999 rates of Bagrut completion, and within each pair one school was randomized to receive a cash-transfer program. \nIn these treatment schools, seniors who completed certification were eligible for payments of approximately \\$1,500. \nStudent-level covariate and outcome data were drawn from administrative records for the school years ending in June of 2000, 2001, and 2002. \nThe incentive program was in effect for the group of seniors in treatment schools taking the Bagrut exams in Spring of 2001, but the program was discontinued for the following year. \nWe therefore treat completion rates for 2000 and 2002 as being unaffected by treatment assignment.\nThe primary outcome of interest is Bagrut completion. \n\nThis study provides an opportunity to examine the AHT test in a situation in which the treatment was assigned at the cluster level, with a smaller number of clusters.\nFor simplicity, we restrict our analysis to the sample of female students, which reduces the total sample to 35 schools.\nFollowing the original analysis of \\citet{Angrist2009effects}, we allow the program's effects to vary depending on whether a students was in the upper or lower half of the distribution of prior-year academic performance. \nLetting $h = 1,2,3$ index the sector of each school (Arab religious, Jewish religious, or Jewish secular), we consider the following analytic model: \n\n", "index": 43, "text": "\\begin{equation}\n\\label{eq:AL_ATE}\ny_{hitj} = z_{hit}{\\mathbf}{r}_{hitj}'{\\boldsymbol}\\beta_h + {\\mathbf}{s}_{hitj}'{\\boldsymbol}\\gamma + \\gamma_{ht} + \\mu_{hi} + \\epsilon_{hitj}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"y_{hitj}=z_{hit}{\\mathbf{}}{r}_{hitj}^{\\prime}{\\boldsymbol{}}\\beta_{h}+{%&#10;\\mathbf{}}{s}_{hitj}^{\\prime}{\\boldsymbol{}}\\gamma+\\gamma_{ht}+\\mu_{hi}+%&#10;\\epsilon_{hitj}\" display=\"block\"><mrow><msub><mi>y</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>z</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>r</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>h</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>s</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\u03b3</mi></mrow><mo>+</mo><msub><mi>\u03b3</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>+</mo><msub><mi>\u03bc</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIn this model, time-point $t$ is nested within state $i$; the outcome $y_{it}$ is the number of deaths in motor vehicle crashes (per 100,000 residents) in state $i$ at time $t$; ${\\mathbf}{r}_{it}$ is a vector of covariates; $\\gamma_t$ is a fixed effect for time point $t$; and $\\mu_i$ is an effect for state $i$. The vector ${\\mathbf}{r}_{it}$ consists of a measure of the proportion of the population between the ages of 18 and 20 years who can legally drink alcohol and a measure of the beer taxation rate, both of which vary across states and across time.\\todo{Possible to get replication data from Carpenter and Dobkin (2011) instead?}\n\nWe apply both random effects (RE) and fixed effects (FE) approaches to estimate the effect of lowering the legal drinking age. \nFor the RE estimates, we use WLS with weights derived under the assumption that  $\\mu_1,...,\\mu_m$ are mutually independent, normally distributed, and independent of $\\epsilon_{it}$ and ${\\mathbf}{r}_{it}$.\nWe also report an artificial Hausman test \\citep{Arellano1993on, Wooldridge2002econometric} for correlation between the covariates ${\\mathbf}{r}_{it}$ and the state effects $\\mu_i$. Such correlation creates bias in the RE estimator of the policy effect, thus necessitating the use of the FE estimator.\nThe artificial Hausman test amends model (\\ref{eq:MLDA}) to include within-cluster deviations for the variables of interest, so that the estimating equation is\n\n", "itemtype": "equation", "pos": 81403, "prevtext": "\nIn this model for student $j$ in year $t$ in school $i$ in sector $h$, $z_{hit}$ is an indicator equal to one in the treatment schools for the 2001 school year and otherwise equal to zero; ${\\mathbf}{r}_{hitj}$ is a vector of indicators for whether the student is in the lower or upper half of the distribution of prior academic performance; and ${\\boldsymbol}\\beta_h = \\left(\\beta_{1h}, \\beta_{2h}\\right)$ is a vector of average treatment effects for schools in sector $h$. \nThe vector ${\\mathbf}{s}_{hitj}$ includes the following individual student demographic measures: mother's and father's education, immigration status, number of siblings, and indicators for each quartile in the distribution of prior-year academic performance. \nThe model also includes fixed effects $\\gamma_{ht}$ for each sector in each year and $\\mu_{hi}$ for each school. \n\nBased on Model (\\ref{eq:AL_ATE}), we test four hypotheses, again with the goal of exploring the use of the AHT tests under a range of conditions. \nFirst, we assume that the program effects are constant across sector (i.e., ${\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3 = {\\boldsymbol}\\beta$) and test for whether the program affected completion rates for students in the upper half of the prior achievement distribution ($H_0: \\beta_2 = 0$, with $q = 1$).\nSecond, we test for whether the program was effective in either half of the prior academic performance ($H_0: {\\boldsymbol}\\beta = 0$, with $q = 2$), still assuming that program effects are constant across sector. \nThird, we test for whether program effects in the upper half of the prior achievement distribution are moderated by school sector ($H_0: \\beta_{21} = \\beta_{22} = \\beta_{23}$, with $q = 3$). \nFinally, we conduct a joint test for whether program effects in either half of the prior achievement distribution are moderated by school sector ($H_0: {\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3$, with $q = 4$). \n\n\n\n\n\n\\begin{table}[bth]\n\\centering\n\\begin{tabular}{lcrrr}\n  \\toprule\nHypothesis & Test & F & df & p \\\\ \n  \\midrule\nATE - upper half (q = 1) & Standard & 5.746 & 34.00 & 0.02217 \\\\ \n   & AHT & 5.169 & 15.86 & 0.03726 \\\\ \n  ATE - joint (q = 2) & Standard & 3.848 & 34.00 & 0.03116 \\\\ \n   & AHT & 3.371 & 15.46 & 0.06096 \\\\ \n   \\midrule\nModeration - upper half (q = 2) & Standard & 3.186 & 34.00 & 0.05393 \\\\ \n   & AHT & 0.091 & 3.19 & 0.91520 \\\\ \n  Moderation - joint (q = 4) & Standard & 8.213 & 34.00 & 0.00010 \\\\ \n   & AHT & 2.895 & 3.21 & 0.19446 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of treatment effects in the Achievement Awards Demonstration} \n\\label{tab:AAD}\n\\end{table}\n\n\nTable \\ref{tab:AAD} reports the results of all four hypothesis tests. \nThese results indicate three important trends.\nFirst, in the case of the first two hypotheses, the AHT test statistics are only slightly smaller than their standard counterparts, but the degrees of freedom are considerably smaller. \nThese differences in degrees of freedom arise because the treatment was assigned at the cluster level, while the subgroups varied within each cluster. \nSecond, the third and fourth hypotheses tests, which compared treatment effects across sectors and subgroups, are cases in which the AHT and standard tests diverge markedly.\nFor these cases, the AHT test statistic and degrees of freedom are both considerably smaller than those from the standard test. \nThis reflects the degree of unbalance in allocations across sectors (19 Jewish secular, 7 Jewish religious, and 9 Arab religious schools), combined with cluster-level randomization. \nIn combination, these smaller test statistics and degrees of freedom result in larger p-values for the AHT test when compared to the standard test.\n\n\\subsection{Effects of minimum legal drinking age on mortality} \n\nOur second example focues on panel data, using an example described in \\citet[see also \\citealp{Carpenter2011minimum}]{Angrist2014mastering}.\nBased on data from the Fatal Accident Reporting System maintained by the National Highway Traffic Safety Administration, we estimated the effects of changes in the minimum legal drinking age over the time period of 1970-1983 on state-level death rates resulting from motor vehicle crashes.\nA standard difference-in-differences specification for such a state-by-year panel is\n\n", "index": 45, "text": "\\begin{equation}\n\\label{eq:MLDA}\ny_{it} = {\\mathbf}{r}_{it}'{\\boldsymbol}\\beta + \\gamma_t + \\mu_i + \\epsilon_{it}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"y_{it}={\\mathbf{}}{r}_{it}^{\\prime}{\\boldsymbol{}}\\beta+\\gamma_{t}+\\mu_{i}+%&#10;\\epsilon_{it}.\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mrow><msubsup><mi>r</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>+</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere ${\\mathbf}{\\ddot{r}}_{it}$ denotes the within-cluster deviations of the covariate.\nThe parameter ${\\boldsymbol}\\delta$ captures the difference between the between-cluster and within-cluster estimates of ${\\boldsymbol}\\beta$. \nWith this setup, the artificial Hausman test amounts to testing the null hypothesis that ${\\boldsymbol}\\delta = {\\mathbf}{0}$, where ${\\boldsymbol}\\delta$ is estimated using RE.  \n\n\n\n\n\n\\begin{table}[bth]\n\\centering\n\\begin{tabular}{lcrrr}\n  \\toprule\nHypothesis & Test & F & df & p \\\\ \n  \\midrule\nRandom effects & Standard & 8.261 & 49.00 & 0.00598 \\\\ \n   & AHT & 7.785 & 24.74 & 0.00999 \\\\ \n  Fixed effects & Standard & 9.660 & 49.00 & 0.00313 \\\\ \n   & AHT & 9.116 & 22.72 & 0.00616 \\\\ \n   \\midrule\nHausman test & Standard & 2.930 & 49.00 & 0.06283 \\\\ \n   & AHT & 2.489 & 8.69 & 0.13980 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of effects of minimum legal drink age and Hausman specification test} \n\\label{tab:MLDA}\n\\end{table}\n\n\nTable \\ref{tab:MLDA} displays the results of the tests for the policy variable and the Hausman tests for each model specification. \nThe results of the policy effect tests are quite similar across specifications and versions of the test. \nOf note is that, for both the RE and FE estimates, the AHT tests have only half the degrees of freedom of the corresponding standard tests. \nFor the artificial Hausman test, the AHT test has fewer than 9 degrees of freedom, which leads to a much larger p-value compared to using the standard test based on CR1. \n\n\\subsection{Tennessee STAR class-size experiment.} \n\nThe final example demonstrates an application in which the AHT and standard tests lead to similar results. The Tennessee STAR class size experiment is one of the most intensively studied interventions in education \\citep[for a detailed review, see][]{Schanzenbach2006what}.  The experiment involved students in kindergarten through third grade across 79 schools. Within each school, students and their teachers were randomized equally to one of three conditions: small class-size (targeted to have 13-17 students), regular class-size, or regular class-size with an aide.\nSubsequent research has focused on the effects of these conditions on kindergarten reading, math, and word recognition \\citep{Achilles2008tennessee}; high school test scores \\citep{Schanzenbach2006what}; college entrance exam participation \\citep{Krueger2001effect}; and home ownership and earnings \\citep{Chetty2011how}, among other outcomes.\n\nThe STAR experiment involved three treatment conditions and multiple outcomes, providing a scenario where both t-tests (with $q = 1$) and F-tests with varying constraint dimensions can be applied. \nFor simplicity, we focus only on the subgroup of students who were in kindergarten during the first year of the study, and on three outcomes measured at the end of the kindergarten year: reading, word recognition, and math \\citep{Achilles2008tennessee}. \nOutcome scores are standardized to percentile ranks, following \\citet{Krueger2001effect}.\nThe analytic model is: \n\n", "itemtype": "equation", "pos": 82972, "prevtext": "\nIn this model, time-point $t$ is nested within state $i$; the outcome $y_{it}$ is the number of deaths in motor vehicle crashes (per 100,000 residents) in state $i$ at time $t$; ${\\mathbf}{r}_{it}$ is a vector of covariates; $\\gamma_t$ is a fixed effect for time point $t$; and $\\mu_i$ is an effect for state $i$. The vector ${\\mathbf}{r}_{it}$ consists of a measure of the proportion of the population between the ages of 18 and 20 years who can legally drink alcohol and a measure of the beer taxation rate, both of which vary across states and across time.\\todo{Possible to get replication data from Carpenter and Dobkin (2011) instead?}\n\nWe apply both random effects (RE) and fixed effects (FE) approaches to estimate the effect of lowering the legal drinking age. \nFor the RE estimates, we use WLS with weights derived under the assumption that  $\\mu_1,...,\\mu_m$ are mutually independent, normally distributed, and independent of $\\epsilon_{it}$ and ${\\mathbf}{r}_{it}$.\nWe also report an artificial Hausman test \\citep{Arellano1993on, Wooldridge2002econometric} for correlation between the covariates ${\\mathbf}{r}_{it}$ and the state effects $\\mu_i$. Such correlation creates bias in the RE estimator of the policy effect, thus necessitating the use of the FE estimator.\nThe artificial Hausman test amends model (\\ref{eq:MLDA}) to include within-cluster deviations for the variables of interest, so that the estimating equation is\n\n", "index": 47, "text": "\\begin{equation}\ny_{it} = {\\mathbf}{r}_{it}\\beta + {\\mathbf}{\\ddot{r}}_{it}{\\boldsymbol}\\delta + \\gamma_t + \\mu_i + \\epsilon_{it},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"y_{it}={\\mathbf{}}{r}_{it}\\beta+{\\mathbf{}}{\\ddot{r}}_{it}{\\boldsymbol{}}%&#10;\\delta+\\gamma_{t}+\\mu_{i}+\\epsilon_{it},\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>\u03b2</mi></mrow><mo>+</mo><mrow><msub><mover accent=\"true\"><mi>r</mi><mo>\u00a8</mo></mover><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>\u03b4</mi></mrow><mo>+</mo><msub><mi>\u03b3</mi><mi>t</mi></msub><mo>+</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>t</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nwhere $y_{ijk}$ is the percentile rank on outcome $k$ for student $j$ in school $i$; ${\\mathbf}{r}_{ij}$ includes indicators for the small-class and regular-plus-aide conditions; ${\\mathbf}{s}_{ij}$ includes student demographic covariates (i.e., free or reduced-price lunch status; race; gender; age); $\\gamma_k$ is a fixed effect for outcome $k$; and $\\mu_i$ is a fixed effect for school $i$. \nIn this model, $\\beta_{1k}$ represents the average effect of being in a small class and $\\beta_{2k}$ represents the average of effect of being in a regular class with an aid, in each case compared to a regular-size class without an aid.\n\nUsing this model, we test four distinct hypotheses that vary in dimension from $q = 1$ to $q = 6$. \nFirst, using only the math achievement scores, we test the effects of small class size ($H_0: \\beta_{11} = 0$) while maintaining the assumption that the additional classroom aide has no effect on student achievement (i.e., constraining $\\beta_{21} = 0$). \nSecond, again only using the data for outcome $k$, we test the hypothesis that there are no differences across the three class-size conditions (i.e., $H_0: {\\boldsymbol}\\beta_1 = {\\mathbf}{0}$). \nThird, combining the data across all three outcomes, we test the hypothesis that small class size (vs regular and regular plus aide) had no effects on any outcome (i.e., $\\beta_{11} = \\beta_{12} = \\beta_{13} = 0$).\nFinally, we test the hypothesis that there are no differences across the three class-size conditions on any outcome (i.e., $H_0: {\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3 = {\\mathbf}{0}$). \nThe third and fourth tests use the seemingly unrelated regression (SUR) framework, in which separate treatment effects are estimated for each outcome, but the student demographic effects and school fixed effects are pooled across outcomes. \nIn all models, we estimated ${\\boldsymbol}\\beta_k$ and ${\\boldsymbol}\\gamma$ after absorbing the school fixed effects and clustering the standard errors by school.\n\n\n\n\n\n\\begin{table}[tbh]\n\\centering\n\\begin{tabular}{lllrrr}\n  \\toprule\nOutcome & Effect & Test & F & df & p \\\\ \n  \\midrule\nMath & Small class (q=1) & Standard & 13.624 & 78.0 & 0.00041 \\\\ \n   &  & AHT & 13.590 & 69.0 & 0.00045 \\\\ \n   & Small class and classroom aide (q=2) & Standard & 6.838 & 78.0 & 0.00183 \\\\ \n   &  & AHT & 6.725 & 68.6 & 0.00215 \\\\ \n   \\midrule\nCombined & Small class (q=3) & Standard & 6.408 & 78.0 & 0.00062 \\\\ \n   &  & AHT & 6.206 & 67.0 & 0.00088 \\\\ \n   & Small class and classroom aide (q=6) & Standard & 3.284 & 78.0 & 0.00622 \\\\ \n   &  & AHT & 3.042 & 64.9 & 0.01103 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of treatment effects in the Tennessee STAR class size experiment} \n\\label{tab:STAR}\n\\end{table}\n\n\nTable \\ref{tab:STAR} displays the results for a representative subset of these hypothesis tests, using either the standard test (with CR1) or the AHT test (with CR2).\nThese results illustrate two important points regarding the use of the AHT test in practice.\nFirst, across all three analyses, the AHT t- and F-tests are only typically slightly smaller than the corresponding standard test.\nSecond, if treatment is randomly allocated in approximately equal proportions within each cluster---as occurred in the TN STAR experiment---the degrees of freedom for the AHT tests are only slightly smaller than those for the standard tests.\nIn combination with the rather large sample size of 79 schools, these differences have only a minimal effect on the p-values for these tests. \nAs the previous two examples illustrate, however, the similarity between these tests is not common, and is a result only of the design of the study, indicating that the standard test is best used only in experiments randomized within clusters. \n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nAcross the field of economics, empirical studies often involve modeling data with a correlated error structure. \nCorrelated errors arise in the analysis of multi-stage samples, cluster-randomized trials, panel data, and regression discontinuities with discrete forcing variables, among other study designs. \nIt is now routine to handle dependent error structures by using cluster-robust variance estimation, which provides asymptotically valid standard errors and hypothesis tests without making strong parametric assumptions about the error structure. \nHowever, a growing body of recent work has drawn attention to the shortcomings of CRVE methods when the data include only a small or moderate number of independent clusters \\citep{Cameron2008bootstrap, Cameron2015practitioners, Imbens2015robust, Webb2013wild}. \nIn particular, Wald tests based on CRVE can have rejection rates far in excess of the nominal Type I error. \nThis problem is compounded by the fact that the performance of standard Wald tests depends on features of the study design beyond just the total number of clusters, which can make it difficult to determine whether standard, asymptotic valid CRVE methods are accurate. \n\n\nOne promising solution to this problem is to use the bias-reduced linearization variance estimator (i.e., CR2) proposed by \\citet{Bell2002bias}, which corrects the CRVE so that it is exactly unbiased under an analyst-specified working model for the error structure, together with degrees of freedom estimated based on the same working model.\nIn this paper, we have demonstrated that the CR2 variance estimator is a fully general solution, which can be applied even in models with fixed effects in multiple dimensions. \nOur re-formulation of the bias-reduced linearization criteria also makes clear how to calculate the CR2 correction when the model includes fixed effects, whether those fixed effects are estimated by OLS or are instead absorbed before estimating the target regression parameters.  \nFinally, we have proposed a method for testing hypotheses that involve multiple constraints on regression parameters, based on an approximation that generalizes the existing Satterthwaite approximation for t-tests.  \nWith the modifications and extensions proposed in this paper, the CR2 variance estimator and small-sample testing procedures can be applied in a wide range of analytic models---essentially, any model estimated by ordinary or weighted least squares. \n\nWe join \\citet{Imbens2015robust} in arguing that the CR2 estimator and corresponding estimated degrees of freedom for hypothesis tests should be applied routinely, whenever analysts use CRVE and hypothesis tests based thereon. \nBecause the performance of standard CRVE methods depends on features of the study design, the total number of clusters in the data is an insufficient guide to whether small-sample corrections are needed. \nInstead, the clearest way to determine whether small-sample corrections are needed is simply to calculate them.\nThe proposed AHT test involves two adjustments: use of the CR2 adjustment for the variance estimator and use of estimated degrees of freedom. \nOur simulation study illustrates that the combined result of these adjustments results in an AHT test with Type I error close to the stated ${\\boldsymbol}\\alpha$ level. \nFurthermore, our empirical examples illustrate that the degrees of freedom adjustment has a relatively larger influence on small-sample performance.\nThese degrees of freedom can be much smaller than the number of clusters, particularly when the covariates involved in the test involve high leverage or are unbalanced across clusters.\nThe estimated degrees of freedom are indicative of the precision of the standard errors, and thus provide diagnostic information that is similar to the effective sample size measure proposed by \\citet{Carter2013asymptotic}. \nWe therefore recommend that the degrees of freedom be reported along with standard errors and $p$-values whenever the method is applied.\n\nThe idea of developing small-sample adjustments based on a working model may seem strange to analysts accustomed to using CRVE---after all, the whole point of clustering standard errors is to avoid making assumptions about the error structure.\nHowever, simulation studies reported here and elsewhere \\citep{Tipton2015small-t, Tipton2015small-F} have demonstrated that the approach is actually robust to a high degree of misspecification in the working model. \nFurthermore, while the working model provides necessary ``scaffolding'' when the number of clusters is small, its  influence tends to fall away as the number of clusters increases, so that the CR2 estimator and AHT maintain the same asymptotic robustness as standard CRVE methods. \n\nOne outstanding problem with the CR2 variance estimator is that it can become computationally costly (or even infeasible) when the within-cluster sample sizes are large \\citep{Mackinnon2014wild}. \nFor example, \\citet{Bertrand2004how} analyzed micro-level data from a 21-year panel of current population survey data, with clustering by state. Their data included some state-level clusters with over $n_i = 10,000$ individual observations. \nThe CR2 adjustment matrices have dimension $n_i \\times n_i$, and would be very expensive to compute in this application. \nMethods for improving the computational efficiency of the CR2 variance estimator (or alternative estimators that have similar performance to CR2), should be investigated further. \n\nThis paper has developed the CR2 estimator and AHT testing procedure for weighted least squares estimation of linear regression models. \nExtensions to linear regression models with clustering in multiple, non-nested dimensions \\citep[cf.][]{Cameron2011robust} appear to be possible, and their utility should be further investigated. \n\\citet{McCaffrey2006improved} have proposed extensions to bias-reduced linearization for use with generalized estimating equations, and future work should consider further extensions to other classes of estimators, including two-stage least squares and generalized method of moments. \n\\citet{McCaffrey2006improved} also found that for single-parameter hypotheses, a saddlepoint approximation to the Wald test statistic provides even more accurate rejection rates than the Satterthwaite approximation given in Equation (\\ref{eq:nu_model}). \nIt would be interesting to investigate whether the saddlepoint approximation could be extended to handle multiple-parameter constraints, although this appears to be far from straight-forward. \n\n\\appendix\n\n\\section{BRL adjustment matrices}\n\\label{app:theorems}\n\nThis appendix provides proof of the two theorems from Section \\ref{sec:BRL}. \n\n\\subsection{Proof of Theorem \\ref{thm:BRL_FE}}\n\nThe Moore-Penrose inverse of ${\\mathbf}{B}_i$ can be computed from its eigen-decomposition. Let $b \\leq n_i$ denote the rank of ${\\mathbf}{B}_i$. \nLet ${\\boldsymbol}\\Lambda$ be the $b \\times b$ diagonal matrix of the positive eigenvalues of ${\\mathbf}{B}_i$ and ${\\mathbf}{V}$ be the $n_i \\times b$ matrix of corresponding eigen-vectors, so that ${\\mathbf}{B}_i = {\\mathbf}{V}{\\boldsymbol}\\Lambda{\\mathbf}{V}'$. \nThen ${\\mathbf}{B}_i^+ = {\\mathbf}{V}{\\boldsymbol}\\Lambda^{-1}{\\mathbf}{V}'$ and ${\\mathbf}{B}_i^{+1/2} = {\\mathbf}{V}{\\boldsymbol}\\Lambda^{-1/2}{\\mathbf}{V}'$. Now, observe that \n\n", "itemtype": "equation", "pos": 86173, "prevtext": "\nwhere ${\\mathbf}{\\ddot{r}}_{it}$ denotes the within-cluster deviations of the covariate.\nThe parameter ${\\boldsymbol}\\delta$ captures the difference between the between-cluster and within-cluster estimates of ${\\boldsymbol}\\beta$. \nWith this setup, the artificial Hausman test amounts to testing the null hypothesis that ${\\boldsymbol}\\delta = {\\mathbf}{0}$, where ${\\boldsymbol}\\delta$ is estimated using RE.  \n\n\n\n\n\n\\begin{table}[bth]\n\\centering\n\\begin{tabular}{lcrrr}\n  \\toprule\nHypothesis & Test & F & df & p \\\\ \n  \\midrule\nRandom effects & Standard & 8.261 & 49.00 & 0.00598 \\\\ \n   & AHT & 7.785 & 24.74 & 0.00999 \\\\ \n  Fixed effects & Standard & 9.660 & 49.00 & 0.00313 \\\\ \n   & AHT & 9.116 & 22.72 & 0.00616 \\\\ \n   \\midrule\nHausman test & Standard & 2.930 & 49.00 & 0.06283 \\\\ \n   & AHT & 2.489 & 8.69 & 0.13980 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of effects of minimum legal drink age and Hausman specification test} \n\\label{tab:MLDA}\n\\end{table}\n\n\nTable \\ref{tab:MLDA} displays the results of the tests for the policy variable and the Hausman tests for each model specification. \nThe results of the policy effect tests are quite similar across specifications and versions of the test. \nOf note is that, for both the RE and FE estimates, the AHT tests have only half the degrees of freedom of the corresponding standard tests. \nFor the artificial Hausman test, the AHT test has fewer than 9 degrees of freedom, which leads to a much larger p-value compared to using the standard test based on CR1. \n\n\\subsection{Tennessee STAR class-size experiment.} \n\nThe final example demonstrates an application in which the AHT and standard tests lead to similar results. The Tennessee STAR class size experiment is one of the most intensively studied interventions in education \\citep[for a detailed review, see][]{Schanzenbach2006what}.  The experiment involved students in kindergarten through third grade across 79 schools. Within each school, students and their teachers were randomized equally to one of three conditions: small class-size (targeted to have 13-17 students), regular class-size, or regular class-size with an aide.\nSubsequent research has focused on the effects of these conditions on kindergarten reading, math, and word recognition \\citep{Achilles2008tennessee}; high school test scores \\citep{Schanzenbach2006what}; college entrance exam participation \\citep{Krueger2001effect}; and home ownership and earnings \\citep{Chetty2011how}, among other outcomes.\n\nThe STAR experiment involved three treatment conditions and multiple outcomes, providing a scenario where both t-tests (with $q = 1$) and F-tests with varying constraint dimensions can be applied. \nFor simplicity, we focus only on the subgroup of students who were in kindergarten during the first year of the study, and on three outcomes measured at the end of the kindergarten year: reading, word recognition, and math \\citep{Achilles2008tennessee}. \nOutcome scores are standardized to percentile ranks, following \\citet{Krueger2001effect}.\nThe analytic model is: \n\n", "index": 49, "text": "\\begin{equation}\ny_{ijk} = {\\mathbf}{r}_{ij}'{\\boldsymbol}\\beta_k + {\\mathbf}{s}_{ij}'{\\boldsymbol}\\gamma_0 + \\gamma_k + \\mu_i + \\epsilon_{ijk}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"y_{ijk}={\\mathbf{}}{r}_{ij}^{\\prime}{\\boldsymbol{}}\\beta_{k}+{\\mathbf{}}{s}_{%&#10;ij}^{\\prime}{\\boldsymbol{}}\\gamma_{0}+\\gamma_{k}+\\mu_{i}+\\epsilon_{ijk},\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mrow><msubsup><mi>r</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\u03b3</mi><mn>0</mn></msub></mrow><mo>+</mo><msub><mi>\u03b3</mi><mi>k</mi></msub><mo>+</mo><msub><mi>\u03bc</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03f5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\n\nBecause ${\\mathbf}{D}_i$, and ${\\boldsymbol}\\Phi$ are positive definite and ${\\mathbf}{B}_i$ is symmetric, the eigen-vectors ${\\mathbf}{V}$ define an orthonormal basis for the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{X}}}\\right)_i$.\nWe now show that ${\\mathbf}{\\ddot{U}}_i$ is in the column space of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. \nLet ${\\mathbf}{Z}_i$ be an $n_i \\times (r + s)$ matrix of zeros. \nLet ${\\mathbf}{Z}_k = - {\\mathbf}{\\ddot{U}}_k {\\mathbf}{L}^{-1}{\\mathbf}{M}_{{\\mathbf}{\\ddot{U}}}^{-1}$, for $k \\neq j$ and take ${\\mathbf}{Z} = \\left({\\mathbf}{Z}_1',...,{\\mathbf}{Z}_m'\\right)'$. \nNow observe that $\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right) {\\mathbf}{Z} = {\\mathbf}{Z}$. \nIt follows that \n\n", "itemtype": "equation", "pos": 97520, "prevtext": "\nwhere $y_{ijk}$ is the percentile rank on outcome $k$ for student $j$ in school $i$; ${\\mathbf}{r}_{ij}$ includes indicators for the small-class and regular-plus-aide conditions; ${\\mathbf}{s}_{ij}$ includes student demographic covariates (i.e., free or reduced-price lunch status; race; gender; age); $\\gamma_k$ is a fixed effect for outcome $k$; and $\\mu_i$ is a fixed effect for school $i$. \nIn this model, $\\beta_{1k}$ represents the average effect of being in a small class and $\\beta_{2k}$ represents the average of effect of being in a regular class with an aid, in each case compared to a regular-size class without an aid.\n\nUsing this model, we test four distinct hypotheses that vary in dimension from $q = 1$ to $q = 6$. \nFirst, using only the math achievement scores, we test the effects of small class size ($H_0: \\beta_{11} = 0$) while maintaining the assumption that the additional classroom aide has no effect on student achievement (i.e., constraining $\\beta_{21} = 0$). \nSecond, again only using the data for outcome $k$, we test the hypothesis that there are no differences across the three class-size conditions (i.e., $H_0: {\\boldsymbol}\\beta_1 = {\\mathbf}{0}$). \nThird, combining the data across all three outcomes, we test the hypothesis that small class size (vs regular and regular plus aide) had no effects on any outcome (i.e., $\\beta_{11} = \\beta_{12} = \\beta_{13} = 0$).\nFinally, we test the hypothesis that there are no differences across the three class-size conditions on any outcome (i.e., $H_0: {\\boldsymbol}\\beta_1 = {\\boldsymbol}\\beta_2 = {\\boldsymbol}\\beta_3 = {\\mathbf}{0}$). \nThe third and fourth tests use the seemingly unrelated regression (SUR) framework, in which separate treatment effects are estimated for each outcome, but the student demographic effects and school fixed effects are pooled across outcomes. \nIn all models, we estimated ${\\boldsymbol}\\beta_k$ and ${\\boldsymbol}\\gamma$ after absorbing the school fixed effects and clustering the standard errors by school.\n\n\n\n\n\n\\begin{table}[tbh]\n\\centering\n\\begin{tabular}{lllrrr}\n  \\toprule\nOutcome & Effect & Test & F & df & p \\\\ \n  \\midrule\nMath & Small class (q=1) & Standard & 13.624 & 78.0 & 0.00041 \\\\ \n   &  & AHT & 13.590 & 69.0 & 0.00045 \\\\ \n   & Small class and classroom aide (q=2) & Standard & 6.838 & 78.0 & 0.00183 \\\\ \n   &  & AHT & 6.725 & 68.6 & 0.00215 \\\\ \n   \\midrule\nCombined & Small class (q=3) & Standard & 6.408 & 78.0 & 0.00062 \\\\ \n   &  & AHT & 6.206 & 67.0 & 0.00088 \\\\ \n   & Small class and classroom aide (q=6) & Standard & 3.284 & 78.0 & 0.00622 \\\\ \n   &  & AHT & 3.042 & 64.9 & 0.01103 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{Tests of treatment effects in the Tennessee STAR class size experiment} \n\\label{tab:STAR}\n\\end{table}\n\n\nTable \\ref{tab:STAR} displays the results for a representative subset of these hypothesis tests, using either the standard test (with CR1) or the AHT test (with CR2).\nThese results illustrate two important points regarding the use of the AHT test in practice.\nFirst, across all three analyses, the AHT t- and F-tests are only typically slightly smaller than the corresponding standard test.\nSecond, if treatment is randomly allocated in approximately equal proportions within each cluster---as occurred in the TN STAR experiment---the degrees of freedom for the AHT tests are only slightly smaller than those for the standard tests.\nIn combination with the rather large sample size of 79 schools, these differences have only a minimal effect on the p-values for these tests. \nAs the previous two examples illustrate, however, the similarity between these tests is not common, and is a result only of the design of the study, indicating that the standard test is best used only in experiments randomized within clusters. \n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nAcross the field of economics, empirical studies often involve modeling data with a correlated error structure. \nCorrelated errors arise in the analysis of multi-stage samples, cluster-randomized trials, panel data, and regression discontinuities with discrete forcing variables, among other study designs. \nIt is now routine to handle dependent error structures by using cluster-robust variance estimation, which provides asymptotically valid standard errors and hypothesis tests without making strong parametric assumptions about the error structure. \nHowever, a growing body of recent work has drawn attention to the shortcomings of CRVE methods when the data include only a small or moderate number of independent clusters \\citep{Cameron2008bootstrap, Cameron2015practitioners, Imbens2015robust, Webb2013wild}. \nIn particular, Wald tests based on CRVE can have rejection rates far in excess of the nominal Type I error. \nThis problem is compounded by the fact that the performance of standard Wald tests depends on features of the study design beyond just the total number of clusters, which can make it difficult to determine whether standard, asymptotic valid CRVE methods are accurate. \n\n\nOne promising solution to this problem is to use the bias-reduced linearization variance estimator (i.e., CR2) proposed by \\citet{Bell2002bias}, which corrects the CRVE so that it is exactly unbiased under an analyst-specified working model for the error structure, together with degrees of freedom estimated based on the same working model.\nIn this paper, we have demonstrated that the CR2 variance estimator is a fully general solution, which can be applied even in models with fixed effects in multiple dimensions. \nOur re-formulation of the bias-reduced linearization criteria also makes clear how to calculate the CR2 correction when the model includes fixed effects, whether those fixed effects are estimated by OLS or are instead absorbed before estimating the target regression parameters.  \nFinally, we have proposed a method for testing hypotheses that involve multiple constraints on regression parameters, based on an approximation that generalizes the existing Satterthwaite approximation for t-tests.  \nWith the modifications and extensions proposed in this paper, the CR2 variance estimator and small-sample testing procedures can be applied in a wide range of analytic models---essentially, any model estimated by ordinary or weighted least squares. \n\nWe join \\citet{Imbens2015robust} in arguing that the CR2 estimator and corresponding estimated degrees of freedom for hypothesis tests should be applied routinely, whenever analysts use CRVE and hypothesis tests based thereon. \nBecause the performance of standard CRVE methods depends on features of the study design, the total number of clusters in the data is an insufficient guide to whether small-sample corrections are needed. \nInstead, the clearest way to determine whether small-sample corrections are needed is simply to calculate them.\nThe proposed AHT test involves two adjustments: use of the CR2 adjustment for the variance estimator and use of estimated degrees of freedom. \nOur simulation study illustrates that the combined result of these adjustments results in an AHT test with Type I error close to the stated ${\\boldsymbol}\\alpha$ level. \nFurthermore, our empirical examples illustrate that the degrees of freedom adjustment has a relatively larger influence on small-sample performance.\nThese degrees of freedom can be much smaller than the number of clusters, particularly when the covariates involved in the test involve high leverage or are unbalanced across clusters.\nThe estimated degrees of freedom are indicative of the precision of the standard errors, and thus provide diagnostic information that is similar to the effective sample size measure proposed by \\citet{Carter2013asymptotic}. \nWe therefore recommend that the degrees of freedom be reported along with standard errors and $p$-values whenever the method is applied.\n\nThe idea of developing small-sample adjustments based on a working model may seem strange to analysts accustomed to using CRVE---after all, the whole point of clustering standard errors is to avoid making assumptions about the error structure.\nHowever, simulation studies reported here and elsewhere \\citep{Tipton2015small-t, Tipton2015small-F} have demonstrated that the approach is actually robust to a high degree of misspecification in the working model. \nFurthermore, while the working model provides necessary ``scaffolding'' when the number of clusters is small, its  influence tends to fall away as the number of clusters increases, so that the CR2 estimator and AHT maintain the same asymptotic robustness as standard CRVE methods. \n\nOne outstanding problem with the CR2 variance estimator is that it can become computationally costly (or even infeasible) when the within-cluster sample sizes are large \\citep{Mackinnon2014wild}. \nFor example, \\citet{Bertrand2004how} analyzed micro-level data from a 21-year panel of current population survey data, with clustering by state. Their data included some state-level clusters with over $n_i = 10,000$ individual observations. \nThe CR2 adjustment matrices have dimension $n_i \\times n_i$, and would be very expensive to compute in this application. \nMethods for improving the computational efficiency of the CR2 variance estimator (or alternative estimators that have similar performance to CR2), should be investigated further. \n\nThis paper has developed the CR2 estimator and AHT testing procedure for weighted least squares estimation of linear regression models. \nExtensions to linear regression models with clustering in multiple, non-nested dimensions \\citep[cf.][]{Cameron2011robust} appear to be possible, and their utility should be further investigated. \n\\citet{McCaffrey2006improved} have proposed extensions to bias-reduced linearization for use with generalized estimating equations, and future work should consider further extensions to other classes of estimators, including two-stage least squares and generalized method of moments. \n\\citet{McCaffrey2006improved} also found that for single-parameter hypotheses, a saddlepoint approximation to the Wald test statistic provides even more accurate rejection rates than the Satterthwaite approximation given in Equation (\\ref{eq:nu_model}). \nIt would be interesting to investigate whether the saddlepoint approximation could be extended to handle multiple-parameter constraints, although this appears to be far from straight-forward. \n\n\\appendix\n\n\\section{BRL adjustment matrices}\n\\label{app:theorems}\n\nThis appendix provides proof of the two theorems from Section \\ref{sec:BRL}. \n\n\\subsection{Proof of Theorem \\ref{thm:BRL_FE}}\n\nThe Moore-Penrose inverse of ${\\mathbf}{B}_i$ can be computed from its eigen-decomposition. Let $b \\leq n_i$ denote the rank of ${\\mathbf}{B}_i$. \nLet ${\\boldsymbol}\\Lambda$ be the $b \\times b$ diagonal matrix of the positive eigenvalues of ${\\mathbf}{B}_i$ and ${\\mathbf}{V}$ be the $n_i \\times b$ matrix of corresponding eigen-vectors, so that ${\\mathbf}{B}_i = {\\mathbf}{V}{\\boldsymbol}\\Lambda{\\mathbf}{V}'$. \nThen ${\\mathbf}{B}_i^+ = {\\mathbf}{V}{\\boldsymbol}\\Lambda^{-1}{\\mathbf}{V}'$ and ${\\mathbf}{B}_i^{+1/2} = {\\mathbf}{V}{\\boldsymbol}\\Lambda^{-1/2}{\\mathbf}{V}'$. Now, observe that \n\n", "index": 51, "text": "\\begin{align}\n\\label{eq:step1}\n{\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{A}_i \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i' {\\mathbf}{A}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i &= {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{D}_i {\\mathbf}{B}_i^{+1/2} {\\mathbf}{B}_i {\\mathbf}{B}_i^{+1/2} {\\mathbf}{D}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i \\nonumber \\\\\n&= {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{D}_i {\\mathbf}{V}{\\mathbf}{V}' {\\mathbf}{D}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{A%&#10;}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}{\\boldsymbol{}}\\Phi%&#10;\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}^{\\prime}{\\mathbf{}}{A}_{i}^%&#10;{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}\" display=\"inline\"><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>A</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{%&#10;D}_{i}{\\mathbf{}}{B}_{i}^{+1/2}{\\mathbf{}}{B}_{i}{\\mathbf{}}{B}_{i}^{+1/2}{%&#10;\\mathbf{}}{D}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>B</mi><mi>i</mi><mrow><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><msub><mi>B</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>B</mi><mi>i</mi><mrow><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{%&#10;D}_{i}{\\mathbf{}}{V}{\\mathbf{}}{V}^{\\prime}{\\mathbf{}}{D}_{i}^{\\prime}{\\mathbf%&#10;{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mi>V</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nThus, there exists an $N \\times (r + s)$ matrix ${\\mathbf}{Z}$ such that $\\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{X}}}\\right)_i {\\mathbf}{Z} = {\\mathbf}{\\ddot{U}}_i$, i.e., ${\\mathbf}{\\ddot{U}}_i$ is in the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. Because ${\\mathbf}{D}_i {\\mathbf}{W}_i$ is positive definite and ${\\mathbf}{\\ddot{R}}_i$ is a sub-matrix of ${\\mathbf}{\\ddot{U}}_i$, ${\\mathbf}{D}_i{\\mathbf}{W}_i{\\mathbf}{\\ddot{R}}_i$ is also in the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. It follows that \n\n", "itemtype": "equation", "pos": 98834, "prevtext": "\n\nBecause ${\\mathbf}{D}_i$, and ${\\boldsymbol}\\Phi$ are positive definite and ${\\mathbf}{B}_i$ is symmetric, the eigen-vectors ${\\mathbf}{V}$ define an orthonormal basis for the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{X}}}\\right)_i$.\nWe now show that ${\\mathbf}{\\ddot{U}}_i$ is in the column space of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. \nLet ${\\mathbf}{Z}_i$ be an $n_i \\times (r + s)$ matrix of zeros. \nLet ${\\mathbf}{Z}_k = - {\\mathbf}{\\ddot{U}}_k {\\mathbf}{L}^{-1}{\\mathbf}{M}_{{\\mathbf}{\\ddot{U}}}^{-1}$, for $k \\neq j$ and take ${\\mathbf}{Z} = \\left({\\mathbf}{Z}_1',...,{\\mathbf}{Z}_m'\\right)'$. \nNow observe that $\\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right) {\\mathbf}{Z} = {\\mathbf}{Z}$. \nIt follows that \n\n", "index": 53, "text": "\\begin{align*}\n\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\mathbf}{Z} &= \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right) {\\mathbf}{Z} = \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i {\\mathbf}{Z} \\\\\n&= {\\mathbf}{Z}_i - {\\mathbf}{\\ddot{U}}_i{\\mathbf}{M_{\\ddot{U}}}\\sum_{k=1}^m {\\mathbf}{\\ddot{U}}_k'{\\mathbf}{W}_k{\\mathbf}{Z}_k = {\\mathbf}{\\ddot{U}}_i{\\mathbf}{M_{\\ddot{U}}} \\left(\\sum_{k \\neq j} {\\mathbf}{\\ddot{U}}_k' {\\mathbf}{W}_k {\\mathbf}{\\ddot{U}} \\right) {\\mathbf}{L}^{-1}{\\mathbf}{M}_{{\\mathbf}{\\ddot{U}}}^{-1} \\\\\n&= {\\mathbf}{\\ddot{U}}_i.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{X}}\\right)_{i}{\\mathbf{}}{Z}\" display=\"inline\"><mrow><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi>Z</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{U}}}\\right)_{i}\\left({%&#10;\\mathbf{}}{I}-{\\mathbf{}}{H_{T}}\\right){\\mathbf{}}{Z}=\\left({\\mathbf{}}{I}-{%&#10;\\mathbf{}}{H_{\\ddot{U}}}\\right)_{i}{\\mathbf{}}{Z}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>T</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>Z</mi></mrow><mo>=</mo><mrow><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi>Z</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{Z}_{i}-{\\mathbf{}}{\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot%&#10;{U}}}\\sum_{k=1}^{m}{\\mathbf{}}{\\ddot{U}}_{k}^{\\prime}{\\mathbf{}}{W}_{k}{%&#10;\\mathbf{}}{Z}_{k}={\\mathbf{}}{\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}}}\\left(\\sum%&#10;_{k\\neq j}{\\mathbf{}}{\\ddot{U}}_{k}^{\\prime}{\\mathbf{}}{W}_{k}{\\mathbf{}}{%&#10;\\ddot{U}}\\right){\\mathbf{}}{L}^{-1}{\\mathbf{}}{M}_{{\\mathbf{}}{\\ddot{U}}}^{-1}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>k</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>Z</mi><mi>k</mi></msub></mrow></mrow></mrow></mrow><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder></mstyle><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>k</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>k</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mi>L</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{\\ddot{U}}_{i}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nSubstituting (\\ref{eq:step2}) into (\\ref{eq:step1}) demonstrates that ${\\mathbf}{A}_i$ satisfies criterion (\\ref{eq:CR2_criterion}).\n\nUnder the working model, the residuals from cluster $i$ have mean ${\\mathbf}{0}$ and variance\n", "itemtype": "equation", "pos": 100004, "prevtext": "\nThus, there exists an $N \\times (r + s)$ matrix ${\\mathbf}{Z}$ such that $\\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{X}}}\\right)_i {\\mathbf}{Z} = {\\mathbf}{\\ddot{U}}_i$, i.e., ${\\mathbf}{\\ddot{U}}_i$ is in the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. Because ${\\mathbf}{D}_i {\\mathbf}{W}_i$ is positive definite and ${\\mathbf}{\\ddot{R}}_i$ is a sub-matrix of ${\\mathbf}{\\ddot{U}}_i$, ${\\mathbf}{D}_i{\\mathbf}{W}_i{\\mathbf}{\\ddot{R}}_i$ is also in the column span of $\\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i$. It follows that \n\n", "index": 55, "text": "\\begin{equation}\n\\label{eq:step2}\n{\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{D}_i {\\mathbf}{V}{\\mathbf}{V}' {\\mathbf}{D}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i = {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\boldsymbol}\\Phi_i {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{D}_{i}{\\mathbf%&#10;{}}{V}{\\mathbf{}}{V}^{\\prime}{\\mathbf{}}{D}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{%&#10;\\mathbf{}}{\\ddot{R}}_{i}={\\mathbf{}}{\\ddot{R}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{%&#10;\\boldsymbol{}}\\Phi_{i}{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot{R}}_{i}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mi>V</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow><mo>=</mo><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": " \nIt follows that \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nSubstituting (\\ref{eq:step2}) into (\\ref{eq:step1}) demonstrates that ${\\mathbf}{A}_i$ satisfies criterion (\\ref{eq:CR2_criterion}).\n\nUnder the working model, the residuals from cluster $i$ have mean ${\\mathbf}{0}$ and variance\n", "index": 57, "text": "\\[\n{\\text{Var}}\\left({\\mathbf}{\\ddot{e}}_i\\right) = \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i',\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{\\text{Var}}\\left({\\mathbf{}}{\\ddot{e}}_{i}\\right)=\\left({\\mathbf{}}{I}-{%&#10;\\mathbf{}}{H_{X}}\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}%&#10;}{H_{X}}\\right)_{i}^{\\prime},\" display=\"block\"><mrow><mrow><mrow><mtext>Var</mtext><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>e</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\n\n\\subsection{Proof of Theorem \\ref{thm:absorb}}\n\nFrom the fact that ${\\mathbf}{\\ddot{U}}_i'{\\mathbf}{W}_i{\\mathbf}{T}_i = {\\mathbf}{0}$ for $i = 1,...,m$, it follows that \n", "itemtype": "equation", "pos": 100693, "prevtext": " \nIt follows that \n\n", "index": 59, "text": "\\begin{align*}\n{\\text{E}}\\left({\\mathbf}{V}^{CR2}\\right) &= {\\mathbf}{M_{\\ddot{R}}}\\left[\\sum_{i=1}^m {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\mathbf}{A}_i \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_X}\\right)_i' {\\mathbf}{A}_i {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i \\right] {\\mathbf}{M_{\\ddot{R}}} \\\\\n&= {\\mathbf}{M_{\\ddot{R}}}\\left[\\sum_{i=1}^m {\\mathbf}{\\ddot{R}}_i' {\\mathbf}{W}_i {\\boldsymbol}\\Phi_i {\\mathbf}{W}_i {\\mathbf}{\\ddot{R}}_i \\right] {\\mathbf}{M_{\\ddot{R}}} \\\\\n&= {\\text{Var}}\\left({\\boldsymbol}{\\hat\\beta}\\right)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\text{E}}\\left({\\mathbf{}}{V}^{CR2}\\right)\" display=\"inline\"><mrow><mtext>E</mtext><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>V</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mn>2</mn></mrow></msup><mo>)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{M_{\\ddot{R}}}\\left[\\sum_{i=1}^{m}{\\mathbf{}}{\\ddot{R%&#10;}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{A}_{i}\\left({\\mathbf{}}{I}-{%&#10;\\mathbf{}}{H_{X}}\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}%&#10;}{H_{X}}\\right)_{i}^{\\prime}{\\mathbf{}}{A}_{i}{\\mathbf{}}{W}_{i}{\\mathbf{}}{%&#10;\\ddot{R}}_{i}\\right]{\\mathbf{}}{M_{\\ddot{R}}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{M_{\\ddot{R}}}\\left[\\sum_{i=1}^{m}{\\mathbf{}}{\\ddot{R%&#10;}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\boldsymbol{}}\\Phi_{i}{\\mathbf{}}{W}_{i}{%&#10;\\mathbf{}}{\\ddot{R}}_{i}\\right]{\\mathbf{}}{M_{\\ddot{R}}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><mrow><mo>[</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msubsup><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover><mi>i</mi></msub></mrow></mrow><mo>]</mo></mrow><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>R</mi><mo>\u00a8</mo></mover></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\text{Var}}\\left({\\boldsymbol{}}{\\hat{\\beta}}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>Var</mtext><mo>\u2062</mo><mrow><mo>(</mo><mover accent=\"true\"><mi>\u03b2</mi><mo stretchy=\"false\">^</mo></mover><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nand \n\n", "itemtype": "equation", "pos": 101456, "prevtext": "\n\n\\subsection{Proof of Theorem \\ref{thm:absorb}}\n\nFrom the fact that ${\\mathbf}{\\ddot{U}}_i'{\\mathbf}{W}_i{\\mathbf}{T}_i = {\\mathbf}{0}$ for $i = 1,...,m$, it follows that \n", "index": 61, "text": "\\begin{align*}\n{\\mathbf}{B}_i &= {\\mathbf}{D}_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right) {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_T}\\right)' \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i' {\\mathbf}{D}_i'\\\\\n&= {\\mathbf}{D}_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}} - {\\mathbf}{H_T}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}} - {\\mathbf}{H_T}\\right)_i' {\\mathbf}{D}_i' \\\\\n&= {\\mathbf}{D}_i \\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i' - {\\mathbf}{T}_i {\\mathbf}{M_T}{\\mathbf}{T}_i'\\right){\\mathbf}{D}_i'\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{}}{B}_{i}\" display=\"inline\"><msub><mi>B</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{D}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{U}}}%&#10;\\right)_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{T}}\\right){\\boldsymbol{}}\\Phi%&#10;\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{T}}\\right)^{\\prime}\\left({\\mathbf{}}{I}-{%&#10;\\mathbf{}}{H_{\\ddot{U}}}\\right)_{i}^{\\prime}{\\mathbf{}}{D}_{i}^{\\prime}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>T</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mi>T</mi></msub></mrow><mo>)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{D}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_{\\ddot{U}}}%&#10;-{\\mathbf{}}{H_{T}}\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf%&#10;{}}{H_{\\ddot{U}}}-{\\mathbf{}}{H_{T}}\\right)_{i}^{\\prime}{\\mathbf{}}{D}_{i}^{\\prime}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>-</mo><msub><mi>H</mi><mi>T</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>-</mo><msub><mi>H</mi><mi>T</mi></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbf{}}{D}_{i}\\left({\\boldsymbol{}}\\Phi_{i}-{\\mathbf{}}{%&#10;\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}}}{\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}-{%&#10;\\mathbf{}}{T}_{i}{\\mathbf{}}{M_{T}}{\\mathbf{}}{T}_{i}^{\\prime}\\right){\\mathbf{%&#10;}}{D}_{i}^{\\prime}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>-</mo><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nLet ${\\boldsymbol}\\Psi_i = \\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i'\\right)^+$.\nUsing a generalized Woodbury identity \\citep{Henderson1981on},\n", "itemtype": "equation", "pos": 102135, "prevtext": "\nand \n\n", "index": 63, "text": "\\begin{equation}\n\\label{eq:B_i_inverse}\n{\\mathbf}{B}_i^+ = \\left({\\mathbf}{D}_i'\\right)^{-1} \\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i' - {\\mathbf}{T}_i {\\mathbf}{M_T}{\\mathbf}{T}_i'\\right)^+ {\\mathbf}{D}_i^{-1}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{B}_{i}^{+}=\\left({\\mathbf{}}{D}_{i}^{\\prime}\\right)^{-1}\\left({%&#10;\\boldsymbol{}}\\Phi_{i}-{\\mathbf{}}{\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}}}{%&#10;\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}-{\\mathbf{}}{T}_{i}{\\mathbf{}}{M_{T}}{\\mathbf%&#10;{}}{T}_{i}^{\\prime}\\right)^{+}{\\mathbf{}}{D}_{i}^{-1}.\" display=\"block\"><mrow><mrow><msubsup><mi>B</mi><mi>i</mi><mo>+</mo></msubsup><mo>=</mo><mrow><msup><mrow><mo>(</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>-</mo><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nIt follows that ${\\boldsymbol}\\Psi_i {\\mathbf}{T}_i = {\\mathbf}{W}_i {\\mathbf}{T}_i$. \nAnother application of the generalized Woodbury identity gives \n\n", "itemtype": "equation", "pos": 102613, "prevtext": "\nLet ${\\boldsymbol}\\Psi_i = \\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i'\\right)^+$.\nUsing a generalized Woodbury identity \\citep{Henderson1981on},\n", "index": 65, "text": "\\[\n{\\boldsymbol}\\Psi_i = {\\mathbf}{W}_i + {\\mathbf}{W}_i {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}\\left({\\mathbf}{M_{\\ddot{U}}} - {\\mathbf}{M_{\\ddot{U}}} {\\mathbf}{\\ddot{U}}_i' {\\mathbf}{W}_i {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}\\right)^+ {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i'{\\mathbf}{W}_i. \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{}}\\Psi_{i}={\\mathbf{}}{W}_{i}+{\\mathbf{}}{W}_{i}{\\mathbf{}}{\\ddot%&#10;{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}}}\\left({\\mathbf{}}{M_{\\ddot{U}}}-{\\mathbf{}}{M%&#10;_{\\ddot{U}}}{\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{%&#10;\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}}}\\right)^{+}{\\mathbf{}}{M_{\\ddot{U}}}{%&#10;\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}{\\mathbf{}}{W}_{i}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>-</mo><mrow><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nThe last equality follows from the fact that ${\\mathbf}{T}_i {\\mathbf}{M_T}\\left({\\mathbf}{M_T} - {\\mathbf}{M_T}{\\mathbf}{T}_i' {\\mathbf}{W}_i {\\mathbf}{T}_i{\\mathbf}{M_T}\\right)^{-} {\\mathbf}{M_T} {\\mathbf}{T}_i' = {\\mathbf}{0}$ because the fixed effects are nested within clusters. \nSubstituting into (\\ref{eq:B_i_inverse}), we then have that ${\\mathbf}{B}_i^+ = \\left({\\mathbf}{D}_i'\\right)^{-1} {\\boldsymbol}\\Psi_i {\\mathbf}{D}_i^{-1}$. \nBut\n", "itemtype": "equation", "pos": 103079, "prevtext": "\nIt follows that ${\\boldsymbol}\\Psi_i {\\mathbf}{T}_i = {\\mathbf}{W}_i {\\mathbf}{T}_i$. \nAnother application of the generalized Woodbury identity gives \n\n", "index": 67, "text": "\\begin{align*}\n\\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i {\\mathbf}{M_{\\ddot{U}}}{\\mathbf}{\\ddot{U}}_i' - {\\mathbf}{T}_i {\\mathbf}{M_T}{\\mathbf}{T}_i'\\right)^+ &= {\\boldsymbol}\\Psi_i + {\\boldsymbol}\\Psi_i {\\mathbf}{T}_i {\\mathbf}{M_T}\\left({\\mathbf}{M_T} - {\\mathbf}{M_T}{\\mathbf}{T}_i' {\\boldsymbol}\\Psi_i {\\mathbf}{T}_i {\\mathbf}{M_T}\\right)^+ {\\mathbf}{M_T} {\\mathbf}{T}_i' {\\boldsymbol}\\Psi_i \\\\\n&= {\\boldsymbol}\\Psi_i + {\\mathbf}{W}_i {\\mathbf}{T}_i {\\mathbf}{M_T}\\left({\\mathbf}{M_T} - {\\mathbf}{M_T}{\\mathbf}{T}_i' {\\mathbf}{W}_i {\\mathbf}{T}_i{\\mathbf}{M_T}\\right)^+ {\\mathbf}{M_T} {\\mathbf}{T}_i' {\\mathbf}{W}_i \\\\\n&= {\\boldsymbol}\\Psi_i.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left({\\boldsymbol{}}\\Phi_{i}-{\\mathbf{}}{\\ddot{U}}_{i}{\\mathbf{}%&#10;}{M_{\\ddot{U}}}{\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}-{\\mathbf{}}{T}_{i}{\\mathbf{}%&#10;}{M_{T}}{\\mathbf{}}{T}_{i}^{\\prime}\\right)^{+}\" display=\"inline\"><msup><mrow><mo>(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>-</mo><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\boldsymbol{}}\\Psi_{i}+{\\boldsymbol{}}\\Psi_{i}{\\mathbf{}}{T}_{i%&#10;}{\\mathbf{}}{M_{T}}\\left({\\mathbf{}}{M_{T}}-{\\mathbf{}}{M_{T}}{\\mathbf{}}{T}_{%&#10;i}^{\\prime}{\\boldsymbol{}}\\Psi_{i}{\\mathbf{}}{T}_{i}{\\mathbf{}}{M_{T}}\\right)^%&#10;{+}{\\mathbf{}}{M_{T}}{\\mathbf{}}{T}_{i}^{\\prime}{\\boldsymbol{}}\\Psi_{i}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>-</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\boldsymbol{}}\\Psi_{i}+{\\mathbf{}}{W}_{i}{\\mathbf{}}{T}_{i}{%&#10;\\mathbf{}}{M_{T}}\\left({\\mathbf{}}{M_{T}}-{\\mathbf{}}{M_{T}}{\\mathbf{}}{T}_{i}%&#10;^{\\prime}{\\mathbf{}}{W}_{i}{\\mathbf{}}{T}_{i}{\\mathbf{}}{M_{T}}\\right)^{+}{%&#10;\\mathbf{}}{M_{T}}{\\mathbf{}}{T}_{i}^{\\prime}{\\mathbf{}}{W}_{i}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>-</mo><mrow><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup><mo>\u2062</mo><msub><mi>M</mi><mi>T</mi></msub><mo>\u2062</mo><msubsup><mi>T</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>i</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\boldsymbol{}}\\Psi_{i}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01981.tex", "nexttext": "\nand so ${\\mathbf}{B}_i^+ = {\\mathbf}{\\tilde{B}}_i^+$. It follows that ${\\mathbf}{A}_i = {\\mathbf}{\\tilde{A}}_i$ for $i = 1,...,m$. \n\n\\section{Details of simulation study}\n\\label{app:simulations}\n\nThis appendix provides further details regarding the design of the simulations reported in Section \\ref{sec:simulation}. The simulations examined six distinct study designs. Outcomes are measured for $n$ units (which may be individuals, as in a cluster-randomized or block-randomized design, or time-points, as in a difference-in-differences panel) in each of $m$ clusters under one of three treatment conditions. Suppose that there are $G$ groups of units that share an identical pattern of treatment assignments, each of size $m_g$. Let $n_{ghi}$ denote the number of units at which cluster $i$ in group $g$ is observed under condition $h$, for $i=1,...,m$, $g = 1,...,G$, and $h = 1,2,3$. The following six designs were simulated:\n\\begin{enumerate}\n\\item A balanced, block-randomized design, with an un-equal allocation within each block. In the balanced design, the treatment allocation is identical for each block, with $G = 1$, $m_1 = m$, $n_{11i} = n / 2$, $n_{12i} = n / 3$, and $n_{13i} = n / 6$.\n\\item An unbalanced, block-randomized design, with two different patterns of treatment allocation. Here, $G = 2$,  $m_1 = m_2 = m / 2$, $n_{11i} = n / 2$, $n_{12i} = n / 3$, $n_{13i} = n / 6$, $n_{21i} = n / 3$, $n_{22i} = 5 n / 9$, and $n_{23i} = n / 9$.\n\\item A balanced, cluster-randomized design, in which units are nested within clusters and an equal number of clusters are assigned to each treatment condition. Here, $G = 3$, $m_g = m / 3$, and $n_{ghi} = n$ for $g = h$ and zero otherwise.\n\\item An unbalanced, cluster-randomized design, in which units are nested within clusters but the number of clusters assigned to each condition is not equal. Here, $G = 3$; $m_1 = 0.5 m, m_2 = 0.3 m, m_3 = 0.2 m$; and $n_{ghi} = n$ for $g = h$ and zero otherwise. \n\\item A balanced difference-in-differences design, with two patterns of treatment allocation ($G = 2$) and clusters allocated equally to each pattern ($m_1 = m_2 = m / 2$). Here, half of the clusters are observed under the first treatment condition only ($n_{11i} = n$) and the remaining half are observed under all three conditions, with $n_{21i} = n / 2$, $n_{22i} = n / 3$, and $n_{23i} = n / 6$.\n\\item An unbalanced difference-in-differences design, again with two patterns of treatment allocation ($G = 2$), but where $m_1 = 2 m / 3$ clusters are observed under the first treatment condition only ($n_{11i} = n$) and the remaining $m_2 = m / 3$ clusters are observed under all three conditions, with  $n_{21i} = n / 2$, $n_{22i} = n / 3$, and $n_{23i} = n / 6$.\n\\end{enumerate}\n\n\\bibliographystyle{agsm}\n\\bibliography{bibliography}\n\n\n", "itemtype": "equation", "pos": 104193, "prevtext": "\nThe last equality follows from the fact that ${\\mathbf}{T}_i {\\mathbf}{M_T}\\left({\\mathbf}{M_T} - {\\mathbf}{M_T}{\\mathbf}{T}_i' {\\mathbf}{W}_i {\\mathbf}{T}_i{\\mathbf}{M_T}\\right)^{-} {\\mathbf}{M_T} {\\mathbf}{T}_i' = {\\mathbf}{0}$ because the fixed effects are nested within clusters. \nSubstituting into (\\ref{eq:B_i_inverse}), we then have that ${\\mathbf}{B}_i^+ = \\left({\\mathbf}{D}_i'\\right)^{-1} {\\boldsymbol}\\Psi_i {\\mathbf}{D}_i^{-1}$. \nBut\n", "index": 69, "text": "\\[\n{\\mathbf}{\\tilde{B}}_i = {\\mathbf}{D}_i \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i {\\boldsymbol}\\Phi \\left({\\mathbf}{I} - {\\mathbf}{H_{\\ddot{U}}}\\right)_i' {\\mathbf}{D}_i' = {\\mathbf}{D}_i \\left({\\boldsymbol}\\Phi_i - {\\mathbf}{\\ddot{U}}_i{\\mathbf}{M_{\\ddot{U}}} {\\mathbf}{\\ddot{U}}_i'\\right) {\\mathbf}{D}_i' = {\\mathbf}{D}_i {\\boldsymbol}\\Psi_i^+ {\\mathbf}{D}_i',\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{}}{\\tilde{B}}_{i}={\\mathbf{}}{D}_{i}\\left({\\mathbf{}}{I}-{\\mathbf{}}{%&#10;H_{\\ddot{U}}}\\right)_{i}{\\boldsymbol{}}\\Phi\\left({\\mathbf{}}{I}-{\\mathbf{}}{H_%&#10;{\\ddot{U}}}\\right)_{i}^{\\prime}{\\mathbf{}}{D}_{i}^{\\prime}={\\mathbf{}}{D}_{i}%&#10;\\left({\\boldsymbol{}}\\Phi_{i}-{\\mathbf{}}{\\ddot{U}}_{i}{\\mathbf{}}{M_{\\ddot{U}%&#10;}}{\\mathbf{}}{\\ddot{U}}_{i}^{\\prime}\\right){\\mathbf{}}{D}_{i}^{\\prime}={%&#10;\\mathbf{}}{D}_{i}{\\boldsymbol{}}\\Psi_{i}^{+}{\\mathbf{}}{D}_{i}^{\\prime},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>B</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><msubsup><mrow><mo>(</mo><mrow><mi>I</mi><mo>-</mo><msub><mi>H</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub></mrow><mo>)</mo></mrow><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi></msub><mo>\u2062</mo><msub><mi>M</mi><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo>\u00a8</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>=</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi><mo>+</mo></msubsup><mo>\u2062</mo><msubsup><mi>D</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]