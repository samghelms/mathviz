[{"file": "1601.03474.tex", "nexttext": "\nwith some noise $\\epsilon_i$ and unknown mean function $h$ that has to be estimated. In the well known local polynomial or kernel regression frameworks \\cite{belkin.2006,fan.1996,oztireli.2009}, \na univariate kernel $G(p)$ is introduced to minimize the weighted least squares of the form\n\\begin{eqnarray} \\widehat{h}(p) =  \\arg \\min_{\\beta_0 \\cdots \\beta_k} \\sum_{i=1}^n G(p-p_i) \\Big| f_i - \\sum_{j=0}^k \\beta_j (p-p_i)^j \\Big|^2. \\label{eq:localpolynomial2}\\end{eqnarray}\nOften Gaussian kernels are used for $G$. \nIn many related local polynomial or kernel regression frameworks, kernel $G$ and polynomial basis $p^j$ are translated by the amount of $p_i$ in fitting the data locally. In this fashion, at each data point $p_i$, exactly the same shape of kernel and distance are used. However, one immediately encounters a difficulty of directly generalizing the Euclidean formulation (\\ref{eq:localpolynomial2}) to an arbitrary surface since it is unclear how to translate the kernel and basis in a coherent fashion.  \n\n\n\nA similar problem is also encountered in wavelets in a Euclidean space. Consider a wavelet basis $W_{t,q}(p)$ obtained from a mother wavelet $W$ with scale and translation parameters $t$ and $q$:\n\\begin{eqnarray} W_{t, q}(p) = \\frac{1}{t}W \\big(\\frac{p-q}{t} \\big). \\label{eq:Wtq} \\end{eqnarray}\nScaling a function on a surface is trivial. But the difficulty arises when one tries to define a mother wavelet and translate it on a surface. It is not straightforward to generalize the Euclidean formulation (\\ref{eq:Wtq}) to an arbitrary manifold. \n\n\nTo remedy these two different but related problems, we propose to use a bivariate kernel and bypass the problem of translating a univariate kernel. By simply changing the second argument, it has the effect of translating the kernel. \n\n\\section{Methods}\n\nIn many anatomical studies in CT, measurements are sampled densely at the resolution 0.5mm or less so it is more practical to model the measurements as functions. Consider a functional measurement $f$ defined on a manifold $\\mathcal{M} \\subset \\mathbb{R}^d$. We assume the following additive model:\n\\begin{eqnarray} f(p) =  h(p) + \\epsilon(p), \\label{eq:model1} \\end{eqnarray}\nwhere $h$ is the unknown signal and $\\epsilon$ is a zero-mean random field, possibly Gaussian. The manifold $\\mathcal{M}$ can be a single connected or multiple disjoint components as our hyoid bone application. We further assume $f \\in L^2(\\mathcal{M})$, the space of square integrable functions on $\\mathcal{M}$ with the inner product \n\n", "itemtype": "equation", "pos": 6486, "prevtext": "\n\n\n\\title{A Unified Statistical Approach to Modeling\\\\\n the Shape of Human Hyoid Bones in CT Images}\n \n \n\\author{Moo K. Chung$^{1,2,3}$, \n\n\n Nagesh Adluru$^{3}$, Houri K. Vorperian$^{2}$\\\\\n$^1$Department of Biostatistics and Medical Informatics\\\\\n$^2$Vocal Tract Laboratory, Waisman Center\\\\\n$^3$Waisman Laboratory for Brain Imaging and Behavior\\\\\nUniversity of Wisconsin-Madison, USA\n\n\n\n\n\n\n\\\\\n\\vspace{0.3cm}\n{\\textcolor{red}{{\\tt mkchung@wisc.edu}}}\n}\n\n\\maketitle\n\n\n\n\n\n\\pagenumbering{arabic}\n\n\n\\begin{abstract}\nWe present a unified statistical approach to modeling 3D anatomical objects extracted from medical images. \nDue to image acquisition and preprocessing noises, it is expected the imaging data is noisy. \nUsing the Laplace-Beltrami (LB) eigenfunctions, we smooth out noisy data and perform statistical analysis.\nThe method is applied in characterizing the 3D growth pattern of human hyoid bone between ages 0 and 20 obtained from computed tomography (CT). We detected a significant age effect on localized parts of the hyoid bone.\n\\end{abstract}\n\n\n\\section{Introduction}\nFor normally developing children, age and gender could be major factors that affect the functions and structures of growing hyoid bone. As in other developmental studies \\cite{chung.2003.ni,vorperian.2011}, we expect highly localized complex growth pattern to emerge between ages 0 and 20 in the hyoid bone. It is expected the growth to be outward with respect to the surface of the bone. However, it is unclear what specific parts of the hyoid bone are growing. This provides a biological motivation for a need to develop a local surface-based morphometric technique beyond simple volumetric techniques that cannot detect localized subtle anatomical changes along the hyoid bone surface. \n\nThe end results of existing surface-based morphometric studies in medical imaging are statistical parametric maps (SPM) that shows statistical significance of growth at each surface mesh vertex \\cite{chung.2003.ni,qiu.2008,xu.2008}. In order to obtain stable and robust SPM, various signal smoothing and filtering methods have been proposed. Among them, diffusion equations, kernel smoothing, and wavelet-based approaches are probably most popular. Diffusion equations have been widely used in image processing as a form of noise reduction starting with Perona and Malik in 1990's \\cite{perona.1990}. Although numerous techniques have been developed for performing diffusion along surfaces\n\\cite{chung.2003.cvpr,andrade.2001,tang.1999,sochen.1998,malladi.2002,taubin.2000},  \nmost  approaches are nonparametric and requires finite element or finite difference schemes which are known to suffer various numerical instabilities \\cite{chung.2005.IPMI}. \n\nKernel smoothing based models have been also proposed for surface and manifolds data \\cite{belkin.2006,chung.2005.IPMI}. The kernel methods basically smooth data as weighted average of neighboring mesh vertices using mostly a Gaussian kernel and its iterative application is supposed to approximates the diffusion process. Recently, wavelets have been popularized for surface and graph data. Spherical wavelets have been used on brain surface data that has been mapped onto a sphere  \\cite{nain.2007,bernal.2008}.\nSince wavelet basis functions have local supports in both space and scale, the\nwavelet coefficients from the scale-space decomposition using the spherical wavelets provides shape features that describe local shape variation at a variety of scales and spatial locations. \nHowever, spherical wavelets have an intrinsic problem that they require to establish a smooth mapping from the surface to a unit sphere, which introduces a serious metric distortion. \nThe spherical mapping such as conformal mapping introduces serious metric distortion which usually compounds SPM. Furthermore, such basis functions defined on sphere seem to be suboptimal rather than those directly defined on anatomical surface, in detecting locations or scales of shape variations.  \nTo remedy the limitation of spherical wavelets, spectral graph wavelet transform defined on a graph has been applied to arbitrary surface meshes by treating surface meshes as graphs \\cite{antoine.2010,hammond.2011,kim.2012.NIPS}. Wavelet transform is a powerful tool decomposing a signal or function into a collection of components localized at both location and scale. \nAlthough all three methods (diffusion-, kernel- and wavelet-based) look different from each other, it is possible to develop a unified framework that relates all of them in a coherent statistical fashion.  \n\nStarting with a symmetric positive definite kernel, we propose a novel unified kernel regression framework within the Hilbert space theory. The proposed kernel regression works for any symmetric positive definite kernel, which behaves like weights between two functional data. We show how this facilitates a coherent statistical inference for functional signals defined on an arbitrary manifold. \nThe outline of this paper is as follows.\n(i) First, we present a unified bivariate kernel regression that is related to diffusion-like equations. \n(ii) We establish the relationship between the kernel regression and recently popular spectral graph wavelets for manifolds. The proposed kernel regression is shown to be equivalent to the wavelet coefficients. This mathematical equivalence levitates a need for constructing wavelets using a complicated computational machinery as often done in previous diffusion wavelet constructions  \\cite{antoine.2010,hammond.2011,kim.2012.NIPS}. (iii) A unified statistical inference framework is developed for neuroimaging applications by linking the kernel regression to the random field theory \\cite{taylor.2007,worsley.2004}. \n(iv) Subsequently, we illustrate how the kernel regression procedure can be used to localize the hyoid bone growth pattern in human. \n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{hyoid-CT.pdf}\n\\caption{CT image showing the location of the hyoid bone and 3D model showing the relative location of the hyoid bone with respect to the mandible (gray) and vocal tract structures (green).}\n\\label{fig:hyoidbone}\n\\end{center} \n\\end{figure}\n\n\n\\section{Preliminary}\nFirst, let us illustrate two statistical problems in an Euclidean space that motivate the development of the proposed kernel regression in manifolds.  \n\nConsider measurements $f_i$ sampled at $p_i \\in \\mathbb{R}^d$. The measurements are usually modeled as  \n\n", "index": 1, "text": "$$f_i = h(p_i) + \\epsilon_i$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"f_{i}=h(p_{i})+\\epsilon_{i}\" display=\"block\"><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>\u03f5</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": " \nwhere $\\mu$ is the Lebesgue measure. \nDefine a self-adjoint operator $\\mathcal{L}$ satisfying\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwith some noise $\\epsilon_i$ and unknown mean function $h$ that has to be estimated. In the well known local polynomial or kernel regression frameworks \\cite{belkin.2006,fan.1996,oztireli.2009}, \na univariate kernel $G(p)$ is introduced to minimize the weighted least squares of the form\n\\begin{eqnarray} \\widehat{h}(p) =  \\arg \\min_{\\beta_0 \\cdots \\beta_k} \\sum_{i=1}^n G(p-p_i) \\Big| f_i - \\sum_{j=0}^k \\beta_j (p-p_i)^j \\Big|^2. \\label{eq:localpolynomial2}\\end{eqnarray}\nOften Gaussian kernels are used for $G$. \nIn many related local polynomial or kernel regression frameworks, kernel $G$ and polynomial basis $p^j$ are translated by the amount of $p_i$ in fitting the data locally. In this fashion, at each data point $p_i$, exactly the same shape of kernel and distance are used. However, one immediately encounters a difficulty of directly generalizing the Euclidean formulation (\\ref{eq:localpolynomial2}) to an arbitrary surface since it is unclear how to translate the kernel and basis in a coherent fashion.  \n\n\n\nA similar problem is also encountered in wavelets in a Euclidean space. Consider a wavelet basis $W_{t,q}(p)$ obtained from a mother wavelet $W$ with scale and translation parameters $t$ and $q$:\n\\begin{eqnarray} W_{t, q}(p) = \\frac{1}{t}W \\big(\\frac{p-q}{t} \\big). \\label{eq:Wtq} \\end{eqnarray}\nScaling a function on a surface is trivial. But the difficulty arises when one tries to define a mother wavelet and translate it on a surface. It is not straightforward to generalize the Euclidean formulation (\\ref{eq:Wtq}) to an arbitrary manifold. \n\n\nTo remedy these two different but related problems, we propose to use a bivariate kernel and bypass the problem of translating a univariate kernel. By simply changing the second argument, it has the effect of translating the kernel. \n\n\\section{Methods}\n\nIn many anatomical studies in CT, measurements are sampled densely at the resolution 0.5mm or less so it is more practical to model the measurements as functions. Consider a functional measurement $f$ defined on a manifold $\\mathcal{M} \\subset \\mathbb{R}^d$. We assume the following additive model:\n\\begin{eqnarray} f(p) =  h(p) + \\epsilon(p), \\label{eq:model1} \\end{eqnarray}\nwhere $h$ is the unknown signal and $\\epsilon$ is a zero-mean random field, possibly Gaussian. The manifold $\\mathcal{M}$ can be a single connected or multiple disjoint components as our hyoid bone application. We further assume $f \\in L^2(\\mathcal{M})$, the space of square integrable functions on $\\mathcal{M}$ with the inner product \n\n", "index": 3, "text": "$$\\langle f,g \\rangle = \\int _{\\mathcal{M}} f(p)g(p) \\;d\\mu(p),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\langle f,g\\rangle=\\int_{\\mathcal{M}}f(p)g(p)\\;d\\mu(p),\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mi>f</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": " \nfor all $g_1, g_2 \\in\nL^2(\\mathcal{M})$. Then $\\mathcal{L}$ induces the eigenvalues $\\lambda_j$ and\neigenfunctions $\\psi_j$ on $\\mathcal{M}$:\n\\begin{eqnarray} \\mathcal{L} \\psi_j = \\lambda_j \\psi_j. \\label{eq:eigen}\\end{eqnarray} \nWithout loss of generality, we can order the\neigenvalues $0 = \\lambda_0 \\leq \\lambda_1 \\leq \\lambda_2 \\leq\n\\cdots.$\nThe eigenfunctions $\\psi_j$ form an orthonormal basis in $L^2(\\mathcal{M})$.\nThen from Mercer's theorem \\cite{conway.1990}, any smooth symmetric positive definite kernel on manifold $\\mathcal{M}$ can be written as\n\\begin{eqnarray} K(p,q) = \\sum_{j=0}^{\\infty} \\tau_j \\psi_{j}(p)\\psi_{j}(q) \\label{eq:Kpq11}\\end{eqnarray}\nfor some $\\tau_j$. \nThe constants $\\tau_j$ are identified as follows. Apply the kernel convolution on the eigenfunction $\\psi_j$:\n\\begin{eqnarray} K*\\psi_j(p) = \\int_{\\mathcal{M}} K(p,q) \\psi_j(q)\\; d\\mu(q). \n\\label{eq:Kf1} \n\\end{eqnarray}\nSubstituting (\\ref{eq:Kpq11}) into (\\ref{eq:Kf1}), we have\n$K*\\psi_j(p) = \\tau_j \\psi_j(p)$\nindicating $\\tau_j$  and $\\psi_j$ must be the eigenvalues and eigenfunctions of the convolution (\\ref{eq:Kf1}).\nHowever, the relationship between the two different sets of eigenvalues $\\lambda_j$ and $\\tau_j$ is not yet given but will be shown shortly afterward. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{hyoid-LBbasis.pdf}  \n\\caption{Laplace-Beltrami eigenfunctions  $\\psi_j$ of various degrees $(j=0, 1, 5, 20, 100, 500)$ on the template. The first eigenfunction is constant in each component. As the degree increases, the spatial frequency increases.}\n\\label{fig:2_md_eigfs}\n\\end{figure} \n\n\n\n\n\n\n\n\n\n\n \n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{hyoid-sigma.pdf}\n\\caption{\\footnotesize  Heat kernel regression with different bandwidth between 0.1 and 1000. As the bandwidth increases, the kernel regression becomes inversely proportional to the square root of the surface area.}\n\\label{fig:wavelets}\n\\end{center} \n\\end{figure}\n\n\n\\subsection{Proposed kernel regression}\nConsider subspace $\\mathcal{H}_k \\subset L^2(\\mathcal{M})$ spanned by the orthonormal basis $\\{\\psi_j \\}$, i.e. \n", "itemtype": "equation", "pos": -1, "prevtext": " \nwhere $\\mu$ is the Lebesgue measure. \nDefine a self-adjoint operator $\\mathcal{L}$ satisfying\n\n", "index": 5, "text": "$$\\langle g_1, \\mathcal{L} g_2 \\rangle = \\langle\n\\mathcal{L}g_1, g_2 \\rangle$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\langle g_{1},\\mathcal{L}g_{2}\\rangle=\\langle\\mathcal{L}g_{1},g_{2}\\rangle\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><msub><mi>g</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><msub><mi>g</mi><mn>1</mn></msub></mrow><mo>,</mo><msub><mi>g</mi><mn>2</mn></msub><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nWe estimate $h$ by minimizing the weighted distance to the space\n\n$\\mathcal{H}_k$:\n\n\\begin{eqnarray} \\widehat{h}(p) = \\arg\\min_{h \\in \\mathcal{H}_k} \\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p). \\label{eq:HKregression}\\end{eqnarray}\nWithout loss of generality, we will assume the kernel to be a probability distribution so that\n\n", "itemtype": "equation", "pos": 11433, "prevtext": " \nfor all $g_1, g_2 \\in\nL^2(\\mathcal{M})$. Then $\\mathcal{L}$ induces the eigenvalues $\\lambda_j$ and\neigenfunctions $\\psi_j$ on $\\mathcal{M}$:\n\\begin{eqnarray} \\mathcal{L} \\psi_j = \\lambda_j \\psi_j. \\label{eq:eigen}\\end{eqnarray} \nWithout loss of generality, we can order the\neigenvalues $0 = \\lambda_0 \\leq \\lambda_1 \\leq \\lambda_2 \\leq\n\\cdots.$\nThe eigenfunctions $\\psi_j$ form an orthonormal basis in $L^2(\\mathcal{M})$.\nThen from Mercer's theorem \\cite{conway.1990}, any smooth symmetric positive definite kernel on manifold $\\mathcal{M}$ can be written as\n\\begin{eqnarray} K(p,q) = \\sum_{j=0}^{\\infty} \\tau_j \\psi_{j}(p)\\psi_{j}(q) \\label{eq:Kpq11}\\end{eqnarray}\nfor some $\\tau_j$. \nThe constants $\\tau_j$ are identified as follows. Apply the kernel convolution on the eigenfunction $\\psi_j$:\n\\begin{eqnarray} K*\\psi_j(p) = \\int_{\\mathcal{M}} K(p,q) \\psi_j(q)\\; d\\mu(q). \n\\label{eq:Kf1} \n\\end{eqnarray}\nSubstituting (\\ref{eq:Kpq11}) into (\\ref{eq:Kf1}), we have\n$K*\\psi_j(p) = \\tau_j \\psi_j(p)$\nindicating $\\tau_j$  and $\\psi_j$ must be the eigenvalues and eigenfunctions of the convolution (\\ref{eq:Kf1}).\nHowever, the relationship between the two different sets of eigenvalues $\\lambda_j$ and $\\tau_j$ is not yet given but will be shown shortly afterward. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{hyoid-LBbasis.pdf}  \n\\caption{Laplace-Beltrami eigenfunctions  $\\psi_j$ of various degrees $(j=0, 1, 5, 20, 100, 500)$ on the template. The first eigenfunction is constant in each component. As the degree increases, the spatial frequency increases.}\n\\label{fig:2_md_eigfs}\n\\end{figure} \n\n\n\n\n\n\n\n\n\n\n \n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{hyoid-sigma.pdf}\n\\caption{\\footnotesize  Heat kernel regression with different bandwidth between 0.1 and 1000. As the bandwidth increases, the kernel regression becomes inversely proportional to the square root of the surface area.}\n\\label{fig:wavelets}\n\\end{center} \n\\end{figure}\n\n\n\\subsection{Proposed kernel regression}\nConsider subspace $\\mathcal{H}_k \\subset L^2(\\mathcal{M})$ spanned by the orthonormal basis $\\{\\psi_j \\}$, i.e. \n", "index": 7, "text": "$$\\mathcal{H}_k =\\{ \\sum_{j=0}^k \\beta_j \\psi_j(p): \\beta_j \\in\n\\mathbb{R}\\}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{H}_{k}=\\{\\sum_{j=0}^{k}\\beta_{j}\\psi_{j}(p):\\beta_{j}\\in\\mathbb{R}\\}.\" display=\"block\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03b2</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><msub><mi>\u03b2</mi><mi>j</mi></msub><mo>\u2208</mo><mi>\u211d</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nfor all $p \\in \\mathcal{M}$. The solution of (\\ref{eq:HKregression}) has the following analytic expression:\n\n\n\n\\begin{theorem}\n\\begin{eqnarray*} \\widehat{h}(p)  = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p) = \\sum_{j=0}^k  \\tau_j f_j \\psi_j,\n\\end{eqnarray*}\n\\label{theorem:kernel1}\nwhere $f_j = \\langle f, \\psi_j \\rangle$ are Fourier coefficients. \n\\end{theorem}\n{\\em Proof.} See the Appendix. Theorem \\ref{theorem:kernel1} generalizes the spherical regression on a unit sphere to an arbitrary manifold \\cite{chung.2008.sinica}. Theorem \\ref{theorem:kernel1} implies that the kernel regression can be performed by simply computing the Fourier coefficients $f_j = \\langle f, \\psi_j \\rangle$ without doing any numerical optimization. The numerically difficult optimization problem is reduced to the problem of computing Fourier coefficients. \nIf the kernel $K$ is a Dirac-delta function, the kernel regression simply collapses to the least squares estimation (LSE) which results in the standard Fourier series, i.e.\n\n", "itemtype": "equation", "pos": 11887, "prevtext": "\nWe estimate $h$ by minimizing the weighted distance to the space\n\n$\\mathcal{H}_k$:\n\n\\begin{eqnarray} \\widehat{h}(p) = \\arg\\min_{h \\in \\mathcal{H}_k} \\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p). \\label{eq:HKregression}\\end{eqnarray}\nWithout loss of generality, we will assume the kernel to be a probability distribution so that\n\n", "index": 9, "text": "$$\\int_{\\mathcal{M}} K(p,q) \\; d\\mu(q) = 1$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\int_{\\mathcal{M}}K(p,q)\\;d\\mu(q)=1\" display=\"block\"><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nIt can be also shown that as $k \\to \\infty$, the kernel regression \n", "itemtype": "equation", "pos": 13023, "prevtext": "\nfor all $p \\in \\mathcal{M}$. The solution of (\\ref{eq:HKregression}) has the following analytic expression:\n\n\n\n\\begin{theorem}\n\\begin{eqnarray*} \\widehat{h}(p)  = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p) = \\sum_{j=0}^k  \\tau_j f_j \\psi_j,\n\\end{eqnarray*}\n\\label{theorem:kernel1}\nwhere $f_j = \\langle f, \\psi_j \\rangle$ are Fourier coefficients. \n\\end{theorem}\n{\\em Proof.} See the Appendix. Theorem \\ref{theorem:kernel1} generalizes the spherical regression on a unit sphere to an arbitrary manifold \\cite{chung.2008.sinica}. Theorem \\ref{theorem:kernel1} implies that the kernel regression can be performed by simply computing the Fourier coefficients $f_j = \\langle f, \\psi_j \\rangle$ without doing any numerical optimization. The numerically difficult optimization problem is reduced to the problem of computing Fourier coefficients. \nIf the kernel $K$ is a Dirac-delta function, the kernel regression simply collapses to the least squares estimation (LSE) which results in the standard Fourier series, i.e.\n\n", "index": 11, "text": "$$\\widehat h(p) = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}} \\Big| f(q) - h(q) \\Big|^2 \\; d\\mu(q)  = \\sum_{j=0}^k  f_j \\psi_j.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\widehat{h}(p)=\\arg\\min_{h\\in\\mathcal{H}_{k}}\\int_{\\mathcal{M}}\\Big{|}f(q)-h(q%&#10;)\\Big{|}^{2}\\;d\\mu(q)=\\sum_{j=0}^{k}f_{j}\\psi_{j}.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>h</mi><mo>^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mrow><mi>h</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mi>k</mi></msub></mrow></munder></mrow><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mpadded width=\"+2.8pt\"><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">|</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mn>2</mn></msup></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": " converges to convolution $K*f$ establishing the connection to the manifold-based kernel smoothing framework \\cite{belkin.2002,chung.2005.IPMI}. Hence, asymptotically the proposed kernel regression should inherit many statistical properties of kernel smoothing. \n\n\n\n\\subsection{Properties}\nThe kernel regression can be shown to be related to the following diffusion-like Cauchy problem. \n\n\\begin{theorem}\\label{theorem:cauchy} For an arbitrary self-adjoint differential operator $\\mathcal{L}$, the unique solution of the following initial value problem\n\\begin{eqnarray} \\frac{\\partial g(p,t)}{\\partial t} + \\mathcal{L} g(p,t) =0 \\label{eq:cauchy1}, g(p,t=0) =f(p) \\label{eq:cauchy2}\n\\end{eqnarray}\nis given by\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} e^{-\\lambda_j t} f_j \\psi_j(p).\\label{eq:cauchy.solution}\\end{eqnarray}\n\\end{theorem}\n{\\em Proof.} See the Appendix. For a particular choice of kernel $K$ with $\\tau_j = e^{-\\lambda_j t}$, the proposed kernel regression $\\widehat{h}= \\sum_{j=0}^k \\tau_j f_j \\psi_j$ should converge to the solution of the diffusion-like equation. If we let $\\mathcal{L}$ be the Laplace-Beltrami operator, (\\ref{eq:cauchy2}) becomes an isotropic diffusion equation as a special case and we are then dealing with heat kernel \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nIt can be also shown that as $k \\to \\infty$, the kernel regression \n", "index": 13, "text": "$$\\widehat{h}= \\sum_{j=0}^k \\tau_j f_j \\psi_j$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\widehat{h}=\\sum_{j=0}^{k}\\tau_{j}f_{j}\\psi_{j}\" display=\"block\"><mrow><mover accent=\"true\"><mi>h</mi><mo>^</mo></mover><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>f</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhich is often explored mathematical objects in various fields \\cite{belkin.2002,chung.2005.IPMI}. \n\n\n\n\n\n\n\n\n\n\n\n\nIn order to construct wavelets on an arbitrary graph and mesh, diffusion wavelet transform has been proposed recently \\cite{antoine.2010,hammond.2011,kim.2012.NIPS}. The diffusion wavelet construction has been fairly involving so far.  However, it can be shown to be a special case of the proposed kernel regression and the proposed method is substantially simpler to construct. Following the notations in \\cite{antoine.2010,hammond.2011,kim.2012.NIPS},  \n diffusion wavelet $W_{t,p}(p)$ at position $p$ and scale $t$ is given by\n\n", "itemtype": "equation", "pos": 14535, "prevtext": " converges to convolution $K*f$ establishing the connection to the manifold-based kernel smoothing framework \\cite{belkin.2002,chung.2005.IPMI}. Hence, asymptotically the proposed kernel regression should inherit many statistical properties of kernel smoothing. \n\n\n\n\\subsection{Properties}\nThe kernel regression can be shown to be related to the following diffusion-like Cauchy problem. \n\n\\begin{theorem}\\label{theorem:cauchy} For an arbitrary self-adjoint differential operator $\\mathcal{L}$, the unique solution of the following initial value problem\n\\begin{eqnarray} \\frac{\\partial g(p,t)}{\\partial t} + \\mathcal{L} g(p,t) =0 \\label{eq:cauchy1}, g(p,t=0) =f(p) \\label{eq:cauchy2}\n\\end{eqnarray}\nis given by\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} e^{-\\lambda_j t} f_j \\psi_j(p).\\label{eq:cauchy.solution}\\end{eqnarray}\n\\end{theorem}\n{\\em Proof.} See the Appendix. For a particular choice of kernel $K$ with $\\tau_j = e^{-\\lambda_j t}$, the proposed kernel regression $\\widehat{h}= \\sum_{j=0}^k \\tau_j f_j \\psi_j$ should converge to the solution of the diffusion-like equation. If we let $\\mathcal{L}$ be the Laplace-Beltrami operator, (\\ref{eq:cauchy2}) becomes an isotropic diffusion equation as a special case and we are then dealing with heat kernel \n\n", "index": 15, "text": "$$H_t(p,q) = \\sum_{j=0}^{\\infty} e^{-\\lambda_j t} \\psi_j(p)\\psi_j(q),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"H_{t}(p,q)=\\sum_{j=0}^{\\infty}e^{-\\lambda_{j}t}\\psi_{j}(p)\\psi_{j}(q),\" display=\"block\"><mrow><mrow><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nfor some scale function $g$. If we let $\\tau_j = g(\\lambda_j t)$, the diffusion wavelet transform  \nis given by \n\n", "itemtype": "equation", "pos": 15249, "prevtext": "\nwhich is often explored mathematical objects in various fields \\cite{belkin.2002,chung.2005.IPMI}. \n\n\n\n\n\n\n\n\n\n\n\n\nIn order to construct wavelets on an arbitrary graph and mesh, diffusion wavelet transform has been proposed recently \\cite{antoine.2010,hammond.2011,kim.2012.NIPS}. The diffusion wavelet construction has been fairly involving so far.  However, it can be shown to be a special case of the proposed kernel regression and the proposed method is substantially simpler to construct. Following the notations in \\cite{antoine.2010,hammond.2011,kim.2012.NIPS},  \n diffusion wavelet $W_{t,p}(p)$ at position $p$ and scale $t$ is given by\n\n", "index": 17, "text": "$$W_{t,q}(p) = \\sum_{j=0}^k g(\\lambda_j t)\\psi_j(p)\\psi_j(q),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"W_{t,q}(p)=\\sum_{j=0}^{k}g(\\lambda_{j}t)\\psi_{j}(p)\\psi_{j}(q),\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": " \nwhich is the exactly kernel regression we introduced. Hence, the diffusion wavelet transform can be simply obtained by doing the kernel regression without an additional wavelet machinery \\cite{kim.2012.NIPS}. \nFurther, if we let $g(\\lambda_j t) = e^{-\\lambda_j t}$, we have \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nfor some scale function $g$. If we let $\\tau_j = g(\\lambda_j t)$, the diffusion wavelet transform  \nis given by \n\n", "index": 19, "text": "$$\\langle W_{t,p}, f \\rangle = \\int_{\\mathcal{M}} W_{t,q}(p)f(p) \\;d\\mu(p) = \\sum_{j=0}^k \\tau_j f_j \\psi_j(q),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\langle W_{t,p},f\\rangle=\\int_{\\mathcal{M}}W_{t,q}(p)f(p)\\;d\\mu(p)=\\sum_{j=0}^%&#10;{k}\\tau_{j}f_{j}\\psi_{j}(q),\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>W</mi><mrow><mi>t</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>,</mo><mi>f</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>f</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhich is a heat kernel. The bandwidth $t$ of heat kernel controls resolution while the translation is done by shifting one argument in the kernel. \n\n\n\\begin{figure}[t]\n \\centering\n\\includegraphics[width=1\\linewidth]{midus-gibbs.pdf}\n  \\caption{\\footnotesize The Gibbs phenomenon on a hat shaped simulated surface showing the ringing effect on the traditional Fourier series expansion (top) and the reduced effect on the heat kernel regression (bottom). 7225 basis functions were used for the both cases and the bandwidth $t = 0.001$ is used for the kernel regression.}\n\\label{fig:midus-gibbs}\n\\end{figure}\n\n\n\nAlthough the kernel regression is constructed using global basis functions $\\psi_j$, the kernel regression at each point $p$ coincides with the diffusion wavelet transform at that point. Hence, just like wavelets, the kernel regression will have the localization property of wavelets. This is demonstrated in a toy example in Figure \\ref{fig:midus-gibbs} with a hat-shaped surface in 3D. The hat-shaped step function is simulated as $z=1$ for $x^2 + y^2 < 1$ and $z=0$ for $1 \\leq  x^2 + y^2 \\leq 2$.  Then the step function is reconstructed using the Fourier series expansion via LSE (top) and kernel regression (bottom). In the both cases,  up to 7225 basis functions were used. For the kernel regression, the heat kernel with bandwidth $t=0.0001$ is used. LSE clearly shows the visible Gibbs phenomenon, i.e., ringing artifact \\cite{chung.2007.TMI,gelb.1997} compared to the kernel regression. \n\n\n\n\\subsection{Numerical Implementation}\nThe Laplace-Beltrami operator is chosen as the self-adjoint operators $\\mathcal{L}$ of choice. The eigenfunctions of the Laplace-Beltrami operator on an arbitrary curved surface is analytically unknown. So it is necessary to discretize (\\ref{eq:eigen}) using the Cotan formulation  as a generalized eigenvalue problem \\cite{\nzhang.2007,qiu.2006}:\n\n", "itemtype": "equation", "pos": 15816, "prevtext": " \nwhich is the exactly kernel regression we introduced. Hence, the diffusion wavelet transform can be simply obtained by doing the kernel regression without an additional wavelet machinery \\cite{kim.2012.NIPS}. \nFurther, if we let $g(\\lambda_j t) = e^{-\\lambda_j t}$, we have \n\n", "index": 21, "text": "$$W_{t,p}(q) = H_t(p,q),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"W_{t,p}(q)=H_{t}(p,q),\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\n\\noindent\nwhere $\\mathbf{C}$ is the stiffness matrix, $\\mathbf{A}$ is the mass matrix and $\\boldsymbol{\\psi}=(\\psi(p_1), \\cdots, \\psi(p_n))'$ is the eigenfunction evaluated at $n$ mesh vertices. \nOnce we obtained the basis functions $\\psi_j$, the corresponding Fourier coefficients $\\beta_j$ are estimated as\n\\begin{eqnarray*}\n\\label{eq:smoothing-FEMbeta}\n\t\\beta_j = {\\bf f}' {\\bf A} \\boldsymbol{\\psi}_j, \n\\end{eqnarray*}\nwhere \n${\\bf f} = (f(p_1), \\cdots, f(p_n))'$\n and \n$\\boldsymbol{\\psi}_j= (\\psi_j(p_1), \\cdots, \\psi_j(p_n))'$ \\cite{zhang.2007}. \n\nFigure \\ref{fig:2_md_eigfs} shows few representative LB-eigenfunctions on the hyoid surface. For  heat kernel regression, we used the bandwidth $\\sigma =5$ and $500$ LB-eigenfunctions on the final template. \nThe number of eigenfunctions used is more than sufficient to guarantee relative error less than $0.3\\%$ in our data.\n\n\n\n\n\n\\subsection{Statistical Inference}\n\nWe are interested in determining the significance of functional signals on a manifold \\ref{fig:agegrouping}. \nWe borrow the statistical parametric mapping (SPM) framework for analyzing and visualizing statistical tests performed on the template surface that is often used in brain image analysis \\cite{andrade.2001,lerch.2005.ni,wang.2010,worsley.1995,yushkevich.2008}. Since test statistics are constructed over all mesh vertices on the surface, multiple comparisons need to be accounted. For continuous functional data,  the random field theory \\cite{taylor.2007,worsley.1995,worsley.2004} is natural to use. The random field theory assumes the measurements to be smooth Gaussian random field. Heat kernel regression will make the data more smooth and Gaussian as well as increase the signal-to-noise ratio \\cite{chung.2005.ni}. \n\n\n\n\n \\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{hyoid-type1errorplot.pdf}\n \\caption{Type-I error plot over bandwidth $t$ of kernel regression for testing the difference between the groups I and III. As the bandwidth increases, the multiple comparisons corrected type-I error decreases. The bandwidth 5 is chosen for the study. The choice of the bandwidth around 5 does not change the over-all Type-I error much.}\n\\label{fig:powerplot}\n\\end{figure}\n\nConsider a functional measurements $f_1, \\cdots, f_n$ on manifold $\\mathcal{M}$. In the simplest statistical setting, the measurements can be modeled as \n\n", "itemtype": "equation", "pos": 17738, "prevtext": "\nwhich is a heat kernel. The bandwidth $t$ of heat kernel controls resolution while the translation is done by shifting one argument in the kernel. \n\n\n\\begin{figure}[t]\n \\centering\n\\includegraphics[width=1\\linewidth]{midus-gibbs.pdf}\n  \\caption{\\footnotesize The Gibbs phenomenon on a hat shaped simulated surface showing the ringing effect on the traditional Fourier series expansion (top) and the reduced effect on the heat kernel regression (bottom). 7225 basis functions were used for the both cases and the bandwidth $t = 0.001$ is used for the kernel regression.}\n\\label{fig:midus-gibbs}\n\\end{figure}\n\n\n\nAlthough the kernel regression is constructed using global basis functions $\\psi_j$, the kernel regression at each point $p$ coincides with the diffusion wavelet transform at that point. Hence, just like wavelets, the kernel regression will have the localization property of wavelets. This is demonstrated in a toy example in Figure \\ref{fig:midus-gibbs} with a hat-shaped surface in 3D. The hat-shaped step function is simulated as $z=1$ for $x^2 + y^2 < 1$ and $z=0$ for $1 \\leq  x^2 + y^2 \\leq 2$.  Then the step function is reconstructed using the Fourier series expansion via LSE (top) and kernel regression (bottom). In the both cases,  up to 7225 basis functions were used. For the kernel regression, the heat kernel with bandwidth $t=0.0001$ is used. LSE clearly shows the visible Gibbs phenomenon, i.e., ringing artifact \\cite{chung.2007.TMI,gelb.1997} compared to the kernel regression. \n\n\n\n\\subsection{Numerical Implementation}\nThe Laplace-Beltrami operator is chosen as the self-adjoint operators $\\mathcal{L}$ of choice. The eigenfunctions of the Laplace-Beltrami operator on an arbitrary curved surface is analytically unknown. So it is necessary to discretize (\\ref{eq:eigen}) using the Cotan formulation  as a generalized eigenvalue problem \\cite{\nzhang.2007,qiu.2006}:\n\n", "index": 23, "text": "\\begin{equation}\n\\mathbf{C}\\psi = \\lambda \\mathbf{A} \\boldsymbol{\\psi},\n\\label{eq:LMA}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{C}\\psi=\\lambda\\mathbf{A}\\boldsymbol{\\psi},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc02</mi><mo>\u2062</mo><mi>\u03c8</mi></mrow><mo>=</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\ud835\udc00</mi><mo>\u2062</mo><mi>\ud835\udf4d</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $h$ is an unknown group level signal and $\\epsilon_i$ is a zero-mean Gaussian random field \\cite{worsley.2004}. At each fixed point $p$, we are assuming $\\epsilon_i \\sim N(0, \\sigma^2)$.\n\n\nWe are interested in determining the significance of $h$, i.e.\n\\begin{eqnarray} H_0: h(p)=0 \\mbox{ for all } p \\in \\mathcal{M} \\; \\mbox{ vs. } \\;\nH_1: h(p)\n> 0 \\mbox { for some } x \\in \\mathcal{M}. \\label{eq:hypothesis}\\end{eqnarray}\nNote that any point $p_0$ that gives $h(p_0) > 0$ is considered as signal. The hypothsis\n(\\ref{eq:hypothesis}) is an infinite dimensional multiple comparisons problem for continuously indexed hypotheses over the manifold $\\mathcal{M}$. The underlying group level signal $h$ is estimated using the proposed heat kernel regression. Subsequently, a test statistic is given by a T-field $T(p)$ or a F-field, which is simply given by the square of the T-field  \\cite{worsley.2004,worsley.1995}. \n\nFor sufficiently high threshold $z$, \nthe corrected type-I error of testing  hypothesis (\\ref{eq:hypothesis}) is given by\n\n\n", "itemtype": "equation", "pos": 20218, "prevtext": "\n\\noindent\nwhere $\\mathbf{C}$ is the stiffness matrix, $\\mathbf{A}$ is the mass matrix and $\\boldsymbol{\\psi}=(\\psi(p_1), \\cdots, \\psi(p_n))'$ is the eigenfunction evaluated at $n$ mesh vertices. \nOnce we obtained the basis functions $\\psi_j$, the corresponding Fourier coefficients $\\beta_j$ are estimated as\n\\begin{eqnarray*}\n\\label{eq:smoothing-FEMbeta}\n\t\\beta_j = {\\bf f}' {\\bf A} \\boldsymbol{\\psi}_j, \n\\end{eqnarray*}\nwhere \n${\\bf f} = (f(p_1), \\cdots, f(p_n))'$\n and \n$\\boldsymbol{\\psi}_j= (\\psi_j(p_1), \\cdots, \\psi_j(p_n))'$ \\cite{zhang.2007}. \n\nFigure \\ref{fig:2_md_eigfs} shows few representative LB-eigenfunctions on the hyoid surface. For  heat kernel regression, we used the bandwidth $\\sigma =5$ and $500$ LB-eigenfunctions on the final template. \nThe number of eigenfunctions used is more than sufficient to guarantee relative error less than $0.3\\%$ in our data.\n\n\n\n\n\n\\subsection{Statistical Inference}\n\nWe are interested in determining the significance of functional signals on a manifold \\ref{fig:agegrouping}. \nWe borrow the statistical parametric mapping (SPM) framework for analyzing and visualizing statistical tests performed on the template surface that is often used in brain image analysis \\cite{andrade.2001,lerch.2005.ni,wang.2010,worsley.1995,yushkevich.2008}. Since test statistics are constructed over all mesh vertices on the surface, multiple comparisons need to be accounted. For continuous functional data,  the random field theory \\cite{taylor.2007,worsley.1995,worsley.2004} is natural to use. The random field theory assumes the measurements to be smooth Gaussian random field. Heat kernel regression will make the data more smooth and Gaussian as well as increase the signal-to-noise ratio \\cite{chung.2005.ni}. \n\n\n\n\n \\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{hyoid-type1errorplot.pdf}\n \\caption{Type-I error plot over bandwidth $t$ of kernel regression for testing the difference between the groups I and III. As the bandwidth increases, the multiple comparisons corrected type-I error decreases. The bandwidth 5 is chosen for the study. The choice of the bandwidth around 5 does not change the over-all Type-I error much.}\n\\label{fig:powerplot}\n\\end{figure}\n\nConsider a functional measurements $f_1, \\cdots, f_n$ on manifold $\\mathcal{M}$. In the simplest statistical setting, the measurements can be modeled as \n\n", "index": 25, "text": "$$ f_i (p) = h(p) + \\epsilon_i(p),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"f_{i}(p)=h(p)+\\epsilon_{i}(p),\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03f5</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $\\mu_d(\\mathcal{M})$ is the $j$-th\nMinkowski functional or intrinsic volume of $\\mathcal{M}$ and\n$\\rho_j$ is the $j$-th Euler characteristic (EC) density of T-field. \n\nSince the hyoid bone is compact with no boundary but has three disconnected components, the Minkowski functionals are simply \n\\begin{eqnarray*} \\mu_2(\\mathcal{M}) &=& \\mbox{area}(\\mathcal{M})/2\\\\\n\\mu_1(\\mathcal{M}) &=& 0\\\\\n\\mu_0(\\mathcal{M}) &=& \\chi(\\mathcal{M}) = 3 \\times 2.\n\\end{eqnarray*} \nThe term $\\mu_1$ is zero since there is no boundary and $\\mu_0$ is simply the Euler characteristic of the template surface. Note that the Euler characteristic of a closed surface with no hole or handle is 2 and there are three such surfaces. The EC-densities of the T-field with $\\nu$ degrees of freedom is given by\n\\begin{eqnarray*} \\rho_0 (z)&=& 1 - P(T_{\\nu} \\leq z),\\\\\n\\rho_1 (z) & = &   \\frac{1}{\\sqrt{2t^2}} \\cdot \\frac{1}{2\\pi} \\Big( 1+  \\frac{z^2}{\\nu})^{-(\\nu-1)/2}, \\\\\n\\rho_2 (z) & = & \\frac{1}{2t^2} \\cdot \\frac{1}{(2\\pi)^{3/2}} \\frac{\\Gamma(\\frac{\\nu +1}{2})}\n{(\\frac{\\nu}{2})^{1/2} \\Gamma(\\frac{\\nu}{2})}  z \\Big( 1 + \\frac{z^2}{\\nu} \\Big)^{-(\\nu -1)/2}.    \\end{eqnarray*}\nThe EC-density of the F-field is similarly given in \\cite{worsley.2004,\ntaylor.2007}. The EC-density has the kernel bandwidth $2t^2$ in the formulation so the inference is done at a particular smoothing scale. Figure \\ref{fig:powerplot} shows the type-I error plot over different bandwidth $t$ of the kernel regression in our application. As the bandwidth $t$ becomes zero, the type-I error increases. When $t=0$, the kernel regression collapse to the usual Fourier series expansion. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data without any smoothing. Hence, the proposed kernel regression can be viewed as having substantially smaller type-I error compared to the Fourier series expansion as well as  the original data demonstrating a better statistical performance. \n\nType-II error and the statistical power can be also computed similarly. \n\\begin{theorem}The statistical power $\\mathcal{P}$ of testing the hypotheses\n\n", "itemtype": "equation", "pos": 21299, "prevtext": "\nwhere $h$ is an unknown group level signal and $\\epsilon_i$ is a zero-mean Gaussian random field \\cite{worsley.2004}. At each fixed point $p$, we are assuming $\\epsilon_i \\sim N(0, \\sigma^2)$.\n\n\nWe are interested in determining the significance of $h$, i.e.\n\\begin{eqnarray} H_0: h(p)=0 \\mbox{ for all } p \\in \\mathcal{M} \\; \\mbox{ vs. } \\;\nH_1: h(p)\n> 0 \\mbox { for some } x \\in \\mathcal{M}. \\label{eq:hypothesis}\\end{eqnarray}\nNote that any point $p_0$ that gives $h(p_0) > 0$ is considered as signal. The hypothsis\n(\\ref{eq:hypothesis}) is an infinite dimensional multiple comparisons problem for continuously indexed hypotheses over the manifold $\\mathcal{M}$. The underlying group level signal $h$ is estimated using the proposed heat kernel regression. Subsequently, a test statistic is given by a T-field $T(p)$ or a F-field, which is simply given by the square of the T-field  \\cite{worsley.2004,worsley.1995}. \n\nFor sufficiently high threshold $z$, \nthe corrected type-I error of testing  hypothesis (\\ref{eq:hypothesis}) is given by\n\n\n", "index": 27, "text": "$$P \\Big(\\sup_{p \\in \\mathcal{M}} T(p) > z \\Big) = \\sum_{j=0}^d\n\\mu_j(\\mathcal{M})\\rho_j(z), \n\n\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"P\\Big{(}\\sup_{p\\in\\mathcal{M}}T(p)&gt;z\\Big{)}=\\sum_{j=0}^{d}\\mu_{j}(\\mathcal{M})%&#10;\\rho_{j}(z),\\par&#10;\\par&#10;\" display=\"block\"><mrow><mi>P</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></munder><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><mi>z</mi><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><msub><mi>\u03bc</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c1</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nusing the T random field $T(p)$  is given by\n\n", "itemtype": "equation", "pos": 23518, "prevtext": "\nwhere $\\mu_d(\\mathcal{M})$ is the $j$-th\nMinkowski functional or intrinsic volume of $\\mathcal{M}$ and\n$\\rho_j$ is the $j$-th Euler characteristic (EC) density of T-field. \n\nSince the hyoid bone is compact with no boundary but has three disconnected components, the Minkowski functionals are simply \n\\begin{eqnarray*} \\mu_2(\\mathcal{M}) &=& \\mbox{area}(\\mathcal{M})/2\\\\\n\\mu_1(\\mathcal{M}) &=& 0\\\\\n\\mu_0(\\mathcal{M}) &=& \\chi(\\mathcal{M}) = 3 \\times 2.\n\\end{eqnarray*} \nThe term $\\mu_1$ is zero since there is no boundary and $\\mu_0$ is simply the Euler characteristic of the template surface. Note that the Euler characteristic of a closed surface with no hole or handle is 2 and there are three such surfaces. The EC-densities of the T-field with $\\nu$ degrees of freedom is given by\n\\begin{eqnarray*} \\rho_0 (z)&=& 1 - P(T_{\\nu} \\leq z),\\\\\n\\rho_1 (z) & = &   \\frac{1}{\\sqrt{2t^2}} \\cdot \\frac{1}{2\\pi} \\Big( 1+  \\frac{z^2}{\\nu})^{-(\\nu-1)/2}, \\\\\n\\rho_2 (z) & = & \\frac{1}{2t^2} \\cdot \\frac{1}{(2\\pi)^{3/2}} \\frac{\\Gamma(\\frac{\\nu +1}{2})}\n{(\\frac{\\nu}{2})^{1/2} \\Gamma(\\frac{\\nu}{2})}  z \\Big( 1 + \\frac{z^2}{\\nu} \\Big)^{-(\\nu -1)/2}.    \\end{eqnarray*}\nThe EC-density of the F-field is similarly given in \\cite{worsley.2004,\ntaylor.2007}. The EC-density has the kernel bandwidth $2t^2$ in the formulation so the inference is done at a particular smoothing scale. Figure \\ref{fig:powerplot} shows the type-I error plot over different bandwidth $t$ of the kernel regression in our application. As the bandwidth $t$ becomes zero, the type-I error increases. When $t=0$, the kernel regression collapse to the usual Fourier series expansion. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data without any smoothing. Hence, the proposed kernel regression can be viewed as having substantially smaller type-I error compared to the Fourier series expansion as well as  the original data demonstrating a better statistical performance. \n\nType-II error and the statistical power can be also computed similarly. \n\\begin{theorem}The statistical power $\\mathcal{P}$ of testing the hypotheses\n\n", "index": 29, "text": "$$ H_0: h(p)  = 0 \\; \\mbox{ for all } p \\in \\mathcal{M} \\; \\mbox{ vs. } \\; H_1: h(p) = c\\sigma > 0 \\mbox{ for some } p \\in \\mathcal{M}. $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"H_{0}:h(p)=0\\;\\mbox{ for all }p\\in\\mathcal{M}\\;\\mbox{ vs. }\\;H_{1}:h(p)=c%&#10;\\sigma&gt;0\\mbox{ for some }p\\in\\mathcal{M}.\" display=\"block\"><mrow><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+2.8pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mtext>\u00a0for all\u00a0</mtext><mo>\u2062</mo><mi>p</mi></mrow><mo>\u2208</mo><mrow><mpadded width=\"+2.8pt\"><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>\u00a0vs.\u00a0</mtext></mpadded><mo>\u2062</mo><msub><mi>H</mi><mn>1</mn></msub></mrow></mrow><mo>:</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>\u03c3</mi></mrow><mo>&gt;</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0for some\u00a0</mtext><mo>\u2062</mo><mi>p</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $t^*_{\\alpha}$ is the $\\alpha$-quantile given by\n\n", "itemtype": "equation", "pos": 23702, "prevtext": "\nusing the T random field $T(p)$  is given by\n\n", "index": 31, "text": "$$\\mathcal{P}(n) \\approx 1- \\exp \\Big[ - \\sum_{j=0}^d \\mu_j(\\mathcal{M}_1)\\rho_j(t^*_{\\alpha} - c\\sqrt{n}) \\Big],$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}(n)\\approx 1-\\exp\\Big{[}-\\sum_{j=0}^{d}\\mu_{j}(\\mathcal{M}_{1})\\rho%&#10;_{j}(t^{*}_{\\alpha}-c\\sqrt{n})\\Big{]},\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">[</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c1</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "  \n\\end{theorem}\n{\\em Proof.} See the Appendix.\n\n\\section{Application}\n\n\\subsection{CT Imaging Data and Preprocessing}\n\nThe study consists of high resolution CT images of 70 normal subjects ages between 0 and 20 years (mean age $=$ 58.0 $\\pm$ 11.3 years). \nCT scans were converted to DICOM format and Analyze $8.1$ software package (AnalyzeDirect, Inc., Overland Park, KS) was then used in segmenting binary hyoid bone images by a trained individual rater in the native space by simple image intensity thresholding and careful manual editing. A nonlinear image registration using the diffeomorphic shape and intensity averaging technique with cross-correlation as similarity metric was performed through Advanced Normalization Tools (ANTS) \\cite{avants.2008}. A study-specific template was constructed. We have chosen a 12 year old subject identified as F155 as the initial template and aligned the remaining 69 hyoids to the initial template affinely to remove the overall size variability. Some subject may have larger hyoid than others so it is necessary to remove the global size differences in local shape modeling. From the affine transformed individual hyoid surfaces, we performed the diffeomorphic nonlinear image registration to the template using ANTS. \n\nThen by averaging the inverse deformation fields from the initial template to individual hyoid, we obtain the yet another final template. Figure \\ref{fig:template} shows the initial and final templates. The isosurface of the final template volume is extracted using the marching cube algorithm \\cite{lorensen.1987}. Figure \\ref{fig:agegrouping} shows the mean displacement differences between the groups I and II (top) and II and III (bottom). Each row shows the group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the growth direction given by the mean displacement differences and colors indicate their lengths in mm. We are interested in localizing the regions of hyoid bone growth between the age groups. \n\n\n70 subjects are binned into three age categories: ages between 0 and 6 years (group I), between 7 and 12 years (group II), and between 13 and 19 years (group III). There are 26, 14 and 30 subjects in group I, II and III respectively. The main biological hypothesis of interest is if there is any localized hyoid bone growth spurts between these specific age groups. \n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-preprocessing.pdf} \n\\caption{Left: Hyoid F155 which forms an initial template $\\mathcal{M}_I$. All other mandibles are affine registered to F155. Middle: The superimposition of affine registered hyois showing local misalignments. Diffeomorphic registration is then performed to register misaligned affine transformed hyoids. \nRight: The average of deformation with respect to F155 provides the final population average template $\\mathcal{M}_F$ where statistical parametric maps will be constructed.}\n\\label{fig:template}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-groupdisp.pdf} \n\\caption{Hyoid bones are binned into three age groups: group I (ages 0 and 6), group II (ages 7 and 12) and group III (ages 13 and 19) and the mean displacements between the groups are visualized. Each row shows the mean group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the mean displacement differences and colors indicate their lengths in mm.}\n\\label{fig:agegrouping}\n\\end{figure}\n\n\n\n\n\\subsection{Results}\n\nThe displacement from the template to an individual surface is obtained at each mesh vertex. Since  the length measurement provides a much easier biological interpretation, we used the length of displacement vector as a response variable among many other possible features. Since the length on the template surface is expected to be noisy due to image acquisition, segmentation and image registration errors, it is necessary perform the proposed kernel regression and subsequently reduce the type-I error and obtain more stable SPM. Figure \\ref{fig:wavelets} shows an example of kernel regression on our data. The kernel regression increases the signal-to-noise ratio (SNR) and improves the smoothness and Gaussianness of data. Subsequently, the heat kernel regression of the displacement length is taken as the response variable. \nWe have chosen $t=5$ as the bandwidth for the study since the bandwidth 5 is where the type-I error starts to flatten out in Figure \\ref{fig:powerplot}. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data (relative error of less than 0.3$\\%$). Hence, performing the proposed kernel regression before the statistical analysis can substantially smaller type-I error demonstrating its effectiveness. \n\n\n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{hyoid-Fstatsigma5.pdf}\n\\caption{F-statistic maps on hyoid showing age effect between the groups. The significant growth regions (red) are identified only between group II and III, and I and III. The growth is highly localized near the regions that connect the disconnected hyoid bones.}\n\\label{fig:surfGLM}\n\\end{center} \n\\end{figure}\n\nAfter the displacement lengths are smoothed, we constructed the F-field, or equivalently the T-field square, for testing the length difference between the age groups I and II, II and III, and I and III showing the regions of growth spurts between different age range (Figure \\ref{fig:surfGLM}). Since test statistics are constructed over all mesh vertices on the mandible, multiple comparisons were account  using the random field theory \\cite{worsley.1995,worsley.2004}. \n\nFor testing the differences between the groups I and II, II and III, and I and III, they are based on F-field with  1 and  38, 1 and 42, and 1 and  54 degrees of freedom respectively. The result is displayed in Figure \\ref{fig:surfGLM}, where the significant results were only found between the groups II and III (middle), and I and III (bottom) at 0.1 level. Between the groups II and III, we obtained the maximum F-statistic value of 9.36 (right hyoid), which corresponds to the p-value of 0.041 (corrected). Between the groups I and III, we obtained the maximum F-statistic value of 10.55 (middle hyoid), which corresponds to the p-value of 0.028 (corrected). In the $F$-statistic maps for middle and bottom rows, red regions are considered as exhibiting significant growth spurts. \n\n\n\n\n\\section{Conclusion}\nWe have developed a new kernel regression framework on a manifold that unifies bivariate kernel regression, heat diffusion and wavelets in a single coherent mathematical framework. The kernel regression is both global and local in a sense it uses global basis functions to perform regression but locally equivalent to diffusion wavelet transform. The proposed framework is demonstrated to reduce type-I error in modeling shape variations compared to the usual Fourier series expansion. The method is then used in developing a statistical inference procedure for functional signals on manifolds. The whole framework \n\n\\section*{Acknowledgment}\nThis work was supported by NIH Research Grant R01 DC6282 (MRI and CT Studies of the Developing Vocal Tract), from the National Institute of Deafness and other Communicative Disorders (NIDCD);  core grant P-30 HD03352 to the Waisman Center from the National Institute of Child Health and Human Development (NICHHD) and Clinical and Translational Science Award (CTSA) program, through the NIH National Center for Advancing Translational Sciences (NCATS) UL1TR000427\n\n\n\\bibliographystyle{plain}\n\\bibliography{reference.2016.01.10}\n\n\n\\newpage\n\\section*{Appendix}\n{\\bf Theorem 1}\\\\\n\\begin{eqnarray*} \\widehat{h}(p)  = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p) = \\sum_{j=0}^k  \\tau_j f_j \\psi_j.\n\\end{eqnarray*}\n{\\bf Proof.}\nAny function $h \\in \\mathcal{H}_k$ can be expressed as \n\\begin{eqnarray} h(p) = \\sum_{j=0}^k \\beta_j \\psi_j(p) \\label{eq:hp}.\\end{eqnarray}\nThen by plugging (\\ref{eq:hp}) into the inner integral $I(p)$, \nit becomes\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $t^*_{\\alpha}$ is the $\\alpha$-quantile given by\n\n", "index": 33, "text": "$$\\alpha = P \\Big( \\sup_{p \\in \\mathcal{M}} T(p) > t^*_{\\alpha} \\Big).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\alpha=P\\Big{(}\\sup_{p\\in\\mathcal{M}}T(p)&gt;t^{*}_{\\alpha}\\Big{)}.\" display=\"block\"><mrow><mi>\u03b1</mi><mo>=</mo><mi>P</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></munder><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nSimplifying the expression, we obtain \n\\begin{eqnarray} I(p) = \\sum_{j=0}^k \\sum_{j'=0}^k  \\psi_j (p)\\psi_{j'}(p)\\beta_j \\beta_{j'} - 2K* f(p)\\sum_{j=0}^k  \\psi_j(p)\\beta_{j} + K*f^2(p).\\label{eq:QP}\\end{eqnarray}\n\n\nFrom Mercer's theorem \\cite{conway.1990}, the kernel can be written as\n\\begin{eqnarray} K(p,q) = \\sum_{j'=0}^{\\infty} \\tau_{j'} \\psi_{j'}(p)\\psi_{j'}(q). \\label{eq:Kpq}\\end{eqnarray}\nThe convolution \nis then written as\n\n", "itemtype": "equation", "pos": 32164, "prevtext": "  \n\\end{theorem}\n{\\em Proof.} See the Appendix.\n\n\\section{Application}\n\n\\subsection{CT Imaging Data and Preprocessing}\n\nThe study consists of high resolution CT images of 70 normal subjects ages between 0 and 20 years (mean age $=$ 58.0 $\\pm$ 11.3 years). \nCT scans were converted to DICOM format and Analyze $8.1$ software package (AnalyzeDirect, Inc., Overland Park, KS) was then used in segmenting binary hyoid bone images by a trained individual rater in the native space by simple image intensity thresholding and careful manual editing. A nonlinear image registration using the diffeomorphic shape and intensity averaging technique with cross-correlation as similarity metric was performed through Advanced Normalization Tools (ANTS) \\cite{avants.2008}. A study-specific template was constructed. We have chosen a 12 year old subject identified as F155 as the initial template and aligned the remaining 69 hyoids to the initial template affinely to remove the overall size variability. Some subject may have larger hyoid than others so it is necessary to remove the global size differences in local shape modeling. From the affine transformed individual hyoid surfaces, we performed the diffeomorphic nonlinear image registration to the template using ANTS. \n\nThen by averaging the inverse deformation fields from the initial template to individual hyoid, we obtain the yet another final template. Figure \\ref{fig:template} shows the initial and final templates. The isosurface of the final template volume is extracted using the marching cube algorithm \\cite{lorensen.1987}. Figure \\ref{fig:agegrouping} shows the mean displacement differences between the groups I and II (top) and II and III (bottom). Each row shows the group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the growth direction given by the mean displacement differences and colors indicate their lengths in mm. We are interested in localizing the regions of hyoid bone growth between the age groups. \n\n\n70 subjects are binned into three age categories: ages between 0 and 6 years (group I), between 7 and 12 years (group II), and between 13 and 19 years (group III). There are 26, 14 and 30 subjects in group I, II and III respectively. The main biological hypothesis of interest is if there is any localized hyoid bone growth spurts between these specific age groups. \n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-preprocessing.pdf} \n\\caption{Left: Hyoid F155 which forms an initial template $\\mathcal{M}_I$. All other mandibles are affine registered to F155. Middle: The superimposition of affine registered hyois showing local misalignments. Diffeomorphic registration is then performed to register misaligned affine transformed hyoids. \nRight: The average of deformation with respect to F155 provides the final population average template $\\mathcal{M}_F$ where statistical parametric maps will be constructed.}\n\\label{fig:template}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-groupdisp.pdf} \n\\caption{Hyoid bones are binned into three age groups: group I (ages 0 and 6), group II (ages 7 and 12) and group III (ages 13 and 19) and the mean displacements between the groups are visualized. Each row shows the mean group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the mean displacement differences and colors indicate their lengths in mm.}\n\\label{fig:agegrouping}\n\\end{figure}\n\n\n\n\n\\subsection{Results}\n\nThe displacement from the template to an individual surface is obtained at each mesh vertex. Since  the length measurement provides a much easier biological interpretation, we used the length of displacement vector as a response variable among many other possible features. Since the length on the template surface is expected to be noisy due to image acquisition, segmentation and image registration errors, it is necessary perform the proposed kernel regression and subsequently reduce the type-I error and obtain more stable SPM. Figure \\ref{fig:wavelets} shows an example of kernel regression on our data. The kernel regression increases the signal-to-noise ratio (SNR) and improves the smoothness and Gaussianness of data. Subsequently, the heat kernel regression of the displacement length is taken as the response variable. \nWe have chosen $t=5$ as the bandwidth for the study since the bandwidth 5 is where the type-I error starts to flatten out in Figure \\ref{fig:powerplot}. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data (relative error of less than 0.3$\\%$). Hence, performing the proposed kernel regression before the statistical analysis can substantially smaller type-I error demonstrating its effectiveness. \n\n\n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{hyoid-Fstatsigma5.pdf}\n\\caption{F-statistic maps on hyoid showing age effect between the groups. The significant growth regions (red) are identified only between group II and III, and I and III. The growth is highly localized near the regions that connect the disconnected hyoid bones.}\n\\label{fig:surfGLM}\n\\end{center} \n\\end{figure}\n\nAfter the displacement lengths are smoothed, we constructed the F-field, or equivalently the T-field square, for testing the length difference between the age groups I and II, II and III, and I and III showing the regions of growth spurts between different age range (Figure \\ref{fig:surfGLM}). Since test statistics are constructed over all mesh vertices on the mandible, multiple comparisons were account  using the random field theory \\cite{worsley.1995,worsley.2004}. \n\nFor testing the differences between the groups I and II, II and III, and I and III, they are based on F-field with  1 and  38, 1 and 42, and 1 and  54 degrees of freedom respectively. The result is displayed in Figure \\ref{fig:surfGLM}, where the significant results were only found between the groups II and III (middle), and I and III (bottom) at 0.1 level. Between the groups II and III, we obtained the maximum F-statistic value of 9.36 (right hyoid), which corresponds to the p-value of 0.041 (corrected). Between the groups I and III, we obtained the maximum F-statistic value of 10.55 (middle hyoid), which corresponds to the p-value of 0.028 (corrected). In the $F$-statistic maps for middle and bottom rows, red regions are considered as exhibiting significant growth spurts. \n\n\n\n\n\\section{Conclusion}\nWe have developed a new kernel regression framework on a manifold that unifies bivariate kernel regression, heat diffusion and wavelets in a single coherent mathematical framework. The kernel regression is both global and local in a sense it uses global basis functions to perform regression but locally equivalent to diffusion wavelet transform. The proposed framework is demonstrated to reduce type-I error in modeling shape variations compared to the usual Fourier series expansion. The method is then used in developing a statistical inference procedure for functional signals on manifolds. The whole framework \n\n\\section*{Acknowledgment}\nThis work was supported by NIH Research Grant R01 DC6282 (MRI and CT Studies of the Developing Vocal Tract), from the National Institute of Deafness and other Communicative Disorders (NIDCD);  core grant P-30 HD03352 to the Waisman Center from the National Institute of Child Health and Human Development (NICHHD) and Clinical and Translational Science Award (CTSA) program, through the NIH National Center for Advancing Translational Sciences (NCATS) UL1TR000427\n\n\n\\bibliographystyle{plain}\n\\bibliography{reference.2016.01.10}\n\n\n\\newpage\n\\section*{Appendix}\n{\\bf Theorem 1}\\\\\n\\begin{eqnarray*} \\widehat{h}(p)  = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p) = \\sum_{j=0}^k  \\tau_j f_j \\psi_j.\n\\end{eqnarray*}\n{\\bf Proof.}\nAny function $h \\in \\mathcal{H}_k$ can be expressed as \n\\begin{eqnarray} h(p) = \\sum_{j=0}^k \\beta_j \\psi_j(p) \\label{eq:hp}.\\end{eqnarray}\nThen by plugging (\\ref{eq:hp}) into the inner integral $I(p)$, \nit becomes\n\n", "index": 35, "text": "$$I(p) =\\int_{\\mathcal{M}} K(p,q)\\Big| f(q) - \\sum_{j=0}^k  \\beta_j \\psi (p) \\Big|^2\\;d\\mu(q).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"I(p)=\\int_{\\mathcal{M}}K(p,q)\\Big{|}f(q)-\\sum_{j=0}^{k}\\beta_{j}\\psi(p)\\Big{|}%&#10;^{2}\\;d\\mu(q).\" display=\"block\"><mrow><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"+2.8pt\"><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">|</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03b2</mi><mi>j</mi></msub><mo>\u2062</mo><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mn>2</mn></msup></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\n\nSince $I$ is an unconstrained positive semidefinite quadratic program (QP) in $\\beta_j$, there is no unique global minimizer of $I$ without additional linear constraints. Integrating $I$ further with respect to $d\\mu(p)$, we collapses (\\ref{eq:QP})  to a positive definite QP, which yields a unique global minimizer:\n\n", "itemtype": "equation", "pos": 32696, "prevtext": "\nSimplifying the expression, we obtain \n\\begin{eqnarray} I(p) = \\sum_{j=0}^k \\sum_{j'=0}^k  \\psi_j (p)\\psi_{j'}(p)\\beta_j \\beta_{j'} - 2K* f(p)\\sum_{j=0}^k  \\psi_j(p)\\beta_{j} + K*f^2(p).\\label{eq:QP}\\end{eqnarray}\n\n\nFrom Mercer's theorem \\cite{conway.1990}, the kernel can be written as\n\\begin{eqnarray} K(p,q) = \\sum_{j'=0}^{\\infty} \\tau_{j'} \\psi_{j'}(p)\\psi_{j'}(q). \\label{eq:Kpq}\\end{eqnarray}\nThe convolution \nis then written as\n\n", "index": 37, "text": "$$K*f(p) = \\sum_{j'=0}^{\\infty} \\tau_{j'} f_{j'} \\psi_{j'}(p).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"K*f(p)=\\sum_{j^{\\prime}=0}^{\\infty}\\tau_{j^{\\prime}}f_{j^{\\prime}}\\psi_{j^{%&#10;\\prime}}(p).\" display=\"block\"><mrow><mrow><mrow><mrow><mi>K</mi><mo>*</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><msub><mi>\u03c4</mi><msup><mi>j</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><msub><mi>f</mi><msup><mi>j</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><msup><mi>j</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nThe minimum of the above integral is obtained when all the partial derivatives with respect to $\\beta_j$ vanish, i.e.\n\n", "itemtype": "equation", "pos": 33079, "prevtext": "\n\nSince $I$ is an unconstrained positive semidefinite quadratic program (QP) in $\\beta_j$, there is no unique global minimizer of $I$ without additional linear constraints. Integrating $I$ further with respect to $d\\mu(p)$, we collapses (\\ref{eq:QP})  to a positive definite QP, which yields a unique global minimizer:\n\n", "index": 39, "text": "$$\\int_{\\mathcal{M}} I(p) \\;d\\mu(p) = \\sum_{j=0}^k   \\beta_{j}^2 - 2\\sum_{j=0}^k \\tau_j f_j \\beta_{j} + \\mbox{ const}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\int_{\\mathcal{M}}I(p)\\;d\\mu(p)=\\sum_{j=0}^{k}\\beta_{j}^{2}-2\\sum_{j=0}^{k}%&#10;\\tau_{j}f_{j}\\beta_{j}+\\mbox{ const}.\" display=\"block\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><msubsup><mi>\u03b2</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>f</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow><mo>+</mo><mtext>\u00a0const</mtext></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nfor all $j$. Hence $\\sum_{j=0}^k  \\tau_j f_j \\psi_j$ must be the unique minimizer. $\\square$\\\\\n\n\n\\noindent{\\bf Theorem 2.}\nFor an arbitrary self-adjoint differential operator $\\mathcal{L}$, the unique solution of the Cauchy problem\n\\begin{eqnarray} \\frac{\\partial g(p,t)}{\\partial t} + \\mathcal{L} g(p,t) =0, \ng(p,t=0) =f(p)\n\\end{eqnarray}\nis given by\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} \\tau_j f_j \\psi_j(p) \\;\\mbox{ with } \\; \\tau_j = e^{-\\lambda_j t}.\\end{eqnarray}\n\\\\\n{\\bf Proof.} For each fixed $t$,\n$g(p,t)$ can be written as\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} c_j(t) \\psi_j(p)\n\\label{eq:expansion}.\\end{eqnarray} \nThen \n\\begin{eqnarray} \\mathcal{L} g(p,t) = \\sum_{j=0}^{\\infty} c_j(t) \\lambda_j \\psi_j(p)\n\\label{eq:expansion2}.\\end{eqnarray} \nSubstituting (\\ref{eq:expansion}) and (\\ref{eq:expansion2})\ninto (\\ref{eq:cauchy1}), we obtain \n\\begin{eqnarray} \\frac{\\partial c_j(t)}{\\partial t} + \n\\lambda_j c_j(t)=0 \\label{eq:ODE}\\end{eqnarray} \nfor all $j$. The solution of equation\n(\\ref{eq:ODE}) is given by $c_j(t) = b_je^{-\\lambda_j t}$. So we\nhave a solution \n", "itemtype": "equation", "pos": 33318, "prevtext": "\nThe minimum of the above integral is obtained when all the partial derivatives with respect to $\\beta_j$ vanish, i.e.\n\n", "index": 41, "text": "$$\\int_{\\mathcal{M}} \\frac{\\partial I}{\\partial \\beta_j} \\;d\\mu(p) = \n2\\beta_j - 2\\tau_j  f_j  =0$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\int_{\\mathcal{M}}\\frac{\\partial I}{\\partial\\beta_{j}}\\;d\\mu(p)=2\\beta_{j}-2%&#10;\\tau_{j}f_{j}=0\" display=\"block\"><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mpadded width=\"+2.8pt\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>I</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow></mfrac></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>\u03b2</mi><mi>j</mi></msub></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>\u03c4</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>f</mi><mi>j</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": " \nAt $t=0$, we have\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nfor all $j$. Hence $\\sum_{j=0}^k  \\tau_j f_j \\psi_j$ must be the unique minimizer. $\\square$\\\\\n\n\n\\noindent{\\bf Theorem 2.}\nFor an arbitrary self-adjoint differential operator $\\mathcal{L}$, the unique solution of the Cauchy problem\n\\begin{eqnarray} \\frac{\\partial g(p,t)}{\\partial t} + \\mathcal{L} g(p,t) =0, \ng(p,t=0) =f(p)\n\\end{eqnarray}\nis given by\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} \\tau_j f_j \\psi_j(p) \\;\\mbox{ with } \\; \\tau_j = e^{-\\lambda_j t}.\\end{eqnarray}\n\\\\\n{\\bf Proof.} For each fixed $t$,\n$g(p,t)$ can be written as\n\\begin{eqnarray} g(p,t) = \\sum_{j=0}^{\\infty} c_j(t) \\psi_j(p)\n\\label{eq:expansion}.\\end{eqnarray} \nThen \n\\begin{eqnarray} \\mathcal{L} g(p,t) = \\sum_{j=0}^{\\infty} c_j(t) \\lambda_j \\psi_j(p)\n\\label{eq:expansion2}.\\end{eqnarray} \nSubstituting (\\ref{eq:expansion}) and (\\ref{eq:expansion2})\ninto (\\ref{eq:cauchy1}), we obtain \n\\begin{eqnarray} \\frac{\\partial c_j(t)}{\\partial t} + \n\\lambda_j c_j(t)=0 \\label{eq:ODE}\\end{eqnarray} \nfor all $j$. The solution of equation\n(\\ref{eq:ODE}) is given by $c_j(t) = b_je^{-\\lambda_j t}$. So we\nhave a solution \n", "index": 43, "text": "$$g(p,t) = \\sum_{j=0}^{\\infty} b_j e^{-\\lambda_j t}\n\\psi_j(p).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"g(p,t)=\\sum_{j=0}^{\\infty}b_{j}e^{-\\lambda_{j}t}\\psi_{j}(p).\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><msub><mi>b</mi><mi>j</mi></msub><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nThe coefficients $b_j$ must be the Fourier coefficients, i.e.\n\n", "itemtype": "equation", "pos": 34592, "prevtext": " \nAt $t=0$, we have\n\n", "index": 45, "text": "$$g(p,0) = \\sum_{j=0}^{\\infty} b_j \\psi_j(p) = f(p).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"g(p,0)=\\sum_{j=0}^{\\infty}b_{j}\\psi_{j}(p)=f(p).\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><msub><mi>b</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\n\\\\\n\n\n\\noindent{\\bf Theorem 3.} The statistical power $\\mathcal{P}$ of testing the hypotheses\n\n", "itemtype": "equation", "pos": 34709, "prevtext": "\nThe coefficients $b_j$ must be the Fourier coefficients, i.e.\n\n", "index": 47, "text": "$$ b_j =  \\langle f,\\psi_j \\rangle = f_j. \\; \\square$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"b_{j}=\\langle f,\\psi_{j}\\rangle=f_{j}.\\;\\square\" display=\"block\"><mrow><mrow><msub><mi>b</mi><mi>j</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>f</mi><mo>,</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><msub><mi>f</mi><mi>j</mi></msub></mrow><mo rspace=\"5.3pt\">.</mo><mi mathvariant=\"normal\">\u25a1</mi></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nusing the T random field $T(p)$  is given by\n\n", "itemtype": "equation", "pos": 23518, "prevtext": "\nwhere $\\mu_d(\\mathcal{M})$ is the $j$-th\nMinkowski functional or intrinsic volume of $\\mathcal{M}$ and\n$\\rho_j$ is the $j$-th Euler characteristic (EC) density of T-field. \n\nSince the hyoid bone is compact with no boundary but has three disconnected components, the Minkowski functionals are simply \n\\begin{eqnarray*} \\mu_2(\\mathcal{M}) &=& \\mbox{area}(\\mathcal{M})/2\\\\\n\\mu_1(\\mathcal{M}) &=& 0\\\\\n\\mu_0(\\mathcal{M}) &=& \\chi(\\mathcal{M}) = 3 \\times 2.\n\\end{eqnarray*} \nThe term $\\mu_1$ is zero since there is no boundary and $\\mu_0$ is simply the Euler characteristic of the template surface. Note that the Euler characteristic of a closed surface with no hole or handle is 2 and there are three such surfaces. The EC-densities of the T-field with $\\nu$ degrees of freedom is given by\n\\begin{eqnarray*} \\rho_0 (z)&=& 1 - P(T_{\\nu} \\leq z),\\\\\n\\rho_1 (z) & = &   \\frac{1}{\\sqrt{2t^2}} \\cdot \\frac{1}{2\\pi} \\Big( 1+  \\frac{z^2}{\\nu})^{-(\\nu-1)/2}, \\\\\n\\rho_2 (z) & = & \\frac{1}{2t^2} \\cdot \\frac{1}{(2\\pi)^{3/2}} \\frac{\\Gamma(\\frac{\\nu +1}{2})}\n{(\\frac{\\nu}{2})^{1/2} \\Gamma(\\frac{\\nu}{2})}  z \\Big( 1 + \\frac{z^2}{\\nu} \\Big)^{-(\\nu -1)/2}.    \\end{eqnarray*}\nThe EC-density of the F-field is similarly given in \\cite{worsley.2004,\ntaylor.2007}. The EC-density has the kernel bandwidth $2t^2$ in the formulation so the inference is done at a particular smoothing scale. Figure \\ref{fig:powerplot} shows the type-I error plot over different bandwidth $t$ of the kernel regression in our application. As the bandwidth $t$ becomes zero, the type-I error increases. When $t=0$, the kernel regression collapse to the usual Fourier series expansion. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data without any smoothing. Hence, the proposed kernel regression can be viewed as having substantially smaller type-I error compared to the Fourier series expansion as well as  the original data demonstrating a better statistical performance. \n\nType-II error and the statistical power can be also computed similarly. \n\\begin{theorem}The statistical power $\\mathcal{P}$ of testing the hypotheses\n\n", "index": 29, "text": "$$ H_0: h(p)  = 0 \\; \\mbox{ for all } p \\in \\mathcal{M} \\; \\mbox{ vs. } \\; H_1: h(p) = c\\sigma > 0 \\mbox{ for some } p \\in \\mathcal{M}. $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"H_{0}:h(p)=0\\;\\mbox{ for all }p\\in\\mathcal{M}\\;\\mbox{ vs. }\\;H_{1}:h(p)=c%&#10;\\sigma&gt;0\\mbox{ for some }p\\in\\mathcal{M}.\" display=\"block\"><mrow><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+2.8pt\"><mn>0</mn></mpadded><mo>\u2062</mo><mtext>\u00a0for all\u00a0</mtext><mo>\u2062</mo><mi>p</mi></mrow><mo>\u2208</mo><mrow><mpadded width=\"+2.8pt\"><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mpadded><mo>\u2062</mo><mpadded width=\"+2.8pt\"><mtext>\u00a0vs.\u00a0</mtext></mpadded><mo>\u2062</mo><msub><mi>H</mi><mn>1</mn></msub></mrow></mrow><mo>:</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>\u03c3</mi></mrow><mo>&gt;</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0for some\u00a0</mtext><mo>\u2062</mo><mi>p</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $t^*_{\\alpha}$ is the $\\alpha$-quantile given by\n\n", "itemtype": "equation", "pos": 23702, "prevtext": "\nusing the T random field $T(p)$  is given by\n\n", "index": 31, "text": "$$\\mathcal{P}(n) \\approx 1- \\exp \\Big[ - \\sum_{j=0}^d \\mu_j(\\mathcal{M}_1)\\rho_j(t^*_{\\alpha} - c\\sqrt{n}) \\Big],$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}(n)\\approx 1-\\exp\\Big{[}-\\sum_{j=0}^{d}\\mu_{j}(\\mathcal{M}_{1})\\rho%&#10;_{j}(t^{*}_{\\alpha}-c\\sqrt{n})\\Big{]},\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">[</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c1</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "  \n\\end{theorem}\n{\\em Proof.} See the Appendix.\n\n\\section{Application}\n\n\\subsection{CT Imaging Data and Preprocessing}\n\nThe study consists of high resolution CT images of 70 normal subjects ages between 0 and 20 years (mean age $=$ 58.0 $\\pm$ 11.3 years). \nCT scans were converted to DICOM format and Analyze $8.1$ software package (AnalyzeDirect, Inc., Overland Park, KS) was then used in segmenting binary hyoid bone images by a trained individual rater in the native space by simple image intensity thresholding and careful manual editing. A nonlinear image registration using the diffeomorphic shape and intensity averaging technique with cross-correlation as similarity metric was performed through Advanced Normalization Tools (ANTS) \\cite{avants.2008}. A study-specific template was constructed. We have chosen a 12 year old subject identified as F155 as the initial template and aligned the remaining 69 hyoids to the initial template affinely to remove the overall size variability. Some subject may have larger hyoid than others so it is necessary to remove the global size differences in local shape modeling. From the affine transformed individual hyoid surfaces, we performed the diffeomorphic nonlinear image registration to the template using ANTS. \n\nThen by averaging the inverse deformation fields from the initial template to individual hyoid, we obtain the yet another final template. Figure \\ref{fig:template} shows the initial and final templates. The isosurface of the final template volume is extracted using the marching cube algorithm \\cite{lorensen.1987}. Figure \\ref{fig:agegrouping} shows the mean displacement differences between the groups I and II (top) and II and III (bottom). Each row shows the group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the growth direction given by the mean displacement differences and colors indicate their lengths in mm. We are interested in localizing the regions of hyoid bone growth between the age groups. \n\n\n70 subjects are binned into three age categories: ages between 0 and 6 years (group I), between 7 and 12 years (group II), and between 13 and 19 years (group III). There are 26, 14 and 30 subjects in group I, II and III respectively. The main biological hypothesis of interest is if there is any localized hyoid bone growth spurts between these specific age groups. \n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-preprocessing.pdf} \n\\caption{Left: Hyoid F155 which forms an initial template $\\mathcal{M}_I$. All other mandibles are affine registered to F155. Middle: The superimposition of affine registered hyois showing local misalignments. Diffeomorphic registration is then performed to register misaligned affine transformed hyoids. \nRight: The average of deformation with respect to F155 provides the final population average template $\\mathcal{M}_F$ where statistical parametric maps will be constructed.}\n\\label{fig:template}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width= 1 \\linewidth]{hyoid-groupdisp.pdf} \n\\caption{Hyoid bones are binned into three age groups: group I (ages 0 and 6), group II (ages 7 and 12) and group III (ages 13 and 19) and the mean displacements between the groups are visualized. Each row shows the mean group differences of the displacement: group II - group I (first row) and group III - group II (second row). The arrows are the mean displacement differences and colors indicate their lengths in mm.}\n\\label{fig:agegrouping}\n\\end{figure}\n\n\n\n\n\\subsection{Results}\n\nThe displacement from the template to an individual surface is obtained at each mesh vertex. Since  the length measurement provides a much easier biological interpretation, we used the length of displacement vector as a response variable among many other possible features. Since the length on the template surface is expected to be noisy due to image acquisition, segmentation and image registration errors, it is necessary perform the proposed kernel regression and subsequently reduce the type-I error and obtain more stable SPM. Figure \\ref{fig:wavelets} shows an example of kernel regression on our data. The kernel regression increases the signal-to-noise ratio (SNR) and improves the smoothness and Gaussianness of data. Subsequently, the heat kernel regression of the displacement length is taken as the response variable. \nWe have chosen $t=5$ as the bandwidth for the study since the bandwidth 5 is where the type-I error starts to flatten out in Figure \\ref{fig:powerplot}. Note that the Fourier expansion with 500 LB-eigenfunctions is close to the original data (relative error of less than 0.3$\\%$). Hence, performing the proposed kernel regression before the statistical analysis can substantially smaller type-I error demonstrating its effectiveness. \n\n\n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{hyoid-Fstatsigma5.pdf}\n\\caption{F-statistic maps on hyoid showing age effect between the groups. The significant growth regions (red) are identified only between group II and III, and I and III. The growth is highly localized near the regions that connect the disconnected hyoid bones.}\n\\label{fig:surfGLM}\n\\end{center} \n\\end{figure}\n\nAfter the displacement lengths are smoothed, we constructed the F-field, or equivalently the T-field square, for testing the length difference between the age groups I and II, II and III, and I and III showing the regions of growth spurts between different age range (Figure \\ref{fig:surfGLM}). Since test statistics are constructed over all mesh vertices on the mandible, multiple comparisons were account  using the random field theory \\cite{worsley.1995,worsley.2004}. \n\nFor testing the differences between the groups I and II, II and III, and I and III, they are based on F-field with  1 and  38, 1 and 42, and 1 and  54 degrees of freedom respectively. The result is displayed in Figure \\ref{fig:surfGLM}, where the significant results were only found between the groups II and III (middle), and I and III (bottom) at 0.1 level. Between the groups II and III, we obtained the maximum F-statistic value of 9.36 (right hyoid), which corresponds to the p-value of 0.041 (corrected). Between the groups I and III, we obtained the maximum F-statistic value of 10.55 (middle hyoid), which corresponds to the p-value of 0.028 (corrected). In the $F$-statistic maps for middle and bottom rows, red regions are considered as exhibiting significant growth spurts. \n\n\n\n\n\\section{Conclusion}\nWe have developed a new kernel regression framework on a manifold that unifies bivariate kernel regression, heat diffusion and wavelets in a single coherent mathematical framework. The kernel regression is both global and local in a sense it uses global basis functions to perform regression but locally equivalent to diffusion wavelet transform. The proposed framework is demonstrated to reduce type-I error in modeling shape variations compared to the usual Fourier series expansion. The method is then used in developing a statistical inference procedure for functional signals on manifolds. The whole framework \n\n\\section*{Acknowledgment}\nThis work was supported by NIH Research Grant R01 DC6282 (MRI and CT Studies of the Developing Vocal Tract), from the National Institute of Deafness and other Communicative Disorders (NIDCD);  core grant P-30 HD03352 to the Waisman Center from the National Institute of Child Health and Human Development (NICHHD) and Clinical and Translational Science Award (CTSA) program, through the NIH National Center for Advancing Translational Sciences (NCATS) UL1TR000427\n\n\n\\bibliographystyle{plain}\n\\bibliography{reference.2016.01.10}\n\n\n\\newpage\n\\section*{Appendix}\n{\\bf Theorem 1}\\\\\n\\begin{eqnarray*} \\widehat{h}(p)  = \\arg\\min_{h \\in \\mathcal{H}_k}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}} K(p,q) \\Big| f(q) - h(p) \\Big|^2 \\; d\\mu(q)\\; d\\mu(p) = \\sum_{j=0}^k  \\tau_j f_j \\psi_j.\n\\end{eqnarray*}\n{\\bf Proof.}\nAny function $h \\in \\mathcal{H}_k$ can be expressed as \n\\begin{eqnarray} h(p) = \\sum_{j=0}^k \\beta_j \\psi_j(p) \\label{eq:hp}.\\end{eqnarray}\nThen by plugging (\\ref{eq:hp}) into the inner integral $I(p)$, \nit becomes\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $t^*_{\\alpha}$ is the $\\alpha$-quantile given by\n\n", "index": 33, "text": "$$\\alpha = P \\Big( \\sup_{p \\in \\mathcal{M}} T(p) > t^*_{\\alpha} \\Big).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\alpha=P\\Big{(}\\sup_{p\\in\\mathcal{M}}T(p)&gt;t^{*}_{\\alpha}\\Big{)}.\" display=\"block\"><mrow><mi>\u03b1</mi><mo>=</mo><mi>P</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></munder><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nIn the region $\\mathcal{M}_1$ corresponding to $H_1$, \n\n", "itemtype": "equation", "pos": 35356, "prevtext": "  \n{\\bf Proof.} \nIn the region $\\mathcal{M}_0$ corresponding to $H_0$, \n\n", "index": 55, "text": "$$f^i(p) \\sim N(0, \\sigma^2).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"f^{i}(p)\\sim N(0,\\sigma^{2}).\" display=\"block\"><mrow><mrow><mrow><msup><mi>f</mi><mi>i</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nFigure \\ref{fig:poweranalysisJ0} illustrates this setting.\n \n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=0.5\\linewidth]{poweranalysisJ0.pdf}\n\\caption{Schematic of when $H_1$ is true.}\n\\label{fig:poweranalysisJ0}\n\\end{center} \n\\end{figure}\n\nConsider the test statistic\n\\begin{eqnarray} T(p) = \\frac{\\bar f(p)}{S(p)/\\sqrt{n}}, \\label{eq:T} \\end{eqnarray} \nwhere $\\bar f$ and $S$ are the sample mean and standard deviation of the measurements $f^i, \\cdots, f^n$. In $\\mathcal{M}_0$, $T(p)$ is a T random field with $n-1$ degrees of freedom \\cite{adler.1981}. In $\\mathcal{M}_1$, $T(p)$ can be written as \n\n", "itemtype": "equation", "pos": 35443, "prevtext": "\nIn the region $\\mathcal{M}_1$ corresponding to $H_1$, \n\n", "index": 57, "text": "$$f^i(p) \\sim N( c \\sigma, \\sigma^2).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"f^{i}(p)\\sim N(c\\sigma,\\sigma^{2}).\" display=\"block\"><mrow><mrow><mrow><msup><mi>f</mi><mi>i</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>\u03c3</mi></mrow><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $T'(p)$ a T random field with $n-1$ degrees of freedom. Since $\\sigma$ is usually estimated using the standard deviation, approximately we have $S(p) = \\sigma$ and the test statistic becomes\n\n", "itemtype": "equation", "pos": 36098, "prevtext": "\nFigure \\ref{fig:poweranalysisJ0} illustrates this setting.\n \n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=0.5\\linewidth]{poweranalysisJ0.pdf}\n\\caption{Schematic of when $H_1$ is true.}\n\\label{fig:poweranalysisJ0}\n\\end{center} \n\\end{figure}\n\nConsider the test statistic\n\\begin{eqnarray} T(p) = \\frac{\\bar f(p)}{S(p)/\\sqrt{n}}, \\label{eq:T} \\end{eqnarray} \nwhere $\\bar f$ and $S$ are the sample mean and standard deviation of the measurements $f^i, \\cdots, f^n$. In $\\mathcal{M}_0$, $T(p)$ is a T random field with $n-1$ degrees of freedom \\cite{adler.1981}. In $\\mathcal{M}_1$, $T(p)$ can be written as \n\n", "index": 59, "text": "$$T(p) = T'(p) + \\frac{c\\sigma}{S(p)/\\sqrt{n}},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"T(p)=T^{\\prime}(p)+\\frac{c\\sigma}{S(p)/\\sqrt{n}},\" display=\"block\"><mrow><mrow><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>\u03c3</mi></mrow><mrow><mrow><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msqrt><mi>n</mi></msqrt></mrow></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nAt each fixed $p$, $T(p)$ is no longer a T random field but a non-central T random field \\cite{hayasaka.2007}. \nSubsequently the power $\\mathcal{P}$ at the given $\\alpha$-level is given by\n\\begin{eqnarray} \\mathcal{P}(n) &=& P \\Big( \\sup_{p \\in \\mathcal{M}_1} T(p) > t^*_{\\alpha} \\Big) \\label{eq:Tdiff}\\\\\n                             &=&  P \\Big(  \\sup_{p \\in \\mathcal{M}_1}  T'(p)  > t^*_{\\alpha} - c\\sqrt{n} \\Big), \\label{eq:supP}\\end{eqnarray}\nwhere $t_{\\alpha}^*$ is the $\\alpha$-quantile of $\\sup_{p \\in \\mathcal{M}} T(p)$ under $H_0$, i.e. \n\n", "itemtype": "equation", "pos": 36345, "prevtext": "\nwhere $T'(p)$ a T random field with $n-1$ degrees of freedom. Since $\\sigma$ is usually estimated using the standard deviation, approximately we have $S(p) = \\sigma$ and the test statistic becomes\n\n", "index": 61, "text": "$$T(p) = T'(p) + c\\sqrt{n}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"T(p)=T^{\\prime}(p)+c\\sqrt{n}.\" display=\"block\"><mrow><mrow><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>T</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\n                   \n\n\n          \nAlthough (\\ref{eq:Tdiff}) is intractable to directly compute, we will approximate (\\ref{eq:supP}) using the expected Euler characteristic (EC) method \\cite{worsley.1996,worsley.2003}. The power (\\ref{eq:supP}) can be written as \n\n", "itemtype": "equation", "pos": 36922, "prevtext": "\nAt each fixed $p$, $T(p)$ is no longer a T random field but a non-central T random field \\cite{hayasaka.2007}. \nSubsequently the power $\\mathcal{P}$ at the given $\\alpha$-level is given by\n\\begin{eqnarray} \\mathcal{P}(n) &=& P \\Big( \\sup_{p \\in \\mathcal{M}_1} T(p) > t^*_{\\alpha} \\Big) \\label{eq:Tdiff}\\\\\n                             &=&  P \\Big(  \\sup_{p \\in \\mathcal{M}_1}  T'(p)  > t^*_{\\alpha} - c\\sqrt{n} \\Big), \\label{eq:supP}\\end{eqnarray}\nwhere $t_{\\alpha}^*$ is the $\\alpha$-quantile of $\\sup_{p \\in \\mathcal{M}} T(p)$ under $H_0$, i.e. \n\n", "index": 63, "text": "$$\\alpha = P \\Big( \\sup_{p \\in \\mathcal{M}} T(p) > t_{\\alpha}^* \\Big).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\alpha=P\\Big{(}\\sup_{p\\in\\mathcal{M}}T(p)&gt;t_{\\alpha}^{*}\\Big{)}.\" display=\"block\"><mrow><mi>\u03b1</mi><mo>=</mo><mi>P</mi><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></munder><mi>T</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nwhere $\\mu_d(\\mathcal{M})$ is the $j$-th\nMinkowski functional or intrinsic volume of $\\mathcal{M}$ and\n$\\rho_j$ is the $j$-th EC-density of\nT-field \\cite{worsley.1998,adler.1981,taylor.2007,worsley.2003}. \nThe expansion only works for sufficiently large $t^*_{\\alpha} - c\\sqrt{n}$. For small threshold, the power may not be bounded between 0 and 1. So it is necessary to use the exponential transform used in \\cite{hayasaka.2007} to bound the power. For small $\\mathcal{P}(n)$, using the Taylor expansion, we can write\n\n", "itemtype": "equation", "pos": 37257, "prevtext": "\n                   \n\n\n          \nAlthough (\\ref{eq:Tdiff}) is intractable to directly compute, we will approximate (\\ref{eq:supP}) using the expected Euler characteristic (EC) method \\cite{worsley.1996,worsley.2003}. The power (\\ref{eq:supP}) can be written as \n\n", "index": 65, "text": "$$\\mathcal{P}(n) = \\sum_{j=0}^d \\mu_j(\\mathcal{M}_1)\\rho_j(t^*_{\\alpha} - c\\sqrt{n}),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}(n)=\\sum_{j=0}^{d}\\mu_{j}(\\mathcal{M}_{1})\\rho_{j}(t^{*}_{\\alpha}-c%&#10;\\sqrt{n}),\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c1</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nEquivalently, it is written as \n\n", "itemtype": "equation", "pos": 37864, "prevtext": "\nwhere $\\mu_d(\\mathcal{M})$ is the $j$-th\nMinkowski functional or intrinsic volume of $\\mathcal{M}$ and\n$\\rho_j$ is the $j$-th EC-density of\nT-field \\cite{worsley.1998,adler.1981,taylor.2007,worsley.2003}. \nThe expansion only works for sufficiently large $t^*_{\\alpha} - c\\sqrt{n}$. For small threshold, the power may not be bounded between 0 and 1. So it is necessary to use the exponential transform used in \\cite{hayasaka.2007} to bound the power. For small $\\mathcal{P}(n)$, using the Taylor expansion, we can write\n\n", "index": 67, "text": "$$\\exp \\big[ - \\mathcal{P}(n) \\big]  \\approx 1 - \\mathcal{P}(n).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\exp\\big{[}-\\mathcal{P}(n)\\big{]}\\approx 1-\\mathcal{P}(n).\" display=\"block\"><mrow><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow><mo>\u2248</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\nThis transformation guarantees the power estimation to be bound between 0 and 1 \\cite{hayasaka.2007}. Subsequently, the power is given by\n\n", "itemtype": "equation", "pos": 37963, "prevtext": "\nEquivalently, it is written as \n\n", "index": 69, "text": "$$\\mathcal{P}(n) \\approx 1 - \\exp \\big[ - \\mathcal{P}(n) \\big].$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}(n)\\approx 1-\\exp\\big{[}-\\mathcal{P}(n)\\big{]}.\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03474.tex", "nexttext": "\n\n\n\n", "itemtype": "equation", "pos": 38167, "prevtext": "\nThis transformation guarantees the power estimation to be bound between 0 and 1 \\cite{hayasaka.2007}. Subsequently, the power is given by\n\n", "index": 71, "text": "$$\\mathcal{P}(n) = 1- \\exp \\Big[- \\sum_{j=0}^d \\mu_j(\\mathcal{M}_1)\\rho_j(t^*_{\\alpha} - c\\sqrt{n}) \\Big]. \\square$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{P}(n)=1-\\exp\\Big{[}-\\sum_{j=0}^{d}\\mu_{j}(\\mathcal{M}_{1})\\rho_{j}(t^%&#10;{*}_{\\alpha}-c\\sqrt{n})\\Big{]}.\\square\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">[</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c1</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>t</mi><mi>\u03b1</mi><mo>*</mo></msubsup><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><msqrt><mi>n</mi></msqrt></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo><mi mathvariant=\"normal\">\u25a1</mi></mrow></math>", "type": "latex"}]