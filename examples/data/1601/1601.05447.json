[{"file": "1601.05447.tex", "nexttext": "\n$M$ and $N$ are the height and width of an image. \n\n\\subsection{Spatio-Temporal Contours and VOP} \\label{spatio-temp-contours}\n\nOptical flow field between pairs of consecutive frames provide approximate  motion contours. We perform 2-frame forward optical flow \\cite{TBroxOF11} for every consecutive frame-pair. At every pixel, the magnitude of the flow field's gradient and the difference in direction of motion from its neighbor \\cite{Papazoglou13ICCV}, contribute to the measure of the motion contour. To address incompleteness and inaccuracies of two-frame optical flow estimation, we analyze mid-range optical flow over a subset of video frames. Within a sub-sequence, we approximate which pixels consistently reside inside a moving  object using inside-outside maps \\cite{Papazoglou13ICCV}. \n\nIn our experiments, the inside-outside maps, accumulated over 3 - 5 frames, provide good estimates of time-consistent gross location priors for moving objects. In Section \\ref{Streaming_Clustering}, We describe how this mid-range accumulation can effectively be exploited for object label propagation through streaming clustering. A simple edge detector on this location prior is called temporal edge, $\\mathbf{E_t}$.   \n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale=0.42]{spatio-temporal-edgebox-1}\n\t\\includegraphics[scale=0.42]{spatio-temporal-edgebox-2}\n\\end{center}\n   \\caption{Spatio-Temporal EdgeBoxes. Clock-wise from top-left - (i) A video frame from Youtube-Objects dataset, (ii) Temporal edge $\\mathbf{E}_t$ \\ie normalized gradient of location prior from motion analysis, (iii) Spatial edge $\\mathbf{E}_s$ \\ie structured edge detection on video frame, (iv) linear combination of spatial and temporal edge $\\mathbf{E}$ brings out the prominent contours of both static and/or moving objects.  }\n\\label{fig:spatio-temporal-edgebox}\n\\end{figure}\n\nEdgeBoxes \\cite{Dollar2014ECCV} employs efficient data structures to score millions of candidates based on the difference of number of spatial contours  that exist in the box and those that straddle box's boundary. We use a similar scoring strategy, but on spatio-temporal edge $\\mathbf{E} \\in \\mathbb{R}^{M\\times N}_{\\geq 0}$ which is formed according to Eq \\ref{spatio-temp-edge}. \n{\\vspace{-2.5mm}} \n\n", "itemtype": "equation", "pos": 8821, "prevtext": "\n\n\n\n\\title{Detecting Temporally Consistent Objects in Videos through Object Class Label Propagation}\n\n\n\\author{Subarna Tripathi\\\\\nUCSD\\\\\n\n{\\tt\\small stripathi@ucsd.edu}\n\n\n\n\n\n\\and\nSerge Belongie\\\\\nCornell University\\\\\n\n{\\tt\\small sjb344@cornell.edu}\n\\and\nYoungbae Hwang\\\\\nKETI\\\\\n\n{\\tt\\small ybhwang@keti.re.kr}\n\\and\nTruong Nguyen\\\\\nUCSD\\\\\n\n{\\tt\\small tqn001@eng.ucsd.edu}\n}\n\n\\maketitle\n\n\n\\begin{abstract}\n\n\nObject proposals for detecting moving or static video objects need to address issues such as speed, memory complexity and temporal consistency. \nWe propose an efficient Video Object Proposal (VOP) generation method and show its efficacy in learning a better video object detector. A deep-learning based  \nvideo object detector learned using the proposed VOP achieves state-of-the-art detection performance on the Youtube-Objects dataset. \n\nWe further propose a clustering of VOPs which can efficiently be used for detecting objects in video in a streaming fashion. As opposed to applying per-frame convolutional neural network (CNN) based object detection, our proposed method called Objects in Video Enabler thRough LAbel Propagation (OVERLAP) needs to classify only a small fraction of all candidate proposals in every video frame through streaming clustering of object proposals and class-label propagation. Source code will be made available upon publication.\n     \n\\end{abstract}\n{\\vspace{-2.5mm}}\n\n\n\\section{Introduction}\n\nObject proposal generation is a common pre-processing step for object detection in images, which is a key challenge in computer vision. Object proposals dramatically decrease the number of detection hypotheses to be assessed. Thus, use of CNN-features \\cite{girshick14CVPR}, which is more effective but computationally expensive, have turned out to be feasible for accurate detection. \nFor the detection of video objects, proposals not only need to consider the space-time complexity, but also need to address the temporal consistency. \nWe propose generating Video Object Proposals (VOP) by scoring candidate windows based on spatio-temporal edge content and show that these VOPs help in learning  better video object detectors. Further, we propose an efficient online clustering of these proposals in order to process arbitrary long videos. \n\n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale=0.45]{OVERLAP.png}\n\\end{center}\n\\setlength{\\abovecaptionskip}{12pt} \n   \\caption{OVERLAP: Object class labels get propagated by streaming clustering of temporally consistent proposals. The system classifies only those VOPs which belong to a new cluster.}\n\\label{OVERLAP framework}\n{\\vspace{-2.5mm}} \n\\end{figure} \n\n\n\nWe show that the joint-analysis of all such windows provides a way towards multiple object segmentations and helps in reducing test time object detection complexity. \n\nWe divide a video into sub-sequences with one-frame overlap in a streaming fashion \\cite{XuXiCoECCV2012, StreamGBHppST14}. We analyze all candidate windows jointly within a sub-sequence followed by affinity-based clustering to produce temporally consistent clusters of object proposals at every video frame. \n\nThe advantage of performing a streaming spatio-temporal clustering on the object proposals is that it enables an easy label propagation through the video in an online framework. Presumably all object proposals of a cluster have the same object class type. We propose deep-learning based video object detection through objects' class label propagation using online clustering of VOPs. \nAs opposed to applying R-CNN \\cite{girshick14CVPR}-like approaches, which essentially classify every window based on the expensive CNN features, at every video frame, the proposed method of label propagation requires detection/classification only on video frames which has new clusters\n(fig. \\ref{OVERLAP framework})\n.\nOur main contributions are as follows:\n{\\vspace{-2.5mm}} \n\\begin{itemize}\n  \\item We present a simple yet effective Video Object Proposal (VOP) method for detecting moving and static video objects by quantifying the spatio-temporal edge contents;\n  {\\vspace{-2.5mm}}\n  \\item We present a \\textbf{novel algorithm}, ``Objects in Video Enabler thRough LAbel Propagation'' (OVERLAP), that exploits objects' class label propagation through streaming clustering of VOPs to efficiently detect objects in video with  temporal consistency. \n\\end{itemize}\n{\\vspace{-2.5mm}}\nWe also present the following minor contributions:\n{\\vspace{-2.5mm}} \n\\begin{itemize}\n  \\item Demonstrating VOP's efficacy in learning a better CNN-based video object detector model;\n  {\\vspace{-2.5mm}} \n  \\item Object segmentation as a by-product of object detection framework, OVERLAP. \n\\end{itemize}\n\n\n\n\n\n\n\n\\section{Related Works}\n\nThere are several approaches towards video object detection which broadly fall into three categories : (1) image object proposals for each frame (2) motion segmentation in video (3) supervoxel aggregation. \n\n\\textbf{Object Proposals and detection:} \nUnsupervised category-independent detection proposals are evidently shown to be effective for object detection in images. \nSome of these methods are Objectness \\cite{Objectness2010CVPR}, category-independent object proposals \\cite{Endres10ECCV}, SelectiveSearch \\cite{SelectiveSearch2013IJCV}, MCG \\cite{APBMM2014}, GOP \\cite{GOP2014}, BING \\cite{BingObj2014}, EdgeBoxes  \\cite{Dollar2014ECCV}. A comparative literature survey on object proposal methods and their evaluations can be found in \\cite{Hosang2014Bmvc,Hosang2015pami}. \n\nAlthough there is no ``best'' detection proposal method, EdgeBoxes, which scores windows based on edge content, achieve better balance between recall and repeatabilty. \n\n\n\nApplying image object proposals directly for each frame in video may be problematic due to time complexity and temporal consistency. In addition, issues like motion blur and compression artifacts can pose significant obstacles to identifying spatial contours, which degrades the object proposal qualities.\n\nRecent advances like SPPnet \\cite{SPP_NET_HeZR014}, Fast R-CNN \\cite{fast_RCNN_15}, and Faster R-CNN \\cite{Faster_RCNN_RenHG015} have dramatically reduced the running time by computing deep features for all image locations at the same time and snapping them on appropriate proposal boxes. Per-frame object detection still needs classification of proposal windows and temporal consistency still remains a challenge. The proposed framework dispenses with the need of classifying every candidate window of every video frame through  spatio-temporal clustering, thus addressing temporal consistency. \n\n\n\n\n\n\n\n\\textbf{Motion Segmentation in Video:}\nMotion based segmentation is the task of separating moving foreground objects from the background. Several popular methods of motion segmentation include the  layered Directed Acyclic Graph (DAG) based framework \\cite{Zhang13CVPR}, Maximal weight cliques \\cite{Ma12CVPR}, fast motion segmentation \\cite{Papazoglou13ICCV}, tracking many segments \\cite{Li_2013_ICCV}, identifying key segments \\cite{key_segments_Lee11}, and many more. \n\nAlthough video motion segmentation can detect moving foregrounds robustly, it is not easy to detect multiple objects or if objects suddenly stop or change motion abruptly.\n\n\\textbf{Supervoxel aggregation:}\nSpatio-temporal object proposals have been considered in the context of aggregating supervoxels with spatio-temporal connectivity between neighboring labels. Jain {\\em et al} \\cite{Jain14CVPR} developed an extension of the hierarchical clustering method of SelectiveSearch \\cite{SelectiveSearch2013IJCV} to obtain object proposals in video. Even though their independent motion evidence effectively segment objects with motions from the background, static objects can not be recovered. Oneata {\\em et al.} \\cite{SpTmp-Obj_prop14} presented spatio-temporal object proposals by a randomized supervoxel merging process. \n\nSharir {\\em et al.} \\cite{VidObjProp12} proposed the extension of category-independent object proposals \\cite{Endres10ECCV} from image to video by extracting object proposals at each frame and linking across frames into object hypotheses in a framework of graph-based segmentation using higher-order potentials leading to a high computational expense.\nAll these supervoxels often cannot replace the object proposal step for object detection either due to its complexity or the associated over-segmentations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Video Object Proposals}\n\nWe extend EdgeBoxes \\cite{Dollar2014ECCV} from generating image object proposals to video object proposals. \nIn addition to the spatial edge responses, $\\mathbf{E_s}$, at every pixel in EdgeBoxes \\cite{Dollar2014ECCV}, we consider exploiting temporal edge responses, $\\mathbf{E_t}$, at every pixel location using mid-range optical flow analysis. \n\n{\\vspace{-2.5mm}} \n\n", "index": 1, "text": "\\begin{equation} \\label{edge_define}\n\\mathbf{E_t}, \\mathbf{E_t} \\in \\mathbb{R}^{M\\times N}_{\\geq 0}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E_{t}},\\mathbf{E_{t}}\\in\\mathbb{R}^{M\\times N}_{\\geq 0}\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc04</mi><mi>\ud835\udc2d</mi></msub><mo>,</mo><msub><mi>\ud835\udc04</mi><mi>\ud835\udc2d</mi></msub></mrow><mo>\u2208</mo><msubsup><mi>\u211d</mi><mrow><mi/><mo>\u2265</mo><mn>0</mn></mrow><mrow><mi>M</mi><mo>\u00d7</mo><mi>N</mi></mrow></msubsup></mrow></math>", "type": "latex"}, {"file": "1601.05447.tex", "nexttext": "\nAs the value of $\\lambda$ increases, the system favors detecting only moving objects. One example of spatio-temporal edge is demonstrated in figure \\ref{fig:spatio-temporal-edgebox}. A linear combination of spatial and temporal edge responses represents spatio-temporal contours. This enables a simple yet efficient strategy for scoring based on spatio-temporal edge content through edge groups. We find intersecting edge groups along horizontal and vertical boundaries using two efficient data structures\\cite{Dollar2014ECCV}. We use the integral image-based implementation to speed up scoring of boxes in sliding window fashion.\n\n\n\n\nAs described in the next section, object proposals based on these spatial-temporal contours outperforms those based on only spatial contours for video object detection. The presence of motion blur in spatial edges affects the performance of spatial contour based proposal prediction in video frames. In practice, $\\lambda=0.2$ to $0.5$ works well for Youtube-Objects dataset. \n\n\n\n\n\\section{Learning Video Object Detector Model}\n\n\n\nWe aim for detecting objects in generic consumer videos. Due to the domain shift issues between images and video frames \\cite{KalogeitonFS15}, our 10-class video object detection uses supervised pre-training from ImageNet reference model for classification and fine-tuning on annotated frames from Youtube-Objects dataset v2.0 \\cite{youtube-Objects, KalogeitonFS15, ObjClsDet2012} for video objects detection.  \n\n\n\\textbf{Youtube-Objects dataset.} The dataset is composed of videos collected from Youtube by querying for the names of 10 object classes of the PASCAL VOC Challenge. It contains 155 videos in total and between 9 and 24 videos for each class. The duration of each video varies between 30 seconds and 3 minutes. However, only $6087$ frames are annotated with a bounding-box around an object instance. Hence, the number of annotated samples is approximately 4 times smaller than in PASCAL VOC.\n\n\nThe bottom-up region proposal methods play an important role. Motion blur and compression artifacts affects the quality of spatial edges in video frames, thus, generating good object proposals becomes more challenging. This is to be noted that R-CNN \\cite{girshick14CVPR}, or, Fast R-CNN \\cite{fast_RCNN_15} are fine-tuned for image object detection task, especially for 20-class PASCAL VOC image object categories which is a superset of Youtube-objects categories\\textit{}.\n\n\\textbf{Feature extraction.} We extract a 4096-dimensional feature vector corresponding to each region proposal using GPU-based (GeForce GTX 680) the Caffe \\cite{Caffe13} implementation of the CNN described by Krizhevsky \\etal \\cite{AlexNet12}. Features are computed by forward propagating a mean-subtracted 227 $\\times$ 227 R-G-B image through five convolutional layers and two fully connected layers.\n\n\\textbf{Region Proposals.} We use approximately $2000$ candidate proposals per video frame to be processed for learning detectors. We investigate the object detection model with different region proposal methods such as selective search\\cite{SelectiveSearch2013IJCV}, EdgeBoxes\\cite{Dollar2014ECCV}. As the resolution of different videos varies from VGA to HD, we re-size every video frame to $500\\times 500$ before performing proposal generation task. \n\n\\textbf{Training.} \n\nWe discriminatively pre-train the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations, followed by domain specific fine-tuning by replacing the last layer of AlexNet \\cite{AlexNet12} model with $10+1$ softmax output layer. We use two-step initialization for fine-tuning as described in \\cite{InitializationTip14}. As per PASCAL detection criteria, we treat all region proposals with $\\geq$ 0.5 IoU overlap with a ground-truth box as positives for that class of the box and the rest as negatives. Once features are extracted and training labels are applied, we optimize one linear SVM per class. \n\n\\textbf{Test time detection.} During test time, approximately $500$ to $2000$ VOPs are generated. Then forward propagation is performed through the CNN to compute features. Finally, we perform scoring using per-class trained SVM similar to \\cite{girshick14CVPR} followed by non-maximum suppression.  \n\n\nIn the below section \\ref{OVERLAP}, we describe the alternate test time detection using OVERLAP through objects' class label propagation. \n\n\n\n\n\n\\section{OVERLAP: Objects in Video Enabler thRough LAbel Propagation } \\label{OVERLAP}\n\nClassical approach for object localization has traditionally been image window classification, where each window is scored independent of other candidate windows. Recently, more success in object detection has been reported by considering spatial relations to all other windows and their appearance similarity \\cite{VezhnevetsF15} with examplar-based associative embedding. \n\nOur approach towards video object detection considers spatial relationship and appearance similarity with windows within and even in other nearby video frames, yet in much simpler way through spatio-temporal clustering, to avoid classifying every candidate windows. \n\n\\subsection{Joint analysis of windows}\n\nWe aim to detect dissimilarity between VOPs generated within a sub-sequence based on a simple underlying principle: proposals corresponding to the same object exhibit higher statistical dependencies than proposals belonging to different objects. \n\nAs a motivation for our approach, we consider generating proposal boxes by perturbing the ground truth locations of PASCAL VOC 2007 objects' bounding boxes and observe the statistical association of those proposal boxes. Let, $A$ and $B$ denote generic features of neighboring proposal windows, where neighborhood is characterized by non-zero Intersection-Over-Union (u). We investigate the joint distribution over pairings ${A,B}$. Let, $\\mathit{p}(A,B;u)$ be the joint probability of features A and B of windows with spatial overlap value, $u$. Then, $P(A,B)$ could ideally be computed as :\n{\\vspace{-2.5mm}} \n\n", "itemtype": "equation", "pos": 11200, "prevtext": "\n$M$ and $N$ are the height and width of an image. \n\n\\subsection{Spatio-Temporal Contours and VOP} \\label{spatio-temp-contours}\n\nOptical flow field between pairs of consecutive frames provide approximate  motion contours. We perform 2-frame forward optical flow \\cite{TBroxOF11} for every consecutive frame-pair. At every pixel, the magnitude of the flow field's gradient and the difference in direction of motion from its neighbor \\cite{Papazoglou13ICCV}, contribute to the measure of the motion contour. To address incompleteness and inaccuracies of two-frame optical flow estimation, we analyze mid-range optical flow over a subset of video frames. Within a sub-sequence, we approximate which pixels consistently reside inside a moving  object using inside-outside maps \\cite{Papazoglou13ICCV}. \n\nIn our experiments, the inside-outside maps, accumulated over 3 - 5 frames, provide good estimates of time-consistent gross location priors for moving objects. In Section \\ref{Streaming_Clustering}, We describe how this mid-range accumulation can effectively be exploited for object label propagation through streaming clustering. A simple edge detector on this location prior is called temporal edge, $\\mathbf{E_t}$.   \n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale=0.42]{spatio-temporal-edgebox-1}\n\t\\includegraphics[scale=0.42]{spatio-temporal-edgebox-2}\n\\end{center}\n   \\caption{Spatio-Temporal EdgeBoxes. Clock-wise from top-left - (i) A video frame from Youtube-Objects dataset, (ii) Temporal edge $\\mathbf{E}_t$ \\ie normalized gradient of location prior from motion analysis, (iii) Spatial edge $\\mathbf{E}_s$ \\ie structured edge detection on video frame, (iv) linear combination of spatial and temporal edge $\\mathbf{E}$ brings out the prominent contours of both static and/or moving objects.  }\n\\label{fig:spatio-temporal-edgebox}\n\\end{figure}\n\nEdgeBoxes \\cite{Dollar2014ECCV} employs efficient data structures to score millions of candidates based on the difference of number of spatial contours  that exist in the box and those that straddle box's boundary. We use a similar scoring strategy, but on spatio-temporal edge $\\mathbf{E} \\in \\mathbb{R}^{M\\times N}_{\\geq 0}$ which is formed according to Eq \\ref{spatio-temp-edge}. \n{\\vspace{-2.5mm}} \n\n", "index": 3, "text": "\\begin{equation} \\label{spatio-temp-edge}\n\\mathbf{E = \\lambda E_t + (1-\\lambda )E_s} , \\mathbf{\\lambda} \\in [0,1]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E=\\lambda E_{t}+(1-\\lambda)E_{s}},\\mathbf{\\lambda}\\in[0,1]\" display=\"block\"><mrow><mrow><mi>\ud835\udc04</mi><mo>=</mo><mrow><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc04</mi><mi>\ud835\udc2d</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>\ud835\udfcf</mn><mo>-</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc04</mi><mi>\ud835\udc2c</mi></msub></mrow></mrow></mrow><mo>,</mo><mrow><mi>\u03bb</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05447.tex", "nexttext": "\n\nwhere, $u \\in [0,1]$, $w$ is a weighting function, $w(0) = 0$, and $Z$ is a normalization constant. To simplify the process, we use uniform weighting function and work in the discrete (quantized) space of $u$ and replace the integral with summation. We take the marginals of the distribution to get $P(A)$ and $P(B)$. Motivated by the  analysis presented in crisp boundary detection work by Isola \\textit{\\etal} \\cite{crisp_boundaries}, we model affinity with point-wise mutual information like function: \n{\\vspace{-2.5mm}} \n\n", "itemtype": "equation", "pos": 17374, "prevtext": "\nAs the value of $\\lambda$ increases, the system favors detecting only moving objects. One example of spatio-temporal edge is demonstrated in figure \\ref{fig:spatio-temporal-edgebox}. A linear combination of spatial and temporal edge responses represents spatio-temporal contours. This enables a simple yet efficient strategy for scoring based on spatio-temporal edge content through edge groups. We find intersecting edge groups along horizontal and vertical boundaries using two efficient data structures\\cite{Dollar2014ECCV}. We use the integral image-based implementation to speed up scoring of boxes in sliding window fashion.\n\n\n\n\nAs described in the next section, object proposals based on these spatial-temporal contours outperforms those based on only spatial contours for video object detection. The presence of motion blur in spatial edges affects the performance of spatial contour based proposal prediction in video frames. In practice, $\\lambda=0.2$ to $0.5$ works well for Youtube-Objects dataset. \n\n\n\n\n\\section{Learning Video Object Detector Model}\n\n\n\nWe aim for detecting objects in generic consumer videos. Due to the domain shift issues between images and video frames \\cite{KalogeitonFS15}, our 10-class video object detection uses supervised pre-training from ImageNet reference model for classification and fine-tuning on annotated frames from Youtube-Objects dataset v2.0 \\cite{youtube-Objects, KalogeitonFS15, ObjClsDet2012} for video objects detection.  \n\n\n\\textbf{Youtube-Objects dataset.} The dataset is composed of videos collected from Youtube by querying for the names of 10 object classes of the PASCAL VOC Challenge. It contains 155 videos in total and between 9 and 24 videos for each class. The duration of each video varies between 30 seconds and 3 minutes. However, only $6087$ frames are annotated with a bounding-box around an object instance. Hence, the number of annotated samples is approximately 4 times smaller than in PASCAL VOC.\n\n\nThe bottom-up region proposal methods play an important role. Motion blur and compression artifacts affects the quality of spatial edges in video frames, thus, generating good object proposals becomes more challenging. This is to be noted that R-CNN \\cite{girshick14CVPR}, or, Fast R-CNN \\cite{fast_RCNN_15} are fine-tuned for image object detection task, especially for 20-class PASCAL VOC image object categories which is a superset of Youtube-objects categories\\textit{}.\n\n\\textbf{Feature extraction.} We extract a 4096-dimensional feature vector corresponding to each region proposal using GPU-based (GeForce GTX 680) the Caffe \\cite{Caffe13} implementation of the CNN described by Krizhevsky \\etal \\cite{AlexNet12}. Features are computed by forward propagating a mean-subtracted 227 $\\times$ 227 R-G-B image through five convolutional layers and two fully connected layers.\n\n\\textbf{Region Proposals.} We use approximately $2000$ candidate proposals per video frame to be processed for learning detectors. We investigate the object detection model with different region proposal methods such as selective search\\cite{SelectiveSearch2013IJCV}, EdgeBoxes\\cite{Dollar2014ECCV}. As the resolution of different videos varies from VGA to HD, we re-size every video frame to $500\\times 500$ before performing proposal generation task. \n\n\\textbf{Training.} \n\nWe discriminatively pre-train the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations, followed by domain specific fine-tuning by replacing the last layer of AlexNet \\cite{AlexNet12} model with $10+1$ softmax output layer. We use two-step initialization for fine-tuning as described in \\cite{InitializationTip14}. As per PASCAL detection criteria, we treat all region proposals with $\\geq$ 0.5 IoU overlap with a ground-truth box as positives for that class of the box and the rest as negatives. Once features are extracted and training labels are applied, we optimize one linear SVM per class. \n\n\\textbf{Test time detection.} During test time, approximately $500$ to $2000$ VOPs are generated. Then forward propagation is performed through the CNN to compute features. Finally, we perform scoring using per-class trained SVM similar to \\cite{girshick14CVPR} followed by non-maximum suppression.  \n\n\nIn the below section \\ref{OVERLAP}, we describe the alternate test time detection using OVERLAP through objects' class label propagation. \n\n\n\n\n\n\\section{OVERLAP: Objects in Video Enabler thRough LAbel Propagation } \\label{OVERLAP}\n\nClassical approach for object localization has traditionally been image window classification, where each window is scored independent of other candidate windows. Recently, more success in object detection has been reported by considering spatial relations to all other windows and their appearance similarity \\cite{VezhnevetsF15} with examplar-based associative embedding. \n\nOur approach towards video object detection considers spatial relationship and appearance similarity with windows within and even in other nearby video frames, yet in much simpler way through spatio-temporal clustering, to avoid classifying every candidate windows. \n\n\\subsection{Joint analysis of windows}\n\nWe aim to detect dissimilarity between VOPs generated within a sub-sequence based on a simple underlying principle: proposals corresponding to the same object exhibit higher statistical dependencies than proposals belonging to different objects. \n\nAs a motivation for our approach, we consider generating proposal boxes by perturbing the ground truth locations of PASCAL VOC 2007 objects' bounding boxes and observe the statistical association of those proposal boxes. Let, $A$ and $B$ denote generic features of neighboring proposal windows, where neighborhood is characterized by non-zero Intersection-Over-Union (u). We investigate the joint distribution over pairings ${A,B}$. Let, $\\mathit{p}(A,B;u)$ be the joint probability of features A and B of windows with spatial overlap value, $u$. Then, $P(A,B)$ could ideally be computed as :\n{\\vspace{-2.5mm}} \n\n", "index": 5, "text": "\\begin{equation} \\label{joint_density}\nP(A,B) = \\frac{1}{Z} \\int_{u}^{1}  {w(u) \\mathit{p}(A,B; u)du}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"P(A,B)=\\frac{1}{Z}\\int_{u}^{1}{w(u)\\mathit{p}(A,B;u)du}\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>Z</mi></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>u</mi><mn>1</mn></msubsup><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo>;</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05447.tex", "nexttext": "\n\nWe choose the value of $\\rho$ to be $1.2$ which produces best performance in PASCAL VOC dataset with perturbed ground truth proposals. In order to identify the boundary between two features, the model needs to be able to capture the low probability regions of P(A,B). We use a non-parametric kernel (Epanechnikov) density estimator. The number of sample points are the number of overlapping candidate windows. We perform affinity-based clustering afterwards. The affinity matrix, $\\mathbf{W}$, for a sub-sequence is created from the affinity function, $PMI_{\\rho}$, as follows:\n\n{\\vspace{-2.5mm}} \n\n", "itemtype": "equation", "pos": 18017, "prevtext": "\n\nwhere, $u \\in [0,1]$, $w$ is a weighting function, $w(0) = 0$, and $Z$ is a normalization constant. To simplify the process, we use uniform weighting function and work in the discrete (quantized) space of $u$ and replace the integral with summation. We take the marginals of the distribution to get $P(A)$ and $P(B)$. Motivated by the  analysis presented in crisp boundary detection work by Isola \\textit{\\etal} \\cite{crisp_boundaries}, we model affinity with point-wise mutual information like function: \n{\\vspace{-2.5mm}} \n\n", "index": 7, "text": "\\begin{equation} \\label{PMI}\nPMI_{\\rho}(A,B) = log \\frac{P(A,B)^{\\rho}} {P(A) P(B)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"PMI_{\\rho}(A,B)=log\\frac{P(A,B)^{\\rho}}{P(A)P(B)}\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><msub><mi>I</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mfrac><mrow><mi>P</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow><mi>\u03c1</mi></msup></mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05447.tex", "nexttext": "\nWhere $i$ and $j$ are the indices of proposal windows and $\\mathbf{f}$ is the feature vector defined for a proposal window. \nFigure \\ref{fig:affinity-based-clustering} shows an example of spatial clustering of proposal windows. The boxes drawn in same colors correspond to same clusters. \n\nIn the streaming VOP clustering framework for Youtube videos, we perform the joint analysis on all proposals within a sub-sequence. Intuitively this is an easy yet effective clustering technique which works on actual test proposals (see Figure \\ref{fig:Streaming-VOP}) which are not simply perturbed ground truth bounding boxes. \n\nThis is to be noted that the proposed method measures the affinity between different object proposal windows (of varying sizes) within a video sub-sequence (in different video frames) unlike \\cite{crisp_boundaries}, where the affinity is between the neighboring pixels in an image. \n\n\n\n\\begin{figure}\n\\begin{center}\n\n\\includegraphics[scale=0.55]{affinity-based-clustering_old}\n\\end{center}\n   \\caption{Synthetic experiment using perturbed ground truth locations in PASCAL VOC showing affinity-based clustering of candidate windows. The clustering technique works efficiently with some exceptions where overlapping object instances share very similar color. }\n\\label{fig:affinity-based-clustering}\n\\end{figure} \n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale = 0.47]{segmentation_mask1-1}\\\\ \\vspace{-0.1cm} \n\t\\includegraphics[scale = 0.42]{segmentation_mask1-2} \\includegraphics[scale = 0.42]{segmentation_mask1-3}\\\\ \\vspace{-0.1cm} \n\t\\includegraphics[scale = 0.42]{segmentation_mask1-4} \\includegraphics[scale = 0.42]{segmentation_mask1-5}\\\\\n\t\\vspace{0.1cm} \n\t\\includegraphics[scale = 0.47]{segmentation_mask2}\n\\end{center}\n   \\caption{Segmentation masks from clustered object proposals. }\n\\label{fig:segmentation_mask1}\n{\\vspace{-2.5mm}} \n\\end{figure} \n\nWe demonstrate object segmentation can be achieved as a by-product of this clustering algorithm. We cast the segmentation problem through random-field based background-foreground segmentation without manual labeling \\cite{SegProp12}.\nUniformly weighted sum of the location of every window corresponding to a unique cluster defines the foreground location prior for that cluster. \nHowever, unlike \\cite{SegProp12}, in our approach, the location prior is not coming from the global neighbors of the image but from within itself and the  clustering allows multiple objects segmentations. The segmentation works well if the proposal boxes tightly enclose an object as shown in figure \\ref{fig:segmentation_mask1} with some failure cases as shown in figure \\ref{fig:segmentation_mask3} where the proposal boxes do not tightly enclose the object in a cluttered background.\n\nFigure \\ref{fig:youtube_segmentation} shows two segmentation masks generated on individual video frames from the clustered video object proposals generated by the proposed VOP on real videos.      \n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale = 0.34]{segmentation_mask3}\n\\end{center}\n   \\caption{Segmentation masks generation is not successful where proposal boxes do not tightly enclose the actual object. This happens in cluttered background cases. }\n\\label{fig:segmentation_mask3}\n{\\vspace{-2.5mm}} \n\\end{figure}  \n\n\n\\begin{figure}\n\\begin{center}\n\t\\includegraphics[scale = 0.48]{youtube_segmentation}\n\\end{center}\n   \\caption{Two segmentation masks from clustered video object proposals from Youtube video ``Bird and Cat'' for frames \\#5, \\#20 and \\#45. }\n\\label{fig:youtube_segmentation}\n{\\vspace{-2.5mm}} \n\\end{figure} \n\n\\subsection{Streaming clustering of proposals}  \\label{Streaming_Clustering}\nOne of the main contributions of this paper is a simple, principled, and unsupervised approach to spatio-temporal grouping of candidate regions in streaming fashion. We describe a clustering framework which enforces a Markovian assumption on the video stream to approximate a batch grouping of  VOPs. A video is divided into a series of sub-sequences with one frame overlap with the previous sub-sequence as described in \\cite{XuXiCoECCV2012}. VOP clustering within the current sub-sequence depends on the results from only the previous sub-sequence. We consider sub-sequence length of 3 to 5 frames as a trade-off between quality and complexity. This is the same sub-sequence volume, where mid-range motion analysis is performed for detecting temporal edges (Section \\ref{spatio-temp-contours} ).\nThe color-histogram features are used for estimating joint probability between any overlapping window-pair and affinity-based clustering is performed afterwards. \nThere are two important aspects in this streaming clustering method. The first is generic to any clustering algorithm \\ie how to select the number of clusters and the second is specific to streaming method \\ie how to associate cluster number of the current sub-sequence with any of the clusters of previous and/or future sub-sequences?\n{\\vspace{-2.5mm}} \n\n\\subsubsection{Number of clusters.} Common consumer or Youtube videos contain limited number of moving objects, often less than five. Youtube-Objects dataset contains maximum of $3$ object instances and quite often a single moving object. We assume the presence of at most 5 objects to keep the computational complexity tractable and amenable to practical applications. We explore two modes of operations. The first method uses fixed number of clusters, $k=5$, with careful initialization of cluster centers using k-mean++ \\cite{Kmeanspp07} during spectral clustering. The second one is the spectral clustering with self-tuning \\cite{ZPClustering07}. We observe that while self-tuning outperforms the fixed cluster number case for hypothetically good object windows $($ such as the perturbed ground truth regions for PASCAL VOC $)$, both modes perform similarly in case of real object proposals generated by some proposal method. \n{\\vspace{-2.5mm}} \n\\subsubsection{Cluster Label Association.}\nIn the streaming framework, any sub-sequence except for the first one, needs to address the problem of either associating a cluster with a cluster number in the previous sub-sequence or generating a new one. We perform density estimation using an Epanechnikov kernel using the KD-tree implementation from \\cite{KDE} for every cluster using the 4-dimensional location (2D center, height and width) and 45-bin color histogram (15 for each color channel) of the regions of the proposals corresponding to each and every cluster. If the minimum KL-divergence between a distribution of the current cluster and a cluster from the previous sub-sequence is less than a threshold, we perform the cluster assignment. Otherwise, we create a new cluster. \n\nThis is to be noted that, for detecting primarily moving objects in videos, the weight of temporal edges could be as high as 0.7 or more as described in section \\ref{spatio-temp-contours}. In such cases, considering as low as only 100 VOPs can potentially detect moving objects. Clusters may contain fewer number of proposal windows than the dimension of the original feature space which is 49-dimensional. Thus we perform PCA-based dimensionality reduction before estimating the distribution. Also, we perform scaling of features to ease the process of selecting kernel evaluation points.  \n\n\\subsection{Object Label Propagation.}\n\nTime-consistent clustering enables object label propagation through the video. We perform CNN-based object detection \\ie classification of every window in CNN feature space at a video frame only when we encounter a new cluster label. An assigned cluster label means the object category is the same as what was already detected in the associated cluster of the previous sub-sequence. We still need to perform the localization, however. In order to address the localization, we fit a 4-D Gaussian distribution on the location parameters \\i.e. center (x,y), height(h) and width(w), of windows in a cluster. We simply keep track of the distance, $\\mathbf{d}$, of the detected final object location (after first-time detection using R-CNN like approach) from the mean of the fitted Gaussian for every cluster. Furthermore, we localize the object by adding $\\mathbf{d}$ with the mean of the 4-D Gaussian location distribution of the cluster in current sub-sequence. In general videos, new objects do not appear in every video frame. Thus, we do not need to detect objects at every video frame. Even when a new object appears, we need to detect/classify only for the proposals assigned to the new cluster. Thus, OVERLAP framework requires to process CNN features for only a small fraction of the number of proposals generated.\n\n\nIn some sense, the spatio-temporal clustering for object detection is related to tracking. However, a set of windows is tracked instead of a single region/object. Ability to bypass critical tracker initialization step and the possibility for applying R-CNN like detection at a more frequent and configurable interval to increase the detection accuracy (if needed) are the major advantages.       \n\n\n\\section{Experimental Results}\n\\subsection{Video Object Detector using VOP}\n\nWe observe that the proposed VOP helps in learning a better object detector model. Table \\ref{table:youtube-detection} shows the per-class detection accuracy and the mean Average Precision (mAP) for the 10-class Youtube-Objects Test set \\cite{youtube-Objects}. \n\n\\begin{figure*}[t]\n\\begin{center}\n\n\t\\includegraphics[scale=0.8]{Obj_detect}\n\\end{center}\n\\begin{center}\n\t\\includegraphics[scale=0.3]{failure-det-1}\n\t\\includegraphics[scale=0.3]{failure-det-2}\n\t\\includegraphics[scale=0.3]{failure-det-3}\n\\end{center}\n   \\caption{ Sample results of Video Object Detection with VOP. First 6 rows show successful detection cases and the last row shows false detection cases.}\n\\label{fig:Obj-det-VOP}\n{\\vspace{-2.5mm}} \n\\end{figure*} \n\n\n\n\n\\begin{table}[!h]\n\n\\centering\n\n\\begin{tabular}{ |p{1cm}|p{0.8cm}|p{0.8cm}|p{1.1cm}|p{1.1cm}|p {1.1cm}| }\n\\hline\nclasses & R-CNN & DPM & Fine-tune SS & Fine-tune EB & Fine-tune VOP \\\\\n\\hline\n\nplane & 14.1 & 28.42 & 25.57 & 26.52 & \\textbf{29.77}\\\\ \nbird & 24.2 & \\textbf{48.14} & 27.27 & 27.27 & 28.82\\\\ \nboat & 16.9 & 25.50 & 27.52 & 33.69 & \\textbf{35.34}\\\\ \ncar & 27.9 & \\textbf{48.99} & 35.18 & 36 & 41\\\\ \ncat & 17.9 & 1.69 & 25.02 & 27.05 & \\textbf{33.7}\\\\ \ncow & 28.6 & 19.24 & 43.01 & 44.76 & \\textbf{57.56} \\\\ \ndog & 12.2 & 15.84 & 24.05 & 27.07 & \\textbf{34.42}\\\\ \nhorse & 29.4 & 35.10 & 41.84 & 44.82 & \\textbf{54.52}\\\\ \nmbike & 21.3 & \\textbf{31.61} & 26.70 & 27.07 & 29.77\\\\ \ntrain & 13.2 & \\textbf{39.58} & 20.48 & 24.93 & 29.23\\\\ \n\\hline\nmAP & 20.57 & 29.41 & 29.67 & 31.92 & \\textbf{37.413}\\\\ \n\\hline\n\\end{tabular}\n\n\\setlength{\\abovecaptionskip}{15pt plus 3pt minus 2pt} \n\\caption{Object Detection Results on Youtube-Objects test set. Pre-trained R-CNN detector\\cite{girshick14CVPR} is downloaded from \\cite{R-CNN-model}. Detection result using Deformable Parts Model \\cite{DPM10} (DPM) are from \\cite{KalogeitonFS15}. Fine-tune SS uses fine-tuning with Selective Search proposals (similar to R-CNN), Fine-tune EB uses fine-tuning with EdgeBoxes proposals and Fine-tune VOP uses fine-tuning with proposed video Object Proposals ($\\lambda = 0.2$).} \n\\label{table:youtube-detection}\n{\\vspace{-2.5mm}} \n\\end{table}\n\nFine-tuning on Youtube-Objects training data with Selective Search proposals \\cite{SelectiveSearch2013IJCV} improves the detection results by at least $9$\\%\\ compared with the model fine-tuned for the image dataset PASCAL VOC. The detector learned with EdgeBoxes performs better than the one learned with Selective Search proposals. However, the detector learned with VOP even outperforms the detection rate by another $5.5$\\%\\ and achieves state-of-the-art detection accuracy ($37.4$\\%) on this dataset. Although detection accuracy for ``cat'', ``cow'', ``dog'' and ``horse'' have improved by a huge margin after using CNN features, categories like ``train'' and ``bird'' are still best detected using DPM \\cite{DPM10} detector. \n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{|c| c|c| c|c| c|c|c|c| c|c|c|c|}\n\\hline\n\\multirow{3}{*}{} & \\multirow{3}{*}{PF SS} & \\multirow{3}{*}{PF EB} & \\multicolumn{2}{|c|}{PF VOP} &\\multicolumn{8}{|c|}{OVERLAP}\\\\ \\cline{4-13}\n& & & \\multirow{2}{*}{CPU OF} & \\multirow{2}{*}{GPU OF} & \\multicolumn{4}{|c|}{CPU Optical Flow} & \\multicolumn{4}{|c|}{GPU Optical Flow}\\\\ \\cline{6-13}\n& & & & &200 V &500 V&1K V &2K V &200 V &500 V &1K V &2K V\\\\ \\hline\nProp time & 10 & 0.3 & 3.8 & 1.3 & \\multicolumn{4}{|c|}{3.8} & \\multicolumn{4}{|c|}{1.3}\\\\ \\hline\n\nOverall time & 30 & 20.3 & 23.8 & 21.3 & 6.2 & 9.3& 14.6 & 28.0 & 3.7 & 6.8 &12.1 & 26.5\\\\ \\hline\nmAP & 29.62 & 31.95 & 37.72 & 37.72 & 28.59 & 33.59 & 35.82 & 36.63 & 28.59 & 33.59 & 35.82 & 36.63\\\\ \\hline \n\\end{tabular}\n\\setlength{\\abovecaptionskip}{15pt plus 3pt minus 2pt} \n\\caption{Complexity and accuracy comparison. Proposal generation (Prop time),  overall detection time for per-frame (PF) baseline methods with Selective Search (SS) proposals, EdgeBoxes (EB) proposals, proposed VOPs and OVERLAP are shown. Baseline methods use 2000 proposals per frame. GPU OF and CPU OF denote GPU-based and CPU-based optical flow (OF) respectively. mAP increases as number of VOPs increases. About 3$\\times$ speedup achived with 500 VOPs with only 4\\%\\ drop in mAP compared to the baseline per-frame detection.} \n\\label{table:OVERLAP-comparison}\n\n\\end{table*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Streaming Clustering of VOP }\nFigure \\ref{fig:Streaming-VOP} shows the results of frame-level clustering vs streaming clustering at sub-sequence levels on arbitrary videos downloaded from Youtube. In these experiments, a sub-sequence contains 3 video frames, with an overlap of one frame from the previous sub-sequence. We use high $\\lambda$ value ( $0.8$ ) to identify only the moving objects with very few number of proposals. For clear visualization, we use only 50 proposal windows at every frame and that makes less than 200 proposals per sub-sequence. We aim to take the advantage of fast approximate spectral clustering algorithm which scales linearly with the problem size. In our current implementation, clustering takes less than 0.1 second for 3 frames streaming-volume with 50 VOPs per frame.\n\n\n\n\\begin{figure}\n\n\n\n\n\n\n\n\n\n\\begin{center}\n\t\\includegraphics[scale = 0.5]{streaming_VOP}\n\\end{center}\n   \\caption{Temporal consistency in streaming clustering of VOPs on arbitrary videos ``Horse riding'', ``Bird-cat'' and ``Alaskan bear'' downloaded from the Internet. Windows drawn in same color belong to same cluster. Top rows of every pair show the proposals clustered on individual frame (frames \\#2, \\#10, \\#25 in each case) level; bottom row shows the results of streaming clustering. }\n\\label{fig:Streaming-VOP}\n{\\vspace{-2.5mm}} \n\\end{figure} \n\n\n\\subsection{Video Object Detection}\n\nTo investigate the relative detection rate with OVERLAP compared with frame-wise R-CNN like approach, we create a subset (955 frames) of Youtube-Objects test-set (1783 frames) where the video frames form a valid video play. We find that for OVERLAP, CNN feature extraction and classification is needed only for $10$\\%\\ to $30$\\%\\ windows among all of them. Table \\ref{table:OVERLAP-comparison} corroborates the fact that the detection accuracy improves as we increase the number of VOPs from 200 to 2000 in OVERLAP at the cost of increased complexity needed for spectral clustering. Difference in mAP is between $1$-$9$\\%\\ . As an example, compared to per-frame detection, OVERLAP achieves about 3$\\times$ speedup at the cost of only 4\\% mean Average Precision (mAP) with 500 VOPs per frame. The non-GPU based spectral clustering implementation in MATLAB makes cases for more than 2000 VOPs even slower than per-frame RCNN. \n\nPer-frame proposal generation with Selective Search and EdgeBoxes takes about 10 seconds \\cite{Hosang2015pami} and 0.3 seconds \\cite{Hosang2015pami} respectively. Overall time for object detection per-frame in R-CNN per-frame becomes about 30 seconds and 20.3 seconds with the above corresponding methods. Generation of VOPs requires optical flow which takes 3.5 sec per frame in CPU-implementation and 1 sec per frame for GPU-implementation for about $500\\times500$ resolution frame-pairs. ``CPU'' and ``GPU'' in Table \\ref{table:OVERLAP-comparison} denote optical flow implementation in CPU \\cite{TBroxOF11} and GPU \\cite{GPU_Optical_flow_10} respectively. \nThe Youtube-Objects dataset  mostly contains moving objects. In addition, the test dataset does not contain significant number of test cases where multiple instances of objects with similar appearances are spatially overlapped. Thus, accuracy of OVERLAP successfully approaches the baseline per-frame detection accuracy as we increase the number of VOPs. For the fastest detection (with 200 VOPs only), we use more weight (lambda = 0.6) for temporal edge and still manage to get acceptable detection accuracy with over 5$\\times$ speedup as shown in the column corresponding to 200 VOP in Table \\ref{table:OVERLAP-comparison}. \n\nGPU-based spectral clustering can potentially lead to further speed up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\vspace{-2.5mm}} \n\n\\section{Conclusion}\nExperimental results show that VOP helps in learning a better moving or static video object detector model and achieves state-of-the-art detection accuracy on Youtube-Video dataset. We show that the proposed OVERLAP framework can detect temporally consistent objects in videos through object class label propagation using streaming clustering of VOPs with significant  speedup compared with naive per-frame detection with acceptable loss of accuracy. We also show that multiple objects segmentation can also be achieved as a by-product of OVERLAP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{egpaper_final}\n}\n\n\n", "itemtype": "equation", "pos": 18715, "prevtext": "\n\nWe choose the value of $\\rho$ to be $1.2$ which produces best performance in PASCAL VOC dataset with perturbed ground truth proposals. In order to identify the boundary between two features, the model needs to be able to capture the low probability regions of P(A,B). We use a non-parametric kernel (Epanechnikov) density estimator. The number of sample points are the number of overlapping candidate windows. We perform affinity-based clustering afterwards. The affinity matrix, $\\mathbf{W}$, for a sub-sequence is created from the affinity function, $PMI_{\\rho}$, as follows:\n\n{\\vspace{-2.5mm}} \n\n", "index": 9, "text": "\\begin{equation} \\label{affinity_matrix}\n\\mathbf{W}_{i,j} = e^{PMI_{\\rho}(\\mathbf{f}_i, \\mathbf{f}_j)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{W}_{i,j}=e^{PMI_{\\rho}(\\mathbf{f}_{i},\\mathbf{f}_{j})}\" display=\"block\"><mrow><msub><mi>\ud835\udc16</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><msup><mi>e</mi><mrow><mi>P</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><msub><mi>I</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc1f</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></math>", "type": "latex"}]