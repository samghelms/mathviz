[{"file": "1601.07576.tex", "nexttext": "\nwhere $\\textbf{W}$ is model weights that parameterize the function $f(\\textbf{x}_i;\\textbf{W})$ . $\\mathcal{L}(\\cdot)$ denotes the loss function, which is typically a hinge loss for our classification task. $\\|\\textbf{W}\\|_2$ is the regularization term. The training of the CNN is to look for a optimized $\\textbf{W}$ that maps $\\emph{I}_i$ from the image space onto its label space.\n\nExtending from the standard CNN, the LCS introduces a new auxiliary loss ($\\ell^a$) to the convolutional layer of the main networks, as shown in Fig. \\ref{fig10}. It can be formulated as,\n\n", "itemtype": "equation", "pos": 28599, "prevtext": "\n\n\n\n\n\\title{Locally-Supervised Deep Hybrid Model\\\\ for Scene Recognition}\n\n\\author{Sheng~Guo,\n        Weilin~Huang,\n        and~Yu~Qiao\n\\thanks{Sheng Guo, Weilin Huang and Yu Qiao are with Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China,\nand Multimedia Laboratory, Chinese University of Hong Kong, Hong Kong.\n\n E-mail: \\{sheng.guo,wl.huang,yu.qiao\\}@siat.ac.cn.}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nConvolutional neural networks (CNN) have recently achieved remarkable successes in various image classification and understanding tasks. The deep features obtained at the top fully-connected layer of the CNN (FC-features) exhibit rich global semantic information and are extremely effective in image classification. On the other hand, the convolutional features in the middle layers of the CNN also contain meaningful local information, but are not fully explored for image representation. In this paper, we propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and explores the  convolutional features for scene recognition.\n\nFirstly, we notice that the  convolutional features capture local objects and fine structures of scene images, which yield important cues for discriminating ambiguous scenes, whereas these features are significantly eliminated in the highly-compressed FC representation.\n  Secondly, we propose a new Local Convolutional Supervision (LCS) layer to enhance the local structure of the image by directly propagating the label information to the convolutional layers.\n\n  Thirdly, we propose an efficient Fisher Convolutional Vector (FCV) that successfully rescues the orderless mid-level semantic information (e.g. objects and textures) of scene image. The FCV encodes the large-sized convolutional maps into a fixed-length mid-level representation, and is demonstrated to be strongly complementary to the high-level FC-features.\n  Finally, both the FCV and FC-features are collaboratively employed in the LS-DHM representation, which\n  achieves outstanding  performance in our experiments. It obtains $83.75\\%$ and $67.56\\%$ accuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397 datasets, advancing the stat-of-the-art substantially.\n\n  \n\n \n\\end{abstract}\n\n\n\\begin{IEEEkeywords}\nScene recognition, convolutional neural networks, local convolutional supervision, Fisher Convolutional Vector.\n\\end{IEEEkeywords}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\\begin{figure}\n\\centering\n\\footnotesize\n\\begin{minipage}[l]{1.2\\linewidth}\n\n\\includegraphics[height=2.6in,width=3.4in,angle=0]{figures/scene67_ex1.png}\n\\end{minipage}\n\n\\vskip +0.2cm\n\n\\begin{minipage}[l]{1.2\\linewidth}\n\\begin{tabular}{l|l||l|l|l}\n\\hline\ncategory (left) & category (right) & FC-Fea. & Conv.-Fea. & Both\\\\\\hline\n$auditorium$ & $movietheater$ & 38.9&22.2& 11.1 \\\\\n$bookstore$ & $library$ & 25.0 & 10.0 & 5.0 \\\\\n$elevator$ & $corridor$ & 9.5&4.8 & 4.8\\\\\n$livingroom$ & $bedroom$ & 15.0& 10.0 & 0.0 \\\\\n$gym$ & $dental office$ & 11.1 &5.6& 0.0 \\\\\n$jewelleryshop$ & $shoeshop$ & 9.1&4.6 & 0.0\\\\\n\\hline\n\\end{tabular}\n\\end{minipage}\n\n\n\\caption{\\textbf{Top Figure}: category pairs with similar global layouts, which are difficult to be discriminated by purely using high-level fully-connected features (FC-features). The category names are listed in the bottom table. \\textbf{Bottom Table}: classification errors (\\%) between paired categories by using the convolutional features, FC-features, or both of them. }\n\\label{fig3}\n\\end{figure}\n\n\n     \\IEEEPARstart {H}{uman} has a remarkable ability to categorize complex scenes very accurately and rapidly. This ability is important for human to infer the current situations and navigate the environments \\cite{oliva2005gist}. Computer scene recognition and understanding aims at imitating this human ability by using algorithms to analyze input images. This is a fundamental problem in computer vision, and plays a crucial role on the success of numerous application areas like image retrieval, human machine interaction, autonomous driving, etc.\n\n    The difficulties of scene recognition come from several aspects. Firstly, scene categories are defined not only by various image contents they contain, such as local objects and background environments, but also by global arrangements, interactions or actions between them, such as eating in restaurants, reading in library, watching in cinema. These cause a large diversity of the scene contents which imposes a huge number of scene categories and large within-class variations. These make it much more challenging than the task of object classification. Furthermore, scene images often include numerous fine-grained categories which exhibit very similar contents and structures, as shown in Fig. \\ref{fig3}. These fine-grained categories are hard to be discriminated by purely using the high-level FC-features of CNN, which often capture highly abstractive and global layout information. These difficulties make it challenging to develop a robust yet discriminative method that accounts for all types of feature cues for scene recognition.\n    \n    \n\n  \n\n    Deep learning models, i.e. CNN \\cite{LeCun1998,LeCun1989}, have been introduced for scene representation and classification, due to their great successes in various related vision tasks \\cite{LeCun2015,krizhevsky2012imagenet,szegedy2014going,zhou2014learning,Girshick2014,Long2015,Wang2015,Huang2014,He2016}. Different from previous methods \\cite{doersch2013mid,juneja2013blocks,sanchez2013image,xie2014orientational,Guo2015} that compute hand-crafted features or descriptors, the CNN directly learns high-level features from raw data with multi-layer hierarchical transformations. Extensive researches demonstrate that, with large-scale training data (such as ImageNet \\cite{russakovsky2014imagenet,deng2009imagenet}), the CNN can learn effective high-level features at top fully-connected (FC) layer. The FC-features generalize well for various different tasks, such as object recognition \\cite{krizhevsky2012imagenet,szegedy2014going,simonyan2014very}, detection \\cite{Girshick2014,Girshick2015} and segmentation \\cite{Long2015,Hariharan2014}.\n\n    However, it has been shown that directly applying the CNNs trained with the ImageNet \\cite{donahue2013decaf,zhou2014learning} for scene classification was difficult to yield a better result than the leading hand-designed features incorporating with a sophisticated classifier \\cite{sanchez2013image}. This can be ascribed to the fact that the ImageNet data \\cite{russakovsky2014imagenet} is mainly made up of images containing large-scale objects, making the learned CNN features globally object-centric. To overcome this problem, Zhou \\emph{et al.} trained a scene-centric CNN by using a large newly-collected  scene dataset, called Places, resulting in a significant performance improvement \\cite{zhou2014learning}. In spite of using different training data, the insight is that the scene-centric CNN is capable of learning more meaningful local structures of the images (e.g. fine-scale objects and local semantic regions) in the convolutional layers, which are crucial to discriminate the ambiguous scenes \\cite{zhou2014object}. Similar observation was also presented in  \\cite{zeiler2014visualizing} that the neurons at middle convolutional layers exhibit strong semantic information. Although it has been demonstrated that the convolutional features include the important scene cues, the classification was still built on the FC-features in these works, without directly exploring the mid-level features from the convolutional layers \\cite{zhou2014learning,zhou2014object}.\n\n   \n\n    In CNN, the convolutional features are highly compressed when they are forwarded to the FC layer, due to computational requirement (i.e. the high-dimensional FC layer will lead to huge weight parameters and computational cost). For example, in the celebrated AlexNet \\cite{krizhevsky2012imagenet}, the $4^{th}$ and $5^{th}$ convolutional layer have 64,896 and 43,264 nodes respectively, which are reduced considerably to 4,096 (about 1/16 or 1/10) in the $6^{th}$ FC layer. And this compression is simply achieved by pooling and transformations with sigmod or ReLU operations. Thus there is a natural question: \\textit{ are the fine sematic features learned in the convolutional layers well preserved in the fully-connected layers? } If not, \\textit{how to rescue the important mid-level  convolutional features lost when forwarded to the FC layers}. In this paper, we explore the questions in the context of scene classification.\n    \n\n \n\n\n\n\n\n     \n\n   \n\n   \n\n     Building on these observations and insightful analysis, this paper strives for a further step by presenting an efficient approach that both enhances and encodes the local semantic features in the convolutional layers of the CNN. We propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) for scene recognition, making the following contributions.\n\n     Firstly, we propose a new local convolutional supervision (LCS) layer  built upon the convolutional layers. The LCS layer directly propagates the label information to the low/mid-level convolutional layers, in an effort to enhance the mid-level semantic information existing in these layers. This avoids the important scene cues to be undermined by transforming them through the highly-compressed FC layers.\n\n     Secondly, we develop the Fisher Convolutional Vector (FCV) that effectively encodes meaningful local detailed information by pooling the convolutional features into a fixed-length representation. The FCV rescues  rich semantic information of local fine-scale objects and regions by extracting mid-level features from the convolutional layers, which endows it with strong ability to discriminate the ambiguous scenes. At the same time, the FCV discards explicit spatial arrangement by using the FV encoding, making it robust to various local image distortions.\n\n     Thirdly, both the FCV and the FC-features are collaboratively explored in the proposed LS-DHM representation. We demonstrate that the FCV with LCS enhancement is strongly complementary to the high-level FC-features, leading to significant performance improvements. The LS-DHM achieves 83.75\\% and 67.56\\% accuracies on the MIT Indoor67 \\cite{quattoni2009recognizing} and SUN397 \\cite{xiao2010sun}, remarkably outperforming all previous methods.\n\n     The rest of paper is organized as follows. Related studies are briefly reviewed in Section II. Then the proposed Locally-Supervised Deep Hybrid Model (LS-DHM), including the local convolutional supervision (LCS) layer and the Fisher Convolutional Vector (FCV), is described in Section III. Experimental results are compared and discussed in Section IV, followed by the conclusions in Section V.\n\n  \n\n    \n    \n\n\n\n\n\\section{Related Works}\n\nScene categorization is an important task in computer vision and image related applications. Early methods utilized hand-crafted holistic features, such as GIST \\cite{oliva2005gist}, for scene representation. Holistic features are usually computationally efficient but fail to deliver rich semantic information, leading to poor performance for indoor scenes with man-made objects \\cite{wu2011centrist}. Later Bag of Visual Words (e.g. SIFT \\cite{lowe2004distinctive}, HoG \\cite{dalal2005histograms}) and its variants (e.g. Fisher vector \\cite{sanchez2013image}, Sparse coding \\cite{lazebnik2006beyond})  became popular in this research area. These methods extract dense local descriptors from input image, then encode and pool these descriptors into a fixed length representation for classification. This representation contains abundant statistics of local regions and achieves good performance in practice. However, local descriptors only exhibit limited semantic meaning and global spatial relationship of local descriptors is generally ignored in these methods. To relieve this problem, semantic part based methods are proposed. Spatial Pyramid Matching (SPM) \\cite{lazebnik2006beyond}, Object Bank (OB) \\cite{li2010object} and Deformable Part based Model (DPM) \\cite{pandey2011scene} are examples along this line.\n\n In recent years, CNNs have achieved record-breaking results on standard image datasets, and there have been a number of attempts to develop deep networks for scene recognition \\cite{donahue2013decaf,zhou2014learning}. Krizhevsky \\emph{et al.}  \\cite{krizhevsky2012imagenet} proposed a seven-layer CNN, named as AlexNet, which achieved significantly better accuracy than other non-deep learning methods in ImageNet LSVRC 2012.\n\n  Along this direction, two very deep convolutional networks, the GoogleNet \\cite{szegedy2014going} and VggNet \\cite{simonyan2014very}, were developed, and they achieved the state-of-the-art performance in LSVRC 2014.\n  However, the classical CNNs trained with ImageNet are object-centric which cannot obtain better performance on scene classification than handcrafted features \\cite{donahue2013decaf}.\n   Recently, Zhou \\emph{et al.} developed a scene-centric dataset called Places, and utilized it to train the CNNs, with significantly performance improvement on scene classification \\cite{zhou2014learning}. Gong \\emph{et al.} employed Vector of Locally Aggregated Descriptors (VLAD) \\cite{jegou2010aggregating} for pooling multi-scale orderless FC-features (MOP-CNN) for scene classification \\cite{gong2014multi}. Despite having powerful capabilities, these successful models are all built on the FC representation for image classification.\n\n \n\n    The GoogleNet introduces several auxiliary supervised layers which were selectively connected to the middle level convolutional layers \\cite{szegedy2014going}. This design encourages the low/mid-level convolutional features to be learned from the label information, avoiding gradient information vanished in the very deep layers.\n    Similarly, Lee \\emph{et al.} \\cite{lee2014deeply} proposed deeply supervised networks (DSN) by adding a auxiliary supervised layer onto each convolutional layer. Wang \\emph{et al.} employed related methods for scene recognition by selectively adding the auxiliary supervision into several convolutional layers \\cite{wang2015training}.\n    Our LCS layer is motivated from these approaches, but it has obvious distinctions by design. The final label is directly connected to the convolutional layer of the LCS, allowing the label to directly supervise each activation in the convolutional layers, while all related approaches keep the FC layers for connecting the label and last convolutional layer \\cite{szegedy2014going,lee2014deeply,wang2015training}.\n   \n     Importantly, all these methods use the FC-features for classification, while our studies focus on exploring the convolutional features enhanced by the LCS.\n\n     Our work is also related to several recent efforts on exploring the convolutional features for object detection and classification. Oquab \\emph{et al.} \\cite{oquab2014learning} demonstrated that the rich mid-level features of CNN pre-trained on the large ImageNet data can been applied to a different task, such as object or action recognition and  localization. Sermanet \\emph{et al.} explored Spare Coding to encode the convolutional and FC features for pedestrian detection \\cite{sermanet2013pedestrian}. Raiko \\emph{et al.} transformed the outputs of each hidden neuron to have zero output and slope on average, making the model advanced in training speed and also generalized better \\cite{raiko2012deep}. Recently, Yang and Ramanan \\cite{Yang2015} proposed directed acyclic graph CNN (DAG-CNN) by leveraging multi-layer convolutional features for scene recognition. In this work, the simple average pooling was used for encoding the convolutional features. Our method differs from these approaches by designing a new LCS layer for local enhancement, and developing the FCV for features encoding with the Fisher kernel.\n    \n\n\n    Our method is also closed to Cimpoi \\emph{et al.}'s work \\cite{cimpoi2015deep}, where a new texture descriptor, FV-CNN, was proposed. Similarly, the FV-CNN applies the Fisher Vector to encode the convolutional features, and achieves excellent performance on texture recognition and segmentation. However, our model is different from the FV-CNN in CNN model design, feature encoding and application tasks. First, the proposed LCS layer allows our model to be trained for learning stronger local semantic features, immediately setting us apart from the FV-CNN which directly computes the convolutional features from the ``off-the-shelf\" CNNs. Second, our LS-DHM uses both the FCV and FC-features, where the FCV is just computed at a single scale, while the FV-CNN purely computes multi-scale convolutional features for image representation, e.g. ten scales. This imposes a significantly larger computational cost, e.g. about 9.3 times of our FCV. Third, the application tasks are different. The FV-CNN is mainly developed for texture  recognition, where the global spatial layout is not crucial, so that the FC-features are not explored. In contrast, our scene recognition requires both global  and local fine-scale information, and our LS-DHM allows both FCV and FC-features to work collaboratively, which eventually boost the performance.\n\n    \n\n\\section{Locally-Supervised Deep Hybrid Model}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[height=2.4in,width=3.4in,angle=0]{figures/zhedang.png}\n\\caption{\\textbf{Top}: images of bedroom  (left) and computer room (right), and  their corresponding convolutional feature maps. \\textbf{Middle}: image with key objects occluded, i.e., \\emph{bed} or \\emph{computers}. \\textbf{Bottom}: image with unimportant areas occluded. Occluding key  objects significantly modifies the structures of convolutional maps, while unimportant regions change the convolutional features slightly. This indicates that the convolutional features are crucial to discriminate the key objects in the scene images.}\n\\label{fig2}\n\\end{figure}\n\n\n In this section, we first  discuss and analyze the properties of convolutional features of the CNN networks. In particular, we pay special attention on the difference of scene semantics computed by the convolutional layers and the FC layers. Then we present details of the proposed Locally-Supervised Deep Hybrid Model (LS-DHM) that computes multi-level deep features. It includes a newly-developed local convolutional supervision (LCS) layer to enhance the convolutional features, and utilizes the Fisher Convolutional Vector (FCV) for encoding the convolutional features.\n \n Finally, we discuss the properties of the LS-DHM by making comparisons with related methods, and explain insights that eventually lead to performance boost.\n\n\n \n\n\\subsection{Properties of Convolutional Features}\n\n \\begin{figure*}\n\\centering\n\\subfigure[\\emph{Bakery} category]{\\includegraphics[height=1.4in,width=3.5in,angle=0]{figures/Scorcesbyconv.png}}\n\\subfigure[\\emph{Church-inside} category]{\\includegraphics[height=1.4in,width=3.5in,angle=0]{figures/Scorcesbyfc.png}}\n\\caption{The classification results of the \\emph{Bakery} and \\emph{Church-inside} categories. We list the images with the \\emph{lowest five} classification scores by using the convolutional features (\\textbf{top row}) and the FC-features (\\textbf{bottom row}). The images with higher scores are generally classified correctly by each type of feature. The image with incorrect classification is labeled by a RED bounding box. We observe that the convolutional features perform better on the \\emph{Bakery} category which can be mainly discriminated by the iconic objects, while the FC-features got better results on the \\emph{Church-inside} category where the global layout information dominate. The FC-features are difficult to discriminate the \\emph{Bakery} and the \\emph{Deli}, which have very closed global structures, but are distinctive in local objects contained. These observations inspire our incorporation of both types of features for scene categorization.}\n\\label{fig:ConvFC}\n\\end{figure*}\n\n\\begin{figure*}\n\\centering\n\\subfigure{\\includegraphics[height=1.8in,width=3.5in,angle=0]{figures/googlenew.eps}}\n\\subfigure{\\includegraphics[height=1.8in,width=3.5in,angle=0]{figures/cov4new.eps}}\n\\caption{Distributions of top 1,000 images with the largest average activations in the FC layer (left) and the convolutional layer (right).}\n\\label{fig7}\n\\end{figure*}\n\n  The remarkable success of the CNN encourages researchers to explore the properties of the CNN features, and to understand why they work so well. In \\cite{zeiler2014visualizing},  Zeiler and Fergus introduced deconvolutional network to visualize the feature activations in different layers. They shown that the CNN features exhibit increasing invariance and class discrimination as we ascend layers. Yosinski \\emph{et al.} \\cite{yosinski2014transferable} analyzed the transferability of CNN features learned at various layers, and found the top layers are more specific to the training tasks. More recently, Zhou \\emph{et al.} \\cite{zhou2014object} show that certain nodes in the Places-CNN, which was trained on the scene data without any object-level label, can surprisingly learn strong object information automatically. Their results demonstrate that the CNN features in different layers correspond to multiple levels of scene abstractions, such as \\emph{edges}, \\emph{textures}, \\emph{objects}, and \\emph{scenes}, from low-level to high-level. A crucial issue is which levels of these abstractions are discriminative yet robust for scene representation.\n\n\n   Generally, scene categories can be discriminated by their global spatial layouts. This \\emph{scene}-level distinctions can be robustly captured by the FC-features of CNN. However, there also exist a large number of ambiguous categories, which do not have distinctive global layout structure. As shown in Fig. \\ref{fig3}, it is more accurate to discriminate these categories by the iconic objects within them. For instance, the $bed$ is the key object to identify the $bedroom$, making it crucial to discriminate the $bedroom$ and $living room$. While the $jewelleryshop$ and $shoeshop$ have a similar global layout, the main difference lies in the subtle object information they contain, such as $jewellery$ and $shoe$. Obviously, the key object information provides important cues for discriminating these ambiguous scenes, and the mid-level convolutional features capture rich such object-level and fine structure information. We conduct a simple experiment by manually occluding  a region of the image. As shown in Fig. \\ref{fig2}, the convolutional feature maps (from the $4^{th}$ convolutional layer) are affected significantly if the key objects defining the scene categories are occluded ($2^{nd}$ row), while the maps show robustness to the irrelevant objects or regions ($3^{rd}$ row).  These results and discussions suggest that \\textit{the middle-level convolutional activations are highly sensitive to the presence of iconic objects which play crucial roles in scene classification}.\n\n  In CNN, the convolutional features are pooled and then transformed nonlinearly layer by layer before feeding to the FC layer. Low-level convolutional layers perform like Gabor filters and color blob detectors \\cite{yosinski2014transferable}, and mainly capture the \\emph{edges} and/or \\emph{textures} information. During the forward layer-wise process of the CNN, the features exhibit more abstractive meaning, and become more robust to local image variations. The FC layers significantly reduce the dimension of the convolutional features, avoiding huge memory and computation cost.\n \n  On the other hand, the high-level nature of the FC-features makes them difficult to extract strong local subtle structures of the images, such as fine-scale objects or their parts. This fact can be also verified in recent work \\cite{mahendran2014understanding}, where the authors shown that the images reconstructed from the FC-features can preserve global layouts of the original images, but they are very fuzzy, losing fine-grained local details and even the positions of the parts. By contrast, the reconstructions from the convolutional features are much more photographically faithful to the original ones. Therefore, the FC-features may not well capture the local object information and fine structures, while these mid-level features are of great importance for scene classification. To  illustrate the complementary capabilities of the two features, we show the classification results by each of them in Fig \\ref{fig:ConvFC}. It can be found that the two types of features are capable of discriminating different scene categories by capturing either local subtle objects information or global structures of the images, providing strong evidence that the convolutional features are indeed beneficial.\n\n   \\begin{figure*}\n\\centering\n\\includegraphics[height=2.1in,width=6.2in,angle=0]{figures/LS_DHM1.jpg}\n\n\\caption{The structure of Locally-Supervised Deep Hybrid Model (LS-DHM) built on 7-layer AlexNet \\cite{krizhevsky2012imagenet}. The LS-DHM can be constructed by incorporating the FCV with external FC-features from various CNN models, such as GoogleNet \\cite{szegedy2014going} or VggNet \\cite{simonyan2014very}.}\n\\label{fig10}\n\\end{figure*}\n\n\n\n  \n   \n\n   To further illustrate the challenge of scene classification, we present several pairs of ambiguous scene categories (from the MIT Indoor 67) in Fig. \\ref{fig3}. The images in each category pair exhibit relatively similar global structure and layout, but have main difference in representative local objects or specific regions. For each pair, we train a SVM classifier with the FC-features, the convolutional features extracted from the $4^{th}$ layer, or their combination. The classification errors on the test sets are summarized in bottom table in Fig. \\ref{fig3}. As can be observed, the FC-features do not perform well on these ambiguous category pairs, while the convolutional features yield better results by capturing more local differences. As expected, combination of them eventually leads to performance boost by computing both global and local image structures. It achieves zero errors on three category pairs which have strong local discriminants between them, e.g. $jewellery$ vs $shoe$.\n\n   To further investigate the different properties of the FC-features and convolutional features, we calculate the statistics of their activations on the MIT Indoor 67. We record the top 1,000 images which have the largest average activations in the last FC layer and the $4^{th}$ convolutional layer, respectively. Fig. \\ref{fig7} shows the distributions of these 1,000 images among 67 categories. As can be seen, there exist obvious difference between two distributions, implying that the representation abilities of the two features are varied significantly across different scene categories. It also means that some scene categories may include strong characteristics of the FC-features, while the others may be more discriminative with the convolutional features.\n\n   These results, together with previous discussions, can readily lead to a conclusion that \\textit{the FC-features and convolutional features can be strongly complementary to each other, and  both global layout and local fine structure are crucial to yield a robust yet discriminative scene representation}.\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Locally-Supervised Deep Hybrid Model}\n In this subsection, we present the details of the proposed Locally-Supervised Deep Hybrid Model (LS-DHM), which incorporates both the FCV representation and FC-features of the CNN. The structure of the LS-DHM is presented in Fig. \\ref{fig10}. It is built on a classical CNN architecture, such as the AlexNet \\cite{krizhevsky2012imagenet}  or the Carifai CNN \\cite{zeiler2014visualizing}, which has five convolutional layers followed by another two FC layers.\n\n \\textbf{Local Convolutional Supervision (LCS)}. We propose the LCS to enhance the local objects and fine structures information in the convolutional layers. Each LCS layer is directly connected to one of the convolutional layers in the main CNN. Specifically, our model can be formulated as follows. Given $N$ training examples, $\\{\\textbf{I}_i,y_i\\}_{i=1}^N$, where $\\textbf{I}_i$ demotes a training image, and $y_i$ is the label, indicating the category of the image.  The goal of the conventional CNN is to minimize,\n\n", "index": 1, "text": "\\begin{equation} \\label{Eq:TraditionalCNN}\n   \\arg\\min_{\\textbf{W}}{\\sum_{i=1}^{N}{\\mathcal{L}(y_i, f(\\textbf{I}_i;\\textbf{W})) + \\|\\textbf{W}\\|_2}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\arg\\min_{\\textbf{W}}{\\sum_{i=1}^{N}{\\mathcal{L}(y_{i},f(\\textbf{I}_{i};%&#10;\\textbf{W}))+\\|\\textbf{W}\\|_{2}}}\" display=\"block\"><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mtext>\ud835\udc16</mtext></munder></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mtext>\ud835\udc08</mtext><mi>i</mi></msub><mo>;</mo><mtext>\ud835\udc16</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mtext>\ud835\udc16</mtext><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></math>", "type": "latex"}, {"file": "1601.07576.tex", "nexttext": "\nwhere $\\ell^a$ is auxiliary loss function, which has the same form as the main loss $\\mathcal{L}$ by using the hinge loss. $\\lambda^a$ and $\\textbf{W}^a$ denote the importance factor and model parameters of the auxiliary loss. Here we drop the regularization term for notational simplicity. Multiple auxiliary loss functions can be applied to a number of convolutional layers selected in set $A$, allowing our design to build multiple LCS layers upon different convolutional layers.  In our model, $\\textbf{W}$ and $\\textbf{W}^a$ share the same parameters in the low convocational layers of the main CNN, but have independent parameters in the high-level convolutional layers or the FC layers. The label used for computing the auxiliary loss is the same as that of the main loss, $y^a_i=y_i$, allowing the LCS to propagate the final label information to the convolutional layers in a more direct way. This is different from recent work on exploring the CNN model for multi-task learning (MTL) (e.g. for face alignment \\cite{Zhang2015} or scene text detection \\cite{He2015}), where the authors applied completely different supervision information to various auxiliary tasks in an effort to facilitate the convergence of the main task.\n\nBy following the conventional CNN, our model is trained with the classical SDG algorithm w.r.t $\\textbf{W}$ and $\\textbf{W}^a$. The structure of our model is presented in Fig. \\ref{fig10}, where the proposed LCS is built on just one convolutional layer (the $4^{th}$ layer) of the main CNN. Similar configuration can be readily extended to multiple convolutional layers. The LCS contains a single convolutional layer followed by a max pooling operation. We apply a small-size kernel of $3\\times3$ with the stride of 1 for the convolutional layer, which allows it to preserve the local detailed information as much as possible. The size of the pooling kernel is set to $3 \\times 3$, with the stride of 2. The feature maps generated by the new convolutional and pooling layers have the sizes of $14\\times14\\times80$ and  $7\\times7\\times80$ respectively, compared to the $14\\times14\\times384$ feature maps generated by the $4^{th}$ layer of the main CNN.\n\nIn particular, the pooling layer in the LCS is directly connected to the final label in our design, without using any FC-layer in the middle of them.  This specific design encourages the activations in the convolutional layer of the LCS to be directly predictive of the final label. Since each independent activation in convolutional layer may include meaningful local semantics information (e.g. local objects or textures located within its receptive field),  further correlating or compressing these activations through a FC layer may undermine these fine-scale but local discriminative information. Thus our design provides a more principled approach to recuse these important local cues by enforcing them to be directly sensitive to the category label. This design also sets the LCS apart from related convolutional supervision approaches developed in \\cite{szegedy2014going,Yang2015,wang2015training,lee2014deeply}, where the FC layer\nis retained in the auxiliary supervision layers. Furthermore, these related approaches only employ the FC-features for image representation, while our method explores both the convolutional features and the FC-features by further developing an efficient FCV descriptor for encoding the convolutional features.\n\n \n\n\n\n\n\n\n  \\textbf{Fisher Convolutional Vector (FCV)}. Although the local object and region information in the convolutional layers can be enhanced by the proposed LCS layers, it is still difficult to preserve  these information sufficiently  in the FC-representation, due to multiple hierarchical compressions and abstractions. A straightforward approach is to directly employ all these convolutional features for image description.  However, it is non-trivial to directly apply them for training a classifier. The convolutional features are computed densely from the original image, so that they often have a large number of feature dimensions, which may be significantly redundant. Furthermore, the densely computing also allows the features to preserve explicit spatial information of the image, which is not robust to various geometric deformations.\n\n  Our goal is to develop a discriminative mid-level representation that robustly encodes rich local semantic information in the convolutional layers.\n  \n  \n\n  Since each activation vector in the convolutional feature maps has a corresponding receptive field (RF) in the origan image, this allows it to capture the local semantics features within its RF, e.g. fine-scale objects or regions. Thus the activation vector can be considered as an independent mid-level representation regardless of its global spatial correlations. For the scene images, such local semantics are of importance for fine-grained categorization, but are required to increase their robustness by discarding explicit spatial information. For example, the images of the \\emph{car} category may include various numbers and multi-scale cars in complectly different locations.\n \n  Therefore, to improve the robustness of the convolutional features without degrading their discriminative power, we develop the FCV representation that computes the orderless mid-level features by leveraging the Fisher Vector (FV) encoding \\cite{Jaakkola1998,sanchez2013image}.\n\n  The Fisher Kernel \\cite{Jaakkola1998} has been proven to be extremely powerful for pooling a set of dense local features (e.g. SIFT \\cite{lowe2004distinctive}), by removing global spatial information \\cite{sanchez2013image}. The convolutional feature maps can be considered as a set of dense local features, where each activation vector works as a feature descriptor. Specifically, given a set of convolutional maps with the size of $H \\times W \\times D$ (from a single CNN layer), where $D$ is the number of the maps (channels) with the size of $H \\times W$, we get a set of $D$-dimensional local convolutional features ($\\textbf{C}$),\n\\begin{eqnarray}\n  \\textbf{C}=\\{C_1,C_2,...,C_T\\}, T=H\\times W\n\\end{eqnarray}\nwhere $\\textbf{C}\\in\\mathbb{R}^{D\\times T}$. $T$ is the number of local features which are spatially arranged in $ H\\times W$. To ensure that each feature vector contributes equally and avoid activation abnormity, we normalize each feature vector into interval [-1, 1] by dividing its maximum magnitude value,\n\\begin{eqnarray}\n  C_t=C_t/\\max\\{|C_t^1|,|C_t^2|,...,|C_t^D|\\}\n\\end{eqnarray}\n\n\n\\begin{algorithm}[tb]\n\\caption{\\small Compute FCV from the Convolutional Maps}\n\\label{alg:alg1}\n\\begin{algorithmic}[1] \n\\REQUIRE ~~\\\\ \nConvolutional features maps with the size of $H \\times W \\times D$. \\\\\nGMM parameters, $\\lambda =\\{\\omega_{k},\\mu_{k},\\sigma_{k},k=1,\\ldots,K \\}$.\n \\\\\n\n\\ENSURE ~~\\\\ \nFCV with $2MK$ dimensions.\\\\\n\n\\underline{\\textbf{Step One:}} Extract Local Convolutional Features.\n\\STATE Get $T=H\\times W$ normalized feature vectors, $\\textbf{C}\\in\\mathbb{R}^{D\\times T}$.\n\\STATE Reduce dimensions using PCA,  $\\hat{\\textbf{C}}\\in\\mathbb{R}^{M\\times T}$, $M<D$.\\\\\n\n\\underline{\\textbf{Step Two:}} Compute the FV Encoding.\n\n\n\n\\STATE Compute the soft assignment of $\\hat{C}_t$ to Gaussian $k$:\\\\\n$\\gamma_t^k=\\frac{\\omega_{k}\\mu_{k}(\\hat{C}_t)}{\\sum_{j=1}^K\\omega_{j}\\mu_{j}(\\hat{C}_t)}$, $k=1,\\ldots,K$.\\\\\n\\STATE Compute Gaussian accumulators:\\\\\n$S_{k}^0=\\sum_{t=1}^T\\gamma_t^k$, $S_{k}^{\\mu}=\\sum_{t=1}^T\\gamma_t^k\\hat{C}_t$, $S_{k}^{\\sigma}=\\sum_{t=1}^T\\gamma_t^k\\hat{C}_t^2$.\\\\\nwhere $S_{k}^0\\in\\mathbb{R}$, and $S_{k}^\\mu,S_{k}^{\\sigma}\\in\\mathbb{R}^M$, $k=1,\\ldots,K$.\\\\\n\n\\STATE Compute FV gradient vectors:\\\\\n$F_k^{\\mu}=(S_k^\\mu-\\mu_kS_k^0)/(\\sqrt{\\omega_{k}\\sigma_{k}})$\\\\\n$F_k^{\\sigma}=(S_k^\\sigma-2\\mu_kS_k^\\mu+(\\mu_k^2-\\sigma_k^2)S_k^0)/(\\sqrt{2\\omega_{k}\\sigma_{k}^2})$\\\\\nwhere $F_k^\\mu,F_k^\\sigma \\in \\mathbb{R}^M$, $k=1,\\ldots,K$.\\\\\n\\STATE Concatenate two gradient vectors from $K$ mixtures:\\\\\n$FCV=\\{F_1^{\\mu},...,F_K^{\\mu},F_1^{\\sigma},...,F_K^{\\sigma}\\} \\in \\mathbb{R}^{2MK}$. \\\\\n\\STATE Implement power and $\\ell_2$ normalization on the FCV.\n\\end{algorithmic}\n\\end{algorithm}\n\nWe aim to pool these normalized feature vectors to achieve an image-level representation.  We adopt the Fisher Vector (FV) encoding \\cite{sanchez2013image} which  models the distribution of the features by using a Gaussian Mixture Model (GMM), and  describe an image by considering the gradient of likelihood w.r.t the GMM parameters, i.e. mean and covariance. By following previous work \\cite{sanchez2013image}, we first apply the Principal Component Analysis (PCA) \\cite{Jolliffe2002} for reducing the number of feature dimensions to $M$. For the FV encoding, we adopt a GMM with $K$ mixtures, $G_{\\lambda}=\\{g_{k},k=1\\ldots K\\}$, where $\\lambda =\\{\\omega_{k},\\mu_{k},\\sigma_{k},k=1\\ldots K \\}$. For each GMM mixture, we compute two gradient vectors, $F_k^{\\mu}\\in \\mathbb{R}^M$ and $F_k^{\\sigma}\\in \\mathbb{R}^M$, with respect to the means and standard deviations respectively. The final FCV representation is constructed by concatenating two gradient vectors from all mixtures, which results in an orderless $2MK$-dimensional representation. The FCV can be feed to a standard classifier like SVM for classification. Note that the dimension number of the FCV is fixed, and is independent to the size of the convolutional maps, allowing it to be directly applicable to various convolutional layers. Details of computing the FCV descriptor is described in Algorithm 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n  \\textbf{Locally-Supervised Deep Hybrid Model (LS-DHM)}. As discussed, scene categories are defined by multi-level image contents, including the mid-level local \\emph{textures} and \\emph{objects}, and the high-level \\emph{scenes}. While these features are captured by various layers of the CNN, it is nature to integrate the mid-level FCV (with LCS enhancement) with the high-level FC-features by simply concatenating them, which forms our final LS-DHM representation. This allows scene categories to be coarsely classified by the FC-features with global structures, and at the same time, many ambiguous categories can be further discriminated finely by the FCV descriptor using local discriminative features. Therefore, both types of features compensate to each other, which leads to performance boost.\n\n  The structure of the LS-DHM is shown in Fig. \\ref{fig10}. Ideally, the proposed FCV and LCS are applicable to multiple convolutional layers or deeper CNN models.  In practice, we only use the single convolutional layer (the $4^{th}$ layer) in the celebrated 7-layer AlexNet for computing the FCV in current work. This makes the computation of FCV very attractive, by only taking about $60ms$ $per$ $image$ on the SUN379 by using a single GPU. Even that we has achieved very promising results in the current case, and better performance can be expected by combining the FCV from multiple layers, which will be investigated in our future work. Furthermore, the construction of the LS-DHM is flexible  by integrating the FCV with various FC-features of different CNNs, such as the AlexNet \\cite{krizhevsky2012imagenet}, GoogleNet \\cite{szegedy2014going} and VggNet \\cite{simonyan2014very}. The performance of the LS-DHM are varied by various capabilities of FC-features.\n\n  The LS-DHM representation is related to the MOP-CNN \\cite{gong2014multi}, which extracts the local features by computing multiple FC-features from various manually-divided local image patches. Each FC-feature of the MOP-CNN is analogous to an activation vector in our convolutional maps. The FCV captures richer local information by densely scanning the whole image with the receptive fields of the activation vectors, and providing a more efficient pooling scheme that effectively trades off  the robustness and discriminative ability. These advantages eventually lead to considerable performance improvements over the MOP-CNN. For example, our LS-DHM achieved 58.72\\% (\\emph{vs} 51.98\\% by MOP-CNN) on the SUN397 and 73.22\\% (\\emph{vs} 68.88\\% by MOP-CNN) on the MIT Indoor76, by building on the same AlexNet architecture.\n\n  Furthermore, the FCV and FC-features of the LS-DHM share the same CNN model,  making it significantly more efficient by avoiding repeatedly computing the network, while the MOP-CNN repeatedly implements the same network 21 times to compute all 3-level local patches \\cite{gong2014multi}.\n  In addition, the LS-DHM representation is flexible to integrate the FCV with more powerful FC-features, leading to further performance improvements, as shown in Section IV.\n\n  \n\n\n\n\\section{Experimental Results and Discussions}\nThe performance of the proposed LS-DHM is evaluated on two heavily benchmarked scene datasets: the MIT Indoor67 \\cite{quattoni2009recognizing} and the SUN397 \\cite{xiao2010sun}. We achieve the best performance ever reported on both benchmarks.\n\n\n \\textbf{The MIT Indoor67 \\cite{quattoni2009recognizing}} contains 67 indoor-scene categories and a total of 15,620 images, with at least 100 images per category. Following the standard evaluation protocol of \\cite{quattoni2009recognizing}, we use 80 images from each category for training, and another 20 images for testing.  Generally, the indoor scenes have strong object information, so that they can be better discriminated by the iconic objects they contain, such as the $bed$ in the $bedroom$ and the $table$ in the $dinning room$.\n\n \\textbf{The SUN397 \\cite{xiao2010sun}} has a large number of scene categories by including 397 categories and totally 108,754 images. This makes it extremely challenging for this task. Each category has at least 100 images.  We follow the standard evaluation protocol provided by the original authors \\cite{xiao2010sun}. We train and test the LS-DHM on ten different partitions, each of which has\n50 training and 50 test images.  The partitions are fixed and publicly available from \\cite{xiao2010sun}. Finally the average classification accuracy of ten different tests is reported.\n \n\\subsection{Implementation Details }\n\nWe discuss the parameters of FCV descriptor, and various CNN models which are applied for computing the FC-features of our LS-DHM. For the FCV parameters, we investigate the number of reduced dimensions by PCA, and the number of Gaussian mixtures for FV encoding.\n\nThe FCV is computed from the $4^{th}$ convolutional layer with the LCS enhancement, building on the 7-layer AlexNet architecture. The performance of the FCV computed on various convolutional layers will be evaluated bellow. The LS-DHM can use various FC-features of different CNN models, such as the AlexNet \\cite{krizhevsky2012imagenet}, GoogleNet \\cite{szegedy2014going} and VggNet \\cite{simonyan2014very}. We refer the LS-DHM with different FC-features as LS-DHM (AlexNet), LS-DHM (GoogleNet) and LS-DHM (VggNet). All deep  CNN models in our experiments are trained with the large-scale Places dataset \\cite{zhou2014learning}. Following previous work \\cite{gong2014multi,zhou2014learning}, the computed LS-DHM descriptor is feeded to a pre-trained linear SVM for final classification.\n\n  \n\n\\begin{figure}\n\\centering\n\n\\subfigure [PCA Dimension Reductions]{\\includegraphics[height=1.1in,width=1.7in,angle=0]{figures/PCAnew.eps}}\n\\subfigure[Gaussian Mixtures]{\\includegraphics[height=1.1in,width=1.7in,angle=0]{figures/GMMnew.eps}}\n\n\\caption{The performance of the FCV and LS-DHM (GoogleNet) with various numbers of  (left) reduced dimensions, and (right) the Gaussian mixtures. Experiments were conducted on the MIT Indoor67.}\n\\label{fig4}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[height=1.6in,width=3.5in,angle=0]{figures/Scene67_conv_new.eps}\n\n\n\\caption{Performance of the FCV computed at various convolutional layers of the AlexNet, and the LS-DHM with different FC-features from the GoogleNet or VggNet. The experiments were conducted on the MIT Indoor67.\n}\n\\label{fig5}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n    \\textbf{Dimension reduction}. The $4^{th}$ convolutional layer of the AlexNet includes 384 feature maps,\n    which are transformed to a set of 384D convolutional features. We verify the effect of the dimension reduction (by using PCA) on the performance of the FCV and LS-DHM. The numbers of retained dimensions are varied from 32 to 256, and experimental results on the MIT Indoor67 are presented in the left of Fig. \\ref{fig4}. As can be found, the number of retained dimensions does not impact the performance of FCV or LS-DHM significantly. By balancing the performance and computational cost, we choose to retain 80 dimensions for computing the FCV descriptor in all our following experiments.\n   \n\n    \\textbf{Gaussian mixtures}. The FV encoding requires learning the GMM as its dictionary. The number of the GMM mixtures also impact the performance and the complexity of FCV. Generally speaking, larger number of the Gaussian mixtures leads to a stronger discriminative power of the FCV, but at the cost of using more FCV dimensions. We investigate the impact of the mixture number on the FCV and LS-DHM by varying it from 64 to 512. We report the classification accuracy on the MIT Indoor67 in the right of Fig. \\ref{fig4}. We found that the results of FCV or LS-DHM are not very sensitive to the  number of the mixtures, and finally used 256 Gaussian mixtures for our FCV.\n\n\n\n\n\\begin{figure*}\n\\centering\n\\subfigure{\\includegraphics[height=2.2in,width=7in,angle=0]{figures/Featuremaps_lcs2.png}}\n\n\\caption{Comparisons of the convolutional maps (the mean map of 4th-convolutional layer) with the LCS enhancement (middle row), and without it (bottom two). The category name is list on the top of each image. Obviously, the LCS enhances the local object information in the convolutional maps significantly, which is crucial to identify a scene category.}\n\\label{fig:featuremap_lcs}\n\\end{figure*}\n\n\n\\begin{table}[tb]\n\n\\centering\n\\caption{Comparisons of various pooling methods on the MIT Indoor67. The LS-DHM is constructed by intergrading the FC-features of GoogleNet and the encoded convolutional features, computed from the AlexNet with and without the LCS layer.}\n\\begin{tabular}{l||c|c||c|c||c|c}\n\\hline\nEncoding & \\multicolumn{2}{c||}{Conv-Features}& \\multicolumn{2}{c||}{FC-Features} &\\multicolumn{2}{c}{LS-DHM}\\\\\n\\cline{2-3}\nMethod & No LCS &LCS &\\multicolumn{2}{c||}{GoogleNet}& No LCS &LCS\\\\\n \\hline\\hline\n Direct & 51.46 & 58.41 & \\multicolumn{2}{c||}{}& 76.95&77.40\\\\\n BoW &37.28 & 57.38 &\\multicolumn{2}{c||}{73.79}& 78.09& 78.64\\\\\n FCV& \\textbf{57.04} & \\textbf{65.67}& \\multicolumn{2}{c||}{}&\\textbf{80.34}&\\textbf{81.68}\\\\ \\hline\n\n\\end{tabular}\n\n\\label{encoding}\n\\end{table}\n\n\n\n \\subsection{Evaluations on the LCS, FCV and LS-DHM}\n\n\n  We investigate the impact of individual LCS  or FCV to the final performance.\n\n  The FC-features from the GoogleNet or VggNet are explored to construct the LS-DHM representation.\n\n\n\\textbf{On various convolutional layers}. The FCV can be computed from various convolutional layers, which capture the feature abstractions from low-level to mid-level, such as \\emph{edges}, \\emph{textures} and \\emph{objects}. In this evaluation, we investigate the performance of FCV and the LS-DHM on different convolutional layers, with the LCS enhancement. The results on the AlexNet, from the \\emph{Pool2} to \\emph{Pool5} layers, are presented in Fig. \\ref{fig5}. Obviously, both FCV and LS-DHM got the best performance on the $4^{th}$ convolutional layer. Thus we select this layer for building the LCS layer and computing the FCV. By integrating the FCV, the LS-DHMs achieve remarkable performance improvements over the original AlexNet or GoogleNet, demonstrating the efficiency of the proposed FCV.\n\n\n\n\n\n\\textbf{On the pooling approaches}. We further evaluate the FCV by investigating various pooling approaches for encoding the convolutional features.\n\n We compare the FV encoding with direct concatenation method and the BoW pooling \\cite{Sivic2003,Csurka2004}. The results on the MIT Indoor67 are shown in Table \\ref{encoding}. As can be seen, the FCV achieves remarkable improvements over the other two approaches, especially on purely exploring the convolutional features where rough global structure is particularly important. In particular, the BoW without the LCS yields a low accuracy of $37.28\\%$.\n It may due to the orderless nature of BoW pooling which completely discarding the global spatial information.\n The convolutional features trained without the LCS are encouraged to be abstracted to the high-level FC features. This enforces the convolutional features to be globally-abstractive by preserving rough spatial information for high-level scene representation.\n \n \n On the contrary, the direct concatenation method preserves explicit spatial arrangements, so as to obtain a much higher accuracy.  But the explicit spatial order is not robust to local distortions, and it also uses a large amount of feature dimensions. The FV pooling increases the robustness by relaxing the explicit spatial arrangements; and at the same time, it explores more feature dimensions to retain its discriminative power,  leading to a performance improvement.\n\n\\textbf{On the LCS}. As shown in Table \\ref{encoding}, the LCS improves the performance of all pooling methods substantially by enhancing the mid-level local semantics (e.g. \\emph{objects} and \\emph{textures}) in the convolutional layers. The accuracy by the BoW is surprisingly increased to $57.38\\%$ with our LCS enhancement. The performance is comparable to that of the direct concatenation which uses a significant larger number of feature dimensions. One of the possible reasons may be that the LCS enhances the local object information by directly enforcing the supervision on each activation in the convolutional layers, allowing the image content within RF of the activation to be directly predictive to the category label. This encourages the convolutional activations to be locally-abstractive, rather than the globally-abstractive in conventional CNN.\n\nThese locally-abstractive convolutional features can be robustly identified without their spatial arrangements, allowing them to be discriminated by the orderless BoW representation. As shown in Fig. \\ref{fig:featuremap_lcs}, our LCS significantly enhances the local object information in the convolutional maps, providing important cues to identify a scene category. For example, strong \\emph{head} information is reliable to recognize the \\emph{person} category, and confident \\emph{plate} detection is important to identify a \\emph{diningtable} image.\n\n\\textbf{On the LS-DHM}. In the Table \\ref{encoding}, the single FC-features yield better results than the convolutional features, suggesting that scene categories are primarily discriminated by the global layout information.\nDespite capturing rich fine-scale semantics, the FCV descriptor  perseveres little global spatial  information by using the FCV pooling. This reduces its discriminative ability to identify many high-level (e.g. \\emph{scene}-level) images, so as to harm its performance.\nHowever, we observed that, by intergrading both types of features, the proposed LS-DHM archives remarkable improvements over the individual FC-features in all cases. The largest gain achieved by our LS-DHM with the LCS improves the accuracy of individual FC-features from 73.79\\% to 81.68\\%. We got a similar large improvement on the SUN397, where our LS-DHM develops the strong baseline of GoogleNet considerably, from 58.79\\% to 65.40\\%.\nFurthermore, these facts are depicted more directly in Fig. \\ref{fig6}, where we show the classification accuracies of various features on a number of scene categories from the MIT Indoor67 and SUN397. The significant impacts of the FCV and LCS to performance improvements are shown clearly.\nThese considerable improvements connivingly demonstrate the strong complementary properties of the convolutional features and the FC-features, giving strong evidence that the proposed FCV with LCS is indeed beneficial to scene classification.\n\n \\begin{figure*}\n\\centering\n\n\\subfigure{\\includegraphics[height=2in,width=7in,angle=0]{figures/Scene67_new.eps}}\n \\subfigure{\\includegraphics[height=2in,width=7in,angle=0]{figures/Sun397_new.eps}}\n\n\\caption{Classification accuracies of several example categories with FC-features (GoogleNet), DHM and LS-DHM on the MIT Indoor67 and SUN397. DHM denotes the LS-DHM without LCS enhancement.}\n\\label{fig6}\n\\end{figure*}\n\n\n\n\\subsection{Comparisons with the state-of-the-art results}\n\n\n\n\n\n\n\n\n\n\nWe compare the performance of our LS-DHM with recent approaches on the MIT Indoor67 and SUN397. The FCV is computed from the AlexNet with LCS. \n\nOur LS-DHM representation is constructed by integrating the FCV with various FC-features of different CNN models. The results are compared extensively in Table \\ref{table2} and \\ref{table3}.\n\n The results show that our LS-DHM with the FC-features of 11-layer VggNet outperforms all previous Deep Learning (DL) and FV methods substantially on both datasets.  For the DL methods, the Places-CNN trained on the Place data by Zhou \\emph{et al.} \\cite{zhou2014learning} provides strong baselines for this task. Our LS-DHM, building on the same AlexNet, improves the performance of Places-CNN with a large margin by exploring the enhanced convolutional features. It achieves about 10\\% and 8\\% improvements over the Places-CNN on the MIT Indoor67 and SUN397 respectively. These considerable improvements confirm the significant impact of FCV representation which captures important mid-level local semantics features for discriminating many ambiguous scenes.\n\nWe further investigate the performance of our LS-DHM by using various FC-features. The LS-DHM obtains consistent large improvements over corresponding baselines, regardless of the underlying FC-features, and achieves the state-of-the-art results on both benchmarks. It obtains 83.75\\% and 67.56\\% accuracies on the MIT Indoor67 and the SUN397 respectively, outperforming the strong baselines of 11-layer VggNet with about 4\\% improvements in both two datasets. On the MIT Indoor67, our results are compared favourable to the closest performance at $81.0\\%$ obtained by the FV-CNN \\cite{cimpoi2015deep}, which also explores the convolutional features from a larger-scale 19-layer VggNet. On the SUN397, we gain a large 7\\% improvement over the closest result archived by the C-HLSTM \\cite{Zuo2015}, which integrates the CNN with hierarchical recurrent neural networks (C-HLSTM). The sizable boost in performance on both benchmarks convincingly confirm the promise of our method. For different FC-features, we note that the LS-DHM obtains larger improvements on the AlexNet and GoogleNet (about 7-8\\%), which are about twice of the improvements on the VggNet. This may due to the utilization of very small 3$\\times$3 convolutional filters by the VggNet. This design essentially captures more local detailed information than the other two. Thus the proposed FCV may compensate less to the VggNet.\n\n\\begin{table}[tb]\n\n\\centering  \n\n\\caption{Comparisons of the proposed LS-DHM with the state-of-the-art on the MIT Indoor67 database.}\n\\begin{tabular}{p{3.3cm}||p{1.6cm}<{\\centering}||p{1.6cm}<{\\centering}}\n\\hline\n\\textbf{Method} & \\textbf{Publication} & \\textbf{Accuracy}($\\%$)\\\\\n\\hline\nPatches+Gist+SP+DPM\\cite{singh2012unsupervised}& ECCV2012 &49.40\\\\\nBFO+HOG\\cite{kobayashi2013bfo} & CVPR2013&58.91\\\\\nFV+BoP\\cite{juneja2013blocks}&CVPR2013 &63.10\\\\\nFV+PC\\cite{doersch2013mid} &  NIPS2013 &68.87\\\\\nFV(SPM+OPM)\\cite{xie2014orientational}& CVPR2014 &63.48\\\\\nDSFL\\cite{zuo2014learning} &ECCV2014& 52.24\\\\\nLCCD+SIFT \\cite{Guo2015} &arXiv2015&65.96\\\\\n\n \\hline\nDSFL+CNN\\cite{zuo2014learning} &ECCV2014& 76.23\\\\\n\nCNNaug-SVM\\cite{razavian2014cnn}& CVPR2014&69.00\\\\\nMOP-CNN \\cite{gong2014multi} &ECCV2014 &68.90\\\\\nMPP \\cite{Yoo2015}& CVPR2015&77.56\\\\\nMPP \\cite{Yoo2015}+DSFL\\cite{zuo2014learning}&CVPR2015&80.78\\\\\nFV-CNN (VggNet19)\\cite{cimpoi2015deep}&CVPR2015 &81.00\\\\\nDAG-VggNet19 \\cite{Yang2015}&ICCV2015&77.50\\\\\nC-HLSTM \\cite{Zuo2015} &arXiv2015&75.67\\\\\nMs-DSP (VggNet16) \\cite{Gao2015} &arXiv2015&78.28\\\\\n\\hline\\hline\n\nPlaces-CNN(AlexNet)\\cite{zhou2014learning} &NIPS2014& 68.24\\\\\nLS-DHM(AlexNet) &--& 78.63\\\\\n\\hline\n GoogleNet &--& 73.96\\\\\n\nLS-DHM(VggNet11)&--& 81.68\\\\\n \\hline\nVggNet11 &--& 79.85\\\\\n\nLS-DHM(VggNet11)&--& \\textbf{83.75}\\\\\n\n\\hline\n\\end{tabular}\n\n\\label{table2}\n\\end{table}\n\n\\begin{table}[tb]\n\n\\centering  \n\\caption{Comparisons of the proposed LS-DHM with the state-of-the-art on the SUN397 database.}\n\n\\begin{tabular}{p{3.3cm}||p{2cm}<{\\centering}||p{1.8cm}<{\\centering}}\n\\hline\n\\textbf{Method} & \\textbf{Publication} & \\textbf{Accuracy}($\\%$)\\\\\n\\hline\n\nXiao \\emph{et al.}\\cite{xiao2010sun} & CVPR2010&38.00\\\\ \\hline\nFV(SIFT)\\cite{sanchez2013image} &  IJCV2013 &43.02\\\\\nFV(SIFT+LCS)\\cite{sanchez2013image}& IJCV2013 &47.20\\\\\nFV(SPM+OPM)\\cite{xie2014orientational}& CVPR2014 &45.91\\\\\nLCCD+SIFT \\cite{Guo2015} &arXiv2015&49.68\\\\\n\\hline\nDeCAF \\cite{donahue2013decaf} & ICML2014& 40.94\\\\\nMOP-CNN \\cite{gong2014multi} &ECCV2014 &51.98\\\\\nKoskela \\textit{et al.}\\cite{koskela2014convolutional} &ACM2014 &54.70\\\\\nDAG-VggNet19 \\cite{Yang2015}&ICCV2015&56.20\\\\\nMs-DSP (VggNet16) \\cite{Gao2015} &arXiv2015&59.78\\\\\nC-HLSTM \\cite{Zuo2015} &arXiv2015&60.34\\\\\n\\hline\\hline\nPlaces-CNN (AlexNet)\\cite{zhou2014learning} &NIPS2014& 54.32\\\\\nLS-DHM (AlexNet) &--&62.97\\\\\n\\hline\nGoogleNet &--& 58.79\\\\\n\nLS-DHM (GoogleNet) &--& 65.40\\\\\\hline\nVggNet11 &--& 64.02\\\\\n\nLS-DHM (VggNet11) &--& \\textbf{67.56}\\\\\\hline\n\n\\hline\n\\end{tabular}\n\\\n\\label{table3}\n\\end{table}\n\n\n\\section{ Conclusions}\n\nWe have presented the Locally-Supervised Deep Hybrid Model (LS-DHM) that explores the convolutional features of the CNN for scene recognition. We observe that the FC representation of the CNN is highly abstractive to global layout of the image, but is not discriminative to local fine-scale object cues. We propose the Local Convolutional Supervision (LCS) to enhance the local semantics of fine-scale objects or regions in the convolutional layers. Then we develop an efficient Fisher Convolutional Vector (FCV) that encodes the important local semantics into an orderless mid-level representation, which compensates strongly to the high-level FC-features for scene classification. Both the FCV and FC-features are collaboratively employed in the LS-DHM representation, leading to substantial performance improvements over current state-of-the-art methods on the MIT Indoor67 and SUN 397.\n\n\n\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{ref}\n}\n\n\n\n", "itemtype": "equation", "pos": 29336, "prevtext": "\nwhere $\\textbf{W}$ is model weights that parameterize the function $f(\\textbf{x}_i;\\textbf{W})$ . $\\mathcal{L}(\\cdot)$ denotes the loss function, which is typically a hinge loss for our classification task. $\\|\\textbf{W}\\|_2$ is the regularization term. The training of the CNN is to look for a optimized $\\textbf{W}$ that maps $\\emph{I}_i$ from the image space onto its label space.\n\nExtending from the standard CNN, the LCS introduces a new auxiliary loss ($\\ell^a$) to the convolutional layer of the main networks, as shown in Fig. \\ref{fig10}. It can be formulated as,\n\n", "index": 3, "text": "\\begin{equation}\\label{MTL_general}\n\n \\arg\\min_{\\textbf{W},\\textbf{W}^a}\\!\\!{\\sum_{i=1}^{N}\\!{\\mathcal{L}(y_{i}\\!,\\!f(\\textbf{I}_i\\!;\\!\\textbf{W})) } \\!\\!+\\!\\! \\sum_{i=1}^{N}\\!\\!{\\sum_{a\\in A}\\!\\!{\\lambda^a\\ell^a(y^a\\!, \\!f(\\textbf{I}_i;\\!\\textbf{W}^a ))} }},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\arg\\min_{\\textbf{W},\\textbf{W}^{a}}\\!\\!{\\sum_{i=1}^{N}\\!{\\mathcal{L}(y_{%&#10;i}\\!,\\!f(\\textbf{I}_{i}\\!;\\!\\textbf{W}))}\\!\\!+\\!\\!\\sum_{i=1}^{N}\\!\\!{\\sum_{a%&#10;\\in A}\\!\\!{\\lambda^{a}\\ell^{a}(y^{a}\\!,\\!f(\\textbf{I}_{i};\\!\\textbf{W}^{a}))}}},\" display=\"block\"><mrow><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mpadded width=\"-3.4pt\"><munder><mi>min</mi><mrow><mtext>\ud835\udc16</mtext><mo>,</mo><msup><mtext>\ud835\udc16</mtext><mi>a</mi></msup></mrow></munder></mpadded></mrow><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mpadded><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"-1.7pt\"><msub><mi>y</mi><mi>i</mi></msub></mpadded><mo rspace=\"0.8pt\">,</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"-1.7pt\"><msub><mtext>\ud835\udc08</mtext><mi>i</mi></msub></mpadded><mo rspace=\"0.8pt\">;</mo><mtext>\ud835\udc16</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"0pt\">+</mo><mrow><mpadded width=\"-3.4pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mpadded><mrow><mpadded width=\"-3.4pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>a</mi><mo>\u2208</mo><mi>A</mi></mrow></munder></mpadded><mrow><msup><mi>\u03bb</mi><mi>a</mi></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u2113</mi><mi>a</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"-1.7pt\"><msup><mi>y</mi><mi>a</mi></msup></mpadded><mo rspace=\"0.8pt\">,</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mtext>\ud835\udc08</mtext><mi>i</mi></msub><mo rspace=\"0.8pt\">;</mo><msup><mtext>\ud835\udc16</mtext><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]