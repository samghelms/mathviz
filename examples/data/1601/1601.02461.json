[{"file": "1601.02461.tex", "nexttext": "\n where ${\\epsilon}_{pi}(t) \\overset{iid}{\\sim} {\\mathrm}{N}(0,\\tau_p^2)$ is random noise and $Z_{pi}(t)$ is a random process. For identification purposes, we fix $\\tau_p=1$ for binary responses. Furthermore, let $Z_{pi}(t) = \\mu_{pi}(t) + f_{pi}(t)$, the sum of a fixed mean function $\\mu_{pi}(t)$ and a smooth subject-specific process $f_{pi}(t)$, assumed to be uncorrelated with ${\\epsilon}_{pi}(t)$.  \n \n\nThe mean can be modeled as $\\mu_{pi}(t)= \\sum_{j=1}^{{\\mathrm}{m}_{p}}x_{pij}(t) \\beta_{1pj} + s_p(t)$ so that it can incorporate ${\\mathrm}{m}_{p}$ covariates $x_{pij}(t)$ with fixed coefficients $\\beta_{1pj}$ and a population-level smooth function $s_p(t)$. It is possible for a subject-specific covariate to depend on the functional location $t$, for example the indicator of jaw in the periodontal data of Section \\ref{s:dental_data}, and it is also possible for the same covariates to affect all responses. The smooth function $s_{p}(t)$ is assumed to be square integrable on ${\\cal L}^2[0,1]$. We use a predetermined basis expansion to approximate $s_p(t)$. Let $\\{ {{\\mathrm}{B}}_{p j}(t): 1 \\le j \\le {\\mathrm}{n}_{p} \\}$ be a basis expansion in ${\\cal L}^2[0,1]$  of dimension ${\\mathrm}{n}_{p}$. We approximate the smooth part by $s_p(t)= \\sum_{j=1}^{{\\mathrm}{n}_{p}} { {{\\mathrm}{B}}_{p j}(t)\\beta_{2p j}}$ where the type of basis expansions are allowed to differ across response $p$. To simplify notation, we write $\\mu_{pi}(t)= {\\bm{u}}^{\\mathrm}{T}_{pi}(t) {{\\bm{\\beta}}_p}$ where ${\\bm{u}}_{pi}(t)=[x_{pi1}(t),\\hdots,x_{pi{\\mathrm}{m}_p}(t),{{\\mathrm}{B}}_{p1}(t),\\hdots,{{\\mathrm}{B}}_{p {\\mathrm}{n}_p}(t)  ]^{\\mathrm}{T}$ is a vector of length ${\\mathrm}{J}_p={\\mathrm}{m}_{p}+{\\mathrm}{n}_{p}$ that combines the covariates and basis functions and has corresponding coefficient vector ${{\\bm{\\beta}}_p}=[ {\\beta}_{1p1},\\hdots, {\\beta}_{1p {\\mathrm}{m}_{p}}, {\\beta}_{2p 1},\\hdots, {\\beta}_{2p {\\mathrm}{n}_{p}}]^{\\mathrm}{T}$.\n\n\nLet  ${\\bm}{f}_i(t)=[f_{1i}(t),\\hdots ,f_{{\\mathrm}{P} i}(t) ]^{\\mathrm}{T}$  be the vector of random subject-specific deviation functions and assume ${\\bm}{f}_i(t)$ are i.i.d. mean-zero Gaussian processes where  ${\\mathrm}{Cov}\\{{\\bm}{f}_i(t),{\\bm}{f}_i(t{^\\prime})\\}= {\\bm}{K}(t,t{^\\prime})$ and $K_{pp{^\\prime}}(t,t{^\\prime})={\\mathrm}{Cov} \\{f_{pi}(t),f_{p{^\\prime} i}(t{^\\prime})\\}$ form the elements of ${\\bm}{K}(t,t{^\\prime})$. The covariance operator $K_{pp{^\\prime}}(t,t{^\\prime})$ captures both auto-dependence ($p=p{^\\prime}$) and cross-dependence ($p\\neq p{^\\prime}$) between two different latent responses. We assume that $f_{pi}(t)$ is a  smooth process in ${\\cal L}^2[0,1]$ and present two ways of specifying basis expansions for  $f_{pi}(t)$: Section \\ref{s:pre} details how to use predetermined bases and Section \\ref{s:datadriven} gives a data-driven approach that uses multivariate FPCA. \n\nWe can write the multivariate model succinctly in matrix form. Let ${{\\bm{\\beta}}}=[{\\bm{\\beta}}_1^{\\mathrm}{T}, \\hdots, {\\bm{\\beta}}_{\\mathrm}{P}^{\\mathrm}{T} ]$  be the fixed effect vector of length ${\\mathrm}{J}= \\sum_{p=1}^{\\mathrm}{P}  {\\mathrm}{J}_p $ with corresponding ${\\mathrm}{P} \\times {\\mathrm}{J}$ matrix ${\\bm}{U}_i(t)$ comprised of appropriate evaluations of ${\\bm{u}}_{pi}(t)$. Let ${\\bm}{\\epsilon}_i(t) \\overset{iid}{\\sim} N(0, {\\bm}{D})$ where ${\\bm}{D}$ is diagonal with elements $\\tau_1^2,\\hdots,\\tau_{\\mathrm}{P}^2$.\n Then \\eqref{latent1} becomes \n", "itemtype": "equation", "pos": 6684, "prevtext": "\n\n\n\\begin{center}\n\t{\\Large {\\bf Modeling Multivariate Mixed-Response Functional Data}}\\\\\n    Beth A. Tidemann-Miller\\footnote{Biogen Inc., Cambridge, Massachusetts},\n\tBrian J. Reich\\footnote{\n\t\tDepartment of Statistics, North Carolina State University, \n\t\tRaleigh, North Carolina},\n\tand Ana-Maria Staicu$^2$\\\\\n\t\\today\n\\end{center}\n\n\n\\begin{abstract}\nWe propose a Bayesian modeling framework for jointly analyzing multiple functional responses of different types (e.g. binary and continuous data). Our approach is based on a multivariate latent Gaussian process and models the dependence among the functional responses through the dependence of the latent process. Our framework easily accommodates additional covariates.  We offer a way to estimate the multivariate latent covariance, allowing for implementation of multivariate functional principal components analysis (FPCA) to specify basis expansions and simplify computation. We demonstrate our method through both simulation studies and an application to real data from a periodontal study. \\\\\n{\\bf Keywords}: Bayesian analysis; Binary data; Markov chain Monte Carlo; Multivariate Functional Principal Components Analysis.\n\n\\end{abstract}\n\n\\section{Introduction}\\label{s:intro}\n\nUntil recently, the primary focus of methods employing functional principal components analysis (FPCA) has been on real-valued functional responses. Methods that can model non-Gaussian functional responses, such as repeatedly observed binary or count data, are only recently appearing for univariate functional responses (for example: \\cite{Hal+:08}, \\cite{van:09}, \\cite{Ser+:13}). Additionally, methods that extend functional modeling from the univariate case (i.e. one response curve) to the multivariate case (i.e. a vector of multiple response curves) are currently undergoing development (for example: \\cite{Zho+:08}, \\cite{Ber+:11}, \\cite{Jac+Pre:14}). These multivariate functional methods are limited in that all curves comprising the multivariate response vector must be real-valued. \n\nHere we propose a Bayesian multivariate functional model that utilizes a multivariate latent Gaussian process and can handle responses of different types, e.g. binary and continuous data. Our method easily incorporates covariates, a feature previously unavailable for modeling non-Gaussian functional responses. As an extension of the methods of  \\cite{Hal+:08}, we propose a way to estimate the multivariate latent covariance, in particular, the cross-covariance of latent functions corresponding to different responses. \nBy using a reliable estimate of the multivariate latent covariance, our proposed method can implement multivariate FPCA to specify basis expansions and simplify computation.\n\nSeveral approaches to modeling non-Gaussian univariate functional responses have appeared in the literature. For binary or count data observed repeatedly, \\cite{Hal+:08} proposed a non-parametric functional approach in which the observed responses are directly related to a latent Gaussian functional process through a link function. In order to implement FPCA, they used a Taylor series approximation to derive estimators of the latent process mean function and covariance operator and used bootstrapping methods for further inference. A similar approach by \\cite{Ser+:13} used logistic functional regression to model multilevel cross-dependent binary-valued functional data. In the case of non-rare events, their approach is an extension of the linear approximation methods of \\cite{Hal+:08} to multilevel data. For rare events, they introduced an approach centered around an exponential approximation. \n\nIn contrast to the aforementioned frequentist methods, \\cite{van:09} offered a Bayesian approach to FPCA for repeatedly observed binary or count data. They extended the variational algorithm for Gaussian responses given in \\cite{van:08}, and focused on canonical links for one-parameter exponential families.\nThe methods of \\cite{Hal+:08}, \\cite{Ser+:13} and \\cite{van:09} offer ways to model univariate functional responses, whereas the approach we propose in this paper jointly models multivariate functional responses of mixed type. \n\nTo date, the literature concerning multivariate FPCA has been sparse. \\cite{Ram+Sil:05} gave a brief example that uses FPCA for a bivariate functional response of hip and knee angle measurements for gait data. After assigning the two functional responses to a fine grid of points, they concatenated the two response functions and proceeded with PCA in the traditional multivariate framework. \\cite{Ber+:11} proposed multivariate FPCA in which the principal components are smooth functions, a result of performing FPCA at each observed location in a domain on which curves have been smoothed. In contrast to the approach of \\cite{Ram+Sil:05}, \\cite{Jac+Pre:14} presented a method that allowed for non-orthonormal bases which made it possible for each curve in the multivariate response vector to have its own basis expansion. Their approach neatly addresses how to handle responses with differing magnitudes of variation within the curves.\n\nTo our knowledge, our method that models multivariate mixed-type responses is the first of its kind within the functional data analysis literature. In the spatial literature, \\cite{Rei+Ban:10} developed a spatial latent factor model for multivariate mixed-response data with informative missingness. Our approach shares several similarities to that of \\cite{Rei+Ban:10}, however our approach is able to examine complex correlation structures that their stationary spatial method is not equipped to handle. \n\n\n\\section{Model}\n\\subsection{General Framework}\\label{s:model}\n\nWe present the following methodology to jointly model ${\\mathrm}{P}$ functional responses. Denote $Y_{pi}(t)$ as the observed functional response of type $p=1,\\hdots,{\\mathrm}{P}$ for subject $i=1,\\hdots,{\\mathrm}{N}$ at location $t \\in {\\cal T}$. The responses $Y_{pi}(t)$ are observed only at a finite set of ${\\mathrm}{L}_{pi}$ locations $t_{pi1},t_{pi2},\\hdots,t_{pi{\\mathrm}{L}_{pi}}$, which may be different for subject and response type. To combine responses with different supports, e.g., binary and continuous, let $Y_{pi}(t) = h_p \\{ W_{pi}(t) \\}$ for link function $h_p(\\cdot)$ and latent response $W_{pi}(t)$. Motivated by the periodontal application in Section \\ref{s:dental_data}, we restrict our attention to Gaussian and binary responses. If response $p$ is Gaussian then we use the identity link $h_p(\\eta)=\\eta$; if response $p$ is binary, then we use the indicator link $h_p(\\eta)={\\mathrm}{I}(\\eta>0)$.\n\nDependence between responses is modeled via the latent Gaussian processes\n\n", "index": 1, "text": "\\begin{align} \\label{latent1}\n\t\tW_{pi}(t) = Z_{pi}(t) + \\epsilon_{pi}(t)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle W_{pi}(t)=Z_{pi}(t)+\\epsilon_{pi}(t)\" display=\"inline\"><mrow><mrow><msub><mi>W</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>Z</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03f5</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\n\n\t\\subsection{Predetermined bases}\\label{s:pre}\n\n\nThe first way in which we specify basis expansions for $f_{pi}(t)$ is by choosing predetermined bases such as B-spline, Fourier, or polynomial bases.\nLet \n\n", "itemtype": "equation", "pos": 10203, "prevtext": "\n where ${\\epsilon}_{pi}(t) \\overset{iid}{\\sim} {\\mathrm}{N}(0,\\tau_p^2)$ is random noise and $Z_{pi}(t)$ is a random process. For identification purposes, we fix $\\tau_p=1$ for binary responses. Furthermore, let $Z_{pi}(t) = \\mu_{pi}(t) + f_{pi}(t)$, the sum of a fixed mean function $\\mu_{pi}(t)$ and a smooth subject-specific process $f_{pi}(t)$, assumed to be uncorrelated with ${\\epsilon}_{pi}(t)$.  \n \n\nThe mean can be modeled as $\\mu_{pi}(t)= \\sum_{j=1}^{{\\mathrm}{m}_{p}}x_{pij}(t) \\beta_{1pj} + s_p(t)$ so that it can incorporate ${\\mathrm}{m}_{p}$ covariates $x_{pij}(t)$ with fixed coefficients $\\beta_{1pj}$ and a population-level smooth function $s_p(t)$. It is possible for a subject-specific covariate to depend on the functional location $t$, for example the indicator of jaw in the periodontal data of Section \\ref{s:dental_data}, and it is also possible for the same covariates to affect all responses. The smooth function $s_{p}(t)$ is assumed to be square integrable on ${\\cal L}^2[0,1]$. We use a predetermined basis expansion to approximate $s_p(t)$. Let $\\{ {{\\mathrm}{B}}_{p j}(t): 1 \\le j \\le {\\mathrm}{n}_{p} \\}$ be a basis expansion in ${\\cal L}^2[0,1]$  of dimension ${\\mathrm}{n}_{p}$. We approximate the smooth part by $s_p(t)= \\sum_{j=1}^{{\\mathrm}{n}_{p}} { {{\\mathrm}{B}}_{p j}(t)\\beta_{2p j}}$ where the type of basis expansions are allowed to differ across response $p$. To simplify notation, we write $\\mu_{pi}(t)= {\\bm{u}}^{\\mathrm}{T}_{pi}(t) {{\\bm{\\beta}}_p}$ where ${\\bm{u}}_{pi}(t)=[x_{pi1}(t),\\hdots,x_{pi{\\mathrm}{m}_p}(t),{{\\mathrm}{B}}_{p1}(t),\\hdots,{{\\mathrm}{B}}_{p {\\mathrm}{n}_p}(t)  ]^{\\mathrm}{T}$ is a vector of length ${\\mathrm}{J}_p={\\mathrm}{m}_{p}+{\\mathrm}{n}_{p}$ that combines the covariates and basis functions and has corresponding coefficient vector ${{\\bm{\\beta}}_p}=[ {\\beta}_{1p1},\\hdots, {\\beta}_{1p {\\mathrm}{m}_{p}}, {\\beta}_{2p 1},\\hdots, {\\beta}_{2p {\\mathrm}{n}_{p}}]^{\\mathrm}{T}$.\n\n\nLet  ${\\bm}{f}_i(t)=[f_{1i}(t),\\hdots ,f_{{\\mathrm}{P} i}(t) ]^{\\mathrm}{T}$  be the vector of random subject-specific deviation functions and assume ${\\bm}{f}_i(t)$ are i.i.d. mean-zero Gaussian processes where  ${\\mathrm}{Cov}\\{{\\bm}{f}_i(t),{\\bm}{f}_i(t{^\\prime})\\}= {\\bm}{K}(t,t{^\\prime})$ and $K_{pp{^\\prime}}(t,t{^\\prime})={\\mathrm}{Cov} \\{f_{pi}(t),f_{p{^\\prime} i}(t{^\\prime})\\}$ form the elements of ${\\bm}{K}(t,t{^\\prime})$. The covariance operator $K_{pp{^\\prime}}(t,t{^\\prime})$ captures both auto-dependence ($p=p{^\\prime}$) and cross-dependence ($p\\neq p{^\\prime}$) between two different latent responses. We assume that $f_{pi}(t)$ is a  smooth process in ${\\cal L}^2[0,1]$ and present two ways of specifying basis expansions for  $f_{pi}(t)$: Section \\ref{s:pre} details how to use predetermined bases and Section \\ref{s:datadriven} gives a data-driven approach that uses multivariate FPCA. \n\nWe can write the multivariate model succinctly in matrix form. Let ${{\\bm{\\beta}}}=[{\\bm{\\beta}}_1^{\\mathrm}{T}, \\hdots, {\\bm{\\beta}}_{\\mathrm}{P}^{\\mathrm}{T} ]$  be the fixed effect vector of length ${\\mathrm}{J}= \\sum_{p=1}^{\\mathrm}{P}  {\\mathrm}{J}_p $ with corresponding ${\\mathrm}{P} \\times {\\mathrm}{J}$ matrix ${\\bm}{U}_i(t)$ comprised of appropriate evaluations of ${\\bm{u}}_{pi}(t)$. Let ${\\bm}{\\epsilon}_i(t) \\overset{iid}{\\sim} N(0, {\\bm}{D})$ where ${\\bm}{D}$ is diagonal with elements $\\tau_1^2,\\hdots,\\tau_{\\mathrm}{P}^2$.\n Then \\eqref{latent1} becomes \n", "index": 3, "text": "\\begin{align}\n\t\\label{mod1}\n\t\t{\\bm}{W}_i(t)=  {\\bm}{U}_i(t) {\\bm{\\beta}} + {\\bm}{f}_i(t) + {\\bm}{\\epsilon}_i(t) .\n\t\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{}}{W}_{i}(t)={\\bm{}}{U}_{i}(t){\\bm{\\beta}}+{\\bm{}}{f}_{i}(t)%&#10;+{\\bm{}}{\\epsilon}_{i}(t).\" display=\"inline\"><mrow><mrow><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>+</mo><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03f5</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\t\n\t where $\\{ \\psi_{p k}(t): 1 \\le k \\le {\\mathrm}{M}_p \\}$ is a basis expansion in ${\\cal L}^2[0,1]$ of dimension ${\\mathrm}{M}_p$ and ${\\bm{\\alpha}}_{pi} = [\\alpha_{pi1},\\hdots,\\alpha_{pi{{\\mathrm}{M}_p}} ]^{\\mathrm}{T}$ are random coefficients with ${\\mathrm{E}}(\\alpha_{pik})=0$ and ${\\mathrm}{Cov}(\\alpha_{pik},\\alpha_{p{^\\prime}  i \\ell })= \\xi_{k \\ell p p{^\\prime}}$. The multivariate covariance function induced by \\eqref{f1} is \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\n\t\\subsection{Predetermined bases}\\label{s:pre}\n\n\nThe first way in which we specify basis expansions for $f_{pi}(t)$ is by choosing predetermined bases such as B-spline, Fourier, or polynomial bases.\nLet \n\n", "index": 5, "text": "\\begin{align} \\label{f1}\n\tf_{pi}(t)= \\sum_{k=1}^{{\\mathrm}{M}_p} \\psi_{pk}(t) \\alpha_{pik}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{pi}(t)=\\sum_{k=1}^{{\\mathrm{}}{M}_{p}}\\psi_{pk}(t)\\alpha_{pik}\" display=\"inline\"><mrow><mrow><msub><mi>f</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>p</mi></msub></munderover></mstyle><mrow><msub><mi>\u03c8</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03b1</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\t\nwhich is a function of both the basis functions and covariance ${\\bm}{\\Sigma}=\\{ \\xi_{k \\ell p p{^\\prime}} \\}$. \n\t Using predetermined basis expansions is extremely flexible; \nin the Appendix, we discuss how the covariance model can approximate the covariance matrix of any arbitrary finite-dimensional distribution. The choice of ${\\mathrm}{M}_p$ is important in that one needs to select a number of basis functions that is sufficient to approximate the covariance well but is not unnecessarily large. We suggest choosing ${\\mathrm}{M}_p$ based on a grid search, using criteria such as deviance information criteria (DIC) for comparison.\n\t \n\n\t\\subsection{Data-driven bases}\\label{s:datadriven}\nAs an alternative to using predetermined bases, we introduce a novel approach in which we use estimated basis functions that are obtained through FPCA of the multivariate latent covariance.\nWe propose FPCA for multivariate mixed-responses, inspired by \\cite{Hal+:08} who introduced FPCA for binary-valued functional responses. We too require that the probability of observing a binary event is sufficiently far from zero or one. For simplicity of presentation, we ignore the covariates and discuss how to account for them later in this section.\n\nRecall from \\eqref{latent1} that we model the $p^{{\\mathrm}{th}}$ response as $Y_{pi}(t) = h_p \\{ W_{pi}(t) \\}$ through the latent Gaussian process $W_{pi}(t) = Z_{pi}(t) + \\epsilon_{pi}(t)$ and link function $h_p(\\eta)$. Linking the latent response directly to the observed response is equivalent to assuming there is a corresponding monotone link function $g_p(\\cdot)$ such that ${\\mathrm{E}}\\{Y_{ip}(t) | Z_{pi}(t) \\} = g_p\\{Z_{pi}(t)\\}$; we focus on $g_p$ here.\nFollowing \\cite{Hal+:08}, assume that $g_p(\\cdot)$ has bounded fourth derivative and that the latent process  satisfies $Z_{pi}(t)= \\mu_{p}(t) + \\delta X_{pi}(t)$ for fixed mean $\\mu_p(t)$, unknown small constant $\\delta>0$, and mean-zero Gaussian random variable $X_{pi}(t)$ that is i.i.d. across subjects $i$ and has both finite variance and finite covariance between $X_{pi}(t)$ and $X_{p{^\\prime} i}(t{^\\prime})$. Our goal is to approximate the latent covariance matrix of $Z_{pi}(t)$ whose covariance operator is $K_{pp{^\\prime}}(t,t{^\\prime})={\\mathrm}{Cov}\\{Z_{pi}(t), Z_{p{^\\prime} i}(t{^\\prime})\\}$. Without loss of generality, we restrict our attention to one continuous Gaussian response ($p=1$) and one binary response ($p=2$) with link functions $g_1(\\eta)=\\eta$ and $g_2(\\eta)=\\Phi(\\eta)$ where $\\Phi(\\cdot)$ is the standard normal cdf function. For simplicity, we use $g$ to denote $g_2$ in the following exposition.\n\n \nThe covariance consists of variance components $K_{pp}$ and cross-covariance components $K_{pp{^\\prime}}$. The variance components $K_{11}$ and $K_{22}$ are estimated using the common FPCA for continuous responses \\cite{Ram+Sil:02, Ram+Sil:05} \nas well as binary-valued responses \\citep{Hal+:08}, respectively. In particular, when the responses are binary valued, the variance $K_{22}$ is estimated using \n \n", "itemtype": "equation", "pos": -1, "prevtext": "\t\n\t where $\\{ \\psi_{p k}(t): 1 \\le k \\le {\\mathrm}{M}_p \\}$ is a basis expansion in ${\\cal L}^2[0,1]$ of dimension ${\\mathrm}{M}_p$ and ${\\bm{\\alpha}}_{pi} = [\\alpha_{pi1},\\hdots,\\alpha_{pi{{\\mathrm}{M}_p}} ]^{\\mathrm}{T}$ are random coefficients with ${\\mathrm{E}}(\\alpha_{pik})=0$ and ${\\mathrm}{Cov}(\\alpha_{pik},\\alpha_{p{^\\prime}  i \\ell })= \\xi_{k \\ell p p{^\\prime}}$. The multivariate covariance function induced by \\eqref{f1} is \n\n", "index": 7, "text": "\\begin{align}\n\tK_{pp{^\\prime}}(t,t{^\\prime})={\\mathrm}{Cov}\\{f_{pi}(t), f_{p{^\\prime} i}(t{^\\prime})\\} = \\sum_{k=1}^{{\\mathrm}{M}_p} \\sum_{\\ell=1}^{{\\mathrm}{M}_{p{^\\prime}}} \\psi_{p k}(t) \\psi_{p{^\\prime} \\ell}(t{^\\prime})   \\xi_{k \\ell p p{^\\prime}},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle K_{pp{{}^{\\prime}}}(t,t{{}^{\\prime}})={\\mathrm{}}{Cov}\\{f_{pi}(t%&#10;),f_{p{{}^{\\prime}}i}(t{{}^{\\prime}})\\}=\\sum_{k=1}^{{\\mathrm{}}{M}_{p}}\\sum_{%&#10;\\ell=1}^{{\\mathrm{}}{M}_{p{{}^{\\prime}}}}\\psi_{pk}(t)\\psi_{p{{}^{\\prime}}\\ell}%&#10;(t{{}^{\\prime}})\\xi_{k\\ell pp{{}^{\\prime}}},\" display=\"inline\"><mrow><msub><mi>K</mi><mrow><mi>p</mi><mi>p</mi><msup><mi/><mo>\u2032</mo></msup></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mi>C</mi><mi>o</mi><mi>v</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msub><mi>f</mi><mrow><mi>p</mi><mo>\u2062</mo><mmultiscripts><mi>i</mi><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>p</mi></msub></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mrow><mi>p</mi><msup><mi/><mo>\u2032</mo></msup></mrow></msub></munderover></mstyle><msub><mi>\u03c8</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03c8</mi><mrow><mi>p</mi><mo>\u2062</mo><mmultiscripts><mi mathvariant=\"normal\">\u2113</mi><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><msub><mi>\u03be</mi><mrow><mi>k</mi><mi mathvariant=\"normal\">\u2113</mi><mi>p</mi><mi>p</mi><msup><mi/><mo>\u2032</mo></msup></mrow></msub><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nwhere $g^{(1)}$ indicates the first derivative of $g$. The latent mean estimator is $\\hat{\\mu}_p(t)=g^{-1}\\{ \\hat{\\eta}_p(t) \\}$ where $\\hat{\\eta}_p(t)$ estimates ${\\mathrm{E}}[g\\{ Z_{pi}(t) \\}] ={\\eta}_p(t)$ and is found by smoothing the data $\\big(t,Y_{p i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. $\\hat{S}_{22}(t,t{^\\prime})$ is the estimator for $S_{22}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{2 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[ g\\{ Z_{2i}(t) \\} g\\{ Z_{2i}(t{^\\prime}) \\} \\big]$ and is obtained through bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{2 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, removing the diagonals before smoothing.\n\n\nFor the cross covariance operator $K_{12}$ we remark that \n\n", "itemtype": "equation", "pos": 14391, "prevtext": "\t\nwhich is a function of both the basis functions and covariance ${\\bm}{\\Sigma}=\\{ \\xi_{k \\ell p p{^\\prime}} \\}$. \n\t Using predetermined basis expansions is extremely flexible; \nin the Appendix, we discuss how the covariance model can approximate the covariance matrix of any arbitrary finite-dimensional distribution. The choice of ${\\mathrm}{M}_p$ is important in that one needs to select a number of basis functions that is sufficient to approximate the covariance well but is not unnecessarily large. We suggest choosing ${\\mathrm}{M}_p$ based on a grid search, using criteria such as deviance information criteria (DIC) for comparison.\n\t \n\n\t\\subsection{Data-driven bases}\\label{s:datadriven}\nAs an alternative to using predetermined bases, we introduce a novel approach in which we use estimated basis functions that are obtained through FPCA of the multivariate latent covariance.\nWe propose FPCA for multivariate mixed-responses, inspired by \\cite{Hal+:08} who introduced FPCA for binary-valued functional responses. We too require that the probability of observing a binary event is sufficiently far from zero or one. For simplicity of presentation, we ignore the covariates and discuss how to account for them later in this section.\n\nRecall from \\eqref{latent1} that we model the $p^{{\\mathrm}{th}}$ response as $Y_{pi}(t) = h_p \\{ W_{pi}(t) \\}$ through the latent Gaussian process $W_{pi}(t) = Z_{pi}(t) + \\epsilon_{pi}(t)$ and link function $h_p(\\eta)$. Linking the latent response directly to the observed response is equivalent to assuming there is a corresponding monotone link function $g_p(\\cdot)$ such that ${\\mathrm{E}}\\{Y_{ip}(t) | Z_{pi}(t) \\} = g_p\\{Z_{pi}(t)\\}$; we focus on $g_p$ here.\nFollowing \\cite{Hal+:08}, assume that $g_p(\\cdot)$ has bounded fourth derivative and that the latent process  satisfies $Z_{pi}(t)= \\mu_{p}(t) + \\delta X_{pi}(t)$ for fixed mean $\\mu_p(t)$, unknown small constant $\\delta>0$, and mean-zero Gaussian random variable $X_{pi}(t)$ that is i.i.d. across subjects $i$ and has both finite variance and finite covariance between $X_{pi}(t)$ and $X_{p{^\\prime} i}(t{^\\prime})$. Our goal is to approximate the latent covariance matrix of $Z_{pi}(t)$ whose covariance operator is $K_{pp{^\\prime}}(t,t{^\\prime})={\\mathrm}{Cov}\\{Z_{pi}(t), Z_{p{^\\prime} i}(t{^\\prime})\\}$. Without loss of generality, we restrict our attention to one continuous Gaussian response ($p=1$) and one binary response ($p=2$) with link functions $g_1(\\eta)=\\eta$ and $g_2(\\eta)=\\Phi(\\eta)$ where $\\Phi(\\cdot)$ is the standard normal cdf function. For simplicity, we use $g$ to denote $g_2$ in the following exposition.\n\n \nThe covariance consists of variance components $K_{pp}$ and cross-covariance components $K_{pp{^\\prime}}$. The variance components $K_{11}$ and $K_{22}$ are estimated using the common FPCA for continuous responses \\cite{Ram+Sil:02, Ram+Sil:05} \nas well as binary-valued responses \\citep{Hal+:08}, respectively. In particular, when the responses are binary valued, the variance $K_{22}$ is estimated using \n \n", "index": 9, "text": "\\begin{align} \\label{Hall_univ}\n\\widetilde{K}_{22}(t,t{^\\prime}) = \\{\\hat{S}_{22}(t,t{^\\prime})- \\hat{\\eta}_2(t)\\hat{\\eta}_2(t{^\\prime}) \\} /[ g^{(1)}\\{\\hat{\\mu}_2(t)\\}  g^{(1)}\\{\\hat{\\mu}_2(t{^\\prime})\\}   ],\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widetilde{K}_{22}(t,t{{}^{\\prime}})=\\{\\hat{S}_{22}(t,t{{}^{%&#10;\\prime}})-\\hat{\\eta}_{2}(t)\\hat{\\eta}_{2}(t{{}^{\\prime}})\\}/[g^{(1)}\\{\\hat{\\mu%&#10;}_{2}(t)\\}g^{(1)}\\{\\hat{\\mu}_{2}(t{{}^{\\prime}})\\}],\" display=\"inline\"><mrow><msub><mover accent=\"true\"><mi>K</mi><mo>~</mo></mover><mn>22</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">^</mo></mover><mn>22</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>/</mo><mrow><mo stretchy=\"false\">[</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nwhich is obtained by approximating \n$ {\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}= {\\mathrm}{Cov} \\big[Z_{1i}(t),g\\{Z_{2 i}(t{^\\prime})\\} \\big]$ \nusing a Taylor expansion of $g\\{Z_{2 i}(t{^\\prime})\\}$ around $\\mu_{2}(t{^\\prime})$. More details are given in the Appendix. \nThis leads to the estimator of the cross component given by \n\n", "itemtype": "equation", "pos": 15350, "prevtext": "\nwhere $g^{(1)}$ indicates the first derivative of $g$. The latent mean estimator is $\\hat{\\mu}_p(t)=g^{-1}\\{ \\hat{\\eta}_p(t) \\}$ where $\\hat{\\eta}_p(t)$ estimates ${\\mathrm{E}}[g\\{ Z_{pi}(t) \\}] ={\\eta}_p(t)$ and is found by smoothing the data $\\big(t,Y_{p i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. $\\hat{S}_{22}(t,t{^\\prime})$ is the estimator for $S_{22}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{2 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[ g\\{ Z_{2i}(t) \\} g\\{ Z_{2i}(t{^\\prime}) \\} \\big]$ and is obtained through bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{2 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, removing the diagonals before smoothing.\n\n\nFor the cross covariance operator $K_{12}$ we remark that \n\n", "index": 11, "text": "\\begin{align} \\label{cross}\n{K}_{12}(t,t{^\\prime}) = {\\mathrm}{Cov} \\big\\{Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}/g^{(1)}\\{\\mu_2(t{^\\prime})\\},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{K}_{12}(t,t{{}^{\\prime}})={\\mathrm{}}{Cov}\\big{\\{}Y_{1i}(t),Y_{2%&#10;i}(t{{}^{\\prime}})\\big{\\}}/g^{(1)}\\{\\mu_{2}(t{{}^{\\prime}})\\},\" display=\"inline\"><mrow><msub><mi>K</mi><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mi>C</mi><mi>o</mi><mi>v</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msub><mi>Y</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow><mo>/</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03bc</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\n\\sloppy\nCombining the individually smoothed estimators $\\widetilde{K}_{11}(t,t{^\\prime})$, $\\widetilde{K}_{22}(t,t{^\\prime})$ and $\\widetilde{K}_{12}(t,t{^\\prime})= \\widetilde{K}_{21}(t{^\\prime},t)$ forms the smooth $2\\times 2$ estimator $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$ of the bivariate latent covariance operator. Note that for smoothing purposes in this paper, we implement a global smoother as opposed to the local least squares smoothing of \\cite{Hal+:08}, though either is appropriate. In the presence of subject-specific covariates, one can find covariate estimates using least squares or logistic regression, depending on the type of response, and then use the residuals to estimate the latent covariance.\n\nThe final step for creating basis functions is to implement bivariate FPCA in which we obtain the eigenfunctions ${{\\boldsymbol}{\\theta}}(t)=[{\\theta}_1(t), \\hdots, {\\theta}_{{\\mathrm}{P}}(t)]^{\\mathrm}{T}$ and the eigenvalues ${\\lambda}$ of the matrix $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$.  Note that the matrix $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$ is not guaranteed to be positive definite, but we can ensure the truncated spectral decomposition $\\widetilde{{\\bm}{K}}(t,t{^\\prime})= \\sum^{{\\mathrm}{M}}_{k=1} {\\lambda}_{k}{{\\boldsymbol}{\\theta}}_{k}(t) \\{{{\\boldsymbol}{\\theta}}_{k}(t{^\\prime})\\}^{\\mathrm}{T}$ is positive definite by restricting the inclusion of only positive eigenvalues and their eigenfunction counterparts.\nThe truncation value ${\\mathrm}{M}$  is chosen based on the proportion of variation explained by the eigenvalues as suggested in \\cite{Di+:09}.\nIn particular,  specify a cumulative explained variance threshold ${\\mathrm}{P}_1$ and individual explained variance threshold ${\\mathrm}{P}_2$. Define\n${\\mathrm}{M} = \\min \\{k: p_{1k} \\ge {\\mathrm}{P}_1, p_{2k} < {\\mathrm}{P}_2 \\} $ where \n$p_{1k}=\\sum_{i=1}^{k}{{{\\lambda}_{i}}/\t\\sum_{j=1}^{n}{{ \\lambda}_{j}}}$, $p_{2k}={\\lambda}_k/\t\\sum_{j=1}^{n}{{ \\lambda}_{j}}$ and the positive eigenvalues are the first $n\\ge k$ eigenvalues. We specify the basis functions to be the eigenfunctions scaled by their associated eigenvalues, ${\\psi}_{pk}(t) = \\sqrt{{\\lambda}_k} {{\\theta}}_{pk}(t)$, and the subject-specific deviation function is approximated by \n$f_{pi}(t)= \\sum_{k=1}^{\\mathrm}{M}  {\\psi}_{pk}(t) \\alpha_{ik}$. \n\nUsing this data-driven basis approach, the correlation across responses is largely captured by the basis functions from FPCA. Additionally, since each basis function combines information from all responses, the data-driven approach results in one set of basis functions, eliminating the need to have a set of basis functions for each response. These distinctions offer important advantages over the predetermined basis approach. First, having only one set of basis functions reduces the dimensionality of the random-effect covariance matrix ${\\bm}{\\Sigma}$, making it easier to fit. Second, it allows for further simplification since ${\\bm}{\\Sigma}$ is diagonal. This will offer computational advantages over the predetermined basis method where the burden of capturing the correlation across responses falls entirely on estimating a non-diagonal ${\\bm}{\\Sigma}$ which can potentially have very large dimension. \n\nOne important consideration to make when implementing this data-driven basis function approach is to ensure that the variance of the latent process for the continuous component is on a scale similar to that of the latent process for the binary component. We suggest scaling the continuous process by $Y_{1 i}(t)/s$ where $s$ is the overall sample standard deviation of the continuous response without regard to $t$. Since $s$ is a scalar quantity, it is straightforward to scale prior to implementing the latent covariance, FPCA and MCMC estimation algorithms, rescaling only the final results back to the original scale. \n\n\\subsection{Prior Specification}\\label{s:priors}\n\n\nTo complete the Bayesian model, we specify priors for the hyperparameters.\nThe fixed effect parameters ${{\\bm{\\beta}}}$ are assigned uninformative Gaussian priors. \nLet the subject random effect ${\\bm{\\alpha}}_i$ have a Gaussian prior with ${\\mathrm}{Cov}({\\bm{\\alpha}}_i)={\\bm}{\\Sigma}$ and assign ${\\bm}{\\Sigma}$ an Inverse Wishart prior. For the error variances of the continuous processes, let $\\tau^2_p$ have an uninformative gamma prior;  for identifiability $\\tau^2_p$ is fixed at 1 for binary processes. In summary,\n\t\n", "itemtype": "equation", "pos": 15856, "prevtext": "\nwhich is obtained by approximating \n$ {\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}= {\\mathrm}{Cov} \\big[Z_{1i}(t),g\\{Z_{2 i}(t{^\\prime})\\} \\big]$ \nusing a Taylor expansion of $g\\{Z_{2 i}(t{^\\prime})\\}$ around $\\mu_{2}(t{^\\prime})$. More details are given in the Appendix. \nThis leads to the estimator of the cross component given by \n\n", "index": 13, "text": "\\begin{align} \\label{cross2}\n\\widetilde{K}_{12}(t,t{^\\prime}) = \\{\\hat{S}_{12}(t,t{^\\prime})- \\hat{\\eta}_1(t)\\hat{\\eta}_2(t{^\\prime}) \\} / g^{(1)}\\{\\hat{\\mu}_2(t{^\\prime})\\}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widetilde{K}_{12}(t,t{{}^{\\prime}})=\\{\\hat{S}_{12}(t,t{{}^{%&#10;\\prime}})-\\hat{\\eta}_{1}(t)\\hat{\\eta}_{2}(t{{}^{\\prime}})\\}/g^{(1)}\\{\\hat{\\mu}%&#10;_{2}(t{{}^{\\prime}})\\}.\" display=\"inline\"><mrow><msub><mover accent=\"true\"><mi>K</mi><mo>~</mo></mover><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">^</mo></mover><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>/</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nfor hyperparameters $\\sigma^2_b$, $q_1$, $q_2$, $\\ell$, and $h$, selected to result in weak priors.\n\n \n\n\n\n\\section{Computational Details}\\label{s:compdetails}\n\nTo facilitate MCMC sampling, we treat the continuous latent processes $W_{pi}(t)$ for binary response as unknown parameters to be updated as part of the sampling as in \\cite{Alb+Chi:93}. Using this auxiliary variable approach, all parameters have conditional conjugacy due to the prior specifications given in Section \\ref{s:priors}, allowing us to implement Gibbs sampling. The Gibbs sampling algorithm uses full-conditional posteriors derived in the Appendix and which use notation that we now describe.\n\nDenote the observation locations as $t_{pi\\ell}$, $\\ell = 1, \\hdots, {\\mathrm}{L}_{pi}$, for each subject $i$ and response $p$, giving a total of ${\\mathrm}{L}_i =  \\sum_{p=1}^{\\mathrm}{P} {\\mathrm}{L}_{pi}$ locations. Let $n=\\sum_{i=1}^{\\mathrm}{N}{{\\mathrm}{L}_i}$ be the total number of locations observed across all subjects. \nLet ${\\bm}{W}_{pi}$ be the vector of length ${\\mathrm}{L}_{pi}$ formed by evaluating $W_{pi}(t)$ at every $t_{pi\\ell}$. Furthermore, combine ${\\bm}{W}_{pi}$ for all responses to form one vector ${\\bm}{W}_{i}$ of length ${\\mathrm}{L}_i$; $ {\\bm}{U}_i$ and ${\\bm{\\Psi}}_i$ are defined analogously. Then ${\\bm}{W}_i$ has mean  ${\\mathrm{E}}({\\bm}{W}_i | {\\bm{\\alpha}}_i)= {\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i$ and precision matrix ${\\bm}{P}_i$ is comprised of the appropriate error variance parameter $\\tau_p^{-2}$.\n\n MCMC begins by setting initial values for all parameters and then sequentially sampling each parameter conditioned on all the others (denoted by `$` |\\cdot$\"). Sampling is performed (using the latest sample to update each parameter) according to the full conditionals in the following manner:\n\\begin{enumerate}[1.]\n\t\\item Select initial values for ${\\bm{\\beta}}$ , ${\\bm{\\alpha}}_i$, ${\\bm}{\\Sigma}$, $W_{pi}(t)$ for binary responses, and $\\tau^2_p$ for continuous responses;\n\t\\item  For each $i=1,\\hdots,{\\mathrm}{N}$ and $\\ell=1,\\hdots,{\\mathrm}{L}_{pi}$, update the latent response corresponding to the observed binary response by drawing from  $W_{p i}(t_{i\\ell})|\\cdot \\sim  {\\mathrm}{N}({\\bm{u}}^{\\mathrm}{T}_{pi}(t_{i\\ell}) {\\bm{\\beta}} + {\\bm{\\psi}}^{\\mathrm}{T}_p(t_{i\\ell}) {\\bm{\\alpha}}_i, 1)$ restricted to the interval $(0,\\infty)$ if $Y_{pi}(t_{i\\ell})=1$ or $(-\\infty,0)$ if $Y_{p i}(t_{i\\ell})=0$;\n\t\n\t\\item Update the population mean parameter by drawing from \n\t${\\bm{\\beta}} | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\beta}}}, {\\bm{V}}_{{\\bm{\\beta}}} )$ for\n\t  ${\\bm{V}}_{{\\bm{\\beta}}}= \\left[ \\left(\\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i \\right)+  \\sigma^{-2}_{{\\bm{\\beta}}}{\\textbf{\\text{I}}}_{\\mathrm}{J}  \\right]^{-1}$ and \n\t  ${\\bm}{\\mu}_{{\\bm{\\beta}}}  =  {\\bm{V}}_{{\\bm{\\beta}}} \\left[ \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  ({\\bm}{W}_i - {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i )\\right]$;\t\n\t\n\t\\item For each  $i=1,\\hdots,{\\mathrm}{N}$, update the random effect by sampling from ${\\bm{\\alpha}}_i | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\alpha}}}, {\\bm{V}}_{{\\bm{\\alpha}}} )$ for \n\t${\\bm{V}}_{{\\bm{\\alpha}}}= \\left( {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{\\Sigma}^{-1} \\right) ^{-1}$ and\n ${\\bm}{\\mu}_{{\\bm{\\alpha}}} = {\\bm{V}}_{{\\bm{\\alpha}}} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i ({\\bm}{W}_i-{\\bm}{U}_i {\\bm{\\beta}})$;\n\n\t\\item Update the random effect covariance matrix through ${\\bm}{\\Sigma}| \\cdot \\sim {\\mathrm}{InvWishart}_{{\\mathrm}{M}}[ \\{ \\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T}}  +(1/q_2){\\textbf{\\text{I}}}_{{\\mathrm}{M}}\\}^{-1}, {\\mathrm}{N}+ q_1]$;\n\t\n\t\\item Update the error variance for the continuous responses according to \n\t$\\tau^2_p | \\cdot \\sim {\\mathrm}{InvGamma}(l_\\omega, h_\\omega)$ with $l_\\omega =n/2+l$ and $h_\\omega= h + 1/2 \\sum_{i=1}^{\\mathrm}{N} \\sum_{\\ell=1}^{{{\\mathrm}{L}_i}} [ W_{p i}(t_{i\\ell}) -  {\\bm{u}}^{\\mathrm}{T}_{pi}(t_{i\\ell}) {\\bm{\\beta}} + {\\bm{\\psi}}^{\\mathrm}{T}_p(t_{i\\ell}) {\\bm{\\alpha}}_i ]^2$.\n\n\\end{enumerate}\nSteps 2-6 are repeated for the desired number of samples.\n\n\n\n\\section{Simulations}\\label{s:sim}\n\n\nFor our simulation study, we compare mean estimation and prediction performance among four estimating models when the generating model has a continuous and binary response, either generated separately (univariate) or jointly (bivariate) with strong cross-correlation, for both small and large sample size. The four estimating models are either univariate models applied separately to each response or bivariate models, and either employ the pre-specified basis function method of Section \\ref{s:pre} or the data-driven approach of Section \\ref{s:datadriven}. \n\n\n\t\n\t\\subsection{Data generation}\\label{s:sim_data}\n\t\n We consider the case where $Y_{1i}(t)$ is continuous and $Y_{2i}(t)$ is binary. Functions are observed at a dense, balanced design with ${\\mathrm}{L}_{pi} \\equiv 30$ equally-spaced locations in $[0,1]$ for each subject $i$ and response $p$. We use the model given in \\eqref{mod1} with predetermined bases as in Section \\ref{s:pre} for data generation. We specify a separable random effect covariance matrix ${\\bm}{\\Sigma}= {\\bm}{A} \\otimes {\\bm{C}}$, where ${\\mathrm}{Cov}([\\alpha_{1 ik}, \\alpha_{2 ik}]^{\\mathrm}{T})={\\bm}{A}$ for ${\\bm}{A}_{11}={\\bm}{A}_{22}=1$ and  ${\\bm}{A}_{12}={\\bm}{A}_{21}={\\rho_{{\\bm{\\alpha}}}}$ so that the parameter ${\\rho_{{\\bm{\\alpha}}}}$ controls the correlation between the latent responses, and ${\\mathrm}{Cov}( [ \\alpha_{p i1},\\hdots, \\alpha_{p i{\\mathrm}{M}}]^{\\mathrm}{T})={\\bm{C}}$ for $p = 1, 2 $ controls the covariance of the random effect basis function coefficients and is the same across responses.  The ${\\bm{C}}$ used for data generation has the AR(1) structure with variance 1 and correlation parameter $\\rho=1/2$. \n\nFor the fixed population mean function we assume there are no subject-level covariates so that $\\bm{\\mu}(t)={\\text{\\textbf{B}}}(t){\\bm{\\beta}}$, and we specify a quadratic basis $\\{ {\\mathrm}{B}_{p j}(t)=t^{(j-1)}: 1 \\le j \\le 3 \\}$ for each response $p$ with coefficients ${\\bm{\\beta}}_1 = [-0.64 , 4 , -4 ]^{\\mathrm}{T}$ and ${\\bm{\\beta}}_2=[0.97 ,   -6,    6]^{\\mathrm}{T}$. The intercepts are chosen such that the curves are positive for approximately half of the observed locations $t$. The basis functions for the subject-specific deviation function ${\\bm}{f}_i(t)= {\\bm{\\Psi}}(t) {\\bm{\\alpha}}_i$ are given by\n$\\psi_{1 k}(t)= \\sin\\{(2\\pi k/ {\\mathrm}{M})(t+2\\pi k/{\\mathrm}{M})\\}$ and $\\psi_{2 k}(t)= \\cos\\{(2\\pi k/ {\\mathrm}{M})(t+2\\pi k/{\\mathrm}{M})\\}$ for $k=1,\\hdots,{\\mathrm}{M}=7$. The error variance for the continuous process is $\\tau^2_1=1$.\nWe generate data from four scenarios given in Table \\ref{t:sim2} by varying the sample size (${\\mathrm}{N}=50, 250$) and the cross-correlation (${\\rho_{{\\bm{\\alpha}}}}=0, 0.8$).  \nAll scenarios use $100$ Monte Carlo (MC) replications. \n\n\n\t\n\t\\subsection{Models and metrics for comparison}\\label{s:sim_metrics}\n\t\n\tWe fit four models to each dataset. \n\t\\begin{enumerate}[I.]\n\t\t\\item Bivariate B-spline ({BBSP}): the multivariate model in \\eqref{mod1} with B-spline bases as in Section \\ref{s:pre};\n\t\t\\item Univariate B-spline ({UBSP}): the model from \\eqref{latent1} applied separately to each response with B-spline bases as in Section \\ref{s:pre};\n\t\t\\item Bivariate FPCA ({BFPCA}): the multivariate model in \\eqref{mod1} with data-driven bases as in Section \\ref{s:datadriven};\n\t\t\\item Univariate FPCA ({UFPCA}): the model in \\eqref{latent1}  applied separately to each response with data-driven bases as in Section \\ref{s:datadriven};\n\t\n\t\\end{enumerate}\n\tFor estimation using the B-spline methods, we choose B-splines of order 4 and the number of B-spline breaks for each replication is fixed at $6$ based on preliminary analyses.  For the FPCA methods, we specify an unstructured ${\\bm}{\\Sigma}$, and the number of basis functions is chosen to explain at least ${\\mathrm}{P}_1=99\\%$ of the cumulative variation. In practice, both the number of basis functions for the B-spline method and the percentage of variation explained for the FPCA method are tuning parameters and one should compare results over a grid parameter values. For the population mean we fit the true polynomial basis ${\\text{\\textbf{B}}}(t)$ for estimation. We perform MCMC sampling with 20,000 draws and the first 5,000 are discarded as burn-in. The hyperparameters are specified as $\\sigma^2_b=100$ and $q_1=q_2=l=h=0.1$.\n\n           \nMethods are compared in terms of their predictive performance and ability to estimate the marginal mean function for each response. Let $\\omega_{1i}(t) = {\\mathrm{E}}\\{Y_{1 i}(t)\\} = {\\bm{u}}^{\\mathrm}{T}_{1i}(t) {\\bm{\\beta}}_1$ and $\\omega_{2i}(t) = {\\mathrm{E}}\\{Y_{2 i}(t)\\} =   \\Phi \\{ \\gamma_i(t) \\}$, where $\\gamma_i(t) = {\\bm{u}}^{\\mathrm}{T}_{2i}(t)  {\\bm{\\beta}}_2/\\sqrt{ v_2(t) }$ is the population effect shrunk toward zero by the square root of the marginal variance \n$v_2(t) ={\\mathrm}{Var}\\{Y_2(t)\\} = {\\bm{\\psi}}_2(t) {\\bm}{\\Sigma}_{22} \\{{\\bm{\\psi}}_2(t)\\}^{\\mathrm}{T} + 1$.  \nLet $\\widehat{\\omega}_{p r}(t)$ and $\\hat{\\nu}_{pr}(t)$ be the posterior mean and variance, respectively, for MC replication $r=1,\\hdots,100$. \n Metrics for comparison of estimated means found in Table \\ref{t:sim2} for each response are mean integrated squared error: ${\\mathrm}{MISE} = \\int_t  {\\mathrm{E}} \\{ \\widehat{\\omega}_{p}(t) - {\\omega}_{p}(t)   \\}^2 dt$; coverage of 95\\% pointwise confidence intervals $\\widehat{\\omega}_{pr}(t) \\pm l_{pr}(t)$ averaged over location $t$ and MC replication $r$ with margin of error $l_{pr}(t)=1.96\\sqrt{\\hat{\\nu}_{pr}(t)}$; and confidence interval length $2 l_{pr}(t)$. \n \nFor prediction, we generate additional data $Y_{prj}(t_l)$ at equally spaced locations $t_\\ell \\in [0,1]$ where $\\ell=1,\\hdots,30$ for subjects $j=1,\\hdots,20$ per response $p=1,2$  for each MC replication $r=1,\\hdots,100$. To assess the value of jointly modeling the two responses, we leave out all of response 1 for 10 subjects and all of response 2 for the remaining 10 subjects per replication. \n\nModels are compared in terms of their predictive performance using mean squared prediction error (MSPE) for each response, defined as  ${\\mathrm}{MSPE} =  (nm{\\mathrm}{L})^{-1} \\sum_{r=1}^{n} \\sum_{j=1}^{m} \\sum_{\\ell=1}^{{\\mathrm}{L}}\\{ Y_{prj}(t_\\ell) - \\hat{Y}_{prj}(t_\\ell) \\}^2$. For binary responses this is known as the Brier score and $\\hat{Y}_{prj}(t_\\ell)$ is the posterior probability that $Y=1$.\n\n\n\t\n\t\\subsection{Results}\\label{s:sim_data}\n\t\n\t\n\t\n\t\nTable \\ref{t:sim2} gives the simulation results. There appears to be little difference in mean function estimation between univariate and bivariate methods for all scenarios. When strong correlation is present (Scenarios 1 \\& 2), the bivariate methods show marked improvement in prediction for both responses over the univariate methods, a difference that becomes more pronounced with an increase in sample size. \nBivariate methods perform well when the generating model is univariate (Scenarios 3 \\& 4). Though prediction is better when fitting the correct univariate model, the differences between the bivariate and univariate methods become very small with an increase in sample size. All methods show slight under-coverage.\n\n\n\n\\begin{table}\n\n\n\\caption{Simulation Results}\n\\centering\n\\resizebox{\\linewidth}{!}\n{\n\t\\begin{tabular}{l     llll  l   llll }\n\n\\multicolumn{1}{c}{     }  & \\multicolumn{4}{c}{ {Continuous Response}} & \n\\multicolumn{1}{c}{     }   & \\multicolumn{4}{c}{{Binary Response}} \\\\\n \\cline{2-5}  \\cline{7-10} \\\\\n\n& MISE    & CI length & 95 \\% Cvg  & MSPE & \\vspace{.2cm}  & MISE   & CI length &  95 \\% Cvg & MSPE   \\\\ \n\\midrule\n\t \\multicolumn{10}{c}{Scenario 1: $n= 50$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0.8$ }  \\\\\n\\midrule \nBFPCA & 3.50        & 65.3         & 92.9 *** & 319    &     & 0.174      & 12.8         & 86.9 *** & 23.0          \\\\\nBBSP  & 3.26       & 61.6 **      & 90.7 *** & 313       &  & 0.168      & 12.9         & 87.3 *** & 22.9        \\\\\nUFPCA & 3.20        & 66.5 *       & 93.6     & 350 *    &   & 0.182      & 14.8 *       & 90.8 *** & 24.4 *      \\\\\nUBSP  & 2.86       & 64.9         & 94.0       & 351 *     &  & 0.185      & 14.5 *       & 90.9 *** & 24.3 *      \\\\\n\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 2: $n= 250$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0.8$ }  \\\\\n\\midrule \nBFPCA & 0.795      & 31.9         & 91.4 *** & 284     &    & 0.039     & 6.66         & 90.7 *** & 21.0          \\\\\nBBSP  & 0.798      & 31.2 **      & 91.0 ***   & 285   &      & 0.037     & 6.67         & 91.3 *** & 21.0          \\\\\nUFPCA & 0.790      & 32.8 *       & 91.9 *** & 351 *  &     & 0.040     & 7.27 *       & 93.2     & 24.3 *      \\\\\nUBSP  & 0.794      & 32.4 *       & 92.0 ***   & 350 * &      & 0.043     & 7.25 *       & 91.6 *** & 24.3 *      \\\\\n\n\n\\midrule\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 3: $n= 50$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0$ }  \\\\\n\\midrule \nBFPCA & 2.96       & 65.9         & 94.2     & 408       &  & 0.172      & 13.4         & 89.3 *** & 26.3        \\\\\nBBSP  & 3.16       & 62.6 **      & 92.8 *** & 421 *    &   & 0.183      & 13.3         & 88.4 *** & 26.6 *      \\\\\nUFPCA & 2.75       & 65.8         & 94.6     & 372 **   &   & 0.166      & 14.6 *       & 93.3     & 24.4 **     \\\\\nUBSP  & 2.85       & 63.9 **      & 93.5     & 371 **   &   & 0.162      & 14.5 *       & 92.8 *** & 24.2 **     \\\\\n\n\n\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 4: $n= 250$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0$ }  \\\\\n\\midrule \nBFPCA & 0.802      & 32.5         & 94.6   & 370        & & 0.044     & 6.96         & 91.1 *** & 24.8        \\\\\nBBSP  & 0.791      & 31.9 **      & 94.3   & 374 *      & & 0.042     & 6.97         & 90.7 *** & 24.9        \\\\\nUFPCA & 0.780       & 32.9 *       & 94.7   & 362 **   &   & 0.042     & 7.35 *       & 93.5     & 24.3 **     \\\\\nUBSP  & 0.765      & 32.5         & 94.5   & 361 **     & & 0.040     & 7.35 *       & 94.1     & 24.3 **     \\\\\n\n\n\\midrule\n\\multicolumn{10}{p{\\linewidth}}{{Results in hundredths. A `**' (`*') indicates better (worse) compared to BFPCA by Wilcoxson rank sum test, $\\alpha=0.05$. For coverage, a `***' indicates that the coverage is not within the nominal $95\\%$ range. }}\\\\\n\t\\bottomrule\n\t\\end{tabular}\n}\n\\label{t:sim2}\n\\end{table} \n\n\n\n\n\nFor Scenarios 1 \\& 2 there is no clear difference between fitting predetermined bases (Section \\ref{s:pre}) or data-driven bases (Section \\ref{s:datadriven}); however, BFPCA has better prediction compared to BBSP in Scenarios 3 \\& 4 when there is no cross-correlation. The univariate models have very similar performance to one another in all scenarios.\n\n\n\n\t\n\t\\section{Periodontal Data Application}\\label{s:dental_data}\n\t\n\nWe demonstrate our methods using data from a periodontal study \\citep{Fer+:09} conducted by the Center for Oral Health\nResearch at the Medical University of South Carolina. In addition to collecting subject-level covariates for over 200 Gullah African Americans, several measures of patients' periodontal health were observed at six sites for each of 28 teeth. The two responses we consider are (continuous) clinical attachment loss (CAL) and (binary-valued) bleeding on probing (BOP). CAL is the distance that a tooth has detached from the bone, rounded to the nearest mm. We use the average CAL over the six sites on each tooth as the tooth's CAL response. \nBOP is the binary indicator of whether the gums bleed when pressed with a dental probe at any of the six sites per tooth. A total of ${\\mathrm}{N}=197$ patients (subjects) are included for analysis after excluding those with more than 50\\% missingness. Any remaining missingness is assumed to be completely at random; \\cite{Rei+Ban:10} and \\cite{Rei+:13} provide methods for accounting for non-random missingness.\n\nFor our analysis, we assign teeth the numbers 1-14 going from left to right in the upper jaw when looking at a patient and 15-28 going from right to left in the lower jaw when looking at a patient; wisdom teeth are excluded. Using this numbering system, teeth 1 \\& 28  are adjacent going from upper jaw to lower jaw, and it is the same for teeth 14 \\& 15 on the other side of the mouth. \nWe consider responses at each tooth to be realizations of a functional process with locations $t \\in [1,28]$. In fitting a bivariate functional model to this data, we hope to gain a better understanding of the dynamics between the responses CAL and BOP through close examination of their cross-covariance. Our extremely flexible approach to modeling the covariance will be able to capture any spatial correlation of adjacent teeth, of teeth on different sides of the mouth, and of teeth on different jaws.\n\n\nThe subject-specific covariates  that we include in modeling the mean function are the same covariates used by \\cite{Rei+Ban:10} and include age (in years), gender (female=1, male=0), body mass index or BMI (in ${\\mathrm}{kg}/{\\mathrm}{m}^2$), smoking status (1=smoker, 0=never), and glycosylated hemoglobin or HbA1c (1 = high, 0 = controlled).  All covariates have been standardized to be zero-mean with standard deviation of 1. For each tooth, we include an indicator of jaw (0=upper, 1=lower). For the smooth part of the mean, we consider a quadratic function \n$s_p(t) = \\beta_{p0} + \\beta_{p1}d(t) + \\beta_{p2}d(t)^2 $ of tooth distance $d$ from the front of the mouth, where $d(t)= t - 7.5$ for teeth in the upper jaw and $d(t)= t- 21.5$ for the lower jaw.\n\nWe present analysis for 8 models given in Table \\ref{t:data2} that all employ the data-driven basis method of Section \\ref{s:datadriven}. \nThe 8 models differ by: 1) whether FPCA is univariate or bivariate; 2) the choice of threshold ${\\mathrm}{P}_1=99\\%,95\\%$ for the cumulative percentage of variation explained for FPCA; and 3) whether a random bivariate subject-level intercept ${\\bm{\\alpha}}_{0i}= [\\alpha_{01i}, \\alpha_{02i}]^{\\mathrm}{T}$ is added to model \\eqref{mod1}. Models using B-splines as in Section \\ref{s:pre} were also considered but are not presented because the best-performing models required a large number of basis functions. \n\n\n\n\n\\begin{table}\n\\normalsize\n\n\\caption{Model comparisons for the periodontal data application}\n\\centering\n\\resizebox{\\linewidth}{!}\n{\n\t\\begin{tabular}{rrrrrrrr }\nModel & Subject RE &   PVE & PCA &Dbar & pD & DIC \\\\\n\\midrule \n1 & Y & 99 & B    & 8114 & 1257 & 9371 \\\\\n2 & Y & 99 & U    & 8228 & 1231 & 9459 \\\\\n3 & Y & 95 & B    & 8849 & 1001  & 9850  \\\\\n4 & Y & 95 & U    & 8536 & 1114 & 9650 \\\\\n5 & N & 99 & B    & 10101 & 942 & 11043 \\\\\n6 & N & 99 & U    & 10586 & 832 & 11418 \\\\\n7 &  N& 95 & B    & 10511 & 788 & 11299 \\\\\n8 & N & 95 & U    & 10722 & 760 & 11482 \\\\ \n\n\\midrule\n\\multicolumn{7}{p{\\linewidth}}{\\small ``Subject RE\" indicates inclusion of a subject-specific random intercept. ``PVE\" is the threshold for cumulative percentage of variation explained. ``PCA\" indicates whether univariate (``U\") or bivariate (``B\") FPCA was performed. }\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{t:data2}\n\\end{table} \n\n\n\n\nFor the purpose of estimating the latent covariance, we ignore the covariates.   When incorporating a bivariate random subject-level intercept, we use residuals $R_{1i}(t)= Y_{1i}(t) - {\\mathrm}{L}_{1i}^{-1}\\sum_{i=1}^{{\\mathrm}{L}_{1i}} Y_{1i}(t)$ of the continuous response CAL to estimate the latent covariance for FPCA; this is not done for the binary responses as the residuals would no longer be binary. For models that include ${\\bm{\\alpha}}_{0i}$, we estimate the covariance term ${\\mathrm}{Cov}(\\alpha_{01i}, \\alpha_{02i})$ in addition to the variance terms ${\\mathrm}{Var}(\\alpha_{0pi})$. We specify a diagonal covariance matrix ${\\bm}{\\Sigma}$ for the remaining random effect parameters.  \n\n\n\n\n\nTable \\ref{t:data2} shows that Models 1-4, which include a subject random effect, outperform (based on DIC) Models 5-8 which omit the subject random effect. For this data, specifying the larger percentage of variation explained for FPCA, and hence including more basis functions, leads to better model performance. In comparing the two leading models 1 \\& 2, implementing FPCA on the full bivariate covariance matrix as in Model 1, taking into account the cross-dependence between the two responses CAL and BOP, leads to superior performance.\nFigure \\ref{f:covariates} shows the subject-level coefficient estimates and 95\\% posterior intervals for Model 1. Models 2-4 had similar coefficient estimates.  For CAL, only the coefficient interval for BMI includes zero. The other coefficient estimates show an increased level of CAL for older patients, males, smokers, patients with high HbA1c counts, and for teeth on the upper jaw. For BOP, the posterior confidence intervals are larger than those for CAL. For intervals that exclude zero, there is an increase of BOP for the upper jaw, yet a slightly lower incidence of BOP for higher BMI. \n\n\n\n\\begin{figure}\n\\centering\n\n\n\\includegraphics[width=\\linewidth]{bxplt_1}\n\\caption{Posterior medians and 95\\% posterior intervals of the subject-specific covariate coefficients by response.}\n\\label{f:covariates}\n\\end{figure}\n\n\n\n\n\n \n\n\n\nFigure \\ref{f:fitted} shows the fitted values (from Model 1) for two individuals in the periodontal data set. The left panels show the posterior means and 95\\% posterior intervals of the subject-specific mean function $\\mu_{1i}(t)$ for the continuous response CAL. Most of the observed CAL values fall within the 95\\% interval for both subjects, indicating a reasonable model fit. The right panels show the posterior mean and 95\\% posterior intervals of the conditional probability of the event, $P(Y_{2i}(t)=1|{\\bm{\\alpha}}_{2i})$. Teeth with observed BOP ($=1$) are indicated by the squares on the bottom of the plot. The higher predicted probabilities tend to correspond to the incidence of BOP, again indicating a reasonable model fit. \n\n\n\n\n\\begin{figure}\n\\centering\n\n\\includegraphics[width=\\linewidth]{plot_subj1}\n\\includegraphics[width=\\linewidth]{plot_subj2}\n\\caption[Fitted values for two individuals from the periodontal study.]{Fitted values for two individuals from the periodontal study (using Model 1). \\emph{Left panels}: Observed values of CAL are shown as dots. The solid black line indicates the posterior mean of $\\mu_{1i}(t)$, the subject-specific mean function, and point-wise 95\\% posterior intervals are given by the dotted lines. \\emph{Right panels}: The squares along the x-axis indicate the teeth for which BOP is observed. The solid black line gives the posterior mean of the conditional probability of the event, $P(Y_{2i}(t)=1|{\\bm{\\alpha}}_{2i})$, and dotted lines show point-wise 95\\% posterior intervals. The label ``UPPER LEFT\" refers to the left side of the the upper jaw when looking at a patient, and it is analogous for the other labels.}\n\\label{f:fitted}\n\\end{figure}\n\n\n\n\nThe posterior summaries of the auto- and cross-correlations of the subject-specific process ${\\bm}{f}_i(t)$ from \\eqref{mod1} are given in Figure \\ref{f:post_corr}; note that the correlation attributed to the subject random intercept is not included in this figure. In this periodontal application, these plots offer important and novel insights into the complex relationships that exist between and within the BOP and CAL responses in different parts of the mouth. The utility of quantifying and visualizing these complex correlation relationships is apparent for many other types of applications. \n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\n\n\\includegraphics[width=\\linewidth]{corr_wori}\n\\includegraphics[width=\\linewidth]{sd_corr_wori}\n\\includegraphics[width=\\linewidth]{probgt0_corr_wori}\n\\caption{Posterior summaries of the within-response and between-response correlation structures for any two teeth when fit with Model 1 (excluding correlation from the subject random intercepts). The label ``UPPER LEFT\" refers to the left side of the the upper jaw when looking at a patient, and it is analogous for the other labels.}\n\\label{f:post_corr}\n\\end{figure}\n\n\n\nExamination of the diagonal of the auto-correlation plot for CAL in Figure \\ref{f:post_corr} shows strong positive spatial correlation between adjacent teeth and between teeth separated by only one or two teeth on the same jaw.  This plot also shows positive correlation between a tooth in the left and a tooth in the right side of the same jaw, and the relationship is particularly strong for teeth in the lower jaw. The correlation for CAL between teeth in opposite sides of the mouth and on different jaws is also positive, yet not as strong as for teeth on the same jaw; this correlation is very similar in magnitude as the correlation for teeth on the left or right side of the mouth but on different jaws. Additionally, there are mild to strong negative correlations between teeth in the center (front) of the mouth and teeth in the back of the mouth, regardless of the jaws on which the teeth are located. This is also seen in the plot of the posterior probability that the auto-correlation is positive.\n\nIn the auto-correlation plot for BOP, again we see strong positive spatial correlation between adjacent teeth and between teeth that are close to one another on the same jaw. Additionally, the plots of the auto-correlation and of the probability of being positive show that the correlation is mostly positive with only a few areas of negative correlation. The correlation is negative between a tooth in the center and a tooth on the right side of the upper jaw, as well as between a tooth in the left and a tooth in the center of the lower jaw. There is also a strong negative correlation for teeth in the lower right and upper right, as well as for teeth in the lower left and lower right. \n\nThe cross-correlation between BOP and CAL ranges from moderately positive to moderately negative. Unlike the auto correlation plots, the cross correlation is not symmetric, which makes interpretation slightly more complex. For instance, BOP in the lower left is positively correlated with CAL in the center and upper right as indicated by the darkest patch near the top center of the cross correlation figure. Alternatively, CAL in the lower left shows slightly negative to no correlation with BOP in the center and upper right of the mouth.  \nAnother demonstration of this non-symmetric property occurs for the negative correlation of BOP in the lower left with CAL in the lower right, though BOP in the lower right shows slightly positive to no correlation with CAL in the lower left. \n\n\n\n\\section{Discussion}\n\n\nWe introduce a methodology to jointly model multivariate functional responses of mixed type (e.g. continuous and binary data) and also propose an extension of FPCA for mixed-responses. \nOur method can account for subject-specific covariates that can be either linear or time-dependent (such as the jaw indicator used in the analysis of the periodontal study in Section \\ref{s:dental_data}). \nThe proposed method is flexible enough for functions to be observed at varying locations for different subjects and different responses.\nFor exposition we focus on modeling a bivariate response vector where one functional response is continuous and the other is binary, though joint modeling of more than two responses is a straightforward extension. Furthermore, the method easily models repeatedly observed categorical responses. This is achieved in a manner similar to thresholding the latent process at zero for binary data, but instead one must impose multiple thresholds on the latent process. Modeling other types of data, such as repeatedly observed count data, is not as straightforward as it would likely require using copulas\n\nBy estimating the multivariate covariance of the latent process, our methodology can offer novel insights into the cross-dependence of different responses, which is of interest in a wide\nvariety of applications. Quantifying and exploring this dependence is an important contribution of our method and is a primary goal of our analysis of the periodontal data presented in Section \\ref{s:dental_data}.  \\cite{Rei+Ban:10} and \\cite{Rei+:13} offer ways to incorporate informative missingness and apply their methods to the same periodontal data. We do not address the informative missingness for our analysis because it is not central to our goals, and leave it for future work.\n\n\n\n\n\n\\section*{Acknowledgments}\nThe authors thank Dipankar Bandypadhyay of the University of Minnesota and Drs. S. London, J. Fernandes, C. Salinas, W. Zhao, Ms. L. Summerlin and Ms. P. Hudson. W of the Center for Oral Health Research (COHR) at the Medical University of South Carolina for providing the data and context for this work.  The work of Beth A. Tidemann-Miller was supported by NIH grant R01 CA085848. \n\n\n\n\\bibliographystyle{rss}\n\\bibliography{researchbib2}\n\n\n\\section*{Appendix}\n\n\\section*{Appendix A}\n\nIn this section, we show that we can approximate any smooth covariance using the predetermined basis method.  \nFor simplicity, assume that the functional responses are observed at the same locations $t_{p\\ell} \\equiv t_\\ell$ for $\\ell = 1, \\hdots, {\\mathrm}{L}$ for each response $p$. We specify this model for an arbitrary subject, and thus drop the subscript $i$. Let ${\\bm{\\psi}}_{pk}$ be the vector  of length ${\\mathrm}{L}$ formed by evaluating at every $t_{\\ell}$ the basis functions $\\psi_{pk}(t)$, $k=1,\\hdots,{\\mathrm}{M}_p$, and define the vector ${\\bm}{f}_{p}$ analogously. Then we form the ${\\mathrm}{L} \\times {\\mathrm}{M}_p$ matrix ${\\bm{\\Psi}}_{p}=[{\\bm{\\psi}}_{pk}, \\hdots, {\\bm{\\psi}}_{p {\\mathrm}{M}_p}]$ and the coefficient vector ${\\bm{\\alpha}}_{pi} = [\\alpha_{p1},\\hdots,\\alpha_{p{{\\mathrm}{M}_p}} ]^{\\mathrm}{T}$ so that we can write ${\\bm}{f}_{p}={\\bm{\\Psi}}_{p} {\\bm{\\alpha}}_{p}$. We combine ${\\bm}{f}_{p}$ for all responses to form one vector ${\\bm}{f}$ of length ${\\mathrm}{n}= {\\mathrm}{P} {\\mathrm}{L}$, and define the coefficient vector ${\\bm{\\alpha}}^{\\mathrm}{T} = [{\\bm{\\alpha}}_{1}^{\\mathrm}{T},\\hdots, {\\bm{\\alpha}}_{{\\mathrm}{P}}^{\\mathrm}{T}]$ of length ${\\mathrm}{m}=\\sum_{p=1}^{\\mathrm}{P} {\\mathrm}{M}_p$ and corresponding block-diagonal matrix ${\\bm{\\Psi}}$ of dimension ${\\mathrm}{n} \\times {\\mathrm}{m}$ with blocks ${\\bm{\\Psi}}_{p}$. The resulting vector  ${\\bm}{f}= {\\bm{\\Psi}} {\\bm{\\alpha}}$ has length ${\\mathrm}{n}$, and we assume ${\\bm{\\alpha}} \\overset{iid}{\\sim}N(0,{\\bm}{\\Sigma})$ where ${\\bm}{\\Sigma}$ is a covariance matrix of dimension ${\\mathrm}{m} \\times {\\mathrm}{m}$ with elements ${\\mathrm}{Cov}(\\alpha_{pk},\\alpha_{p{^\\prime}  \\ell })= \\xi_{k \\ell p p{^\\prime}}$.\n\nTo illustrate the flexibility of the model, assume ${\\bm}{\\Omega}_0$ is the true ${\\mathrm}{n} \\times {\\mathrm}{n}$ covariance matrix of ${\\bm}{f}$ evaluated at locations $t_{l}$. ${\\bm}{\\Omega}_0$ is now approximated by the variance-covariance matrix \n${\\bm}{\\Omega}={\\mathrm}{Cov}({\\bm{\\Psi}} {\\bm{\\alpha}}) = {\\bm{\\Psi}} {\\bm}{\\Sigma} {\\bm{\\Psi}}^{\\mathrm}{T}$. Since the basis comprising ${\\bm{\\Psi}}$ is pre-specified, the quality of the approximation ${\\bm}{\\Omega}\\approx {\\bm}{\\Omega}_0$ is reliant on ${\\bm}{\\Sigma}$. By fitting a large number of basis functions, i.e. setting ${\\mathrm}{m}={\\mathrm}{n}$, it is possible to fit any smooth covariance function. When ${\\mathrm}{m}={\\mathrm}{n}$ then ${\\bm{\\Psi}}_i$ is a square matrix. Assume ${\\bm{\\Psi}}_i$ is full rank and thus ${\\bm{\\Psi}}_i^{\\mathrm}{T}{\\bm{\\Psi}}_i $ is invertible. Pre- and post- multiplication gives ${\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{\\Omega} {\\bm{\\Psi}}_i = {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm{\\Psi}}_i {\\bm}{\\Sigma} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm{\\Psi}}_i$. Since ${\\bm}{\\Theta}=\\{{\\bm{\\Psi}}_i^{\\mathrm}{T}{\\bm{\\Psi}}_i\\}^{-1}$ exists we can recover ${\\bm}{\\Sigma} = {\\bm}{\\Theta}{\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{\\Omega} {\\bm{\\Psi}}_i {\\bm}{\\Theta}$. Though this approach is quite flexible, it is \nhard to estimate ${\\bm}{\\Sigma}$ if it is high-dimension; therefore it is unlikely to perform well if the processes cannot be represented by a small number of basis functions.\n\n\n\n\n\\section*{Appendix B}\n\nHere we describe in more detail the derivation of the latent cross covariance estimator. As our approach is inspired by \\cite{Hal+:08}, we start with a brief summary of the method they proposed for finding the auto-covariance of the latent process corresponding to the binary response, that is, response $p=2$. First, estimate the mean function for $p=2$, $\\hat{\\mu}_2(t)=g^{-1}\\{ \\hat{\\eta}_2(t) \\}$ where $\\hat{\\eta}_2(t)$ estimates ${\\mathrm{E}}[{ g\\{Z_{{2} i}(t)  \\}} ]={\\eta}_2(t)$ and is found by smoothing the data $\\big(t,Y_{2 i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. Next, find the estimator $\\hat{S}_{22}(t,t{^\\prime})$ of $S_{22}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{2 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[  g\\{Z_{2i}(t)\\}  g\\{Z_{2i}(t{^\\prime})\\} \\big]$ by performing bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{2 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, once again removing the diagonals before smoothing. The estimator of the latent process covariance operator for the second response is given by \n\n", "itemtype": "equation", "pos": 20470, "prevtext": "\n\n\\sloppy\nCombining the individually smoothed estimators $\\widetilde{K}_{11}(t,t{^\\prime})$, $\\widetilde{K}_{22}(t,t{^\\prime})$ and $\\widetilde{K}_{12}(t,t{^\\prime})= \\widetilde{K}_{21}(t{^\\prime},t)$ forms the smooth $2\\times 2$ estimator $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$ of the bivariate latent covariance operator. Note that for smoothing purposes in this paper, we implement a global smoother as opposed to the local least squares smoothing of \\cite{Hal+:08}, though either is appropriate. In the presence of subject-specific covariates, one can find covariate estimates using least squares or logistic regression, depending on the type of response, and then use the residuals to estimate the latent covariance.\n\nThe final step for creating basis functions is to implement bivariate FPCA in which we obtain the eigenfunctions ${{\\boldsymbol}{\\theta}}(t)=[{\\theta}_1(t), \\hdots, {\\theta}_{{\\mathrm}{P}}(t)]^{\\mathrm}{T}$ and the eigenvalues ${\\lambda}$ of the matrix $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$.  Note that the matrix $\\widetilde{{\\bm}{K}}(t,t{^\\prime})$ is not guaranteed to be positive definite, but we can ensure the truncated spectral decomposition $\\widetilde{{\\bm}{K}}(t,t{^\\prime})= \\sum^{{\\mathrm}{M}}_{k=1} {\\lambda}_{k}{{\\boldsymbol}{\\theta}}_{k}(t) \\{{{\\boldsymbol}{\\theta}}_{k}(t{^\\prime})\\}^{\\mathrm}{T}$ is positive definite by restricting the inclusion of only positive eigenvalues and their eigenfunction counterparts.\nThe truncation value ${\\mathrm}{M}$  is chosen based on the proportion of variation explained by the eigenvalues as suggested in \\cite{Di+:09}.\nIn particular,  specify a cumulative explained variance threshold ${\\mathrm}{P}_1$ and individual explained variance threshold ${\\mathrm}{P}_2$. Define\n${\\mathrm}{M} = \\min \\{k: p_{1k} \\ge {\\mathrm}{P}_1, p_{2k} < {\\mathrm}{P}_2 \\} $ where \n$p_{1k}=\\sum_{i=1}^{k}{{{\\lambda}_{i}}/\t\\sum_{j=1}^{n}{{ \\lambda}_{j}}}$, $p_{2k}={\\lambda}_k/\t\\sum_{j=1}^{n}{{ \\lambda}_{j}}$ and the positive eigenvalues are the first $n\\ge k$ eigenvalues. We specify the basis functions to be the eigenfunctions scaled by their associated eigenvalues, ${\\psi}_{pk}(t) = \\sqrt{{\\lambda}_k} {{\\theta}}_{pk}(t)$, and the subject-specific deviation function is approximated by \n$f_{pi}(t)= \\sum_{k=1}^{\\mathrm}{M}  {\\psi}_{pk}(t) \\alpha_{ik}$. \n\nUsing this data-driven basis approach, the correlation across responses is largely captured by the basis functions from FPCA. Additionally, since each basis function combines information from all responses, the data-driven approach results in one set of basis functions, eliminating the need to have a set of basis functions for each response. These distinctions offer important advantages over the predetermined basis approach. First, having only one set of basis functions reduces the dimensionality of the random-effect covariance matrix ${\\bm}{\\Sigma}$, making it easier to fit. Second, it allows for further simplification since ${\\bm}{\\Sigma}$ is diagonal. This will offer computational advantages over the predetermined basis method where the burden of capturing the correlation across responses falls entirely on estimating a non-diagonal ${\\bm}{\\Sigma}$ which can potentially have very large dimension. \n\nOne important consideration to make when implementing this data-driven basis function approach is to ensure that the variance of the latent process for the continuous component is on a scale similar to that of the latent process for the binary component. We suggest scaling the continuous process by $Y_{1 i}(t)/s$ where $s$ is the overall sample standard deviation of the continuous response without regard to $t$. Since $s$ is a scalar quantity, it is straightforward to scale prior to implementing the latent covariance, FPCA and MCMC estimation algorithms, rescaling only the final results back to the original scale. \n\n\\subsection{Prior Specification}\\label{s:priors}\n\n\nTo complete the Bayesian model, we specify priors for the hyperparameters.\nThe fixed effect parameters ${{\\bm{\\beta}}}$ are assigned uninformative Gaussian priors. \nLet the subject random effect ${\\bm{\\alpha}}_i$ have a Gaussian prior with ${\\mathrm}{Cov}({\\bm{\\alpha}}_i)={\\bm}{\\Sigma}$ and assign ${\\bm}{\\Sigma}$ an Inverse Wishart prior. For the error variances of the continuous processes, let $\\tau^2_p$ have an uninformative gamma prior;  for identifiability $\\tau^2_p$ is fixed at 1 for binary processes. In summary,\n\t\n", "index": 15, "text": "\\begin{align}\n\t\\label{eqn:priors}\n\t\t&{\\bm{\\beta}} | \\sigma^2_\\beta \\sim {\\mathrm}{N}_{\\mathrm}{J}({\\bm}{0},  \\sigma^2_\\beta{\\textbf{\\text{I}}}_{\\mathrm}{J}) \\nonumber\\\\\n\t\t&{\\bm{\\alpha}}_i | {\\bm}{\\Sigma} \\sim {\\mathrm}{N}_{\\mathrm}{M}({\\bm}{0}, {\\bm}{\\Sigma}) \\nonumber \\\\\n\t\t&{\\bm}{\\Sigma} | q_1, q_2 \\sim \\text{InvWishart}_{\\mathrm}{M}( {\\bm{V}}=q_2{\\textbf{\\text{I}}}_{{\\mathrm}{M}}, \\nu=q_1)\\\\\n\t\t& \\tau^2_p | l,h \\sim {\\mathrm}{InvGamma}(l,h) \\nonumber\n\t\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{\\beta}}|\\sigma^{2}_{\\beta}\\sim{\\mathrm{}}{N}_{\\mathrm{}}{J}(%&#10;{\\bm{}}{0},\\sigma^{2}_{\\beta}{\\textbf{\\text{I}}}_{\\mathrm{}}{J})\" display=\"inline\"><mrow><mi>\ud835\udf37</mi><mo stretchy=\"false\">|</mo><msubsup><mi>\u03c3</mi><mi>\u03b2</mi><mn>2</mn></msubsup><mo>\u223c</mo><msub><mi>N</mi><mi/></msub><mi>J</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>\u03c3</mi><mi>\u03b2</mi><mn>2</mn></msubsup><msub><mtext>\ud835\udc08</mtext><mi/></msub><mi>J</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{\\alpha}}_{i}|{\\bm{}}{\\Sigma}\\sim{\\mathrm{}}{N}_{\\mathrm{}}{M%&#10;}({\\bm{}}{0},{\\bm{}}{\\Sigma})\" display=\"inline\"><mrow><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u223c</mo><msub><mi>N</mi><mi/></msub><mi>M</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{}}{\\Sigma}|q_{1},q_{2}\\sim\\text{InvWishart}_{\\mathrm{}}{M}({%&#10;\\bm{V}}=q_{2}{\\textbf{\\text{I}}}_{{\\mathrm{}}{M}},\\nu=q_{1})\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">|</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><msub><mi>q</mi><mn>2</mn></msub><mo>\u223c</mo><msub><mtext>InvWishart</mtext><mi/></msub><mi>M</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc7d</mi><mo>=</mo><msub><mi>q</mi><mn>2</mn></msub><msub><mtext>\ud835\udc08</mtext><mi>M</mi></msub><mo>,</mo><mi>\u03bd</mi><mo>=</mo><msub><mi>q</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tau^{2}_{p}|l,h\\sim{\\mathrm{}}{InvGamma}(l,h)\" display=\"inline\"><mrow><msubsup><mi>\u03c4</mi><mi>p</mi><mn>2</mn></msubsup><mo stretchy=\"false\">|</mo><mi>l</mi><mo>,</mo><mi>h</mi><mo>\u223c</mo><mi>I</mi><mi>n</mi><mi>v</mi><mi>G</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\nEquation \\eqref{Hall_univ} was developed for a univariate response, so in order to estimate the latent cross covariance operator $K_{12}(t,t{^\\prime}) = K_{21}(t{^\\prime},t)= {\\mathrm}{Cov}\\{Z_{1 i}(t), Z_{2 i}(t{^\\prime})\\}$, we must derive an analogous estimator. This requires the following Taylor expansion, also given by equation (5) in \\cite{Hal+:08},\n\n", "itemtype": "equation", "pos": 54068, "prevtext": "\nfor hyperparameters $\\sigma^2_b$, $q_1$, $q_2$, $\\ell$, and $h$, selected to result in weak priors.\n\n \n\n\n\n\\section{Computational Details}\\label{s:compdetails}\n\nTo facilitate MCMC sampling, we treat the continuous latent processes $W_{pi}(t)$ for binary response as unknown parameters to be updated as part of the sampling as in \\cite{Alb+Chi:93}. Using this auxiliary variable approach, all parameters have conditional conjugacy due to the prior specifications given in Section \\ref{s:priors}, allowing us to implement Gibbs sampling. The Gibbs sampling algorithm uses full-conditional posteriors derived in the Appendix and which use notation that we now describe.\n\nDenote the observation locations as $t_{pi\\ell}$, $\\ell = 1, \\hdots, {\\mathrm}{L}_{pi}$, for each subject $i$ and response $p$, giving a total of ${\\mathrm}{L}_i =  \\sum_{p=1}^{\\mathrm}{P} {\\mathrm}{L}_{pi}$ locations. Let $n=\\sum_{i=1}^{\\mathrm}{N}{{\\mathrm}{L}_i}$ be the total number of locations observed across all subjects. \nLet ${\\bm}{W}_{pi}$ be the vector of length ${\\mathrm}{L}_{pi}$ formed by evaluating $W_{pi}(t)$ at every $t_{pi\\ell}$. Furthermore, combine ${\\bm}{W}_{pi}$ for all responses to form one vector ${\\bm}{W}_{i}$ of length ${\\mathrm}{L}_i$; $ {\\bm}{U}_i$ and ${\\bm{\\Psi}}_i$ are defined analogously. Then ${\\bm}{W}_i$ has mean  ${\\mathrm{E}}({\\bm}{W}_i | {\\bm{\\alpha}}_i)= {\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i$ and precision matrix ${\\bm}{P}_i$ is comprised of the appropriate error variance parameter $\\tau_p^{-2}$.\n\n MCMC begins by setting initial values for all parameters and then sequentially sampling each parameter conditioned on all the others (denoted by `$` |\\cdot$\"). Sampling is performed (using the latest sample to update each parameter) according to the full conditionals in the following manner:\n\\begin{enumerate}[1.]\n\t\\item Select initial values for ${\\bm{\\beta}}$ , ${\\bm{\\alpha}}_i$, ${\\bm}{\\Sigma}$, $W_{pi}(t)$ for binary responses, and $\\tau^2_p$ for continuous responses;\n\t\\item  For each $i=1,\\hdots,{\\mathrm}{N}$ and $\\ell=1,\\hdots,{\\mathrm}{L}_{pi}$, update the latent response corresponding to the observed binary response by drawing from  $W_{p i}(t_{i\\ell})|\\cdot \\sim  {\\mathrm}{N}({\\bm{u}}^{\\mathrm}{T}_{pi}(t_{i\\ell}) {\\bm{\\beta}} + {\\bm{\\psi}}^{\\mathrm}{T}_p(t_{i\\ell}) {\\bm{\\alpha}}_i, 1)$ restricted to the interval $(0,\\infty)$ if $Y_{pi}(t_{i\\ell})=1$ or $(-\\infty,0)$ if $Y_{p i}(t_{i\\ell})=0$;\n\t\n\t\\item Update the population mean parameter by drawing from \n\t${\\bm{\\beta}} | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\beta}}}, {\\bm{V}}_{{\\bm{\\beta}}} )$ for\n\t  ${\\bm{V}}_{{\\bm{\\beta}}}= \\left[ \\left(\\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i \\right)+  \\sigma^{-2}_{{\\bm{\\beta}}}{\\textbf{\\text{I}}}_{\\mathrm}{J}  \\right]^{-1}$ and \n\t  ${\\bm}{\\mu}_{{\\bm{\\beta}}}  =  {\\bm{V}}_{{\\bm{\\beta}}} \\left[ \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  ({\\bm}{W}_i - {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i )\\right]$;\t\n\t\n\t\\item For each  $i=1,\\hdots,{\\mathrm}{N}$, update the random effect by sampling from ${\\bm{\\alpha}}_i | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\alpha}}}, {\\bm{V}}_{{\\bm{\\alpha}}} )$ for \n\t${\\bm{V}}_{{\\bm{\\alpha}}}= \\left( {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{\\Sigma}^{-1} \\right) ^{-1}$ and\n ${\\bm}{\\mu}_{{\\bm{\\alpha}}} = {\\bm{V}}_{{\\bm{\\alpha}}} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i ({\\bm}{W}_i-{\\bm}{U}_i {\\bm{\\beta}})$;\n\n\t\\item Update the random effect covariance matrix through ${\\bm}{\\Sigma}| \\cdot \\sim {\\mathrm}{InvWishart}_{{\\mathrm}{M}}[ \\{ \\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T}}  +(1/q_2){\\textbf{\\text{I}}}_{{\\mathrm}{M}}\\}^{-1}, {\\mathrm}{N}+ q_1]$;\n\t\n\t\\item Update the error variance for the continuous responses according to \n\t$\\tau^2_p | \\cdot \\sim {\\mathrm}{InvGamma}(l_\\omega, h_\\omega)$ with $l_\\omega =n/2+l$ and $h_\\omega= h + 1/2 \\sum_{i=1}^{\\mathrm}{N} \\sum_{\\ell=1}^{{{\\mathrm}{L}_i}} [ W_{p i}(t_{i\\ell}) -  {\\bm{u}}^{\\mathrm}{T}_{pi}(t_{i\\ell}) {\\bm{\\beta}} + {\\bm{\\psi}}^{\\mathrm}{T}_p(t_{i\\ell}) {\\bm{\\alpha}}_i ]^2$.\n\n\\end{enumerate}\nSteps 2-6 are repeated for the desired number of samples.\n\n\n\n\\section{Simulations}\\label{s:sim}\n\n\nFor our simulation study, we compare mean estimation and prediction performance among four estimating models when the generating model has a continuous and binary response, either generated separately (univariate) or jointly (bivariate) with strong cross-correlation, for both small and large sample size. The four estimating models are either univariate models applied separately to each response or bivariate models, and either employ the pre-specified basis function method of Section \\ref{s:pre} or the data-driven approach of Section \\ref{s:datadriven}. \n\n\n\t\n\t\\subsection{Data generation}\\label{s:sim_data}\n\t\n We consider the case where $Y_{1i}(t)$ is continuous and $Y_{2i}(t)$ is binary. Functions are observed at a dense, balanced design with ${\\mathrm}{L}_{pi} \\equiv 30$ equally-spaced locations in $[0,1]$ for each subject $i$ and response $p$. We use the model given in \\eqref{mod1} with predetermined bases as in Section \\ref{s:pre} for data generation. We specify a separable random effect covariance matrix ${\\bm}{\\Sigma}= {\\bm}{A} \\otimes {\\bm{C}}$, where ${\\mathrm}{Cov}([\\alpha_{1 ik}, \\alpha_{2 ik}]^{\\mathrm}{T})={\\bm}{A}$ for ${\\bm}{A}_{11}={\\bm}{A}_{22}=1$ and  ${\\bm}{A}_{12}={\\bm}{A}_{21}={\\rho_{{\\bm{\\alpha}}}}$ so that the parameter ${\\rho_{{\\bm{\\alpha}}}}$ controls the correlation between the latent responses, and ${\\mathrm}{Cov}( [ \\alpha_{p i1},\\hdots, \\alpha_{p i{\\mathrm}{M}}]^{\\mathrm}{T})={\\bm{C}}$ for $p = 1, 2 $ controls the covariance of the random effect basis function coefficients and is the same across responses.  The ${\\bm{C}}$ used for data generation has the AR(1) structure with variance 1 and correlation parameter $\\rho=1/2$. \n\nFor the fixed population mean function we assume there are no subject-level covariates so that $\\bm{\\mu}(t)={\\text{\\textbf{B}}}(t){\\bm{\\beta}}$, and we specify a quadratic basis $\\{ {\\mathrm}{B}_{p j}(t)=t^{(j-1)}: 1 \\le j \\le 3 \\}$ for each response $p$ with coefficients ${\\bm{\\beta}}_1 = [-0.64 , 4 , -4 ]^{\\mathrm}{T}$ and ${\\bm{\\beta}}_2=[0.97 ,   -6,    6]^{\\mathrm}{T}$. The intercepts are chosen such that the curves are positive for approximately half of the observed locations $t$. The basis functions for the subject-specific deviation function ${\\bm}{f}_i(t)= {\\bm{\\Psi}}(t) {\\bm{\\alpha}}_i$ are given by\n$\\psi_{1 k}(t)= \\sin\\{(2\\pi k/ {\\mathrm}{M})(t+2\\pi k/{\\mathrm}{M})\\}$ and $\\psi_{2 k}(t)= \\cos\\{(2\\pi k/ {\\mathrm}{M})(t+2\\pi k/{\\mathrm}{M})\\}$ for $k=1,\\hdots,{\\mathrm}{M}=7$. The error variance for the continuous process is $\\tau^2_1=1$.\nWe generate data from four scenarios given in Table \\ref{t:sim2} by varying the sample size (${\\mathrm}{N}=50, 250$) and the cross-correlation (${\\rho_{{\\bm{\\alpha}}}}=0, 0.8$).  \nAll scenarios use $100$ Monte Carlo (MC) replications. \n\n\n\t\n\t\\subsection{Models and metrics for comparison}\\label{s:sim_metrics}\n\t\n\tWe fit four models to each dataset. \n\t\\begin{enumerate}[I.]\n\t\t\\item Bivariate B-spline ({BBSP}): the multivariate model in \\eqref{mod1} with B-spline bases as in Section \\ref{s:pre};\n\t\t\\item Univariate B-spline ({UBSP}): the model from \\eqref{latent1} applied separately to each response with B-spline bases as in Section \\ref{s:pre};\n\t\t\\item Bivariate FPCA ({BFPCA}): the multivariate model in \\eqref{mod1} with data-driven bases as in Section \\ref{s:datadriven};\n\t\t\\item Univariate FPCA ({UFPCA}): the model in \\eqref{latent1}  applied separately to each response with data-driven bases as in Section \\ref{s:datadriven};\n\t\n\t\\end{enumerate}\n\tFor estimation using the B-spline methods, we choose B-splines of order 4 and the number of B-spline breaks for each replication is fixed at $6$ based on preliminary analyses.  For the FPCA methods, we specify an unstructured ${\\bm}{\\Sigma}$, and the number of basis functions is chosen to explain at least ${\\mathrm}{P}_1=99\\%$ of the cumulative variation. In practice, both the number of basis functions for the B-spline method and the percentage of variation explained for the FPCA method are tuning parameters and one should compare results over a grid parameter values. For the population mean we fit the true polynomial basis ${\\text{\\textbf{B}}}(t)$ for estimation. We perform MCMC sampling with 20,000 draws and the first 5,000 are discarded as burn-in. The hyperparameters are specified as $\\sigma^2_b=100$ and $q_1=q_2=l=h=0.1$.\n\n           \nMethods are compared in terms of their predictive performance and ability to estimate the marginal mean function for each response. Let $\\omega_{1i}(t) = {\\mathrm{E}}\\{Y_{1 i}(t)\\} = {\\bm{u}}^{\\mathrm}{T}_{1i}(t) {\\bm{\\beta}}_1$ and $\\omega_{2i}(t) = {\\mathrm{E}}\\{Y_{2 i}(t)\\} =   \\Phi \\{ \\gamma_i(t) \\}$, where $\\gamma_i(t) = {\\bm{u}}^{\\mathrm}{T}_{2i}(t)  {\\bm{\\beta}}_2/\\sqrt{ v_2(t) }$ is the population effect shrunk toward zero by the square root of the marginal variance \n$v_2(t) ={\\mathrm}{Var}\\{Y_2(t)\\} = {\\bm{\\psi}}_2(t) {\\bm}{\\Sigma}_{22} \\{{\\bm{\\psi}}_2(t)\\}^{\\mathrm}{T} + 1$.  \nLet $\\widehat{\\omega}_{p r}(t)$ and $\\hat{\\nu}_{pr}(t)$ be the posterior mean and variance, respectively, for MC replication $r=1,\\hdots,100$. \n Metrics for comparison of estimated means found in Table \\ref{t:sim2} for each response are mean integrated squared error: ${\\mathrm}{MISE} = \\int_t  {\\mathrm{E}} \\{ \\widehat{\\omega}_{p}(t) - {\\omega}_{p}(t)   \\}^2 dt$; coverage of 95\\% pointwise confidence intervals $\\widehat{\\omega}_{pr}(t) \\pm l_{pr}(t)$ averaged over location $t$ and MC replication $r$ with margin of error $l_{pr}(t)=1.96\\sqrt{\\hat{\\nu}_{pr}(t)}$; and confidence interval length $2 l_{pr}(t)$. \n \nFor prediction, we generate additional data $Y_{prj}(t_l)$ at equally spaced locations $t_\\ell \\in [0,1]$ where $\\ell=1,\\hdots,30$ for subjects $j=1,\\hdots,20$ per response $p=1,2$  for each MC replication $r=1,\\hdots,100$. To assess the value of jointly modeling the two responses, we leave out all of response 1 for 10 subjects and all of response 2 for the remaining 10 subjects per replication. \n\nModels are compared in terms of their predictive performance using mean squared prediction error (MSPE) for each response, defined as  ${\\mathrm}{MSPE} =  (nm{\\mathrm}{L})^{-1} \\sum_{r=1}^{n} \\sum_{j=1}^{m} \\sum_{\\ell=1}^{{\\mathrm}{L}}\\{ Y_{prj}(t_\\ell) - \\hat{Y}_{prj}(t_\\ell) \\}^2$. For binary responses this is known as the Brier score and $\\hat{Y}_{prj}(t_\\ell)$ is the posterior probability that $Y=1$.\n\n\n\t\n\t\\subsection{Results}\\label{s:sim_data}\n\t\n\t\n\t\n\t\nTable \\ref{t:sim2} gives the simulation results. There appears to be little difference in mean function estimation between univariate and bivariate methods for all scenarios. When strong correlation is present (Scenarios 1 \\& 2), the bivariate methods show marked improvement in prediction for both responses over the univariate methods, a difference that becomes more pronounced with an increase in sample size. \nBivariate methods perform well when the generating model is univariate (Scenarios 3 \\& 4). Though prediction is better when fitting the correct univariate model, the differences between the bivariate and univariate methods become very small with an increase in sample size. All methods show slight under-coverage.\n\n\n\n\\begin{table}\n\n\n\\caption{Simulation Results}\n\\centering\n\\resizebox{\\linewidth}{!}\n{\n\t\\begin{tabular}{l     llll  l   llll }\n\n\\multicolumn{1}{c}{     }  & \\multicolumn{4}{c}{ {Continuous Response}} & \n\\multicolumn{1}{c}{     }   & \\multicolumn{4}{c}{{Binary Response}} \\\\\n \\cline{2-5}  \\cline{7-10} \\\\\n\n& MISE    & CI length & 95 \\% Cvg  & MSPE & \\vspace{.2cm}  & MISE   & CI length &  95 \\% Cvg & MSPE   \\\\ \n\\midrule\n\t \\multicolumn{10}{c}{Scenario 1: $n= 50$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0.8$ }  \\\\\n\\midrule \nBFPCA & 3.50        & 65.3         & 92.9 *** & 319    &     & 0.174      & 12.8         & 86.9 *** & 23.0          \\\\\nBBSP  & 3.26       & 61.6 **      & 90.7 *** & 313       &  & 0.168      & 12.9         & 87.3 *** & 22.9        \\\\\nUFPCA & 3.20        & 66.5 *       & 93.6     & 350 *    &   & 0.182      & 14.8 *       & 90.8 *** & 24.4 *      \\\\\nUBSP  & 2.86       & 64.9         & 94.0       & 351 *     &  & 0.185      & 14.5 *       & 90.9 *** & 24.3 *      \\\\\n\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 2: $n= 250$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0.8$ }  \\\\\n\\midrule \nBFPCA & 0.795      & 31.9         & 91.4 *** & 284     &    & 0.039     & 6.66         & 90.7 *** & 21.0          \\\\\nBBSP  & 0.798      & 31.2 **      & 91.0 ***   & 285   &      & 0.037     & 6.67         & 91.3 *** & 21.0          \\\\\nUFPCA & 0.790      & 32.8 *       & 91.9 *** & 351 *  &     & 0.040     & 7.27 *       & 93.2     & 24.3 *      \\\\\nUBSP  & 0.794      & 32.4 *       & 92.0 ***   & 350 * &      & 0.043     & 7.25 *       & 91.6 *** & 24.3 *      \\\\\n\n\n\\midrule\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 3: $n= 50$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0$ }  \\\\\n\\midrule \nBFPCA & 2.96       & 65.9         & 94.2     & 408       &  & 0.172      & 13.4         & 89.3 *** & 26.3        \\\\\nBBSP  & 3.16       & 62.6 **      & 92.8 *** & 421 *    &   & 0.183      & 13.3         & 88.4 *** & 26.6 *      \\\\\nUFPCA & 2.75       & 65.8         & 94.6     & 372 **   &   & 0.166      & 14.6 *       & 93.3     & 24.4 **     \\\\\nUBSP  & 2.85       & 63.9 **      & 93.5     & 371 **   &   & 0.162      & 14.5 *       & 92.8 *** & 24.2 **     \\\\\n\n\n\n\\midrule\n\t \\multicolumn{10}{c}{Scenario 4: $n= 250$,  $ {\\rho_{{\\bm{\\alpha}}}}=  0$ }  \\\\\n\\midrule \nBFPCA & 0.802      & 32.5         & 94.6   & 370        & & 0.044     & 6.96         & 91.1 *** & 24.8        \\\\\nBBSP  & 0.791      & 31.9 **      & 94.3   & 374 *      & & 0.042     & 6.97         & 90.7 *** & 24.9        \\\\\nUFPCA & 0.780       & 32.9 *       & 94.7   & 362 **   &   & 0.042     & 7.35 *       & 93.5     & 24.3 **     \\\\\nUBSP  & 0.765      & 32.5         & 94.5   & 361 **     & & 0.040     & 7.35 *       & 94.1     & 24.3 **     \\\\\n\n\n\\midrule\n\\multicolumn{10}{p{\\linewidth}}{{Results in hundredths. A `**' (`*') indicates better (worse) compared to BFPCA by Wilcoxson rank sum test, $\\alpha=0.05$. For coverage, a `***' indicates that the coverage is not within the nominal $95\\%$ range. }}\\\\\n\t\\bottomrule\n\t\\end{tabular}\n}\n\\label{t:sim2}\n\\end{table} \n\n\n\n\n\nFor Scenarios 1 \\& 2 there is no clear difference between fitting predetermined bases (Section \\ref{s:pre}) or data-driven bases (Section \\ref{s:datadriven}); however, BFPCA has better prediction compared to BBSP in Scenarios 3 \\& 4 when there is no cross-correlation. The univariate models have very similar performance to one another in all scenarios.\n\n\n\n\t\n\t\\section{Periodontal Data Application}\\label{s:dental_data}\n\t\n\nWe demonstrate our methods using data from a periodontal study \\citep{Fer+:09} conducted by the Center for Oral Health\nResearch at the Medical University of South Carolina. In addition to collecting subject-level covariates for over 200 Gullah African Americans, several measures of patients' periodontal health were observed at six sites for each of 28 teeth. The two responses we consider are (continuous) clinical attachment loss (CAL) and (binary-valued) bleeding on probing (BOP). CAL is the distance that a tooth has detached from the bone, rounded to the nearest mm. We use the average CAL over the six sites on each tooth as the tooth's CAL response. \nBOP is the binary indicator of whether the gums bleed when pressed with a dental probe at any of the six sites per tooth. A total of ${\\mathrm}{N}=197$ patients (subjects) are included for analysis after excluding those with more than 50\\% missingness. Any remaining missingness is assumed to be completely at random; \\cite{Rei+Ban:10} and \\cite{Rei+:13} provide methods for accounting for non-random missingness.\n\nFor our analysis, we assign teeth the numbers 1-14 going from left to right in the upper jaw when looking at a patient and 15-28 going from right to left in the lower jaw when looking at a patient; wisdom teeth are excluded. Using this numbering system, teeth 1 \\& 28  are adjacent going from upper jaw to lower jaw, and it is the same for teeth 14 \\& 15 on the other side of the mouth. \nWe consider responses at each tooth to be realizations of a functional process with locations $t \\in [1,28]$. In fitting a bivariate functional model to this data, we hope to gain a better understanding of the dynamics between the responses CAL and BOP through close examination of their cross-covariance. Our extremely flexible approach to modeling the covariance will be able to capture any spatial correlation of adjacent teeth, of teeth on different sides of the mouth, and of teeth on different jaws.\n\n\nThe subject-specific covariates  that we include in modeling the mean function are the same covariates used by \\cite{Rei+Ban:10} and include age (in years), gender (female=1, male=0), body mass index or BMI (in ${\\mathrm}{kg}/{\\mathrm}{m}^2$), smoking status (1=smoker, 0=never), and glycosylated hemoglobin or HbA1c (1 = high, 0 = controlled).  All covariates have been standardized to be zero-mean with standard deviation of 1. For each tooth, we include an indicator of jaw (0=upper, 1=lower). For the smooth part of the mean, we consider a quadratic function \n$s_p(t) = \\beta_{p0} + \\beta_{p1}d(t) + \\beta_{p2}d(t)^2 $ of tooth distance $d$ from the front of the mouth, where $d(t)= t - 7.5$ for teeth in the upper jaw and $d(t)= t- 21.5$ for the lower jaw.\n\nWe present analysis for 8 models given in Table \\ref{t:data2} that all employ the data-driven basis method of Section \\ref{s:datadriven}. \nThe 8 models differ by: 1) whether FPCA is univariate or bivariate; 2) the choice of threshold ${\\mathrm}{P}_1=99\\%,95\\%$ for the cumulative percentage of variation explained for FPCA; and 3) whether a random bivariate subject-level intercept ${\\bm{\\alpha}}_{0i}= [\\alpha_{01i}, \\alpha_{02i}]^{\\mathrm}{T}$ is added to model \\eqref{mod1}. Models using B-splines as in Section \\ref{s:pre} were also considered but are not presented because the best-performing models required a large number of basis functions. \n\n\n\n\n\\begin{table}\n\\normalsize\n\n\\caption{Model comparisons for the periodontal data application}\n\\centering\n\\resizebox{\\linewidth}{!}\n{\n\t\\begin{tabular}{rrrrrrrr }\nModel & Subject RE &   PVE & PCA &Dbar & pD & DIC \\\\\n\\midrule \n1 & Y & 99 & B    & 8114 & 1257 & 9371 \\\\\n2 & Y & 99 & U    & 8228 & 1231 & 9459 \\\\\n3 & Y & 95 & B    & 8849 & 1001  & 9850  \\\\\n4 & Y & 95 & U    & 8536 & 1114 & 9650 \\\\\n5 & N & 99 & B    & 10101 & 942 & 11043 \\\\\n6 & N & 99 & U    & 10586 & 832 & 11418 \\\\\n7 &  N& 95 & B    & 10511 & 788 & 11299 \\\\\n8 & N & 95 & U    & 10722 & 760 & 11482 \\\\ \n\n\\midrule\n\\multicolumn{7}{p{\\linewidth}}{\\small ``Subject RE\" indicates inclusion of a subject-specific random intercept. ``PVE\" is the threshold for cumulative percentage of variation explained. ``PCA\" indicates whether univariate (``U\") or bivariate (``B\") FPCA was performed. }\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{t:data2}\n\\end{table} \n\n\n\n\nFor the purpose of estimating the latent covariance, we ignore the covariates.   When incorporating a bivariate random subject-level intercept, we use residuals $R_{1i}(t)= Y_{1i}(t) - {\\mathrm}{L}_{1i}^{-1}\\sum_{i=1}^{{\\mathrm}{L}_{1i}} Y_{1i}(t)$ of the continuous response CAL to estimate the latent covariance for FPCA; this is not done for the binary responses as the residuals would no longer be binary. For models that include ${\\bm{\\alpha}}_{0i}$, we estimate the covariance term ${\\mathrm}{Cov}(\\alpha_{01i}, \\alpha_{02i})$ in addition to the variance terms ${\\mathrm}{Var}(\\alpha_{0pi})$. We specify a diagonal covariance matrix ${\\bm}{\\Sigma}$ for the remaining random effect parameters.  \n\n\n\n\n\nTable \\ref{t:data2} shows that Models 1-4, which include a subject random effect, outperform (based on DIC) Models 5-8 which omit the subject random effect. For this data, specifying the larger percentage of variation explained for FPCA, and hence including more basis functions, leads to better model performance. In comparing the two leading models 1 \\& 2, implementing FPCA on the full bivariate covariance matrix as in Model 1, taking into account the cross-dependence between the two responses CAL and BOP, leads to superior performance.\nFigure \\ref{f:covariates} shows the subject-level coefficient estimates and 95\\% posterior intervals for Model 1. Models 2-4 had similar coefficient estimates.  For CAL, only the coefficient interval for BMI includes zero. The other coefficient estimates show an increased level of CAL for older patients, males, smokers, patients with high HbA1c counts, and for teeth on the upper jaw. For BOP, the posterior confidence intervals are larger than those for CAL. For intervals that exclude zero, there is an increase of BOP for the upper jaw, yet a slightly lower incidence of BOP for higher BMI. \n\n\n\n\\begin{figure}\n\\centering\n\n\n\\includegraphics[width=\\linewidth]{bxplt_1}\n\\caption{Posterior medians and 95\\% posterior intervals of the subject-specific covariate coefficients by response.}\n\\label{f:covariates}\n\\end{figure}\n\n\n\n\n\n \n\n\n\nFigure \\ref{f:fitted} shows the fitted values (from Model 1) for two individuals in the periodontal data set. The left panels show the posterior means and 95\\% posterior intervals of the subject-specific mean function $\\mu_{1i}(t)$ for the continuous response CAL. Most of the observed CAL values fall within the 95\\% interval for both subjects, indicating a reasonable model fit. The right panels show the posterior mean and 95\\% posterior intervals of the conditional probability of the event, $P(Y_{2i}(t)=1|{\\bm{\\alpha}}_{2i})$. Teeth with observed BOP ($=1$) are indicated by the squares on the bottom of the plot. The higher predicted probabilities tend to correspond to the incidence of BOP, again indicating a reasonable model fit. \n\n\n\n\n\\begin{figure}\n\\centering\n\n\\includegraphics[width=\\linewidth]{plot_subj1}\n\\includegraphics[width=\\linewidth]{plot_subj2}\n\\caption[Fitted values for two individuals from the periodontal study.]{Fitted values for two individuals from the periodontal study (using Model 1). \\emph{Left panels}: Observed values of CAL are shown as dots. The solid black line indicates the posterior mean of $\\mu_{1i}(t)$, the subject-specific mean function, and point-wise 95\\% posterior intervals are given by the dotted lines. \\emph{Right panels}: The squares along the x-axis indicate the teeth for which BOP is observed. The solid black line gives the posterior mean of the conditional probability of the event, $P(Y_{2i}(t)=1|{\\bm{\\alpha}}_{2i})$, and dotted lines show point-wise 95\\% posterior intervals. The label ``UPPER LEFT\" refers to the left side of the the upper jaw when looking at a patient, and it is analogous for the other labels.}\n\\label{f:fitted}\n\\end{figure}\n\n\n\n\nThe posterior summaries of the auto- and cross-correlations of the subject-specific process ${\\bm}{f}_i(t)$ from \\eqref{mod1} are given in Figure \\ref{f:post_corr}; note that the correlation attributed to the subject random intercept is not included in this figure. In this periodontal application, these plots offer important and novel insights into the complex relationships that exist between and within the BOP and CAL responses in different parts of the mouth. The utility of quantifying and visualizing these complex correlation relationships is apparent for many other types of applications. \n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\n\n\\includegraphics[width=\\linewidth]{corr_wori}\n\\includegraphics[width=\\linewidth]{sd_corr_wori}\n\\includegraphics[width=\\linewidth]{probgt0_corr_wori}\n\\caption{Posterior summaries of the within-response and between-response correlation structures for any two teeth when fit with Model 1 (excluding correlation from the subject random intercepts). The label ``UPPER LEFT\" refers to the left side of the the upper jaw when looking at a patient, and it is analogous for the other labels.}\n\\label{f:post_corr}\n\\end{figure}\n\n\n\nExamination of the diagonal of the auto-correlation plot for CAL in Figure \\ref{f:post_corr} shows strong positive spatial correlation between adjacent teeth and between teeth separated by only one or two teeth on the same jaw.  This plot also shows positive correlation between a tooth in the left and a tooth in the right side of the same jaw, and the relationship is particularly strong for teeth in the lower jaw. The correlation for CAL between teeth in opposite sides of the mouth and on different jaws is also positive, yet not as strong as for teeth on the same jaw; this correlation is very similar in magnitude as the correlation for teeth on the left or right side of the mouth but on different jaws. Additionally, there are mild to strong negative correlations between teeth in the center (front) of the mouth and teeth in the back of the mouth, regardless of the jaws on which the teeth are located. This is also seen in the plot of the posterior probability that the auto-correlation is positive.\n\nIn the auto-correlation plot for BOP, again we see strong positive spatial correlation between adjacent teeth and between teeth that are close to one another on the same jaw. Additionally, the plots of the auto-correlation and of the probability of being positive show that the correlation is mostly positive with only a few areas of negative correlation. The correlation is negative between a tooth in the center and a tooth on the right side of the upper jaw, as well as between a tooth in the left and a tooth in the center of the lower jaw. There is also a strong negative correlation for teeth in the lower right and upper right, as well as for teeth in the lower left and lower right. \n\nThe cross-correlation between BOP and CAL ranges from moderately positive to moderately negative. Unlike the auto correlation plots, the cross correlation is not symmetric, which makes interpretation slightly more complex. For instance, BOP in the lower left is positively correlated with CAL in the center and upper right as indicated by the darkest patch near the top center of the cross correlation figure. Alternatively, CAL in the lower left shows slightly negative to no correlation with BOP in the center and upper right of the mouth.  \nAnother demonstration of this non-symmetric property occurs for the negative correlation of BOP in the lower left with CAL in the lower right, though BOP in the lower right shows slightly positive to no correlation with CAL in the lower left. \n\n\n\n\\section{Discussion}\n\n\nWe introduce a methodology to jointly model multivariate functional responses of mixed type (e.g. continuous and binary data) and also propose an extension of FPCA for mixed-responses. \nOur method can account for subject-specific covariates that can be either linear or time-dependent (such as the jaw indicator used in the analysis of the periodontal study in Section \\ref{s:dental_data}). \nThe proposed method is flexible enough for functions to be observed at varying locations for different subjects and different responses.\nFor exposition we focus on modeling a bivariate response vector where one functional response is continuous and the other is binary, though joint modeling of more than two responses is a straightforward extension. Furthermore, the method easily models repeatedly observed categorical responses. This is achieved in a manner similar to thresholding the latent process at zero for binary data, but instead one must impose multiple thresholds on the latent process. Modeling other types of data, such as repeatedly observed count data, is not as straightforward as it would likely require using copulas\n\nBy estimating the multivariate covariance of the latent process, our methodology can offer novel insights into the cross-dependence of different responses, which is of interest in a wide\nvariety of applications. Quantifying and exploring this dependence is an important contribution of our method and is a primary goal of our analysis of the periodontal data presented in Section \\ref{s:dental_data}.  \\cite{Rei+Ban:10} and \\cite{Rei+:13} offer ways to incorporate informative missingness and apply their methods to the same periodontal data. We do not address the informative missingness for our analysis because it is not central to our goals, and leave it for future work.\n\n\n\n\n\n\\section*{Acknowledgments}\nThe authors thank Dipankar Bandypadhyay of the University of Minnesota and Drs. S. London, J. Fernandes, C. Salinas, W. Zhao, Ms. L. Summerlin and Ms. P. Hudson. W of the Center for Oral Health Research (COHR) at the Medical University of South Carolina for providing the data and context for this work.  The work of Beth A. Tidemann-Miller was supported by NIH grant R01 CA085848. \n\n\n\n\\bibliographystyle{rss}\n\\bibliography{researchbib2}\n\n\n\\section*{Appendix}\n\n\\section*{Appendix A}\n\nIn this section, we show that we can approximate any smooth covariance using the predetermined basis method.  \nFor simplicity, assume that the functional responses are observed at the same locations $t_{p\\ell} \\equiv t_\\ell$ for $\\ell = 1, \\hdots, {\\mathrm}{L}$ for each response $p$. We specify this model for an arbitrary subject, and thus drop the subscript $i$. Let ${\\bm{\\psi}}_{pk}$ be the vector  of length ${\\mathrm}{L}$ formed by evaluating at every $t_{\\ell}$ the basis functions $\\psi_{pk}(t)$, $k=1,\\hdots,{\\mathrm}{M}_p$, and define the vector ${\\bm}{f}_{p}$ analogously. Then we form the ${\\mathrm}{L} \\times {\\mathrm}{M}_p$ matrix ${\\bm{\\Psi}}_{p}=[{\\bm{\\psi}}_{pk}, \\hdots, {\\bm{\\psi}}_{p {\\mathrm}{M}_p}]$ and the coefficient vector ${\\bm{\\alpha}}_{pi} = [\\alpha_{p1},\\hdots,\\alpha_{p{{\\mathrm}{M}_p}} ]^{\\mathrm}{T}$ so that we can write ${\\bm}{f}_{p}={\\bm{\\Psi}}_{p} {\\bm{\\alpha}}_{p}$. We combine ${\\bm}{f}_{p}$ for all responses to form one vector ${\\bm}{f}$ of length ${\\mathrm}{n}= {\\mathrm}{P} {\\mathrm}{L}$, and define the coefficient vector ${\\bm{\\alpha}}^{\\mathrm}{T} = [{\\bm{\\alpha}}_{1}^{\\mathrm}{T},\\hdots, {\\bm{\\alpha}}_{{\\mathrm}{P}}^{\\mathrm}{T}]$ of length ${\\mathrm}{m}=\\sum_{p=1}^{\\mathrm}{P} {\\mathrm}{M}_p$ and corresponding block-diagonal matrix ${\\bm{\\Psi}}$ of dimension ${\\mathrm}{n} \\times {\\mathrm}{m}$ with blocks ${\\bm{\\Psi}}_{p}$. The resulting vector  ${\\bm}{f}= {\\bm{\\Psi}} {\\bm{\\alpha}}$ has length ${\\mathrm}{n}$, and we assume ${\\bm{\\alpha}} \\overset{iid}{\\sim}N(0,{\\bm}{\\Sigma})$ where ${\\bm}{\\Sigma}$ is a covariance matrix of dimension ${\\mathrm}{m} \\times {\\mathrm}{m}$ with elements ${\\mathrm}{Cov}(\\alpha_{pk},\\alpha_{p{^\\prime}  \\ell })= \\xi_{k \\ell p p{^\\prime}}$.\n\nTo illustrate the flexibility of the model, assume ${\\bm}{\\Omega}_0$ is the true ${\\mathrm}{n} \\times {\\mathrm}{n}$ covariance matrix of ${\\bm}{f}$ evaluated at locations $t_{l}$. ${\\bm}{\\Omega}_0$ is now approximated by the variance-covariance matrix \n${\\bm}{\\Omega}={\\mathrm}{Cov}({\\bm{\\Psi}} {\\bm{\\alpha}}) = {\\bm{\\Psi}} {\\bm}{\\Sigma} {\\bm{\\Psi}}^{\\mathrm}{T}$. Since the basis comprising ${\\bm{\\Psi}}$ is pre-specified, the quality of the approximation ${\\bm}{\\Omega}\\approx {\\bm}{\\Omega}_0$ is reliant on ${\\bm}{\\Sigma}$. By fitting a large number of basis functions, i.e. setting ${\\mathrm}{m}={\\mathrm}{n}$, it is possible to fit any smooth covariance function. When ${\\mathrm}{m}={\\mathrm}{n}$ then ${\\bm{\\Psi}}_i$ is a square matrix. Assume ${\\bm{\\Psi}}_i$ is full rank and thus ${\\bm{\\Psi}}_i^{\\mathrm}{T}{\\bm{\\Psi}}_i $ is invertible. Pre- and post- multiplication gives ${\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{\\Omega} {\\bm{\\Psi}}_i = {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm{\\Psi}}_i {\\bm}{\\Sigma} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm{\\Psi}}_i$. Since ${\\bm}{\\Theta}=\\{{\\bm{\\Psi}}_i^{\\mathrm}{T}{\\bm{\\Psi}}_i\\}^{-1}$ exists we can recover ${\\bm}{\\Sigma} = {\\bm}{\\Theta}{\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{\\Omega} {\\bm{\\Psi}}_i {\\bm}{\\Theta}$. Though this approach is quite flexible, it is \nhard to estimate ${\\bm}{\\Sigma}$ if it is high-dimension; therefore it is unlikely to perform well if the processes cannot be represented by a small number of basis functions.\n\n\n\n\n\\section*{Appendix B}\n\nHere we describe in more detail the derivation of the latent cross covariance estimator. As our approach is inspired by \\cite{Hal+:08}, we start with a brief summary of the method they proposed for finding the auto-covariance of the latent process corresponding to the binary response, that is, response $p=2$. First, estimate the mean function for $p=2$, $\\hat{\\mu}_2(t)=g^{-1}\\{ \\hat{\\eta}_2(t) \\}$ where $\\hat{\\eta}_2(t)$ estimates ${\\mathrm{E}}[{ g\\{Z_{{2} i}(t)  \\}} ]={\\eta}_2(t)$ and is found by smoothing the data $\\big(t,Y_{2 i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. Next, find the estimator $\\hat{S}_{22}(t,t{^\\prime})$ of $S_{22}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{2 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[  g\\{Z_{2i}(t)\\}  g\\{Z_{2i}(t{^\\prime})\\} \\big]$ by performing bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{2 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, once again removing the diagonals before smoothing. The estimator of the latent process covariance operator for the second response is given by \n\n", "index": 17, "text": "\\begin{align} \\label{Hall_univ}\n\\widetilde{K}_{22}(t,t{^\\prime}) = \\{\\hat{S}_{22}(t,t{^\\prime})- \\hat{\\eta}_2(t)\\hat{\\eta}_2(t{^\\prime}) \\} /[ g^{(1)}\\{\\hat{\\mu}_2(t)\\}  g^{(1)}\\{\\hat{\\mu}_2(t{^\\prime})\\}   ].\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widetilde{K}_{22}(t,t{{}^{\\prime}})=\\{\\hat{S}_{22}(t,t{{}^{%&#10;\\prime}})-\\hat{\\eta}_{2}(t)\\hat{\\eta}_{2}(t{{}^{\\prime}})\\}/[g^{(1)}\\{\\hat{\\mu%&#10;}_{2}(t)\\}g^{(1)}\\{\\hat{\\mu}_{2}(t{{}^{\\prime}})\\}].\" display=\"inline\"><mrow><msub><mover accent=\"true\"><mi>K</mi><mo>~</mo></mover><mn>22</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">^</mo></mover><mn>22</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>/</mo><mrow><mo stretchy=\"false\">[</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nWe can expand the covariance of the observed processes $ {\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}= {\\mathrm}{Cov} \\big[Z_{1 i}(t),g\\{Z_{2 i}(t{^\\prime})\\} \\big]$ by substituting \\eqref{Taylor1} for $g\\{Z_{2 i}(t{^\\prime})\\}$ and $\\mu_1(t) + \\delta X_{1 i}(t)$ for $Z_{1 i}(t)$, which gives\n\n", "itemtype": "equation", "pos": 54649, "prevtext": "\n\nEquation \\eqref{Hall_univ} was developed for a univariate response, so in order to estimate the latent cross covariance operator $K_{12}(t,t{^\\prime}) = K_{21}(t{^\\prime},t)= {\\mathrm}{Cov}\\{Z_{1 i}(t), Z_{2 i}(t{^\\prime})\\}$, we must derive an analogous estimator. This requires the following Taylor expansion, also given by equation (5) in \\cite{Hal+:08},\n\n", "index": 19, "text": "\\begin{align}\\label{Taylor1}\ng\\{Z_i(t)\\}=&g\\{\\mu(t)\\}+\\delta X_i(t) g^{(1)}\\{\\mu(t)\\}+ \\frac{1}{2}\\delta^2 \\{X_i(t)\\}^2 g^{(2)}\\{\\mu(t)\\} \\nonumber\\\\\n&+\\frac{1}{6}\\delta^3 \\{X_i(t)\\}^3g^{(3)}\\{\\mu(t)\\}+ O_p(\\delta^4).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g\\{Z_{i}(t)\\}=\" display=\"inline\"><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle g\\{\\mu(t)\\}+\\delta X_{i}(t)g^{(1)}\\{\\mu(t)\\}+\\frac{1}{2}\\delta^{%&#10;2}\\{X_{i}(t)\\}^{2}g^{(2)}\\{\\mu(t)\\}\" display=\"inline\"><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{6}\\delta^{3}\\{X_{i}(t)\\}^{3}g^{(3)}\\{\\mu(t)\\}+O_{p}(%&#10;\\delta^{4}).\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>6</mn></mfrac></mstyle><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>3</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mn>3</mn></msup><mo>\u2062</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>O</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03b4</mi><mn>4</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\t \nNote that the term (suppressed from equation \\eqref{Taylor}) $\\delta^3 \\frac{1}{2}g^{(2)}\\{\\mu_2(t{^\\prime})\\} {\\mathrm}{Cov}\\{X_{1 i}(t),  X^2_{2 i}(t{^\\prime}) \\}=0$ due to \n${\\mathrm}{Cov}\\{X_{1 i}(t),  X^2_{2 i}(t{^\\prime}) \\}  = {\\mathrm{E}}\\{X_{1 i}(t) X^2_{2 i}(t{^\\prime}) \\} =\n{\\mathrm{E}}[X^2_{2 i}(t{^\\prime}) {\\mathrm{E}}\\{X_{1 i}(t) |X_{2 i}(t{^\\prime}) \\}]=\\sigma_1/\\sigma_2\\rho {\\mathrm{E}}[  X^3_{2 i}(t{^\\prime}) ] = 0$ since $X_{1 i}(t) |X_{2 i}(t{^\\prime}) \\sim {\\mathrm}{N}\\big(\\sigma_1/\\sigma_2\\rho X_{2 i}(t{^\\prime}) ,(1-\\rho^2)\\sigma^2_1\\big)$. \nNow, because ${\\mathrm}{Cov} \\{Z_{1 i}(t),Z_{2 i}(t{^\\prime})\\} = {\\mathrm}{Cov}\\{\\delta X_{1 i}(t), \\delta X_{2 i}(t{^\\prime}) \\}$,  we have from \\eqref{Taylor} that \n${K}_{12}(t,t{^\\prime}) =  {\\mathrm}{Cov} \\{Z_{1 i}(t),Z_{2 i}(t{^\\prime})\\}={\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}/g^{(1)}\\{\\mu_2(t{^\\prime})\\} +O(\\delta^4)$,\nwhich, assuming the effect of $O(\\delta^4)$ is negligible, leads to\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nWe can expand the covariance of the observed processes $ {\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}= {\\mathrm}{Cov} \\big[Z_{1 i}(t),g\\{Z_{2 i}(t{^\\prime})\\} \\big]$ by substituting \\eqref{Taylor1} for $g\\{Z_{2 i}(t{^\\prime})\\}$ and $\\mu_1(t) + \\delta X_{1 i}(t)$ for $Z_{1 i}(t)$, which gives\n\n", "index": 21, "text": "\\begin{align} \\label{Taylor}\n{\\mathrm}{Cov} \\big\\{Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big\\} &=\tg^{(1)}\\{\\mu_2(t{^\\prime})\\} {\\mathrm}{Cov}\\{\\delta X_{1 i}(t), \\delta X_{2 i}(t{^\\prime}) \\}   +  O(\\delta^4).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathrm{}}{Cov}\\big{\\{}Y_{1i}(t),Y_{2i}(t{{}^{\\prime}})\\big{\\}}\" display=\"inline\"><mrow><mi>C</mi><mi>o</mi><mi>v</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msub><mi>Y</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=g^{(1)}\\{\\mu_{2}(t{{}^{\\prime}})\\}{\\mathrm{}}{Cov}\\{\\delta X_{1i%&#10;}(t),\\delta X_{2i}(t{{}^{\\prime}})\\}+O(\\delta^{4}).\" display=\"inline\"><mrow><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03bc</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mi>C</mi><mi>o</mi><mi>v</mi><mrow><mo stretchy=\"false\">{</mo><mi>\u03b4</mi><msub><mi>X</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>\u03b4</mi><msub><mi>X</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>+</mo><mi>O</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03b4</mi><mn>4</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\nEstimation of \\eqref{cross} requires a smooth estimate $\\hat{\\eta}_1(t)$ of ${\\mathrm{E}}[Y_{1 i}(t) ]={\\eta}_1(t)$ which is found by smoothing the data $\\big(t,Y_{1 i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. We obtain the estimator $\\hat{S}_{12}(t,t{^\\prime})$ of $S_{12}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{1 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[ Y_{1 i}(t) g\\{ Z_{2i}(t{^\\prime}) \\} \\big]$ by performing bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{1 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, removing the diagonals before smoothing. The resulting smooth estimator of the latent cross covariance is \n\n", "itemtype": "equation", "pos": 56396, "prevtext": "\t \nNote that the term (suppressed from equation \\eqref{Taylor}) $\\delta^3 \\frac{1}{2}g^{(2)}\\{\\mu_2(t{^\\prime})\\} {\\mathrm}{Cov}\\{X_{1 i}(t),  X^2_{2 i}(t{^\\prime}) \\}=0$ due to \n${\\mathrm}{Cov}\\{X_{1 i}(t),  X^2_{2 i}(t{^\\prime}) \\}  = {\\mathrm{E}}\\{X_{1 i}(t) X^2_{2 i}(t{^\\prime}) \\} =\n{\\mathrm{E}}[X^2_{2 i}(t{^\\prime}) {\\mathrm{E}}\\{X_{1 i}(t) |X_{2 i}(t{^\\prime}) \\}]=\\sigma_1/\\sigma_2\\rho {\\mathrm{E}}[  X^3_{2 i}(t{^\\prime}) ] = 0$ since $X_{1 i}(t) |X_{2 i}(t{^\\prime}) \\sim {\\mathrm}{N}\\big(\\sigma_1/\\sigma_2\\rho X_{2 i}(t{^\\prime}) ,(1-\\rho^2)\\sigma^2_1\\big)$. \nNow, because ${\\mathrm}{Cov} \\{Z_{1 i}(t),Z_{2 i}(t{^\\prime})\\} = {\\mathrm}{Cov}\\{\\delta X_{1 i}(t), \\delta X_{2 i}(t{^\\prime}) \\}$,  we have from \\eqref{Taylor} that \n${K}_{12}(t,t{^\\prime}) =  {\\mathrm}{Cov} \\{Z_{1 i}(t),Z_{2 i}(t{^\\prime})\\}={\\mathrm}{Cov} \\big \\{ Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}/g^{(1)}\\{\\mu_2(t{^\\prime})\\} +O(\\delta^4)$,\nwhich, assuming the effect of $O(\\delta^4)$ is negligible, leads to\n\n", "index": 23, "text": "\\begin{align} \\label{cross}\n{K}_{12}(t,t{^\\prime}) = {\\mathrm}{Cov} \\big\\{Y_{1 i}(t),Y_{2 i}(t{^\\prime}) \\big \\}/g^{(1)}\\{\\mu_2(t{^\\prime})\\}. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{K}_{12}(t,t{{}^{\\prime}})={\\mathrm{}}{Cov}\\big{\\{}Y_{1i}(t),Y_{2%&#10;i}(t{{}^{\\prime}})\\big{\\}}/g^{(1)}\\{\\mu_{2}(t{{}^{\\prime}})\\}.\" display=\"inline\"><mrow><msub><mi>K</mi><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mi>C</mi><mi>o</mi><mi>v</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><msub><mi>Y</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msub><mi>Y</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow><mo>/</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03bc</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nwhich is the direct analogue to \\eqref{Hall_univ}. \n\n\n\\section*{Appendix C}\n\nIn this section we present the derivations of the conditional posterior distributions. \n\n\\subsection*{Random effects}\nLet ${\\mathrm}{L}_i$ be the number of subunits $t$ observed for subject $i$ and define the latent response vector for subject $i$ as ${\\bm}{W}_i = [W_{1}(t_1),...,W_{1}(t_{{\\mathrm}{L}_i}), W^2(t_1),...,W^2(t_{{\\mathrm}{L}_i})]^{\\mathrm}{T}$, with corresponding mean vector ${\\mathrm{E}}({\\bm}{W}_i)={\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i$.  Assume ${\\bm}{W}_i|{\\bm{\\alpha}}_i \\sim {\\mathrm}{N}_{2 {\\mathrm}{L}_i} \\big({\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i, {\\bm}{D}_i\\big)$ where ${\\bm}{D}_i={\\mathrm}{diag}(\\tau^2_{1},1) \\otimes {\\bm}{{\\mathrm}{I}}_{{\\mathrm}{L}_i} $, or in terms of the precision, ${\\bm}{P}_i={\\bm}{D}_i^{-1}={\\mathrm}{diag}(\\omega_{1}, 1) \\otimes {\\bm}{{\\mathrm}{I}}_{{\\mathrm}{L}_i} $.\n\n\nAlso assume the ${\\mathrm}{m} \\times 1$ vector ${\\bm{\\alpha}}_i|{\\bm}{Q} \\sim {{\\mathrm}{N}}_{\\mathrm}{m}(0, {\\bm}{Q}^{-1})$ for $i=1,\\hdots,{\\mathrm}{N}$ and for the ${\\mathrm}{m} \\times {\\mathrm}{m}$ covariance matrix ${\\bm}{Q}^{-1}$, or equivalently, the precision matrix ${\\bm}{Q}$. Define ${\\bm}{R}_i={\\bm}{W}_i-{\\bm}{U}_i {\\bm{\\beta}}$. To find the posterior for ${\\bm{\\alpha}}_i | \\cdot$ we know \n\n", "itemtype": "equation", "pos": 57194, "prevtext": "\n\nEstimation of \\eqref{cross} requires a smooth estimate $\\hat{\\eta}_1(t)$ of ${\\mathrm{E}}[Y_{1 i}(t) ]={\\eta}_1(t)$ which is found by smoothing the data $\\big(t,Y_{1 i}(t)\\big)$ for $i=1,\\hdots,{\\mathrm}{N}$. We obtain the estimator $\\hat{S}_{12}(t,t{^\\prime})$ of $S_{12}(t,t{^\\prime})= {\\mathrm{E}}\\{Y_{1 i}(t)Y_{2 i}(t{^\\prime})\\}= {\\mathrm{E}}\\big[ Y_{1 i}(t) g\\{ Z_{2i}(t{^\\prime}) \\} \\big]$ by performing bivariate smoothing of the data  $\\big((t,t{^\\prime}),Y_{1 i}(t) Y_{2 i}(t{^\\prime}) \\big)$ for $i=1,\\hdots,{\\mathrm}{N}$, removing the diagonals before smoothing. The resulting smooth estimator of the latent cross covariance is \n\n", "index": 25, "text": "\\begin{align} \\label{cross2}\n\\widetilde{K}_{12}(t,t{^\\prime}) = \\{\\hat{S}_{12}(t,t{^\\prime})- \\hat{\\eta}_1(t)\\hat{\\eta}_2(t{^\\prime}) \\} / g^{(1)}\\{\\hat{\\mu}_2(t{^\\prime})\\},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\widetilde{K}_{12}(t,t{{}^{\\prime}})=\\{\\hat{S}_{12}(t,t{{}^{%&#10;\\prime}})-\\hat{\\eta}_{1}(t)\\hat{\\eta}_{2}(t{{}^{\\prime}})\\}/g^{(1)}\\{\\hat{\\mu}%&#10;_{2}(t{{}^{\\prime}})\\},\" display=\"inline\"><mrow><msub><mover accent=\"true\"><mi>K</mi><mo>~</mo></mover><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">^</mo></mover><mn>12</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>/</mo><msup><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo>\u2032</mo></mmultiscripts></mrow><mo stretchy=\"false\">}</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nWe want to form this term into the kernel of a Gaussian distribution  where the exponent is $ -1/2({\\bm{\\alpha}}_i - {\\bm}{M})^{\\mathrm}{T} {\\bm{V}}^{-1}  ({\\bm{\\alpha}}_i - {\\bm}{M})= -1/2({\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm{V}}^{-1} {\\bm{\\alpha}}_i  - 2{\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm{\\alpha}}_i + {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm}{M}) $ for some matrices ${\\bm}{M}$ and ${\\bm{V}}$. To complete the square, set ${\\bm{V}}= \\{ {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{Q} \\}^{-1}$ and match the coefficients of ${\\bm{\\alpha}}_i$, giving  $ {\\bm}{R}^{\\mathrm}{T}_i {\\bm}{P}_i{\\bm{\\Psi}}_i = {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1} \\implies {\\bm}{M}  = {\\bm{V}} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i {\\bm}{R}_i$.\nThus, the full conditional posterior for ${\\bm{\\alpha}}_i$ is given by\n\n", "itemtype": "equation", "pos": 58725, "prevtext": "\nwhich is the direct analogue to \\eqref{Hall_univ}. \n\n\n\\section*{Appendix C}\n\nIn this section we present the derivations of the conditional posterior distributions. \n\n\\subsection*{Random effects}\nLet ${\\mathrm}{L}_i$ be the number of subunits $t$ observed for subject $i$ and define the latent response vector for subject $i$ as ${\\bm}{W}_i = [W_{1}(t_1),...,W_{1}(t_{{\\mathrm}{L}_i}), W^2(t_1),...,W^2(t_{{\\mathrm}{L}_i})]^{\\mathrm}{T}$, with corresponding mean vector ${\\mathrm{E}}({\\bm}{W}_i)={\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i$.  Assume ${\\bm}{W}_i|{\\bm{\\alpha}}_i \\sim {\\mathrm}{N}_{2 {\\mathrm}{L}_i} \\big({\\bm}{U}_i {\\bm{\\beta}} + {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i, {\\bm}{D}_i\\big)$ where ${\\bm}{D}_i={\\mathrm}{diag}(\\tau^2_{1},1) \\otimes {\\bm}{{\\mathrm}{I}}_{{\\mathrm}{L}_i} $, or in terms of the precision, ${\\bm}{P}_i={\\bm}{D}_i^{-1}={\\mathrm}{diag}(\\omega_{1}, 1) \\otimes {\\bm}{{\\mathrm}{I}}_{{\\mathrm}{L}_i} $.\n\n\nAlso assume the ${\\mathrm}{m} \\times 1$ vector ${\\bm{\\alpha}}_i|{\\bm}{Q} \\sim {{\\mathrm}{N}}_{\\mathrm}{m}(0, {\\bm}{Q}^{-1})$ for $i=1,\\hdots,{\\mathrm}{N}$ and for the ${\\mathrm}{m} \\times {\\mathrm}{m}$ covariance matrix ${\\bm}{Q}^{-1}$, or equivalently, the precision matrix ${\\bm}{Q}$. Define ${\\bm}{R}_i={\\bm}{W}_i-{\\bm}{U}_i {\\bm{\\beta}}$. To find the posterior for ${\\bm{\\alpha}}_i | \\cdot$ we know \n\n", "index": 27, "text": "\\begin{align}\np({\\bm{\\alpha}}_i | \\cdot) & \\propto   p({\\bm}{W}_i | \\cdot    )           \\times p({\\bm{\\alpha}}_i| {\\bm}{Q})   \\\\\n& \\propto  \\exp \\left[  -\\frac{1}{2} \\left \\{\n( {\\bm{\\Psi}}_i{\\bm{\\alpha}}_i -  {\\bm}{R}_i )^{\\mathrm}{T} {\\bm}{P}_i ( {\\bm{\\Psi}}_i{\\bm{\\alpha}}_i -  {\\bm}{R}_i )+ {\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i    \n\\right \\}  \\right ],  \\\\\n& \\propto  \\exp \\left[  -\\frac{1}{2} \\left \\{\n{\\bm{\\alpha}}^{\\mathrm}{T}_i {\\bm{\\Psi}}_i^{\\mathrm}{T}{\\bm}{P}_i  {\\bm{\\Psi}}_i{\\bm{\\alpha}}_i \n-2 {\\bm}{R}^{\\mathrm}{T}_i {\\bm}{P}_i  {\\bm{\\Psi}}_i{\\bm{\\alpha}}_i \n+ {\\bm}{R}_i^{\\mathrm}{T}{\\bm}{P}_i{\\bm}{R}_i\n+  {\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i\n\\right \\}  \\right ],  \\\\\t\t\t     \t\t     \n\\intertext{\\centering and ignoring terms not involving ${\\bm{\\alpha}}_i$ or ${\\bm}{Q}$ results in}  \np({\\bm{\\alpha}}_i | \\cdot) & \\propto  \\exp \\left(  -\\frac{1}{2}  \\left [  -2 {\\bm}{R}^{\\mathrm}{T}_i {\\bm}{P}_i{\\bm{\\Psi}}_i{\\bm{\\alpha}}_i   +  \n{\\bm{\\alpha}}_i^{\\mathrm}{T} \\left\\{ {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{Q} \\right \\}{\\bm{\\alpha}}_i       \\right]  \\right ). \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\bm{\\alpha}}_{i}|\\cdot)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto p({\\bm{}}{W}_{i}|\\cdot)\\times p({\\bm{\\alpha}}_{i}|{\\bm{}}%&#10;{Q})\" display=\"inline\"><mrow><mo>\u221d</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\left\\{({\\bm{\\Psi}}_{i}{\\bm{\\alpha}}%&#10;_{i}-{\\bm{}}{R}_{i})^{\\mathrm{}}{T}{\\bm{}}{P}_{i}({\\bm{\\Psi}}_{i}{\\bm{\\alpha}}%&#10;_{i}-{\\bm{}}{R}_{i})+{\\bm{\\alpha}}_{i}^{\\mathrm{}}{T}{\\bm{}}{Q}{\\bm{\\alpha}}_{%&#10;i}\\right\\}\\right],\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow><mo>-</mo><msub><mi>R</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow><mo>-</mo><msub><mi>R</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udf36</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\left\\{{\\bm{\\alpha}}^{\\mathrm{}}{T}_%&#10;{i}{\\bm{\\Psi}}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{\\Psi}}_{i}{\\bm{\\alpha}}_{i%&#10;}-2{\\bm{}}{R}^{\\mathrm{}}{T}_{i}{\\bm{}}{P}_{i}{\\bm{\\Psi}}_{i}{\\bm{\\alpha}}_{i}%&#10;+{\\bm{}}{R}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{R}_{i}+{\\bm{\\alpha}}_{i}^{%&#10;\\mathrm{}}{T}{\\bm{}}{Q}{\\bm{\\alpha}}_{i}\\right\\}\\right],\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mrow><msup><mi>\ud835\udf36</mi><mi/></msup><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>R</mi><mi/></msup><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mrow><msubsup><mi>R</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>R</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udf36</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_centering\" alttext=\"{\\bm{\\alpha}}_{i}\" display=\"inline\"><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_centering\" alttext=\"{\\bm{}}{Q}\" display=\"inline\"><mi>Q</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\bm{\\alpha}}_{i}|\\cdot)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left(-\\frac{1}{2}\\left[-2{\\bm{}}{R}^{\\mathrm{}}{T}_{i%&#10;}{\\bm{}}{P}_{i}{\\bm{\\Psi}}_{i}{\\bm{\\alpha}}_{i}+{\\bm{\\alpha}}_{i}^{\\mathrm{}}{%&#10;T}\\left\\{{\\bm{\\Psi}}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{\\Psi}}_{i}+{\\bm{}}{Q%&#10;}\\right\\}{\\bm{\\alpha}}_{i}\\right]\\right).\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>R</mi><mi/></msup><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udf36</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>Q</mi></mrow><mo>}</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": " \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nWe want to form this term into the kernel of a Gaussian distribution  where the exponent is $ -1/2({\\bm{\\alpha}}_i - {\\bm}{M})^{\\mathrm}{T} {\\bm{V}}^{-1}  ({\\bm{\\alpha}}_i - {\\bm}{M})= -1/2({\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm{V}}^{-1} {\\bm{\\alpha}}_i  - 2{\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm{\\alpha}}_i + {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm}{M}) $ for some matrices ${\\bm}{M}$ and ${\\bm{V}}$. To complete the square, set ${\\bm{V}}= \\{ {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{Q} \\}^{-1}$ and match the coefficients of ${\\bm{\\alpha}}_i$, giving  $ {\\bm}{R}^{\\mathrm}{T}_i {\\bm}{P}_i{\\bm{\\Psi}}_i = {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1} \\implies {\\bm}{M}  = {\\bm{V}} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i {\\bm}{R}_i$.\nThus, the full conditional posterior for ${\\bm{\\alpha}}_i$ is given by\n\n", "index": 29, "text": "$${\\bm{\\alpha}}_i | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\alpha}}}, {\\bm{V}}_{{\\bm{\\alpha}}} )$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\alpha}}_{i}|\\cdot\\sim{\\mathrm{}}{N}({\\bm{}}{\\mu}_{{\\bm{\\alpha}}},{\\bm{V}%&#10;}_{{\\bm{\\alpha}}})\" display=\"block\"><mrow><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo>\u223c</mo><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>\ud835\udf36</mi></msub><mo>,</mo><msub><mi>\ud835\udc7d</mi><mi>\ud835\udf36</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\n\n\\subsection*{Random effects precision matrix}\nAssume the ${\\mathrm}{m} \\times 1$ vector ${\\bm{\\alpha}}_i|{\\bm}{Q} \\sim {{\\mathrm}{N}}_{\\mathrm}{m}(0, {\\bm}{Q}^{-1})$ for $i=1,\\hdots,{\\mathrm}{N}$ and the ${\\mathrm}{m} \\times {\\mathrm}{m}$ precision matrix ${\\bm}{Q} \\sim {\\mathrm}{Wishart}_{{\\mathrm}{m}}({\\bm}{V}, \\nu)$ for which the kernel of the density is given by $ |{\\bm}{Q}|^{(\\nu - {\\mathrm}{m} - 1)/2} {\\mathrm}{exp} \\left \\{ -\\frac{1}{2} {\\mathrm}{tr}({\\bm}{V}^{-1}{\\bm}{Q})  \\right \\} $.\nDefine ${\\bm}{S}= \\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T} }$ as the sum of squares matrix of ${\\bm{\\alpha}}_i$. We use ${\\bm}{S}$ to write \n$\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i}={\\mathrm}{tr}(\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i})= {\\mathrm}{tr}(\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} }) = {\\mathrm}{tr}({\\bm}{S}{\\bm}{Q})$ in the kernel of the multivariate normal density, using the properties ${\\mathrm}{tr}(a)=a$ for scalar $a$ and ${\\mathrm}{tr}({\\bm}{A}{\\bm}{B})= {\\mathrm}{tr}({\\bm}{B}{\\bm}{A})$.\nWe also use the following properties of the trace to combine like-terms: 1) $|\\bm{A}|^{-1}=|\\bm{A}^{-1}|$ for $\\bm{A}$ invertible and 2) for two square matrices $\\bm{A}$ and $\\bm{B}$ of the same dimension, ${\\mathrm}{tr}(\\bm{A}+\\bm{B})={\\mathrm}{tr}(\\bm{A})+{\\mathrm}{tr}(\\bm{B})$.\nUsing this, we can show the conditional posterior $p({\\bm}{Q}| \\cdot)$ for ${\\bm}{Q}$ is proportional to the kernel of a ${\\mathrm}{Wishart}_{{\\mathrm}{m}}\\{ ({\\bm}{S}+{\\bm}{V}^{-1})^{-1}, {\\mathrm}{N}+ \\nu\\}$:\n\n", "itemtype": "equation", "pos": 60777, "prevtext": " \n\n", "index": 31, "text": "$$ \\text{for } {\\bm}{\\mu}_{{\\bm{\\alpha}}} = \\{ {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{Q} \\}^{-1} {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i ({\\bm}{W}_i-{\\bm}{U}_i {\\bm{\\beta}}) \\text{ and }   {\\bm{V}}_{{\\bm{\\alpha}}}= \\{ {\\bm{\\Psi}}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm{\\Psi}}_i+ {\\bm}{Q} \\}^{-1}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\text{for }{\\bm{}}{\\mu}_{{\\bm{\\alpha}}}=\\{{\\bm{\\Psi}}_{i}^{\\mathrm{}}{T}{\\bm{}%&#10;}{P}_{i}{\\bm{\\Psi}}_{i}+{\\bm{}}{Q}\\}^{-1}{\\bm{\\Psi}}_{i}^{\\mathrm{}}{T}{\\bm{}}%&#10;{P}_{i}({\\bm{}}{W}_{i}-{\\bm{}}{U}_{i}{\\bm{\\beta}})\\text{ and }{\\bm{V}}_{{\\bm{%&#10;\\alpha}}}=\\{{\\bm{\\Psi}}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{\\Psi}}_{i}+{\\bm{}%&#10;}{Q}\\}^{-1}.\" display=\"block\"><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>\ud835\udf36</mi></msub></mrow><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>Q</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udc7d</mi><mi>\ud835\udf36</mi></msub></mrow><mo>=</mo><msup><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><msubsup><mi>\ud835\udebf</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udebf</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>Q</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nThus, ${\\bm}{Q}| \\cdot \\sim {\\mathrm}{Wishart}_{{\\mathrm}{m}}\\{ ({\\bm}{S}+{\\bm}{V}^{-1})^{-1}, {\\mathrm}{N}+ \\nu\\}$.\n\n\n\\subsection*{Fixed effects}\n\n\n\n", "itemtype": "equation", "pos": 62733, "prevtext": "\n\n\n\\subsection*{Random effects precision matrix}\nAssume the ${\\mathrm}{m} \\times 1$ vector ${\\bm{\\alpha}}_i|{\\bm}{Q} \\sim {{\\mathrm}{N}}_{\\mathrm}{m}(0, {\\bm}{Q}^{-1})$ for $i=1,\\hdots,{\\mathrm}{N}$ and the ${\\mathrm}{m} \\times {\\mathrm}{m}$ precision matrix ${\\bm}{Q} \\sim {\\mathrm}{Wishart}_{{\\mathrm}{m}}({\\bm}{V}, \\nu)$ for which the kernel of the density is given by $ |{\\bm}{Q}|^{(\\nu - {\\mathrm}{m} - 1)/2} {\\mathrm}{exp} \\left \\{ -\\frac{1}{2} {\\mathrm}{tr}({\\bm}{V}^{-1}{\\bm}{Q})  \\right \\} $.\nDefine ${\\bm}{S}= \\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T} }$ as the sum of squares matrix of ${\\bm{\\alpha}}_i$. We use ${\\bm}{S}$ to write \n$\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i}={\\mathrm}{tr}(\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} {\\bm{\\alpha}}_i})= {\\mathrm}{tr}(\\sum_{i=1}^{\\mathrm}{N}{{\\bm{\\alpha}}_i{\\bm{\\alpha}}_i^{\\mathrm}{T} {\\bm}{Q} }) = {\\mathrm}{tr}({\\bm}{S}{\\bm}{Q})$ in the kernel of the multivariate normal density, using the properties ${\\mathrm}{tr}(a)=a$ for scalar $a$ and ${\\mathrm}{tr}({\\bm}{A}{\\bm}{B})= {\\mathrm}{tr}({\\bm}{B}{\\bm}{A})$.\nWe also use the following properties of the trace to combine like-terms: 1) $|\\bm{A}|^{-1}=|\\bm{A}^{-1}|$ for $\\bm{A}$ invertible and 2) for two square matrices $\\bm{A}$ and $\\bm{B}$ of the same dimension, ${\\mathrm}{tr}(\\bm{A}+\\bm{B})={\\mathrm}{tr}(\\bm{A})+{\\mathrm}{tr}(\\bm{B})$.\nUsing this, we can show the conditional posterior $p({\\bm}{Q}| \\cdot)$ for ${\\bm}{Q}$ is proportional to the kernel of a ${\\mathrm}{Wishart}_{{\\mathrm}{m}}\\{ ({\\bm}{S}+{\\bm}{V}^{-1})^{-1}, {\\mathrm}{N}+ \\nu\\}$:\n\n", "index": 33, "text": "\\begin{align}\n\\label{postQ}\np({\\bm}{Q}| \\cdot) & \\propto \\prod_{i=1}^{\\mathrm}{N} p({\\bm{\\alpha}}_i| {\\bm}{Q})\\times p({\\bm}{Q}) \\nonumber\\\\\n& \\propto |{\\bm}{Q}^{-1}|^{-{\\mathrm}{N}/2} {\\mathrm}{exp} \\left \\{ -\\frac{1}{2} {\\mathrm}{tr}({\\bm}{S}{\\bm}{Q})  \\right \\}  \\times \n|{\\bm}{Q}|^{(\\nu - {\\mathrm}{m} - 1)/2} {\\mathrm}{exp} \\left \\{ -\\frac{1}{2} {\\mathrm}{tr}({\\bm}{V}^{-1}{\\bm}{Q})  \\right \\}  \\\\\n&  \\propto |{\\bm}{Q}|^ {({\\mathrm}{N}+ \\nu - {\\mathrm}{m} - 1)/2}  {\\mathrm}{exp} \\left [ -\\frac{1}{2} {\\mathrm}{tr} \\{ ({\\bm}{S}+{\\bm}{V}^{-1}){\\bm}{Q}^{-1} \\}  \\right ]. \\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\bm{}}{Q}|\\cdot)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\prod_{i=1}^{\\mathrm{}}{N}p({\\bm{\\alpha}}_{i}|{\\bm{}}{Q})%&#10;\\times p({\\bm{}}{Q})\" display=\"inline\"><mrow><mo>\u221d</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mi>N</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto|{\\bm{}}{Q}^{-1}|^{-{\\mathrm{}}{N}/2}{\\mathrm{}}{exp}\\left%&#10;\\{-\\frac{1}{2}{\\mathrm{}}{tr}({\\bm{}}{S}{\\bm{}}{Q})\\right\\}\\times|{\\bm{}}{Q}|^%&#10;{(\\nu-{\\mathrm{}}{m}-1)/2}{\\mathrm{}}{exp}\\left\\{-\\frac{1}{2}{\\mathrm{}}{tr}({%&#10;\\bm{}}{V}^{-1}{\\bm{}}{Q})\\right\\}\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mrow><mrow><msup><mrow><mo stretchy=\"false\">|</mo><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">|</mo></mrow><mrow><mo>-</mo><mrow><mi>N</mi><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">|</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bd</mi><mo>-</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>Q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto|{\\bm{}}{Q}|^{({\\mathrm{}}{N}+\\nu-{\\mathrm{}}{m}-1)/2}{%&#10;\\mathrm{}}{exp}\\left[-\\frac{1}{2}{\\mathrm{}}{tr}\\{({\\bm{}}{S}+{\\bm{}}{V}^{-1})%&#10;{\\bm{}}{Q}^{-1}\\}\\right].\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mrow><msup><mrow><mo stretchy=\"false\">|</mo><mi>Q</mi><mo stretchy=\"false\">|</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>N</mi><mo>+</mo><mi>\u03bd</mi></mrow><mo>-</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>+</mo><msup><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\nAs we did before, we want to form this term into the kernel of a Gaussian distribution and where the exponent is $ -1/2({\\bm{\\beta}} - {\\bm}{M})^{\\mathrm}{T} {\\bm{V}}^{-1}  ({\\bm{\\beta}} - {\\bm}{M})= -1/2({\\bm{\\beta}}^{\\mathrm}{T} {\\bm{V}}^{-1} {\\bm{\\beta}} - 2{\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm{\\beta}} + {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm}{M}) $ for some matrices ${\\bm}{M}$ and ${\\bm{V}}$. To complete the square, set ${\\bm{V}}= \\left({\\bm{C}} + \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i  \\right)^{-1}$ and match the coefficients of ${\\bm{\\beta}}$, giving  $\\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i = {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1} \\implies {\\bm}{M}  = {\\bm{V}} \\left( \\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i \\right)^{\\mathrm}{T}$.\nThus, the full conditional posterior for ${\\bm{\\beta}}$ is given by\n\n", "itemtype": "equation", "pos": 63479, "prevtext": "\nThus, ${\\bm}{Q}| \\cdot \\sim {\\mathrm}{Wishart}_{{\\mathrm}{m}}\\{ ({\\bm}{S}+{\\bm}{V}^{-1})^{-1}, {\\mathrm}{N}+ \\nu\\}$.\n\n\n\\subsection*{Fixed effects}\n\n\n\n", "index": 35, "text": "\\begin{align}\np({\\bm{\\beta}} | \\cdot) & \\propto \\prod_{i=1}^{\\mathrm}{N}  p({\\bm}{W}_i | \\cdot ) \\times p({\\bm{\\beta}})   \\\\\n& \\propto  \\exp \\left(  -\\frac{1}{2}  \\left[ \\sum_{i=1}^{\\mathrm}{N} \\left \\{\n(  {\\bm}{U}_i {\\bm{\\beta}} -  {\\bm}{U}_i )^{\\mathrm}{T} {\\bm}{P}_i (  {\\bm}{U}_i {\\bm{\\beta}} -  {\\bm}{U}_i )  \\right\\}+ {\\bm{\\beta}}^{\\mathrm}{T} {\\bm{C}} {\\bm{\\beta}}     \\right] \\right )  \\\\\n& \\propto  \\exp \\left[  -\\frac{1}{2}   \\sum_{i=1}^{\\mathrm}{N} \\left \\{\n(  {\\bm}{U}_i {\\bm{\\beta}} -  {\\bm}{U}_i )^{\\mathrm}{T} {\\bm}{P}_i (  {\\bm}{U}_i {\\bm{\\beta}} -  {\\bm}{U}_i )  \\right\\}  -\\frac{1}{2}   {\\bm{\\beta}}^{\\mathrm}{T} {\\bm{C}} {\\bm{\\beta}}     \\right]  \\\\\n& \\propto  \\exp \\left[  -\\frac{1}{2}   \\sum_{i=1}^{\\mathrm}{N} \\left \\{\n{\\bm{\\beta}}^{\\mathrm}{T} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i {\\bm{\\beta}} \n-2{\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i {\\bm{\\beta}}\n+ {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i {\\bm}{U}_i\n\\right\\} -\\frac{1}{2}   {\\bm{\\beta}}^{\\mathrm}{T} {\\bm{C}} {\\bm{\\beta}} \\right]  \\\\\n\\intertext{\\centering and ignoring constant terms results in}  \n& \\propto  \\exp \\left[  -\\frac{1}{2}  \\left \\{{\\bm{\\beta}}^{\\mathrm}{T} {\\bm{C}} {\\bm{\\beta}} +\n{\\bm{\\beta}}^{\\mathrm}{T}\\left(\\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i \\right){\\bm{\\beta}} \n-2 \\left( \\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i \\right) {\\bm{\\beta}}\n\\right\\}  \\right]  \\\\\n& \\propto  \\exp \\left[  -\\frac{1}{2}  \\left \\{\n{\\bm{\\beta}}^{\\mathrm}{T}\\left({\\bm{C}} + \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i  \\right){\\bm{\\beta}} \n-2 \\left( \\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i \\right) {\\bm{\\beta}}\n\\right\\}  \\right]  \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\bm{\\beta}}|\\cdot)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\prod_{i=1}^{\\mathrm{}}{N}p({\\bm{}}{W}_{i}|\\cdot)\\times p(%&#10;{\\bm{\\beta}})\" display=\"inline\"><mrow><mo>\u221d</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mi>N</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf37</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left(-\\frac{1}{2}\\left[\\sum_{i=1}^{\\mathrm{}}{N}\\left%&#10;\\{({\\bm{}}{U}_{i}{\\bm{\\beta}}-{\\bm{}}{U}_{i})^{\\mathrm{}}{T}{\\bm{}}{P}_{i}({%&#10;\\bm{}}{U}_{i}{\\bm{\\beta}}-{\\bm{}}{U}_{i})\\right\\}+{\\bm{\\beta}}^{\\mathrm{}}{T}{%&#10;\\bm{C}}{\\bm{\\beta}}\\right]\\right)\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><msub><mi>U</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><msub><mi>U</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>+</mo><mrow><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>\ud835\udc6a</mi><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\sum_{i=1}^{\\mathrm{}}{N}\\left\\{({%&#10;\\bm{}}{U}_{i}{\\bm{\\beta}}-{\\bm{}}{U}_{i})^{\\mathrm{}}{T}{\\bm{}}{P}_{i}({\\bm{}}%&#10;{U}_{i}{\\bm{\\beta}}-{\\bm{}}{U}_{i})\\right\\}-\\frac{1}{2}{\\bm{\\beta}}^{\\mathrm{}%&#10;}{T}{\\bm{C}}{\\bm{\\beta}}\\right]\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><msub><mi>U</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><msub><mi>U</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>\ud835\udc6a</mi><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\sum_{i=1}^{\\mathrm{}}{N}\\left\\{{\\bm%&#10;{\\beta}}^{\\mathrm{}}{T}{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{%&#10;i}{\\bm{\\beta}}-2{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{i}{\\bm{%&#10;\\beta}}+{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{i}\\right\\}-%&#10;\\frac{1}{2}{\\bm{\\beta}}^{\\mathrm{}}{T}{\\bm{C}}{\\bm{\\beta}}\\right]\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mrow><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>+</mo><mrow><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>\ud835\udc6a</mi><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\left\\{{\\bm{\\beta}}^{\\mathrm{}}{T}{%&#10;\\bm{C}}{\\bm{\\beta}}+{\\bm{\\beta}}^{\\mathrm{}}{T}\\left(\\sum_{i=1}^{\\mathrm{}}{N}%&#10;{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{i}\\right){\\bm{\\beta}}-2%&#10;\\left(\\sum_{i=1}^{\\mathrm{}}{N}{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm%&#10;{}}{U}_{i}\\right){\\bm{\\beta}}\\right\\}\\right]\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mrow><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>\ud835\udc6a</mi><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>+</mo><mrow><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\exp\\left[-\\frac{1}{2}\\left\\{{\\bm{\\beta}}^{\\mathrm{}}{T}%&#10;\\left({\\bm{C}}+\\sum_{i=1}^{\\mathrm{}}{N}{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P%&#10;}_{i}{\\bm{}}{U}_{i}\\right){\\bm{\\beta}}-2\\left(\\sum_{i=1}^{\\mathrm{}}{N}{\\bm{}}%&#10;{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{i}\\right){\\bm{\\beta}}\\right\\}\\right]\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><msup><mi>\ud835\udf37</mi><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\ud835\udc6a</mi><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": " \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nAs we did before, we want to form this term into the kernel of a Gaussian distribution and where the exponent is $ -1/2({\\bm{\\beta}} - {\\bm}{M})^{\\mathrm}{T} {\\bm{V}}^{-1}  ({\\bm{\\beta}} - {\\bm}{M})= -1/2({\\bm{\\beta}}^{\\mathrm}{T} {\\bm{V}}^{-1} {\\bm{\\beta}} - 2{\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm{\\beta}} + {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1}{\\bm}{M}) $ for some matrices ${\\bm}{M}$ and ${\\bm{V}}$. To complete the square, set ${\\bm{V}}= \\left({\\bm{C}} + \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i  \\right)^{-1}$ and match the coefficients of ${\\bm{\\beta}}$, giving  $\\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i = {\\bm}{M}^{\\mathrm}{T}{\\bm{V}}^{-1} \\implies {\\bm}{M}  = {\\bm{V}} \\left( \\sum_{i=1}^{\\mathrm}{N}  {\\bm}{U}_i^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i \\right)^{\\mathrm}{T}$.\nThus, the full conditional posterior for ${\\bm{\\beta}}$ is given by\n\n", "index": 37, "text": "$${\\bm{\\beta}} | \\cdot \\sim {\\mathrm}{N}( {\\bm}{\\mu}_{{\\bm{\\beta}}}, {\\bm{V}}_{{\\bm{\\beta}}} )$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\beta}}|\\cdot\\sim{\\mathrm{}}{N}({\\bm{}}{\\mu}_{{\\bm{\\beta}}},{\\bm{V}}_{{%&#10;\\bm{\\beta}}})\" display=\"block\"><mrow><mi>\ud835\udf37</mi><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo>\u223c</mo><mi>N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>\ud835\udf37</mi></msub><mo>,</mo><msub><mi>\ud835\udc7d</mi><mi>\ud835\udf37</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\n\n\n\n\n\n\\subsection*{Error Variance (Precision)}\n\nAssume that the error precision $\\omega_{1} \\sim {\\mathrm}{Gamma}(g,h)$ where we parameterize the density such that $g$ is the shape parameter and $h=1/s$ is the inverse of the scale parameter, called the rate parameter. Specifically, if $X \\sim {\\mathrm}{Gamma}(g,h)$ then $ p(x|g,h) = x^{g-1} e^{-xh} \\{h^g/ \\Gamma(g)\\}$. For simplicity of notation, denote the continuous response at $t_\\ell$ for subject $i$  as $Y_{i\\ell}=Y_{{1} i}(t_\\ell)$ for $i=1,\\hdots,{\\mathrm}{N}$ and $\\ell= 1,\\hdots,{{\\mathrm}{L}_i}$, and define the total number of responses observed as $n=\\sum_{i=1}^{\\mathrm}{N}{{\\mathrm}{L}_i}$. Let $Y_{i\\ell} \\overset{indep}{\\sim} {\\mathrm}{N}(\\mu_{i\\ell}, {\\mathrm}{precision}= \\omega_{1}  )$. Then \n\n", "itemtype": "equation", "pos": 66222, "prevtext": " \n\n", "index": 39, "text": "$$ \\text{for }  {\\bm}{\\mu}_{{\\bm{\\beta}}}  =   {\\bm{V}}_{{\\bm{\\beta}}} \\left\\{ \\sum_{i=1}^{\\mathrm}{N}  ({\\bm}{W}_i - {\\bm{\\Psi}}_i {\\bm{\\alpha}}_i )^{\\mathrm}{T}{\\bm}{P}_i {\\bm}{U}_i \\right \\}^{\\mathrm}{T}\n\\text{ and }  {\\bm{V}}_{{\\bm{\\beta}}}= \\left( \\sigma^{-2}_{{\\bm{\\beta}}}{\\textbf{\\text{I}}}_{\\mathrm}{r} + \\sum_{i=1}^{\\mathrm}{N} {\\bm}{U}_i^{\\mathrm}{T} {\\bm}{P}_i  {\\bm}{U}_i  \\right)^{-1}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\text{for }{\\bm{}}{\\mu}_{{\\bm{\\beta}}}={\\bm{V}}_{{\\bm{\\beta}}}\\left\\{\\sum_{i=1%&#10;}^{\\mathrm{}}{N}({\\bm{}}{W}_{i}-{\\bm{\\Psi}}_{i}{\\bm{\\alpha}}_{i})^{\\mathrm{}}{%&#10;T}{\\bm{}}{P}_{i}{\\bm{}}{U}_{i}\\right\\}^{\\mathrm{}}{T}\\text{ and }{\\bm{V}}_{{%&#10;\\bm{\\beta}}}=\\left(\\sigma^{-2}_{{\\bm{\\beta}}}{\\textbf{\\text{I}}}_{\\mathrm{}}{r%&#10;}+\\sum_{i=1}^{\\mathrm{}}{N}{\\bm{}}{U}_{i}^{\\mathrm{}}{T}{\\bm{}}{P}_{i}{\\bm{}}{%&#10;U}_{i}\\right)^{-1}.\" display=\"block\"><mrow><mrow><mrow><mtext>for\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>\ud835\udf37</mi></msub></mrow><mo>=</mo><mrow><msub><mi>\ud835\udc7d</mi><mi>\ud835\udf37</mi></msub><mo>\u2062</mo><msup><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover><mrow><mi>N</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>\ud835\udebf</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf36</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow><mo>}</mo></mrow><mi/></msup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udc7d</mi><mi>\ud835\udf37</mi></msub></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\u03c3</mi><mi>\ud835\udf37</mi><mrow><mo>-</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><msub><mtext>\ud835\udc08</mtext><mi/></msub><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover><mrow><mi>N</mi><mo>\u2062</mo><msubsup><mi>U</mi><mi>i</mi><mi/></msubsup><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02461.tex", "nexttext": "\nThis is the kernel of a Gamma density, so the posterior for $\\omega_{1} | \\cdot \\sim {\\mathrm}{Gamma}(g_\\omega, h_\\omega)$ with shape and rate parameters\t$g_\\omega =n/2+g$ and $h_\\omega= 1/2 \\sum_{i=1}^{\\mathrm}{N} \\sum_{\\ell=1}^{{{\\mathrm}{L}_i}} ( Y_{i\\ell} - \\mu_{i\\ell} )^2 + h $. \n\n\n\n\n\n", "itemtype": "equation", "pos": 67390, "prevtext": "\n\n\n\n\n\n\\subsection*{Error Variance (Precision)}\n\nAssume that the error precision $\\omega_{1} \\sim {\\mathrm}{Gamma}(g,h)$ where we parameterize the density such that $g$ is the shape parameter and $h=1/s$ is the inverse of the scale parameter, called the rate parameter. Specifically, if $X \\sim {\\mathrm}{Gamma}(g,h)$ then $ p(x|g,h) = x^{g-1} e^{-xh} \\{h^g/ \\Gamma(g)\\}$. For simplicity of notation, denote the continuous response at $t_\\ell$ for subject $i$  as $Y_{i\\ell}=Y_{{1} i}(t_\\ell)$ for $i=1,\\hdots,{\\mathrm}{N}$ and $\\ell= 1,\\hdots,{{\\mathrm}{L}_i}$, and define the total number of responses observed as $n=\\sum_{i=1}^{\\mathrm}{N}{{\\mathrm}{L}_i}$. Let $Y_{i\\ell} \\overset{indep}{\\sim} {\\mathrm}{N}(\\mu_{i\\ell}, {\\mathrm}{precision}= \\omega_{1}  )$. Then \n\n", "index": 41, "text": "\\begin{align}\np(\\omega_{1} | \\cdot) & \\propto \\prod_{i=1}^{\\mathrm}{N} \\prod_{\\ell=1}^{{{\\mathrm}{L}_i}} p( Y_{i\\ell} | \\cdot ) \\times p(\\omega_{1})   \\\\\n& \\propto    \\omega_{1}^{n/2}     \\exp \\left\\{ -\\frac{\\omega_{1}}{2} \\sum_{i=1}^{\\mathrm}{N} \\sum_{\\ell=1}^{{{\\mathrm}{L}_i}} ( Y_{i\\ell} - \\mu_{i\\ell} )^2   \\right \\}  \\times\n\\omega_{1}^{g-1} \\exp(- \\omega_{1} h)\\\\\n& \\propto    \\omega_{1}^{(n/2+g)-1}     \\exp \\left[ -\\omega_{1} \\left \\{\n\\frac{1}{2} \\sum_{i=1}^{\\mathrm}{N} \\sum_{\\ell=1}^{{{\\mathrm}{L}_i}} ( Y_{i\\ell} - \\mu_{i\\ell} )^2 + h  \\right \\} \\right ]. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(\\omega_{1}|\\cdot)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c9</mi><mn>1</mn></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\prod_{i=1}^{\\mathrm{}}{N}\\prod_{\\ell=1}^{{{\\mathrm{}}{L}_%&#10;{i}}}p(Y_{i\\ell}|\\cdot)\\times p(\\omega_{1})\" display=\"inline\"><mrow><mo>\u221d</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mi>N</mi><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>L</mi><mi>i</mi></msub></munderover></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub><mo stretchy=\"false\">|</mo><mo>\u22c5</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c9</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\omega_{1}^{n/2}\\exp\\left\\{-\\frac{\\omega_{1}}{2}\\sum_{i=1}%&#10;^{\\mathrm{}}{N}\\sum_{\\ell=1}^{{{\\mathrm{}}{L}_{i}}}(Y_{i\\ell}-\\mu_{i\\ell})^{2}%&#10;\\right\\}\\times\\omega_{1}^{g-1}\\exp(-\\omega_{1}h)\" display=\"inline\"><mrow><mi/><mo>\u221d</mo><mrow><mrow><mrow><msubsup><mi>\u03c9</mi><mn>1</mn><mrow><mi>n</mi><mo>/</mo><mn>2</mn></mrow></msubsup><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03c9</mi><mn>1</mn></msub><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>L</mi><mi>i</mi></msub></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub><mo>-</mo><msub><mi>\u03bc</mi><mrow><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>\u00d7</mo><msubsup><mi>\u03c9</mi><mn>1</mn><mrow><mi>g</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><msub><mi>\u03c9</mi><mn>1</mn></msub><mo>\u2062</mo><mi>h</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\propto\\omega_{1}^{(n/2+g)-1}\\exp\\left[-\\omega_{1}\\left\\{\\frac{1}%&#10;{2}\\sum_{i=1}^{\\mathrm{}}{N}\\sum_{\\ell=1}^{{{\\mathrm{}}{L}_{i}}}(Y_{i\\ell}-\\mu%&#10;_{i\\ell})^{2}+h\\right\\}\\right].\" display=\"inline\"><mrow><mrow><mi/><mo>\u221d</mo><mrow><msubsup><mi>\u03c9</mi><mn>1</mn><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>/</mo><mn>2</mn></mrow><mo>+</mo><mi>g</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><msub><mi>\u03c9</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi/></munderover></mstyle><mrow><mi>N</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>L</mi><mi>i</mi></msub></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub><mo>-</mo><msub><mi>\u03bc</mi><mrow><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>+</mo><mi>h</mi></mrow><mo>}</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]