[{"file": "1601.02331.tex", "nexttext": "\ncontrolled by the conjugate variable $\\theta$. In one then equates $\\overline{m}$ from Eq.~{(\\ref{{eredges}})} with the actual number of edges $m$ in the network data under study, this serves as the \\emph{null model} of the network under study which the number of edges as the only observable. Note again that only the number of edges $m$ is an explicit variable in constructing the network ensemble~\\footnote{Typically we consider the number of nodes $n$ as given.}; whether the model is sufficient (i.e., is a good approximation of network data) is to be judged on the given model's ability to reproduce other (not used as an input to the model) network characteristics such as the degree distribution, cluster size distribution, degree-degree correlation,~\\emph{et cetera}. A significant disagreement between the expected characteristic of a model and the data may indicate that the choice Hamiltonian needs to be reformulated; for instance, the ubiquity of scale-free (power-law) networks where the degree distribution is fat-tailed renders the simplest Erd\\\"os-R\\'enyi network model (which has a Poissonian degree distribution) inadequate, necessitating the introduction of alternative forms of the graph Hamiltonian. One possibility is to incorporate explicitly the node degrees e${\\{{k_i}\\}}$ ($i\\in{\\mathcal{N}}={1,\\cdots,n}$ is the node indices) themselves to form the so-called {\\textbf}{Linear Degree Hamiltonian} ${H_{LD}}(G)$:\n\n", "itemtype": "equation", "pos": 3023, "prevtext": "\n\\title{Linear and Optimization Hamiltonians in Clustered Exponential Random Graph Modeling}\n\\author{Juyong Park}\n\\affiliation{Department of Physics, Kyunghee University, Seoul, Korea}\n\\author{Soon-Hyung Yook}\n\\affiliation{Department of Physics, Kyunghee University, Seoul, Korea}\n\n\\begin{abstract}\nExponential random graph theory is the complex network analog of the canonical ensemble theory from statistical physics. While it has been particularly successful in modeling networks with specified degree distributions, a na\\\"ive model of a clustered network using a graph Hamiltonian linear in the number of triangles has been shown to undergo an abrupt transition into an unrealistic phase of extreme clustering via triangle condensation. Here we study a non-linear graph Hamiltonian that explicitly forbids such a condensation and show numerically that it generates an equilibrium phase with specified intermediate clustering.\n\\end{abstract}\n\\maketitle\n\n\\section{Introduction}\n\\label{introduction}\nThe study of complex systems found in various disciplines including engineering, biology, sociology that can be represented as networked systems composed of nodes and edges have garnered much interest from statistical physicists in recent years. Building upon a rich and long tradition of studies on many-body systems, they have successfully adapted the analytical and computation tools to understanding networks.~\\cite{Dorogovtsev:2003,Albert:2002rmp,Newman:2008}\n\nA network modeling methodology that shows a striking resemblance to the canonical ensemble theory from statistical physics is the Exponential Random Graph (ERG) theory, originally developed in statistics and currently the most actively studied in the Social Network Analysis (SNA) circles~\\cite{Frank:1986,Park:2004sm,Robins:2007}. Given that the potential readership of this paper will be composed of statistical physicist, the premise of ERG is perhaps most simply explained using the language of statistical physics. Here, as in the canonical ensemble theory, one considers an ensemble $\\Gamma$ of graph configurations (microstates) $G$ whose probabilities in $\\Gamma$ are given by $P(G)=\\sum_{G\\in\\Gamma}{\\mathrm{e}}^{{-H(G)}}/Z$ where $H(G)$ is the \\emph{graph Hamiltonian}, a function of network characteristics of $G$, and $Z=\\sum_{G\\in\\Gamma}{\\mathrm{e}}^{{-H(G)}}$ is the partition function. Both in social network analysis and statistical physics, the Hamiltonian $H(G)$ is typically set up to be a linear function of \\emph{network variables} or \\emph{network statistics} such as the number of edges $m(G)$ in the graph. When the network is simple and unweighted (i.e. the number of edge between two nodes is either $0$ or $1$) it is straightforward to show that $H(G)=\\theta m(G)$ generates the so-called Erd\\\"os-R\\'enyi random graph in which two nodes are connected with probability $p=1/(1+{\\mathrm{e}}^{{\\theta}})$~\\cite{Park:2004sm}. The expected number of edges $\\overline{m}$ in a network of $n$ nodes is in this case, therefore, given as\n\n", "index": 1, "text": "\\begin{align}\n\\overline{m} &= {n\\choose 2}p = \\frac{n(n-1)}{2}\\frac{1}{1+{\\mathrm{e}}^{{\\theta}}},\n\\label{eredges}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\overline{m}\" display=\"inline\"><mover accent=\"true\"><mi>m</mi><mo>\u00af</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={n\\choose 2}p=\\frac{n(n-1)}{2}\\frac{1}{1+{\\mathrm{e}}^{{\\theta}}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><mi>p</mi></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi mathvariant=\"normal\">e</mi><mi>\u03b8</mi></msup></mrow></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere ${\\{{\\theta_i}\\}}$ are the conjugate variables that now control the expected degrees ${\\{{\\overline{k_i}}\\}}$ in a manner similar to what $\\theta$ did to $\\overline{m}$ in Eq.~{(\\ref{{eredges}})}~\\footnote{Note the absence of the temperature $\\beta$ in Eqs.~{(\\ref{{eredges}})}~and~{(\\ref{{ldham}})}. Here, one may consider $\\beta$ as having been absorbed into ${\\{{\\theta}\\}}$.}. On a historical note, the study of ${H_{LD}}$ was prompted by the hypothesis that heavily skewed degree distributions such as the power law may cause the observed negative correlation between degrees of connected nodes, while the Erd\\\"os-R\\'enyi network produces no such correlation in the thermodynamic limit ($n\\to\\infty$)~\\cite{Park:2004sm}. On the other hand, it was shown analytically that power-law networks generated via Eq.~{(\\ref{{ldham}})} exhibited negative degree correlation, proving the hypothesis, and thus that the skewed degree distribution was indeed responsible for the negative degree-degree correlation. This is, in fact, a typical example of the ERG modeling (also of general statistical modeling procedure) procedure   -- identifying ``important'' features of the observed system and testing its sufficiency via comparing the model's predictions and real data (i.e. the ``goodness of fit'' of the model in statistical sense. See Ref.~\\cite{Hunter:2008}) and, when a closer agreement is desired, refining the hypothesis and repeating the procedure. This process is presented schematically in Fig.~\\ref{00_procedure}.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{00_procedure.eps}\n\\caption{The schematics of the Exponential Random Graph modeling of network data. From the network data of interest (top) one selects network variables such as the node degrees ${\\{{k_i}\\}}$ (left) from which one then forms a Hamiltonian $H({\\{{k_i}\\}})$, whose solutions and predictions are compared with the network data. Significant disagreements may necessitate a new selection of variables or reformulation of the Hamiltonian.}\n\\label{00_procedure}\n\\end{figure}\n\nNot surprisingly, the development of ERG as a network modeling framework closely follows the study of graph Hamiltonians of increasing complexity. ERG models of historical import include, in addition to the simplest $H(G)=\\theta m(G)$, the Holland and Leinhardt model of reciprocity, the Strauss model of clustering, the 2-star model, and the generalized $k$-star models~\\cite{Park:2004sm}. We refer interested readers to introductory articles and significant recent work from the SNA community for more detail~\\cite{Anderson:1999,Robins:2007,Hunter:2007,Hunter:2008}.\n\nTo a statistical physicist, the benefits of such a formalism is obvious: one can utilize appropriate computational (such as the Metropolis-Hastings algorithm) and analytical (such as Feynman-diagrammatic method) tools to study the properties of the model~\\cite{Metropolis:1953,Park:2004sm,Park:2010}. It should also be noted that the Hamiltonian need not be linear at all. For instance, when one wishes to construct an exponential random graph model of a network with a specified degree sequence, $H(G)$ only needs to be a function of the node degrees ${\\{{k_i(G)}\\}}$ in $G$, i.e. $H(G)=H[{\\{{k_i(G)}\\}}]$ where $i\\in{\\mathcal{N}}={\\{{1,\\ldots,n}\\}}$ is the node index. This is sufficient to guarantee that two configurations $G$, $G'$ with an identical degree sequence have the same probability in the ensemble, and the aforementioned ${H_{LD}}$ is one possibility. Thus there is much freedom in choosing the form of the $H(G)$, meaning there exists ample avenues for exploration of various possible forms of Hamiltonians as one sees fit, not limited to linear forms. In fact, linear forms such as ${H_{LD}}$ of Eq.~{(\\ref{{ldham}})} are often not robust in the presence of a perturbation, in the sense that when a composite Hamiltonian $H={H_{LD}}+H'$ is used the equilibrium degree distribution may differ significantly from the one specified from ${H_{LD}}$, defeating the modeler's intention to generate a desired degree distribution using ${H_{LD}}$. The purpose of this paper is to review the clustering perturbation and compare the characteristics of linear and nonlinear Hamiltonians under it. For simplicity, we here consider only unweighted and undirected graphs.\n\n\\section{Degree Hamiltonians}\nHere we briefly review $H_{LD}(G)=\\sum_i\\theta_ik_i$, Eq.~{(\\ref{{ldham}})}, specifically when the network is \\emph{sparse} ($k_i\\sim O(1)\\ll \\sqrt{n}$). In such a case it is well known that the probability $p_{ij}$ that nodes $i$ and $j$ are connected is ${\\mathrm{e}}^{{-\\theta_i}}{\\mathrm{e}}^{{-\\theta_j}}$, leading to the average degree ${\\langle{k_i}\\rangle}$ of node $i$~\\cite{Park:2004sm}\n\n", "itemtype": "equation", "pos": 4590, "prevtext": "\ncontrolled by the conjugate variable $\\theta$. In one then equates $\\overline{m}$ from Eq.~{(\\ref{{eredges}})} with the actual number of edges $m$ in the network data under study, this serves as the \\emph{null model} of the network under study which the number of edges as the only observable. Note again that only the number of edges $m$ is an explicit variable in constructing the network ensemble~\\footnote{Typically we consider the number of nodes $n$ as given.}; whether the model is sufficient (i.e., is a good approximation of network data) is to be judged on the given model's ability to reproduce other (not used as an input to the model) network characteristics such as the degree distribution, cluster size distribution, degree-degree correlation,~\\emph{et cetera}. A significant disagreement between the expected characteristic of a model and the data may indicate that the choice Hamiltonian needs to be reformulated; for instance, the ubiquity of scale-free (power-law) networks where the degree distribution is fat-tailed renders the simplest Erd\\\"os-R\\'enyi network model (which has a Poissonian degree distribution) inadequate, necessitating the introduction of alternative forms of the graph Hamiltonian. One possibility is to incorporate explicitly the node degrees e${\\{{k_i}\\}}$ ($i\\in{\\mathcal{N}}={1,\\cdots,n}$ is the node indices) themselves to form the so-called {\\textbf}{Linear Degree Hamiltonian} ${H_{LD}}(G)$:\n\n", "index": 3, "text": "\\begin{align}\nH_{LD}(G)=\\theta_1k_1(G)+\\cdots+\\theta_nk_n(G),\n\\label{ldham}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H_{LD}(G)=\\theta_{1}k_{1}(G)+\\cdots+\\theta_{n}k_{n}(G),\" display=\"inline\"><mrow><mrow><mrow><msub><mi>H</mi><mrow><mi>L</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>\u03b8</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>k</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>+</mo><mrow><msub><mi>\u03b8</mi><mi>n</mi></msub><mo>\u2062</mo><msub><mi>k</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere the latter integral form is valid for a large network ($n\\gg1$), $\\rho(\\theta)$ is the distribution density of $\\theta$, and $A\\equiv\\int_{-\\infty}^{\\infty}{\\mathrm{e}}^{{-\\theta}}\\rho(\\theta){\\mathrm{d}}\\theta$ is thus a constant. Setting ${\\langle{k_i}\\rangle}=q_i$, the specified (desired) degree of node $i$ and inverting Eq.~{(\\ref{{avk}})}, we obtain $\\theta_i = -\\ln\\bigl(q_i/A(n-1)\\bigr)$. ${H_{LD}}$ then becomes\n\n", "itemtype": "equation", "pos": 9412, "prevtext": "\nwhere ${\\{{\\theta_i}\\}}$ are the conjugate variables that now control the expected degrees ${\\{{\\overline{k_i}}\\}}$ in a manner similar to what $\\theta$ did to $\\overline{m}$ in Eq.~{(\\ref{{eredges}})}~\\footnote{Note the absence of the temperature $\\beta$ in Eqs.~{(\\ref{{eredges}})}~and~{(\\ref{{ldham}})}. Here, one may consider $\\beta$ as having been absorbed into ${\\{{\\theta}\\}}$.}. On a historical note, the study of ${H_{LD}}$ was prompted by the hypothesis that heavily skewed degree distributions such as the power law may cause the observed negative correlation between degrees of connected nodes, while the Erd\\\"os-R\\'enyi network produces no such correlation in the thermodynamic limit ($n\\to\\infty$)~\\cite{Park:2004sm}. On the other hand, it was shown analytically that power-law networks generated via Eq.~{(\\ref{{ldham}})} exhibited negative degree correlation, proving the hypothesis, and thus that the skewed degree distribution was indeed responsible for the negative degree-degree correlation. This is, in fact, a typical example of the ERG modeling (also of general statistical modeling procedure) procedure   -- identifying ``important'' features of the observed system and testing its sufficiency via comparing the model's predictions and real data (i.e. the ``goodness of fit'' of the model in statistical sense. See Ref.~\\cite{Hunter:2008}) and, when a closer agreement is desired, refining the hypothesis and repeating the procedure. This process is presented schematically in Fig.~\\ref{00_procedure}.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{00_procedure.eps}\n\\caption{The schematics of the Exponential Random Graph modeling of network data. From the network data of interest (top) one selects network variables such as the node degrees ${\\{{k_i}\\}}$ (left) from which one then forms a Hamiltonian $H({\\{{k_i}\\}})$, whose solutions and predictions are compared with the network data. Significant disagreements may necessitate a new selection of variables or reformulation of the Hamiltonian.}\n\\label{00_procedure}\n\\end{figure}\n\nNot surprisingly, the development of ERG as a network modeling framework closely follows the study of graph Hamiltonians of increasing complexity. ERG models of historical import include, in addition to the simplest $H(G)=\\theta m(G)$, the Holland and Leinhardt model of reciprocity, the Strauss model of clustering, the 2-star model, and the generalized $k$-star models~\\cite{Park:2004sm}. We refer interested readers to introductory articles and significant recent work from the SNA community for more detail~\\cite{Anderson:1999,Robins:2007,Hunter:2007,Hunter:2008}.\n\nTo a statistical physicist, the benefits of such a formalism is obvious: one can utilize appropriate computational (such as the Metropolis-Hastings algorithm) and analytical (such as Feynman-diagrammatic method) tools to study the properties of the model~\\cite{Metropolis:1953,Park:2004sm,Park:2010}. It should also be noted that the Hamiltonian need not be linear at all. For instance, when one wishes to construct an exponential random graph model of a network with a specified degree sequence, $H(G)$ only needs to be a function of the node degrees ${\\{{k_i(G)}\\}}$ in $G$, i.e. $H(G)=H[{\\{{k_i(G)}\\}}]$ where $i\\in{\\mathcal{N}}={\\{{1,\\ldots,n}\\}}$ is the node index. This is sufficient to guarantee that two configurations $G$, $G'$ with an identical degree sequence have the same probability in the ensemble, and the aforementioned ${H_{LD}}$ is one possibility. Thus there is much freedom in choosing the form of the $H(G)$, meaning there exists ample avenues for exploration of various possible forms of Hamiltonians as one sees fit, not limited to linear forms. In fact, linear forms such as ${H_{LD}}$ of Eq.~{(\\ref{{ldham}})} are often not robust in the presence of a perturbation, in the sense that when a composite Hamiltonian $H={H_{LD}}+H'$ is used the equilibrium degree distribution may differ significantly from the one specified from ${H_{LD}}$, defeating the modeler's intention to generate a desired degree distribution using ${H_{LD}}$. The purpose of this paper is to review the clustering perturbation and compare the characteristics of linear and nonlinear Hamiltonians under it. For simplicity, we here consider only unweighted and undirected graphs.\n\n\\section{Degree Hamiltonians}\nHere we briefly review $H_{LD}(G)=\\sum_i\\theta_ik_i$, Eq.~{(\\ref{{ldham}})}, specifically when the network is \\emph{sparse} ($k_i\\sim O(1)\\ll \\sqrt{n}$). In such a case it is well known that the probability $p_{ij}$ that nodes $i$ and $j$ are connected is ${\\mathrm{e}}^{{-\\theta_i}}{\\mathrm{e}}^{{-\\theta_j}}$, leading to the average degree ${\\langle{k_i}\\rangle}$ of node $i$~\\cite{Park:2004sm}\n\n", "index": 5, "text": "\\begin{align}\n{\\langle{k_i}\\rangle}\n\t&= \\sum_{j\\ne i}p_{ij} = {\\mathrm{e}}^{{-\\theta_i}}\\sum_{j\\ne i}{\\mathrm{e}}^{{-\\theta_j}}\\nonumber \\\\\n\t&=(n-1){\\mathrm{e}}^{{-\\theta_i}}\\int_{-\\infty}^{\\infty}{\\mathrm{e}}^{{-\\theta}}\\rho(\\theta){\\mathrm{d}}\\theta\\equiv A(n-1){\\mathrm{e}}^{{-\\theta_i}},\n\\label{avk}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\langle{k_{i}}\\rangle}\" display=\"inline\"><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{j\\neq i}p_{ij}={\\mathrm{e}}^{{-\\theta_{i}}}\\sum_{j\\neq i}{%&#10;\\mathrm{e}}^{{-\\theta_{j}}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2260</mo><mi>i</mi></mrow></munder></mstyle><msub><mi>p</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mrow><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow></msup><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2260</mo><mi>i</mi></mrow></munder></mstyle><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><msub><mi>\u03b8</mi><mi>j</mi></msub></mrow></msup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(n-1){\\mathrm{e}}^{{-\\theta_{i}}}\\int_{-\\infty}^{\\infty}{\\mathrm%&#10;{e}}^{{-\\theta}}\\rho(\\theta){\\mathrm{d}}\\theta\\equiv A(n-1){\\mathrm{e}}^{{-%&#10;\\theta_{i}}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow></msup><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mi>\u03b8</mi></mrow></msup><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>\u03b8</mi></mrow></mrow></mrow></mrow><mo>\u2261</mo><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $M(G)={1\\over2}\\sum_ik_i(G)$ is the number of edges in $G$. One can also show that the ensemble generated via ${H_{LD}}$ is equivalent to the \\emph{configuration model}, a popular and useful framework for studying graphs with arbitrary degree distributions~\\cite{Newman:2001arb}.\n\nNow, if we restrict the ensemble $\\Gamma={\\{{G}\\}}$ to contain only network configurations $G$ with a fixed number of edges $M(G)=M_0={1\\over2}\\sum_iq_i$ (corresponding to the canonical ensemble of particles), the second term $2M(G)\\ln A(n-1))$ becomes a constant. Therefore, we can safely ignore it and use an even simpler form\n\n", "itemtype": "equation", "pos": 10156, "prevtext": "\nwhere the latter integral form is valid for a large network ($n\\gg1$), $\\rho(\\theta)$ is the distribution density of $\\theta$, and $A\\equiv\\int_{-\\infty}^{\\infty}{\\mathrm{e}}^{{-\\theta}}\\rho(\\theta){\\mathrm{d}}\\theta$ is thus a constant. Setting ${\\langle{k_i}\\rangle}=q_i$, the specified (desired) degree of node $i$ and inverting Eq.~{(\\ref{{avk}})}, we obtain $\\theta_i = -\\ln\\bigl(q_i/A(n-1)\\bigr)$. ${H_{LD}}$ then becomes\n\n", "index": 7, "text": "\\begin{align}\n{H_{LD}}(G)\t&= \\sum_{i\\in{\\mathcal{N}}}\\theta_ik_i(G) \\nonumber \\\\\n\t&= -\\sum_{i\\in{\\mathcal{N}}}k_i(G)\\ln q_i+\\ln \\bigl(A(n-1)\\bigr)\\sum_{i\\in{\\mathcal{N}}}k_i(G) \\nonumber \\\\\n\t&= -\\sum_{i\\in{\\mathcal{N}}}k_i(G)\\ln q_i+2M(G)\\ln \\bigl(A(n-1)\\bigr),\n\\label{linham1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{H_{LD}}(G)\" display=\"inline\"><mrow><msub><mi>H</mi><mrow><mi>L</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i\\in{\\mathcal{N}}}\\theta_{i}k_{i}(G)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>\u03b8</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\sum_{i\\in{\\mathcal{N}}}k_{i}(G)\\ln q_{i}+\\ln\\bigl{(}A(n-1)%&#10;\\bigr{)}\\sum_{i\\in{\\mathcal{N}}}k_{i}(G)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\sum_{i\\in{\\mathcal{N}}}k_{i}(G)\\ln q_{i}+2M(G)\\ln\\bigl{(}A(n-1%&#10;)\\bigr{)},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nThis is particularly useful in edge-conserving Monte Carlo simulations, where the Metropolis-Hastings algorithm would consist of relocating the edge between a randomly selected connected node pair to between a randomly selected unconnected pair with probability $1$ if it results in a lower energy, and with probability ${\\mathrm{e}}^{{-\\Delta H(G)}}<1$ when it results in a higher energy.\n\nIt is important to note that it is the ensemble average ${\\langle{k_i}\\rangle}$ of a node that is to be matched with its prescribed degree $q_i$, and there is no guarantee that $k_i=q_i$ strictly, even at equilibrium: In fact, $P(k_i|q_i)$, the probability that a node with a prescribed degree $q_i$ has degree $k_i$ at equilibrium, is\n\n", "itemtype": "equation", "pos": 11062, "prevtext": "\nwhere $M(G)={1\\over2}\\sum_ik_i(G)$ is the number of edges in $G$. One can also show that the ensemble generated via ${H_{LD}}$ is equivalent to the \\emph{configuration model}, a popular and useful framework for studying graphs with arbitrary degree distributions~\\cite{Newman:2001arb}.\n\nNow, if we restrict the ensemble $\\Gamma={\\{{G}\\}}$ to contain only network configurations $G$ with a fixed number of edges $M(G)=M_0={1\\over2}\\sum_iq_i$ (corresponding to the canonical ensemble of particles), the second term $2M(G)\\ln A(n-1))$ becomes a constant. Therefore, we can safely ignore it and use an even simpler form\n\n", "index": 9, "text": "\\begin{align}\n{H_{LD}}(G) = -\\sum_{i\\in{\\mathcal{N}}}k_i(G)\\ln q_i.\n\\label{linham2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{H_{LD}}(G)=-\\sum_{i\\in{\\mathcal{N}}}k_{i}(G)\\ln q_{i}.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>H</mi><mrow><mi>L</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $\\theta_i=-\\ln q_i/A(n-1)$, and ${\\{{{\\mathcal{N}}_k}\\}}$ is the set of all possible combinations of $k$ nodes from ${\\mathcal{N}}$ excluding $i$. From this, the total degree distribution $P(k)$ in equilibrium is given as\n\n", "itemtype": "equation", "pos": 11885, "prevtext": "\nThis is particularly useful in edge-conserving Monte Carlo simulations, where the Metropolis-Hastings algorithm would consist of relocating the edge between a randomly selected connected node pair to between a randomly selected unconnected pair with probability $1$ if it results in a lower energy, and with probability ${\\mathrm{e}}^{{-\\Delta H(G)}}<1$ when it results in a higher energy.\n\nIt is important to note that it is the ensemble average ${\\langle{k_i}\\rangle}$ of a node that is to be matched with its prescribed degree $q_i$, and there is no guarantee that $k_i=q_i$ strictly, even at equilibrium: In fact, $P(k_i|q_i)$, the probability that a node with a prescribed degree $q_i$ has degree $k_i$ at equilibrium, is\n\n", "index": 11, "text": "\\begin{align}\nP(k_i|q_i) = \\sum_{{\\{{{\\mathcal{N}}_k}\\}}}\\biggl[\\prod_{j\\in {\\mathcal{N}}_k}{\\mathrm{e}}^{{-(\\theta_j+\\theta_i)}}\\prod_{l\\in{\\mathcal{N}}_k'}\\bigl(1-{\\mathrm{e}}^{{-(\\theta_l+\\theta_i)}}\\bigr)\\biggr],\n\\label{pkq}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(k_{i}|q_{i})=\\sum_{{\\{{{\\mathcal{N}}_{k}}\\}}}\\biggl{[}\\prod_{j%&#10;\\in{\\mathcal{N}}_{k}}{\\mathrm{e}}^{{-(\\theta_{j}+\\theta_{i})}}\\prod_{l\\in{%&#10;\\mathcal{N}}_{k}^{\\prime}}\\bigl{(}1-{\\mathrm{e}}^{{-(\\theta_{l}+\\theta_{i})}}%&#10;\\bigr{)}\\biggr{]},\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></munder></mstyle><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>k</mi></msub></mrow></munder></mstyle><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03b8</mi><mi>j</mi></msub><mo>+</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>l</mi><mo>\u2208</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>k</mi><mo>\u2032</mo></msubsup></mrow></munder></mstyle><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mn>1</mn><mo>-</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03b8</mi><mi>l</mi></msub><mo>+</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $P(q)$ is the prescribed degree distribution. It is unlikely that $P(k=q|q)\\equiv1$ in Eq.~{(\\ref{{pkq}})}, and thus we can not expect $P(k)\\equiv P(q)$.  To find the general characteristics of $P(k)$ from Eq.~{(\\ref{{linPk}})} in comparison with $P(q)$, we performed a Monte Carlo simulation (using the Metropolis-Hastings method described above) of ${H_{LD}}$ for a network of $n=500$ and $P(q=5)=P(q=15)={1\\over2}$ for illustrative purposes, whose results are shown in Fig.~\\ref{01_degree}. In the figure, the prescribed $P(q)$ is shown in gray, and the equilibrium $P(k)$ is shown in blue. While $P(k)$ does exhibit peaks at $k=5$ and $k=15$, it also shows a fairly wide distribution (although small in comparison with $n$), and the fluctuation is visibly larger at $k=15$ resulting in a lower peak.  If the specified degree $q$ had been the same for all nodes (i.e. a $q$-regular graph) it is well known that ${H_{LD}}$ creates an Erd\\\"os-R\\'enyi graph with a Poissonian degree distribution, locally not unlike the peaks in Fig.~\\ref{01_degree}~\\cite{Park:2004sm}. Thus we call the peaks we see in Fig.~\\ref{01_degree} Poisson-like.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{01_degree.eps}\n\\caption{The degree distributions from exponential random graph simulations. For simplicity we set the specified degree distribution to be $P(q=5)={1\\over2}$ and $P(q=15)={1\\over2}$ (shown in gray). The linear degree Hamiltonian ${H_{LD}}=-\\sum_{i\\in{\\mathcal{N}}}k_i\\ln q_i$ generates a smooth distribution over a wide range of degrees with Poissonian-like peaks at $k=5$ and $k=15$ (blue). The degree distribution from the optimization degree Hamiltonian ${H_{OD}}=\\sum_{i\\in{\\mathcal{N}}}|k_i-q_i|$, by contrast, is noticeably closer to the specified, with sharper peaks of roughly equal heights at $k=5$ and $k=10$.}\n\\label{01_degree}\n\\end{figure}\n\nThe well-documented success of the configuration model implies that the fluctuations we see in $P(k)$ may not be problematic in general, though in certain circumstances (we see later such as a case) a more faithful reproduction of the specified degree distribution may be desired. This means that a graph Hamiltonian is needed that imposes a larger penalty when $k_i$ deviates from $q_i$ than ${H_{LD}}$ does.  It is unclear how ${H_{LD}}$ can be modified while retaining the linear form. Instead, we introduce a nonlinear Hamiltonian\n\n", "itemtype": "equation", "pos": 12354, "prevtext": "\nwhere $\\theta_i=-\\ln q_i/A(n-1)$, and ${\\{{{\\mathcal{N}}_k}\\}}$ is the set of all possible combinations of $k$ nodes from ${\\mathcal{N}}$ excluding $i$. From this, the total degree distribution $P(k)$ in equilibrium is given as\n\n", "index": 13, "text": "\\begin{align}\nP(k) = \\sum_{{\\{{q}\\}}}P(k|q)P(q),\n\\label{linPk}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(k)=\\sum_{{\\{{q}\\}}}P(k|q)P(q),\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo stretchy=\"false\">}</mo></mrow></munder></mstyle><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhich we call the {\\textbf}{Optimization Degree Hamiltonian}, being reminiscent of Hamiltonians used in certain optimization problems such as number partitioning~\\cite{Mertens:1998sd}~\\footnote{Note that the so-called ``curved'' Exponential Random Graph Model is similar to our formalism in that the graph Hamiltonians are nonlinear functions of network variables~\\cite{Hunter:2008}.}. The $P(k)$ that results from $H_{OD}$ with $\\beta=1$ for simplicity (the penalty can be controlled via $\\beta_d$ when necessary) is shown in Fig.~\\ref{01_degree} in red, which is indeed a more faithful reproduction of $P(q)$ in comparison with ${H_{LD}}$, showing sharper peaks at $k=5$ and $k=15$ of equal heights similar to $P(q)$. The broadening of the peaks around the specified degrees from the ${H_{LD}}$ in comparison to ${H_{OD}}$ in Fig.~\\ref{01_degree} is persistent in cases of more heterogeneous (thus less artificial) specified $P(q)$ as seen in Fig.~\\ref{01A_smoothdegree} where we compare ${H_{LD}}$ and ${H_{OD}}$ for a Poissonian $P(q)$ and a double Gaussian\n\n", "itemtype": "equation", "pos": 14824, "prevtext": "\nwhere $P(q)$ is the prescribed degree distribution. It is unlikely that $P(k=q|q)\\equiv1$ in Eq.~{(\\ref{{pkq}})}, and thus we can not expect $P(k)\\equiv P(q)$.  To find the general characteristics of $P(k)$ from Eq.~{(\\ref{{linPk}})} in comparison with $P(q)$, we performed a Monte Carlo simulation (using the Metropolis-Hastings method described above) of ${H_{LD}}$ for a network of $n=500$ and $P(q=5)=P(q=15)={1\\over2}$ for illustrative purposes, whose results are shown in Fig.~\\ref{01_degree}. In the figure, the prescribed $P(q)$ is shown in gray, and the equilibrium $P(k)$ is shown in blue. While $P(k)$ does exhibit peaks at $k=5$ and $k=15$, it also shows a fairly wide distribution (although small in comparison with $n$), and the fluctuation is visibly larger at $k=15$ resulting in a lower peak.  If the specified degree $q$ had been the same for all nodes (i.e. a $q$-regular graph) it is well known that ${H_{LD}}$ creates an Erd\\\"os-R\\'enyi graph with a Poissonian degree distribution, locally not unlike the peaks in Fig.~\\ref{01_degree}~\\cite{Park:2004sm}. Thus we call the peaks we see in Fig.~\\ref{01_degree} Poisson-like.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{01_degree.eps}\n\\caption{The degree distributions from exponential random graph simulations. For simplicity we set the specified degree distribution to be $P(q=5)={1\\over2}$ and $P(q=15)={1\\over2}$ (shown in gray). The linear degree Hamiltonian ${H_{LD}}=-\\sum_{i\\in{\\mathcal{N}}}k_i\\ln q_i$ generates a smooth distribution over a wide range of degrees with Poissonian-like peaks at $k=5$ and $k=15$ (blue). The degree distribution from the optimization degree Hamiltonian ${H_{OD}}=\\sum_{i\\in{\\mathcal{N}}}|k_i-q_i|$, by contrast, is noticeably closer to the specified, with sharper peaks of roughly equal heights at $k=5$ and $k=10$.}\n\\label{01_degree}\n\\end{figure}\n\nThe well-documented success of the configuration model implies that the fluctuations we see in $P(k)$ may not be problematic in general, though in certain circumstances (we see later such as a case) a more faithful reproduction of the specified degree distribution may be desired. This means that a graph Hamiltonian is needed that imposes a larger penalty when $k_i$ deviates from $q_i$ than ${H_{LD}}$ does.  It is unclear how ${H_{LD}}$ can be modified while retaining the linear form. Instead, we introduce a nonlinear Hamiltonian\n\n", "index": 15, "text": "\\begin{align}\nH_{OD}(G) = \\sum_{i\\in{\\mathcal{N}}}\\beta_d|k_i(G)-q_i|\n\\label{optham}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H_{OD}(G)=\\sum_{i\\in{\\mathcal{N}}}\\beta_{d}|k_{i}(G)-q_{i}|\" display=\"inline\"><mrow><mrow><msub><mi>H</mi><mrow><mi>O</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>\u03b2</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $\\Phi(q,\\mu,\\sigma)$ is a Gaussian of mean $\\mu$ and variance $\\sigma^2$, and $\\alpha\\in[0,1]$ sets the relative weights between the two Gaussian peaks. For the Poissonian case we set ${\\langle{q}\\rangle}=10$ (Fig.~\\ref{01A_smoothdegree}~(a)), and for the double Gaussian we try three cases of varying weights and variances (Fig.~\\ref{01A_smoothdegree}~(b)-(d)). The behavior of ${H_{LD}}$ and ${H_{OD}}$ are consistent with what we see from Fig.~\\ref{01_degree}: in terms of the goodness of fit to $P(q)$ (including the relative heights at the peaks) ${H_{OD}}$ is superior to ${H_{LD}}$~\\footnote{For the purposes of this paper we are showing some numerical examples. For more systematic studies one could investigate various moments of the degree distributions from the two Hamiltonians, or a difference measure between two distributions $P$ and $Q$ such as $D(P,Q)\\equiv\\sum_k|P(k)-Q(k)|$.}.\n\n\\begin{figure*}[t]\n\\includegraphics[width=140mm]{01A_smoothdegree.eps}\n\\caption{The degree distributions generated via ${H_{LD}}$ and ${H_{OD}}$ for heterogeneous specified degree distribution. (a) With $P(q)$ a Poissonian (with ${\\langle{q}\\rangle}=10$). In (b)--(d) $P(q)$ is a double Gaussian with peaks at $q=5$ and $q=10$ with varying relative heights ($\\alpha\\in[0,1]$ for the peak at $q=5$, and $1-\\alpha$ for the peak at $q=10$) and variances $\\sigma_1$, $\\sigma_2$ of the peaks. (b) $(\\alpha,\\sigma_1,\\sigma_2)=(0.5,1.0,1.0)$. This is the most similar to Fig.~\\ref{01_degree}. (c) $(\\alpha,\\sigma_1,\\sigma_2)=(0.75,1.0,1.0)$ and (d) $(\\alpha,\\sigma_1,\\sigma_2)=(0.5,1.0,5.0)$. Here, ${H_{OD}}$ again consistently reproduce $P(q)$ more faithfully.}\n\\label{01A_smoothdegree}\n\\end{figure*}\n\n\\section{Robustness of Degree Reproduction under perturbation: targeted clustering}\nBesides degree distribution, a network characteristic that has been widely studied is clustering. Intuitively, a clustered network contains significantly more triadic closures (triangles) than expected in a random graph with the same number of edges. (A common definition of the strength of clustering of a network is given using the so-called \\emph{clustering coefficient}, which we present later). \n\nIn exponential random graph literature, studies have been made on graph Hamiltonians that incorporate the number of triangles $T$ linearly, the simplest case being the Strauss Model with $H_S(G)=\\theta M+\\tau T$~\\cite{Strauss:1986}. The motivation for $H_S$ is that by controlling $\\theta$ and $\\tau$, one could hopefully generate a network with any desired value of $M$ and $T$, i.e. a smooth, controllable transition between a non-clustered configuration (small $T$) and a clustered one (large $T$). Unfortunately, it has been shown that $H_S$ does not show such a behavior: depending on $\\theta$ and $\\tau$, the system undergoes a first-order phase transition from a sparse ER-like phase with vanishing clustering and a nearly-fully connected phase~\\cite{Strauss:1986,Park:2005}, of which most real networks are neither.  More recently, Foster~{\\textit{et al.}}~performed an extensive study of the Hamiltonian $H(G)=\\tau T$ on an ensemble of networks of fixed degree sequences (and thus a fixed number of edges), and found that as $\\tau$ is tuned, $T$ shows a series of jumps consisting of first-order phase transitions~\\cite{Foster:2010}, each transition indicating the formation of densely connected local cliques.\n\nThis pathology renders the linear Hamiltonian for modeling real clustered networks, where the triangles are distributed over the the network without such extreme ``condensation'' of triangles.  The lack of such an intermediate phase in $H_S$ stems from the fact that the addition of a single edge in an already densely connected part of the network can lead to a disproportionately large increase in $T$ and decrease in $H_S(G)$, resulting in the condensed phase energetically favorable. Therefore, it is understood that a Hamiltonian or, more generally, a mathematical formalism is necessary that explicitly discourages such condensation~\\cite{Park:2005,Newman:2009}.\n\nBefore we find such Hamiltonian in our context of exponential random graphs, let us first review how clustering in networks is quantified. It is often done via the \\emph{clustering coefficient} $C$. In wide use are three versions, one local (node-level) and two global (network-wide). On the individual node level, the \\emph{local} clustering coefficient is defined as\n\n", "itemtype": "equation", "pos": 15983, "prevtext": "\nwhich we call the {\\textbf}{Optimization Degree Hamiltonian}, being reminiscent of Hamiltonians used in certain optimization problems such as number partitioning~\\cite{Mertens:1998sd}~\\footnote{Note that the so-called ``curved'' Exponential Random Graph Model is similar to our formalism in that the graph Hamiltonians are nonlinear functions of network variables~\\cite{Hunter:2008}.}. The $P(k)$ that results from $H_{OD}$ with $\\beta=1$ for simplicity (the penalty can be controlled via $\\beta_d$ when necessary) is shown in Fig.~\\ref{01_degree} in red, which is indeed a more faithful reproduction of $P(q)$ in comparison with ${H_{LD}}$, showing sharper peaks at $k=5$ and $k=15$ of equal heights similar to $P(q)$. The broadening of the peaks around the specified degrees from the ${H_{LD}}$ in comparison to ${H_{OD}}$ in Fig.~\\ref{01_degree} is persistent in cases of more heterogeneous (thus less artificial) specified $P(q)$ as seen in Fig.~\\ref{01A_smoothdegree} where we compare ${H_{LD}}$ and ${H_{OD}}$ for a Poissonian $P(q)$ and a double Gaussian\n\n", "index": 17, "text": "\\begin{align}\nP(q)=\\alpha \\Phi(q;\\mu_1,\\sigma_1)+(1-\\alpha)\\Phi(q;\\mu_2,\\sigma_2)\n\\label{Pqdoublegauss}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(q)=\\alpha\\Phi(q;\\mu_{1},\\sigma_{1})+(1-\\alpha)\\Phi(q;\\mu_{2},%&#10;\\sigma_{2})\" display=\"inline\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>;</mo><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c3</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>;</mo><msub><mi>\u03bc</mi><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c3</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $t_i$ is the number of triangles of which the node $i$ is at a corner, and $s(k_i)={k_i\\choose 2}$ is the number of pairs of neighbors of node $i$, also called a two-star centered on $i$. $C_i$ is therefore the probability that two neighbors of node $i$ are themselves neighbors. The global measure of clustering is commonly given by two measures. One is the average of $C_i$ which we write $\\overline{C}$, defined as $\\overline{C}\\equiv{\\langle{C_i}\\rangle}=\\sum_{i\\in{\\mathcal{N}}}C_i/N$, i.e. the average of the local clustering coefficients. The other, which we call ${\\widetilde{C}}$, is defined as\n\n", "itemtype": "equation", "pos": 20545, "prevtext": "\nwhere $\\Phi(q,\\mu,\\sigma)$ is a Gaussian of mean $\\mu$ and variance $\\sigma^2$, and $\\alpha\\in[0,1]$ sets the relative weights between the two Gaussian peaks. For the Poissonian case we set ${\\langle{q}\\rangle}=10$ (Fig.~\\ref{01A_smoothdegree}~(a)), and for the double Gaussian we try three cases of varying weights and variances (Fig.~\\ref{01A_smoothdegree}~(b)-(d)). The behavior of ${H_{LD}}$ and ${H_{OD}}$ are consistent with what we see from Fig.~\\ref{01_degree}: in terms of the goodness of fit to $P(q)$ (including the relative heights at the peaks) ${H_{OD}}$ is superior to ${H_{LD}}$~\\footnote{For the purposes of this paper we are showing some numerical examples. For more systematic studies one could investigate various moments of the degree distributions from the two Hamiltonians, or a difference measure between two distributions $P$ and $Q$ such as $D(P,Q)\\equiv\\sum_k|P(k)-Q(k)|$.}.\n\n\\begin{figure*}[t]\n\\includegraphics[width=140mm]{01A_smoothdegree.eps}\n\\caption{The degree distributions generated via ${H_{LD}}$ and ${H_{OD}}$ for heterogeneous specified degree distribution. (a) With $P(q)$ a Poissonian (with ${\\langle{q}\\rangle}=10$). In (b)--(d) $P(q)$ is a double Gaussian with peaks at $q=5$ and $q=10$ with varying relative heights ($\\alpha\\in[0,1]$ for the peak at $q=5$, and $1-\\alpha$ for the peak at $q=10$) and variances $\\sigma_1$, $\\sigma_2$ of the peaks. (b) $(\\alpha,\\sigma_1,\\sigma_2)=(0.5,1.0,1.0)$. This is the most similar to Fig.~\\ref{01_degree}. (c) $(\\alpha,\\sigma_1,\\sigma_2)=(0.75,1.0,1.0)$ and (d) $(\\alpha,\\sigma_1,\\sigma_2)=(0.5,1.0,5.0)$. Here, ${H_{OD}}$ again consistently reproduce $P(q)$ more faithfully.}\n\\label{01A_smoothdegree}\n\\end{figure*}\n\n\\section{Robustness of Degree Reproduction under perturbation: targeted clustering}\nBesides degree distribution, a network characteristic that has been widely studied is clustering. Intuitively, a clustered network contains significantly more triadic closures (triangles) than expected in a random graph with the same number of edges. (A common definition of the strength of clustering of a network is given using the so-called \\emph{clustering coefficient}, which we present later). \n\nIn exponential random graph literature, studies have been made on graph Hamiltonians that incorporate the number of triangles $T$ linearly, the simplest case being the Strauss Model with $H_S(G)=\\theta M+\\tau T$~\\cite{Strauss:1986}. The motivation for $H_S$ is that by controlling $\\theta$ and $\\tau$, one could hopefully generate a network with any desired value of $M$ and $T$, i.e. a smooth, controllable transition between a non-clustered configuration (small $T$) and a clustered one (large $T$). Unfortunately, it has been shown that $H_S$ does not show such a behavior: depending on $\\theta$ and $\\tau$, the system undergoes a first-order phase transition from a sparse ER-like phase with vanishing clustering and a nearly-fully connected phase~\\cite{Strauss:1986,Park:2005}, of which most real networks are neither.  More recently, Foster~{\\textit{et al.}}~performed an extensive study of the Hamiltonian $H(G)=\\tau T$ on an ensemble of networks of fixed degree sequences (and thus a fixed number of edges), and found that as $\\tau$ is tuned, $T$ shows a series of jumps consisting of first-order phase transitions~\\cite{Foster:2010}, each transition indicating the formation of densely connected local cliques.\n\nThis pathology renders the linear Hamiltonian for modeling real clustered networks, where the triangles are distributed over the the network without such extreme ``condensation'' of triangles.  The lack of such an intermediate phase in $H_S$ stems from the fact that the addition of a single edge in an already densely connected part of the network can lead to a disproportionately large increase in $T$ and decrease in $H_S(G)$, resulting in the condensed phase energetically favorable. Therefore, it is understood that a Hamiltonian or, more generally, a mathematical formalism is necessary that explicitly discourages such condensation~\\cite{Park:2005,Newman:2009}.\n\nBefore we find such Hamiltonian in our context of exponential random graphs, let us first review how clustering in networks is quantified. It is often done via the \\emph{clustering coefficient} $C$. In wide use are three versions, one local (node-level) and two global (network-wide). On the individual node level, the \\emph{local} clustering coefficient is defined as\n\n", "index": 19, "text": "\\begin{align}\nC_i \\equiv \\frac{t_i}{s(k_i)} = \\frac{t_i}{{1\\over2} k_i(k_i-1)}~~~(k_i\\le2),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{i}\\equiv\\frac{t_{i}}{s(k_{i})}=\\frac{t_{i}}{{1\\over 2}k_{i}(k%&#10;_{i}-1)}~{}~{}~{}(k_{i}\\leq 2),\" display=\"inline\"><mrow><msub><mi>C</mi><mi>i</mi></msub><mo>\u2261</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>t</mi><mi>i</mi></msub><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>=</mo><mpadded width=\"+9.9pt\"><mstyle displaystyle=\"true\"><mfrac><msub><mi>t</mi><mi>i</mi></msub><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mpadded><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo>\u2264</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $T=\\frac{1}{3}\\sum_{i\\in{\\mathcal{N}}}t_i$ is again the number of triangles in the network. Therefore this is the probability that a randomly selected two-star is a part of a triangle ($3$ exists in the numerator because one triangle contains three two-stars).  Although ${\\widetilde{C}}$ and $\\overline{C}$ are not identical, ${\\widetilde{C}}=\\overline{C}$ when $C_i=C_0$ for all $i$. In terms of these quantities, the aforementioned behavior of the Strauss Hamiltonian $H_S=\\theta M+\\tau T$ can be summarized as the clustering coefficient (local or global) being either ${\\widetilde{C}}(\\mathrm{or~}\\overline{C})\\simeq0$ (sparse ER-like phase) or ${\\widetilde{C}}(\\mathrm{or~}\\overline{C})\\simeq1$ (condensed phase) or, in other words, $t_i\\simeq0$ or $t_i\\simeq s(k_i)$ for all $i$ while in a network of intermediate clustering coefficient $C$, $t_i$ would be $\\sim Cs(k_i)$. Taking a cue from the latter and Eq.~{(\\ref{{optham}})}, we propose the following nonlinear targeted clustering Hamiltonian\n\n", "itemtype": "equation", "pos": 21259, "prevtext": "\nwhere $t_i$ is the number of triangles of which the node $i$ is at a corner, and $s(k_i)={k_i\\choose 2}$ is the number of pairs of neighbors of node $i$, also called a two-star centered on $i$. $C_i$ is therefore the probability that two neighbors of node $i$ are themselves neighbors. The global measure of clustering is commonly given by two measures. One is the average of $C_i$ which we write $\\overline{C}$, defined as $\\overline{C}\\equiv{\\langle{C_i}\\rangle}=\\sum_{i\\in{\\mathcal{N}}}C_i/N$, i.e. the average of the local clustering coefficients. The other, which we call ${\\widetilde{C}}$, is defined as\n\n", "index": 21, "text": "\\begin{align}\n{\\widetilde{C}} \\equiv \\frac{3T}{\\sum_{i\\in{\\mathcal{N}}}s(k_i)}=\\frac{3T}{\\sum_i{1\\over2} k_i(k_i-1)},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\widetilde{C}}\\equiv\\frac{3T}{\\sum_{i\\in{\\mathcal{N}}}s(k_{i})}=%&#10;\\frac{3T}{\\sum_{i}{1\\over 2}k_{i}(k_{i}-1)},\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>C</mi><mo>~</mo></mover><mo>\u2261</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>3</mn><mo>\u2062</mo><mi>T</mi></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></msub><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>3</mn><mo>\u2062</mo><mi>T</mi></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nwhere $\\gamma_i$ is now the specified (i.e. targeted) clustering coefficient for node $i$. The difference between ${H_C}$ and the model of Milo~{\\textit{et al.}}~\\cite{Milo:2002}, where $H_{\\textrm{Milo}}=|T-T'|$ and $T'$ is the specified number of triangles, is that $H_C$ allows us to control local clustering. Similar to ${H_{OD}}$ of Eq.~{(\\ref{{optham}})}, ${H_C}$ explicitly penalizes $t_i$ when it diverges from a prescribed value $\\gamma_is(k_i)$. In studying the effectiveness of ${H_C}$ in reproducing the specified local clustering, we would also like to have the option of controlling the degrees ${\\{{k_i}\\}}$ simultaneously.  We have already discussed two Hamiltonians designed specifically for that purpose, ${H_{LD}}$ and ${H_{OD}}$.  In the remainder of this paper, therefore, we study the following two composite Hamiltonians\n\n", "itemtype": "equation", "pos": 22398, "prevtext": "\nwhere $T=\\frac{1}{3}\\sum_{i\\in{\\mathcal{N}}}t_i$ is again the number of triangles in the network. Therefore this is the probability that a randomly selected two-star is a part of a triangle ($3$ exists in the numerator because one triangle contains three two-stars).  Although ${\\widetilde{C}}$ and $\\overline{C}$ are not identical, ${\\widetilde{C}}=\\overline{C}$ when $C_i=C_0$ for all $i$. In terms of these quantities, the aforementioned behavior of the Strauss Hamiltonian $H_S=\\theta M+\\tau T$ can be summarized as the clustering coefficient (local or global) being either ${\\widetilde{C}}(\\mathrm{or~}\\overline{C})\\simeq0$ (sparse ER-like phase) or ${\\widetilde{C}}(\\mathrm{or~}\\overline{C})\\simeq1$ (condensed phase) or, in other words, $t_i\\simeq0$ or $t_i\\simeq s(k_i)$ for all $i$ while in a network of intermediate clustering coefficient $C$, $t_i$ would be $\\sim Cs(k_i)$. Taking a cue from the latter and Eq.~{(\\ref{{optham}})}, we propose the following nonlinear targeted clustering Hamiltonian\n\n", "index": 23, "text": "\\begin{align}\n{H_C} = \\sum_{i\\in{\\mathcal{N}}}\\beta_c|t_i-\\gamma_is(k_i)| = \\sum_{i\\in{\\mathcal{N}}}\\beta_c\\bigl|t_i-\\gamma_i{1\\over2} k_i(k_i-1)\\bigr|,\n\\label{pertclus}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{H_{C}}=\\sum_{i\\in{\\mathcal{N}}}\\beta_{c}|t_{i}-\\gamma_{i}s(k_{i}%&#10;)|=\\sum_{i\\in{\\mathcal{N}}}\\beta_{c}\\bigl{|}t_{i}-\\gamma_{i}{1\\over 2}k_{i}(k_%&#10;{i}-1)\\bigr{|},\" display=\"inline\"><mrow><mrow><msub><mi>H</mi><mi>C</mi></msub><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>\u03b2</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>i</mi></msub><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><msub><mi>\u03b2</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>i</mi></msub><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nto find out whether either is capable of generating network ensembles exhibiting both the specified degrees and local clustering. \n\nA Monte Carlo simulation was performed for a network of size $n=500$ and ${\\langle{k}\\rangle}=10$. For simplicity, we again set $\\beta_d=\\beta_c=1$, $P(q)=\\delta_{q,10}$ (a regular graph), and ${\\gamma_i}={C_{\\textrm{target}}}$, a universal value for all $i$, varied between $0$ and $1$. First, Fig.~\\ref{02_clustering} shows the mean global clustering ${\\langle{{\\widetilde{C}}}\\rangle}$ from the simulation, which shows us that both $H_1$ and $H_2$ ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ generate networks with the specified clustering. This arises from the fact that ${\\langle{C_i}\\rangle}\\simeq{C_{\\textrm{target}}}$ on the individual level as well (not shown).  The difference between the $H_1$ and $H_2$, however, is most striking in the equilibrium $P(k)$, shown in Fig.~\\ref{03_simdegree}. When ${C_{\\textrm{target}}}=0$, perturbation $H_C$ is insignificant since the expected clustering without it is $0$ anyway, and therefore $P(k)$ are simply as expected -- a true Poissonian for $H_1$, and a sharper peak at $q=10$ for $H_2$, similar to the ones we saw in Fig.~\\ref{01_degree}.  When ${C_{\\textrm{target}}}\\ne0$, on the other hand, the peak in $P(k)$ under $H_1$ gradually shifts towards a smaller $k$ while high-degree nodes are created in order to compensate for the number of edges $M$ which is a constant. As ${C_{\\textrm{target}}}$ is tuned higher it resembles the specified distribution less and less, and at ${C_{\\textrm{target}}}\\simeq0.4$ we even observe multiple peaks (at $k=5$ and $15$ -- the values for which $|t-{C_{\\textrm{target}}} s(k)|=0$, meaning the peaks will shift for a different ${C_{\\textrm{target}}}$ and thus are not very meaningful). $P(k)$ under $H_2$, in contrast, is robust, without noticeable change up to ${C_{\\textrm{target}}}=0.5$, already an unusually high value for real-world networks, until it too shows similar (but milder) behavior at a higher value of ${C_{\\textrm{target}}}\\sim 0.6$ and up~\\footnote{We performed similar simulations for the four heterogeneous $P(q)$ shown in Fig.~\\ref{01A_smoothdegree} and found similar results.}.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{02_clustering.eps}\n\\caption{The global clustering ${\\langle{{\\widetilde{C}}}\\rangle}$ of graph ensembles generated from the linear (blue) and the optimization (red) degree Hamiltonians perturbed with targeted clustering Hamiltonian $H_C=\\sum_{i\\in{\\mathcal{N}}}|t_i-{C_{\\textrm{target}}} s(k_i)|$. ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ is the result of node-level local clustering coefficients being $\\simeq{C_{\\textrm{target}}}$, regardless of the degree distribution.}\n\\label{02_clustering}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\includegraphics[width=180mm]{03_simdegree.eps}\n\\caption{The equilibrium degree distributions $P(k)$ generated from $H_1={H_{LD}}+{H_C}$ (blue) and $H_2={H_{OD}}+{H_C}$ (red) for various values of ${C_{\\textrm{target}}}$. The specified degree distribution $P(q)=\\delta_{q,10}$ is in gray. When ${C_{\\textrm{target}}}=0$, both Hamiltonians generate their natural $P(k)$ -- a true Poissonian for ${H_{LD}}$, and a sharp peak for ${H_{OD}}$. As ${C_{\\textrm{target}}}$ is tuned higher, however, $P(k)$ peaks at a smaller $k$ for ${H_{LD}}+{H_C}$ and even exhibits multiple peaks when ${C_{\\textrm{target}}}$ is too large, while it stays virtually unchanged for ${H_{OD}}+{H_C}$ up to ${C_{\\textrm{target}}}\\simeq0.5$, an unusually high value in real networks.}\n\\label{03_simdegree}\n\\end{figure*}\n\nLet us now discuss the implications of the findings in Figs.~\\ref{02_clustering}~and~\\ref{03_simdegree} on the topology of networks generated from $H_1={H_{LD}}+{H_C}$ and $H_2={H_{LD}}+{H_C}$. First of all, Fig.~\\ref{02_clustering} tells us that, unlike the Strauss clustering perturbation $\\tau T$, ${H_C}$ was able to discourage an extreme condensation of triangles, resulting in ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ by way of $C_i\\simeq{C_{\\textrm{target}}}$ for both $H_1$ and $H_2$. However it was not enough to completely overcome the cooperative tendency of triangles under ${H_{LD}}$. The telltale sign of this is the creation of high-degree nodes Fig.~\\ref{03_simdegree} which shows the creation of high-degree nodes  in $H_1$: now many triangles exist between the high-degree nodes, forming a core of densely interconnected high-degree nodes although $C_i\\simeq{C_{\\textrm{target}}}$ as specified. On the other hand, under $H_2$ where $P(k)$ is sharply peaked at the specified degree $q=10$ such cores does not exist; with $k_i\\simeq q$ and $C_i\\simeq{C_{\\textrm{target}}}$ for all $i$ as specified, $H_2$ generates a network that truly has a uniform distribution of triangles, lacking any unspecified, accidental local structures.\n\n\\begin{figure*}[t]\n\\includegraphics[width=180mm]{04_degcortopo.eps}\n\\caption{(a) The degree-degree correlation ${r_{\\mathrm{deg}}}$ under $H_1={H_{LD}}+{H_C}$ and $H_2={H_{OD}}+{H_C}$. $H_1$ generates positive degree correlation for any positive ${C_{\\textrm{target}}}$, while $H_2$ exhibits very little correlation up to ${C_{\\textrm{target}}}\\simeq0.5$. (b) The mean corner degree of triangles contained in the networks in equilibrium. Under $H_1$ most triangles exist between high-degree nodes, indicating the persistence of the cooperative nature of triangles. (c) Equilibrium topologies of clustered networks under $H_1$ (left) and $H_2$ (right). $H_1$ generates a core of high-degree nodes that are densely connected and share a large number of triangles (enclosed in orange oval). $H_2$, in contrast, maintains the specified degree distribution $P(q)=\\delta_{q,10}$ while the triangles are distributed uniformly, features expected of a maximally random configuration given the the degree and local clustering constraints.}\n\\label{04_degcortopo}\n\\end{figure*}\n\nWe check our claim via the following two quantities: the degree-degree correlation ${r_{\\mathrm{deg}}}$ (the Pearson correlation between the degrees of adjacent nodes) and the {\\textbf}{mean corner degree} of the triangles in the network, shown in Figs~\\ref{04_degcortopo}~(a)~and~(b). First, the plot of ${\\langle{{r_{\\mathrm{deg}}}}\\rangle}$ in Fig.~\\ref{04_degcortopo}~(a) indicates  that adjacent degrees in the network are highly correlated under $H_1$, so that high-degree nodes are indeed connected with other high-degree nodes and vice versa, while $H_2$ shows no such effect. This leads naturally to what we see in Fig.~\\ref{04_degcortopo}~(b): under $H_1$, the mean corner degree is significantly higher than ${\\langle{k}\\rangle}=q$, unlike $H_2$ where it is practically equal to $q$. These observations are presented visually in Fig.~\\ref{04_degcortopo}~(c) (an actual snapshot of an equilibrium configuration of a network with $n=50$, $P(q)=\\delta_{q,5}$, and ${C_{\\textrm{target}}}=0.4$. ${\\langle{{\\widetilde{C}}}\\rangle}$ are $0.35\\pm0.02$ and $0.31\\pm0.02$, respectively). As expected, for $H_1$ (left) we clearly see that the ten highest-degree nodes (blue, average degree $9.7$) forms a densely interconnected core (encircled in orange), with ten lowest-degree nodes (yellow, average degree $1.0$) pushed to the periphery with low triangle participation rate. For $H_2$ (right), no significant difference between highest- and lowest-degree nodes exists, and the triangles are distributed uniformly, expected of a maximally random configuration given the degree and local clustering constraints.\n\n\\section{Discussion and Future Directions}\nHere we have studied two forms of graph Hamiltonian in exponential random graph theory that take node degrees and local clustering as specified input. The tendency of triangles to coalesce in the Strauss model was shown to persist when the linear clustering perturbation was replaced by an optimized clustering form, albeit in a milder fashion, rendering the composite Hamiltonian unable to generate the specified degree distribution~\\footnote{The reverse case of ${H_{OD}}+\\tau T$ was also numerically studied with varying $\\tau$. As $\\tau$ is tuned more negative (thus favoring triangles) here also occurs a sudden onset of the emergence of a densely connected cluster of high-degree nodes, signifying a condensation of triangles similar to ${H_{LD}}+\\tau T$. This demonstrates that to create finite clustering degree and local clustering optimization are necessary.}. The optimization degree Hamiltonian, on the other hand, was able to satisfy both, exhibiting significant robustness under the same perturbation.\n\nThat the optimization Hamiltonian form was able to reproduce both the targeted degree and clustering presents an appealing possibility from the viewpoint of network modeling via exponential random graph theory: given a set of network variables $\\Phi={\\{{\\phi_v|v=1,\\cdots,l}\\}}$, it may act as a practical computational method to generate a null model of network data with actual values of the variables ${\\{{\\tilde{\\phi}_v|v=1,\\ldots,l}\\}}$ using the Hamiltonian~\\cite{Foster:2010}\n\n", "itemtype": "equation", "pos": 23424, "prevtext": "\nwhere $\\gamma_i$ is now the specified (i.e. targeted) clustering coefficient for node $i$. The difference between ${H_C}$ and the model of Milo~{\\textit{et al.}}~\\cite{Milo:2002}, where $H_{\\textrm{Milo}}=|T-T'|$ and $T'$ is the specified number of triangles, is that $H_C$ allows us to control local clustering. Similar to ${H_{OD}}$ of Eq.~{(\\ref{{optham}})}, ${H_C}$ explicitly penalizes $t_i$ when it diverges from a prescribed value $\\gamma_is(k_i)$. In studying the effectiveness of ${H_C}$ in reproducing the specified local clustering, we would also like to have the option of controlling the degrees ${\\{{k_i}\\}}$ simultaneously.  We have already discussed two Hamiltonians designed specifically for that purpose, ${H_{LD}}$ and ${H_{OD}}$.  In the remainder of this paper, therefore, we study the following two composite Hamiltonians\n\n", "index": 25, "text": "\\begin{align}\n\tH_1 &= {H_{LD}}+H_C = \\sum_{i\\in{\\mathcal{N}}}\\bigl[-k_i\\ln q_i+\\beta_c|t_i-\\gamma_i s(k_i)|\\bigr] \\\\\n\t\\textrm{and} \\nonumber \\\\\n\tH_2 &= {H_{OD}}+H_C = \\sum_{i\\in{\\mathcal{N}}}\\bigl[\\beta_d|k_i-q_i|+\\beta_c|t_i-\\gamma_i s(k_i)|\\bigr]\n\\label{compham}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H_{1}\" display=\"inline\"><msub><mi>H</mi><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={H_{LD}}+H_{C}=\\sum_{i\\in{\\mathcal{N}}}\\bigl{[}-k_{i}\\ln q_{i}+%&#10;\\beta_{c}|t_{i}-\\gamma_{i}s(k_{i})|\\bigr{]}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>H</mi><mrow><mi>L</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>+</mo><msub><mi>H</mi><mi>C</mi></msub></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b2</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>i</mi></msub><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H_{2}\" display=\"inline\"><msub><mi>H</mi><mn>2</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={H_{OD}}+H_{C}=\\sum_{i\\in{\\mathcal{N}}}\\bigl{[}\\beta_{d}|k_{i}-q%&#10;_{i}|+\\beta_{c}|t_{i}-\\gamma_{i}s(k_{i})|\\bigr{]}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>H</mi><mrow><mi>O</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo>+</mo><msub><mi>H</mi><mi>C</mi></msub></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><msub><mi>\u03b2</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>k</mi><mi>i</mi></msub><mo>-</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b2</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>\u03b3</mi><mi>i</mi></msub><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02331.tex", "nexttext": "\nthereby enabling the modeler to assess quickly the sufficiency of the particular set of variables in characterizing the network. An interesting recent application of a related framework was provided by Foster~{\\textit{et al.}}~\\cite{Foster:2011}: specifically, they generated networks with specified global clustering coefficient ${\\widetilde{C}}$ or degree-degree correlation $r$ using the optimization hamiltonian and measured their effect on each other and the modular structure of the network (although they kept the the degree sequence fixed as the network data). In doing so, they demonstrated the utility of the Hamiltonian of the form Eq.~{(\\ref{{genoptham}})} in creating network ensembles with desired characteristics. Naturally, more study must be made on the properties of Eq.~{(\\ref{{genoptham}})} in relation to various network variables --- global as well as local --- in order to establish its general utility, and also the more recent. In light of the fact that new, complex measures of network properties are frequently devised and introduced, we hope the formalism proves to be a useful tool for network scientists.\n\n\\begin{acknowledgments}\nThe author would like to thank Doochul Kim for helpful comments. This work was supported by Kyung Hee University Grant KHU-20100116 and the Korea Research Foundation Grant KRF-20100004910.\n\\end{acknowledgments}\n\n\n\\begin{thebibliography}{100}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\\expandafter\\ifx\\csname bibnamefont\\endcsname\\relax\n  \\def\\bibnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname bibfnamefont\\endcsname\\relax\n  \\def\\bibfnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname citenamefont\\endcsname\\relax\n  \\def\\citenamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname url\\endcsname\\relax\n  \\def\\url#1{\\texttt{#1}}\\fi\n\\expandafter\\ifx\\csname urlprefix\\endcsname\\relax\\def\\urlprefix{URL }\\fi\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\eprint}[2][]{\\url{#2}}\n\n\n\\bibitem[{\\citenamefont{Barabasi}(2002)}]{Albert:2002rmp}\n\\bibinfo{author}{\\bibfnamefont{R.} \\bibnamefont{Albert}},\n\\bibinfo{author}{\\bibfnamefont{A.-L.} \\bibnamefont{Barab\\'asi}},\n  \\bibinfo{journal}{Review of Modern Physics}\n  \\textbf{\\bibinfo{volume}{74}}, \\bibinfo{pages}{47} (\\bibinfo{year}{2002}).\n\n\\bibitem[{\\citenamefont{Dorogovtsev}(2002)}]{Dorogovtsev:2003}\n\\bibinfo{author}{\\bibfnamefont{S.~N.} \\bibnamefont{Dorogovtsev}},\n\\bibinfo{author}{\\bibfnamefont{J.~F.~F.} \\bibnamefont{Mendes}},\n\\bibinfo{author}{\\bibfnamefont{A.-N.} \\bibnamefont{Samukhin}},\n  \\bibinfo{journal}{Nucl. Phys. B}\n  \\textbf{\\bibinfo{volume}{666}}, \\bibinfo{pages}{396} (\\bibinfo{year}{2003}).\n\n\n\\bibitem[{\\citenamefont{Newman}(2008)}]{Newman:2008}\n\\bibinfo{author}{\\bibfnamefont{M.~E.~J.} \\bibnamefont{Newman}},\n  \\bibinfo{journal}{Physics Today}\n  \\textbf{\\bibinfo{volume}{61}}, \\bibinfo{pages}{33}, \\bibinfo{pages}{066117} (\\bibinfo{year}{2008}).\n\n\\bibitem[{\\citenamefont{Frank}(1986)}]{Frank:1986}\n\\bibinfo{author}{\\bibfnamefont{O.} \\bibnamefont{Frank}},\n\\bibinfo{author}{\\bibfnamefont{D.} \\bibnamefont{Strauss}},\n  \\bibinfo{journal}{Journal of the American Statistical Association}\n  \\textbf{\\bibinfo{volume}{81}}, \\bibinfo{pages}{832} (\\bibinfo{year}{1986}).\n\n\\bibitem[{\\citenamefont{Park and Newman}(2004)}]{Park:2004sm}\n\\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Park}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.~E.~J.} \\bibnamefont{Newman}},\n  \\bibinfo{journal}{Physical Review E} \\textbf{\\bibinfo{volume}{70}}\n  (\\bibinfo{year}{2004}). \n\n\\bibitem[{\\citenamefont{Robins}(2007)}]{Robins:2007}\n\\bibinfo{author}{\\bibfnamefont{G.} \\bibnamefont{Robins}},\n\\bibinfo{author}{\\bibfnamefont{T.} \\bibnamefont{Snijders}},\n\\bibinfo{author}{\\bibfnamefont{P.} \\bibnamefont{Wang}},\n\\bibinfo{author}{\\bibfnamefont{M.} \\bibnamefont{Handcock}},\n\\bibinfo{author}{\\bibfnamefont{P.} \\bibnamefont{Pattison}},\n  \\bibinfo{journal}{Social Networks}\n  \\textbf{\\bibinfo{volume}{29}}, \\bibinfo{pages}{192} (\\bibinfo{year}{2007}).\n  \n\\bibitem[{\\citenamefont{Hunter}(2007)}]{Hunter:2007}\n\\bibinfo{author}{\\bibfnamefont{D.~R.} \\bibnamefont{Hunter}},\n  \\bibinfo{journal}{Social Networks}\n  \\textbf{\\bibinfo{volume}{29}}, \\bibinfo{pages}{216} (\\bibinfo{year}{2007}).\n\n\\bibitem[{\\citenamefont{Anderson}(1999)}]{Anderson:1999}\n\\bibinfo{author}{\\bibfnamefont{C.~J.} \\bibnamefont{Anderson}},\n\\bibinfo{author}{\\bibfnamefont{S.} \\bibnamefont{Wasserman}},\n\\bibinfo{author}{\\bibfnamefont{B.} \\bibnamefont{Crouch}},\n  \\bibinfo{journal}{Social Networks}\n  \\textbf{\\bibinfo{volume}{21}}, \\bibinfo{pages}{37} (\\bibinfo{year}{1999}).\n  \n\\bibitem[{\\citenamefont{Snijders}(2006)}]{Snijders:2006}\n\\bibinfo{author}{\\bibfnamefont{T.~A.~B.} \\bibnamefont{Snijders}},\n\\bibinfo{author}{\\bibfnamefont{P.} \\bibnamefont{Pattison}},\n\\bibinfo{author}{\\bibfnamefont{G.} \\bibnamefont{Robins}},\n\\bibinfo{author}{\\bibfnamefont{M.} \\bibnamefont{Handcock}},\n  \\bibinfo{journal}{Sociological Methodology}\n  \\textbf{\\bibinfo{volume}{36}}, \\bibinfo{pages}{99153} (\\bibinfo{year}{2006}).\n\n\\bibitem[{\\citenamefont{Hunter}(2008)}]{Hunter:2008}\n\\bibinfo{author}{\\bibfnamefont{D.} \\bibnamefont{Hunter}},\n\\bibinfo{author}{\\bibfnamefont{S.} \\bibnamefont{Goodreau}},\n\\bibinfo{author}{\\bibfnamefont{M.} \\bibnamefont{Handcock}},\n  \\bibinfo{journal}{Journal of the American Statistical Association}\n  \\textbf{\\bibinfo{volume}{103}}, \\bibinfo{pages}{248268} (\\bibinfo{year}{2008}).\n\n\\bibitem[{\\citenamefont{Metropolis}(1953)}]{Metropolis:1953}\n\\bibinfo{author}{\\bibfnamefont{N.} \\bibnamefont{Metropolis}},\n\\bibinfo{author}{\\bibfnamefont{A.~W.} \\bibnamefont{Rosenbluth}},\n\\bibinfo{author}{\\bibfnamefont{M.~N.} \\bibnamefont{Rosenbluth}},\n\\bibinfo{author}{\\bibfnamefont{A.~H.} \\bibnamefont{Teller}},\n\\bibinfo{author}{\\bibfnamefont{A.} \\bibnamefont{Teller}},\n  \\bibinfo{journal}{Journal of Chemical Physics}\n  \\textbf{\\bibinfo{volume}{21}}, \\bibinfo{pages}{1087} (\\bibinfo{year}{1953}).\n\n\\bibitem[{\\citenamefont{Park}(2010)}]{Park:2010}\n\\bibinfo{author}{\\bibfnamefont{J.} \\bibnamefont{Park}},\n  \\bibinfo{journal}{Journal of Statistical Mechanics}\n\t\\bibinfo{pages}{P04006} (\\bibinfo{year}{2010}).\n\n\\bibitem[{\\citenamefont{Newman}(2001)}]{Newman:2001arb}\n\\bibinfo{author}{\\bibfnamefont{M.~E.~J.} \\bibnamefont{Newman}},\n\\bibinfo{author}{\\bibfnamefont{S.~H.} \\bibnamefont{Strogatz}},\n\\bibinfo{author}{\\bibfnamefont{D.~J.} \\bibnamefont{Watts}},\n  \\bibinfo{journal}{Physical Review E}\n  \\textbf{\\bibinfo{volume}{64}}, \\bibinfo{pages}{026118} (\\bibinfo{year}{2001}).\n  \n\\bibitem[{\\citenamefont{Mertens}(1998)}]{Mertens:1998sd}\n\\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{Mertens}},\n  \\bibinfo{journal}{Physical Review Letters} \\textbf{\\bibinfo{volume}{81}},\n  \\bibinfo{pages}{4281} (\\bibinfo{year}{1998}).\n\n\\bibitem[{\\citenamefont{Strauss}(1986)}]{Strauss:1986}\n\\bibinfo{author}{\\bibfnamefont{D.} \\bibnamefont{Strauss}},\n  \\bibinfo{journal}{SIAM Review}\n  \\textbf{\\bibinfo{volume}{28}}, \\bibinfo{pages}{513} (\\bibinfo{year}{1986}).\n  \n\\bibitem[{\\citenamefont{Park}(2004)}]{Park:2005}\n\\bibinfo{author}{\\bibfnamefont{J.} \\bibnamefont{Park}},\n\\bibinfo{author}{\\bibfnamefont{M.~E.~J.} \\bibnamefont{Newman}},\n  \\bibinfo{journal}{Physical Review E}\n  \\textbf{\\bibinfo{volume}{72}}, \\bibinfo{pages}{026136} (\\bibinfo{year}{2005}).\n\n\\bibitem[{\\citenamefont{Foster}(2010)}]{Foster:2010}\n\\bibinfo{author}{\\bibfnamefont{D.} \\bibnamefont{Foster}},\n\\bibinfo{author}{\\bibfnamefont{J.} \\bibnamefont{Foster}},\n\\bibinfo{author}{\\bibfnamefont{M.} \\bibnamefont{Paczuski}},\n\\bibinfo{author}{\\bibfnamefont{P.} \\bibnamefont{Grassberger}},\n  \\bibinfo{journal}{Physical Review E}\n  \\textbf{\\bibinfo{volume}{81}}, \\bibinfo{pages}{046115} (\\bibinfo{year}{2010}).\n\n\\bibitem[{\\citenamefont{Newman}(2009)}]{Newman:2009}\n\\bibinfo{author}{\\bibfnamefont{M.~E.~J.} \\bibnamefont{Newman}},\n  \\bibinfo{journal}{Physical Review Letters}\n  \\textbf{\\bibinfo{volume}{103}}, \\bibinfo{pages}{058701} (\\bibinfo{year}{2009}).\n\n\\bibitem[{\\citenamefont{Milo}(2002)}]{Milo:2002}\n\\bibinfo{author}{\\bibfnamefont{R.} \\bibnamefont{Milo}},\n\\bibinfo{author}{\\bibfnamefont{S.} \\bibnamefont{Shen-Orr}},\n\\bibinfo{author}{\\bibfnamefont{S.} \\bibnamefont{Itzkovitz}},\n\\bibinfo{author}{\\bibfnamefont{N.} \\bibnamefont{Kashtan}},\n\\bibinfo{author}{\\bibfnamefont{D.} \\bibnamefont{Chklovskii}},\n\\bibinfo{author}{\\bibfnamefont{U.} \\bibnamefont{Alon}},\n  \\bibinfo{journal}{Science}\n  \\textbf{\\bibinfo{volume}{298}}, \\bibinfo{pages}{824} (\\bibinfo{year}{2002}).\n  \n\\bibitem[{\\citenamefont{Foster}(2011)}]{Foster:2011}\n\\bibinfo{author}{\\bibfnamefont{D.~V.} \\bibnamefont{Foster}},\n\\bibinfo{author}{\\bibfnamefont{J.~G.} \\bibnamefont{Foster}},\n\\bibinfo{author}{\\bibfnamefont{P.} \\bibnamefont{Grassberger}},\n\\bibinfo{author}{\\bibfnamefont{M.} \\bibnamefont{Paczuski}},\n\t\\textit{arXiv:1012.2348}\n\n\n\n\\end{thebibliography}\n\n", "itemtype": "equation", "pos": 32832, "prevtext": "\nto find out whether either is capable of generating network ensembles exhibiting both the specified degrees and local clustering. \n\nA Monte Carlo simulation was performed for a network of size $n=500$ and ${\\langle{k}\\rangle}=10$. For simplicity, we again set $\\beta_d=\\beta_c=1$, $P(q)=\\delta_{q,10}$ (a regular graph), and ${\\gamma_i}={C_{\\textrm{target}}}$, a universal value for all $i$, varied between $0$ and $1$. First, Fig.~\\ref{02_clustering} shows the mean global clustering ${\\langle{{\\widetilde{C}}}\\rangle}$ from the simulation, which shows us that both $H_1$ and $H_2$ ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ generate networks with the specified clustering. This arises from the fact that ${\\langle{C_i}\\rangle}\\simeq{C_{\\textrm{target}}}$ on the individual level as well (not shown).  The difference between the $H_1$ and $H_2$, however, is most striking in the equilibrium $P(k)$, shown in Fig.~\\ref{03_simdegree}. When ${C_{\\textrm{target}}}=0$, perturbation $H_C$ is insignificant since the expected clustering without it is $0$ anyway, and therefore $P(k)$ are simply as expected -- a true Poissonian for $H_1$, and a sharper peak at $q=10$ for $H_2$, similar to the ones we saw in Fig.~\\ref{01_degree}.  When ${C_{\\textrm{target}}}\\ne0$, on the other hand, the peak in $P(k)$ under $H_1$ gradually shifts towards a smaller $k$ while high-degree nodes are created in order to compensate for the number of edges $M$ which is a constant. As ${C_{\\textrm{target}}}$ is tuned higher it resembles the specified distribution less and less, and at ${C_{\\textrm{target}}}\\simeq0.4$ we even observe multiple peaks (at $k=5$ and $15$ -- the values for which $|t-{C_{\\textrm{target}}} s(k)|=0$, meaning the peaks will shift for a different ${C_{\\textrm{target}}}$ and thus are not very meaningful). $P(k)$ under $H_2$, in contrast, is robust, without noticeable change up to ${C_{\\textrm{target}}}=0.5$, already an unusually high value for real-world networks, until it too shows similar (but milder) behavior at a higher value of ${C_{\\textrm{target}}}\\sim 0.6$ and up~\\footnote{We performed similar simulations for the four heterogeneous $P(q)$ shown in Fig.~\\ref{01A_smoothdegree} and found similar results.}.\n\n\\begin{figure}[t]\n\\includegraphics[width=80mm]{02_clustering.eps}\n\\caption{The global clustering ${\\langle{{\\widetilde{C}}}\\rangle}$ of graph ensembles generated from the linear (blue) and the optimization (red) degree Hamiltonians perturbed with targeted clustering Hamiltonian $H_C=\\sum_{i\\in{\\mathcal{N}}}|t_i-{C_{\\textrm{target}}} s(k_i)|$. ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ is the result of node-level local clustering coefficients being $\\simeq{C_{\\textrm{target}}}$, regardless of the degree distribution.}\n\\label{02_clustering}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\includegraphics[width=180mm]{03_simdegree.eps}\n\\caption{The equilibrium degree distributions $P(k)$ generated from $H_1={H_{LD}}+{H_C}$ (blue) and $H_2={H_{OD}}+{H_C}$ (red) for various values of ${C_{\\textrm{target}}}$. The specified degree distribution $P(q)=\\delta_{q,10}$ is in gray. When ${C_{\\textrm{target}}}=0$, both Hamiltonians generate their natural $P(k)$ -- a true Poissonian for ${H_{LD}}$, and a sharp peak for ${H_{OD}}$. As ${C_{\\textrm{target}}}$ is tuned higher, however, $P(k)$ peaks at a smaller $k$ for ${H_{LD}}+{H_C}$ and even exhibits multiple peaks when ${C_{\\textrm{target}}}$ is too large, while it stays virtually unchanged for ${H_{OD}}+{H_C}$ up to ${C_{\\textrm{target}}}\\simeq0.5$, an unusually high value in real networks.}\n\\label{03_simdegree}\n\\end{figure*}\n\nLet us now discuss the implications of the findings in Figs.~\\ref{02_clustering}~and~\\ref{03_simdegree} on the topology of networks generated from $H_1={H_{LD}}+{H_C}$ and $H_2={H_{LD}}+{H_C}$. First of all, Fig.~\\ref{02_clustering} tells us that, unlike the Strauss clustering perturbation $\\tau T$, ${H_C}$ was able to discourage an extreme condensation of triangles, resulting in ${\\langle{{\\widetilde{C}}}\\rangle}\\simeq{C_{\\textrm{target}}}$ by way of $C_i\\simeq{C_{\\textrm{target}}}$ for both $H_1$ and $H_2$. However it was not enough to completely overcome the cooperative tendency of triangles under ${H_{LD}}$. The telltale sign of this is the creation of high-degree nodes Fig.~\\ref{03_simdegree} which shows the creation of high-degree nodes  in $H_1$: now many triangles exist between the high-degree nodes, forming a core of densely interconnected high-degree nodes although $C_i\\simeq{C_{\\textrm{target}}}$ as specified. On the other hand, under $H_2$ where $P(k)$ is sharply peaked at the specified degree $q=10$ such cores does not exist; with $k_i\\simeq q$ and $C_i\\simeq{C_{\\textrm{target}}}$ for all $i$ as specified, $H_2$ generates a network that truly has a uniform distribution of triangles, lacking any unspecified, accidental local structures.\n\n\\begin{figure*}[t]\n\\includegraphics[width=180mm]{04_degcortopo.eps}\n\\caption{(a) The degree-degree correlation ${r_{\\mathrm{deg}}}$ under $H_1={H_{LD}}+{H_C}$ and $H_2={H_{OD}}+{H_C}$. $H_1$ generates positive degree correlation for any positive ${C_{\\textrm{target}}}$, while $H_2$ exhibits very little correlation up to ${C_{\\textrm{target}}}\\simeq0.5$. (b) The mean corner degree of triangles contained in the networks in equilibrium. Under $H_1$ most triangles exist between high-degree nodes, indicating the persistence of the cooperative nature of triangles. (c) Equilibrium topologies of clustered networks under $H_1$ (left) and $H_2$ (right). $H_1$ generates a core of high-degree nodes that are densely connected and share a large number of triangles (enclosed in orange oval). $H_2$, in contrast, maintains the specified degree distribution $P(q)=\\delta_{q,10}$ while the triangles are distributed uniformly, features expected of a maximally random configuration given the the degree and local clustering constraints.}\n\\label{04_degcortopo}\n\\end{figure*}\n\nWe check our claim via the following two quantities: the degree-degree correlation ${r_{\\mathrm{deg}}}$ (the Pearson correlation between the degrees of adjacent nodes) and the {\\textbf}{mean corner degree} of the triangles in the network, shown in Figs~\\ref{04_degcortopo}~(a)~and~(b). First, the plot of ${\\langle{{r_{\\mathrm{deg}}}}\\rangle}$ in Fig.~\\ref{04_degcortopo}~(a) indicates  that adjacent degrees in the network are highly correlated under $H_1$, so that high-degree nodes are indeed connected with other high-degree nodes and vice versa, while $H_2$ shows no such effect. This leads naturally to what we see in Fig.~\\ref{04_degcortopo}~(b): under $H_1$, the mean corner degree is significantly higher than ${\\langle{k}\\rangle}=q$, unlike $H_2$ where it is practically equal to $q$. These observations are presented visually in Fig.~\\ref{04_degcortopo}~(c) (an actual snapshot of an equilibrium configuration of a network with $n=50$, $P(q)=\\delta_{q,5}$, and ${C_{\\textrm{target}}}=0.4$. ${\\langle{{\\widetilde{C}}}\\rangle}$ are $0.35\\pm0.02$ and $0.31\\pm0.02$, respectively). As expected, for $H_1$ (left) we clearly see that the ten highest-degree nodes (blue, average degree $9.7$) forms a densely interconnected core (encircled in orange), with ten lowest-degree nodes (yellow, average degree $1.0$) pushed to the periphery with low triangle participation rate. For $H_2$ (right), no significant difference between highest- and lowest-degree nodes exists, and the triangles are distributed uniformly, expected of a maximally random configuration given the degree and local clustering constraints.\n\n\\section{Discussion and Future Directions}\nHere we have studied two forms of graph Hamiltonian in exponential random graph theory that take node degrees and local clustering as specified input. The tendency of triangles to coalesce in the Strauss model was shown to persist when the linear clustering perturbation was replaced by an optimized clustering form, albeit in a milder fashion, rendering the composite Hamiltonian unable to generate the specified degree distribution~\\footnote{The reverse case of ${H_{OD}}+\\tau T$ was also numerically studied with varying $\\tau$. As $\\tau$ is tuned more negative (thus favoring triangles) here also occurs a sudden onset of the emergence of a densely connected cluster of high-degree nodes, signifying a condensation of triangles similar to ${H_{LD}}+\\tau T$. This demonstrates that to create finite clustering degree and local clustering optimization are necessary.}. The optimization degree Hamiltonian, on the other hand, was able to satisfy both, exhibiting significant robustness under the same perturbation.\n\nThat the optimization Hamiltonian form was able to reproduce both the targeted degree and clustering presents an appealing possibility from the viewpoint of network modeling via exponential random graph theory: given a set of network variables $\\Phi={\\{{\\phi_v|v=1,\\cdots,l}\\}}$, it may act as a practical computational method to generate a null model of network data with actual values of the variables ${\\{{\\tilde{\\phi}_v|v=1,\\ldots,l}\\}}$ using the Hamiltonian~\\cite{Foster:2010}\n\n", "index": 27, "text": "\\begin{align}\nH(G) = \\sum_{\\phi\\in\\Phi}\\beta_v|\\phi_v(G)-\\tilde{\\phi}_v|,\n\\label{genoptham}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H(G)=\\sum_{\\phi\\in\\Phi}\\beta_{v}|\\phi_{v}(G)-\\tilde{\\phi}_{v}|,\" display=\"inline\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\u03d5</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow></munder></mstyle><mrow><msub><mi>\u03b2</mi><mi>v</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>\u03d5</mi><mi>v</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">~</mo></mover><mi>v</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]