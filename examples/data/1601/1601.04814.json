[{"file": "1601.04814.tex", "nexttext": "\n\n\n\nWe assume that all vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} are\nnormalized to unit length, i.e.,\n${\\ensuremath{||{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}||_2}\\xspace} =1$,\nwhich implies that the\ndot-product\nof two vectors is equal to their cosine similarity;\n$\\cos({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) = {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$.\n\nIn the standard \\emph{all-pairs similarity search} problem ({{\\sc{apss}}\\xspace}),\nalso known as \\emph{similarity self-join},\nwe are given a set of vectors and a similarity threshold {\\ensuremath{\\theta}\\xspace},\nand the goal is to find all pairs of vectors\n$({{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}, {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}})$ for which ${\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$.\n\nIn this paper we assume that\nthe input items arrive as a data stream.\nEach vector ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}$ in the input stream\nis timestamped with the time of its arrival ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$,\nand the stream is denoted by\n${\\ensuremath{\\mathcal{S}}\\xspace}=\\langle\\ldots,({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_i,{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_i}})}\\xspace}),({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{i+1},{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{i+1}}})}\\xspace},\\ldots\\rangle$.\n\n\n\n\nWe define the similarity of two vectors\n{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nby considering not only their coordinates (${\\ensuremath{\\mathrm{x}}\\xspace}_j, {\\ensuremath{\\mathrm{y}}\\xspace}_j$),\nbut also the difference in their arrival times in the input stream\n${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} = {\\ensuremath{|{{{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}-{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}}}|}\\xspace}$.\nFor fixed coordinates,\nthe larger the arrival time difference of two vectors,\nthe smaller their similarity.\nIn particular,\ngiven two timestamped vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nwe define their \\emph{time-dependent similarity} ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$ as\n", "itemtype": "equation", "pos": 15486, "prevtext": "\n\\maketitle\n\n\\enlargethispage{\\baselineskip}\n\\begin{abstract}\nWe introduce and study the problem of\ncomputing the similarity self-join in a streaming context ({{\\sc{sssj}}\\xspace}),\nwhere the input is an unbounded stream of items arriving continuously.\nThe goal is to find all pairs of items in the stream\nwhose similarity is greater than a given threshold.\nThe simplest formulation of the problem requires unbounded memory,\nand thus, it is intractable.\nTo make the problem feasible, we introduce the notion of {\\em time-dependent similarity:}\nthe similarity of two items decreases with the difference in their arrival time.\n\nBy leveraging the properties of this time-dependent similarity function,\nwe design two algorithmic frameworks to solve the {{\\sc{sssj}}\\xspace} problem.\nThe first one, MiniBatch ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}),\nuses existing index-based filtering techniques for the static version of the problem,\nand combines them in a pipeline.\nThe second framework, Streaming ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}),\nadds \\emph{time filtering} to the existing indexes,\nand integrates new time-based bounds deeply in the working of the algorithms.\nWe also introduce a new indexing technique ({\\ensuremath{\\text{\\tt{L2}}}\\xspace}),\nwhich is based on an existing state-of-the-art indexing technique ({\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}),\nbut is optimized for the streaming case.\n\nExtensive experiments show that the {\\ensuremath{\\text{\\tt{STR}}}\\xspace} algorithm,\nwhen instantiated with the {\\ensuremath{\\text{\\tt{L2}}}\\xspace} index,\nis the most scalable option across a wide array of datasets and parameters.\n\\end{abstract}\n\n\n\\iffalse\n\nABSTRACT PLAIN TEXT\n\nWe introduce and study the problem of computing the similarity self-join in a streaming context (SSSJ), where the input is an unbounded stream of items arriving continuously. The goal is to find all pairs of items in the stream whose similarity is greater than a given threshold. The simplest formulation of the problem requires unbounded memory, and thus, it is intractable. To make the problem feasible, we introduce the notion of time-dependent similarity: the similarity of two items decreases with the difference in their arrival time.\n\nBy leveraging the properties of this time-dependent similarity function, we design two algorithmic frameworks to solve the SSSJ problem. The first one, MiniBatch, uses existing index-based filtering techniques for the static version of the problem, and combines them in a pipeline. The second framework, Streaming, adds time filtering to the existing indexes, and integrates new time-based bounds deeply in the working of the algorithms. We also introduce a new indexing technique (L2), which is based on an existing state-of-the-art indexing technique (L2AP), but is optimized for the streaming case.\n\nExtensive experiments show that the Streaming algorithm, when instantiated with the L2 index, is the most scalable option across a wide array of datasets and parameters.\n\n\\fi\n\n\n\\section{Introduction}\n\n\\begin{figure}[t]\n\\vspace{-\\baselineskip}\n\\begin{center}\n\\includegraphics[width=0.8\\columnwidth]{fig1}\n\\caption{\\label{fig:illustration}\nTimestamped documents arrive as stream.\nThe documents on top (in red) have similar content.\nAmong all pairs of similar documents,\nwe are interested in those that arrive close in time.\nIn the example,\nout of all 4-choose-2 pairs\nonly two pairs are reported\n(shown with blue arrows).\n}\n\\end{center}\n\n\\end{figure}\n\n\\emph{Similarity self-join}\nis the problem of finding\n\\emph{all pairs} of similar objects in a given dataset.\nThe problem is related to the \\emph{similarity join}\noperator~\\cite{arasu2006efficient,chaudhuri2006primitive},\n\nwhich has been studied extensively in the database and data-mining communities.\nThe similarity self-join is an essential component\nin several applications, including\nplagiarism detection~\\cite{cho2000finding,hoad2003methods},\nquery refinement~\\cite{sahami2006web},\ndocument clustering~\\cite{broder1997syntactic,haveliwala2000scalable,bohm2000clustering},\ndata cleaning~\\cite{chaudhuri2006primitive},\ncommunity mining~\\cite{spertus2005evaluating},\nnear-duplicate record detection~\\cite{xiao2011efficient},\nand collaborative filtering~\\cite{das2007google}.\n\nThe similarity self-join problem is inherently quadratic.\nIn fact, the brute-force ${\\ensuremath{\\mathcal{O}}\\xspace}(n^2)$ algorithm\nthat computes the similarity between all pairs\nis the best one can hope for, in the worst case,\nwhen exact similarity is required\nand when dealing with arbitrary objects.\nIn practice, it is possible to obtain scalable algorithms\nby leveraging structural properties of specific problem instances.\nA typical desiderata here is to design {\\em output-sensitive} algorithms.\n\nIn many real-world applications\nobjects are represented as {\\em sparse} vectors in a high-dimensional Euclidean space,\nand similarity is measured as the dot-product of unit-normalized vectors ---\nor equivalently, cosine similarity.\nThe similarity self-join problem\nasks for all pairs of objects whose similarity is above a predefined threshold~{\\ensuremath{\\theta}\\xspace}.\nEfficient algorithms for this setting are well-developed,\nand rely on pruning based on inverted indexes,\nas well as a number of different geometric\nbounds~\\cite{anastasiu2014l2ap,bayardo2007scaling-up,chaudhuri2006primitive,xiao2011efficient};\ndetails on those methods are presented in the following sections.\n\nComputing similarity self-join is a problem\nthat  is germane not only for static data collections,\nbut also for data streams.\nTwo examples of real-world applications that motivate\nthe problem of similarity self-join in the streaming setting\nare the following.\n\n\\enlargethispage{\\baselineskip}\n{\\smallskip\\noindent\\textbf{{Trend detection}}}:\nexisting algorithms for trend detection in microblogging platforms,\nsuch as Twitter,\nrely on identifying hashtags or other terms\nwhose frequency suddenly increases.\nA more focused and more granular trend-detection approach\nwould be to identify a set of posts, whose frequency increases,\nand which share a certain fraction of hashtags or terms.\nFor such a trend-detection method\nit is essential to be able to find similar pairs of posts in a data stream.\n\n{\\smallskip\\noindent\\textbf{{Near-duplicate item filtering}}}:\nconsider again a\nmicro\\-blog\\-ging platform,\nsuch as Twitter.\nWhen an event occurs, users may receive\nmultiple near-copies of posts related to the event.\nSuch posts often appear consecutively in the feed of users,\nthus cluttering their information stream and degrading their experience.\nGrouping these near-copies or filtering them out is\na simple way to improve the user experience.\n\n\\smallskip\nSurprisingly, the problem of streaming similarity self-join\nhas not been considered in the literature so far.\n\nPossibly, because the similarity self-join operator in principle\nrequires \\emph{unbounded} memory:\none can never forget a data item,\nas it may be similar\n\nto another one that comes far in the future.\n\nTo the best of our knowledge,\nthis paper is the first to address the similarity self-join problem\nin the streaming setting.\nWe overcome the inherent unbounded-memory bottleneck\nby introducing a temporal factor to the similarity operator.\nNamely, we consider two data items similar,\nonly if they have \narrived within a \\emph{short time span}. \nMore precisely, we define the \\emph{time-dependent similarity} of two data items\nto be their content-based cosine similarity\nmultiplied by a factor that decays exponentially with the\ndifference in their arrival times.\nThis time-dependent factor allows us to drop old items, \nas they cannot be similar to any item that arrives after a certain time horizon~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nThe concept is illustrated in Figure~\\ref{fig:illustration}.\n\nThe time-dependent similarity outlined above\nis very well-suited for both our motivating applications ---\ntrend detection and near-duplicate filtering:\\\nin both cases we are interested in identifying\nitems that are not only similar content-wise,\nbut have also arrived within a short time interval. \n\nAkin to previous approaches for the similarity self-join problem,\nour method relies on indexing techniques.\nPrevious approaches use different types of \\emph{index filtering}\nto reduce the number of candidates\nreturned by the index for full similarity evaluation.\nFollowing this terminology,\nwe use the term \\emph{time filtering} to refer to the\nproperty of the time-dependent similarity\nthat allows to drop old items from the index.\n\n\nWe present two different algorithmic frameworks for\nthe streaming similarity self-join problem,\nboth of which rely on the time-filtering property.\nBoth frameworks can be instantiated with indexing schemes\nbased on existing ones for the batch version of the problem.\nThe first framework, named MiniBatch ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}),\nuses existing indexing schemes off-the-shelf.\nIn particular, it uses two indexes in a pipeline,\nand it drops the older one when it becomes old enough.\nThe second framework we propose, named Streaming ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}),\nmodifies existing indexing schemes so that time filtering is incorporated internally\n\nin the index.\n\n\n\n\n\n\n\n\n\nOne of our contributions is a new index, {\\ensuremath{\\text{\\tt{L2}}}\\xspace},\nwhich combines state-of-the art bounds for index pruning from {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}~\\cite{anastasiu2014l2ap}\nin a way that is optimized for stream data.\nThe superior performance of {\\ensuremath{\\text{\\tt{L2}}}\\xspace} stems from the fact\nthat it uses bounds that\n($i$) are effective in reducing the number of candidate pairs,\n($ii$) do not require collecting statistics over the data stream\n(which need to be updated as the stream evolves),\n($iii$) lead to very lightweight index maintenance,\nwhile\n($iv$) allowing to drop data as soon as they become too old to be similar to any item currently read.\nOur experimental evaluation demonstrates\nthat the {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} index, incorporated in the {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} framework,\nis the method of choice for all datasets we try across a wide range of parameters.\n\n\\enlargethispage{1.5\\baselineskip}\nIn brief, our contributions can be summarized as follows.\n\n\\begin{squishlist}\n\\item\nWe introduce the similarity self-join problem in data streams.\n\\item\nWe propose a novel time-dependent similarity measure,\nwhich allows to forget data items when they become old.\n\\item\nWe show how to solve the proposed problem\nwithin two different algorithmic frameworks, {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} and {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}:\nFor {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}},\nany state-of-the-art indexing scheme can be used as a black box.\nFor {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}},\nwe adapt and extend the existing schemes to be well-suited for streaming data.\n\\item\nWe perform an extensive experimental evaluation,\nand test the proposed algorithms\non datasets with different characteristics and for a large range of\nparameters.\n\\end{squishlist}\n\n\nFinally, we note that\nall the code\\footnote{Code: \\url{http://github.com/gdfm/sssj}} we developed and\nall datasets\\footnote{Data: \\url{http://research.ics.aalto.fi/dmg/datasets/}} used for validation\nare publicly available.\n\n\n\\enlargethispage{\\baselineskip}\n\n\\section{Related Work}\nThe problem of \\emph{streaming similarity self-join} has not been studied previously.\nWork related to this problem can be classified into two main areas:\nsimilarity self-join\n(also known as all-pairs similarity), and streaming similarity.\n\n{\\smallskip\\noindent\\textbf{{Similarity self-join.}}}\nThe similarity self-join problem has been studied extensively.\nIt was introduced by~\\citet{chaudhuri2006primitive},\nand many works have followed up~\\citep{arasu2006efficient,awekar2009fast-matching,bayardo2007scaling-up,chaudhuri2006primitive,xiao2011efficient,xiao2008ppjoin},\nincluding approaches that use parallel computation~\\citep{afrati2011fuzzyjoin,deFrancisciMorales2010aps,baraglia2010document,vernica2010efficient,alabduljalil2013optimizing}.\nDiscussing all these related papers is out of the scope of this work.\nThe most relevant approaches are those by \\citet{bayardo2007scaling-up},\nwhich significantly improved the scalability of the previous indexing-based methods, and\nby \\citet{anastasiu2014l2ap},\nwhich represents the current state-of-the-art for sequential batch algorithms.\nOur work relies heavily on these two papers, and we summarize their common \\emph{filtering framework} in Section~\\ref{section:framework}.\nFor a good overview, refer to the work of~\\citet{anastasiu2014l2ap}.\n\n{\\smallskip\\noindent\\textbf{{Streaming similarity.}}}\nThe problem of computing similarities in a stream of vectors has received surprisingly little attention so far.\nMost of the work on streaming similarity focuses on time series~\\citep{gao2002continually,keogh2005exact,lian2009efficient}.\n\n\\citet{lian2011similarity} study the similarity join problem for {\\em uncertain} streams of vectors\nin the sliding-window model.\n\nThey focus on uncertain objects moving in a low-dimensional Euclidean space (${\\ensuremath{d}\\xspace} \\in [2,5]$).\nSimilarity is measured by the Euclidean distance,\nand therefore the pruning techniques are very different than the ones employed in our work;\ne.g., they use space-partition techniques,\nand use temporal correlation of samples to improve the efficiency of their algorithm.\n\n\\citet{wang2009scalable} study the more general problem of multi-way join with expensive\nuser-defined functions (UDFs) in the sliding-window model.\nThey present a time-slicing approach,\nwhich has some resemblance to the time-based pruning presented in this paper.\nHowever, their focus in on distributing the computation on a cluster via pipelined parallelism,\nand ensuring that the multiple windows of the multi-way join align correctly.\nDifferently from this paper,\n\ntheir method treats UDFs as a black box.\n\n\\citet{valari2013continuous} focus on graphs, rather than vectors, and study the case of Jaccard similarity of nodes on a sliding window over an edge stream.\nThey apply a count-based pruning on edges by keeping a fixed-size window,\nand leveraging an upper bound on the similarity within the next window.\nInstead, in this work we consider time-based pruning,\nwithout any assumption on the frequency of arrival of items in the stream.\nFurthermore, the arrival of edges changes the similarity of the nodes,\nand therefore different indexing techniques need to be applied;\ne.g., nodes can never be pruned.\n\n\\section{Problem Statement}\n\\label{section:problem}\n\nWe consider data items represented as vectors in a ${\\ensuremath{d}\\xspace}$-dimensional\nEuclidean space ${\\ensuremath{\\mathbb{R}}\\xspace}^{\\ensuremath{d}\\xspace}$.\nIn real-world applications, the dimensionality ${\\ensuremath{d}\\xspace}$ is typically high,\nwhile the data items are sparse vectors.\nGiven two vectors ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}=\\langle {\\ensuremath{\\mathrm{x}}\\xspace}_1\\ldots {\\ensuremath{\\mathrm{x}}\\xspace}_{\\ensuremath{d}\\xspace} \\rangle$ and\n${\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}=\\langle {\\ensuremath{\\mathrm{y}}\\xspace}_1\\ldots {\\ensuremath{\\mathrm{y}}\\xspace}_{\\ensuremath{d}\\xspace} \\rangle$,\ntheir similarity ${\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$\nis the dot-product\n", "index": 1, "text": "\n\\[\n{\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) =\n{\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) =\n{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\cdot{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} =\n\\sum_{j=1}^{{\\ensuremath{d}\\xspace}} {\\ensuremath{\\mathrm{x}}\\xspace}_j\\, {\\ensuremath{\\mathrm{y}}\\xspace}_j.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{sim}}({{\\mathbf{{x}}}},{{\\mathbf{{y}}}})={\\mathrm{dot}}({{\\mathbf{{x}%&#10;}}},{{\\mathbf{{y}}}})={{\\mathbf{{x}}}}\\cdot{{\\mathbf{{y}}}}=\\sum_{j=1}^{{d}}{%&#10;\\mathrm{x}}_{j}\\,{\\mathrm{y}}_{j}.\" display=\"block\"><mrow><mrow><mrow><mi>sim</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>dot</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udc31</mi><mo>\u22c5</mo><mi>\ud835\udc32</mi></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mpadded width=\"+1.7pt\"><msub><mi mathvariant=\"normal\">x</mi><mi>j</mi></msub></mpadded><mo>\u2062</mo><msub><mi mathvariant=\"normal\">y</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\nwhere {\\ensuremath{\\lambda}\\xspace} is a \\emph{time-decay} parameter.\n\n\n\n\n\nThis time-dependent similarity reverts to the standard dot-product (or cosine)\nsimilarity when ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}=0$ or ${\\ensuremath{\\lambda}\\xspace}=0$,\nand it goes to zero as ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}$ approaches infinity,\nat an exponential rate modulated by ${\\ensuremath{\\lambda}\\xspace}$.\nAs in the standard {{\\sc{apss}}\\xspace}\\ problem,\ngiven a similarity threshold {\\ensuremath{\\theta}\\xspace},\ntwo vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} are called \\emph{similar}\nif their time-dependent similarity is above the threshold, i.e.,\n${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\geq {\\ensuremath{\\theta}\\xspace}$.\n\nWe can now define the streaming version of the {{\\sc{apss}}\\xspace} problem,\ncalled the \\emph{streaming similarity self-join} problem ({{\\sc{sssj}}\\xspace}).\n\n\\begin{problem}[{{\\sc{sssj}}\\xspace}]\n\\label{problem:streaming-apss}\nGiven a stream of timestamped vectors {\\ensuremath{\\mathcal{S}}\\xspace},\na similarity threshold {\\ensuremath{\\theta}\\xspace},\nand a time-decay factor {\\ensuremath{\\lambda}\\xspace},\noutput all pairs of vectors\n$({{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}, {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}})$ in the stream such that ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$.\n\\end{problem}\n\n{\\smallskip\\noindent\\textbf{{Time filtering.}}}\nThe main challenge of computing a self-join in a data stream\nis that, in principle, unbounded memory is required,\nas an item can be similar with any other item that will arrive\narbitrarily far in the future.\n\n\n\n\n\n\n\nBy adopting the time-dependent similarity measure {{\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}},\nwe introduce a forgetting mechanism,\nwhich not only is intuitive from an application point-of-view,\nbut also allows to overcome the unbounded-memory requirement.\nIn particular,\nsince for {{\\ensuremath{\\ell_2}\\xspace}}-normalized vectors $ {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\le 1$,\nwe have\n", "itemtype": "equation", "pos": 19116, "prevtext": "\n\n\n\nWe assume that all vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} are\nnormalized to unit length, i.e.,\n${\\ensuremath{||{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}||_2}\\xspace} =1$,\nwhich implies that the\ndot-product\nof two vectors is equal to their cosine similarity;\n$\\cos({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) = {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$.\n\nIn the standard \\emph{all-pairs similarity search} problem ({{\\sc{apss}}\\xspace}),\nalso known as \\emph{similarity self-join},\nwe are given a set of vectors and a similarity threshold {\\ensuremath{\\theta}\\xspace},\nand the goal is to find all pairs of vectors\n$({{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}, {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}})$ for which ${\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$.\n\nIn this paper we assume that\nthe input items arrive as a data stream.\nEach vector ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}$ in the input stream\nis timestamped with the time of its arrival ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$,\nand the stream is denoted by\n${\\ensuremath{\\mathcal{S}}\\xspace}=\\langle\\ldots,({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_i,{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_i}})}\\xspace}),({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{i+1},{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{i+1}}})}\\xspace},\\ldots\\rangle$.\n\n\n\n\nWe define the similarity of two vectors\n{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nby considering not only their coordinates (${\\ensuremath{\\mathrm{x}}\\xspace}_j, {\\ensuremath{\\mathrm{y}}\\xspace}_j$),\nbut also the difference in their arrival times in the input stream\n${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} = {\\ensuremath{|{{{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}-{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}}}|}\\xspace}$.\nFor fixed coordinates,\nthe larger the arrival time difference of two vectors,\nthe smaller their similarity.\nIn particular,\ngiven two timestamped vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nwe define their \\emph{time-dependent similarity} ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$ as\n", "index": 3, "text": "\n\\[\n{\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) =\n{\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\, e^{- {\\ensuremath{\\lambda}\\xspace}{\\ensuremath{|{{{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}-{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}}}|}\\xspace}},\n\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{{\\mathrm{sim}}_{{\\Delta t}}}({{\\mathbf{{x}}}},{{\\mathbf{{y}}}})={\\mathrm{dot}%&#10;}({{\\mathbf{{x}}}},{{\\mathbf{{y}}}})\\,e^{-{\\lambda}{|{{{t({{{{\\mathbf{{x}}}}}}%&#10;)}-{t({{{{\\mathbf{{y}}}}}})}}}|}},\\par&#10;\" display=\"block\"><mrow><mrow><mrow><msub><mi>sim</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>dot</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\nThus, ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} > {\\ensuremath{\\lambda}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$\nimplies ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})<{\\ensuremath{\\theta}\\xspace}$,\nand as a result\na given vector cannot be similar to any vector that arrived\nmore than\n", "itemtype": "equation", "pos": 22564, "prevtext": "\nwhere {\\ensuremath{\\lambda}\\xspace} is a \\emph{time-decay} parameter.\n\n\n\n\n\nThis time-dependent similarity reverts to the standard dot-product (or cosine)\nsimilarity when ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}=0$ or ${\\ensuremath{\\lambda}\\xspace}=0$,\nand it goes to zero as ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}$ approaches infinity,\nat an exponential rate modulated by ${\\ensuremath{\\lambda}\\xspace}$.\nAs in the standard {{\\sc{apss}}\\xspace}\\ problem,\ngiven a similarity threshold {\\ensuremath{\\theta}\\xspace},\ntwo vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} are called \\emph{similar}\nif their time-dependent similarity is above the threshold, i.e.,\n${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\geq {\\ensuremath{\\theta}\\xspace}$.\n\nWe can now define the streaming version of the {{\\sc{apss}}\\xspace} problem,\ncalled the \\emph{streaming similarity self-join} problem ({{\\sc{sssj}}\\xspace}).\n\n\\begin{problem}[{{\\sc{sssj}}\\xspace}]\n\\label{problem:streaming-apss}\nGiven a stream of timestamped vectors {\\ensuremath{\\mathcal{S}}\\xspace},\na similarity threshold {\\ensuremath{\\theta}\\xspace},\nand a time-decay factor {\\ensuremath{\\lambda}\\xspace},\noutput all pairs of vectors\n$({{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}, {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}})$ in the stream such that ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$.\n\\end{problem}\n\n{\\smallskip\\noindent\\textbf{{Time filtering.}}}\nThe main challenge of computing a self-join in a data stream\nis that, in principle, unbounded memory is required,\nas an item can be similar with any other item that will arrive\narbitrarily far in the future.\n\n\n\n\n\n\n\nBy adopting the time-dependent similarity measure {{\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}},\nwe introduce a forgetting mechanism,\nwhich not only is intuitive from an application point-of-view,\nbut also allows to overcome the unbounded-memory requirement.\nIn particular,\nsince for {{\\ensuremath{\\ell_2}\\xspace}}-normalized vectors $ {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\le 1$,\nwe have\n", "index": 5, "text": "\n\\[\n{\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) =  {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace} \\leq {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"{{\\mathrm{sim}}_{{\\Delta t}}}({{\\mathbf{{x}}}},{{\\mathbf{{y}}}})={\\mathrm{dot}%&#10;}({{\\mathbf{{x}}}},{{\\mathbf{{y}}}})\\,{e^{-{\\lambda}{\\Delta t}_{{{{\\mathbf{{x}%&#10;}}}{{\\mathbf{{y}}}}}}}}\\leq{e^{-{\\lambda}{\\Delta t}_{{{{\\mathbf{{x}}}}{{%&#10;\\mathbf{{y}}}}}}}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>sim</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>dot</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>\ud835\udc31\ud835\udc32</mi></msub></mrow></mrow></msup></mrow><mo>\u2264</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>t</mi><mi>\ud835\udc31\ud835\udc32</mi></msub></mrow></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\ntime units earlier.\nConsequently, we can safely prune vectors that are older than {\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nWe call {\\ensuremath{\\mathcal{\\tau}}\\xspace} the \\emph{time horizon}.\n\n\n\n\n{\\smallskip\\noindent\\textbf{{Parameter setting.}}}\nThe {{\\sc{sssj}}\\xspace} problem, defined in Problem~\\ref{problem:streaming-apss},\nrequires two parameters: a similarity threshold {\\ensuremath{\\theta}\\xspace}\nand a time-decay factor {\\ensuremath{\\lambda}\\xspace}.\n\nThe time-filtering property\nsuggests a simple methodology to set these two parameters:\n\n\\begin{squishlist}\n\\item [1.]\nSelect \n${\\ensuremath{\\theta}\\xspace}$\nas the lowest value of the similarity\nbetween two \\emph{simultaneously-arriving}\nvectors that are deemed \\emph{similar}.\n\\item [2.]\nSelect \n${\\ensuremath{\\mathcal{\\tau}}\\xspace}$\nas the smallest difference in arrival times\nbetween two \\emph{identical}\nvectors that are deemed \\emph{dissimilar}.\n\\item [3.]\nSet ${\\ensuremath{\\lambda}\\xspace} = {\\ensuremath{\\mathcal{\\tau}}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$.\n\\end{squishlist}\nSetting ${\\ensuremath{\\theta}\\xspace}$ and  ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$ in steps 1 and 2\ndepends on the application.\n\n{\\smallskip\\noindent\\textbf{{Additional notation.}}}\nWhen referring to the non-streaming setting,\nwe consider a dataset\n${\\ensuremath{\\mathcal{D}}\\xspace}=\\{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_1,\\ldots,{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{\\ensuremath{n}\\xspace}\\}$\nconsisting of {{\\ensuremath{n}\\xspace}} vectors in ${\\ensuremath{\\mathbb{R}}\\xspace}^{\\ensuremath{d}\\xspace}$.\n\nFollowing \\citeauthor{anastasiu2014l2ap},\nwe use the notation\n${\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}={\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_p=\\langle{\\ensuremath{\\mathrm{x}}\\xspace}_1,..,{\\ensuremath{\\mathrm{x}}\\xspace}_{p-1},0,..,0\\rangle$\nand\n\nto denote the \\emph{prefix} \nof a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}. \nFor a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\nwe denote by\n${\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}$ its maximum coordinate, by\n${\\Sigma}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} = \\sum_{j}{\\ensuremath{\\mathrm{x}}\\xspace}_j$ the sum of its coordinates, and by\n${\\ensuremath{|{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}|}\\xspace}$ the number of its non-zero coordinates.\n\nGiven a static dataset {{\\ensuremath{\\mathcal{D}}\\xspace}} we use ${\\ensuremath{\\mathrm{{m}}}\\xspace}_{j}$\nto refer to the maximum value along the $j$-th coordinate over all vectors in~{{\\ensuremath{\\mathcal{D}}\\xspace}}.\nAll values of ${\\ensuremath{\\mathrm{{m}}}\\xspace}_{j}$ together compose the vector ${\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}$.\nOur methods use indexing schemes\nwhich build an index incrementally, vector-by-vector.\nWe use ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}$ to refer to the vector ${\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}$,\nrestricted to the dataset that is already indexed,\nand we write ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}}\\xspace}_{j}$ for its $j$-th coordinate.\nFurthermore,\nwe use ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}$ (and ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_{j}$ for its $j$-th coordinate)\nto denote a time-decayed variant of ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}$,\nwhose precise definition is given in Section~\\ref{section:framework:l2ap}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Overview of the approach}\n\\label{section:overview}\n\nIn this section we present\na high-level overview of\nour main algorithms for the {{{\\sc{sssj}}\\xspace}} problem.\n\nWe present two different algorithmic frameworks:\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} (MiniBatch-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}) and {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} (Streaming-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}),\nwhere {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} is an indexing method for the {{{\\sc{apss}}\\xspace}} problem on static data.\nTo make the presentation of our algorithms more clear,\nwe first give an overview of the indexing methods for the static {{{\\sc{apss}}\\xspace}} problem,\nas introduced in earlier\npapers~\\cite{anastasiu2014l2ap,bayardo2007scaling-up,chaudhuri2006primitive}.\n\nAll indexing schemes \nare based on building an \\emph{inverted index}.\n\nThe index is a collection of {\\ensuremath{d}\\xspace} \\emph{posting lists},\n${\\ensuremath{\\mathcal{I}}\\xspace}=\\{{\\ensuremath{I}\\xspace}_1,\\ldots,{\\ensuremath{I}\\xspace}_{\\ensuremath{d}\\xspace}\\}$,\n\nwhere the list ${\\ensuremath{I}\\xspace}_j$ contains all pairs\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j)$\nsuch that the $j$-th coordinate of vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is non-zero, i.e., ${\\ensuremath{\\mathrm{x}}\\xspace}_j \\neq 0$.\nHere ${\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$ denotes a reference to vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\n\n\\iffalse\nThis representation of the data already provides an improvement over the na\\\"{i}ve approach of checking every pair.\nNamely, when querying the index with a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, any other vector {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} that has no non-zero\ncoordinate shared with {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} will be ignored.\n\\note[gdfm]{Previous paragraph might not be needed here, as it is explained in the next section}\n\\fi\n\nSome methods optimize computation by not indexing the whole dataset.\nIn these cases, the un-indexed part is still needed in order to compute exact similarities.\n\nThus, we assume that a separate part of the index {{\\ensuremath{\\mathcal{I}}\\xspace}}\ncontains the un-indexed part of the dataset,\ncalled \\emph{residual direct index} {\\ensuremath{\\mathcal{R}}\\xspace}.\n\nAll schemes build the index incrementally,\nwhile also computing similar pairs.\nIn particular,\nwe start with an empty index,\nand we iteratively process the vectors in {{\\ensuremath{\\mathcal{D}}\\xspace}}.\nFor each newly-processed vector~{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe compute similar pairs $({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$\nfor all vectors~{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} that are already in the index.\nThereafter, we add\n(some of) the non-zero coordinates of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to the index.\n\nBuilding the index and computing similar pairs\ncan be seen as a three-phase process:\n\n\\begin{squishlist}\n\\item [\\textbf{index construction ({\\ensuremath{\\text{\\tt{IC}}}\\xspace})}:]\nadds new vectors to the \nindex~{{\\ensuremath{\\mathcal{I}}\\xspace}}.\n\\item [\\textbf{candidate generation ({\\ensuremath{\\text{\\tt{CG}}}\\xspace})}:]\nuses the index {{\\ensuremath{\\mathcal{I}}\\xspace}} to generate candidate similar pairs.\nThe candidate pairs may contain false positives\nbut no false negatives.\n\\item [\\textbf{candidate verification ({\\ensuremath{\\text{\\tt{CV}}}\\xspace})}:]\ncomputes true similarities between candidate pairs,\nand reports true similar pairs,\nwhile dismissing false positives.\n\\end{squishlist}\n\nMore concretely,\nfor an indexing scheme {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nwe assume that the following three primitives are available,\nwhich correspond to the three phases outlined above:\n\n\\begin{squishlist}\n\\item[$({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace}) \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{D}}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven a dataset ${\\ensuremath{\\mathcal{D}}\\xspace}$\nconsisting of {\\ensuremath{n}\\xspace} vectors,\nand a similarity threshold {\\ensuremath{\\theta}\\xspace},\nthe function {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\nreturns in ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\}$ all similar pairs $({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$,\nwith ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace}$.\nAdditionally, {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}\nbuilds an index {{\\ensuremath{\\mathcal{I}}\\xspace}},\nwhich can be used to find similar pairs between the vectors in {{\\ensuremath{\\mathcal{D}}\\xspace}}\nand another query vector~{\\ensuremath{\\mathbf{{z}}}\\xspace}.\n\n\n\n\n\\item[${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven an index {{\\ensuremath{\\mathcal{I}}\\xspace}}, built on a dataset {\\ensuremath{\\mathcal{D}}\\xspace},\na vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\nand a similarity threshold~{\\ensuremath{\\theta}\\xspace},\nthe function {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} returns\na  set of candidate vectors ${\\ensuremath{C}\\xspace}=\\{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\}$,\nwhich is a superset of all vectors that are similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\n\n\\item[${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven an index {{\\ensuremath{\\mathcal{I}}\\xspace}},\nbuilt on a dataset {\\ensuremath{\\mathcal{D}}\\xspace},\na vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\na set of candidate vectors~${\\ensuremath{C}\\xspace}$,\nand a similarity threshold~{\\ensuremath{\\theta}\\xspace},\nthe function {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} returns\nthe set\n${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\}$ of true similar pairs.\n\\end{squishlist}\n\n\nBoth proposed frameworks,\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} and {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}},\nrely on an indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace},\nand adapt it to the streaming setting by\naugmenting it with the time-filtering property.\nThe difference between the two frameworks is in how this adaptation is done.\n\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} uses {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} as a ``black box'':\nit uses time filtering in order to\nbuild independent instances of {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} indexes,\nand drops them when they become obsolete.\nConversely,\nfor {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}\nwe opportunely modify the indexing method {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nby directly applying time filtering.\n\nThe presentation of the {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework\nis deferred to the next section,\nafter we present the details of the specific indexing methods {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nwe use.\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\\ (MiniBatch with index {\\ensuremath{\\text{\\tt{IDX}}}\\xspace})}\n\\label{algorithm:minibatchx}\n\\Input{Data stream {\\ensuremath{\\mathcal{S}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{All pairs ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{S}}\\xspace}$ s.t.\\ ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$}\n\n${\\ensuremath{\\mathcal{I}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n$t_0 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0; t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$\\;\n${\\ensuremath{\\mathcal{\\tau}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\lambda}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$\\;\n\n\\While{$\\text{\\emph{true}}$} {\n\t${\\ensuremath{{W}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\t$t_0 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace t_0 + {\\ensuremath{\\mathcal{\\tau}}\\xspace}$\\;\n\t\\While{$t \\le t_0 + {\\ensuremath{\\mathcal{\\tau}}\\xspace}$} {\n\t\t${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\texttt{read}({\\ensuremath{\\mathcal{S}}\\xspace})$\\;\n\t\t$t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace t({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace})$\\;\n\t\t${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\t\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\n\t\t\\Report{$\\mathtt{ApplyDecay} ({\\ensuremath{P}\\xspace}, {\\ensuremath{\\lambda}\\xspace})$}\\;\n\t\t${\\ensuremath{{W}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{W}}\\xspace} \\cup \\{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\}$\\;\n\t}\n\n\n\t$({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace}) \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{{W}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\n\t\\Report{$\\mathtt{ApplyDecay} ({\\ensuremath{P}\\xspace}, {\\ensuremath{\\lambda}\\xspace})$}\\;\n}\n\\end{algorithm}\n\n\nThe {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework is presented below,\nas we only need to know the specifications of an index {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}},\nnot its internal workings.\nIn particular, we only assume the three primitives,\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}},\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}, and\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}, are available.\n\nThe {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework\nworks in time intervals of duration~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nDuring the $k$-th time interval\nit reads all vectors from the stream, and stores them in a buffer ${\\ensuremath{{W}}\\xspace}$.\nAt the end of the time interval,\nit invokes {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} to report all similar pairs in ${\\ensuremath{{W}}\\xspace}$,\nand to build an index ${\\ensuremath{\\mathcal{I}}\\xspace}$ on ${\\ensuremath{{W}}\\xspace}$.\nDuring the $(k+1)$-th time interval,\nthe buffer ${\\ensuremath{{W}}\\xspace}$ is reset and used to store the new vectors from the stream.\nAt the same time,\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} queries ${\\ensuremath{\\mathcal{I}}\\xspace}$ with\neach vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} read from the stream,\nto find\nsimilar pairs between {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and vectors in the previous time interval.\nSimilar pairs are computed by invoking the functions\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\\ and {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}.\nAt the end of the $(k+1)$-th time interval,\nthe index ${\\ensuremath{\\mathcal{I}}\\xspace}$ is replaced\nwith a new index on the vectors in the $(k+1)$-th time interval.\n\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} guarantees that all pairs of vectors\nwhose time difference is smaller than {{\\ensuremath{\\mathcal{\\tau}}\\xspace}}\nare tested for similarity.\nThus, due to the time-filtering property,\nit returns the complete set of similar pairs.\nAlgorithm~\\ref{algorithm:minibatchx} shows pseudocode for {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}.\n\nOne drawback of {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} is that\nit reports some similar pairs with a delay.\nIn particular,\nall similar pairs that span across two time intervals\nare reported after the end of the first interval.\nIn applications that require to report similar pairs as soon as both vectors are present,\nthis behavior is undesirable.\nMoreover, to guarantee correctness, {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} needs to tests pairs of vectors\nwhose time difference is as large as $2{\\ensuremath{\\mathcal{\\tau}}\\xspace}$, which can be pruned only\nafter they are reported by the indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace}, thus wasting computational power.\n\n\n\n\n\\section{Filtering framework}\n\\label{section:framework}\n\nWe now review the main indexing schemes\nused for the {{\\sc{apss}}\\xspace} problem.\n\nFor each indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace},\nwe describe its three phases \n({\\ensuremath{\\text{\\tt{IC}}}\\xspace}, {\\ensuremath{\\text{\\tt{CG}}}\\xspace}, {\\ensuremath{\\text{\\tt{CV}}}\\xspace}),\n\nand discuss how\nto adapt it to the streaming setting,\ngiving the {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} algorithm.\n\nFor all the indexing schemes we present, except {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, we provide pseudocode.\nDue to lack of space,\nand in order to highlight the differences among the indexing schemes,\nwe present our pseudocode using a color convention:\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index: all lines are included (black, {\\color{myred}{{red}}}, and {\\color{mygreen}{{green}}}).\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} index:\n{\\color{myred}{{red}}} lines are included; {\\color{mygreen}{{green}}} lines excluded.\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} index:\n{\\color{mygreen}{{green}}} lines are included; {\\color{myred}{{red}}} lines excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Inverted index}\n\\label{section:framework:inverted-index}\n\n\n\nThe first scheme \nis a simple \\emph{inverted index} ({{\\ensuremath{\\text{\\tt{INV}}}\\xspace}})\nwith no index-pruning optimizations.\nAs with all indexing schemes,\nthe main observation is that for two vectors to be similar,\nthey need to have at least one common coordinate.\nThus, two similar vectors can be found together in some list~${\\ensuremath{I}\\xspace}_j$.\n\nIn the {{\\ensuremath{\\text{\\tt{IC}}}\\xspace}} phase,\nfor each new vector~{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nall its coordinates ${\\ensuremath{\\mathrm{x}}\\xspace}_j$ are added to the index.\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase,\ngiven the query vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe use the index {{\\ensuremath{\\mathcal{I}}\\xspace}} to retrieve candidate vectors similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nIn particular, the candidates {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nare all vectors in the posting lists\nwhere {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} has non-zero coordinates.\n\n\nThe result of the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase is the exact similarity score\nbetween {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} and each candidate vector {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}.\nTherefore, the {\\ensuremath{\\text{\\tt{CV}}}\\xspace} phase\nsimply applies the similarity threshold {\\ensuremath{\\theta}\\xspace}\nand reports the true similar pairs.\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}):}}}\nThe functions\n{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, {{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, and {{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}},\nneeded to specify the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} algorithm\nfollow directly from the discussion above.\nSince they are rather straightforward,\nwe omit further details.\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}):}}}\nThe description of {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} given above considers\nthe {{{\\sc{apss}}\\xspace}} problem, i.e., a static dataset {\\ensuremath{\\mathcal{D}}\\xspace}.\nWe now consider a data stream {{\\ensuremath{\\mathcal{S}}\\xspace}},\nwhere each vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} in the stream is associated with a timestamp ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$.\nWe apply the {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} scheme\nby adding the data items in the index in the order they appear in the stream.\nThe lists ${\\ensuremath{I}\\xspace}_j$ of the index keep pairs\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j)$\nordered by timestamps ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$.\nMaintaining a time-respecting order inside the lists is easy.\nWe process the data in the same order,\nso we can can just append new vector coordinates at the end of the lists.\n\nBefore adding a new item {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to the index,\nwe use the index to retrieve all the earlier vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nthat are {\\ensuremath{\\Delta t}\\xspace}-similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nDue to the time-filtering property,\nvectors that are older than ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$ cannot be similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nThis observation has two implications:\n($i$)\nwe can stop retrieving candidate vectors from a list ${\\ensuremath{I}\\xspace}_j$\nupon encountering a vector that is older than ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$;\nand\n($ii$)\nwhen encountering such a vector,\n\n\nthe part of the list preceding it can be pruned away.\n\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:ic-unified}\n\\Input{Dataset {\\ensuremath{\\mathcal{D}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{Similarity index {{\\ensuremath{\\mathcal{I}}\\xspace}} on {{\\ensuremath{\\mathcal{D}}\\xspace}}, and \\\\\nset of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n\n${\\ensuremath{\\mathcal{I}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\n\n\n\n\\ForEach{${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace}$}{\n\t${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{P}\\xspace} \\cup \\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\t{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0;  \\; {\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n\t\\ForEach{$j=1\\ldots{\\ensuremath{d}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n\t\t${\\ensuremath{\\mathtt{pscore}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\}$\\;\n\t\t{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_1 + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, \\min\\{{\\ensuremath{\\mathrm{{m}}}\\xspace}_j,{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}\\}$}}} \\label{line:ic-unified:apbound}\\;\n\t\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_t + {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2};  \\; {\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{b}}}\\xspace}_t}$}}}\\;\n\t\t\\If{$\\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\} \\ge{\\ensuremath{\\theta}\\xspace}$} {\n\t\t\t\\If{${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] = \\emptyset$} {\n\t\t\t\t${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j$\\;\n\t\t\t\t{\\color{mygreen}{{${\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathtt{pscore}}\\xspace}$}}} \\label{line:ic-unified:ps}\\;\n\t\t\t}\n\t\t\t${\\ensuremath{I}\\xspace}_j \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{I}\\xspace}_j\\cup \\{({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j,||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)\\}$\\;\n\t\t}\n\t}\n}\n\\Return $({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{P}\\xspace})$\\;\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:cg-unified}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{\nAccum.\\ score array ${\\ensuremath{C}\\xspace}$ for candidate vectors \\\\\n(candidate set is ${\\ensuremath{C}\\xspace}=\\{ {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace} \\mid {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0\\})$\n}\n${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{{\\mathrm{sz}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\theta}\\xspace} / {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}$\\;\n\n\n{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace})$}}}\\;\n{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1;  \\; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1$}}}\\;\n\\ForEach(\\CommentSty{//reverse order}){$j={\\ensuremath{d}\\xspace} \\ldots 1 \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n \t\\ForEach{$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{y}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||) \\in {\\ensuremath{I}\\xspace}_j$}{\n\t\t${\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_2}}}\\}$\\;\n \t\t\\If{$|{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}| \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} \\ge {\\ensuremath{{\\mathrm{sz}}}\\xspace}_1$ \\label{line:cg-unified:lowerbound}} {\n \t\t\t\\If{${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0 \\text{ \\em or } {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ge{\\ensuremath{\\theta}\\xspace}$ \\label{line:cg-unified:upperbound}} {\n\t\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\mathrm{y}}\\xspace}_j$\\;\n\t\t\t\t{\\color{mygreen}{{ \t\t\t\t\t${\\ensuremath{\\mathtt{l2bound}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j|| \\, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||$\\; \t\t\t\t\t\\If{${\\ensuremath{\\mathtt{l2bound}}\\xspace} < {\\ensuremath{\\theta}\\xspace}$\\label{line:cg-unified:earlypruning}} { \t\t\t\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$\\; \t\t\t\t\t} \t\t\t\t}}}\n \t\t\t}\n \t\t}\n \t}\n \t{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 - {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}}\\xspace}_j$}}}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t - {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2};  \\; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_t}$}}} \\;\n\\label{line:cg-unified:end}\n}\n\n\\Return ${\\ensuremath{C}\\xspace}$;\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:cv-unified}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, candidate vector array ${\\ensuremath{C}\\xspace}$, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{Set of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\n\\ForEach{${\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] > 0$\\label{line:cv-unified:start}}{\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]$}}}\\;\n\t${\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\}$\\;\n\t${\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|,|{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}|\\} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}$\\;\n\t\\If{${\\color{mygreen}{{{\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace}}}} \\Andkw {\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace} \\Andkw {\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ge {\\ensuremath{\\theta}\\xspace})$\\label{line:cv-unified:dpscorebound}} {\n\t\t$s \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace})$\\;\n\t\t\\If{$(s\\ge {\\ensuremath{\\theta}\\xspace})$} {\n\t\t\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{P}\\xspace} \\cup \\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace},s)\\}$\\;\n\t\t}\n\t}\n\\label{line:cv-unified:end}}\n\\Return {\\ensuremath{P}\\xspace}\\;\n\\end{algorithm}\n\n\n\\subsection{All-pairs indexing scheme}\n\\label{section:framework:all-pairs}\n\nThe {\\ensuremath{\\text{\\tt{AP}}}\\xspace}~\\citep{bayardo2007scaling-up} scheme\n\nimproves over the simple {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} method\nby reducing the size of the index.\n\n\nWhen using {\\ensuremath{\\text{\\tt{AP}}}\\xspace}, not all the coordinates of a vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} need to be indexed,\nas long as it can be guaranteed that\n{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and all its similar vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\n\nshare at least one common coordinate in the index {{\\ensuremath{\\mathcal{I}}\\xspace}}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly to the {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} scheme,\n{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} incrementally builds an index by processing one vector at a time.\n\\iffalse\nDue to the index-filtering optimization, not all coordinates of all vectors are added to the index.\n\n\n\nThe coordinates that are not added to the index,\nare kept in the residual direct index {{\\ensuremath{\\mathcal{R}}\\xspace}} so as to complete similarity computations.\n\\fi\nIn the {\\ensuremath{\\text{\\tt{IC}}}\\xspace} phase\n(function {\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}, shown as Algorithm~\\ref{algorithm:ic-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\nfor each new vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nthe algorithm scans its coordinates in a predefined order.\nIt keeps a score {{\\ensuremath{\\mathtt{pscore}}\\xspace}},\nwhich represents an upper bound on the similarity between a prefix of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}\nand any other vector in the dataset.\nTo compute the upper bound, {\\ensuremath{\\text{\\tt{AP}}}\\xspace} uses the vector {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}},\nwhich keeps the maximum of each coordinate in the dataset.\nAs long as {\\ensuremath{\\mathtt{pscore}}\\xspace} is smaller than the threshold {{\\ensuremath{\\theta}\\xspace}},\ngiven that the similarity of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to any other vector cannot exceed {{\\ensuremath{\\theta}\\xspace}},\nthe coordinates of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} scanned so far can be omitted from the index\nwithout the danger of missing any similar pair.\nAs soon as {{\\ensuremath{\\mathtt{pscore}}\\xspace}} exceeds {{\\ensuremath{\\theta}\\xspace}},\nthe remaining suffix of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is added to the index,\nand the prefix {\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace} is saved in the residual direct index~{\\ensuremath{\\mathcal{R}}\\xspace}.\n\n\n\n\n\n\n\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase\n(function {\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}, shown as Algorithm~\\ref{algorithm:cg-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\n{\\ensuremath{\\text{\\tt{AP}}}\\xspace} uses a lower bound ${\\ensuremath{{\\mathrm{sz}}}\\xspace}_1$ for the size of any vector {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nthat is similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nso that vectors that have too few non-zero entries can be ignored\n(line~\\ref{line:cg-unified:lowerbound}).\nAdditionally, it uses a variable ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$\nto keep an upper bound on the similarity\nbetween {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and any other vector {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}.\nThis upper bound is computed by using the residual direct index {\\ensuremath{\\mathcal{R}}\\xspace}\nand the already accumulated dot-product,\nand is updated as the algorithm processes the different posting lists.\nWhen the upper bound becomes smaller than {{\\ensuremath{\\theta}\\xspace}},\nvectors that have not already been added to the set of candidates\ncan be ignored (line~\\ref{line:cg-unified:upperbound}).\nThe array ${\\ensuremath{C}\\xspace}$ holds the candidate vectors,\ntogether with the partial dot-product that is due to the indexed part of the vectors.\n\nFinally, in the {\\ensuremath{\\text{\\tt{CV}}}\\xspace} phase\n({\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}\\ function, shown as Algorithm~\\ref{algorithm:cv-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\nwe compute the final similarities by using the residual index {\\ensuremath{\\mathcal{R}}\\xspace},\nand report the true similar pairs.\n\nThe streaming versions of {\\ensuremath{\\text{\\tt{AP}}}\\xspace},\nin both {\\ensuremath{\\text{\\tt{MB}}}\\xspace} and {\\ensuremath{\\text{\\tt{STR}}}\\xspace} frameworks,\nare not efficient in practice,\nand thus, we omit further details.\nInstead, we proceed presenting the next scheme,\nthe {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} index, which is a generalization of {\\ensuremath{\\text{\\tt{AP}}}\\xspace}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{L2-based indexing scheme}\n\\label{section:framework:l2ap}\n\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}~\\citep{anastasiu2014l2ap}\nis the state-of-the-art for computing similarity self-join.\n\n\nThe scheme uses tighter {\\ensuremath{\\ell_2}\\xspace}-bounds,\nwhich reduce the size of the index,\nbut also the number of generated candidates\nand the number of fully-computed similarities.\n\nThe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} scheme primarily leverages the\nCauchy-Schwarz inequality, which states that\n${\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\leq ||{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|| \\, ||{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}||$.\nThe same bound applies when considering a prefix of a query vector~{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}.\nSince we consider unit-normalized vectors ($||{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}||=1$),\n", "itemtype": "equation", "pos": 23909, "prevtext": "\nThus, ${\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} > {\\ensuremath{\\lambda}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$\nimplies ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})<{\\ensuremath{\\theta}\\xspace}$,\nand as a result\na given vector cannot be similar to any vector that arrived\nmore than\n", "index": 7, "text": "\n\\[\n{\\ensuremath{\\mathcal{\\tau}}\\xspace} = \\frac{1}{{\\ensuremath{\\lambda}\\xspace}} \\log \\frac{1}{{\\ensuremath{\\theta}\\xspace}}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{\\tau}}=\\frac{1}{{\\lambda}}\\log\\frac{1}{{\\theta}}\" display=\"block\"><mrow><mi>\u03c4</mi><mo>=</mo><mrow><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mfrac><mn>1</mn><mi>\u03b8</mi></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\n\nThe previous bound produces a tighter value for {{\\ensuremath{\\mathtt{pscore}}\\xspace}},\nwhich is used in the {\\ensuremath{\\text{\\tt{IC}}}\\xspace} phase\nto bound the similarity of the vector currently being indexed,\nto the rest of the dataset.\nIn particular,  {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} sets\n${\\ensuremath{\\mathtt{pscore}}\\xspace} = \\min\\{{\\ensuremath{\\mathtt{pscore}}\\xspace}_{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}||\\}$.\n\n\nAdditionally,\nthe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index stores the value of {{\\ensuremath{\\mathtt{pscore}}\\xspace}} computed\n\nwhen {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is indexed.\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} keeps these values in an array {{\\ensuremath{Q}\\xspace}}, index by ${\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$,\nas shown in line~\\ref{line:ic-unified:ps} of Algorithm~\\ref{algorithm:ic-unified}.\nThe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index also\nstores in the index the magnitude of the prefix of each vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, up to coordinate $j$.\nThat is,\nthe entries of the posting lists are now triples of the type\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{x}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)$.\n\nBoth pieces of additional information,\nthe array {{\\ensuremath{Q}\\xspace}} and the values $||{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_j||$ stored in the posting lists,\nare used during the {{\\ensuremath{\\text{\\tt{CG}}}\\xspace}} phase\nto reduce the number of candidates.\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase, for a given query vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe scan its coordinates backwards,\ni.e., in reverse order with respect to the one used during indexing,\nand we accumulate similarity scores for suffixes of {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}.\nWe keep a {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} bound on the remaining similarity score,\nwhich combines the ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$ bound used in {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}\nand a new {\\ensuremath{\\ell_2}\\xspace}-based ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2$ bound that uses the prefix magnitude values\n($||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||$)\nstored in the posting lists.\nThe {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} bound is an upper bound on the similarity\nof the prefix of the current query vector and any other vector in the index,\nthus as long as {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} is smaller than {\\ensuremath{\\theta}\\xspace}, the algorithm can prune the candidate.\n\nPseudocode for the three phases of {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}}, {\\ensuremath{\\text{\\tt{IC}}}\\xspace}, {\\ensuremath{\\text{\\tt{CG}}}\\xspace}, and {\\ensuremath{\\text{\\tt{CV}}}\\xspace},\nis shown in Algorithms~\\ref{algorithm:ic-unified},\n\\ref{algorithm:cg-unified}, and~\\ref{algorithm:cv-unified}, respectively,\nincluding both {\\color{myred}{{red}}} and {\\color{mygreen}{{green}}} lines.\n\nMore details on the scheme\ncan be found in the original paper of~\\citet{anastasiu2014l2ap}.\n\n\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}):}}}\nAs before, the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} algorithm\nis a direct instantiation of the generic\nAlgorithm~\\ref{algorithm:minibatchx},\nusing the functions that implement the three different phases of {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}}.\n\n\n\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}):}}}\nTo describe the modifications required for adapting\nthe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} scheme in the streaming framework,\nwe need to introduce some additional notation.\n\nFirst, we assume that the input is a stream {{\\ensuremath{\\mathcal{S}}\\xspace}} of vectors.\nThe main difference is that now both {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} and {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} are function of time,\nas the maximum values in the stream evolve over time.\nWe adapt the use of the two vectors to the streaming case differently.\n\nThe vector {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} is used in the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase to generate candidate vectors.\nGiven that we are looking at vectors that are already indexed (i.e., in the \\emph{past}),\nit is possible to apply the definition of {\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace} between the query vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} and {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}.\nIn particular, given that the coordinates of {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} originate from different vectors in the index,\nwe can apply the decay factor to each coordinate to {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} separately.\n\nMore formally, let ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}(t)$ be the \\emph{worst case indexed vector} at time $t$,\nthat is, ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}(t)$ is the representation of the vector in the index\nthat is most {\\ensuremath{\\Delta t}\\xspace}-similar to any vector in {\\ensuremath{\\mathcal{S}}\\xspace} arriving at time $t$.\nIts $j$-th coordinate ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_j(t)$ \n\nis given by\n", "itemtype": "equation", "pos": 66903, "prevtext": "\ntime units earlier.\nConsequently, we can safely prune vectors that are older than {\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nWe call {\\ensuremath{\\mathcal{\\tau}}\\xspace} the \\emph{time horizon}.\n\n\n\n\n{\\smallskip\\noindent\\textbf{{Parameter setting.}}}\nThe {{\\sc{sssj}}\\xspace} problem, defined in Problem~\\ref{problem:streaming-apss},\nrequires two parameters: a similarity threshold {\\ensuremath{\\theta}\\xspace}\nand a time-decay factor {\\ensuremath{\\lambda}\\xspace}.\n\nThe time-filtering property\nsuggests a simple methodology to set these two parameters:\n\n\\begin{squishlist}\n\\item [1.]\nSelect \n${\\ensuremath{\\theta}\\xspace}$\nas the lowest value of the similarity\nbetween two \\emph{simultaneously-arriving}\nvectors that are deemed \\emph{similar}.\n\\item [2.]\nSelect \n${\\ensuremath{\\mathcal{\\tau}}\\xspace}$\nas the smallest difference in arrival times\nbetween two \\emph{identical}\nvectors that are deemed \\emph{dissimilar}.\n\\item [3.]\nSet ${\\ensuremath{\\lambda}\\xspace} = {\\ensuremath{\\mathcal{\\tau}}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$.\n\\end{squishlist}\nSetting ${\\ensuremath{\\theta}\\xspace}$ and  ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$ in steps 1 and 2\ndepends on the application.\n\n{\\smallskip\\noindent\\textbf{{Additional notation.}}}\nWhen referring to the non-streaming setting,\nwe consider a dataset\n${\\ensuremath{\\mathcal{D}}\\xspace}=\\{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_1,\\ldots,{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_{\\ensuremath{n}\\xspace}\\}$\nconsisting of {{\\ensuremath{n}\\xspace}} vectors in ${\\ensuremath{\\mathbb{R}}\\xspace}^{\\ensuremath{d}\\xspace}$.\n\nFollowing \\citeauthor{anastasiu2014l2ap},\nwe use the notation\n${\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}={\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_p=\\langle{\\ensuremath{\\mathrm{x}}\\xspace}_1,..,{\\ensuremath{\\mathrm{x}}\\xspace}_{p-1},0,..,0\\rangle$\nand\n\nto denote the \\emph{prefix} \nof a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}. \nFor a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\nwe denote by\n${\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}$ its maximum coordinate, by\n${\\Sigma}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} = \\sum_{j}{\\ensuremath{\\mathrm{x}}\\xspace}_j$ the sum of its coordinates, and by\n${\\ensuremath{|{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}|}\\xspace}$ the number of its non-zero coordinates.\n\nGiven a static dataset {{\\ensuremath{\\mathcal{D}}\\xspace}} we use ${\\ensuremath{\\mathrm{{m}}}\\xspace}_{j}$\nto refer to the maximum value along the $j$-th coordinate over all vectors in~{{\\ensuremath{\\mathcal{D}}\\xspace}}.\nAll values of ${\\ensuremath{\\mathrm{{m}}}\\xspace}_{j}$ together compose the vector ${\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}$.\nOur methods use indexing schemes\nwhich build an index incrementally, vector-by-vector.\nWe use ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}$ to refer to the vector ${\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}$,\nrestricted to the dataset that is already indexed,\nand we write ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}}\\xspace}_{j}$ for its $j$-th coordinate.\nFurthermore,\nwe use ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}$ (and ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_{j}$ for its $j$-th coordinate)\nto denote a time-decayed variant of ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}$,\nwhose precise definition is given in Section~\\ref{section:framework:l2ap}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Overview of the approach}\n\\label{section:overview}\n\nIn this section we present\na high-level overview of\nour main algorithms for the {{{\\sc{sssj}}\\xspace}} problem.\n\nWe present two different algorithmic frameworks:\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} (MiniBatch-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}) and {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} (Streaming-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}),\nwhere {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} is an indexing method for the {{{\\sc{apss}}\\xspace}} problem on static data.\nTo make the presentation of our algorithms more clear,\nwe first give an overview of the indexing methods for the static {{{\\sc{apss}}\\xspace}} problem,\nas introduced in earlier\npapers~\\cite{anastasiu2014l2ap,bayardo2007scaling-up,chaudhuri2006primitive}.\n\nAll indexing schemes \nare based on building an \\emph{inverted index}.\n\nThe index is a collection of {\\ensuremath{d}\\xspace} \\emph{posting lists},\n${\\ensuremath{\\mathcal{I}}\\xspace}=\\{{\\ensuremath{I}\\xspace}_1,\\ldots,{\\ensuremath{I}\\xspace}_{\\ensuremath{d}\\xspace}\\}$,\n\nwhere the list ${\\ensuremath{I}\\xspace}_j$ contains all pairs\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j)$\nsuch that the $j$-th coordinate of vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is non-zero, i.e., ${\\ensuremath{\\mathrm{x}}\\xspace}_j \\neq 0$.\nHere ${\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$ denotes a reference to vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\n\n\\iffalse\nThis representation of the data already provides an improvement over the na\\\"{i}ve approach of checking every pair.\nNamely, when querying the index with a vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, any other vector {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} that has no non-zero\ncoordinate shared with {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} will be ignored.\n\\note[gdfm]{Previous paragraph might not be needed here, as it is explained in the next section}\n\\fi\n\nSome methods optimize computation by not indexing the whole dataset.\nIn these cases, the un-indexed part is still needed in order to compute exact similarities.\n\nThus, we assume that a separate part of the index {{\\ensuremath{\\mathcal{I}}\\xspace}}\ncontains the un-indexed part of the dataset,\ncalled \\emph{residual direct index} {\\ensuremath{\\mathcal{R}}\\xspace}.\n\nAll schemes build the index incrementally,\nwhile also computing similar pairs.\nIn particular,\nwe start with an empty index,\nand we iteratively process the vectors in {{\\ensuremath{\\mathcal{D}}\\xspace}}.\nFor each newly-processed vector~{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe compute similar pairs $({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$\nfor all vectors~{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} that are already in the index.\nThereafter, we add\n(some of) the non-zero coordinates of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to the index.\n\nBuilding the index and computing similar pairs\ncan be seen as a three-phase process:\n\n\\begin{squishlist}\n\\item [\\textbf{index construction ({\\ensuremath{\\text{\\tt{IC}}}\\xspace})}:]\nadds new vectors to the \nindex~{{\\ensuremath{\\mathcal{I}}\\xspace}}.\n\\item [\\textbf{candidate generation ({\\ensuremath{\\text{\\tt{CG}}}\\xspace})}:]\nuses the index {{\\ensuremath{\\mathcal{I}}\\xspace}} to generate candidate similar pairs.\nThe candidate pairs may contain false positives\nbut no false negatives.\n\\item [\\textbf{candidate verification ({\\ensuremath{\\text{\\tt{CV}}}\\xspace})}:]\ncomputes true similarities between candidate pairs,\nand reports true similar pairs,\nwhile dismissing false positives.\n\\end{squishlist}\n\nMore concretely,\nfor an indexing scheme {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nwe assume that the following three primitives are available,\nwhich correspond to the three phases outlined above:\n\n\\begin{squishlist}\n\\item[$({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace}) \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{D}}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven a dataset ${\\ensuremath{\\mathcal{D}}\\xspace}$\nconsisting of {\\ensuremath{n}\\xspace} vectors,\nand a similarity threshold {\\ensuremath{\\theta}\\xspace},\nthe function {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\nreturns in ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\}$ all similar pairs $({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})$,\nwith ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace}$.\nAdditionally, {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}\nbuilds an index {{\\ensuremath{\\mathcal{I}}\\xspace}},\nwhich can be used to find similar pairs between the vectors in {{\\ensuremath{\\mathcal{D}}\\xspace}}\nand another query vector~{\\ensuremath{\\mathbf{{z}}}\\xspace}.\n\n\n\n\n\\item[${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven an index {{\\ensuremath{\\mathcal{I}}\\xspace}}, built on a dataset {\\ensuremath{\\mathcal{D}}\\xspace},\na vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\nand a similarity threshold~{\\ensuremath{\\theta}\\xspace},\nthe function {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} returns\na  set of candidate vectors ${\\ensuremath{C}\\xspace}=\\{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\}$,\nwhich is a superset of all vectors that are similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\n\n\\item[${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$:]\nGiven an index {{\\ensuremath{\\mathcal{I}}\\xspace}},\nbuilt on a dataset {\\ensuremath{\\mathcal{D}}\\xspace},\na vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\na set of candidate vectors~${\\ensuremath{C}\\xspace}$,\nand a similarity threshold~{\\ensuremath{\\theta}\\xspace},\nthe function {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} returns\nthe set\n${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\}$ of true similar pairs.\n\\end{squishlist}\n\n\nBoth proposed frameworks,\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} and {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}},\nrely on an indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace},\nand adapt it to the streaming setting by\naugmenting it with the time-filtering property.\nThe difference between the two frameworks is in how this adaptation is done.\n\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} uses {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} as a ``black box'':\nit uses time filtering in order to\nbuild independent instances of {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}} indexes,\nand drops them when they become obsolete.\nConversely,\nfor {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}\nwe opportunely modify the indexing method {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nby directly applying time filtering.\n\nThe presentation of the {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework\nis deferred to the next section,\nafter we present the details of the specific indexing methods {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}\nwe use.\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\\ (MiniBatch with index {\\ensuremath{\\text{\\tt{IDX}}}\\xspace})}\n\\label{algorithm:minibatchx}\n\\Input{Data stream {\\ensuremath{\\mathcal{S}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{All pairs ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{S}}\\xspace}$ s.t.\\ ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$}\n\n${\\ensuremath{\\mathcal{I}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n$t_0 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0; t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$\\;\n${\\ensuremath{\\mathcal{\\tau}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\lambda}\\xspace}^{-1} \\log {\\ensuremath{\\theta}\\xspace}^{-1}$\\;\n\n\\While{$\\text{\\emph{true}}$} {\n\t${\\ensuremath{{W}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\t$t_0 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace t_0 + {\\ensuremath{\\mathcal{\\tau}}\\xspace}$\\;\n\t\\While{$t \\le t_0 + {\\ensuremath{\\mathcal{\\tau}}\\xspace}$} {\n\t\t${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\texttt{read}({\\ensuremath{\\mathcal{S}}\\xspace})$\\;\n\t\t$t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace t({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace})$\\;\n\t\t${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\t\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\n\t\t\\Report{$\\mathtt{ApplyDecay} ({\\ensuremath{P}\\xspace}, {\\ensuremath{\\lambda}\\xspace})$}\\;\n\t\t${\\ensuremath{{W}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{W}}\\xspace} \\cup \\{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\}$\\;\n\t}\n\n\n\t$({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace}) \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}({\\ensuremath{{W}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\n\t\\Report{$\\mathtt{ApplyDecay} ({\\ensuremath{P}\\xspace}, {\\ensuremath{\\lambda}\\xspace})$}\\;\n}\n\\end{algorithm}\n\n\nThe {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework is presented below,\nas we only need to know the specifications of an index {{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}},\nnot its internal workings.\nIn particular, we only assume the three primitives,\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}},\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}, and\n{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}}, are available.\n\nThe {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} framework\nworks in time intervals of duration~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nDuring the $k$-th time interval\nit reads all vectors from the stream, and stores them in a buffer ${\\ensuremath{{W}}\\xspace}$.\nAt the end of the time interval,\nit invokes {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} to report all similar pairs in ${\\ensuremath{{W}}\\xspace}$,\nand to build an index ${\\ensuremath{\\mathcal{I}}\\xspace}$ on ${\\ensuremath{{W}}\\xspace}$.\nDuring the $(k+1)$-th time interval,\nthe buffer ${\\ensuremath{{W}}\\xspace}$ is reset and used to store the new vectors from the stream.\nAt the same time,\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} queries ${\\ensuremath{\\mathcal{I}}\\xspace}$ with\neach vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} read from the stream,\nto find\nsimilar pairs between {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and vectors in the previous time interval.\nSimilar pairs are computed by invoking the functions\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}\\ and {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}.\nAt the end of the $(k+1)$-th time interval,\nthe index ${\\ensuremath{\\mathcal{I}}\\xspace}$ is replaced\nwith a new index on the vectors in the $(k+1)$-th time interval.\n\n{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} guarantees that all pairs of vectors\nwhose time difference is smaller than {{\\ensuremath{\\mathcal{\\tau}}\\xspace}}\nare tested for similarity.\nThus, due to the time-filtering property,\nit returns the complete set of similar pairs.\nAlgorithm~\\ref{algorithm:minibatchx} shows pseudocode for {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}.\n\nOne drawback of {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} is that\nit reports some similar pairs with a delay.\nIn particular,\nall similar pairs that span across two time intervals\nare reported after the end of the first interval.\nIn applications that require to report similar pairs as soon as both vectors are present,\nthis behavior is undesirable.\nMoreover, to guarantee correctness, {\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace} needs to tests pairs of vectors\nwhose time difference is as large as $2{\\ensuremath{\\mathcal{\\tau}}\\xspace}$, which can be pruned only\nafter they are reported by the indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace}, thus wasting computational power.\n\n\n\n\n\\section{Filtering framework}\n\\label{section:framework}\n\nWe now review the main indexing schemes\nused for the {{\\sc{apss}}\\xspace} problem.\n\nFor each indexing scheme {\\ensuremath{\\text{\\tt{IDX}}}\\xspace},\nwe describe its three phases \n({\\ensuremath{\\text{\\tt{IC}}}\\xspace}, {\\ensuremath{\\text{\\tt{CG}}}\\xspace}, {\\ensuremath{\\text{\\tt{CV}}}\\xspace}),\n\nand discuss how\nto adapt it to the streaming setting,\ngiving the {{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}} algorithm.\n\nFor all the indexing schemes we present, except {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, we provide pseudocode.\nDue to lack of space,\nand in order to highlight the differences among the indexing schemes,\nwe present our pseudocode using a color convention:\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index: all lines are included (black, {\\color{myred}{{red}}}, and {\\color{mygreen}{{green}}}).\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} index:\n{\\color{myred}{{red}}} lines are included; {\\color{mygreen}{{green}}} lines excluded.\n\n\\noindent\n$-$ {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} index:\n{\\color{mygreen}{{green}}} lines are included; {\\color{myred}{{red}}} lines excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Inverted index}\n\\label{section:framework:inverted-index}\n\n\n\nThe first scheme \nis a simple \\emph{inverted index} ({{\\ensuremath{\\text{\\tt{INV}}}\\xspace}})\nwith no index-pruning optimizations.\nAs with all indexing schemes,\nthe main observation is that for two vectors to be similar,\nthey need to have at least one common coordinate.\nThus, two similar vectors can be found together in some list~${\\ensuremath{I}\\xspace}_j$.\n\nIn the {{\\ensuremath{\\text{\\tt{IC}}}\\xspace}} phase,\nfor each new vector~{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nall its coordinates ${\\ensuremath{\\mathrm{x}}\\xspace}_j$ are added to the index.\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase,\ngiven the query vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe use the index {{\\ensuremath{\\mathcal{I}}\\xspace}} to retrieve candidate vectors similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nIn particular, the candidates {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nare all vectors in the posting lists\nwhere {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} has non-zero coordinates.\n\n\nThe result of the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase is the exact similarity score\nbetween {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} and each candidate vector {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}.\nTherefore, the {\\ensuremath{\\text{\\tt{CV}}}\\xspace} phase\nsimply applies the similarity threshold {\\ensuremath{\\theta}\\xspace}\nand reports the true similar pairs.\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}):}}}\nThe functions\n{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, {{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}}, and {{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}},\nneeded to specify the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} algorithm\nfollow directly from the discussion above.\nSince they are rather straightforward,\nwe omit further details.\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{INV}}}\\xspace}):}}}\nThe description of {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} given above considers\nthe {{{\\sc{apss}}\\xspace}} problem, i.e., a static dataset {\\ensuremath{\\mathcal{D}}\\xspace}.\nWe now consider a data stream {{\\ensuremath{\\mathcal{S}}\\xspace}},\nwhere each vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} in the stream is associated with a timestamp ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$.\nWe apply the {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} scheme\nby adding the data items in the index in the order they appear in the stream.\nThe lists ${\\ensuremath{I}\\xspace}_j$ of the index keep pairs\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j)$\nordered by timestamps ${\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$.\nMaintaining a time-respecting order inside the lists is easy.\nWe process the data in the same order,\nso we can can just append new vector coordinates at the end of the lists.\n\nBefore adding a new item {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to the index,\nwe use the index to retrieve all the earlier vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nthat are {\\ensuremath{\\Delta t}\\xspace}-similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nDue to the time-filtering property,\nvectors that are older than ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$ cannot be similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}.\nThis observation has two implications:\n($i$)\nwe can stop retrieving candidate vectors from a list ${\\ensuremath{I}\\xspace}_j$\nupon encountering a vector that is older than ${\\ensuremath{\\mathcal{\\tau}}\\xspace}$;\nand\n($ii$)\nwhen encountering such a vector,\n\n\nthe part of the list preceding it can be pruned away.\n\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:ic-unified}\n\\Input{Dataset {\\ensuremath{\\mathcal{D}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{Similarity index {{\\ensuremath{\\mathcal{I}}\\xspace}} on {{\\ensuremath{\\mathcal{D}}\\xspace}}, and \\\\\nset of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n\n${\\ensuremath{\\mathcal{I}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\n\n\n\n\\ForEach{${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace}$}{\n\t${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{P}\\xspace} \\cup \\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace})$\\;\n\n\t{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0;  \\; {\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n\t\\ForEach{$j=1\\ldots{\\ensuremath{d}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n\t\t${\\ensuremath{\\mathtt{pscore}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\}$\\;\n\t\t{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_1 + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, \\min\\{{\\ensuremath{\\mathrm{{m}}}\\xspace}_j,{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}\\}$}}} \\label{line:ic-unified:apbound}\\;\n\t\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_t + {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2};  \\; {\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{b}}}\\xspace}_t}$}}}\\;\n\t\t\\If{$\\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\} \\ge{\\ensuremath{\\theta}\\xspace}$} {\n\t\t\t\\If{${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] = \\emptyset$} {\n\t\t\t\t${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j$\\;\n\t\t\t\t{\\color{mygreen}{{${\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathtt{pscore}}\\xspace}$}}} \\label{line:ic-unified:ps}\\;\n\t\t\t}\n\t\t\t${\\ensuremath{I}\\xspace}_j \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{I}\\xspace}_j\\cup \\{({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace},{\\ensuremath{\\mathrm{x}}\\xspace}_j,||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)\\}$\\;\n\t\t}\n\t}\n}\n\\Return $({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{P}\\xspace})$\\;\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:cg-unified}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{\nAccum.\\ score array ${\\ensuremath{C}\\xspace}$ for candidate vectors \\\\\n(candidate set is ${\\ensuremath{C}\\xspace}=\\{ {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace} \\mid {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0\\})$\n}\n${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{{\\mathrm{sz}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\theta}\\xspace} / {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}$\\;\n\n\n{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace})$}}}\\;\n{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1;  \\; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1$}}}\\;\n\\ForEach(\\CommentSty{//reverse order}){$j={\\ensuremath{d}\\xspace} \\ldots 1 \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n \t\\ForEach{$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{y}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||) \\in {\\ensuremath{I}\\xspace}_j$}{\n\t\t${\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_2}}}\\}$\\;\n \t\t\\If{$|{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}| \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} \\ge {\\ensuremath{{\\mathrm{sz}}}\\xspace}_1$ \\label{line:cg-unified:lowerbound}} {\n \t\t\t\\If{${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0 \\text{ \\em or } {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ge{\\ensuremath{\\theta}\\xspace}$ \\label{line:cg-unified:upperbound}} {\n\t\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\mathrm{y}}\\xspace}_j$\\;\n\t\t\t\t{\\color{mygreen}{{ \t\t\t\t\t${\\ensuremath{\\mathtt{l2bound}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j|| \\, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||$\\; \t\t\t\t\t\\If{${\\ensuremath{\\mathtt{l2bound}}\\xspace} < {\\ensuremath{\\theta}\\xspace}$\\label{line:cg-unified:earlypruning}} { \t\t\t\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$\\; \t\t\t\t\t} \t\t\t\t}}}\n \t\t\t}\n \t\t}\n \t}\n \t{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 - {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}}\\xspace}_j$}}}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t - {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2};  \\; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_t}$}}} \\;\n\\label{line:cg-unified:end}\n}\n\n\\Return ${\\ensuremath{C}\\xspace}$;\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}}\n\\label{algorithm:cv-unified}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, candidate vector array ${\\ensuremath{C}\\xspace}$, threshold {\\ensuremath{\\theta}\\xspace}}\n\\Output{Set of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{\\mathrm{sim}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\n\\ForEach{${\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] > 0$\\label{line:cv-unified:start}}{\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]$}}}\\;\n\t${\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\}$\\;\n\t${\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|,|{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}|\\} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}$\\;\n\t\\If{${\\color{mygreen}{{{\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace}}}} \\Andkw {\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace} \\Andkw {\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ge {\\ensuremath{\\theta}\\xspace})$\\label{line:cv-unified:dpscorebound}} {\n\t\t$s \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace})$\\;\n\t\t\\If{$(s\\ge {\\ensuremath{\\theta}\\xspace})$} {\n\t\t\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{P}\\xspace} \\cup \\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace},s)\\}$\\;\n\t\t}\n\t}\n\\label{line:cv-unified:end}}\n\\Return {\\ensuremath{P}\\xspace}\\;\n\\end{algorithm}\n\n\n\\subsection{All-pairs indexing scheme}\n\\label{section:framework:all-pairs}\n\nThe {\\ensuremath{\\text{\\tt{AP}}}\\xspace}~\\citep{bayardo2007scaling-up} scheme\n\nimproves over the simple {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} method\nby reducing the size of the index.\n\n\nWhen using {\\ensuremath{\\text{\\tt{AP}}}\\xspace}, not all the coordinates of a vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} need to be indexed,\nas long as it can be guaranteed that\n{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and all its similar vectors {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\n\nshare at least one common coordinate in the index {{\\ensuremath{\\mathcal{I}}\\xspace}}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly to the {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} scheme,\n{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} incrementally builds an index by processing one vector at a time.\n\\iffalse\nDue to the index-filtering optimization, not all coordinates of all vectors are added to the index.\n\n\n\nThe coordinates that are not added to the index,\nare kept in the residual direct index {{\\ensuremath{\\mathcal{R}}\\xspace}} so as to complete similarity computations.\n\\fi\nIn the {\\ensuremath{\\text{\\tt{IC}}}\\xspace} phase\n(function {\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}, shown as Algorithm~\\ref{algorithm:ic-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\nfor each new vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nthe algorithm scans its coordinates in a predefined order.\nIt keeps a score {{\\ensuremath{\\mathtt{pscore}}\\xspace}},\nwhich represents an upper bound on the similarity between a prefix of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}\nand any other vector in the dataset.\nTo compute the upper bound, {\\ensuremath{\\text{\\tt{AP}}}\\xspace} uses the vector {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}},\nwhich keeps the maximum of each coordinate in the dataset.\nAs long as {\\ensuremath{\\mathtt{pscore}}\\xspace} is smaller than the threshold {{\\ensuremath{\\theta}\\xspace}},\ngiven that the similarity of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to any other vector cannot exceed {{\\ensuremath{\\theta}\\xspace}},\nthe coordinates of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} scanned so far can be omitted from the index\nwithout the danger of missing any similar pair.\nAs soon as {{\\ensuremath{\\mathtt{pscore}}\\xspace}} exceeds {{\\ensuremath{\\theta}\\xspace}},\nthe remaining suffix of {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is added to the index,\nand the prefix {\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace} is saved in the residual direct index~{\\ensuremath{\\mathcal{R}}\\xspace}.\n\n\n\n\n\n\n\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase\n(function {\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}, shown as Algorithm~\\ref{algorithm:cg-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\n{\\ensuremath{\\text{\\tt{AP}}}\\xspace} uses a lower bound ${\\ensuremath{{\\mathrm{sz}}}\\xspace}_1$ for the size of any vector {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}\nthat is similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nso that vectors that have too few non-zero entries can be ignored\n(line~\\ref{line:cg-unified:lowerbound}).\nAdditionally, it uses a variable ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$\nto keep an upper bound on the similarity\nbetween {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} and any other vector {{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}.\nThis upper bound is computed by using the residual direct index {\\ensuremath{\\mathcal{R}}\\xspace}\nand the already accumulated dot-product,\nand is updated as the algorithm processes the different posting lists.\nWhen the upper bound becomes smaller than {{\\ensuremath{\\theta}\\xspace}},\nvectors that have not already been added to the set of candidates\ncan be ignored (line~\\ref{line:cg-unified:upperbound}).\nThe array ${\\ensuremath{C}\\xspace}$ holds the candidate vectors,\ntogether with the partial dot-product that is due to the indexed part of the vectors.\n\nFinally, in the {\\ensuremath{\\text{\\tt{CV}}}\\xspace} phase\n({\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\ensuremath{\\text{\\tt{AP}}}\\xspace}\\ function, shown as Algorithm~\\ref{algorithm:cv-unified},\nincluding {\\color{myred}{{red}}} lines and excluding {\\color{mygreen}{{green}}}),\nwe compute the final similarities by using the residual index {\\ensuremath{\\mathcal{R}}\\xspace},\nand report the true similar pairs.\n\nThe streaming versions of {\\ensuremath{\\text{\\tt{AP}}}\\xspace},\nin both {\\ensuremath{\\text{\\tt{MB}}}\\xspace} and {\\ensuremath{\\text{\\tt{STR}}}\\xspace} frameworks,\nare not efficient in practice,\nand thus, we omit further details.\nInstead, we proceed presenting the next scheme,\nthe {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} index, which is a generalization of {\\ensuremath{\\text{\\tt{AP}}}\\xspace}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{L2-based indexing scheme}\n\\label{section:framework:l2ap}\n\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}~\\citep{anastasiu2014l2ap}\nis the state-of-the-art for computing similarity self-join.\n\n\nThe scheme uses tighter {\\ensuremath{\\ell_2}\\xspace}-bounds,\nwhich reduce the size of the index,\nbut also the number of generated candidates\nand the number of fully-computed similarities.\n\nThe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} scheme primarily leverages the\nCauchy-Schwarz inequality, which states that\n${\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\leq ||{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|| \\, ||{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}||$.\nThe same bound applies when considering a prefix of a query vector~{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}.\nSince we consider unit-normalized vectors ($||{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}||=1$),\n", "index": 9, "text": "\n\\[\n{\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}, {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\leq ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}|| \\, ||{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}|| \\leq ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}||.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{dot}}({{{{{\\mathbf{{x}}}}}}^{\\prime}},{{\\mathbf{{y}}}})\\leq||{{{{{%&#10;\\mathbf{{x}}}}}}^{\\prime}}||\\,||{{\\mathbf{{y}}}}||\\leq||{{{{{\\mathbf{{x}}}}}}^%&#10;{\\prime}}||.\" display=\"block\"><mrow><mrow><mrow><mi>dot</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc31</mi><mo>\u2032</mo></msup><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo fence=\"true\">||</mo><msup><mi>\ud835\udc31</mi><mo>\u2032</mo></msup><mo fence=\"true\">||</mo></mrow><mo>\u2062</mo><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc32</mi><mo fence=\"true\">||</mo></mrow></mrow><mo>\u2264</mo><mrow><mo fence=\"true\">||</mo><msup><mi>\ud835\udc31</mi><mo>\u2032</mo></msup><mo fence=\"true\">||</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\n\n\n\n\n\n\n\n\nTo simplify the notation,\nwe omit the dependency from time when obvious from the context,\nand write simply~{\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}.\n\nAn upper bound on the time-dependent\ncosine similarity of any newly arrived vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} at time $t$ can be obtained\nby\n", "itemtype": "equation", "pos": 73552, "prevtext": "\n\nThe previous bound produces a tighter value for {{\\ensuremath{\\mathtt{pscore}}\\xspace}},\nwhich is used in the {\\ensuremath{\\text{\\tt{IC}}}\\xspace} phase\nto bound the similarity of the vector currently being indexed,\nto the rest of the dataset.\nIn particular,  {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} sets\n${\\ensuremath{\\mathtt{pscore}}\\xspace} = \\min\\{{\\ensuremath{\\mathtt{pscore}}\\xspace}_{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}||\\}$.\n\n\nAdditionally,\nthe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index stores the value of {{\\ensuremath{\\mathtt{pscore}}\\xspace}} computed\n\nwhen {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} is indexed.\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} keeps these values in an array {{\\ensuremath{Q}\\xspace}}, index by ${\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}$,\nas shown in line~\\ref{line:ic-unified:ps} of Algorithm~\\ref{algorithm:ic-unified}.\nThe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} index also\nstores in the index the magnitude of the prefix of each vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, up to coordinate $j$.\nThat is,\nthe entries of the posting lists are now triples of the type\n$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{x}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)$.\n\nBoth pieces of additional information,\nthe array {{\\ensuremath{Q}\\xspace}} and the values $||{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}_j||$ stored in the posting lists,\nare used during the {{\\ensuremath{\\text{\\tt{CG}}}\\xspace}} phase\nto reduce the number of candidates.\n\nIn the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase, for a given query vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}},\nwe scan its coordinates backwards,\ni.e., in reverse order with respect to the one used during indexing,\nand we accumulate similarity scores for suffixes of {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}.\nWe keep a {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} bound on the remaining similarity score,\nwhich combines the ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$ bound used in {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}\nand a new {\\ensuremath{\\ell_2}\\xspace}-based ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2$ bound that uses the prefix magnitude values\n($||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||$)\nstored in the posting lists.\nThe {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} bound is an upper bound on the similarity\nof the prefix of the current query vector and any other vector in the index,\nthus as long as {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} is smaller than {\\ensuremath{\\theta}\\xspace}, the algorithm can prune the candidate.\n\nPseudocode for the three phases of {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}}, {\\ensuremath{\\text{\\tt{IC}}}\\xspace}, {\\ensuremath{\\text{\\tt{CG}}}\\xspace}, and {\\ensuremath{\\text{\\tt{CV}}}\\xspace},\nis shown in Algorithms~\\ref{algorithm:ic-unified},\n\\ref{algorithm:cg-unified}, and~\\ref{algorithm:cv-unified}, respectively,\nincluding both {\\color{myred}{{red}}} and {\\color{mygreen}{{green}}} lines.\n\nMore details on the scheme\ncan be found in the original paper of~\\citet{anastasiu2014l2ap}.\n\n\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}):}}}\nAs before, the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} algorithm\nis a direct instantiation of the generic\nAlgorithm~\\ref{algorithm:minibatchx},\nusing the functions that implement the three different phases of {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}}.\n\n\n\n\n{\\smallskip\\noindent\\textbf{{{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} framework ({\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}):}}}\nTo describe the modifications required for adapting\nthe {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} scheme in the streaming framework,\nwe need to introduce some additional notation.\n\nFirst, we assume that the input is a stream {{\\ensuremath{\\mathcal{S}}\\xspace}} of vectors.\nThe main difference is that now both {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} and {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} are function of time,\nas the maximum values in the stream evolve over time.\nWe adapt the use of the two vectors to the streaming case differently.\n\nThe vector {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} is used in the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase to generate candidate vectors.\nGiven that we are looking at vectors that are already indexed (i.e., in the \\emph{past}),\nit is possible to apply the definition of {\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace} between the query vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} and {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace}.\nIn particular, given that the coordinates of {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} originate from different vectors in the index,\nwe can apply the decay factor to each coordinate to {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}}\\xspace} separately.\n\nMore formally, let ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}(t)$ be the \\emph{worst case indexed vector} at time $t$,\nthat is, ${\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}(t)$ is the representation of the vector in the index\nthat is most {\\ensuremath{\\Delta t}\\xspace}-similar to any vector in {\\ensuremath{\\mathcal{S}}\\xspace} arriving at time $t$.\nIts $j$-th coordinate ${\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_j(t)$ \n\nis given by\n", "index": 11, "text": "\n\\[\n{\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_j(t) =\n\\max_{\\substack{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{S}}\\xspace} \\\\ {\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}\\le t}} \\left\\{ {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, e^{- {\\ensuremath{\\lambda}\\xspace}{\\ensuremath{|{{t-{\\ensuremath{t({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}}}|}\\xspace}} \\right\\}.\n\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"{\\widehat{{\\mathrm{{m}}}}^{{\\lambda}}}_{j}(t)=\\max_{\\begin{subarray}{c}{{%&#10;\\mathbf{{x}}}}\\in{\\mathcal{S}}\\\\&#10;{t({{{{\\mathbf{{x}}}}}})}\\leq t\\end{subarray}}\\left\\{{\\mathrm{x}}_{j}\\,e^{-{%&#10;\\lambda}{|{{t-{t({{{{\\mathbf{{x}}}}}})}}}|}}\\right\\}.\\par&#10;\" display=\"block\"><mrow><mrow><mrow><mmultiscripts><mover accent=\"true\"><mi mathvariant=\"normal\">m</mi><mo>^</mo></mover><none/><mi>\u03bb</mi><mi>j</mi><none/></mmultiscripts><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mi>t</mi></mrow></mtd></mtr></mtable></munder><mo>\u2061</mo><mrow><mo>{</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi mathvariant=\"normal\">x</mi><mi>j</mi></msub></mpadded><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></msup></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04814.tex", "nexttext": "\n\nConversely, the vector {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} is used in the {\\ensuremath{\\text{\\tt{IC}}}\\xspace} phase to decide which coordinates to add to the index.\nIts purpose is to guarantee that any two similar vectors in the stream share at least a coordinate.\nAs such, it needs to look at the \\emph{future}.\nIn a static setting, and in {\\ensuremath{\\text{\\tt{MB}}}\\xspace}, maximum values in the whole dataset\ncan easily be accumulated beforehand.\nHowever, in a streaming setting these maximum values need to be kept online.\nThis difference implies that when the vector {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} changes,\nthe invariant on which the {\\ensuremath{\\text{\\tt{AP}}}\\xspace} index is built is lost, and we need to restore it.\nWe call this process \\emph{re-indexing}. \n\nAt a first glance, it might seem straightforward to use {\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace} to decide what to index,\ni.e., adding the decay factor to line~\\ref{line:ic-unified:apbound} in Algorithm~\\ref{algorithm:ic-unified}.\nHowever, the decay factor causes {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} to change rapidly,\nwhich in turn leads to a larger number of re-indexings.\nRe-indexing, as explained next, is an expensive operation.\nTherefore, we opt to avoid applying the time decay when computing the ${\\ensuremath{{\\mathrm{b}}}\\xspace}_1$ bound with {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}.\n\n\n\n\n\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace} (Streaming with index {\\ensuremath{\\text{\\tt{IDX}}}\\xspace})}\n\\label{algorithm:streamingx}\n\\Input{Data stream {\\ensuremath{\\mathcal{S}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{All pairs ${\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} \\in {\\ensuremath{\\mathcal{S}}\\xspace}$ s.t.\\ ${\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}$}\n${\\ensuremath{\\mathcal{I}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\n\n\n\\While{$\\text{\\emph{true}}$} {\n\t{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\texttt{read}$({\\ensuremath{\\mathcal{S}}\\xspace})$\\;\n\n\n\t$({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace}) \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\text{{\\ensuremath{\\text{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{IDX}}}\\xspace}}}\\xspace}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace},{\\ensuremath{\\lambda}\\xspace})$\\;\n\t\\Report{{\\ensuremath{P}\\xspace}}\\;\n}\n\\end{algorithm}\n\n{\\smallskip\\noindent\\textbf{{Re-indexing ({\\ensuremath{\\text{\\tt{IC}}}\\xspace}).}}}\nFor each coordinate ${\\ensuremath{\\mathrm{x}}\\xspace}_j$ of a newly arrived vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\none of three cases may occurr.\n\\begin{squishlist}\n\\item  [$-$]\n${\\ensuremath{\\mathrm{x}}\\xspace}_j = 0$: no action is needed.\n\\item  [$-$]\n$0 < {\\ensuremath{\\mathrm{x}}\\xspace}_j \\leq {\\ensuremath{\\mathrm{{m}}}\\xspace}_j$:\nthe posting list ${\\ensuremath{I}\\xspace}_j$ can be pruned via time filtering, as explained next.\n\\item  [$-$]\n${\\ensuremath{\\mathrm{x}}\\xspace}_j > {\\ensuremath{\\mathrm{{m}}}\\xspace}_j$:\nthe upper bound vector {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}} needs to be updated with the new value just read,\ni.e., ${\\ensuremath{\\mathrm{{m}}}\\xspace}_j\\ensuremath{{\\,\\leftarrow\\,}}\\xspace{\\ensuremath{\\mathrm{x}}\\xspace}_j$.\nIn this case, parts of the pruned vectors may need to be re-indexed.\n\\end{squishlist}\n\n\nWhen {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} is updated (${\\ensuremath{\\mathrm{x}}\\xspace}_j > {\\ensuremath{\\mathrm{{m}}}\\xspace}_j$)\nthe invariant of prefix filtering does not hold anymore.\nThat is, there may be vectors similar to {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} that are not matched in {\\ensuremath{\\mathcal{I}}\\xspace}.\nTo restore the invariant, we re-scan {\\ensuremath{\\mathcal{R}}\\xspace}\n(un-indexed part of vectors within horizon {{\\ensuremath{\\mathcal{\\tau}}\\xspace}}).\nIf the similarity of a vector ${\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} \\in {\\ensuremath{\\mathcal{R}}\\xspace}$ and {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace} has increased,\nwe will reach the threshold {\\ensuremath{\\theta}\\xspace} while scanning the prefix ${\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_p$\n(before reaching its end).\nTherefore, we just need to index those coordinates ${\\ensuremath{\\mathrm{y}}\\xspace}_{p'} < {\\ensuremath{\\mathrm{y}}\\xspace}_{j} \\leq {\\ensuremath{\\mathrm{y}}\\xspace}_{p}$,\nwhere $p'$ is the newly computed boundary.\n\nGiven that the similarity can increase only for vectors\nthat have non-zero value in the dimensions of {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}} that got updated,\nwe can keep an inverted index of {{\\ensuremath{\\mathcal{R}}\\xspace}} to avoid scanning every vector.\nWe use the updated dimensions of the max vector ${\\ensuremath{\\mathrm{{m}}}\\xspace}_j$\nto select the possible candidates for re-indexing,\nand then scan only those from the residual index {\\ensuremath{\\mathcal{R}}\\xspace}.\n\nNote that re-indexing inserts older vectors in the index.\nAs the lists always append items at the end,\nre-indexing introduces out-of-order items in the list.\nAs explained in Section~\\ref{section:implementation},\nthe loss of the time-ordered property hinders one of\nthe optimizations in pruning the index based on time filtering (explained next).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\smallskip\\noindent\\textbf{{Time filtering ({\\ensuremath{\\text{\\tt{CG}}}\\xspace}).}}}\nTo avoid continuously scanning the index, we adopt a lazy approach.\nThe posting lists ${\\ensuremath{I}\\xspace}_j$ are (partially) sorted by time,\ni.e., newest items are always appended to the tail of the lists.\nThus, a simple linear scan from the head of the lists\nis able to prune the lists lazily\nwhile accumulating the similarity in the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase.\nMore specifically, when a new vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace} arrives,\nwe scan all the posting lists $I_j$ such that ${\\ensuremath{\\mathrm{x}}\\xspace}_j \\neq 0$,\nin order to generate its candidates.\nWhile scanning the lists, we drop any item $({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{y}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)$\nsuch that ${\\ensuremath{|{{t({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}) - t({\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})}}|}\\xspace} > {\\ensuremath{\\mathcal{\\tau}}\\xspace}$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main loop of {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} is shown\nin Algorithm~\\ref{algorithm:streamingx}.\nThe algorithm simply reads each item {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} in the stream,\nadds  {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} to the index, and computes the vectors that are similar to it,\nby calling {\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}\n(Algorithm~\\ref{algorithm:ic-unified-str},\nincluding both {\\color{myred}{{red}}} and {\\color{mygreen}{{green}}} lines).\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}\n\\label{algorithm:ic-unified-str}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{Updated similarity index {\\ensuremath{\\mathcal{I}}\\xspace} including {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, and\nset of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace +\\infty; \\; {\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace +\\infty$\\;\n{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0; {\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$}}}\\;\n${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\text{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{\\theta}\\xspace},{\\ensuremath{\\lambda}\\xspace})$\\;\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\text{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}(({\\ensuremath{\\mathcal{I}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{C}\\xspace},{\\ensuremath{\\theta}\\xspace},{\\ensuremath{\\lambda}\\xspace})$\\;\n\\ForEach{$j=1\\ldots{\\ensuremath{d}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n\t${\\ensuremath{\\mathtt{pscore}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\}$\\;\n\n\t{\\color{myred}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_1 + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, \\min\\{{\\ensuremath{\\mathrm{{m}}}\\xspace}_j,{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}\\}$}}}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{b}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{b}}}\\xspace}_t + {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2}; {\\ensuremath{{\\mathrm{b}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{b}}}\\xspace}_t}$}}}\\;\n\t\\If{$\\min\\{{\\color{myred}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{b}}}\\xspace}_2}}}\\} \\ge{\\ensuremath{\\theta}\\xspace}$} {\n\t\t\\If{${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] = \\emptyset$} {\n\t\t\t${\\ensuremath{\\mathcal{R}}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j$\\;\n\t\t\t{\\color{mygreen}{{${\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathtt{pscore}}\\xspace}$}}} \\label{line:ic-unified-str:ps}\\;\n\t\t}\n\t\t${\\ensuremath{I}\\xspace}_j \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{I}\\xspace}_j\\cup \\{({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{x}}\\xspace}_j,||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j||)\\}$\\;\n\t}\n}\n\\Return $({\\ensuremath{\\mathcal{I}}\\xspace}, {\\ensuremath{P}\\xspace})$\\;\n\\end{algorithm}\n\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Gen}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}\n\\label{algorithm:cg-unified-str}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{\nAccum.\\ score array ${\\ensuremath{C}\\xspace}$ for candidate vectors \\\\\n(candidate set is ${\\ensuremath{C}\\xspace}=\\{ {\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}\\in{\\ensuremath{\\mathcal{D}}\\xspace} \\mid {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0\\})$\n}\n${\\ensuremath{C}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace +\\infty; \\; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace +\\infty$\\;\n{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace})$}}}\\label{line:cg-unified-str:start}\\;\n{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 1$}}}\\;\n\\ForEach(\\CommentSty{//reverse order})\n\t{$j={\\ensuremath{d}\\xspace} \\ldots 1 \\text{ \\emph{s.t.} } {\\ensuremath{\\mathrm{x}}\\xspace}_j > 0$} {\n \t\\ForEach{$({\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}, {\\ensuremath{\\mathrm{y}}\\xspace}_j, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j||) \\in {\\ensuremath{I}\\xspace}_j \\text{ \\emph{s.t.} }\n \t\t{\\ensuremath{\\Delta t}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}} \\leq {\\ensuremath{\\mathcal{\\tau}}\\xspace}$ } {\n\t\t${\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\min\\{ {\\color{myred}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_1}}}, {\\color{mygreen}{{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}}}} \\}$\\label{line:cg-unified-str:upperbound}\\;\n\t\t\\If{$({\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]>0 \\text{ \\em or } {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} \\ge{\\ensuremath{\\theta}\\xspace})$} {\n\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\mathrm{y}}\\xspace}_j$\\;\n\t\t\t{\\color{mygreen}{{ \t\t\t\t${\\ensuremath{\\mathtt{l2bound}}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}}}'}\\xspace}_j|| \\, ||{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}_j|| \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}$\\label{line:cg-unified-str:earlypruning}\\; \t\t\t\t\\If{${\\ensuremath{\\mathtt{l2bound}}\\xspace} < {\\ensuremath{\\theta}\\xspace}$} { \t\t\t\t\t${\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] \\ensuremath{{\\,\\leftarrow\\,}}\\xspace 0$\\; \t\t\t\t} \t\t\t}}}\n\t\t}\n\t}\n \t{\\color{myred}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_1 - {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_j$}}}\\label{line:cg-unified-str:maxdecay}\\;\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{rs}}}\\xspace}_t \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{{\\mathrm{rs}}}\\xspace}_t - {{\\ensuremath{\\mathrm{x}}\\xspace}_j}^{2}; {\\ensuremath{{\\mathrm{rs}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\sqrt{{\\ensuremath{{\\mathrm{rs}}}\\xspace}_t}$}}} \\;\n\\label{line:cg-unified-str:end}\n}\n\n\\Return ${\\ensuremath{C}\\xspace}$;\n\\end{algorithm}\n\n\\begin{algorithm}[t]\n\\caption{{\\ensuremath{\\text{\\tt{Cand\\-Ver}}}\\xspace}-{\\color{mygreen}{{{\\ensuremath{\\text{\\tt{L2}}}\\xspace}}}}{\\color{myred}{{{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}}}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}\n\\label{algorithm:cv-unified-str}\n\\Input{Index {\\ensuremath{\\mathcal{I}}\\xspace}, vector {\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},\ncandidate vector array ${\\ensuremath{C}\\xspace}$, threshold {\\ensuremath{\\theta}\\xspace}, decay {\\ensuremath{\\lambda}\\xspace}}\n\\Output{Set of pairs ${\\ensuremath{P}\\xspace}=\\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}) \\mid {\\ensuremath{{\\ensuremath{\\mathrm{sim}}\\xspace}_{{\\ensuremath{\\Delta t}\\xspace}}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace})\\ge{\\ensuremath{\\theta}\\xspace}\\}$}\n${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace \\emptyset$\\;\n\\ForEach{${\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace} \\text{ \\emph{s.t.} } {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] > 0$}{\n\t{\\color{mygreen}{{${\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace ({\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{Q}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}]) \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}$}}}\\; \\label{line:cv-unified-str:boundstart}\n\t${\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace ({\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{{\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}} \\, {\\Sigma}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\}) \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}$\\;\n\t${\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ensuremath{{\\,\\leftarrow\\,}}\\xspace ({\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + \\min\\{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|,|{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}|\\} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} \\, {\\ensuremath{{\\mathrm{vm}}}\\xspace}_{{\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace}}) \\, {\\ensuremath{e^{-{\\ensuremath{\\lambda}\\xspace}{\\ensuremath{\\Delta t}\\xspace}_{{{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}}}\\xspace}$\\;\\label{line:cv-unified-str:boundend}\n\t\\If{${\\color{mygreen}{{{\\ensuremath{{\\mathrm{ps}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace}}}} \\Andkw {\\ensuremath{{\\mathrm{ds}}}\\xspace}_1 \\ge {\\ensuremath{\\theta}\\xspace} \\Andkw {\\ensuremath{{\\mathrm{sz}}}\\xspace}_2 \\ge {\\ensuremath{\\theta}\\xspace})$} {\n\t\t$s \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{C}\\xspace}[{\\ensuremath{\\iota({{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}})}\\xspace}] + {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{{{{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace}}}'}\\xspace})$\\;\n\t\t\\If{$s\\ge {\\ensuremath{\\theta}\\xspace}$} {\n\t\t\t${\\ensuremath{P}\\xspace} \\ensuremath{{\\,\\leftarrow\\,}}\\xspace {\\ensuremath{P}\\xspace} \\cup \\{({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace},{\\ensuremath{{\\ensuremath{\\mathbf{{y}}}\\xspace}}\\xspace},s)\\}$\\;\n\t\t}\n\t}\n\\label{line:cv-unified-str:end}}\n\\Return {\\ensuremath{P}\\xspace}\\;\n\\end{algorithm}\n\n\\newpage\n\\subsection{Improved L2-based indexing scheme}\n\\label{section:framework:pure-l2ap}\n\nThe last indexing scheme we present, {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}},\nis an adaptation of {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}},\noptimized for stream data.\n\nThe idea for the improved index\nis based on the observation that {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}}\ncombines a number of different bounds:\nthe bounds inherited from the {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} scheme\n(e.g., ${\\ensuremath{{\\mathrm{b}}}\\xspace}_1$ for index construction, and ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$ for candidate generation)\nand the new {{\\ensuremath{\\ell_2}\\xspace}}-based bounds\nintroduced by~\\citet{anastasiu2014l2ap}\n(e.g., ${\\ensuremath{{\\mathrm{b}}}\\xspace}_2$ for index construction, and ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_2$ and {\\ensuremath{\\mathtt{l2bound}}\\xspace} for candidate generation).\n\nWe have observed that in general\nthe {{\\ensuremath{\\ell_2}\\xspace}}-based bounds are more effective than the {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}-based bounds.\nIn almost all cases the {{\\ensuremath{\\ell_2}\\xspace}}-based bounds are the ones that trigger.\nThis observation is also verified by the results of~\\citet{anastasiu2014l2ap}.\nFurthermore, as can be easily seen\n(by inspecting\nthe red and green lines of Algorithms~\\ref{algorithm:ic-unified},\n\\ref{algorithm:cg-unified}, and~\\ref{algorithm:cv-unified}),\nwhile the {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}} bounds use statistics of the data in the index,\nthe  {{\\ensuremath{\\ell_2}\\xspace}}-based bounds depend only on the vector being index.\nThis implies that by using only the {{\\ensuremath{\\ell_2}\\xspace}}-based bounds,\none does not need to maintain the worst case vector ${\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}(t)$,\nand thus, no re-indexing is required.\n\nThus, the {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} index uses only the\n{{\\ensuremath{\\ell_2}\\xspace}}-based bounds and discards the {{\\ensuremath{\\text{\\tt{AP}}}\\xspace}}-based bounds.\nThe static version of {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} (used in {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}})\nis shown at Algorithms~\\ref{algorithm:ic-unified},\n\\ref{algorithm:cg-unified}, and~\\ref{algorithm:cv-unified},\nincluding the {\\color{mygreen}{{green}}} lines and excluding the {\\color{myred}{{red}}} ones.\nThe main loop for the streaming case is {\\ensuremath{\\text{\\tt{Ind\\-Constr}}}\\xspace}-{\\ensuremath{\\text{\\tt{L2}}}\\xspace}-{\\ensuremath{\\text{\\tt{STR}}}\\xspace},\nshown as Algorithm~\\ref{algorithm:ic-unified-str},\nincluding the {\\color{mygreen}{{green}}} lines and excluding the {\\color{myred}{{red}}} ones.\n\n\n\n\\section{Implementation}\n\\label{section:implementation}\n\nThis section presents additional details\nand optimizations for both of our algorithmic frameworks,\n{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} and {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}.\n\n\\iffalse\nThe first obstacle in implementing any of the indexing schemes\npresented in the previous section,\nis the ordering of the input vectors and the dimensions.\nIn particular, it is assumed that during index construction\nthe data vectors are processed in decreasing order of their maximum coordinate (${\\ensuremath{{\\mathrm{vm}}}\\xspace}_{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}$),\nand also that prefixes are defined by ordering the dimensions in decreasing frequency.\nWhen dealing with data streams it is not possible to pre-order the data.\nRe-indexing allows to simulate a run of the algorithm over ordered data,\nhowever, this can be very expensive.\nOn the other hand, both orderings are used only as optimization heuristics\nand do not affect the correctness of the algorithms.\n\nThus, our design strategy is to evaluate the effects of ordering in the performance of the algorithms,\nand decide whether it is worth paying the extra re-indexing cost required to enforce ordering,\nor drop it.\n\\fi\n\n\\subsection{Minibatch}\n\n\\iffalse\nOrdering the input vectors and dimensions can be done\neasily in the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework,\nas the index is constructed for a whole batch of input:\\\nall vectors that arrive within horizon~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\n\\fi\n\nRecall that in the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework,\nwe construct the index for a set of data items\nthat arrive within horizon~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nHowever,\nthe {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} scheme\nrequires that we know not only the data used to construct the index,\nbut also the data used to query the index,\ne.g., see Algorithm~\\ref{algorithm:ic-unified}, line~\\ref{line:ic-unified:apbound}\nThis assumption does not hold for {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}},\nas shown in Algorithm~\\ref{algorithm:minibatchx},\nas the data used to query the index arrive in the stream after\nthe index has been constructed.\n\nTo address this problem,\nwe modify the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} framework as follows.\nAt any point of time we consider two windows,\n${\\ensuremath{{W}}\\xspace}_{k-1}$ and ${\\ensuremath{{W}}\\xspace}_{k}$,\neach of size~{{\\ensuremath{\\mathcal{\\tau}}\\xspace}},\nwhere\n${\\ensuremath{{W}}\\xspace}_{k}$ is the current window and ${\\ensuremath{{W}}\\xspace}_{k-1}$ is the previous one.\nThe input data are accumulated in the current window ${\\ensuremath{{W}}\\xspace}_{k}$,\nand the vector {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}} is maintained.\nWhen we arrive at the end of the current window,\nwe compute the global vector {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}, defined over both ${\\ensuremath{{W}}\\xspace}_{k-1}$ and ${\\ensuremath{{W}}\\xspace}_{k}$,\nby combining the {{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}} vectors of the two windows.\nWe then use the data in ${\\ensuremath{{W}}\\xspace}_{k-1}$ to build the index,\nand the data in ${\\ensuremath{{W}}\\xspace}_{k}$ to query the index built over ${\\ensuremath{{W}}\\xspace}_{k-1}$.\n\nWhen moving to the next window,\n${\\ensuremath{{W}}\\xspace}_{k+1}$ becomes current,\n${\\ensuremath{{W}}\\xspace}_{k}$ becomes previous,\nand ${\\ensuremath{{W}}\\xspace}_{k-1}$ is dropped.\n\n\\subsection{Streaming}\n\nNext we discuss implementation issues related to the streaming framework.\n\n{\\smallskip\\noindent\\textbf{{Variable-size lists.}}}\nThe streaming framework introduces posting lists of variable size.\nThe size of the posting lists not only increases,\nbut due to time filtering it may also decrease.\nIn order to avoid many and small memory (de)al\\-lo\\-ca\\-tions,\nwe implement posting lists using a circular byte buffer.\nWhen the buffer becomes full we double its capacity,\nwhile when its size drops below 1/4 we halve it.\n\n{\\smallskip\\noindent\\textbf{{Time filtering.}}}\nIn the {{\\ensuremath{\\text{\\tt{INV}}}\\xspace}} and {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} schemes\nit is easy to maintain the posting lists in time-increasing order.\nThis ordering introduces a simple optimization:\nby scanning the posting lists backwards\n--- from the newest to the oldest item ---\nduring candidate generation,\nit is possible to stop scanning and truncate the posting list as soon as we find the first item over the horizon~{\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nTruncating the circular buffer requires constant time (if no shrinking occurs).\n\nIn {{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}} it is not possible to keep the posting lists in time order,\nas re-indexing may introduce out-of-order items.\nThus, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} scans the lists forward and needs to go through all the expired items to prune the posting list.\n\n{\\smallskip\\noindent\\textbf{{Data structures.}}}\nThe residual direct index {\\ensuremath{\\mathcal{R}}\\xspace}\nand the {\\ensuremath{Q}\\xspace} array in {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} and {\\ensuremath{\\text{\\tt{L2}}}\\xspace}\nneed to be continuously pruned.\nTo support the required operations for these data strunctures,\nwe implement them using a \\emph{linked hash-map},\nwhich combines a hash-map for fast retrieval, and\na linked list for sequential access.\nThe sequential access is the order in which the data items are inserted in the data structure,\nwhich is also the time order.\nMaintaining these data structures requires amortized constant time,\nand memory linear in the number of vectors that arrive within a time interval {\\ensuremath{\\mathcal{\\tau}}\\xspace}.\n\n\n\n\n\n\n\n\n\n\n\n\n\\iffalse\n{\\smallskip\\noindent\\textbf{{Maintaining the max vectors.}}}\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} uses two different max vectors:\none \\emph{global} vector defined over the whole stream processed so far {\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace},\nand one \\emph{progressive} vector defined over the current index {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}\n(items with age less than~{\\ensuremath{\\mathcal{\\tau}}\\xspace}).\nThe two vectors are used in a different way.\n\nThe global vector is used in the index-construction phase.\nFor this vector we ignore the decay factor,\nwhich results in fewer updates,\nwhich in turn implies fewer re-indexings, at the cost of a possibly larger index.\n\nThe progressive vector is used in the candidate-generation phase.\nFor this vector we include the decay factor while updating,\nwhich results in better pruning.\n\\note{previous spara can be removed or merged with previous section}\n\\fi\n\n{\\smallskip\\noindent\\textbf{{Applying decay-factor pruning ({{\\ensuremath{\\lambda}\\xspace}}).}}}\nCentral to adapting the indexing methods in the streaming setting,\nis the use of the decay factor {\\ensuremath{\\lambda}\\xspace} for pruning.\nIn order to make decay-factor pruning effective\nwe apply the following principles.\n\n\\begin{squishlist}\n\\item \n[Index construction:]\nAs the decay factor is used to prune candidates from the data seen in the past,\ndecay-factor pruning is \\emph{never} applied during the index-construction phase.\n\n\\item \n[Candidate generation:]\nDecay-factor pruning is applied during candidate generation\nwhen computing the {\\ensuremath{\\mathtt{rem\\-score}}\\xspace} bound\n(line~\\ref{line:cg-unified-str:upperbound} of Algorithm~\\ref{algorithm:cg-unified-str}),\nas well as when applying the early {\\ensuremath{\\ell_2}\\xspace} pruning (line~\\ref{line:cg-unified-str:earlypruning}).\n\nFor {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}, it is also applied when computing ${\\ensuremath{{\\mathrm{rs}}}\\xspace}_1$ (line~\\ref{line:cg-unified-str:maxdecay}),\nwith a different decay factor for each coordinate\n(as specified in the definition of {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}).\n\n\\item \n[Candidate verification:]\nIn the candidate-verification phase,\ndecay-factor pruning is easily applied when computing all of the bounds\n(lines~\\ref{line:cv-unified-str:boundstart}--\\ref{line:cv-unified-str:boundend}\nof Algorithm~\\ref{algorithm:cv-unified-str}).\n\\end{squishlist}\n\n\\iffalse\nAs a final remark,\nwe note that the {{\\ensuremath{\\ell_2}\\xspace}} bounds depend only on the\ncandidate and query vectors,\nnot on the data seen earlier in the stream.\nThis property makes {\\ensuremath{\\text{\\tt{L2}}}\\xspace}\nvery suitable for the streaming setting.\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\newpage\n\\section{Experimental evaluation}\n\\label{section:experiments}\n\nThe methods presented in the previous sections\ncombine four indexing schemes into two algorithmic frameworks (minibatch vs.\\ streaming).\nAltogether, they lead to a large number of trade-offs for\nincreased pruning power vs.\\ index maintainance.\nIn order to better understand the behavior and evaluate the efficiency of each method\nwe perform an extensive experimental study.\nThe objective of our evaluation is to experimentally answer\nthe following questions:\n\n\\begin{squishlist}\n\\item [\\textbf{Q1:}] Which framework performs better, {\\ensuremath{\\text{\\tt{STR}}}\\xspace} or {\\ensuremath{\\text{\\tt{MB}}}\\xspace}?\n\\item [\\textbf{Q2:}] How effective is {\\ensuremath{\\text{\\tt{L2}}}\\xspace}, compared to {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} and {\\ensuremath{\\text{\\tt{INV}}}\\xspace}?\n\\item [\\textbf{Q3:}] What are the effects of the parameters {\\ensuremath{\\lambda}\\xspace} and {\\ensuremath{\\theta}\\xspace}?\n\\end{squishlist}\n\n{\\smallskip\\noindent\\textbf{{Datasets.}}}\nWe test the proposed algorithms on several real-world datasets.\n{\\texttt{{RCV1}}\\xspace} is the Reuters Corpus volume~1 dataset of newswires~\\citep{lewis2004rcv1},\n{\\texttt{{WebSpam}}\\xspace} is a corpus of spam web pages~\\citep{wang2012evolutionary},\n{\\texttt{{Blogs}}\\xspace} is a collection of one month of WordPress blog posts crawled in June 2015, and\n{\\texttt{{Tweets}}\\xspace} is a sample of tweets collected in June 2009~\\citep{yang2011patterns}.\nAll datasets are available online in text format,\\footnote{All datasets will be available at time of publication.}\nwhile for the experiments we use a more compact and faster-to-read binary format;\nthe text-to-binary converter is also included in the source code we provide.\nThese datasets exhibit a wide variety in their characteristics,\nas summarized in Table~\\ref{tab:datasets}, and\nallow us to evaluate our methods under very different scenarios.\nSpecifically, we see that the density of the four datasets varies greatly.\nWith respect to timestamps,\nitems in {\\texttt{{WebSpam}}\\xspace} and {\\texttt{{RCV1}}\\xspace} are assigned an artificial timestamp,\nsampled from a Poisson and a sequential arrival process, respectively.\nFor {\\texttt{{Blogs}}\\xspace} and {\\texttt{{Tweets}}\\xspace} the publication time of each item is available,\nand we use it as a timestamp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table*}[t]\n\\caption{Datasets used in the experimental evaluation.\n$n$:\\ number of vectors;\n$m$:\\ number of coordinates;\n$\\sum{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|}$:\\ number of non-zero coordinates;\n$\\rho = \\nicefrac{\\sum{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|}}{n m}$:\\ density;\n$\\overline{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|} = \\nicefrac{\\sum{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|}}{n}$:\\ average number of non-zero coordinates; \nand type of timestamps.}\n\\begin{center}\n\\small\n\\begin{tabular}{l r r r S[table-format=1.3] r r}\n\\toprule\n\\multicolumn{1}{c}{Dataset}\t&\t\\multicolumn{1}{c}{$n$}\t&\t\\multicolumn{1}{c}{$m$}\t&\t\\multicolumn{1}{c}{$\\sum{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|}$}\t&\t\\multicolumn{1}{c}{$\\rho$ (\\%)}\t&\t\\multicolumn{1}{c}{$\\overline{|{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}|}$}\t&\t\\multicolumn{1}{c}{Timestamps} \\\\\n\\midrule\n{\\texttt{{WebSpam}}\\xspace}\t&\t\\num{350000}\t\t&\t\\num{680715}\t&\t\\num{1305} M\t&\t\\num{0.55}\t&\t\\num{3728.00}\t&\tpoisson \\\\ \n{\\texttt{{RCV1}}\\xspace}\t&\t\\num{804414}\t\t&\t\\num{43001}\t&\t\\num{61} M\t&\t\\num{0.18}\t&\t\\num{75.72}\t&\tsequential \\\\ \n{\\texttt{{Blogs}}\\xspace}\t&\t\\num{2532437}\t\t&\t\\num{356043}\t&\t\\num{356} M\t&\t\\num{0.04}\t&\t\\num{140.40}\t&\tpublishing date \\\\ \n{\\texttt{{Tweets}}\\xspace}\t&\t\\num{18266589}\t&\t\\num{1048576}\t&\t\\num{173} M\t&\t\\num{0.001}\t&\t\\num{9.46}\t&\tpublishing date \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\label{tab:datasets}\n\\vspace{-\\baselineskip}\n\\end{table*}\n\n\\begin{table}[t]\n\\caption{Fraction of the $24$ configurations of ({\\ensuremath{\\theta}\\xspace},{\\ensuremath{\\lambda}\\xspace}) that successfully terminate within the allowed time budget (closer to $1.00$ is better).}\n\\vspace{-\\baselineskip}\n\\begin{center}\n\\small\n\\begin{tabular}{l ccc ccc}\n\\toprule\n\\multirow{2}{*}{Dataset}\t&\t\\multicolumn{3}{c}{{\\ensuremath{\\text{\\tt{MB}}}\\xspace}}\t&\t\\multicolumn{3}{c}{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n&\t{\\ensuremath{\\text{\\tt{INV}}}\\xspace}\t&\t{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}\t&\t{\\ensuremath{\\text{\\tt{L2}}}\\xspace}\t\t&\t{\\ensuremath{\\text{\\tt{INV}}}\\xspace}\t&\t{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}\t&\t{\\ensuremath{\\text{\\tt{L2}}}\\xspace} \\\\\n\\midrule\n{\\texttt{{WebSpam}}\\xspace}\t&1.00&1.00&1.00&1.00&0.83&0.96 \\\\\n{\\texttt{{RCV1}}\\xspace}\t\t&1.00&1.00&1.00&1.00&0.96&1.00 \\\\\n{\\texttt{{Blogs}}\\xspace}\t&0.25&0.25&0.25&1.00&0.96&1.00 \\\\\n{\\texttt{{Tweets}}\\xspace}\t&0.25&0.25&0.25&1.00&0.96&0.96 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\label{tab:summary}\n\\end{table}\n\n{\\smallskip\\noindent\\textbf{{Algorithms.}}}\nWe test the two algorithmic frameworks,\n{\\ensuremath{\\text{\\tt{STR}}}\\xspace} and {\\ensuremath{\\text{\\tt{MB}}}\\xspace}, with the three index variants,\n{\\ensuremath{\\text{\\tt{INV}}}\\xspace}, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}, and {\\ensuremath{\\text{\\tt{L2}}}\\xspace}, on all four datasets.\nFor the similarity threshold {\\ensuremath{\\theta}\\xspace}\nwe explore a range of values in $[0.5, 0.99]$,\nwhich is typical for the {{\\sc{apss}}\\xspace} problem~\\citep{anastasiu2014l2ap,bayardo2007scaling-up},\nwhile for the time-decay factor {\\ensuremath{\\lambda}\\xspace} we use exponentially increasing values in the range\n$[10^{-4}, 10^{-1}]$.\n\nOur code also includes an implementation of {\\ensuremath{\\text{\\tt{AP}}}\\xspace} for {\\ensuremath{\\text{\\tt{MB}}}\\xspace},\nbut in preliminary experiments we found it much slower than {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace},\ntherefore we omit it from the set of indexing strategies under study.\nSome configurations are very expensive in terms of time or memory,\nand we are unable to run some of the algorithms.\nWe set a timeout of 3 hours for each experiment,\nand we abort the run if this timout limit is exceeded, or if the JVM crashes because of lack of memory.\nIn all cases of failure during our experiments, {\\ensuremath{\\text{\\tt{MB}}}\\xspace} fails due to timeout,\nwhile {\\ensuremath{\\text{\\tt{STR}}}\\xspace} because of memory requirements.\n\n{\\smallskip\\noindent\\textbf{{Setting.}}}\nWe run all the experiments on a computer with an Intel Xeon CPU E31230 @ 3.20$\\,$GHz with 8$\\,$MB of cache,\n32$\\,$GB of RAM (of which 16$\\,$GB allocated for the JVM heap), and a 500$\\,$GB SATA disk.\nAll experiments run sequentially on a single core of the CPU.\nWe warm up the disk cache for each dataset by performing one run of the algorithm before taking running times.\nTimes are averaged over three runs.\n\n\\subsection{Results}\n\\label{section:experiments:results}\n\n{\\smallskip\\noindent\\textbf{{Q1 ({{\\ensuremath{\\text{\\tt{MB}}}\\xspace}} vs.\\ {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}}):}}}\nOf the four datasets we use,\n{\\ensuremath{\\text{\\tt{MB}}}\\xspace} successfully runs with all configurations on only two of them ({\\texttt{{RCV1}}\\xspace} and {\\texttt{{WebSpam}}\\xspace}).\nAs can be seen from Table~\\ref{tab:datasets}, {\\texttt{{Blogs}}\\xspace} and {\\texttt{{Tweets}}\\xspace} are the largest datasets,\nand {\\ensuremath{\\text{\\tt{MB}}}\\xspace} is not able to scale to these sizes.\nOn the other hand, {\\ensuremath{\\text{\\tt{STR}}}\\xspace} is able to run with all configurations on all four datasets.\nThe overhead of {\\ensuremath{\\text{\\tt{MB}}}\\xspace} becomes too large when the horizon {\\ensuremath{\\mathcal{\\tau}}\\xspace} becomes small enough,\n\nas a new index needs to be initialized too frequently.\nTable~\\ref{tab:summary} shows a summary of the outcomes of the experimental evaluations for the various configuration.\nTherefore, we restrict this comparison to {\\texttt{{RCV1}}\\xspace} and {\\texttt{{WebSpam}}\\xspace}.\n\nMost of the time of {\\ensuremath{\\text{\\tt{STR}}}\\xspace} and {\\ensuremath{\\text{\\tt{MB}}}\\xspace}, according to our profiling results,\nis spent in the {\\ensuremath{\\text{\\tt{CG}}}\\xspace} phase while scanning the posting lists.\nHence, we first compare the algorithms on the number of posting entries traversed.\nFigure~\\ref{fig:entries-ratio} shows that {\\ensuremath{\\text{\\tt{STR}}}\\xspace} usually does less total work,\nand traverses around $65\\%$ of the entries traversed by {\\ensuremath{\\text{\\tt{MB}}}\\xspace}.\nThe figure shows the ratio for {\\ensuremath{\\text{\\tt{L2}}}\\xspace},\nbut other indexing strategies show the same trend (plots are omitted for brevity).\n\nWe now turn our attention to running time.\nFigures~\\ref{fig:time-rcv1} and~\\ref{fig:time-webspam}\ncompare the running time of {\\ensuremath{\\text{\\tt{STR}}}\\xspace} and {\\ensuremath{\\text{\\tt{MB}}}\\xspace}\non {\\texttt{{RCV1}}\\xspace} and {\\texttt{{WebSpam}}\\xspace} for all configuration on which both are able to run.\nThe decay factor {\\ensuremath{\\lambda}\\xspace} varies with the columns of the grid,\nwhile the indexing scheme with the rows.\nThe two datasets present a different picture.\n\nClearly, for {\\texttt{{RCV1}}\\xspace} (Figure~\\ref{fig:time-rcv1}) algorithm {\\ensuremath{\\text{\\tt{STR}}}\\xspace} is faster than {\\ensuremath{\\text{\\tt{MB}}}\\xspace} in most cases.\nMore aggressive pruning reduces the difference,\nwhile in some cases with low {\\ensuremath{\\theta}\\xspace} (useful for recommender systems) algorithm {\\ensuremath{\\text{\\tt{STR}}}\\xspace}\ncan be up to $4$ times faster than~{\\ensuremath{\\text{\\tt{MB}}}\\xspace}.\n{\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} is the exception, and for smaller {\\ensuremath{\\mathcal{\\tau}}\\xspace} its performance is subpar, as detailed next.\n\nConversely, for {\\texttt{{WebSpam}}\\xspace} the {\\ensuremath{\\text{\\tt{MB}}}\\xspace} algorithm\nhas the upper hand in most cases,\nespecially for larger decay factors (i.e., shorter horizons).\nThe different behavior is caused by the higher density of {\\texttt{{WebSpam}}\\xspace},\nwhich has an average number of non-zero coordinates per vector,\nwhich is almost two orders of magnitude larger than {\\texttt{{RCV1}}\\xspace} (see Table~\\ref{tab:datasets}).\nFor {\\ensuremath{\\text{\\tt{STR}}}\\xspace}, this high density renders the lazy update approach inefficient,\nas a large number of posting lists need to be updated and pruned for each vector,\nespecially for short horizons.\n{\\ensuremath{\\text{\\tt{MB}}}\\xspace} can simply throw away old indexes, rather than mending them.\n\n\n\nSummarizing our findings related to the {{\\ensuremath{\\text{\\tt{MB}}}\\xspace}}-vs.-{{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} study,\nwe conclude that {\\ensuremath{\\text{\\tt{STR}}}\\xspace} is more efficient in most cases,\nas it is able to run on all datasets, while {\\ensuremath{\\text{\\tt{MB}}}\\xspace} becomes too slow for larger datasets.\nFor very specific conditions of high density, {\\ensuremath{\\text{\\tt{MB}}}\\xspace} has a small advantage on {\\ensuremath{\\text{\\tt{STR}}}\\xspace}.\n\n\n\\begin{figure}[t]\n\\begin{center}\n\n\\includegraphics[width=\\columnwidth]{ratio-MBvsSTR-entries-tau}\n\\caption{\\label{fig:entries-ratio}\nRatio of index entries traversed during {\\ensuremath{\\text{\\tt{CG}}}\\xspace} for {\\ensuremath{\\text{\\tt{STR}}}\\xspace} compared to {\\ensuremath{\\text{\\tt{MB}}}\\xspace}.\nFor small {\\ensuremath{\\mathcal{\\tau}}\\xspace}, the ratio tends to one, while for larger horizons {\\ensuremath{\\mathcal{\\tau}}\\xspace},\n{\\ensuremath{\\text{\\tt{STR}}}\\xspace} needs to traverse only $65\\%$ of the entries compared to {\\ensuremath{\\text{\\tt{MB}}}\\xspace}.}\n\\end{center}\n\n\\end{figure}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{factor-time-rcv1}\n\n\\caption{Time taken by the {\\ensuremath{\\text{\\tt{MB}}}\\xspace} and {\\ensuremath{\\text{\\tt{STR}}}\\xspace} algorithms\nas a function of the similarity threshold {\\ensuremath{\\theta}\\xspace}, on the {\\texttt{{RCV1}}\\xspace} dataset.}\n\\label{fig:time-rcv1}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{factor-time-webspam}\n\n\\caption{Time taken by the {\\ensuremath{\\text{\\tt{MB}}}\\xspace} and {\\ensuremath{\\text{\\tt{STR}}}\\xspace} algorithms\nas a function of the similarity threshold {\\ensuremath{\\theta}\\xspace}, on the {\\texttt{{WebSpam}}\\xspace} dataset.}\n\\label{fig:time-webspam}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{streaming-indexing-comparison-time}\n\n\\caption{Time taken by {\\ensuremath{\\text{\\tt{STR}}}\\xspace} using different indexes\nas a function of the similarity threshold {\\ensuremath{\\theta}\\xspace}, for the {\\texttt{{RCV1}}\\xspace} dataset.}\n\\label{fig:streaming-indexing-comparison-time}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{streaming-indexing-comparison-entries}\n\n\\caption{Entries traversed by {\\ensuremath{\\text{\\tt{STR}}}\\xspace} using different indexes\nas a function of the similarity threshold {\\ensuremath{\\theta}\\xspace}, for the {\\texttt{{Tweets}}\\xspace} dataset.}\n\\label{fig:streaming-indexing-comparison-entries}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n{\\smallskip\\noindent\\textbf{{Q2 (Indexing schemes):}}}\nNow that we have established that {\\ensuremath{\\text{\\tt{STR}}}\\xspace}  scales better than {\\ensuremath{\\text{\\tt{MB}}}\\xspace},\nwe focus on the former algorithm:\nthe rest of the experiments are performed on {\\ensuremath{\\text{\\tt{STR}}}\\xspace}.\nWe turn our attention to the effectiveness of pruning, and compare {\\ensuremath{\\text{\\tt{L2}}}\\xspace} with {\\ensuremath{\\text{\\tt{INV}}}\\xspace} and {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace}.\nFor brevity, we show results for {\\texttt{{RCV1}}\\xspace} only.\nThe other datasets follow the same trends.\n\n\n\n\nFigure~\\ref{fig:streaming-indexing-comparison-time} shows the comparison in time.\nSeveral interesting patterns emerge.\nFirst, {\\ensuremath{\\text{\\tt{L2}}}\\xspace} is almost always the fastest indexing scheme.\nSecond, {\\ensuremath{\\text{\\tt{INV}}}\\xspace} works well for short horizons,\nwhere the overheads incurred by pruning are not compensated by a large reduction in candidates.\nInstead, for larger horizons (low {\\ensuremath{\\theta}\\xspace} and {\\ensuremath{\\lambda}\\xspace})\nthe gains of pruning are more pronounced, thus making {\\ensuremath{\\text{\\tt{INV}}}\\xspace} a poor choice.\nFinally, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} is slightly slower than {\\ensuremath{\\text{\\tt{L2}}}\\xspace} in most cases,\nand much slower when the horizon is short.\nEven though {\\ensuremath{\\text{\\tt{L2}}}\\xspace} uses a subset of the bounds of {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace},\nthe overhead of computing the {\\ensuremath{\\text{\\tt{AP}}}\\xspace} bounds offsets any possible gain in pruning.\n\nIn addition, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} may need to re-index residuals of vectors,\nand a shorter horizon causes more frequent re-indexings.\nTherefore, the two rightmost plots show an upward trend in {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace},\nwhich is due to the re-indexing overhead.\nThe overhead is so large that we were not able to run {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} for ${\\ensuremath{\\theta}\\xspace}=0.99$ and ${\\ensuremath{\\lambda}\\xspace}=0.1$.\nWhile there are possible practical workarounds\n(e.g., use a more lax bound to decrease the frequency of re-indexing),\n{\\ensuremath{\\text{\\tt{L2}}}\\xspace} always provides a better choice and does not require tinkering with the algorithm.\n\nIn general, a shorter horizon makes other pruning strategies less relevant compared to time filtering,\nas can be seen from the progressive flattening of the curves from left to right as {\\ensuremath{\\lambda}\\xspace} increases.\n\n\nFigure~\\ref{fig:streaming-indexing-comparison-entries} shows the comparison\nin the number of index entries traversed.\nClearly, {\\ensuremath{\\text{\\tt{INV}}}\\xspace} has usually the largest amount\ndue to the absence of pruning. \nThe relative effectiveness of pruning for {\\ensuremath{\\text{\\tt{L2}}}\\xspace} is almost constant\nthroughout the range of {\\ensuremath{\\lambda}\\xspace},\nhowever the number of entries traversed decreases,\nand so does the importance of filtering the index (compared to the time filtering).\nThis is in line with the behavior exhibited\nin Figure~\\ref{fig:streaming-indexing-comparison-time}\nwhere the difference in time decreases for larger~{\\ensuremath{\\lambda}\\xspace}.\n\nInterestingly, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} starts very close to {\\ensuremath{\\text{\\tt{L2}}}\\xspace},\nbut as the horizon shortens\nthe number of entries traversed increases significantly,\nevens surpassing {\\ensuremath{\\text{\\tt{INV}}}\\xspace} in the rightmost plot.\nThis result shows that {\\ensuremath{\\text{\\tt{L2}}}\\xspace} does not lose much in terms of pruning power,\ndespite not using the bounds from {\\ensuremath{\\text{\\tt{AP}}}\\xspace}.\nFurthermore, the optimization to the implementation of time filtering\ndescribed in Section~\\ref{section:implementation} (backwards posting-list scanning)\nis not applicable.\nThe reason is that the {\\ensuremath{\\text{\\tt{AP}}}\\xspace} bounds required during indexing are data-dependent,\nwhich leads to re-indexing and therefore there is no guarantee that the posting lists are sorted by time.\nTherefore, {\\ensuremath{\\text{\\tt{L2AP}}}\\xspace} ends up traversing more entries than {\\ensuremath{\\text{\\tt{L2}}}\\xspace}.\n\nNote that the last points on the rightmost plot of\nFigure~\\ref{fig:streaming-indexing-comparison-entries}\n(${\\ensuremath{\\theta}\\xspace}=0.99$ and ${\\ensuremath{\\lambda}\\xspace}=0.1$) are not shown because their value is zero,\nwhich cannot be represented on a logarithmic axis.\nIndeed, in this configuration ${\\ensuremath{\\mathcal{\\tau}}\\xspace}=0.1$ which is smaller than any timestamp delta, so the index gets continuously pruned before bing traversed, and therefore the output is empty.\n\nSimilar trends are observed for the number of candidates generated and\nthe number of full similarities computed.\nThose results are omitted due to space constraints.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{parameters-lambda-time}\n\n\\caption{Time taken by {\\ensuremath{\\text{\\tt{STR}}}\\xspace} using the {\\ensuremath{\\text{\\tt{L2}}}\\xspace} index\nas a function of the decay factor {\\ensuremath{\\lambda}\\xspace},\nfor different similarity thresholds~{\\ensuremath{\\theta}\\xspace}.}\n\\label{fig:parameters-lambda-time}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{parameters-theta-time}\n\n\\caption{Time taken by {\\ensuremath{\\text{\\tt{STR}}}\\xspace} using the {\\ensuremath{\\text{\\tt{L2}}}\\xspace} index\nas a function of the similarity thresholds {\\ensuremath{\\theta}\\xspace},\nfor different decay factors {\\ensuremath{\\lambda}\\xspace}.}\n\\label{fig:parameters-theta-time}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure*}\n\n{\\smallskip\\noindent\\textbf{{Q3 (Parameters {\\ensuremath{\\theta}\\xspace} and {\\ensuremath{\\lambda}\\xspace}):}}}\nNext we study the effect of the parameters of our methods:\nthe decay factor {\\ensuremath{\\lambda}\\xspace} and the similarity threshold {\\ensuremath{\\theta}\\xspace}.\nHereinafter, we present results only for {\\ensuremath{\\text{\\tt{L2}}}\\xspace},\nfor which we have established that it is an effective and efficient pruning scheme.\n\nFigure~\\ref{fig:parameters-lambda-time} shows the effect of the decay factor {\\ensuremath{\\lambda}\\xspace}.\nIncreasing the decay factor decreases the computation time for all datasets.\nHowever, the decrease is more marked for lower threshold {\\ensuremath{\\theta}\\xspace}, and\nflattens out quickly for higher values of {\\ensuremath{\\lambda}\\xspace}.\n\nFigure~\\ref{fig:parameters-theta-time} shows the effect of the similarity threshold {\\ensuremath{\\theta}\\xspace}.\nThe pattern is similar to the previous figure, with the roles of {\\ensuremath{\\theta}\\xspace} and {\\ensuremath{\\lambda}\\xspace} reversed.\nIncreasing the threshold decreases the computation time for all datasets, but is more marked for lower {\\ensuremath{\\lambda}\\xspace}, and flattens out quickly.\n\nBy combining the insights from the previous two figures, we can infer that both parameters\njointly affect the computation time.\nThe reason is that time is mainly affected by time filtering,\nwhich is the most effective pruning strategy in this setting,\nand its effect depends on the horizon {\\ensuremath{\\mathcal{\\tau}}\\xspace},\nwhich jointly depends on the other two main parameters.\n\nTo explore this hypothesis, we perform a linear regression of the computation time on the value of the horizon {\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nFigure~\\ref{fig:parameters-tau-time} shows that the computation time is roughly a linear function of {\\ensuremath{\\mathcal{\\tau}}\\xspace}.\nIn addition, from the inclination of the regression line\nit is clear that {\\texttt{{WebSpam}}\\xspace} is an outlier when compared to the other datasets, as previously mentioned.\n\n\n\\section{Conclusion}\nWe introduced the problem\nof computing the similarity self-join in data streams.\nOur approach relies on incorporating a forgetting\nfactor in the similarity measure so as to\nbe able to prune data items when they become old enough.\nGiven the new definition of time-dependent similarity,\nwe developed two algorithmic frameworks\nthat incorporate existing indexing techniques\nfor computing similarity self-join on static data,\nand we extended those techniques to the streaming case.\nWe explored several different combinations of bounds used for index pruning,\nwhich, in the context of streaming data,\nlead to interesting performance trade-offs.\nOur extensive analysis allows to understand better these trade-offs,\nand consequently, to design an index that is optimized for streaming data.\nOur analysis indicates that the {{\\ensuremath{\\text{\\tt{STR}}}\\xspace}} algorithm\nequipped with the {{\\ensuremath{\\text{\\tt{L2}}}\\xspace}} index is the most scalable\nand robust across all datasets and configurations.\n\nOne promising direction for future\nwork is to experiment with dimension-ordering strategies\nand evaluate the cost-benefit trade-off of maintaining a dimension ordering.\nOther directions include\napplying the developed techniques in real-world applications\nfor filtering near-duplicate items in data streams,\nas well as extending our model for different\ndefinitions of time-dependent similarity.\n\n\n\n\\bibliography{sssj}\n\\bibliographystyle{nourlabbrvnat}\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=\\columnwidth]{parameters-tau-time}\n\n\\caption{Time taken by the {\\ensuremath{\\text{\\tt{STR}}}\\xspace} algorithm when using the {\\ensuremath{\\text{\\tt{L2}}}\\xspace} index\nas a function of the time horizon {\\ensuremath{\\mathcal{\\tau}}\\xspace}.}\n\\label{fig:parameters-tau-time}\n\\end{center}\n\\vspace{-\\baselineskip}\n\\end{figure}\n\n\n", "itemtype": "equation", "pos": 74490, "prevtext": "\n\n\n\n\n\n\n\n\nTo simplify the notation,\nwe omit the dependency from time when obvious from the context,\nand write simply~{\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}.\n\nAn upper bound on the time-dependent\ncosine similarity of any newly arrived vector {{\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}} at time $t$ can be obtained\nby\n", "index": 13, "text": "\n\\[\n{\\ensuremath{\\mathtt{rem\\-score}}\\xspace}(t)\n= {\\ensuremath{\\mathrm{dot}}\\xspace}({\\ensuremath{{\\ensuremath{\\mathbf{{x}}}\\xspace}}\\xspace}, {\\ensuremath{\\widehat{{\\ensuremath{{\\ensuremath{\\mathbf{{{m}}}}\\xspace}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace})\n= \\sum_{j=1}^{{\\ensuremath{d}\\xspace}} {\\ensuremath{\\mathrm{x}}\\xspace}_j \\, {\\ensuremath{\\widehat{{\\ensuremath{\\mathrm{{m}}}\\xspace}}^{{\\ensuremath{\\lambda}\\xspace}}}\\xspace}_j .\n\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"{\\mathtt{rem\\-score}}(t)={\\mathrm{dot}}({{\\mathbf{{x}}}},{\\widehat{{{\\mathbf{{%&#10;{m}}}}}}^{{\\lambda}}})=\\sum_{j=1}^{{d}}{\\mathrm{x}}_{j}\\,{\\widehat{{\\mathrm{{m%&#10;}}}}^{{\\lambda}}}_{j}.\\par&#10;\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\ude9b\ud835\ude8e\ud835\ude96\ud835\ude9c\ud835\ude8c\ud835\ude98\ud835\ude9b\ud835\ude8e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>dot</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><msup><mover accent=\"true\"><mi>\ud835\udc26</mi><mo>^</mo></mover><mi>\u03bb</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mpadded width=\"+1.7pt\"><msub><mi mathvariant=\"normal\">x</mi><mi>j</mi></msub></mpadded><mo>\u2062</mo><mmultiscripts><mover accent=\"true\"><mi mathvariant=\"normal\">m</mi><mo>^</mo></mover><none/><mi>\u03bb</mi><mi>j</mi><none/></mmultiscripts></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]