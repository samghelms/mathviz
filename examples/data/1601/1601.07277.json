[{"file": "1601.07277.tex", "nexttext": "\nWe will assume that these relaxation constraints are included in the convex\nconstraints of~(\\ref{e-prob}).\nAdding these relaxation constraints\nto the original problem yields an equivalent problem (since the\nadded constraints are redundant),\nbut can improve the convergence of any method, global or heuristic.\nBy tractable, we mean that the number of added constraints is modest,\nand in particular, polynomial in $q$.\n\nFor example, when $\\C = \\{0,1\\}^q$, we have the inequalities\n$0 \\leq z_i \\leq 1$, $i=1,\\ldots, q$.\n(These inequalities define the convex hull of $\\C$, \\ie, all other\nconvex inequalities that hold for all $z\\in \\C$ are implied by them.)\nWhen\n", "itemtype": "equation", "pos": 3885, "prevtext": "\n\\maketitle\n\\begin{abstract}\nWe describe general heuristics to approximately\nsolve a wide variety of problems with convex objective\nand decision variables from a nonconvex set.\nThe heuristics, which employ convex relaxations, convex restrictions,\nlocal neighbor search methods,\nand the alternating direction method of multipliers (ADMM),\nrequire the solution of a modest number of convex problems,\nand are meant to apply to general problems, without much tuning.\nWe describe an implementation of these methods in a package called NCVX,\nas an extension of CVXPY, a Python package\nfor formulating and solving convex optimization problems.\nWe study several examples of well known nonconvex problems, and show that\nour general purpose heuristics are effective in finding approximate\nsolutions to a wide variety of problems.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\subsection{The problem}\n\nWe consider the optimization problem\n\\BEQ\\label{e-prob}\n\\begin{array}{ll}\n\\mbox{minimize}   & f_0(x, z) \\\\\n\\mbox{subject to} & f_i(x, z) \\leq 0, \\quad i=1,\\ldots,m \\\\\n& Ax + Bz = c \\\\\n& z \\in \\C,\n\\end{array}\n\\EEQ\nwhere $x \\in \\reals^n$ and $z \\in \\reals^q$\nare the decision variables,\n$A \\in \\reals^{p \\times n}$, $B \\in \\reals^{p \\times q}$,\n$c \\in \\reals^p$ are problem data, and $\\C\\subseteq \\reals^q$ is compact.\n\nWe assume that the objective and inequality constraint functions\n$f_0,\\ldots,f_m: \\reals^n \\times \\reals^q \\to \\reals$\nare jointly convex in $x$ and $z$.\nWhen the set $\\C$ is convex, (\\ref{e-prob}) is a convex\noptimization problem,\nbut we are interested here in the case where $\\C$ is not convex.\nRoughly speaking, the problem~(\\ref{e-prob}) is a convex optimization\nproblem, with some additional nonconvex constraints, $z\\in \\C$.\nWe can think of $x$ as the collection of\ndecision variables that appear only\nin convex constraints, and $z$ as the decision variables that\nare directly constrained to lie in the (generally) nonconvex\nset $\\C$.\nThe set $\\C$ is often a Cartesian product, $\\C = \\C_1 \\times \\cdots \\times\n\\C_k$, where\n$\\C_i \\subset \\reals^{q_i}$ are sets that are simple to describe,\n\\eg, $\\C_i = \\{0,1\\}$.\nWe denote the optimal value of the problem~(\\ref{e-prob}) as $p^\\star$,\nwith the usual conventions that $p^\\star=+\\infty$ if the problem\nis infeasible, and $p^\\star = -\\infty$ if the problem is unbounded\nbelow.\n\n\\subsection{Special cases}\n\\paragraph{Mixed-integer convex optimization.}\nWhen $\\C = \\{0,1\\}^q$, the problem~(\\ref{e-prob}) is a general\nmixed integer convex program, \\ie,\na convex optimization problem in which some variables are constrained\nto be Boolean.\n(Mixed Boolean convex program would be a more accurate\nname for such a problem, but `mixed integer' is commonly used.)\nIt follows that the problem~(\\ref{e-prob}) is hard; it includes as\na special case, for example,\nthe general Boolean satisfaction problem.\n\n\\paragraph{Cardinality constrained convex optimization.}\nAs another broad special case of~(\\ref{e-prob}),\nconsider the case $\\C =\\{z \\in \\reals^q \\mid \\card(z) \\leq k,~\n\\|z\\|_\\infty \\leq M \\}$, where $\\card(z)$ is the number of nonzero\nelements of $z$, and $k$ and $M$ are given.\nWe call this the general \\emph{cardinality-constrained convex problem}.\nIt arises in many interesting applications, such as regressor selection.\n\n\\paragraph{Other special cases.}\nAs we will see in \\S\\ref{s-examples},\nmany (hard) problems can be formulated in the form~(\\ref{e-prob}).\nMore examples include regressor selection, 3-SAT, circle packing,\nthe traveling salesman problem,\nfactor analysis modeling, job selection,\nthe maximum coverage problem, inexact graph isomorphism, and many more.\n\n\n\\subsection{Convex relaxation}\n\\paragraph{Convex relaxation of a set.}\nA compact set $\\C$ always has a tractable \\emph{convex relaxation}.\nBy this we mean a (modest-sized) set of convex inequality and linear\nequality constraints that hold for every $z\\in \\C$:\n", "index": 1, "text": "\n\\[\nz\\in \\C ~\\Longrightarrow~ h_i(z) \\leq 0, \\quad i=1,\\ldots,s, \\qquad Fz=g.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"z\\in\\C~{}\\Longrightarrow~{}h_{i}(z)\\leq 0,\\quad i=1,\\ldots,s,\\qquad Fz=g.\" display=\"block\"><mrow><mrow><mrow><mi>z</mi><mo>\u2208</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\C</mtext></merror><mo rspace=\"5.8pt\">\u27f9</mo><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>s</mi></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mi>F</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>=</mo><mi>g</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwe have the convex inequalities\n", "itemtype": "equation", "pos": 4624, "prevtext": "\nWe will assume that these relaxation constraints are included in the convex\nconstraints of~(\\ref{e-prob}).\nAdding these relaxation constraints\nto the original problem yields an equivalent problem (since the\nadded constraints are redundant),\nbut can improve the convergence of any method, global or heuristic.\nBy tractable, we mean that the number of added constraints is modest,\nand in particular, polynomial in $q$.\n\nFor example, when $\\C = \\{0,1\\}^q$, we have the inequalities\n$0 \\leq z_i \\leq 1$, $i=1,\\ldots, q$.\n(These inequalities define the convex hull of $\\C$, \\ie, all other\nconvex inequalities that hold for all $z\\in \\C$ are implied by them.)\nWhen\n", "index": 3, "text": "\n\\[\n\\C =\\{z \\in \\reals^q \\mid \\card(z) \\leq k,~\n\\|z\\|_\\infty \\leq M \\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\C=\\{z\\in\\reals^{q}\\mid\\card(z)\\leq k,~{}\\|z\\|_{\\infty}\\leq M\\},\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\C</mtext></merror><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>z</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>q</mi></msup></mrow><mo>\u2223</mo><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\card</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mi>k</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>M</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\n(These inequalities define the convex hull of $\\C$.)\nFor general compact $\\C$ the inequality $\\|z\\|_\\infty \\leq M$ will\nalways be a convex relaxation for some $M$.\n\n\\paragraph{Relaxed problem.}\nIf we remove the nonconvex constraint $z\\in\\C$,\nwe get a \\emph{convex relaxation} of the original problem:\n\\BEQ\\label{e-relaxation}\n\\begin{array}{ll}\n\\mbox{minimize}   & f_0(x,z)\\\\\n\\mbox{subject to} & f_i(x, z) \\leq 0, \\quad i=1,\\ldots,m\\\\\n& Ax + Bz = c.\n\\end{array}\n\\EEQ\n(Recall that convex equalities and inequalities known to hold for\n$z\\in \\C$ have been incorporated in the convex constraints.)\nThe relaxed problem is convex; its optimal value is a lower bound\non the optimal value $p^\\star$ of~(\\ref{e-prob}).\nA solution $(x^*, z^*)$ to problem~(\\ref{e-relaxation}) need not satisfy\n$z^* \\in \\C$, but if it does, the pair $(x^*,z^*)$ is optimal for~(\\ref{e-prob}).\n\n\\begin{incomplete}\n\\paragraph{Breaking symmetry.}\nIn a convex relaxation, we add inequalities that are known to hold\nfor \\emph{any} point $z \\in \\C$.\nWhen there are symmetries in the problem~(\\ref{e-prob}),\nwe can add additional convex constraints to the problem, with the\nproperty that there is always at least one optimal point that satisfies\nthe new constraints.  (It follows that solving the problem with the\nadditional constraints is equivalent to the original problem.)\n\nThe symmetry typically arises due to arbitrary labeling of objects,\nfor example when we are\nassigning colors, or machines or jobs from a pool of equivalent ones.\nWe describe a simple case here to give the idea; we will see symmetry\narising in several examples as well.\n\nSuppose the problem~(\\ref{e-prob}) is invariant under permutations of the\nvariables $w_1, \\ldots, w_t \\in \\reals^\\ell$ (which are subvectors of $(x,z)$).\nThat is, permuting the $w_i$ does not change the objective,\nor change their feasibility.   Let $h\\in \\reals^\\ell$ be arbitrary.\nWe can without loss of generality (by permuting $w_i$)\nassume that $h^T w_i$ are sorted, \\ie,\n", "itemtype": "equation", "pos": 4730, "prevtext": "\nwe have the convex inequalities\n", "index": 5, "text": "\n\\[\n\\|z\\|_1 \\leq kM, \\qquad \\|z\\|_\\infty \\leq M.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\|z\\|_{1}\\leq kM,\\qquad\\|z\\|_{\\infty}\\leq M.\" display=\"block\"><mrow><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2264</mo><mrow><mi>k</mi><mo>\u2062</mo><mi>M</mi></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>M</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nThus we can impose this constraint on the problem without loss of\ngenerality.\nAs a variation, we can add the (convex) penalty function\n", "itemtype": "equation", "pos": 6769, "prevtext": "\n(These inequalities define the convex hull of $\\C$.)\nFor general compact $\\C$ the inequality $\\|z\\|_\\infty \\leq M$ will\nalways be a convex relaxation for some $M$.\n\n\\paragraph{Relaxed problem.}\nIf we remove the nonconvex constraint $z\\in\\C$,\nwe get a \\emph{convex relaxation} of the original problem:\n\\BEQ\\label{e-relaxation}\n\\begin{array}{ll}\n\\mbox{minimize}   & f_0(x,z)\\\\\n\\mbox{subject to} & f_i(x, z) \\leq 0, \\quad i=1,\\ldots,m\\\\\n& Ax + Bz = c.\n\\end{array}\n\\EEQ\n(Recall that convex equalities and inequalities known to hold for\n$z\\in \\C$ have been incorporated in the convex constraints.)\nThe relaxed problem is convex; its optimal value is a lower bound\non the optimal value $p^\\star$ of~(\\ref{e-prob}).\nA solution $(x^*, z^*)$ to problem~(\\ref{e-relaxation}) need not satisfy\n$z^* \\in \\C$, but if it does, the pair $(x^*,z^*)$ is optimal for~(\\ref{e-prob}).\n\n\\begin{incomplete}\n\\paragraph{Breaking symmetry.}\nIn a convex relaxation, we add inequalities that are known to hold\nfor \\emph{any} point $z \\in \\C$.\nWhen there are symmetries in the problem~(\\ref{e-prob}),\nwe can add additional convex constraints to the problem, with the\nproperty that there is always at least one optimal point that satisfies\nthe new constraints.  (It follows that solving the problem with the\nadditional constraints is equivalent to the original problem.)\n\nThe symmetry typically arises due to arbitrary labeling of objects,\nfor example when we are\nassigning colors, or machines or jobs from a pool of equivalent ones.\nWe describe a simple case here to give the idea; we will see symmetry\narising in several examples as well.\n\nSuppose the problem~(\\ref{e-prob}) is invariant under permutations of the\nvariables $w_1, \\ldots, w_t \\in \\reals^\\ell$ (which are subvectors of $(x,z)$).\nThat is, permuting the $w_i$ does not change the objective,\nor change their feasibility.   Let $h\\in \\reals^\\ell$ be arbitrary.\nWe can without loss of generality (by permuting $w_i$)\nassume that $h^T w_i$ are sorted, \\ie,\n", "index": 7, "text": "\n\\[\nh^T w_1 \\leq \\cdots \\leq h^T w_t.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"h^{T}w_{1}\\leq\\cdots\\leq h^{T}w_{t}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>h</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>w</mi><mn>1</mn></msub></mrow><mo>\u2264</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2264</mo><mrow><msup><mi>h</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>w</mi><mi>t</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhich penalizes non-sorted choices of $w_i$,\nto the objective.\nIt is easy to see that this yields an equivalent problem:\ngiven any feasible point $(x,z)$ for~(\\ref{e-prob}),\nwe can always permute $w_i$ so that $\\tau(x,z) = 0$.\n\nUnlike the basic convex relaxations, which pertain to the set $\\C$\nand are independent of the convex parts of the problem,\nthese symmetry-breaking constraints or penalty depend on the\nentire problem, and not just the set $\\C$.\n\\end{incomplete}\n\n\\subsection{Projections and approximate projections}\nOur methods will make use of tractable projection, or\ntractable approximate projection, onto the set $\\C$.  The usual Euclidean\nprojection onto $\\C$ will be denoted $\\Pi$.\n(It need not be unique when $\\C$ is not convex.)\nBy approximate projection,\nwe mean any function $\\hat \\Pi: \\reals^q \\to \\C$ that satisfies\n$\\hat \\Pi (z) = z $ for $z \\in \\C$.\nFor example, when $\\C = \\{0,1\\}^q$, exact projection is given by\nrounding the entries to $\\{0,1\\}$.\n\nAs a less trivial example, consider the cardinality-constrained problem.\nThe projection of $z$ onto $\\C$ is given by\n", "itemtype": "equation", "pos": 6944, "prevtext": "\nThus we can impose this constraint on the problem without loss of\ngenerality.\nAs a variation, we can add the (convex) penalty function\n", "index": 9, "text": "\n\\[\n\\tau(x,z) = \\sum_{i=1}^{t-1} \\max\\{h^T(w_i - w_{i+1}),0\\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\tau(x,z)=\\sum_{i=1}^{t-1}\\max\\{h^{T}(w_{i}-w_{i+1}),0\\},\" display=\"block\"><mrow><mrow><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>h</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>-</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\mathcal I \\subseteq \\{1,\\ldots, q\\}$ is a set of indices of\n$k$ largest values of $|z_i|$.\nWe will describe many projections, and some approximate projections,\nin \\S\\ref{s-set}.\n\n\\subsection{Residual and merit functions}\nFor any $(x,z)$ with $z \\in \\C$, we define the \\emph{constraint\nresidual} as\n", "itemtype": "equation", "pos": 8101, "prevtext": "\nwhich penalizes non-sorted choices of $w_i$,\nto the objective.\nIt is easy to see that this yields an equivalent problem:\ngiven any feasible point $(x,z)$ for~(\\ref{e-prob}),\nwe can always permute $w_i$ so that $\\tau(x,z) = 0$.\n\nUnlike the basic convex relaxations, which pertain to the set $\\C$\nand are independent of the convex parts of the problem,\nthese symmetry-breaking constraints or penalty depend on the\nentire problem, and not just the set $\\C$.\n\\end{incomplete}\n\n\\subsection{Projections and approximate projections}\nOur methods will make use of tractable projection, or\ntractable approximate projection, onto the set $\\C$.  The usual Euclidean\nprojection onto $\\C$ will be denoted $\\Pi$.\n(It need not be unique when $\\C$ is not convex.)\nBy approximate projection,\nwe mean any function $\\hat \\Pi: \\reals^q \\to \\C$ that satisfies\n$\\hat \\Pi (z) = z $ for $z \\in \\C$.\nFor example, when $\\C = \\{0,1\\}^q$, exact projection is given by\nrounding the entries to $\\{0,1\\}$.\n\nAs a less trivial example, consider the cardinality-constrained problem.\nThe projection of $z$ onto $\\C$ is given by\n", "index": 11, "text": "\n\\[\n\\left(\\Pi \\left(z\\right)\\right)_i = \\left\\{ \\begin{array}{ll}\nM & z_i > M, ~i \\in \\mathcal I\\\\\n-M & z_i < -M, ~i \\in \\mathcal I\\\\\nz_i & |z_i| \\leq M, ~i \\in \\mathcal I\\\\\n0 & i \\not\\in \\mathcal I,\n\\end{array}\\right.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\left(\\Pi\\left(z\\right)\\right)_{i}=\\left\\{\\begin{array}[]{ll}M&amp;z_{i}&gt;M,~{}i\\in%&#10;\\mathcal{I}\\\\&#10;-M&amp;z_{i}&lt;-M,~{}i\\in\\mathcal{I}\\\\&#10;z_{i}&amp;|z_{i}|\\leq M,~{}i\\in\\mathcal{I}\\\\&#10;0&amp;i\\not\\in\\mathcal{I},\\end{array}\\right.\" display=\"block\"><mrow><msub><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi>M</mi></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&gt;</mo><mi>M</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mi>M</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&lt;</mo><mrow><mo>-</mo><mi>M</mi></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>z</mi><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>M</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>i</mi><mo>\u2209</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $(u)_+=\\max\\{u,0\\}$ denotes the positive part;\n$(x,z)$ is feasible if and only if $r(x,z)=0$.\nNote that $r(x,z)$ is a convex function of $(x,z)$.\nWe define the \\emph{merit function} of a pair $(x,z)$ as\n", "itemtype": "equation", "pos": 8628, "prevtext": "\nwhere $\\mathcal I \\subseteq \\{1,\\ldots, q\\}$ is a set of indices of\n$k$ largest values of $|z_i|$.\nWe will describe many projections, and some approximate projections,\nin \\S\\ref{s-set}.\n\n\\subsection{Residual and merit functions}\nFor any $(x,z)$ with $z \\in \\C$, we define the \\emph{constraint\nresidual} as\n", "index": 13, "text": "\n\\[\nr(x,z) = \\sum_{i=1}^m (f_i(x,z))_+ + \\|Ax+Bz-c\\|_1,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"r(x,z)=\\sum_{i=1}^{m}(f_{i}(x,z))_{+}+\\|Ax+Bz-c\\|_{1},\" display=\"block\"><mrow><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></msub></mrow><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>z</mi></mrow></mrow><mo>-</mo><mi>c</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\lambda>0$ is a parameter.\nThe merit function is also a convex function of $(x,z)$.\n\nWhen $\\C$ is convex and the problem is feasible,\nminimizing $\\eta(x,z)$ for large enough $\\lambda$\nyields a solution of the original problem~(\\ref{e-prob})\n(that is, the residual is a so-called exct penalty function);\nwhen the problem is not feasible, it tends to find\napproximate solutions that satisfy many of the constraints\n\\cite{han1979exact,di1989exact,fletcher1973exact}.\n\nWe will use the merit function to judge candidate approximate solutions\n$(x,z)$ with $z \\in \\C$; that is, we take a pair with lower\nmerit function value to be a better approximate solution than one\nwith higher merit function value.\nFor some problems (for example, unconstrained problems)\nit is easy to find feasible points, so all candidate points\nwill be feasible.\nThe merit function then reduces to the objective value.\nAt the other extreme, for feasibility problems the objective is\nzero, and goal is to find a feasible point.  In this case the\nmerit function reduces to $\\lambda r(x,z)$, \\ie, a positive\nmultiple of the residual function.\n\n\\subsection{Solution methods}\n\nIn this section we describe various methods for solving the\nproblem~(\\ref{e-prob}), either exactly (globally) or approximately.\n\n\\paragraph{Global methods}\nDepending on the set $\\C$, the problem~(\\ref{e-prob}) can be solved\nglobally by a variety of algorithms, including (or mixing)\nbranch-and-bound\n\\cite{lawler1966branch,narendra1977branch,brucker1994branch},\nbranch-and-cut\n\\cite{padberg1991branch,tawarmalani2005polyhedral,stubbs1999branch},\nsemidefinite hierarchies \\cite{sherali1990hierarchy}, or even\ndirect enumeration when $\\C$ is a finite set.\nIn each iteration of these methods, a convex optimization problem derived\nfrom~(\\ref{e-prob}) is solved, with $\\C$ removed, and (possibly) additional\nvariables and convex constraints added.\nThese global methods are generally thought to have high worst-case\ncomplexities and indeed can be very slow in practice, even for modest\nsize problem instances.\n\n\\paragraph{Local solution methods and heuristics}\nA local method for~(\\ref{e-prob}) solves a modest number of convex\nproblems, in an attempt to find a good approximate solution,\n\\ie, a pair $(x,z)$ with $z\\in \\C$ and a low value of the merit\nfunction $\\eta(x,z)$.\nFor a feasibility problem, we might hope to find a solution;\nand if not, find one with a small constraint residual.\nFor a general problem, we can hope to find a feasible point with\nlow objective value, ideally near the lower bound on $p^\\star$ from\nthe relaxed problem.\nIf we cannot find any feasible points, we can settle for a pair $(x,z)$\nwith $z \\in \\C$ and low merit function value.\nAll of these methods are heuristics, in the sense that they cannot\nin general be guaranteed to find an optimal, or even good, or even\nfeasible, point in only a modest number of iterations.\n\nThere are of course many heuristics for the general\nproblem~(\\ref{e-prob}) and for many of its special cases.\nFor example, any global optimization method can be stopped\nafter some modest number of iterations; we take the best point found\n(in terms of the merit function) as our approximate solution.\nWe will discuss some local search methods, including neighbor search and\npolishing, in \\S\\ref{local}.\n\n\\paragraph{Existing solvers}\nThere are numerous open source and commercial solvers that can handle\nproblems with nonconvex constraints. We only mention a few of them here.\nGurobi \\cite{gurobi}, CPLEX \\cite{cplex2009v12}, MOSEK \\cite{mosek}\nprovide global methods for mixed integer linear programs,\nmixed integer quadratic programs, and mixed integer\nsecond order cone programs.\nBARON \\cite{ts:05}, Couenne \\cite{cplex2009v12}, and\nSCIP \\cite{achterberg2009scip} use global methods for\nnonlinear programs\nand mixed integer nonlinear programs.\nBonmin \\cite{bonami2008algorithmic} and Knitro \\cite{byrd2006knitro}\nprovide global methods for mixed integer convex programs and\nheuristic methods for mixed integer nonliner programs.\nIPOPT \\cite{waechter2009introduction} and\nNLopt \\cite{johnson2014nlopt} use heuristic methods for nonlinear\nprograms.\n\n\n\\subsection{Our approach}\nThe purpose of this paper is to describe a general system\nfor heuristic solution of~(\\ref{e-prob}), based on solving\na \\emph{modest} number of convex problems derived from~(\\ref{e-prob}).\nBy heuristic, we mean that the algorithm need not find an optimal point,\nor indeed, even a feasible point, even when one exists.\nWe would hope that for many feasible problem instances from some\napplication, the algorithm does find\na feasible point, and one with objective not too far from\nthe optimal value.\nThe disadvantage of a heuristic over a global method is clear and simple:\nit need not find an optimal point.\nThe advantage of a heuristic is that it can be (and often is)\ndramatically faster to carry out than a global method.\nMoreover there are many applications where a heuristic method\nfor~(\\ref{e-prob}) is sufficient.  This might be the case when\nthe objective and constraints are already approximations of what\nwe really want, so the added effort of solving it globally is not\nworth it.\n\n\n\\paragraph{ADMM.}\nOne of the heuristic methods described in this paper\nis based on the alternating directions method of multipliers\n(ADMM), an operator splitting algorithm originally devised to solve\nconvex optimization problems \\cite{boyd2011distributed}.\nWe call this heuristic nonconvex alternating directions method of\nmultipliers  (NC-ADMM).\nThe idea of using ADMM as a heuristic to solve nonconvex problems\nwas mentioned in \\cite[Ch. 9]{boyd2011distributed}, and has been explored by\nYedidia and others \\cite{Derbinsky2013AnImp} as a message\npassing algorithm. Consensus ADMM has been used\nfor general quadratically constrained quadratic programming\nin \\cite{huang2016consensus}. In \\cite{xu2012alternating}, ADMM\nhas been applied to non-negative matrix factorization with missing\nvalues. ADMM also has been used for real and complex polynomial\noptimization models in \\cite{jiang2014alternating},for constrained\ntensor factorization in \\cite{liavas2014parallel}, and for optimal\npower flow in \\cite{erseghe2014distributed}.\nADMM is a generalization of the method of multipliers\n\\cite{hestenes1969multiplier, bertsekas2014constrained},\nand there is a long history of using the method of multipliers\nto (attempt to) solve nonconvex problems\n\\cite{chartrand2012nonconvex, chartrand2013nonconvex,hong2014distributed,\nhong2014convergence,peng2015proximal,wang2014convergence,LP:15}.\nSeveral related methods, such as the Douglas-Rachford method\n\\cite{eckstein1992douglas}\nor Spingarn's method of partial inverses \\cite{spingarn1985applications},\ncould just as well have been used.\n\n\\paragraph{Our contribution.}\nThe paper has the following structure.\nIn \\S\\ref{s-algorithm} we discuss local search methods and describe how\nthey can be used as solution improvement methods. This will enable us to\nstudy simple but sophisticated methods such as relax-round-polish, and\niterative neighbor search.\nIn \\S\\ref{s-set} we catalog a variety of nonconvex\nsets for which Euclidean projection or approximated projection\nis easily evaluated and, when applicable, we discuss relaxations, restrictions,\nand the set of neighbors for a given point.\nIn \\S\\ref{s-implementation} we discuss an implementation\nof our general system for heuristic solution NCVX,\nas an extension of CVXPY \\cite{cvxpy},\na Python package for\nformulating and solving convex optimization problems.\nThe object-oriented features of CVXPY make the extension\nparticularly simple to implement.\nFinally, in \\S\\ref{s-examples} we demonstrate the performance\nof our methods on several example problems.\n\n\\section{Local improvement methods}\n\\label{local}\nIn this section we describe some simple general local search methods.\nThese methods take a point $z\\in\\C$ and by performing a local search on $z$ they\nfind a candidate pair $(\\hat x, \\hat z)$, with $\\hat z\\in\\C$ and a\nlower merit function. We will see that for many applications\nthese methods with a good initialization can be used to obtain an\napproximate solution. We will also see how we can use these methods\nto improve solution candidates from other heuristics, hence we refer to\nthese methods as \\emph{solution improvement}.\n\n\\subsection{Polishing}\n\\paragraph{Convex restriction.}\nWe can have a tractable \\emph{convex restriction} of $\\C$,\nthat includes a given point in $\\C$.\nThis means that for each point $\\tilde z\\in\\C$,\nwe have a set of convex equalities and inequalities on $z$, that hold for\n$\\tilde z$, and imply $z \\in \\C$.\nWe denote the set of points that satisfy the restrictions as\n$\\Crestrict (\\tilde z)$, and call this set the \\emph{restriction} of\n$\\C$ at $\\tilde z$.  The restriction set $\\Crestrict(\\tilde z)$ is convex,\nand satisfies $\\tilde z \\in \\Crestrict (\\tilde z) \\subseteq \\C$.\nThe trivial restriction is given by $\\Crestrict(\\tilde z)=\\{\\tilde z\\}$.\n\nWhen $\\C$ is discrete, for example $\\C= \\{0,1\\}^q$, the trivial restriction\nis the only restriction. In other cases we can have interesting nontrivial\nrestrictions, as we will see below.\nFor example, with\n$\\C=\\{z\\in \\reals^q \\mid \\card(z)\\leq k,~ \\|z\\|_\\infty \\leq M\\}$,\nwe can take as restriction\n$\\Crestrict(\\tilde z)$, the set of vectors $z$ with\nthe same sparsity pattern as $\\tilde z$, and $\\|z \\|_\\infty \\leq M$.\n\n\\paragraph{Polishing.}\nGiven any point $\\tilde z\\in\\C$, we can replace the constraint $z\\in\\C$\nwith $z\\in \\Crestrict(\\tilde z)$ to get the convex problem\n\\BEQ\\label{e-polish}\n\\begin{array}{ll}\n\\mbox{minimize}   &\\eta(x,z)\\\\\n\\mbox{subject to}  &z\\in \\Crestrict(\\tilde z),\n\\end{array}\n\\EEQ\nwith variables $x,z$.\n(When the restriction $\\Crestrict(\\tilde z)$ is the trivial one, \\ie,\na singleton, this is equivalent to fixing $z=\\tilde z$ and\nminimizing over $x$.)\nWe call this problem the \\emph{convex restriction} of~(\\ref{e-prob})\nat the point $\\tilde z$.\nThe restricted problem is convex,\nand its optimal value is an upper bound on $p^\\star$.\n\nAs a simple example of polishing consider the mixed\ninteger convex problem. The only restriction is the trivial one, so\nthe polishing problem for a given Boolean vector $\\tilde z$\nsimply fixes the values of the Boolean variables, and solves the\nconvex problem over the remaining variables, \\ie, $x$.\nFor the cardinality-constrained convex problem, polishing\nfixes the sparsity pattern of $z$ and solves the resulting convex\nproblem over $z$ and $x$.\n\nFor problems with nontrivial restrictions, we can solve the\npolishing problem repeatedly until convergence. In other\nwords we can use the output of the polishing problem as\nan initial point for another polishing problem and keep iterating\nuntil convergence or until a maximum number of iterations is reached.\nThis technique is called \\emph{iterated polishing} and\ndescribed in algorithm \\ref{iter-polish}.\n\\begin{algorithm}[H]\n\\caption{Iterated polishing}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\Do\n\\State $z^\\mathrm{old} \\gets \\tilde z$.\n\\State Find $(\\tilde x, \\tilde z)$\nby solving the polishing problem with restriction\n$z \\in \\Crestrict(z^\\mathrm{old})$.\n\\doWhile{$\\tilde z \\neq z^\\mathrm{old}$}\\\\\n\\Return $(\\hat x,\\hat z)$.\n\\end{algorithmic}\n\\label{iter-polish}\n\\end{algorithm}\n\nIf there exists a point $\\tilde x$ such that $(\\tilde x, \\tilde z)$ is feasible, the\nrestricted problem is feasible too.\nThe restricted problem need not be feasible in general, but if it is,\nwith solution $(\\hat x, \\hat z)$, then the pair\n$(\\hat x,\\hat z)$ is feasible for the original problem~(\\ref{e-prob})\nand satisfies $f(\\hat x,\\hat z) \\leq f(\\tilde x,\\tilde z)$\nfor any $\\tilde x$ for which $(\\tilde x,\\tilde z)$ is feasible.\nSo polishing can take a point $\\tilde z\\in\\C$ (or a pair\n$(\\tilde x,\\tilde z)$) and produce another pair $(\\hat x, \\hat z)$ with\na possibly better objective value.\n\n\n\\subsection{Relax-round-polish}\nWith the simple tools described so far\n(\\ie, relaxation, polishing, and\nprojection) we can create several heuristics for approximately\nsolving the problem~(\\ref{e-prob}).\nA basic version solves the relaxation, projects the relaxed\nvalue of $z$ onto $\\C$, and then polishes the result.\n\n\n\\begin{minipage}[c]{1 \\textwidth}\n\\begin{algorithm}[H]\n\\caption{Relax-round-polish heuristic}\n\\begin{algorithmic}[1]\n\\State Solve the convex relaxation~(\\ref{e-relaxation}) to obtain\n$(x^\\mathrm{rlx}, z^\\mathrm{rlx})$.\n\\State Find $z^\\mathrm{rnd}=\\Pi(z^\\mathrm{rlx})$.\n\\State Find $(\\hat x, \\hat z)$\nby solving the polishing problem with restriction\n$z \\in \\Crestrict(z^\\mathrm{rnd})$.\n\\end{algorithmic}\n\\label{alg_summary}\n\\end{algorithm}\n\\end{minipage}\n\\bigskip\n\nNote that in the first step we also obtain a lower bound on the\noptimal value $p^\\star$; in the polishing step we obtain\nan upper bound, and a feasible pair $(\\hat{x},\\hat{z})$ that achieves the upper\nbound (provided that polishing is successful).\nThe best outcome is for these bounds to be equal, which means that\nwe have found a (global) solution of~(\\ref{e-prob}) (for this problem\ninstance).\nBut relax-round-polish can fail; for example, it can fail to find a\nfeasible point even though one exists.\n\nMany variations on relax-round-polish are possible.\nWe can introduce randomization by replacing the round step with\n", "itemtype": "equation", "pos": 8895, "prevtext": "\nwhere $(u)_+=\\max\\{u,0\\}$ denotes the positive part;\n$(x,z)$ is feasible if and only if $r(x,z)=0$.\nNote that $r(x,z)$ is a convex function of $(x,z)$.\nWe define the \\emph{merit function} of a pair $(x,z)$ as\n", "index": 15, "text": "\n\\[\n\\eta (x,z)= f_0(x,z)+\\lambda r(x,z),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\eta(x,z)=f_{0}(x,z)+\\lambda r(x,z),\" display=\"block\"><mrow><mrow><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $w$ is a random vector.\nWe can repeat this heuristic with $K$ different random instances of $w$.\nFor each of $K$ samples of $w$, we polish, giving us a set of $K$ candidate\napproximate solutions.\nWe then take as our final approximate solution the best among\nthese $K$ candidates,\n\\ie,\nthe one with least merit function.\n\n\\subsection{Neighbor search} \\label{s-neighbor}\n\\paragraph{Neighbors.}\nWe describe the concept of \\emph{neighbors} for a point $z\\in\\C$ when\n$\\C$ is discrete. The set of neighbors of a point $z\\in\\C$,\ndenoted $\\Cneighbor(z)$, is the set of points\nwith distance one from $z$ in a natural (integer valued) distance,\nwhich depends on the set $\\C$.\nFor example for the set of Boolean vectors in $\\reals^n$ we use\n\\emph{Hamming distance},\nthe number of entries in which two Boolean vectors differ.\nHence neighbors of a Boolean\nvector $z$ are the set of vectors that differ from $z$ in one component.\nThe distance between two permutation matrices is defined as the minimum number of\nswaps of adjacent rows and columns necessary to transform the first\nmatrix into the second.\nWith this distance,\nneighbors of a permutation matrix $Z$ are the\nset of permutation matrices generated by swapping any two adjacent rows\nor columns in $Z$.\n\nFor Cartesian products of discrete sets we use the sum of\ndistances.  In this case, for\n$z = (z_1,z_2,\\ldots,z_k) \\in \\C = \\C_1 \\times \\C_2 \\times \\ldots \\times \\C_k$, neighbors\nof $z$ are points of the form\n$(z_1,\\ldots,z_{i-1},\\tilde z_i,z_{i+1},\\ldots, z_k)$ where\n$\\tilde z_i$ is a neighbor of $z_i$ in $\\C_i$.\n\n\\paragraph{Basic neighbor search.}\nWe introduced polishing as a tool that can find a pair $(\\hat x,\\hat z)$ given an\ninput $\\tilde z\\in\\C$ by solving a sequence of convex problems. In basic\nneighbor search we solve the polishing problem for $\\tilde z$ and all neighbors\nof $\\tilde z$ and return the pair $(x^*, z^*)$ with the smallest merit function value.\nIn practice, we can sample from $\\Cneighbor(\\tilde z)$ instead of iterating over\nall points in $\\Cneighbor(\\tilde z)$ if $\\left|\\Cneighbor(\\tilde z)\\right|$ is large.\n\n\n\\begin{algorithm}[H]\n\\caption{Basic neighbor search}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\State Initialize $(x_\\mathrm{best}, z_\\mathrm{best})=\\emptyset$, $\\eta_\\mathrm{best}=\\infty$.\n\\For{$\\hat z \\in \\{ \\tilde z \\} \\cup \\Cneighbor(\\tilde z)$}\n\\State Find $(x^*, z^*)$,\nby solving the polishing problem~(\\ref{e-polish}),\nwith constraint $z\\in \\Crestrict(\\hat z)$.\n\\If{$\\eta(x^*, z^*) < \\eta_\\mathrm{best}$}\n\\State $(x_\\mathrm{best}, z_\\mathrm{best})= (x^*, z^*)$, $\\eta_\\mathrm{best} =\\eta(x^*, z^*)$.\n\\EndIf\n\\EndFor\\\\\n\\Return $(x_\\mathrm{best},z_\\mathrm{best})$.\n\\end{algorithmic}\n\\label{iter-hill-climb}\n\\end{algorithm}\n\n\\paragraph{Iterated neighbor search.}\nWe can carry out the described neighbor search iteratively as follows.\nWe maintain a current value of $z$, corresponding to the\nbest pair $(x,z)$ found so far.  We then consider a neighbor of $z$ and\npolish.  If the new point $(x,z)$ is better than the current best one,\nwe reset our best and continue; otherwise we examine another neighbor.\nThis is done until a maximum number of iterations is reached, or\nall neighbors of the current best $z$ produce (under polishing) no better\npairs. This procedure is sometimes called \\emph{hill climbing},\nsince it resembles an attempt to find the top of a mountain by repeatedly\ntaking steps towards an ascent direction.\n\n\\begin{algorithm}[H]\n\\caption{Iterative neighbor search}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\State Find $(x_\\mathrm{best}, z_\\mathrm{best})$ by solving the polishing problem~(\\ref{e-polish})\n\\State$\\eta_\\mathrm{best}\\gets\\eta(x_\\mathrm{best}, z_\\mathrm{best})$.\n\\For{$\\hat z \\in\\Cneighbor(z_\\mathrm{best})$}\\label{marker}\n\\State Find $(x^*, z^*)$,\nby solving the polishing problem~(\\ref{e-polish}),\nwith constraint $z\\in \\Crestrict(\\hat z)$.\n\\If{$\\eta(x^*, z^*) < \\eta_\\mathrm{best}$}\n\\State $(x_\\mathrm{best}, z_\\mathrm{best})= (x^*, z^*)$, $\\eta_\\mathrm{best} =\\eta(x^*, z^*)$.\n\\State \\Goto{marker}\n\\EndIf\n\\EndFor\\\\\n\\Return $(x_\\mathrm{best},z_\\mathrm{best})$.\n\\end{algorithmic}\n\\label{alg:iter-neighbor}\n\\end{algorithm}\nNotice that when no neighbors are available for $\\tilde z\\in\\C$,\nthis algorithm reduces to simple polishing.\n\n\\section{NC-ADMM}\\label{s-algorithm}\nWe already can use the simple tools described in the previous section\nas heuristics to find approximate solutions to problem~(\\ref{e-prob}).\nIn this section, we describe the alternating direction method of multipliers (ADMM)\nas a mechanism to generate candidate points $\\tilde z$ to carry out local search methods\nsuch as iterated neighbor search. We call this method nonconvex ADMM, or NC-ADMM.\n\\subsection{ADMM}\nDefine $\\phi:\\reals^q \\to \\reals \\cup \\{ -\\infty, +\\infty\\}$ such that\n$\\phi(z)$ is the best objective value of problem~(\\ref{e-prob})\nafter fixing $z$. In other words,\n", "itemtype": "equation", "pos": 22195, "prevtext": "\nwhere $\\lambda>0$ is a parameter.\nThe merit function is also a convex function of $(x,z)$.\n\nWhen $\\C$ is convex and the problem is feasible,\nminimizing $\\eta(x,z)$ for large enough $\\lambda$\nyields a solution of the original problem~(\\ref{e-prob})\n(that is, the residual is a so-called exct penalty function);\nwhen the problem is not feasible, it tends to find\napproximate solutions that satisfy many of the constraints\n\\cite{han1979exact,di1989exact,fletcher1973exact}.\n\nWe will use the merit function to judge candidate approximate solutions\n$(x,z)$ with $z \\in \\C$; that is, we take a pair with lower\nmerit function value to be a better approximate solution than one\nwith higher merit function value.\nFor some problems (for example, unconstrained problems)\nit is easy to find feasible points, so all candidate points\nwill be feasible.\nThe merit function then reduces to the objective value.\nAt the other extreme, for feasibility problems the objective is\nzero, and goal is to find a feasible point.  In this case the\nmerit function reduces to $\\lambda r(x,z)$, \\ie, a positive\nmultiple of the residual function.\n\n\\subsection{Solution methods}\n\nIn this section we describe various methods for solving the\nproblem~(\\ref{e-prob}), either exactly (globally) or approximately.\n\n\\paragraph{Global methods}\nDepending on the set $\\C$, the problem~(\\ref{e-prob}) can be solved\nglobally by a variety of algorithms, including (or mixing)\nbranch-and-bound\n\\cite{lawler1966branch,narendra1977branch,brucker1994branch},\nbranch-and-cut\n\\cite{padberg1991branch,tawarmalani2005polyhedral,stubbs1999branch},\nsemidefinite hierarchies \\cite{sherali1990hierarchy}, or even\ndirect enumeration when $\\C$ is a finite set.\nIn each iteration of these methods, a convex optimization problem derived\nfrom~(\\ref{e-prob}) is solved, with $\\C$ removed, and (possibly) additional\nvariables and convex constraints added.\nThese global methods are generally thought to have high worst-case\ncomplexities and indeed can be very slow in practice, even for modest\nsize problem instances.\n\n\\paragraph{Local solution methods and heuristics}\nA local method for~(\\ref{e-prob}) solves a modest number of convex\nproblems, in an attempt to find a good approximate solution,\n\\ie, a pair $(x,z)$ with $z\\in \\C$ and a low value of the merit\nfunction $\\eta(x,z)$.\nFor a feasibility problem, we might hope to find a solution;\nand if not, find one with a small constraint residual.\nFor a general problem, we can hope to find a feasible point with\nlow objective value, ideally near the lower bound on $p^\\star$ from\nthe relaxed problem.\nIf we cannot find any feasible points, we can settle for a pair $(x,z)$\nwith $z \\in \\C$ and low merit function value.\nAll of these methods are heuristics, in the sense that they cannot\nin general be guaranteed to find an optimal, or even good, or even\nfeasible, point in only a modest number of iterations.\n\nThere are of course many heuristics for the general\nproblem~(\\ref{e-prob}) and for many of its special cases.\nFor example, any global optimization method can be stopped\nafter some modest number of iterations; we take the best point found\n(in terms of the merit function) as our approximate solution.\nWe will discuss some local search methods, including neighbor search and\npolishing, in \\S\\ref{local}.\n\n\\paragraph{Existing solvers}\nThere are numerous open source and commercial solvers that can handle\nproblems with nonconvex constraints. We only mention a few of them here.\nGurobi \\cite{gurobi}, CPLEX \\cite{cplex2009v12}, MOSEK \\cite{mosek}\nprovide global methods for mixed integer linear programs,\nmixed integer quadratic programs, and mixed integer\nsecond order cone programs.\nBARON \\cite{ts:05}, Couenne \\cite{cplex2009v12}, and\nSCIP \\cite{achterberg2009scip} use global methods for\nnonlinear programs\nand mixed integer nonlinear programs.\nBonmin \\cite{bonami2008algorithmic} and Knitro \\cite{byrd2006knitro}\nprovide global methods for mixed integer convex programs and\nheuristic methods for mixed integer nonliner programs.\nIPOPT \\cite{waechter2009introduction} and\nNLopt \\cite{johnson2014nlopt} use heuristic methods for nonlinear\nprograms.\n\n\n\\subsection{Our approach}\nThe purpose of this paper is to describe a general system\nfor heuristic solution of~(\\ref{e-prob}), based on solving\na \\emph{modest} number of convex problems derived from~(\\ref{e-prob}).\nBy heuristic, we mean that the algorithm need not find an optimal point,\nor indeed, even a feasible point, even when one exists.\nWe would hope that for many feasible problem instances from some\napplication, the algorithm does find\na feasible point, and one with objective not too far from\nthe optimal value.\nThe disadvantage of a heuristic over a global method is clear and simple:\nit need not find an optimal point.\nThe advantage of a heuristic is that it can be (and often is)\ndramatically faster to carry out than a global method.\nMoreover there are many applications where a heuristic method\nfor~(\\ref{e-prob}) is sufficient.  This might be the case when\nthe objective and constraints are already approximations of what\nwe really want, so the added effort of solving it globally is not\nworth it.\n\n\n\\paragraph{ADMM.}\nOne of the heuristic methods described in this paper\nis based on the alternating directions method of multipliers\n(ADMM), an operator splitting algorithm originally devised to solve\nconvex optimization problems \\cite{boyd2011distributed}.\nWe call this heuristic nonconvex alternating directions method of\nmultipliers  (NC-ADMM).\nThe idea of using ADMM as a heuristic to solve nonconvex problems\nwas mentioned in \\cite[Ch. 9]{boyd2011distributed}, and has been explored by\nYedidia and others \\cite{Derbinsky2013AnImp} as a message\npassing algorithm. Consensus ADMM has been used\nfor general quadratically constrained quadratic programming\nin \\cite{huang2016consensus}. In \\cite{xu2012alternating}, ADMM\nhas been applied to non-negative matrix factorization with missing\nvalues. ADMM also has been used for real and complex polynomial\noptimization models in \\cite{jiang2014alternating},for constrained\ntensor factorization in \\cite{liavas2014parallel}, and for optimal\npower flow in \\cite{erseghe2014distributed}.\nADMM is a generalization of the method of multipliers\n\\cite{hestenes1969multiplier, bertsekas2014constrained},\nand there is a long history of using the method of multipliers\nto (attempt to) solve nonconvex problems\n\\cite{chartrand2012nonconvex, chartrand2013nonconvex,hong2014distributed,\nhong2014convergence,peng2015proximal,wang2014convergence,LP:15}.\nSeveral related methods, such as the Douglas-Rachford method\n\\cite{eckstein1992douglas}\nor Spingarn's method of partial inverses \\cite{spingarn1985applications},\ncould just as well have been used.\n\n\\paragraph{Our contribution.}\nThe paper has the following structure.\nIn \\S\\ref{s-algorithm} we discuss local search methods and describe how\nthey can be used as solution improvement methods. This will enable us to\nstudy simple but sophisticated methods such as relax-round-polish, and\niterative neighbor search.\nIn \\S\\ref{s-set} we catalog a variety of nonconvex\nsets for which Euclidean projection or approximated projection\nis easily evaluated and, when applicable, we discuss relaxations, restrictions,\nand the set of neighbors for a given point.\nIn \\S\\ref{s-implementation} we discuss an implementation\nof our general system for heuristic solution NCVX,\nas an extension of CVXPY \\cite{cvxpy},\na Python package for\nformulating and solving convex optimization problems.\nThe object-oriented features of CVXPY make the extension\nparticularly simple to implement.\nFinally, in \\S\\ref{s-examples} we demonstrate the performance\nof our methods on several example problems.\n\n\\section{Local improvement methods}\n\\label{local}\nIn this section we describe some simple general local search methods.\nThese methods take a point $z\\in\\C$ and by performing a local search on $z$ they\nfind a candidate pair $(\\hat x, \\hat z)$, with $\\hat z\\in\\C$ and a\nlower merit function. We will see that for many applications\nthese methods with a good initialization can be used to obtain an\napproximate solution. We will also see how we can use these methods\nto improve solution candidates from other heuristics, hence we refer to\nthese methods as \\emph{solution improvement}.\n\n\\subsection{Polishing}\n\\paragraph{Convex restriction.}\nWe can have a tractable \\emph{convex restriction} of $\\C$,\nthat includes a given point in $\\C$.\nThis means that for each point $\\tilde z\\in\\C$,\nwe have a set of convex equalities and inequalities on $z$, that hold for\n$\\tilde z$, and imply $z \\in \\C$.\nWe denote the set of points that satisfy the restrictions as\n$\\Crestrict (\\tilde z)$, and call this set the \\emph{restriction} of\n$\\C$ at $\\tilde z$.  The restriction set $\\Crestrict(\\tilde z)$ is convex,\nand satisfies $\\tilde z \\in \\Crestrict (\\tilde z) \\subseteq \\C$.\nThe trivial restriction is given by $\\Crestrict(\\tilde z)=\\{\\tilde z\\}$.\n\nWhen $\\C$ is discrete, for example $\\C= \\{0,1\\}^q$, the trivial restriction\nis the only restriction. In other cases we can have interesting nontrivial\nrestrictions, as we will see below.\nFor example, with\n$\\C=\\{z\\in \\reals^q \\mid \\card(z)\\leq k,~ \\|z\\|_\\infty \\leq M\\}$,\nwe can take as restriction\n$\\Crestrict(\\tilde z)$, the set of vectors $z$ with\nthe same sparsity pattern as $\\tilde z$, and $\\|z \\|_\\infty \\leq M$.\n\n\\paragraph{Polishing.}\nGiven any point $\\tilde z\\in\\C$, we can replace the constraint $z\\in\\C$\nwith $z\\in \\Crestrict(\\tilde z)$ to get the convex problem\n\\BEQ\\label{e-polish}\n\\begin{array}{ll}\n\\mbox{minimize}   &\\eta(x,z)\\\\\n\\mbox{subject to}  &z\\in \\Crestrict(\\tilde z),\n\\end{array}\n\\EEQ\nwith variables $x,z$.\n(When the restriction $\\Crestrict(\\tilde z)$ is the trivial one, \\ie,\na singleton, this is equivalent to fixing $z=\\tilde z$ and\nminimizing over $x$.)\nWe call this problem the \\emph{convex restriction} of~(\\ref{e-prob})\nat the point $\\tilde z$.\nThe restricted problem is convex,\nand its optimal value is an upper bound on $p^\\star$.\n\nAs a simple example of polishing consider the mixed\ninteger convex problem. The only restriction is the trivial one, so\nthe polishing problem for a given Boolean vector $\\tilde z$\nsimply fixes the values of the Boolean variables, and solves the\nconvex problem over the remaining variables, \\ie, $x$.\nFor the cardinality-constrained convex problem, polishing\nfixes the sparsity pattern of $z$ and solves the resulting convex\nproblem over $z$ and $x$.\n\nFor problems with nontrivial restrictions, we can solve the\npolishing problem repeatedly until convergence. In other\nwords we can use the output of the polishing problem as\nan initial point for another polishing problem and keep iterating\nuntil convergence or until a maximum number of iterations is reached.\nThis technique is called \\emph{iterated polishing} and\ndescribed in algorithm \\ref{iter-polish}.\n\\begin{algorithm}[H]\n\\caption{Iterated polishing}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\Do\n\\State $z^\\mathrm{old} \\gets \\tilde z$.\n\\State Find $(\\tilde x, \\tilde z)$\nby solving the polishing problem with restriction\n$z \\in \\Crestrict(z^\\mathrm{old})$.\n\\doWhile{$\\tilde z \\neq z^\\mathrm{old}$}\\\\\n\\Return $(\\hat x,\\hat z)$.\n\\end{algorithmic}\n\\label{iter-polish}\n\\end{algorithm}\n\nIf there exists a point $\\tilde x$ such that $(\\tilde x, \\tilde z)$ is feasible, the\nrestricted problem is feasible too.\nThe restricted problem need not be feasible in general, but if it is,\nwith solution $(\\hat x, \\hat z)$, then the pair\n$(\\hat x,\\hat z)$ is feasible for the original problem~(\\ref{e-prob})\nand satisfies $f(\\hat x,\\hat z) \\leq f(\\tilde x,\\tilde z)$\nfor any $\\tilde x$ for which $(\\tilde x,\\tilde z)$ is feasible.\nSo polishing can take a point $\\tilde z\\in\\C$ (or a pair\n$(\\tilde x,\\tilde z)$) and produce another pair $(\\hat x, \\hat z)$ with\na possibly better objective value.\n\n\n\\subsection{Relax-round-polish}\nWith the simple tools described so far\n(\\ie, relaxation, polishing, and\nprojection) we can create several heuristics for approximately\nsolving the problem~(\\ref{e-prob}).\nA basic version solves the relaxation, projects the relaxed\nvalue of $z$ onto $\\C$, and then polishes the result.\n\n\n\\begin{minipage}[c]{1 \\textwidth}\n\\begin{algorithm}[H]\n\\caption{Relax-round-polish heuristic}\n\\begin{algorithmic}[1]\n\\State Solve the convex relaxation~(\\ref{e-relaxation}) to obtain\n$(x^\\mathrm{rlx}, z^\\mathrm{rlx})$.\n\\State Find $z^\\mathrm{rnd}=\\Pi(z^\\mathrm{rlx})$.\n\\State Find $(\\hat x, \\hat z)$\nby solving the polishing problem with restriction\n$z \\in \\Crestrict(z^\\mathrm{rnd})$.\n\\end{algorithmic}\n\\label{alg_summary}\n\\end{algorithm}\n\\end{minipage}\n\\bigskip\n\nNote that in the first step we also obtain a lower bound on the\noptimal value $p^\\star$; in the polishing step we obtain\nan upper bound, and a feasible pair $(\\hat{x},\\hat{z})$ that achieves the upper\nbound (provided that polishing is successful).\nThe best outcome is for these bounds to be equal, which means that\nwe have found a (global) solution of~(\\ref{e-prob}) (for this problem\ninstance).\nBut relax-round-polish can fail; for example, it can fail to find a\nfeasible point even though one exists.\n\nMany variations on relax-round-polish are possible.\nWe can introduce randomization by replacing the round step with\n", "index": 17, "text": "\n\\[\nz^\\mathrm{rnd}=\\Pi(z^\\mathrm{rlx}+w),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"z^{\\mathrm{rnd}}=\\Pi(z^{\\mathrm{rlx}}+w),\" display=\"block\"><mrow><mrow><msup><mi>z</mi><mi>rnd</mi></msup><mo>=</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>z</mi><mi>rlx</mi></msup><mo>+</mo><mi>w</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nNotice that $\\phi(z)$ can be $+\\infty$ or $-\\infty$ in case the problem is not feasible\nfor this particular value of $z$, or\nproblem~(\\ref{e-relaxation}) is unbounded below after fixing $z$.\nThe function $\\phi$ is convex, since it is the\npartial minimization of a convex function over a\nconvex set \\cite[\\S 3.4.4]{boyd2004convex}.\nIt is defined over all points $z\\in\\reals^q$, but we are\ninterested in finding its minimum value over\nthe nonconvex set $\\C$. In other words,\nproblem~(\\ref{e-prob}) can be formulated as\n\\BEQ\\label{e-prob-ref}\n\\begin{array}{ll}\n\\mbox{minimize}   & \\phi(z) \\\\\n\\mbox{subject to} & z\\in\\C.\n\\end{array}\n\\EEQ\n\nAs discussed in \\cite[Chapter\\ 9]{boyd2011distributed}, ADMM can be used\nas a heuristic to solve nonconvex constrained problems. ADMM has the form\n\n", "itemtype": "equation", "pos": 27136, "prevtext": "\nwhere $w$ is a random vector.\nWe can repeat this heuristic with $K$ different random instances of $w$.\nFor each of $K$ samples of $w$, we polish, giving us a set of $K$ candidate\napproximate solutions.\nWe then take as our final approximate solution the best among\nthese $K$ candidates,\n\\ie,\nthe one with least merit function.\n\n\\subsection{Neighbor search} \\label{s-neighbor}\n\\paragraph{Neighbors.}\nWe describe the concept of \\emph{neighbors} for a point $z\\in\\C$ when\n$\\C$ is discrete. The set of neighbors of a point $z\\in\\C$,\ndenoted $\\Cneighbor(z)$, is the set of points\nwith distance one from $z$ in a natural (integer valued) distance,\nwhich depends on the set $\\C$.\nFor example for the set of Boolean vectors in $\\reals^n$ we use\n\\emph{Hamming distance},\nthe number of entries in which two Boolean vectors differ.\nHence neighbors of a Boolean\nvector $z$ are the set of vectors that differ from $z$ in one component.\nThe distance between two permutation matrices is defined as the minimum number of\nswaps of adjacent rows and columns necessary to transform the first\nmatrix into the second.\nWith this distance,\nneighbors of a permutation matrix $Z$ are the\nset of permutation matrices generated by swapping any two adjacent rows\nor columns in $Z$.\n\nFor Cartesian products of discrete sets we use the sum of\ndistances.  In this case, for\n$z = (z_1,z_2,\\ldots,z_k) \\in \\C = \\C_1 \\times \\C_2 \\times \\ldots \\times \\C_k$, neighbors\nof $z$ are points of the form\n$(z_1,\\ldots,z_{i-1},\\tilde z_i,z_{i+1},\\ldots, z_k)$ where\n$\\tilde z_i$ is a neighbor of $z_i$ in $\\C_i$.\n\n\\paragraph{Basic neighbor search.}\nWe introduced polishing as a tool that can find a pair $(\\hat x,\\hat z)$ given an\ninput $\\tilde z\\in\\C$ by solving a sequence of convex problems. In basic\nneighbor search we solve the polishing problem for $\\tilde z$ and all neighbors\nof $\\tilde z$ and return the pair $(x^*, z^*)$ with the smallest merit function value.\nIn practice, we can sample from $\\Cneighbor(\\tilde z)$ instead of iterating over\nall points in $\\Cneighbor(\\tilde z)$ if $\\left|\\Cneighbor(\\tilde z)\\right|$ is large.\n\n\n\\begin{algorithm}[H]\n\\caption{Basic neighbor search}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\State Initialize $(x_\\mathrm{best}, z_\\mathrm{best})=\\emptyset$, $\\eta_\\mathrm{best}=\\infty$.\n\\For{$\\hat z \\in \\{ \\tilde z \\} \\cup \\Cneighbor(\\tilde z)$}\n\\State Find $(x^*, z^*)$,\nby solving the polishing problem~(\\ref{e-polish}),\nwith constraint $z\\in \\Crestrict(\\hat z)$.\n\\If{$\\eta(x^*, z^*) < \\eta_\\mathrm{best}$}\n\\State $(x_\\mathrm{best}, z_\\mathrm{best})= (x^*, z^*)$, $\\eta_\\mathrm{best} =\\eta(x^*, z^*)$.\n\\EndIf\n\\EndFor\\\\\n\\Return $(x_\\mathrm{best},z_\\mathrm{best})$.\n\\end{algorithmic}\n\\label{iter-hill-climb}\n\\end{algorithm}\n\n\\paragraph{Iterated neighbor search.}\nWe can carry out the described neighbor search iteratively as follows.\nWe maintain a current value of $z$, corresponding to the\nbest pair $(x,z)$ found so far.  We then consider a neighbor of $z$ and\npolish.  If the new point $(x,z)$ is better than the current best one,\nwe reset our best and continue; otherwise we examine another neighbor.\nThis is done until a maximum number of iterations is reached, or\nall neighbors of the current best $z$ produce (under polishing) no better\npairs. This procedure is sometimes called \\emph{hill climbing},\nsince it resembles an attempt to find the top of a mountain by repeatedly\ntaking steps towards an ascent direction.\n\n\\begin{algorithm}[H]\n\\caption{Iterative neighbor search}\n\\begin{algorithmic}[1]\n\\State Input: $\\tilde z$\n\\State Find $(x_\\mathrm{best}, z_\\mathrm{best})$ by solving the polishing problem~(\\ref{e-polish})\n\\State$\\eta_\\mathrm{best}\\gets\\eta(x_\\mathrm{best}, z_\\mathrm{best})$.\n\\For{$\\hat z \\in\\Cneighbor(z_\\mathrm{best})$}\\label{marker}\n\\State Find $(x^*, z^*)$,\nby solving the polishing problem~(\\ref{e-polish}),\nwith constraint $z\\in \\Crestrict(\\hat z)$.\n\\If{$\\eta(x^*, z^*) < \\eta_\\mathrm{best}$}\n\\State $(x_\\mathrm{best}, z_\\mathrm{best})= (x^*, z^*)$, $\\eta_\\mathrm{best} =\\eta(x^*, z^*)$.\n\\State \\Goto{marker}\n\\EndIf\n\\EndFor\\\\\n\\Return $(x_\\mathrm{best},z_\\mathrm{best})$.\n\\end{algorithmic}\n\\label{alg:iter-neighbor}\n\\end{algorithm}\nNotice that when no neighbors are available for $\\tilde z\\in\\C$,\nthis algorithm reduces to simple polishing.\n\n\\section{NC-ADMM}\\label{s-algorithm}\nWe already can use the simple tools described in the previous section\nas heuristics to find approximate solutions to problem~(\\ref{e-prob}).\nIn this section, we describe the alternating direction method of multipliers (ADMM)\nas a mechanism to generate candidate points $\\tilde z$ to carry out local search methods\nsuch as iterated neighbor search. We call this method nonconvex ADMM, or NC-ADMM.\n\\subsection{ADMM}\nDefine $\\phi:\\reals^q \\to \\reals \\cup \\{ -\\infty, +\\infty\\}$ such that\n$\\phi(z)$ is the best objective value of problem~(\\ref{e-prob})\nafter fixing $z$. In other words,\n", "index": 19, "text": "\n\\[\n\\phi(z) = \\inf_x \\left\\{ f_0(x,z) \\mid\nf_i(x, z) \\leq 0,~i=1,\\ldots,m,~ Ax + Bz = c \\right\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\phi(z)=\\inf_{x}\\left\\{f_{0}(x,z)\\mid f_{i}(x,z)\\leq 0,~{}i=1,\\ldots,m,~{}Ax+%&#10;Bz=c\\right\\}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo movablelimits=\"false\">inf</mo><mi>x</mi></munder><mo>\u2061</mo><mrow><mo>{</mo><mrow><msub><mi>f</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo><mrow><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>z</mi></mrow></mrow><mo>=</mo><mi>c</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\rho>0$ is an algorithm parameter,\n$k$ is the iteration counter,\nand $\\Pi$ denotes Euclidean projection onto $\\C$\n(which need not be unique when $\\C$ is not convex).\n\nThe initial values $u^0$ and $z^0$ are additional algorithm parameters.\nWe always set $u^0 = 0$ and draw $z^0$\nrandomly from a normal distribution $\\mathcal N(0, \\sigma^2 I)$,\nwhere $\\sigma > 0$ is an algorithm parameter.\n\n\\subsection{Algorithm subroutines}\n\\paragraph{Convex proximal step}\nCarrying out the first step of the\nalgorithm, \\ie, evaluating the proximal operator of $\\phi$,\ninvolves solving the convex optimization problem\n\\BEQ\\label{e-prox-step}\n\\begin{array}{ll}\n\\mbox{minimize}   & f_0(x,z) + (\\rho/2) \\|z-z^k+u^k\\|_2^2 \\\\\n\\mbox{subject to} & f_i(x, z) \\leq 0, \\quad i=1,\\ldots,m,\\\\\n& Ax + Bz = c,\n\\end{array}\n\\EEQ\nover the variables $x \\in \\reals^n$ and $z \\in \\reals^q$.\nThis is the original problem~(\\ref{e-prob}), with the nonconvex\nconstraint $z\\in \\C$ removed, and an additional\nconvex quadratic term involving $z$ added to the objective.\nWe let $(x^{k+1}, w^{k+1})$ denote a solution of~(\\ref{e-prox-step}).\nIf the problem~(\\ref{e-prox-step}) is infeasible, then so is\nthe original problem~(\\ref{e-prob}); should this happen, we can terminate\nthe algorithm with the certain conclusion that~(\\ref{e-prob})\nis infeasible.\n\n\\paragraph{Projection}\nThe (nonconvex) projection step consists of finding the closest point in $\\C$ to\n$w^{k+1} - z^{k} + u^k$.\nIf more than one point has the smallest distance,\nwe can choose one of the minimizers arbitrarily.\n\n\n\\paragraph{Dual update}\nThe iterate $u^k \\in \\reals^q$\ncan be interpreted as a scaled dual variable, or\nas the running sum of the error values $w^{k+1}-z^k$.\n\n\\subsection{Discussion}\n\\paragraph{Convergence.}\nWhen $\\C$ is convex (and a solution of~(\\ref{e-prob}) exists),\nthis algorithm is guaranteed to converge to a solution,\nin the sense that $f_0(x^{k+1},w^{k+1})$ converges to the optimal\nvalue of the problem~(\\ref{e-prob}),\nand $w^{k+1}-z^{k+1}\\to 0$, \\ie, $w^{k+1}\\to \\C$.\nSee~\\cite[\\S 3]{boyd2011distributed} and the references therein for a\nmore technical description and details.\nBut in the general case, when $\\C$ is not convex,\nthe algorithm is not guaranteed to converge, and even when it does,\nit need not be to a global, or even local, minimum.\nSome recent progress has been made on understanding convergence\nin the nonconvex case \\cite{LP:15}.\n\n\\paragraph{Parameters.}\nAnother difference with the convex case is that the convergence and the\nquality of solution depends on $\\rho$, whereas for convex problems\nthis algorithm is guaranteed to converge to the optimal value regardless\nof the choice of $\\rho$.\nIn other words, in the convex case the choice of parameter $\\rho$ only\naffects the speed of the convergence, while in the nonconvex case the\nchoice of $\\rho$ can have a\ncritical role in the quality of approximate solution,\nas well as the speed at which this solution is found.\n\nThe optimal parameter selection for ADMM is still an active research area.\nIn \\cite{ghadimi2015optimal} the optimal parameter selection\nfor quadratic problems is discussed.\nIn a more generalized setting, Giselsson discusses the optimal parameter selection\nfor ADMM for strongly convex functions \\cite{giselsson2014diagonal,\ngiselsson2014monotonicity,\ngiselsson2014preconditioning}.\nThe dependency of global and local convergence\nproperties of ADMM on parameter choice has been studied in \\cite{hong2012linear,\nboley2013local}.\n\n\\paragraph{Initialization.}\nIn the convex case the choice of initial point $z^0$ affects the number\nof iterations to find a solution, but not the quality of the solution.\nUnsurprisingly, the nonconvex case differs in that the choice of $z^0$\nhas a major effect on the the quality of the approximate solution.\nAs with the choice of $\\rho$,\nthe initialization in the nonconvex case is currently an active\narea of research; see, \\eg, \\cite{huang2016consensus,LP:15,takapoui2015simple}.\nGetting the best possible results on a particular problem requires\na careful and problem specific choice of initialization.\nWe draw initial points randomly from $\\mathcal N(0,\\sigma^2 I)$\nbecause we want a method that generalizes easily across many different problems.\n\n\\subsection{Solution improvement}\nNow we describe two techniques to obtain better solutions after carrying\nout ADMM. The first technique relies on iterated neighbor search and\nthe second one is using multiple restarts with random initial points in order to\nincrease the chance of obtaining a better solution.\n\n\\paragraph{Iterated neighbor search}\nAfter each iteration, we can carry out iterated polishing (as described in \\S \\ref{s-neighbor})\nwith $\\Crestrict(z^{k+1})$ to obtain $(\\hat x^{k+1},\\hat z^{k+1})$. We will return the pair\nwith the smallest merit function as the output\nof the algorithm.\n\n\\paragraph{Multiple restarts}\nAs we mentioned, we choose the initial value $z^0$ from a normal\ndistribution $\\mathcal N(0,\\sigma^2 I)$. We can run the algorithm multiple\ntimes from different initial points to increase the chance of a feasible point\nwith a smaller objective value.\n\n\\subsection{Overall algorithm}\nThe following is a summary of the algorithm with solution improvement.\n\n\\begin{algorithm}[H]\n\\caption{NC-ADMM heuristic} \n\\begin{algorithmic}[1]\n\\State Initialize $u^0=0$, $(x_{\\textrm{best}},z_{\\textrm{best}}) = \\emptyset$, $\\eta_\\mathrm{best}=\\infty$.\n\\For{algorithm repeats $1, 2, \\ldots, M$}\n\\State Initialize $z^0\\sim\\mathcal N(0,\\sigma^2I)$.\n\\For{$k=1, 2, \\dots, N$}\n\\State $(x^{k+1}, w^{k+1}) \\gets \\argmin_z \\left(\\phi(z)+ (\\rho/2)\\|z - z^k + u^k\\|_2^2\\right)$.\n\\State $z^{k+1} \\gets  \\Pi\\left(w^{k+1} - z^{k} + u^k \\right)$.\n\\State Use algorithm (\\ref{alg:iter-neighbor}) on $z^{k+1}$ to get the improved iterate $(\\hat x, \\hat z)$.\n\\If {$\\eta(\\hat x, \\hat z)<\\eta_{\\textrm{best}}$}\n\\State $(x_{\\textrm{best}},z_{\\textrm{best}}) \\gets (\\hat x, \\hat z)$, $\\eta_{\\textrm{best}} = \\eta(\\hat x, \\hat z)$.\n\\EndIf\n\\State $u^{k+1} \\gets  u^k + w^{k+1} - z^{k+1}$.\n\\EndFor\n\\EndFor \\\\\n\\Return $x_{\\textrm{best}},z_{\\textrm{best}}$.\n\\end{algorithmic}\n\\label{alg_summary}\n\\end{algorithm}\n\\bigskip\n\n\n\\section{Projections onto nonconvex sets}\\label{s-set}\nIn this section we catalog various nonconvex sets\nwith their implied convex constraints which will be included in the convex\nconstraints of\nproblem~(\\ref{e-prob}). We also provide a Euclidean\nprojection (or approximate projection)\n$\\Pi$ for these sets.\nAlso, when applicable, we introduce a nontrivial restriction and set\nof neighbors.\n\n\\subsection{Subsets of $\\reals$}\n\\paragraph{Booleans}\nFor $\\C = \\{0,1\\}$, a convex relaxation (in fact, the convex hull of $\\C$) is $[0,1]$.\nAlso, a projection is simple rounding:\n$\\Pi(z) = 0$ for $z \\leq 1/2$, and $\\Pi(z) =1$ for $z > 1/2$.\n($z=1/2$ can be mapped to either point.)\nMoreover, $\\Cneighbor(0)=\\{1\\}$ and $\\Cneighbor(1)=\\{0\\}$.\n\n\\paragraph{Finite sets}\nIf $\\C$ has $M$ elements, the convex hull of $\\C$ is the interval from the\nsmallest to the largest element. We can project onto $\\C$ with\nno more than $\\log_2 M$ comparisons.\nFor each $z\\in\\C$ the set of neighbors of $\\C$ are the\nimmediate points to the right and left of $z$ (if they exist).\n\n\\paragraph{Bounded integers}\nLet $\\C = \\integers \\cap [-M,M]$, where $M > 0$.\nThe convex hull is the interval from the smallest to the largest element\ninteger in $[-M,M]$,\n\\ie, $[-\\lfloor M \\rfloor, \\lfloor M \\rfloor]$.\nThe projection onto $\\C$ is simple: if $z>\\lfloor M \\rfloor$ ($z< - \\lfloor M \\rfloor$)\nthen $\\Pi(z) = \\lfloor M \\rfloor$\n($\\Pi(z) = -\\lfloor M \\rfloor$).\nOtherwise, the projection of $z$ can be found by simple rounding.\nFor each $z \\in \\C$ the set of neighbors of $\\C$\nis $\\{z-1,z+1\\}\\cap[-M,M]$\n\n\\subsection{Subsets of $\\reals^n$}\n\n\\paragraph{Boolean vectors with fixed cardinality}\\label{sec:bool_card}\nLet $\\C = \\{ z\\in \\{0,1\\}^n \\mid \\card(z) = k\\}$.\nAny $z \\in \\C$ satisfies $0\\leq z\\leq 1$ and $\\ones^T z=k$.\nWe can project $z \\in \\reals^n$ onto $\\C$\nby setting the $k$ entries of $z$ with largest value to\none and the remaining entries to zero.\nFor any point $z\\in\\C$, the set of neighbors of $z$ is all points\ngenerated by swapping an adjacent $1$ and $0$ in $z$.\n\n\\paragraph{Vectors with bounded cardinality}\nLet $\\C=\\{x \\in [-M,M]^n \\mid \\card(x) \\leq k\\}$, where $M > 0$ and\n$k \\in \\integers_+$.\n(Vectors $z\\in \\C$ are called $k$-sparse.)\nAny point $z\\in\\C$ satisfies\n$-M\\leq z\\leq M$ and $-Mk\\leq \\ones^T z\\leq Mk$.\nThe projection $\\Pi(z)$ is found as follows\n", "itemtype": "equation", "pos": 28019, "prevtext": "\nNotice that $\\phi(z)$ can be $+\\infty$ or $-\\infty$ in case the problem is not feasible\nfor this particular value of $z$, or\nproblem~(\\ref{e-relaxation}) is unbounded below after fixing $z$.\nThe function $\\phi$ is convex, since it is the\npartial minimization of a convex function over a\nconvex set \\cite[\\S 3.4.4]{boyd2004convex}.\nIt is defined over all points $z\\in\\reals^q$, but we are\ninterested in finding its minimum value over\nthe nonconvex set $\\C$. In other words,\nproblem~(\\ref{e-prob}) can be formulated as\n\\BEQ\\label{e-prob-ref}\n\\begin{array}{ll}\n\\mbox{minimize}   & \\phi(z) \\\\\n\\mbox{subject to} & z\\in\\C.\n\\end{array}\n\\EEQ\n\nAs discussed in \\cite[Chapter\\ 9]{boyd2011distributed}, ADMM can be used\nas a heuristic to solve nonconvex constrained problems. ADMM has the form\n\n", "index": 21, "text": "\\begin{equation}\n\\label{e-update}\n  \\begin{split}\nw^{k+1} &:=  \\argmin_z \\left(\\phi(z)+ (\\rho/2)\\|z - z^k + u^k\\|_2^2\\right)\\\\\nz^{k+1} &:=  \\Pi\\left(w^{k+1} - z^{k} + u^k\\right)\\\\\nu^{k+1} &:=  u^k + w^{k+1} - z^{k+1},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle w^{k+1}&amp;\\displaystyle:=\\argmin_{z}\\left(\\phi(z)+(%&#10;\\rho/2)\\|z-z^{k}+u^{k}\\|_{2}^{2}\\right)\\\\&#10;\\displaystyle z^{k+1}&amp;\\displaystyle:=\\Pi\\left(w^{k+1}-z^{k}+u^{k}\\right)\\\\&#10;\\displaystyle u^{k+1}&amp;\\displaystyle:=u^{k}+w^{k+1}-z^{k+1},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msup><mi>w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>:=</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\argmin</mtext></merror><mi>z</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c1</mi><mo>/</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>z</mi><mo>-</mo><msup><mi>z</mi><mi>k</mi></msup></mrow><mo>+</mo><msup><mi>u</mi><mi>k</mi></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msup><mi>z</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>:=</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>-</mo><msup><mi>z</mi><mi>k</mi></msup></mrow><mo>+</mo><msup><mi>u</mi><mi>k</mi></msup></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msup><mi>u</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>:=</mo><mrow><mrow><msup><mi>u</mi><mi>k</mi></msup><mo>+</mo><msup><mi>w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo>-</mo><msup><mi>z</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\mathcal I \\subseteq \\{1,\\ldots, q\\}$ is a set of indices of\n$k$ largest values of $|z_i|$.\nWe will describe many projections, and some approximate projections,\nin \\S\\ref{s-set}.\n\n\\subsection{Residual and merit functions}\nFor any $(x,z)$ with $z \\in \\C$, we define the \\emph{constraint\nresidual} as\n", "itemtype": "equation", "pos": 8101, "prevtext": "\nwhich penalizes non-sorted choices of $w_i$,\nto the objective.\nIt is easy to see that this yields an equivalent problem:\ngiven any feasible point $(x,z)$ for~(\\ref{e-prob}),\nwe can always permute $w_i$ so that $\\tau(x,z) = 0$.\n\nUnlike the basic convex relaxations, which pertain to the set $\\C$\nand are independent of the convex parts of the problem,\nthese symmetry-breaking constraints or penalty depend on the\nentire problem, and not just the set $\\C$.\n\\end{incomplete}\n\n\\subsection{Projections and approximate projections}\nOur methods will make use of tractable projection, or\ntractable approximate projection, onto the set $\\C$.  The usual Euclidean\nprojection onto $\\C$ will be denoted $\\Pi$.\n(It need not be unique when $\\C$ is not convex.)\nBy approximate projection,\nwe mean any function $\\hat \\Pi: \\reals^q \\to \\C$ that satisfies\n$\\hat \\Pi (z) = z $ for $z \\in \\C$.\nFor example, when $\\C = \\{0,1\\}^q$, exact projection is given by\nrounding the entries to $\\{0,1\\}$.\n\nAs a less trivial example, consider the cardinality-constrained problem.\nThe projection of $z$ onto $\\C$ is given by\n", "index": 11, "text": "\n\\[\n\\left(\\Pi \\left(z\\right)\\right)_i = \\left\\{ \\begin{array}{ll}\nM & z_i > M, ~i \\in \\mathcal I\\\\\n-M & z_i < -M, ~i \\in \\mathcal I\\\\\nz_i & |z_i| \\leq M, ~i \\in \\mathcal I\\\\\n0 & i \\not\\in \\mathcal I,\n\\end{array}\\right.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\left(\\Pi\\left(z\\right)\\right)_{i}=\\left\\{\\begin{array}[]{ll}M&amp;z_{i}&gt;M,~{}i\\in%&#10;\\mathcal{I}\\\\&#10;-M&amp;z_{i}&lt;-M,~{}i\\in\\mathcal{I}\\\\&#10;z_{i}&amp;|z_{i}|\\leq M,~{}i\\in\\mathcal{I}\\\\&#10;0&amp;i\\not\\in\\mathcal{I},\\end{array}\\right.\" display=\"block\"><mrow><msub><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi>M</mi></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&gt;</mo><mi>M</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mi>M</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&lt;</mo><mrow><mo>-</mo><mi>M</mi></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>z</mi><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>M</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>i</mi><mo>\u2209</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $A \\in \\symm_{++}^n$,\n$b \\in \\reals^n$, and $\\beta \\geq \\alpha \\geq -b^TA^{-1}b$.\nWe assume $\\alpha \\geq -b^TA^{-1}b$ because $z^T A z + 2b^Tz  \\geq -b^TA^{-1}b$\nfor all $z \\in \\reals^n$.\nAny point $z\\in\\C$ satisfies the convex inequality $z^TAz + 2b^Tz \\leq \\beta$.\n\nWe can find the projection onto $\\C$ as follows.\nIf $z^T A z + 2b^Tz > \\beta$, it suffices to solve\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|x - z\\|_2^2 \\\\\n\\mbox{subject to} & x^T A x + 2b^Tx \\leq \\beta,\n\\end{array}\n\\EEQ\nand if $z^T A z + 2b^Tz < \\alpha$, it suffices to solve\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|x - z\\|_2^2 \\\\\n\\mbox{subject to} & x^T A x + 2b^Tx \\geq \\alpha.\n\\end{array}\n\\EEQ\n(If $\\alpha \\leq z^T A z + 2b^Tz \\leq \\beta$, clearly $\\Pi(z)=z$.)\nThe first problem is a convex quadratically constrained quadratic program\nand the second problem can be solved by\nsolving a simple semidefinite program as described in \\cite[Appendix B]{boyd2004convex}.\nFurthermore, there is a more\nefficient way to find the projection by finding the roots of a single-variable polynomial of\ndegree $2p+1$, where $p$ is the number of distinct eigenvalues of $A$\n\\cite{huang2016consensus,hmam2010quadratic}.\nNote that the projection can be easily found even if $A$ is not positive definite;\nwe assume $A\\in\\symm_{++}^n$ only to make $\\C$ compact and have a useful convex relaxation.\n\nA restriction of $\\C$ at $z \\in \\C$ is the set\n", "itemtype": "equation", "pos": 37676, "prevtext": "\nwhere $\\mathcal I \\subseteq \\{1,\\ldots, n\\}$ is a set of indices of\n$k$ largest values of $|z_i|$.\n\nA restriction of $\\C$ at $z\\in\\C$ is the set of\nall points in $[-M,M]^n$ that have the same sparsity pattern as $z$.\nFor any point $z\\in\\C$, the set of neighbors of $z$ are all points\n$x \\in \\C$ whose sparsity pattern $\\tilde{x} \\in \\{0,1\\}^n$ is a neighbor\nof $z$'s sparsity pattern $\\tilde{z} \\in \\{0,1\\}^n$.\nIn other words, $\\tilde{x}$ can be obtained by swapping an adjacent\n$1$ and $0$ in $\\tilde{z}$.\n\n\\paragraph{Quadratic sets}\nLet $\\symm_{+}^n$ and $\\symm_{++}^n$ denote the set of $n\\times n$\nsymmetric positive semidefinite and symmetric positive definite matrices,\nrespectively.\nConsider the set\n", "index": 25, "text": "\n\\[\n\\C = \\{ z\\in \\reals^n \\mid \\alpha \\leq z^T A z + 2b^Tz \\leq \\beta \\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\C=\\{z\\in\\reals^{n}\\mid\\alpha\\leq z^{T}Az+2b^{T}z\\leq\\beta\\},\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\C</mtext></merror><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>z</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>n</mi></msup></mrow><mo>\u2223</mo><mrow><mi>\u03b1</mi><mo>\u2264</mo><mrow><mrow><msup><mi>z</mi><mi>T</mi></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><mi>z</mi></mrow></mrow><mo>\u2264</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nRecall that $z^TAz + 2b^Tz + b^TA^{-1}b \\geq 0$ for all $z \\in \\reals^n$\nand we assume $\\alpha \\geq -b^TA^{-1}b$,\nso $\\Crestrict(z)$ is always well defined.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{Annulus and sphere}\nConsider the set\n", "itemtype": "equation", "pos": 39163, "prevtext": "\nwhere $A \\in \\symm_{++}^n$,\n$b \\in \\reals^n$, and $\\beta \\geq \\alpha \\geq -b^TA^{-1}b$.\nWe assume $\\alpha \\geq -b^TA^{-1}b$ because $z^T A z + 2b^Tz  \\geq -b^TA^{-1}b$\nfor all $z \\in \\reals^n$.\nAny point $z\\in\\C$ satisfies the convex inequality $z^TAz + 2b^Tz \\leq \\beta$.\n\nWe can find the projection onto $\\C$ as follows.\nIf $z^T A z + 2b^Tz > \\beta$, it suffices to solve\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|x - z\\|_2^2 \\\\\n\\mbox{subject to} & x^T A x + 2b^Tx \\leq \\beta,\n\\end{array}\n\\EEQ\nand if $z^T A z + 2b^Tz < \\alpha$, it suffices to solve\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|x - z\\|_2^2 \\\\\n\\mbox{subject to} & x^T A x + 2b^Tx \\geq \\alpha.\n\\end{array}\n\\EEQ\n(If $\\alpha \\leq z^T A z + 2b^Tz \\leq \\beta$, clearly $\\Pi(z)=z$.)\nThe first problem is a convex quadratically constrained quadratic program\nand the second problem can be solved by\nsolving a simple semidefinite program as described in \\cite[Appendix B]{boyd2004convex}.\nFurthermore, there is a more\nefficient way to find the projection by finding the roots of a single-variable polynomial of\ndegree $2p+1$, where $p$ is the number of distinct eigenvalues of $A$\n\\cite{huang2016consensus,hmam2010quadratic}.\nNote that the projection can be easily found even if $A$ is not positive definite;\nwe assume $A\\in\\symm_{++}^n$ only to make $\\C$ compact and have a useful convex relaxation.\n\nA restriction of $\\C$ at $z \\in \\C$ is the set\n", "index": 27, "text": "\n\\[\n\\Crestrict(z) = \\{x \\in \\reals^n \\mid\n\\frac{x^TAz + b^T(x + z) + b^TA^{-1}b}{\\sqrt{z^TAz + 2b^Tz + b^TA^{-1}b}} \\geq\n\\sqrt{\\alpha + b^TA^{-1}b},\n\\quad x^TAx + 2b^Tx \\leq \\beta \\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(z)=\\{x\\in\\reals^{n}\\mid\\frac{x^{T}Az+b^{T}(x+z)+b^{T}A^{-1}b}{\\sqrt%&#10;{z^{T}Az+2b^{T}z+b^{T}A^{-1}b}}\\geq\\sqrt{\\alpha+b^{T}A^{-1}b},\\quad x^{T}Ax+2b%&#10;^{T}x\\leq\\beta\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>x</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>n</mi></msup></mrow><mo>\u2223</mo><mrow><mrow><mfrac><mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>+</mo><mrow><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mi>z</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>b</mi></mrow></mrow><msqrt><mrow><mrow><msup><mi>z</mi><mi>T</mi></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><mi>z</mi></mrow><mo>+</mo><mrow><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>b</mi></mrow></mrow></msqrt></mfrac><mo>\u2265</mo><msqrt><mrow><mi>\u03b1</mi><mo>+</mo><mrow><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>b</mi></mrow></mrow></msqrt></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>b</mi><mi>T</mi></msup><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>\u2264</mo><mi>\u03b2</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $R \\geq r$.\n\nAny point $z\\in\\C$ satisfies\n$\\|z\\|_2\\leq R$.\nWe can project $z \\in \\reals^n \\setminus \\{0\\}$\nonto $\\C$ by the following scaling\n", "itemtype": "equation", "pos": 39647, "prevtext": "\nRecall that $z^TAz + 2b^Tz + b^TA^{-1}b \\geq 0$ for all $z \\in \\reals^n$\nand we assume $\\alpha \\geq -b^TA^{-1}b$,\nso $\\Crestrict(z)$ is always well defined.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{Annulus and sphere}\nConsider the set\n", "index": 29, "text": "\n\\[\n\\mathcal C = \\{z \\in \\reals^n \\mid r \\leq \\|z\\|_2 \\leq R\\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{C}=\\{z\\in\\reals^{n}\\mid r\\leq\\|z\\|_{2}\\leq R\\},\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>z</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>n</mi></msup></mrow><mo>\u2223</mo><mrow><mi>r</mi><mo>\u2264</mo><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>R</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nIf $z = 0$, any point with Euclidean norm $r$ is a valid projection.\n\nA restriction of $\\C$ at $z\\in\\C$ is the set\n", "itemtype": "equation", "pos": 39861, "prevtext": "\nwhere $R \\geq r$.\n\nAny point $z\\in\\C$ satisfies\n$\\|z\\|_2\\leq R$.\nWe can project $z \\in \\reals^n \\setminus \\{0\\}$\nonto $\\C$ by the following scaling\n", "index": 31, "text": "\n\\[\n\\Pi(z) = \\begin{cases}\nrz/\\|z\\|_2 & \\mbox{if $\\|z\\|_2 < r$}\\\\\nz & \\mbox{if $z\\in\\C$}\\\\\nR z/\\|z\\|_2 & \\mbox{if $\\|z\\|_2 > R$},\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\Pi(z)=\\begin{cases}rz/\\|z\\|_{2}&amp;\\mbox{if $\\|z\\|_{2}&lt;r$}\\\\&#10;z&amp;\\mbox{if $z\\in\\C$}\\\\&#10;Rz/\\|z\\|_{2}&amp;\\mbox{if $\\|z\\|_{2}&gt;R$},\\end{cases}\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>r</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>/</mo><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>&lt;</mo><mi>r</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi>z</mi></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><mi>z</mi><mo>\u2208</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\C</mtext></merror></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>z</mi></mrow><mo>/</mo><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>&gt;</mo><mi>R</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nNotice that if $r=R$, then $\\C$ is a sphere and the restriction will be a singleton.\n\n\\paragraph{Box complement and cube surface}\nConsider the set\n", "itemtype": "equation", "pos": 40120, "prevtext": "\nIf $z = 0$, any point with Euclidean norm $r$ is a valid projection.\n\nA restriction of $\\C$ at $z\\in\\C$ is the set\n", "index": 33, "text": "\n\\[\n\\Crestrict(z) = \\{x \\in \\reals^n \\mid x^Tz\\geq r\\|z\\|_2,~ \\|x\\|_2 \\leq R\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(z)=\\{x\\in\\reals^{n}\\mid x^{T}z\\geq r\\|z\\|_{2},~{}\\|x\\|_{2}\\leq R\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>x</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>n</mi></msup></mrow><mo>\u2223</mo><mrow><mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi>z</mi></mrow><mo>\u2265</mo><mrow><mi>r</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>R</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nAny point $z\\in\\C$ satisfies $\\|z\\|_\\infty \\leq b$. For any point $z$ we can find\nthe projection $\\Pi(z)$ by projecting $z$ component-wise onto $[a,b]$.\n\nGiven $z\\in\\C$ we can obtain a restriction by finding $k=\\argmin_i \\max\\{|z_i|,a\\}$ and if $z_k \\geq 0$\nthen\n", "itemtype": "equation", "pos": 40349, "prevtext": "\nNotice that if $r=R$, then $\\C$ is a sphere and the restriction will be a singleton.\n\n\\paragraph{Box complement and cube surface}\nConsider the set\n", "index": 35, "text": "\n\\[\n\\mathcal C = \\{z \\in \\reals^n \\mid a \\leq \\|z\\|_\\infty \\leq b\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{C}=\\{z\\in\\reals^{n}\\mid a\\leq\\|z\\|_{\\infty}\\leq b\\}.\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>z</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mi>n</mi></msup></mrow><mo>\u2223</mo><mrow><mi>a</mi><mo>\u2264</mo><msub><mrow><mo>\u2225</mo><mi>z</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>b</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nIf $z_k<0$, then\n", "itemtype": "equation", "pos": 40683, "prevtext": "\nAny point $z\\in\\C$ satisfies $\\|z\\|_\\infty \\leq b$. For any point $z$ we can find\nthe projection $\\Pi(z)$ by projecting $z$ component-wise onto $[a,b]$.\n\nGiven $z\\in\\C$ we can obtain a restriction by finding $k=\\argmin_i \\max\\{|z_i|,a\\}$ and if $z_k \\geq 0$\nthen\n", "index": 37, "text": "\n\\[\n\\Crestrict(z) = \\{x \\mid x_k\\geq a,~ \\|x\\|_\\infty \\leq b\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(z)=\\{x\\mid x_{k}\\geq a,~{}\\|x\\|_{\\infty}\\leq b\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>\u2223</mo><mrow><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>\u2265</mo><mi>a</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>b</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nNotice that if $a = b$, then $\\C$ is a cube surface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Subsets of $\\reals^{m\\times n}$}\nRemember that the projection of a point $X\\in\\reals^{m \\times n}$ on a set\n$\\C\\subset\\reals^{m \\times n}$ is a point $Z\\in\\C$ such that\nthe Frobenius norm $\\|X-Z\\|_\\mathrm F$ is minimized.\nAs always, if there is more\nthan one point $Z$ that minimizes\n$\\|X-Z\\|_\\mathrm F$, we accept any of them.\n\n\\paragraph{Matrices with bounded singular values and orthogonal matrices}\nConsider the set of $m \\times n$ matrices whose singular values lie between\n$1$ and $\\alpha$\n", "itemtype": "equation", "pos": 40766, "prevtext": "\nIf $z_k<0$, then\n", "index": 39, "text": "\n\\[\n\\Crestrict(z) = \\{x \\mid x_k \\leq -a,~ \\|x\\|_\\infty \\leq b\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(z)=\\{x\\mid x_{k}\\leq-a,~{}\\|x\\|_{\\infty}\\leq b\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>\u2223</mo><mrow><mrow><msub><mi>x</mi><mi>k</mi></msub><mo>\u2264</mo><mrow><mo>-</mo><mi>a</mi></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>b</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\alpha\\geq 1$, and $A \\preceq B$ means $B-A\\in\\symm_+^n$ .\nAny point $Z\\in\\C$ satisfies $\\|Z\\|_2\\leq \\alpha$.\n\nIf $Z=U\\Sigma V^T$ is the singular value decomposition of\n$Z$ with singular values $(\\sigma_z)_{\\min\\{m,n\\}}\\leq\\cdots\\leq (\\sigma_z)_1$ and $X\\in\\C$\nwith singular values $(\\sigma_x)_{\\min\\{m,n\\}}\\leq\\cdots\\leq (\\sigma_x)_1$,\naccording to the von Neumann trace inequality \\cite{von1937some} we will have\n", "itemtype": "equation", "pos": 41412, "prevtext": "\nNotice that if $a = b$, then $\\C$ is a cube surface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Subsets of $\\reals^{m\\times n}$}\nRemember that the projection of a point $X\\in\\reals^{m \\times n}$ on a set\n$\\C\\subset\\reals^{m \\times n}$ is a point $Z\\in\\C$ such that\nthe Frobenius norm $\\|X-Z\\|_\\mathrm F$ is minimized.\nAs always, if there is more\nthan one point $Z$ that minimizes\n$\\|X-Z\\|_\\mathrm F$, we accept any of them.\n\n\\paragraph{Matrices with bounded singular values and orthogonal matrices}\nConsider the set of $m \\times n$ matrices whose singular values lie between\n$1$ and $\\alpha$\n", "index": 41, "text": "\n\\[\n\\C=\\{Z\\in \\reals^{m \\times n} \\mid I\\preceq Z^TZ \\preceq \\alpha^2 I\\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\C=\\{Z\\in\\reals^{m\\times n}\\mid I\\preceq Z^{T}Z\\preceq\\alpha^{2}I\\},\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\C</mtext></merror><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>Z</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow><mo>\u2223</mo><mrow><mi>I</mi><mo>\u2aaf</mo><mrow><msup><mi>Z</mi><mi>T</mi></msup><mo>\u2062</mo><mi>Z</mi></mrow><mo>\u2aaf</mo><mrow><msup><mi>\u03b1</mi><mn>2</mn></msup><mo>\u2062</mo><mi>I</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nHence\n", "itemtype": "equation", "pos": 41911, "prevtext": "\nwhere $\\alpha\\geq 1$, and $A \\preceq B$ means $B-A\\in\\symm_+^n$ .\nAny point $Z\\in\\C$ satisfies $\\|Z\\|_2\\leq \\alpha$.\n\nIf $Z=U\\Sigma V^T$ is the singular value decomposition of\n$Z$ with singular values $(\\sigma_z)_{\\min\\{m,n\\}}\\leq\\cdots\\leq (\\sigma_z)_1$ and $X\\in\\C$\nwith singular values $(\\sigma_x)_{\\min\\{m,n\\}}\\leq\\cdots\\leq (\\sigma_x)_1$,\naccording to the von Neumann trace inequality \\cite{von1937some} we will have\n", "index": 43, "text": "\n\\[\n\\Tr (Z^TX) \\leq \\sum_{i=1}^{\\min\\{m,n\\}} (\\sigma_z)_i(\\sigma_x)_i.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\Tr(Z^{T}X)\\leq\\sum_{i=1}^{\\min\\{m,n\\}}(\\sigma_{z})_{i}(\\sigma_{x})_{i}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Tr</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Z</mi><mi>T</mi></msup><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c3</mi><mi>z</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c3</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwith equality when $X=U\\diag(\\sigma_x)V^T$.\nThis inequality implies that $\\Pi(Z)=U\\tilde\\Sigma V^T$, where $\\tilde\\Sigma$ is a\ndiagonal matrix and $\\tilde\\Sigma_{ii}$ is the projection of\n$\\Sigma_{ii}$ on interval $[1,\\alpha]$.\nWhen $Z = 0$, the projection $\\Pi(Z)$ is any matrix.\n\nGiven $Z=U\\Sigma V^T\\in\\C$, we can have the following restriction \\cite{boyd2014mimo}\n", "itemtype": "equation", "pos": 41991, "prevtext": "\nHence\n", "index": 45, "text": "\\[\n\\|Z-X\\|_F^2 \\geq \\sum_{i=1}^{\\min\\{m,n\\}} \\left((\\sigma_z)_i - (\\sigma_x)_i\\right)^2,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\|Z-X\\|_{F}^{2}\\geq\\sum_{i=1}^{\\min\\{m,n\\}}\\left((\\sigma_{z})_{i}-(\\sigma_{x})%&#10;_{i}\\right)^{2},\" display=\"block\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>Z</mi><mo>-</mo><mi>X</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>\u2265</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><msup><mrow><mo>(</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c3</mi><mi>z</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><mo>-</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c3</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\n(Notice that $X\\in\\Crestrict(Z)$ satisfies $X^TX\\succeq I + (X-UV^T)^T(X-UV^T)\\succeq I$.)\n\nThere are several noteworthy special cases.\nWhen $\\alpha=1$ and $m=n$ we have the set of orthogonal matrices.\nIn this case, the restriction will be a singleton.\nWhen $n=1$, the set $\\C$ is equivalent to the annulus\n$\\{z \\in \\reals^m \\mid 1 \\leq \\|z\\|_2 \\leq \\alpha\\}$.\n\n\\paragraph{Matrices with bounded rank}\nLet $\\C = \\{ Z \\in \\reals^{m \\times n} \\mid \\Rank(Z) \\leq k,~ \\|Z\\|_2\\leq M \\}$.\nAny point $Z\\in\\C$ satisfies $\\|Z\\|_2\\leq M$ and\n$\\|Z\\|_*\\leq Mk$, where $\\|\\cdot\\|_*$ denotes\nthe trace norm.\nIf $Z = U\\Sigma V^T$ is the singular value decomposition of $Z$,\nwe will have $\\Pi(Z) = U \\tilde \\Sigma V^T$, where $\\tilde \\Sigma$ is\na diagonal matrix with $\\tilde \\Sigma_{ii} = \\min\\{\\Sigma_{ii},M\\}$ for\n$i=1,\\ldots k$, and $\\tilde \\Sigma_{ii} =0$ otherwise.\n\nGiven a point $Z\\in\\C$, we can write the singular value decomposition of\n$Z$ as $Z=U\\Sigma V^T$ with $U\\in\\reals^{m\\times k}$,\n$\\Sigma\\in\\reals^{r\\times r}$ and $V\\in\\reals^{n\\times k}$.\nA restriction of $\\C$ at $Z$ is\n", "itemtype": "equation", "pos": 42450, "prevtext": "\nwith equality when $X=U\\diag(\\sigma_x)V^T$.\nThis inequality implies that $\\Pi(Z)=U\\tilde\\Sigma V^T$, where $\\tilde\\Sigma$ is a\ndiagonal matrix and $\\tilde\\Sigma_{ii}$ is the projection of\n$\\Sigma_{ii}$ on interval $[1,\\alpha]$.\nWhen $Z = 0$, the projection $\\Pi(Z)$ is any matrix.\n\nGiven $Z=U\\Sigma V^T\\in\\C$, we can have the following restriction \\cite{boyd2014mimo}\n", "index": 47, "text": "\n\\[\n\\Crestrict(Z) = \\{X \\in \\reals^{m \\times n} \\mid \\|X\\|_2\\leq \\alpha,~ V^TX^TU+U^TXV\\succeq 2I\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(Z)=\\{X\\in\\reals^{m\\times n}\\mid\\|X\\|_{2}\\leq\\alpha,~{}V^{T}X^{T}U+U%&#10;^{T}XV\\succeq 2I\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>X</mi><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mrow><mi>m</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></mrow><mo>\u2223</mo><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03b1</mi></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mrow><msup><mi>V</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>X</mi><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi></mrow><mo>+</mo><mrow><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>V</mi></mrow></mrow><mo>\u2ab0</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>I</mi></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\n\n\\paragraph{Assignment and permutation matrices}\nThe set of \\emph{assignment matrices} are Boolean matrices\nwith exactly one non-zero\nelement in each column and at most one non-zero element in each row. (They\nrepresent an assignment of the columns to the rows.) In other words, the\nset of assignment matrices on $\\{0,1\\}^{m \\times n}$, where $m\\geq n$, satisfy\n", "itemtype": "equation", "pos": 43628, "prevtext": "\n(Notice that $X\\in\\Crestrict(Z)$ satisfies $X^TX\\succeq I + (X-UV^T)^T(X-UV^T)\\succeq I$.)\n\nThere are several noteworthy special cases.\nWhen $\\alpha=1$ and $m=n$ we have the set of orthogonal matrices.\nIn this case, the restriction will be a singleton.\nWhen $n=1$, the set $\\C$ is equivalent to the annulus\n$\\{z \\in \\reals^m \\mid 1 \\leq \\|z\\|_2 \\leq \\alpha\\}$.\n\n\\paragraph{Matrices with bounded rank}\nLet $\\C = \\{ Z \\in \\reals^{m \\times n} \\mid \\Rank(Z) \\leq k,~ \\|Z\\|_2\\leq M \\}$.\nAny point $Z\\in\\C$ satisfies $\\|Z\\|_2\\leq M$ and\n$\\|Z\\|_*\\leq Mk$, where $\\|\\cdot\\|_*$ denotes\nthe trace norm.\nIf $Z = U\\Sigma V^T$ is the singular value decomposition of $Z$,\nwe will have $\\Pi(Z) = U \\tilde \\Sigma V^T$, where $\\tilde \\Sigma$ is\na diagonal matrix with $\\tilde \\Sigma_{ii} = \\min\\{\\Sigma_{ii},M\\}$ for\n$i=1,\\ldots k$, and $\\tilde \\Sigma_{ii} =0$ otherwise.\n\nGiven a point $Z\\in\\C$, we can write the singular value decomposition of\n$Z$ as $Z=U\\Sigma V^T$ with $U\\in\\reals^{m\\times k}$,\n$\\Sigma\\in\\reals^{r\\times r}$ and $V\\in\\reals^{n\\times k}$.\nA restriction of $\\C$ at $Z$ is\n", "index": 49, "text": "\n\\[\n\\Crestrict(Z) = \\{U\\tilde\\Sigma V^T \\mid \\tilde\\Sigma\\in\\reals^{r\\times r}\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\Crestrict(Z)=\\{U\\tilde{\\Sigma}V^{T}\\mid\\tilde{\\Sigma}\\in\\reals^{r\\times r}\\}.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crestrict</mtext></merror><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>U</mi><mo>\u2062</mo><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>\u2223</mo><mrow><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a3</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2208</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\reals</mtext></merror><mrow><mi>r</mi><mo>\u00d7</mo><mi>r</mi></mrow></msup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nThese two sets of inequalities, along with $0\\leq Z_{ij}\\leq 1$ are the implied\nconvex inequalities.\nWhen $m=n$, this set becomes the set of permutation matrices, which we show\nby $\\mathcal P_n$.\n\nProjecting $Z\\in\\reals^{m\\times n}$ (with $m\\geq n$) onto the set of\nassignment matrices involves choosing an entry from each column of $Z$\nsuch that no two chosen entries are from the same row and the sum of\nchosen entries is maximized. Assuming that the entries of $Z$ are the weights\nof edges in a bipartite graph, the projection onto the set of assignment matrices will\nbe equivalent to finding a maximum-weight matching in a bipartite graph.\nThe Hungarian method \\cite{kuhn2005hungarian}\nis a well-know polynomial time algorithm\nto find the maximum weight matching, and hence also the projection onto assignment\nmatrices.\n\nThe neighbors of an assignment or permutation matrix $Z \\in\\reals^{m\\times n}$\nare the matrices generated by swapping two adjacent rows or columns of $Z$.\n\n\\paragraph{Hamiltonian cycles}\nA \\emph{Hamiltonian cycle} is a cycle in a graph that visits every node\nexactly once. Every Hamiltonian cycle in a complete graph can be represented\nby its adjacency matrix, for example\n", "itemtype": "equation", "pos": 44073, "prevtext": "\n\n\\paragraph{Assignment and permutation matrices}\nThe set of \\emph{assignment matrices} are Boolean matrices\nwith exactly one non-zero\nelement in each column and at most one non-zero element in each row. (They\nrepresent an assignment of the columns to the rows.) In other words, the\nset of assignment matrices on $\\{0,1\\}^{m \\times n}$, where $m\\geq n$, satisfy\n", "index": 51, "text": "\n\\[\n\\begin{array}{cc}\n\\sum_{j=1}^n Z_{ij} \\leq 1, &\\quad i=1,\\ldots,m \\\\\n\\sum_{i=1}^m Z_{ij} = 1, &\\quad j=1,\\ldots,n.\n\\end{array}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{cc}\\sum_{j=1}^{n}Z_{ij}\\leq 1,&amp;\\quad i=1,\\ldots,m\\\\&#10;\\sum_{i=1}^{m}Z_{ij}=1,&amp;\\quad j=1,\\ldots,n.\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>i</mi></mpadded><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>j</mi></mpadded><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>n</mi></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nrepresents a Hamiltonian cycle that visits nodes $(3,2,4,1)$ sequentially.\nLet $\\mathcal H_n$ be the set of $n\\times n$ matrices that represent a\nHamiltonian cycle.\n\nEvery point $Z\\in\\mathcal H_n$ satisfies $0\\leq Z_{ij} \\leq 1$ for\n$i,j = 1,\\ldots,n$, and $Z=Z^T$, $(1/2)Z\\ones = \\ones$, and\n", "itemtype": "equation", "pos": 45404, "prevtext": "\nThese two sets of inequalities, along with $0\\leq Z_{ij}\\leq 1$ are the implied\nconvex inequalities.\nWhen $m=n$, this set becomes the set of permutation matrices, which we show\nby $\\mathcal P_n$.\n\nProjecting $Z\\in\\reals^{m\\times n}$ (with $m\\geq n$) onto the set of\nassignment matrices involves choosing an entry from each column of $Z$\nsuch that no two chosen entries are from the same row and the sum of\nchosen entries is maximized. Assuming that the entries of $Z$ are the weights\nof edges in a bipartite graph, the projection onto the set of assignment matrices will\nbe equivalent to finding a maximum-weight matching in a bipartite graph.\nThe Hungarian method \\cite{kuhn2005hungarian}\nis a well-know polynomial time algorithm\nto find the maximum weight matching, and hence also the projection onto assignment\nmatrices.\n\nThe neighbors of an assignment or permutation matrix $Z \\in\\reals^{m\\times n}$\nare the matrices generated by swapping two adjacent rows or columns of $Z$.\n\n\\paragraph{Hamiltonian cycles}\nA \\emph{Hamiltonian cycle} is a cycle in a graph that visits every node\nexactly once. Every Hamiltonian cycle in a complete graph can be represented\nby its adjacency matrix, for example\n", "index": 53, "text": "\n\\[\n\\left[\\begin{array}{cccc}\n0 & 0 & 1 & 1       \\\\\n0 & 0 & 1 & 1       \\\\\n1 & 1 & 0 & 0       \\\\\n1 & 1 & 0 & 0\n\\end{array}\\right]\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\left[\\begin{array}[]{cccc}0&amp;0&amp;1&amp;1\\\\&#10;0&amp;0&amp;1&amp;1\\\\&#10;1&amp;1&amp;0&amp;0\\\\&#10;1&amp;1&amp;0&amp;0\\end{array}\\right]\" display=\"block\"><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr></mtable><mo>]</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\mathbf I$ denotes the identity matrix. In order to see why the last\ninequality holds, it's enough to notice that $2\\mathbf I - Z$ is the Laplacian\nof the cycle represented by $Z$\n\\cite{merris1994laplacian, anderson1985eigenvalues}.\nIt can be shown that the smallest eigenvalue\nof $2\\mathbf I - Z$ is zero (which corresponds to the eigenvector $\\ones$),\nand the second smallest eigenvalue of $2\\mathbf I - Z$ is\n$2 (1-\\cos \\frac{2\\pi}{n})$. Hence all eigenvalues of\n$2\\mathbf I - Z + 4\\frac{\\ones \\ones^T}{n}$ must be no smaller than\n$2 (1-\\cos \\frac{2\\pi}{n})$.\n\nWe are not aware of a polynomial time algorithm to find the projection of\na given real $n\\times n$ matrix onto $\\mathcal H_n$.\nWe can find an \\emph{approximate projection} of $Z$ by the following greedy\nalgorithm: construct a graph with $n$ vertices where the edge between\n$i$ and $j$ is weighted by $z_{ij}$. Start with the edge with largest weight and at\neach step, among all the edges that don't create a cycle, choose the edge with\nthe largest weight (except for the last step where a cycle is created).\n\nFor a matrix $Z\\in\\mathcal H_n$, the set of neighbors of $Z$ are matrices\nobtained after swapping two adjacent nodes, \\ie, matrices in form $P_{(i,j)}ZP_{(i,j)}^T$\nwhere $Z_{ij}=1$ and\n$P_{(i,j)}$ is a permutation matrix that transposes connected\nnodes $i$ and $j$ and keeps other nodes unchanged.\n\n\\subsection{Combinations of sets}\n\n\\paragraph{Cartesian product.}\nLet $\\C = \\C_1 \\times \\cdots \\times \\C_k \\subset \\reals^n$,\nwhere $\\C_1,\\ldots,\\C_k$ are compact sets with known projections (or approximate projections).\nA convex relaxation of $\\C$ is the Cartesian product\n$\\Crelax_1 \\times \\cdots \\times \\Crelax_k$,\nwhere $\\Crelax_i$ is the set described by the convex relaxation of $\\C_i$.\nThe projection of $z \\in \\reals^n$ onto $\\C$ is\n$\\left(\\Pi_1(z_1),\\ldots,\\Pi_k(z_k)\\right)$, where $\\Pi_i$ denotes the projection\nonto $\\C_i$ for $i=1,\\ldots,k$.\n\nA restriction of $\\C$ at a point\n$z = (z_1,z_2,\\ldots,z_k) \\in \\C$\nis the Cartesian product\n$\\Crestrict(z) = \\Crestrict_1(z_1) \\times \\cdots \\times \\Crestrict_k(z_k)$.\nThe neighbors of $z$ are all points\n$(z_1,\\ldots,z_{i-1},\\tilde z_i,z_{i+1},\\ldots, z_k)$ where\n$\\tilde z_i$ is a neighbor of $z_i$ in $\\C_i$.\n\n\n\\paragraph{Union.}\nLet $\\C = \\cup_{i=1}^k\\C_i$,\nwhere $\\C_1,\\ldots,\\C_k$ are compact sets with known projections\n(or approximate projections).\nA convex relaxation of $\\C$ is the constraints\n", "itemtype": "equation", "pos": 45831, "prevtext": "\nrepresents a Hamiltonian cycle that visits nodes $(3,2,4,1)$ sequentially.\nLet $\\mathcal H_n$ be the set of $n\\times n$ matrices that represent a\nHamiltonian cycle.\n\nEvery point $Z\\in\\mathcal H_n$ satisfies $0\\leq Z_{ij} \\leq 1$ for\n$i,j = 1,\\ldots,n$, and $Z=Z^T$, $(1/2)Z\\ones = \\ones$, and\n", "index": 55, "text": "\n\\[\n2\\mathbf I - Z + 4\\frac{\\ones \\ones^T}{n} \\geq 2 (1-\\cos \\frac{2\\pi}{n}) \\mathbf I,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"2\\mathbf{I}-Z+4\\frac{\\ones\\ones^{T}}{n}\\geq 2(1-\\cos\\frac{2\\pi}{n})\\mathbf{I},\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow><mo>-</mo><mi>Z</mi></mrow><mo>+</mo><mrow><mn>4</mn><mo>\u2062</mo><mfrac><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ones</mtext></merror><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ones</mtext></merror><mi>T</mi></msup></mrow><mi>n</mi></mfrac></mrow></mrow><mo>\u2265</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>cos</mi><mo>\u2061</mo><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow><mi>n</mi></mfrac></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $\\Crelax_i$ is the set described by the convex relaxation of $\\C_i$\nand $M_i > 0$ is the minimum value such that $\\|z_i\\|_\\infty \\leq M_i$\nholds for all $z_i \\in \\C_i$.\n\nWe can project $z \\in \\reals^n$ onto $\\C$ by projecting onto each set\nseparately and keeping the projection closest to $z$:\n", "itemtype": "equation", "pos": 48359, "prevtext": "\nwhere $\\mathbf I$ denotes the identity matrix. In order to see why the last\ninequality holds, it's enough to notice that $2\\mathbf I - Z$ is the Laplacian\nof the cycle represented by $Z$\n\\cite{merris1994laplacian, anderson1985eigenvalues}.\nIt can be shown that the smallest eigenvalue\nof $2\\mathbf I - Z$ is zero (which corresponds to the eigenvector $\\ones$),\nand the second smallest eigenvalue of $2\\mathbf I - Z$ is\n$2 (1-\\cos \\frac{2\\pi}{n})$. Hence all eigenvalues of\n$2\\mathbf I - Z + 4\\frac{\\ones \\ones^T}{n}$ must be no smaller than\n$2 (1-\\cos \\frac{2\\pi}{n})$.\n\nWe are not aware of a polynomial time algorithm to find the projection of\na given real $n\\times n$ matrix onto $\\mathcal H_n$.\nWe can find an \\emph{approximate projection} of $Z$ by the following greedy\nalgorithm: construct a graph with $n$ vertices where the edge between\n$i$ and $j$ is weighted by $z_{ij}$. Start with the edge with largest weight and at\neach step, among all the edges that don't create a cycle, choose the edge with\nthe largest weight (except for the last step where a cycle is created).\n\nFor a matrix $Z\\in\\mathcal H_n$, the set of neighbors of $Z$ are matrices\nobtained after swapping two adjacent nodes, \\ie, matrices in form $P_{(i,j)}ZP_{(i,j)}^T$\nwhere $Z_{ij}=1$ and\n$P_{(i,j)}$ is a permutation matrix that transposes connected\nnodes $i$ and $j$ and keeps other nodes unchanged.\n\n\\subsection{Combinations of sets}\n\n\\paragraph{Cartesian product.}\nLet $\\C = \\C_1 \\times \\cdots \\times \\C_k \\subset \\reals^n$,\nwhere $\\C_1,\\ldots,\\C_k$ are compact sets with known projections (or approximate projections).\nA convex relaxation of $\\C$ is the Cartesian product\n$\\Crelax_1 \\times \\cdots \\times \\Crelax_k$,\nwhere $\\Crelax_i$ is the set described by the convex relaxation of $\\C_i$.\nThe projection of $z \\in \\reals^n$ onto $\\C$ is\n$\\left(\\Pi_1(z_1),\\ldots,\\Pi_k(z_k)\\right)$, where $\\Pi_i$ denotes the projection\nonto $\\C_i$ for $i=1,\\ldots,k$.\n\nA restriction of $\\C$ at a point\n$z = (z_1,z_2,\\ldots,z_k) \\in \\C$\nis the Cartesian product\n$\\Crestrict(z) = \\Crestrict_1(z_1) \\times \\cdots \\times \\Crestrict_k(z_k)$.\nThe neighbors of $z$ are all points\n$(z_1,\\ldots,z_{i-1},\\tilde z_i,z_{i+1},\\ldots, z_k)$ where\n$\\tilde z_i$ is a neighbor of $z_i$ in $\\C_i$.\n\n\n\\paragraph{Union.}\nLet $\\C = \\cup_{i=1}^k\\C_i$,\nwhere $\\C_1,\\ldots,\\C_k$ are compact sets with known projections\n(or approximate projections).\nA convex relaxation of $\\C$ is the constraints\n", "index": 57, "text": "\n\\[\n\\begin{array}{cc}\nx_i \\in \\Crelax_i,& \\quad i = 1,\\ldots,k \\\\\ns_i \\in [0,1],& \\quad i = 1,\\ldots,k \\\\\nz = \\sum_{i=1}^k x_i& \\\\\n\\sum_{i=1}^k s_i = 1& \\\\\n\\|x_i\\|_\\infty \\leq M_i s_i,& \\quad i = 1,\\ldots,k,\n\\end{array}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{cc}x_{i}\\in\\Crelax_{i},&amp;\\quad i=1,\\ldots,k\\\\&#10;s_{i}\\in[0,1],&amp;\\quad i=1,\\ldots,k\\\\&#10;z=\\sum_{i=1}^{k}x_{i}&amp;\\\\&#10;\\sum_{i=1}^{k}s_{i}=1&amp;\\\\&#10;\\|x_{i}\\|_{\\infty}\\leq M_{i}s_{i},&amp;\\quad i=1,\\ldots,k,\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\Crelax</mtext></merror><mi>i</mi></msub></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>i</mi></mpadded><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>i</mi></mpadded><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>z</mi><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>s</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><msub><mrow><mo>\u2225</mo><msub><mi>x</mi><mi>i</mi></msub><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mrow><msub><mi>M</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>s</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>i</mi></mpadded><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>k</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nHere $\\Pi_i$ denotes the projection onto $\\C_i$.\n\nA restriction of $\\C$ at a point $z$ is $\\Crestrict_i(z)$ for any\n$\\C_i$ containing $z$.\nThe neighbors of $z$ are similarly the neighbors for any\n$\\C_i$ containing $z$.\n\n\\section{Implementation}\\label{s-implementation}\nWe have implemented the NCVX Python package for modeling\nproblems of the form~(\\ref{e-prob}) and applying the NC-ADMM heuristic,\nalong with the relax-round-polish and relax methods.\nThe NCVX package is an extension of CVXPY \\cite{cvxpy}.\nThe problem\nobjective and convex constraints are expressed using standard CVXPY\nsemantics. Nonconvex constraints are expressed implicitly by creating\na variable constrained to lie in one of the sets described in \\S\\ref{s-set}.\nFor example, the code snippet\n\\begin{verbatim}\nx = Boolean()\n\\end{verbatim}\ncreates a variable $x \\in \\reals$ with the\nimplicit nonconvex constraint $x \\in \\{0,1\\}$.\nThe convex relaxation, in this case $x \\in [0,1]$, is\nalso implicit in the variable\ndefinition.\nThe source code for NCVX is available at\n\\url{https://github.com/cvxgrp/ncvx}.\n\n\\subsection{Variable constructors}\\label{create-var-sec}\n\nThe NCVX package provides the following functions for creating\nvariables with implicit nonconvex constraints,\nalong with many others not listed:\n\\begin{itemize}\n\\item \\verb+Boolean(n)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraint\n$x \\in \\{0,1\\}^{n}$.\n\\item \\verb+Integer(n, M)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints\n$x \\in \\integers^{n}$ and $\\|x\\|_\\infty \\leq \\lfloor M \\rfloor$.\n\\item \\verb+Card(n, k, M)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints\nthat at most $k$ entries are nonzero and $\\|x\\|_\\infty \\leq M$.\n\\item \\verb+Choose(n, k)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints that\n$x \\in \\{0,1\\}^{n}$ and has exactly $k$ nonzero entries.\n\\item \\verb+Rank(m, n, k, M)+ creates a variable\n$X \\in \\reals^{m \\times n}$ with the implicit constraints\n$\\Rank(X) \\leq k$ and $\\|X\\|_2 \\leq M$.\n\\item \\verb+Assign(m, n)+ creates a variable\n$X \\in \\reals^{m \\times n}$ with the implicit constraint\nthat $X$ is an assignment matrix.\n\\item \\verb+Permute(n)+ creates a variable\n$X \\in \\reals^{n \\times n}$ with the implicit constraint\nthat $X$ is a permutation matrix.\n\\item \\verb+Cycle(n)+ creates a variable\n$X \\in \\reals^{n \\times n}$ with the implicit constraint\nthat $X$ is the adjacency matrix of a Hamiltonian cycle.\n\\item \\verb+Annulus(n,r,R)+ creates a variable $x \\in \\reals^n$ with\nthe implicit constraint that $r \\leq \\|x\\|_2 \\leq R$.\n\\item \\verb+Sphere(n, r)+ creates a variable $x \\in \\reals^n$ with\nthe implicit constraint that $\\|x\\|_2 = r$.\n\\end{itemize}\n\n\\subsection{Variable methods}\nAdditionally, each variable created by the functions in \\S\\ref{create-var-sec}\nsupports the following methods:\n\\begin{itemize}\n\\item \\verb+variable.relax()+ returns a list of convex constraints\nthat represent a convex relaxation of the nonconvex set $\\C$,\nto which the variable belongs.\n\\item \\verb+variable.project(z)+ returns the Euclidean (or approximate)\nprojection of $z$ onto the nonconvex set $\\C$,\nto which the variable belongs.\n\\item \\verb+variable.restrict(z)+ returns a list of convex constraints\ndescribing the convex restriction $\\Crestrict(z)$ at $z$ of the\nnonconvex set $\\C$, to which the variable belongs.\n\\item \\verb+variable.neighbors(z)+ returns a list of neighbors $\\Cneighbor(z)$\nof $z$ contained in the\nnonconvex set $\\C$, to which the variable belongs.\n\\end{itemize}\nUsers can add support for additional nonconvex sets by implementing\nfunctions that return variables with these four methods.\n\n\\subsection{Constructing and solving problems}\nTo construct a problem of the form (\\ref{e-prob}), the user creates\nvariables $z_1,\\ldots,z_k$ with the implicit constraints\n$z_1 \\in \\C_1,\\ldots,z_k \\in \\C_k$, where $\\C_1,\\ldots,\\C_k$ are\nnonconvex sets,\nusing the functions described in \\S\\ref{create-var-sec}.\nThe variable $z$ in problem (\\ref{e-prob}) corresponds to the vector\n$(z_1,\\ldots,z_k)$.\nThe components of the variable $x$, the objective,\nand the constraints are constructed using standard CVXPY syntax.\n\nOnce the user has constructed a problem object,\nthey can apply the following solve methods:\n\\begin{itemize}\n\\item \\verb+problem.solve(method=\"relax\")+ solves the convex relaxation\nof the problem.\n\\item \\verb+problem.solve(method=\"relax-round-polish\")+\napplies the relax-round-polish heuristic.\nAdditional arguments can be used to specify the parameters $K$ and $\\lambda$.\nBy default the parameter values are $K=1$ and $\\lambda = 10^4$.\nWhen $K > 1$, the first sample $w_1 \\in \\reals^q$ is always 0.\nSubsequent samples are drawn i.i.d.\\ from\n$N(0,\\sigma^2I)$, where $\\sigma$ is another parameter the user can set.\n\\item \\verb+problem.solve(method=\"nc-admm\")+\napplies the NC-ADMM heuristic.\nAdditional arguments can be used to specify the number of starting points,\nthe number of iterations the algorithm is run from each starting point,\nand the values of the parameters $\\rho$, $\\sigma$, and $\\lambda$.\nBy default the algorithm is run from 5 starting points for 50 iterations,\nthe value of $\\rho$ is drawn uniformly from $[0,1]$, and the other\nparameter values are $\\sigma = 1$ and $\\lambda = 10^4$.\nThe first starting point is always\n$z^0 = 0$ and subsequent starting points are drawn i.i.d.\\ from\n$\\mathcal N(0,\\sigma^2I)$.\n\\end{itemize}\nThe relax-round-polish and NC-ADMM methods\nrecord the best point found $(x_\\mathrm{best},z_\\mathrm{best})$\naccording to the merit function.\nThe methods return the objective value $f_0(x_\\mathrm{best},z_\\mathrm{best})$ and\nthe residual $r(x_\\mathrm{best},z_\\mathrm{best})$,\nand set the \\verb+value+\nfield of each variable to the appropriate segment\nof $x_\\mathrm{best}$ and $z_\\mathrm{best}$.\n\nFor example, consider the \\emph{regressor selection} problem, which we will\ndiscuss in \\S\\ref{regressor_prob}. This problem can be formulated as\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|Ax-b\\|_2^2 \\\\\n\\mbox{subject to} & \\|x\\|_\\infty \\leq M \\\\\n& \\card(x)\\leq k,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variable $x\\in\\reals^n$ and problem data $A\\in\\reals^{m\\times n}$,\n$b\\in\\reals^m$, $M > 0$, and $k \\in \\integers_+$. The following code attempts to approximately solve\nthis problem using our heuristic.\n\n\\begin{verbatim}\nx = Card(n,k,M)\nprob = Problem(Minimize(sum_squares(A*x-b)))\nobjective, residual = prob.solve(method=\"nc-admm\")\n\\end{verbatim}\nThe first line constructs a variable\n$x \\in \\reals^{n}$ with the implicit constraints\nthat at most $k$ entries are nonzero, $\\|x\\|_\\infty \\leq M$,\nand $\\|x\\|_1 \\leq kM$.\nThe second line\ncreates a minimization problem with objective $\\|Ax-b\\|_2^2$ and\nno constraints.\nThe last line applies the NC-ADMM heuristic to the problem\nand returns the objective value and residual of the best point found.\n\n\\section{Examples}\\label{s-examples}\nIn this section we apply the NC-ADMM heuristic to a wide variety of hard\nproblems, \\ie, that generally cannot be solved in polynomial time.\nExtensive research has been done on specialized algorithms\nfor each of the problems discussed in this section.\nOur intention is not to seek better performance than these specialized\nalgorithms, but rather to show that our\ngeneral purpose heuristic can yield decent results with minimal tuning.\nUnless otherwise specified, the algorithm parameters are the defaults\ndescribed in \\S\\ref{s-implementation}. Whenever possible, we compare our heuristic to\nGUROBI \\cite{gurobi}, a commercial global optimization solver.\nSince our implementation of\nNC-ADMM supports minimal parallelization, we compare the number of\nconvex subproblems solved (and not the solve time).\n\n\\subsection{Regressor selection}\\label{regressor_prob}\nWe consider the problem of approximating\na vector $b$ with a linear combination of at most $k$ columns of $A$\nwith bounded coefficients.\nThis problem can be formulated as\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|Ax-b\\|_2^2 \\\\\n\\mbox{subject to} & \\card(x)\\leq k, \\quad \\|x\\|_\\infty \\leq M,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variable $x\\in\\reals^n$ and problem data $A\\in\\reals^{m\\times n}$,\n$b\\in\\reals^m$, $k \\in \\integers_+$, and $M > 0$.\nLasso (least absolute shrinkage and selection operator) is\na well-known heuristic for solving this problem by adding $\\ell_1$\nregularization and minimizing $ \\|Ax-b\\|_2^2 +\\lambda\\|x\\|_1$.\nThe value of $\\lambda$ is chosen as the smallest value possible such\nthat $\\card(x) \\leq k$. (See\n\\cite[\\S 3.4]{friedman2001elements} and\n\\cite[\\S 6.3]{boyd2004convex}.)\n\n\\paragraph{Problem instances.}\nWe generated the matrix $A \\in \\reals^{m \\times 2m}$\nwith i.i.d.\\ $\\mathcal N(0,1)$ entries, and\nchose $b=A\\hat x + v$, where $\\hat x$ was drawn uniformly at random\nfrom the set of vectors satisfying\n$\\card(\\hat x) \\leq \\lfloor m/5 \\rfloor$\nand $\\|x\\|_\\infty \\leq 1$, and\n$v \\in \\reals^m$ was a noise vector drawn from\n$\\mathcal{N}(0,\\sigma^2 I)$.\nWe set $\\sigma^2 = \\|A \\hat{x}\\|^2/(400m)$ so that the signal-to-noise\nratio was near 20.\n\n\\paragraph{Results.}\nFor each value of $m$, we generated $40$ instances of the problem\nas described in the previous paragraph.\nFigure \\ref{regressor_results} compares the average sum of squares error\nfor the $x^*$ values found by the Lasso heuristic, relax-round-polish,\nand NC-ADMM. For Lasso, we solved the problem for 100 values of $\\lambda$\nand then solved the polishing problem after fixing the sparsity pattern suggested by Lasso.\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{regressor_results}\n\\end{center}\n\\caption{The average error of solutions found by Lasso, relax-round-polish,\nand NC-ADMM for 40 instances of the regressor selection problem.\n}\\label{regressor_results}\n\\end{figure}\n\n\\subsection{3-satisfiability}\nGiven Boolean variables $x_1,\\cdots, x_n$, a \\emph{literal} is either a\nvariable or the negation of a variable, for example $x_1$ and $\\neg x_2$.\nA \\emph{clause} is disjunction of literals (or a single literal), for\nexample $(\\neg x_1 \\lor x_2 \\lor \\neg x_3)$. Finally a formula is\nin conjunctive normal form (CNF) if it is a conduction of clauses (or a single\nclause), for example $(\\neg x_1 \\lor x_2 \\lor \\neg x_3) \\land(x_1 \\lor \\neg x_2)$.\nDetermining the satisfiability of a formula in conjunctive normal form where each\nclause is limited to at most three literals is called \\emph{3-satisfiability} or\nsimply the \\emph{3-SAT} problem. It is known\nthat 3-SAT is NP-complete, hence we do not expect to be able to solve a 3-SAT\nin general using our heuristic.\n\n\n\n\n\n\nA 3-SAT problem\ncan be formulated as the following\n\\BEQ\n\\label{3satlp}\n\\begin{array}{ll}\n\\mbox{minimize} &0\\\\\n\\mbox{subject to} & Az \\leq b,\\\\\n& z \\in \\{0,1\\}^n,\\\\\n\\end{array}\n\\EEQ\nwhere entries of $A$ are given by\n", "itemtype": "equation", "pos": 48881, "prevtext": "\nwhere $\\Crelax_i$ is the set described by the convex relaxation of $\\C_i$\nand $M_i > 0$ is the minimum value such that $\\|z_i\\|_\\infty \\leq M_i$\nholds for all $z_i \\in \\C_i$.\n\nWe can project $z \\in \\reals^n$ onto $\\C$ by projecting onto each set\nseparately and keeping the projection closest to $z$:\n", "index": 59, "text": "\n\\[\n\\Pi(z) = \\argmin_{x\\in\\{\\Pi_1(z),\\cdots,\\Pi_k(z)\\}}\\|z - x\\|_2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\Pi(z)=\\argmin_{x\\in\\{\\Pi_{1}(z),\\cdots,\\Pi_{k}(z)\\}}\\|z-x\\|_{2}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\argmin</mtext></merror><mrow><mi>x</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>,</mo><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>z</mi><mo>-</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nand the entries of $b$ are given by\n", "itemtype": "equation", "pos": 59725, "prevtext": "\nHere $\\Pi_i$ denotes the projection onto $\\C_i$.\n\nA restriction of $\\C$ at a point $z$ is $\\Crestrict_i(z)$ for any\n$\\C_i$ containing $z$.\nThe neighbors of $z$ are similarly the neighbors for any\n$\\C_i$ containing $z$.\n\n\\section{Implementation}\\label{s-implementation}\nWe have implemented the NCVX Python package for modeling\nproblems of the form~(\\ref{e-prob}) and applying the NC-ADMM heuristic,\nalong with the relax-round-polish and relax methods.\nThe NCVX package is an extension of CVXPY \\cite{cvxpy}.\nThe problem\nobjective and convex constraints are expressed using standard CVXPY\nsemantics. Nonconvex constraints are expressed implicitly by creating\na variable constrained to lie in one of the sets described in \\S\\ref{s-set}.\nFor example, the code snippet\n\\begin{verbatim}\nx = Boolean()\n\\end{verbatim}\ncreates a variable $x \\in \\reals$ with the\nimplicit nonconvex constraint $x \\in \\{0,1\\}$.\nThe convex relaxation, in this case $x \\in [0,1]$, is\nalso implicit in the variable\ndefinition.\nThe source code for NCVX is available at\n\\url{https://github.com/cvxgrp/ncvx}.\n\n\\subsection{Variable constructors}\\label{create-var-sec}\n\nThe NCVX package provides the following functions for creating\nvariables with implicit nonconvex constraints,\nalong with many others not listed:\n\\begin{itemize}\n\\item \\verb+Boolean(n)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraint\n$x \\in \\{0,1\\}^{n}$.\n\\item \\verb+Integer(n, M)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints\n$x \\in \\integers^{n}$ and $\\|x\\|_\\infty \\leq \\lfloor M \\rfloor$.\n\\item \\verb+Card(n, k, M)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints\nthat at most $k$ entries are nonzero and $\\|x\\|_\\infty \\leq M$.\n\\item \\verb+Choose(n, k)+ creates a variable\n$x \\in \\reals^{n}$ with the implicit constraints that\n$x \\in \\{0,1\\}^{n}$ and has exactly $k$ nonzero entries.\n\\item \\verb+Rank(m, n, k, M)+ creates a variable\n$X \\in \\reals^{m \\times n}$ with the implicit constraints\n$\\Rank(X) \\leq k$ and $\\|X\\|_2 \\leq M$.\n\\item \\verb+Assign(m, n)+ creates a variable\n$X \\in \\reals^{m \\times n}$ with the implicit constraint\nthat $X$ is an assignment matrix.\n\\item \\verb+Permute(n)+ creates a variable\n$X \\in \\reals^{n \\times n}$ with the implicit constraint\nthat $X$ is a permutation matrix.\n\\item \\verb+Cycle(n)+ creates a variable\n$X \\in \\reals^{n \\times n}$ with the implicit constraint\nthat $X$ is the adjacency matrix of a Hamiltonian cycle.\n\\item \\verb+Annulus(n,r,R)+ creates a variable $x \\in \\reals^n$ with\nthe implicit constraint that $r \\leq \\|x\\|_2 \\leq R$.\n\\item \\verb+Sphere(n, r)+ creates a variable $x \\in \\reals^n$ with\nthe implicit constraint that $\\|x\\|_2 = r$.\n\\end{itemize}\n\n\\subsection{Variable methods}\nAdditionally, each variable created by the functions in \\S\\ref{create-var-sec}\nsupports the following methods:\n\\begin{itemize}\n\\item \\verb+variable.relax()+ returns a list of convex constraints\nthat represent a convex relaxation of the nonconvex set $\\C$,\nto which the variable belongs.\n\\item \\verb+variable.project(z)+ returns the Euclidean (or approximate)\nprojection of $z$ onto the nonconvex set $\\C$,\nto which the variable belongs.\n\\item \\verb+variable.restrict(z)+ returns a list of convex constraints\ndescribing the convex restriction $\\Crestrict(z)$ at $z$ of the\nnonconvex set $\\C$, to which the variable belongs.\n\\item \\verb+variable.neighbors(z)+ returns a list of neighbors $\\Cneighbor(z)$\nof $z$ contained in the\nnonconvex set $\\C$, to which the variable belongs.\n\\end{itemize}\nUsers can add support for additional nonconvex sets by implementing\nfunctions that return variables with these four methods.\n\n\\subsection{Constructing and solving problems}\nTo construct a problem of the form (\\ref{e-prob}), the user creates\nvariables $z_1,\\ldots,z_k$ with the implicit constraints\n$z_1 \\in \\C_1,\\ldots,z_k \\in \\C_k$, where $\\C_1,\\ldots,\\C_k$ are\nnonconvex sets,\nusing the functions described in \\S\\ref{create-var-sec}.\nThe variable $z$ in problem (\\ref{e-prob}) corresponds to the vector\n$(z_1,\\ldots,z_k)$.\nThe components of the variable $x$, the objective,\nand the constraints are constructed using standard CVXPY syntax.\n\nOnce the user has constructed a problem object,\nthey can apply the following solve methods:\n\\begin{itemize}\n\\item \\verb+problem.solve(method=\"relax\")+ solves the convex relaxation\nof the problem.\n\\item \\verb+problem.solve(method=\"relax-round-polish\")+\napplies the relax-round-polish heuristic.\nAdditional arguments can be used to specify the parameters $K$ and $\\lambda$.\nBy default the parameter values are $K=1$ and $\\lambda = 10^4$.\nWhen $K > 1$, the first sample $w_1 \\in \\reals^q$ is always 0.\nSubsequent samples are drawn i.i.d.\\ from\n$N(0,\\sigma^2I)$, where $\\sigma$ is another parameter the user can set.\n\\item \\verb+problem.solve(method=\"nc-admm\")+\napplies the NC-ADMM heuristic.\nAdditional arguments can be used to specify the number of starting points,\nthe number of iterations the algorithm is run from each starting point,\nand the values of the parameters $\\rho$, $\\sigma$, and $\\lambda$.\nBy default the algorithm is run from 5 starting points for 50 iterations,\nthe value of $\\rho$ is drawn uniformly from $[0,1]$, and the other\nparameter values are $\\sigma = 1$ and $\\lambda = 10^4$.\nThe first starting point is always\n$z^0 = 0$ and subsequent starting points are drawn i.i.d.\\ from\n$\\mathcal N(0,\\sigma^2I)$.\n\\end{itemize}\nThe relax-round-polish and NC-ADMM methods\nrecord the best point found $(x_\\mathrm{best},z_\\mathrm{best})$\naccording to the merit function.\nThe methods return the objective value $f_0(x_\\mathrm{best},z_\\mathrm{best})$ and\nthe residual $r(x_\\mathrm{best},z_\\mathrm{best})$,\nand set the \\verb+value+\nfield of each variable to the appropriate segment\nof $x_\\mathrm{best}$ and $z_\\mathrm{best}$.\n\nFor example, consider the \\emph{regressor selection} problem, which we will\ndiscuss in \\S\\ref{regressor_prob}. This problem can be formulated as\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|Ax-b\\|_2^2 \\\\\n\\mbox{subject to} & \\|x\\|_\\infty \\leq M \\\\\n& \\card(x)\\leq k,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variable $x\\in\\reals^n$ and problem data $A\\in\\reals^{m\\times n}$,\n$b\\in\\reals^m$, $M > 0$, and $k \\in \\integers_+$. The following code attempts to approximately solve\nthis problem using our heuristic.\n\n\\begin{verbatim}\nx = Card(n,k,M)\nprob = Problem(Minimize(sum_squares(A*x-b)))\nobjective, residual = prob.solve(method=\"nc-admm\")\n\\end{verbatim}\nThe first line constructs a variable\n$x \\in \\reals^{n}$ with the implicit constraints\nthat at most $k$ entries are nonzero, $\\|x\\|_\\infty \\leq M$,\nand $\\|x\\|_1 \\leq kM$.\nThe second line\ncreates a minimization problem with objective $\\|Ax-b\\|_2^2$ and\nno constraints.\nThe last line applies the NC-ADMM heuristic to the problem\nand returns the objective value and residual of the best point found.\n\n\\section{Examples}\\label{s-examples}\nIn this section we apply the NC-ADMM heuristic to a wide variety of hard\nproblems, \\ie, that generally cannot be solved in polynomial time.\nExtensive research has been done on specialized algorithms\nfor each of the problems discussed in this section.\nOur intention is not to seek better performance than these specialized\nalgorithms, but rather to show that our\ngeneral purpose heuristic can yield decent results with minimal tuning.\nUnless otherwise specified, the algorithm parameters are the defaults\ndescribed in \\S\\ref{s-implementation}. Whenever possible, we compare our heuristic to\nGUROBI \\cite{gurobi}, a commercial global optimization solver.\nSince our implementation of\nNC-ADMM supports minimal parallelization, we compare the number of\nconvex subproblems solved (and not the solve time).\n\n\\subsection{Regressor selection}\\label{regressor_prob}\nWe consider the problem of approximating\na vector $b$ with a linear combination of at most $k$ columns of $A$\nwith bounded coefficients.\nThis problem can be formulated as\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & \\|Ax-b\\|_2^2 \\\\\n\\mbox{subject to} & \\card(x)\\leq k, \\quad \\|x\\|_\\infty \\leq M,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variable $x\\in\\reals^n$ and problem data $A\\in\\reals^{m\\times n}$,\n$b\\in\\reals^m$, $k \\in \\integers_+$, and $M > 0$.\nLasso (least absolute shrinkage and selection operator) is\na well-known heuristic for solving this problem by adding $\\ell_1$\nregularization and minimizing $ \\|Ax-b\\|_2^2 +\\lambda\\|x\\|_1$.\nThe value of $\\lambda$ is chosen as the smallest value possible such\nthat $\\card(x) \\leq k$. (See\n\\cite[\\S 3.4]{friedman2001elements} and\n\\cite[\\S 6.3]{boyd2004convex}.)\n\n\\paragraph{Problem instances.}\nWe generated the matrix $A \\in \\reals^{m \\times 2m}$\nwith i.i.d.\\ $\\mathcal N(0,1)$ entries, and\nchose $b=A\\hat x + v$, where $\\hat x$ was drawn uniformly at random\nfrom the set of vectors satisfying\n$\\card(\\hat x) \\leq \\lfloor m/5 \\rfloor$\nand $\\|x\\|_\\infty \\leq 1$, and\n$v \\in \\reals^m$ was a noise vector drawn from\n$\\mathcal{N}(0,\\sigma^2 I)$.\nWe set $\\sigma^2 = \\|A \\hat{x}\\|^2/(400m)$ so that the signal-to-noise\nratio was near 20.\n\n\\paragraph{Results.}\nFor each value of $m$, we generated $40$ instances of the problem\nas described in the previous paragraph.\nFigure \\ref{regressor_results} compares the average sum of squares error\nfor the $x^*$ values found by the Lasso heuristic, relax-round-polish,\nand NC-ADMM. For Lasso, we solved the problem for 100 values of $\\lambda$\nand then solved the polishing problem after fixing the sparsity pattern suggested by Lasso.\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{regressor_results}\n\\end{center}\n\\caption{The average error of solutions found by Lasso, relax-round-polish,\nand NC-ADMM for 40 instances of the regressor selection problem.\n}\\label{regressor_results}\n\\end{figure}\n\n\\subsection{3-satisfiability}\nGiven Boolean variables $x_1,\\cdots, x_n$, a \\emph{literal} is either a\nvariable or the negation of a variable, for example $x_1$ and $\\neg x_2$.\nA \\emph{clause} is disjunction of literals (or a single literal), for\nexample $(\\neg x_1 \\lor x_2 \\lor \\neg x_3)$. Finally a formula is\nin conjunctive normal form (CNF) if it is a conduction of clauses (or a single\nclause), for example $(\\neg x_1 \\lor x_2 \\lor \\neg x_3) \\land(x_1 \\lor \\neg x_2)$.\nDetermining the satisfiability of a formula in conjunctive normal form where each\nclause is limited to at most three literals is called \\emph{3-satisfiability} or\nsimply the \\emph{3-SAT} problem. It is known\nthat 3-SAT is NP-complete, hence we do not expect to be able to solve a 3-SAT\nin general using our heuristic.\n\n\n\n\n\n\nA 3-SAT problem\ncan be formulated as the following\n\\BEQ\n\\label{3satlp}\n\\begin{array}{ll}\n\\mbox{minimize} &0\\\\\n\\mbox{subject to} & Az \\leq b,\\\\\n& z \\in \\{0,1\\}^n,\\\\\n\\end{array}\n\\EEQ\nwhere entries of $A$ are given by\n", "index": 61, "text": "\n\\[\na_{ij} =\n\\begin{cases}\n-1 & \\mbox{if clause $i$ contains $x_j$}\\\\\n1 & \\mbox{if clause $i$ contains $\\neg x_j$}\\\\\n0 & \\mbox{otherwise},\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"a_{ij}=\\begin{cases}-1&amp;\\mbox{if clause $i$ contains $x_{j}$}\\\\&#10;1&amp;\\mbox{if clause $i$ contains $\\neg x_{j}$}\\\\&#10;0&amp;\\mbox{otherwise},\\end{cases}\" display=\"block\"><mrow><msub><mi>a</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if clause\u00a0</mtext><mi>i</mi><mtext>\u00a0contains\u00a0</mtext><msub><mi>x</mi><mi>j</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if clause\u00a0</mtext><mi>i</mi><mtext>\u00a0contains\u00a0</mtext><mrow><mi mathvariant=\"normal\">\u00ac</mi><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\n\n\n\\paragraph{Problem instances.}\nWe generated 3-SAT problems with varying numbers of clauses and variables\nrandomly as in \\cite{mitchell1992hard,lipp2014variations}.\nAs discussed in \\cite{crawford1996experimental},\nthere is a threshold around $4.25$ clauses\nper variable when problems transition from being feasible to being infeasible.\nProblems near this threshold are generally found\nto be hard satisfiability problems.\nWe generated 10 instances for each choice of number of clauses and variables,\nverifying that each instance is feasible using GUROBI \\cite{gurobi}.\n\n\\paragraph{Results.}\nWe ran NC-ADMM heuristic on each instance, with 10 restarts,\nand 100 iterations, and we chose the step size $\\rho=10$.\nFigure~\\ref{3sat_results} shows the fraction of instances solved correctly\nwith NC-ADMM. We see that using this heuristic, satisfying assignments\ncan be found consistently for up to 3.2\nconstraints per variable, at which point success starts to decrease.\nProblems in the gray region in figure~\\ref{3sat_results} were not\ntested since they are infeasible with high probability.\nWe also tried the relax-round-polish heuristic, but it often failed to\nsolve problems with more than $50$ clauses.\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{three_sat}\n\\end{center}\n\\caption{Fraction of runs for which a satisfying assignment to\nrandom 3-SAT problems were found for problems of varying sizes.\nThe problems in the gray region were not tested.}\n\\label{3sat_results}\n\\end{figure}\n\n\\begin{incomplete}\n\\subsection{Graph coloring}\nGiven a graph $G$, the smallest number of colors needed to color the vertices of\n$G$ such that no two adjacent vertices have the same color is called the\n\\emph{chromatic} number of $G$ and is often denoted by $\\chi(G)$. It is easy to\nshow that $\\omega(G)\\leq\\chi(G)\\leq \\Delta(G) + 1$,\nwhere $\\omega(G)$ is the size of a maximum clique in $G$ and\n$\\Delta(G)$ is the maximum degree in $G$.\nFinding the chromatic number of a graph is known to be NP-hard.\nFor a given graph $G$ we can construct a \\emph{coloring matrix}\n$Z\\in\\{0,1\\}^{n\\times \\Delta(G) + 1}$,\nwhere $Z_{ij}=1$ means that vertex $i$ is colored with color $j$.\nEach vertex must be colored with exactly one color, so there should be\nexactly one nonzero entry in every row. In other words, $Z\\ones = \\ones$.\nFor any adjacent vertices $i$ and $j$,\nwe must have $Z_{i,:}+Z_{j,:}\\leq\\ones$,\nwhere $Z_{i,:}$ denotes the $i$th row of $Z$.\nWe want to minimize the number of colors used,\nwhich can be written as $\\sum_j \\| Z_{:,j}\\|_\\infty$, since\n$\\| Z_{:,j}\\|_\\infty$ is $0$ if color $j$ is not used, and $1$ if it is.\nIf $Z^\\star$ is an optimal solution, permuting the columns of $Z^\\star$\ngives additional optimal solutions.\nTo break this symmetry, we add the penalty term\n", "itemtype": "equation", "pos": 59914, "prevtext": "\nand the entries of $b$ are given by\n", "index": 63, "text": "\n\\[\nb_{i} = (\\mbox{number of negated literals in clause $i$}) -1.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"b_{i}=(\\mbox{number of negated literals in clause $i$})-1.\" display=\"block\"><mrow><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>number of negated literals in clause\u00a0</mtext><mi>i</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mn>1</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nwhere $h \\in \\reals^n$ is an arbitrary vector.\nAdding the penalty term does not alter the problem, since there\nis always a permutation of $Z^\\star$ for which $\\tau(Z) = 0$.\nThe graph coloring problem can thus be formulated as\n\\BEQ\n\\label{graphcolor}\n\\begin{array}{ll}\n\\mbox{minimize} &\\| Z_{:,j}\\|_\\infty + \\tau(Z)\\\\\n\\mbox{subject to} & Z_{i,:}+Z_{j,:}\\leq \\ones \\quad \\mbox{$i,j$ adjacent}\\\\\n& Z_{i,:} \\in \\mathcal{C}, \\quad i=1,\\ldots,n,\n\\end{array}\n\\EEQ\nwhere $\\C = \\{ x\\in \\{0,1\\}^n \\mid \\card(x) = 1\\}$,\nor \\verb+Choose(n,1)+ in CVXPY.\n\n\n\n\n\n\n\n\n\n\\paragraph{Problem instances.}\nWe generated random graphs drawn uniformly from the set of graphs\nwith $n$ vertices and $\\lfloor n(n-1)/2k \\rfloor$ edges,\nfor a range of $n$ and $k$.\nThe parameter $k$ is the ratio of edges in a complete graph to\nedges in the generated graph.\nWe generated 10 graphs for each choice of $n$ and $k$.\nWe found the chromatic number of each graph by solving problem\n(\\ref{graphcolor}) using GUROBI.\nThe vector $h \\in \\reals^n$ for each problem instance was chosen\nuniformly at random on the $n$-sphere.\n\n\n\\paragraph{Results.}\nFigure \\ref{graph_coloring_results} shows the average difference between\nthe result found by NC-ADMM and the true chromatic number.\nNC-ADMM was run from 5 random starting points for 50 iterations.\nThe value of $\\rho$ for each starting point was drawn uniformly at\nrandom from $[0,1]$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{graph_coloring_results}\n\\end{center}\n\\caption{Graph coloring results.\n}\\label{graph_coloring_results}\n\\end{figure}\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Assignment (not final yet)}\nIn the most general form of \\emph{assignment problem}, there are $n$ tasks and\n$m$ processors. Processor $j$ can perform task $i$ with cost $W_{ij}$. The problem\nis assigning tasks to processors so that the overall cost is minimized assuming that\neach processor can be assigned to at most one task. This problem is equivalent to\nfinding a minimum weight matching of a bipartite graph\n\nThe goal is to find an assignment matrix $X\\in\\{0,1\\}^{m\\times n}$ such that\n$\\Tr(X^TW)$ is minimized. So the problem can be formulated as\n\\BEQ\n\\label{assignment}\n\\begin{array}{ll}\n\\mbox{minimize} & \\Tr(Z^TW)\\\\\n\\mbox{subject to} & Z \\quad \\mbox{assignment matrix},\\\\\n\\end{array}\n\\EEQ\n\\end{incomplete}\n\n\\subsection{Circle packing}\nIn \\emph{circle packing problem} we are interested in finding the smallest square\nin which we can place $n$ non-overlapping circles with radii $r_1,\\ldots,r_n$\n\\cite{goldberg1970packing}.\nThis problem has been studied extensively\n\\cite{stephenson2005introduction, castillo2008solving, collins2003circle}\nand a database of densest known packings for different numbers of circles can be found\nin \\cite{Specht2013}.\nThe problem can be formulated as\n\\BEQ\n\\label{packing}\n\\begin{array}{ll}\n\\mbox{minimize} & l\\\\\n\\mbox{subject to} & r_i\\ones \\leq x_i \\leq (l-r_i) \\ones, \\quad i = 1,\\ldots,n\\\\\n& x_i-x_j = z_{ij},\\quad i= 1,\\ldots , n-1,\\quad j = i+1,\\ldots, n \\\\\n& 2\\sum_{k=1}^n r_i \\geq \\|z_{ij}\\|_2 \\geq r_i + r_j, \\quad i= 1,\\ldots , n -1,\\quad j = i+1,\\ldots, n,\n\\end{array}\n\\EEQ\nwhere $x_1,\\ldots,x_n \\in \\reals^2$ are variables representing the circle centers\nand $z_{12},z_{13},\\ldots,z_{n-1,n} \\in \\reals^2$ are additional variables\nrepresenting the offset between pairs $(x_i,x_j)$.\nNote that each $z_{ij}$ is an element of an annulus.\n\n\\paragraph{Problem instances.}\nWe generated problems with different numbers of circles. Here we report the performance\nof the relax-round-polish heuristic for a problem with $n=41$,\nin two cases:\na problem with all circle radii equal to $0.5$, and a problem where the radii\nwere chosen uniformly at random from the interval $[0.2,0.5]$.\n\n\\paragraph{Results.}\nWe run the relax-round-polish heuristic in both cases.\nFor this problem, the heuristic is effectively equivalent to\nmany well-known methods like the convex-concave procedure and the majorization-minimization\n(MM) algorithm. Figure \\ref{circles41} shows the packing found by our heuristic for $n=41$.\nThe obtained packing covers $78.68\\%$ of the area of the bounding square, which is close\nto the densest known packing, which covers $79.27\\%$ of the area.\nWe observed that NC-ADMM is no more effective than\nrelax-round-polish for this problem.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=.46\\linewidth]{cp_41_eq}\n\\hspace*{\\fill}\n\\includegraphics[width=.46\\linewidth]{cp_41_diff}\n\\caption{\nPacking for $n=41$ circles with equal and different radii.\n\\label{circles41}}\n\\end{center}\n\\end{figure}\n\n\\subsection{Traveling salesman problem}\nIn the traveling salesman problem (TSP), we wish to find\nthe minimum weight Hamiltonian cycle in a weighted graph.\nA Hamiltonian cycle is a path that starts and ends on the same\nvertex and visits each other vertex in the graph exactly once.\nLet $G$ be a graph with $n$ vertices and $D\\in\\symm^n$ be the (weighted)\nadjacency matrix, \\ie, the real number $d_{ij}$ denotes the distance between $i$ and $j$.\nWe can formulate the TSP problem for $G$ as follows\n\\BEQ\n\\label{tsp}\n\\begin{array}{ll}\n\\mbox{minimize} & (1/2)\\Tr(D^TZ)\\\\\n\\mbox{subject to} &Z \\in \\mathcal H_n,\n\\end{array}\n\\EEQ\nwhere $Z$ is the decision variable \\cite{lawler1985traveling,kruskal1956shortest,\ndantzig1954solution,hoffman2013traveling}.\n\n\\paragraph{Problem instances.}\nWe generated $n=75$ points in $[-1,1]^2$.\nWe set $d_{ij}$ to be the Euclidean distance between\npoints $i$ and $j$.\n\n\\paragraph{Results.}\n\n\nFigure \\ref{tsp} compares the Hamiltonian cycle\nfound by the NC-ADMM heuristic, which had cost $14.47$,\nwith the optimal Hamiltonian cycle,\nwhich had cost $14.16$.\nThe cycle found by our heuristic has a few clearly suboptimal paths,\nbut overall is a reasonable approximate solution.\nWe ran NC-ADMM with 5 restarts and 100 iterations.\nGUROBI solved $4190$ subproblems before finding a solution as\ngood as that found by NC-ADMM,\nwhich solved only 500 subproblems.\nThe relax-round-polish heuristic does not perform well on this problem.\nThe best objective value found by the heuristic is $35.6$.\n\n\\begin{figure}\n\\begin{center}\n\\begin{psfrags}\n\\psfrag{x}[B][B]{\\raisebox{-1.2ex}{\\tiny $t$}}\n\\psfrag{p1}[B][B]{\\raisebox{+.5ex}{\\tiny{$P^\\mathrm{Eng}_t$}}}\n\\psfrag{p2}[B][B]{\\raisebox{+.5ex}{{\\tiny $P^\\mathrm{batt}_t$}}}\n\\psfrag{p3}[B][B]{\\raisebox{+.5ex}{\\tiny{$E_t$}}}\n\\psfrag{p4}[B][B]{\\raisebox{+.5ex}{\\tiny{$z_t$}}}\n\\includegraphics[width=.46\\linewidth]{NC_ADMM_tsp_sltn}\n\\end{psfrags}\n\\hspace*{\\fill}\n\\begin{psfrags}\n\\psfrag{x}[B][B]{\\raisebox{-1.2ex}{\\tiny $t$}}\n\\psfrag{p1}[B][B]{\\raisebox{+.5ex}{\\tiny{$P^\\mathrm{Eng}_t$}}}\n\\psfrag{p2}[B][B]{\\raisebox{+.5ex}{{\\tiny $P^\\mathrm{batt}_t$}}}\n\\psfrag{p3}[B][B]{\\raisebox{+.5ex}{\\tiny{$E_t$}}}\n\\psfrag{p4}[B][B]{\\raisebox{+.5ex}{\\tiny{$z_t$}}}\n\\includegraphics[width=.46\\linewidth]{GUROBI_tsp_sltn}\n\\end{psfrags}\n\\caption{\nLeft: Hamiltonian cycle found by NC-ADMM.\nRight: optimal Hamiltonian cycle, found using GUROBI.\n\\label{tsp}}\n\\end{center}\n\\end{figure}\n\n\\subsection{Factor analysis model}\nThe factor analysis problem decomposes a matrix as a sum of a low-rank\nand a diagonal matrix and has been studied extensively (for example\nin \\cite{saunderson2012diagonal,ning2015linear}).\nIt is also known as the \\emph{Frisch} scheme in\nthe system identification literature \\cite{kalman1985identification,\ndavid1993opposite}.\nThe problem is the following\n\\BEQ\n\\label{factor}\n\\begin{array}{ll}\n\\mbox{minimize} & \\|\\Sigma-\\Sigma^\\mathrm{lr}-D\\|_F^2\\\\\n\\mbox{subject to} & D = \\diag(d),\\quad d\\geq 0\\\\\n&\\Sigma^\\mathrm{lr} \\succeq 0\\\\\n&\\Rank(\\Sigma^\\mathrm{lr}) \\leq k,\n\n\\end{array}\n\\EEQ\nwhere $\\Sigma^\\mathrm{lr}\\in\\symm_+^{n}$ and diagonal matrix\n$D\\in\\reals^{n\\times n}$ with nonnegative diagonal entries\nare the decision variables, and $\\Sigma\\in\\symm_+^n$ and\n$k\\in\\integers_+$ are problem data.\nOne well-known heuristic for solving this problem is adding\n$\\|\\cdot\\|_*$, or nuclear norm, regularization and minimizing\n$ \\|\\Sigma-\\Sigma^\\mathrm{lr}-D\\|_F^2 +\\lambda\\|\\Sigma^\\mathrm{lr}\\|_*$.\nThe value of $\\lambda$ is chosen as the smallest value possible such\nthat $\\Rank(\\Sigma^\\mathrm{lr}) \\leq k$.\nSince $\\Sigma^\\mathrm{lr}$ is positive semidefinite,\n$\\|\\Sigma^\\mathrm{lr}\\|_* = \\Tr(\\Sigma^\\mathrm{lr})$.\n\n\\paragraph{Problem instances.}\nWe set $k= \\lfloor n/2 \\rfloor$ and\ngenerated the matrix $F \\in \\reals^{n \\times k}$\nby drawing the entries i.i.d.\\ from a standard normal distribution.\nWe generated a diagonal matrix $\\hat D$ with\ndiagonal entries drawn i.i.d.\\ from an exponential distribution\nwith mean $1$.\nWe set $\\Sigma = FF^T + \\hat D + V$,\nwhere $V \\in \\reals^{n \\times n}$ is a noise matrix with\nentries drawn i.i.d.\\ from $\\mathcal{N}(0,\\sigma^2)$.\nWe set $\\sigma^2 = \\|FF^T + \\diag(d)\\|^2_F/(400n^2)$ so that\nthe signal-to-noise ratio was near 20.\n\n\\paragraph{Results.}\nFigure \\ref{factor_analysis_results} compares the average sum of squares error\nfor the $\\Sigma^\\mathrm{lr}$ and $d$ values found by NC-ADMM,\nrelax-round-polish, and the nuclear norm heuristic over 50 instances per value of $n$.\n\n\n\nWe observe that the sum of squares error obtained by NC-ADMM is smaller\nthan that obtained by the nuclear norm and relax-round-polish heuristics.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{factor_analysis_results}\n\\end{center}\n\\caption{The average error of solutions found by the nuclear norm,\nrelax-round-polish, and NC-ADMM heuristics for 50 instances of the factor analysis problem.\n}\\label{factor_analysis_results}\n\\end{figure}\n\n\\subsection{Job selection}\nIn the job selection problem there are $n$ jobs and $m$ resources.\nEach job $i$ consumes $A_{ij} \\geq 0$ units of resource $j$, and\nup to $d_i > 0$ instances of job $i$ can be accepted.\nExecuting job $i$ produces $c_i > 0$ units of profit.\nThe goal is to maximize profit subject to the constraint that\nat most $b_j > 0$ units of each resource $j$ are consumed.\nThe job selection problem can be formulated as\n\\BEQ\n\\label{job}\n\\begin{array}{ll}\n\\mbox{maximize} & c^Tz \\\\x\n\\mbox{subject to} & Az \\leq b \\\\\n& 0 \\leq z \\leq d\\\\\n& z \\in \\integers^n,\n\\end{array}\n\\EEQ\nwhere $z$ is the decision variable and $A \\in \\reals^{m \\times n}$,\n$b \\in \\reals^m_+$, and $d \\in \\integers_+^n$ are problem data.\nThis problem is NP-hard in general.\nWhen $m=1$, this problem is\nequivalent to the \\emph{knapsack problem}, which\nhas been studied extensively; see, \\eg,\n\\cite{chu1998genetic, chekuri2005polynomial}.\n\n\\paragraph{Problem instances.}\nWe set $m=\\lfloor n/10 \\rfloor$ and generated $A \\in \\reals^{m \\times n}$\nby randomly selecting $\\lfloor mn/10 \\rfloor$ entries to be nonzero.\n\nThe nonzero entries were drawn i.i.d.\\ from the uniform distribution\nover $[0,5]$.\nEntries of $c \\in \\reals^n$ were drawn i.i.d.\\ from\nthe uniform distribution over $[0,1]$.\nEntries of $d \\in \\integers^n$ were drawn i.i.d.\\ from the uniform\ndistribution over the set $\\{1,\\ldots,5\\}$.\nWe generated $b \\in \\reals^{m}$ by first generating\n$\\hat{z} \\in \\integers^n$, where each $\\hat{z}_i$ was drawn\nfrom the uniform distribution over the set $\\{0,\\ldots,d_i\\}$,\nand then setting $b = A\\hat{z}$.\n\n\\paragraph{Results.}\nWe generated problem instances for a range of $10 \\leq n \\leq 100$.\nFigure \\ref{job_selection_results} compares, for each $n$,\nthe average value of $c^Tz$ found by the NC-ADMM heuristic and by GUROBI over $10$ instances.\nNC-ADMM was run from 10 random starting points for 100 iterations.\nThe value of $\\rho$ for each starting point was drawn\nfrom the uniform distribution over $[0,5]$.\nGUROBI's run time was limited to 10 minutes.\nNC-ADMM always found a feasible $z$ with an objective value not much\nworse than that found by GUROBI.\nWe also tried the relax-round-polish heuristic on the problem instances,\nbut it never found a feasible $z$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{job_selection_results}\n\\end{center}\n\\caption{The average objective value for the job selection problem over 10 different instances.\n}\\label{job_selection_results}\n\\end{figure}\n\n\\begin{incomplete}\n\\subsection{Interpretable models}\nDeveloping interpretable patient-level predictive models using medical data has\nattracted researchers attention \\cite{LethamRuMcMa15}. A widely used example\nof an interpretable model is CHADS\\textsubscript2 \\cite{gage2001validation}\nto predict stroke in patients with atrial fibrillation,\nin which the patient's score is calculated by assigning one\nor two `points'\nfor the presence of $5$ different features (such as age above $75$).\n\n\n\n\n\nIn a similar framework here we study a logistic regression model\nto predict a binary result (\\eg, stroke or no stroke),\nwhere the\ncoefficients (individual scores) come from a discrete\nset for example $\\{0,\\pm1,\\pm2\\}$. Let\n$u_1,\\ldots, u_q\\in \\reals^n$ denote the training points where the\nresult is positive and let $u_{q+1},\\ldots, u_m\\in \\reals^n$ denote the\ntraining points where the result is negative. The problem is\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & -\\sum_{i=1}^q(a^Tu_i + b) + \\sum_{i=1}^n\n\\log\\left(1+\\exp(a^Tu_i+b)\\right)\\\\\n\\mbox{subject to} & a \\in \\{0,\\pm1,\\pm2\\}^n,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variables $a,b$.\n\n\\paragraph{Problem instances.}\nTODO\n\n\\paragraph{Results.}\nTODO\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Pooling problems}\nThe pooling problem is a multi-commodity flow problem, where each node\nis representative of a pool where (typically fluid) material are continuously mixed\n\\cite{gupte2013pooling, bodington1990history}.\nConsider a directed acyclic graph with $n$ nodes where each edge $e$\nis characterized by a vector $m_e\\in\\reals_+^q$\nrepresenting the flow of each of\n$q$ constituents.\nFor each node $v$, let $D_v^+$($D_v^-$) be the set of edges entering to\n(exiting from) node\n$v$. At each node\n$v$, we have the (mass conservation) equations\n", "itemtype": "equation", "pos": 62764, "prevtext": "\n\n\n\\paragraph{Problem instances.}\nWe generated 3-SAT problems with varying numbers of clauses and variables\nrandomly as in \\cite{mitchell1992hard,lipp2014variations}.\nAs discussed in \\cite{crawford1996experimental},\nthere is a threshold around $4.25$ clauses\nper variable when problems transition from being feasible to being infeasible.\nProblems near this threshold are generally found\nto be hard satisfiability problems.\nWe generated 10 instances for each choice of number of clauses and variables,\nverifying that each instance is feasible using GUROBI \\cite{gurobi}.\n\n\\paragraph{Results.}\nWe ran NC-ADMM heuristic on each instance, with 10 restarts,\nand 100 iterations, and we chose the step size $\\rho=10$.\nFigure~\\ref{3sat_results} shows the fraction of instances solved correctly\nwith NC-ADMM. We see that using this heuristic, satisfying assignments\ncan be found consistently for up to 3.2\nconstraints per variable, at which point success starts to decrease.\nProblems in the gray region in figure~\\ref{3sat_results} were not\ntested since they are infeasible with high probability.\nWe also tried the relax-round-polish heuristic, but it often failed to\nsolve problems with more than $50$ clauses.\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{three_sat}\n\\end{center}\n\\caption{Fraction of runs for which a satisfying assignment to\nrandom 3-SAT problems were found for problems of varying sizes.\nThe problems in the gray region were not tested.}\n\\label{3sat_results}\n\\end{figure}\n\n\\begin{incomplete}\n\\subsection{Graph coloring}\nGiven a graph $G$, the smallest number of colors needed to color the vertices of\n$G$ such that no two adjacent vertices have the same color is called the\n\\emph{chromatic} number of $G$ and is often denoted by $\\chi(G)$. It is easy to\nshow that $\\omega(G)\\leq\\chi(G)\\leq \\Delta(G) + 1$,\nwhere $\\omega(G)$ is the size of a maximum clique in $G$ and\n$\\Delta(G)$ is the maximum degree in $G$.\nFinding the chromatic number of a graph is known to be NP-hard.\nFor a given graph $G$ we can construct a \\emph{coloring matrix}\n$Z\\in\\{0,1\\}^{n\\times \\Delta(G) + 1}$,\nwhere $Z_{ij}=1$ means that vertex $i$ is colored with color $j$.\nEach vertex must be colored with exactly one color, so there should be\nexactly one nonzero entry in every row. In other words, $Z\\ones = \\ones$.\nFor any adjacent vertices $i$ and $j$,\nwe must have $Z_{i,:}+Z_{j,:}\\leq\\ones$,\nwhere $Z_{i,:}$ denotes the $i$th row of $Z$.\nWe want to minimize the number of colors used,\nwhich can be written as $\\sum_j \\| Z_{:,j}\\|_\\infty$, since\n$\\| Z_{:,j}\\|_\\infty$ is $0$ if color $j$ is not used, and $1$ if it is.\nIf $Z^\\star$ is an optimal solution, permuting the columns of $Z^\\star$\ngives additional optimal solutions.\nTo break this symmetry, we add the penalty term\n", "index": 65, "text": "\n\\[\n\\tau(Z) = \\sum_{i=1}^{n-1} \\max\\{h^T(Z_{:,i} - Z_{:,i+1}),0\\},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\tau(Z)=\\sum_{i=1}^{n-1}\\max\\{h^{T}(Z_{:,i}-Z_{:,i+1}),0\\},\" display=\"block\"><mrow><mrow><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>h</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Z</mi><mrow><mo>:</mo><mo>,</mo><mi>i</mi></mrow></msub><mo>-</mo><msub><mi>Z</mi><mrow><mo>:</mo><mo>,</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nAlso, the mass flows from any node should have the same concentration of each of\n$q$ constituents. In other words, for any node $v$ there exists a vector $c_v$ such\nthat for every edge $e\\in D_v^-$,\n", "itemtype": "equation", "pos": 76638, "prevtext": "\nwhere $h \\in \\reals^n$ is an arbitrary vector.\nAdding the penalty term does not alter the problem, since there\nis always a permutation of $Z^\\star$ for which $\\tau(Z) = 0$.\nThe graph coloring problem can thus be formulated as\n\\BEQ\n\\label{graphcolor}\n\\begin{array}{ll}\n\\mbox{minimize} &\\| Z_{:,j}\\|_\\infty + \\tau(Z)\\\\\n\\mbox{subject to} & Z_{i,:}+Z_{j,:}\\leq \\ones \\quad \\mbox{$i,j$ adjacent}\\\\\n& Z_{i,:} \\in \\mathcal{C}, \\quad i=1,\\ldots,n,\n\\end{array}\n\\EEQ\nwhere $\\C = \\{ x\\in \\{0,1\\}^n \\mid \\card(x) = 1\\}$,\nor \\verb+Choose(n,1)+ in CVXPY.\n\n\n\n\n\n\n\n\n\n\\paragraph{Problem instances.}\nWe generated random graphs drawn uniformly from the set of graphs\nwith $n$ vertices and $\\lfloor n(n-1)/2k \\rfloor$ edges,\nfor a range of $n$ and $k$.\nThe parameter $k$ is the ratio of edges in a complete graph to\nedges in the generated graph.\nWe generated 10 graphs for each choice of $n$ and $k$.\nWe found the chromatic number of each graph by solving problem\n(\\ref{graphcolor}) using GUROBI.\nThe vector $h \\in \\reals^n$ for each problem instance was chosen\nuniformly at random on the $n$-sphere.\n\n\n\\paragraph{Results.}\nFigure \\ref{graph_coloring_results} shows the average difference between\nthe result found by NC-ADMM and the true chromatic number.\nNC-ADMM was run from 5 random starting points for 50 iterations.\nThe value of $\\rho$ for each starting point was drawn uniformly at\nrandom from $[0,1]$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{graph_coloring_results}\n\\end{center}\n\\caption{Graph coloring results.\n}\\label{graph_coloring_results}\n\\end{figure}\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Assignment (not final yet)}\nIn the most general form of \\emph{assignment problem}, there are $n$ tasks and\n$m$ processors. Processor $j$ can perform task $i$ with cost $W_{ij}$. The problem\nis assigning tasks to processors so that the overall cost is minimized assuming that\neach processor can be assigned to at most one task. This problem is equivalent to\nfinding a minimum weight matching of a bipartite graph\n\nThe goal is to find an assignment matrix $X\\in\\{0,1\\}^{m\\times n}$ such that\n$\\Tr(X^TW)$ is minimized. So the problem can be formulated as\n\\BEQ\n\\label{assignment}\n\\begin{array}{ll}\n\\mbox{minimize} & \\Tr(Z^TW)\\\\\n\\mbox{subject to} & Z \\quad \\mbox{assignment matrix},\\\\\n\\end{array}\n\\EEQ\n\\end{incomplete}\n\n\\subsection{Circle packing}\nIn \\emph{circle packing problem} we are interested in finding the smallest square\nin which we can place $n$ non-overlapping circles with radii $r_1,\\ldots,r_n$\n\\cite{goldberg1970packing}.\nThis problem has been studied extensively\n\\cite{stephenson2005introduction, castillo2008solving, collins2003circle}\nand a database of densest known packings for different numbers of circles can be found\nin \\cite{Specht2013}.\nThe problem can be formulated as\n\\BEQ\n\\label{packing}\n\\begin{array}{ll}\n\\mbox{minimize} & l\\\\\n\\mbox{subject to} & r_i\\ones \\leq x_i \\leq (l-r_i) \\ones, \\quad i = 1,\\ldots,n\\\\\n& x_i-x_j = z_{ij},\\quad i= 1,\\ldots , n-1,\\quad j = i+1,\\ldots, n \\\\\n& 2\\sum_{k=1}^n r_i \\geq \\|z_{ij}\\|_2 \\geq r_i + r_j, \\quad i= 1,\\ldots , n -1,\\quad j = i+1,\\ldots, n,\n\\end{array}\n\\EEQ\nwhere $x_1,\\ldots,x_n \\in \\reals^2$ are variables representing the circle centers\nand $z_{12},z_{13},\\ldots,z_{n-1,n} \\in \\reals^2$ are additional variables\nrepresenting the offset between pairs $(x_i,x_j)$.\nNote that each $z_{ij}$ is an element of an annulus.\n\n\\paragraph{Problem instances.}\nWe generated problems with different numbers of circles. Here we report the performance\nof the relax-round-polish heuristic for a problem with $n=41$,\nin two cases:\na problem with all circle radii equal to $0.5$, and a problem where the radii\nwere chosen uniformly at random from the interval $[0.2,0.5]$.\n\n\\paragraph{Results.}\nWe run the relax-round-polish heuristic in both cases.\nFor this problem, the heuristic is effectively equivalent to\nmany well-known methods like the convex-concave procedure and the majorization-minimization\n(MM) algorithm. Figure \\ref{circles41} shows the packing found by our heuristic for $n=41$.\nThe obtained packing covers $78.68\\%$ of the area of the bounding square, which is close\nto the densest known packing, which covers $79.27\\%$ of the area.\nWe observed that NC-ADMM is no more effective than\nrelax-round-polish for this problem.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=.46\\linewidth]{cp_41_eq}\n\\hspace*{\\fill}\n\\includegraphics[width=.46\\linewidth]{cp_41_diff}\n\\caption{\nPacking for $n=41$ circles with equal and different radii.\n\\label{circles41}}\n\\end{center}\n\\end{figure}\n\n\\subsection{Traveling salesman problem}\nIn the traveling salesman problem (TSP), we wish to find\nthe minimum weight Hamiltonian cycle in a weighted graph.\nA Hamiltonian cycle is a path that starts and ends on the same\nvertex and visits each other vertex in the graph exactly once.\nLet $G$ be a graph with $n$ vertices and $D\\in\\symm^n$ be the (weighted)\nadjacency matrix, \\ie, the real number $d_{ij}$ denotes the distance between $i$ and $j$.\nWe can formulate the TSP problem for $G$ as follows\n\\BEQ\n\\label{tsp}\n\\begin{array}{ll}\n\\mbox{minimize} & (1/2)\\Tr(D^TZ)\\\\\n\\mbox{subject to} &Z \\in \\mathcal H_n,\n\\end{array}\n\\EEQ\nwhere $Z$ is the decision variable \\cite{lawler1985traveling,kruskal1956shortest,\ndantzig1954solution,hoffman2013traveling}.\n\n\\paragraph{Problem instances.}\nWe generated $n=75$ points in $[-1,1]^2$.\nWe set $d_{ij}$ to be the Euclidean distance between\npoints $i$ and $j$.\n\n\\paragraph{Results.}\n\n\nFigure \\ref{tsp} compares the Hamiltonian cycle\nfound by the NC-ADMM heuristic, which had cost $14.47$,\nwith the optimal Hamiltonian cycle,\nwhich had cost $14.16$.\nThe cycle found by our heuristic has a few clearly suboptimal paths,\nbut overall is a reasonable approximate solution.\nWe ran NC-ADMM with 5 restarts and 100 iterations.\nGUROBI solved $4190$ subproblems before finding a solution as\ngood as that found by NC-ADMM,\nwhich solved only 500 subproblems.\nThe relax-round-polish heuristic does not perform well on this problem.\nThe best objective value found by the heuristic is $35.6$.\n\n\\begin{figure}\n\\begin{center}\n\\begin{psfrags}\n\\psfrag{x}[B][B]{\\raisebox{-1.2ex}{\\tiny $t$}}\n\\psfrag{p1}[B][B]{\\raisebox{+.5ex}{\\tiny{$P^\\mathrm{Eng}_t$}}}\n\\psfrag{p2}[B][B]{\\raisebox{+.5ex}{{\\tiny $P^\\mathrm{batt}_t$}}}\n\\psfrag{p3}[B][B]{\\raisebox{+.5ex}{\\tiny{$E_t$}}}\n\\psfrag{p4}[B][B]{\\raisebox{+.5ex}{\\tiny{$z_t$}}}\n\\includegraphics[width=.46\\linewidth]{NC_ADMM_tsp_sltn}\n\\end{psfrags}\n\\hspace*{\\fill}\n\\begin{psfrags}\n\\psfrag{x}[B][B]{\\raisebox{-1.2ex}{\\tiny $t$}}\n\\psfrag{p1}[B][B]{\\raisebox{+.5ex}{\\tiny{$P^\\mathrm{Eng}_t$}}}\n\\psfrag{p2}[B][B]{\\raisebox{+.5ex}{{\\tiny $P^\\mathrm{batt}_t$}}}\n\\psfrag{p3}[B][B]{\\raisebox{+.5ex}{\\tiny{$E_t$}}}\n\\psfrag{p4}[B][B]{\\raisebox{+.5ex}{\\tiny{$z_t$}}}\n\\includegraphics[width=.46\\linewidth]{GUROBI_tsp_sltn}\n\\end{psfrags}\n\\caption{\nLeft: Hamiltonian cycle found by NC-ADMM.\nRight: optimal Hamiltonian cycle, found using GUROBI.\n\\label{tsp}}\n\\end{center}\n\\end{figure}\n\n\\subsection{Factor analysis model}\nThe factor analysis problem decomposes a matrix as a sum of a low-rank\nand a diagonal matrix and has been studied extensively (for example\nin \\cite{saunderson2012diagonal,ning2015linear}).\nIt is also known as the \\emph{Frisch} scheme in\nthe system identification literature \\cite{kalman1985identification,\ndavid1993opposite}.\nThe problem is the following\n\\BEQ\n\\label{factor}\n\\begin{array}{ll}\n\\mbox{minimize} & \\|\\Sigma-\\Sigma^\\mathrm{lr}-D\\|_F^2\\\\\n\\mbox{subject to} & D = \\diag(d),\\quad d\\geq 0\\\\\n&\\Sigma^\\mathrm{lr} \\succeq 0\\\\\n&\\Rank(\\Sigma^\\mathrm{lr}) \\leq k,\n\n\\end{array}\n\\EEQ\nwhere $\\Sigma^\\mathrm{lr}\\in\\symm_+^{n}$ and diagonal matrix\n$D\\in\\reals^{n\\times n}$ with nonnegative diagonal entries\nare the decision variables, and $\\Sigma\\in\\symm_+^n$ and\n$k\\in\\integers_+$ are problem data.\nOne well-known heuristic for solving this problem is adding\n$\\|\\cdot\\|_*$, or nuclear norm, regularization and minimizing\n$ \\|\\Sigma-\\Sigma^\\mathrm{lr}-D\\|_F^2 +\\lambda\\|\\Sigma^\\mathrm{lr}\\|_*$.\nThe value of $\\lambda$ is chosen as the smallest value possible such\nthat $\\Rank(\\Sigma^\\mathrm{lr}) \\leq k$.\nSince $\\Sigma^\\mathrm{lr}$ is positive semidefinite,\n$\\|\\Sigma^\\mathrm{lr}\\|_* = \\Tr(\\Sigma^\\mathrm{lr})$.\n\n\\paragraph{Problem instances.}\nWe set $k= \\lfloor n/2 \\rfloor$ and\ngenerated the matrix $F \\in \\reals^{n \\times k}$\nby drawing the entries i.i.d.\\ from a standard normal distribution.\nWe generated a diagonal matrix $\\hat D$ with\ndiagonal entries drawn i.i.d.\\ from an exponential distribution\nwith mean $1$.\nWe set $\\Sigma = FF^T + \\hat D + V$,\nwhere $V \\in \\reals^{n \\times n}$ is a noise matrix with\nentries drawn i.i.d.\\ from $\\mathcal{N}(0,\\sigma^2)$.\nWe set $\\sigma^2 = \\|FF^T + \\diag(d)\\|^2_F/(400n^2)$ so that\nthe signal-to-noise ratio was near 20.\n\n\\paragraph{Results.}\nFigure \\ref{factor_analysis_results} compares the average sum of squares error\nfor the $\\Sigma^\\mathrm{lr}$ and $d$ values found by NC-ADMM,\nrelax-round-polish, and the nuclear norm heuristic over 50 instances per value of $n$.\n\n\n\nWe observe that the sum of squares error obtained by NC-ADMM is smaller\nthan that obtained by the nuclear norm and relax-round-polish heuristics.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{factor_analysis_results}\n\\end{center}\n\\caption{The average error of solutions found by the nuclear norm,\nrelax-round-polish, and NC-ADMM heuristics for 50 instances of the factor analysis problem.\n}\\label{factor_analysis_results}\n\\end{figure}\n\n\\subsection{Job selection}\nIn the job selection problem there are $n$ jobs and $m$ resources.\nEach job $i$ consumes $A_{ij} \\geq 0$ units of resource $j$, and\nup to $d_i > 0$ instances of job $i$ can be accepted.\nExecuting job $i$ produces $c_i > 0$ units of profit.\nThe goal is to maximize profit subject to the constraint that\nat most $b_j > 0$ units of each resource $j$ are consumed.\nThe job selection problem can be formulated as\n\\BEQ\n\\label{job}\n\\begin{array}{ll}\n\\mbox{maximize} & c^Tz \\\\x\n\\mbox{subject to} & Az \\leq b \\\\\n& 0 \\leq z \\leq d\\\\\n& z \\in \\integers^n,\n\\end{array}\n\\EEQ\nwhere $z$ is the decision variable and $A \\in \\reals^{m \\times n}$,\n$b \\in \\reals^m_+$, and $d \\in \\integers_+^n$ are problem data.\nThis problem is NP-hard in general.\nWhen $m=1$, this problem is\nequivalent to the \\emph{knapsack problem}, which\nhas been studied extensively; see, \\eg,\n\\cite{chu1998genetic, chekuri2005polynomial}.\n\n\\paragraph{Problem instances.}\nWe set $m=\\lfloor n/10 \\rfloor$ and generated $A \\in \\reals^{m \\times n}$\nby randomly selecting $\\lfloor mn/10 \\rfloor$ entries to be nonzero.\n\nThe nonzero entries were drawn i.i.d.\\ from the uniform distribution\nover $[0,5]$.\nEntries of $c \\in \\reals^n$ were drawn i.i.d.\\ from\nthe uniform distribution over $[0,1]$.\nEntries of $d \\in \\integers^n$ were drawn i.i.d.\\ from the uniform\ndistribution over the set $\\{1,\\ldots,5\\}$.\nWe generated $b \\in \\reals^{m}$ by first generating\n$\\hat{z} \\in \\integers^n$, where each $\\hat{z}_i$ was drawn\nfrom the uniform distribution over the set $\\{0,\\ldots,d_i\\}$,\nand then setting $b = A\\hat{z}$.\n\n\\paragraph{Results.}\nWe generated problem instances for a range of $10 \\leq n \\leq 100$.\nFigure \\ref{job_selection_results} compares, for each $n$,\nthe average value of $c^Tz$ found by the NC-ADMM heuristic and by GUROBI over $10$ instances.\nNC-ADMM was run from 10 random starting points for 100 iterations.\nThe value of $\\rho$ for each starting point was drawn\nfrom the uniform distribution over $[0,5]$.\nGUROBI's run time was limited to 10 minutes.\nNC-ADMM always found a feasible $z$ with an objective value not much\nworse than that found by GUROBI.\nWe also tried the relax-round-polish heuristic on the problem instances,\nbut it never found a feasible $z$.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{job_selection_results}\n\\end{center}\n\\caption{The average objective value for the job selection problem over 10 different instances.\n}\\label{job_selection_results}\n\\end{figure}\n\n\\begin{incomplete}\n\\subsection{Interpretable models}\nDeveloping interpretable patient-level predictive models using medical data has\nattracted researchers attention \\cite{LethamRuMcMa15}. A widely used example\nof an interpretable model is CHADS\\textsubscript2 \\cite{gage2001validation}\nto predict stroke in patients with atrial fibrillation,\nin which the patient's score is calculated by assigning one\nor two `points'\nfor the presence of $5$ different features (such as age above $75$).\n\n\n\n\n\nIn a similar framework here we study a logistic regression model\nto predict a binary result (\\eg, stroke or no stroke),\nwhere the\ncoefficients (individual scores) come from a discrete\nset for example $\\{0,\\pm1,\\pm2\\}$. Let\n$u_1,\\ldots, u_q\\in \\reals^n$ denote the training points where the\nresult is positive and let $u_{q+1},\\ldots, u_m\\in \\reals^n$ denote the\ntraining points where the result is negative. The problem is\n\\BEQ\n\\begin{array}{ll}\n\\mbox{minimize}   & -\\sum_{i=1}^q(a^Tu_i + b) + \\sum_{i=1}^n\n\\log\\left(1+\\exp(a^Tu_i+b)\\right)\\\\\n\\mbox{subject to} & a \\in \\{0,\\pm1,\\pm2\\}^n,\n\\end{array}\n\\label{e-card-ls}\n\\EEQ\nwith decision variables $a,b$.\n\n\\paragraph{Problem instances.}\nTODO\n\n\\paragraph{Results.}\nTODO\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Pooling problems}\nThe pooling problem is a multi-commodity flow problem, where each node\nis representative of a pool where (typically fluid) material are continuously mixed\n\\cite{gupte2013pooling, bodington1990history}.\nConsider a directed acyclic graph with $n$ nodes where each edge $e$\nis characterized by a vector $m_e\\in\\reals_+^q$\nrepresenting the flow of each of\n$q$ constituents.\nFor each node $v$, let $D_v^+$($D_v^-$) be the set of edges entering to\n(exiting from) node\n$v$. At each node\n$v$, we have the (mass conservation) equations\n", "index": 67, "text": "\n\\[\n\\sum_{e\\in D_v^+}m_e = \\sum_{e\\in D_v^-}m_e.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\sum_{e\\in D_{v}^{+}}m_{e}=\\sum_{e\\in D_{v}^{-}}m_{e}.\" display=\"block\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><msubsup><mi>D</mi><mi>v</mi><mo>+</mo></msubsup></mrow></munder><msub><mi>m</mi><mi>e</mi></msub></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><msubsup><mi>D</mi><mi>v</mi><mo>-</mo></msubsup></mrow></munder><msub><mi>m</mi><mi>e</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\nSome nodes are \\emph{source} nodes, with a single input edge with known\nmass flow rates. Some vertices are \\emph{sink} nodes, with a single output\nedge with desired mass flow rates.\nLet $I_+$($I_i$) denote the set of source (sink) nodes.\nIn the simplest case, we pay $p_v$ units\nfor every unit of the raw material in node $v\\in I_+$,\nand get paid $p_w$ units for every unit of the blended material in node $w\\in I_-$.\nFor nodes $v\\in I_+$, the vector $c_v$ is given and there is a upper bound on\nthe total amount of input $\\ones^T m_e \\leq B_e$ for $e \\in D_v^+$.\nHence the problem is\n\\BEQ\n\\label{pooling}\n\\begin{array}{ll}\n\\mbox{minimize} & \\sum_{v\\in I_+}p_v\\sum_{e\\in D_v^+}m_e -\n\\sum_{v\\in I_-}p_v\\sum_{e\\in D_v^-}m_e\\\\\n\\mbox{subject to} & \\sum_{e\\in D_v^+}m_e = \\sum_{e\\in D_v^-}m_e\n\\quad v=1,\\ldots,n\\\\\n& c_v = \\frac{m_e}{\\ones^Tm_e} \\quad v=1,\\ldots,n, \\quad e\\in D_v^-\\\\\n& \\ones^T m_e \\leq B_e \\quad v \\in I_-,\\quad e\\in D_v^+\\\\\n& \\tilde c_v \\leq c_v \\leq \\tilde c_v \\quad v \\in I_+.\n\\end{array}\n\\EEQ\nNotice that if a network only consists of source and sink nodes, this problem\nis convex. Also for fixed concentration vectors $c_v$, this is a convex problem.\n\n\n\n\n\n\n\n\n\\paragraph{Problem instances.}\nTODO the class isn't in the Python package yet.\n\n\\paragraph{Results.}\nTODO\n\\end{incomplete}\n\n\\subsection{Maximum coverage problem}\nA collection of sets $\\mathcal S = \\{S_1,S_2,\\ldots, S_m\\}$\n\nis defined over a domain of elements $\\{e_1,e_2,\\ldots,e_n\\}$ with associated weights\n$\\{w_i\\}_{i=1}^n$. The goal is to find the collection of no more than $k$\nsets $\\mathcal S^\\prime \\subseteq \\mathcal S$ that maximizes the total weight\nof elements covered by $\\mathcal S^\\prime$\n\\cite{khuller1999budgeted,hochbaum1996approximating,cohen2008generalized}.\nLet $x_i\\in\\{0,1\\}$,  for  $i=1,\\ldots, n$, be\na variable that takes $1$ if element $e_i$ is covered and $0$ otherwise.\nLet $y \\in\\{0,1\\}^m$ be\na variable with entry $y_j = 1$ if set $j$ is selected.\nThe problem is\n\\BEQ\n\\label{coverage}\n\\begin{array}{ll}\n\\mbox{maximize} & w^Tx \\\\\n\\mbox{subject to} & \\sum_{j \\in S_j}y_j \\geq x_i, \\quad i=1,\\ldots,n\\\\\n& x_i \\in\\{0,1\\}, \\quad i=1,\\ldots,n\\\\\n& y \\in \\{0,1\\}^m \\\\\n& \\card(y) = k.\n\\end{array}\n\\EEQ\nNote that $y$ is a Boolean vector with fixed cardinality.\n\n\\paragraph{Problem instances.}\nWe generated problems as follows.\nEach set contained each of the elements independently\nwith a constant probability $p$. Hence the expected size of each set was $np$.\nThere were $m=3/p$ sets, so the expected total\nnumber of elements in all sets (with repetition) was equal to $mnp=3n$.\nWe set $k=1/(3p)$.\n\n\nEach $w_i$ was chosen uniformly at random from the interval $[0,1]$.\n\n\\paragraph{Results.}\nWe generated problems as described above for $n=50,60,\\ldots,240$ and $p=0.01$.\nFor each value of $n$, we generated $10$ problems and recorded\nthe average weight $w^Tx$ of the approximate solutions found by NC-ADMM\nand the optimal solutions found by GUROBI.\nFigure \\ref{max_coverage_results} shows the results of our comparison\nof NC-ADMM and GUROBI.\nApproximate solutions found by the relax-round-polish\nheuristic were far worse than those found by NC-ADMM for this problem.\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{max_coverage_results}\n\\end{center}\n\\caption{The average solution weight over 10 different instances.\n}\\label{max_coverage_results}\n\\end{figure}\n\n\n\\subsection{Inexact graph isomorphism}\nTwo (undirected) graphs are isomorphic if we can permute the vertices of one so it\nis the same as the other (i.e., the same pairs of vertices are connected by edges).\nIf we describe them by their adjacency matrices $A$ and $B$,\nisomorphism is equivalent to\nthe existence of a permutation matrix $Z\\in\\reals^{n\\times n}$\nsuch that $ZAZ^T = B$, or equivalently $ZA=BZ$.\n\nSince in practical applications isomorphic graphs might be contaminated\nby noise, the inexact graph isomorphism problem is usually stated\n\\cite{aflalo2014graph, umeyama1988eigendecomposition,cross1997inexact},\nin which we want to find a permutation matrix $Z$ such that the\ndisagreement $\\|ZAZ^T-B\\|_F^2$\nbetween the transformed matrix and the target matrix is minimized.\nSince $\\|ZAZ^T-B\\|_F^2=\\|ZA-BZ\\|_F^2$ for any permutation matrix $Z$,\nthe inexact graph isomorphism problem can be formulated as\n\\BEQ\n\\label{isomorphism}\n\\begin{array}{ll}\n\\mbox{minimize} & \\|ZA-BZ\\|_F^2\\\\\n\\mbox{subject to} & Z\\in \\mathcal P_n.\n\\end{array}\n\\EEQ\nIf the optimal value of this problem is zero, it means that $A$ and $B$\nare isomorphic. Otherwise, the solution of this problem minimizes the\ndisagreement of $ZAZ^T$ and $B$ in the Frobenius norm sense.\n\nSolving inexact graph isomorphism problems is of interest in pattern recognition\n\\cite{conte2004thirty, rocha1994shape}, computer vision\n\\cite{schellewald2001evaluation}, shape analysis\n\\cite{sebastian2004recognition, he2006object},\nimage and video indexing \\cite{lee2006graph}, and neuroscience\n\\cite{vogelstein2011large}.\nIn many of the aforementioned fields\ngraphs are used to represent geometric structures, and\n$\\|ZAZ^T-B\\|_F^2$\ncan be interpreted as the strength of geometric deformation.\n\n\\paragraph{Problem instances.}\nIt can be shown that if $A$ and $B$ are isomorphic and\n$A$ has distinct eigenvalues\nand for all\neigenvectors $v$ of $A$ for which $\\ones^T v \\neq 0$, then the relaxed\nproblem has a unique solution which is the permutation matrix that\nrelates $A$ and $B$ \\cite{aflalo2014graph}.\nHence, in order to generate harder problems,\nwe generated the matrix $A$ such that it violated these conditions.\nIn particular, we constructed $A$ for the Peterson graph ($3$-regular with $10$ vertices),\nicosahedral graph ($5$-regular with $12$ vertices),\nRamsey graph ($8$-regular with $17$ vertices),\ndodecahedral graph ($3$-regular with $20$ vertices),\nand the Tutte-Coxeter graph ($3$-regular with $30$ vertices).\nFor each example we randomly permuted the vertices to obtain two isomorphic graphs.\n\n\\paragraph{Results.}\nWe ran NC-ADMM with $20$ iterations and $5$ restarts.\nFor all of our examples NC-ADMM was able to find the permutation relating\nthe two graphs.\nIt is interesting to notice that running the algorithm multiple times\ncan find different solutions if there is more than one permutation\nrelating the two graphs.\nThe relax-round-polish heuristic failed to find a solution for all\nof the aforementioned problems.\n\n\\begin{incomplete}\n\\subsection{Phase retrieval}\n\nThe problem of recovery of a signal given the magnitude of its Fourier\ntransform\n\\cite{candes2015phase,shechtman2014phase}\narises in various fields of science and engineering, including\nX-ray crystallography \\cite{harrison1993phase,millane1990phase},\nmicroscopy \\cite{miao2008extending},\nastronomy \\cite{fienup1987phase},\ndiffraction and array imaging \\cite{bunk2007diffractive},\nand optics \\cite{walther1963question}.\nIn its most general form, the phase retrieval problem can be formulated as\nfinding $x\\in\\reals^n$ such that $|(Fx)_i|$ is given for $i=1,\\ldots,m$,\nwhere $F\\in\\complex^{m\\times n}$. (A special case is when $Fx$ represents\nthe Fourier transform of $x$.) Decomposing $F$ into its real and imaginary\npart by $F=F_R+iF_I$ for $F_R,F_I\\in\\reals^{m\\times n}$, we have the following\nformulation\n\\BEQ\n\\label{phase}\n\\begin{array}{ll}\n\\mbox{find} & x\\\\\n\\mbox{subject to} & F_Rx = u\\\\\n& F_Ix=v\\\\\n&u_i^2+v_i^2=y_i^2\\quad i=1,\\ldots,m,\n\\end{array}\n\\EEQ\nwith decision variable $x,u,v$ and problem data $F,y$.\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Sudoku}\nA Sudoku puzzle is a partially completed $N\\times N$ grid of cells divided\ninto N segments. The goal is to fill the grid using a prescribed set of\n$N$ distinct symbols such that each element of the set appears exactly\nonce in each row, column, and segment. We define $N^3$ binary variables\n$z_{ijk}$ for $i,j,k=1,\\ldots, n$ where $z_{ijk}=1$ if and only the element $(i,j)$\nis filled with $k$. The problem then can be formulated as\n", "itemtype": "equation", "pos": 76888, "prevtext": "\nAlso, the mass flows from any node should have the same concentration of each of\n$q$ constituents. In other words, for any node $v$ there exists a vector $c_v$ such\nthat for every edge $e\\in D_v^-$,\n", "index": 69, "text": "\n\\[\nc_v = \\frac{m_e}{\\ones^Tm_e}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"c_{v}=\\frac{m_{e}}{\\ones^{T}m_{e}}.\" display=\"block\"><mrow><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><mfrac><msub><mi>m</mi><mi>e</mi></msub><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\ones</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><msub><mi>m</mi><mi>e</mi></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07277.tex", "nexttext": "\n\nIt is shown that solving Sudoku puzzles is NP-complete\n\\cite{takayuki2003complexity,kendall2008survey}.\nIn \\cite{Derbinsky2013AnImp} a message-passing version of ADMM has been used to solve Sudoku puzzles.\n\\end{incomplete}\n\n\\section{Conclusions}\nWe have discussed the relax-round-polish and NC-ADMM heuristics and\ndemonstrated their performance on many different problems with\nconvex objectives and decision variables from a nonconvex set.\nOur heuristics are easy to extend to additional problems because\nthey rely on a simple mathematical interface for nonconvex\nsets.\nWe need only know a method for (approximate) projection onto the set.\nWe do not require but benefit from knowing\na convex relaxation of the set, a convex restriction at any point in the set,\nand the neighbors of any point in the set under some discrete distance metric.\nAdapting our heuristics to any particular problem is straightforward,\nand we have fully automated the process in the NCVX package.\n\nWe do not claim that our heuristics give state-of-the-art results\nfor any particular problem.\nRather, the purpose of our heuristics is to give a fast and reasonable\nsolution with minimal tuning for a wide variety of problems.\n\nOur heuristics also take advantage of the tremendous progress in\ntechnology for solving general convex optimization problems,\nwhich makes it practical to treat solving a convex problem\nas a black box.\n\n\\clearpage\n\\nocite{*}\n\\bibliography{noncvx_admm}\n\n\n", "itemtype": "equation", "pos": 84847, "prevtext": "\nSome nodes are \\emph{source} nodes, with a single input edge with known\nmass flow rates. Some vertices are \\emph{sink} nodes, with a single output\nedge with desired mass flow rates.\nLet $I_+$($I_i$) denote the set of source (sink) nodes.\nIn the simplest case, we pay $p_v$ units\nfor every unit of the raw material in node $v\\in I_+$,\nand get paid $p_w$ units for every unit of the blended material in node $w\\in I_-$.\nFor nodes $v\\in I_+$, the vector $c_v$ is given and there is a upper bound on\nthe total amount of input $\\ones^T m_e \\leq B_e$ for $e \\in D_v^+$.\nHence the problem is\n\\BEQ\n\\label{pooling}\n\\begin{array}{ll}\n\\mbox{minimize} & \\sum_{v\\in I_+}p_v\\sum_{e\\in D_v^+}m_e -\n\\sum_{v\\in I_-}p_v\\sum_{e\\in D_v^-}m_e\\\\\n\\mbox{subject to} & \\sum_{e\\in D_v^+}m_e = \\sum_{e\\in D_v^-}m_e\n\\quad v=1,\\ldots,n\\\\\n& c_v = \\frac{m_e}{\\ones^Tm_e} \\quad v=1,\\ldots,n, \\quad e\\in D_v^-\\\\\n& \\ones^T m_e \\leq B_e \\quad v \\in I_-,\\quad e\\in D_v^+\\\\\n& \\tilde c_v \\leq c_v \\leq \\tilde c_v \\quad v \\in I_+.\n\\end{array}\n\\EEQ\nNotice that if a network only consists of source and sink nodes, this problem\nis convex. Also for fixed concentration vectors $c_v$, this is a convex problem.\n\n\n\n\n\n\n\n\n\\paragraph{Problem instances.}\nTODO the class isn't in the Python package yet.\n\n\\paragraph{Results.}\nTODO\n\\end{incomplete}\n\n\\subsection{Maximum coverage problem}\nA collection of sets $\\mathcal S = \\{S_1,S_2,\\ldots, S_m\\}$\n\nis defined over a domain of elements $\\{e_1,e_2,\\ldots,e_n\\}$ with associated weights\n$\\{w_i\\}_{i=1}^n$. The goal is to find the collection of no more than $k$\nsets $\\mathcal S^\\prime \\subseteq \\mathcal S$ that maximizes the total weight\nof elements covered by $\\mathcal S^\\prime$\n\\cite{khuller1999budgeted,hochbaum1996approximating,cohen2008generalized}.\nLet $x_i\\in\\{0,1\\}$,  for  $i=1,\\ldots, n$, be\na variable that takes $1$ if element $e_i$ is covered and $0$ otherwise.\nLet $y \\in\\{0,1\\}^m$ be\na variable with entry $y_j = 1$ if set $j$ is selected.\nThe problem is\n\\BEQ\n\\label{coverage}\n\\begin{array}{ll}\n\\mbox{maximize} & w^Tx \\\\\n\\mbox{subject to} & \\sum_{j \\in S_j}y_j \\geq x_i, \\quad i=1,\\ldots,n\\\\\n& x_i \\in\\{0,1\\}, \\quad i=1,\\ldots,n\\\\\n& y \\in \\{0,1\\}^m \\\\\n& \\card(y) = k.\n\\end{array}\n\\EEQ\nNote that $y$ is a Boolean vector with fixed cardinality.\n\n\\paragraph{Problem instances.}\nWe generated problems as follows.\nEach set contained each of the elements independently\nwith a constant probability $p$. Hence the expected size of each set was $np$.\nThere were $m=3/p$ sets, so the expected total\nnumber of elements in all sets (with repetition) was equal to $mnp=3n$.\nWe set $k=1/(3p)$.\n\n\nEach $w_i$ was chosen uniformly at random from the interval $[0,1]$.\n\n\\paragraph{Results.}\nWe generated problems as described above for $n=50,60,\\ldots,240$ and $p=0.01$.\nFor each value of $n$, we generated $10$ problems and recorded\nthe average weight $w^Tx$ of the approximate solutions found by NC-ADMM\nand the optimal solutions found by GUROBI.\nFigure \\ref{max_coverage_results} shows the results of our comparison\nof NC-ADMM and GUROBI.\nApproximate solutions found by the relax-round-polish\nheuristic were far worse than those found by NC-ADMM for this problem.\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.7\\textwidth]{max_coverage_results}\n\\end{center}\n\\caption{The average solution weight over 10 different instances.\n}\\label{max_coverage_results}\n\\end{figure}\n\n\n\\subsection{Inexact graph isomorphism}\nTwo (undirected) graphs are isomorphic if we can permute the vertices of one so it\nis the same as the other (i.e., the same pairs of vertices are connected by edges).\nIf we describe them by their adjacency matrices $A$ and $B$,\nisomorphism is equivalent to\nthe existence of a permutation matrix $Z\\in\\reals^{n\\times n}$\nsuch that $ZAZ^T = B$, or equivalently $ZA=BZ$.\n\nSince in practical applications isomorphic graphs might be contaminated\nby noise, the inexact graph isomorphism problem is usually stated\n\\cite{aflalo2014graph, umeyama1988eigendecomposition,cross1997inexact},\nin which we want to find a permutation matrix $Z$ such that the\ndisagreement $\\|ZAZ^T-B\\|_F^2$\nbetween the transformed matrix and the target matrix is minimized.\nSince $\\|ZAZ^T-B\\|_F^2=\\|ZA-BZ\\|_F^2$ for any permutation matrix $Z$,\nthe inexact graph isomorphism problem can be formulated as\n\\BEQ\n\\label{isomorphism}\n\\begin{array}{ll}\n\\mbox{minimize} & \\|ZA-BZ\\|_F^2\\\\\n\\mbox{subject to} & Z\\in \\mathcal P_n.\n\\end{array}\n\\EEQ\nIf the optimal value of this problem is zero, it means that $A$ and $B$\nare isomorphic. Otherwise, the solution of this problem minimizes the\ndisagreement of $ZAZ^T$ and $B$ in the Frobenius norm sense.\n\nSolving inexact graph isomorphism problems is of interest in pattern recognition\n\\cite{conte2004thirty, rocha1994shape}, computer vision\n\\cite{schellewald2001evaluation}, shape analysis\n\\cite{sebastian2004recognition, he2006object},\nimage and video indexing \\cite{lee2006graph}, and neuroscience\n\\cite{vogelstein2011large}.\nIn many of the aforementioned fields\ngraphs are used to represent geometric structures, and\n$\\|ZAZ^T-B\\|_F^2$\ncan be interpreted as the strength of geometric deformation.\n\n\\paragraph{Problem instances.}\nIt can be shown that if $A$ and $B$ are isomorphic and\n$A$ has distinct eigenvalues\nand for all\neigenvectors $v$ of $A$ for which $\\ones^T v \\neq 0$, then the relaxed\nproblem has a unique solution which is the permutation matrix that\nrelates $A$ and $B$ \\cite{aflalo2014graph}.\nHence, in order to generate harder problems,\nwe generated the matrix $A$ such that it violated these conditions.\nIn particular, we constructed $A$ for the Peterson graph ($3$-regular with $10$ vertices),\nicosahedral graph ($5$-regular with $12$ vertices),\nRamsey graph ($8$-regular with $17$ vertices),\ndodecahedral graph ($3$-regular with $20$ vertices),\nand the Tutte-Coxeter graph ($3$-regular with $30$ vertices).\nFor each example we randomly permuted the vertices to obtain two isomorphic graphs.\n\n\\paragraph{Results.}\nWe ran NC-ADMM with $20$ iterations and $5$ restarts.\nFor all of our examples NC-ADMM was able to find the permutation relating\nthe two graphs.\nIt is interesting to notice that running the algorithm multiple times\ncan find different solutions if there is more than one permutation\nrelating the two graphs.\nThe relax-round-polish heuristic failed to find a solution for all\nof the aforementioned problems.\n\n\\begin{incomplete}\n\\subsection{Phase retrieval}\n\nThe problem of recovery of a signal given the magnitude of its Fourier\ntransform\n\\cite{candes2015phase,shechtman2014phase}\narises in various fields of science and engineering, including\nX-ray crystallography \\cite{harrison1993phase,millane1990phase},\nmicroscopy \\cite{miao2008extending},\nastronomy \\cite{fienup1987phase},\ndiffraction and array imaging \\cite{bunk2007diffractive},\nand optics \\cite{walther1963question}.\nIn its most general form, the phase retrieval problem can be formulated as\nfinding $x\\in\\reals^n$ such that $|(Fx)_i|$ is given for $i=1,\\ldots,m$,\nwhere $F\\in\\complex^{m\\times n}$. (A special case is when $Fx$ represents\nthe Fourier transform of $x$.) Decomposing $F$ into its real and imaginary\npart by $F=F_R+iF_I$ for $F_R,F_I\\in\\reals^{m\\times n}$, we have the following\nformulation\n\\BEQ\n\\label{phase}\n\\begin{array}{ll}\n\\mbox{find} & x\\\\\n\\mbox{subject to} & F_Rx = u\\\\\n& F_Ix=v\\\\\n&u_i^2+v_i^2=y_i^2\\quad i=1,\\ldots,m,\n\\end{array}\n\\EEQ\nwith decision variable $x,u,v$ and problem data $F,y$.\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{incomplete}\n\n\\begin{incomplete}\n\\subsection{Sudoku}\nA Sudoku puzzle is a partially completed $N\\times N$ grid of cells divided\ninto N segments. The goal is to fill the grid using a prescribed set of\n$N$ distinct symbols such that each element of the set appears exactly\nonce in each row, column, and segment. We define $N^3$ binary variables\n$z_{ijk}$ for $i,j,k=1,\\ldots, n$ where $z_{ijk}=1$ if and only the element $(i,j)$\nis filled with $k$. The problem then can be formulated as\n", "index": 71, "text": "\n\\[\n\\label{sudoku}\n\\begin{array}{ll}\n\\mbox{find} & z_{ijk}\\quad1\\leq i,j,k \\leq n\\\\\n\\mbox{subject to} &\\sum_{k} z_{ijk} = 1 \\quad 1\\leq i,j \\leq n\\\\\n&\\sum_{i} z_{ijk} = 1 \\quad 1\\leq j,k \\leq n\\\\\n&\\sum_{j} z_{ijk} = 1 \\quad 1\\leq i,k \\leq n\\\\\n&\\sum_{k} z_{ijk} = 1 \\quad 1\\leq i,j \\leq n\\\\\n&\\sum_{(i,j)\\in\\mathcal I} z_{ijk} = 1 \\quad \\mathcal I~\\mbox{is a region}\\\\\n&z_{ijk}=1 \\quad A_{ij}=k\n\\end{array}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{ll}\\mbox{find}&amp;z_{ijk}\\quad 1\\leq i,j,k\\leq n\\\\&#10;\\mbox{subject to}&amp;\\sum_{k}z_{ijk}=1\\quad 1\\leq i,j\\leq n\\\\&#10;&amp;\\sum_{i}z_{ijk}=1\\quad 1\\leq j,k\\leq n\\\\&#10;&amp;\\sum_{j}z_{ijk}=1\\quad 1\\leq i,k\\leq n\\\\&#10;&amp;\\sum_{k}z_{ijk}=1\\quad 1\\leq i,j\\leq n\\\\&#10;&amp;\\sum_{(i,j)\\in\\mathcal{I}}z_{ijk}=1\\quad\\mathcal{I}~{}\\mbox{is a region}\\\\&#10;&amp;z_{ijk}=1\\quad A_{ij}=k\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mtext>find</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mn>1</mn></mrow><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mo>\u2264</mo><mi>n</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mtext>subject to</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>\u2264</mo><mi>n</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2264</mo><mi>n</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>k</mi><mo>\u2264</mo><mi>n</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>\u2264</mo><mi>n</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mrow></munder><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mrow><mn>1</mn><mo separator=\"true\">\u2003</mo><mrow><mpadded width=\"+3.3pt\"><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi></mpadded><mo>\u2062</mo><mtext>is a region</mtext></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><msub><mi>A</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>k</mi></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}]