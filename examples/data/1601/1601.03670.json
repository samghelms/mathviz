[{"file": "1601.03670.tex", "nexttext": "\nMoreover $K(p,q) = \\sum_{j=1}^\\infty \\kappa_j\\psi_j(p)\\psi_j(q)$ for each $p,q \\in \\mathcal{M}$. Thus $X$ can be expanded as $X = \\mu + \\sum_{j=1}^\\infty \\varepsilon_j \\psi_j$, where the random variables $\\varepsilon_1,\\varepsilon_2,\\ldots$ are uncorrelated and are given by $\\varepsilon_j = \\int_{\\mathcal{M}} \\{X(p) - \\mu(p) \\}\\psi_j(p)dp$. This is also known as the Karhunen-Lo\\`{e}ve (KL) expansion of $X$.\n\nThe collection $(\\psi_j)$ defines the strongest modes of variation in the random function $X$ and are called Principal Component (PC) functions. In fact $\\psi_1$ is such that\n\n", "itemtype": "equation", "pos": 6992, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nMotivated by the analysis of high-dimensional neuroimaging signals located over the cortical surface, we introduce a novel Principal Component Analysis technique that can handle functional data located over a two-dimensional manifold. For this purpose a regularization approach is adopted, introducing a smoothing penalty coherent with the geodesic distance over the manifold. The model introduced can be applied to any manifold topology, can naturally handle missing data and functional samples evaluated in different grids of points. We approach the discretization task by means of finite element analysis and propose an efficient iterative algorithm for its resolution. We compare the performances of the proposed algorithm with other approaches classically adopted in literature. We finally apply the proposed algorithm to resting state functional magnetic resonance imaging data, showing that a cross-validation approach justifies the presence of the penalization term in the analysis of human brain connectome data.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\\label{sec:intro}\n\nThe recent growth of data arising from neuroimaging has led to profound changes in the understanding of the brain. Neuroimaging is a multidisciplinary activity and the role of statistics in its success should not be underestimated. Much of the work to date has been to determine how to use statistical models in high-dimensional settings that arise out of such imaging modalities as functional Magnetic Resonance Imaging (fMRI) and Electroencephalography (EEG). However, it is becoming increasingly clear that there is now a need to incorporate more and more complex information about brain structure and function into the statistical analysis in order to take understanding of the brain to the next level.\n\nConsiderable amounts of the brain signal captured, for example, by fMRI arise from the cerebral cortex. The cerebral cortex is the highly convoluted thin sheet where most neural activity is focused. It is natural to represent this thin sheet as a 2D surface embedded in a 3D space, structured with a 2D geodesic distance, rather than the 3D Euclidean distance within the volume. In fact, functionally distinct areas may be close to each other if measured with Euclidean distance, but due to the highly convoluted morphology of the cerebral cortex, their 2D geodesic distance along the cortical surface can be far greater. While early approaches to the analysis of hemodynamic signals ignore the morphology of the cortical surface, it has now been well established [\\cite{glasser2013} and references therein] that is beneficial to analyze neuroimaging data through the processing of the signals on the cortical surface using surface-constrained techniques. Classical tools such as non-parametric smoothing models have already been adapted to deal with this kind of data, see e.g. \\cite{chung2014}.\n\nThe goal of the present paper is to introduce a novel Principal Component Analysis (PCA) technique suitable for working with functional signals distributed over curved domains and specifically over two-dimensional smooth Riemannian manifolds, such as the cortical surface.\n\nThe cortical surface can be extracted from structural Magnetic Resonance Imaging (MRI), a non-invasive scanning technique to visualize the internal structure of the brain, rendering it as a 3D image with high spatial resolution. The signal of interest, which we want to analyse with respect to the surface, comes from fMRI, which detects a Blood Oxygen Level Dependent (BOLD) signal [\\cite{Ogawa90}] as a series of repeated measurements in time, yielding a time series of 3D images. An increased neural activity in a particular area of the brain causes an increased demand for oxygen. As the fMRI signal is related to changes in the relative ratio of oxy- to deoxy-hemoglobin, due to their differing magnetic properties, the signal captured within an fMRI scan is considered to be a surrogate for neural activity and is used to produce activation maps or investigate brain functional connectivity. The fMRI signal of each individual related to the neuronal activity in the cerebral cortex is generally mapped on a common template cortical surface, to allow multi-subject statistical analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nIn this paper, in particular, we will focus our attention on functional connectivity (FC). FC maps, on the cortical surface, can be constructed computing the pairwise correlation between all vertex's fMRI time-series and the mean time-series of a region of interest. \nThis will result in one FC map for each subject that provides a clear view of areas to which the region of interest is functionally connected.\n\nIn practice, the template cortical surface is represented by a triangulated surface that can be considered a discrete approximation of the underlying smooth compact two-dimensional Riemannian manifold $\\mathcal{M} \\subset \\mathbb{R}^3$ modelling the cortical surface. Each resting state functional connectivity map can be represented by a function $x_i: \\mathcal{M} \\rightarrow \\mathbb{R}$.\nOnce we have the correlation maps on the cortical surface we want to study how the phenomena varies from subject to subject. A statistical technique that enables to do that is PCA.  It is natural to contextualize this task in the framework of Functional Data Analysis [\\cite{ramsay2005}].\n\nThe rest of the article is organized as follows. In Section~\\ref{sec:FPCA} we establish the formal theoretical properties of Functional PCA (FPCA) to the case of random functions whose domain is a manifold $\\mathcal{M}$. In Section~\\ref{sec:SM-FPCA} we introduce a novel FPCA model and propose an algorithm for its resolution. We then give some simulation results in Section~\\ref{sec:simulations}, indicating the performance of our methodology, as compared to other methods in literature. We then return to the FC maps example in Section~\\ref{sec:app}, to consider how the surface based PCA analysis might be used in this case and make some concluding remarks in Section~\\ref{sec:discussion}.\n\n\\section{Functional principal component analysis}\\label{sec:FPCA}\nConsider the space of square integrable functions on $\\mathcal{M}$: $L^2(\\mathcal{M}) = \\{f : \\mathcal{M} \\rightarrow \\mathbb{R}: \\int_{\\mathcal{M}} |f(p)|^2 dp < \\infty\\}$ with the inner product ${\\langle f,g \\rangle}_\\mathcal{M} = \\int_{\\mathcal{M}} f(p)g(p) dp$ and norm $\\|f\\|_{\\mathcal{M}}=\\int_{\\mathcal{M}} |f(p)|^2 dp$. Consider the random variable $X:\\Omega \\rightarrow L^2(\\mathcal{M})$, with mean $\\mu = \\mathbb{E}[X]$ and a finite second moment, i.e. $\\int_{\\mathcal{M}} \\mathbb{E}[X^2] < \\infty $. Moreover assume that its covariance function $K(p, q) = \\mathbb{E}[(X(p)-\\mu(p))(X(q)-\\mu(q))]$ is square integrable. Mercer's Lemma [\\cite{riesz1955}] guarantees the existence of a non-increasing sequence $(\\kappa_j)$ of eigenvalues of $K$ and an orthonormal sequence of corresponding eigenfunctions $(\\psi_j)$, such that\n\n", "index": 1, "text": "\\begin{equation}\n\\int_{\\mathcal{M}} K(p,q) \\psi_j(p) dp = \\kappa_j \\psi_j(q), \\qquad \\forall q \\in \\mathcal{M}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\int_{\\mathcal{M}}K(p,q)\\psi_{j}(p)dp=\\kappa_{j}\\psi_{j}(q),\\qquad\\forall q\\in%&#10;\\mathcal{M}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>p</mi></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03ba</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>q</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhile $\\psi_m$, for $m>1$, solves an analogous problem with the added constraint of $\\psi_m$ being orthogonal to the previous $m-1$ functions $\\psi_1, \\ldots, \\psi_{m-1}$, i.e.\n\n", "itemtype": "equation", "pos": 7706, "prevtext": "\nMoreover $K(p,q) = \\sum_{j=1}^\\infty \\kappa_j\\psi_j(p)\\psi_j(q)$ for each $p,q \\in \\mathcal{M}$. Thus $X$ can be expanded as $X = \\mu + \\sum_{j=1}^\\infty \\varepsilon_j \\psi_j$, where the random variables $\\varepsilon_1,\\varepsilon_2,\\ldots$ are uncorrelated and are given by $\\varepsilon_j = \\int_{\\mathcal{M}} \\{X(p) - \\mu(p) \\}\\psi_j(p)dp$. This is also known as the Karhunen-Lo\\`{e}ve (KL) expansion of $X$.\n\nThe collection $(\\psi_j)$ defines the strongest modes of variation in the random function $X$ and are called Principal Component (PC) functions. In fact $\\psi_1$ is such that\n\n", "index": 3, "text": "\\begin{equation*}\n\\psi_1 = {\\operatornamewithlimits{argmax}}_{\\phi:\\|\\phi\\|_{\\mathcal{M}}=1}\\int_{\\mathcal{M}}\\int_{\\mathcal{M}}\\phi(p)K(p,q)\n\\phi(q)dpdq,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\psi_{1}={\\operatornamewithlimits{argmax}}_{\\phi:\\|\\phi\\|_{\\mathcal{M}}=1}\\int%&#10;_{\\mathcal{M}}\\int_{\\mathcal{M}}\\phi(p)K(p,q)\\phi(q)dpdq,\" display=\"block\"><mrow><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>=</mo><mrow><munder><mo movablelimits=\"false\">argmax</mo><mrow><mi>\u03d5</mi><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\u03d5</mi><mo>\u2225</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>=</mo><mn>1</mn></mrow></mrow></munder><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>q</mi></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nThe random variables $\\varepsilon_1,\\varepsilon_2,\\ldots$ are usually called PC scores.\n\nAnother important property of PC functions is the best $M$ basis approximation. In fact, for any fixed $M \\in \\mathbb{N}$, the first $M$ PC functions of $X$ satisfies\n\n", "itemtype": "equation", "pos": 8054, "prevtext": "\nwhile $\\psi_m$, for $m>1$, solves an analogous problem with the added constraint of $\\psi_m$ being orthogonal to the previous $m-1$ functions $\\psi_1, \\ldots, \\psi_{m-1}$, i.e.\n\n", "index": 5, "text": "\\begin{equation*}\n\\psi_m = {\\operatornamewithlimits{argmax}}_{\\scriptsize \\begin{array}{clc}\\phi:\\|\\phi\\|_{\\mathcal{M}}=1 \\\\ \\langle \\phi, \\psi_j \\rangle_{\\mathcal{M}} = 0 \\quad j = 1,\\ldots,m-1 \\end{array}} \\int_{\\mathcal{M}}\\int_{\\mathcal{M}}\\phi(p)K(p,q)\n\\phi(q)dpdq.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\psi_{m}={\\operatornamewithlimits{argmax}}_{\\scriptsize\\begin{array}[]{clc}%&#10;\\phi:\\|\\phi\\|_{\\mathcal{M}}=1\\\\&#10;\\langle\\phi,\\psi_{j}\\rangle_{\\mathcal{M}}=0\\quad j=1,\\ldots,m-1\\end{array}}%&#10;\\int_{\\mathcal{M}}\\int_{\\mathcal{M}}\\phi(p)K(p,q)\\phi(q)dpdq.\" display=\"block\"><mrow><mrow><msub><mi>\u03c8</mi><mi>m</mi></msub><mo>=</mo><mrow><munder><mo movablelimits=\"false\">argmax</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03d5</mi><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>\u03d5</mi><mo>\u2225</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>=</mo><mn>1</mn></mrow></mrow></mtd><mtd/><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03d5</mi><mo>,</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>=</mo><mn>0</mn></mrow><mo separator=\"true\">\u2003</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mrow></mrow></mrow></mtd><mtd/><mtd/></mtr></mtable></munder><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>q</mi></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere $\\delta_{ml}$ is the Kronecker delta, such that $\\delta_{ml}=1$ for $m=l$ and $0$ otherwise.\n\nSuppose $x_1, \\ldots , x_n$ are $n$ smooth samples from $X$. Usually, for each of these functions, only noisy evaluations $x_i(p_j)$ on a fixed discrete grid of points $p_1,\\ldots,p_s$ are given. In this setting, we will now recall the two standard approaches to FPCA. The pre-smoothing approach and the regularized PCA approach.\n\nThe pre-smoothing approach is based on the two following steps. In the first step the observations associated to each function are smoothed, in order to obtain smooth representations of $x_1, \\ldots , x_n$. Then the sample mean $\\bar{x} = n^{-1} \\sum_i x_i$ and the sample covariance $\\hat{K}(p, q) = \\frac{1}{n} \\sum_{i=1}^n(x_i(p)-\\bar{x}(p))(x_i(q)-\\bar{x}(q))$ are used to estimate $\\mu$ and $K$ respectively. Applying the orthonormal basis expansion to $\\hat{K}$ is then possible to write $\\hat{K}(p,q) = \\sum_{j=1}^\\infty \\hat{\\kappa}_j \\hat{\\psi}_j(p)\\hat{\\psi}_j(q)$ with $p,q \\in \\mathcal{M}$.\nThe sequence $\\hat{\\psi_1},\\hat{\\psi_2},\\ldots$ is usually treated as an approximation of $\\psi_1,\\psi_2,\\ldots$. The estimates $\\hat{\\psi_1},\\hat{\\psi_2},\\ldots$ are computed through the characterization $\\int_{\\mathcal{M}} \\hat{K}(p,q) \\hat{\\psi}_j(p) dp = \\hat{\\kappa}_j \\hat{\\psi}_j(q)$, which is solved by the discretization of the problem on a fine grid or by the basis expansion of estimated smooth functions. In the case where the domain is an interval of the real line, a theoretical study on the accuracy of $\\hat{\\psi_j}$ as an estimate of $\\psi_j$ is offered for example in \\cite{hall2006}.\n\n\n\n\n\n\n\n\nDefine the $n \\times s$ matrix ${\\boldsymbol{\\mathbf{{X}}}} = (x_i(p_j))$, the column vector ${\\boldsymbol{\\mathbf{{\\mu}}}} = (\\frac{1}{n}\\sum_{i=1}^n x_i(p_j))$ of length $s$, the $n \\times M$ matrix ${\\boldsymbol{\\mathbf{{A}}}} = (\\langle X_i, \\phi_m \\rangle)$ and the $s \\times M$ matrix ${\\boldsymbol{\\mathbf{{\\Phi}}}} = (\\phi_m(p_j))$. Let ${\\boldsymbol{\\mathbf{{1}}}}$ denote the column vector of length $n$ with all entries equal to $1$. The empirical counterpart of the objective function in (\\ref{eq:minimization}) becomes\n\n", "itemtype": "equation", "pos": 8597, "prevtext": "\nThe random variables $\\varepsilon_1,\\varepsilon_2,\\ldots$ are usually called PC scores.\n\nAnother important property of PC functions is the best $M$ basis approximation. In fact, for any fixed $M \\in \\mathbb{N}$, the first $M$ PC functions of $X$ satisfies\n\n", "index": 7, "text": "\\begin{equation}\\label{eq:minimization}\n{(\\psi_i)}_{m=1}^M =\n{\\operatornamewithlimits{argmin}}_{(\\{\\phi_m\\}_{m=1}^M: \\langle \\phi_m, \\phi_l \\rangle = \\delta_{ml})}\n\\mathbb{E}\\int_{\\mathcal{M}}\\bigg\\{X-\\mu - \\sum_{m=1}^M \\langle X, \\phi_m \\rangle \\phi_m \\bigg\\}^2,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{(\\psi_{i})}_{m=1}^{M}={\\operatornamewithlimits{argmin}}_{(\\{\\phi_{m}\\}_{m=1}^%&#10;{M}:\\langle\\phi_{m},\\phi_{l}\\rangle=\\delta_{ml})}\\mathbb{E}\\int_{\\mathcal{M}}%&#10;\\bigg{\\{}X-\\mu-\\sum_{m=1}^{M}\\langle X,\\phi_{m}\\rangle\\phi_{m}\\bigg{\\}}^{2},\" display=\"block\"><mrow><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c8</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmin</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>\u03d5</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mo>:</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>\u03d5</mi><mi>m</mi></msub><mo>,</mo><msub><mi>\u03d5</mi><mi>l</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>=</mo><msub><mi>\u03b4</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></munder><mo>\u2061</mo><mi>\ud835\udd3c</mi></mrow><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">{</mo><mrow><mi>X</mi><mo>-</mo><mi>\u03bc</mi><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mi>X</mi><mo>,</mo><msub><mi>\u03d5</mi><mi>m</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msub><mi>\u03d5</mi><mi>m</mi></msub></mrow></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">}</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere $\\| \\cdot \\|_F$ is the Frobenius norm, defined as the square root of the sum of the squares of its elements.\nThis last formulation gives a natural way to deal with the fact that only pointwise and noisy evaluations $x_i(p_j), \\, i=1,\\ldots,n, j=1,\\ldots,s$ of the underlying functional samples are usually available. However, it does not incorporate any information on the smoothness of the functional data. In fact, considering the Singular Value Decomposition (SVD) of ${\\boldsymbol{\\mathbf{{X}}}} - {\\boldsymbol{\\mathbf{{1}}}} {\\boldsymbol{\\mathbf{{\\mu}}}}^T = {\\boldsymbol{\\mathbf{{U}}}} {\\boldsymbol{\\mathbf{{D}}}} {\\boldsymbol{\\mathbf{{V}}}}^T$, it can be shown that the minimizing arguments of (\\ref{eq:SVD}) are ${\\boldsymbol{\\mathbf{{\\hat{\\Phi}}}}} = {\\boldsymbol{\\mathbf{{V}}}}$ and ${\\boldsymbol{\\mathbf{{\\hat{A}}}}} = {\\boldsymbol{\\mathbf{{U}}}} {\\boldsymbol{\\mathbf{{D}}}}$, thus the obtained formulation is a multivariate PCA applied to the data-matrix ${\\boldsymbol{\\mathbf{{X}}}}$.\n\nThe regularized PCA approach consists on adding a penalization term to the classic formulation of the PCA, in order to recover a desired feature of the estimated underlying functions. In particular the formulation (\\ref{eq:SVD}) has shown a great flexibility for this purpose. Examples of models where a sparseness property is assumed on the data are offered for instance in \\cite{jolliffe2003,zou2005,shen2008}. In the specific case of functional data analysis the penalization term usually encourages the PC functions to be smooth.  Examples of PCA models that explicitely incorporates a smoothing penalization term are given by \\cite{rice1991,silverman1996,huang2008}. The cited works deal with functions whose domain is a limited interval in $\\mathbb{R}$. \\cite{zhou2014} recently proposed a smooth FPCA for two-dimensional functions on irregular planar domains. Their approach is based on a mixed effects model that specifies the PC functions as bivariate splines on triangulations and the PC scores as random effects. Here we propose a FPCA model that can handle real functions observable on a two-dimensional manifold. In particular we shall consider a smoothing penalty operator, coherent with the 2D geodesic distances on the manifold. This leads to the definition of a model that can fully exploit the information about the geometry of the manifold.\n\n\\section{Smooth FPCA over two-dimensional manifolds}\\label{sec:SM-FPCA}\nSuppose the sample of $n$ functions $x_i: \\mathcal{M} \\rightarrow \\mathbb{R}$ is observed at a fixed set of points ${p_1, \\ldots, p_s}$ in $\\mathcal{M}$ (this will be relaxed later). Let $f$ be a real valued and twice differentiable function and ${\\boldsymbol{\\mathbf{{u}}}} = \\{u_i\\}_{i=1,\\ldots,n}$ a n-dimensional real column vector. We propose to estimate the first PC function $\\hat{f}:\\mathcal{M}\\rightarrow\\mathbb{R}$ and the associated PC scores vector $\\hat{{\\boldsymbol{\\mathbf{{u}}}}}$, by solving the equation:\n\n", "itemtype": "equation", "pos": 11053, "prevtext": "\nwhere $\\delta_{ml}$ is the Kronecker delta, such that $\\delta_{ml}=1$ for $m=l$ and $0$ otherwise.\n\nSuppose $x_1, \\ldots , x_n$ are $n$ smooth samples from $X$. Usually, for each of these functions, only noisy evaluations $x_i(p_j)$ on a fixed discrete grid of points $p_1,\\ldots,p_s$ are given. In this setting, we will now recall the two standard approaches to FPCA. The pre-smoothing approach and the regularized PCA approach.\n\nThe pre-smoothing approach is based on the two following steps. In the first step the observations associated to each function are smoothed, in order to obtain smooth representations of $x_1, \\ldots , x_n$. Then the sample mean $\\bar{x} = n^{-1} \\sum_i x_i$ and the sample covariance $\\hat{K}(p, q) = \\frac{1}{n} \\sum_{i=1}^n(x_i(p)-\\bar{x}(p))(x_i(q)-\\bar{x}(q))$ are used to estimate $\\mu$ and $K$ respectively. Applying the orthonormal basis expansion to $\\hat{K}$ is then possible to write $\\hat{K}(p,q) = \\sum_{j=1}^\\infty \\hat{\\kappa}_j \\hat{\\psi}_j(p)\\hat{\\psi}_j(q)$ with $p,q \\in \\mathcal{M}$.\nThe sequence $\\hat{\\psi_1},\\hat{\\psi_2},\\ldots$ is usually treated as an approximation of $\\psi_1,\\psi_2,\\ldots$. The estimates $\\hat{\\psi_1},\\hat{\\psi_2},\\ldots$ are computed through the characterization $\\int_{\\mathcal{M}} \\hat{K}(p,q) \\hat{\\psi}_j(p) dp = \\hat{\\kappa}_j \\hat{\\psi}_j(q)$, which is solved by the discretization of the problem on a fine grid or by the basis expansion of estimated smooth functions. In the case where the domain is an interval of the real line, a theoretical study on the accuracy of $\\hat{\\psi_j}$ as an estimate of $\\psi_j$ is offered for example in \\cite{hall2006}.\n\n\n\n\n\n\n\n\nDefine the $n \\times s$ matrix ${\\boldsymbol{\\mathbf{{X}}}} = (x_i(p_j))$, the column vector ${\\boldsymbol{\\mathbf{{\\mu}}}} = (\\frac{1}{n}\\sum_{i=1}^n x_i(p_j))$ of length $s$, the $n \\times M$ matrix ${\\boldsymbol{\\mathbf{{A}}}} = (\\langle X_i, \\phi_m \\rangle)$ and the $s \\times M$ matrix ${\\boldsymbol{\\mathbf{{\\Phi}}}} = (\\phi_m(p_j))$. Let ${\\boldsymbol{\\mathbf{{1}}}}$ denote the column vector of length $n$ with all entries equal to $1$. The empirical counterpart of the objective function in (\\ref{eq:minimization}) becomes\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:SVD}\n\\frac{1}{n} \\| {\\boldsymbol{\\mathbf{{X}}}} - {\\boldsymbol{\\mathbf{{1}}}} {\\boldsymbol{\\mathbf{{\\mu}}}}^T - {\\boldsymbol{\\mathbf{{A}}}} {\\boldsymbol{\\mathbf{{\\Phi}}}}^T \\|_F^2,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{n}\\|{\\boldsymbol{\\mathbf{{X}}}}-{\\boldsymbol{\\mathbf{{1}}}}{%&#10;\\boldsymbol{\\mathbf{{\\mu}}}}^{T}-{\\boldsymbol{\\mathbf{{A}}}}{\\boldsymbol{%&#10;\\mathbf{{\\Phi}}}}^{T}\\|_{F}^{2},\" display=\"block\"><mrow><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><mrow><mn>\ud835\udfcf</mn><mo>\u2062</mo><msup><mi>\ud835\udf41</mi><mi>T</mi></msup></mrow><mo>-</mo><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><msup><mi>\ud835\udebd</mi><mi>T</mi></msup></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere $\\Delta_{\\mathcal{M}}$ is the Laplace-Beltrami operator for functions defined over the manifold $\\mathcal{M}$.\nThe Laplace-Beltrami operator is a generalization of the standard Laplacian to the case of functions defined on surfaces in Euclidean spaces. It is related to the local curvature of $f$ on $\\mathcal{M}$. The parameter $\\lambda$ controls the trade-off between the empirical term of the objective function and roughness penalizing term. The ${\\boldsymbol{\\mathbf{{u}}}}^T{\\boldsymbol{\\mathbf{{u}}}}$ term is justified by some invariance considerations on the objective function as done in the case of one dimensional domains, in \\cite{huang2008}. \nConsider the transformation $({\\boldsymbol{\\mathbf{{u}}}} \\rightarrow c{\\boldsymbol{\\mathbf{{u}}}}, f \\rightarrow \\frac{1}{c}f)$, with $c$ a constant, and the transformation $({\\boldsymbol{\\mathbf{{X}}}} \\rightarrow c {\\boldsymbol{\\mathbf{{X}}}}, {\\boldsymbol{\\mathbf{{u}}}} \\rightarrow c{\\boldsymbol{\\mathbf{{u}}}})$, where ${\\boldsymbol{\\mathbf{{X}}}} = (x_i(p_j))$. Then the objective function in (\\ref{eq:model}) is invariant with respect to the first transformation, while the empirical and the smoothness terms are re-scaled by the same coefficient with the second transformation.\n\nThe subsequent PCs can be extracted sequentially by removing the preceding estimated components from the data matrix ${\\boldsymbol{\\mathbf{{X}}}}$. This allows the selection of a different penalization parameter $\\lambda$ for each PC estimate. We will refer to this model as Smooth-Manifold FPCA (SM-FPCA).\n\n\\subsection{Iterative algorithm}\n\nHere we present the numerical algorithm for the resolution of the model introduced above. Our approach for the minimization of the functional (\\ref{eq:model}) can be summarized in the following two steps:\n\\begin{itemize}\n\\item Splitting the optimization in a finite dimensional optimization in ${\\boldsymbol{\\mathbf{{u}}}}$ and an infinite-dimensional optimization in $f$;\n\\item Approximating the infinite-dimensional solution thanks to a Surface Finite Element discretization.\n\\end{itemize}\n\n\nLet ${\\boldsymbol{\\mathbf{{f}}}}_s$ be the vector of length $s$ such that ${\\boldsymbol{\\mathbf{{f}}}}_s = (f(p_1), \\ldots, f(p_s))^T$. The expression in (\\ref{eq:model}) can be rewritten as\n\n", "itemtype": "equation", "pos": 14236, "prevtext": "\nwhere $\\| \\cdot \\|_F$ is the Frobenius norm, defined as the square root of the sum of the squares of its elements.\nThis last formulation gives a natural way to deal with the fact that only pointwise and noisy evaluations $x_i(p_j), \\, i=1,\\ldots,n, j=1,\\ldots,s$ of the underlying functional samples are usually available. However, it does not incorporate any information on the smoothness of the functional data. In fact, considering the Singular Value Decomposition (SVD) of ${\\boldsymbol{\\mathbf{{X}}}} - {\\boldsymbol{\\mathbf{{1}}}} {\\boldsymbol{\\mathbf{{\\mu}}}}^T = {\\boldsymbol{\\mathbf{{U}}}} {\\boldsymbol{\\mathbf{{D}}}} {\\boldsymbol{\\mathbf{{V}}}}^T$, it can be shown that the minimizing arguments of (\\ref{eq:SVD}) are ${\\boldsymbol{\\mathbf{{\\hat{\\Phi}}}}} = {\\boldsymbol{\\mathbf{{V}}}}$ and ${\\boldsymbol{\\mathbf{{\\hat{A}}}}} = {\\boldsymbol{\\mathbf{{U}}}} {\\boldsymbol{\\mathbf{{D}}}}$, thus the obtained formulation is a multivariate PCA applied to the data-matrix ${\\boldsymbol{\\mathbf{{X}}}}$.\n\nThe regularized PCA approach consists on adding a penalization term to the classic formulation of the PCA, in order to recover a desired feature of the estimated underlying functions. In particular the formulation (\\ref{eq:SVD}) has shown a great flexibility for this purpose. Examples of models where a sparseness property is assumed on the data are offered for instance in \\cite{jolliffe2003,zou2005,shen2008}. In the specific case of functional data analysis the penalization term usually encourages the PC functions to be smooth.  Examples of PCA models that explicitely incorporates a smoothing penalization term are given by \\cite{rice1991,silverman1996,huang2008}. The cited works deal with functions whose domain is a limited interval in $\\mathbb{R}$. \\cite{zhou2014} recently proposed a smooth FPCA for two-dimensional functions on irregular planar domains. Their approach is based on a mixed effects model that specifies the PC functions as bivariate splines on triangulations and the PC scores as random effects. Here we propose a FPCA model that can handle real functions observable on a two-dimensional manifold. In particular we shall consider a smoothing penalty operator, coherent with the 2D geodesic distances on the manifold. This leads to the definition of a model that can fully exploit the information about the geometry of the manifold.\n\n\\section{Smooth FPCA over two-dimensional manifolds}\\label{sec:SM-FPCA}\nSuppose the sample of $n$ functions $x_i: \\mathcal{M} \\rightarrow \\mathbb{R}$ is observed at a fixed set of points ${p_1, \\ldots, p_s}$ in $\\mathcal{M}$ (this will be relaxed later). Let $f$ be a real valued and twice differentiable function and ${\\boldsymbol{\\mathbf{{u}}}} = \\{u_i\\}_{i=1,\\ldots,n}$ a n-dimensional real column vector. We propose to estimate the first PC function $\\hat{f}:\\mathcal{M}\\rightarrow\\mathbb{R}$ and the associated PC scores vector $\\hat{{\\boldsymbol{\\mathbf{{u}}}}}$, by solving the equation:\n\n", "index": 11, "text": "\\begin{equation}\\label{eq:model}\n(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f}) = {\\operatornamewithlimits{argmin}} \\limits_{{\\boldsymbol{\\mathbf{{u}}}},f} \\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^s (x_i(p_j) - u_i f(p_j))^2 + \\lambda {\\boldsymbol{\\mathbf{{u}}}}^T{\\boldsymbol{\\mathbf{{u}}}} \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f})={\\operatornamewithlimits{argmin}}%&#10;\\limits_{{\\boldsymbol{\\mathbf{{u}}}},f}\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^%&#10;{s}(x_{i}(p_{j})-u_{i}f(p_{j}))^{2}+\\lambda{\\boldsymbol{\\mathbf{{u}}}}^{T}{%&#10;\\boldsymbol{\\mathbf{{u}}}}\\int_{\\mathcal{M}}\\!\\Delta^{2}_{\\mathcal{M}}f\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc2e</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmin</mo><mrow><mi>\ud835\udc2e</mi><mo>,</mo><mi>f</mi></mrow></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\ud835\udc2e</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nA normalization constraint must be considered in this minimization problem to make the representation unique, as in fact multiplying ${\\boldsymbol{\\mathbf{{u}}}}$ by a constant and dividing $f$ by the same constant does not change the objective function. For simplicity of implementation, we here optimize in ${\\boldsymbol{\\mathbf{{u}}}}$ under the constraint $\\|{\\boldsymbol{\\mathbf{{u}}}}\\|_2 = 1$, and leave the infinite-dimensional optimization in $f$ unconstrained.\n\nOur proposal for the minimization of the criterion (\\ref{eq:model_matricial}) is to alternate the minimization of ${\\boldsymbol{\\mathbf{{u}}}}$ and $f$ in an iterative algorithm:\n\n\\begin{enumerate}[label=\\textit{Step} \\arabic*,align=left, leftmargin=1.0cm]\n\n\\step Estimation of ${\\boldsymbol{\\mathbf{{u}}}}$ given $f$. For a given $f$, the minimizing ${\\boldsymbol{\\mathbf{{u}}}}$ of the objective function in (\\ref{eq:model_matricial}) is\n\n", "itemtype": "equation", "pos": 16864, "prevtext": "\nwhere $\\Delta_{\\mathcal{M}}$ is the Laplace-Beltrami operator for functions defined over the manifold $\\mathcal{M}$.\nThe Laplace-Beltrami operator is a generalization of the standard Laplacian to the case of functions defined on surfaces in Euclidean spaces. It is related to the local curvature of $f$ on $\\mathcal{M}$. The parameter $\\lambda$ controls the trade-off between the empirical term of the objective function and roughness penalizing term. The ${\\boldsymbol{\\mathbf{{u}}}}^T{\\boldsymbol{\\mathbf{{u}}}}$ term is justified by some invariance considerations on the objective function as done in the case of one dimensional domains, in \\cite{huang2008}. \nConsider the transformation $({\\boldsymbol{\\mathbf{{u}}}} \\rightarrow c{\\boldsymbol{\\mathbf{{u}}}}, f \\rightarrow \\frac{1}{c}f)$, with $c$ a constant, and the transformation $({\\boldsymbol{\\mathbf{{X}}}} \\rightarrow c {\\boldsymbol{\\mathbf{{X}}}}, {\\boldsymbol{\\mathbf{{u}}}} \\rightarrow c{\\boldsymbol{\\mathbf{{u}}}})$, where ${\\boldsymbol{\\mathbf{{X}}}} = (x_i(p_j))$. Then the objective function in (\\ref{eq:model}) is invariant with respect to the first transformation, while the empirical and the smoothness terms are re-scaled by the same coefficient with the second transformation.\n\nThe subsequent PCs can be extracted sequentially by removing the preceding estimated components from the data matrix ${\\boldsymbol{\\mathbf{{X}}}}$. This allows the selection of a different penalization parameter $\\lambda$ for each PC estimate. We will refer to this model as Smooth-Manifold FPCA (SM-FPCA).\n\n\\subsection{Iterative algorithm}\n\nHere we present the numerical algorithm for the resolution of the model introduced above. Our approach for the minimization of the functional (\\ref{eq:model}) can be summarized in the following two steps:\n\\begin{itemize}\n\\item Splitting the optimization in a finite dimensional optimization in ${\\boldsymbol{\\mathbf{{u}}}}$ and an infinite-dimensional optimization in $f$;\n\\item Approximating the infinite-dimensional solution thanks to a Surface Finite Element discretization.\n\\end{itemize}\n\n\nLet ${\\boldsymbol{\\mathbf{{f}}}}_s$ be the vector of length $s$ such that ${\\boldsymbol{\\mathbf{{f}}}}_s = (f(p_1), \\ldots, f(p_s))^T$. The expression in (\\ref{eq:model}) can be rewritten as\n\n", "index": 13, "text": "\\begin{equation}\\label{eq:model_matricial}\n(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f}) = {\\operatornamewithlimits{argmin}} \\limits_{{\\boldsymbol{\\mathbf{{u}}}},f} \\|{\\boldsymbol{\\mathbf{{X}}}} - {\\boldsymbol{\\mathbf{{u}}}} {\\boldsymbol{\\mathbf{{f}}}}^T_s \\|_F^2 + \\lambda {\\boldsymbol{\\mathbf{{u}}}}^T{\\boldsymbol{\\mathbf{{u}}}} \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f})={\\operatornamewithlimits{argmin}}%&#10;\\limits_{{\\boldsymbol{\\mathbf{{u}}}},f}\\|{\\boldsymbol{\\mathbf{{X}}}}-{%&#10;\\boldsymbol{\\mathbf{{u}}}}{\\boldsymbol{\\mathbf{{f}}}}^{T}_{s}\\|_{F}^{2}+%&#10;\\lambda{\\boldsymbol{\\mathbf{{u}}}}^{T}{\\boldsymbol{\\mathbf{{u}}}}\\int_{%&#10;\\mathcal{M}}\\!\\Delta^{2}_{\\mathcal{M}}f.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc2e</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmin</mo><mrow><mi>\ud835\udc2e</mi><mo>,</mo><mi>f</mi></mrow></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17</mi><mo>-</mo><msubsup><mi>\ud835\udc2e\ud835\udc1f</mi><mi>s</mi><mi>T</mi></msubsup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\ud835\udc2e</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nand the minimizing unitary-norm vector ${\\boldsymbol{\\mathbf{{u}}}}$ is\n\n", "itemtype": "equation", "pos": 18168, "prevtext": "\nA normalization constraint must be considered in this minimization problem to make the representation unique, as in fact multiplying ${\\boldsymbol{\\mathbf{{u}}}}$ by a constant and dividing $f$ by the same constant does not change the objective function. For simplicity of implementation, we here optimize in ${\\boldsymbol{\\mathbf{{u}}}}$ under the constraint $\\|{\\boldsymbol{\\mathbf{{u}}}}\\|_2 = 1$, and leave the infinite-dimensional optimization in $f$ unconstrained.\n\nOur proposal for the minimization of the criterion (\\ref{eq:model_matricial}) is to alternate the minimization of ${\\boldsymbol{\\mathbf{{u}}}}$ and $f$ in an iterative algorithm:\n\n\\begin{enumerate}[label=\\textit{Step} \\arabic*,align=left, leftmargin=1.0cm]\n\n\\step Estimation of ${\\boldsymbol{\\mathbf{{u}}}}$ given $f$. For a given $f$, the minimizing ${\\boldsymbol{\\mathbf{{u}}}}$ of the objective function in (\\ref{eq:model_matricial}) is\n\n", "index": 15, "text": "\\begin{equation}\\label{eq:u_full}\n{\\boldsymbol{\\mathbf{{u}}}} = \\frac{{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_s}{\\| {\\boldsymbol{\\mathbf{{f}}}}_s \\|_2^2 +  \\lambda \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{u}}}}=\\frac{{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{%&#10;\\mathbf{{f}}}}_{s}}{\\|{\\boldsymbol{\\mathbf{{f}}}}_{s}\\|_{2}^{2}+\\lambda\\int_{%&#10;\\mathcal{M}}\\!\\Delta^{2}_{\\mathcal{M}}f},\" display=\"block\"><mrow><mrow><mi>\ud835\udc2e</mi><mo>=</mo><mfrac><msub><mi>\ud835\udc17\ud835\udc1f</mi><mi>s</mi></msub><mrow><msubsup><mrow><mo>\u2225</mo><msub><mi>\ud835\udc1f</mi><mi>s</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\step Estimation of $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$. For a given ${\\boldsymbol{\\mathbf{{u}}}}$, solving (\\ref{eq:model_matricial}) with respect to $f$ is equivalent to finding the $f$ that minimizes\n\n\n", "itemtype": "equation", "pos": 18483, "prevtext": "\nand the minimizing unitary-norm vector ${\\boldsymbol{\\mathbf{{u}}}}$ is\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:u}\n{\\boldsymbol{\\mathbf{{u}}}} = \\frac{{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_s}{\\|{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_s\\|_2}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{u}}}}=\\frac{{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{%&#10;\\mathbf{{f}}}}_{s}}{\\|{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_{%&#10;s}\\|_{2}}.\" display=\"block\"><mrow><mrow><mi>\ud835\udc2e</mi><mo>=</mo><mfrac><msub><mi>\ud835\udc17\ud835\udc1f</mi><mi>s</mi></msub><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc17\ud835\udc1f</mi><mi>s</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\end{enumerate}\n\n\n\nStep 1 is basically the classical expression of the score vector given the loadings vector, where in this case the loading vector is given by ${\\boldsymbol{\\mathbf{{f}}}}_s$, the evaluations of the PC function in $p_1,\\ldots,p_s$. The problem in Step 2 is not trivial, consisting in an infinite-dimensional minimization problem. Let $z_j$ denote the $j$th element of the vector ${\\boldsymbol{\\mathbf{{X}}}}^T {\\boldsymbol{\\mathbf{{u}}}}$, then minimizing the functional in (\\ref{eq:f_min}) is equivalent to minimizing\n\n", "itemtype": "equation", "pos": 18895, "prevtext": "\n\n\\step Estimation of $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$. For a given ${\\boldsymbol{\\mathbf{{u}}}}$, solving (\\ref{eq:model_matricial}) with respect to $f$ is equivalent to finding the $f$ that minimizes\n\n\n", "index": 19, "text": "\\begin{equation}\\label{eq:f_min}\nJ_{\\lambda, {\\boldsymbol{\\mathbf{{u}}}}}(f) = {\\boldsymbol{\\mathbf{{f}}}}_s^T{\\boldsymbol{\\mathbf{{f}}}}_s+\\lambda \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f -2{\\boldsymbol{\\mathbf{{f}}}}_s^T{\\boldsymbol{\\mathbf{{X}}}}^T{\\boldsymbol{\\mathbf{{u}}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"J_{\\lambda,{\\boldsymbol{\\mathbf{{u}}}}}(f)={\\boldsymbol{\\mathbf{{f}}}}_{s}^{T}%&#10;{\\boldsymbol{\\mathbf{{f}}}}_{s}+\\lambda\\int_{\\mathcal{M}}\\!\\Delta^{2}_{%&#10;\\mathcal{M}}f-2{\\boldsymbol{\\mathbf{{f}}}}_{s}^{T}{\\boldsymbol{\\mathbf{{X}}}}^%&#10;{T}{\\boldsymbol{\\mathbf{{u}}}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\ud835\udc2e</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msubsup><mi>\ud835\udc1f</mi><mi>s</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><mi>s</mi></msub></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\ud835\udc1f</mi><mi>s</mi><mi>T</mi></msubsup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\\\\\n\nThis problem consists in estimating a smooth scalar field $f$ defined on a manifold, starting from noisy observations $z_j$ at points $p_j$. In the special case where $\\mathcal{M}$ is a sphere or a sphere-like surface, that is $\\mathcal{M} = \\{\\sigma(v) = \\rho(v)v : v \\in S \\}$ where $S \\subset \\mathbb{R}^3$ is the unit sphere centered at the originand, this smoothing problem has been considered for instance by \\cite{wahba1981} and \\cite{alfeld1996}. Moreover, the functional (\\ref{eq:f_reg}) is considered for instance by \\cite{SSRM1} and \\cite{SSRM2}. Here $\\mathcal{M}$ is respectively a manifold homeomorphic to an open ended cylinder and a manifold homeomorphic to a sphere. In the later two works the field $f$ is estimated by first conformally recasting the problem to a planar domain and then discretizing it by means of planar finite elements, generalizing the planar smoothing model in \\cite{ramsay2002}. Our approach is also based on a Finite Element (FE) discretization, but differently from \\cite{SSRM1} and \\cite{SSRM2}, we construct here a FE space directly on the triangulated surface $\\mathcal{M}_{\\mathcal{T}}$ that approximates the manifold $\\mathcal{M}$, i.e. we use surface FE, avoiding any flattening step and allowing the formulation to be applicable to any manifold topology.\n\n\\subsection{Surface Finite Element discretization}\nAssume, for clarity of exposition only, that $\\mathcal{M}$ is a closed surface, as in our motivating application. The case of non-closed surfaces can be dealt with by considering some appropriate boundary conditions as done for instance in the planar case in \\cite{sangalli2013}. Consider the linear functional space $H^2(\\mathcal{M})$, the space of functions in $L^2(\\mathcal{M})$ with first and second weak derivatives in $L^2(\\mathcal{M})$. The infinite dimensional part of the estimation problem can be reformulated as follows: find $\\hat{f} \\in H^2(\\mathcal{M})$ such that\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\end{enumerate}\n\n\n\nStep 1 is basically the classical expression of the score vector given the loadings vector, where in this case the loading vector is given by ${\\boldsymbol{\\mathbf{{f}}}}_s$, the evaluations of the PC function in $p_1,\\ldots,p_s$. The problem in Step 2 is not trivial, consisting in an infinite-dimensional minimization problem. Let $z_j$ denote the $j$th element of the vector ${\\boldsymbol{\\mathbf{{X}}}}^T {\\boldsymbol{\\mathbf{{u}}}}$, then minimizing the functional in (\\ref{eq:f_min}) is equivalent to minimizing\n\n", "index": 21, "text": "\\begin{equation}\\label{eq:f_reg}\n\\sum_{j=1}^{s} \\bigg(z_j - f(p_j)\\bigg)^2 +\\lambda \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\sum_{j=1}^{s}\\bigg{(}z_{j}-f(p_{j})\\bigg{)}^{2}+\\lambda\\int_{\\mathcal{M}}\\!%&#10;\\Delta^{2}_{\\mathcal{M}}f\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\textbf{Proposition 1.} \\textit{The solution $\\hat{f} \\in H^2(\\mathcal{M})$ exists and is unique and is such that\n\n", "itemtype": "equation", "pos": 21817, "prevtext": "\\\\\n\nThis problem consists in estimating a smooth scalar field $f$ defined on a manifold, starting from noisy observations $z_j$ at points $p_j$. In the special case where $\\mathcal{M}$ is a sphere or a sphere-like surface, that is $\\mathcal{M} = \\{\\sigma(v) = \\rho(v)v : v \\in S \\}$ where $S \\subset \\mathbb{R}^3$ is the unit sphere centered at the originand, this smoothing problem has been considered for instance by \\cite{wahba1981} and \\cite{alfeld1996}. Moreover, the functional (\\ref{eq:f_reg}) is considered for instance by \\cite{SSRM1} and \\cite{SSRM2}. Here $\\mathcal{M}$ is respectively a manifold homeomorphic to an open ended cylinder and a manifold homeomorphic to a sphere. In the later two works the field $f$ is estimated by first conformally recasting the problem to a planar domain and then discretizing it by means of planar finite elements, generalizing the planar smoothing model in \\cite{ramsay2002}. Our approach is also based on a Finite Element (FE) discretization, but differently from \\cite{SSRM1} and \\cite{SSRM2}, we construct here a FE space directly on the triangulated surface $\\mathcal{M}_{\\mathcal{T}}$ that approximates the manifold $\\mathcal{M}$, i.e. we use surface FE, avoiding any flattening step and allowing the formulation to be applicable to any manifold topology.\n\n\\subsection{Surface Finite Element discretization}\nAssume, for clarity of exposition only, that $\\mathcal{M}$ is a closed surface, as in our motivating application. The case of non-closed surfaces can be dealt with by considering some appropriate boundary conditions as done for instance in the planar case in \\cite{sangalli2013}. Consider the linear functional space $H^2(\\mathcal{M})$, the space of functions in $L^2(\\mathcal{M})$ with first and second weak derivatives in $L^2(\\mathcal{M})$. The infinite dimensional part of the estimation problem can be reformulated as follows: find $\\hat{f} \\in H^2(\\mathcal{M})$ such that\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:problem}\n\\hat{f}={\\operatornamewithlimits{argmin}}_{f \\in H^2(\\mathcal{M})} J_{\\lambda, {\\boldsymbol{\\mathbf{{u}}}}}(f).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}={\\operatornamewithlimits{argmin}}_{f\\in H^{2}(\\mathcal{M})}J_{\\lambda,%&#10;{\\boldsymbol{\\mathbf{{u}}}}}(f).\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmin</mo><mrow><mi>f</mi><mo>\u2208</mo><mrow><msup><mi>H</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><msub><mi>J</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\ud835\udc2e</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor every $\\varphi \\in H^2(\\mathcal{M})$.}\n\nAs detailed in the Supplementary Material, the key idea is to minimize $J_{\\lambda, {\\boldsymbol{\\mathbf{{u}}}}} (f)$ by differentiating this functional with respect to $f$. This leads to (\\ref{eq:eulero-lagrange}), that characterizes the estimate $\\hat{f}$ as the solution of a linear fourth-order problem.\n\nConsider now a triangulated surface $\\mathcal{M}_\\mathcal{T}$, union of the finite set of triangles $\\mathcal{T}$, giving an approximated representation of the manifold $\\mathcal{M}$. Figure~\\ref{fig:brain_mesh} for instance shows the triangulated surface approximating the left hemisphere of a template brain. We then consider the linear finite element space $V$ consisting in a set of globally continuous functions over $\\mathcal{M}_\\mathcal{T}$ that are linear affine where restricted to any triangle $\\tau$ in $\\mathcal{T}$, i.e.\n\n", "itemtype": "equation", "pos": 22094, "prevtext": "\n\n\\textbf{Proposition 1.} \\textit{The solution $\\hat{f} \\in H^2(\\mathcal{M})$ exists and is unique and is such that\n\n", "index": 25, "text": "\\begin{equation} \\label{eq:eulero-lagrange}\n\\sum_{j=1}^s \\varphi(p_j)\\hat{f}(p_j) + \\lambda \\int_\\mathcal{M} \\Delta_\\mathcal{M} \\varphi \\Delta_\\mathcal{M} \\hat{f} = \\sum_{j=1}^s \\varphi(p_j) \\sum_{i=1}^n x_i(p_j)u_i\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\sum_{j=1}^{s}\\varphi(p_{j})\\hat{f}(p_{j})+\\lambda\\int_{\\mathcal{M}}\\Delta_{%&#10;\\mathcal{M}}\\varphi\\Delta_{\\mathcal{M}}\\hat{f}=\\sum_{j=1}^{s}\\varphi(p_{j})%&#10;\\sum_{i=1}^{n}x_{i}(p_{j})u_{i}\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mi>\u03c6</mi><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\begin{figure}[h]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.9\\textwidth]{Figures/mesh_brain_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.9\\textwidth]{Figures/mesh_brain_2.png}\n\\end{subfigure}\n\\caption{The triangulated surface approximating the left hemisphere of the template brain. The mesh is composed by 32K nodes and by 64K triangles}\n\\label{fig:brain_mesh}\n\\end{figure}\n\nThis space is spanned by the nodal basis $\\psi_1, \\ldots, \\psi_K$  associated to the nodes $\\xi_1,\\ldots,\\xi_K$, corresponding to the vertices of the triangulation $\\mathcal{M}_\\mathcal{T}$. Such basis functions are lagrangian, meaning that $\\psi_i(\\xi_j)=1$ if $i=j$ and $\\psi_i(\\xi_j)=0$ otherwise. Setting ${\\boldsymbol{\\mathbf{{f}}}} = (f(\\xi_1),\\ldots,f(\\xi_K))^T$ and ${\\boldsymbol{\\mathbf{{\\psi}}}} = (\\psi_1,\\ldots,\\psi_K)^T$, every function $f \\in V$ has the form\n\n", "itemtype": "equation", "pos": 23212, "prevtext": "\nfor every $\\varphi \\in H^2(\\mathcal{M})$.}\n\nAs detailed in the Supplementary Material, the key idea is to minimize $J_{\\lambda, {\\boldsymbol{\\mathbf{{u}}}}} (f)$ by differentiating this functional with respect to $f$. This leads to (\\ref{eq:eulero-lagrange}), that characterizes the estimate $\\hat{f}$ as the solution of a linear fourth-order problem.\n\nConsider now a triangulated surface $\\mathcal{M}_\\mathcal{T}$, union of the finite set of triangles $\\mathcal{T}$, giving an approximated representation of the manifold $\\mathcal{M}$. Figure~\\ref{fig:brain_mesh} for instance shows the triangulated surface approximating the left hemisphere of a template brain. We then consider the linear finite element space $V$ consisting in a set of globally continuous functions over $\\mathcal{M}_\\mathcal{T}$ that are linear affine where restricted to any triangle $\\tau$ in $\\mathcal{T}$, i.e.\n\n", "index": 27, "text": "\\begin{equation*}\nV = \\{ v \\in C^0(\\mathcal{M}_\\mathcal{T}): v|_{\\tau} \\text{ is linear affine for each } \\tau \\in \\mathcal{T} \\}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"V=\\{v\\in C^{0}(\\mathcal{M}_{\\mathcal{T}}):v|_{\\tau}\\text{ is linear affine for%&#10; each }\\tau\\in\\mathcal{T}\\}.\" display=\"block\"><mrow><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>v</mi><mo>\u2208</mo><mrow><msup><mi>C</mi><mn>0</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><msub><mrow><mi>v</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mi>\u03c4</mi></msub><mo>\u2062</mo><mtext>\u00a0is linear affine for each\u00a0</mtext><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor each $p \\in \\mathcal{M}_\\mathcal{T}$. The surface finite element space provides a finite dimensional subspace of $H^1(\\mathcal{M})$ [\\cite{dziuk}]. To use this finite element space to discretize the infinite-dimensional problem (\\ref{eq:eulero-lagrange}), that is well posed in $H^2(\\mathcal{M})$, we first need a reformulation of (\\ref{eq:eulero-lagrange}) that only involves first order derivatives. This can be obtained by introducing an auxiliary function $g = \\Delta_\\mathcal{M} f$, splitting the equation (\\ref{eq:eulero-lagrange}) into a coupled system of second order problems and finally integrating by parts the second order terms. The details of this derivation can be found in the supplementary material.\nThe discrete estimators $\\hat{f}_h, \\hat{g}_h \\in V$ are then obtained solving\n{\\small\n\n", "itemtype": "equation", "pos": 24298, "prevtext": "\n\n\\begin{figure}[h]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.9\\textwidth]{Figures/mesh_brain_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=.9\\textwidth]{Figures/mesh_brain_2.png}\n\\end{subfigure}\n\\caption{The triangulated surface approximating the left hemisphere of the template brain. The mesh is composed by 32K nodes and by 64K triangles}\n\\label{fig:brain_mesh}\n\\end{figure}\n\nThis space is spanned by the nodal basis $\\psi_1, \\ldots, \\psi_K$  associated to the nodes $\\xi_1,\\ldots,\\xi_K$, corresponding to the vertices of the triangulation $\\mathcal{M}_\\mathcal{T}$. Such basis functions are lagrangian, meaning that $\\psi_i(\\xi_j)=1$ if $i=j$ and $\\psi_i(\\xi_j)=0$ otherwise. Setting ${\\boldsymbol{\\mathbf{{f}}}} = (f(\\xi_1),\\ldots,f(\\xi_K))^T$ and ${\\boldsymbol{\\mathbf{{\\psi}}}} = (\\psi_1,\\ldots,\\psi_K)^T$, every function $f \\in V$ has the form\n\n", "index": 29, "text": "\\begin{equation}\\label{eq:basis}\nf(p) = \\sum_{k=1}^K f(\\xi_k) \\psi_k(p) = {\\boldsymbol{\\mathbf{{f}}}}^T {\\boldsymbol{\\mathbf{{\\psi}}}}(p)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"f(p)=\\sum_{k=1}^{K}f(\\xi_{k})\\psi_{k}(p)={\\boldsymbol{\\mathbf{{f}}}}^{T}{%&#10;\\boldsymbol{\\mathbf{{\\psi}}}}(p)\" display=\"block\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>\ud835\udc1f</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udf4d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n}\nfor all $\\varphi_h, v_h \\in V$. Define the $s \\times K$ matrix ${\\boldsymbol{\\mathbf{{\\Psi}}}} = (\\psi_k(p_j))$ and the $K \\times K$ matrices ${\\boldsymbol{\\mathbf{{R}}}}_0=\\int_{\\mathcal{M}_{\\mathcal{T}}}({\\boldsymbol{\\mathbf{{\\psi}}}} {\\boldsymbol{\\mathbf{{\\psi}}}}^T)$ and $\\mathbf{R}_1=\\int_{\\mathcal{M}_\\mathcal{T}} (\\nabla_{\\mathcal{M}_\\mathcal{T}} {\\boldsymbol{\\mathbf{{\\psi}}}}) (\\nabla_{\\mathcal{M}_\\mathcal{T}} {\\boldsymbol{\\mathbf{{\\psi}}}})^T$. Then exploiting the representation (\\ref{eq:basis}) of functions in $V$ we can rewrite (\\ref{eq:discrete}) as a linear system. Specifically the Finite Element solution $\\hat{f}_h(p)$ of the discrete counterpart (\\ref{eq:discrete}) is given by $\\hat{f}_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ where ${\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ is the solution of\n\n", "itemtype": "equation", "pos": 25259, "prevtext": "\nfor each $p \\in \\mathcal{M}_\\mathcal{T}$. The surface finite element space provides a finite dimensional subspace of $H^1(\\mathcal{M})$ [\\cite{dziuk}]. To use this finite element space to discretize the infinite-dimensional problem (\\ref{eq:eulero-lagrange}), that is well posed in $H^2(\\mathcal{M})$, we first need a reformulation of (\\ref{eq:eulero-lagrange}) that only involves first order derivatives. This can be obtained by introducing an auxiliary function $g = \\Delta_\\mathcal{M} f$, splitting the equation (\\ref{eq:eulero-lagrange}) into a coupled system of second order problems and finally integrating by parts the second order terms. The details of this derivation can be found in the supplementary material.\nThe discrete estimators $\\hat{f}_h, \\hat{g}_h \\in V$ are then obtained solving\n{\\small\n\n", "index": 31, "text": "\\begin{align}\\label{eq:discrete}\n\\begin{cases}\n& \\int_{\\mathcal{M}_\\mathcal{T}} \\nabla_{\\mathcal{M}_\\mathcal{T}} \\hat{f}_h \\nabla_{\\mathcal{M}_\\mathcal{T}} \\varphi_h - \\int_{\\mathcal{M}_\\mathcal{T}} \\hat{g}_h \\varphi_h = 0\\\\\n& \\lambda \\!\\int_{\\mathcal{M}_\\mathcal{T}} \\nabla_{\\mathcal{M}_\\mathcal{T}} \\hat{g}_h \\nabla_{\\mathcal{M}_\\mathcal{T}} v_h + \\sum\\limits_{j=1}^s \\hat{f}_h(p_j)v_h(p_j) = \\sum\\limits_{j=1}^s v_h(p_j) \\sum\\limits_{i=1}^n x_i(p_j)u_i\n\\end{cases}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{cases}&amp;\\int_{\\mathcal{M}_{\\mathcal{T}}}\\nabla_{\\mathcal{M}%&#10;_{\\mathcal{T}}}\\hat{f}_{h}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}\\varphi_{h}-\\int_{%&#10;\\mathcal{M}_{\\mathcal{T}}}\\hat{g}_{h}\\varphi_{h}=0\\\\&#10;&amp;\\lambda\\!\\int_{\\mathcal{M}_{\\mathcal{T}}}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}%&#10;\\hat{g}_{h}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}v_{h}+\\sum\\limits_{j=1}^{s}\\hat{f%&#10;}_{h}(p_{j})v_{h}(p_{j})=\\sum\\limits_{j=1}^{s}v_{h}(p_{j})\\sum\\limits_{i=1}^{n%&#10;}x_{i}(p_{j})u_{i}\\end{cases}\" display=\"inline\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub></mrow><mo>\u2062</mo><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mi>\u03c6</mi><mi>h</mi></msub></mrow></mrow></mrow><mo>-</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><msub><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>\u2062</mo><msub><mi>\u03c6</mi><mi>h</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded width=\"-1.7pt\"><mi>\u03bb</mi></mpadded><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub></mrow><mo>\u2062</mo><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mi>v</mi><mi>h</mi></msub></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>v</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mi>v</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nSolving (\\ref{eq:linear_system}) leads to\n\n", "itemtype": "equation", "pos": 26589, "prevtext": "\n}\nfor all $\\varphi_h, v_h \\in V$. Define the $s \\times K$ matrix ${\\boldsymbol{\\mathbf{{\\Psi}}}} = (\\psi_k(p_j))$ and the $K \\times K$ matrices ${\\boldsymbol{\\mathbf{{R}}}}_0=\\int_{\\mathcal{M}_{\\mathcal{T}}}({\\boldsymbol{\\mathbf{{\\psi}}}} {\\boldsymbol{\\mathbf{{\\psi}}}}^T)$ and $\\mathbf{R}_1=\\int_{\\mathcal{M}_\\mathcal{T}} (\\nabla_{\\mathcal{M}_\\mathcal{T}} {\\boldsymbol{\\mathbf{{\\psi}}}}) (\\nabla_{\\mathcal{M}_\\mathcal{T}} {\\boldsymbol{\\mathbf{{\\psi}}}})^T$. Then exploiting the representation (\\ref{eq:basis}) of functions in $V$ we can rewrite (\\ref{eq:discrete}) as a linear system. Specifically the Finite Element solution $\\hat{f}_h(p)$ of the discrete counterpart (\\ref{eq:discrete}) is given by $\\hat{f}_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ where ${\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ is the solution of\n\n", "index": 33, "text": "\\begin{equation}\\label{eq:linear_system}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{\\Psi}}}}& \\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}\\\\\n\t\t\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}& -\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{\\hat{f}}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{\\hat{g}}}}}\n\t\\end{bmatrix}\n=\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{X}}}}^T{\\boldsymbol{\\mathbf{{u}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{0}}}}\n\t\\end{bmatrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}{\\boldsymbol{\\mathbf{{\\Psi}}}}^{T}{\\boldsymbol{\\mathbf{{\\Psi}}}%&#10;}&amp;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}\\\\&#10;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}&amp;-\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}%&#10;\\end{bmatrix}\\begin{bmatrix}{\\boldsymbol{\\mathbf{{\\hat{f}}}}}\\\\&#10;{\\boldsymbol{\\mathbf{{\\hat{g}}}}}\\end{bmatrix}=\\begin{bmatrix}{\\boldsymbol{%&#10;\\mathbf{{\\Psi}}}}^{T}{\\boldsymbol{\\mathbf{{X}}}}^{T}{\\boldsymbol{\\mathbf{{u}}}%&#10;}\\\\&#10;{\\boldsymbol{\\mathbf{{0}}}}\\end{bmatrix}\" display=\"block\"><mrow><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udebf</mi></mrow></mtd><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>0</mn></msub></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo mathvariant=\"bold\" stretchy=\"false\">^</mo></mover></mtd></mtr><mtr><mtd columnalign=\"center\"><mover accent=\"true\"><mi>\ud835\udc20</mi><mo mathvariant=\"bold\" stretchy=\"false\">^</mo></mover></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nAlthough this last formula is a compact expression of the solution, it is preferable to compute the solution from the linear system (\\ref{eq:linear_system}) due to the sparsity property of its left-hand side. As an example, in the simulations and the application shown is Sections~\\ref{sec:simulations}-\\ref{sec:app}, respectively less then $1\\%$ and less then $0.1\\%$ of the elements in the matrix in the left hand side of (\\ref{eq:linear_system}) are different from zero, allowing a very efficient solution of the linear system.\n\nIn the model introduced, we assume that all the observed functions $x_i: \\mathcal{M} \\rightarrow \\mathbb{R}$ are sampled on the common set of points  $p_1, \\ldots, p_s \\in \\mathcal{M}$. Suppose moreover, $p_1, \\ldots, p_s \\in \\mathcal{M}$ coincide with the vertices of the approximating triangulated surface $\\mathcal{M}_\\mathcal{T}$. In this particular case, an alternative approach could consist of interpreting the points $p_1, \\ldots, p_s \\in \\mathcal{M}_\\mathcal{T}$ as the nodes of a graph linked by the edges of the triangulation and considering the model (\\ref{eq:model}) with a discrete smoothness operator term instead of the Laplace-Beltrami operator (see e.g. \\cite{belkin2001} for the choice of the penalization term and \\cite{deng2011} for an application to matrix decomposition). However, thanks to its functional nature, the formulation (\\ref{eq:model}) can be easily extended to the case of missing data or sparsely sampled functional data. Suppose now each function $x_i$ is observable on a set of points $p_1^{i}, \\ldots, p_{s_i}^{i}$, then the natural extension of the model (\\ref{eq:model}) becomes\n\n", "itemtype": "equation", "pos": 27171, "prevtext": "\nSolving (\\ref{eq:linear_system}) leads to\n\n", "index": 35, "text": "\\begin{equation}\n{\\boldsymbol{\\mathbf{{\\hat{f}}}}} = ({\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{\\Psi}}}} + \\lambda {\\boldsymbol{\\mathbf{{R}}}}_{1} {\\boldsymbol{\\mathbf{{R}}}}_{0}^{-1} {\\boldsymbol{\\mathbf{{R}}}}_{1})^{-1} {\\boldsymbol{\\mathbf{{\\Psi}}}}^T {\\boldsymbol{\\mathbf{{X}}}}^T{\\boldsymbol{\\mathbf{{u}}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{\\hat{f}}}}}=({\\boldsymbol{\\mathbf{{\\Psi}}}}^{T}{%&#10;\\boldsymbol{\\mathbf{{\\Psi}}}}+\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}{%&#10;\\boldsymbol{\\mathbf{{R}}}}_{0}^{-1}{\\boldsymbol{\\mathbf{{R}}}}_{1})^{-1}{%&#10;\\boldsymbol{\\mathbf{{\\Psi}}}}^{T}{\\boldsymbol{\\mathbf{{X}}}}^{T}{\\boldsymbol{%&#10;\\mathbf{{u}}}}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo mathvariant=\"bold\" stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udebf</mi></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mn>0</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nFollowing the same procedure, we can achieve an analogous algorithm based on the following two steps.\n\\begin{enumerate}[label=\\textit{Step} \\arabic*,align=left, leftmargin=1.0cm]\n\n\\step Estimation of the unitary-norm vector ${\\boldsymbol{\\mathbf{{u}}}}$ given $f$.\n\n", "itemtype": "equation", "pos": 29163, "prevtext": "\nAlthough this last formula is a compact expression of the solution, it is preferable to compute the solution from the linear system (\\ref{eq:linear_system}) due to the sparsity property of its left-hand side. As an example, in the simulations and the application shown is Sections~\\ref{sec:simulations}-\\ref{sec:app}, respectively less then $1\\%$ and less then $0.1\\%$ of the elements in the matrix in the left hand side of (\\ref{eq:linear_system}) are different from zero, allowing a very efficient solution of the linear system.\n\nIn the model introduced, we assume that all the observed functions $x_i: \\mathcal{M} \\rightarrow \\mathbb{R}$ are sampled on the common set of points  $p_1, \\ldots, p_s \\in \\mathcal{M}$. Suppose moreover, $p_1, \\ldots, p_s \\in \\mathcal{M}$ coincide with the vertices of the approximating triangulated surface $\\mathcal{M}_\\mathcal{T}$. In this particular case, an alternative approach could consist of interpreting the points $p_1, \\ldots, p_s \\in \\mathcal{M}_\\mathcal{T}$ as the nodes of a graph linked by the edges of the triangulation and considering the model (\\ref{eq:model}) with a discrete smoothness operator term instead of the Laplace-Beltrami operator (see e.g. \\cite{belkin2001} for the choice of the penalization term and \\cite{deng2011} for an application to matrix decomposition). However, thanks to its functional nature, the formulation (\\ref{eq:model}) can be easily extended to the case of missing data or sparsely sampled functional data. Suppose now each function $x_i$ is observable on a set of points $p_1^{i}, \\ldots, p_{s_i}^{i}$, then the natural extension of the model (\\ref{eq:model}) becomes\n\n", "index": 37, "text": "\\begin{equation*}\\label{eq:model_sparsedata}\n(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f}) = {\\operatornamewithlimits{argmin}} \\limits_{{\\boldsymbol{\\mathbf{{u}}}},f} \\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^{s_i} (x_i(p_j^{i}) - u_i f(p_j^{i}))^2 + \\lambda {\\boldsymbol{\\mathbf{{u}}}}^T{\\boldsymbol{\\mathbf{{u}}}} \\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}} f.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"(\\hat{{\\boldsymbol{\\mathbf{{u}}}}},\\hat{f})={\\operatornamewithlimits{argmin}}%&#10;\\limits_{{\\boldsymbol{\\mathbf{{u}}}},f}\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^%&#10;{s_{i}}(x_{i}(p_{j}^{i})-u_{i}f(p_{j}^{i}))^{2}+\\lambda{\\boldsymbol{\\mathbf{{u%&#10;}}}}^{T}{\\boldsymbol{\\mathbf{{u}}}}\\int_{\\mathcal{M}}\\!\\Delta^{2}_{\\mathcal{M}%&#10;}f.\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc2e</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmin</mo><mrow><mi>\ud835\udc2e</mi><mo>,</mo><mi>f</mi></mrow></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\ud835\udc2e</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi><mo>\u2062</mo><mrow><mpadded width=\"-1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub></mpadded><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>f</mi></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\step Estimation of $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$.\\\\\n$f = {\\boldsymbol{\\mathbf{{f}}}}^T {\\boldsymbol{\\mathbf{{\\psi}}}}$ with ${\\boldsymbol{\\mathbf{{f}}}}$ such that\n\n", "itemtype": "equation", "pos": 29804, "prevtext": "\nFollowing the same procedure, we can achieve an analogous algorithm based on the following two steps.\n\\begin{enumerate}[label=\\textit{Step} \\arabic*,align=left, leftmargin=1.0cm]\n\n\\step Estimation of the unitary-norm vector ${\\boldsymbol{\\mathbf{{u}}}}$ given $f$.\n\n", "index": 39, "text": "\\begin{equation*}\n{\\boldsymbol{\\mathbf{{u}}}} \\text{ such that } u_i = \\frac{\\\n\\sum_{j=1}^{s_i} x_i(p_j^{i}) f(p_j^{i})}{\\surd \\sum_{i=1}^n (\\sum_{j=1}^{s_i} x_i(p_j^{i}) f(p_j^{i}))^2}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{u}}}}\\text{ such that }u_{i}=\\frac{\\ \\sum_{j=1}^{s_{i}}x%&#10;_{i}(p_{j}^{i})f(p_{j}^{i})}{\\surd\\sum_{i=1}^{n}(\\sum_{j=1}^{s_{i}}x_{i}(p_{j}%&#10;^{i})f(p_{j}^{i}))^{2}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc2e</mi><mo>\u2062</mo><mtext>\u00a0such that\u00a0</mtext><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" lspace=\"7.5pt\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></msubsup><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo>\u221a</mo><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></msubsup><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 30183, "prevtext": "\n\n\\step Estimation of $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$.\\\\\n$f = {\\boldsymbol{\\mathbf{{f}}}}^T {\\boldsymbol{\\mathbf{{\\psi}}}}$ with ${\\boldsymbol{\\mathbf{{f}}}}$ such that\n\n", "index": 41, "text": "\\begin{equation*}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{L}}}}& \\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}\\\\\n\t\t\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}& -\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{f}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{g}}}}\n\t\\end{bmatrix}\n=\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{D}}}}^T{\\boldsymbol{\\mathbf{{u}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{0}}}}\n\t\\end{bmatrix}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}{\\boldsymbol{\\mathbf{{L}}}}&amp;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_%&#10;{1}\\\\&#10;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}&amp;-\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}%&#10;\\end{bmatrix}\\begin{bmatrix}{\\boldsymbol{\\mathbf{{f}}}}\\\\&#10;{\\boldsymbol{\\mathbf{{g}}}}\\end{bmatrix}=\\begin{bmatrix}{\\boldsymbol{\\mathbf{{%&#10;D}}}}^{T}{\\boldsymbol{\\mathbf{{u}}}}\\\\&#10;{\\boldsymbol{\\mathbf{{0}}}}\\end{bmatrix}\" display=\"block\"><mrow><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc0b</mi></mtd><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>0</mn></msub></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc1f</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc20</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udc03</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\\end{enumerate}\n\n\\subsection{SM-FPCA Algorithm}\nThe algorithm for the resolution of the model SM-FPCA (\\ref{eq:model}) can be summarized in the following steps.\n\n\\begin{algorithm}[H]\n\\caption{SM-FPCA Algorithm}\\label{alg:fPCA}\n\\begin{algorithmic}[1]\n\\State Initialization:\n\\begin{enumerate}[label=(\\alph*)]\n\\item Computation of ${\\boldsymbol{\\mathbf{{\\Psi}}}}$, ${\\boldsymbol{\\mathbf{{R}}}}_0$ and ${\\boldsymbol{\\mathbf{{R}}}}_1$\n\\item Perform the SVD: ${\\boldsymbol{\\mathbf{{X}}}} = {\\boldsymbol{\\mathbf{{UDV}}}}^T$\n\\item ${\\boldsymbol{\\mathbf{{f}}}}_s \\gets {\\boldsymbol{\\mathbf{{V}}}}[:,1]$, where ${\\boldsymbol{\\mathbf{{V}}}}[:,1]$ are the loadings of the first PC\n\\end{enumerate}\n\\State Scores estimation:\n\n", "itemtype": "equation", "pos": 30628, "prevtext": "\nwhere\n\n", "index": 43, "text": "\\begin{equation*}\n\\begin{array}{clc}\n{\\boldsymbol{\\mathbf{{L}}}} &=\n\\begin{bmatrix}\n\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^{s_i} u_i^2 \\psi_1(p_j^{i}) \\psi_1({p}_j^i)\n\\ldots\n\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^{s_i} u_i^2 \\psi_1(p_j^{i}) \\psi_K({p}_j^i)\\\\\n\\ldots & \\\\\n\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^{s_i} u_i^2 \\psi_K(p_j^{i}) \\psi_1({p}_j^i)\n\\ldots\n\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^{s_i} u_i^2 \\psi_K(p_j^{i}) \\psi_K({p}_j^i)\n\\end{bmatrix}\\\\\n{\\boldsymbol{\\mathbf{{D}}}} &=\n\\begin{bmatrix}\n\\sum \\limits_{j=1}^{s_1} \\psi_1({p}_j^1) x_1({p}_j^1)\n\\ldots\n\\sum \\limits_{j=1}^{s_n} \\psi_1({p}_j^n) x_n({p}_j^n) \\\\\n\\ldots & \\\\\n\\sum \\limits_{j=1}^{s_1} \\psi_K({p}_j^1) x_1({p}_j^1)\n\\ldots\n\\sum \\limits_{j=1}^{s_n} \\psi_K({p}_j^n) x_n({p}_j^n)\n\\end{bmatrix}\n\\end{array}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{clc}{\\boldsymbol{\\mathbf{{L}}}}&amp;=\\begin{bmatrix}\\sum\\limits_{i%&#10;=1}^{n}\\sum\\limits_{j=1}^{s_{i}}u_{i}^{2}\\psi_{1}(p_{j}^{i})\\psi_{1}({p}_{j}^{%&#10;i})\\ldots\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{s_{i}}u_{i}^{2}\\psi_{1}(p_{j}%&#10;^{i})\\psi_{K}({p}_{j}^{i})\\\\&#10;\\ldots&amp;\\\\&#10;\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{s_{i}}u_{i}^{2}\\psi_{K}(p_{j}^{i})\\psi%&#10;_{1}({p}_{j}^{i})\\ldots\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{s_{i}}u_{i}^{2}%&#10;\\psi_{K}(p_{j}^{i})\\psi_{K}({p}_{j}^{i})\\end{bmatrix}\\\\&#10;{\\boldsymbol{\\mathbf{{D}}}}&amp;=\\begin{bmatrix}\\sum\\limits_{j=1}^{s_{1}}\\psi_{1}(%&#10;{p}_{j}^{1})x_{1}({p}_{j}^{1})\\ldots\\sum\\limits_{j=1}^{s_{n}}\\psi_{1}({p}_{j}^%&#10;{n})x_{n}({p}_{j}^{n})\\\\&#10;\\ldots&amp;\\\\&#10;\\sum\\limits_{j=1}^{s_{1}}\\psi_{K}({p}_{j}^{1})x_{1}({p}_{j}^{1})\\ldots\\sum%&#10;\\limits_{j=1}^{s_{n}}\\psi_{K}({p}_{j}^{n})x_{n}({p}_{j}^{n})\\end{bmatrix}\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc0b</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></munderover></mstyle><mrow><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></munderover></mstyle><mrow><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mi/></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></munderover></mstyle><mrow><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>i</mi></msub></munderover></mstyle><mrow><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc03</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mn>1</mn></msub></munderover></mstyle><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mn>1</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>x</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mn>1</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>n</mi></msub></munderover></mstyle><mrow><msub><mi>\u03c8</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>x</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mi/></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mn>1</mn></msub></munderover></mstyle><mrow><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mn>1</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>x</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mn>1</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"false\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>s</mi><mi>n</mi></msub></munderover></mstyle><mrow><msub><mi>\u03c8</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>x</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>p</mi><mi>j</mi><mi>n</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mtd><mtd/></mtr></mtable></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\State PC function's estimation:\n\n\n\n${\\boldsymbol{\\mathbf{{f}}}}$ such that \n\n", "itemtype": "equation", "pos": 32137, "prevtext": "\n\\end{enumerate}\n\n\\subsection{SM-FPCA Algorithm}\nThe algorithm for the resolution of the model SM-FPCA (\\ref{eq:model}) can be summarized in the following steps.\n\n\\begin{algorithm}[H]\n\\caption{SM-FPCA Algorithm}\\label{alg:fPCA}\n\\begin{algorithmic}[1]\n\\State Initialization:\n\\begin{enumerate}[label=(\\alph*)]\n\\item Computation of ${\\boldsymbol{\\mathbf{{\\Psi}}}}$, ${\\boldsymbol{\\mathbf{{R}}}}_0$ and ${\\boldsymbol{\\mathbf{{R}}}}_1$\n\\item Perform the SVD: ${\\boldsymbol{\\mathbf{{X}}}} = {\\boldsymbol{\\mathbf{{UDV}}}}^T$\n\\item ${\\boldsymbol{\\mathbf{{f}}}}_s \\gets {\\boldsymbol{\\mathbf{{V}}}}[:,1]$, where ${\\boldsymbol{\\mathbf{{V}}}}[:,1]$ are the loadings of the first PC\n\\end{enumerate}\n\\State Scores estimation:\n\n", "index": 45, "text": "\\begin{equation*}\n{\\boldsymbol{\\mathbf{{u}}}} \\gets \\frac{{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_s}{\\|{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{\\mathbf{{f}}}}_s\\|_2}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{u}}}}\\leftarrow\\frac{{\\boldsymbol{\\mathbf{{X}}}}{%&#10;\\boldsymbol{\\mathbf{{f}}}}_{s}}{\\|{\\boldsymbol{\\mathbf{{X}}}}{\\boldsymbol{%&#10;\\mathbf{{f}}}}_{s}\\|_{2}}\" display=\"block\"><mrow><mi>\ud835\udc2e</mi><mo>\u2190</mo><mfrac><msub><mi>\ud835\udc17\ud835\udc1f</mi><mi>s</mi></msub><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc17\ud835\udc1f</mi><mi>s</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\\State PC function's evaluation:\n\n", "itemtype": "equation", "pos": 32411, "prevtext": "\n\n\\State PC function's estimation:\n\n\n\n${\\boldsymbol{\\mathbf{{f}}}}$ such that \n\n", "index": 47, "text": "\\begin{equation*}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{\\Psi}}}}& \\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}\\\\\n\t\t\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}& -\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}\n\t\\end{bmatrix}\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{f}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{g}}}}\n\t\\end{bmatrix}\n=\n\t\\begin{bmatrix}\n\t\t{\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{X}}}}^T{\\boldsymbol{\\mathbf{{u}}}}\\\\\n\t\t{\\boldsymbol{\\mathbf{{0}}}}\n\t\\end{bmatrix}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}{\\boldsymbol{\\mathbf{{\\Psi}}}}^{T}{\\boldsymbol{\\mathbf{{\\Psi}}}%&#10;}&amp;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}\\\\&#10;\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{1}&amp;-\\lambda{\\boldsymbol{\\mathbf{{R}}}}_{0}%&#10;\\end{bmatrix}\\begin{bmatrix}{\\boldsymbol{\\mathbf{{f}}}}\\\\&#10;{\\boldsymbol{\\mathbf{{g}}}}\\end{bmatrix}=\\begin{bmatrix}{\\boldsymbol{\\mathbf{{%&#10;\\Psi}}}}^{T}{\\boldsymbol{\\mathbf{{X}}}}^{T}{\\boldsymbol{\\mathbf{{u}}}}\\\\&#10;{\\boldsymbol{\\mathbf{{0}}}}\\end{bmatrix}\" display=\"block\"><mrow><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udebf</mi></mrow></mtd><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mn>0</mn></msub></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc1f</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc20</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\n\\State Repeat Steps 2--4 until convergence\n\n\\State Normalization:\n\n", "itemtype": "equation", "pos": 32950, "prevtext": "\n\\State PC function's evaluation:\n\n", "index": 49, "text": "\\begin{equation*}\n{\\boldsymbol{\\mathbf{{f}}}}_s \\gets {\\boldsymbol{\\mathbf{{\\Psi}}}}^T {\\boldsymbol{\\mathbf{{f}}}}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"{\\boldsymbol{\\mathbf{{f}}}}_{s}\\leftarrow{\\boldsymbol{\\mathbf{{\\Psi}}}}^{T}{%&#10;\\boldsymbol{\\mathbf{{f}}}}\" display=\"block\"><mrow><msub><mi>\ud835\udc1f</mi><mi>s</mi></msub><mo>\u2190</mo><mrow><msup><mi>\ud835\udebf</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\\end{algorithmic}\n\\end{algorithm}\n\nThe model (\\ref{eq:model}) is a non-convex minimization problem in $({\\boldsymbol{\\mathbf{{u}}}}, f)$. However, in the previous section we proved the existence and uniqueness of the minimizing $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$ and vice-versa. This implies that the objective function is non-increasing under the update rules of the Algorithm~\\ref{alg:fPCA}. Since the first guess of the PC function, given by the SVD, is usually a good starting point, in all our simulations no convergence problem has been detected.\n\n\\subsection{Parameters selection}\nThe proposed models have a smoothing parameter $\\lambda > 0$ that adjusts the trade-off between the fidelity of the estimate to the data via the sum of the squared errors and the smoothness of the solution via the penalty term. The problem of choosing the smoothing parameter is common to all smoothing problems.\n\nThe flexibility given by the smoothing parameter can be seen as an advantageous feature; by varying the smoothing parameter the data can be explored on different scales. However, in many cases a data-driven automatic method is necessary. In the following simulations we consider two different criteria.\nThe first approach consists on a $K$-fold cross validation. The data matrix ${\\boldsymbol{\\mathbf{{X}}}}$ is partitioned by rows into $K$ roughly equal groups. For each group of data $k=1,\\ldots,K$ the dataset can be split into a validation set ${\\boldsymbol{\\mathbf{{X}}}}^{k}$, composed of the elements of the $k$th group, and a training set, composed of the remaining elements. For different smoothing parameters, the loading function $f^{-k}$ is estimated from the training dataset. Given the estimated loading function $f^{-k}$, the associated score vector ${\\boldsymbol{\\mathbf{{u}}}}^{k}$ is computed on the validation dataset. Since $f^{-k}$ has been computed on the training dataset, ${\\boldsymbol{\\mathbf{{u}}}}^{k}$ should be computed on the validation dataset via the formula (\\ref{eq:u_full}), where $\\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}}$ can be approximated by ${\\boldsymbol{\\mathbf{{g}}}}^T {\\boldsymbol{\\mathbf{{R}}}}_0 {\\boldsymbol{\\mathbf{{g}}}}$, being $g_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{g}}}}$ the auxiliary function approximating $\\Delta_{\\mathcal{M}} f$.\nFinally we select the value of the parameter $\\lambda$ that minimizes the following score:\n\n", "itemtype": "equation", "pos": 33148, "prevtext": "\n\n\\State Repeat Steps 2--4 until convergence\n\n\\State Normalization:\n\n", "index": 51, "text": "\\begin{equation*}\n\\hat{f}(p) \\gets \\frac{{\\boldsymbol{\\mathbf{{f}}}}^T{\\boldsymbol{\\mathbf{{\\psi}}}}(p)}{\\|{\\boldsymbol{\\mathbf{{f}}}}^T{\\boldsymbol{\\mathbf{{\\psi}}}}\\|_{L^2(\\mathcal{M}_\\mathcal{T})}}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}(p)\\leftarrow\\frac{{\\boldsymbol{\\mathbf{{f}}}}^{T}{\\boldsymbol{\\mathbf{%&#10;{\\psi}}}}(p)}{\\|{\\boldsymbol{\\mathbf{{f}}}}^{T}{\\boldsymbol{\\mathbf{{\\psi}}}}%&#10;\\|_{L^{2}(\\mathcal{M}_{\\mathcal{T}})}}\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2190</mo><mfrac><mrow><msup><mi>\ud835\udc1f</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udf4d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mrow><mo>\u2225</mo><mrow><msup><mi>\ud835\udc1f</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udf4d</mi></mrow><mo>\u2225</mo></mrow><mrow><msup><mi>L</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\n\nThe second approach is based on the minimization of a generalized cross-validation (GCV) criteria integrated on the regression step of the iterative algorithm.\nDenoting ${\\boldsymbol{\\mathbf{{S}}}}(\\lambda) = {\\boldsymbol{\\mathbf{{\\Psi}}}}^T ({\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{\\Psi}}}} + \\lambda {\\boldsymbol{\\mathbf{{R}}}}_{1} {\\boldsymbol{\\mathbf{{R}}}}_{0}^{-1} {\\boldsymbol{\\mathbf{{R}}}}_{1})^{-1} {\\boldsymbol{\\mathbf{{\\Psi}}}}^T$ the GCV score is defined as\n\n", "itemtype": "equation", "pos": 35784, "prevtext": "\n\\end{algorithmic}\n\\end{algorithm}\n\nThe model (\\ref{eq:model}) is a non-convex minimization problem in $({\\boldsymbol{\\mathbf{{u}}}}, f)$. However, in the previous section we proved the existence and uniqueness of the minimizing $f$ given ${\\boldsymbol{\\mathbf{{u}}}}$ and vice-versa. This implies that the objective function is non-increasing under the update rules of the Algorithm~\\ref{alg:fPCA}. Since the first guess of the PC function, given by the SVD, is usually a good starting point, in all our simulations no convergence problem has been detected.\n\n\\subsection{Parameters selection}\nThe proposed models have a smoothing parameter $\\lambda > 0$ that adjusts the trade-off between the fidelity of the estimate to the data via the sum of the squared errors and the smoothness of the solution via the penalty term. The problem of choosing the smoothing parameter is common to all smoothing problems.\n\nThe flexibility given by the smoothing parameter can be seen as an advantageous feature; by varying the smoothing parameter the data can be explored on different scales. However, in many cases a data-driven automatic method is necessary. In the following simulations we consider two different criteria.\nThe first approach consists on a $K$-fold cross validation. The data matrix ${\\boldsymbol{\\mathbf{{X}}}}$ is partitioned by rows into $K$ roughly equal groups. For each group of data $k=1,\\ldots,K$ the dataset can be split into a validation set ${\\boldsymbol{\\mathbf{{X}}}}^{k}$, composed of the elements of the $k$th group, and a training set, composed of the remaining elements. For different smoothing parameters, the loading function $f^{-k}$ is estimated from the training dataset. Given the estimated loading function $f^{-k}$, the associated score vector ${\\boldsymbol{\\mathbf{{u}}}}^{k}$ is computed on the validation dataset. Since $f^{-k}$ has been computed on the training dataset, ${\\boldsymbol{\\mathbf{{u}}}}^{k}$ should be computed on the validation dataset via the formula (\\ref{eq:u_full}), where $\\int_{\\mathcal{M}} \\! \\Delta^2_{\\mathcal{M}}$ can be approximated by ${\\boldsymbol{\\mathbf{{g}}}}^T {\\boldsymbol{\\mathbf{{R}}}}_0 {\\boldsymbol{\\mathbf{{g}}}}$, being $g_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{g}}}}$ the auxiliary function approximating $\\Delta_{\\mathcal{M}} f$.\nFinally we select the value of the parameter $\\lambda$ that minimizes the following score:\n\n", "index": 53, "text": "\\begin{equation}\\label{eq:CV}\nCV(\\lambda) = \\sum_{k=1}^K  \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{s} x_i(p_j) - u_i^{k} f^{-k}(p_j))^2}{n p}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"CV(\\lambda)=\\sum_{k=1}^{K}\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{s}x_{i}(p_{j})-u_{i}%&#10;^{k}f^{-k}(p_{j}))^{2}}{np}.\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msubsup><mi>u</mi><mi>i</mi><mi>k</mi></msubsup><msup><mi>f</mi><mrow><mo>-</mo><mi>k</mi></mrow></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo><msup><mi/><mn>2</mn></msup></mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>p</mi></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nWhile the $K$-fold approach is generally slower, this does not require the inversion of any matrix. This is an advantageous feature, since generally the inverse of sparse matrix is not sparse. It is thus applicable also to datasets ${\\boldsymbol{\\mathbf{{X}}}}$ with a large number of columns $s$.\n\n\n\n\n\n\nAnother parameter that must be chosen in the application of the algorithm is the number of PCs to be considered for the dimension reduction of the data. A classical approach is to select this parameter on the basis of cumulated explained variance of the PC.\nWhile in the ordinary PC, the scores vectors are uncorrelated and their loadings are orthogonal, in our formulation neither the loadings are explicitly imposed to be orthogonal nor the PC scores to be uncorrelated. Let ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ be the $n \\times k$ matrix such that the columns of ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ are the first $k$ PC scores vectors. Without the uncorrelation assumption, it is meaningless to compute the total variance explained by the first $k$ PCs by tr$({\\boldsymbol{\\mathbf{{\\hat{U}}}}}^T{\\boldsymbol{\\mathbf{{\\hat{U}}}}})$. To overcome this problem \\cite{zou2004} propose to remove linear dependence between correlated PC scores vectors, by regression projection. Thus they compute the QR decomposition of ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ as ${\\boldsymbol{\\mathbf{{\\hat{U}}}}} = {\\boldsymbol{\\mathbf{{Q}}}}{\\boldsymbol{\\mathbf{{R}}}}$ and define the \\textit{adjusted total variance} as $\\sum_{j=1}^k {\\boldsymbol{\\mathbf{{R}}}}_{jj}^2$.\n\n\\section{Simulation studies}\\label{sec:simulations}\nIn this section we conduct simulations to assess the performance of the SM-FPCA algorithm as compared to other methods.\n\nWe consider as domain of the functional observations a triangulated surface $\\mathcal{M}_\\mathcal{T}$ with 642 nodes that approximates the brainstem. On this triangulated surface we generate the orthonormal functions $v_1$ and $v_2$, consisting in two eigenfunctions of the Laplace-Beltrami operator, as shown in Figure~\\ref{fig:brain_stem}. These represent respectively the first and second PC functions. We then generate $n = 50$ smooth functions $x_1,\\ldots,x_{50}$ on $\\mathcal{M}_\\mathcal{T}$ by\n\n", "itemtype": "equation", "pos": 36420, "prevtext": "\n\nThe second approach is based on the minimization of a generalized cross-validation (GCV) criteria integrated on the regression step of the iterative algorithm.\nDenoting ${\\boldsymbol{\\mathbf{{S}}}}(\\lambda) = {\\boldsymbol{\\mathbf{{\\Psi}}}}^T ({\\boldsymbol{\\mathbf{{\\Psi}}}}^T{\\boldsymbol{\\mathbf{{\\Psi}}}} + \\lambda {\\boldsymbol{\\mathbf{{R}}}}_{1} {\\boldsymbol{\\mathbf{{R}}}}_{0}^{-1} {\\boldsymbol{\\mathbf{{R}}}}_{1})^{-1} {\\boldsymbol{\\mathbf{{\\Psi}}}}^T$ the GCV score is defined as\n\n", "index": 55, "text": "\\begin{equation*}\\label{eq:GCV}\n\\text{GCV}(\\lambda) = \\frac{1}{s} \\frac{\\|({\\boldsymbol{\\mathbf{{I}}}} - {\\boldsymbol{\\mathbf{{S}}}}(\\lambda))({\\boldsymbol{\\mathbf{{X}}}}^T{\\boldsymbol{\\mathbf{{u}}}})\\|^2}{(1 - \\frac{1}{s}\ntr \\{{\\boldsymbol{\\mathbf{{S}}}}(\\lambda)\\})^2}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\text{GCV}(\\lambda)=\\frac{1}{s}\\frac{\\|({\\boldsymbol{\\mathbf{{I}}}}-{%&#10;\\boldsymbol{\\mathbf{{S}}}}(\\lambda))({\\boldsymbol{\\mathbf{{X}}}}^{T}{%&#10;\\boldsymbol{\\mathbf{{u}}}})\\|^{2}}{(1-\\frac{1}{s}tr\\{{\\boldsymbol{\\mathbf{{S}}%&#10;}}(\\lambda)\\})^{2}}.\" display=\"block\"><mrow><mrow><mrow><mtext>GCV</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>s</mi></mfrac><mo>\u2062</mo><mfrac><msup><mrow><mo>\u2225</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc17</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc2e</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mfrac><mn>1</mn><mi>s</mi></mfrac><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere $u_{i1}$, $u_{i2}$ are independent random variables, distributed as $u_{i1} \\sim \\mathcal{N}(0,\\sigma_1^2)$, $u_{i2} \\sim \\mathcal{N}(0,\\sigma_2^2)$, with $\\sigma_1 = 4$ and $\\sigma_2 = 2$. They represent the PC scores.\n\n\\begin{figure}[h] \n\n\\includegraphics[width=\\textwidth]{Figures/original.png}\n\\caption[]{From left to right, the triangulated surface representing the brainstem, a plot of the true first and second PC functions and a plot of a noisy observation on the brainstem.}\n\\label{fig:brain_stem}\n\\end{figure}\n\nThe smooth functions $x_i$ are sampled at locations $p_j \\in \\mathbb{R}^3$ with $j = 1,\\ldots, s$ coinciding with the nodes of the triangulates surface. Moreover at each of these points has been added to the functions a Gaussian noise with mean zero and standard deviation $\\sigma = 0.1$ to obtain the noisy observations denoted with $x_i(p_j)$. We are interested in recovering the smooth PC functions $v_1$ and $v_2$ from these noisy observations over $\\mathcal{M}_\\mathcal{T}$. We compare the proposed SM-FPCA technqiue to two alternative approaches.\n\nThe first basic approach we consider is a simple multivariate PCA (MV-PCA) applied to the data-matrix ${\\boldsymbol{\\mathbf{{X}}}}$. The PC functions are thus obtained by piecewise linear interpolation over the mesh $\\mathcal{M}_{\\mathcal{T}}$. Finally they are normalized to have unitary norm in $L^2(\\mathcal{M}_{\\mathcal{T}})$.\n\nA second natural approach is based on a pre-smoothing of the noisy observations that tries to recover the smooth functions $x_i, \\, i=1,\\ldots,n$ from their noisy observations $x_i(p_j)$, followed by a MV-PCA on the denoised evaluations of the functions on $p_j, \\, j=1,\\ldots,s$. The smoothing problem for a field defined on a Riemannian manifold is not trivial. In this case the smoothing technique applied is Iterated Heat Kernel (IHK) smoothing [\\cite{chung2005}]. The heat kernel smoothing of the noisy observation $x_i(p_j)$, is given by\n$K_\\eta \\times x_i(p_j) = \\int_{\\mathcal{M}} K_\\eta (p,q) x_i(p_j) dq$, where $\\eta$ is the smoothing parameter and $K_\\eta$ is the heat kernel, whose analytic expression can be extracted from the eigenfunctions of the Laplace-Beltrami operator. However, for numerical approximation, it can shown that for $\\eta$ small and for $q$ close to $p$ we have\n\n", "itemtype": "equation", "pos": 38937, "prevtext": "\nWhile the $K$-fold approach is generally slower, this does not require the inversion of any matrix. This is an advantageous feature, since generally the inverse of sparse matrix is not sparse. It is thus applicable also to datasets ${\\boldsymbol{\\mathbf{{X}}}}$ with a large number of columns $s$.\n\n\n\n\n\n\nAnother parameter that must be chosen in the application of the algorithm is the number of PCs to be considered for the dimension reduction of the data. A classical approach is to select this parameter on the basis of cumulated explained variance of the PC.\nWhile in the ordinary PC, the scores vectors are uncorrelated and their loadings are orthogonal, in our formulation neither the loadings are explicitly imposed to be orthogonal nor the PC scores to be uncorrelated. Let ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ be the $n \\times k$ matrix such that the columns of ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ are the first $k$ PC scores vectors. Without the uncorrelation assumption, it is meaningless to compute the total variance explained by the first $k$ PCs by tr$({\\boldsymbol{\\mathbf{{\\hat{U}}}}}^T{\\boldsymbol{\\mathbf{{\\hat{U}}}}})$. To overcome this problem \\cite{zou2004} propose to remove linear dependence between correlated PC scores vectors, by regression projection. Thus they compute the QR decomposition of ${\\boldsymbol{\\mathbf{{\\hat{U}}}}}$ as ${\\boldsymbol{\\mathbf{{\\hat{U}}}}} = {\\boldsymbol{\\mathbf{{Q}}}}{\\boldsymbol{\\mathbf{{R}}}}$ and define the \\textit{adjusted total variance} as $\\sum_{j=1}^k {\\boldsymbol{\\mathbf{{R}}}}_{jj}^2$.\n\n\\section{Simulation studies}\\label{sec:simulations}\nIn this section we conduct simulations to assess the performance of the SM-FPCA algorithm as compared to other methods.\n\nWe consider as domain of the functional observations a triangulated surface $\\mathcal{M}_\\mathcal{T}$ with 642 nodes that approximates the brainstem. On this triangulated surface we generate the orthonormal functions $v_1$ and $v_2$, consisting in two eigenfunctions of the Laplace-Beltrami operator, as shown in Figure~\\ref{fig:brain_stem}. These represent respectively the first and second PC functions. We then generate $n = 50$ smooth functions $x_1,\\ldots,x_{50}$ on $\\mathcal{M}_\\mathcal{T}$ by\n\n", "index": 57, "text": "\\begin{equation}\nx_i = u_{i1} v_1 + u_{i2} v_2 \\quad i=1,\\ldots,n,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"x_{i}=u_{i1}v_{1}+u_{i2}v_{2}\\quad i=1,\\ldots,n,\" display=\"block\"><mrow><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>2</mn></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mn>2</mn></msub></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nThe desired level of smoothing can be reached after $k$ iterations, thanks to the following property: $K_\\eta^{k} \\times f = K_\\eta \\times \\ldots \\times K_\\eta \\times f = K_{\\sqrt{k}\\eta}$. For a fixed bandwidth $\\eta$, the level of smoothing is determined by an optimal number of iterations selected via the F-test criterion outlined in \\cite{chung2005}. In these simulations the bandwidth has been set at $\\eta=2.5$, heuristically selecting the one with the best performance after some initial pilot studies. We will refer to this approach as IHK-PCA.\n\nThe proposed SM-FPCA technique is implemented as follows. For each PC we run Algorithm 1 with 15 iterations of the steps 2-4. For the choice of the optimal smoothing parameter $\\lambda$, both $K$-fold, with $K = 5$, and GCV approaches have been applied.\n\nThe reconstructed PC functions, using the three different approaches are shown in Figures~\\ref{fig:comparison1}-\\ref{fig:comparison2}. It is evident that applying the MV-PCA yields to a reconstruction far from the true, because of the absence of any spatial information. The reconstruction through the IHK-PCA approach and the SM-FPCA model are considerably more satisfactory.\n\n\\begin{figure}[!htb] \n\n\\includegraphics[width=\\textwidth]{Figures/PC2_1_comparative.png}\n\\caption[]{From left to right, contours of the first PC function reconstructed respectively with MV-PCA, IHK-PCA, SM-FPCA GCV and SM-FPCA K-fold. From a visual inspection, MV-PCA shows unsatisfactory results, while a better estimation is achieved by IHK-PCA and SM-FPCA. In particular SM-FPCA shows an almost optimal reconstruction of the contours' horizontal pattern in the original first PC function. }\n\\label{fig:comparison1}\n\\end{figure}\n\n\\begin{figure}[!htb] \n\n\\includegraphics[width=\\textwidth]{Figures/PC2_2_comparative.png}\n\\caption[]{From the left to the right, contours of the second PC function reconstructed respectively with MV-PCA, IHK-PCA, SM-FPCA GCV and SM-FPCA K-fold. A visual inspection confirms that a better estimation is achieved by IHK-PCA and SM-FPCA respect to MV-PCA.}\n\\label{fig:comparison2}\n\\end{figure}\n\n\\begin{figure}[H] \n\n\\includegraphics[width=\\textwidth]{Figures/boxplot.png}\n\\caption[]{Boxplots summarizing the performance of IHK-PCA and SM-FPCA. For the SM-FPCA both GCV and $K$-fold have been applied for the selection of the smoothing parameter.}\n\\label{fig:boxplots}\n\\end{figure}\nWhile the poor performance of the MV-PCA is evident, to assess the performance of the other two methods, we apply them to 100 datasets generated as previously detailed. The quality of estimated individual surfaces is then measured using the mean square error (MSE) over all the locations $p_j, \\, j=1,\\ldots,s$. MSEs are also used to evaluate the reconstruction of the PC scores vectors.  Another performance measure used is the principal angle between the subspace spanned by the estimated PC functions and the subspace spanned by the true PC functions, as used in \\cite{shen2008}. Intuitively, the principal angle measures how similar two subspaces are. For this purpose we construct the $s \\times 2$ matrices $\\mathbb{V} = (v_i(p_j))$ and $\\mathbb{\\hat{V}} = (\\hat{v}_i(p_j))$, where $\\hat{v}_i$ is the $i$th estimate of the true PC function $v_i$. Then we compute the orthonormal set of basis ${\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{V}$ and ${\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{\\hat{V}}$ from the QR decomposition of $\\mathbb{V}$ and $\\mathbb{\\hat{V}}$. The principal angle is defined as the angle $cos^{-1}(\\rho)$, where $\\rho$ is the minimum singular value of ${\\boldsymbol{\\mathbf{{Q}}}}^T_\\mathbb{\\hat{V}}{\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{V}$. The results are summarized in the boxplots in Figure~\\ref{fig:boxplots}, which compares the IHK-PCA and SM-FPCA algorithms with respect to the reconstruction's errors of the PC functions $v_1$ and $v_2$, the PC scores ${\\boldsymbol{\\mathbf{{u}}}}_1 = (u_{i1})$ and ${\\boldsymbol{\\mathbf{{u}}}}_2 = (u_{i2})$, the reconstructed signals $x_i = u_{i1} v_1 + u_{i2} v_2$ and the principle angles between the subspaces spanned by the true and estimated PC functions.\n\nThe boxplots highlight the fact that SM-FPCA provides the best estimates of the PC functions and corresponding scores vectors. The $K$-fold approach to the choice of the smoothing parameter in the SM-FPCA model performs better then the GCV approach.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application}\\label{sec:app}\nThe data set which we consider in this paper arises from the Human Connectome Project Consortium [\\cite{vanessen2012}], which is collecting data such as structural scans, resting-state and task-based functional MRI scans, and diffusion-weighted MRI scans from a large number of healthy volunteers to help elucidate the understanding of normal brain function.\nMany preprocessing considerations have already been resolved in the so called minimally preprocessed dataset. Among the various preprocessing pipelines applied to the HCP original data, of particular interest for us is the one named \\textit{fMRISurface} [\\cite{glasser2013}]. This pipeline provides a transformation of the 3D structural MRI and 4D signal from the functional MRI scan, so to enable the application of statistical analysis techniques on brain surfaces. For each subject, the personal cortical surface is extracted as a triangulated surface from the structural MRI and to each vertex of this mesh is associated a BOLD time-series derived from the BOLD signal of the underlying gray-matter ribbon. The cortical surfaces extracted are aligned to a template cortical surface generated from the cortical surfaces of 69 healthy adults. In practice this cortical surface is represented by two triangulated surfaces with 32k vertices, one for each hemisphere. In Figure~\\ref{fig:brain_mesh} the left hemisphere is shown. Through this anatomical transformation map, the patients' BOLD time-series, on the cortical surface, are coherently located to the vertices of the template cortical surface. The fMRI signal used for our analysis has been acquired in absence of any task and for this reason is also called resting state fMRI. Finally each time-series is filtered to the band of frequencies $[0.009,0.08]$Hz.\n\n\nAs already mentioned in Section~\\ref{sec:intro}, a classic approach in the study of the resting state fMRI, is to exploit the time dimension of the data, for the extraction of a connectivity measure among the different parts of the cortical surface. A standard choice for this purpose is the computation of the temporal correlation. It first consists of identifying a Region of Interest (ROI) on the cortical surface. For each subject, all the time-series in the ROI are used to find a representative mean time-series. To each vertex of the cortical surface we associate the pairwise correlation of the time-series located in that vertex with the time-series representative of the ROI. Finally each correlation value is transformed using Fisher's r-to-z transformation, yielding a resting state functional connectivity (RSFC) map for each subject. The total number of subjects considered for this analysis is 491.\n\nFor the choice of the ROI, we consider the cortical parcellation derived in \\cite{gordon2014}, where a group-avarage boundary map of the cortical surface is derived from resting state fMRI (Figure~\\ref{fig:parcellation}). The identified cortical areas are unlikely to correspond the individual parcellation of each subject, since they are derived from a group average study. However, they can serve as a reasonable ROIs in individual subjects. The parcel that served as ROI in the following analysis is visible in red in Figure~\\ref{fig:parcellation}.\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/parcellation_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/parcellation_2.png}\n\\end{subfigure}\n\\caption{Parcellation of the cortical surface derived in \\cite{gordon2014}. In red the Region of Interest chosen for the computation of the RSFC maps. This region is localized on an area of the cerebral cortex called precuneus.}\n\\label{fig:parcellation}\n\\end{figure}\nFor the chosen ROI, a snapshot of the RSFC map of one subject is shown in Figure~\\ref{fig:RSFC}.\n\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/fisher_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/fisher_2.png}\n\\end{subfigure}\n\\caption{A snapshot of the RSFC map of one subject.}\n\\label{fig:RSFC}\n\\end{figure}\n\nThe mean RSFC map is shown in Figure~\\ref{fig:mean}. As expected high correlation values are visible inside the ROI. The mean RSFC over 491 subjects shows a variability coherent with the parcellation, in the sense that the vertices inside each parcel show similar values. We wish now to understand which are the main modes of variation of this RSFC maps among the different subjects, by applying a PCA.\n\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.55\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean1.png}\n\\end{subfigure}\n\\begin{subfigure}{.55\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean2.png}\n\\end{subfigure}\n\\caption{The mean RSFC map computed over 491 subject. As expected, high correlation values are visible inside the ROI.}\n\\label{fig:mean}\n\\end{figure}\n\n\nThe first three PC functions, estimated with SM-FPCA, are shown in Figures~\\ref{fig:PC1_brain}-\\ref{fig:PC2_brain}-\\ref{fig:PC3_brain} as compared to the PC functions derived from MV-PCA and IHK-PCA. The choice of the smoothing parameter for the SM-FPCA is based on the $K$-fold cross validation, with $K = 5$.\n\nThe PC functions estimated from the MV-PCA shows an excessive variability, since the sample size is not sufficiently large to deal with the extremely high dimensionality of the data. In fact, even recent attempts to model the subject variability from resting state fMRI leads to the conclusion that spatial mismatches, introduce by the alignment problem, are one of the biggest sources of currently observable differences between subjects [\\cite{harrison2015}]. This registration process can result in misalignments, due to the lack to functional regions being perfectly coincident or due to situations where the local topology is strongly different among subjects. These misalignments can introduce fictitious effects on the computed PC functions. Data misalignment is a well known problem in FDA [\\cite{ramsay2005}]. For functional data with one-dimensional domains, typical approaches are based on shifting or (monotone) transformations of the domain of each function. But neither shifting nor monotonic transformations make sense on a generic non-euclidean domain, so it is not clear how to generalize the standard FDA approaches. The introduction of a smoothing penalty in the PCA model should reduce the variability effects due to misalignment. In fact the smoothing parameter in the SM-FPCA algorithm can be seen as a further degree of freedom that allows a multiscale analysis, meaning that by increasing the smoothing penalty parameter is possible to constrain the results to show only the macroscopical effects of the phenomena and to remove the artifacts introduced by the preprocessing steps.\n\nThe PC functions estimated through IHK-PCA and SM-FPCA seem both to resolve the problem of the presence of some residual noise in the PC function estimates. However, we would like to emphasize the fact that, for instance, the third PC function computed with IHK-PCA shows some differences with the one computed with SM-FPCA. This is probably due to the fact that an individual pre-smoothing approach with the presented data tends to delete part of the information contained in the single RSFC map, while the SM-FPCA model incorporates all the noisy data as is and introduces a smoothness penalization directly on the PC function estimates. Contrary to MV-PCA and IHK-PCA, in all three PC functions, SM-FPCA shows a satisfactory level of smoothness, without deleting sharper changes in some locations, which may indicate the presence of a boundary between two cortical areas, since it is well known that the cortical surface is organized in a number of interacting cortical areas.\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{Figures/brain_comparative_PC1_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{Figures/brain_comparative_PC1_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the first PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC1_brain}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC2_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC2_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the second PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC2_brain}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC3_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC3_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the third PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC3_brain}\n\\end{figure}\n\nFor the purpose of interpretation of the PC functions we might prefer to plot the functions $\\mu \\pm 2\\sigma f$, where $\\mu$ denotes the mean RSFC map, $\\sigma$ denotes the standard deviation of the PC scores vector and $f$ denotes the associated PC function. In Figure~\\ref{fig:PC1_interpretation} we show the described plot for the first PC function. We can observe that while the high correlation value in the ROI and inferior parietal are in first approximation preserved from subject to subject, a high variability between subjects can be observed in the areas surrounding the ROI and the inferior parietal, which is understood due to individual inter-subject differences [\\cite{buckner2008brain} and references therein].\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_meno2std_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean_pc_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_piu2std_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_meno2std_2.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean_pc_2.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_piu2std_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of $\\mu - 2\\sigma f$, $\\mu$, $\\mu + 2\\sigma f$, where $\\mu$ denotes the mean RSFC map, $\\sigma$ denotes the standard deviation of the first PC scores vector and $f$ denotes the first PC function.}\n\\label{fig:PC1_interpretation}\n\\end{figure}\n\n\\section{Discussion}\\label{sec:discussion}\nIn this paper we introduce a novel PCA technique that can handle functional data located over a two-dimensional manifold. The adopted approach is based on a regularized PCA model. In particular a smoothness penalty term that measures the curvature of a function over a manifold is introduced and the estimation problem is solved via an iterative algorithm that uses finite elements. The motivating application is the analysis the RSFC maps over the cortical surface, derived from fMRI.  In this setting the adoption of a MV-PCA suffers of the high-dimensionality of the data with respect to the relatively small sample size. The adoption of an approach based on individual pre-smoothing of the functional samples, followed by a MV-PCA, gives smooth estimates of the PC functions; However, this pre-smoothing step tends to remove also part of useful information from the original data. The proposed SM-FPCA instead returns smooth PC functions that nevertheless are able to capture localized features of the estimated PC function.\n\nA further important feature of SM-FPCA is its computational efficiency. The most computationally intensive operation is the resolution of the linear system in the iterative algorithm. However this linear system enjoys two important properties. The first is the independence between its dimension, related to the number of nodes of the triangular mesh, and the number of point-wise observations available for each functional sample or the sample size. In fact, since its resolution time depends mostly on the mesh size, a mesh simplification approach [\\cite{SSRM2}] could be adopted to speed up the algorithm. The second and most fundamental property is the sparsity of the linear system. The use of a sparse solver allows an efficient computation of the solution. For instance, in the final application the dimension of the linear system is $64$K$\\times64$K. Despite its dimension, the solving time is less than a second. The application of the entire algorithm, for a fixed smoothing parameter, with 15 iterations is less than 15 seconds on a Intel Core i5-3470 3.20GHz workstation, with 4 GB of RAM.\n\n\\appendix\n\n\\section{Appendices}\n\\subsection{Well-posedness of the estimation problem (\\ref{eq:problem})}\n\\begin{proof}{Proposition 1.}\nWe exploit a characterization theorem [\\cite{Breass}, chapter 2] which states that if $G$ is a symmetric, positive definite, bilinear form on a vector space $L$, and $F$ is a linear functional on $L$, then $v$ is the unique minimizer of\n\n", "itemtype": "equation", "pos": 41328, "prevtext": "\nwhere $u_{i1}$, $u_{i2}$ are independent random variables, distributed as $u_{i1} \\sim \\mathcal{N}(0,\\sigma_1^2)$, $u_{i2} \\sim \\mathcal{N}(0,\\sigma_2^2)$, with $\\sigma_1 = 4$ and $\\sigma_2 = 2$. They represent the PC scores.\n\n\\begin{figure}[h] \n\n\\includegraphics[width=\\textwidth]{Figures/original.png}\n\\caption[]{From left to right, the triangulated surface representing the brainstem, a plot of the true first and second PC functions and a plot of a noisy observation on the brainstem.}\n\\label{fig:brain_stem}\n\\end{figure}\n\nThe smooth functions $x_i$ are sampled at locations $p_j \\in \\mathbb{R}^3$ with $j = 1,\\ldots, s$ coinciding with the nodes of the triangulates surface. Moreover at each of these points has been added to the functions a Gaussian noise with mean zero and standard deviation $\\sigma = 0.1$ to obtain the noisy observations denoted with $x_i(p_j)$. We are interested in recovering the smooth PC functions $v_1$ and $v_2$ from these noisy observations over $\\mathcal{M}_\\mathcal{T}$. We compare the proposed SM-FPCA technqiue to two alternative approaches.\n\nThe first basic approach we consider is a simple multivariate PCA (MV-PCA) applied to the data-matrix ${\\boldsymbol{\\mathbf{{X}}}}$. The PC functions are thus obtained by piecewise linear interpolation over the mesh $\\mathcal{M}_{\\mathcal{T}}$. Finally they are normalized to have unitary norm in $L^2(\\mathcal{M}_{\\mathcal{T}})$.\n\nA second natural approach is based on a pre-smoothing of the noisy observations that tries to recover the smooth functions $x_i, \\, i=1,\\ldots,n$ from their noisy observations $x_i(p_j)$, followed by a MV-PCA on the denoised evaluations of the functions on $p_j, \\, j=1,\\ldots,s$. The smoothing problem for a field defined on a Riemannian manifold is not trivial. In this case the smoothing technique applied is Iterated Heat Kernel (IHK) smoothing [\\cite{chung2005}]. The heat kernel smoothing of the noisy observation $x_i(p_j)$, is given by\n$K_\\eta \\times x_i(p_j) = \\int_{\\mathcal{M}} K_\\eta (p,q) x_i(p_j) dq$, where $\\eta$ is the smoothing parameter and $K_\\eta$ is the heat kernel, whose analytic expression can be extracted from the eigenfunctions of the Laplace-Beltrami operator. However, for numerical approximation, it can shown that for $\\eta$ small and for $q$ close to $p$ we have\n\n", "index": 59, "text": "\\begin{equation*}\nK_\\eta (p,q) \\approx \\frac{1}{(2 \\pi \\eta)^{\\frac{1}{2}}} exp[-\\frac{d^2(p,q)}{2 \\eta^2}].\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"K_{\\eta}(p,q)\\approx\\frac{1}{(2\\pi\\eta)^{\\frac{1}{2}}}exp[-\\frac{d^{2}(p,q)}{2%&#10;\\eta^{2}}].\" display=\"block\"><mrow><mrow><mrow><msub><mi>K</mi><mi>\u03b7</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mfrac><mn>1</mn><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mfrac><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo>-</mo><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03b7</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nin $V$ if and only if\n\n", "itemtype": "equation", "pos": 59543, "prevtext": "\nThe desired level of smoothing can be reached after $k$ iterations, thanks to the following property: $K_\\eta^{k} \\times f = K_\\eta \\times \\ldots \\times K_\\eta \\times f = K_{\\sqrt{k}\\eta}$. For a fixed bandwidth $\\eta$, the level of smoothing is determined by an optimal number of iterations selected via the F-test criterion outlined in \\cite{chung2005}. In these simulations the bandwidth has been set at $\\eta=2.5$, heuristically selecting the one with the best performance after some initial pilot studies. We will refer to this approach as IHK-PCA.\n\nThe proposed SM-FPCA technique is implemented as follows. For each PC we run Algorithm 1 with 15 iterations of the steps 2-4. For the choice of the optimal smoothing parameter $\\lambda$, both $K$-fold, with $K = 5$, and GCV approaches have been applied.\n\nThe reconstructed PC functions, using the three different approaches are shown in Figures~\\ref{fig:comparison1}-\\ref{fig:comparison2}. It is evident that applying the MV-PCA yields to a reconstruction far from the true, because of the absence of any spatial information. The reconstruction through the IHK-PCA approach and the SM-FPCA model are considerably more satisfactory.\n\n\\begin{figure}[!htb] \n\n\\includegraphics[width=\\textwidth]{Figures/PC2_1_comparative.png}\n\\caption[]{From left to right, contours of the first PC function reconstructed respectively with MV-PCA, IHK-PCA, SM-FPCA GCV and SM-FPCA K-fold. From a visual inspection, MV-PCA shows unsatisfactory results, while a better estimation is achieved by IHK-PCA and SM-FPCA. In particular SM-FPCA shows an almost optimal reconstruction of the contours' horizontal pattern in the original first PC function. }\n\\label{fig:comparison1}\n\\end{figure}\n\n\\begin{figure}[!htb] \n\n\\includegraphics[width=\\textwidth]{Figures/PC2_2_comparative.png}\n\\caption[]{From the left to the right, contours of the second PC function reconstructed respectively with MV-PCA, IHK-PCA, SM-FPCA GCV and SM-FPCA K-fold. A visual inspection confirms that a better estimation is achieved by IHK-PCA and SM-FPCA respect to MV-PCA.}\n\\label{fig:comparison2}\n\\end{figure}\n\n\\begin{figure}[H] \n\n\\includegraphics[width=\\textwidth]{Figures/boxplot.png}\n\\caption[]{Boxplots summarizing the performance of IHK-PCA and SM-FPCA. For the SM-FPCA both GCV and $K$-fold have been applied for the selection of the smoothing parameter.}\n\\label{fig:boxplots}\n\\end{figure}\nWhile the poor performance of the MV-PCA is evident, to assess the performance of the other two methods, we apply them to 100 datasets generated as previously detailed. The quality of estimated individual surfaces is then measured using the mean square error (MSE) over all the locations $p_j, \\, j=1,\\ldots,s$. MSEs are also used to evaluate the reconstruction of the PC scores vectors.  Another performance measure used is the principal angle between the subspace spanned by the estimated PC functions and the subspace spanned by the true PC functions, as used in \\cite{shen2008}. Intuitively, the principal angle measures how similar two subspaces are. For this purpose we construct the $s \\times 2$ matrices $\\mathbb{V} = (v_i(p_j))$ and $\\mathbb{\\hat{V}} = (\\hat{v}_i(p_j))$, where $\\hat{v}_i$ is the $i$th estimate of the true PC function $v_i$. Then we compute the orthonormal set of basis ${\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{V}$ and ${\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{\\hat{V}}$ from the QR decomposition of $\\mathbb{V}$ and $\\mathbb{\\hat{V}}$. The principal angle is defined as the angle $cos^{-1}(\\rho)$, where $\\rho$ is the minimum singular value of ${\\boldsymbol{\\mathbf{{Q}}}}^T_\\mathbb{\\hat{V}}{\\boldsymbol{\\mathbf{{Q}}}}_\\mathbb{V}$. The results are summarized in the boxplots in Figure~\\ref{fig:boxplots}, which compares the IHK-PCA and SM-FPCA algorithms with respect to the reconstruction's errors of the PC functions $v_1$ and $v_2$, the PC scores ${\\boldsymbol{\\mathbf{{u}}}}_1 = (u_{i1})$ and ${\\boldsymbol{\\mathbf{{u}}}}_2 = (u_{i2})$, the reconstructed signals $x_i = u_{i1} v_1 + u_{i2} v_2$ and the principle angles between the subspaces spanned by the true and estimated PC functions.\n\nThe boxplots highlight the fact that SM-FPCA provides the best estimates of the PC functions and corresponding scores vectors. The $K$-fold approach to the choice of the smoothing parameter in the SM-FPCA model performs better then the GCV approach.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application}\\label{sec:app}\nThe data set which we consider in this paper arises from the Human Connectome Project Consortium [\\cite{vanessen2012}], which is collecting data such as structural scans, resting-state and task-based functional MRI scans, and diffusion-weighted MRI scans from a large number of healthy volunteers to help elucidate the understanding of normal brain function.\nMany preprocessing considerations have already been resolved in the so called minimally preprocessed dataset. Among the various preprocessing pipelines applied to the HCP original data, of particular interest for us is the one named \\textit{fMRISurface} [\\cite{glasser2013}]. This pipeline provides a transformation of the 3D structural MRI and 4D signal from the functional MRI scan, so to enable the application of statistical analysis techniques on brain surfaces. For each subject, the personal cortical surface is extracted as a triangulated surface from the structural MRI and to each vertex of this mesh is associated a BOLD time-series derived from the BOLD signal of the underlying gray-matter ribbon. The cortical surfaces extracted are aligned to a template cortical surface generated from the cortical surfaces of 69 healthy adults. In practice this cortical surface is represented by two triangulated surfaces with 32k vertices, one for each hemisphere. In Figure~\\ref{fig:brain_mesh} the left hemisphere is shown. Through this anatomical transformation map, the patients' BOLD time-series, on the cortical surface, are coherently located to the vertices of the template cortical surface. The fMRI signal used for our analysis has been acquired in absence of any task and for this reason is also called resting state fMRI. Finally each time-series is filtered to the band of frequencies $[0.009,0.08]$Hz.\n\n\nAs already mentioned in Section~\\ref{sec:intro}, a classic approach in the study of the resting state fMRI, is to exploit the time dimension of the data, for the extraction of a connectivity measure among the different parts of the cortical surface. A standard choice for this purpose is the computation of the temporal correlation. It first consists of identifying a Region of Interest (ROI) on the cortical surface. For each subject, all the time-series in the ROI are used to find a representative mean time-series. To each vertex of the cortical surface we associate the pairwise correlation of the time-series located in that vertex with the time-series representative of the ROI. Finally each correlation value is transformed using Fisher's r-to-z transformation, yielding a resting state functional connectivity (RSFC) map for each subject. The total number of subjects considered for this analysis is 491.\n\nFor the choice of the ROI, we consider the cortical parcellation derived in \\cite{gordon2014}, where a group-avarage boundary map of the cortical surface is derived from resting state fMRI (Figure~\\ref{fig:parcellation}). The identified cortical areas are unlikely to correspond the individual parcellation of each subject, since they are derived from a group average study. However, they can serve as a reasonable ROIs in individual subjects. The parcel that served as ROI in the following analysis is visible in red in Figure~\\ref{fig:parcellation}.\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/parcellation_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/parcellation_2.png}\n\\end{subfigure}\n\\caption{Parcellation of the cortical surface derived in \\cite{gordon2014}. In red the Region of Interest chosen for the computation of the RSFC maps. This region is localized on an area of the cerebral cortex called precuneus.}\n\\label{fig:parcellation}\n\\end{figure}\nFor the chosen ROI, a snapshot of the RSFC map of one subject is shown in Figure~\\ref{fig:RSFC}.\n\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/fisher_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/fisher_2.png}\n\\end{subfigure}\n\\caption{A snapshot of the RSFC map of one subject.}\n\\label{fig:RSFC}\n\\end{figure}\n\nThe mean RSFC map is shown in Figure~\\ref{fig:mean}. As expected high correlation values are visible inside the ROI. The mean RSFC over 491 subjects shows a variability coherent with the parcellation, in the sense that the vertices inside each parcel show similar values. We wish now to understand which are the main modes of variation of this RSFC maps among the different subjects, by applying a PCA.\n\n\\begin{figure}[!htb]\n\\centering\n\\begin{subfigure}{.55\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean1.png}\n\\end{subfigure}\n\\begin{subfigure}{.55\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean2.png}\n\\end{subfigure}\n\\caption{The mean RSFC map computed over 491 subject. As expected, high correlation values are visible inside the ROI.}\n\\label{fig:mean}\n\\end{figure}\n\n\nThe first three PC functions, estimated with SM-FPCA, are shown in Figures~\\ref{fig:PC1_brain}-\\ref{fig:PC2_brain}-\\ref{fig:PC3_brain} as compared to the PC functions derived from MV-PCA and IHK-PCA. The choice of the smoothing parameter for the SM-FPCA is based on the $K$-fold cross validation, with $K = 5$.\n\nThe PC functions estimated from the MV-PCA shows an excessive variability, since the sample size is not sufficiently large to deal with the extremely high dimensionality of the data. In fact, even recent attempts to model the subject variability from resting state fMRI leads to the conclusion that spatial mismatches, introduce by the alignment problem, are one of the biggest sources of currently observable differences between subjects [\\cite{harrison2015}]. This registration process can result in misalignments, due to the lack to functional regions being perfectly coincident or due to situations where the local topology is strongly different among subjects. These misalignments can introduce fictitious effects on the computed PC functions. Data misalignment is a well known problem in FDA [\\cite{ramsay2005}]. For functional data with one-dimensional domains, typical approaches are based on shifting or (monotone) transformations of the domain of each function. But neither shifting nor monotonic transformations make sense on a generic non-euclidean domain, so it is not clear how to generalize the standard FDA approaches. The introduction of a smoothing penalty in the PCA model should reduce the variability effects due to misalignment. In fact the smoothing parameter in the SM-FPCA algorithm can be seen as a further degree of freedom that allows a multiscale analysis, meaning that by increasing the smoothing penalty parameter is possible to constrain the results to show only the macroscopical effects of the phenomena and to remove the artifacts introduced by the preprocessing steps.\n\nThe PC functions estimated through IHK-PCA and SM-FPCA seem both to resolve the problem of the presence of some residual noise in the PC function estimates. However, we would like to emphasize the fact that, for instance, the third PC function computed with IHK-PCA shows some differences with the one computed with SM-FPCA. This is probably due to the fact that an individual pre-smoothing approach with the presented data tends to delete part of the information contained in the single RSFC map, while the SM-FPCA model incorporates all the noisy data as is and introduces a smoothness penalization directly on the PC function estimates. Contrary to MV-PCA and IHK-PCA, in all three PC functions, SM-FPCA shows a satisfactory level of smoothness, without deleting sharper changes in some locations, which may indicate the presence of a boundary between two cortical areas, since it is well known that the cortical surface is organized in a number of interacting cortical areas.\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{Figures/brain_comparative_PC1_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{Figures/brain_comparative_PC1_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the first PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC1_brain}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC2_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC2_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the second PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC2_brain}\n\\end{figure}\n\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC3_1.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/brain_comparative_PC3_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of the third PC function computed respectively with MV-PCA, IHK-PCA and SM-FPCA.}\n\\label{fig:PC3_brain}\n\\end{figure}\n\nFor the purpose of interpretation of the PC functions we might prefer to plot the functions $\\mu \\pm 2\\sigma f$, where $\\mu$ denotes the mean RSFC map, $\\sigma$ denotes the standard deviation of the PC scores vector and $f$ denotes the associated PC function. In Figure~\\ref{fig:PC1_interpretation} we show the described plot for the first PC function. We can observe that while the high correlation value in the ROI and inferior parietal are in first approximation preserved from subject to subject, a high variability between subjects can be observed in the areas surrounding the ROI and the inferior parietal, which is understood due to individual inter-subject differences [\\cite{buckner2008brain} and references therein].\n\\begin{figure}[H]\n\\centering\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_meno2std_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean_pc_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_piu2std_1.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_meno2std_2.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/mean_pc_2.png}\n\\end{subfigure}\n\\begin{subfigure}{.33\\textwidth}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{Figures/pc1_piu2std_2.png}\n\\end{subfigure}\n\\caption{From left to right, two views of $\\mu - 2\\sigma f$, $\\mu$, $\\mu + 2\\sigma f$, where $\\mu$ denotes the mean RSFC map, $\\sigma$ denotes the standard deviation of the first PC scores vector and $f$ denotes the first PC function.}\n\\label{fig:PC1_interpretation}\n\\end{figure}\n\n\\section{Discussion}\\label{sec:discussion}\nIn this paper we introduce a novel PCA technique that can handle functional data located over a two-dimensional manifold. The adopted approach is based on a regularized PCA model. In particular a smoothness penalty term that measures the curvature of a function over a manifold is introduced and the estimation problem is solved via an iterative algorithm that uses finite elements. The motivating application is the analysis the RSFC maps over the cortical surface, derived from fMRI.  In this setting the adoption of a MV-PCA suffers of the high-dimensionality of the data with respect to the relatively small sample size. The adoption of an approach based on individual pre-smoothing of the functional samples, followed by a MV-PCA, gives smooth estimates of the PC functions; However, this pre-smoothing step tends to remove also part of useful information from the original data. The proposed SM-FPCA instead returns smooth PC functions that nevertheless are able to capture localized features of the estimated PC function.\n\nA further important feature of SM-FPCA is its computational efficiency. The most computationally intensive operation is the resolution of the linear system in the iterative algorithm. However this linear system enjoys two important properties. The first is the independence between its dimension, related to the number of nodes of the triangular mesh, and the number of point-wise observations available for each functional sample or the sample size. In fact, since its resolution time depends mostly on the mesh size, a mesh simplification approach [\\cite{SSRM2}] could be adopted to speed up the algorithm. The second and most fundamental property is the sparsity of the linear system. The use of a sparse solver allows an efficient computation of the solution. For instance, in the final application the dimension of the linear system is $64$K$\\times64$K. Despite its dimension, the solving time is less than a second. The application of the entire algorithm, for a fixed smoothing parameter, with 15 iterations is less than 15 seconds on a Intel Core i5-3470 3.20GHz workstation, with 4 GB of RAM.\n\n\\appendix\n\n\\section{Appendices}\n\\subsection{Well-posedness of the estimation problem (\\ref{eq:problem})}\n\\begin{proof}{Proposition 1.}\nWe exploit a characterization theorem [\\cite{Breass}, chapter 2] which states that if $G$ is a symmetric, positive definite, bilinear form on a vector space $L$, and $F$ is a linear functional on $L$, then $v$ is the unique minimizer of\n\n", "index": 61, "text": "\\begin{equation*}\nG(v,v) - 2F(v)\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"G(v,v)-2F(v)\" display=\"block\"><mrow><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nMoreover, there is at most one solution to problem (\\ref{eq:var_app}).\n\nThe desired result follows from application of the above theorem considering the vector space $L = H^2(\\mathcal{M})$, the symmetric, positive definite, bilinear form $G(f,\\varphi):= \\sum_{j=1}^p\\varphi(p_j)f(p_j) + \\lambda \\int_\\mathcal{M} \\Delta\\varphi \\Delta f$ and the linear functional\\\\ $F(f)=\\sum_{j=1}^p f(p_j) \\sum_{i=1}^n x_i(p_j)u_i$.\nPositive definitiveness of the form $G$, in $ H^2(\\mathcal{M})$, is shown by the following argument.\nSuppose that $G(f,f)=0$ for some $f \\in H^2(\\mathcal{M})$; then $\\int_\\mathcal{M} \\Delta_\\mathcal{M}^2 f=0$ and $\\sum_{j=1}^p f(p_j)^2=0$. Each element $f \\in H^2(\\mathcal{M})$ can be written such that, for any $p \\in \\mathcal{M}$, $f(p) = \\tilde{f}(p) + c$, with $\\tilde{f} \\in U =\\{ \\tilde{f} \\in H^2(\\mathcal{M}): \\int_\\mathcal{M} \\tilde{f} = 0 \\}$ and $c$ a constant. The solution of $\\Delta_\\mathcal{M} \\tilde{f} = 0$ in $U$ exists unique and is $\\tilde{f}=0$ [\\cite{dziuk2013}]. Thus $\\int_{\\mathcal{M}} \\Delta_\\mathcal{M}^2 f=0$ for $f \\in H^2(\\mathcal{M})$ implies that $f(p)=c$, for any $p \\in \\mathcal{M}$, then $\\sum_{j=1}^p f(p_j)^2 = p c^2$. But $pc^2=0$ if and only if $c=0$, so $f(\\cdot)=0$. Consequently, $G$ is positive definite on $H^2(\\mathcal{M})$.\n\nThe estimator $\\hat{f}$ is thus\n\n", "itemtype": "equation", "pos": 59614, "prevtext": "\nin $V$ if and only if\n\n", "index": 63, "text": "\\begin{equation}\\label{eq:var_app}\nG(v,\\varphi) = F(\\varphi) \\qquad \\text{ for all } \\varphi \\in L.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"G(v,\\varphi)=F(\\varphi)\\qquad\\text{ for all }\\varphi\\in L.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo>,</mo><mi>\u03c6</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c6</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mrow><mtext>\u00a0for all\u00a0</mtext><mo>\u2062</mo><mi>\u03c6</mi></mrow><mo>\u2208</mo><mi>L</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor every $\\varphi \\in H^2(\\mathcal{M})$.\n\n\\end{proof}\n\n\n\\subsection{Reformulation of the estimation problem}\n\nThe problem of finding $f \\in H^2(\\mathcal{M})$ that satisfies condition (\\ref{eq:eulero-lagrange2}) for every $\\varphi \\in H^2(\\mathcal{M})$ can be rewritten as the problem of finding $(\\hat{f},g) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$ that satisfies:\n\n", "itemtype": "equation", "pos": 61049, "prevtext": "\nMoreover, there is at most one solution to problem (\\ref{eq:var_app}).\n\nThe desired result follows from application of the above theorem considering the vector space $L = H^2(\\mathcal{M})$, the symmetric, positive definite, bilinear form $G(f,\\varphi):= \\sum_{j=1}^p\\varphi(p_j)f(p_j) + \\lambda \\int_\\mathcal{M} \\Delta\\varphi \\Delta f$ and the linear functional\\\\ $F(f)=\\sum_{j=1}^p f(p_j) \\sum_{i=1}^n x_i(p_j)u_i$.\nPositive definitiveness of the form $G$, in $ H^2(\\mathcal{M})$, is shown by the following argument.\nSuppose that $G(f,f)=0$ for some $f \\in H^2(\\mathcal{M})$; then $\\int_\\mathcal{M} \\Delta_\\mathcal{M}^2 f=0$ and $\\sum_{j=1}^p f(p_j)^2=0$. Each element $f \\in H^2(\\mathcal{M})$ can be written such that, for any $p \\in \\mathcal{M}$, $f(p) = \\tilde{f}(p) + c$, with $\\tilde{f} \\in U =\\{ \\tilde{f} \\in H^2(\\mathcal{M}): \\int_\\mathcal{M} \\tilde{f} = 0 \\}$ and $c$ a constant. The solution of $\\Delta_\\mathcal{M} \\tilde{f} = 0$ in $U$ exists unique and is $\\tilde{f}=0$ [\\cite{dziuk2013}]. Thus $\\int_{\\mathcal{M}} \\Delta_\\mathcal{M}^2 f=0$ for $f \\in H^2(\\mathcal{M})$ implies that $f(p)=c$, for any $p \\in \\mathcal{M}$, then $\\sum_{j=1}^p f(p_j)^2 = p c^2$. But $pc^2=0$ if and only if $c=0$, so $f(\\cdot)=0$. Consequently, $G$ is positive definite on $H^2(\\mathcal{M})$.\n\nThe estimator $\\hat{f}$ is thus\n\n", "index": 65, "text": "\\begin{equation} \\label{eq:eulero-lagrange2}\n\\sum_{j=1}^p \\varphi(p_j)\\hat{f}(p_j) + \\lambda \\int_\\mathcal{M} \\Delta_\\mathcal{M} \\varphi \\Delta_\\mathcal{M} \\hat{f} = \\sum_{j=1}^p \\varphi(p_j)\\sum_{i=1}^n x_i(p_j)u_i\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\sum_{j=1}^{p}\\varphi(p_{j})\\hat{f}(p_{j})+\\lambda\\int_{\\mathcal{M}}\\Delta_{%&#10;\\mathcal{M}}\\varphi\\Delta_{\\mathcal{M}}\\hat{f}=\\sum_{j=1}^{p}\\varphi(p_{j})%&#10;\\sum_{i=1}^{n}x_{i}(p_{j})u_{i}\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mi>\u03c6</mi><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor all $(\\varphi,v) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$. In fact, if the pair of functions $(\\hat{f},g) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$ satisfies condition (\\ref{eq:eulero-double}) for all $(\\varphi,v) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$, then $\\hat{f}$ also satisfies problem (\\ref{eq:eulero-lagrange2}). In contrast, if $\\hat{f} \\in H^2(\\mathcal{M})$ satisfies problem (\\ref{eq:eulero-lagrange2}), then the pair $(\\hat{f},\\Delta \\hat{f})$ automatically satisfies the two equations in problem (\\ref{eq:eulero-double}).\nOwing to integration by part and to the fact that $\\mathcal{M}$ has no boundaries, we get:\n\\begin{eqnarray*}\n\\int_\\mathcal{M} (\\Delta_\\mathcal{M} \\varphi) g = - \\int_\\mathcal{M} \\nabla_\\mathcal{M} \\varphi \\nabla_\\mathcal{M} g\\\\\n\\int_\\mathcal{M} v (\\Delta_\\mathcal{M} \\hat{f}) = - \\int_\\mathcal{M} \\nabla_\\mathcal{M} v \\nabla_\\mathcal{M} \\hat{f}\n\\end{eqnarray*}\n\nNow, asking the auxiliary function $g$ and of the test functions $v$ to be such that $g, v \\in H^1(\\mathcal{M})$, the problem of finding $\\hat{f}\\in H^2(\\mathcal{M})$ that satisfies (\\ref{eq:eulero-lagrange2}) for each $\\varphi \\in H^2(\\mathcal{M})$ can be reformulated  as finding $(\\hat{f},g) \\in (H^1(\\mathcal{M}) \\cap C^0(\\mathcal{M}) ) \\times H^1(\\mathcal{M})$\n\n", "itemtype": "equation", "pos": 61651, "prevtext": "\nfor every $\\varphi \\in H^2(\\mathcal{M})$.\n\n\\end{proof}\n\n\n\\subsection{Reformulation of the estimation problem}\n\nThe problem of finding $f \\in H^2(\\mathcal{M})$ that satisfies condition (\\ref{eq:eulero-lagrange2}) for every $\\varphi \\in H^2(\\mathcal{M})$ can be rewritten as the problem of finding $(\\hat{f},g) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$ that satisfies:\n\n", "index": 67, "text": "\\begin{align}\\label{eq:eulero-double}\n\\begin{cases}\n\\sum_{j=1}^p \\varphi(p_j)\\hat{f}(p_j) + \\lambda \\int_\\mathcal{M} (\\Delta \\varphi) g = \\sum_{j=1}^p \\varphi(p_j)\\sum_{i=1}^n x_i(p_j)u_i\\\\\n\\int_\\mathcal{M} vg - \\int_\\mathcal{M} v (\\Delta \\hat{f}) = 0\n\\end{cases}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{cases}\\sum_{j=1}^{p}\\varphi(p_{j})\\hat{f}(p_{j})+\\lambda%&#10;\\int_{\\mathcal{M}}(\\Delta\\varphi)g=\\sum_{j=1}^{p}\\varphi(p_{j})\\sum_{i=1}^{n}x%&#10;_{i}(p_{j})u_{i}\\\\&#10;\\int_{\\mathcal{M}}vg-\\int_{\\mathcal{M}}v(\\Delta\\hat{f})=0\\end{cases}\" display=\"inline\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03c6</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi></mrow></mrow></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>v</mi><mo>\u2062</mo><mi>g</mi></mrow></mrow><mo>-</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd><mtd/></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor all $(\\varphi,v) \\in (H^1(\\mathcal{M}) \\cap C^0(\\mathcal{M}) ) \\times H^1(\\mathcal{M})$; Moreover, the theory of problems of elliptic regularity ensure that such $\\hat{f}$ still belongs to $H^2(\\mathcal{M})$ [\\cite{dziuk2013} and reference therein].\nFinally the discrete estimators $\\hat{f}_h, \\hat{g}_h \\in V \\subset H^1(\\mathcal{M})$ are obtained solving\n\n", "itemtype": "equation", "pos": 63210, "prevtext": "\nfor all $(\\varphi,v) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$. In fact, if the pair of functions $(\\hat{f},g) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$ satisfies condition (\\ref{eq:eulero-double}) for all $(\\varphi,v) \\in H^2(\\mathcal{M}) \\times L^2(\\mathcal{M})$, then $\\hat{f}$ also satisfies problem (\\ref{eq:eulero-lagrange2}). In contrast, if $\\hat{f} \\in H^2(\\mathcal{M})$ satisfies problem (\\ref{eq:eulero-lagrange2}), then the pair $(\\hat{f},\\Delta \\hat{f})$ automatically satisfies the two equations in problem (\\ref{eq:eulero-double}).\nOwing to integration by part and to the fact that $\\mathcal{M}$ has no boundaries, we get:\n\\begin{eqnarray*}\n\\int_\\mathcal{M} (\\Delta_\\mathcal{M} \\varphi) g = - \\int_\\mathcal{M} \\nabla_\\mathcal{M} \\varphi \\nabla_\\mathcal{M} g\\\\\n\\int_\\mathcal{M} v (\\Delta_\\mathcal{M} \\hat{f}) = - \\int_\\mathcal{M} \\nabla_\\mathcal{M} v \\nabla_\\mathcal{M} \\hat{f}\n\\end{eqnarray*}\n\nNow, asking the auxiliary function $g$ and of the test functions $v$ to be such that $g, v \\in H^1(\\mathcal{M})$, the problem of finding $\\hat{f}\\in H^2(\\mathcal{M})$ that satisfies (\\ref{eq:eulero-lagrange2}) for each $\\varphi \\in H^2(\\mathcal{M})$ can be reformulated  as finding $(\\hat{f},g) \\in (H^1(\\mathcal{M}) \\cap C^0(\\mathcal{M}) ) \\times H^1(\\mathcal{M})$\n\n", "index": 69, "text": "\\begin{align}\\label{eq:weak_eulero}\n\\begin{cases}\n\\sum_{j=1}^p \\varphi(p_j)\\hat{f}(p_j) + \\lambda \\int_\\mathcal{M} \\nabla \\varphi \\nabla g = \\sum_{j=1}^p \\varphi(p_j)\\sum_{i=1}^n x_i(p_j)u_i\\\\\n\\int_\\mathcal{M} vg - \\int_\\mathcal{M} \\nabla v \\nabla \\hat{f} = 0\n\\end{cases}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{cases}\\sum_{j=1}^{p}\\varphi(p_{j})\\hat{f}(p_{j})+\\lambda%&#10;\\int_{\\mathcal{M}}\\nabla\\varphi\\nabla g=\\sum_{j=1}^{p}\\varphi(p_{j})\\sum_{i=1}%&#10;^{n}x_{i}(p_{j})u_{i}\\\\&#10;\\int_{\\mathcal{M}}vg-\\int_{\\mathcal{M}}\\nabla v\\nabla\\hat{f}=0\\end{cases}\" display=\"inline\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>\u03c6</mi></mrow><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>g</mi></mrow></mrow></mrow></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mi>v</mi><mo>\u2062</mo><mi>g</mi></mrow></mrow><mo>-</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>v</mi></mrow><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd><mtd/></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nfor all $\\varphi_h, v_h \\in V$. A generic function in $V$ can be written as the linear combination of the finite number of basis spanning $V$. This allows the solution $\\hat{f}_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ to be characterized by the linear system (\\ref{eq:linear_system}) in the original paper.\n\n\\subsection{Simulation on the sphere}\\label{app2}\nHere we present some further simulation studies on a domain $\\mathcal{M}$ that is a sphere centered on the origin and with radius $r=1$, approximated by the triangulated surface $\\mathcal{M}_\\mathcal{T}$ in Figure~\\ref{fig:mesh_s}.\n\\begin{figure}[H] \n\n\\centering\n\\includegraphics[width=0.4\\textwidth]{Figures/mesh_sphere.png}\n\\caption[]{The triangulated surface approximating the sphere with 488 points.}\n\\label{fig:mesh_s}\n\\end{figure}\nWe generate $n = 50$ smooth functions $x_1,\\ldots,x_{50}$ on $\\mathcal{M}_\\mathcal{T}$ by\n\n", "itemtype": "equation", "pos": 63855, "prevtext": "\nfor all $(\\varphi,v) \\in (H^1(\\mathcal{M}) \\cap C^0(\\mathcal{M}) ) \\times H^1(\\mathcal{M})$; Moreover, the theory of problems of elliptic regularity ensure that such $\\hat{f}$ still belongs to $H^2(\\mathcal{M})$ [\\cite{dziuk2013} and reference therein].\nFinally the discrete estimators $\\hat{f}_h, \\hat{g}_h \\in V \\subset H^1(\\mathcal{M})$ are obtained solving\n\n", "index": 71, "text": "\\begin{align*}\n\\begin{cases}\n&\\! \\int_{\\mathcal{M}_\\mathcal{T}} \\nabla_{\\mathcal{M}_\\mathcal{T}} \\hat{f}_h \\nabla_{\\mathcal{M}_\\mathcal{T}} \\varphi_h - \\int_{\\mathcal{M}_\\mathcal{T}} \\hat{g}_h \\varphi_h = 0\\\\\n&\\! \\lambda \\!\\int_{\\mathcal{M}_\\mathcal{T}} \\nabla_{\\mathcal{M}_\\mathcal{T}} \\hat{g}_h \\nabla_{\\mathcal{M}_\\mathcal{T}} v_h + \\sum\\limits_{j=1}^s \\hat{f}_h(p_j)v_h(p_j) = \\sum\\limits_{j=1}^s v_h(p_j) \\sum\\limits_{i=1}^n x_i(p_j)u_i\n\\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{cases}&amp;\\!\\int_{\\mathcal{M}_{\\mathcal{T}}}\\nabla_{\\mathcal{%&#10;M}_{\\mathcal{T}}}\\hat{f}_{h}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}\\varphi_{h}-\\int%&#10;_{\\mathcal{M}_{\\mathcal{T}}}\\hat{g}_{h}\\varphi_{h}=0\\\\&#10;&amp;\\!\\lambda\\!\\int_{\\mathcal{M}_{\\mathcal{T}}}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}%&#10;\\hat{g}_{h}\\nabla_{\\mathcal{M}_{\\mathcal{T}}}v_{h}+\\sum\\limits_{j=1}^{s}\\hat{f%&#10;}_{h}(p_{j})v_{h}(p_{j})=\\sum\\limits_{j=1}^{s}v_{h}(p_{j})\\sum\\limits_{i=1}^{n%&#10;}x_{i}(p_{j})u_{i}\\end{cases}\" display=\"inline\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mo largeop=\"true\" lspace=\"0.8pt\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub></mrow><mo>\u2062</mo><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mi>\u03c6</mi><mi>h</mi></msub></mrow></mrow></mrow><mo>-</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><msub><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>\u2062</mo><msub><mi>\u03c6</mi><mi>h</mi></msub></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded lspace=\"-1.7pt\" width=\"-3.4pt\"><mi>\u03bb</mi></mpadded><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mrow><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub></mrow><mo>\u2062</mo><mrow><msub><mo>\u2207</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></msub><mo>\u2061</mo><msub><mi>v</mi><mi>h</mi></msub></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>v</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><msub><mi>v</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.03670.tex", "nexttext": "\nwhere $v_1$ and $v_2$ represent the two PC functions with expressions\n\\begin{eqnarray*}\n\\begin{cases}\nv_1(x,y,z)= \\frac{1}{2} \\sqrt{\\frac{15}{\\pi}} \\frac{xy}{r^2} \\\\\nv_2(x,y,z)= \\frac{3}{4} \\sqrt{\\frac{35}{\\pi}} \\frac{xy(x^2-y^2)}{r^4}\n\\end{cases}\n\\end{eqnarray*}\nand $u_{i1}$, $u_{i2}$ represent the PC scores, generated independently and distributed as $u_{i1} \\sim N(0, \\sigma_1^2)$, $u_{i2} \\sim N(0, \\sigma_2^2)$ with $\\sigma_1 = 4$, $\\sigma_2 = 2$. The PC functions are two components of the Spherical Harmonics basis set, so they are orthonormal on the sphere, i.e. $\\int_\\mathcal{M} v_i^2 = 1$ for each $i \\in \\{1,2\\}$ and $\\int_\\mathcal{M} v_iv_k = 0$ for each $i \\neq k$ with $i,k \\in \\{1,2\\}$. The PC functions are plotted in Figure~\\ref{fig:original_s}.\nThe functions $x_i$ are sampled at locations coinciding with the nodes of the\nmesh in Figure~\\ref{fig:mesh_s}. At these locations, a Gaussian white\nnoise with standard deviation $\\sigma = 0.1$ has been added to the true function $x_i$. We are then interested in recovering the smooth PC functions $v_1$ and $v_2$ from these noisy observations.\n\n\\begin{figure}[H] \n\n\\includegraphics[width=1\\textwidth]{Figures/sphere_original.png}\n\\caption[]{From the left to the righta plot of the true first and second PC functions.}\n\\label{fig:original_s}\n\\end{figure}\n\nWe apply the proposed  SM-FPCA method, choosing the optimal smoothing parameter $\\lambda$, both with the K-fold and with GCV. We compare to the approach based on pre-smoothing followed by MV-PCA on the denoised evaluations of the functions at the locations $p_j$, $j = 1,\\ldots,p$. In this case, the smoothing techniques used is Spherical Splines [\\cite{wahba1981}], using the implementation in the R package \\textit{mgcv}. The smoothing parameter choice is based on the GCV criterion. We will refer to this approach as SSpline-PCA. The results are summarized in Figure~\\ref{fig:boxplots_s}.\n\n\\begin{figure}[H] \n\n\\includegraphics[width=1\\textwidth]{Figures/boxplot_sphere.png}\n\\caption[]{Boxplots summarizing the performance of SSpline-PCA and SM-FPCA. For the SM-FPCA both GCV and $K$-fold has been applied for the selection of the smoothing parameter.}\n\\label{fig:boxplots_s}\n\\end{figure}\n\nThe best estimates of the first two PC functions and corresponding scores are provided by the proposed SM-FPCA with selection of the smoothing parameter base on the $K$-fold approach. SSpline-PCA does a comparable job on the first principal component, but a significantly worst on the second. A possible explanation for this is the fact that SSpline-PCA tends to over-smooth the data, due to the low signal-to-noise setting of the simulations. This results in good performances for the first PC, but causes a loss of information that worsen the estimation of the second PC. The measure based on the computation of the principal angle between the original space spanned by $\\{v_i\\}_{i=1,2}$ and the estimated PC functions $\\{ \\hat{v}_i \\}_{i=1,2}$ emphasizes the good performance of the introduced algorithm.\n\n\n\n\\bibliographystyle{abbrvnat}\n\n\\bibliography{Bibliography} \n\n\n\n", "itemtype": "equation", "pos": 65244, "prevtext": "\nfor all $\\varphi_h, v_h \\in V$. A generic function in $V$ can be written as the linear combination of the finite number of basis spanning $V$. This allows the solution $\\hat{f}_h(p) = {\\boldsymbol{\\mathbf{{\\psi}}}}(p)^T {\\boldsymbol{\\mathbf{{\\hat{f}}}}}$ to be characterized by the linear system (\\ref{eq:linear_system}) in the original paper.\n\n\\subsection{Simulation on the sphere}\\label{app2}\nHere we present some further simulation studies on a domain $\\mathcal{M}$ that is a sphere centered on the origin and with radius $r=1$, approximated by the triangulated surface $\\mathcal{M}_\\mathcal{T}$ in Figure~\\ref{fig:mesh_s}.\n\\begin{figure}[H] \n\n\\centering\n\\includegraphics[width=0.4\\textwidth]{Figures/mesh_sphere.png}\n\\caption[]{The triangulated surface approximating the sphere with 488 points.}\n\\label{fig:mesh_s}\n\\end{figure}\nWe generate $n = 50$ smooth functions $x_1,\\ldots,x_{50}$ on $\\mathcal{M}_\\mathcal{T}$ by\n\n", "index": 73, "text": "\\begin{equation*}\nx_i = u_{i1}v_1 + u_{i2}v_2, \\hspace{35pt} i = 1,\\ldots,n\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"x_{i}=u_{i1}v_{1}+u_{i2}v_{2},\\hskip 35.0pti=1,\\ldots,n\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mn>2</mn></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mn>2</mn></msub></mrow></mrow></mrow><mo rspace=\"37.5pt\">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow></math>", "type": "latex"}]