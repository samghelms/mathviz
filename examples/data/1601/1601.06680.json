[{"file": "1601.06680.tex", "nexttext": "\nIn our implementation of the discrete entropy estimator we added the simple \\cite{miller1955nbi} bias correction term to finally obtain\n\n", "itemtype": "equation", "pos": 5780, "prevtext": "\n\n\\title{Conditional distribution variability measures for causality detection}\n\n\\author{\\name Jos\\'e A. R. Fonollosa \\email jose.fonollosa@upc.edu \\\\\n       \\addr \\\\Universitat Polit\\`ecnica de Catalunya. Barcelona Tech.\n      c/ Jordi Girona 1-3, Edifici D5\\\\\n      Barcelona 08034, Spain\n\n\n\n\n\n}\n\n\n\\editor{}\n\n\\maketitle\n\n\\begin{abstract}\nIn this paper we derive variability measures for the conditional probability distributions of a pair of random variables, and we study its application in the inference of causal-effect relationships. We also study the combination of the proposed measures with standard statistical measures in the the framework of the ChaLearn cause-effect pair challenge. The developed model obtains an AUC score of 0.82 on the final test database and ranked second in the challenge.\n\\end{abstract}\n\n\\begin{keywords}\ncausality detection, cause-effect pair challenge\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nThere is no doubt that causality detection is a task of great practical interest. In a wide sense, attributing causes to effects guides all our efforts to understand our world and to solve any kind of real life problems. There is not, however, a simple and general definition of causality and the topic remains a staple in contemporary philosophy.\n\nThe development of analytical methods for detecting a cause-effect relationship in a set of ordered pairs of values also lacks of a universal formal definition of causality. From a pure statistical point of view any bivariate joint distribution can be expressed as the product of any of the two marginal distributions by the conditional distribution of the other variable given the first. And these two equivalent expressions can also be used to explain the generation process in both directions.\n\nIn order to be able to attack the causality detection problem we need to introduce one or more assumptions about the generation process or the shape of the joint distribution. Most of those assumptions come from the Occam's razor succinctness principle. We expect to have a simpler model in the correct direction that in the opposite, i.e. the algorithmic complexity or minimum description length of the generation process should be lower in the true causal direction than in the opposite direction. To be more precise, if the random variable $X$ is the cause of the random variable $Y$ we usually expect the conditional distribution $p(Y|X=x)$ to be unimodal or at least to have a similar shape for different given values $x$ of $X$.\n\nSeveral methods have been proposed in the literature as practical measures of the uncomputable Kolmogorov complexity of the generation model in the hypothetical causal direction. See \\cite{Statnikov2012} for a review of the usual assumptions and generation models. In this paper we develop new causality measures based on the assumption that the shape of the conditional distribution $p(Y|X=x)$ tends to be very similar for different values of $x$ if the random variable $X$ is the cause of $Y$. The main difference with respect to previous methods is that we do not impose a strict independence between the conditional distribution (or noise) and the cause. However we still expect the conditional distribution to have a similar shape or similar statistical characteristics for different values $x$ of the cause.\n\nThe developed features are combined with standard statistical features following a machine learning approach: the selection of a good set of relevant features and of an adequate learning model.\n\n\\section{Features}\nIn this section we enumerate the features used by our model. All the measures are computed in both directions, i.e., exchanging the role of the two random variables X and Y, except if the measure is symmetric.\n\n\\subsection{Preprocessing}\n    \\textbf{Mean and Variance Normalization.}\nNumerical data is normalized to have zero mean and unit variance. All of our features are scale and mean invariant.\\[4 pt]\n    \\textbf{Discretization of numerical variables.}\\label{sec:quantization}\nDiscrete measures as the discrete entropy and discrete mutual information are also used as features of numerical date after a discretization or quantization process. The quantization uses $2*maxdev*sfactor+1$ equally spaced segments of $\\sigma/sfactor$ length and truncates all absolute values above $maxdev*\\sigma$. For almost all measures requiring a discretization of the input we selected $sfactor=3$ and $maxdev=3$ in our experiments, i.e, a quantization to 19 different values.\\[4 pt]\n    \\textbf{Relabeling of categorical variables.}\n    \\label{sec:sort}\nThe specific values assigned to categorical data are assumed to have no information by themselves. However, in some cases we considered the calculation of \\textit{numerical} measures (as skewness) for categorical variables. For these computations we assigned integer values to the categorical variables as a function of its probability. After the relabeling of variables with M different categories we have: $p(x=0) \\geq p(x=1) \\ldots \\geq p(x=M-1)$. This step let us obtain \\textit{numerical} features of categorical variables that do not depend on the labels but on the sorted probabilities.\n\n\\subsection{Information-theoretic measures}\nIn the baseline system we include the standard information-theoretic features as entropy and mutual information. Both the discrete and the continuous version of the entropy estimator are applied to numerical and categorical data after the preprocessing described above.\\[4 pt]\n    \\textbf{Discrete entropy and joint entropy.}\nThe entropy of a random variable is a information-theoretic measure that quantifies the uncertainty in a random variable. In the case of a discrete random variable X, the entropy of X is defined as:\n\n", "index": 1, "text": "\\[H(X) = -\\sum_{x} p(x)\\operatorname{log}(p(x))\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"H(X)=-\\sum_{x}p(x)\\operatorname{log}(p(x))\" display=\"block\"><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>x</mi></munder><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>log</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\nwhere M is the number of different values of the random variable X in the data set. We also considered the normalized version\t$\\hat{H}_n(X) = \\hat{H}_m(X)/\\operatorname{log}(N)$ where $log(N)$ is the maximum entropy a discrete random variable with N different values. The definition and estimation of the entropy can be extended to a pair of variables replacing the counts $n_x$ by the counts $n_{x,y}$ of the number of times the pair $(x,y)$ appears in the sample set.\\[4 pt]\n    \\textbf{Discrete conditional entropy.}\nThe conditional entropy quantifies the average amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known. In our implementation, the discrete conditional entropy $H(Y|X)$ is computed as the difference between the discrete joint entropy $H(Y,X)$ and the marginal entropy $H(X)$\\[4 pt]\n    \\textbf{Discrete mutual information.}\nThe Mutual Information is the information-theoretic measure of the dependence of two random variables. It can be computed from the entropy of each of the variables and its joint entropy as $I(X;Y) = H(X) + H(Y) - H(X,Y)$\nIn addition to the above unnormalized version, we also included as features two normalized versions. The mutual information normalized by the joint entropy and the mutual information normalized by the minimum of the marginal entropies:\n\n", "itemtype": "equation", "pos": 5967, "prevtext": "\nIn our implementation of the discrete entropy estimator we added the simple \\cite{miller1955nbi} bias correction term to finally obtain\n\n", "index": 3, "text": "\\[\\hat{H}_m(X) = -\\sum_{x}\\frac{n_x}{N}\\operatorname{log}(\\frac{n_x}{N})+\\frac{M-1}{2N}\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\hat{H}_{m}(X)=-\\sum_{x}\\frac{n_{x}}{N}\\operatorname{log}(\\frac{n_{x}}{N})+%&#10;\\frac{M-1}{2N}\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>x</mi></munder><mrow><mfrac><msub><mi>n</mi><mi>x</mi></msub><mi>N</mi></mfrac><mo>\u2062</mo><mrow><mo>log</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><msub><mi>n</mi><mi>x</mi></msub><mi>N</mi></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>+</mo><mfrac><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>N</mi></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\n    \\textbf{Adjusted mutual information.}\nThe Adjusted Mutual Information score is an adjustment of the Mutual Information measure. It corrects the effect of agreement solely due to chance, \\cite{Vinh:2009:ITM:1553374.1553511}. This feature is computed with the scikit-learn python package, \\cite{scikit-learn}.\\[4 pt]\n    \\textbf{Gaussian and uniform divergence.}\nThese features are an estimation of the Kullback-Leibler divergence or \\textit{distance} of the distribution of the data with respect to a normalized Gaussian distribution and a uniform distribution respectively. After mean and variance normalization, the estimation of the Gaussian divergence is equivalent to the estimation of the differential entropy except for a constant factor.\n\n", "itemtype": "equation", "pos": 7440, "prevtext": "\nwhere M is the number of different values of the random variable X in the data set. We also considered the normalized version\t$\\hat{H}_n(X) = \\hat{H}_m(X)/\\operatorname{log}(N)$ where $log(N)$ is the maximum entropy a discrete random variable with N different values. The definition and estimation of the entropy can be extended to a pair of variables replacing the counts $n_x$ by the counts $n_{x,y}$ of the number of times the pair $(x,y)$ appears in the sample set.\\[4 pt]\n    \\textbf{Discrete conditional entropy.}\nThe conditional entropy quantifies the average amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known. In our implementation, the discrete conditional entropy $H(Y|X)$ is computed as the difference between the discrete joint entropy $H(Y,X)$ and the marginal entropy $H(X)$\\[4 pt]\n    \\textbf{Discrete mutual information.}\nThe Mutual Information is the information-theoretic measure of the dependence of two random variables. It can be computed from the entropy of each of the variables and its joint entropy as $I(X;Y) = H(X) + H(Y) - H(X,Y)$\nIn addition to the above unnormalized version, we also included as features two normalized versions. The mutual information normalized by the joint entropy and the mutual information normalized by the minimum of the marginal entropies:\n\n", "index": 5, "text": "\\[I_j(X;Y) = \\frac{I(X;Y)}{H(X,Y)}\\hspace{10 mm}I_h(X;Y) = \\frac{I(X;Y)}{min(H(X),H(Y)}\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"I_{j}(X;Y)=\\frac{I(X;Y)}{H(X,Y)}\\hskip 28.452756ptI_{h}(X;Y)=\\frac{I(X;Y)}{min%&#10;(H(X),H(Y)}\" display=\"block\"><mrow><mrow><mrow><msub><mi>I</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2002\u2003</mo><mrow><mrow><msub><mi>I</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\nAn estimator of the differential entropy can also be used to compute the divergence respect an uniform distribution if the samples are first normalized in range:\n\n", "itemtype": "equation", "pos": 8281, "prevtext": "\n    \\textbf{Adjusted mutual information.}\nThe Adjusted Mutual Information score is an adjustment of the Mutual Information measure. It corrects the effect of agreement solely due to chance, \\cite{Vinh:2009:ITM:1553374.1553511}. This feature is computed with the scikit-learn python package, \\cite{scikit-learn}.\\[4 pt]\n    \\textbf{Gaussian and uniform divergence.}\nThese features are an estimation of the Kullback-Leibler divergence or \\textit{distance} of the distribution of the data with respect to a normalized Gaussian distribution and a uniform distribution respectively. After mean and variance normalization, the estimation of the Gaussian divergence is equivalent to the estimation of the differential entropy except for a constant factor.\n\n", "index": 7, "text": "\\[D_g(X)=D(X||G)=H(X)-H(G)=H(X) - \\frac{1}{2}\\operatorname{log}(2\\pi e)\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"D_{g}(X)=D(X||G)=H(X)-H(G)=H(X)-\\frac{1}{2}\\operatorname{log}(2\\pi e)\" display=\"block\"><mrow><msub><mi>D</mi><mi>g</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mi>\u03c0</mi><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\n\n\\subsection{Conditional distribution variability measures.}\nIn this section we define distribution variability measures that are used as tests of the spread of the conditional distribution $p(Y|X=x)$ for different values of $x$. If this variable is numerical we apply first the quantization process described in \\ref{sec:quantization}.\\[4 pt]\n    \\textbf{Standard deviation of the conditional distributions.}\nThis is a direct measure of the spread of the conditional distributions after normalization. If $Y$ is a numerical variable, the conditional distribution $p(Y|X=x)$ is normalized for each value of $x$ to have zero mean and then quantized as in section \\ref{sec:quantization}. If $Y$ is a categorical variable, the variability of the conditional distribution $p(Y|X=x)$ for different values of $x$ is calculated after sorting these probabilities for each $x$. The standard deviation of the preprocessed conditional distributions is then computed as:\n\n", "itemtype": "equation", "pos": 8518, "prevtext": "\nAn estimator of the differential entropy can also be used to compute the divergence respect an uniform distribution if the samples are first normalized in range:\n\n", "index": 9, "text": "\\[X_u = \\frac{X-min(X)}{max(X)-min(X)}\\hspace{10 mm}D_u(X) = D(X_u||U)=H(X_u) - H(U)=H(X_u)\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"X_{u}=\\frac{X-min(X)}{max(X)-min(X)}\\hskip 28.452756ptD_{u}(X)=D(X_{u}||U)=H(X%&#10;_{u})-H(U)=H(X_{u})\" display=\"block\"><mrow><msub><mi>X</mi><mi>u</mi></msub><mo>=</mo><mfrac><mrow><mi>X</mi><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2002\u2003</mo><msub><mi>D</mi><mi>u</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>u</mi></msub><mo stretchy=\"false\">|</mo><mo stretchy=\"false\">|</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\nwhere $p_n(y|x)$ refers to the normalized conditional probability and $var_x$ to the sample variance over $x$.\\[4 pt]\n    \\textbf{Standard deviation of the entropy,  skewness and kurtosis}\nThese additional features use the standard deviation to quantify the spread of the entropy, variance and skewness of the conditional distributions for different values $x$ of the hypothetical cause\n\n", "itemtype": "equation", "pos": 9573, "prevtext": "\n\n\\subsection{Conditional distribution variability measures.}\nIn this section we define distribution variability measures that are used as tests of the spread of the conditional distribution $p(Y|X=x)$ for different values of $x$. If this variable is numerical we apply first the quantization process described in \\ref{sec:quantization}.\\[4 pt]\n    \\textbf{Standard deviation of the conditional distributions.}\nThis is a direct measure of the spread of the conditional distributions after normalization. If $Y$ is a numerical variable, the conditional distribution $p(Y|X=x)$ is normalized for each value of $x$ to have zero mean and then quantized as in section \\ref{sec:quantization}. If $Y$ is a categorical variable, the variability of the conditional distribution $p(Y|X=x)$ for different values of $x$ is calculated after sorting these probabilities for each $x$. The standard deviation of the preprocessed conditional distributions is then computed as:\n\n", "index": 11, "text": "\\[CDS(X,Y) = \\sqrt{\\frac{1}{M}\\sum_{y=0}^{M-1}var_x(p_n(y|x))}\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"CDS(X,Y)=\\sqrt{\\frac{1}{M}\\sum_{y=0}^{M-1}var_{x}(p_{n}(y|x))}\" display=\"block\"><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>v</mi><mi>a</mi><msub><mi>r</mi><mi>x</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 10027, "prevtext": "\nwhere $p_n(y|x)$ refers to the normalized conditional probability and $var_x$ to the sample variance over $x$.\\[4 pt]\n    \\textbf{Standard deviation of the entropy,  skewness and kurtosis}\nThese additional features use the standard deviation to quantify the spread of the entropy, variance and skewness of the conditional distributions for different values $x$ of the hypothetical cause\n\n", "index": 13, "text": "\\[HS(X,Y) = std_x(H(Y|X=x))\\hspace{10 mm}SS(X,Y) = std_x(skew(Y|X=x))\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"HS(X,Y)=std_{x}(H(Y|X=x))\\hskip 28.452756ptSS(X,Y)=std_{x}(skew(Y|X=x))\" display=\"block\"><mrow><mi>H</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>s</mi><mi>t</mi><msub><mi>d</mi><mi>x</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2002\u2003</mo><mi>S</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>s</mi><mi>t</mi><msub><mi>d</mi><mi>x</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mi>k</mi><mi>e</mi><mi>w</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06680.tex", "nexttext": "\n    \\textbf{Bayesian error probability}This feature is an estimation of the average probability of error using the (discretized) conditional distributions . For each value of $x$ the probability of error is computed as one minus the probability of guessing $y$ given $x$ if we choose for the prediction $\\hat{y}$ the value that maximizes $p(Y|X=x)$. $EP(X,Y) = E[p_e(x)]$ where $p_e(x) = 1-max_y(p(Y|X=x))$\n\n\\subsection{Other features}\n    \\textbf{Number of samples and number of unique samples}\\[4 pt]\n    \\textbf{Hilbert Schmidt Independence Criterion (HSIC)} This standard independence measure is computed using a python version of the MATLAB script provided by the organizers.\\[4 pt]\n    \\textbf{Slope-based Information Geometric Causal Inference (IGCI)} The IGCI approach for causality detection, \\cite{Janzing20121} proposes measures based on the relative entropy and a \\textit{slope-based} measure that we also added to our set of features.\\[4 pt]\n    \\textbf{Moments and mixed moments} We included the skewness and kurtosis of each of the variables as features, as well as the mixed moments $m_{1,2} = E[xy^2]$ and $m_{1,3}=E[xy^3]$\\[4 pt]\n    \\textbf{Pearson correlation} The \\textit{Pearson r} correlation coefficient computed by the \\textit{scipy} python package, \\cite{scipy}\\[4 pt]\n    \\textbf{Polynomial fit} We propose two features based on a polynomial regression of order 2. The first feature is based on the absolute value of the second order coefficient. We have observed that the causal direction usually requires a smaller coefficient. The second feature measures the regression mean squared error or residual.\n\n\\section{Classification model selection}\n\nWe tested different learning methods for classification and regression. Gradient Boosting, \\cite{hastie01statisticallearning}, significantly performed better that the rest of algorithms in our 10-Fold cross-validation experiments on the training set after a manual hyperparameter tuning. We used the scikit-learn implementation (GradientBoostingClassifier) with 500 boosting stages and individual regression estimators with a large depth (9).\n\nThe classification task of the ChaLearn cause-effect pair challenge is in fact a three-class problem. For each pair of variables $A$ and $B$, we have a ternary truth value indicating whether $A$ is a cause of $B$ (+1), $B$ is a cause of $A$ (-1), or neither (0). The participants have to provide a single predicted value between $-\\infty$ and $+\\infty$, large positive values indicating that $A$ is a cause of $B$ with certainty, large negative values indicating that $B$ is a cause of $A$ with certainty, and middle range scores (near zero) indicate that neither $A$ causes $B$ nor $B$ causes $A$.\nThe official evaluation metric was the average of two Area Under the ROC curve (AUC) scores. The first AUC is computed associating the truth values 0 and -1 to the same class (the class 1 versus the rest), while the second AUC is computed grouping toghether the 1 and 0 classes (the class -1 versus the rest).\n\nNote that the symmetry of the task allow us to \\textit{duplicate} the training sample pairs. Exchanging $A$ with $B$ in an example of class $c$ provides a \\textit{new} example of the class $-c$.\n\nTo deal with this ternary classification problem we tested 3 different schemes:\n\\begin{enumerate}[leftmargin=*]\n\t\\item A single ternary classification or regression model. The predicted value is computed in this case as $p_1=p(1)-p(-1)$ where $p(1)$ and $p(-1)$ are the estimated probabilities assigned by the classifier to class 1 and class -1 respectively. Alternatively, we can use the output of any regression model. In the case of the selected Gradient Boosting model the classifier version with the \\textit{deviance} loss function gave better results than the regressor loss functions in our experiments.\n\t\\item A binary model for estimating the \\textit{direction} (class 1 versus class -1) and a binary model for \\textit{independence} classification (class 0 versus the rest). The first direction model is trained only with training sample pairs classified as 1 or -1, while the second independence model is trained with all the data after grouping class 1 and -1 in a single class. The predicted value is computed in this case as the product of the probabilities given by each of the models $p_2=p_d(1)p_i(0)$ where $p_d(1)$ is the probability of class 1 given by the direction model and $p_i(0)$ is the independence probability provided by the second model.\n\t\\item A symmetric model based on two binary models. In this scheme we also have two binary models: a model for class 1 versus the rest and another model for class \\mbox{-1} versus the rest. In this sense, this configuration follows the same scheme of the evaluation metric. Both binary models are trained with all the training data after the corresponding relabeling of classes. The predicted value is then computed as the difference of the probability given by the first model to class 1 and the probability given by the second model to class -1, $p_3=\\frac{1}{2}p_{3,1}(1)-\\frac{1}{2}p_{3,2}(-1)$.\n \\end{enumerate}\nUsing the same set of selected features, the three schemes provide similar results as shown in Table \\ref{tab:schemes}. The proposed final model uses a equally weighted linear combination of the output of each of the three models to obtain an additional significant gain respect to the best performing scheme.\n\\begin{table}\n\t\\begin{center}\n\t\t\\begin{tabular}{|l|l|}\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{Scheme} & Score \\\\\n\t\t\\hline\n\t\t\t1. Single ternary model & 0.81223\\\\\n\t\t\t2. Direction / Independence models & 0.81487 \\\\\n\t\t\t3. Symmetric models &  0.81476 \\\\\n\t\t\\hline\n\t\t\tSystem combination  & 0.81960 \\\\\n\t\t\\hline\n\t\t\\end{tabular}\n\t\t\\caption{Performance of the proposed schemes for the ternary model}\n\t\t\\label{tab:schemes}\n\t\\end{center}\n\\end{table}\n\n\\section{Results} \nThe main training database includes hundreds of pairs of real variables with known causal relationships from diverse domains. The organizers of the challenge also intermixed those pairs with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome). In addition, they also provided training datasets artificially generated,\n\\footnote{http://www.causality.inf.ethz.ch/cause-effect.php?page=data}.\n\nThe results presented in this section correspond to the score of the test data given by the web submission system of the cause-effect pair challenge hosted by Kaggle. Previous cross-validation experiments on the training set provided similar results. The table \\ref{tab:results} summarizes the results for different subsets of the proposed complete set of features. The baseline system includes 21 features:\nnumber of samples(1),\nnumber of unique samples(2),\ndiscrete entropy(2),\nnormalized discrete entropy(2),\ndiscrete conditional entropy(2),\ndiscrete mutual information and the two normalized versions(3),\nadjusted mutual information(1),\nGaussian divergence(2),\nuniform divergence(2),\nIGCI(2),\nHSIC(1), and \nPearson R(1)\n \n\\begin{table}[h]\n\t\\begin{center}\n\t\t\\begin{tabular}{|l|l|}\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{Features} & Score \\\\\n\t\t\\hline\nBaseline(21)                                     & 0.742 \\\\\n\t\t\\hline\nBaseline(21) + Moment31(2)           & 0.750 \\\\\nBaseline(21) + Moment21(2)           & 0.757 \\\\\nBaseline(21) + Error probability(2)  & 0.749 \\\\\nBaseline(21) + Polyfit(2)                  & 0.757 \\\\\nBaseline(21) + Polyfit error(2)         & 0.757 \\\\\nBaseline(21) + Skewness(2)            & 0.754 \\\\\nBaseline(21) + Kurtosis(2)               & 0.744 \\\\\n\t\t\\hline\nBaseline(21) + the above statistics set (14)   & 0.790 \\\\\n\t\t\\hline\nBaseline(21) + Standard deviation of conditional distributions(2)                            & 0.779 \\\\\nBaseline(21) + Standard deviation of the skewness of conditional distributions(2) & 0.765 \\\\\nBaseline(21) + Standard deviation of the kurtosis of conditional distributions(2)    & 0.759 \\\\\nBaseline(21) + Standard deviation of the entropy of conditional distributions(2)    & 0.759 \\\\\n\t\t\\hline\nBaseline(21) + Measures of variability of the conditional distribution(8)                  & 0.789 \\\\\n\t\t\\hline\nFull set(43 features)                                                                                                  & 0.820 \\\\\n\t\t\\hline\n\t\t\\end{tabular}\n\t\t\\caption{Results for different subset of the proposed features}\n\t\t\\label{tab:results}\n\t\\end{center}\n\\end{table}\n\nA more detailed analysis of the results of the proposed system and of other top ranking systems can be found in \\cite{Guyon2014}.\n\n\\section{Conclusions} \nWe have proposed several measures of the variability of conditional distributions as features to infer causal relationships in a given pair of variables. In particular, the proposed standard deviation of the normalized conditional distributions stands out as one of the best features in our results. The combination of the developed measures with standard information-theoretic and statistical measures provides a robust set of features to address the causality problem in the framework of the ChaLearn cause-effect pair challenge. In a test set with categorical, numerical and mixed pairs from diverse domains, the proposed method achieves an AUC score of 0.82.\n\n\n\n\n\\newpage\n\\acks{This work has been supported in part by Spanish Ministerio de Econom\\' ia y Competitividad, contract TEC2012-38939-C03-02 as well as from the European Regional Development Fund (ERDF/FEDER)}\n\n\n\n\\bibliography{jarfo}\n\n\\newpage\n\n\\section*{Appendix A. ChaLearn cause-effect pair challenge. FACT SHEET.}\n\\label{sec:factsheet}\n\n\n\n\\noindent {\\bf Title:} Conditional distribution variability measures for causality detection\\\\\n\\noindent {\\bf Participant name, address, email and website:} Jos\\'e A. R. Fonollosa, Universitat Polit\u00c3\u00a8cnica de Catalunya, c/Jordi Girona 1-3, Edifici D5, Barcelona 08034, SPAIN. jose.fonollosa@upc.edu, \\url{www.talp.upc.edu}\\\\\n\\noindent {\\bf Task solved:}  cause-effect pairs\\\\\n\\noindent {\\bf Reference:} Jos\\'e A. R. Fonollosa: Conditional distribution variability measures for causality detection. NIPS 2013 Workshop on Causality\\\\\n\n\\noindent {\\bf Method:}\\\\\n\n\\begin{itemize}\n\\item Preprocessing. Normalization of numerical variables. Relabeling of categorical variables\n\\item Causal discovery. Standard features plus new measures base on variability measures of the conditional distributions $p(Y|X=x)$ for different values of $x$\n\\item Feature selection. Greedy selection\n\\item Classification. Gradient Boosting. Combination of three different multiclass schemes\n\\item Model selection/hyperparameter selection. Manual hyperparameter selection\\\\\n\\end{itemize}\n\n\\noindent {\\bf Results:}\n\n\\begin{table}[h]\n\\begin{center}\n\\label{tab:table1}\n\\begin{tabular}{|c|c|c|}\n\\hline\nDataset/Task & Official score & Post-deadline score\\\\\n\\hline\nFinal test & 0.81052 & 0.81960 \\\\\n\\hline\n\n\\end{tabular}\n\\caption{Result table.}\n\\end{center}\n\\end{table}\n\n\\begin{itemize}\n\\item quantitative advantages: the developed model is simple and very fast compared to other top ranking models\n\\item qualitative advantages: it relaxes the noise independence assumption introducing less strict similarity measures for the conditional probability $p(Y|X=x)$.\n\\end{itemize}\nThe complete python code for training the model and reproducing the presented results is available at \\url{https://github.com/jarfo/cause-effect}. The training time is about 45 minutes on a 4-core server, and computing the predictions for the test test takes about 12 minutes.\\\\\n\n\n", "itemtype": "equation", "pos": 10100, "prevtext": "\n\n", "index": 15, "text": "\\[KS(X,Y) = std_x(kurtosis(Y|X=x))\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"KS(X,Y)=std_{x}(kurtosis(Y|X=x))\" display=\"block\"><mrow><mi>K</mi><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>s</mi><mi>t</mi><msub><mi>d</mi><mi>x</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mi>u</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}]