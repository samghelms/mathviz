[{"file": "1601.00199.tex", "nexttext": "\nSetting $\\rho=0$, $\\gamma=1$ reduces the previous cost function to the original project-out loss proposed in \\cite{Matthews2004}; completely disregarding the contribution of the prior distribution over the appearance parameters i.e the Mahalanobis distance \\emph{within} the appearance subspace. On the contrary, setting $\\rho=1$, $\\gamma=0$ reduces the cost function to the first term; completely disregarding the contribution of the project-out term i.e. the distance \\emph{to} the appearance subspace. Finally setting $\\rho=\\gamma=0.5$ leads to the standard Bayesian project-out cost function proposed in Section \\ref{sec:po_pi}.\n \nIn order to assess the impact that each term has on the fitting accuracy obtained by the previous \\emph{Project-Out} algorithm we repeat the experimental set up of the first experiment and test all \\emph{Project-Out Gauss-Newton} algorithms for different values of the parameters $\\rho = 1 -\\gamma$. Notice that, in this case, we only report the performance of \\emph{Gauss-Newton} algorithms because they were shown to vastly outperform \\emph{Newton} algorithms and to be virtually equivalent to \\emph{Wiberg} algorithms in the first experiment.\n\nResults for this experiment are reported by Figure \\ref{fig:rho}. We can see that, regardless of the type of composition, a weighted combination of the two previous terms always leads to a smaller mean normalized point-to-point error compared to either term on its own. Note that the final fitting accuracy obtained with the standard Bayesian project-out cost function is substantially better than the one obtained by the original project-out loss (this is specially noticeable for the \\emph{Inverse} and \\emph{Bidirectional} algorithms); fully justifying the inclusion of the first term, i.e the Mahalanobis distance \\emph{within} the appearance subspace, into the cost function. Finally, in this particular experiment, the final fitting accuracy of all algorithms is maximized by setting $\\rho=0.1$, $\\gamma=0.9$, further highlighting the importance of the first term in the Bayesian formulation.\n\n\n\n\n\n\n\n\n\n\\subsection{Optimal asymmetric composition}\n\\label{exp:3}\n\nThis experiment quantifies the effect that varying the value of the parameters $\\alpha \\in [0, 1]$ and $\\beta = 1 -\\alpha$ in Equation \\ref{eq:ssd_ac} has in the fitting accuracy obtained by the \\emph{Asymmetric} algorithms. Note that for $\\alpha=1$, $\\beta=0$ and $\\alpha=0$, $\\beta=1$ these algorithms reduce to their \\emph{Forward} and \\emph{Inverse} versions respectively. Recall that, in previous experiments, we used the \\emph{Symmetric} case $\\alpha=\\beta=0.5$ to generate the results reported for \\emph{Asymmetric} algorithms. Again, we only report performance for \\emph{Gauss-Newton} algorithms.\n\nWe again repeat the experimental set up described in the first experiments and report the fitting accuracy obtained by the \\emph{Project Out} and \\emph{SSD Asymmetric Gauss-Newton} algorithms for different values of the parameters $\\alpha = 1 - \\beta$. Results are shown in Figure \\ref{fig:alpha}. For the \\emph{BPO Asymmetric} algorithm, the best results are obtain by setting $\\alpha=0.4$, $\\beta=0.6$, Figures \\ref{fig:asy_gn_vs_alpha_5} (top) and \\ref{fig:ced_po_asy_gn_5}. These results slightly outperform those obtain by the default \\emph{Symmetric} algorithm and this particular configuration of the \\emph{BPO Asymmetric} algorithm is the best performing one on the LFPW test dataset. For the \\emph{SSD Asymmetric Gauss-Newton} algorithm the best results are obtained by setting $\\alpha=0.2$, $\\beta=0.8$, Figures \\ref{fig:asy_gn_vs_alpha_5} (bottom) and \\ref{fig:ced_ssd_asy_gn_5}. In this case, the boost in performance with respect to the default \\emph{Symmetric} algorithm is significant and, with this particular configuration, the \\emph{SSD Asymmetric Gauss-Newton} algorithm is the best performing \\emph{SSD} algorithm on the LFPW test dataset, outperforming \\emph{Inverse} and \\emph{Bidirectional} algorithms.\n\n\n\n\n\n\n\n\n\n\\subsection{Sampling and Number of Iterations}\n\\label{exp:4}\n\nIn this experiment, we explore two different strategies to reduce the running time of the previous CGD algorithms. \n\nThe first one consists of optimizing the SSD and Project-Out cost functions using only a subset of all pixels in the reference frame. In AAMs the total number of pixels on the reference frame, $F$, is typically several orders of magnitude bigger than the number of shape, $n$, and appearance, $m$, components i.e. $F>>m>>n$. Therefore, a significant reduction in the complexity (and running time) of CGD algorithms can be obtained by decreasing the number of pixels that are used to optimize the previous cost functions. To this end, we compare the accuracy obtained by using $100\\%$, $50\\%$, $25\\%$ and $12\\%$ of the total number of pixels on the reference frame. Note that, pixels are (approximately) evenly sampled across the reference frame in all cases, Figure \\ref{fig:sampling_masks}.\n\nThe second strategy consists of simply reducing the number of iterations that each algorithm is run. Based on the figures used to assess the convergence properties of CGD algorithms in previous experiments, we compare the accuracy obtained by running the algorithms for $40$ $(24 + 16)$ and $20$ $(12 + 8)$ iterations.\n\nNote that, in order to further highlight the advantages and disadvantages of using the previous strategies we report the fitting accuracy obtained by initializing the algorithms using different amounts of uniform noise.\n\nOnce more we repeat the experimental set up of the first experiment and report the fitting accuracy obtained by the Project Out and SSD Asymmetric Gauss-Newton algorithms. Results for this experiment are shown in Figure \\ref{fig:sampling}. It can be seen that reducing the number of pixels up to $25\\%$ while maintaining the original number of iterations to $40$ $(24 + 16)$ has little impact on the fitting accuracy achieved by both algorithms while reducing them to $12\\%$ has a clear negative impact, Figures \\ref{fig:sampling_vs_noise_ssd_asy_gn} and \\ref{fig:sampling_vs_noise_po_asy_gn}. Also, performance seems to be consistent along the amount of noise. In terms of run time, Table \\ref{tab:runtime_40}, reducing the number of pixels to $50\\%$, $25\\%$ and $12\\%$ offers speed ups of $\\sim2.0$x, $\\sim2.9$x and $\\sim3.7$x for the \\emph{BPO} algorithm and of $\\sim1.8$x, $\\sim2.6$x and $\\sim2.8$x for the \\emph{SSD} algorithm respectively.  \n\nOn the other hand, reducing the number of iterations from $40$ $(24 + 16)$ to $20$ $(12 + 8)$ has no negative impact in performance for levels of noise smaller than $2\\%$ but has a noticeable negative impact for levels of noise bigger than $5\\%$. Notice that remarkable speed ups, Table \\ref{tab:runtime_20}, can be obtain for both algorithms by combining the previous two strategies at the expenses of small but noticeable decreases in fitting accuracy.\n\n\n\\subsection{Comparison on Helen and AFW}\n\\label{exp:5}\n\nIn order to facilitate comparisons with recent prior work on AAMs \\cite{Tzimiropoulos2013, Antonakos2014, Kossaifi2014} and with other state-of-the-art approaches in face alignment \\cite{Xiong2013, Asthana2013}, in this experiment, we report the fitting accuracy of the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms on the widely used test set of the Helen database and on the entire AFW database. Furthermore we compare the performance of the previous two algorithms with the one obtained by the recently proposed Gauss-Newton Deformable Part Models (GN-DPMs) proposed by Tzimiropoulos and Pantic in \\cite{Tzimiropoulos2014}; which was shown to achieve state-of-the-art results in the problem of face alignment in-the-wild.\n\nFor both our algorithms, we report two different types of results:\n\\begin{inparaenum}[\\itshape i\\upshape)]\n\\item sampling rate of $25\\%$ and $20$ $(12 + 8)$ iterations; and\n\\item sampling rate of $50\\%$ and $40$ $(24 + 16)$ iterations,\n\\end{inparaenum}. For GN-DPMs we use the authors public implementation to generate the results. In this case, we report, again, two different types of results by letting the algorithm run for $20$ and $40$ iterations.\n\nResult for this experiment are shown in Figure \\ref{fig:helen_afw}. Looking at Figure \\ref{fig:ced_helen} we can see that both, \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms, obtain similar fitting accuracy on the Helen test dataset. Note that, in all cases, their accuracy is comparable to the one achieved by GN-DPMs for normalized point-to-point errors $<0.2$ and significantly better for $<0.3$, $<0.4$. As expected, the best results for both our algorithms are obtained using $50\\%$ of the total amount of pixels and $40$ $(24 + 16)$ iterations. However, the results obtained by using only $25\\%$ of the total amount of pixels and $20$ $(12 + 8)$ iterations are comparable to the previous ones; specially for the \\emph{Project-Out Asymmetric Gauss-Newton}. In general, these results are consistent with the ones obtained on the LFPW test dataset, Experiments \\ref{exp:1} and \\ref{exp:3}. \n\nOn the other hand, the performance of both algorithms drops significantly on the AFW database, Figure \\ref{fig:ced_afw} . In this case, GN-DPMs achieves slightly better results than the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms for normalized point-to-point errors $<0.2$ and slightly worst for $<0.3$, $<0.4$. Again, both our algorithms obtain better results by using $50\\%$ sampling rate and $40$ $(24 + 16)$ iterations and the difference in accuracy with respect to the versions using $25\\%$ sampling rate and $20$ $(12 + 8)$ iterations slightly widens when compared to the results obtained on the Helen test dataset. This drop in performance is consistent with other recent works on AAMs \\cite{Tzimiropoulos2014, Alabort2014, Antonakos2014, Alabort2015} and it is attributed to large difference in terms of shape and appearance statistics between the images of the AFW dataset and the ones of the training sets of the LFPW and Helen datasets where the AAM model was trained on. \n\nExemplar results for this experiment are shown in Figures \\ref{fig:helen} and \\ref{fig:afw}. \n\n\n\\subsection{Comparison on MIT StreetScene}\n\\label{exp:6}\n\nIn this final experiment, we present results for a different type of object: cars. To this end, we use the first view of the MIT StreetScene\\footnoteref{car_db_url} dataset containing a wide variety of frontal car images obtained in the wild. We use 10-fold cross-validation on the $\\sim 500$ images of the previous dataset to train and test our algorithms. We report results for the two versions of the \\emph{SSD Asymmetric Gauss-Newton} and the \\emph{Project-Out Asymmetric Gauss-Newton} algorithms used in Experiment \\ref{exp:5}.\n\nResult for this experiment are shown in Figure \\ref{fig:ced_cars}. We can observe that all algorithms obtain similar performance and that they vastly improve upon the original initialization. \n\nExemplar results for this experiment are shown in Figure \\ref{fig:cars}.\n\n\n\\subsection{Analysis}\n\\label{exp:analysis}\n\nGiven the results reported by the previous six experiments we conclude that:\n\n\\begin{enumerate}\n\\item Overall, \\emph{Gauss-Newton} and \\emph{Wiberg} algorithms vastly outperform \\emph{Newton} algorithms for fitting AAMs. Experiment \\ref{exp:1} clearly shows that the former algorithms provide significantly higher levels of fitting accuracy at considerably lower computational complexities and run times. These findings are consistent with existent literature in the related field of parametric image alignment \\cite{Matthews2004} and also, to certain extend, with prior work on \\emph{Newton} algorithms for AAM fitting \\cite{Kossaifi2014}. We attribute the bad performance of \\emph{Newton} algorithms to the difficulty of accurately computing a (noiseless) estimate of the full Hessian matrix using finite differences.\n\n\\item \\emph{Gauss-Newton} and \\emph{Wiberg} algorithms are virtually equivalent in performance. The results in Experiment \\ref{exp:1} show that the difference in accuracy between both types of algorithms is minimal and the small differences in their respective solutions are, in practice, insignificant.\n\n\\item Our \\emph{Bayesian} project-out formulation leads to significant improvements in fitting accuracy without adding extra computational cost. Experiment \\ref{exp:2} shows that a weighted combination of the two terms forming \\emph{Bayesian} project-out loss always outperforms the \\emph{classic} project out formulation.\n\n\\item The \\emph{Asymmetric} composition proposed in this work leads to CGD algorithms that are more accurate and that converge faster. In particular, the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms are shown to achieve significantly better performance than their \\emph{Forward} and \\emph{Inverse} counterparts in Experiments \\ref{exp:1} and \\ref{exp:3}.\n\n\\item Finally, a significant reduction in the computational complexity and runtime of CDG algorithms can be obtained by limiting the number of pixels considered during optimization of the loss function and by adjusting the number of iterations that the algorithms are run for, Experiment \\ref{exp:4}.\n\\end{enumerate}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/ced_ssd_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_error_vs_iters_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_cost_vs_iters1_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_cost_vs_iters2_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t\t\\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\\n\t\t    SSD\\_For\\_GN\\_Sch & 0.456 & 0.707 & 0.777 & 0.033 & 0.030 & 0.021 \n\t\t    \\\\\n\t\t    SSD\\_For\\_GN\\_Alt & 0.445 & 0.702 & 0.766 & 0.033 & 0.030 & 0.021\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_GN\\_Sch & 0.686 & 0.906 & 0.939 & 0.022 & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_GN\\_Alt & 0.673 & 0.897 & 0.933 & 0.022 & 0.020 & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_GN\\_Sch & 0.640 & 0.891 & 0.929 & 0.023 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_GN\\_Alt & 0.635 & 0.882 & 0.924 & 0.023 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_GN\\_Sch & 0.674 & 0.917 & 0.946 & 0.022 & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_GN\\_Alt & \\textbf{0.680} & \\textbf{0.924} & \\textbf{0.951} & \\textbf{0.021} & \\textbf{0.019} & \\textbf{0.017} \n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Gauss-Newton algorithms on the LFPW test dataset initialized with $5\\%$ uniform noise.}\n\t\\label{fig:ssd_gn_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/ced_ssd_n_5.jpeg}\n\t    \\caption{Cumulative error distribution on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_error_vs_iters_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_cost_vs_iters1_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_cost_vs_iters2_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t\t\\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\\n\t\t    SSD\\_For\\_N\\_Sch & 0.249 & 0.479 & 0.603 & 0.044 & 0.033 & 0.031\n\t\t    \\\\\n\t\t    SSD\\_For\\_N\\_Alt & 0.244 & 0.476 & 0.600 & 0.044 & 0.033 & 0.032\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_N\\_Sch & 0.626 & 0.876 & 0.909 & 0.024 & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_N\\_Alt & 0.613 & 0.876 & 0.909 & 0.024 & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_N\\_Sch & 0.562 & 0.812 & 0.863 & 0.030 & 0.076 & 0.019\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_N\\_Alt & 0.557 & 0.808 & 0.862 & 0.027 & 0.025 & 0.019\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_N\\_Sch & \\textbf{0.641} & 0.897 & 0.932 & \\textbf{0.023} & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_N\\_Alt & 0.600 & \\textbf{0.903} & \\textbf{0.939} & \\textbf{0.023} & \\textbf{0.021} & \\textbf{0.018}\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error Mean, Std and Median for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_n_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Newton algorithms on the LFPW test dataset initialized with $5\\%$ uniform noise.}\n\t\\label{fig:ssd_n_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/ced_ssd_w_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_error_vs_iters_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_cost_vs_iters1_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_cost_vs_iters2_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    SSD\\_For\\_W & 0.457 & 0.707 & 0.777 & 0.33 & 0.030 & 0.021\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_W & \\textbf{0.689} & 0.903 & 0.939 & \\textbf{0.22} & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_W & 0.635 & 0.887 & 0.926 & 0.23 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_W & 0.686 & \\textbf{0.911} & \\textbf{0.942} & \\textbf{0.22} & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_w_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Wiberg algorithms on the LFPW test dataset.}\n\t\\label{fig:ssd_w_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/ced_po_gn_5.jpeg}\n\t    \\caption{CED graph on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_error_vs_iters_po_gn_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_cost_vs_iters1_po_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_cost_vs_iters2_po_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_For\\_GN\\_Sch & 0.470 & 0.729 & 0.799 & 0.031 & 0.029 & 0.021\n\t\t    \\\\\n\t\t    PO\\_For\\_GN\\_Alt & 0.458 & 0.719 & 0.780 & 0.035 & 0.044 & 0.021\n\t\t    \\\\\n\t\t    PO\\_Inv\\_GN\\_Sch & \\textbf{0.637} & \\textbf{0.891} & \\textbf{0.938} & \\textbf{0.023} & \\textbf{0.021} & \\textbf{0.018}\n\t\t    \\\\\n\t\t    PO\\_Bid\\_GN\\_Sch & 0.528 & 0.802 & 0.862 & 0.030 & 0.039 & 0.020\n\t\t    \\\\\n\t\t    PO\\_Bid\\_GN\\_Alt & 0.528 & 0.805 & 0.865 & 0.030 & 0.040 & 0.019\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Gauss-Newton algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_gn_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/ced_po_n_5.jpeg}\n\t    \\caption{CED graph on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_error_vs_iters_po_n_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_cost_vs_iters1_po_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_cost_vs_iters2_po_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_For\\_N\\_Sch & 0.280 & 0.503 & 0.626 & 0.043 & 0.033 & 0.030\n\t\t    \\\\\n\t\t    PO\\_Inv\\_N\\_Alt & 0.265 & 0.516 & 0.586 & 11.929 & 179.525 & 0.029\n\t\t    \\\\\n\t\t    PO\\_Asy\\_N\\_Sch & \\textbf{0.494} & \\textbf{0.744} & \\textbf{0.826} & \\textbf{0.030} & \\textbf{0.028} & \\textbf{0.020}\n\t\t    \\\\\n\t\t    PO\\_Bid\\_N\\_Sch & 0.314 & 0.536 & 0.649 & 0.287 & 1.347 & 0.027\n\t\t    \\\\\n\t\t    PO\\_Bid\\_N\\_Alt & 0.329 & 0.570 & 0.649 & 0.280 & 1.465 & 0.026\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_n_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Newton algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_n_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/ced_po_w_5.jpeg}\n\t    \\caption{Cumulative Error Distribution graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_error_vs_iters_po_w_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_cost_vs_iters1_po_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_cost_vs_iters2_po_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_Bid\\_W\\_Sch & 0.524 & 0.801 & 0.862 & 0.030 & 0.039 & 0.020\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_w_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Wiberg algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_w_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/convergence_vs_rho_po_gn_5.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out and SSD Asymmetric Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:convergence_vs_rho_po_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_for_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Forward Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_for_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_inv_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Inverse Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_inv_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Asymmetric Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_asy_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_bid_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Bidirectional Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_bid_gn}\n\t\\end{subfigure}\n\t\\caption{Results quantifying the effect of varying the value of the parameters $\\rho = 1 -\\gamma$ in Project-Out Gauss-Newton algorithms.}\n\t\\label{fig:rho}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_ssd_asy_gn.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the SSD Asymmetric Gauss-Newton algorithm using different sampling rates, $40$ $(24 + 16)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_ssd_asy_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_po_asy_gn.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out Asymmetric Gauss-Newton algorithm using different sampling rates, $40$ $(24 + 16)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_po_asy_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    & $100\\%$ & $<50\\%$ & $<25\\%$ & $<12\\%$ \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    SSD\\_Asy\\_GN\\_Sch & $\\sim1680$ ms & $\\sim930$ ms & $\\sim650$ ms & $\\sim590$ ms\n\t\t    \\\\ \n\t\t    PO\\_Asy\\_GN & $\\sim1400$ ms & $\\sim680$ ms & $\\sim480$ ms & $\\sim380$ ms\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing run time of each algorithm for different amounts of sampling and $40$ $(24 + 16)$ iterations.}\n\t    \\label{tab:runtime_40}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_po_asy_gn_20.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out Asymmetric Gauss-Newton algorithm using different sampling rates, $20$ $(12 + 8)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_ssd_asy_gn_20}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_ssd_asy_gn_20.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the SSD Asymmetric Gauss-Newton algorithm using different sampling rates, $20$ $(12 + 8)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_po_asy_gn_20}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    & $100\\%$ & $<50\\%$ & $<25\\%$ & $<12\\%$ \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    SSD\\_Asy\\_GN\\_Sch & $\\sim892$ ms & $\\sim519$ ms & $\\sim369$ ms & $\\sim331$ ms\n\t\t    \\\\\n\t\t    PO\\_Asy\\_GN & $\\sim707$ ms & $\\sim365$ ms & $\\sim235$ ms & $\\sim211$ ms\n\t\t    \\\\ \n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing run time of each algorithm for different amounts of sampling and $20$ $(12 + 8)$ iterations.}\n\t    \\label{tab:runtime_20}\n\t\\end{subfigure}\n\t\\caption{Results assessing the effectiveness of sampling for the best performing Project-Out and SSD algorithms on the LFPW database.}\n\t\\label{fig:sampling}\n\\end{figure*}\n\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/asy_gn_vs_alpha_5.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out and SSD Asymmetric Gauss-Newton algorithms for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:asy_gn_vs_alpha_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/ced_po_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Asymmetric Gauss-Newton algorithm for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_asy_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/ced_ssd_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for the the SSD Asymmetric Gauss-Newton algorithm for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_ssd_asy_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results quantifying the effect of varying the value of the parameters $\\alpha = 1 - \\beta$ in Asymmetric algorithms.}\n\t\\label{fig:alpha}\n\\end{figure*}\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/best/ced_helen_5.jpeg}\n\t    \\caption{CED on the Helen test dataset for the Project-Out and SSD Asymmetric Gauss-Newton algorithms initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_helen}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/best/ced_afw_5.jpeg}\n\t    \\caption{CED on the AFW database for the Project-Out and SSD Asymmetric Gauss-Newton algorithm initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_afw}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy of the SSD and Project-Out Asymmetric Gauss-Newton algorithms on the Helen and AFW databases.}\n\t\\label{fig:helen_afw}\n\\end{figure*}\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n    \t\\includegraphics[width=\\textwidth]{experiments/best/ced_cars_5.jpeg}\n    \\end{subfigure}\n    \\caption{CED on the first view of the MIT StreetScene test dataset for the Project-Out and SSD Asymmetric Gauss-Newton algorithms initialized with $5\\%$ noise.}\n    \\label{fig:ced_cars}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_5.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:helen_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_5.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:helen_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the Helen test dataset.}\n\t\\label{fig:helen}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:afw_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_6.jpeg}\n\t\t\\caption{Exemplar results from the AFW dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:afw_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the AFW dataset.}\n\t\\label{fig:afw}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth]{figures/car00.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car01.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car02.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car03.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car04.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car05.jpeg}\n\t\t\\caption{Exemplar results from the MIT StreetScene test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:cars_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth]{figures/car10.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car11.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car12.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car13.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car14.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car15.jpeg}\n\t\t\\caption{Exemplar results from the MIT StreetScene test dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:cars_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the MIT StreetScene test dataset.}\n\t\\label{fig:cars}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this paper we have thoroughly studied the problem of fitting AAMs using CGD algorithms. We have presented a unified and complete framework for these algorithms and classified them with respect to three of their main characteristics:\n\\begin{inparaenum}[\\itshape i\\upshape)]\n\t\\item \\emph{cost function};\n\t\\item type of \\emph{composition}; and\n\t\\item \\emph{optimization method}.\n\\end{inparaenum}\n\nFurthermore, we have extended the previous framework by:\n\\begin{itemize}\n\t\\item Proposing a novel \\emph{Bayesian cost function} for fitting AAMs that can be interpreted as a more general formulation of the well-known project-out loss. We have assumed a probabilistic model for appearance generation with both Gaussian noise and a Gaussian prior over a latent appearance space. Marginalizing out the latent appearance space, we have derived a novel cost function that only depends on shape parameters and that can be interpreted as a valid and more general probabilistic formulation of the well-known project-out cost function \\cite{Matthews2004}. In the experiments, we have showed that our Bayesian formulation considerably outperforms the original project-out cost function.\n\t\n\t\\item Proposing \\emph{asymmetric} and \\emph{bidirectional} compositions for CGD algorithms. We have shown the connection between Gauss-Newton Asymmetric algorithms and ESM algorithms and experimentally proved that these two novel types of composition lead to better convergent and more robust CGD algorithm for fitting AAMs.\n\t\n\t\\item Providing new valuable insights into existent CGD algorithms by reinterpreting them as direct applications of the \\emph{Schur complement} and the \\emph{Wiberg method}.\n\\end{itemize} \n\nFinally, in terms of future work, we plan to:\n\\begin{itemize}\n\t\\item Adapt existent Supervised Descent (SD) algorithms for face alignment \\cite{Xiong2013, Tzimiropoulos2015} to AAMs and investigate their relationship with the CGD algorithms studied in this paper. \n\t\n\t\\item Investigate if our Bayesian cost function and the proposed asymmetric and bidirectional compositions can also be successfully applied to similar generative parametric models, such as the Gauss-Newton Parts-Based Deformable Model (GN-DPM) proposed in \\cite{Tzimiropoulos2014}.   \n\\end{itemize} \n\n\\begin{acknowledgements}\nThe work of Joan Alabort-i-Medina\nis funded by a DTA studentship from Imperial College London\nand by the Qualcomm Innovation Fellowship. The work of S. Zafeiriou has been partly funded by the EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT), EP/L026813/1.\n\\end{acknowledgements}\n\n\n\\bibliographystyle{spbasic}      \n\\bibliography{main.bib}   \n\n\\appendix \n\n\\pagebreak\n\n\\section{Terms in SSD Newton Hessians}\n\\label{sec:app1}\n\nIn this section we define the individual terms of the Hessian matrices used by the \\emph{SSD Asymmetric} and \\emph{Bidirectional Newton} optimization algorithms derived in Section \\ref{sec:newton}.\n\n\\subsection{Asymmetric}\n\\label{sec:app11}\n\nThe individual terms forming the Hessian matrix of the \\emph{SSD Asymmetric Newton} algorithm defined by Equation \\ref{eq:asymmetric_newton_hessian} are defined as follows:\n\n", "itemtype": "equation", "pos": 26091, "prevtext": "\n\n\\title{A Unified Framework for Compositional Fitting of \\mbox{Active Appearance Models}}\n\n\n\n\n\n\\author{Joan Alabort-i-Medina\n        \\and\n        Stefanos Zafeiriou}\n\n\n\n\\institute{J. Alabort-i-Medina \\at\n           Department of Computing, Imperial College London, \\\\\n           180 Queen's Gate, London SW7 2AZ, UK \\\\\n           \\email{ja310@imperial.ac.uk}\n           \\and\n           Stefanos Zafeiriou \\at\n           \\email{s.zafeiriou@imperial.ac.uk}}\n\n\\date{Received: date / Accepted: date}\n\n\n\\maketitle\n\n\\begin{abstract}+\nActive Appearance Models (AAMs) are one of the most popular and well-established techniques for modeling deformable objects in computer vision. In this paper, we study the problem of fitting AAMs using Compositional Gradient Descent (CGD) algorithms. We present a unified and complete view of these algorithms and classify them with respect to three main characteristics:\n\\begin{inparaenum}[\\itshape i\\upshape)]\n\t\\item \\emph{cost function};\n\t\\item type of \\emph{composition}; and\n\t\\item \\emph{optimization method}.\n\\end{inparaenum}\nFurthermore, we extend the previous view by:\n\\begin{inparaenum}[\\itshape a\\upshape)]\n\t\\item proposing a novel \\emph{Bayesian cost function} that can be interpreted as a general probabilistic formulation of the well-known project-out loss;\n\t\\item introducing two new types of composition, \\emph{asymmetric} and \\emph{bidirectional}, that combine the gradients of both image and appearance model to derive better convergent and more robust CGD algorithms; and\n\t\\item providing new valuable insights into existent CGD algorithms by reinterpreting them as direct applications of the \\emph{Schur complement} and the \\emph{Wiberg method}.\n\\end{inparaenum}\nFinally, in order to encourage open research and facilitate future comparisons with our work, we make the implementation of the algorithms studied in this paper publicly available as part of the Menpo Project\\footnote{\\label{menpo_url}\\url{http://www.menpo.org}}.\n\n\\keywords{Active Appearance Models \\and Non-linear Optimization \\and Compositional Gradient Descent \\and Bayesian Inference \\and Asymmetric and Bidirectional Composition \\and Schur Complement \\and Wiberg Algorithm}\n\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nActive Appearance Models (AAMs) \\cite{Cootes2001, Matthews2004} are one of the most popular and well-established techniques for modeling and segmenting deformable objects in computer vision. AAMs are generative parametric models of shape and appearance that can be \\emph{fitted} to images to recover the set of model parameters that best describe a particular instance of the object being modeled.\n\nFitting AAMs is a non-linear optimization problem that requires the  minimization (maximization) of a global error (similarity) measure between the input image and the appearance model. Several approaches \\cite{Cootes2001, Hou2001, Matthews2004, Batur2005, Gross2005, Donner2006, Papandreou2008, Liu2009, Saragih2009, Amberg2009, Tresadern2010, Martins2010, Sauer2011, Tzimiropoulos2013, Kossaifi2014, Antonakos2014} have been proposed to define and solve the previous optimization problem. Broadly speaking, they can be divided into two different groups: \n\\begin{itemize}\n\\item \\emph{Regression} based \\cite{Cootes2001, Hou2001, Batur2005, Donner2006, Saragih2009, Tresadern2010, Sauer2011}\n\\item \\emph{Optimization} based \\cite{Matthews2004, Gross2005, Papandreou2008, Amberg2009, Martins2010, Tzimiropoulos2013, Kossaifi2014}\n\\end{itemize}\n\nRegression based techniques attempt to solve the problem by learning a direct function mapping between the error measure and the optimal values of the parameters. Most notable approaches include variations on the original \\cite{Cootes2001} fixed linear regression approach of \\cite{Hou2001, Donner2006}, the adaptive linear regression approach of \\cite{Batur2005}, and the works of \\cite{Saragih2009} and \\cite{Tresadern2010} which considerably improved upon previous techniques by using boosted regression. Also, Cootes and Taylor \\cite{Cootes2001b} and Tresadern et al. \\cite{Tresadern2010} showed that the use of non-linear gradient-based and Haar-like appearance representations, respectively, lead to better fitting accuracy in regression based AAMs. \n\nOptimization based methods for fitting AAMs were proposed by Matthews and Baker in \\cite{Matthews2004}. These techniques are known as Compositional Gradient Decent (CGD) algorithms and are based on direct analytical optimization of the error measure. Popular CGD algorithms include the very efficient project-out Inverse Compositional (PIC) algorithm \\cite{Matthews2004}, the accurate but costly Simultaneous Inverse Compositional (SIC) algorithm \\cite{Gross2005}, and the more efficient versions of SIC presented in \\cite{Papandreou2008} and \\cite{Tzimiropoulos2013}. Lucey et al. \\cite{Lucey2013} extended these algorithms to the Fourier domain to efficiently enable convolution with Gabor filters, increasing their robustness; and the authors of \\cite{Antonakos2014} showed that optimization based AAMs using non-linear feature based (e.g. SIFT\\cite{Lowe1999} and HOG \\cite{Dalal2005}) appearance models were competitive with modern state-of-the-art techniques in non-rigid face alignment \\cite{Xiong2013, Asthana2013} in terms of fitting accuracy.\n\nAAMs have often been criticized for several reasons: \n\\begin{inparaenum}[\\itshape i\\upshape)] \n\\item the limited representational power of their linear appearance model; \n\\item the difficulty of optimizing shape and appearance parameters simultaneously; and\n\\item the complexity involved in handling occlusions. \n\\end{inparaenum}\nHowever, recent works in this area \\cite{Papandreou2008, Saragih2009,Tresadern2010, Lucey2013, Tzimiropoulos2013, Antonakos2014} suggest that these limitations might have been over-stressed in the literature and that AAMs can produce highly accurate results if appropriate training data \\cite{Tzimiropoulos2013}, appearance representations \\cite{Tresadern2010, Lucey2013, Antonakos2014} and fitting strategies \\cite{Papandreou2008, Saragih2009, Tresadern2010, Tzimiropoulos2013} are employed.\n\nIn this paper, we study the problem of fitting AAMs using CGD algorithms thoroughly. Summarizing, our main contributions are:\n\\begin{itemize}\n\t\\item To present a unified and complete overview of the most relevant and recently published CGD algorithms for fitting AAMs \\cite{Matthews2004, Gross2005, Papandreou2008, Amberg2009, Martins2010, Tzimiropoulos2012, Tzimiropoulos2013, Kossaifi2014}. To this end, we classify CGD algorithms with respect to three main characteristics: \n\t\\begin{inparaenum}[\\itshape i\\upshape)] \n\t\t\\item the \\emph{cost function} defining the fitting problem; \n\t\t\\item the type of \\emph{composition} used; and \n\t\t\\item the \\emph{optimization method} employed to solve the non-linear optimization problem. \n\t\\end{inparaenum}\n\n\t\\item To review the probabilistic interpretation of AAMs and propose a novel \\emph{Bayesian formulation}\\footnote{A preliminary version of this work \\cite{Alabort2014} was presented at CVPR 2014.} of the fitting problem. We assume a probabilistic model for appearance generation with both Gaussian noise and a Gaussian prior over a latent appearance space. Marginalizing out the latent appearance space, we derive a novel cost function that only depends on shape parameters and that can be interpreted as a valid and more general probabilistic formulation of the well-known project-out cost function \\cite{Matthews2004}. Our Bayesian formulation is motivated by seminal works on probabilistic component analysis and object tracking \\cite{Moghaddam1997, Roweis1998, Tipping1999}.\n\n\t\\item To propose the use of two novel types of composition for AAMs:\n\t\\begin{inparaenum}[\\itshape i\\upshape)] \n\t\t\\item \\emph{asymmetric}; and \n\t\t\\item \\emph{bidirectional}. \n\t\\end{inparaenum} These types of composition have been widely used in the related field of parametric image alignment \\cite{Malis2004, Megret2008, Autheserre2009, Megret2010} and use the gradients of both image and appearance model to derive better convergent and more robust CGD algorithms.\n\n\t\\item To provide valuable insights into existent strategies used to derive fast and exact simultaneous algorithms for fitting AAMs by reinterpreting them as direct applications of the \\emph{Schur complement} \\cite{Boyd2004} and the \\emph{Wiberg method} \\cite{Okatani2006, Strelow2012}.\n\\end{itemize}\n\nThe remainder of the paper is structured as follows. Section \\ref{sec:aam} introduces AAMs and reviews their probabilistic interpretation. Section \\ref{sec:fitting} constitutes the main section of the paper and contains the discussion and derivations related to the cost functions \\ref{sec:cost_function}; composition types \\ref{sec:composition}; and optimization methods \\ref{sec:optimization}. Implementation details and experimental results are reported in Section \\ref{sec:experiment}. Finally, conclusions are drawn in Section \\ref{sec:conclusion}.\n\n\n\\input{sections/aam/aam}\n\\input{sections/aam/probabilistic}\n\n\\input{sections/fitting/fitting}\n\\input{sections/fitting/cost}\n\\input{sections/fitting/composition}\n\\input{sections/fitting/optimization}\n\\input{sections/fitting/relation}\n\n\\section{Experiments}\n\\label{sec:experiment}\n\nIn this section, we analyze the performance of the CGD algorithms derived in Section \\ref{sec:fitting} on the specific problems of non-rigid face alignment in-the-wild. Results for five experiments are reported. The first experiment compares the fitting accuracy and convergence properties of all algorithms on the test set of the popular Labeled Faces in-the-Wild (LFPW) \\cite{Belhumeur2011} database. The second experiment quantifies the importance of the two terms in the Bayesian project-out cost function in relation to the fitting accuracy obtained by \\emph{Project-Out} algorithms. In the third experiment, we study the effect that varying the value of the parameters $\\alpha$ and $\\beta$ has on the performance of \\emph{Asymmetric} algorithms. The fourth experiment explores the effect of optimizing the cost functions using reduced subsets of the total number of pixels and quantifies the impact that this has on the accuracy and computational efficiency of CGD algorithms. Finally, in the fifth experiment, we report the performance of the most accurate CGD algorithms on the test set of the Helen \\cite{Le2012} database and on the entire Annotated Faces in-the-Wild (AFW) \\cite{Zhu2012} database. \n\nThroughout this section, we abbreviate CGD algorithms using the following convention: \\emph{CF\\_TC\\_OM(\\_OS)} where: \n\\begin{inparaenum}[\\itshape a\\upshape)]\n\\item \\emph{CF} stands for \\emph{Cost Function} and can be either \\emph{SSD} or \\emph{PO} depending on whether the algorithm uses the \\emph{Sum of Squared Differences} or the \\emph{Project Out} cost function;\n\\item \\emph{TC} stands for \\emph{Type of Composition} and can be \\emph{For}, \\emph{Inv}, \\emph{Asy} or \\emph{Bid} depending on whether the algorithm uses \\emph{Forward}, \\emph{Inverse}, \\emph{Asymmetric} or \\emph{Bidirectional} compositions;\n\\item \\emph{OM} stands for \\emph{Optimization Method} and can be \\emph{GN}, \\emph{N} or \\emph{W} depending on whether the algorithm uses the \\emph{Gauss-Newton}, \\emph{Newton} or \\emph{Wiberg} optimization methods; and, finally,\n\\item if \\emph{Gauss-Newton} or \\emph{Newton} methods are used, the optional field \\emph{OS}, which stands for \\emph{Optimization Strategy}, can be \\emph{Sch} or \\emph{Alt} depending on whether the algorithm solves for the parameters simultaneously using the \\emph{Schur complement} or using \\emph{Alternated optimization}.\n\\end{inparaenum}\nFor example, following the previous convention the \\emph{Project Out Bidirectional Gauss-Newton Schur} algorithm is denoted by \\emph{PO\\_Bid\\_GN\\_Sch}.\n\nLandmark annotations for all databases are provided by the iBUG group\\footnote{\\url{http://ibug.doc.ic.ac.uk/resources/300-W/}} \\cite{Sagonas2013,Sagonas2013b} and fitting accuracy is reported using the point-to-point error measure normalized by the \\emph{face size}\\footnote{\\label{facesize}The face size is computed as the mean of the height and width of the bounding box containing a face.}  proposed in \\cite{Zhu2012} over the 49 interior points of the iBug annotation scheme.\n\nIn all face alignment experiments, we use a single AAM, trained using the $\\sim800$ and $\\sim2000$ training images of the LFPW and Helen databases. Similar to \\cite{Tzimiropoulos2014}, we use a modified version of the \\emph{Dense} Scale Invariant Feature Transform (DSIFT) \\cite{Lowe1999, Dalal2005} to define the appearance representation of the previous AAM. In particular, we describe each pixel with a reduced SIFT descriptor of length $8$ using the public implementation provided by the authors of \\cite{Vedaldi2008vlfeat}. All algorithms are implemented in a coarse to fine manner using a Gaussian pyramid with $2$ levels (face images are normalized to a \\emph{face size}\\footnoteref{facesize} of roughly $150$ pixels at the top level). In all experiments, we optimize over $7$ shape parameters ($4$ similarity transform and $3$ non-rigid shape parameters) at the first pyramid level and over $16$ shape parameters ($4$ similarity transform and $12$ non-rigid shape parameters) at the second one. The dimensionality of the appearance models is kept to represent $75\\%$ of the total variance in both levels. This results in $225$ and $280$ appearance parameters at the first and second pyramid levels respectively. The previous choices were determined by testing on a small hold out set of the training data. \n\nIn all experiments, algorithms are initialized by perturbing the similarity transform that perfectly aligns the model's mean shape (a frontal pose and neutral expression looking shape) with the ground truth shape of each image. These transforms are perturbed by adding uniformly distributed random noise to their scale, rotation and translation parameters. Exemplar initializations obtained by this procedure for different amounts of noise are shown in Figure \\ref{fig:ini}. Notice that, we found that initializing using $5\\%$ uniform noise is (statistically) equivalent to initializing with the popular OpenCV \\cite{opencv_library} implementation of the well-known Viola and Jones face detector \\cite{Viola2001} on the test images of the LFPW database.\n\nUnless stated otherwise:\n\\begin{inparaenum}[\\itshape i\\upshape)] \n\\item algorithms are initialized with $5\\%$ uniform noise\n\\item test images are fitted three times using different random initializations (the same exact random initializations are used for all algorithms);\n\\item algorithms are left to run for 40 iterations (24 iterations at the first pyramid level and 16 at the second);  \n\\item results for \\emph{Project-Out} algorithms are obtained using the Bayesian project-out cost function defined by Equation \\ref{eq:prob_po}; and\n\\item results for \\emph{Asymmetric} algorithms are reported for the special case of symmetric composition i.e. $\\alpha=\\beta=0.5$ in Equation \\ref{eq:ssd_ac}.\n\\end{inparaenum}\n\nIn order to showcase the broader applicability of AAMs, we complete the previous performance analysis by performing a sixth and last experiment on the problem of non-rigid car alignment in-the-wild. To this end, we report the fitting accuracy of the best performing CGD algorithms on the MIT StreetScene\\footnote{\\label{car_db_url}\\url{http://cbcl.mit.edu/software-datasets/streetscenes}} database.\n\nFinally, in order to encourage open research and facilitate future comparisons with the results presented in this section, we make the implementation of all algorithms publicly available as part of the Menpo Project\\footnoteref{menpo_url} \\cite{Menpo2014}.\n\n\n\\subsection{Comparison on LFPW}\n\\label{exp:1}\n\nIn this experiment, we report the fitting accuracy and convergence properties of all CGD algorithms studied in this paper. Results are reported on the $\\sim220$ test images of the LFPW database. In order to keep the information easily readable and interpretable, we group algorithms by cost function (i.e. \\emph{SSD} or \\emph{Project-Out}), and optimization method (i.e. \\emph{Gauss-Newton}, \\emph{Newton} or \\emph{Wiberg}).\n\nResults for this experiment are reported in Figures \\ref{fig:ssd_gn_5}, \\ref{fig:ssd_n_5}, \\ref{fig:ssd_w_5}, \\ref{fig:bpo_gn_5}, \\ref{fig:bpo_n_5} and \\ref{fig:bpo_w_5}. These figures have all the same structure and are composed of four figures and a table. Figures \\ref{fig:ced_ssd_gn_5}, \\ref{fig:ced_ssd_n_5}, \\ref{fig:ced_ssd_w_5}, \\ref{fig:ced_bpo_gn_5}, \\ref{fig:ced_bpo_n_5} and \\ref{fig:ced_bpo_w_5} report the Cumulative Error Distribution (CED), i.e the proportion of images vs normalized point-to-point error for each of the algorithms' groups. Tables \\ref{tab:stats_ssd_gn_5}, \\ref{tab:stats_ssd_n_5}, \\ref{tab:stats_ssd_w_5}, \\ref{tab:stats_bpo_gn_5}, \\ref{tab:stats_bpo_n_5}, and \\ref{tab:stats_bpo_w_5} summarize and complete the information on the previous CEDs by stating the proportion of images fitted with a normalized point-to-point error smaller than $0.02$, $0.03$ and $0.04$; and by stating the mean, std and median of the final normalized point-to-point error. The aim of the previous figures and tables is to help us compare the final fitting accuracy obtained by each algorithm. On the other hand, Figures \\ref{fig:mean_error_vs_iters_ssd_gn_5}, \\ref{fig:mean_error_vs_iters_ssd_n_5}, \\ref{fig:mean_error_vs_iters_ssd_w_5}, \\ref{fig:mean_error_vs_iters_bpo_gn_5}, \\ref{fig:mean_error_vs_iters_bpo_n_5} and \\ref{fig:mean_error_vs_iters_bpo_w_5} report the mean normalized point-to-point error at each iteration while Figures \\ref{fig:mean_cost_vs_iters1_ssd_gn_5}, \\ref{fig:mean_cost_vs_iters2_ssd_gn_5}, \\ref{fig:mean_cost_vs_iters1_ssd_n_5}, \\ref{fig:mean_cost_vs_iters2_ssd_n_5}, \\ref{fig:mean_cost_vs_iters1_ssd_w_5}, \\ref{fig:mean_cost_vs_iters2_ssd_w_5}, \\ref{fig:mean_cost_vs_iters1_bpo_gn_5}, \\ref{fig:mean_cost_vs_iters2_bpo_gn_5}, \\ref{fig:mean_cost_vs_iters1_bpo_n_5}, \\ref{fig:mean_cost_vs_iters2_bpo_n_5} and \\ref{fig:mean_cost_vs_iters1_bpo_w_5}, \\ref{fig:mean_cost_vs_iters2_bpo_w_5} report the mean normalized cost at each iteration\\footnote{These figures are produced by dividing the value of the cost function at each iteration by its initial value and averaging for all images.}. The aim of these figures is to help us compare the convergence properties of every algorithm.\n\n\n\\subsubsection{SSD Gauss-Newton algorithms}\n\nResults for \\emph{SSD Gauss-Newton} algorithms are reported in Figure \\ref{fig:ssd_gn_5}. We can observe that \\emph{Inverse}, \\emph{Asymmetric} and \\emph{Bidirectional} algorithms obtain a similar performance and significantly outperform \\emph{Forward} algorithms in terms of fitting accuracy, Figure \\ref{fig:ced_ssd_gn_5} and Table \\ref{tab:stats_ssd_gn_5}. In absolute terms, \\emph{Bidirectional} algorithms slightly outperform \\emph{Inverse} and \\emph{Asymmetric} algorithms. On the other hand, the difference in performance between the \\emph{Simultaneous Schur} and \\emph{Alternated} optimizations strategies are minimal for all algorithms and they were found to have no statistical significance.\n\nLooking at Figures \\ref{fig:mean_error_vs_iters_ssd_gn_5}, \\ref{fig:mean_cost_vs_iters1_ssd_gn_5} and \\ref{fig:mean_cost_vs_iters2_ssd_gn_5} there seems to be a clear (and obviously expected) correlation between the normalized point-to-point error and the normalized value of the cost function at each iteration. In terms of convergence, it can be seen that \\emph{Forward} algorithms converge slower than \\emph{Inverse}, \\emph{Asymmetric} and \\emph{Bidirectional}. \\emph{Bidirectional} algorithms converge slightly faster than \\emph{Inverse} algorithms and these slightly faster than \\emph{Asymmetric} algorithms. In this case, the \\emph{Simultaneous Schur} optimization strategy seems to converge slightly faster than the \\emph{Alternated} one for all \\emph{SSD Gauss-Newton} algorithms.\n\n\n\\subsubsection{SSD Newton algorithms}\n\nResults for \\emph{SSD Newton} algorithms are reported on Figure \\ref{fig:ssd_n_5}. In this case, we can observe that the fitting performance of all algorithms decreases with respect to their \\emph{Gauss-Newton} counterparts Figure \\ref{fig:ced_ssd_n_5} and Table \\ref{tab:stats_ssd_n_5}. This is most noticeable in the case of \\emph{Forward} algorithms for which there is $\\sim20\\%$ drop in the proportion of images fitted below $0.02$, $0.03$ and $0.04$ with respect to its \\emph{Gauss-Newton} equivalents. For these algorithms there is also a significant increase in the mean and median of the normalized point-to-point error. \\emph{Asymmetric Newton} algorithms also perform considerably worse, between $5\\%$ and $10\\%$, than their \\emph{Gauss-Newton} versions. The drop in performance is reduced for \\emph{Inverse} and \\emph{Bidirectional Newton} algorithms for which accuracy is only reduced by around $3\\%$ with respect their \\emph{Gauss-Newton} equivalent. \n\nWithin \\emph{Newton} algorithms, there are clear differences in terms of speed of convergence \\ref{fig:mean_error_vs_iters_ssd_n_5}, \\ref{fig:mean_cost_vs_iters1_ssd_n_5} and \\ref{fig:mean_cost_vs_iters2_ssd_n_5}. \\emph{Bidirectional} algorithms are the fastest to converge followed by \\emph{Inverse} and \\emph{Asymmetric} algorithms, in this order, and lastly \\emph{Forward} algorithms. In this case, the \\emph{Simultaneous Schur} optimization strategy seems to converge again slightly faster than the \\emph{Alternated} one for all algorithms but \\emph{Bidirectional} algorithms, for which the \\emph{Alternated} strategy converges slightly faster. Overall, \\emph{SSD Newton} algorithms converge slower than \\emph{SSD Gauss-Newton} algorithms.\n\n\\subsubsection{SSD Wiberg algorithms}\n\nResults for \\emph{SSD Wiberg} algorithms are reported on Figure \\ref{fig:ssd_w_5}. Figure \\ref{fig:ced_ssd_w_5} and Table \\ref{tab:stats_ssd_w_5} and Figures \\ref{fig:mean_error_vs_iters_ssd_w_5}, \\ref{fig:mean_cost_vs_iters1_ssd_w_5} and \\ref{fig:mean_cost_vs_iters2_ssd_w_5} show that these results are (as one would expect) virtually equivalent to those obtained by their \\emph{Gauss-Newton} counterparts.\n\n\n\\subsubsection{Project-Out Gauss-Newton algorithms}\n\nResults for \\emph{Project-Out Gauss-Newton} algorithms are reported on Figure \\ref{fig:bpo_gn_5}. We can observe that, there is significant drop in terms of fitting accuracy for \\emph{Inverse} and \\emph{Bidirectional} algorithms with respect to their \\emph{SSD} versions, \\ref{fig:ced_bpo_gn_5} and Table \\ref{tab:stats_bpo_gn_5}. As expected, the \\emph{Forward} algorithm achieves virtually the same results as its \\emph{SSD} counterpart. The \\emph{Asymmetric} algorithm obtains similar accuracy to that of the best performing \\emph{SSD} algorithms.\n\nLooking at Figures \\ref{fig:mean_error_vs_iters_bpo_gn_5}, \\ref{fig:mean_cost_vs_iters1_bpo_gn_5} and \\ref{fig:mean_cost_vs_iters2_bpo_gn_5} we can see that \\emph{Inverse} and \\emph{Bidirectional} algorithms converge slightly faster than the \\emph{Asymmetric} algorithm. However, the \\emph{Asymmetric} algorithm ends up descending to a significant lower value of the mean normalized cost which also translates to a lower value for the final mean normalized point-to-point error. Similar to \\emph{SSD} algorithms, the \\emph{Forward} algorithm  is the worst convergent algorithm.\n\nFinally notice that, in this case, there is virtually no difference, in terms of both final fitting accuracy and speed of convergence, between the \\emph{Simultaneous Schur} and \\emph{Alternated} optimizations strategies used by the \\emph{Bidirectional} algorithm.\n\n\n\\subsubsection{Project-Out Newton algorithms}\n\nResults for \\emph{Project-Out Newton} algorithms are reported on Figure \\ref{fig:bpo_n_5}. It can be clearly seen that \\emph{Project-Out Newton} algorithms perform much worse than their \\emph{Gauss-Newton} and \\emph{SSD} counterparts. The final fitting accuracy obtained by these algorithms is very poor compared to the one obtained by the best \\emph{SSD} and \\emph{Project-Out Gauss-Newton} algorithms, Figures \\ref{fig:ced_bpo_n_5} and Table \\ref{tab:stats_bpo_n_5}. In fact, by looking at Figures \\ref{fig:mean_error_vs_iters_bpo_n_5}, \\ref{fig:mean_cost_vs_iters1_bpo_n_5} and \\ref{fig:mean_cost_vs_iters2_bpo_n_5} only the \\emph{Forward} and \\emph{Asymmetric} algorithms seem to be stable at the second level of the Gaussian pyramid with \\emph{Inverse} and \\emph{Bidirectional} algorithms completely diverging for some of the images as shown by the large mean and std of their final normalized point-to-point errors. \n\n\n\\subsubsection{Project-Out Wiberg algorithms}\n\nResults for the \\emph{Project-Out Bidirectional Wiberg} algorithm are reported on Figure \\ref{fig:bpo_n_5}. As expected, the results are virtually identical to those of the obtained by \\emph{Project-Out Bidirectional Gauss-Newton} algorithms.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\begin{subfigure}{0.16\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/sampling_100.jpeg}\n\t\t\\caption{$100\\%$}\n\t\t\\label{fig:sampling_100}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.16\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/sampling_50.jpeg}\n\t\t\\caption{$50\\%$}\n\t\t\\label{fig:sampling_50}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.16\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/sampling_25.jpeg}\n\t\t\\caption{$25\\%$}\n\t\t\\label{fig:sampling_25}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.16\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{figures/sampling_12.jpeg}\n\t\t\\caption{$12\\%$}\n\t\t\\label{fig:ini_12}\n\t\\end{subfigure}\n\t\\caption{Subset of pixels on the reference frame used to optimize the SSD and Project-Out cost functions for different sampling rates.}\n    \\label{fig:sampling_masks}\n\\end{figure*}\n\n\n\\subsection{Weighted Bayesian project-out}\n\\label{exp:2}\n\nIn this experiment, we quantify the importance of each of the two terms in our Bayesian project-out cost function, Equation \\ref{eq:prob_po}. To this end, we introduce the parameters, $\\rho \\in [0, 1]$ and $\\gamma = 1 - \\rho$, to weight up the relative contribution of both terms:\n\n", "index": 1, "text": "\\begin{equation}\n    \\begin{aligned}\n        \\rho|| \\mathbf{i}[\\mathbf{p}] - \\mathbf{\\bar{a}} ||^2_{\\mathbf{A}\\mathbf{D}^{-1}\\mathbf{A}^T} \n        + \n        \\frac{\\gamma}{\\sigma^2}|| \\mathbf{i}[\\mathbf{p}] - \\mathbf{\\bar{a}} ||^2_{\\bar{\\mathbf{A}}}\n    \\end{aligned}\n    \\label{eq:weighted_po}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\rho||\\mathbf{i}[\\mathbf{p}]-\\mathbf{\\bar{a}}||^{2}_{\\mathbf{A}%&#10;\\mathbf{D}^{-1}\\mathbf{A}^{T}}+\\frac{\\gamma}{\\sigma^{2}}||\\mathbf{i}[\\mathbf{p%&#10;}]-\\mathbf{\\bar{a}}||^{2}_{\\bar{\\mathbf{A}}}\" display=\"inline\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>\ud835\udc22</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc29</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo fence=\"true\">||</mo></mrow><mrow><msup><mi>\ud835\udc00\ud835\udc03</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc00</mi><mi>T</mi></msup></mrow><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b3</mi><msup><mi>\u03c3</mi><mn>2</mn></msup></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>\ud835\udc22</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc29</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mover accent=\"true\"><mi>\ud835\udc1a</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo fence=\"true\">||</mo></mrow><mover accent=\"true\"><mi>\ud835\udc00</mi><mo stretchy=\"false\">\u00af</mo></mover><mn>2</mn></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 72360, "prevtext": "\nSetting $\\rho=0$, $\\gamma=1$ reduces the previous cost function to the original project-out loss proposed in \\cite{Matthews2004}; completely disregarding the contribution of the prior distribution over the appearance parameters i.e the Mahalanobis distance \\emph{within} the appearance subspace. On the contrary, setting $\\rho=1$, $\\gamma=0$ reduces the cost function to the first term; completely disregarding the contribution of the project-out term i.e. the distance \\emph{to} the appearance subspace. Finally setting $\\rho=\\gamma=0.5$ leads to the standard Bayesian project-out cost function proposed in Section \\ref{sec:po_pi}.\n \nIn order to assess the impact that each term has on the fitting accuracy obtained by the previous \\emph{Project-Out} algorithm we repeat the experimental set up of the first experiment and test all \\emph{Project-Out Gauss-Newton} algorithms for different values of the parameters $\\rho = 1 -\\gamma$. Notice that, in this case, we only report the performance of \\emph{Gauss-Newton} algorithms because they were shown to vastly outperform \\emph{Newton} algorithms and to be virtually equivalent to \\emph{Wiberg} algorithms in the first experiment.\n\nResults for this experiment are reported by Figure \\ref{fig:rho}. We can see that, regardless of the type of composition, a weighted combination of the two previous terms always leads to a smaller mean normalized point-to-point error compared to either term on its own. Note that the final fitting accuracy obtained with the standard Bayesian project-out cost function is substantially better than the one obtained by the original project-out loss (this is specially noticeable for the \\emph{Inverse} and \\emph{Bidirectional} algorithms); fully justifying the inclusion of the first term, i.e the Mahalanobis distance \\emph{within} the appearance subspace, into the cost function. Finally, in this particular experiment, the final fitting accuracy of all algorithms is maximized by setting $\\rho=0.1$, $\\gamma=0.9$, further highlighting the importance of the first term in the Bayesian formulation.\n\n\n\n\n\n\n\n\n\n\\subsection{Optimal asymmetric composition}\n\\label{exp:3}\n\nThis experiment quantifies the effect that varying the value of the parameters $\\alpha \\in [0, 1]$ and $\\beta = 1 -\\alpha$ in Equation \\ref{eq:ssd_ac} has in the fitting accuracy obtained by the \\emph{Asymmetric} algorithms. Note that for $\\alpha=1$, $\\beta=0$ and $\\alpha=0$, $\\beta=1$ these algorithms reduce to their \\emph{Forward} and \\emph{Inverse} versions respectively. Recall that, in previous experiments, we used the \\emph{Symmetric} case $\\alpha=\\beta=0.5$ to generate the results reported for \\emph{Asymmetric} algorithms. Again, we only report performance for \\emph{Gauss-Newton} algorithms.\n\nWe again repeat the experimental set up described in the first experiments and report the fitting accuracy obtained by the \\emph{Project Out} and \\emph{SSD Asymmetric Gauss-Newton} algorithms for different values of the parameters $\\alpha = 1 - \\beta$. Results are shown in Figure \\ref{fig:alpha}. For the \\emph{BPO Asymmetric} algorithm, the best results are obtain by setting $\\alpha=0.4$, $\\beta=0.6$, Figures \\ref{fig:asy_gn_vs_alpha_5} (top) and \\ref{fig:ced_po_asy_gn_5}. These results slightly outperform those obtain by the default \\emph{Symmetric} algorithm and this particular configuration of the \\emph{BPO Asymmetric} algorithm is the best performing one on the LFPW test dataset. For the \\emph{SSD Asymmetric Gauss-Newton} algorithm the best results are obtained by setting $\\alpha=0.2$, $\\beta=0.8$, Figures \\ref{fig:asy_gn_vs_alpha_5} (bottom) and \\ref{fig:ced_ssd_asy_gn_5}. In this case, the boost in performance with respect to the default \\emph{Symmetric} algorithm is significant and, with this particular configuration, the \\emph{SSD Asymmetric Gauss-Newton} algorithm is the best performing \\emph{SSD} algorithm on the LFPW test dataset, outperforming \\emph{Inverse} and \\emph{Bidirectional} algorithms.\n\n\n\n\n\n\n\n\n\n\\subsection{Sampling and Number of Iterations}\n\\label{exp:4}\n\nIn this experiment, we explore two different strategies to reduce the running time of the previous CGD algorithms. \n\nThe first one consists of optimizing the SSD and Project-Out cost functions using only a subset of all pixels in the reference frame. In AAMs the total number of pixels on the reference frame, $F$, is typically several orders of magnitude bigger than the number of shape, $n$, and appearance, $m$, components i.e. $F>>m>>n$. Therefore, a significant reduction in the complexity (and running time) of CGD algorithms can be obtained by decreasing the number of pixels that are used to optimize the previous cost functions. To this end, we compare the accuracy obtained by using $100\\%$, $50\\%$, $25\\%$ and $12\\%$ of the total number of pixels on the reference frame. Note that, pixels are (approximately) evenly sampled across the reference frame in all cases, Figure \\ref{fig:sampling_masks}.\n\nThe second strategy consists of simply reducing the number of iterations that each algorithm is run. Based on the figures used to assess the convergence properties of CGD algorithms in previous experiments, we compare the accuracy obtained by running the algorithms for $40$ $(24 + 16)$ and $20$ $(12 + 8)$ iterations.\n\nNote that, in order to further highlight the advantages and disadvantages of using the previous strategies we report the fitting accuracy obtained by initializing the algorithms using different amounts of uniform noise.\n\nOnce more we repeat the experimental set up of the first experiment and report the fitting accuracy obtained by the Project Out and SSD Asymmetric Gauss-Newton algorithms. Results for this experiment are shown in Figure \\ref{fig:sampling}. It can be seen that reducing the number of pixels up to $25\\%$ while maintaining the original number of iterations to $40$ $(24 + 16)$ has little impact on the fitting accuracy achieved by both algorithms while reducing them to $12\\%$ has a clear negative impact, Figures \\ref{fig:sampling_vs_noise_ssd_asy_gn} and \\ref{fig:sampling_vs_noise_po_asy_gn}. Also, performance seems to be consistent along the amount of noise. In terms of run time, Table \\ref{tab:runtime_40}, reducing the number of pixels to $50\\%$, $25\\%$ and $12\\%$ offers speed ups of $\\sim2.0$x, $\\sim2.9$x and $\\sim3.7$x for the \\emph{BPO} algorithm and of $\\sim1.8$x, $\\sim2.6$x and $\\sim2.8$x for the \\emph{SSD} algorithm respectively.  \n\nOn the other hand, reducing the number of iterations from $40$ $(24 + 16)$ to $20$ $(12 + 8)$ has no negative impact in performance for levels of noise smaller than $2\\%$ but has a noticeable negative impact for levels of noise bigger than $5\\%$. Notice that remarkable speed ups, Table \\ref{tab:runtime_20}, can be obtain for both algorithms by combining the previous two strategies at the expenses of small but noticeable decreases in fitting accuracy.\n\n\n\\subsection{Comparison on Helen and AFW}\n\\label{exp:5}\n\nIn order to facilitate comparisons with recent prior work on AAMs \\cite{Tzimiropoulos2013, Antonakos2014, Kossaifi2014} and with other state-of-the-art approaches in face alignment \\cite{Xiong2013, Asthana2013}, in this experiment, we report the fitting accuracy of the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms on the widely used test set of the Helen database and on the entire AFW database. Furthermore we compare the performance of the previous two algorithms with the one obtained by the recently proposed Gauss-Newton Deformable Part Models (GN-DPMs) proposed by Tzimiropoulos and Pantic in \\cite{Tzimiropoulos2014}; which was shown to achieve state-of-the-art results in the problem of face alignment in-the-wild.\n\nFor both our algorithms, we report two different types of results:\n\\begin{inparaenum}[\\itshape i\\upshape)]\n\\item sampling rate of $25\\%$ and $20$ $(12 + 8)$ iterations; and\n\\item sampling rate of $50\\%$ and $40$ $(24 + 16)$ iterations,\n\\end{inparaenum}. For GN-DPMs we use the authors public implementation to generate the results. In this case, we report, again, two different types of results by letting the algorithm run for $20$ and $40$ iterations.\n\nResult for this experiment are shown in Figure \\ref{fig:helen_afw}. Looking at Figure \\ref{fig:ced_helen} we can see that both, \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms, obtain similar fitting accuracy on the Helen test dataset. Note that, in all cases, their accuracy is comparable to the one achieved by GN-DPMs for normalized point-to-point errors $<0.2$ and significantly better for $<0.3$, $<0.4$. As expected, the best results for both our algorithms are obtained using $50\\%$ of the total amount of pixels and $40$ $(24 + 16)$ iterations. However, the results obtained by using only $25\\%$ of the total amount of pixels and $20$ $(12 + 8)$ iterations are comparable to the previous ones; specially for the \\emph{Project-Out Asymmetric Gauss-Newton}. In general, these results are consistent with the ones obtained on the LFPW test dataset, Experiments \\ref{exp:1} and \\ref{exp:3}. \n\nOn the other hand, the performance of both algorithms drops significantly on the AFW database, Figure \\ref{fig:ced_afw} . In this case, GN-DPMs achieves slightly better results than the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms for normalized point-to-point errors $<0.2$ and slightly worst for $<0.3$, $<0.4$. Again, both our algorithms obtain better results by using $50\\%$ sampling rate and $40$ $(24 + 16)$ iterations and the difference in accuracy with respect to the versions using $25\\%$ sampling rate and $20$ $(12 + 8)$ iterations slightly widens when compared to the results obtained on the Helen test dataset. This drop in performance is consistent with other recent works on AAMs \\cite{Tzimiropoulos2014, Alabort2014, Antonakos2014, Alabort2015} and it is attributed to large difference in terms of shape and appearance statistics between the images of the AFW dataset and the ones of the training sets of the LFPW and Helen datasets where the AAM model was trained on. \n\nExemplar results for this experiment are shown in Figures \\ref{fig:helen} and \\ref{fig:afw}. \n\n\n\\subsection{Comparison on MIT StreetScene}\n\\label{exp:6}\n\nIn this final experiment, we present results for a different type of object: cars. To this end, we use the first view of the MIT StreetScene\\footnoteref{car_db_url} dataset containing a wide variety of frontal car images obtained in the wild. We use 10-fold cross-validation on the $\\sim 500$ images of the previous dataset to train and test our algorithms. We report results for the two versions of the \\emph{SSD Asymmetric Gauss-Newton} and the \\emph{Project-Out Asymmetric Gauss-Newton} algorithms used in Experiment \\ref{exp:5}.\n\nResult for this experiment are shown in Figure \\ref{fig:ced_cars}. We can observe that all algorithms obtain similar performance and that they vastly improve upon the original initialization. \n\nExemplar results for this experiment are shown in Figure \\ref{fig:cars}.\n\n\n\\subsection{Analysis}\n\\label{exp:analysis}\n\nGiven the results reported by the previous six experiments we conclude that:\n\n\\begin{enumerate}\n\\item Overall, \\emph{Gauss-Newton} and \\emph{Wiberg} algorithms vastly outperform \\emph{Newton} algorithms for fitting AAMs. Experiment \\ref{exp:1} clearly shows that the former algorithms provide significantly higher levels of fitting accuracy at considerably lower computational complexities and run times. These findings are consistent with existent literature in the related field of parametric image alignment \\cite{Matthews2004} and also, to certain extend, with prior work on \\emph{Newton} algorithms for AAM fitting \\cite{Kossaifi2014}. We attribute the bad performance of \\emph{Newton} algorithms to the difficulty of accurately computing a (noiseless) estimate of the full Hessian matrix using finite differences.\n\n\\item \\emph{Gauss-Newton} and \\emph{Wiberg} algorithms are virtually equivalent in performance. The results in Experiment \\ref{exp:1} show that the difference in accuracy between both types of algorithms is minimal and the small differences in their respective solutions are, in practice, insignificant.\n\n\\item Our \\emph{Bayesian} project-out formulation leads to significant improvements in fitting accuracy without adding extra computational cost. Experiment \\ref{exp:2} shows that a weighted combination of the two terms forming \\emph{Bayesian} project-out loss always outperforms the \\emph{classic} project out formulation.\n\n\\item The \\emph{Asymmetric} composition proposed in this work leads to CGD algorithms that are more accurate and that converge faster. In particular, the \\emph{SSD} and \\emph{Project-Out Asymmetric Gauss-Newton} algorithms are shown to achieve significantly better performance than their \\emph{Forward} and \\emph{Inverse} counterparts in Experiments \\ref{exp:1} and \\ref{exp:3}.\n\n\\item Finally, a significant reduction in the computational complexity and runtime of CDG algorithms can be obtained by limiting the number of pixels considered during optimization of the loss function and by adjusting the number of iterations that the algorithms are run for, Experiment \\ref{exp:4}.\n\\end{enumerate}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/ced_ssd_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_error_vs_iters_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_cost_vs_iters1_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_gn/mean_cost_vs_iters2_ssd_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t\t\\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\\n\t\t    SSD\\_For\\_GN\\_Sch & 0.456 & 0.707 & 0.777 & 0.033 & 0.030 & 0.021 \n\t\t    \\\\\n\t\t    SSD\\_For\\_GN\\_Alt & 0.445 & 0.702 & 0.766 & 0.033 & 0.030 & 0.021\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_GN\\_Sch & 0.686 & 0.906 & 0.939 & 0.022 & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_GN\\_Alt & 0.673 & 0.897 & 0.933 & 0.022 & 0.020 & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_GN\\_Sch & 0.640 & 0.891 & 0.929 & 0.023 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_GN\\_Alt & 0.635 & 0.882 & 0.924 & 0.023 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_GN\\_Sch & 0.674 & 0.917 & 0.946 & 0.022 & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_GN\\_Alt & \\textbf{0.680} & \\textbf{0.924} & \\textbf{0.951} & \\textbf{0.021} & \\textbf{0.019} & \\textbf{0.017} \n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all SSD Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Gauss-Newton algorithms on the LFPW test dataset initialized with $5\\%$ uniform noise.}\n\t\\label{fig:ssd_gn_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/ced_ssd_n_5.jpeg}\n\t    \\caption{Cumulative error distribution on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_error_vs_iters_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_cost_vs_iters1_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_n/mean_cost_vs_iters2_ssd_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t\t\\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\\n\t\t    SSD\\_For\\_N\\_Sch & 0.249 & 0.479 & 0.603 & 0.044 & 0.033 & 0.031\n\t\t    \\\\\n\t\t    SSD\\_For\\_N\\_Alt & 0.244 & 0.476 & 0.600 & 0.044 & 0.033 & 0.032\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_N\\_Sch & 0.626 & 0.876 & 0.909 & 0.024 & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_N\\_Alt & 0.613 & 0.876 & 0.909 & 0.024 & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_N\\_Sch & 0.562 & 0.812 & 0.863 & 0.030 & 0.076 & 0.019\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_N\\_Alt & 0.557 & 0.808 & 0.862 & 0.027 & 0.025 & 0.019\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_N\\_Sch & \\textbf{0.641} & 0.897 & 0.932 & \\textbf{0.023} & 0.022 & \\textbf{0.018}\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_N\\_Alt & 0.600 & \\textbf{0.903} & \\textbf{0.939} & \\textbf{0.023} & \\textbf{0.021} & \\textbf{0.018}\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error Mean, Std and Median for all SSD Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_n_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Newton algorithms on the LFPW test dataset initialized with $5\\%$ uniform noise.}\n\t\\label{fig:ssd_n_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/ced_ssd_w_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_ssd_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_error_vs_iters_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_ssd_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_cost_vs_iters1_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_ssd_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/ssd_w/mean_cost_vs_iters2_ssd_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_ssd_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    SSD\\_For\\_W & 0.457 & 0.707 & 0.777 & 0.33 & 0.030 & 0.021\n\t\t    \\\\\n\t\t    SSD\\_Inv\\_W & \\textbf{0.689} & 0.903 & 0.939 & \\textbf{0.22} & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    SSD\\_Asy\\_W & 0.635 & 0.887 & 0.926 & 0.23 & 0.021 & 0.018\n\t\t    \\\\\n\t\t    SSD\\_Bid\\_W & 0.686 & \\textbf{0.911} & \\textbf{0.942} & \\textbf{0.22} & \\textbf{0.019} & \\textbf{0.017}\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all SSD Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_ssd_w_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the SSD Wiberg algorithms on the LFPW test dataset.}\n\t\\label{fig:ssd_w_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/ced_po_gn_5.jpeg}\n\t    \\caption{CED graph on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_error_vs_iters_po_gn_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_cost_vs_iters1_po_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_gn/mean_cost_vs_iters2_po_gn_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_gn_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_For\\_GN\\_Sch & 0.470 & 0.729 & 0.799 & 0.031 & 0.029 & 0.021\n\t\t    \\\\\n\t\t    PO\\_For\\_GN\\_Alt & 0.458 & 0.719 & 0.780 & 0.035 & 0.044 & 0.021\n\t\t    \\\\\n\t\t    PO\\_Inv\\_GN\\_Sch & \\textbf{0.637} & \\textbf{0.891} & \\textbf{0.938} & \\textbf{0.023} & \\textbf{0.021} & \\textbf{0.018}\n\t\t    \\\\\n\t\t    PO\\_Bid\\_GN\\_Sch & 0.528 & 0.802 & 0.862 & 0.030 & 0.039 & 0.020\n\t\t    \\\\\n\t\t    PO\\_Bid\\_GN\\_Alt & 0.528 & 0.805 & 0.865 & 0.030 & 0.040 & 0.019\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Gauss-Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Gauss-Newton algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_gn_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/ced_po_n_5.jpeg}\n\t    \\caption{CED graph on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_error_vs_iters_po_n_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_cost_vs_iters1_po_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_n_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_n/mean_cost_vs_iters2_po_n_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations on the LFPW test dataset for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_n_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_For\\_N\\_Sch & 0.280 & 0.503 & 0.626 & 0.043 & 0.033 & 0.030\n\t\t    \\\\\n\t\t    PO\\_Inv\\_N\\_Alt & 0.265 & 0.516 & 0.586 & 11.929 & 179.525 & 0.029\n\t\t    \\\\\n\t\t    PO\\_Asy\\_N\\_Sch & \\textbf{0.494} & \\textbf{0.744} & \\textbf{0.826} & \\textbf{0.030} & \\textbf{0.028} & \\textbf{0.020}\n\t\t    \\\\\n\t\t    PO\\_Bid\\_N\\_Sch & 0.314 & 0.536 & 0.649 & 0.287 & 1.347 & 0.027\n\t\t    \\\\\n\t\t    PO\\_Bid\\_N\\_Alt & 0.329 & 0.570 & 0.649 & 0.280 & 1.465 & 0.026\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Newton algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_n_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Newton algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_n_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/ced_po_w_5.jpeg}\n\t    \\caption{Cumulative Error Distribution graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:ced_bpo_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_error_vs_iters_po_w_5.jpeg}\n\t    \\caption{Mean normalized point-to-point error vs number of iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_error_vs_iters_bpo_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_cost_vs_iters1_po_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of first scale iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters1_bpo_w_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/algorithms/po_w/mean_cost_vs_iters2_po_w_5.jpeg}\n\t    \\caption{Mean normalized cost vs number of second scale iterations graph on the LFPW test dataset for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{fig:mean_cost_vs_iters2_bpo_w_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    Algorithm & $<0.02$ & $<0.03$ & $<0.04$ & Mean & Std & Median \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    Initialization & 0.000 & 0.004 & 0.055 & 0.080 & 0.028 & 0.078\n\t\t    \\\\ \n\t\t    PO\\_Bid\\_W\\_Sch & 0.524 & 0.801 & 0.862 & 0.030 & 0.039 & 0.020\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing the proportion of images fitted with a normalized point-to-point error below $0.02$, $0.03$ and $0.04$ together with the normalized point-to-point error mean, std and median for all Project-Out Wiberg algorithms initialized with $5\\%$ uniform noise.}\n\t    \\label{tab:stats_bpo_w_5}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy and convergence properties of the Project-Out Wiberg algorithms on the LFPW test dataset.}\n\t\\label{fig:bpo_w_5}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/convergence_vs_rho_po_gn_5.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out and SSD Asymmetric Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:convergence_vs_rho_po_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_for_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Forward Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_for_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_inv_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Inverse Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_inv_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Asymmetric Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_asy_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/rho/ced_po_bid_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Bidirectional Gauss-Newton algorithms for different values of $\\rho = 1 -\\gamma$  and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_bid_gn}\n\t\\end{subfigure}\n\t\\caption{Results quantifying the effect of varying the value of the parameters $\\rho = 1 -\\gamma$ in Project-Out Gauss-Newton algorithms.}\n\t\\label{fig:rho}\n\\end{figure*}\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_ssd_asy_gn.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the SSD Asymmetric Gauss-Newton algorithm using different sampling rates, $40$ $(24 + 16)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_ssd_asy_gn}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_po_asy_gn.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out Asymmetric Gauss-Newton algorithm using different sampling rates, $40$ $(24 + 16)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_po_asy_gn}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    & $100\\%$ & $<50\\%$ & $<25\\%$ & $<12\\%$ \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    SSD\\_Asy\\_GN\\_Sch & $\\sim1680$ ms & $\\sim930$ ms & $\\sim650$ ms & $\\sim590$ ms\n\t\t    \\\\ \n\t\t    PO\\_Asy\\_GN & $\\sim1400$ ms & $\\sim680$ ms & $\\sim480$ ms & $\\sim380$ ms\n\t\t    \\\\\n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing run time of each algorithm for different amounts of sampling and $40$ $(24 + 16)$ iterations.}\n\t    \\label{tab:runtime_40}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_po_asy_gn_20.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out Asymmetric Gauss-Newton algorithm using different sampling rates, $20$ $(12 + 8)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_ssd_asy_gn_20}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/sampling/sampling_vs_noise_ssd_asy_gn_20.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the SSD Asymmetric Gauss-Newton algorithm using different sampling rates, $20$ $(12 + 8)$ iterations, and initialized with different amounts of noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:sampling_vs_noise_po_asy_gn_20}\n\t\\end{subfigure}\n\t\\par\\bigskip\n\t\\begin{subfigure}{\\textwidth}\n\t\t\\center\n\t\t\\begin{tabular}{lcccccc}\n\t\t    \\toprule\n\t\t    & $100\\%$ & $<50\\%$ & $<25\\%$ & $<12\\%$ \n\t\t    \\\\\n\t\t    \\midrule\n\t\t    SSD\\_Asy\\_GN\\_Sch & $\\sim892$ ms & $\\sim519$ ms & $\\sim369$ ms & $\\sim331$ ms\n\t\t    \\\\\n\t\t    PO\\_Asy\\_GN & $\\sim707$ ms & $\\sim365$ ms & $\\sim235$ ms & $\\sim211$ ms\n\t\t    \\\\ \n\t\t    \\bottomrule\n\t  \t\\end{tabular}\n\t  \t\\caption{Table showing run time of each algorithm for different amounts of sampling and $20$ $(12 + 8)$ iterations.}\n\t    \\label{tab:runtime_20}\n\t\\end{subfigure}\n\t\\caption{Results assessing the effectiveness of sampling for the best performing Project-Out and SSD algorithms on the LFPW database.}\n\t\\label{fig:sampling}\n\\end{figure*}\n\n\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/asy_gn_vs_alpha_5.jpeg}\n\t    \\caption{Proportion of images with normalized point-to-point errors smaller than $0.02$, $0.03$ and $0.04$ for the Project-Out and SSD Asymmetric Gauss-Newton algorithms for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise. Colors encode overall fitting accuracy, from highest to lowest: red, orange, yellow, green, blue and purple.}\n\t    \\label{fig:asy_gn_vs_alpha_5}\n\t\\end{subfigure}\n\t\\par\\bigskip\\bigskip\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/ced_po_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for Project-Out Asymmetric Gauss-Newton algorithm for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_po_asy_gn_5}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/alpha/ced_ssd_asy_gn_5.jpeg}\n\t    \\caption{CED on the LFPW test dataset for the the SSD Asymmetric Gauss-Newton algorithm for different values of $\\alpha = 1 - \\beta$ and initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_ssd_asy_gn_5}\n\t\\end{subfigure}\n\t\\caption{Results quantifying the effect of varying the value of the parameters $\\alpha = 1 - \\beta$ in Asymmetric algorithms.}\n\t\\label{fig:alpha}\n\\end{figure*}\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/best/ced_helen_5.jpeg}\n\t    \\caption{CED on the Helen test dataset for the Project-Out and SSD Asymmetric Gauss-Newton algorithms initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_helen}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{0.48\\textwidth}\n\t    \\includegraphics[width=\\textwidth]{experiments/best/ced_afw_5.jpeg}\n\t    \\caption{CED on the AFW database for the Project-Out and SSD Asymmetric Gauss-Newton algorithm initialized with $5\\%$ noise.}\n\t    \\label{fig:ced_afw}\n\t\\end{subfigure}\n\t\\caption{Results showing the fitting accuracy of the SSD and Project-Out Asymmetric Gauss-Newton algorithms on the Helen and AFW databases.}\n\t\\label{fig:helen_afw}\n\\end{figure*}\n\n\\begin{figure*}[p]\n\t\\centering\n\t\\begin{subfigure}{0.48\\textwidth}\n    \t\\includegraphics[width=\\textwidth]{experiments/best/ced_cars_5.jpeg}\n    \\end{subfigure}\n    \\caption{CED on the first view of the MIT StreetScene test dataset for the Project-Out and SSD Asymmetric Gauss-Newton algorithms initialized with $5\\%$ noise.}\n    \\label{fig:ced_cars}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_5.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_po_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:helen_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_5.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/helen_ssd_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:helen_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the Helen test dataset.}\n\t\\label{fig:helen}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_po_6.jpeg}\n\t\t\\caption{Exemplar results from the Helen test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:afw_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_0.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_1.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_2.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_3.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_4.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth, height=0.16\\textwidth]{figures/afw_ssd_6.jpeg}\n\t\t\\caption{Exemplar results from the AFW dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:afw_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the AFW dataset.}\n\t\\label{fig:afw}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth]{figures/car00.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car01.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car02.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car03.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car04.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car05.jpeg}\n\t\t\\caption{Exemplar results from the MIT StreetScene test dataset obtained by the Project-Out Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:cars_po}\n\t\\end{subfigure}\n\t\\begin{subfigure}{\\textwidth}\n\t    \\includegraphics[width=0.16\\textwidth]{figures/car10.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car11.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car12.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car13.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car14.jpeg}\n\t\t\\includegraphics[width=0.16\\textwidth]{figures/car15.jpeg}\n\t\t\\caption{Exemplar results from the MIT StreetScene test dataset obtained by the SSD Asymmetric Gauss-Newton Schur algorithm.}\n\t\t\\label{fig:cars_ssd}\n\t\\end{subfigure}\n\t\\caption{Exemplar results from the MIT StreetScene test dataset.}\n\t\\label{fig:cars}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this paper we have thoroughly studied the problem of fitting AAMs using CGD algorithms. We have presented a unified and complete framework for these algorithms and classified them with respect to three of their main characteristics:\n\\begin{inparaenum}[\\itshape i\\upshape)]\n\t\\item \\emph{cost function};\n\t\\item type of \\emph{composition}; and\n\t\\item \\emph{optimization method}.\n\\end{inparaenum}\n\nFurthermore, we have extended the previous framework by:\n\\begin{itemize}\n\t\\item Proposing a novel \\emph{Bayesian cost function} for fitting AAMs that can be interpreted as a more general formulation of the well-known project-out loss. We have assumed a probabilistic model for appearance generation with both Gaussian noise and a Gaussian prior over a latent appearance space. Marginalizing out the latent appearance space, we have derived a novel cost function that only depends on shape parameters and that can be interpreted as a valid and more general probabilistic formulation of the well-known project-out cost function \\cite{Matthews2004}. In the experiments, we have showed that our Bayesian formulation considerably outperforms the original project-out cost function.\n\t\n\t\\item Proposing \\emph{asymmetric} and \\emph{bidirectional} compositions for CGD algorithms. We have shown the connection between Gauss-Newton Asymmetric algorithms and ESM algorithms and experimentally proved that these two novel types of composition lead to better convergent and more robust CGD algorithm for fitting AAMs.\n\t\n\t\\item Providing new valuable insights into existent CGD algorithms by reinterpreting them as direct applications of the \\emph{Schur complement} and the \\emph{Wiberg method}.\n\\end{itemize} \n\nFinally, in terms of future work, we plan to:\n\\begin{itemize}\n\t\\item Adapt existent Supervised Descent (SD) algorithms for face alignment \\cite{Xiong2013, Tzimiropoulos2015} to AAMs and investigate their relationship with the CGD algorithms studied in this paper. \n\t\n\t\\item Investigate if our Bayesian cost function and the proposed asymmetric and bidirectional compositions can also be successfully applied to similar generative parametric models, such as the Gauss-Newton Parts-Based Deformable Model (GN-DPM) proposed in \\cite{Tzimiropoulos2014}.   \n\\end{itemize} \n\n\\begin{acknowledgements}\nThe work of Joan Alabort-i-Medina\nis funded by a DTA studentship from Imperial College London\nand by the Qualcomm Innovation Fellowship. The work of S. Zafeiriou has been partly funded by the EPSRC project Adaptive Facial Deformable Models for Tracking (ADAManT), EP/L026813/1.\n\\end{acknowledgements}\n\n\n\\bibliographystyle{spbasic}      \n\\bibliography{main.bib}   \n\n\\appendix \n\n\\pagebreak\n\n\\section{Terms in SSD Newton Hessians}\n\\label{sec:app1}\n\nIn this section we define the individual terms of the Hessian matrices used by the \\emph{SSD Asymmetric} and \\emph{Bidirectional Newton} optimization algorithms derived in Section \\ref{sec:newton}.\n\n\\subsection{Asymmetric}\n\\label{sec:app11}\n\nThe individual terms forming the Hessian matrix of the \\emph{SSD Asymmetric Newton} algorithm defined by Equation \\ref{eq:asymmetric_newton_hessian} are defined as follows:\n\n", "index": 3, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_a}{\\partial^2 \\Delta \\mathbf{c}} & = \\frac{\\partial -\\mathbf{A}^T \\mathbf{r}_a}{\\partial \\Delta \\mathbf{c}}\n\t\t\\\\\n\t\t& = -\\mathbf{A}^T \\frac{\\partial \\mathbf{r}_a}{\\partial \\Delta \\mathbf{c}}\n\t\t\\\\\n\t\t& = \\underbrace{\\mathbf{A}^T \\mathbf{A}}_{\\mathbf{I}}\n    \\label{eq:asymmetric_hessian_term1}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{a}}{\\partial^{2}\\Delta\\mathbf{c}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>a</mi></msub></mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}\\mathbf{r}_{a}}{\\partial\\Delta%&#10;\\mathbf{c}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{A}^{T}\\frac{\\partial\\mathbf{r}_{a}}{\\partial\\Delta%&#10;\\mathbf{c}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underbrace{\\mathbf{A}^{T}\\mathbf{A}}_{\\mathbf{I}}\" display=\"inline\"><mrow><mi/><mo>=</mo><munder><munder accentunder=\"true\"><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo movablelimits=\"false\">\u2062</mo><mi>\ud835\udc00</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mi>\ud835\udc08</mi></munder></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\nwhere we have defined $\\mathbf{J}_\\mathbf{A} = [\\nabla \\mathbf{a}_1, \\cdots, \\nabla \\mathbf{a}_m]^T \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}}$.\n\n", "itemtype": "equation", "pos": 72752, "prevtext": "\n\n", "index": 5, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_a}{\\partial \\Delta \\mathbf{c} \\partial \\Delta \\mathbf{p}} & = \\frac{\\partial -\\mathbf{A}^T \\mathbf{r}_a}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = \\frac{\\partial -\\mathbf{A}^T}{\\partial \\Delta \\mathbf{p}} \\mathbf{r}_a - \\mathbf{A}^T \\frac{\\partial\\mathbf{r}_a}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = -\\beta \\mathbf{J}_\\mathbf{A}^T \\mathbf{r}_a - \\mathbf{A}^T \\mathbf{J}_{\\mathbf{t}}\n    \\label{eq:asymmetric_hessian_term2}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{a}}{\\partial\\Delta\\mathbf{c}%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>a</mi></msub></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}\\mathbf{r}_{a}}{\\partial\\Delta%&#10;\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}}{\\partial\\Delta\\mathbf{p}}\\mathbf{%&#10;r}_{a}-\\mathbf{A}^{T}\\frac{\\partial\\mathbf{r}_{a}}{\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><msup><mi>\ud835\udc00</mi><mi>T</mi></msup></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\beta\\mathbf{J}_{\\mathbf{A}}^{T}\\mathbf{r}_{a}-\\mathbf{A}^{T}%&#10;\\mathbf{J}_{\\mathbf{t}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc00</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n\n\\subsection{Bidirectional}\n\\label{sec:app12}\n\nThe individual terms forming the Hessian matrix of the \\emph{SSD Bidirectional Newton} algorithm defined by Equation \\ref{eq:bidirectional_newton_hessian} are defined as follows:\n\n", "itemtype": "equation", "pos": 73430, "prevtext": "\nwhere we have defined $\\mathbf{J}_\\mathbf{A} = [\\nabla \\mathbf{a}_1, \\cdots, \\nabla \\mathbf{a}_m]^T \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}}$.\n\n", "index": 7, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_a}{\\partial^2 \\Delta \\mathbf{p}} & =  \\frac{\\partial \\mathbf{J}_{\\mathbf{t}}^T \\mathbf{r}_a}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = \\frac{\\partial \\mathbf{J}_{\\mathbf{t}}^T}{\\partial \\Delta \\mathbf{p}} \\mathbf{r}_a + \\mathbf{J}_{\\mathbf{t}}^T \\frac{\\partial \\mathbf{r}_a}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = \\left( \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}}^T \\nabla^2 \\mathbf{t} \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}} + \\underbrace{\\nabla \\mathbf{t} \\overbrace{\\frac{\\partial^2 \\mathcal{W}}{\\partial^2 \\mathbf{p}}}^{\\mathbf{0}}}_{\\mathbf{0}} \\right) \\mathbf{r}_a +\n\t\t\\\\\n\t\t& \\qquad \\, \\mathbf{J}_{\\mathbf{t}}^T \\mathbf{J}_{\\mathbf{t}}\n\t\t\\\\\n\t\t& = \\left( \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}}^T \\nabla^2 \\mathbf{t} \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}} \\right) \\mathbf{r}_a + \\mathbf{J}_{\\mathbf{t}}^T \\mathbf{J}_{\\mathbf{t}}\n    \\label{eq:asymmetric_hessian_term3}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{a}}{\\partial^{2}\\Delta\\mathbf{p}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>a</mi></msub></mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial\\mathbf{J}_{\\mathbf{t}}^{T}\\mathbf{r}_{a}}{%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial\\mathbf{J}_{\\mathbf{t}}^{T}}{\\partial\\Delta\\mathbf%&#10;{p}}\\mathbf{r}_{a}+\\mathbf{J}_{\\mathbf{t}}^{T}\\frac{\\partial\\mathbf{r}_{a}}{%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi><mi>T</mi></msubsup></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi><mi>T</mi></msubsup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{p}}^{T}%&#10;\\nabla^{2}\\mathbf{t}\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{p}}+%&#10;\\underbrace{\\nabla\\mathbf{t}\\overbrace{\\frac{\\partial^{2}\\mathcal{W}}{\\partial%&#10;^{2}\\mathbf{p}}}^{\\mathbf{0}}}_{\\mathbf{0}}\\right)\\mathbf{r}_{a}+\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><mrow><msup><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mi>T</mi></msup><mo>\u2062</mo><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi>\ud835\udc2d</mi><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>+</mo><munder><munder accentunder=\"true\"><mrow><mo movablelimits=\"false\">\u2207</mo><mo movablelimits=\"false\">\u2061</mo><mrow><mi>\ud835\udc2d</mi><mo movablelimits=\"false\">\u2062</mo><mover><mover accent=\"true\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo movablelimits=\"false\">\u2202</mo><mn>2</mn></msup><mo movablelimits=\"false\">\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><msup><mo movablelimits=\"false\">\u2202</mo><mn>2</mn></msup><mo movablelimits=\"false\">\u2061</mo><mi>\ud835\udc29</mi></mrow></mfrac></mstyle><mo movablelimits=\"false\">\u23de</mo></mover><mn/></mover></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mn/></munder></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad\\,\\mathbf{J}_{\\mathbf{t}}^{T}\\mathbf{J}_{\\mathbf{t}}\" display=\"inline\"><mrow><msubsup><mpadded lspace=\"21.7pt\" width=\"+21.7pt\"><mi>\ud835\udc09</mi></mpadded><mi>\ud835\udc2d</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4Xd.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{p}}^{T}%&#10;\\nabla^{2}\\mathbf{t}\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{p}}\\right%&#10;)\\mathbf{r}_{a}+\\mathbf{J}_{\\mathbf{t}}^{T}\\mathbf{J}_{\\mathbf{t}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><msup><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mi>T</mi></msup><mo>\u2062</mo><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi>\ud835\udc2d</mi><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>a</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc2d</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 74687, "prevtext": "\n\n\n\\subsection{Bidirectional}\n\\label{sec:app12}\n\nThe individual terms forming the Hessian matrix of the \\emph{SSD Bidirectional Newton} algorithm defined by Equation \\ref{eq:bidirectional_newton_hessian} are defined as follows:\n\n", "index": 9, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial^2 \\Delta \\mathbf{c}} & = \\frac{\\partial -\\mathbf{A}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{c}}\n\t\t\\\\\n\t\t& = -\\mathbf{A}^T \\frac{\\partial \\mathbf{r}_b}{\\partial \\Delta \\mathbf{c}}\n\t\t\\\\\n\t\t& = \\underbrace{\\mathbf{A}^T \\mathbf{A}}_{\\mathbf{I}}\n    \\label{eq:bidirectional_hessian_term1}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial^{2}\\Delta\\mathbf{c}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}\\mathbf{r}_{b}}{\\partial\\Delta%&#10;\\mathbf{c}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{A}^{T}\\frac{\\partial\\mathbf{r}_{b}}{\\partial\\Delta%&#10;\\mathbf{c}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underbrace{\\mathbf{A}^{T}\\mathbf{A}}_{\\mathbf{I}}\" display=\"inline\"><mrow><mi/><mo>=</mo><munder><munder accentunder=\"true\"><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo movablelimits=\"false\">\u2062</mo><mi>\ud835\udc00</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mi>\ud835\udc08</mi></munder></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 75082, "prevtext": "\n\n", "index": 11, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial \\Delta \\mathbf{c} \\partial \\Delta \\mathbf{p}} & = \\frac{\\partial -\\mathbf{A}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = -\\mathbf{A}^T \\frac{\\partial \\mathbf{r}_b}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = -\\mathbf{A}^T \\mathbf{J}_{\\mathbf{i}}\n    \\label{eq:bidirectional_hessian_term2}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial\\Delta\\mathbf{c}%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}\\mathbf{r}_{b}}{\\partial\\Delta%&#10;\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{A}^{T}\\frac{\\partial\\mathbf{r}_{b}}{\\partial\\Delta%&#10;\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{A}^{T}\\mathbf{J}_{\\mathbf{i}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 75490, "prevtext": "\n\n", "index": 13, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial \\Delta \\mathbf{c} \\partial \\Delta \\mathbf{q}} & =  \\frac{\\partial -\\mathbf{A}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{q}}\n\t\t\\\\\n\t\t&= \\frac{\\partial -\\mathbf{A}^T}{\\partial \\Delta \\mathbf{q}} \\mathbf{r}_b - \\mathbf{A}^T \\frac{\\partial \\mathbf{r}_b}{\\partial \\Delta \\mathbf{q}}\n\t\t\\\\\n\t\t& = -\\mathbf{J}_{\\mathbf{A}}^T \\mathbf{r}_b + \\mathbf{A}^T \\mathbf{J}_{\\mathbf{a}}\n    \\label{eq:bidirectional_hessian_term3}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial\\Delta\\mathbf{c}%&#10;\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc1c</mi></mrow></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}\\mathbf{r}_{b}}{\\partial\\Delta%&#10;\\mathbf{q}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{A}^{T}}{\\partial\\Delta\\mathbf{q}}\\mathbf{%&#10;r}_{b}-\\mathbf{A}^{T}\\frac{\\partial\\mathbf{r}_{b}}{\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><msup><mi>\ud835\udc00</mi><mi>T</mi></msup></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{J}_{\\mathbf{A}}^{T}\\mathbf{r}_{b}+\\mathbf{A}^{T}\\mathbf%&#10;{J}_{\\mathbf{a}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc00</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc00</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 76011, "prevtext": "\n\n", "index": 15, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial^2 \\Delta \\mathbf{p}} & =  \\frac{\\partial \\mathbf{J}_{\\mathbf{i}}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = \\frac{\\partial \\mathbf{J}_{\\mathbf{i}}^T}{\\partial \\Delta \\mathbf{p}} \\mathbf{r}_b + \\mathbf{J}_{\\mathbf{i}}^T \\frac{\\partial \\mathbf{r}_b}{\\partial \\Delta \\mathbf{p}}\n\t\t\\\\\n\t\t& = \\left( \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}}^T \\nabla^2 \\mathbf{i}[\\mathbf{p}] \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{p}} \\right) \\mathbf{r}_b + \\mathbf{J}_{\\mathbf{i}}^T \\mathbf{J}_{\\mathbf{i}}\n    \\label{q:bidirectional_hessian_term4}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial^{2}\\Delta\\mathbf{p}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial\\mathbf{J}_{\\mathbf{i}}^{T}\\mathbf{r}_{b}}{%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial\\mathbf{J}_{\\mathbf{i}}^{T}}{\\partial\\Delta\\mathbf%&#10;{p}}\\mathbf{r}_{b}+\\mathbf{J}_{\\mathbf{i}}^{T}\\frac{\\partial\\mathbf{r}_{b}}{%&#10;\\partial\\Delta\\mathbf{p}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{p}}^{T}%&#10;\\nabla^{2}\\mathbf{i}[\\mathbf{p}]\\frac{\\partial\\mathcal{W}}{\\partial\\Delta%&#10;\\mathbf{p}}\\right)\\mathbf{r}_{b}+\\mathbf{J}_{\\mathbf{i}}^{T}\\mathbf{J}_{%&#10;\\mathbf{i}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><msup><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle><mi>T</mi></msup><mo>\u2062</mo><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><mi>\ud835\udc22</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc29</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 76689, "prevtext": "\n\n", "index": 17, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial \\Delta \\mathbf{p} \\partial \\Delta \\mathbf{q}} & =  \\frac{\\partial \\mathbf{J}_{\\mathbf{i}}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{q}}\n\t\t\\\\\n\t\t& = -\\mathbf{J}_{\\mathbf{i}}^T \\mathbf{J}_{\\mathbf{a}}\n    \\label{eq:bidirectional_hessian_term5}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial\\Delta\\mathbf{p}%&#10;\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial\\mathbf{J}_{\\mathbf{i}}^{T}\\mathbf{r}_{b}}{%&#10;\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\mathbf{J}_{\\mathbf{i}}^{T}\\mathbf{J}_{\\mathbf{a}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc22</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00199.tex", "nexttext": "\n\n\n\\section{Iterative solutions of all algorithms}\n\\label{sec:app2}\n\nIn this section we report the iterative solutions of all CGD algorithms studied in this paper. In order to keep the information structured algorithms are grouped by their cost function. Consequently, iterative solutions for all SSD and Project-Out algorithms are stated in Tables \\ref{tab:ssd_solution} and \\ref{tab:po_solution}.\n\n\n\\begin{table*}\n\\centering\n\\begin{tabular}{l|l|l|l}\n\t\\toprule\n\t\\multirow{2}{*}{SSD algorithms}  & \\multicolumn{3}{c}{Iterative solutions}\n\t\\\\\n\t& \\multicolumn{1}{c|}{$\\Delta\\mathbf{p}$} & \\multicolumn{1}{c|}{$\\Delta\\mathbf{q}$} & \\multicolumn{1}{c}{$\\Delta\\mathbf{c}$}\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_For\\_GN\\_Sch \\cite{Amberg2009, Tzimiropoulos2013}} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{i}} = \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_For\\_GN\\_Alt} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\mathbf{H}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T \\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{i}} = \\mathbf{J}_{\\mathbf{i}}^T\\mathbf{J}_{\\mathbf{i}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_For\\_N\\_Sch} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\hat{\\mathbf{H}}_{\\mathbf{i}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{i}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{i} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\mathbf{i}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_For\\_N\\_Alt} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\mathbf{H}_{\\mathbf{i}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{i}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{i} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\mathbf{H}_{\\mathbf{i}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\tSSD\\_For\\_W\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}\n\t$\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Inv\\_GN\\_Sch \\cite{Papandreou2008, Tzimiropoulos2013}} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\hat{\\mathbf{H}}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{a}} = \\mathbf{J}_{\\mathbf{a}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{a}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Inv\\_GN\\_Alt \\cite{Tzimiropoulos2012, Antonakos2014}} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\mathbf{H}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T \\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{a}} = \\mathbf{J}_{\\mathbf{a}}^T\\mathbf{J}_{\\mathbf{a}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Inv\\_N\\_Sch} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\left(\\hat{\\mathbf{H}}_{\\mathbf{a}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{a}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{a} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\mathbf{a}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Inv\\_N\\_Alt} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\left(\\mathbf{H}_{\\mathbf{a}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\bar{\\mathbf{A}}\\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{a}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{i} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\mathbf{H}_{\\mathbf{a}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\tSSD\\_Inv\\_W\n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\hat{\\mathbf{H}}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}\n\t$\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Asy\\_GN\\_Sch} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{t}}^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{t}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{t}} = \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{t}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Asy\\_GN\\_Alt} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\mathbf{H}_{\\mathbf{t}}^{-1} \\mathbf{J}_{\\mathbf{t}}^T \\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{t}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{t}} = \\mathbf{J}_{\\mathbf{t}}^T\\mathbf{J}_{\\mathbf{t}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Asy\\_N\\_Sch} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\hat{\\mathbf{H}}_{\\mathbf{t}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{t}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{t}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{t} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\mathbf{t}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Asy\\_N\\_Alt} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\mathbf{H}_{\\mathbf{t}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\left( \\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} \\right)\n\t$\n\t&\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{t}}\\Delta\\mathbf{p} \\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t$\n\t\\mathbf{H}_{\\mathbf{t}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{t} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\mathbf{H}_{\\mathbf{t}}\n\t$\n\t&\n\t&\n\t\\\\\n\t\\midrule\n\tSSD\\_Asy\\_W\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{t}}^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}\n\t$\n\t\\\\\n\t\\midrule\n\t\\multirow{3}{*}{SSD\\_Bid\\_GN\\_Sch} \n\t& \n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\mathbf{r}_1\n\t$\n\t}\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\check{\\mathbf{H}}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\mathbf{P}\\mathbf{r}\n\t$\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}_2\n\t$\n\t}\n\t\\\\\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\mathbf{r}_1 = \\left(\\mathbf{r} -\\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{q}\\right)\n\t$\n\t}\n\t&\n\t$\n\t\\check{\\mathbf{H}}_{\\mathbf{a}} = \\mathbf{J}_{\\mathbf{a}}^T\\mathbf{P}\\mathbf{J}_{\\mathbf{a}}\n\t$\n\t&\n\t\\multirow{2}{*}{\n\t$\n\t\\mathbf{r}_2 = \\left (\\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{q}\\right)\n\t$\n\t}\n\t\\\\\n\t&\n\t&\n\t$\n\t\\mathbf{P} = \\bar{\\mathbf{A}} - \\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}}\\hat{\\mathbf{H}}_\\mathbf{i}^{-1}\\mathbf{J}^T_{\\mathbf{i}}\\bar{\\mathbf{A}} \n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{SSD\\_Bid\\_GN\\_Alt}\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\mathbf{H}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T \\mathbf{r}_3\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\mathbf{H}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T \\mathbf{r}_4\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}_2\n\t$\n\t\\\\\n\t&\n\t$\n\t\\mathbf{r}_3 = \\left(\\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} - \\mathbf{J}_{\\mathbf{a}}\\Delta\\mathbf{q} \\right)\n\t$\n\t&\n\t$\n\t\\mathbf{r}_4 = \\left(\\mathbf{r} - \\mathbf{A}\\Delta\\mathbf{c} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{3}{*}{SSD\\_Bid\\_N\\_Sch} \n\t& \n\t\\multirow{3}{*}{\n\t$\n\t\\Delta\\mathbf{p} = -\\left( \\hat{\\mathbf{H}}_{\\mathbf{i}}^{\\textrm{N}} \\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\mathbf{r}_1\n\t$\n\t}\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\left( \\check{\\mathbf{H}}_{\\mathbf{a}}^{\\textrm{N}} \\right)^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\mathbf{P}^{\\textrm{N}}\\mathbf{r}\n\t$\n\t&\n\t\\multirow{3}{*}{\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}_2\n\t$\n\t}\n\t\\\\\n\t&\n\t&\n\t$\n\t\\check{\\mathbf{H}}_{\\mathbf{a}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{t} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\mathbf{r} + \\check{\\mathbf{H}}_{\\mathbf{a}}\n\t$\n\t&\n\t\\\\\n\t&\n\t&\n\t$\n\t\\mathbf{P}^{\\textrm{N}} = \\bar{\\mathbf{A}} - \\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}}\\left(\\hat{\\mathbf{H}}_\\mathbf{i}^{\\textrm{N}}\\right)^{-1}\\mathbf{J}^T_{\\mathbf{i}}\\bar{\\mathbf{A}} \n\t$\n\t&\n\t\\\\\n\t\\midrule\n\tSSD\\_Bid\\_N\\_Alt\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\mathbf{H}_{\\mathbf{i}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T \\mathbf{r}_3\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\left(\\mathbf{H}_{\\mathbf{a}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{a}}^T \\mathbf{r}_4\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}_2\n\t$\n\t\\\\\n\t\\midrule\n\tSSD\\_Bid\\_W\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\mathbf{r}\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\check{\\mathbf{H}}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\mathbf{a}}^T\\mathbf{P}\\mathbf{r}\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{c} = \\mathbf{A} \\mathbf{r}\n\t$\n\t\\\\\n\t\\bottomrule\n\\end{tabular}\n\\caption{Iterative solutions of all SSD algorithms studied in this paper.}\n\\label{tab:ssd_solution}\n\\end{table*}\n\n\n\\begin{table*}\n\\centering\n\\begin{tabular}{l|l|l}\n\t\\toprule\n\t\\multirow{2}{*}{Project-Out algorithms} & \\multicolumn{2}{c}{Iterative solutions}\n\t\\\\\n\t& \\multicolumn{1}{c|}{$\\Delta\\mathbf{p}$} & \\multicolumn{1}{c}{$\\Delta\\mathbf{q}$}\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_For\\_GN \\cite{Amberg2009, Tzimiropoulos2013}} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{i}} = \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}} \n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_For\\_N}\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left( \\hat{\\mathbf{H}}_{\\mathbf{i}}^\\textrm{N} \\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{i}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\partial\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{i} \\frac{\\partial \\mathcal{W}}{\\partial\\Delta \\mathbf{p}}\\bar{\\mathbf{A}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\mathbf{i}}\n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_Inv\\_GN \\cite{Matthews2004}}\n\t& \n\t$\n\t\\Delta\\mathbf{p} = \\hat{\\mathbf{H}}_{\\mathbf{a}}^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}} = \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\bar{\\mathbf{a}}}\n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_Inv\\_N}\n\t&\n\t$\n\t\\Delta\\mathbf{p} = \\left( \\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{\\textrm{N}} \\right)^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{\\mathrm{N}} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\bar{\\mathbf{a}} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\bar{\\mathbf{A}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}}\n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_Asy\\_GN} \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{t}}^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{t}} = \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{t}} \n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{2}{*}{PO\\_Asy\\_N}\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left( \\hat{\\mathbf{H}}_{\\mathbf{t}}^\\textrm{N} \\right)^{-1} \\mathbf{J}_{\\mathbf{t}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t\\\\\n\t& \n\t$\n\t\\hat{\\mathbf{H}}_{\\mathbf{t}}^{\\textrm{N}} = \\frac{\\partial \\mathcal{W}}{\\partial\\Delta \\mathbf{p}}^T \\nabla^2\\mathbf{t} \\frac{\\partial \\mathcal{W}}{\\partial\\Delta \\mathbf{p}}\\bar{\\mathbf{A}}\\mathbf{r} + \\hat{\\mathbf{H}}_{\\mathbf{t}}\n\t$\n\t&\n\t\\\\\n\t\\midrule\n\t\\multirow{3}{*}{PO\\_Bid\\_GN\\_Sch} \n\t& \n\t\\multirow{3}{*}{\n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\left( \\mathbf{r} - \\mathbf{J}_{\\bar{\\mathbf{a}}}\\Delta\\mathbf{q} \\right)\n\t$\n\t}\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\mathbf{P}\\mathbf{r}\n\t$\n\t\\\\\n\t& \n\t&\n\t$\n\t\\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}} = \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\mathbf{P}\\mathbf{J}_{\\bar{\\mathbf{a}}}\n\t$\n\t\\\\\n\t& \n\t&\n\t$\n\t\\mathbf{P} = \\bar{\\mathbf{A}} - \\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}}\\hat{\\mathbf{H}}_\\mathbf{i}^{-1}\\mathbf{J}^T_{\\mathbf{i}}\\bar{\\mathbf{A}} \n\t$\n\t\\\\\n\t\\midrule\n\tPO\\_Bid\\_GN\\_Alt \n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\left( \\mathbf{r} - \\mathbf{J}_{\\bar{\\mathbf{a}}}\\Delta\\mathbf{q} \\right)\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\bar{\\mathbf{A}} \\left( \\mathbf{r} + \\mathbf{J}_{\\mathbf{i}}\\Delta\\mathbf{p} \\right)\n\t$\n\t\\\\\n\t\\midrule\n\t\\multirow{3}{*}{PO\\_Bid\\_N\\_Sch} \n\t& \n\t\\multirow{3}{*}{\n\t$\n\t\\Delta\\mathbf{p} = -\\left( \\hat{\\mathbf{H}}_\\mathbf{i}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\left( \\mathbf{r} - \\mathbf{J}_{\\bar{\\mathbf{a}}} \\Delta\\mathbf{q} \\right) \n\t$\n\t}\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\left(\\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{\\textrm{N}}\\right)^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T{\\mathbf{P}}^{\\mathrm{N}}\\mathbf{r}\n\t$\n\t\\\\\n\t& \n\t&\n\t$\n\t\\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^\\textrm{N} = \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}^T \\nabla^2\\bar{\\mathbf{a}} \\frac{\\partial \\mathcal{W}}{\\Delta \\mathbf{p}}\\bar{\\mathbf{A}}\\mathbf{r} + \\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}}\n\t$\n\t\\\\\n\t& \n\t&\n\t$\n\t\\mathbf{P}^{\\mathrm{N}} = \\bar{\\mathbf{A}} - \\bar{\\mathbf{A}}\\mathbf{J}_{\\mathbf{i}}\\left(\\hat{\\mathbf{H}}_\\mathbf{i}^{\\textrm{N}}\\right)^{-1}\\mathbf{J}^T_{\\mathbf{i}}\\bar{\\mathbf{A}} \n\t$\n\t\\\\\n\t\\midrule\n\tPO\\_Bid\\_N\\_Alt\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\left(\\hat{\\mathbf{H}}_{\\mathbf{i}}^{\\mathrm{N}}\\right)^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}} \\left( \\mathbf{r} - \\mathbf{J}_{\\bar{\\mathbf{a}}} \\Delta\\mathbf{q} \\right) \n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\left(\\hat{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{\\mathrm{N}}\\right)^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\bar{\\mathbf{A}} \\left( \\mathbf{r} + \\mathbf{J}_{\\mathbf{i}} \\Delta\\mathbf{p} \\right) \n\t$\n\t\\\\\n\t\\midrule\n\tPO\\_Bid\\_W\n\t& \n\t$\n\t\\Delta\\mathbf{p} = -\\hat{\\mathbf{H}}_{\\mathbf{i}}^{-1} \\mathbf{J}_{\\mathbf{i}}^T\\bar{\\mathbf{A}}\\mathbf{r}\n\t$\n\t&\n\t$\n\t\\Delta\\mathbf{q} = \\check{\\mathbf{H}}_{\\bar{\\mathbf{a}}}^{-1} \\mathbf{J}_{\\bar{\\mathbf{a}}}^T\\mathbf{P}\\mathbf{r}\n\t$\n\t\\\\\n\t\\bottomrule\n\\end{tabular}\n\\caption{Iterative solutions of all Project-Out algorithms studied in this paper.}\n\\label{tab:po_solution}\n\\end{table*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 77041, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation}\n    \\begin{aligned}\n\t\t\\frac{\\partial^2 \\mathcal{D}_b}{\\partial^2 \\Delta \\mathbf{q}} & =  \\frac{\\partial -\\mathbf{J}_{\\mathbf{a}}^T \\mathbf{r}_b}{\\partial \\Delta \\mathbf{q}}\n\t\t\\\\\n\t\t& = \\frac{\\partial -\\mathbf{J}_{\\mathbf{a}}^T}{\\partial \\Delta \\mathbf{q}} \\mathbf{r}_b - \\mathbf{J}_{\\mathbf{a}}^T \\frac{\\partial \\mathbf{r}_b}{\\partial \\Delta \\mathbf{q}}\n\t\t\\\\\n\t\t& = -\\left( \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{q}}^T \\nabla^2 (\\mathbf{a} + \\mathbf{A}\\mathbf{c}) \\frac{\\partial\\mathcal{W}}{\\partial \\Delta \\mathbf{q}} \\right) \\mathbf{r}_b + \\mathbf{J}_{\\mathbf{a}}^T \\mathbf{J}_{\\mathbf{a}}\n    \\label{q:bidirectional_hessian_term6}\n    \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial^{2}\\mathcal{D}_{b}}{\\partial^{2}\\Delta\\mathbf{q}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>b</mi></msub></mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{J}_{\\mathbf{a}}^{T}\\mathbf{r}_{b}}{%&#10;\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial-\\mathbf{J}_{\\mathbf{a}}^{T}}{\\partial\\Delta%&#10;\\mathbf{q}}\\mathbf{r}_{b}-\\mathbf{J}_{\\mathbf{a}}^{T}\\frac{\\partial\\mathbf{r}_%&#10;{b}}{\\partial\\Delta\\mathbf{q}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>-</mo><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi><mi>T</mi></msubsup></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mo>-</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi><mi>T</mi></msubsup><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\left(\\frac{\\partial\\mathcal{W}}{\\partial\\Delta\\mathbf{q}}^{T}%&#10;\\nabla^{2}(\\mathbf{a}+\\mathbf{A}\\mathbf{c})\\frac{\\partial\\mathcal{W}}{\\partial%&#10;\\Delta\\mathbf{q}}\\right)\\mathbf{r}_{b}+\\mathbf{J}_{\\mathbf{a}}^{T}\\mathbf{J}_{%&#10;\\mathbf{a}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mrow><mo>(</mo><mrow><msup><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle><mi>T</mi></msup><mo>\u2062</mo><mrow><mrow><msup><mo>\u2207</mo><mn>2</mn></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc1a</mi><mo>+</mo><mi>\ud835\udc00\ud835\udc1c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc2a</mi></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mi>b</mi></msub></mrow></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc09</mi><mi>\ud835\udc1a</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}]