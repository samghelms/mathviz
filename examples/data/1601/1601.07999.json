[{"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\gamma=(\\gamma_{1}(X),\\ldots,\\gamma_{p}(X))$ is a random vector\nin $\\mathbb{R}^{p}$, and the number $p$ of basis functions is assumed\nto be fixed and known. The basis expansion of each observed curve\n$x_{i}(t)=\\sum_{j=1}^{p}\\gamma_{ij}\\psi_{j}(t)$ can be estimated by\nan interpolation procedure [see \\citet{Esc2005}, e.g.] if the\ncurves are observed without noise or by least squares smoothing if they\nare observed with error:\n\n", "itemtype": "equation", "pos": 17809, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\\title{The discriminative functional mixture model for a~comparative\nanalysis of bike sharing systems}\n\\runtitle{The DFM model for a comparative analysis of BSS}\n\n\\begin{aug}\n\n\n\n\\author[A]{\\fnms{Charles}~\\snm{Bouveyron}\\corref{}\\ead[label=e1]{charles.bouveyron@parisdescartes.fr}},\n\\author[B]{\\fnms{Etienne}~\\snm{C\\^ome}\\ead[label=e2]{etienne.come@ifsttar.fr}}\n\\and\n\\author[C]{\\fnms{Julien}~\\snm{Jacques}\\ead[label=e3]{julien.jacques@univ-lyon2.fr}}\n\n\\runauthor{C. Bouveyron, E. C\\^ome and J. Jacques}\n\n\\affiliation{Universit\\'e Paris Descartes,\nIFSTTAR and\nUniversit\\'e Lumi\\'ere Lyon 2}\n\n\n\n\n\n\n\n\n\n\n\n\n\\address[A]{C. Bouveyron\\\\\nLaboratoire MAP5\\\\\n\nUniversit\\'e Paris Descartes\\\\\n45 rue des Saints P\\`{e}res\\\\\n75006 Paris\\\\\nFrance\\\\\n\\printead{e1}}\n\\address[B]{E. C\\^ome\\\\\nIFSTTAR, Grettia\\\\\n14-20 Boulevard Newton\\\\\nCit\\'{e} Descartes, Champs sur Marne\\\\\n77447 Marne la Vall\\'{e}e Cedex 2\\\\\nFrance\\\\\n\n\n\n\\printead{e2}}\n\\address[C]{J. Jacques\\\\\nLaboratoire ERIC\\\\\nUniversit\\'e Lumi\\'ere Lyon 2\\\\\nBureau K067\\\\\n5 av. Mend\\`{e}s France\\\\\n69676 BRON Cedex\\\\\nFrance\\\\\n\\printead{e3}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{aug}\n\n\n\n\\received{\\smonth{7} \\syear{2014}}\n\n\n\\revised{\\smonth{4} \\syear{2015}}\n\n\n\n\n\\begin{abstract}\nBike sharing systems (BSSs) have become a means of sustainable\nintermodal transport and are now proposed in many cities worldwide.\nMost BSSs also provide open access to their data, particularly to\nreal-time status reports on their bike stations. The analysis of the\nmass of data generated by such systems is of particular interest to BSS\nproviders to update system structures and policies. This work was\nmotivated by interest in analyzing and comparing several European BSSs\nto identify common operating patterns in BSSs and to propose practical\nsolutions to avoid potential issues. Our approach relies on the\nidentification of common patterns between and within systems. To this\nend, a model-based clustering method, called FunFEM, for time series\n(or more generally functional data) is developed. It is based on a\nfunctional mixture model that allows the clustering of the data in a\ndiscriminative functional subspace. This model presents the advantage\nin this context to be parsimonious and to allow the visualization of\nthe clustered systems. Numerical experiments confirm the good behavior\nof FunFEM, particularly compared to state-of-the-art methods. The\napplication of FunFEM to BSS data from JCDecaux and the Transport for\nLondon Initiative allows us to identify 10 general patterns, including\npathological ones, and to propose practical improvement strategies\nbased on the system comparison. The visualization of the clustered data\nwithin the discriminative subspace turns out to be particularly\ninformative regarding the system efficiency. The proposed methodology\nis implemented in a package for the R software, named \\texttt{funFEM},\nwhich is available on the CRAN. The package also provides a subset of\nthe data analyzed in this work.\n\\end{abstract}\n\n\n\n\n\\begin{keyword}\n\\kwd{Model-based clustering}\n\\kwd{functional data}\n\\kwd{dimension reduction}\n\\kwd{open data}\n\\kwd{bike sharing systems}\n\\end{keyword}\n\\end{frontmatter}\n\n\n\\section{Introduction}\\label{sec1}\nThis work was motivated by the will to analyze and compare bike sharing\nsystems (BSSs) to identify their common strengths and weaknesses. This\ntype of study is possible because most BSS operators, in dozens of\ncities worldwide, provide open access to real-time status reports on\ntheir bike stations (e.g., the number of available bikes, the number of\nfree bike stands). The implementation of bike sharing systems is one of\nthe urban mobility services proposed in cities across the world as an\nadditional means of sustainable intermodal transport. Several studies\n[\\citet{Froehlich2009,Borgnat11,Vogel2011a,lathia12}] have shown the\nusefulness of analyzing\\vadjust{\\goodbreak} the data collected by BSS operators and city\nauthorities. A~statistical analysis of these data helps in the\ndevelopment of new and innovative approaches for a better understanding\nof both urban mobility and BSS use. The design of BSSs, the adjustment\nof pricing policies and the improvement of system services (e.g.,\nredistribution of bikes over stations) can all benefit from this\ntype of analysis [\\citet{Dellolio2011,Lin2011}].\n\nHowever, the amount of data collected on such systems is often very\nlarge. It is therefore difficult to acquire knowledge using it without\nthe help of automatic algorithms that extract mobility patterns and\ngive a synthetic view of the information. This task is usually achieved\nin the literature using clustering approaches. In almost all clustering\nstudies conducted until now, bicycle sharing stations are grouped\naccording to their usage profiles, thus highlighting the relationships\nbetween time of day, location and usage. In this way, the global\nbehavior of each station can be efficiently summarized using a few\nclusters. These data can be used afterward to analyze the effect of\nchanging pricing policies or opening new sets of stations [\\citet\n{lathia12}]. Clustering results can also be used to study the cause of\nnetwork imbalance [\\citet{Vogel2011a,Vogel2011b,come2014}] and serve\nas a first step toward providing automatic reallocation strategies. In\nthe same way, the clustered results can be used to compare the level of\nservices reached by the systems of several cities through the\ninspection of the proportions of stations that belong to each cluster\nin the different cities.\n\nFrom a methodological point of view, the first attempt in this line of\nwork was made by \\citet{Froehlichmeasuring}, who analyzed a data set\nfrom the Barcelona Bicing system. The data correspond to station\noccupancy statistics in the form of free slots, available bikes over\nseveral time frames and other station activity statistics derived from\nstation occupancy data collected every 5 minutes. The clustering is\nperformed using a Gaussian mixture model based on features such as the\naverage number of available bikes at different periods of the day. It\nshould be noted that such techniques do not really take advantage of\nthe temporal dynamic of data. In \\citet{Froehlich2009}, two types of\nclustering are compared, both of which are performed by hierarchical\naggregation. The first one uses activity statistics derived from the\nevolution of station occupancy, whereas the second directly uses the\nnumber of available bicycles throughout the day. Other studies, such as\n\\citet{lathia12}, use similar clustering techniques and data. As in\n\\citet{Froehlich2009}, each station is described by a time series\nvector that corresponds to the normalized available bicycle value of\nthe station throughout the day. Each element of the feature vector is\ntherefore equal to the number of available bicycles divided by the\nstation size. These time series are then smoothed using a moving\naverage and clustered using a hierarchical agglomerative algorithm [see\npage 552 of \\citet{Duda01}], with a cosine distance. Another work that\nuses the same type of data was proposed by \\citet\n{Vogel2011a,Vogel2011b}; it uses feature vectors to describe the\nstations that come from normalizing arrival and departure counts per\\vadjust{\\goodbreak}\nhour and also handles weekdays and weekends separately. Classical\nclustering algorithms, that is, $k$-means, Gaussian mixture\nmodels and sequential information bottleneck (sIB), are then compared.\nFinally, \\citet{come2014} recently proposed an original approach\nconsidering a generative model based on Poisson mixtures to cluster\nstations with respect to hourly usage profiles build from trip data.\nThe results obtained for the V\\'elib' system (Paris) were then analyzed\nwith respect to the city geography and sociology.\n\nHowever, all of these works share two limitative characteristics: They\nare limited to one BSS (one city), and they do not explicitly model the\nfunctional nature of the data. Indeed, the observed time series are\nclustered in those works using either geometric methods based on\ndistances between time series or by creating features that summarize\nthe activity in the given periods of the day (and thus omitting the\ntemporal dynamics of the data). In this work, we aim to go beyond the\nanalyses made in those works by comparing several European BSSs using a\nclustering approach designed for time series data. To this end, we\nintroduce a novel model-based clustering method devoted to time series\n(and, more generally, functional data) that is able to take into\naccount the nature of the BSS data. The proposed methodology, called\nFunFEM, is based on the discriminative functional mixture (DFM) model,\nwhich models the data into a single discriminative functional subspace.\nThis subspace subsequently allows an insightful visualization of the\nclustered data and eases the comparison of systems regarding the\nidentified patterns. A family of 12 models is also proposed by relaxing\nor constraining the main DFM model, allowing it to handle a wide range\nof situations. The FunFEM algorithm is proposed for the inference of\nthe DFM models, and model selection can be performed either by BIC or\nthe ``slope heuristic.'' In addition, the selection of the most\ndiscriminative basis functions can be made afterward by introducing\nsparsity through a $\\ell_1$-type penalization. The comparison of 8\nEuropean BSS using FunFEM allows us to identify pathological and\nhealthy patterns in the system dynamic and to propose practical\nimprovement strategies based on the most efficient systems.\n\nThe paper is organized as follows. Section~\\ref{sec2} presents the BSS data used\nto analyze and compare several European bike sharing systems. Section~\\ref{sec3}\nintroduces the DFM model, its model family and the FunFEM algorithm.\nThe model choice and selection of the discriminative functions are also\ndiscussed in Section~\\ref{sec3}. Numerical experiments on simulated and\nbenchmark data sets are then presented in Section~\\ref{sec4} to validate the\nproposed approach. Section~\\ref{sec5} presents the analyses and comparisons of 8\nbike sharing systems using the FunFEM algorithm. Based on the\ncomparison results, recommendations to BSS providers and city planners\nare made. Finally, Section~\\ref{sec6} provides concluding remarks.\n\n\n\\begin{table}\n\\tablewidth=250pt\n\\caption{Summary statistics for the eight bike\nsharing systems involved in the study}\n\\label{tab:bsslist}\n\\begin{tabular*}{250pt}{@{\\extracolsep{\\fill}}lcc@{}}\n\n\\hline\n\\textbf{City} & \\textbf{Stations} & \\textbf{Bikes}\\\\\n\\hline\nParis & 1230 & 18,000\\\\\nLondon & \\phantom{0}740 & \\phantom{0.}9500\\\\\nLyon & \\phantom{0}345 & \\phantom{0.}3200 \\\\\nBruxelles & \\phantom{0}330 & \\phantom{0.}3800\\\\\nValencia & \\phantom{0}280 & \\phantom{0.}2400 \\\\\nSevilla & \\phantom{0}260 & \\phantom{0.}2150 \\\\\nMarseille & \\phantom{0}120 & \\phantom{00.}650 \\\\\nNantes & \\phantom{0}102 & \\phantom{00.}880 \\\\\n\\hline\n\\end{tabular*}\n\n\\end{table}\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f01.eps}\n\n\\caption{Map of the eight European bike sharing\nsystems involved in the study. The dot size denotes the system size.}\\label{fig:Map}\n\\end{figure}\n\n\n\n\n\n\n\n\\section{The BSS data}\\label{sec2}\n\n\nIn this work we want to analyze station occupancy data collected over\nthe course of one month on eight bike sharing systems in Europe. The\ndata were collected over 5 weeks, between February, 24 and March, 30,\n2014. Table~\\ref{tab:bsslist} lists the BSSs included in this study\nand some summary statistics on the systems. Figure~\\ref{fig:Map}\nvisualizes the locations of the studied systems. The cities were chosen\nto cover different cases in terms of the geographic positions of the\ncity (south/north of Europe) and to cover a range of system sizes,\nfrom small-scale systems, such as Nantes, to much larger systems, such\nas Paris.\n\nThe station status information, in terms of available bikes and docks,\nwere downloaded every hour during the study period for the seven\nsystems from the open-data APIs provided by the JCDecaux\ncompany\\footnote{The real-time data are available at\n\\url{https://developer.jcdecaux.com/} (with an api key).} and by the\nTransport for London initiative.\\footnote{The real-time data are\navailable at \\url{https://www.tfl.gov.uk/info-for/open-data-users/} (with an\napi key).} To accommodate the varying stations sizes (in terms of the\nnumber of docking points), we normalized the number of available bikes\nby the station size and obtained a loading profile for each station.\nThe final data set contains 3230 loading profiles, one per station,\nsampled at 1448 time points. Notice that the sampling is not perfectly\nregular; there is an hour, on average, between the two sample points.\n\n\n\n\nThe daily and weekly habits of inhabitants introduce a periodic\nbehavior in the BSS station loading profiles, with a natural period of\none week. It is then natural to use a Fourier basis to smooth the\ncurves, with basis functions corresponding to sine and cosine functions\nof periods equal to fractions of this natural period of the data. Using\nsuch a procedure, the profiles of the 3230 stations were projected on a\nbasis of 41 Fourier functions (see Section~\\ref{sec3} for details); the smoothed\ncurves obtained for 6 different stations are depicted in Figure~\\ref{fig:smoothing},\ntogether with the curve samples. A typical periodic\nbehavior is clearly visible in this figure for some stations. Some\nother stations exhibit, however, a less clear pattern, such as curves\n2, 4 and 5. Our study aims, therefore, to identify the different\npatterns hidden in the data using functional clustering and to use them\nto compare the eight studied systems.\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f02.eps}\n\n\\caption{Some examples of smoothed station\nprofiles, with the corresponding observations. One month of\nobservations is depicted here using a period of one week.} \\label{fig:smoothing}\n\\end{figure}\n\n\n\\section{The discriminative functional mixture model}\\label{sec3}\n\nFrom a theoretical point of view, the aim of this work is to cluster a\nset of observed curves $\\{x_{1},\\ldots,x_{n}\\}$ (the loading function of\nthe bike stations) into $K$ homogenous groups (or clusters), allowing\nfor the analysis of the studied process. After a short review of\nrelated works in functional data clustering, this section introduces a\nlatent functional model that adapts the model of \\citet\n{Bouveyron12FEM} proposed in the multivariate case to functional data.\nAn original inference algorithm for the functional model is then\nproposed, subsequently allowing for the clustering of the curves. The\nmodel choice and variable selection are also discussed.\n\n\n\\subsection{Related work in functional clustering}\\label{sec3.1}\n\nThis work is rooted in the recent advances in functional data analysis\nthat have contributed to the development of efficient clustering\ntechniques specific to functional data. One of the earlier works in\nthat domain was by \\citet{Jam2003}, who defined an approach that is\nparticularly effective for sparsely sampled functional data. This\nmethod, called fclust, considers that the basis expansion coefficients\nof curves into a spline basis are distributed according to a mixture of\nGaussians with cluster-specific means and common variances. The use of\na spline basis is convenient when the curves are regular but are not\nappropriate for peak-like data, for instance, the data encountered in\nmass spectrometry. For this reason, \\citet{Gia2012} recently proposed a\nGaussian model on a wavelet decomposition of curves. This approach\nallows for addressing a wider range of functional shapes than splines.\nAn interesting approach has also been considered by \\citet{Sam2011},\nwho assume that curves arise from a mixture of regressions based on\npolynomial functions, with possible regime changes at each instant of\nobservation. Let us also mention the work of \\citet{Fru2008}, who have\nbuilt a specific clustering algorithm based on parametric time series\nmodels. \\citet{Bou2011} extended the high-dimensional data clustering\n(HDDC) algorithm [\\citet{Bouveyron07b}] to the functional case. The\nresulting model assumes a parsimonious cluster-specific Gaussian\ndistribution for the basis expansion coefficients. More recently, \\citet\n{Jac2013} proposed a model-based clustering built on the approximation\nof the notion of density for functional variables, extended to\nmultivariate functional data in \\citet{Jac2014}. These models assume\nthat the functional principal component scores of curves have a\nGaussian distribution whose parameters are cluster-specific. Bayesian\napproaches have also been proposed: For example, \\citet{Hea2006}\nconsider that the basis expansion coefficients are distributed as a\nmixture of Gaussians whose variances are modeled by an Inverse-Gamma\ndistribution. Further, \\citet{Ray2006} propose a nonparametric Bayes\nwavelet model for curve clustering based on a mixture of Dirichlet processes.\n\n\n\\subsection{Transformation of the observed curves}\\label{sec3.2}\nLet us first assume that the observed curves $\\{x_{1},\\ldots,x_{n}\\}$ are\nindependent realizations of a $L_{2}$-continuous stochastic process\n$X=\\{X(t)\\}_{t\\in[0,T]}$ for which the sample paths, that\nis, the observed curves, belong to $L_{2}[0,T]$. In practice, the\nfunctional expressions of the observed curves are not known, and we\nhave access only to the discrete observations $x_{ij}=x_{i}(t_{is})$ at\na finite set of ordered times $\\{t_{is}:s=1,\\ldots,m_{i}\\}$. It is\ntherefore necessary to first reconstruct the functional form of the\ndata from their discrete observations. A common way to do this is to\nassume that the curves belong to a finite dimensional space spanned by\na basis of functions [see, e.g., \\citet{Ram2005}]. Let us\ntherefore consider such a basis $\\{\\psi_{1},\\ldots,\\psi_{p}\\}$ and\nassume that the stochastic process $X$ admits the following basis expansion:\n\n\n\n", "index": 1, "text": "\\begin{equation}\nX(t)=\\sum_{j=1}^{p}\\gamma_{j}(X)\n\\psi_{j}(t),\\label{eq:X}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"X(t)=\\sum_{j=1}^{p}\\gamma_{j}(X)\\psi_{j}(t),\" display=\"block\"><mrow><mrow><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>\u03b3</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nThe latter option is used in the present work. In this case, the basis\ncoefficients of each sample path $x_{i}$ are approximated by\n\n", "itemtype": "equation", "pos": 18334, "prevtext": "\n\nwhere $\\gamma=(\\gamma_{1}(X),\\ldots,\\gamma_{p}(X))$ is a random vector\nin $\\mathbb{R}^{p}$, and the number $p$ of basis functions is assumed\nto be fixed and known. The basis expansion of each observed curve\n$x_{i}(t)=\\sum_{j=1}^{p}\\gamma_{ij}\\psi_{j}(t)$ can be estimated by\nan interpolation procedure [see \\citet{Esc2005}, e.g.] if the\ncurves are observed without noise or by least squares smoothing if they\nare observed with error:\n\n", "index": 3, "text": "\n\\[\nx_{i}^{\\mathrm{obs}}(t_{is})=x_{i}(t_{is})+\n\\varepsilon_{is},\\qquad s=1,\\ldots,m_{i}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"x_{i}^{\\mathrm{obs}}(t_{is})=x_{i}(t_{is})+\\varepsilon_{is},\\qquad s=1,\\ldots,%&#10;m_{i}.\" display=\"block\"><mrow><mrow><mrow><mrow><msubsup><mi>x</mi><mi>i</mi><mi>obs</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>\u03b5</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>s</mi></mrow></msub></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mi>s</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwith $\\Theta_{i}=(\\psi_{j}(t_{is}))_{1\\leq j\\leq n,1\\leq s\\leq\nm_{i}}$ and $X_{i}^{\\mathrm{obs}}=(x_{i}^{\\mathrm{obs}}(t_{i_{1}}),\\ldots,x_{i}^{\\mathrm{obs}}(t_{i_{m_{i}}}))^{\\prime}$.\n\n\n\\subsection{The model}\\label{sec3.3}\n\nThe goal is to cluster the observed curves $\\{x_{1},\\ldots,x_{n}\\}$\ninto $K$ homogeneous groups. Let us assume that there exists an unobserved\nrandom variable $Z=(Z_{1},\\ldots,Z_{K})\\in\\{0,1\\}^{K}$ indicating\nthe group membership of $X$: $Z_{k}$ is equal to 1 if $X$ belongs\nto the $k$th group and 0 otherwise. The clustering task aims therefore\nto predict the value $z_{i}=(z_{i1},\\ldots,z_{iK})$ of $Z$ for each\nobserved curve $x_{i}$.\n\nLet $F[0,T]$ be a latent subspace of $L_{2}[0,T]$ assumed to be\nthe most discriminative subspace for the $K$ groups spanned by a\nbasis of $d$ basis functions $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$ in $L_{2}[0,T]$,\nwith $d<K$ and $d<p$. The assumption $d<K$ is motivated by the fact\nthat a subspace of $d=K-1$ dimensions is sufficient to discriminate $K$\ngroups [\\citet{Fisher36,Fukunaga90}]. The basis $\\{\\varphi_{j}\\}\n_{j=1,\\ldots,d}$ is obtained\nfrom $\\{\\psi_{j}\\}_{j=1,\\ldots,p}$ through a linear transformation\n$\\varphi_{j}=\\sum_{\\ell=1}^{p}u_{j\\ell}\\psi_{\\ell}$\nsuch that the $p\\times d$ matrix $U=(u_{j\\ell})$ is orthogonal.\nLet $\\{\\lambda_{1},\\ldots,\\lambda_{n}\\}$ be the latent expansion coefficients\nof the curves $\\{x_{1},\\ldots,x_{n}\\}$ on the basis $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$.\nThese coefficients are assumed to be independent realizations of a\nlatent random vector $\\Lambda\\in\\mathbb{R}^{d}$. The relationship\nbetween the bases $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$ and $\\{\\psi_{j}\\}_{j=1,\\ldots,p}$\nsuggests that the random vectors $\\Gamma$ and $\\Lambda$ are linked\nthrough the following linear transformation:\n\n\n\n", "itemtype": "equation", "pos": 18560, "prevtext": "\n\nThe latter option is used in the present work. In this case, the basis\ncoefficients of each sample path $x_{i}$ are approximated by\n\n", "index": 5, "text": "\n\\[\n\\widehat{\\gamma}_{i}= \\bigl(\\Theta_{i}^{\\prime}\n\\Theta_{i} \\bigr)^{-1}\\Theta_{i}^{\\prime}X_{i}^{\\mathrm{obs}},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\widehat{\\gamma}_{i}=\\bigl{(}\\Theta_{i}^{\\prime}\\Theta_{i}\\bigr{)}^{-1}\\Theta_%&#10;{i}^{\\prime}X_{i}^{\\mathrm{obs}},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mi>i</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u0398</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msubsup><mi>X</mi><mi>i</mi><mi>obs</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\varepsilon\\in\\mathbb{R}^{p}$ is an independent and random\nnoise term.\n\nLet us now make distributional assumptions on the random vectors\n$\\Lambda$ and $\\varepsilon$. First, conditionally on $Z$, $\\Lambda$\nis assumed to be\ndistributed according to a multivariate Gaussian density:\n\n\n\n", "itemtype": "equation", "pos": 20464, "prevtext": "\n\nwith $\\Theta_{i}=(\\psi_{j}(t_{is}))_{1\\leq j\\leq n,1\\leq s\\leq\nm_{i}}$ and $X_{i}^{\\mathrm{obs}}=(x_{i}^{\\mathrm{obs}}(t_{i_{1}}),\\ldots,x_{i}^{\\mathrm{obs}}(t_{i_{m_{i}}}))^{\\prime}$.\n\n\n\\subsection{The model}\\label{sec3.3}\n\nThe goal is to cluster the observed curves $\\{x_{1},\\ldots,x_{n}\\}$\ninto $K$ homogeneous groups. Let us assume that there exists an unobserved\nrandom variable $Z=(Z_{1},\\ldots,Z_{K})\\in\\{0,1\\}^{K}$ indicating\nthe group membership of $X$: $Z_{k}$ is equal to 1 if $X$ belongs\nto the $k$th group and 0 otherwise. The clustering task aims therefore\nto predict the value $z_{i}=(z_{i1},\\ldots,z_{iK})$ of $Z$ for each\nobserved curve $x_{i}$.\n\nLet $F[0,T]$ be a latent subspace of $L_{2}[0,T]$ assumed to be\nthe most discriminative subspace for the $K$ groups spanned by a\nbasis of $d$ basis functions $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$ in $L_{2}[0,T]$,\nwith $d<K$ and $d<p$. The assumption $d<K$ is motivated by the fact\nthat a subspace of $d=K-1$ dimensions is sufficient to discriminate $K$\ngroups [\\citet{Fisher36,Fukunaga90}]. The basis $\\{\\varphi_{j}\\}\n_{j=1,\\ldots,d}$ is obtained\nfrom $\\{\\psi_{j}\\}_{j=1,\\ldots,p}$ through a linear transformation\n$\\varphi_{j}=\\sum_{\\ell=1}^{p}u_{j\\ell}\\psi_{\\ell}$\nsuch that the $p\\times d$ matrix $U=(u_{j\\ell})$ is orthogonal.\nLet $\\{\\lambda_{1},\\ldots,\\lambda_{n}\\}$ be the latent expansion coefficients\nof the curves $\\{x_{1},\\ldots,x_{n}\\}$ on the basis $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$.\nThese coefficients are assumed to be independent realizations of a\nlatent random vector $\\Lambda\\in\\mathbb{R}^{d}$. The relationship\nbetween the bases $\\{\\varphi_{j}\\}_{j=1,\\ldots,d}$ and $\\{\\psi_{j}\\}_{j=1,\\ldots,p}$\nsuggests that the random vectors $\\Gamma$ and $\\Lambda$ are linked\nthrough the following linear transformation:\n\n\n\n", "index": 7, "text": "\\begin{equation}\n\\Gamma=U\\Lambda+\\varepsilon,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\Gamma=U\\Lambda+\\varepsilon,\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>=</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi></mrow><mo>+</mo><mi>\u03b5</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\mu_{k}$ and $\\Sigma_{k}$ are, respectively, the mean and the\ncovariance\nmatrix of the $k$th group. Second, $\\varepsilon$ is also assumed to be\ndistributed according to a multivariate Gaussian density:\n\n\n\n", "itemtype": "equation", "pos": 20815, "prevtext": "\n\nwhere $\\varepsilon\\in\\mathbb{R}^{p}$ is an independent and random\nnoise term.\n\nLet us now make distributional assumptions on the random vectors\n$\\Lambda$ and $\\varepsilon$. First, conditionally on $Z$, $\\Lambda$\nis assumed to be\ndistributed according to a multivariate Gaussian density:\n\n\n\n", "index": 9, "text": "\\begin{equation}\n\\Lambda_{|Z=k}\\sim\\mathcal{N}(\\mu_{k},\\Sigma_{k}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\Lambda_{|Z=k}\\sim\\mathcal{N}(\\mu_{k},\\Sigma_{k}),\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u039b</mi><mrow><mo stretchy=\"false\">|</mo><mi>Z</mi><mo>=</mo><mi>k</mi></mrow></msub><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>k</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nWith these distributional assumptions, the marginal distribution of\n$\\Gamma$ is a mixture of Gaussians:\n\n\n\n", "itemtype": "equation", "pos": 21110, "prevtext": "\n\nwhere $\\mu_{k}$ and $\\Sigma_{k}$ are, respectively, the mean and the\ncovariance\nmatrix of the $k$th group. Second, $\\varepsilon$ is also assumed to be\ndistributed according to a multivariate Gaussian density:\n\n\n\n", "index": 11, "text": "\\begin{equation}\n\\varepsilon\\sim\\mathcal{N}(0,\\Xi).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\varepsilon\\sim\\mathcal{N}(0,\\Xi).\" display=\"block\"><mrow><mrow><mi>\u03b5</mi><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">\u039e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\phi$ is the standard Gaussian density function, and $\\pi\n_{k}=P(Z=k)$ is the prior probability of the $k$th group.\n\nWe finally assume that the noise covariance matrix $\\Xi$ is such that\n$\\Delta_{k}=\\operatorname{cov}(W^{t}\\Gamma|Z=k)=W^{t}\\Sigma_{k}W$ has the\nfollowing form:\n\n\n\n\n\\begin{eqnarray}\n\\small{\\Delta_{k}=\\left(\\begin{array}{c@{}c}\n\\begin{array}{|ccc|}\n\\hline  &  & \\\\\n & \\Sigma_{k} & \\\\\n &  & \\\\\n\\hline \\end{array} & \\mathbf{0}\\\\\n\\mathbf{0} & \\begin{array}{|ccc|}\n\\hline \\beta &  & 0\\\\\n & \\ddots & \\\\\n0 &  & \\beta\n\\\\\\hline \\end{array}\n\\end{array}\\right)\\begin{array}{cc}\n\\left.\\begin{array}{c}\n\\\\\n\\\\\n\\end{array}\\right\\}  & d\\vspace*{10pt}\\\\\n\\left.\\begin{array}{c}\n\\\\\n\\\\\n\\\\\n\\end{array}\\right\\}  & p-d\n\\end{array}\\label{eqdelta}}\\label{eq:Delta}\n\\end{eqnarray}\n\nwith $W=[U,V]$, where $V$ is the orthogonal complement of $U$. With\nthis notation, and from a practical point of view, one can say that the\nvariance of the actual data of the $k$th group is therefore modeled by\n$\\Sigma_{k}$, whereas the parameter $\\beta$ models the variance\nof the noise outside the functional subspace. This model is referred to\nin the sequel as $\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$, and\nFigure~\\ref{fig:GraphModel} summarizes the modeling.\n\n\n\\begin{figure}\n\n\\includegraphics{861f03.eps}\n\n\\caption{Graphical representation for the model\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$.} \\label{fig:GraphModel}\n\\end{figure}\n\n\n\n\\begin{table}\n\n\\caption{Number of free parameters in covariance matrices\nwhen $d=K-1$ for the DFM models}\n\\label{Tab:models}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lccc@{}}\n\\hline\n\\multicolumn{1}{@{}l}{\\textbf{Model}} & \\multicolumn{1}{c}{$\\bolds{\\Sigma_{k}}$} & \\multicolumn{1}{c}{$\\bolds{\\beta_{k}}$} & \\multicolumn{1}{c@{}}{\\textbf{Nb. of variance parameters}}\n\\\\\n\\hline\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ & Free & Free &\n$(K-1)(p-K/2)+K^{2}(K-1)/2+K$ \\\\\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ & Free & Common &\n$(K-1)(p-K/2)+K^{2}(K-1)/2+1$\\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$ & Common & Free &\n$(K-1)(p-K/2)+K(K-1)/2+K$ \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta]}$ & Common & Common &\n$(K-1)(p-K/2)+K(K-1)/2+1$\\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta_{k}]}$ & Diagonal & Free &\n$(K-1)(p-K/2)+K^{2}$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ & Diagonal & Common &\n$(K-1)(p-K/2)+K(K-1)+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$ & Spherical & Free &\n$(K-1)(K-1)(p-K/2)+2K$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta]}$ & Spherical & Common &\n$(K-1)(p-K/2)+K+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta_{k}]}$ & Diagonal \\& Common & Free &\n$(K-1)(p-K/2)+(K-1)+K$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta]}$ & Diagonal \\& Common & Common &\n$(K-1)(p-K/2)+(K-1)+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta_{k}]}$ & Spherical \\& Common & Free &\n$(K-1)(p-K/2)+K+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta]}$ & Spherical \\& Common & Common &\n$(K-1)(p-K/2)+2$ \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\\subsection{A family of discriminative functional model}\\label{sec3.4}\n\nStarting with the model $\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ and\nfollowing the strategy of \\citet{Fraley99}, several submodels can be\ngenerated by applying constraints on the parameters of the matrix\n$\\Delta_{k}$. For instance, it is first possible to relax the\nconstraint that the noise variance is common across groups. This\ngenerates the model $\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$, which is\nthe more general model of the family. It is also possible to constrain\nthis new model such that the covariance matrices $\\Sigma_{1},\\ldots\n,\\Sigma_{K}$ in the latent space are common across groups. This\nsubmodel will be referred to as $\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$.\nSimilarly, in each group, $\\Sigma_{k}$ can be assumed to be diagonal,\nthat is, $\\Sigma_{k}=\\operatorname{diag}(\\alpha_{k1},\\ldots\n,\\alpha_{kd})$, and this submodel will be referred to as $\\mathrm\n{DFM}_{[\\alpha_{kj}\\beta_{k}]}$. The variance within the latent\nsubspace $F$ can also be assumed to be isotropic for each group, and\nthe associated submodel is $\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$.\nFollowing this strategy, $12$ different DFM models can be enumerated,\nand an overview of them is proposed in Table~\\ref{Tab:models}. The\ntable also provides, for each model, the number of variance parameters\nto estimate as a function of the number $K$ of groups and the number\n$p$ of basis functions.\nOne can note that the models turn out to be particularly parsimonious\nbecause their complexity is a linear function of $p$, whereas most\nmodel-based approaches usually have a complexity that is a quadratic\nfunction of $p$.\n\n\n\n\n\n\\subsection{Model inference: The FunFEM algorithm}\\label{sec3.5}\n\nBecause the group memberships $\\{z_{1},\\ldots,z_{n}\\}$ of the curves are\nunknown, the direct maximization of the likelihood associated with the\nmodel described above is intractable. In such a case, a classical\nsolution for model inference is to use the EM algorithm. Here, however,\nthe use of the EM algorithm is prohibited due to the particular nature\nof the functional subspace $F$. Indeed, maximizing the likelihood over\nthe subspace orientation matrix $U$ is equivalent to maximizing the\nprojected variance, and it yields the functional principal component\nanalysis (fPCA) subspace. Because $F$ is here assumed to be the most\ndiscriminative subspace, $U$ has to be estimated separately, and we\ntherefore propose the algorithm described hereafter and named FunFEM.\nThe FunFEM algorithm alternates, at iteration $q$, over the three\nfollowing steps:\n\n\\textit{The F step}.\nLet us first suppose that at iteration $q$, the posterior probabilities\n$t_{ik}^{(q)}=E[z_{ik}|\\gamma_{i},\\theta^{(q-1)}]$ are known (they\nhave been estimated in the E step of iteration $q-1$). The F step aims\ntherefore to determine, conditionally on the $t_{ik}^{(q)}$, the\norientation matrix $U$ of the discriminative latent subspace $F$ in\nwhich the $K$ clusters are best separated. Following the original idea\nof Fisher (\\citeyear{Fisher36}), the functional subspace $F$ should be such that the\nvariance within the groups should be minimal, whereas the variance\nbetween groups should be maximal.\nLet $\\mathbf{C}$ be the covariance operator of $X$ with kernel\n\n", "itemtype": "equation", "pos": 21284, "prevtext": "\n\nWith these distributional assumptions, the marginal distribution of\n$\\Gamma$ is a mixture of Gaussians:\n\n\n\n", "index": 13, "text": "\\begin{equation}\np(\\gamma)=\\sum_{k=1}^{K}\\pi_{k}\n\\phi\\bigl(\\gamma;U\\mu_{k},U^{t}\\Sigma _{k}U+\\Xi\n\\bigr),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"p(\\gamma)=\\sum_{k=1}^{K}\\pi_{k}\\phi\\bigl{(}\\gamma;U\\mu_{k},U^{t}\\Sigma_{k}U+%&#10;\\Xi\\bigr{)},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\u03b3</mi><mo>;</mo><mrow><mi>U</mi><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>k</mi></msub></mrow><mo>,</mo><mrow><mrow><msup><mi>U</mi><mi>t</mi></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>k</mi></msub><mo>\u2062</mo><mi>U</mi></mrow><mo>+</mo><mi mathvariant=\"normal\">\u039e</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nand $\\mathbf{B}$ be the integral between-cluster covariance operator\nwith kernel\n\n", "itemtype": "equation", "pos": 27528, "prevtext": "\n\nwhere $\\phi$ is the standard Gaussian density function, and $\\pi\n_{k}=P(Z=k)$ is the prior probability of the $k$th group.\n\nWe finally assume that the noise covariance matrix $\\Xi$ is such that\n$\\Delta_{k}=\\operatorname{cov}(W^{t}\\Gamma|Z=k)=W^{t}\\Sigma_{k}W$ has the\nfollowing form:\n\n\n\n\n\\begin{eqnarray}\n\\small{\\Delta_{k}=\\left(\\begin{array}{c@{}c}\n\\begin{array}{|ccc|}\n\\hline  &  & \\\\\n & \\Sigma_{k} & \\\\\n &  & \\\\\n\\hline \\end{array} & \\mathbf{0}\\\\\n\\mathbf{0} & \\begin{array}{|ccc|}\n\\hline \\beta &  & 0\\\\\n & \\ddots & \\\\\n0 &  & \\beta\n\\\\\\hline \\end{array}\n\\end{array}\\right)\\begin{array}{cc}\n\\left.\\begin{array}{c}\n\\\\\n\\\\\n\\end{array}\\right\\}  & d\\vspace*{10pt}\\\\\n\\left.\\begin{array}{c}\n\\\\\n\\\\\n\\\\\n\\end{array}\\right\\}  & p-d\n\\end{array}\\label{eqdelta}}\\label{eq:Delta}\n\\end{eqnarray}\n\nwith $W=[U,V]$, where $V$ is the orthogonal complement of $U$. With\nthis notation, and from a practical point of view, one can say that the\nvariance of the actual data of the $k$th group is therefore modeled by\n$\\Sigma_{k}$, whereas the parameter $\\beta$ models the variance\nof the noise outside the functional subspace. This model is referred to\nin the sequel as $\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$, and\nFigure~\\ref{fig:GraphModel} summarizes the modeling.\n\n\n\\begin{figure}\n\n\\includegraphics{861f03.eps}\n\n\\caption{Graphical representation for the model\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$.} \\label{fig:GraphModel}\n\\end{figure}\n\n\n\n\\begin{table}\n\n\\caption{Number of free parameters in covariance matrices\nwhen $d=K-1$ for the DFM models}\n\\label{Tab:models}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lccc@{}}\n\\hline\n\\multicolumn{1}{@{}l}{\\textbf{Model}} & \\multicolumn{1}{c}{$\\bolds{\\Sigma_{k}}$} & \\multicolumn{1}{c}{$\\bolds{\\beta_{k}}$} & \\multicolumn{1}{c@{}}{\\textbf{Nb. of variance parameters}}\n\\\\\n\\hline\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ & Free & Free &\n$(K-1)(p-K/2)+K^{2}(K-1)/2+K$ \\\\\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ & Free & Common &\n$(K-1)(p-K/2)+K^{2}(K-1)/2+1$\\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$ & Common & Free &\n$(K-1)(p-K/2)+K(K-1)/2+K$ \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta]}$ & Common & Common &\n$(K-1)(p-K/2)+K(K-1)/2+1$\\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta_{k}]}$ & Diagonal & Free &\n$(K-1)(p-K/2)+K^{2}$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ & Diagonal & Common &\n$(K-1)(p-K/2)+K(K-1)+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$ & Spherical & Free &\n$(K-1)(K-1)(p-K/2)+2K$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta]}$ & Spherical & Common &\n$(K-1)(p-K/2)+K+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta_{k}]}$ & Diagonal \\& Common & Free &\n$(K-1)(p-K/2)+(K-1)+K$ \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta]}$ & Diagonal \\& Common & Common &\n$(K-1)(p-K/2)+(K-1)+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta_{k}]}$ & Spherical \\& Common & Free &\n$(K-1)(p-K/2)+K+1$ \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta]}$ & Spherical \\& Common & Common &\n$(K-1)(p-K/2)+2$ \\\\\n\\hline\n\\end{tabular*}\n\\end{table}\n\n\n\\subsection{A family of discriminative functional model}\\label{sec3.4}\n\nStarting with the model $\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ and\nfollowing the strategy of \\citet{Fraley99}, several submodels can be\ngenerated by applying constraints on the parameters of the matrix\n$\\Delta_{k}$. For instance, it is first possible to relax the\nconstraint that the noise variance is common across groups. This\ngenerates the model $\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$, which is\nthe more general model of the family. It is also possible to constrain\nthis new model such that the covariance matrices $\\Sigma_{1},\\ldots\n,\\Sigma_{K}$ in the latent space are common across groups. This\nsubmodel will be referred to as $\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$.\nSimilarly, in each group, $\\Sigma_{k}$ can be assumed to be diagonal,\nthat is, $\\Sigma_{k}=\\operatorname{diag}(\\alpha_{k1},\\ldots\n,\\alpha_{kd})$, and this submodel will be referred to as $\\mathrm\n{DFM}_{[\\alpha_{kj}\\beta_{k}]}$. The variance within the latent\nsubspace $F$ can also be assumed to be isotropic for each group, and\nthe associated submodel is $\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$.\nFollowing this strategy, $12$ different DFM models can be enumerated,\nand an overview of them is proposed in Table~\\ref{Tab:models}. The\ntable also provides, for each model, the number of variance parameters\nto estimate as a function of the number $K$ of groups and the number\n$p$ of basis functions.\nOne can note that the models turn out to be particularly parsimonious\nbecause their complexity is a linear function of $p$, whereas most\nmodel-based approaches usually have a complexity that is a quadratic\nfunction of $p$.\n\n\n\n\n\n\\subsection{Model inference: The FunFEM algorithm}\\label{sec3.5}\n\nBecause the group memberships $\\{z_{1},\\ldots,z_{n}\\}$ of the curves are\nunknown, the direct maximization of the likelihood associated with the\nmodel described above is intractable. In such a case, a classical\nsolution for model inference is to use the EM algorithm. Here, however,\nthe use of the EM algorithm is prohibited due to the particular nature\nof the functional subspace $F$. Indeed, maximizing the likelihood over\nthe subspace orientation matrix $U$ is equivalent to maximizing the\nprojected variance, and it yields the functional principal component\nanalysis (fPCA) subspace. Because $F$ is here assumed to be the most\ndiscriminative subspace, $U$ has to be estimated separately, and we\ntherefore propose the algorithm described hereafter and named FunFEM.\nThe FunFEM algorithm alternates, at iteration $q$, over the three\nfollowing steps:\n\n\\textit{The F step}.\nLet us first suppose that at iteration $q$, the posterior probabilities\n$t_{ik}^{(q)}=E[z_{ik}|\\gamma_{i},\\theta^{(q-1)}]$ are known (they\nhave been estimated in the E step of iteration $q-1$). The F step aims\ntherefore to determine, conditionally on the $t_{ik}^{(q)}$, the\norientation matrix $U$ of the discriminative latent subspace $F$ in\nwhich the $K$ clusters are best separated. Following the original idea\nof Fisher (\\citeyear{Fisher36}), the functional subspace $F$ should be such that the\nvariance within the groups should be minimal, whereas the variance\nbetween groups should be maximal.\nLet $\\mathbf{C}$ be the covariance operator of $X$ with kernel\n\n", "index": 15, "text": "\n\\[\nC(t,s)=\\mathbb{E} \\bigl[\\bigl(X(t)-m(t)\\bigr) \\bigl(X(s)-m(s)\\bigr) \\bigr],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"C(t,s)=\\mathbb{E}\\bigl{[}\\bigl{(}X(t)-m(t)\\bigr{)}\\bigl{(}X(s)-m(s)\\bigr{)}%&#10;\\bigr{]},\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mathbb</mtext></merror><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $m(t)=\\mathbb{E}[X(t)]$. In the following, and without a loss of\ngenerality, the curves are assumed to be centered, that is,\n$m(t)=0$. The operator $\\mathbf{B}$ can thus be rewritten as follows:\n\n\\begin{eqnarray*}\nB(t,s) & = & \\mathbb{E} \\bigl[\\mathbb{E}\\bigl[X(t)|Z\\bigr]\\mathbb {E}\n\\bigl[X(s)|Z\\bigr] \\bigr]\n\\\\\n& = & \\mathbb{E} \\Biggl[\\sum_{k=1}^{K}\n\\mathbf{1}_{\\{Z=k\\}}\\mathbb {E}\\bigl[X(t)|Z=k\\bigr]\\sum\n_{\\ell=1}^{K}\\mathbf{1}_{\\{Z=\\ell\\}}\\mathbb {E}\n\\bigl[X(s)|Z=\\ell\\bigr] \\Biggr]\n\\\\\n& = & \\sum_{k=1}^{K}P(Z=k)\\mathbb{E}\n\\bigl[X(t)|Z=k\\bigr]\\mathbb{E}\\bigl[X(s)|Z=k\\bigr].\n\\end{eqnarray*}\n\nThe Fisher criterion, in the functional case and the supervised setting\n[\\citet{Pre2007}], looks for the discriminative function $u\\in L_{2}[0,T]$\nwhich is solution of\n\n\n\n", "itemtype": "equation", "pos": 27693, "prevtext": "\n\nand $\\mathbf{B}$ be the integral between-cluster covariance operator\nwith kernel\n\n", "index": 17, "text": "\n\\[\nB(t,s)=\\mathbb{E} \\bigl[\\mathbb{E}\\bigl[X(t)-m(t)|Z\\bigr]\\mathbb {E}\n\\bigl[X(s)-m(s)|Z\\bigr] \\bigr],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"B(t,s)=\\mathbb{E}\\bigl{[}\\mathbb{E}\\bigl{[}X(t)-m(t)|Z\\bigr{]}\\mathbb{E}\\bigl{%&#10;[}X(s)-m(s)|Z\\bigr{]}\\bigr{]},\" display=\"block\"><mrow><mi>B</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mathbb</mtext></merror><mi>E</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mathbb</mtext></merror><mi>E</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mi>Z</mi><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mathbb</mtext></merror><mi>E</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mi>Z</mi><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\Phi(X)=\\int_{[0,T]}X(t)u(t)\\,dt$ is the projection of $X$ on\nthe discriminative function $u$. Let us recall that we consider here\nthe unsupervised setting, and $Z$ is an unobserved variable. The\nsolution of (\\ref{eq:FisherCrit}) is the eigenfunction $u$ associated\nwith the largest eigenvalue $\\eta\\in\\mathbb{R}$ of the following\ngeneralized eigenproblem:\n\n\n\\begin{eqnarray}\\label\n{eq:GEProblem}\n\\mathbf{B}u & = & \\eta\\mathbf{C}u,\n\\nonumber\n\\[-8pt]\n\\[-8pt]\n\\nonumber\n\\int_{[0,T]}B(t,s)u(s)\\,ds & = & \\eta\\int_{[0,T]}C(t,s)u(s)\\,ds,\n\\end{eqnarray}\n\nunder the constraint $<u,\\mathbf{C}u>_{L_{2}[0,T]}=1$.\nThe estimator for $C(t, s)$ from the sample $\\{x_{1},\\ldots,x_{n}\\}$,\nexpanded on the basis $ (\\psi_{j} )_{j=1,\\ldots,p}$, is\n\n\\begin{eqnarray*}\n\\hat{C}(t,s) & = & \\frac{1}{n}\\sum_{i=1}^{n}\n\\Biggl(\\sum_{j=1}^{p}\\gamma _{ij}\n\\psi_{j}(t)\\Biggr) \\Biggl(\\sum_{j=1}^{p}\n\\gamma_{ij}\\psi_{j}(s)\\Biggr)\n\\\\\n& = & \\frac{1}{n}\\Psi'(t)\\bolds{\\Gamma}'\\bolds{\n\\Gamma}\\Psi(s),\n\\end{eqnarray*}\n\nwhere $\\bolds{\\Gamma}=(\\gamma_{ij})_{i, j}$ is the $n\\times p$-matrix\nof basis expansion coefficients and $\\Psi(s)$ is the $p$-vector of the\nbasis functions $\\psi_{j}(s)$ ($1\\leq i\\leq n$ and $1\\leq j\\leq p$).\nBecause the variable $Z$ is unobserved, $B(t, s)$ has to be estimated\nconditionally on the posterior probabilities\n$t_{ik}^{(q-1)}=E[z_{ik}|\\gamma_{i},\\theta^{(q-1)}]$ obtained from\nthe E step at iteration $q-1$:\n\n\\begin{eqnarray*}\n\\hat{B}^{(q)}(t,s) & = & \\sum_{k=1}^{K}\n\\frac{n_{k}^{(q-1)}}{n} \\Biggl(\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}x_{i}(t)\n\\Biggr) \\Biggl(\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}x_{i}(s)\n\\Biggr)\n\\\\\n& = & \\frac{1}{n}\\sum_{k=1}^{K}\n\\frac{1}{n_{k}^{(q-1)}} \\Biggl(\\sum_{i=1}^{n}t_{ik}^{(q-1)}\n\\sum_{j=1}^{p}\\gamma_{ij}\n\\psi_{j}(t) \\Biggr) \\Biggl(\\sum_{i=1}^{n}t_{ik}^{(q-1)}\n\\sum_{j=1}^{p}\\gamma_{ij}\\psi\n_{j}(s) \\Biggr),\n\\end{eqnarray*}\n\nand in a matrix form:\n\n", "itemtype": "equation", "pos": 28575, "prevtext": "\n\nwhere $m(t)=\\mathbb{E}[X(t)]$. In the following, and without a loss of\ngenerality, the curves are assumed to be centered, that is,\n$m(t)=0$. The operator $\\mathbf{B}$ can thus be rewritten as follows:\n\n\\begin{eqnarray*}\nB(t,s) & = & \\mathbb{E} \\bigl[\\mathbb{E}\\bigl[X(t)|Z\\bigr]\\mathbb {E}\n\\bigl[X(s)|Z\\bigr] \\bigr]\n\\\\\n& = & \\mathbb{E} \\Biggl[\\sum_{k=1}^{K}\n\\mathbf{1}_{\\{Z=k\\}}\\mathbb {E}\\bigl[X(t)|Z=k\\bigr]\\sum\n_{\\ell=1}^{K}\\mathbf{1}_{\\{Z=\\ell\\}}\\mathbb {E}\n\\bigl[X(s)|Z=\\ell\\bigr] \\Biggr]\n\\\\\n& = & \\sum_{k=1}^{K}P(Z=k)\\mathbb{E}\n\\bigl[X(t)|Z=k\\bigr]\\mathbb{E}\\bigl[X(s)|Z=k\\bigr].\n\\end{eqnarray*}\n\nThe Fisher criterion, in the functional case and the supervised setting\n[\\citet{Pre2007}], looks for the discriminative function $u\\in L_{2}[0,T]$\nwhich is solution of\n\n\n\n", "index": 19, "text": "\\begin{equation}\n\\max_{u}\\frac{\\operatorname{Var}(\\mathbb{E}[\\Phi(X)|Z])}\n{\\operatorname{Var}(\\Phi(X))},\\label\n{eq:FisherCrit}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\max_{u}\\frac{\\operatorname{Var}(\\mathbb{E}[\\Phi(X)|Z])}{\\operatorname{Var}(%&#10;\\Phi(X))},\" display=\"block\"><mrow><mrow><munder><mi>max</mi><mi>u</mi></munder><mo>\u2061</mo><mfrac><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mi>V</mi><mi>a</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mathbb</mtext></merror><mi>E</mi><mrow><mo stretchy=\"false\">[</mo><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo><mi>Z</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwith $n_{k}^{(q-1)}=\\sum_{i=1}^{n}t_{ik}^{(q-1)}$ and $\\mathbf{T}=\n(\\frac{t_{ik}^{(q-1)}}{\\sqrt{n_{k}^{(q-1)}}} )_{i, k}$ is a\n$n\\times K$-matrix.\nAssuming that the discriminative function $u$ can be decomposed in the\nsame basis as the observed curves,\n\n\n\n", "itemtype": "equation", "pos": 30629, "prevtext": "\n\nwhere $\\Phi(X)=\\int_{[0,T]}X(t)u(t)\\,dt$ is the projection of $X$ on\nthe discriminative function $u$. Let us recall that we consider here\nthe unsupervised setting, and $Z$ is an unobserved variable. The\nsolution of (\\ref{eq:FisherCrit}) is the eigenfunction $u$ associated\nwith the largest eigenvalue $\\eta\\in\\mathbb{R}$ of the following\ngeneralized eigenproblem:\n\n\n\\begin{eqnarray}\\label\n{eq:GEProblem}\n\\mathbf{B}u & = & \\eta\\mathbf{C}u,\n\\nonumber\n\\[-8pt]\n\\[-8pt]\n\\nonumber\n\\int_{[0,T]}B(t,s)u(s)\\,ds & = & \\eta\\int_{[0,T]}C(t,s)u(s)\\,ds,\n\\end{eqnarray}\n\nunder the constraint $<u,\\mathbf{C}u>_{L_{2}[0,T]}=1$.\nThe estimator for $C(t, s)$ from the sample $\\{x_{1},\\ldots,x_{n}\\}$,\nexpanded on the basis $ (\\psi_{j} )_{j=1,\\ldots,p}$, is\n\n\\begin{eqnarray*}\n\\hat{C}(t,s) & = & \\frac{1}{n}\\sum_{i=1}^{n}\n\\Biggl(\\sum_{j=1}^{p}\\gamma _{ij}\n\\psi_{j}(t)\\Biggr) \\Biggl(\\sum_{j=1}^{p}\n\\gamma_{ij}\\psi_{j}(s)\\Biggr)\n\\\\\n& = & \\frac{1}{n}\\Psi'(t)\\bolds{\\Gamma}'\\bolds{\n\\Gamma}\\Psi(s),\n\\end{eqnarray*}\n\nwhere $\\bolds{\\Gamma}=(\\gamma_{ij})_{i, j}$ is the $n\\times p$-matrix\nof basis expansion coefficients and $\\Psi(s)$ is the $p$-vector of the\nbasis functions $\\psi_{j}(s)$ ($1\\leq i\\leq n$ and $1\\leq j\\leq p$).\nBecause the variable $Z$ is unobserved, $B(t, s)$ has to be estimated\nconditionally on the posterior probabilities\n$t_{ik}^{(q-1)}=E[z_{ik}|\\gamma_{i},\\theta^{(q-1)}]$ obtained from\nthe E step at iteration $q-1$:\n\n\\begin{eqnarray*}\n\\hat{B}^{(q)}(t,s) & = & \\sum_{k=1}^{K}\n\\frac{n_{k}^{(q-1)}}{n} \\Biggl(\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}x_{i}(t)\n\\Biggr) \\Biggl(\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}x_{i}(s)\n\\Biggr)\n\\\\\n& = & \\frac{1}{n}\\sum_{k=1}^{K}\n\\frac{1}{n_{k}^{(q-1)}} \\Biggl(\\sum_{i=1}^{n}t_{ik}^{(q-1)}\n\\sum_{j=1}^{p}\\gamma_{ij}\n\\psi_{j}(t) \\Biggr) \\Biggl(\\sum_{i=1}^{n}t_{ik}^{(q-1)}\n\\sum_{j=1}^{p}\\gamma_{ij}\\psi\n_{j}(s) \\Biggr),\n\\end{eqnarray*}\n\nand in a matrix form:\n\n", "index": 21, "text": "\n\\[\n\\hat{B}^{(q)}(t,s)=\\frac{1}{n}\\Psi'(t)\\bolds{\n\\Gamma}'\\mathbf{T}\\mathbf {T}'\\bolds{\\Gamma}\\Psi(s),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\hat{B}^{(q)}(t,s)=\\frac{1}{n}\\Psi^{\\prime}(t)\\bolds{\\Gamma}^{\\prime}\\mathbf{T%&#10;}\\mathbf{T}^{\\prime}\\bolds{\\Gamma}\\Psi(s),\" display=\"block\"><mrow><mrow><mrow><msup><mover accent=\"true\"><mi>B</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc13\ud835\udc13</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nthe generalized eigenproblem (\\ref{eq:GEProblem}) becomes\n\n", "itemtype": "equation", "pos": 30991, "prevtext": "\n\nwith $n_{k}^{(q-1)}=\\sum_{i=1}^{n}t_{ik}^{(q-1)}$ and $\\mathbf{T}=\n(\\frac{t_{ik}^{(q-1)}}{\\sqrt{n_{k}^{(q-1)}}} )_{i, k}$ is a\n$n\\times K$-matrix.\nAssuming that the discriminative function $u$ can be decomposed in the\nsame basis as the observed curves,\n\n\n\n", "index": 23, "text": "\\begin{equation}\nu(t)=\\sum_{j=1}^{p}\\nu_{j}\n\\psi_{j}(t)=\\Psi'(t)\\bolds{\\nu},\\label{eq:X-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"u(t)=\\sum_{j=1}^{p}\\nu_{j}\\psi_{j}(t)=\\Psi^{\\prime}(t)\\bolds{\\nu},\" display=\"block\"><mrow><mrow><mrow><mi>u</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>\u03bd</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhich is equivalent to\n\n", "itemtype": "equation", "pos": 31156, "prevtext": "\n\nthe generalized eigenproblem (\\ref{eq:GEProblem}) becomes\n\n", "index": 25, "text": "\n\\[\n\\int_{[0,T]}\\frac{1}{n}\\Psi'(t)\\bolds{\n\\Gamma}'\\mathbf{T}\\mathbf {T}'\\bolds{\\Gamma}\\Psi(s)\n\\Psi'(s)\\bolds{\\nu}\\,ds=\\eta\\int_{[0,T]}\n\\frac{1}{n}\\Psi'(t)\\bolds{\\Gamma}'\\bolds{\\Gamma}\n\\Psi (s)\\Psi'(s)\\bolds{\\nu}\\,ds,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\int_{[0,T]}\\frac{1}{n}\\Psi^{\\prime}(t)\\bolds{\\Gamma}^{\\prime}\\mathbf{T}%&#10;\\mathbf{T}^{\\prime}\\bolds{\\Gamma}\\Psi(s)\\Psi^{\\prime}(s)\\bolds{\\nu}\\,ds=\\eta%&#10;\\int_{[0,T]}\\frac{1}{n}\\Psi^{\\prime}(t)\\bolds{\\Gamma}^{\\prime}\\bolds{\\Gamma}%&#10;\\Psi(s)\\Psi^{\\prime}(s)\\bolds{\\nu}\\,ds,\" display=\"block\"><mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></msub><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc13\ud835\udc13</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>\u03bd</mi></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>s</mi></mrow></mrow></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></msub><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>\u03bd</mi></mpadded><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>s</mi></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwith $\\mathbf{W}=\\int_{[0,T]}\\Psi(s)\\Psi'(s)\\,ds$. Because this equality\nholds for all $t\\in[0,T]$, we have\n\n", "itemtype": "equation", "pos": 31401, "prevtext": "\n\nwhich is equivalent to\n\n", "index": 27, "text": "\n\\[\n\\frac{1}{n}\\Psi'(t)\\bolds{\\Gamma}'\\mathbf{T}\n\\mathbf{T}'\\bolds{\\Gamma }W\\bolds{\\nu}=\\eta\\frac{1}{n}\n\\Psi'(t)\\bolds{\\Gamma}'\\bolds{\\Gamma }W\\bolds{\\nu},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{n}\\Psi^{\\prime}(t)\\bolds{\\Gamma}^{\\prime}\\mathbf{T}\\mathbf{T}^{\\prime%&#10;}\\bolds{\\Gamma}W\\bolds{\\nu}=\\eta\\frac{1}{n}\\Psi^{\\prime}(t)\\bolds{\\Gamma}^{%&#10;\\prime}\\bolds{\\Gamma}W\\bolds{\\nu},\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc13\ud835\udc13</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nor, equivalently,\n\n\n\n", "itemtype": "equation", "pos": 31669, "prevtext": "\n\nwith $\\mathbf{W}=\\int_{[0,T]}\\Psi(s)\\Psi'(s)\\,ds$. Because this equality\nholds for all $t\\in[0,T]$, we have\n\n", "index": 29, "text": "\n\\[\n\\bolds{\\Gamma}'\\mathbf{T}\\mathbf{T}'\\bolds{\\Gamma}W\n\\bolds{\\nu}=\\eta \\bolds{\\Gamma}'\\bolds{\\Gamma}W\\bolds{\\nu},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\bolds{\\Gamma}^{\\prime}\\mathbf{T}\\mathbf{T}^{\\prime}\\bolds{\\Gamma}W\\bolds{\\nu}%&#10;=\\eta\\bolds{\\Gamma}^{\\prime}\\bolds{\\Gamma}W\\bolds{\\nu},\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc13\ud835\udc13</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nFinally, the basis expansion coefficient $\\nu=(\\nu_{1},\\ldots,\\nu\n_{p})'$ of the discriminative function $u$ is the eigenvector of the\nabove generalized eigenproblem associated with the largest eigenvalue.\nOnce the first discriminative function, let us say $u_{1}$, is\ndetermined, the second discriminative function is obtained by solving\nthe generalized eigenproblem (\\ref{eq:GEP2}) in the complementary\nspace of $u_{1}$. This procedure is recursively applied until the $d$\ndiscriminative functions $\\{u_{1},\\ldots,u_{d}\\}$ are obtained. The basis\nexpansion coefficients $\\nu_{j}^{(q)}=(\\nu_{j1}^{(q)},\\ldots,\\nu\n_{jp}^{(q)})'$, $j=1,\\ldots, d$ of the estimated discriminative\nfunctions are gathered in the $p\\times d$ matrix $U^{(q)}= (\\nu\n_{j\\ell}^{(q)} )_{j,\\ell}$.\n\n\\textit{The M step}.\nFollowing the classical scheme of the EM algorithm, this step aims to\nmaximize, conditionally on the orientation matrix $U^{(q)}$ obtained\nfrom the previous step, the conditional expectation of the complete\ndata log-likelihood $Q(\\theta;\\theta^{(q-1)})=E [\\ell(\\theta\n;\\bolds{\\Gamma},z_{1},\\ldots,z_{n})|\\bolds{\\Gamma},\\theta^{(q-1)} ]$:\n\n\\begin{eqnarray*}\n&&Q\\bigl(\\theta;\\theta^{(q-1)}\\bigr) \\\\\n&&\\qquad =  -\\frac{1}{2}\\sum\n_{k=1}^{K}n_{k}^{(q-1)} \\Biggl[\\log|\n\\Sigma_{k}|+(p-d)\\log(\\beta )-2\\log(\\pi_{k})+p\\log(2\\pi)\n\\\\\n& &\\qquad\\quad{} +\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}(\n\\gamma_{i}-\\mu_{k})^{t}U^{(q)}\\Delta\n_{k}^{-1}U^{(q)}{}^{t}(\n\\gamma_{i}-\\mu_{k}) \\Biggr]\n\\\\\n&&\\qquad =  -\\frac{1}{2}\\sum_{k=1}^{K}n_{k}^{(q-1)}\n\\Biggl[\\log|\\Sigma _{k}|+(p-d)\\log(\\beta)-2\\log(\\pi_{k})+p\n\\log(2\\pi)\n\\\\\n& &\\qquad\\quad{} +\\operatorname{trace}\\bigl(\\Sigma _{k}^{-1}U^{(q)}{}^{t}C_{k}U^{(q)}\n\\bigr)+\\frac{1}{\\beta} \\Biggl(\\operatorname {trace}(C_{k})-\\sum\n_{j=1}^{d}\\nu_{j}^{(q)t}C_{k}\n\\nu_{j}^{(q)} \\Biggr) \\Biggr],\n\\end{eqnarray*}\n\nwhere $\\theta=(\\pi_{k},\\mu_{k},\\Sigma_{k},\\beta)_{k}$, for $1\\leq\nk\\leq K$, and $C_{k}=\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}(\\gamma_{i}-\\mu_{k}^{(q-1)})(\\gamma_{i}-\\mu\n_{k}^{(q-1)})^{t}$.\nThe maximization of $Q(\\theta;\\theta^{(q-1)})$, according to $\\pi\n_{k},\\mu_{k},\\Sigma_{k}$ and $\\beta$, yields the following updates\nfor model parameters:\n\n\\begin{itemize}\n\n\\item$\\pi_{k}^{(q)}=n_{k}^{(q-1)}/n$,\n\n\\item$\\mu_{k}^{(q)}=\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}U^{(q)t}\\gamma_{i}$,\n\n\\item$\\Sigma_{k}^{(q)}=U^{(q)t}C_{k}^{(q)}U^{(q)}$,\n\n\\item$\\beta^{(q)}= (\\operatorname{trace}(C^{(q)})-\\sum_{j=1}^{d}u_{j}^{(q)t}C^{(q)}u_{j}^{(q)} )/ (p-d )$.\n\\end{itemize}\n\nUpdated formula for other models of the family can be easily obtained\nfrom \\citet{Bouveyron12FEM}.\n\n\\textit{The E step}.\nThis last step reduces to update, at iteration $q$, the posterior\nprobabilities $t_{ik}^{(q)}=E[z_{ik}|\\gamma_{i},\\theta^{(q)}]$. Let\nus also recall that $t_{ik}^{(q)}$ is also the posterior probability\n$P(z_{ik}=1|\\gamma_{i},\\theta^{(q)})$ that the curve $x_{i}$ belongs\nto the $k$th component of the mixture under the current model. Using\nBayes' theorem, the posterior probabilities $t_{ik}^{(q)}$,\n$i=1,\\ldots,n$, $k=1,\\ldots,K$, can be expressed as follows:\n\n\n\n", "itemtype": "equation", "pos": 31809, "prevtext": "\n\nor, equivalently,\n\n\n\n", "index": 31, "text": "\\begin{equation}\n\\bigl(\\bolds{\\Gamma}'\\bolds{\\Gamma}W\\bigr)^{-1}\\bolds{\n\\Gamma}'\\mathbf{T}\\mathbf {T}'\\bolds{\\Gamma}W\\bolds{\n\\nu}=\\eta\\bolds{\\nu}.\\label{eq:GEP2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\bigl{(}\\bolds{\\Gamma}^{\\prime}\\bolds{\\Gamma}W\\bigr{)}^{-1}\\bolds{\\Gamma}^{%&#10;\\prime}\\mathbf{T}\\mathbf{T}^{\\prime}\\bolds{\\Gamma}W\\bolds{\\nu}=\\eta\\bolds{\\nu}.\" display=\"block\"><mrow><mrow><mrow><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc13\ud835\udc13</mi><mo>\u2032</mo></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow><mo>=</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bolds</mtext></merror><mo>\u2062</mo><mi>\u03bd</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\theta_{k}^{(q)}=(\\pi_{k}^{(q)},\\mu_{k}^{(q)},\\Sigma\n_{k}^{(q)},\\beta^{(q)})$ is the set of parameters for the $k$th\ncomponent updated in the M step.\n\n\n\\subsection{Model selection}\\label{sec3.6}\nWe now discuss both the choice of the most appropriate model within the\nfamily and the problem of selecting the number $K$ of groups and the\nintrinsic dimension $d$. On the one hand, it is first of interest to\nselect the model of the DFM family that is the most appropriate to\nmodel the data at hand. On the other hand, the problem of selecting $K$\nand $d$ can be, in fact, recast as a model selection problem. The idea\nhere is to consider, for instance, a DFM model with $K=2$ and the same\nDFM model with $K=3$ as two different models among which one wants to\nchoose. Thus, because a model is defined by its parametrization, its\nnumber of components $K$ and its intrinsic dimensionality $d$, model\nselection criteria allow us to select the best combination of those\nthree features required for modeling the data.\n\nClassical tools for model selection include the AIC [\\citet{Akaike74}]\nand BIC [\\citet{Schwarz78}] criteria, which penalize the log-likelihood\n$\\ell(\\hat\\theta)$ as follows, for model $\\mathcal{M}$:\n\n\n\n", "itemtype": "equation", "pos": 35085, "prevtext": "\n\nFinally, the basis expansion coefficient $\\nu=(\\nu_{1},\\ldots,\\nu\n_{p})'$ of the discriminative function $u$ is the eigenvector of the\nabove generalized eigenproblem associated with the largest eigenvalue.\nOnce the first discriminative function, let us say $u_{1}$, is\ndetermined, the second discriminative function is obtained by solving\nthe generalized eigenproblem (\\ref{eq:GEP2}) in the complementary\nspace of $u_{1}$. This procedure is recursively applied until the $d$\ndiscriminative functions $\\{u_{1},\\ldots,u_{d}\\}$ are obtained. The basis\nexpansion coefficients $\\nu_{j}^{(q)}=(\\nu_{j1}^{(q)},\\ldots,\\nu\n_{jp}^{(q)})'$, $j=1,\\ldots, d$ of the estimated discriminative\nfunctions are gathered in the $p\\times d$ matrix $U^{(q)}= (\\nu\n_{j\\ell}^{(q)} )_{j,\\ell}$.\n\n\\textit{The M step}.\nFollowing the classical scheme of the EM algorithm, this step aims to\nmaximize, conditionally on the orientation matrix $U^{(q)}$ obtained\nfrom the previous step, the conditional expectation of the complete\ndata log-likelihood $Q(\\theta;\\theta^{(q-1)})=E [\\ell(\\theta\n;\\bolds{\\Gamma},z_{1},\\ldots,z_{n})|\\bolds{\\Gamma},\\theta^{(q-1)} ]$:\n\n\\begin{eqnarray*}\n&&Q\\bigl(\\theta;\\theta^{(q-1)}\\bigr) \\\\\n&&\\qquad =  -\\frac{1}{2}\\sum\n_{k=1}^{K}n_{k}^{(q-1)} \\Biggl[\\log|\n\\Sigma_{k}|+(p-d)\\log(\\beta )-2\\log(\\pi_{k})+p\\log(2\\pi)\n\\\\\n& &\\qquad\\quad{} +\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}(\n\\gamma_{i}-\\mu_{k})^{t}U^{(q)}\\Delta\n_{k}^{-1}U^{(q)}{}^{t}(\n\\gamma_{i}-\\mu_{k}) \\Biggr]\n\\\\\n&&\\qquad =  -\\frac{1}{2}\\sum_{k=1}^{K}n_{k}^{(q-1)}\n\\Biggl[\\log|\\Sigma _{k}|+(p-d)\\log(\\beta)-2\\log(\\pi_{k})+p\n\\log(2\\pi)\n\\\\\n& &\\qquad\\quad{} +\\operatorname{trace}\\bigl(\\Sigma _{k}^{-1}U^{(q)}{}^{t}C_{k}U^{(q)}\n\\bigr)+\\frac{1}{\\beta} \\Biggl(\\operatorname {trace}(C_{k})-\\sum\n_{j=1}^{d}\\nu_{j}^{(q)t}C_{k}\n\\nu_{j}^{(q)} \\Biggr) \\Biggr],\n\\end{eqnarray*}\n\nwhere $\\theta=(\\pi_{k},\\mu_{k},\\Sigma_{k},\\beta)_{k}$, for $1\\leq\nk\\leq K$, and $C_{k}=\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}(\\gamma_{i}-\\mu_{k}^{(q-1)})(\\gamma_{i}-\\mu\n_{k}^{(q-1)})^{t}$.\nThe maximization of $Q(\\theta;\\theta^{(q-1)})$, according to $\\pi\n_{k},\\mu_{k},\\Sigma_{k}$ and $\\beta$, yields the following updates\nfor model parameters:\n\n\\begin{itemize}\n\n\\item$\\pi_{k}^{(q)}=n_{k}^{(q-1)}/n$,\n\n\\item$\\mu_{k}^{(q)}=\\frac{1}{n_{k}^{(q-1)}}\\sum_{i=1}^{n}t_{ik}^{(q-1)}U^{(q)t}\\gamma_{i}$,\n\n\\item$\\Sigma_{k}^{(q)}=U^{(q)t}C_{k}^{(q)}U^{(q)}$,\n\n\\item$\\beta^{(q)}= (\\operatorname{trace}(C^{(q)})-\\sum_{j=1}^{d}u_{j}^{(q)t}C^{(q)}u_{j}^{(q)} )/ (p-d )$.\n\\end{itemize}\n\nUpdated formula for other models of the family can be easily obtained\nfrom \\citet{Bouveyron12FEM}.\n\n\\textit{The E step}.\nThis last step reduces to update, at iteration $q$, the posterior\nprobabilities $t_{ik}^{(q)}=E[z_{ik}|\\gamma_{i},\\theta^{(q)}]$. Let\nus also recall that $t_{ik}^{(q)}$ is also the posterior probability\n$P(z_{ik}=1|\\gamma_{i},\\theta^{(q)})$ that the curve $x_{i}$ belongs\nto the $k$th component of the mixture under the current model. Using\nBayes' theorem, the posterior probabilities $t_{ik}^{(q)}$,\n$i=1,\\ldots,n$, $k=1,\\ldots,K$, can be expressed as follows:\n\n\n\n", "index": 33, "text": "\\begin{equation}\nt_{ik}^{(q)}=\\frac{\\pi_{k}^{(q)}\\phi(\\gamma_{i},\\theta\n_{k}^{(q)})}{\\sum_{l=1}^{K}\\pi_{l}^{(q)}\\phi(\\gamma_{i}|\\theta\n_{l}^{(q)})},\\label{eq:tik_GMM-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"t_{ik}^{(q)}=\\frac{\\pi_{k}^{(q)}\\phi(\\gamma_{i},\\theta{}_{k}^{(q)})}{\\sum_{l=1%&#10;}^{K}\\pi_{l}^{(q)}\\phi(\\gamma_{i}|\\theta_{l}^{(q)})},\" display=\"block\"><mrow><mrow><msubsup><mi>t</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi>\u03c0</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mi>\u03d5</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b3</mi><mi>i</mi></msub><mo>,</mo><mi>\u03b8</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mmultiscripts></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msubsup><mi>\u03c0</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mi>\u03d5</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b3</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msubsup><mi>\u03b8</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\xi(\\mathcal{M})$ is the number of free parameters of the\nmodel, and $n$ is the number of observations.\nThe value of $\\xi(\\mathcal{M})$ is, of course, specific to the model\nselected by the practitioner (cf. Table~\\ref{Tab:models}).\nAlthough penalized likelihood criteria are widely used, AIC and BIC are\nalso known to be less efficient in practical situations than in\nsimulated cases. In particular, the required regularity conditions are\nnot fully satisfied in the mixture framework [\\citet\n{Lindsay95,Lindsay2008}] and, hence, the criteria might not be appropriate.\n\nTo overcome this drawback, \\citet{birge2007minimal} recently proposed a\ndata-driven technique, called the ``slope heuristic,'' to calibrate the\npenalty involved in penalized criteria. The slope heuristic was first\nproposed in the context of Gaussian homoscedastic least squares\nregression and was then used in different situations, including\nmodel-based clustering. \\citet{birge2007minimal} showed that there\nexists a minimal penalty and that considering a penalty equal to twice\nthis minimal penalty allows for approximating the oracle model in terms\nof risk. The minimal penalty is, in practice, estimated by the slope of\nthe linear part when plotting the log-likelihood $\\ell(\\hat\\theta)$\nwith regard to the number of model parameters (or model dimension). The\ncriterion associated with the slope heuristic is therefore defined by\n\n\n\n", "itemtype": "equation", "pos": 36488, "prevtext": "\n\nwhere $\\theta_{k}^{(q)}=(\\pi_{k}^{(q)},\\mu_{k}^{(q)},\\Sigma\n_{k}^{(q)},\\beta^{(q)})$ is the set of parameters for the $k$th\ncomponent updated in the M step.\n\n\n\\subsection{Model selection}\\label{sec3.6}\nWe now discuss both the choice of the most appropriate model within the\nfamily and the problem of selecting the number $K$ of groups and the\nintrinsic dimension $d$. On the one hand, it is first of interest to\nselect the model of the DFM family that is the most appropriate to\nmodel the data at hand. On the other hand, the problem of selecting $K$\nand $d$ can be, in fact, recast as a model selection problem. The idea\nhere is to consider, for instance, a DFM model with $K=2$ and the same\nDFM model with $K=3$ as two different models among which one wants to\nchoose. Thus, because a model is defined by its parametrization, its\nnumber of components $K$ and its intrinsic dimensionality $d$, model\nselection criteria allow us to select the best combination of those\nthree features required for modeling the data.\n\nClassical tools for model selection include the AIC [\\citet{Akaike74}]\nand BIC [\\citet{Schwarz78}] criteria, which penalize the log-likelihood\n$\\ell(\\hat\\theta)$ as follows, for model $\\mathcal{M}$:\n\n\n\n", "index": 35, "text": "\\begin{equation}\n\\operatorname{AIC}(\\mathcal{M}) = \\ell(\\hat\\theta) - \\xi(\\mathcal {M}),\\qquad \\operatorname{BIC}(\n\\mathcal{M}) = \\ell(\\hat\\theta) - \\frac{\\xi\n(\\mathcal{M})}{2}\\log(n),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\operatorname{AIC}(\\mathcal{M})=\\ell(\\hat{\\theta})-\\xi(\\mathcal{M}),\\qquad%&#10;\\operatorname{BIC}(\\mathcal{M})=\\ell(\\hat{\\theta})-\\frac{\\xi(\\mathcal{M})}{2}%&#10;\\log(n),\" display=\"block\"><mrow><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03be</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>B</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mfrac><mrow><mi>\u03be</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07999.tex", "nexttext": "\n\nwhere $\\hat{s}$ is the slope of the linear part of $\\ell(\\hat\\theta)$.\nA detailed overview and advice for implementation are provided in\n\\citet{baudry2012slope}. Section~\\ref{sec3} proposes a comparison of the slope\nheuristic with classical model selection criteria. In Section~\\ref{sec4} the\nslope heuristic criterion is used for the model selection for the BSS data.\n\n\n\\subsection{Selection of discriminative basis functions}\\label{sec3.7}\n\nAnother advantage of the proposed modeling is the possibility of using\nthe discriminative subspace to select the relevant basis functions for\ndiscriminating between the groups. Indeed, the functional subspace $F$\nallows for determining the discriminative basis functions through the\nloading matrix $U$, which contains the coefficients of the linear\nrelation that links the basis functions with the subspace $F$. It is\ntherefore expected that basis functions associated with large absolute\nvalues of $U$ are particularly relevant for discriminating between the groups.\nAn intuitive way to identify the discriminative basis functions would\nbe to keep only large absolute loading variables by, for instance,\nthresholding. Although this approach is commonly used in practice, it\nhas been particularly criticized by \\citet{Cadima1995} because it\ninduces some misleading information. Here, we propose selecting the\ndiscriminative basis functions by constraining the optimization\nproblem (\\ref{eq:FisherCrit}) of the F step such that the loading\nmatrix $U$ is sparse (i.e., such that $U$ contains as many\nzeros as possible). To this end, we follow the approach proposed\nby \\citet{Bouveyron14a}, who rewrite the constrained Fisher criterion\nas a $\\ell_{1}$-penalized regression problem. We therefore use their\nalgorithm [Algorithm 2 of \\citet{Bouveyron14a}] to maximize the\noptimization problem (\\ref{eq:FisherCrit}) under $\\ell_{1}$-penalization.\n\n\n\\begin{figure}\n\n\\includegraphics{861f04.eps}\n\n\\caption{Raw and smoothed simulated curves.} \\label{fig:SimulatedCurves}\n\\end{figure}\n\n\n\n\\section{Numerical experimentations}\\label{sec4}\n\n\n\n\nThis section presents numerical experiments to validate on simulated\nand benchmark data the approach presented above, before applying it on\nthe BSS data.\n\n\\subsection{Model selection}\\label{sec4.1}\n\nWe first focus on the problem of model selection. Here, BIC and the\nslope heuristic are challenged on a set of simulated curves. A sample\nof $n=100$ curves is simulated according to the following model,\ninspired by \\citet{Fer2003,Pre2007a}:\n\n\\begin{eqnarray*}\n&&\\mbox{Cluster 1:}\\quad X(t)  =  U+(1-U)h_{1}(t)+{\\varepsilon}(t),\\qquad t\\in[1,21],\n\\\\\n&&\\mbox{Cluster 2:}\\quad X(t) = U+(1-U)h_{2}(t)+{\\varepsilon}(t),\\qquad t\\in[1,21],\n\\\\\n&&\\mbox{Cluster 3:}\\quad X(t) = U+(0.5-U)h_{1}(t)+{\\varepsilon} (t),\\qquad t\n\\in[1,21],\n\\\\\n&&\\mbox{Cluster 4:}\\quad X(t) = U+(0.5-U)h_{1}(t)+{\\varepsilon} (t),\\qquad t\n\\in[1,21],\n\\end{eqnarray*}\n\nwhere $U$ is uniformly distributed on $[0,1]$, and ${\\varepsilon}(t)$ is\nwhite noise that is independent from $U$ such that $\\operatorname{\\mathbb{V}ar}({\\varepsilon}_{t})=0.5$. The functions $h_{1}$ and $h_{2}$ are\ndefined, for $t\\in[1,21]$, by $h_{1}(t)=6-|t-7|$ and\n$h_{2}(t)=6-|t-15|$. The mixing proportions are equal, and the curves\nare observed in $101$ equidistant points ($t=1,1.2,\\ldots,21$). The\nfunctional form of the data is reconstructed using a Fourier basis\nsmoothing with 25 basis functions. Figure~\\ref{fig:SimulatedCurves}\nplots the simulated curves and the smoothed ones.\n\n\n\\begin{figure}\n\n\\includegraphics{861f05.eps}\n\n\\caption{Selection of the number of clusters\nusing BIC on the simulated data (actual value of $K$ is~4).} \\label{fig:simul-BIC}\n\\end{figure}\n\n\n\\begin{figure}[b]\n\n\\includegraphics{861f06.eps}\n\n\\caption{Selection of the number of clusters\nusing the slope heuristic on the simulated data (actual value of $K$ is 4).}\\label{fig:simul-Slope}\n\\end{figure}\n\n\n\nFor each simulated data set, the number $K$ of clusters is estimated\nbased on both the BIC and the slope heuristic criteria. As an example\nof the results, Figures \\ref{fig:simul-BIC} and \\ref{fig:simul-Slope}\n(right panel) plot, respectively, the values of the BIC criterion and\nthe slope heuristic for one simulation with the model $\\mathrm\n{DFM}_{[\\Sigma_{k}\\beta_{k}]}$.\nOn this run, both criteria succeed in selecting the actual number of\nclusters ($K=4$). Figure~\\ref{fig:simul-Slope} may require further\nexplanation. The left panel plots the log-likelihood function with\nregard to the number of free model parameters, the latter being a\nfunction of $K$ (see Table~\\ref{Tab:models}). The slope heuristic\nconsists of using the slope of the linear part of the objective\nfunction to calibrate the penalty. The linear part is here represented\nby the red dashed line and was automatically determined using a robust\nlinear regression. The slope coefficient is then used to compute the\npenalized log-likelihood function, shown on the right panel. We can see\nhere that the slope heuristic provides a penalty close to the one of BIC.\n\nBoth criteria were then used to select the appropriate model and number\nof groups on 100 simulated data sets.\nTables \\ref{tab:simul-result} and \\ref{tab:simul-result-1} present respectively the selected\nnumber of clusters by BIC and the slope heuristic\nover 100 simulations for each of the 12 DFM models. It turns out that\nalthough BIC can be very efficient when the model is appropriate, it\ncan provide unsatisfactory results in more difficult inference\nsituations. Conversely, the slope heuristic appears to be more\nconsistent in the selection of the number of clusters while keeping\nvery good overall results. For this reason, the selection of models and\nthe number of groups will be addressed in the following section with\nthe slope heuristic.\n\n\n\\begin{table}\n\\caption{Number of clusters selected by BIC\nover 100\nsimulations for the 12 DFM models. Actual value for $K$ is 4}\n\\label{tab:simul-result}\n\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lccccccccc@{}}\n\\hline\n& \\multicolumn{9}{c@{}}{\\textbf{Number} $\\bolds{K}$ \\textbf{of clusters}}\\[-6pt]\n& \\multicolumn{9}{c@{}}{\\hrulefill}\\\\\n\\textbf{Model} & \\textbf{2} & \\textbf{3} & \\textbf{4} & \\textbf{5} &\n\\textbf{6} & \\textbf{7} & \\textbf{8} & \\textbf{9} & \\textbf{10}\\\\\n\\hline\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ & 0 & 0 & \\phantom{0}\\textbf{99} &\n \\phantom{0}0 &\n\\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}1 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ & 0 & 0 & \\phantom{0}27 & \\textbf{37} &\n 23\n& 12 & \\phantom{0}1 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$ & 0 & 0 & \\textbf{100} & \\phantom{0}0 &\n\\phantom{0}0 &\n\\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta]}$ & 0 & 0 & \\phantom{00}2 & \\phantom{0}2 & \\phantom{0}8 & 10 & 10 & 10 &\n\\textbf{58}\\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta_{k}]}$ & 0 & 0 & \\textbf{100} & \\phantom{0}0\n& \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ & 0 & 0 & \\phantom{00}1 & \\phantom{0}5 &\n\\phantom{0}8 & 12 & 10 &\n\\phantom{0}7 & \\textbf{57}\\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$ & 0 & 0 & \\textbf{100} & \\phantom{0}0 &\n\\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta]}$ & 0 & 0 & \\phantom{00}0 & \\phantom{0}0 &\n\\phantom{0}1 & \\phantom{0}1 & \\phantom{0}4 & \\phantom{0}7 &\n\\textbf{87}\\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta_{k}]}$ & 0 & 0 & \\textbf{100} & \\phantom{0}0\n& \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta]}$ & 0 & 0 & \\phantom{0}\\textbf{91} &\n \\phantom{0}5 & \\phantom{0}1 &\n\\phantom{0}1 & \\phantom{0}1 & \\phantom{0}0 & \\phantom{0}1 \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta_{k}]}$ & 0 & 0 & \\textbf{100} & \\phantom{0}0 & \\phantom{0}0 &\n\\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta]}$ & 0 & 0 & \\phantom{0}\\textbf{97} &\n\\phantom{0}2 & \\phantom{0}1 & \\phantom{0}0 &\n\\phantom{0}0 & \\phantom{0}0 & \\phantom{0}0 \\\\\n\\hline\n\\end{tabular*}\n\n\\end{table}\n\n\n\\begin{table}[b]\n\\caption{Number of clusters selected by the slope\nheuristic over 100 simulations for the 12 DFM models. Actual value for\n$K$ is 4}\n\\label{tab:simul-result-1}\n\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lccccccccc@{}}\n\\hline\n& \\multicolumn{9}{c@{}}{\\textbf{Number} $\\bolds{K}$ \\textbf{of clusters}}\\[-6pt]\n& \\multicolumn{9}{c@{}}{\\hrulefill}\\\\\n\\multicolumn{1}{@{}l}{\\textbf{Model}} & \\multicolumn{1}{c}{\\textbf{2}} & \\multicolumn{1}{c}{\\textbf{3}} &\n\\multicolumn{1}{c}{\\textbf{4}} & \\multicolumn{1}{c}{\\textbf{5}} & \\multicolumn{1}{c}{\\textbf{6}} &\n\\multicolumn{1}{c}{\\textbf{7}} & \\multicolumn{1}{c}{\\textbf{8}} & \\multicolumn{1}{c}{\\textbf{9}} &\n\\multicolumn{1}{c@{}}{\\textbf{10}}\\\\\n\\hline\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ & \\phantom{0}6 & 9 & \\textbf{84} &\n\\phantom{0}0 &\n\\phantom{0}0 & 0 & 0 & 1 & 0 \\\\\n$\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ & 15 & 1 & \\textbf{81} & \\phantom{0}3 & \\phantom{0}0\n& 0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$ & \\phantom{0}0 & 0 & \\textbf{91} &\\phantom{0}8 & \\phantom{0}1 &\n0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\Sigma\\beta]}$ & \\phantom{0}0 & 0 & \\textbf{77} & 17 & \\phantom{0}5 & 1\n& 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta_{k}]}$ & \\phantom{0}0 & 0 & \\textbf{97} & \\phantom{0}3\n& \\phantom{0}0 & 0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ & \\phantom{0}0 & 0 & \\textbf{65} & 17 &\n14 & 3 & 1 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$ & \\phantom{0}0 & 0 & \\textbf{85} & 14\n& \\phantom{0}1 & 0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{k}\\beta]}$ & \\phantom{0}0 & 0 & \\textbf{78} & 14 & \\phantom{0}7\n& 1 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta_{k}]}$ & \\phantom{0}0 & 1 & \\textbf{87} & 11\n& \\phantom{0}1 & 0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha_{j}\\beta]}$ & \\phantom{0}0 & 0 & \\textbf{67} &\n\\phantom{0}8 & \\phantom{0}6 &\n6 & 4 & 3 & 6 \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta_{k}]}$ & \\phantom{0}4 & 0 & \\textbf{96} &\n\\phantom{0}0 & \\phantom{0}0 &\n0 & 0 & 0 & 0 \\\\\n$\\mathrm{DFM}_{[\\alpha\\beta]}$ & \\phantom{0}0 & 0 & \\textbf{87} &\n\\phantom{0}6 & \\phantom{0}4 & 2 &\n1 & 0 & 0 \\\\\n\\hline\n\\end{tabular*}\n\n\\end{table}\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f07.eps}\n\n\\caption{Simulated curves with cubic spline\nsmoothing (left) and Fourier basis smoothing (right).} \\label{fig:simul-curves2}\n\\end{figure}\n\n\n\n\n\n\n\\subsection{Selection of discriminative basis functions}\\label{sec4.2}\nThis experiment is concerned with the selection of the discriminative\nbasis functions, that is, the most relevant ones for\ndiscriminating the clusters. In this work, the selection of the\ndiscriminative basis functions is viewed as solving the optimization\nproblem (\\ref{eq:FisherCrit}) of the F step under sparsity constraints\n(i.e., such that the loading matrix $U$ contains as many zeros\nas possible). To evaluate the ability of our approach to select the\nrelevant discriminative basis functions, we consider now a simulation\nsetting in which two primarily different frequencies are involved. The\nsimulation setup is as follows:\n\n\\begin{eqnarray*}\n&&\\mbox{Cluster 1:}\\quad X(t)  =  U+(1-U)h_{1}(t)+{\\varepsilon}(t),\\qquad t\\in[1,21],\n\\\\\n&&\\mbox{Cluster 2:}\\quad X(t)  =  U+(1-U)h_{2}(t)+{\\varepsilon}(t),\\qquad t\\in[1,21],\n\\\\\n&&\\mbox{Cluster 3:}\\quad X(t)  =  U+(1-U)\\cos(2t)+{\\varepsilon} (t),\\qquad t\\in[1,21],\n\\\\\n&&\\mbox{Cluster 4:}\\quad X(t)  =  U+(1-U)\\sin(2t-2)+{\\varepsilon} (t),\\qquad t\\in[1,21],\n\\end{eqnarray*}\n\nwhere $U$, ${\\varepsilon}(t)$, $h_{1}$, $h_{2}$, the mixing proportions\nwhere $U$, ${\\varepsilon}(t)$, $h_{1}$, $h_{2}$, the mixing proportions and\nthe observation points are the same as in the previous simulation\nsetting. The functional form of the data is reconstructed using both\nFourier basis smoothing (with 25 basis functions) and a cubic spline\nbasis (with 50 basis functions). Figure~\\ref{fig:simul-curves2} plots\nthe simulated curves, respectively smoothed on cubic splines and\nFourier basis functions.\nStarting from the partition estimated with FunFEM and the $\\mathrm\n{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ model, the sparse version of the\nalgorithm is launched with the sparsity parameter $\\lambda=0.1$ on\nboth Fourier and spline smoothed curves.\n\n\n\nFigures \\ref{fig:simul-curves3} and \\ref{fig:simul-curves3-1} plot\nthe selected basis functions on both spline and Fourier bases. For the\nFourier basis, the selection of the basis functions indicates which\nperiodicity in the observed curves are the most discriminative,\nwhereas, for the spline smoothing, it indicates which time intervals\nare the most discriminant.\nOn the one hand, for the Fourier basis, the sparse version of FunFEM\nselects only two discriminative periodicities over the $25$ original\nbasis functions (left panel of Figure~\\ref{fig:simul-curves3}). The\nselected basis functions turn out to be relevant because they actually\ncorrespond to the two periodicities present in the simulated data. The\nright panel of the figure plots the smoothed curves on the two selected\nbasis functions. One can observe that the basis selection is actually\nrelevant because the main features of the data are kept.\n\nOn the other hand, for the spline basis, sparse FunFEM has selected\nthree basis functions among the 25 original ones (left panel of Figure~\\ref{fig:simul-curves3-1}). The three selected functions indicate the\nmost discriminative time intervals. Those time intervals are reported\non the right panel of the figure in addition to the curves. One can,\nfor instance, note that the first (from the left) selected function\ndiscriminates the green clusters from the three other groups.\nSimilarly, the second discriminative function allows for separating the\nblack and green clusters from the blue and red curves. Finally, the\nlast selected function aims at discriminating the black group from the others.\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f08.eps}\n\n\\caption{Discriminative functions among the\nFourier basis functions: selected basis functions (left) and data\nprojected on the selected basis functions (right).} \\label{fig:simul-curves3}\n\\end{figure}\n\n\n\\begin{figure}[b]\n\n\\includegraphics{861f09.eps}\n\n\\caption{Discriminative functions among\nthe spline basis functions: selected basis functions (left) and\noriginal data with, highlighted in grey, the time periods associated\nwith the selected basis functions (right).}\\label{fig:simul-curves3-1}\n\\end{figure}\n\n\n\n\\begin{table}\n\\caption{Clustering accuracies (in percentage) on the Kneading,\nFace, ECG and Wafer data sets for FunFEM and state-of-the-art methods.\nBold results correspond to best clustering accuracies and the stars\nindicate the DFM model selected by BIC}\n\\label{Table_3}\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lcccc@{}}\n\n\\hline\n\\multicolumn{1}{@{}l}{\\textbf{Method}} & \\multicolumn{1}{c}{\\textbf{Kneading}} & \\multicolumn{1}{c}{\\textbf{ECG}} & \\multicolumn{1}{c}{\\textbf{Face}} &\n\\multicolumn{1}{c@{}}{\\textbf{Wafer}}\\\\\n\\hline\nkmeans-$d_{0}$ & 62.61 & 74.50 & 48.21 & 63.34\\\\\nkmeans-$d_{1}$ & 64.35 & 61.50 & 34.80 & 62.53\\\\\nFunclust & 66.96 & \\textbf{84.00} & 33.03 & 63.10\\\\\nFunHDDC & 62.61 & 75.00 & 57.14 & 63.41\\\\\nFclust & 64.00 & 74.50 & -- & --\\\\\nCurvclust & 65.21 & 74.50 & 58.92 & 63.30\\[3pt]\nFunFEM $\\mathrm{DFM}_{[\\Sigma_{k}\\beta_{k}]}$ & 67.74 & 71.00 &\n59.82 & \\textbf{66.89}\\\\\nFunFEM $\\mathrm{DFM}_{[\\Sigma_{k}\\beta]}$ & \\textbf{70.97} & 73.00\n& 54.46 & 64.10\\\\\nFunFEM $\\mathrm{DFM}_{[\\Sigma\\beta_{k}]}$ & 67.74 & 72.00 & \\textbf\n{61.60} & 66.35\\\\\nFunFEM $\\mathrm{DFM}_{[\\Sigma\\beta]}$ & 66.66 & 75.00 & 54.46 &\n64.17\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{kj}\\beta_{k}]}$ & 67.74 &  \\phantom{*}71.00{*} &\n \\phantom{*}53.57{*} & \\textbf{66.89}\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ & \\textbf{70.97} & 73.50\n& 54.46 & 64.10\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{k}\\beta_{k}]}$ & 67.74 & 71.00 &\n53.57 & \\phantom{*}\\textbf{66.89{*}}\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{k}\\beta]}$ & \\textbf{70.97} & 73.00\n& 57.14 & 64.10\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{j}\\beta_{k}]}$ & 67.74 & 72.00 &\n55.35 & 66.40\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha_{j}\\beta]}$ & 66.66 & 75.00 & 53.57 &\n64.17\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha\\beta_{k}]}$ &  \\phantom{*}67.74{*} & 72.00 &\n53.57 & 66.40\\\\\nFunFEM $\\mathrm{DFM}_{[\\alpha\\beta]}$ & 66.66 & 75.00 & 56.25 &\n64.17\\\\\n\\hline\n\\end{tabular*}\n\n\\end{table}\n\n\n\n\n\\subsection{Comparison with state-of-the-art methods}\\label{sec4.3}\n\nThis last numerical study aims at comparing the FunFEM algorithm with\nstate-of-the-art methods on four real data sets that are commonly used\nin the functional clustering literature. The data sets considered here\nare as follows: the \\textit{Kneading}, \\textit{ECG}, \\textit{Face},\nand \\textit{Wafer} data sets. Appendix \\ref{App-A} provides a\ndetailed description of those data sets.\n\nFunFEM is here compared with the six state-of-the-art methods:\nkmeans-$d_{0}$ and kmeans-$d_{1}$ [\\citet{Iev2012}], funclust [\\citet\n{Jac2013}], funHDDC [\\citet{Bou2011}], fclust [\\citet{Jam2003}] and\ncurvclust [\\citet{Gia2012}]. The two kmeans-based methods use,\nrespectively, the $L_2$-metric between curves (kmeans-$d_0$) and\nbetween their derivatives (kmeans-$d_1$). The four other methods assume\na probabilistic modeling. Funclust assumes a Gaussian distribution for\nthe functional principal components scores, whereas funHDDC, fclust and\ncurvclust directly model the basis expansion coefficients.\n\nTable~\\ref{Table_3} presents the clustering accuracies (according to\nthe known labels) on the four data sets for FunFEM and the six\nclustering methods. FunFEM turns out to be very competitive with its\nchallengers on those data sets. FunFEM outperforms the other methods on\nall data sets except the second one where it is the second best method.\nOn the kneading, ECG and wafer sets, the improvement over\nstate-of-the-art methods is significant. It is also worth noticing that\nthe model selected by BIC (the model associated with the higher BIC\nvalue) often provides some of the best possible results.\n\n\n\\section{Analysis of bike sharing systems}\\label{sec5}\n\nThis section now presents the results of the application of FunFEM to\none month of stock data from eight bike sharing systems (Managed by\nJCDecaux Ciclocity and Serco) in Europe. As explained in the\n\\hyperref[sec1]{Introduction}, clustering is a principal way to summarize the behavior\nof BSS stations, and this approach has already been used in the\nliterature. This study proposes going further here. The FunFEM\nalgorithm presents a few advantages compared to existing works for\ndealing with the BSS data considered here and for comparing the eight\nstudied systems. First, conversely to previous works, FunFEM explicitly\naddresses the functional nature of BSS stock data and, as we saw\nearlier, it outperforms multivariate and functional clustering\ntechniques in most situations. FunFEM is therefore expected to perform\nwell on the BSS data and to provide meaningful clusters from the\noperational point of view. Second, FunFEM is able to easily handle\nlarge data sets, in term of time points, due to its parsimonious\nmodeling. This is an important point here because we consider time\nseries over one month (1448 time points, cf. Section~\\ref{sec2}). Last\nbut not least, FunFEM helps visualize the clustered data into a\ndiscriminative subspace. As we will see, this specific feature will be\nparticularly informative when analyzing the clustering results on the\nBSS data. The visualization of the different cities within the\ndiscriminative subspace will allow us to identify the systems with\noperating issues and to propose practical solutions to improve those systems.\n\n\n\n\n\\subsection{Clustering results for Paris stations}\\label{sec5.1}\n\nWe first begin the data analysis with solely the Paris stations. The\nFunFEM algorithm has been applied on the data with a varying number of\nclusters, from $2$ to $40$, and using the $\\mathrm{DFM}_{[\\alpha\n_{kj}\\beta]}$ model. This model was selected based on the good results\nit obtained in the simulation study we performed. Note that it would\nalso be possible to test all models and select the most appropriate one\nfor the data using model selection. We, however, use BIC, AIC and slope\nheuristic criteria to choose an appropriate value for the number $K$ of\nclusters. BIC and AIC provided hard-to-use values for $K$ because even\nfor 40 clusters, they do not reach a maximum. Conversely, the slope\nheuristic gave a satisfying value for $K$ because it reaches its\nmaximum for $K=10$. Figure~\\ref{fig:llparis} shows the evolution of\nthe log-likelihood with respect to the model dimensionality and the\nassociated slope heuristic criterion. On the right panel, the slope\nheuristic criterion peaks at $K=10$, which corresponds to an elbow in\nthe log-likelihood function: Above this value, the gain in\nlog-likelihood is linear with respect to the model dimensionality.\nThis value of $K$ was used for the cluster analysis. The mean profiles\nof the obtained clusters are depicted in Figure~\\ref{fig:clusterprotoparis}, together with the cluster proportions and a\nsample of curves that belong to each cluster.\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f10.eps}\n\n\\caption{Model selection plots for Paris:\nlog-likelihood with respect to model dimensionality and its estimated\nlinear part (left), slope heuristic criterion with respect to $K$ (right).} \\label{fig:llparis}\n\\end{figure}\n\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f11.eps}\n\n\\caption{Cluster mean profiles together\nwith 1000 randomly sampled curves. The name of the clusters and their\nproportions are also provided.}  \\label{fig:clusterprotoparis}\n\\end{figure}\n\n\nThe obtained clusters are fairly balanced, with approximately ten\npercent of the stations in each. The clusters are also easily\ndistinguishable. The stations of the first two clusters get bikes\nduring the afternoon and the evening. These stations differ during the\nweekend; the first cluster presents high values throughout this period,\nwhereas the second cluster experiences a lack of bikes on Saturday mornings.\nTaking into account these observations, we named the first cluster\n\\textit{Afternoon}, \\textit{Weekend} and the second \\textit{Afternoon} as a\nreference to the periods where these stations are full. The next two\nclusters present a phase opposition with respect to the previous ones;\nthese stations are full at the end of the morning rush hour\n(approximately 9 a.m.). Because these two clusters differ in their\nweekend behavior, we named the first one \\textit{Morning} because\nthese stations are almost empty throughout the entire weekend, and we\nnamed the second one \\textit{Morning}, \\textit{Weekend} because bikes are\navailable at these stations for a good part of the weekend. The next\ntwo clusters do not present the same types of variations; their loading\nprofiles are considerably stable throughout the week. The difference is\nin the level of fullness, with one cluster loading at approximately\n0.85 and one at approximately 0.7. The first cluster also presents day\nvariations that are not visible for the second one. We named these\nclusters \\textit{Full} and \\textit{Almost Full}.\n\n\n\\begin{figure}\n\n\\includegraphics{861f12.eps}\n\n\\caption{Map of the clustering results for Paris\nstations.}  \\label{fig:mapparis}\n\\end{figure}\n\n\n\nClusters 7 and 8 present overall small activity: Cluster 7 get bikes at\nnight, but this does not saturate the stations that reach a balanced\nstate in these time periods. This phenomenon may be due to the\nreallocation journey performed by the operator to balance the system at\nnight. Cluster 8 oscillates around a balanced state, receiving slightly\nmore bikes during the afternoons. Taking into account these remarks, we\ncall these clusters \\textit{Night rebalancing} and \\textit{Balanced}.\nFinally, clusters 9 and 10 gather stations that are almost empty\nthroughout the week. Cluster 9 presents considerably stable behavior\nwith a constant loading profile of approximately 0.25, whereas the\nsecond one smoothly oscillates at approximately 0.1. We respectively\ncall these clusters \\textit{Almost empty} and \\textit{Empty}.\n\nTo complement this analysis of the clustering results, Figure~\\ref{fig:mapparis}\\footnote{Map build using the ggmap package for R [\\citet\n{Kahle2013}].} presents the spatial location of the clustering results.\nOne of the first things that catches the eye when looking at this\nfigure concerns the relatively good spatial organization of the\nresults, although this information was never used in the clustering\nprocess. Stations from the same clusters are frequently grouped\ntogether on the map. From a Parisian perspective, those results are\nnatural: The \\textit{Morning} and \\textit{Morning}, \\textit{Weekend} clusters\n(in green on the map) are located in areas with a high employment\ndensity, which therefore correspond to destinations during the morning\ncommute. This phenomenon explains why these stations experience a\nsaturation at the end of the morning rush hour. On the contrary, the\nblue clusters, which correspond to the \\textit{Afternoon} and \\textit\n{Afternoon}, \\textit{Weekend} clusters, are located in more residential\nneighborhoods with a higher population density. They therefore\ncorrespond to classical origins during the morning rush hour and lose\ntheir bikes during this time period. The stations that belong to these\nclusters are located in regions that are close to \\textit{Empty},\n\\textit{Almost empty} stations, which are more problematic from a user\nperspective. These neighborhoods are not in the hyper-center of Paris,\nand they are also located close to stations that belong to the \\textit\n{Night rebalancing} cluster. The \\textit{Night rebalancing} cluster is\nfrequently located in uphill locations, such as the ``Butte Montmarte,''\nthe ``P\\`ere Lachaise'' cemetery and the ``Butte Chaumont'' garden.\nFinally, the \\textit{Full} and \\textit{Almost full} stations are\nlocated in the center, whereas the \\textit{Balanced} stations are\nlocated primarily in the periphery of the system.\n\n\n\nIn comparison with previous results obtained based on Paris bike share\norigin/destination data, such as in \\citet{come2014}, these\nobservations are considerably consistent. One of the major differences\nconcerns parks and leisure locations, which do not emerge from the\nclustering in our study. This phenomenon may be explained by the\ndifference in the nature of the input data. The stock data that are\nused in this paper do not enable the differentiation of these stations,\nwhereas origin/destination data do. However, stock data are easier to\nobtain on a large scale and thus will allow cross-city comparisons,\nwhich is the subject of the next section.\n\n\n\\subsection{Clustering results on several cities}\\label{sec5.2}\n\n\nThe clustering was also performed on the entire data set, which\nincludes stations from the eight systems (see Table~\\ref{tab:bsslist}). The same methodology was used; the curves were\nprojected on the same Fourier basis, and, as prior, the clustering was\nperformed with the model $\\mathrm{DFM}_{[\\alpha_{kj}\\beta]}$ and\nwith a varying number of clusters, from 2 to 40. The slope heuristic\nleads to the same number of clusters ($K=10$ clusters) in this larger\ndata set. The obtained clusters are also close to those obtained only\nin Paris. Their profiles, which are supplied in the \\hyperref[app]{Appendix}, are close\nto those shown in Figure~\\ref{fig:clusterprotoparis}, and their\ninterpretation does not differ significantly. We kept the same labels\nfor the clusters because the main difference comes from the amplitude\nof the profile variations, which are smaller in the entire data set. An\ninteresting point in the obtained results concerns the proportions of\nthe different clusters for each city. This indeed enables an aggregate\nview of the systems that eases their comparison. These proportions are\nshown in Figure~\\ref{fig:clusterprop}.\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f13.eps}\n\n\\caption{Cluster proportions by city.} \\label{fig:clusterprop}\n\\end{figure}\n\n\n\n\nDifferences between the cities are visible in this figure. The\nproportion of the \\textit{Night rebalancing} cluster is, for example,\nmuch more important for Paris than in any other city. This cluster,\nwhich corresponds to stations that are rebalanced during the night, is\nnot visible in cities other than Paris. On the contrary, the proportion\nof the \\textit{Balanced} cluster is much smaller in Paris than in the\nother cities. Another clear difference concerns the \\textit{Empty} and\n\\textit{Almost empty} cluster stations, which are important in\nMarseille and Bruxelles. In Marseille, the \\textit{Full} and \\textit\n{Almost full} clusters are also over-represented, corresponding to more\nthan 25\\% of the city stations. This system seems, therefore, the more\nunbalanced system with many stations frequently full or empty.\nConversely, the cities on the left of the plot, such as Valencia or\nLyon, seem to be more active and balanced, with an important proportion\nof stations that belong to the \\textit{Afternoon} and \\textit\n{Morning} clusters. This aggregate view helps identify the BSSs that do\nnot have satisfying behavior from the exploitation point of view.\nIndeed, Bruxelles and Marseille have exploitation profiles with low or\neven very low proportions of the active clusters (\\textit\n{Afternoon}, \\textit{WE}, \\textit{Afternoon}, \\textit{Morning},\n\\textit{WE} and \\textit\n{Morning}). Conversely, the BSS of Valencia, Lyon and London seem to be\nthe most efficient systems. Some of the factors that may explain these\nbehaviors are the ratio between bikes and docks, the topography and\ngeography of the cities and the bike redistribution policy.\n\n\n\\begin{figure}\n\n\\includegraphics{861f14.eps}\n\n\\caption{Bike stations projected into the two\nfirst axes of the discriminative functional subspace. Colors indicate\nthe cluster memberships.}  \\label{fig:disc-axis}\n\\end{figure}\n\n\n\\begin{figure}[b]\n\n\\includegraphics{861f15.eps}\n\n\\caption{Bike stations projected into the\nthird and fourth axes of the discriminative functional subspace. Colors\nindicate the cluster memberships.} \\label{fig:disc-axis-bis}\\vspace*{6pt}\n\\end{figure}\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f16.eps}\n\n\\caption{Density of bike stations per\ncity projected into the two first axes of the discriminative functional\nsubspace.}\\label{fig:disc-axis-facet}\n\\end{figure}\n\n\nThe observations made on the cluster proportions from Figure~\\ref{fig:clusterprop} can be confirmed by looking at the discriminative\nfunctional subspace estimated by FunFEM. Figure~\\ref{fig:disc-axis}\nshows the bike stations of the 8 cities projected into the two first\naxes of the discriminative subspace. Figure~\\ref{fig:disc-axis-bis}\nshows the projection into the third and fourth discriminative axes. The\ncolors indicate the cluster memberships of the stations. It may first\nbe useful to interpret the discriminative axes from the cluster\nmeanings. The first axis puts in opposition the \\textit{Full} and\n\\textit{Empty} clusters and can be therefore viewed as a station\nloading axis. The second axis opposes the \\textit{Afternoon} and\n\\textit{Morning} clusters. It can therefore be linked with the phase\nof the curves. The third and fourth axes are less interpretable and\nseem primarily linked with the \\textit{Night rebalancing} cluster.\nKnowing the meaning of the discriminative subspace axes enables the\ncomparison of the studied systems through the analysis of their station\nbehaviors. Figure~\\ref{fig:disc-axis-facet}\\ shows the projection of\nthe bike stations for each city on the two first axes of the\ndiscriminative subspace. A kernel density estimation is also proposed\nto visualize the relative density of stations in this subspace. This\nvisual representation confirms the first comparison results of the\ncluster proportions. In particular, Marseille and Bruxelles present a\ndistribution in the discriminative subspace that is considerably\ndifferent from that of other cities. Indeed, both are oriented along\nthe first discriminative axis and do not present significant variations\nalong the second axis. The signature of those two cities within the\nsubspace can be qualified as problematic from an operational point of\nview because the first discriminative axis opposes the \\textit{Full}\nand \\textit{Empty} clusters, whereas the second axis is associated\nwith the \\textit{Afternoon} and \\textit{Morning} clusters.\n\nThe spatial analysis of the results was also performed by mapping the\nclustering results (see Figure~\\ref{fig:mapall} in\nAppendix \\ref{appb}). As\nwith Paris, it turns out that the different clusters are also\nfrequently spatially clustered. Furthermore, the same type of global\norganization is visible for the different cities. Stations from the\n\\textit{Morning} clusters are located in the center of the systems,\nwhereas the other clusters are located in the periphery of the system.\n\n\n\n\\subsection{Recommendations for BSS operators}\\label{sec5.3}\n\n\n\nIn light of the analyses and comparisons made above, it is possible to\nmake some recommendations for BSS operators regarding system structures\nand policies. On the one hand, the BSS systems of Marseille and\nBruxelles appear to be composed primarily of \\textit{Full} and \\textit\n{Empty} stations, which necessarily implies user dissatisfaction.\nPossible ways to improve these situations would be either to use a\n``bonus'' policy or to increase the rebalancing performed by the\noperator. The ``bonus'' policy is attractive for both users and\nproviders. It consists of offering extra free minutes of bike usage to\nusers willing to return the bike to elevated stations. In theory, this\nstrategy should help rebalance the system. Bonus stations, for\ninstance, are available in Paris and Bruxelles. Thanks to our\ndiscriminative subspace, it is in fact possible to check the real\neffect of such a policy. Figure~\\ref{fig:Bonus} shows the density of\n``bonus'' and regular stations within the two first axes of the\ndiscriminative functional subspace. It appears that the effect of the\nbonus policy is globally limited because there is no significant\ndistribution difference between the regular and bonus statuses of the\n\\textit{Full} stations (stations projected on the left of the first\ndiscriminative axis). We therefore recommend to BSS operators to either\nmodify their bonus policies (e.g., extra time bonus, cash reward) or to\nincrease the nighttime rebalancing of the stations for those two cities.\n\n\n\n\\begin{figure}\n\n\\includegraphics{861f17.eps}\n\n\\caption{Density of ``bonus'' (left) and regular\nstations (right) projected into the two first axes of the\ndiscriminative functional subspace.} \\label{fig:Bonus}\\vspace*{9pt}\n\\end{figure}\n\n\nHowever, the comparison of the largest and most efficient systems has\nhighlighted some weaknesses of the Paris system. Although the Velib\nsystem is one of the largest and most popular systems in the world, it\nalso appears to have too many \\textit{Full} and \\textit{Empty}\\vadjust{\\goodbreak}\nstations, particularly compared to London. The night rebalancing\noperated by JCDecaux seems to be efficient but not sufficient to\ncompletely solve this issue. As we have seen, shared bikes are used\nprimarily for home-work journeys, and the stations from the \\textit\n{Afternoon} and \\textit{Morning} clusters therefore play a key role in\nthe system efficiency. This situation emphasizes the importance of\ncommutes in the use of the service, and city bike policies must\nseriously consider this aspect when designing bike paths. London's\n``Cycle Superhighways'' initiative, which connects suburbs with the city\ncenter, seems particularly effective with respect to this point in our\nanalyses. Those specific bike paths indeed connect stations from the\n\\textit{Afternoon} and \\textit{Morning} clusters (see Appendix \\ref{appb}). We\ntherefore recommend to city planners to develop bike paths in a similar\nway to improve the performance of system commutes.\n\n\n\\section{Conclusion}\\label{sec6}\n\nThis work was motivated by interest in analyzing and comparing several\nEuropean BSSs to identify common operating patterns and to propose\npractical solutions to avoid potential issues. To this end, the\ndiscriminative functional mixture (DFM) model was proposed to model the\nfunctional data generated by the systems. In this framework, the data\nare modeled into a discriminative functional subspace. The FunFEM\nalgorithm has been proposed for the inference of the DFM model. The\nselection of the most discriminative basis functions can also be made\nafterward by introducing sparsity through a $\\ell_1$-type\npenalization. Numerical experiments have demonstrated the efficiency of\nthe proposed clustering technique for both simulated and benchmark\ndata. FunFEM appears to be a good challenger to the best\nstate-of-the-art methods. The numerical experiments have also shown the\ngood behavior of the ``slope heuristic'' for model selection in this\ncontext.\n\nThe proposed methodology has been applied to one-month usage statistics\nof 8 bike sharing systems. FunFEM presents several advantages over\nexisting works for analyzing and comparing bike sharing systems. FunFEM\nbenefits from its parsimonious modeling and its discriminative\nsubspace. The obtained results were easily interpretable and useful to\nobtain a compact representation of BSS system behaviors. In particular,\nthe discriminative subspace appears to be a useful tool to compare the\ndifferent systems with regard to the identified operating patterns.\nRecommendations to BSS operators are made based on the clustering\nresults.\n\nFinally, the discriminative subspace offers an interesting tool from an\noperational point of view to track changes in the behavior of bike\nstations. Using a sliding window and projecting the station functional\ndescription within this window into the discriminative subspace, one\nmay obtain a trajectory for each station within the subspace, allowing\nfor the detection of any changes in the station behavior. This may be\nuseful when trying new pricing or bonus policies to check their effects\non the system.\n\n\n\n\n\\begin{appendix}\\label{app}\n\n\\section{Additional information about the benchmark data sets}\\label{App-A}\nThe Kneading data set [\\citet{Lev2004}] comes from Danone VitaPole\nParis Research Center and concerns the quality of cookies and the\nrelationship with the flour kneading process. There are 115 different\nflours for which the dough resistance is measured during the kneading\nprocess for 480 seconds. The data set contains 115 kneading curves\nobserved at 241 equispaced instants of time in the interval $[0, 480]$.\nThe $115$ flours produce cookies of different quality: $50$ of them\nproduced cookies of \\textit{good} quality, $25$ produced \\textit\n{medium} quality, and $40$ produced \\textit{low} quality. Following\n\\citet{Lev2004,Pre2007}, least squares approximation based on cubic\nB-spline functions (with 18 knots) is used to reconstruct the true\nfunctional form of each sample curve. The ECG, Face and Wafer data sets\nare benchmarks taken from the \\textit{UCR Time Series Classification\nand Clustering} website.\\footnote{\\url{http://www.cs.ucr.edu/\\textasciitilde eamonn/time\\_series\\_data/}.} The ECG data set consists of 200\nelectrocardiograms from 2 groups of patients sampled at 96 time\ninstants and has already been studied in \\citet{Ols2001}. The Face data\nset [\\citet{Xi2006}] consists of 112 curves sampled from 4 groups at\n350 instants of time. The Wafer data set [\\citet{Ols2001}] consists of\n7174 curves sampled from 2 groups at 152 instants of time. For these\nthree data sets, the same basis of functions as for the kneading data\nset has been arbitrarily chosen (20 cubic B-splines).\n\\newpage\n\n\n\\section{Detailed clustering results on the 8 BSS}\\label{appb}\\vspace*{-15pt}\n\n\\setcounter{figure}{0}\n\\begin{figure}[b]\n\n\\includegraphics[scale=0.99]{861f18.eps}\n\n\\caption{Cluster mean profiles together with\n1000 randomly sampled curves for the whole data set (Paris, London,\nBruxelles, Lyon, Valencia, Sevilla and Nantes).} \\label{fig:clustall}\n\\end{figure}\n\n\n\\begin{figure}\n\n\\includegraphics{861f19.eps}\n\n\\caption{Maps of the clustering results (from left\nto right and top to bottom) for Paris, London, Bruxelles, Lyon,\nValencia, Sevilla and Nantes.} \\label{fig:mapall}\n\\end{figure}\n\n\\end{appendix}\n\n\\mbox{}\n\\newpage\n\\section*{Acknowledgments}\nThe authors would like to thank the Editors and the reviewers for their\nmeaningful comments which have greatly contributed to improving the manuscript.\n\n\n\n\n\n\n\n\\begin{thebibliography}{42}\n\n\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Akaike}{1974}]{Akaike74}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Akaike},~\\bfnm{Hirotugu}\\binits{H.}}\n(\\byear{1974}).\n\\btitle{A new look at the statistical model identification}.\n\\bjournal{IEEE Trans. Automat. Control}\n\\bvolume{19}\n\\bpages{716--723}.\n\n\\bid{issn={0018-9286}, mr={0423716}}\n\\bptnote{check volume}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Baudry, Maugis and Michel}{2012}]{baudry2012slope}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Baudry},~\\bfnm{Jean-Patrick}\\binits{J.-P.}},\n\\bauthor{\\bsnm{Maugis},~\\bfnm{Cathy}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Michel},~\\bfnm{Bertrand}\\binits{B.}}\n(\\byear{2012}).\n\\btitle{Slope heuristics: Overview and implementation}.\n\\bjournal{Stat. Comput.}\n\\bvolume{22}\n\\bpages{455--470}.\n\\bid{doi={10.1007/s11222-011-9236-1}, issn={0960-3174}, mr={2865029}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Birg{\\'e} and Massart}{2007}]{birge2007minimal}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Birg{\\'e}},~\\bfnm{Lucien}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Massart},~\\bfnm{Pascal}\\binits{P.}}\n(\\byear{2007}).\n\\btitle{Minimal penalties for {G}aussian model selection}.\n\\bjournal{Probab. Theory Related Fields}\n\\bvolume{138}\n\\bpages{33--73}.\n\\bid{doi={10.1007/s00440-006-0011-8}, issn={0178-8051}, mr={2288064}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Borgnat et~al.}{2011}]{Borgnat11}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Borgnat},~\\bfnm{P.}\\binits{P.}},\n\\bauthor{\\bsnm{Robardet},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Rouquier},~\\bfnm{J.~B.}\\binits{J.~B.}},\n\\bauthor{\\bsnm{Parice},~\\bfnm{Abry}\\binits{A.}},\n\\bauthor{\\bsnm{Fleury},~\\bfnm{E.}\\binits{E.}} \\AND\n\\bauthor{\\bsnm{Flandrin},~\\bfnm{P.}\\binits{P.}}\n(\\byear{2011}).\n\\btitle{{Shared bicycles in a city: A signal processing and data analysis perspective}}.\n\\bjournal{Adv. Complex Syst.}\n\\bvolume{14}\n\\bpages{1--24}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Bouveyron and Brunet}{2012}]{Bouveyron12FEM}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bouveyron},~\\bfnm{Charles}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Brunet},~\\bfnm{Camille}\\binits{C.}}\n(\\byear{2012}).\n\\btitle{Simultaneous model-based clustering and visualization in the {F}isher discriminative subspace}.\n\\bjournal{Stat. Comput.}\n\\bvolume{22}\n\\bpages{301--324}.\n\\bid{doi={10.1007/s11222-011-9249-9}, issn={0960-3174}, mr={2865072}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Bouveyron and Brunet}{2014}]{Bouveyron14a}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bouveyron},~\\bfnm{C.}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Brunet},~\\bfnm{C.}\\binits{C.}}\n(\\byear{2014}).\n\\btitle{Discriminative variable selection for clustering with the sparse Fisher-EM algorithm}.\n\\bjournal{Comput. Statist.}\n\\bvolume{29}\n\\bpages{489--513}.\n\\bid{mr={3261825}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Bouveyron, Girard and Schmid}{2007}]{Bouveyron07b}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bouveyron},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Girard},~\\bfnm{S.}\\binits{S.}} \\AND\n\\bauthor{\\bsnm{Schmid},~\\bfnm{C.}\\binits{C.}}\n(\\byear{2007}).\n\\btitle{High-dimensional data clustering}.\n\\bjournal{Comput. Statist. Data Anal.}\n\\bvolume{52}\n\\bpages{502--519}.\n\\bid{doi={10.1016/j.csda.2007.02.009}, issn={0167-9473}, mr={2409998}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Bouveyron and Jacques}{2011}]{Bou2011}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Bouveyron},~\\bfnm{Charles}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Jacques},~\\bfnm{Julien}\\binits{J.}}\n(\\byear{2011}).\n\\btitle{Model-based clustering of time series in group-specific functional subspaces}.\n\\bjournal{Adv. Data Anal. Classif.}\n\\bvolume{5}\n\\bpages{281--300}.\n\\bid{doi={10.1007/s11634-011-0095-6}, issn={1862-5347}, mr={2860102}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Cadima and Jolliffe}{1995}]{Cadima1995}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Cadima},~\\bfnm{Jorge}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Jolliffe},~\\bfnm{Ian~T.}\\binits{I.~T.}}\n(\\byear{1995}).\n\\btitle{Loadings and correlations in the interpretation of principal components}.\n\\bjournal{J. Appl. Stat.}\n\\bvolume{22}\n\\bpages{203--214}.\n\\bid{doi={10.1080/757584614}, issn={0266-4763}, mr={1342655}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{C{\\^o}me and Oukhellou}{2014}]{come2014}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{C{\\^o}me},~\\bfnm{E.}\\binits{E.}} \\AND\n\\bauthor{\\bsnm{Oukhellou},~\\bfnm{L.}\\binits{L.}}\n(\\byear{2014}).\n\\btitle{Model-based count series clustering for bike-sharing system usage mining, a case study with the V\\'elib system of Paris}.\n\\bjournal{Transportation Research---Part C Emerging Technologies}\n\\bvolume{22}\n\\bpages{88}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Dell'Olio, Ibeas and Moura}{2011}]{Dellolio2011}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{Dell'Olio},~\\bfnm{L.}\\binits{L.}},\n\\bauthor{\\bsnm{Ibeas},~\\bfnm{A.}\\binits{A.}} \\AND\n\\bauthor{\\bsnm{Moura},~\\bfnm{J.~L.}\\binits{J.~L.}}\n(\\byear{2011}).\n\\btitle{Implementing bike-sharing systems}.\nIn \\bbooktitle{ICE---Municipal Engineer}\n\\bvolume{164}\n\\bpages{89--101}.\n\\bpublisher{ICE publishing},\n\\blocation{London}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Duda, Hart and Stork}{2001}]{Duda01}\n\\begin{bbook}[mr]\n\\bauthor{\\bsnm{Duda},~\\bfnm{Richard~O.}\\binits{R.~O.}},\n\\bauthor{\\bsnm{Hart},~\\bfnm{Peter~E.}\\binits{P.~E.}} \\AND\n\\bauthor{\\bsnm{Stork},~\\bfnm{David~G.}\\binits{D.~G.}}\n(\\byear{2001}).\n\\btitle{Pattern Classification},\n\\bedition{2nd} ed.\n\\bpublisher{Wiley},\n\\blocation{New York}.\n\\bid{mr={1802993}}\n\\end{bbook}\n\n\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Escabias, Aguilera and Valderrama}{2005}]{Esc2005}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Escabias},~\\bfnm{M.}\\binits{M.}},\n\\bauthor{\\bsnm{Aguilera},~\\bfnm{A.~M.}\\binits{A.~M.}} \\AND\n\\bauthor{\\bsnm{Valderrama},~\\bfnm{M.~J.}\\binits{M.~J.}}\n(\\byear{2005}).\n\\btitle{Modeling environmental data by functional principal component logistic regression}.\n\\bjournal{Environmetrics}\n\\bvolume{16}\n\\bpages{95--107}.\n\\bid{doi={10.1002/env.696}, issn={1180-4009}, mr={2146901}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Ferraty and Vieu}{2003}]{Fer2003}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Ferraty},~\\bfnm{F.}\\binits{F.}} \\AND\n\\bauthor{\\bsnm{Vieu},~\\bfnm{P.}\\binits{P.}}\n(\\byear{2003}).\n\\btitle{Curves discrimination: A nonparametric functional approach}.\n\\bjournal{Comput. Statist. Data Anal.}\n\\bvolume{44}\n\\bpages{161--173}.\n\n\\bid{doi={10.1016/S0167-9473(03)00032-X}, issn={0167-9473}, mr={2020144}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Fisher}{1936}]{Fisher36}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Fisher},~\\bfnm{R.~A.}\\binits{R.~A.}}\n(\\byear{1936}).\n\\btitle{The use of multiple measurements in taxonomic problems}.\n\\bjournal{Annals of Eugenics}\n\\bvolume{7}\n\\bpages{179--188}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Fraley and Raftery}{1999}]{Fraley99}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Fraley},~\\bfnm{C.}\\binits{C.}} \\AND\n\\bauthor{\\bsnm{Raftery},~\\bfnm{A.}\\binits{A.}}\n(\\byear{1999}).\n\\btitle{{MCLUST: Software for model-based cluster analysis}}.\n\\bjournal{J.~Classification}\n\\bvolume{16}\n\\bpages{297--306}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Froehlich, Neumann and Oliver}{2008}]{Froehlichmeasuring}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{Froehlich},~\\bfnm{J.}\\binits{J.}},\n\\bauthor{\\bsnm{Neumann},~\\bfnm{J.}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Oliver},~\\bfnm{N.}\\binits{N.}}\n(\\byear{2008}).\n\\btitle{Measuring the pulse of the city through shared bicycle programs}.\nIn \\bbooktitle{International Workshop on Urban, Community, and Social Applications of Networked Sensing Systems. UrbanSense08}\n\\bpages{16--20}.\n\\blocation{Raleigh, NC}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Froehlich, Neumann and Oliver}{2009}]{Froehlich2009}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{Froehlich},~\\bfnm{J.}\\binits{J.}},\n\\bauthor{\\bsnm{Neumann},~\\bfnm{J.}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Oliver},~\\bfnm{N.}\\binits{N.}}\n(\\byear{2009}).\n\\btitle{Sensing and predicting the pulse of the city through shared bicycling}.\nIn \\bbooktitle{21st International Joint Conference on Artificial Intelligence, IJCAI'09}\n\\bpages{1420--1426}.\n\\bpublisher{AAAI Press},\n\\blocation{Menlo Park, CA}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Fr{\\\"u}hwirth-Schnatter and Kaufmann}{2008}]{Fru2008}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Fr{\\\"u}hwirth-Schnatter},~\\bfnm{Sylvia}\\binits{S.}} \\AND\n\\bauthor{\\bsnm{Kaufmann},~\\bfnm{Sylvia}\\binits{S.}}\n(\\byear{2008}).\n\\btitle{Model-based clustering of multiple time series}.\n\\bjournal{J. Bus. Econom. Statist.}\n\\bvolume{26}\n\\bpages{78--89}.\n\\bid{doi={10.1198/073500107000000106}, issn={0735-0015}, mr={2422063}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\\\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Fukunaga}{1990}]{Fukunaga90}\n\\begin{bbook}[mr]\n\\bauthor{\\bsnm{Fukunaga},~\\bfnm{Keinosuke}\\binits{K.}}\n(\\byear{1990}).\n\\btitle{Introduction to Statistical Pattern Recognition},\n\\bedition{2nd} ed.\n\n\\bpublisher{Academic Press},\n\\blocation{Boston, MA}.\n\\bid{mr={1075415}}\n\\end{bbook}\n\n\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Giacofci et~al.}{2013}]{Gia2012}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Giacofci},~\\bfnm{M.}\\binits{M.}},\n\\bauthor{\\bsnm{Lambert-Lacroix},~\\bfnm{S.}\\binits{S.}},\n\\bauthor{\\bsnm{Marot},~\\bfnm{G.}\\binits{G.}} \\AND\n\\bauthor{\\bsnm{Picard},~\\bfnm{F.}\\binits{F.}}\n(\\byear{2013}).\n\\btitle{Wavelet-based clustering for mixed-effects functional models in high dimension}.\n\\bjournal{Biometrics}\n\\bvolume{69}\n\\bpages{31--40}.\n\\bid{mr={3058049}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Heard, Holmes and Stephens}{2006}]{Hea2006}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Heard},~\\bfnm{Nicholas~A.}\\binits{N.~A.}},\n\\bauthor{\\bsnm{Holmes},~\\bfnm{Christopher~C.}\\binits{C.~C.}} \\AND\n\\bauthor{\\bsnm{Stephens},~\\bfnm{David~A.}\\binits{D.~A.}}\n(\\byear{2006}).\n\\btitle{A quantitative study of gene regulation involved in the immune response of anopheline mosquitoes: An application of {B}ayesian hierarchical clustering of curves}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{101}\n\\bpages{18--29}.\n\\bid{doi={10.1198/016214505000000187}, issn={0162-1459}, mr={2252430}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Ieva et~al.}{2013}]{Iev2012}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Ieva},~\\bfnm{Francesca}\\binits{F.}},\n\\bauthor{\\bsnm{Paganoni},~\\bfnm{Anna~M.}\\binits{A.~M.}},\n\\bauthor{\\bsnm{Pigoli},~\\bfnm{Davide}\\binits{D.}} \\AND\n\\bauthor{\\bsnm{Vitelli},~\\bfnm{Valeria}\\binits{V.}}\n(\\byear{2013}).\n\\btitle{Multivariate functional clustering for the morphological analysis of electrocardiograph curves}.\n\\bjournal{J. R. Stat. Soc. Ser. C. Appl. Stat.}\n\\bvolume{62}\n\\bpages{401--418}.\n\\bid{doi={10.1111/j.1467-9876.2012.01062.x}, issn={0035-9254}, mr={3060623}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Jacques and Preda}{2013}]{Jac2013}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Jacques},~\\bfnm{J.}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Preda},~\\bfnm{C.}\\binits{C.}}\n(\\byear{2013}).\n\\btitle{Funclust: A curves clustering method using functional random variable density approximation}.\n\\bjournal{Neurocomputing}\n\\bvolume{112}\n\\bpages{164--171}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Jacques and Preda}{2014}]{Jac2014}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Jacques},~\\bfnm{Julien}\\binits{J.}} \\AND\n\\bauthor{\\bsnm{Preda},~\\bfnm{Cristian}\\binits{C.}}\n(\\byear{2014}).\n\\btitle{Model-based clustering for multivariate functional data}.\n\\bjournal{Comput. Statist. Data Anal.}\n\\bvolume{71}\n\\bpages{92--106}.\n\\bid{doi={10.1016/j.csda.2012.12.004}, issn={0167-9473}, mr={3131956}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{James and Sugar}{2003}]{Jam2003}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{James},~\\bfnm{Gareth~M.}\\binits{G.~M.}} \\AND\n\\bauthor{\\bsnm{Sugar},~\\bfnm{Catherine~A.}\\binits{C.~A.}}\n(\\byear{2003}).\n\\btitle{Clustering for sparsely sampled functional data}.\n\\bjournal{J. Amer. Statist. Assoc.}\n\\bvolume{98}\n\\bpages{397--408}.\n\\bid{doi={10.1198/016214503000189}, issn={0162-1459}, mr={1995716}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Kahle and Wickham}{2013}]{Kahle2013}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Kahle},~\\bfnm{David}\\binits{D.}} \\AND\n\\bauthor{\\bsnm{Wickham},~\\bfnm{Hadley}\\binits{H.}}\n(\\byear{2013}).\n\\btitle{ggmap: Spatial visualization with ggplot2}.\n\\bjournal{The R Journal}\n\\bvolume{5}\n\\bpages{144--161}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Lathia, Saniul and Capra}{2012}]{lathia12}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Lathia},~\\bfnm{Neal}\\binits{N.}},\n\\bauthor{\\bsnm{Saniul},~\\bfnm{A.}\\binits{A.}} \\AND\n\\bauthor{\\bsnm{Capra},~\\bfnm{L.}\\binits{L.}}\n(\\byear{2012}).\n\\btitle{Measuring the impact of opening the London shared bicycle scheme to casual users}.\n\\bjournal{Transportation Research Part C: Emerging Technologies}\n\\bvolume{22}\n\\bpages{88--102}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{L{\\'e}v{\\'e}der et~al.}{2004}]{Lev2004}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{L{\\'e}v{\\'e}der},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Abraham},~\\bfnm{P.~A.}\\binits{P.~A.}},\n\\bauthor{\\bsnm{Cornillon},~\\bfnm{E.}\\binits{E.}},\n\\bauthor{\\bsnm{Matzner-Lober},~\\bfnm{E.}\\binits{E.}} \\AND\n\\bauthor{\\bsnm{Molinari},~\\bfnm{N.}\\binits{N.}}\n(\\byear{2004}).\n\\btitle{Discrimination de courbes de pr\\`Etrissage}.\nIn \\bbooktitle{Chimiom\\`Etrie 2004}\n\\bpages{37--43}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Lin and Yang}{2011}]{Lin2011}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Lin},~\\bfnm{J.~R.}\\binits{J.~R.}} \\AND\n\\bauthor{\\bsnm{Yang},~\\bfnm{T.}\\binits{T.}}\n(\\byear{2011}).\n\\btitle{Strategic design of public bicycle sharing systems with service level constraints}.\n\\bjournal{Transportation Research Part E: Logistics and Transportation Review}\n\\bvolume{47}\n\\bpages{284--294}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Lindsay}{1995}]{Lindsay95}\n\\begin{bbook}[author]\n\\bauthor{\\bsnm{Lindsay},~\\bfnm{Bruce~G.}\\binits{B.~G.}}\n(\\byear{1995}).\n\\btitle{Mixture Models: Theory, Geometry and Applications}.\n\\bpublisher{IMS},\n\\blocation{Hayward, CA}.\n\\end{bbook}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Olszewski}{2001}]{Ols2001}\n\\begin{bmisc}[author]\n\\bauthor{\\bsnm{Olszewski},~\\bfnm{R.~T.}\\binits{R.~T.}}\n(\\byear{2001}).\n\\bhowpublished{Generalized feature extraction for structural pattern recognition in time-series data.\nPh.D. thesis,\nCarnegie Mellon Univ.,\nPittsburgh, PA}.\n\\end{bmisc}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Preda}{2007}]{Pre2007a}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Preda},~\\bfnm{Cristian}\\binits{C.}}\n(\\byear{2007}).\n\\btitle{Regression models for functional data by reproducing kernel {H}ilbert spaces methods}.\n\\bjournal{J. Statist. Plann. Inference}\n\\bvolume{137}\n\\bpages{829--840}.\n\\bid{doi={10.1016/j.jspi.2006.06.011}, issn={0378-3758}, mr={2301719}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Preda, Saporta and L{\\'e}v{\\'e}der}{2007}]{Pre2007}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Preda},~\\bfnm{Cristian}\\binits{C.}},\n\\bauthor{\\bsnm{Saporta},~\\bfnm{Gilbert}\\binits{G.}} \\AND\n\\bauthor{\\bsnm{L{\\'e}v{\\'e}der},~\\bfnm{Caroline}\\binits{C.}}\n(\\byear{2007}).\n\\btitle{P{LS} classification of functional data}.\n\\bjournal{Comput. Statist.}\n\\bvolume{22}\n\\bpages{223--235}.\n\\bid{doi={10.1007/s00180-007-0041-4}, issn={0943-4062}, mr={2318457}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Ramsay and Silverman}{2005}]{Ram2005}\n\\begin{bbook}[mr]\n\\bauthor{\\bsnm{Ramsay},~\\bfnm{J.~O.}\\binits{J.~O.}} \\AND\n\\bauthor{\\bsnm{Silverman},~\\bfnm{B.~W.}\\binits{B.~W.}}\n(\\byear{2005}).\n\\btitle{Functional Data Analysis},\n\\bedition{2nd} ed.\n\n\\bpublisher{Springer},\n\\blocation{New York}.\n\\bid{mr={2168993}}\n\\end{bbook}\n\n\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Ray and Lindsay}{2008}]{Lindsay2008}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Ray},~\\bfnm{Surajit}\\binits{S.}} \\AND\n\\bauthor{\\bsnm{Lindsay},~\\bfnm{Bruce~G.}\\binits{B.~G.}}\n(\\byear{2008}).\n\\btitle{Model selection in high dimensions: A quadratic-risk-based approach}.\n\\bjournal{J. R. Stat. Soc. Ser. B. Stat. Methodol.}\n\\bvolume{70}\n\\bpages{95--118}.\n\\bid{doi={10.1111/j.1467-9868.2007.00623.x}, issn={1369-7412}, mr={2412633}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Ray and Mallick}{2006}]{Ray2006}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Ray},~\\bfnm{Shubhankar}\\binits{S.}} \\AND\n\\bauthor{\\bsnm{Mallick},~\\bfnm{Bani}\\binits{B.}}\n(\\byear{2006}).\n\\btitle{Functional clustering by {B}ayesian wavelet methods}.\n\\bjournal{J. R. Stat. Soc. Ser. B. Stat. Methodol.}\n\\bvolume{68}\n\\bpages{305--332}.\n\\bid{doi={10.1111/j.1467-9868.2006.00545.x}, issn={1369-7412}, mr={2188987}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Sam{\\'e} et~al.}{2011}]{Sam2011}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Sam{\\'e}},~\\bfnm{Allou}\\binits{A.}},\n\\bauthor{\\bsnm{Chamroukhi},~\\bfnm{Faicel}\\binits{F.}},\n\\bauthor{\\bsnm{Govaert},~\\bfnm{G{\\'e}rard}\\binits{G.}} \\AND\n\\bauthor{\\bsnm{Aknin},~\\bfnm{Patrice}\\binits{P.}}\n(\\byear{2011}).\n\\btitle{Model-based clustering and segmentation of time series with changes in regime}.\n\\bjournal{Adv. Data Anal. Classif.}\n\\bvolume{5}\n\\bpages{301--321}.\n\\bid{doi={10.1007/s11634-011-0096-5}, issn={1862-5347}, mr={2860103}}\n\\bptnote{check pages}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Schwarz}{1978}]{Schwarz78}\n\\begin{barticle}[mr]\n\\bauthor{\\bsnm{Schwarz},~\\bfnm{Gideon}\\binits{G.}}\n(\\byear{1978}).\n\\btitle{Estimating the dimension of a model}.\n\\bjournal{Ann. Statist.}\n\\bvolume{6}\n\\bpages{461--464}.\n\\bid{issn={0090-5364}, mr={0468014}}\n\\end{barticle}\n\n\n\\bptok{imsref}\n\n\n\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Vogel, Greiser and Mattfeld}{2011}]{Vogel2011b}\n\\begin{barticle}[author]\n\\bauthor{\\bsnm{Vogel},~\\bfnm{P.}\\binits{P.}},\n\\bauthor{\\bsnm{Greiser},~\\bfnm{T.}\\binits{T.}} \\AND\n\\bauthor{\\bsnm{Mattfeld},~\\bfnm{D.~C.}\\binits{D.~C.}}\n(\\byear{2011}).\n\\btitle{Understanding bike-sharing systems using data mining: Exploring activity patterns}.\n\\bjournal{Procedia---Social and Behavioral Sciences}\n\\bvolume{20}\n\\bpages{514--523}.\n\\end{barticle}\n\n\n\\bptok{imsref}\n\\endbibitem\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Vogel and Mattfeld}{2011}]{Vogel2011a}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{Vogel},~\\bfnm{P.}\\binits{P.}} \\AND\n\\bauthor{\\bsnm{Mattfeld},~\\bfnm{D.~C.}\\binits{D.~C.}}\n(\\byear{2011}).\n\\btitle{Strategic and operational planning of bike-sharing systems by data mining---A case study}.\nIn \\bbooktitle{ICCL}\n\\bpages{127--141}.\n\\bpublisher{Springer},\n\\blocation{Berlin}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\\\n\n\n\n\\bibitem[\\protect\\citeauthoryear{Xi et~al.}{2006}]{Xi2006}\n\\begin{binproceedings}[author]\n\\bauthor{\\bsnm{Xi},~\\bfnm{X.}\\binits{X.}},\n\\bauthor{\\bsnm{Keogh},~\\bfnm{E.}\\binits{E.}},\n\\bauthor{\\bsnm{Shelton},~\\bfnm{C.}\\binits{C.}},\n\\bauthor{\\bsnm{Wei},~\\bfnm{L.}\\binits{L.}} \\AND\n\\bauthor{\\bsnm{Ratanamahatana},~\\bfnm{C.~A.}\\binits{C.~A.}}\n(\\byear{2006}).\n\\btitle{Fast time series classification using numerosity reduction}.\nIn \\bbooktitle{23rd International Conference on Machine Learning (ICML 2006)}\n\\bpages{1033--1040}.\n\\end{binproceedings}\n\n\n\\bptok{imsref}\n\\endbibitem\n\\end{thebibliography}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\printaddresses\n\n", "itemtype": "equation", "pos": 38101, "prevtext": "\n\nwhere $\\xi(\\mathcal{M})$ is the number of free parameters of the\nmodel, and $n$ is the number of observations.\nThe value of $\\xi(\\mathcal{M})$ is, of course, specific to the model\nselected by the practitioner (cf. Table~\\ref{Tab:models}).\nAlthough penalized likelihood criteria are widely used, AIC and BIC are\nalso known to be less efficient in practical situations than in\nsimulated cases. In particular, the required regularity conditions are\nnot fully satisfied in the mixture framework [\\citet\n{Lindsay95,Lindsay2008}] and, hence, the criteria might not be appropriate.\n\nTo overcome this drawback, \\citet{birge2007minimal} recently proposed a\ndata-driven technique, called the ``slope heuristic,'' to calibrate the\npenalty involved in penalized criteria. The slope heuristic was first\nproposed in the context of Gaussian homoscedastic least squares\nregression and was then used in different situations, including\nmodel-based clustering. \\citet{birge2007minimal} showed that there\nexists a minimal penalty and that considering a penalty equal to twice\nthis minimal penalty allows for approximating the oracle model in terms\nof risk. The minimal penalty is, in practice, estimated by the slope of\nthe linear part when plotting the log-likelihood $\\ell(\\hat\\theta)$\nwith regard to the number of model parameters (or model dimension). The\ncriterion associated with the slope heuristic is therefore defined by\n\n\n\n", "index": 37, "text": "\\begin{equation}\n\\operatorname{SHC}(\\mathcal{M}) = \\ell(\\hat\\theta) - 2\\hat{s}\\xi (\\mathcal{M}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\operatorname{SHC}(\\mathcal{M})=\\ell(\\hat{\\theta})-2\\hat{s}\\xi(\\mathcal{M}),\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\operatorname</mtext></merror><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mover accent=\"true\"><mi>s</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mi>\u03be</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]